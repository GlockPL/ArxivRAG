{"title": "MarineFormer: A Transformer-based Navigation Policy Model for Collision Avoidance in Marine Environment", "authors": ["Ehsan Kazemi", "Iman Soltani"], "abstract": "In this work, we investigate the problem of Unmanned Surface Vehicle (USV) navigation in a dense marine environment with a high-intensity current flow. The complexities arising from static and dynamic obstacles and the disturbance forces caused by current flow render existing navigation protocols inadequate for ensuring safety and avoiding collisions at sea. To learn a safe and efficient robot policy, we propose a novel methodology that leverages attention mechanisms to capture heterogeneous interactions of the agents with the static and moving obstacles and the flow disturbances from the environment in space and time. In particular, we refine a temporal function with MarineFormer, a Transformer navigation policy for spatially variable Marine environment, trained end-to-end with reinforcement learning (RL). MarineFormer uses foundational spatio-temporal graph attention with transformer architecture to process spatial attention and temporal sequences in an environment that simulates a 2D turbulent marine condition. We propose architectural modifications that improve the stability and learning speed of the recurrent models. The flow velocity estimation, which can be derived from flow simulations or sensors, is incorporated into a model-free RL framework to prevent the robot from entering into high-intensity current flow regions including intense vortices, while potentially leveraging the flow to assist in transportation. The investigated 2D marine environment encompasses flow singularities, including vortices, sinks, and sources, representing fundamental planar flow patterns associated with flood or maritime thunderstorms. Our proposed method is trained with a new reward model to deal with static and dynamic obstacles and disturbances from the current flow. We evaluate the performance of the proposed deep policy versus the state-of-the-art RL methods and the classical planners. Our method outperforms the existing methods, obtaining 20% improvement over the state-of-the-art baseline model for marine navigation. We attribute the superior performance of the proposed method to several factors including the acquisition of critical sensory information from the environment, optimal design of the network architecture, and the reward functions, tailored to the nature of the obstacles and the environmental disturbances.", "sections": [{"title": "I. INTRODUCTION", "content": "The main goal of the Unmanned Surface Vehicle (USV) studied here is to overcome disturbances from current flow, avoid obstacles, and reach the goal using the shortest path from the departure point to the destination. Collision avoidance when employing USVs in a dense environment, involving several static and dynamic obstacles, is a challenging problem [1]. In strong currents characterized by flood dynamics, moving agents are heavily influenced by current flows, leading to complex interactions with both dynamic and static objects. Adherence to the existing protocols [2] for collision avoidance may lead to conflicting actions, particularly in dense marine environments with numerous static and dynamic agents. Maneuvers in marine environment may require deviation from established protocols, especially when interactions with nearby vessels are influenced by strong current forces. In addition, a lack of knowledge of the goal and trajectories of dynamic obstacles presents an additional layer of complexity for the navigation of USVs.\nThe objective of this work is to propose an obstacle avoidance policy for the marine environment as shown in Fig. 2. Compared with the previous works that ignore the presence of either dynamic obstacles [3] or static obstacles [4], our method considers a heterogeneous environment including static obstacles, moving obstacles as well as strong disturbances due to current flow. In a large-scale oceanic environment, variable ocean currents are more prevalent than dense obstacles. In this work, we assume that noisy information about the flow around the agent can be obtained using satellite imagery, sensors mounted on the USVs, or computational fluid dynamic solvers, although the flow field information farther away from the agent might not be available.\nOur method recognizes the heterogeneous nature of underlying dynamical fluid and agent interactions with static and dynamic obstacles. One remarkable characteristic of the marine environment is uncertainty in path planning since the current flow has a significant impact on the navigation trajectory of USVs. Most existing USV navigation approaches overlook the potential benefits of incorporating local current flow observations for trajectory planning, as well as steering and velocity control. Although such measurements are feasible in practice, their exclusion from control and planning reduces flexibility and adaptability, often leading to policy estimation error, unstable trajectories, and an increased risk of collision. In this work, we propose an algorithm that is informed of the local disturbances caused by current flow to enhance the navigation performance of USVs and demonstrate a reduction in the likelihood of collision, travel time, and path length. Our proposed framework integrates the current flow measurements on a grid around the ego-vessel as shown by the blue box in Fig. 1(b). We study how to leverage the information from the local current flow field for the agent to navigate through obstacles while minimally deviating from the optimal trajectory to the goal.\nFor autonomous path planning we use Deep Reinforcement Learning (DRL). In recent years, the transformer architecture [5] has significantly impacted the performance of Supervised Learning (SL) over a variety of tasks from computer vision to natural language processing [6], [7], [8], while demonstrating superior performance over traditional architectures such as RNNs and CNNs. The common practice to deal with temporal dependency in DRL with the underlying partial observability problem is to apply recurrent neural networks (RNNs) [9], [10]. Inspired by the recent success in employing transformers in RL and also the deficiencies of RNNs to capture the long-term temporal dependencies, this study focuses on further exploring the benefits of transformers to RL for autonomous navigation tasks in particular for the challenging problem of marine environments involving dynamic and static obstacles and external forces induced by the current flow. Building upon the previous works on crowd navigation [11], [12], we propose a new architecture, MarineFormer, which uses transformers instead of GRUs to capture the temporal correlations.\nOur approach uses the ability of attention mechanisms to extract temporal correlation embedded in the current flow measurements, as well as the ego-vessel/obstacles interactions. We use spatio-temporal graphs (sp-graph) for embedding the features of the interaction of robots with environmental disturbances as well as obstacles. In the sp-graph, the link between the agent node and other nodes, i.e., other obstacles is learned using attention models. Our approach seeks to learn an interaction-aware navigation policy that reasons different interactions with the current flow and moving as well as static obstacles as shown in Fig. 1(b). The attention mechanism helps the agent determine the relative importance of each obstacle to the robot while considering the overall flow and avoiding strong disturbances due to singularities in the flow dynamics. This architecture enables the robot to pay more attention to the important interactions for decision-making, especially when dealing with a high number of static and moving obstacles in the presence of nonlinear hydrodynamics leading to overly complex graph dynamics. This approach can better encode the interaction of the agent and the environmental components than directly combining the information of other agents through concatenation or using an LSTM encoder [13], [14].\nWe use distance measurements of all static obstacle sensors as observations of the deep policy model. The size of the distance measurements is determined by the number of sensor beams and remains consistent regardless of the number of encountered obstacles. In addition, in our setting the number of dynamic obstacles is fixed and a binary mask is used for the obstacles that are not in the FOV of the robot. The robot's belief of the future trajectory of moving obstacles is used to capture the interaction of the robot and the future trajectory of moving obstacles. It was shown previously [15], [16], [11] that incorporating the interactions across moving obstacles can enhance the performance of learning-based methods for obstacle avoidance. By integrating the information from the subsequently predicted trajectories of moving agents, USV avoids trajectories that might unsafely intersect with those of the dynamic obstacles or can lead to potential collision due to the limited maneuverability of USVs. The partial observation from the local flow is considered as another input feature to the policy model, when transformed into the local coordinate system of the robot. Although DRL has witnessed significant success in recent years, it suffers from the issue of sample efficiency. Using current flow measurement as input can introduce inductive bias which mitigates the sample efficiency problem in DRL. Given the dynamics of the fluid and the presence of moving obstacles, we design a reward function that encourages the robot to balance between avoiding static obstacles and avoiding navigation toward the intended trajectories of dynamic obstacles, while mitigating the risks posed by the attraction forces of sinks. Due to the limited maneuverability of the USV, the reward function proactively discourages the robot from getting too close to obstacles, as avoidance becomes more challenging at short distances.\nThe main contributions of this paper are as follows. 1) We propose a transformer-based policy network that uses an attention mechanism to effectively capture the spatial and temporal interaction of the robot, obstacles, and an environment with spatially variable hydrodynamics. 2) We propose a method to model the partial information from the flow dynamics surrounding the robot to learn a navigation policy. 3) We propose a new approach to fuse information from sensors that detect dynamic and static obstacles to avoid collisions. 4) A reward function is designed to mitigate the effect of disturbances from the current flow and properly shape the behavior of the ego-agent when coming across other agents or obstacles. 5) We provide a training recipe that enables effective end-to-end policy learning in a highly dynamic marine environment. 6) We compare the proposed policy against state-of-the-art RL-based approaches and conventional reaction-based methods. The results show that by using a reward function that takes into account the local current flow measurements, the future trajectory predictions of dynamic agents, and the likelihood of collision with static obstacles, the navigation safety of USVs in a dense environment is significantly improved. In particular, our proposed approach improves the success rate by 20% compared to the state-of-the-art [17]."}, {"title": "II. RELATED WORKS", "content": "Robot navigation in a dynamic environment with external disturbances and obstacles consisting of static and dynamic obstacles has been studied for decades [18], [19], [20], [21]. The path-planning algorithms for USVs can be generally classified into pre-generative, reactive approaches, and learning-based methods. Pre-generative approaches aim to find a safe path avoiding known obstacles and satisfying constraints such as shortest or energy consumption minimization. Previously, researchers have developed diverse geometry-based algorithms such as A* [22], Dijkstra [23], and FMM [24]; and sampling-based algorithms such as RRT [25]. Geometry-based methods can generate an optimal path by selecting a suitable heuristic cost function, while computation time grows exponentially with the map resolution. Comparatively, sampling-based methods have a low computational cost, however, they generate nonoptimal and bumpy paths. Conventional reaction-based methods are extensively used for robot navigation and obstacle avoidance. For instance, Artificial Potential Fields (APF) [26] for robot navigation generated repulsive forces to avoid obstacles. Reciprocal Velocity Obstacles (RVO) [27], [28] and Optimal Reciprocal Collision Avoidance (ORCA) [29], [30] improved the performance of velocity collision avoidance approaches [31] in multi-agent navigation problem by applying velocities following reciprocal obstacle avoidance behaviors. OCRA models other agents as velocity obstacles and assumes that agents avoid each other under the reciprocal rules [27], [29]. In addition, both methods are prone to failure if the assumptions such as the reciprocal rule no longer hold [32].\nLearning-based methods tackle the issues of reaction-based approaches by learning the interaction in the environment and making decisions. One approach is Deep V-learning methods which uses SL with ORCA as the expert and then uses RL to learn a value function for path planning [4], [13], [14], [33]. However, this approach assumes that state transitions of all agents are known and inherit the same problems as the ORCA expert [12]. To address these problems, decentralized structural-RNN (DS-RNN) uses a partial graph to model the interactions between the robot and obstacles. [12]. Model-free RL is used to train the robot policy without supervised learning or assumptions on state transitions, which achieves better results than ORCA. In [14] a deep policy model based on LSTM architecture is proposed to improve the policy performance when the number of agents is increasing. In [14], [15] higher-order interactions among agents are modeled which consider the interaction of ego-agent and other agents and the indirect effects of interaction among other agents on the ego-agents. Further, in [34] an occupancy grid map is used to encode static obstacle information, improving the obstacle avoidance performance in dense environments. A distributional RL-based method for decentralized multi-agent navigation of USVs is developed in [17]. In their work, it is assumed that the state transitions of all dynamic obstacles are identical, as they follow the same policy. Their studied navigation policy does not take into account the dynamics of environmental disturbances or the future trajectories of dynamic obstacles, resulting in shortsighted behaviors of the robots, as illustrated in Fig. 1. Furthermore, the method relies solely on the current flow velocity measurement at the USV's location as a model-free observation, while the action space is discrete and limited, which restricts the generalization of the approach.\nIn this work we study transformers as the backbone of the policy architecture. The attention mechanism in transformers does not require a recurrent context input, which makes it ideal for capturing long-term temporal dependencies. RL generally has not advanced beyond shallow GRU models due to challenges by training instability and long training times because of large number of parameters of large and wide models, such as transformers. Recently, a surge of using transformers has appeared in the RL domain, but it is faced with unique design choices and challenges brought by the nature of RL. In [35], a self-attention framework is leveraged for relational reasoning over state representations. However, in [36], it is shown that vanilla transformers are not suitable for processing temporal sequences and perform worse than a random policy for certain tasks. Gated transformer-XL (GTrXL) [37] proposed to augment causal transformer policies with gating layers with Identity Map Reordering towards stable RL training. Furthermore, [38] suggested a shortcut mechanism with memory vectors for long-term dependency. [39] combined the linear transformer with Fast Weight Programmers for better performance. [36] applied a self-attention mechanism to mimic memory reinstatement for memory-based meta RL. [40] used a transformer with Bellman loss to process observation history as the input of Q-network. Motivated by offline RL, recent efforts have shown that the transformer architecture can directly serve as a model for sequential decisions [41]. Decision transformer [41] learns a policy offline, conditioning on previous states, actions, and future returns, providing a path for using transformers as sequential decision models. There has been a huge effort to improve the scale and training stability of policies. Performer-MPC [42] proposes learnable Model Predictive Control policies with low-rank attention transformers learnable cost functions. In this work, we leverage transformers as temporal encoders. In a similar direction, [43] utilized supervised tasks, while [44], [45] employed pre-trained transformers as temporal encoders. By feeding observations from the current flow, we can mitigate the data efficiency problem often associated with transformers. We demonstrate that transformers outperform GRUs as the memory horizon increases. The problem of poor data efficiency with RL signals can be handled by employing longer episodes, and larger batches in training along with the current flow information.\nIn large-scale oceanic environments, configurations such as current flows are often more prevalent than dense obstacles. For USV path planning, it is essential to model the effects of uncertainties and external disturbances stemming from environmental factors like ocean currents. In [46], path planning that considers ocean currents is formulated as a multi-objective nonlinear optimization problem with generous constraints. [47] introduced an anisotropic GPMP2 method that employs an energy consumption likelihood function for USV motion planning. [48] utilized motion constraints to reduce the sampling space and superimposed ocean currents on vehicle velocity to update new nodes. [49] derived time-optimal trajectories and strategies by defining and solving the level-set equation for the chasing problem in the presence of dynamic environmental disturbances. In [50] comprehensive ocean information and physical constraints are taken into account to shorten time, improve speed, and save energy. In our work, we use only partial information from the current flow on a grid surrounding the robot."}, {"title": "III. METHODOLOGY", "content": "In this section, we formulate robot navigation as a DRL problem and introduce the reward function. A robot is required to operate in a simulated marine environment and navigate through obstacles and towards the goal upon the availability of sensory information about the obstacles in the FOV of sensors and also given information on the local current flow.\nConsider a robot navigating in a marine environment with forces from current flow as disturbances and interacting with obstacles including static and dynamic obstacles. We formulate the RL problem as a Markov Decision Process (MDP), defined by the tuple < S, A, P, R, \u03b3, So >, where S and A are the sets of states and actions of the agent and So is the set of initial states of the robot. P(s'|s, a) is the state transition function that exhibits the dynamics of the environment, which is unknown to the agent due to factors that include the intentions of other agents, noisy sensor measurements, and disturbances from current flow. R(s, a) is the reward function and \u03b3 \u2208 [0, 1) is the discount factor. At each timestep t, given the observation of current state st, the agent chooses an action at, which leads to the transition st+1 ~ P(. |st, at) and the reward rt = R(st,at). The goal of the agent is to maximize the expected return, Rt = \u0395[\u03a3\u03b3^(t)*rt], where \u03b3 is a discount factor. The value function V\u03c0(s) is defined as the expected return starting from the state s, and successively following policy \u03c0. A common approach to finding the optimal policy \u03c0* in to maximize V\u03c0 (s), and the resulting optimal value function V* (s) satisfies the Bellman optimality equation\nV** (s) = E[rt + \u03b3 max_st+1 V** (st+1)|st = s]\nwhere st+1 ~ P(. |s, a) and a ~ \u03c0*(s).\nThe robot is considered to reach the predefined goal if its Euclidean distance to the goal is within the threshold dthres within an episode of length T. In this work, we assume the dynamic obstacles following the policy of ORCA. The agent assumes constant velocity for dynamic agents to predict their future trajectory. For the marine environment, we consider a planar flow, a uniform velocity featuring a random angle of attack and including singularities such as vortices, sinks, and sources. This simulation setup is more challenging compared to [3], [17] as they only consider flow with vortices. We use the partial information from the current flow, e.g., the velocity of current flow in the near vicinity of the agent, as an observation. The policy model should be able to extract the pattern across spatially variable dynamics of flows that are useful for finding the optimal path to the goal.\nAt each timestep t, the policy receives information from the robot's ego-state, including its velocity and location, as well as the goal position, the states of static and dynamic obstacles within the robot's FOV, and the current flow conditions in the vicinity of the vehicle. The observation at timestep t is expressed as St = [Sego, Sstatic, Sdynamic, Scurrent]. Sego shows the robot's information given by\nSt_{ego} = [P_t, V_t, O_t]\nwhere Pt = [px, py], Vt = [vx, vy] are respectively the position and velocity of the robot at timestep t. Ot is the heading angle of the robot at timestep t. Sstatic contains the information related to reflection from static obstacles in the FOV of the robot,\nSt_{static} = [O^{t,1}_{S},..., O^{t,rb}_{S}]\nwhere rb is the number of radial beams emitted from the sensor to detect the static obstacles in the FOV of the sensor and O^{t,i}_{S} is the reflection from i-th beam of the sensor when the robot is in state s(t). If no obstacle is detected in the sensor range in the i-th beam direction, we set O^{ti} to be twice the sensor range. Sdynamic contains information related to the position and velocity of dynamic agents in the FOV of the agent.\nSt_{dynamic} = [O^{t,1}_{d},..., O^{t,dn}_{d}]\nwhere dn is the total number of dynamic obstacles in the environment, O^{t,i}_{d} is the concatenation of the current state of the dynamic obstacle i and the belief of the robot from the future trajectory of agent i, obtained from the trajectory predictor model. In particular,\nO^{t,i}_{d} = [P^{t,i}_{d,x}, P^{t,i}_{d,y}, V^{t,i}_{d,x}, V^{t,i}_{d,y}, P^{t+1:t+K,i}_{d,x}, P^{t+1:t+K,i}_{d,y}]\nwhere Pt = [P^{t,i}_{d,x}, P^{t,i}_{d,y}], Vt = [V^{t,i}_{d,x}, V^{t,i}_{d,y}] are the position and velocity of the i-th dynamic obstacle at timestep t, and P^{t+1:t+K,i}_{d,x} = [P^{t+1:t+K,i}_{d,x}, P^{t+1:t+K,i}_{d,y}], the predicted future trajectory of agent from timestamp t + 1 to t + K. The trajectory prediction can be obtained by simply assuming constant velocity for the dynamic obstacles or using the ground-truth trajectories of agents in distributed centralized multi-agent applications. As we use a fixed-size input for the dynamic obstacles, a binary mask is applied to filter out the dynamic obstacles that are outside the sensor's FOV.\nTo encourage the agent to account for current flow disturbances and avoid regions with strong currents, we provide the policy model with Scurrent, representing the current flow velocity on an m \u00d7 m grid in the robot's ego-coordinate system within a rectangular area surrounding the USV.\nS^{t}_{current} = \\{(v^{t}_{i,x},v^{t}_{i,y})\\}_{i=0}\nB. Network Architecture\nThe network architecture for the policy model is shown in Fig. 2. We drive our network architecture from a spatial-temporal graph. Observations of the ego-robot, the seafloor-relative velocity, static obstacles, and dynamic obstacles are transformed into corresponding latent features through attention models. We represent the attention of the robot to dynamic obstacles (RD), to sensor reflection (RS) and the attention to current flow (RC) functions as feedforward networks attention models. We represent the temporal function as a transformer unit Tr. In Fig. 2, at each timestep t, our spatio-temporal graph Gt = (Vt, et) consists of a set of nodes Vt and a set of edges et. The nodes represent the detected static and dynamic obstacles. The edges connect the robot with different detected obstacles and represent the spatial interaction between the robot and the agents at the same timestamp. These interactions are modeled using feed-forward attention mechanisms. The obstacles that are not in the FOV of the agent do not integrate into the motion planning.\nThe attention modules assign weights to each edge that connects to an agent, allowing it to pay more attention to important interactions including obstacles and disturbances from the flow field. The attention is similar to the scaled dot-product attention and is computed using a query Q and a key K, and applies the normalized score to a value V.\nAttn(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V\nwhere d is the dimension of the queries and keys. For dynamic obstacles, we use binary masks that indicate the visibility of each obstacle to prevent attention to obstacles that are not in the FOV of the agent. To compute the RCt attention at timestep t, the current flow observation Scurrent is transformed to the robot coordinate and scaled using tanh to the interval [-1,1] for stable training. We then apply the convolution layer to obtain the embedding from the flow denoted by Ct. In RCt attention, QRC \u2208 R^{1XdRC} is the linear embedding of the robot states, KRC \u2208 R^{2XdRC} and VRC \u2208 R^{2XdRC} are linear embedding of the feature Ct. The attention score is computed from QRC, KRC and Vre to obtain the weighted current flow features as in Eq. 3.\nInspired by the Bendy Ruler navigation technique [52] in Fig. 3 we compute the similarity scores across the moving obstacles. The Bendy Ruler technique probes around the robot in multiple directions looking for open spaces and then picks the direction that is sufficiently open while also moving the robot towards the goal. The similarity score is computed as\nSim(V) = softmax (VDD^TV^T)\nwhere D is a trainable diagonal weight matrix and V is the linear embedding of the graph related to the dynamic obstacles in the FOV of the robot. This information allows the robot to assign lower navigation priority to areas with dense, cluttered obstacles, where the probability of collision is higher. It is important to note that as the visibility of obstacles changes due to the movement of agents, and external disturbances fluctuate with the spatially varying current flow, the set of nodes Vt and edges et and the parameters of interaction function may change correspondingly. For this purpose, we include the temporal correlation of graph Gt in different timesteps using another function denoted by the transformer module in Fig. 2. The input to the temporal function of navigation policy is composed of the attention graphs, obtained from computing the robot's interaction with the environment. This embedding, denoted by At = [RC, RS, RDt] in the latent space, provides information for long-term decision-making by prioritizing the obstacles and navigation paths. For example, the robot might give a lower priority (lower desirability) to high circulating flow regions of the environment and try to move through the regions that are located near calmer flow conditions. The temporal function links the graphs across consecutive timesteps, overcoming the short-sightedness of reactive methods and enabling the robot to make long-horizon decisions. Our natural goal of using transformers is to use it as a temporal sequence encoder [37], [43]. The latent features from the observations are transformed into corresponding temporal features using this encoder:\nh^l,t = Emb_{0:t} = Tr(h^{l,0}, ..., h^{l,t}),\nwhere l = 1,..., L and Emb0:t represent the embedding of historical from initial observation to current observation. Finally, the hLt is fed to a fully connected layer to obtain the value V (st) and the policy \u03c0(at|st). Following the stream in Fig. 2, the transformer for the temporal function is composed of a stack of L identical layers, denoted as Trl for the l-th layer. Each layer has two sub-layers. The first is a multi-head cross-attention mechanism between hl-1,t and the output of l-th layer from the previous timestep hl,t-1 and the second is a simple, fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, preceded by layer normalization. That is, the output of each sub-layer is x + Sublayer(LayerNorm(x)), where Sublayer(x) is the function implemented by the sub-layer itself. All the L layers produce outputs of the same dimension. Coupled with the residual connections, there is a gradient path that flows from output to input without any transformations. At each timestep, the hl-1,t-1 and hlt are passed to the l \u2013 th layer of transformer for l = 1, . . ., L\nh^{l,t} = Tr'(h^{l,t-1}, h^{l-1,t})\nwhere hl,t-1 is the hidden state of the l-th layer of transformer at time t \u2212 1 and Tr' is the l-th layer of Tr. For t = 0 we assume h*,t-1 is an arbitrarily-obtained input embedding, e.g., the embedding from observation in the RL environment per-timestep. Since hl,-1 is chosen arbitrarily, to ensure it does not introduce a biased gradient to the attention model, we align hl-1 and hl-1,0 by setting the initial feature values for current flow to zero. This guarantees that the starting point does not contribute misleading information to the model's learning process, thereby maintaining unbiased gradient flow through the network and stabilizing the training. We demonstrate in the experiments that our architecture has stability and performance that surpasses the baseline models."}, {"title": "C. Reward Function", "content": "The reward function is defined as follows\nrt =1collision(St)rcollision + 1goal(St)rgoal\n+ 1collision\u22c2nogoal(St)(rpot,t + rdo,t + rso,t\n+ rcg,t + rck,t + rcs,t)\n1 is the indicator function, and rcollision and rgoal are given when the agent in the state st reaches the goal or has a collision with any obstacles, respectively. The indicator function 1collision\u22c2nogoal is defined for states st that are in neither the collision state nor the goal state. In addition, we add a potential-based reward rpot = fstep (distgoal,t-1 \u2013 distgoal,t) to guide the robot to approach the goal, where distgoal,t is the L2 distance between the robot position and goal position at time t. The factor fstep is a hyperparameter that is set during training. Due to the limited maneuverability of USVs with differential drive in marine environments, proximity to obstacles poses a high risk of collision in subsequent steps. To discourage the robot from encroaching the obstacles, we use the rewards rdo,t and rso,t to penalize such intrusions:\nr_{do,t} = min_{i=1,...,dn} min_{k=1,..., K_{do}} (\\frac{1-\\unicode{x1D7D9}_{t+k}^{(collision)}}{2^{k+2}})\nr_{so,t} = min_{i=1,...,rb} min_{k=1,..., K_{so}} (\\frac{1-\\unicode{x1D7D9}_{t+k}^{(collision)}}{2^{k+2}})\n1t+k indicates whether the robot collides with i-th obstacle or in the direction of i-th radial beam of the sensor at the predicted position at the timestep t + k and rcollision is the penalty for collision. We assign different weights to the intrusions at different prediction timesteps, allowing the robot to incur a smaller penalty for intruding into the obstacle zones that are farther in the future. The main difference between rdo,t and rso,t is that for computing do,t we take into account the predicted trajectory of dynamic obstacles for potential collision, while for rso,t we only use the information from the dynamics of the robot to predict a potential intrusion with the zones of static obstacle in the future. In summary, we hypothesize that when faced with complex obstacles that are difficult to avoid given the robot's limited maneuverability, adopting a proactive policy and maintaining distance from those obstacles is the most effective approach. The definition of the rewards rdo,t and rso,t are inspired by the Bendy Ruler algorithm, illustrated in Fig. 3, which probes the area around the robot in multiple directions to identify open spaces while progressing towards the target. This is also similar to the concept of velocity obstacles [31], which identifies the set of velocities leading to a future collision with static and dynamic obstacles. The attraction force from sinks can lead to a stagnation problem where thrusters may be unable to generate enough force to escape, hence, trapping the agent at the center of the sink. To discourage being trapped in the attraction zones, we design the reward component rs to penalize the robot when it fails to move closer to the target by at least a threshold distance d within two consecutive timesteps:\nr_{disp,t} = \\begin{cases}\n    -0.2, & \\text{if } r_{pot,t} < dr, \\\\\n    0, & \\text{otherwise}.\n  \\end{cases}"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we conduct various obstacle avoidance experiments. Our 2D simulation environment simulates a robot navigating in a dense obstacle condition in Fig. 1.\nWe consider two training scenarios as shown in Fig. 1; the first scenario (Fig. 1c) consists of 10 dynamic obstacles and 5 static obstacles. The second scenario (Fig. 1d) consists of 25 dynamic obstacles and 10 static obstacles. The range of radius of static obstacles in Scenario I is chosen randomly from the interval [1.0,1.5]m, and the range of static obstacles for Scenario II is randomly selected from the interval [0.2, 0.3]m. In other words, the size of static obstacles for Scenario I is on average 5 times bigger than Scenario II. All dynamic obstacles are controlled by ORCA and react to other dynamic obstacles, not the robot. This prevents the robot from learning a policy that considers dynamic obstacles yielding to it during the navigation. The radius of dynamic agents is set to 0.3m with the preferred velocity of 2m/s and FOV of 2\u03c0rad. We assume the dynamic obstacles occasionally are changing their goals to other random positions within an episode and current flows do not impact the trajectory of dynamic obstacles. Our environment is denser concerning the number of obstacles compared to previous work [3], [17] as we consider 10 static obstacles and 25 dynamic obstacles in a rectangle of size 40m \u00d7 40m versus 8 static obstacles in an environment of size 50m\u00d750m in their experiments. Therefore, our simulated environment sets forth a more challenging collision avoidance problem and upon successful training leads to a more interactive navigation policy.\nThe simulated current flow is generated from the interaction of uniform flow, vortices, sinks, and sources. The arrows as shown in Fig. 1 denote the velocity vector of current data at the start position of each arrow. We apply the interpolation between two adjacent nodes to compute the current flow at each point. In each episode, the starting and the goal position of the robot, the location of obstacles, the angle of attack for the uniform flow, and the distribution of flow singularities, i.e., vortices, sinks, and sources, are randomly initialized. The uniform flow has a magnitude of Vinf = 1m/s with a random angle of attack chosen from the interval [0, \u03c0/4] rad. The velocity of the current flow is simulated using potential flow techniques with vortices [53]. For each environment, we generate 4 vortices 4 sinks, and sources. The intensity of vortices, sinks, and sources are chosen randomly from the interval [5\u03c0, 10\u03c0] rad. The centers of flow singularities are chosen so that the distance of each paired singularity is greater than the sum of the radius corresponding to the velocity threshold of 1m/s at the interaction point. These randomly generated flow singularities pose extra challenges for learning a policy that can generalize to varying current disturbances."}, {"title": "B. Kinematic of USV", "content": "The kinematic model in [54", "\u03c0": "S \u2192 (\u2206\u03c5\u03c4", "0.1,0.1": "m/s. Similarly"}, {"0.1,0.1": "rad. The steering speed is clipped to [0", "2m/s": "at the inference time. During training we allow the velocity to become negative but we add a penalty for generating a negative velocity. During the inference time", "equations": "nv_s[t+1", "v_s[t": "Delta v_a\\\\\n\\theta_s[t+1", "t": "Delta \\theta_a\nand therefore the steering velocity Vs(t + 1) is obtained as follows\nVs(t+1) = [vs[t+1", "t+1": "vs[t+1"}]}