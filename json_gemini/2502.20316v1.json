{"title": "Multi-Scale Neighborhood Occupancy Masked Autoencoder for Self-Supervised Learning in LiDAR Point Clouds", "authors": ["Mohamed Abdelsamad", "Michael Ulrich", "Claudius Gl\u00e4ser", "Abhinav Valada"], "abstract": "Masked autoencoders (MAE) have shown tremendous potential for self-supervised learning (SSL) in vision and beyond. However, point clouds from LiDARs used in automated driving are particularly challenging for MAEs since large areas of the 3D volume are empty. Consequently, existing work suffers from leaking occupancy information into the decoder and has significant computational complexity, thereby limiting the SSL pre-training to only 2D bird's eye view encoders in practice. In this work, we propose the novel neighborhood occupancy MAE (NOMAE) that overcomes the aforementioned challenges by employing masked occupancy reconstruction only in the neighborhood of non-masked voxels. We incorporate voxel masking and occupancy reconstruction at multiple scales with our proposed hierarchical mask generation technique to capture features of objects of different sizes in the point cloud. NOMAEs are extremely flexible and can be directly employed for SSL in existing 3D architectures. We perform extensive evaluations on the nuScenes and Waymo Open datasets for the downstream perception tasks of semantic segmentation and 3D object detection, comparing with both discriminative and generative SSL methods. The results demonstrate that NOMAE sets the new state-of-the-art on multiple benchmarks for multiple point cloud perception tasks.", "sections": [{"title": "1. Introduction", "content": "Sensors that generate point clouds, such as LiDARs or radars, have become a cornerstone in automated driving as they provide high-resolution three-dimensional representations of the environment [29]. The rich spatial information represented in point clouds enables vehicles to accurately detect and classify objects, navigate complex environments, and enhance safety through real-time situational awareness. However, annotated point cloud datasets are significantly smaller than their image-based counterparts, which makes learning large-scale perception models extremely challenging. Self-supervised learning (SSL) [2, 5\u20137, 14-17, 21, 22, 32], through contrastive learning or masked modeling, provides an effective solution to this problem by learning meaningful representations from vast amounts of unlabeled data. SSL also reduces the reliance on arduous annotation processes while improving performance and generalization. Pioneering works [24, 27, 45, 47] have successfully employed SSL to small-scale indoor point clouds, with more recent efforts extending it to large-scale outdoor point clouds [18, 33, 40].\nOutdoor point clouds, however, pose a unique challenge for masked modeling as most of the measured 3D volume is empty space. Current approaches resort to reconstructing precise point locations within occupied voxels [18, 33, 40], but this often leaks information to the decoder, signaling that the queried voxel is occupied. Recent methods [26, 43] attempt to overcome this problem by reconstructing the entire scene, but the computational complexity and class imbalance caused by the large number of empty voxels limit these approaches to either 2D bird's-eye-view representations or coarse-grained 3D reconstructions."}, {"title": "2. Related Work", "content": "In this section, we review the existing works related on point cloud SSL and Automotive LiDAR SSL.\n3D Self-Supervised Learning: The success of generative self-supervised learning in natural language processing and computer vision has inspired several works [24, 27, 34, 46] to explore masked auto-encoders for 3D point clouds. These approaches are typically tailored towards small-scale single object recognition tasks, where a standard ViT [13] architecture suffices to encode the point cloud. For example, PointMAE [27] explicitly reconstructs point cloud patches using the Chamfer distance. MaskPoint [24] discriminates between the reconstructed points and noise points. In OcCo [34], point cloud completion is performed on occluded regions. Most prominently, Point2Vec [46] reconstructs the encoded features of a teacher model for the masked patches.\nAlternatively, contrastive methods can be employed with point clouds to distinguish multiple partial views [39] or point-level correspondences [19]. These pioneering works achieve promising results on object-scale and room-scale 3D point clouds but are not usable for large-scale automotive LiDAR point clouds due to their inefficient scaling with the size of the point cloud. In contrast, our work focuses on large-scale automotive LiDAR point clouds and employs efficient hierarchical architectures. Additionally, our work proposes self-supervision for multiple feature levels, contrary to the single-scale supervision employed in these works.\nAutomotive LiDAR Self-Supervised Learning: The focus of existing works on automotive large-scale point clouds is computational efficiency. One common technique is to reduce 3D point clouds to a 2D bird's-eye-view (BEV) grid, using pillar architectures [3, 18, 33, 40, 43]. [18] reconstructs 3D points inside masked 2D pillars and [40] additionally predicts the order of the 2D BEV pillars. GeoMAE [33] predicts centroids and 3D sub-occupancy in the pretraining. GD-MAE [43] utilizes a generative decoder to reconstruct the whole scene to alleviate the leakage of positional information to the decoder. ALSO [3] reconstructs the surface occupancy of the point cloud. UniPAD [44] is a pioneering work that generates coarse 3D features of the whole scene and uses a neural rendering approach for supervision. Occupancy-MAE [26] proposes to utilize the occupancy as a compressed representation of the point cloud and reconstruct the occupancy in the 3D space using a masked point cloud. Despite the significant performance achieved by them, reconstructing occupancy or features over the entire 3D volume is an expensive task, which allows supervision only on a coarse-scale. Furthermore, the large computational cost prohibits employing modern 3D scene understanding architectures with high voxel resolutions. As a result, the state of the art for self-supervised learning lags behind the performance of a fully supervised training of transformer architectures from scratch. In contrast, this paper addresses fully sparse SSL as a remedy, scaling well with higher 3D voxel resolutions."}, {"title": "3. Technical Approach", "content": "Fig. 2 presents an overview of our proposed framework for self-supervised representation learning. The network consists of an encoder (to be trained), a token upsampling module, and multiple decoders for the hierarchical masked voxel reconstruction. We employ PTv3 [37] as the encoder. In contrast to earlier works, we maintain a sparse feature space and generate fine-grained features only for the visible voxels using an upsampling module. We employ a sparse decoder for masked voxel reconstruction. This decoder is designed to be simple and lightweight, as described in Sec. 3.2, allowing us to deploy a separate decoder instance at each feature scale in the multi-scale pretext (MSP), which is further detailed in Sec. 3.3. The input point clouds are first voxelized and then masked before being fed into the encoder. Our masking strategy ensures that there is adequate masking coverage while also maintaining a sufficient number of occupied voxels in the reconstructed neighborhoods at multiple hierarchical scales, as explained in Sec. 3.4.\n3.1. Encoder and Token Upsampling\nThis input point cloud $P$ is first voxelized to obtain the set of all occupied voxels $V$, which is then split into a set of visible voxels $V_v$ and masked voxels $V_m$, as detailed in Sec. 3.4. A sparse transformer encoder $E$ based on PTv3 [37] is used to encode the features $V$ at the positions of $V_v$ to generate the set of tokens\n$F(s) = E(V)^{(s)}.$ (1)\nPTv3 employs partition-based pooling on the tokens to generate more abstract representations for coarser resolutions, similar to pooling in CNNs. $F(s)$ is the features (tokens) at the s-th scale level of PTv3 and $s \\in \\{0, ..., S \u2013 1\\}$.\nNOMAE uses an upsampling module $M_u$ to propagate the abstract encoding of coarser resolution tokens to the tokens of finer resolution while keeping the representation sparse. This is similar to a feature pyramid network for CNNs. $M_u$ consists of a single PTv3 transformer block at every scale, which is very lightweight.\n3.2. Neighboring Decoder\nPrior work on self-supervised learning for large-scale point clouds reconstruct exact locations of points [18], geometrical properties [33] or voxel ordering [40], for each masked voxel $V_m$. This requires passing $V_m$ to the decoder, causing information leakage. [26, 43] avoid this well-known information leakage by reconstructing the scene as a whole, which is computationally expensive. In contrast, NOMAE reconstructs the occupancy $O(v_n, s)$ of all voxels $v_n \\in V_n$ within a certain neighborhood $n$ of visible voxels $V_v$ at scale $s$. To achieve this, we employ a decoder $D_s$ consisting of sparse convolution layers.\n$\\tilde{O}(v_n, s) = D_s(M_u(F)^{(s)}),$ (2)\nwhere $O$ is the networks prediction of $O$. Visible voxels of $V_v$ are excluded from $V_n$. An example is depicted in Fig. 3. We note that $V_n$ will not cover masked voxels in $V_m$ that are not nearby visible voxels. This is an approximation that we make in our approach and we observed that LiDAR points in outdoor point clouds typically lie in the proximity of other points on the surfaces of objects. Hence, sufficient number of masked voxels in $V_m$ are included in $V_n$. Far-away isolated points do not contribute to the loss, which improves performance, as evaluated in the ablation study presented in Sec. 4.6. Our interpretation is that such isolated points belong to strongly occluded or masked objects and are infeasible to reconstruct, hence affecting the pretraining. This approximation allows us to avoid reconstructing large volumes of unoccupied space without leaking information about $V_m$ to the decoder $D_s$ at the same time.\n3.3. Multi-Scale Pretext\nGeoMAE [33] is the only prior work that exploits multiple hierarchical scales in the reconstruction during self-supervised training. Most other works [3, 18, 26, 33, 43] use a single scale for their pretraining tasks. This is unexpected since it is common practice in automated driving perception models to attach task heads to feature representations at different scales. The intuition is that finer resolutions are more suitable for small objects, such as pedestrians, while coarser resolutions are more suitable for larger objects, such as trucks. However, the multi-scale reconstruction in GeoMAE [33] is derived from a single feature map. In our approach, we use s instances $D_s$ of the decoder architecture ID with separate weights. Coupled with the neighborhood size being scale-dependent, this implicitly encourages the coarse-grained features to contain information from a larger area, while the fine-grained features contain more localized details.\n3.4. Hierarchal Mask Generator\nGenerally, the scale of the masking can be different from the scale of reconstruction. For example, a random mask can be generated on a coarse scale and then upsampled to match the resolution of the reconstruction. However, experiments in [18, 33, 43] showed that random masking on the same scale as the reconstruction scale performs best, which is intuitive. Consequently, we generate random masks for multiple scales in the MSP. These masks should be consistent to avoid information leakage, i.e., a voxel that is masked on a coarser scale should not be visible on a finer scale [47]. A straightforward manner to generate consistent masks would be to create a random mask for the finest scale and then derive the masks of coarser scales by defining a coarse-scale voxel as masked if all its corresponding fine-scale sub-voxels are masked. However, this would lead to rapidly decreasing masking ratios when moving to coarser scales because a single visible sub-voxel is sufficient to mask a coarser-scale voxel visible. Another alternative is masking at the coarsest scale and upsampling the mask [47] to finer scales which leads to a more consistent masking ratio across scales. However, we found that this approach rapidly decreased the size of reconstructed neighborhoods n moving to finer scales.\nHence, we propose the masking scheme depicted in Fig. 3a. We mask the coarsest scale first, using a random sampling of all occupied voxels $V$, and the probability that a voxel is masked equals masking ratio r. Next, we take all voxels at scale s - 1 within visible voxels of the previous, coarser, scale s and repeat the sampling of additional masked voxels at scale s - 1 with probability r. Consequently, the total masking ratio at scale $r_t(s)$ is approximately\n$r_t(s) \\approx 1 \u2013 (1 - r)^{S-s+1}.$ (3)\nBy doing so, we ensure that coarser scales have a sufficient number of masked voxels without reducing the size of reconstructed neighborhoods at finer scales. Only the visible voxels $V_v$ of the finest scale are fed to the encoder backbone $E$. An ablation study presented in Sec. 4.6 quantifies the positive effect of HMG on the pretraining and downstream task performance.\n3.5. Pretrainging Loss\nWe use the Binary Cross Entropy loss (BCE) as the occupancy loss per scale. The final loss $L$ is the average of all single scale losses $L^{(s)}$:\n$L = \\frac{1}{S} \\sum_{s=0}^{S-1} \\frac{1}{|V_n(s)|} \\sum_{V \\in V_n(s)} BCE (\\tilde{O}(v, s), O(v, s)),$ (4)\nwith the ground truth occupancy $O(v, s) \\in \\{0, 1\\}$."}, {"title": "4. Experiments", "content": "In this section, we discuss the datasets, metrics, and the evaluation protocol that we use for benchmarking. We compare our proposed approach with state-of-the-art methods in the benchmarks and present extensive ablation studies to demonstrate the novelty of our contributions.\n4.1. Datasets and Evaluation Metrics\nThe nuScenes dataset [4] is a challenging dataset due to the sparsity of the LiDAR point cloud. It consists of 700 driving sequences for training, 150 for validation, and 150 for testing, with annotations for a variety of tasks. We evaluate both semantic segmentation and object detection on the nuScenes dataset. We use the mean intersection over union (mIoU) as the main evaluation metric for semantic segmentation and the nuScenes detection score (NDS) and mean average precision (mAP) for 3D object detection.\nThe Waymo Open Dataset [30] is a large-scale autonomous driving dataset. It consists of 798 driving sequences for training, 202 validation sequences, and 150 test sequences. We use the mean intersection over union (mIoU) and mean accuracy (mAcc) as the main metrics for evaluating the semantic segmentation performance.\n4.2. Task Heads\nThis section discusses the methods to evaluate the effectiveness of the pretraining and the quality of the learned representation."}, {"title": "4.3. Implementation Details", "content": "We perform the experiments for semantic segmentation in the Pointcept [11] framework and in the MMDetection3D [10] framework for the object detection task. We use PTv3 [37] as the encoder E, unless stated otherwise. We use a single NVIDIA A100 GPU for the pretraining. We use S = 4 for the reconstruction and masking scales, corresponding to target voxel sizes of {0.05, 0.10, 0.20, 0.40} meters. The input voxel dimension is 0.05 meters. The masking ratio rs of the finest scale is 70% for nuScenes and 85% for Waymo. In the self-supervised pre-training, the decoder D consists of sparse convolution layers [12] of kernel size 5, followed by a single sparse submanifold convolution layer to generate O. We use common augmentation techniques such as rotation, scaling, and jittering from the semantic segmentation literature [9, 31, 37] during pretraining."}, {"title": "4.4. Benchmarking Results", "content": "In this section, we present the benchmarking results for both 3D semantic segmentation and 3D object detection.\n3D Semantic Segmentation: Tab. 1 summarizes the best-performing methods on the nuScenes and Waymo Open Dataset leaderboards for the LiDAR semantic segmentation task. The results include the most performant self-supervised pretraining methods. We fine-tuned our model as described in Sec. 4.2. We observe that our proposed self-supervised pretraining achieves a mIoU score of 81.8 on the nuScenes validation set, adding 1.4 mIoU points over the baseline and setting the state-of-the-art. The results on the nuScenes semantic segmentation test set are on par with the strong PTv3 baseline for the mIoU score and outperforms it in the frequency-weighted IoU (fwIoU) score by 0.4%. The improvement on the test set is lesser than the validation dataset because NOMAE has relatively poor performance for the minority class bicycle, and we did not make special adaptations for the test set submission.\nOn the Waymo Open dataset val set, our method achieves a mIoU score of 72.3 and mAcc of 75.2. NOMAE improves by 1.0 and 2.0 points in the mIoU and mAcc, respectively, over the baseline PTv3 [37]. The current version of the semantic segmentation challenge is relatively new for the Waymo Open dataset, with only a few submissions. With 70.3% mIoU score on the test set, NOMAE sets a new state-of-the-art on the Waymo Open Dataset single frame semantic segmentation challenge.\n3D Object Detection: We present results for object detection on the nuScenes validation set using the fine-tuning approach described in Sec. 4.2. We follow the experiment setup of GD-MAE [43] and UniPAD [44], utilizing only 20% of the annotated frames during fine-tuning, without the use of CBGS [48] or Copy-and-Paste[42] augmentation. We adopt the same training settings as the baseline UVTR [23] and do not use test-time augmentation or model ensembling.\nTab. 2 shows that our approach achieves 60.9 and 54.4 NDS and mAP scores respectively, improving by 14.2 in NDS points and 15.4 in mAP points over the UVTR-L [37] baseline, and by 5.1 NDS points and 5.6 mAP over the closest contrastive SSL method Learning-from-2D [44]. The significant improvement demonstrates the effectiveness of our proposed localized multi-scale SSL for 3D object detection with limited annotated data."}, {"title": "4.5. Comparison with SSL Methods", "content": "In this experiment, we compare the performance of our proposed method with the self-supervised pretraining methods of Occupancy-MAE [26] and GeoMAE [33], with the same encoder architecture of PTv3 [37]. Tab. 1 shows that our reimplementation of Occupancy-MAE [26] and GeoMAE [33] with the state-of-the-art PTv3 [37] backbones (marked with *) outperforms the results reported in the original papers by 0.3 and 7.1 mIoU points respectively for semantic segmentation with fine-tuning on the nuScenes dataset.\nTab. 3 shows the results of non-linear probing (NonLP)."}, {"title": "4.6. Ablation Study", "content": "In this section, we present ablation studies on the nuScenes semantic segmentation validation set to investigate the design choices of the proposed method. We performed the experiments using the pre-trained frozen encoder using NonLP. Please refer to Sec. 4.2 for further details.\nDetailed Study of NOMAE: This experiment evaluates the improvement due to our proposed contributions, and the results are presented in Tab. 4. We start from our implementation of Occupancy-MAE [26] as in Tab. 3. Reconstructing only the local neighborhood Vn of visible voxels Vy increases the NonLP mIoU from 59.0% to 66.7%. This requires replacing the Occupancy-MAE decoder with our proposed upsampling module and neighborhood decoder. Adding the multi-scale reconstruction (MSP) from Sec. 3.3 further improves the mIoU to 70.1 for naive mask construction and to 70.5 for masking strategy from Point-M2AE [47], as opposed to single-scale reconstruction in Occupancy-MAE. Our proposed hierarchical mask generation from Sec. 3.4 yields an improvement of 72.46 mIoU, as further investigated in Sec. 4.6. Moreover, increasing the reconstruction neighborhood size from 5 to 9 improves the mIoU to 74.8, as further investigated in Sec. 4.6. Reducing the batch size to 8 (further investigated in Sec. 6.1 of the supplementary material) yields the final NonLP performance of 74.8% and fine-tuning performance of 81.8% mIoU score.\nMask Block Size, MSP and HMG: This experiment investigates different reconstruction and mask scales s for our SSL task."}, {"title": "5. Conclusion", "content": "In this work, we proposed NOMAE, a novel multi-scale self-supervised learning framework for large-scale point clouds. Observing the large-scale nature of LiDAR point clouds, NOMAE reconstructs only local neighborhoods, keeping the computation tractable at higher voxel resolutions, avoiding information leakage, and learning a localized representation suitable for diverse downstream perception tasks. Enabled by its efficiency, NOMAE utilizes multiple scales in the pre-training, enabling the model to learn both coarse and fine representations. A novel hierarchical mask generation scheme balances the pre-training of coarse and fine features, which is important for objects of different sizes, such as pedestrians and trucks We presented experimental results that underline the benefit of our proposed contributions, achieving state-of-the-art performance on multiple benchmarks.\nLimitations: NOMAE is sensitive to the density of the point cloud and future work will investigate the proposed method for sparse 3D sensors such as radar. Additionally, NOMAE does not utilize the temporal nature of LiDAR data which can open the door for further performance improvement. Furthermore, the application of high-resolution sparse 3D representations in the encoders should be further investigated for the object detection task, where 2D bird's eye view and low-resolution 3D approaches are still dominant."}]}