{"title": "Multi-Scale Neighborhood Occupancy Masked Autoencoder for Self-Supervised Learning in LiDAR Point Clouds", "authors": ["Mohamed Abdelsamad", "Michael Ulrich", "Claudius Gl\u00e4ser", "Abhinav Valada"], "abstract": "Masked autoencoders (MAE) have shown tremendous potential for self-supervised learning (SSL) in vision and beyond. However, point clouds from LiDARs used in automated driving are particularly challenging for MAEs since large areas of the 3D volume are empty. Consequently, existing work suffers from leaking occupancy information into the decoder and has significant computational complexity, thereby limiting the SSL pre-training to only 2D bird's eye view encoders in practice. In this work, we propose the novel neighborhood occupancy MAE (NOMAE) that overcomes the aforementioned challenges by employing masked occupancy reconstruction only in the neighborhood of non-masked voxels. We incorporate voxel masking and occupancy reconstruction at multiple scales with our proposed hierarchical mask generation technique to capture features of objects of different sizes in the point cloud. NOMAEs are extremely flexible and can be directly employed for SSL in existing 3D architectures. We perform extensive evaluations on the nuScenes and Waymo Open datasets for the downstream perception tasks of semantic segmentation and 3D object detection, comparing with both discriminative and generative SSL methods. The results demonstrate that NOMAE sets the new state-of-the-art on multiple benchmarks for multiple point cloud perception tasks.", "sections": [{"title": "1. Introduction", "content": "Sensors that generate point clouds, such as LiDARs or radars, have become a cornerstone in automated driving as they provide high-resolution three-dimensional representations of the environment [29]. The rich spatial information represented in point clouds enables vehicles to accurately detect and classify objects, navigate complex environments, and enhance safety through real-time situational awareness. However, annotated point cloud datasets are significantly smaller than their image-based counterparts, which makes learning large-scale perception models extremely challenging. Self-supervised learning (SSL) [2, 5\u20137, 14-17, 21, 22, 32], through contrastive learning or masked modeling, provides an effective solution to this problem by learning meaningful representations from vast amounts of unlabeled data. SSL also reduces the reliance on arduous annotation processes while improving performance and generalization. Pioneering works [24, 27, 45, 47] have successfully employed SSL to small-scale indoor point clouds, with more recent efforts extending it to large-scale outdoor point clouds [18, 33, 40].\nOutdoor point clouds, however, pose a unique challenge for masked modeling as most of the measured 3D volume is empty space. Current approaches resort to reconstructing precise point locations within occupied voxels [18, 33, 40], but this often leaks information to the decoder, signaling that the queried voxel is occupied. Recent methods [26, 43] attempt to overcome this problem by reconstructing the entire scene, but the computational complexity and class imbalance caused by the large number of empty voxels limit these approaches to either 2D bird's-eye-view representations or coarse-grained 3D reconstructions."}, {"title": "2. Related Work", "content": "In this section, we review the existing works related on point cloud SSL and Automotive LiDAR SSL.\n3D Self-Supervised Learning: The success of generative self-supervised learning in natural language processing and computer vision has inspired several works [24, 27, 34, 46] to explore masked auto-encoders for 3D point clouds. These approaches are typically tailored towards small-scale single object recognition tasks, where a standard ViT [13] architecture suffices to encode the point cloud. For example, PointMAE [27] explicitly reconstructs point cloud patches using the Chamfer distance. MaskPoint [24] discriminates between the reconstructed points and noise points. In OcCo [34], point cloud completion is performed on occluded regions. Most prominently, Point2Vec [46] reconstructs the encoded features of a teacher model for the masked patches.\nAlternatively, contrastive methods can be employed with point clouds to distinguish multiple partial views [39] or point-level correspondences [19]. These pioneering works achieve promising results on object-scale and room-scale 3D point clouds but are not usable for large-scale automotive LiDAR point clouds due to their inefficient scaling with the size of the point cloud. In contrast, our work focuses on large-scale automotive LiDAR point clouds and employs efficient hierarchical architectures. Additionally, our work proposes self-supervision for multiple feature levels, contrary to the single-scale supervision employed in these works.\nAutomotive LiDAR Self-Supervised Learning: The focus of existing works on automotive large-scale point clouds is computational efficiency. One common technique is to reduce 3D point clouds to a 2D bird's-eye-view (BEV) grid, using pillar architectures [3, 18, 33, 40, 43]. [18] reconstructs 3D points inside masked 2D pillars and [40] additionally predicts the order of the 2D BEV pillars. GeoMAE [33] predicts centroids and 3D sub-occupancy in the pretraining. GD-MAE [43] utilizes a generative decoder to reconstruct the whole scene to alleviate the leakage of positional information to the decoder. ALSO [3] reconstructs the surface occupancy of the point cloud. UniPAD [44] is a pioneering work that generates coarse 3D features of the whole scene and uses a neural rendering approach for supervision. Occupancy-MAE [26] proposes to utilize the occupancy as a compressed representation of the point cloud and reconstruct the occupancy in the 3D space using a masked point cloud. Despite the significant performance achieved by them, reconstructing occupancy or features over the entire 3D volume is an expensive task, which allows supervision only on a coarse-scale. Furthermore, the large computational cost prohibits employing modern 3D scene understanding architectures with high voxel resolutions. As a result, the state of the art for self-supervised learning lags behind the performance of a fully supervised training of transformer architectures from scratch. In contrast, this paper addresses fully sparse SSL as a remedy, scaling well with higher 3D voxel resolutions."}, {"title": "3. Technical Approach", "content": "Fig. 2 presents an overview of our proposed framework for self-supervised representation learning. The network consists of an encoder (to be trained), a token upsampling module, and multiple decoders for the hierarchical masked voxel reconstruction. We employ PTv3 [37] as the encoder. In contrast to earlier works, we maintain a sparse feature space and generate fine-grained features only for the visible voxels using an upsampling module. We employ a sparse decoder for masked voxel reconstruction. This decoder is designed to be simple and lightweight, as described in Sec. 3.2, allowing us to deploy a separate decoder instance at each feature scale in the multi-scale pretext (MSP), which is further detailed in Sec. 3.3. The input point clouds are first voxelized and then masked before being fed into the encoder.\nOur masking strategy ensures that there is adequate masking coverage while also maintaining a sufficient number of occupied voxels in the reconstructed neighborhoods at multiple hierarchical scales, as explained in Sec. 3.4."}, {"title": "3.1. Encoder and Token Upsampling", "content": "This input point cloud P is first voxelized to obtain the set of all occupied voxels V, which is then split into a set of visible voxels Vv and masked voxels Vm, as detailed in Sec. 3.4. A sparse transformer encoder E based on PTv3 [37] is used to encode the features V at the positions of Vv to generate the set of tokens\nF(s) = E(V)(s).\nPTv3 employs partition-based pooling on the tokens to generate more abstract representations for coarser resolutions, similar to pooling in CNNs. F(s) is the features (tokens) at the s-th scale level of PTv3 and s \u2208 {0, ..., S \u2013 1}.\nNOMAE uses an upsampling module Mu to propagate the abstract encoding of coarser resolution tokens to the tokens of finer resolution while keeping the representation sparse. This is similar to a feature pyramid network for CNNs. Mu consists of a single PTv3 transformer block at every scale, which is very lightweight."}, {"title": "3.2. Neighboring Decoder", "content": "Prior work on self-supervised learning for large-scale point clouds reconstruct exact locations of points [18], geometrical properties [33] or voxel ordering [40], for each masked voxel Vm. This requires passing Vm to the decoder, causing information leakage. [26, 43] avoid this well-known information leakage by reconstructing the scene as a whole, which is computationally expensive. In contrast, NOMAE reconstructs the occupancy O(vn, s) of all voxels un \u2208 Vn within a certain neighborhood n of visible voxels V at scale s. To achieve this, we employ a decoder Ds consisting of sparse convolution layers.\n\u00d5(vn, s) = Ds(Mu(F)(s)),\nwhere O is the networks prediction of O. Visible voxels of Vv are excluded from Vn. An example is depicted in Fig. 3.\nWe note that Vn will not cover masked voxels in Vm that are not nearby visible voxels. This is an approximation that we make in our approach and we observed that LiDAR points in outdoor point clouds typically lie in the proximity of other points on the surfaces of objects. Hence, sufficient number of masked voxels in Vm are included in Vn. Far-away isolated points do not contribute to the loss, which improves performance, as evaluated in the ablation study presented in Sec. 4.6. Our interpretation is that such isolated points belong to strongly occluded or masked objects and are infeasible to reconstruct, hence affecting the pretraining. This approximation allows us to avoid reconstructing large volumes of unoccupied space without leaking information about Vm to the decoder Ds at the same time."}, {"title": "3.3. Multi-Scale Pretext", "content": "GeoMAE [33] is the only prior work that exploits multiple hierarchical scales in the reconstruction during self-supervised training. Most other works [3, 18, 26, 33, 43] use a single scale for their pretraining tasks. This is unexpected since it is common practice in automated driving perception models to attach task heads to feature representations at different scales. The intuition is that finer resolutions are more suitable for small objects, such as pedestrians, while coarser resolutions are more suitable for larger objects, such as trucks. However, the multi-scale reconstruction in GeoMAE [33] is derived from a single feature map. In our approach, we use s instances Ds of the decoder architecture ID with separate weights. Coupled with the neighborhood size being scale-dependent, this implicitly encourages the coarse-grained features to contain information from a larger area, while the fine-grained features contain more localized details."}, {"title": "3.4. Hierarchal Mask Generator", "content": "Generally, the scale of the masking can be different from the scale of reconstruction. For example, a random mask can be generated on a coarse scale and then upsampled to match the resolution of the reconstruction. However, experiments in [18, 33, 43] showed that random masking on the same scale as the reconstruction scale performs best, which is intuitive. Consequently, we generate random masks for multiple scales in the MSP. These masks should be consistent to avoid information leakage, i.e., a voxel that is masked on a coarser scale should not be visible on a finer scale [47]. A straightforward manner to generate consistent masks would be to create a random mask for the finest scale and then derive the masks of coarser scales by defining a coarse-scale voxel as masked if all its corresponding fine-scale sub-voxels are masked. However, this would lead to rapidly decreasing masking ratios when moving to coarser scales because a single visible sub-voxel is sufficient to mask a coarser-scale voxel visible. Another alternative is masking at the coarsest scale and upsampling the mask [47] to finer scales which leads to a more consistent masking ratio across scales. However, we found that this approach rapidly decreased the size of reconstructed neighborhoods n moving to finer scales.\nHence, we propose the masking scheme depicted in Fig. 3a. We mask the coarsest scale first, using a random sampling of all occupied voxels V, and the probability that a voxel is masked equals masking ratio r. Next, we take all voxels at scale s \u2013 1 within visible voxels of the previous, coarser, scale s and repeat the sampling of additional masked voxels at scale s \u2013 1 with probability r. Consequently, the total masking ratio at scale rt(s) is approximately\nrt(s) \u2248 1 \u2013 (1 - r)S-s+1.\nBy doing so, we ensure that coarser scales have a sufficient number of masked voxels without reducing the size of reconstructed neighborhoods at finer scales. Only the visible voxels Vv of the finest scale are fed to the encoder backbone E. An ablation study presented in Sec. 4.6 quantifies the positive effect of HMG on the pretraining and downstream task performance."}, {"title": "3.5. Pretrainging Loss", "content": "We use the Binary Cross Entropy loss (BCE) as the occupancy loss per scale. The final loss L is the average of all single scale losses L(s):\nL = 1/S \u2211s=0S-1 1/|Vn(s)| \u2211v\u2208Vn(s) BCE(\u00d5(v, s), O(v, s)),\nwith the ground truth occupancy O(v, s) \u2208 {0,1}."}, {"title": "4. Experiments", "content": "In this section, we discuss the datasets, metrics, and the evaluation protocol that we use for benchmarking. We compare our proposed approach with state-of-the-art methods in the benchmarks and present extensive ablation studies to demonstrate the novelty of our contributions."}, {"title": "4.1. Datasets and Evaluation Metrics", "content": "The nuScenes dataset [4] is a challenging dataset due to the sparsity of the LiDAR point cloud. It consists of 700 driving sequences for training, 150 for validation, and 150 for testing, with annotations for a variety of tasks. We evaluate both semantic segmentation and object detection on the nuScenes dataset. We use the mean intersection over union (mIoU) as the main evaluation metric for semantic segmentation and the nuScenes detection score (NDS) and mean average precision (mAP) for 3D object detection.\nThe Waymo Open Dataset [30] is a large-scale autonomous driving dataset. It consists of 798 driving sequences for training, 202 validation sequences, and 150 test sequences. We use the mean intersection over union (mIoU) and mean accuracy (mAcc) as the main metrics for evaluating the semantic segmentation performance."}, {"title": "4.2. Task Heads", "content": "This section discusses the methods to evaluate the effectiveness of the pretraining and the quality of the learned representation.\nFine-tuning: In our comparisons with state-of-the-art methods, fine-tuning follows the self-supervised pre-training of the encoder. For this purpose, a task-specific head is added with randomly initialized weights, and both the encoder E and the task-specific head are trained using the annotated dataset. We use the same head as PTv3 [37] for the semantic segmentation tasks and the same head as our baseline UVTR [23] for the object detection task. We use layer-wise learning rate decay (LLRD) [17] to avoid forgetting the SSL representations in the encoder.\nNon-linear Probing: The purpose of the ablation study is to evaluate the learned representation from our SSL approach on the downstream semantic segmentation task. Therefore, the encoder is kept frozen after pre-training, i.e., no fine-tuning. Following Probe3D [1] and the insights of earlier works [6, 17], we use a multi-scale non-linear probe (NonLP) instead of the commonly used linear probing protocol. NonLP aggregates the feature tokens of all scales after up-sampling tokens of coarser scales before passing them to a voxel-wise small MLP. NonLP avoids that the representation learning happens in the head. Still, it is probing the stronger but non-linear features, correlating better with transfer performance [6, 17]. Similar to the commonly used linear probe, NonLP is trained for a few epochs using annotated data."}, {"title": "4.3. Implementation Details", "content": "We perform the experiments for semantic segmentation in the Pointcept [11] framework and in the MMDetection3D [10] framework for the object detection task. We use PTv3 [37] as the encoder E, unless stated otherwise. We use a single NVIDIA A100 GPU for the pretraining. We use S = 4 for the reconstruction and masking scales, corresponding to target voxel sizes of {0.05, 0.10, 0.20, 0.40} meters. The input voxel dimension is 0.05 meters. The masking ratio rs of the finest scale is 70% for nuScenes and 85% for Waymo. In the self-supervised pre-training, the decoder D consists of sparse convolution layers [12] of kernel size 5, followed by a single sparse submanifold convolution layer to generate O. We use common augmentation techniques such as rotation, scaling, and jittering from the semantic segmentation literature [9, 31, 37] during pretraining."}, {"title": "4.4. Benchmarking Results", "content": "In this section, we present the benchmarking results for both 3D semantic segmentation and 3D object detection.\n3D Semantic Segmentation: Tab. 1 summarizes the best-performing methods on the nuScenes and Waymo Open Dataset leaderboards for the LiDAR semantic segmentation task. The results include the most performant self-supervised pretraining methods. We fine-tuned our model as described in Sec. 4.2. We observe that our proposed self-supervised pretraining achieves a mIoU score of 81.8 on the nuScenes validation set, adding 1.4 mIoU points over the baseline and setting the state-of-the-art. The results on the nuScenes semantic segmentation test set are on par with the strong PTv3 baseline for the mIoU score and outperforms it in the frequency-weighted IoU (fwIoU) score by 0.4%. The improvement on the test set is lesser than the validation dataset because NOMAE has relatively poor performance for the minority class bicycle, and we did not make special adaptations for the test set submission.\nOn the Waymo Open dataset val set, our method achieves a mIoU score of 72.3 and mAcc of 75.2. NOMAE improves by 1.0 and 2.0 points in the mIoU and mAcc, respectively, over the baseline PTv3 [37]. The current version of the semantic segmentation challenge is relatively new for the Waymo Open dataset, with only a few submissions. With 70.3% mIoU score on the test set, NOMAE sets a new state-of-the-art on the Waymo Open Dataset single frame semantic segmentation challenge.\n3D Object Detection: We present results for object detection on the nuScenes validation set using the fine-tuning approach described in Sec. 4.2. We follow the experiment setup of GD-MAE [43] and UniPAD [44], utilizing only 20% of the annotated frames during fine-tuning, without the use of CBGS [48] or Copy-and-Paste[42] augmentation. We adopt the same training settings as the baseline UVTR [23] and do not use test-time augmentation or model ensembling.\nTab. 2 shows that our approach achieves 60.9 and 54.4 NDS and mAP scores respectively, improving by 14.2 in NDS points and 15.4 in mAP points over the UVTR-L [37] baseline, and by 5.1 NDS points and 5.6 mAP over the closest contrastive SSL method Learning-from-2D [44]. The significant improvement demonstrates the effectiveness of our proposed localized multi-scale SSL for 3D object detection with limited annotated data."}, {"title": "4.5. Comparison with SSL Methods", "content": "In this experiment, we compare the performance of our proposed method with the self-supervised pretraining methods of Occupancy-MAE [26] and GeoMAE [33], with the same encoder architecture of PTv3 [37]. Tab. 1 shows that our reimplementation of Occupancy-MAE [26] and GeoMAE [33] with the state-of-the-art PTv3 [37] backbones (marked with *) outperforms the results reported in the original papers by 0.3 and 7.1 mIoU points respectively for semantic segmentation with fine-tuning on the nuScenes dataset.\nTab. 3 shows the results of non-linear probing (NonLP). We observe that the NonLP performance in Tab. 3 correlates with the fine-tuning results in Tab. 1. Furthermore, the relative performance improvement of NOMAE over the baselines is higher for NonLP in comparison to fine-tuning, which indicates richer representation learning. Additionally, the time of a single training step (iteration time) is lowest for NOMAE, despite the multi-scale pretraining and a much finer resolution of the pretraining supervision. We note that the iteration time of NOMAE is 10ms for the pretraining with a single scale."}, {"title": "4.6. Ablation Study", "content": "In this section, we present ablation studies on the nuScenes semantic segmentation validation set to investigate the design choices of the proposed method. We performed the experiments using the pre-trained frozen encoder using NonLP. Please refer to Sec. 4.2 for further details.\nDetailed Study of NOMAE: This experiment evaluates the improvement due to our proposed contributions, and the results are presented in Tab. 4. We start from our implementation of Occupancy-MAE [26] as in Tab. 3. Reconstructing only the local neighborhood Vn of visible voxels Vv increases the NonLP mIoU from 59.0% to 66.7%. This requires replacing the Occupancy-MAE decoder with our proposed upsampling module and neighborhood decoder. Adding the multi-scale reconstruction (MSP) from Sec. 3.3 further improves the mIoU to 70.1 for naive mask construction and to 70.5 for masking strategy from Point-M2AE [47], as opposed to single-scale reconstruction in Occupancy-MAE. Our proposed hierarchical mask generation from Sec. 3.4 yields an improvement of 72.46 mIoU, as further investigated in Sec. 4.6. Moreover, increasing the reconstruction neighborhood size from 5 to 9 improves the mIoU to 74.8, as further investigated in Sec. 4.6. Reducing the batch size to 8 (further investigated in Sec. 6.1 of the supplementary material) yields the final NonLP performance of 74.8% and fine-tuning performance of 81.8% mIoU score.\nMask Block Size, MSP and HMG: This experiment investigates different reconstruction and mask scales s for our SSL task."}, {"title": "4.7. Qualitative Evaluations", "content": "We present qualitative comparisons in Fig. 6. In (a) and (b) for semantic segmentation, we observe that NOMAE improves the accuracy by reducing class mix-up and by improving the boundaries between objects. In (a), we observe that the baseline mis-segments the truck while NOMAE accurately recognizes it. In (b), we see that NOMAE recognizes the smaller object missed by the baseline (the bottom left pole). We can also see that it fails to recognize some drivable areas in (a). In Example (c), we visualize the detections from NOMAE. We see that compared to the baseline, NOMAE is able to more accurately estimate the orientation of the objects and has higher true positive detections. We also see both models hallucinating in further away regions (on the left), and NOMAE fails to detect the pedestrian on the left. More qualitative results are presented in Sec. 12 of the supplementary material."}, {"title": "5. Conclusion", "content": "In this work, we proposed NOMAE, a novel multi-scale self-supervised learning framework for large-scale point clouds. Observing the large-scale nature of LiDAR point clouds, NOMAE reconstructs only local neighborhoods, keeping the computation tractable at higher voxel resolutions, avoiding information leakage, and learning a localized representation suitable for diverse downstream perception tasks. Enabled by its efficiency, NOMAE utilizes multiple scales in the pre-training, enabling the model to learn both coarse and fine representations. A novel hierarchical mask generation scheme balances the pre-training of coarse and fine features, which is important for objects of different sizes, such as pedestrians and trucks We presented experimental results that underline the benefit of our proposed contributions, achieving state-of-the-art performance on multiple benchmarks.\nLimitations: NOMAE is sensitive to the density of the point cloud and future work will investigate the proposed method for sparse 3D sensors such as radar. Additionally, NOMAE does not utilize the temporal nature of LiDAR data which can open the door for further performance improvement. Furthermore, the application of high-resolution sparse 3D representations in the encoders should be further investigated for the object detection task, where 2D bird's eye view and low-resolution 3D approaches are still dominant."}, {"title": "6. Implementation Details", "content": "In this section, we report further details of our implementation for both pretraining and finetuning. We use the Pointcept [11] framework for pretraining and finetuning semantic segmentation. We performed the fine-tuning for object detection in the MMDetection3D framework [10]."}, {"title": "6.1. Training Settings", "content": "In Tab. 7, we report the training details for both pretraining and semantic segmentation finetuning. For the criteria, the weights between brackets represent the weight of the loss in the final objective. During finetuning, we employ LLRD [17], more specifically, we decay the learning rate of every block in the encoder exponentially with a factor of 0.65 compared to the next block. For the downsampling blocks, we utilize the same learning rate as the next transformer block in the architecture.\nIn Tab. 8, we highlight the pretraining and finetuning details for object detection experiments. We follow the standard 10 sweeps aggregation for nuScenes object detection [23]. To this end, we pretrain NOMAE on the aggregated point clouds and use a masking ratio of 85% to accommodate for the denser point cloud. For other training settings, we utilize the same implementation details as our object detection baseline UVTR [23]."}, {"title": "6.2. Model Settings", "content": "For the encoder E, we use the same implementation details as the Encoder of PTv3 [37]. For the upsampling module Mu, we use a PTv3 decoder as a baseline and experiment with different sizes. We found empirically that reducing the size of the upsampling module improves the downstream performance after finetuning. Hence, we utilize a single transformer block per resolution in the upsampling module.\nThe neighboring decoder uses sparse convolution layers to generate representations for neighboring voxels Vn and a single sub-manifold convolution layer to convert the representation into occupancy predictions. Sec. 7 gives further details of the hyperparameter choices of the neighborhood decoder."}, {"title": "6.3. Data Augmentations", "content": "We adopt common data augmentation methods used in [23, 37] during fine-tuning semantic segmentation and object detection. Pretraining uses the same data augmentations as semantic segmentation finetuning. Details about the data augmentations used are reported in Tab. 10."}, {"title": "7. Further Ablations", "content": "In this section, we present further ablation studies on the nuScenes semantic segmentation validation set. We evaluate the settings by freezing weights in the encoder after pretraining and using NonLP as described in Sec. 4.2."}, {"title": "8. Generalization to Different Architectures", "content": "In this section, we present more quantitative results on the Nuscenes Semseg task for pretraining and finetuning different architecutres (MinkUnet [9], OACNN [28] and OctFormer [35]). We choose another transformer based architecture and two different CNN based ones. For fair comparison with the older architectures we reproduce their results for training from scratch using the modern optimizers and schedulers which already improves their performance by whopping 5.3 points. For the modern ones, the results are consistent with the original works. In Tab. 14 we can see that NOMAE consistently improves the downstream performance by ~ 1.4 \u2013 2.2 mIoU points compared to training from scratch. It's worth mentioning that such improvement comes without tuning any of the pretraining hyperparameters, demonstrating the superb generalization capabilities of NOMAE pretraining. We hypothesize that the generalization capabilities arises from the diverse feature learning promoted by our MSP (see Sec. 3.3)"}, {"title": "9. Multi-Scale SSL", "content": "In this section we compare between our formulation of multi-scale SSL (assigning different granularity reconstruction targets to different feature levels at different scales, and the formulation of multi-scale SSL in GeoMAE [33] (assigning different granularity reconstruction targets to a single feature level). Intuitively, our formulation encourages feature diversity between different feature levels. While GeoMAE's formulation improves the semantics of a single layer level, yet it doesn't promote feature diversity between different layers and can make said layer a bottleneck. In Tab 15 we quantify the improvement from each formulation using neighborhood occupancy as the pretext. The results show that the gain from MSP is ~ 3\u00d7 GeoMAE's multi-scale (MS) on nuScenes Semseg val set. This demonstrates the importance of assigning different tasks to different feature levels for an effective SSL on large-scale point clouds."}, {"title": "10. NOMAE Improves Sample Efficiency", "content": "The results of our baseline PTv3 in Tab. 1 are taken from the original paper [37], To demonstrate that the improvement from NOMAE pretraining is not merely due to faster convergence of the pretrained weights, we report additional results for training the model from scratch using longer schedules of 50, 100, and 200 epochs. The results in Tab. 16 show no significant improvement from longer training, confirming that just 50 pretraining epochs of NOMAE followed by 50 epochs of finetuning improves sample efficiency and reaches higher final performance. We would like to highlight to the reader that we did not experiment with longer pretraining schedules and we suspect that further improvement can be brought by it."}, {"title": "11. Additional Object Detection Results", "content": "In this section we report more results on the object detection downstream task. We use the same pretrained model as in Sec. 4.4, however we finetune the model using 100% of the labelled data. Additionally we use all modern strategies commonly used for object detection such as CBGS [48], and Copy-Paste augmentation [42]. We compare between NOMAE pretraining followed by 5 epochs fineuning and traning from scratch for 20 epochs (any longer schedule does not provide improvement). We also perform the same comparison using the commonly used CNN based SparseEncoder backbone. The results in Tab. 17 shows that NOMAE brings consistent improvement to both NDS and mAP irrespective of the backbone architecture. Using NOMAE pretraining UVTR+PTv3 backbone achieves new SOTA for object detection on nuScenes val set."}, {"title": "12. Qualitative Analysis", "content": "In this section, we present more qualitative results. Fig. 8 shows results for semantic segmentation and Fig. 9 object detection. For semantic segmentation, we observe the general trend of NOMAE correctly classifying smaller objects that are misclassified by the baseline (examples (c), (e), and (g)). Additionally, the boundaries between different semantic classes are more pronounced in the case of NOMAE (examples (a), (d), and (f)), which shows the importance of self-supervision at finer resolutions. We can also observe that NOMAE helps with the mixing between similar classes, such as trucks and buses in example (b), as well as different ground types in other examples. NOMAE also more accurately estimates the orientation of the objects and has higher true positive detections in the case of object detection."}]}