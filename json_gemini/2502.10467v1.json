{"title": "YNote: A Novel Music Notation for Fine-Tuning LLMs in Music Generation", "authors": ["Shao-Chien Lu", "Chen-Chen Yeh", "Hui-Lin Cho", "Chun-Chieh Hsu", "Tsai-Ling Hsu", "Cheng-Han Wu", "Timothy K. Shih", "Yu-Cheng Lin"], "abstract": "The field of music generation using Large Language Models (LLMs) is evolving rapidly, yet existing music notation systems, such as MIDI, ABC Notation, and MusicXML, remain too complex for effective fine-tuning of LLMs. These formats are difficult for both machines and humans to interpret due to their variability and intricate structure. To address these challenges, we introduce YNote, a simplified music notation system that uses only four characters to represent a note and its pitch. YNote's fixed format ensures consistency, making it easy to read and more suitable for fine-tuning LLMs.\nIn our experiments, we fine-tuned GPT-2 (124M) on a YNote-encoded dataset and achieved BLEU and ROUGE scores of 0.883 and 0.766, respectively. With just two notes as prompts, the model was able to generate coherent and stylistically relevant music. We believe YNote offers a practical alternative to existing music notations for machine learning applications and has the potential to significantly enhance the quality of music generation using LLMs.", "sections": [{"title": "Introduction", "content": "Recently, Large Language Models (LLMs) have made significant advances. By fine-tuning pre-trained LLMs with task-specific datasets, we can generate desired outputs across various domains. For instance, previous work [3] has used ABC notation [1] to interact with GPT-2 [12] for music generation, yielding impressive results. However, ABC notation is difficult for humans to read due to its complex"}, {"title": "Related Work", "content": "In this section, we introduce three common music notation systems\u2014MIDI, MusicXML, and ABC Notation\u2014and use Boat on Tai Lake, a Chinese folk tune, as an example to demonstrate each format."}, {"title": "MIDI", "content": "MIDI (Musical Instrument Digital Interface) [10] is a standardized protocol for communication between musical devices, first developed in 1983 by several elec-tronic music equipment manufacturers. Unlike audio signals, MIDI transmits digital control information, enabling electronic instruments, computers, synthe-sizers, and audio equipment to communicate with one another. Its primary pur-pose is to send instructions, such as which notes to play, their velocity, and timbre, guiding devices on how to generate sound.\nThe MIDI protocol specifies a transmission rate of 31.25 kbps, which is fast enough to ensure low-latency signal transmission necessary for real-time musi-cal control. MIDI data is generally divided into three main categories: Channel Messages, System Messages, and Control Change Messages.\n- Channel Messages convey essential performance information, such as note-on and note-off events, note velocity, and other parameters needed for playing back notes.\n- System Messages handle global controls, including timing synchronization and system resets.\n- Control Change Messages are used to make real-time adjustments to sound characteristics, such as pitch bends, modulation, and pedal controls.\nMIDI technology is widely used across various fields, including music pro-duction, live performance control, sheet music generation, music education, film scoring, and sound design. It is also integral to automated and interactive music systems. Thanks to its efficient data transmission and standardized protocol, MIDI allows musicians, producers, and sound designers to easily compose mu-sic, control effects, and synchronize digital audio elements, providing powerful support for both music creation and performance."}, {"title": "MusicXML", "content": "MusicXML (Music Extensible Markup Language) [8,9] is a standard open format based on XML, creating a universal format for common Western musical notation used for the interchange and distribution of digital scores. Musical information can be utilized by score programs, sequencers, other performance applications, music education software, and music databases. The hierarchical structure of MusicXML reflects the structure of music itself from higher-level elements like pieces and sections, down to lower-level details like notes and beats.\nMusicXML has two top-level elements, representing the partwise and time-wise score formats, which are determined by the root element. The top-level element also defines the structural framework of the lower-level elements. If the root element is <score-partwise>, the musical part is the primary structure, containing multiple  elements, each of which includes several  elements. On the other hand, if the root element is <score-timewise>, the mea-sure is the primary structure, containing multiple  elements, each of which includes several  elements.\nThe actual music in the score is represented by lower-level elements, each of which contains a group of music data. These elements are composed of zero or more , , , , , and other elements. Sub-elements like  and  within , as well as  within , define the basic structure of each measure and the specific details of the notes.\nThe score header refers to the elements located at the top of the score, providing basic information and settings about the piece. This includes the ti-tle, composer, arranger, performer, copyright details, and more. It is typically composed of elements like , , , , and others.\nOverall, MusicXML provides a standardized format that is widely supported by many applications and offers high readability for developers. Its ability to thoroughly describe elements like notes, harmony, and rhythm allows for a more complete representation of the score. However, the detailed information also leads to larger file sizes, and in some cases, the structure may be overly com-plex or verbose. Additionally, MusicXML can be more difficult for musicians to understand."}, {"title": "ABC Notation", "content": "ABC Notation [1] is a simple, text-based music notation system that is relatively easy for humans to read. Over the past three decades, it has been used to rep-resent tens of thousands of musical pieces, which are commonly referred to as ABC tunes.\nAn ABC tune consists of two main parts: the tune header and the tune body. The tune header contains basic information about the music, including reference number, title, composer, origin, region, meter, unit note length, tempo, parts,"}, {"title": "Introduction of YNote", "content": "YNote is a new data structure designed to represent sheet music, consisting of two key components: the pitch and duration of notes."}, {"title": "Pitch", "content": "In YNote, pitch is represented by an English note name followed by a single digit. The English note names are: C, D, E, F, G, A, and B\u2014also known as Do, Re, Mi, Fa, Sol, La, and Si. Notes with a half-step are represented by the corresponding lowercase letter. For example, \"C#\" or \"Db\" is denoted as \"c\".\nThe digit indicates the octave, where the number 4 represents middle C (C4), while a lower and higher octave are denoted as C3 and C5, respectively.\nWe define the frequency of A4 as 440 Hz, and the frequency of each subse-quent note is calculated based on the equal temperament system. To maintain a simple and consistent format, we use \"00\" to represent a rest."}, {"title": "Duration", "content": "Duration in YNote is represented with two characters. For example, \"01\" de-notes a whole note, \"02\" represents a half note, \"04\" is a quarter note, and so on. Dotted, double-dotted, and triplet notes are also represented using two characters. A dotted note adds a period after the note length (e.g., a dotted quarter note is written as \"4.\"). Double-dotted notes add a colon (e.g., \"4:\"), and triplets are indicated by adding the digit 3 (e.g., \"43\"). When encountering dotted sixteenth, thirty-second, or sixty-fourth notes, we avoid using \"16.\" since"}, {"title": "Methodology", "content": "In this section, we describe how YNote-encoded music was used to fine-tune the GPT-2 model and generate music in a similar style."}, {"title": "Background", "content": "Music Generation is the process of creating music using algorithms and ma-chine learning models, and it has become a popular research topic in recent years. Various methods can be used for music generation. For example, [3] fine-tuned the GPT-2 model using music in ABC Notation, achieving impressive results, while [2] fine-tuned GPT-2 using MIDI data and carefully designed metrics. Even diffusion models, such as those described in [5], have been used to generate music.\nGPT-2 (Generative Pre-trained Transformer 2) [12] is a large-scale, unsu-pervised language model developed by OpenAI, trained on 8 million web pages. It is built on the Transformer architecture [13], which leverages self-attention mechanisms to model dependencies between input and output tokens. GPT-2 is pre-trained on a large corpus of text data and can be fine-tuned for specific tasks to generate high-quality text outputs. There are several versions of GPT-2, ranging from 124M to 1.5B parameters, with the 124M model being the smallest, which we used in our experiments."}, {"title": "Fine-tuning of GPT-2", "content": "Figure 3 illustrates the step of fine-tuning GPT-2 and generating music in a similar style. The architecture of the pre-trained GPT-2 model is based on [4]. In the fine-tuning step, we first convert the music into YNote format and then fine-tune the pretrained GPT-2 model using PyTorch. During the inference step, we provide the fine-tuned GPT-2 model with prompts, such as the first bar or the first and last notes of each bar, to generate new music. However, sometimes the YNote format generated by GPT-2 is invalid. Therefore, we must normalize it to conform to the standard YNote format."}, {"title": "Experiments and Results", "content": "In this section, we describe how YNote-encoded music was used to fine-tune the GPT-2 model [12], and present the corresponding BLEU [11] and ROUGE [6] scores."}, {"title": "Dataset Description", "content": "We converted 190 pieces of Jiangnan-style music into the YNote format. The prompt consisted of the first and last notes of each bar to guide GPT-2 in learning and generating music. Additionally, we experimented with using the entire first bar as the prompt, which also yielded satisfactory results."}, {"title": "Dataset Description", "content": "Due to hardware constraints and speed requirements, we selected GPT-2 (124M) as the base model. The data was split, reserving 30% as the test set, and five pieces of music were withheld from both the training and test sets for evaluating BLEU and ROUGE scores. The AdamW optimizer [7] was used with a learning rate of 5e-5, a warmup of 100 steps, and training over 20 epochs."}, {"title": "Results", "content": "We fine-tuned GPT-2 on a single NVIDIA GeForce RTX 4070 Ti Super, and the training took approximately half an hour. To evaluate the model, we gen-erated five pieces of music for each of the reserved pieces and calculated their corresponding BLEU and ROUGE scores.\nBLEU measures how well the generated text overlaps with a reference text, using n-grams. Its values range from 0 to 1, with 1 indicating perfect overlap and 0 indicating no overlap. ROUGE is also used to assess the quality of machine-generated text, with values similarly ranging from 0 to 1, where higher is better. Both metrics are commonly used for evaluating the performance of language models.\nIn our results (see Figure 4 and 5), we observe that the music generated using our method closely resembles the original compositions, even when the prompt is not part of the training set. This outcome aligns with our expectations. However, on some occasions, the GPT-2-generated music in YNote format does not fully adhere to the intended structure, requiring preprocessing to correct these errors. Specifically, we modified 125 out of 7569 characters (approximately 1.6%) in the first bar and 149 out of 6485 characters (about 2.2%) when focusing on the first and last notes of the prompt. Additionally, Figure 6 and 7 shows some examples of music generated by GPT-2."}, {"title": "Conclusion", "content": "In this paper, we introduced a new data structure for representing sheet music. To simplify the format, we excluded certain uncommon notes and situations. This makes the structure more readable and suitable for machine learning, thanks to its fixed format. Our approach achieved BLEU and ROUGE scores of 0.883 and 0.766, respectively. Additionally, we demonstrated the ability to generate music in specific styles based on given notes. We believe that the YNote format offers a promising alternative for recording music information, with potential for further development.\nAdditionally, GPT-2's robust network allows us to easily generate music in different styles by preparing the corresponding dataset in YNote format and feeding it into the model. This is made possible due to the structured and easily understandable nature of the YNote format."}]}