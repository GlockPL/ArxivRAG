{"title": "Knowledge-Aware Parsimony Learning: A Perspective from Relational Graphs", "authors": ["QUANMING Yao", "YONGQI ZHANG", "YAQING WANG", "NAN YIN", "JAMES Kwok", "QIANG YANG"], "abstract": "The scaling law, a strategy that involves the brute-force scaling of the training dataset and learnable parameters, has become a prevalent approach for developing stronger learning models. In this paper, we examine its rationale in terms of learning from relational graphs. We demonstrate that directly adhering to such a scaling law does not necessarily yield stronger models due to architectural incompatibility and representation bottlenecks. To tackle this challenge, we propose a novel framework for learning from relational graphs via knowledge-aware parsimony learning. Our method draws inspiration from the duality between data and knowledge inherent in these graphs. Specifically, we first extract knowledge (like symbolic logic and physical laws) during the learning process, and then apply combinatorial generalization to the task at hand. This extracted knowledge serves as the \u201cbuilding blocks\" for achieving parsimony learning. By applying this philosophy to architecture, parameters, and inference, we can effectively achieve versatile, sample-efficient, and interpretable learning. Experimental results show that our proposed framework surpasses methods that strictly follow the traditional scaling-up roadmap. This highlights the importance of incorporating knowledge in the development of next-generation learning technologies.", "sections": [{"title": "Introduction", "content": "The learning techniques have progressed from manual feature engineering to shallow models, then to deep networks, and now to foundation models [Bommasani et al., 2021], achieving great success in the field of computer vision [He et al., 2016], natural language understanding [Brown et al., 2020] and speech processing [Rabiner and Juang, 1993]. Specifically, large language model, like ChatGPT [Ouyang et al., 2022], as representatives of foundation model, has shown strong performance in versatile learning, which can adopted in many different tasks. The belief is that larger models can be more expressive, thus are likely to generalize better given sufficient training data [Jordan and Mitchell, 2015, Donoho, 2017]. This gives birth to the current roadmap, i.e., achieving stronger performance by aggressively scaling up the size of data and models, which is also observed as scaling law [Kaplan et al., 2020].\nDifferent from the data form of images, natural language and speech, relational graphs are a way of representing knowledge using nodes and edges, while also capturing the meaning of that knowledge in a structured form that can be used for machine learning. The application of graphs are widely spread in real-life scenarios. As shown in the left of Figure 1, examples include urban network design and planning [Porta et al., 2006], prediction of molecular properties [Dai et al., 2016], reasoning from knowledge graphs [Liang et al., 2018], and recommendation systems [Ying et al., 2018]. To meet the learning demands of diverse application mentioned above, we want to develop the methods that can efficiently train on different tasks with a unified architecture (versatile), fast adapt to new tasks with few samples (sample-efficient), and exhibit inference process with evidence (interpretable).\nHowever, simply following the previous technical roadmap would not face the requirement for relational graphs. The roadmap would first fail on versatile learning, the reason can be attributed to the specialness of learning from such a data type. The primary problem is the architectural incompatibility. LLMs are implemented through Transformer architectures Vaswani et al. [2017], achieving superior representation capabilities by scaling up model and data size, thereby facilitating versatile learning across diverse tasks. In contrast, Transformer architectures are inherently designed for sequential structures and thus struggle with relational graphs. The limitations of positional encoding [Vaswani et al., 2017] and scalability issues [Liu et al., 2024] in handling graph structures hinder their effectiveness in versatile learning. Besides, the learning methods for relational graphs would also suffer from the representation bottleneck when layers are stacked up [Nt and Maehara, 2019, Topping et al., 2021]. Limited by the models' learning ability, simply stacking up the model layers would lead to oversmoothing, where node representations become overly uniform, and oversquashing, where information is compressed or distorted with increasing depth. These issues lead to information loss, ultimately degrade the models' representational capabilities.\nBesides, the roadmap is not yet applicable for sample-efficient and interpretable learning. Sample-efficient learning typically has access to only a few labeled samples, which cannot fundamentally satisfy the requirement of scaling up. For sample-efficient learning, such as the molecular properties prediction task, obtaining sample labels is costly and difficult. Simply enlarging the model size will lead to overfitting on the labeled samples, reducing its ability to generalize to test data. The other challenge is interpretability learning, which focuses more on explaining how the features or model structures map to the predictions, rather than relying on the expressiveness of the models. For the interpretable learning, particularly in the task of reliable reasoning on knowledge graphs, users require reliable insights through model analysis. However, simply scaling up the model is insufficient for interpreting data effectively.\nAs a consequence, simply following the previous roadmap would degrade models' generalization performance, and one can observe the point of inflection (highlighted by the red circle) in the right of Figure 1. This indicates that the path is not sustainable here, which motivates us to ask one question:"}, {"title": "How to develop stronger models for learning from relational graphs if common scaling law is not applicable?", "content": "Inspired by [Donoho, 2006, Candes and Recht, 2012, Liu et al., 2017], which emphasize using the simplest model that explains the data effectively, we attempt to find a parsimony way in machine learning, and use simple solutions to achieve the effects of \"making big gains with small things\". Intuitively, humans can can accumulate knowledge, such as symbols and concepts, as well as rules and principles based on them. These knowledge enables us to learn a wide range of subjects or skills and to apply them in various tasks (versatile), fast adapt to new a task with a few or even without demonstrations (sample-efficient), and reveal the underneath reasons that explain different phenomena (interpretability). These properties are what we pursue on learning from relational graphs. This approach is denoted as knowledge-aware parsimony learning, uses knowledge as a bridge to employ the simplest possible models and solutions, achieving effective and efficient results while ensuring generalizability and interpretability.\nFortunately, distinct from the images and texts, which are passively collected, relational graphs are typically actively constructed. Thus, relational graphs can naturally contain knowledge delivered by humans, and we can mine the knowledge from the data and use the knowledge combinatorial generalize to different scenarios. As illustrated in Table 1, in the urban network, the data consists of spatial-temporal trajectory interactions, and the knowledge encompasses the traffic flow patterns, land use patterns and socioeconomic activities. For the molecular 3D structure, typically the sample-efficient learning task, the data comprises limited labeled 3D graph structured molecular samples, and the knowledge includes the properties of functional group. In knowledge graph, the data is the triplet data with head, tail and relation, and the knowledge is the critical path for information delivery. Therefore, the duality, i.e., the coexistence of data and knowledge, is the key properties in relational graphs.\nBased on such duality property, the key to achieve parsimony learning is to develop a learning framework that can benefit from both data and knowledge. The complex data is governed by relative simple symbolic logic and physical laws (denote as knowledge), just like various colored can be determined by only three basic one (i.e., red, blue and green). Thus, it is necessary to design a framework to first mine the knowledge in the symbolic space by searching for symbolic logic and physical laws during the learning process. Then, by data fitting the combination of these knowledge in functional space, it can be efficiently generalized to the task at hand. In this way, these knowledge act as \u201cbuilding blocks\" to achieve parsimony learning, as shown in Figure 2.\nExisting knowledge-based learning methods, such as inductive logic programming [Lavrac and Dzeroski, 1994] and symbolic regression [Landajuela et al., 2022], rely on strict logical operation search rules, and provide good interpretability. The data-driven based learning methods, including GNNs [Kipf and Welling, 2017] and LLM [Ouyang et al., 2022] methods, primarily utilize deep learning techniques [Vaswani et al., 2017], which typically learn from the data space and demonstrate significantly performance. Additionally, some works attempt to incorporate knowledge into graph learning [Liu et al., 2023, Huang et al., 2022]. They utilize knowledge of molecular to learn the properties of graphs, thereby improving the model's expressiveness efficiently. However, while these methods integrate knowledge into the model learning framework, they lack the ability to combinatorially generalize to other tasks to achieve parsimony learning. Our proposed framework diverges from the classical roadmap and leverages knowledge as the bridge from intricate data to parsimony learning. Consequently, by learning the duality of data and knowledge in relational graphs, we can extract knowledge in the symbolic space instead of directly learning from the intricate data space. By combinatorially generalizing the knowledge in the functional space, we can efficiently achieve the versatile learning, sample-efficient learning and interpretable learning, ultimately towards the parsimony learning.\nThe remainder of this paper is organized as follows: Section \u201cResearch Landscape\" briefly describes how our approach address issues of versatile, sample-efficient, and interpretability learning. Section \u201cParsimony on Architecture\u201d details how to utilize semantic relationships to recombine simple architectures, achieving versatile learning by parsimony on architecture. Section \"Parsimony on Parameters\" discusses how to mine functional group properties to selectively tune parameters, achieving sample-efficient learning by parsimony on parameters. Then, Section \u201cParsimony on Inference\" explores how to identify logical structures to support concise inference process, achieving interpretable learning by parsimony on inference. Lastly, we introduce the \"Future Works\" from theory, methodology and application perspectives."}, {"title": "Research Landscape", "content": "Knowledge-aware parsimony learning is an approach in machine learning that emphasizes using knowledge as a bridge to employ the simplest possible models and solutions, achieving effective and efficient results while ensuring generalizability and interpretability. In our framework, we develop stronger models, which follow the way of knowledge-aware parsimony learning, for versatile learning, sample-efficient learning, and interoperability. Our research framework is in Figure 3, specifically,\n\u2022 Parsimony on Architecture. To achieve versatile learning on relational graphs, we propose parsimony learning on architecture by utilizing semantic relationships to recombine simple architectures. The representative works are AutoBLM [Zhang et al., 2022a] and KGTuner [Zhang et al., 2022b]. AutoBLM introduces to search task-adaptive scoring functions by bilevel optimization, resulting in significant performance improvements. KGTuner introduces an efficient two-stage hyper-parameter search algorithm. When combined with AutoBLM, it enables the efficient recombination of simple architectures for versatile learning on relational graphs.\n\u2022 Parsimony on Parameters. To achieve sample-efficient learning, we propose parsimony on parameters by mining functional group properties to selective tune parameters (specific in the field of molecule). The representative works are PAR [Wang et al., 2021, Yao et al., 2024] and PACIA [Wu et al., 2024]. Empirical results show that PAR achieves the state-of-the-art performance in molecular property prediction, and PACIA greatly improves the adaptation speed while significantly reduce the number of learnable parameters.\n\u2022 Parsimony on Inference. To achieve interpretable learning, we propose parsimony on inference by identify logics structures to support concise inference process. The representative works are RED-GNN [Zhang and Yao, 2022] and EmerGNN [Zhang et al., 2023a]. The results show that subgraph learning not only improves representation ability, but also demonstrates the superiority of the proposed methods in capturing logical relationships in graphs.\nBy applying parsimony learning to architecture, parameters and inference, we search for structures in a knowledge-based way rather than simply stacking them, effectively solving the problem of architectural incompatibility. Additionally, by utilizing subgraphs as the knowledge for representation learning, we address the challenge of the representation bottleneck."}, {"title": "Parsimony on Architecture", "content": "The structure of relational graph is complex and highly heterogeneous. For example, in a knowledge graph, there can be a lot of variations in the semantic relationships (such as symmetric, asymmetric and composition patterns) [Ji et al., 2021]. As a result, it is challenging to directly adapt existing hand-designed models for processing specific properties across different tasks and graphs. Moreover, there is no universal model that can well adapt to all the semantic relationships. To achieve versatile learning on relational graphs, we propose parsimony learning on architecture by utilizing semantic relationships to recombine simple architectures. Specifically, it is formulated as the following bi-level learning problem.\n##### A Bi-level Learning Formulation\nThe key idea of parsimony learning on architectures is to mine the shared knowledge about semantic relationships in model design, and recombine the simple architectures to fit specific properties in different tasks. In general, this contains three main steps: (i) extracting symbolic representations for state-of-the-art model architectures, (ii) building a search space to summarize these models, and (iii) adapting the model for different tasks by searching in the search space. Mathematically, this problem can be represented as the bi-level optimization problem:\n$\\max_{\\alpha\\in\\mathcal{A}} P(F(w^{*}; G,\\alpha), D_{val})\\qquad s.t.\\ w^{*} = \\arg \\min_{w} L(F(w; G, \\alpha), D_{tra}),$ (1)\nwhere the lower level minimizes model $F$'s parameters $w$ on training data $D_{tra}$ with architecture $\\alpha$, and the upper level maximizes the model's performance on validation data $D_{val}$ by adaptively searching architectures $\\alpha$ in the search space $\\mathcal{A}$. The space $\\mathcal{A}$ contains the transferable knowledge of different models. By establishing a well-defined $\\mathcal{A}$ with simple components and searching the optimal $\\alpha \\in \\mathcal{A}$ for different tasks on graph $G$, the model can adapt to different tasks according to the performance on validation data. In the following, we illustrate this by examples in automated scoring function design and hyper-parameter optimization on knowledge graphs.\n##### Automated Bi-linear Scoring Function Design\nIn knowledge graph learning, scoring function is a key component that measures plausibility of the edges [Wang et al., 2017]. Given the embeddings $h,r,t$ of a triplet $(h,r,t)$ in the knowledge graph, human experts often design different kinds of scoring functions to measure plausibility of the edge. Among them, bi-linear scoring functions achieve outstanding performance by explicitly modeling the relation patterns. For instance, DistMult [Yang et al., 2015] models symmetric relations (e.g., spouse, friend), while Complex [Trouillon et al., 2016] enhances the ability on modeling anti-symmetric relations (e.g., ancestor). To achieve versatile learning over different knowledge graphs, AutoBLM provides a unified view of bi-linear scoring functions, and formulates the design of scoring functions as a bi-level optimization problem in (1).\nAs illustrated in Figure 4 (left), AutoBLM formulates a unified search space where a symbolic matrix $\\alpha \\in {0,\\pm 1,...,\\pm K}^{KxK}$ represents the architecture of a scoring function. With this symbolic formulation, representative works can be obtained with different choices of $\\alpha$. Furthermore, new models that adapt better for a specific dataset can be learned with an efficient evolutionary search algorithm, achieving transfer-ability.\nSimilarly, Interstellar [Zhang et al., 2020] builds a recurrent symbolic search space to capture the semantic and structural patterns in knowledge graphs and proposes a hybrid search algorithm after analyzing properties of configurations in the search space. SANE [Zhao et al., 2021] builds a unified supernet of many representative graph neural networks, transforms the discrete symbols into continuous space, and proposes a gradient descent based search algorithm for efficient search.\n##### Efficient Hyper-parameter Optimization\nAs in general machine learning models, hyper-parameters (HPs) play a crucial role in the learning process of knowledge graphs [Claesen and De Moor, 2015, Ruffinelli et al., 2020]. However, efficiently identifying good configurations for different knowledge graphs to sufficiently evaluate a model is challenging due to the limited resources and a primitive understanding of HPs'. KGTuner [Zhang et al., 2022b] proposes an efficient HP search algorithm. By conducting a comprehensive study of key factors in the search space of HPs, shared knowledge is extracted that (i) some configurations such as stochastic gradient descent optimizer, and margin ranking loss are not effective choices; (ii) HPs that perform well in random subgraphs tend to perform well on the original graph; and (iii) tree-based surrogate models, such as the random forest, can fit better of the complex search space.\nBased on the important knowledge obtained from the huge search space, KGTuner proposes a two-stage HP search algorithm, (Figure 4 (right)). In the first stage, KGTuner uses Bayesian optimization with a random forest surrogate [Breiman, 2001] and BORE acquisition function [Tiao et al., 2021] on a sampled subgraph to efficiently explore various HP configurations. This stage aims to approximately evaluate as many configurations as possible in a limited time budget. In the second stage, it then fine-tunes the most promising configurations by applying them to the entire graph, finding the configuration that fit the dataset best. The two-stage search algorithm enables KGTuner to explore efficiently in the search space of hyper-parameters."}, {"title": "Parsimony on Parameters", "content": "In AI-assisted scientific research, particularly in drug discovery and biomedicine, the scarcity of labeled data presents significant challenges. It is known that drug discovery takes more than 2 billion compounds and at least 10 years on average, while the clinical success rate is around 10% [Paul et al., 2010]. To address drug discovery in a sample-efficient manner, we have developed meta-learning learning techniques that enforce parsimony on parameters. These approaches ensure that parameters can be efficiently adapted in relation to functional groups, optimizing their use and enhancing model adaptability.\n##### A Meta-learning Formulation\nTo improve sample efficiency, one can harnesses knowledge to efficiently enable models to learn from datasets with scarce labeled data, reducing reliance on extensive datasets and computational resources. Meta-learning [Wang et al., 2020], a key aspect of this field, focuses on how models can be quickly generalized to new tasks with minimal labeled data. This approach can drastically cut the costs associated with collecting, processing, and analyzing large-scale supervised data but also facilitates learning and mining for scarce or emerging tasks.\nFormally, meta-learning targets at learning a predictive model $f_{\\theta}$ with parameters $\\theta$ from a set of tasks {$T_i$}$_{i=1}^{N}$ that can generalize to new task $T'$ given the few-shot support set. Specifically, the target properties are different across tasks. Consider training a few-shot molecular property prediction task $T$ with respect to a specific property, each sample $X_{T,i}$ is a molecular graph and its label $y_{T,i} \\in {0,1}$ records whether the molecule is active or inactive on the target property. Only a few labeled samples are available in $T_t$. This $T_t$ is formulated as a binary classification task, associating with a support set $S = {(X_{T,S},y_{T,S})}_{i=1}^{l}$ containing labeled samples from active/inactive class, and a query set $Q = {(X_{T,Q},y_{T,Q})}_{i=1}^{M_t}$ containing $M_t$ samples whose labels are only used for evaluation.\nThe learning objective can be formulated as follows:\n$\\min_\\Phi  \\sum_T l_\\theta (f_\\theta),$\n$\\qquad s.t.\\  \\Phi_T = g(\\Phi, l_{S_t}).$ (3)\nThe parameter update process typically consists of two iterative loops:\n\u2022 Inner Loop Update (2): This loop adapts the provided parameter set $\\theta$ to the specific requirements of the current task $T_t$. The adaptation uses the loss $l_{S_t}$, which is calculated on the support set $S_t$ in task $T_t$. The function $g(\\cdot)$ is used to adjust $\\Phi$ to better fit the idiosyncrasies of the given task by optimizing performance on this support set.\n\u2022 Outer Loop Update (3): This loop updates the shared parameter set $\\Phi$ using information generalized across all training tasks. It utilizes the loss $l_Q$, calculated on the query sets $Q_t$ across these tasks. The aim here is to refine $\\Phi$ so that it not only performs well on individual tasks but also retains a broad applicability across multiple tasks.\nThe objective encompasses both ensuring effective task-specific adaptation by the inner loop and promoting generalization across tasks in the outer loop. This dual focus helps in developing a model that can quickly adapt to new tasks while maintaining robustness across varied scenarios."}, {"title": "Property-Aware Relation Networks", "content": "Our first work towards this problem is the property-aware relation networks (PAR) [Wang et al., 2021, Yao et al., 2024]. PAR uses a property-aware molecular encoder to transform the generic molecular embeddings to property-aware ones. To fully leverage the supervised learning signal, PAR learns to estimate the molecular relation graph by a query-dependent relation graph learning module, in which molecular embeddings are refined w.r.t. the target property. Thus, the facts that both property-related information and relationships among molecules change across different properties are utilized to better learn and propagate molecular embeddings. Besides, we propose to selectively update $\\theta$ that represent generic information and $\\Phi$ that represent property-aware information. We only update $\\theta$ in the inner-loop update, while simultaneously update $\\theta$ and $\\Phi$ in the outer-loop update. We use gradient descents to update parameters in both loops. Through the selective update strategy, the model can capture generic and property-aware information separately in the training procedure.\n##### Parameter-Efficient GNN Adapter\nWe further introduce parameter-efficient GNN adapter (PACIA) [Wu et al., 2024]. By adopting this approach, PACIA significantly reduces the risk of overfitting. Moreover, it offers the advantage of faster inference speeds, as the adapted parameters are generated through a single forward pass rather than through iterative optimization steps. Additionally, initializing the function $g(\\cdot)$ with neural networks allows for more flexible forms of updates compared to traditional gradient descent methods. We first summarize existing works into an encoder-predictor framework: an encoder which maps $X$ to molecular representation $\\mathcal{X}$ which is a fixed-length vector, and a predictor which assigns label for a query molecule given support molecules. Recall that GNN-based predictor proposed in PAR, which operates on relation graphs of molecules is found to effectively compensate for the lack of supervised information, outperforms existing predictors. In PACIA, we let the GNN act as both encoder and predictor. Further, we design a hierarchical adaptation mechanism in the framework: Task-level adaptation is achieved in the encoder since the structural features in molecular graphs needs to be captured in a property-adaptive manner, while query-level adaptation is achieved in the predictor based on the property-adaptive representations. To adapt GNN's parameter-efficiently, we design a hypernetwork-based GNN adapter to generate a few adaptive parameters to modulate the node embedding and propagation depth, which are essential in message passing process. No further fine-tuning is required."}, {"title": "Parsimony on Inference", "content": "Interpretability is the foundation for downstream applications of machine learning. In many fields, experts need to interpret and understand the results of models in order to make informed decisions. In graph learning tasks, GNNs have brought great benefits for the effectiveness in learning from relational graphs. However, it is very challenging to interpret the inference process when facing a massive amount of associated information on graphs. For graph learning, the core problem in achieving model interpretability lies in accurately capturing strong logical relationships and exhibiting inference process with evidence. To interpret the inference processes on graph, our key idea is to capture the logical rules inside graphs as supporting evidence and use the logical interpretation of knowledge to clarify models' reasoning process. In the following, we introduce the idea of subgraph learning for intepreting and its application to drug discovery.\n##### Interpret with subgraph learning\nThe relational graph structures are complex and hard to understand by humans directly. In comparison, relational paths with multiple connected edges are more interpretable [Xiong et al., 2017, Das et al., 2018]. Based on this observation, RED-GNN [Zhang and Yao, 2022] introduces a new relational structure, called relational di-graph (r-digraph). The r-digraphs generalize relational paths to subgraphs by preserving the overlapped relational paths and structures of relations for reasoning. By leveraging the GNN model with attention mechanism to propagate information over the subgraph, significant performance improvement has been achieved and logical paths in r-digraphs can be captured.\nAn example of the r-digraph is shown in Figure 8(a), where all the relational paths with lengths three between Sam and Spider-2 are used to construct this subgraph. This is achieved by introducing reverse and identity relations as in [Das et al., 2018, Vashishth et al., 2020, Zhang and Yao, 2022], and separately representing the entities in different hops into different layers. In this way, we can start encoding from the query entity (Sam), aggregate information for three steps, and obtain the subgraph representation with the final layer encoding of the answer entity (Spider-2). By introducing attention mechanism in each layer, important edges leading to correct prediction can be learned so that we can obtain some representative evidence from the subgraph for interpretation.\nSimilar achievements can be found in NBFNet [Zhu et al., 2021] which develops upon the Bellman Fold algorithm to encode paths in graphs. To reduce the size of subgraphs, AdaProp [Zhang et al., 2023b] and A*Net [Zhu et al., 2024] introduce pruning algorithms that gradually remove irrelevant edges in each propagation layer. Benefited by the attention mechanism and subgraph learning, all these methods achieve significant improvement over the traditional methods, maintaining interpretable inference process as well.\nIn addition, the effectiveness of RED-GNN is also theoretically analyzed in the context of learning rules from graphs. The special message-passing mechanism of RED-GNN and NBFNet is first concluded as QL-GNN [Qiu et al., 2024]. To analyze expressiveness, the rules are described by first-order logic with counting, and then the expressiveness is presented as the type of logical formulas, i.e., logical formulas from model logic with counting (Theorem 3.2 in Qiu et al. [2024]), that QL-GNN can learn. However, classical methods, e.g., CompGCN and RGCN, fail to learn these rules (Theorem 3.4 in Qiu et al. [2024]). The theoretical work demonstrates that QL-GNN can learn more expressive rules than classical methods, thus showing the superiority of RED-GNN in capturing logical relationships in graphs.\n##### Application in drug discovery\nIn the ever-changing world of pharmaceuticals, the intersection of scientific advancements and regulatory changes has led to a significant breakthrough, particularly in the rapid development of new drugs aimed at treating rare, severe, or life-threatening diseases [Su et al., 2022, Ledford, 2022]. Machine learning models, particularly GNNs, have become powerful tools for predicting drug interactions, capable of comprehensively capturing intricate relationships on the interaction graphs [Vilar et al., 2014, Yu et al., 2021]. EmerGNN [Zhang et al., 2023a] takes this a step further by introducing a graph learning framework that predicts interactions between emerging and existing drugs by integrating drug-drug interaction (DDI) and biomedical networks, extracting path-based subgraphs, and learning subgraph representations.\nThe general idea of EmerGNN is illustrated in Figure 8(b). Emerging drugs typically have limited interactions with existing drugs. To address this issue, EmerGNN utilizes biomedical network and extracts subgraph $G_{uv}$ from it to connect emerging drug $u$ with existing drug $v$. Drawing inspiration from the subgraph learning methods [Zhang and Yao, 2022, Zhu et al., 2021], EmerGNN proposes a flow-based GNN $g(G_{uv};\\Theta)$ with an attention mechanism to encode pair-wise subgraph representations for drug pairs. These subgraph representations are then used to directly predict the interaction of the drug pair. This approach enables a more nuanced understanding of interactions involving emerging drugs, incorporating knowledge from the interconnected network of biomedical entities and their relationships, and providing interpretable insights."}, {"title": "Theory: Representation of diverse knowledge", "content": "Throughout the history of machine learning, parsimony has been a crucial insight in developing effective models and algorithms. Prior to the deep-learning era, parsimony was applied to vectors and matrices to facilitate tractable and reliable solutions for various data recovery problems. For instance, compressed sensing [Donoho, 2006] enforces sparsity on vectors to enable signal recovery from only a few measurements. Similarly, low-rank matrix recovery [Candes and Recht, 2012] assumes the restricted isometry property of matrices to recover a low-rank matrix from limited observations. For relational graphs, parsimony is formulated as more diverse and complex forms of knowledge representations, which are essential for capturing the complex relationships in the data. For example, the knowledge behind relational graphs can be represented as symbolic logic, physical laws, programs, and common sense. Different forms of knowledge are jointly trained in deep neural networks. However, existing knowledge representations are often limited by their singular focus, which hinders the capture of correlations between different knowledge representations. To overcome this limitation, a way of unified knowledge representation is necessary to capture and integrate diverse knowledge, ultimately enhancing model performance.\n##### Method: Integration with foundation models\nFoundation models have achieved remarkable success in natural language processing [Achiam et al., 2023], including tasks such as text generation, question answering, and language understanding. Previous studies consider foundation models to be powerful text compressors capable of capturing essential information from textual data and generating coherent and contextually relevant responses. Therefore, foundation models can be viewed as a powerful parsimony learning model for text. However, foundation models' performance is inferior to that of GNN when it comes to relational graphs [Pan et al., 2024]. The main reason behind this phenomenon is the difference in knowledge representation between texts and relational graphs. Foundation models extract knowledge from a sequence of tokens, resembling a chain-like graph. On the other hand, relational graphs are more diverse and contain encoded knowledge within their structure. Consequently, the knowledge representation of foundation models does not align well with relational graphs, resulting in subpar performance on tasks involving such graphs. To improve foundation models, our future research aims to incorporate parsimony learning into foundation models that can effectively reduce redundant information in the models and enable reliable reasoning for various tasks using different types of knowledge."}, {"title": "Applications: Usage in AI4Science", "content": "In terms of application, our future work involves applying parsimony learning to various scenarios in scientific problems, particularly focusing on material science, drug development, and physics. Parsimony learning aims to simplify models while retaining their ability to effectively explain and predict complex data. In material science, we will use parsimony learning to predict material properties and discover new materials by analyzing relational graphs of atomic structures and extracting key physical and chemical knowledge. For drug development, we will enhance drug discovery processes by modeling molecular interactions with biological targets using simplified, yet effective, graph-based representations. In physics, parsimony learning will help us understand and predict phenomena in areas such as particle physics and quantum mechanics by leveraging simplified models that capture the essential relationships between physical entities. Through these applications, we aim to demonstrate that parsimony learning can address diverse and complex scientific challenges more efficiently and interpretably than traditional methods."}, {"title": "Conclusion", "content": "This paper introduces a novel framework for learning from relational graphs using knowledge-aware Parsimony Learning. By leveraging the duality between data and knowledge, our method extracts symbolic logic and physical laws during the learning process and applies combinatorial generalization to various tasks. This approach effectively overcomes the limitations of traditional scaling methods, such as architectural incompatibility and representation bottlenecks. Experimental results demonstrate that our framework significantly improves model performance, showcasing its ability to achieve versatile, sample-efficient, and interpretable learning. These findings underscore the potential of integrating knowledge into machine learning models, offering a promising direction for future research and applications."}]}