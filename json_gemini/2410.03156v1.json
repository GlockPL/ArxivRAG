{"title": "MELODI: EXPLORING MEMORY COMPRESSION FOR LONG CONTEXTS", "authors": ["Yinpeng Chen", "DeLesley Hutchins", "Aren Jansen", "Andrey Zhmoginov", "David Racz", "Jesper Andersen"], "abstract": "We present MELODI, a novel memory architecture designed to efficiently process long documents using short context windows. The key principle behind MELODI is to represent short-term and long-term memory as a hierarchical compression scheme across both network layers and context windows. Specifically, the short-term memory is achieved through recurrent compression of context windows across multiple layers, ensuring smooth transitions between windows. In contrast, the long-term memory performs further compression within a single middle layer and aggregates information across context windows, effectively consolidating crucial information from the entire history. Compared to a strong baseline the Memorizing Transformer employing dense attention over a large long-term memory (64K key-value pairs) - our method demonstrates superior performance on various long-context datasets while remarkably reducing the memory footprint by a factor of 8.", "sections": [{"title": "INTRODUCTION", "content": "Long-context language models, exemplified by Gemini (Gemini-Team, 2024) and GPT (OpenAI, 2024), showcase remarkable capabilities across diverse modalities (e.g., text, images, audio, code, video) and seamlessly integrate various machine learning techniques, including many-shot in-context learning (Agarwal et al., 2024), chain-of-thought prompting (Wei et al., 2022b), and the incorporation of explicit instructions (Chung et al., 2024; Wei et al., 2022a). However, the quadratic complexity of attention mechanisms within transformer models necessitates significant computational resources to handle long contexts effectively. This has spurred the development of efficient solutions (Dai et al., 2019; Wu et al., 2022; Bulatov et al., 2022) that process long contexts via short context windows, much like how humans process information by reading a book chapter by chapter. A central question underlying these solutions is: how can we effectively model and manage memory to bridge the gaps between these short context windows over long context?\nMemory fundamentally revolves around compressing and storing information for future utilization, all within the constraints of limited capacity. The Long Short-Term Memory (LSTM) architecture (Hochreiter & Schmidhuber, 1997) tackles this by recurrently compressing historical information into hidden states after processing each token. With the rise of Transformer models (Vaswani et al., 2017) dominating the language modeling landscape, recent memory designs have shifted towards utilizing Transformers to process a context window, thereby moving the focus of memory management from the token level to the context window level.\nTransformer-XL (Dai et al., 2019) employs a caching mechanism to store multi-layer key-value (KV) pairs from the preceding window as memory. Memorizing Transformer (Wu et al., 2022) builds upon this foundation by incorporating a dedicated layer to memorize all KV pairs from that layer across all prior windows. Meanwhile, Block Recurrent Transformer (Hutchins et al., 2022) and Recurrent Memory Transformer (Bulatov et al., 2022) introduce distinct recurrent compression mechanisms, implemented in a middle layer and at the output, respectively.\nIn this paper, we introduce MELODI (short for \"MEmory with LOw DImension\"), an efficient memory architecture designed to handle long contexts despite operating on short context windows (e.g., 512 tokens per window). MELODI integrates both short-term and long-term memory through a compression-based approach. The short-term memory, recurrent in nature and possessing low capacity, spans multiple network layers, progressively compressing context tokens and prior memory at each layer. In contrast, the long-term memory, incremental and high-capacity, resides within a single network layer. It maintains a record of the entire history by further compressing each context window and stacking them. Both short-term and long-term memory are seamlessly incorporated into a multi-layer transformer model using a \u201csandwich\u201d structure (see Figure 1), incurring negligible additional parameters.\nMELODI demonstrates strong performance on various long-context datasets. For instance, utilizing a 13-layer transformer network with 1024 embedding dimensions and 512-token context windows, MELODI achieves perplexity scores of 10.44 and 2.11 on PG-19 (T5 vocabulary) and arXiv Math (Meena vocabulary), respectively. This represents a clear improvement over the Memorizing Transformer (10.62 on PG-19, 2.14 on arXiv) with dense attention (as opposed to top-k attention), while significantly reducing memory usage by a factor of 8. Furthermore, ablation studies confirm the complementary nature of short-term and long-term memory in MELODI, highlighting their synergistic contribution to an efficient and effective memory architecture."}, {"title": "OUR METHOD: MELODI", "content": "MELODI focuses on efficiently comprehending long contexts through the utilization of short context windows, thus circumventing the quadratic complexity associated with attention mechanisms over long sequences. This approach necessitates a memory design that not only ensures smooth transitions between windows but also preserves crucial information from all preceding windows."}, {"title": "ARCHITECTURE OVERVIEW", "content": "Design principle: The core principle behind MELODI is to represent short-term and long-term memory through a hierarchical compression scheme. Specifically, the short-term memory recurrently compresses context tokens across multiple network layers (e.g., condensing a 512-token context window into 128 memory tokens). This process not only facilitates seamless transitions between context windows but also aggregates information across them, effectively functioning as a fixed-size multi-layer long short-term memory (LSTM) mechanism (Hochreiter & Schmidhuber, 1997). Furthermore, each context window undergoes additional compression within a middle layer and is then stacked into a long-term memory. This long-term memory retains essential information from the"}, {"title": "SHORT-TERM MEMORY: MULTI-LAYER RECURRENT COMPRESSION", "content": "The short-term memory is distributed across multiple short-term layers (see Figure 1). This subsection delves into the specifics of the short-term layer, using the Ith layer as an illustrative example for processing the kth context window. The short-term layer serves a dual purpose: (a) transforming context tokens $x_k^{l-1}$ via a transformer block (yielding output $x_k^l$), and (b) recurrently compressing the current context window into the short-term memory $z_k^l$. It accomplishes this by updating both context tokens and short-term memory through a shared transformer block, albeit along separate pathways. As visualized in Figure 2, context tokens traverse vertically across layers (from $x_k^{l-1}$ to $x_k^l$), whereas short-term memory flows horizontally across context windows (from $z_{k-1}^l$ to $z_k^l$). To enable inter-layer communication within the short-term memory, we introduce summary tokens $u_k^l$ that propagate through the layers. We elaborate on the key components below.\nShort-term memory $z_k^l$: The short-term memory (illustrated in Figure 2) is implemented as a sequence of vectors with length S, each having the same dimensionality as context tokens (e.g., 1024 channels). Notably, the number of short-term memory vectors is substantially smaller than the length of the context window (e.g., 128 memory tokens per window of 512 context tokens). Within each context window, the short-term memory serves initially as a previous context for auto-regressive prediction of subsequent context tokens (except for the first window in which the short-term memory is empty). It is then updated by compressing and incorporating information from the context tokens within the current window.\nTransforming context tokens: The context tokens $x_k^l$ are generated through causal attention to both (a) the preceding short-term memory $z_{k-1}^l$ and (b) preceding tokens within the current context window. This attention mechanism is followed by the application of a feed-forward network (FFN). Relative position embeddings are employed for both the context tokens $x_k^{l-1}$ and the preceding"}, {"title": null, "content": "short-term memory $z_{k-1}^l$. Mathematically, this can be represented as:\n$x_k^l = T(x_k^{l-1}, z_{k-1}^l),$ (1)\nwhere $T(x|z)$ indicates applying a transformer block on x for a given preceding context z.\nRecurrent compression: Beyond transforming context tokens, the short-term layer also recurrently compresses the current context window into short-term memory. Similar to the approach in RMT (Bulatov et al., 2022) and AutoCompressors (Chevalier et al., 2023), this compression is achieved by appending summary tokens $u_k^{l-1}$ after context tokens $x_k^{l-1}$ and passing the combined sequence through the transformer block. Consequently, the resulting summary tokens compresses both the preceding short-term memory and the current context window via attentional pooling, expressed as: $u_k^l = T(\\overline{u}_k^{l-1},x_k^{l-1})$, where the input summary tokens $\\overline{u}_k^{l-1}$ originate from the previous layer (refer to Figure 2). We use $\\hat{u}_k^l$ (instead of $u_k^l$) to denote an intermediate result that is further processed in the subsequent summary branching step. In practice, both context and summary tokens can be updated simultaneously within a single transformer operation: $x_k^l, \\hat{u}_k^l = T(x_k^{l-1}, u_{k-1}^l)$. Relative position embeddings are applied on the short-term memory $z_{k-1}^l$, context $x_k^{l-1}$, and summary $u_{k-1}^l$, while a causal mask is applied on the combined sequence of $x_k^{l-1}$ and $u_{k-1}^l$.\nSummary tokens (containing U tokens) are initialized from learnable embeddings (prior to the first layer) and set to the same length as the short-term memory (U = S). Propagating through all layers, they facilitate inter-layer communication within the short-term memory. Moreover, branching summary tokens both upwards to the next layer and rightwards to the next window improves performance, a strategy we will discuss in more detail subsequently.\nSummary branching: We employ distinct linear token mixers (Tolstikhin et al., 2021) on the context and summary tokens to generate separate summary tokens for the subsequent layer and short-term memory tokens for the next window. Unlike channel mixing, a linear token mixer linearly combines the Mi input tokens across each channel to produce Mo output tokens with the same dimensionality by using an M\u2081 \u00d7 Mo matrix. This is mathematically represented as follows:\n$u_k^l = M_{\\uparrow}(x_k^l, \\hat{u}_k^l), z_k^l = M_{\\rightarrow}(x_k^l,\\hat{u}_k^l), where \\hat{u}_k^l = T(u_{k-1}^l,x_k^{l-1}),$ (2)\nwhere $M_{\\uparrow}$ and $M_{\\rightarrow}$ denote the two linear token mixers towards the subsequent layer and the next window respectively. The resulting summary and memory tokens exhibit distinct linear combination of context and summary tokens $\\hat{u}_k^l$, implying divergent compression flows across layers and windows. Since the summary and short-term memory share the same number of tokens (S'), each mixer comprises (W + S) \u00d7 S parameters, where W represent the number of context tokens per window. This parameter count is negligible for short context windows. For instance, with a context window of 512 tokens and 128 summary tokens, the two mixers collectively require (512+128)\u00d7128\u00d72=164K parameters, constituting a mere 1.3% of a transformer block with 1024 dimensions.\nSummary: The short-term memory layer can be succinctly represented as a function $z_k^l, x_k^l, u_k^l = h(z_{k-1}^l,x_k^{l-1}, u_{k-1}^l)$. This function transforms context tokens $x_k^{l-1}$ and summary tokens $u_{k-1}^l$ upward to the next layer (from l \u2212 1 to l) while simultaneously propagating short-term memory $z_{k-1}^l$ rightward to the next window (from k \u2212 1 to k). Built upon a standard transformer block, this layer introduces negligible additional parameters through the summary branching mechanism.\nRelation to Block Recurrent Transformer (BRT) (Hutchins et al., 2022): While BRT represents short-term memory by combining multi-layer KV caching from Transformer XL with a dedicated recurrent memory layer, Melodi models short-term memory as a consistent recurrent compression mechanism across multiple layers. Moreover, MELODI updates the short-term tokens by adding residual connections (cross-attending to the previous memory) over the previous layer, instead of over the previous memory tokens directly as in BRT.\nWhile short-term memory facilitates smooth transitions between context windows, the inherent limitation of its capacity can lead to the inevitable loss of information, particularly for contexts located further back in the sequence. In the following subsection, we will demonstrate how long-term memory can be leveraged to mitigate this forgetting issue."}, {"title": "LONG-TERM MEMORY: SINGLE-LAYER MEMORIZING COMPRESSED KEY-VALUE PAIRS", "content": "In this subsection, we utilize long-term memory to retain information from all previous windows, thereby alleviating the forgetting inherent in short-term memory. The key idea involves further compressing the context window and storing the compressed representations across the entire history.\nFurther compression: In contrast to the short-term memory, which compresses information across multiple layers, the long-term memory achieves a higher compression rate within a single layer. Specifically, it compresses a context window into L long-term tokens at a designated middle layer (refer to Figure 1), where L is less than the number of short-term tokens per layer (L < S). Illustratively, we might compress a context window of 512 tokens into S = 128 short-term tokens at each layer, but further condense it into L = 64 long-term tokens at a single layer.\nStoring long-term memory: The key-value (KV) pairs of long-term tokens are sequentially stored in a first-in-first-out (FIFO) queue with a maximum capacity of $Q_{max}$ windows. Consequently, the long-term memory can hold up to L \u00d7 $Q_{max}$ KV pairs. For contexts shorter than $Q_{max}$ windows, a compressed representation of the entire prior history is preserved. For longer documents exceeding $Q_{max}$ windows, a substantial portion of the recent history ($Q_{max}$ windows) is still retained. We opt to store KV pairs (rather than the tokens themselves) because they are repeatedly utilized in cross-attention mechanisms for subsequent context windows, a point we will elaborate on later.\nLong-term layer: Figure 3 illustrates the implementation of a long-term layer, which builds upon a short-term layer but incorporates three key additions. First, it introduces a long-term memory component (denoted as $m_{1:k\u22121}$ prior to the kth context window) and enables the current context tokens $x_k^{l-1}$ and summary tokens $u_k^{l-1}$ to cross-attend to it. Second, the cross-attention shares parameters with the self-attention and their results (cross attention: $A_x$, self-attention: $A_s$) are combined through a gating mechanism using a learnable scalar $\u03b1$ per attention head, formulated as $\u03b1A_x + (1 \u2212 \u03b1)A_s$. Finally, an additional linear token mixer is introduced to generate long-term tokens for the current window, and their key-value (KV) pairs $m_k$ are appended to the long-term memory. This token mixer comprises (W + U) \u00d7 L parameters, where W, U and L represent the number of context, summary and long-term memory tokens, respectively. It is notably smaller than the mixers in the short-term layer because: (a) L is less than S (the number of short-term memory tokens), and (b) it is present in only one layer."}, {"title": "CONCLUSION", "content": "In this work, we have introduced MELODI, a novel memory architecture designed to address the challenges of long document processing within the constraints of short context windows. The core innovation of MELODI lies in its hierarchical compression approach, wherein short-term memory facilitates smooth transitions between context windows through recurrent compression across multiple layers, and long-term memory preserves crucial information from the entire history by performing further compression and aggregation within a single middle layer. Our empirical evaluations on multiple long-context datasets have validated MELODI as an efficient and effective solution. The success of MELODI underscores the potential of hierarchical memory compression for tackling the complexities of long document processing. We anticipate that further research in this direction will enhance long context understanding and generation over multiple modalities."}, {"title": "LIMITATIONS", "content": "A primary limitation of our current method is its focus on training from scratch, without addressing the fine-tuning of pre-trained models with fixed context window sizes. In future work, we plan to explore adapting MELODI (short-term and long-term memory) to enhance the long-context capabilities of pre-trained models through techniques like LoRA fine-tuning."}, {"title": "TRAINING DETAILS", "content": "We use Adafactor optimizer (Shazeer & Stern, 2018) with a learning rate schedule that employs a linear warmup for the first 1000 steps, followed by cosine decay. The maximum and minimum learning rates are set to 0.01 and 0.001, respectively, as recommended in Hoffmann et al. (2022). A dropout rate of 0.05 is applied. All models are trained for 500k steps (200k for ablations) on 32 TPU cores with a batch size of 32 (1 per core)."}, {"title": "MORE COMPARISON WITH BASELINES", "content": "Comparison with baselines using a fixed context window size: Table 5 presents a comparison of MELODI against three baseline models (Transformer XL, Block Recurrent Transformer, and Memorizing Transformer) across three datasets, using a consistent segment length of 4096 tokens and a context window size of 512. The evaluation includes both 12-layer and 13-layer transformer architectures to assess the impact of model depth on performance.\nNotably, even with fewer layers, MELODI S192+L32 consistently outperforms both Transformer XL and Block Recurrent Transformer across all datasets while consuming less memory. For instance, the 12-layer variant of MELODI S192+L32 achieves a perplexity of 10.66 on PG-19 (T5 vocabulary), surpassing the 13-layer variants of Transformer XL (11.41) and Block Recurrent Transformer (10.98)."}]}