{"title": "Single-word Auditory Attention Decoding Using Deep Learning Model", "authors": ["Nhan Duc Thanh Nguyen", "Huy Phan", "Kaare Mikkelsen", "Preben Kidmose"], "abstract": "Identifying auditory attention by comparing auditory stimuli and corresponding brain responses, is known as auditory attention decoding (AAD). The majority of AAD algorithms utilize the so-called envelope entrainment mechanism, whereby auditory attention is identified by how the envelope of the auditory stream drives variation in the electroencephalography (EEG) signal. However, neural processing can also be decoded based on endogenous cognitive responses, in this case, neural responses evoked by attention to specific words in a speech stream. This approach is largely unexplored in the field of AAD but leads to a single-word auditory attention decoding problem in which an epoch of an EEG signal timed to a specific word is labeled as attended or unattended. This paper presents a deep learning approach, based on EEGNet, to address this challenge. We conducted a subject-independent evaluation on an event-based AAD dataset with three different paradigms: word category oddball, word category with competing speakers, and competing speech streams with targets. The results demonstrate that the adapted model is capable of exploiting cognitive-related spatiotemporal EEG features and achieving at least 58% accuracy on the most realistic competing paradigm for the unseen subjects. To our knowledge, this is the first study dealing with this problem.", "sections": [{"title": "I. INTRODUCTION", "content": "THE human brain possesses a remarkable ability to perceptually segregate concurrent auditory objects and selectively attend to the objects of interest. Identifying these objects, based on electrophysiological signals such as electroencephalography (EEG), electrocorticography (ECOG), and magnetoencephalography (MEG), is commonly referred to as auditory attention decoding (AAD). Current state-of-the-art AAD studies have focused on an audio envelope reconstruction approach in which the neural response to auditory attention is considered as entrainment to the audio envelope of the attended auditory stream [1]\u2013[5]. The attended stream is identified as the stream whose envelope exhibits the highest correlation with the reconstructed envelope. These methods require detailed knowledge of stimuli and are primarily based on the exogenous response. Recently, a new study, which focuses on the cognitive responses to specific auditory events in a stream [6], has shown that the cognitive processing of single-word speech events in a multi-talker environment elicits an event-related potential (ERP). This finding opens the door to a simplified version of the AAD problem, involving classifying a single EEG epoch around a word onset as either attended or unattended. In this paper, we investigate the feasibility of this approach.\nAs mentioned earlier, this is a different approach to address the AAD problem compared to the envelope following response [1]\u2013[5]. However, the proposed approach can be compared to single-trial ERP classification, as addressed in various Brain-Computer Interface (BCI) applications. One of the very first published BCI systems, based on a cognitive evoked potential known as the P300 component, was the so-called P300 speller, introduced by Farwell et al. in 1988 [7]. Since then, various methods have been proposed using different signal processing algorithms and conventional classifiers such as an ensemble of support vector machines [8], gradient-boosting [9], linear discriminant analysis [10]\u2013[12], ICA [13], [14], and xDAWN [15]. Recently, the development of algorithms for single-trial ERP classification has been significantly influenced by deep learning methods. The convolutional neural networks (CNNs) approach has been reported to outperform the other methods when the number of electrodes is restricted to 8 [16]. In 2018, Lawhern et al. introduced EEGNet, a compact convolutional neural network [17], which can extract EEG features and generalize across BCI paradigms. Shi et al. [18] proposed a convolutional long short-term memory (ConvLSTM) architecture that could effectively capture spatiotemporal correlations for predicting rainfall. This architecture was later integrated into ensemble models [19] to achieve better performance in P300 speller classification. However, compared to the EEGNet, the drawback of ConvLSTM, and other modern structures such as the transformer, is that it, in general, requires more data to train. Several approaches have attempted to improve the performance by combining recurrent neural network (RNN) with CNN such as CNN-RNN-Net [20] and CNN-LSTM [21]. However, while these models have tended to be larger than EEGNet, they have not exhibited superiority over the EEGNet in generalization.\nIn this study, we investigate the feasibility of single-word attention classification, based on the data set described in [6]. However, due to its specialized nature, the data set is quite"}, {"title": "II. MATERIALS AND METHODS", "content": "A. Experimental dataset\nIn this work, we use data from the first three out of the four paradigms in the event-based AAD dataset [6] which focuses on AAD based on the cognitive processing of speech events. Paradigm 1, word category oddball, is similar to a conventional oddball paradigm in which the standard events and target events are short spoken words: cardinal numbers and animal names/color names, respectively. In Paradigm 2, word category with competing speakers, similar stimuli were simultaneously presented from two loudspeakers. The participant was instructed to pay attention to only the target events in one of the streams, completely ignore the other and passively count the number of target events in the attended stream. In Paradigm 3, competing speech streams with targets, a similar setup of two competing streams was used as in Paradigm 2. However, in each stream, audio snippets of different stories were used instead of discrete spoken words. Within each story, a category of words was designated as target words, chosen from one of four categories: animal names, human names, color names, and plant species.\nEEG data from 24 normal-hearing subjects were recorded and sampled at 1000 Hz using 32 scalp electrodes. The data was pre-processed by: down-sampling to 256 Hz, applying a zero-phase FIR band-pass filter from 0.5 to 40 Hz, applying the Independent Component Analysis (ICA) method to reject eye artifacts, and epoching data from -200 milliseconds to 1 second relative to the middle of words with 200-\u00b5V peak-to-peak rejection. Finally, epochs were labeled as attended or unattended according to the experimental conditions. It is important to note that attended epochs in Paradigm 2 and 3 are based on the target events in the attended stream and unattended epochs are based on all the events in the unattended stream. In the end, for each subject, the number of attended and unattended epochs across the three paradigms (before applying epoch rejection) are 60 and 260, 74 and 400, 109 and 1172, respectively. More details on the experimental protocol and setup can be found in the original study [6]. Hereafter, this dataset is referred to as EventAAD data.\nB. Data augmentation\nAs mentioned above, the EventAAD dataset is relatively small and inherently imbalanced due to the oddball design. This is a common challenge in BCI systems data, and the challenge with limited data is even more pressing with a deep learning model as the number of parameters increases. We address these problems with two data augmentation methods tailored to ERPs: upsampling by averaging and simulation of target ERPs.\nUpsampling by averaging method is inspired by the traditional averaging method of ERP quantification. For each subject and for each class (attended / unattended), a new epoch is created by averaging $k$ random epochs drawn without replacement from that class. In this work, $k$ was randomly chosen from 1 to 3. It is important to note the effect of $k$: as it increases, a clearer ERP pattern is induced into the training data. Besides giving a greater variation in the signal-to-noise ratios seen by the model, it potentially helps the model focus on the most promising patterns in the data. In the case of $k = 1$, the randomly drawn epochs are merely duplicated. Upsampling was repeated until the number of epochs in each class was double the number of unattended epochs in the original dataset, resulting in a new dataset called the upsampled experimental dataset.\nERP simulation method was introduced by Depuydt et al. [22] for training neural networks to quantify ERPs. The method is to synthesize a new target (attended) epoch by adding a target (attended) ERP waveform at a known latency to a random non-target (unattended) epoch. In this work, instead of using a half-cycle sinusoidal wave as in the original study, the attended ERP waveform was estimated individually by averaging all attended epochs for each subject to closely resemble the experimental data. To take the inter-trial variability into account, the latency of the ERP waveform was uniformly sampled around the real latency of the individual ERP with a standard deviation of 10 ms. The width of the ERP waveform was uniformly sampled between 300 ms and 600 ms, which is the expected range for the P300 component in literature [6], [23]. Variation of the ERP amplitude was induced by randomly amplifying the ERP amplitude by 0 dB, 3 dB, and 6 dB. To increase the number of epochs in the simulated dataset, first, the unattended epochs were upsampled by a factor of 4. Subsequently, half of these scaled epochs were used to synthesize new attended epochs. A visual overview of the augmentation process is shown in Fig. 1.\nIn summary, two augmentation methods were applied to the data of three paradigms for each subject to create an augmented dataset which included one upsampled experimental (upsampled exp.) dataset and three simulated datasets corresponding to 3 SNR levels: 0 dB, 3 dB, and 6 dB. Details of these datasets are shown in Table I"}, {"title": "C. EEGNet", "content": "The proposed network is a compact CNN architecture with depthwise and separable convolutions, adapted from the EEGNet [17], originally introduced for EEG-based BCIs. The network comprises two main convolutional blocks and one fully connected classification layer. The first block contains $F_1$ 2D frequency-oriented convolutional filters of kernel size $(1, K_1)$ to capture the frequency information followed by $D \\cdot F_1$ depthwise spatial-oriented convolution of size $(C, 1)$ to learn a spatial filter. $K_1$ is set at half the sampling rate. $D$ is a depth parameter to control the number of spatial filters. $C$ is the number of EEG channels. The next layer sequentially includes a batch normalization, an exponential linear unit (ELU) activation, an average pooling, and a dropout layer. In the second block, to decouple the relationship between feature maps outputted by the first block, it is designed to have $F_2$ separable convolutional filters of size $(1, K_2)$, followed by batch normalization, ELU activation, average pooling, and dropout. Finally, the output of the second block is passed to the classification layer which consists of a fully-connected layer and a sigmoid activation. Here, to fit the model to a 32-channel, 256 Hz sampling rate dataset, the following set of parameters: $F_1 = 8, K_1 = 128, D = 2, C = 32, F_2 = 32$ and $K_2 = 16$ were used."}, {"title": "D. Training and validation", "content": "We performed two comparisons:\n\u2022 The performance of the models trained with and without data augmentation.\n\u2022 The performance of the paradigm-independent model (trained on the combined augmented dataset) and paradigm-specific models (trained separately on the augmented data of each paradigm)\nThe performance of each model was evaluated by two validation schemes: multi-fold cross validation and leave-one-subject-out (LOSO) validation.\n1) Multi-fold cross validation: An 8-fold cross-validation was performed for each model. The data was split into 8 folds. 7 out of the 8 folds were split into a training set and a validation set with a ratio of 4:1 to train the model while the remaining fold was held out for the test set. In the cases of augmented datasets, to ensure that the training set and validation set were balanced and that all unattended epochs used to simulate new attended epochs in the validation set were not seen in the training set, the 8-fold split was performed before the data augmentation and based on a trial basis across all paradigms. The trained models were then evaluated on each paradigm of the test set which was not augmented. This process was repeated for the different folds and the mean and standard deviation of accuracy were then calculated. The result of this validation is called subject-pooled performance. To compensate for the imbalance in the original test set, the accuracy for each class was first calculated. The average of these accuracies was then taken as the reported accuracy.\n2) LOSO validation: To evaluate the performance of models on unseen subjects, a LOSO validation was performed. The data of 23 subjects was split into a training set and a validation set in a 4:1 ratio to train the model. The remaining subject"}, {"title": "III. RESULTS", "content": "A. Subject-pooled performance\nFig. 2 shows the 8-fold average performance of the models trained on: the original dataset (paradigm-independent model without augmentation), the augmented dataset (paradigm-independent model with augmentation), the augmented data of each paradigm (paradigm-specific model with augmentation), and the envelope-based linear model (only for Paradigm 3). The columns 'Prdm. 1', 'Prdm. 2' and 'Prdm. 3' indicate the model performances on the non-augmented data of the test set of Paradigms 1, 2, and 3, respectively.\nComparing models with and without data augmentation, the model trained on the original data performed significantly above the chance level but significantly worse than the model trained on the augmented dataset for all three paradigms ($p < 0.001$): 0.568 vs. 0.722 for Paradigm 1, 0.557 vs. 0.707, for Paradigm 2 and 0.507 vs. 0.594 for Paradigm 3.\nComparing paradigm-specific and paradigm-independent models, the paradigm-specific models performed significantly better than the paradigm-independent model in Paradigm 1 (0.751 vs. 0.722, $p < 0.001$) and Paradigm 2 (0.732 vs. 0.707, $p = 0.016$) but not in Paradigm 3 (0.596 vs. 0.594, $p = 0.4$). Additionally, we compare the performance of the model (trained on the augmented dataset) on different paradigms and observe that the model performs significantly better on Paradigm 1 and 2 than on Paradigm 3 ($p < 0.001$).\nWhen comparing our model to the envelope-based linear model, we only use Paradigm 3, which is the most similar to what is done in the literature. The linear model performance is evaluated using the analysis window length of 1.2 s, corresponding to the length of the epoch in this study. We find that the paradigm-specific model significantly outperforms the linear model (0.596 vs 0.571, $p < 0.001$).\nB. LOSO performance\nThe LOSO classification results illustrate a similar pattern as of the subject-pooled performance (see Fig. 3). The performance of the model trained on the augmented dataset is"}, {"title": "IV. DISCUSSION AND CONCLUSIONS", "content": "We find that it is in fact possible to decode attention on a single-word basis, however, even with a relatively small model, significant amounts of data are needed. Fortunately, the data augmentation strategy outlined here turned out to be effective. In this regard, it is interesting to note that even though the constructed epochs generally have a higher signal-to-noise ratio than the raw data, their inclusion still leads to a significant improvement in final model performance (it is crucial to remember here that data augmentation was not applied to the test data). Additionally, the upsampling by averaging method tends to preserve the latency and width of the ERP components and is, therefore, suitable for classifying ERPs with less inter-subject variation in latency and width. The ERP simulation method, on the other hand, introduces variability in new data points for both latency and width and could therefore be beneficial to apply for classifying highly variable ERP components across subjects.\nThe previous work with this data set [6] found distinctly 'cleaner' ERPs for Paradigms 1 and 2 compared to Paradigm 3, and, not surprisingly, we find that this makes single-word decoding easier. Additionally, the paradigm-specific models showed better performance than the paradigm-independent model, even though the three paradigms represent the same cognitive ERP component triggered by the same task as in the original work [6]. A potential reason for this could be that the model's capability to generalize across all three paradigms is limited. This suggests a direction for future work towards a more advanced model, possibly to uncover what part of the ERP the model relies on.\nThe LOSO validation results show a slight drop in performance for the unseen subjects compared to the performance of the subject-pooled data. This decrease in performance is likely due to the inter-subject variability in cognitive response latency and topography. Nevertheless, while this challenge is common in BCI systems, it is not insurmountable. We believe the accuracy can be improved by fine-tuning the model using individual subject data.\nThe results show that a relatively light-weight EEGNet-based model effectively extracts spatial and temporal features from the cognitive component triggered by single-word auditory attention. As mentioned in the introduction, the method proposed here differs from the conventional approach of detecting the envelope-following response. This difference makes it difficult to perform an apples-to-apples comparison between the results shown here and the state-of-the-art in AAD. However, we tested the performance of the linear model [1], which is reported as the most consistent AAD method using the envelope feature [25], for time windows of 1.2 seconds, and found that our proposed method obtained a consistently better performance.\nIt is worth noting that since this approach focuses on the endogenous response instead of the (stimuli-driven) exogenous response, a model could possibly be developed that combines this approach with the envelope-following response. We believe that this possibility is one of the main points of interest in our findings."}]}