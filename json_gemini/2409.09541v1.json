{"title": "Autonomous Goal Detection and Cessation in Reinforcement Learning: A Case Study on Source Term Estimation", "authors": ["Yiwei Shi", "Muning Wen", "Qi Zhang", "Weinan Zhang", "Cunjia Liu", "Weiru Liu"], "abstract": "Reinforcement Learning has revolutionized decision-making processes in dynamic environments, yet it often struggles with autonomously detecting and achieving goals without clear feedback signals. For example, in a Source Term Estimation problem, the lack of precise environmental information makes it challenging to provide clear feedback signals and to define and evaluate how the source's location is determined. To address this challenge, the Autonomous Goal Detection and Cessation (AGDC) module was developed, enhancing various RL algorithms by incorporating a self-feedback mechanism for autonomous goal detection and cessation upon task completion. Our method effectively identifies and ceases undefined goals by approximating the agent's belief, significantly enhancing the capabilities of RL algorithms in environments with limited feedback. To validate effectiveness of our approach, we integrated AGDC with deep Q-Network, proximal policy optimization, and deep deterministic policy gradient algorithms, and evaluated its performance on the Source Term Estimation problem. The experimental results showed that AGDC-enhanced RL algorithms significantly outperformed traditional statistical methods such as infotaxis, entrotaxis, and dual control for exploitation and exploration, as well as a non-statistical random action selection method. These improvements were evident in terms of success rate, mean traveled distance, and search time, highlighting AGDC's effectiveness and efficiency in complex, real-world scenarios.", "sections": [{"title": "Introduction", "content": "Reinforcement Learning (RL) optimizes decision-making and behavior in dynamic environments through trial and error and reward mechanisms. It is widely used in fields such as gaming [1,2], robotic control [3], autonomous driving [4], and industrial automation [5]. RL involves agents interacting with the environment, learning through exploration and exploitation to achieve their goals. However, in real-world applications like the Source Term Estimation (STE) problem [6], the environment often lacks clear signals for the end of an episode or direct rewards, making it difficult for the agent to determine task completion.\nSource Term Estimation involves identifying the location and characteristics (such as release rate) of hazardous gas emissions in the atmosphere, which is crucial for environmental monitoring and emergency response. However, this task presents several challenges. Firstly, the environment often does not provide clear cessation signals, and gas releases are typically invisible. Secondly, atmospheric turbulence results in sensor data that is sparse, intermittent, and time-varying. Additionally, direct measurements within the hazardous material dispersion area are often dangerous. Furthermore, sensor measurements are highly affected by noise and discontinuity. Consequently, agents must rapidly adjust their strategies in response to environmental changes and accurately estimate the source term despite unclear feedback. These challenges render source term estimation a complex task with significant uncertainty.\nStatistical-based methods such as Infotaxis, Entrotaxis, and Dual Control for Exploitation and Exploration (DCEE) provide some solutions to these challenges. Infotaxis in [7,8,9] guides the search process by maximizing the rate of information acquisition,\nWhile Reinforcement Learning offers control capabilities, significantly improving efficiency and success rates, and avoiding local optima with good generalization capabilities, it effectively overcomes the limitations of statistical methods. However, RL struggles to effectively identify and autonomously cease goals in the absence of clear feedback. Traditional statistical methods consist of two modules: one for providing the agent with reward signals based on estimates, where rewards or information gain originate, and the other for determining the optimal action based on information gain. By replacing the control module of traditional statistical methods with RL while retaining the estimation module, a self-feedback mechanism can be introduced to RL. This helps RL algorithms check and cease goals at appropriate times. Although the rewards for the agent remain sparse in this context, the self-feedback mechanism provides signals from the agent itself rather than the environment, thus better supporting the training process.\nBuilding on this foundation, we propose the concept of Autonomous Goal Detection and Cessation to address the STE problem using RL. AGDC introduces an autonomous goal detection and cessation mechanism within the RL framework, enabling the agent to automatically recognize and cease actions upon task completion. Specifically, the AGDC module employs Bayesian inference to estimate environmental dynamics and dynamically assess task progress. When the standard deviation of the estimated parameter values for environmental dynamics reaches a preset threshold, a cessation signal is automatically triggered. This approach not only addresses the issue of insufficient environmental feedback but also significantly enhances the adaptability and effectiveness of RL in complex and dynamic environments. By integrating the self-feedback mechanism with AGDC, RL algorithms can more efficiently accomplish STE tasks, thereby significantly improving capabilities in environmental monitoring and emergency response. \nThe main contributions of this paper are as follows: (1) We are the first to introduce, to the best of our knowledge, the AGDC module, which enables agents to autonomously recognize and evaluate goals, significantly increasing the capability of RL algorithms in feedback-limited environments. (2) We demonstrate the successful integration of AGDC with multiple RL algorithms in addressing the STE problem, achieving notable"}, {"title": "Problem Formulation and Peliminaries", "content": "In a two-dimensional search area denoted as $\\Omega \\subseteq R^2$, where there is an expectation of encountering a hazardous release, a robot equipped with a gas sensor is tasked with traversing the area to calculate the release parameters, also referred to as the source term . This data will serve as the requisite input for a convection-diffusion model [7], enabling the generation of hazard forecasts. Within this context, in an environment characterized by average wind speed $u_s \\in R^+$ in meters per second (m/s), wind direction $\\phi_s$ in radians (rad), and diffusivity $d_s$ in meters squared per second (m\u00b2/s), we detect the concentration of a hazardous substance $x_k \\in R^+$ using a robot or a sensor located at position $p_k = [x_k, y_k]^T \\in \\Omega$ in meters (m). This hazardous material stems from a source term located at $p_s \\in \\Omega$, and it is released at a rate/strength $q_s \\in R^+$ in grams per second (g/s) with an average lifespan of $\\tau_s \\in R^+$ in seconds (s).\nConsequently, the parameters of the source term can be represented as follows:\n$\\Theta_s = [p_s^T, q_s, u_s, \\phi_s, d_s, \\tau_s]^T$ (1)\nNote: It is assumed in this study that all other parameters, except for the location of the leak source, can be obtained through direct measurement. Therefore, the primary objective is to successfully obtain the source term $p_s^T$, as it is the most important information about the source term."}, {"title": "Gaseous Diffusion Model", "content": "The average gas concentration [14], denoted as $m(p|\\Theta_s)$, for a given location of the mobile sensor at position $p_k$ over a time duration of $\\tau_s$, can be computed utilizing the parameters of the source terms as follows:\n$m(p_k|\\Theta_s) = \\frac{q_s}{4 \\pi d_s ||p_k - p_s ||} exp {-\\frac{||p_k - p_s ||}{\\lambda} + \\psi}$,\nwhere\n$\\psi = -(x_k - x_s)u_s cos \\phi_s/2d_s-(y_k \u2013 y_s)u_s sin \\phi_s/2d_s$\n$\\lambda = \\sqrt{d_s\\tau_s/[1+ (u_s \\tau_s/4d_s)]}$."}, {"title": "Sensor Models", "content": "When mobile robots equipped with sensors conduct gas concentration detection in an environment, the measurement results are influenced by both internal and external factors. Internal factors include the sensitivity and calibration issues of the sensors, as"}, {"title": "Partially Observable Markov Decision Process", "content": "In the search area, the robot's task is to estimate the location of a gas leak source with minimal movement steps. The robot lacks prior knowledge of the leak source's location and gathers data through measurements at each step, making decisions based on this information. To address this challenge, we use the Partially Observable Markov Decision Process (POMDP) Reinforcement Learning method. This approach enables the robot to make effective decisions in an environment that is not fully observable, allowing for accurate localization of the gas leak source. A POMDP is a tuple (S, O, A, T, Pr, R, \u03b3), where S is a set of states, O is a set of observations, A is a set of actions, T(s'|s, a) : S \u00d7 A \u2192 S is transition probabilities, Pr(o|s, a) : S \u2192 O is observation probabilities, R(s,a): S \u00d7 A \u2192 R is a reward function, and y is a discount factor. The objective in POMDP is to obtain an optimal policy to maximize expected cumulative rewards $\\sum_{k=0}^{\\infty} \\gamma^k R$."}, {"title": "Methodology", "content": null}, {"title": "Bayesian Approximation for Belief Distribution", "content": "The STE challenge is characterized as a Partially Observable Markov Decision Process, which implies that optimal decision-making cannot rely solely on the present observation, as complete state information in environment is not available. To address this issue, we have adopted this approach combining RL and Bayesian inference to effectively estimate environmental dynamics (belief) distribution and provide more unobservable information to assist decision-making.\nVariational inference [18] and particle filter [19] are common Bayesian inference methods. The former approximates the target distribution by optimizing a differentiable lower bound but typically requires simplifying assumptions that may not suit complex environments. The latter, on the other hand, handles nonlinear and non-Gaussian distributions more effectively and adapts to dynamic conditions without needing such assumptions. Particle filter approximate the true distribution through sufficient sampling and offer flexibility in managing multimodal distributions, tracking potential source locations and release rates, and providing real-time updates. Therefore, particle filter outperform variational inference in STE problems.\nThe Particle filter is employed for iterative calculations of the environmental dynamics distribution. The belief distribution at time step k is approximated using a set of N particles, which are random samples {$\\Theta_k^i, w_k^i$}$_{i=1:N}$, where $\\Theta_k^i$ represents the ith point estimation of source parameters (i.e., the belief), and $w_k^i$ is the associated weight, with $\\sum_{i=1}^{N} w_k^i=1$. The approximated belief distribution $b(\\Theta_k)$ is expressed using samples"}, {"title": "Autonomous Goal Detection and Cessation", "content": "Particle filter not only estimates the distribution of unknown environments but also combines with observational data to provide a more accurate representation of the true state. Additionally, it can generate self-cessation evaluation signals, indicating when computation can be ceased. When the goal is achieved, the particles forming the point estimate converge to a smaller range. By calculating the standard deviation of the"}, {"title": "Reinforcement Learning with AGDC", "content": "particle parameters, the degree of parameter convergence can be observed, facilitating the judgment of goal completion progress.\nWhen the standard deviation (STD) of the belief $\\Theta_k$ by particle filter is lower than the given Cessation Threshold $\\zeta$, the search ceases, and we consider the source term to be successfully estimated, meaning the goal is achieved. As a result, the agent rewards itself rather than relying on the environment. STD is denoted as:\n$STD = \\sqrt{diag(Cov(\\Theta))}$, (7)\nwhere $Cov(\u00b7)$ represents the covariance, and $diag(\u00b7)$ denotes the trace of the matrix.\nCalculating the STD of the particle filter belief and comparing it with a threshold as a signal to cease the search process is effective because the standard deviation reflects the concentration level of the belief distribution. A smaller standard deviation indicates that most particles are clustered around a certain estimate, suggesting that the belief state has converged. This convergence implies that the robot's estimation of the source term parameters (such as the leak location) has become more accurate and stable. When the STD is less than the preset threshold $\\zeta$, it indicates that the belief state estimation is sufficiently precise, making further algorithm execution yield diminishing returns, thus allowing the search process to cease, saving computational resources and time. Additionally, using the standard deviation as a cessation condition is relatively simple"}, {"title": "Experiment", "content": "In this paper, we utilized the STE Environment (STEenv) as the experimental platform to investigate the application of AGDC in RL. The STE Environment, described in Section 2, comprises a Gaussian diffusion model and a sensor model. Instead of providing rewards to the Agent, it transmits the current position and the test concentration at that position as observations. Any RL algorithm can be integrated with the AGDC module to address the challenges posed by the STE problem."}, {"title": "Baseline Algorithms and Evaluation Metrics", "content": "There are three evaluation metrics in this paper: Success Rate (SR) to measure the number of successful source term estimations in the trajectories, Mean Traveled Distance (MTD) to represent the average distance traveled before a successful estimation occurs, and Search Time (ST) to measure the duration from the initiation of a search to the successful estimation of the target information. In this context, achieving a shorter traveled distance while successfully estimating the source's location more frequently, along with a shorter search time indicating a quicker and more effective search process, signifies a more efficient approach.\nWe integrate the AGDC module into the DQN, PPO, and DDPG algorithms to address the STE problem. Additionally, we use four baseline approaches: Infotaxis, Entrotaxis, Dual Control for Exploitation and Exploration (DCEE), and a method that randomly selects actions for control. The first three are statistical methods, while the last one is a non-statistical method based on random action selection.\nInfotaxis [7] aims to minimize the predicted posterior variance of the source location. It treats the search process as an information-gathering problem, where the objective is to reduce uncertainty about the source location."}, {"title": "Scenario Parameterization and Evaluation", "content": "The parameters for the training scenarios, set within a 30 \u00d7 30 area, are constructed by randomly initializing the source and environmental properties at the beginning of each training episode, including parameters such as the gas source location, wind speed, and wind direction, all of which are sampled from the probability distributions presented in Table 1. The agent starts its search from a random location within the (0,5) \u00d7 (0,5) area, with a speed of 1 meter per step, ensuring that it encounters a diverse set of scenarios during training, thereby promoting robust learning. The parameters for the"}, {"title": "Particle Numbers and Cessation Thresholds", "content": "We will discuss the effectiveness of integrating the AGDC module with RL by comparing three AGDC-enhanced RL algorithms against four baseline methods. Before delving into these comparisons, it is important to introduce two critical hyperparameters: Particle Numbers (PN) and Cessation Thresholds (CT) $\\zeta$. The Particle Numbers are pivotal for AGDC as they determine the number of samples used to approximate the point distribution, which is essential for accurately estimating the belief necessary for goal detection. The Cessation Thresholds are directly linked to the cessation aspect of AGDC, as they define the criteria for deciding when to stop the task, ensuring that the process concludes at the most appropriate moment. Two groups of experiments were conducted to investigate the impact of two key hyperparameters on the performance of different algorithms. The first group analyzed the variations in success rate, mean traveled distance, and search time as the number of particles increased from 100 to 4000. The second group examined algorithm performance under different cessation thresholds, ranging from 0.1 to 0.9. The results are presented in Figure 3. \nAnalysis Across Various Particle Numbers: The success rates of AGDC-enhanced RL algorithms, such as AGDC-DQN, AGDC-PPO, and AGDC-DDPG, improve steadily with increasing particle numbers, reaching 95.27% for AGDC-PPO at 4000 particles. These algorithms outperform baseline methods like Infotaxis, Entrotaxis, and DCEE, which also benefit from more particles but remain less effective. The Random method remains largely ineffective, with success rates below 5%, regardless of particle count. Notably, DDPG's continuous action space allows for more flexible movement compared to the discrete steps of other methods. AGDC-enhanced algorithms also show a consistent reduction in mean traveled distance as particle numbers increase. For instance, AGDC-DDPG reduces its mean distance from 34.33 at 100 particles to 16.83 at 4000,\ndemonstrating greater efficiency. In contrast, baseline methods like Entrotaxis exhibit significantly higher distances, highlighting their inefficiency. The Random method, unaffected by particle count, continues to exhibit very high traveled distances. Regarding search time, AGDC-enhanced algorithms consistently achieve quicker convergence compared to baseline methods. AGDC-DQN, for example, maintains a search time of 0.94 units at 4000 particles. In contrast, methods like Infotaxis take significantly longer, reaching 3.15 units, indicating slower and less effective strategies. The Random method, despite its speed, fails to produce meaningful results, underscoring its overall inefficiency.\nAnalysis Across Various Cessation Thresholds: The success rates of AGDC-enhanced algorithms, including AGDC-DQN, AGDC-PPO, and AGDC-DDPG, improve with higher cessation thresholds, with AGDC-PPO achieving 99.13% at a threshold of 0.9. This is due to the algorithms accumulating more confidence before stopping, leading to greater accuracy. Although Infotaxis and Entrotaxis also improve with higher thresholds, they require significantly more time and distance to reach similar success levels. As cessation thresholds rise, AGDC-enhanced algorithms also show reduced mean traveled distances, reflecting more efficient search cessation. AGDC-DDPG, for instance, reduces its distance to 12.8 units at a threshold of 0.9, while Infotaxis, even at its best, remains at 56.4 units, much higher than AGDC-DDPG. Search times for AGDC-enhanced algorithms decrease with higher thresholds, with AGDC-PPO reducing its time to 0.17 units at a threshold of 0.9. This indicates that these algorithms can efficiently conclude the search once confident in their estimation. However, excessively high thresholds may"}, {"title": "Further Research Experiments", "content": "An additional experiment was conducted to explore the AGDC module through a visualization approach. This experiment uses only the AGDC-based QDN method, with the environment set within a 20x20 area, and the source randomly located within the x, y from U(15, 20). The experimental results are shown in Figure 5 and 6. In Figure 5, 100 trajectories are displayed, with the red lines representing the agent's actual paths. The color intensity correlates with the time step count, with darker lines indicating more steps. The blue lines show the changes in the estimated source position. As the agent gets closer to the true source location (the goal), the estimated position (derived from Belief) also becomes more accurate, converging towards the goal.\nSimilarly, in Figure 6, the red line represents the distance between the agent and the true goal at each step, while the blue line shows the distance between the agent and"}, {"title": "Conclusion", "content": "In this study, we introduced the concept of Autonomous Goal Detection and Cessation within the Reinforcement Learning framework to address the challenges of STE. By in-"}]}