{"title": "Generative Physical AI in Vision: A Survey", "authors": ["Daochang Liu", "Junyu Zhang", "Anh-Dung Dinh", "Eunbyung Park", "Shichao Zhang", "Chang Xu"], "abstract": "Generative Artificial Intelligence (AI) has rapidly advanced the field of computer vision by enabling machines to create and interpret visual data with unprecedented sophistication. This transformation builds upon a foundation of generative models to produce realistic images, videos, and 3D/4D content. Traditionally, generative models primarily focus on visual fidelity while often neglecting the physical plausibility of generated content. This gap limits their effectiveness in applications requiring adherence to real-world physical laws, such as robotics, autonomous systems, and scientific simulations. As generative AI evolves to increasingly integrate physical realism and dynamic simulation, its potential to function as a \"world simulator\" expands-enabling the modeling of interactions governed by physics and bridging the divide between virtual and physical realities. This survey systematically reviews this emerging field of physics-aware generative AI in computer vision, categorizing methods based on how they incorporate physical knowledge-either through explicit simulation or implicit learning. We analyze key paradigms, discuss evaluation protocols, and identify future research directions. By offering a comprehensive overview, this survey aims to help future developments in physically grounded generation for vision. The reviewed papers are summarized at https://github.com/BestJunYu/Awesome-Physics-aware-Generation.", "sections": [{"title": "INTRODUCTION", "content": "GENERATIVE learning has long been a foundational pillar of modern computer vision, addressing key challenges in understanding, synthesizing, and manipulating visual data. Over the past decade, this field has witnessed rapid evolution of diverse generative models, including Variational Autoencoders (VAEs) [1], [2], Generative Adversarial Networks (GANs) [3], Diffusion Models (DMs) [4], [5], [6], Neural Radiance Fields (NeRFs) [7], Gaussian Splatting (GS) [8], and Visual Autoregressive Models (VARs) [9]. These models have continually pushed the boundaries of generative learning, leveraging increasingly powerful architectures to capture the underlying distributions of visual data. The goal is to enable machines to reason about the visual world in ways that mirror human creativity and understanding, by imagining novel instances of visual content under unseen scenarios. Among these advancements, diffusion models have emerged as particularly noteworthy for their ability to produce highly realistic outputs. By iteratively refining random noise through learned denoising processes, diffusion models demonstrate exceptional robustness and versatility, making them a cornerstone of recent generative methodologies. The applications of generative models span diverse modalities of visual contents, including image generation with semantics understanding, video generation with dynamic temporal understanding, 3D content generation with enhanced spatial understanding [10], [11], [12], and 4D contents\nSurvey Scope. The scope of this survey is about generative models in computer vision that enhance the physical awareness of the generation outputs. Therefore, we exclude from our scope the literature that incorporates physical principles as prior knowledge or inductive biases in model or neural architecture design, such as Physics-Informed Neural Networks (PINNs) [67], [68], even if the task is related to generative learning, e.g., [69], [70], [71]. We focus on generative tasks, and thus exclude image processing tasks such as de-blurring, de-hazing, and enhancement from our scope, though we notice that a large body of these works incorporate physics. To concentrate on computer vision, pure graphic and rendering research combined with physical simulation is also excluded from our scope."}, {"title": "FORMULATION", "content": "In this section, we first provide a definition of physics-aware generation and related concepts, such as physical simulation and physical understanding. Based on the definition, we further identify the common paradigms of incorporating physics into generative models in vision, to provide a structural perspective for later sections of this survey."}, {"title": "Definitions", "content": "Let Pe denotes a physical simulation model with physical parameters 0, and G represents a generative model, we can then provide the following definitions:\nPhysics Simulation (PS):\n$P_\\theta(X) \\rightarrow X'$.\nPhysics simulation is the process evolving the input observation X into the output observation X' using the physical model $P_\\theta$, where the observations X, X' can be from different simulation time steps.\nPhysics Understanding (PU):\n$X \\rightarrow P_\\theta$.\nPhysics understanding is the process inferring the underlying physical model Pe from the observation X such as video data. Physics understanding can also be conducted to only infer the physical parameters @ given a predefined physical model P.\nGeneration (G):\n$G(X) \\rightarrow X'$.\nGeneration is the process creating new content X' from the input condition X using the generative model G. The input and output can take various forms of modalities depending on the specific task. If the generation process is general-purposed and does not necessarily involve a strong understanding of the physical world, we call it as physics-unaware generation (PUG)."}, {"title": "Formulation of Physics-Aware Generation", "content": "Based on the above concepts, we further define the concept of physics-aware generation (PAG) as the generation process with a strong understanding of real-world physics. We divide physics-aware generation into two major categories, i.e., physics-aware generation with explicit physical simulation (PAG-E), and physics-aware generation without explicit physical simulation (PAG-I). This division is based on whether the generative model utilizes a physical simulation model explicitly to improve physical awareness."}, {"title": "PHYSICAL SIMULATION", "content": "Physical simulation refers to the use of simulated environments or models that mimic real-world physical systems, allowing the generative models to learn and infer without direct interaction with the physical world. In the following, we summarize the core elements of physical simulations commonly used in physics-aware generation research, along the dimensions of physical material properties, simulation methods, and off-the-shelf physical engines."}, {"title": "Physical Materials", "content": "In this section, we introduce several physical materials widely used in physical simulation when integrated with physics-aware generation, as detailed below.\nRigid Body. This refers to a material or object that does not deform when subjected to applied forces [83], [84], [85], [86], [87], [88], [89], [90], [91]. In physics-aware generative models, rigid bodies are often used to represent components that maintain their shape and volume. These models simulate rigid body dynamics, including translation, rotation, and collision, to predict the movement of objects.\nSoft Body. Soft body is a material [85], [92] that can undergo large deformations when subjected to external forces such as biological tissues, rubber, gels, cloth, and polymers. They exhibit properties like elasticity, viscoelasticity, and non-linear behavior, and they often have complex internal structures.\nNewtonian Fluids. Newtonian fluids [89], [93], [94], [95] are fluids whose viscosity (resistance to flow) remains constant regardless of the applied stress or the rate of flow. Common examples include water, air, and most gases. In the modeling process, fluid motion can be described using standard equations like the Navier-Stokes equations.\nNon-Newtonian Fluids. These are fluids whose viscosity changes when a stress is applied or when the rate of shear changes [93], [94], [96]. Examples include ketchup, cornstarch in water, and blood. Non-Newtonian fluids can exhibit behaviors such as shear-thinning (viscosity decreases with increasing shear rate), shear-thickening (viscosity increases with increasing shear rate), or even more complex behaviors like viscoelasticity.\nPlasticine. Plasticine [86], [88], [91], [93], [94], [97], [98], as a soft and viscoelastic material, is often modeled using viscoelastic and soft body physics approaches. These methods capture both the elastic (rebound) and viscous (flowing or slow deformation) aspects of plasticine's behavior, making it suitable for simulations of materials that deform and retain new shapes.\nElastic Entities. Elastic materials [84], [86], [88], [91], [93], [94], [96], [97], [98], [99], [100], [101] are characterized by their ability to return to their original shape after deformation, following a linear or nonlinear stress-strain relationship.\nGranular Media. Granular media [91], [96], such as sand [93], [94], [102], [103] and snow, behave in complex ways that combine both solid and fluid properties. Their behavior is governed by discrete particle interactions, which are often modeled using Discrete Element Method (DEM) or adapted continuum models that capture phenomena like flow, compaction, and jamming."}, {"title": "Physical Parameters", "content": "Physical materials are substances that have specific properties, such as density, elasticity, and thermal conductivity, which define their behavior under different conditions [83], [86], [87], [88], [89], [92], [93], [94], [95], [97], [98], [99], [101], [102], [104], [105], [107], [108], [109], [110], [111]. These properties are quantified by physical parameters, which are measurable values that describe the material's response to external forces, heat, or other stimuli. For example, the Young's modulus of a material defines its stiffness, while thermal conductivity measures its ability to conduct heat. The relationship between materials and their physical parameters is crucial for predicting how materials will behave. The definition of physical parameters is usually correlated with the selection of the physical material and the simulation method."}, {"title": "Simulation Methods", "content": "Below we introduce methods for simulating and analyzing physical dynamics, which appear in the literature of physics-aware generation reviewed in this survey.\nContinuum Mechanics-based Method (CMBM). Continuum mechanics-based method [106] solves the governing partial differential equations that describe the behavior of materials as continuous media, focusing on stress, strain, and motion in solids, fluids, and gases. These simulations use numerical methods like FEM or Computational Fluid Dynamics to approximate solutions for complex, real-world systems involving large deformations, fluid flow, and heat transfer.\nMaterial Point Method (MPM). The MPM [84], [86], [88], [91], [93], [94], [96], [97], [98], [101], [103], [107], [112] is a hybrid simulation technique that combines Lagrangian particles with an Eulerian grid to model the behavior of materials. In this method, particles carry material properties such as mass, velocity, and stress, while a background grid is used to solve the governing equations of motion. The particles interact with the grid, allowing MPM to efficiently handle large deformations, phase transitions, and multiphase flow while capturing both material behavior and complex interactions.\nFinite Element Method (FEM). FEM [96], [100], [103], [110] discretizes a physical domain into small, interconnected elements and solves the governing equations of motion or energy using approximation functions within each element. These element-level equations are then assembled into a global system, which is solved to obtain the behavior of the entire system under various loads and boundary conditions.\nPosition-Based Dynamics (PBD). PBD [91], [102] is a physics simulation method that focuses on solving constraints directly in terms of particle positions, rather than velocities or forces. The technique iteratively adjusts particle positions to satisfy physical constraints, such as distance, volume preservation, or collision, ensuring stability and efficiency, especially for real-time applications involving deformable objects and soft materials.\nEulerian Method (EM). EM [93], [94], [95], [112] is a grid-based method where the physical domain is fixed, and quantities like velocity, pressure, or temperature are computed at fixed points in space. As the material flows through the grid, EM solves the governing equations (e.g., conservation of mass, momentum) at each grid point to model the evolution of physical fields over time.\nLagrangian Method (LM). LM [93], [94], [95], [100], [109], [112] follows individual particles or material points as they move through space and time, solving the equations of motion based on their positions, velocities, and forces. In practice simulation, LM is particularly useful for tracking the deformation and motion of materials with moving boundaries or interfaces, such as in fluid dynamics or solid mechanics.\n3D Spring-Mass Model. This is a physical simulation technique where a deformable object is discretized into masses connected by springs [99], each spring representing elastic forces that govern the object's deformation. The motion of the masses is computed over time by solving the equations of motion using numerical methods, accounting for forces such as spring tension, damping, and external interactions.\nNewtonian Dynamics. The Newtonian Dynamics represents a physical simulation method [103] that models the motion of objects based on Newton's laws of motion, where the force acting on an object is equal to its mass times its acceleration. It uses numerical integration techniques to solve the equations of motion over time, simulating the object's response to forces such as gravity, collisions, and applied external forces.\n\u0424-Flow. This is a physics-based simulation method that uses a level-set function \u03a6 to represent fluid interfaces and track free-surface flows or multiphase interactions [95]. The method solves the Navier-Stokes equations for fluid dynamics while evolving the level-set function to handle complex interfaces and simulate realistic fluid behaviors."}, {"title": "Physics Engines and Platforms", "content": "To perform physics simulations, several popular engines and platforms have been developed, as summarized below.\nBullet Physics. This is an open-source physics engine widely used in games, animation, and robotics simulations [113]. It enables to support rigid body dynamics, soft body dynamics, collision detection, and physical constraints.\nHavok Physics. Havok [114] is a commercial physics engine widely used in games and real-time simulations. It offers efficient rigid body, fluid, cloth, and vehicle physics simulations.\nNVIDIA PhysX. This is a physics engine developed by NVIDIA that supports GPU-accelerated real-time simulation [115]. It includes particle systems, cloth, rigid body dynamics, and fluid simulations, optimized with high-performance.\nUnity Physics. The built-in physics engine of Unity [91], [116] supports both 2D and 3D physics simulations. It provides collision detection, rigid body dynamics, cloth, and soft body physics.\nUnreal Engine Physics (Chaos Physics). The physics engine in Unreal [90] supports large-scale destruction and fluid/cloth simulations, also providing realistic rigid body and soft body physics.\nOpen Dynamics Engine. This an open-source physics engine [117] focusing on rigid body dynamics and collision detection, widely used in games, robotics, and physical simulation systems.\nBox2D. Box2D [118] is a lightweight, open-source 2D physics engine designed specifically for 2D simulation, offering efficient rigid body dynamics and collision detection.\nMantaflow. Mantaflow [119] is an open-source fluid and smoke simulation engine, with the capability to support high-quality fluid, fire, and gas simulations.\nSimulink/Matlab (Simscape Multibody). This is a platform within Matlab used for multi-body dynamics and mechanical system simulations [120]. It is highly suitable for detailed modeling and simulation of mechanical systems, robotics, and vehicles.\nBlender. This is a free and open-source 3D creation suite [89], [103] that supports modeling, sculpting, animation, rendering, and simulation. It offers a powerful and versatile toolset for productions and scientific simulations.\nIsaac Gym. Isaac Gym [109] is a high-performance physics simulation platform developed by NVIDIA, designed specifically for training and testing robotic systems using reinforcement learning. It provides a GPU-accelerated framework that allows to simulate multiple environments in parallel, enabling faster training of agents.\nVortex Studio. This is a high-performance simulation platform primarily designed for robotics, vehicles, and mechanical systems [121]. Its most attractive feature is its ability to offer real-time physics simulation, with a focus on large systems and complex dynamics.\nPyBullet. PyBullet is a Python interface for Bullet Physics [90], [108], [122], [123], commonly used in robotics and reinforcement learning, providing simulations of rigid body dynamics in machine learning environments.\nGazebo. Gazebo is a robot simulation platform that provides high-fidelity physics and sensor modeling [124]. It integrates with Robot Operating System for realistic robotics simulations.\nGenesis. Genesis [125] is an open-source platform for embodied artificial intelligence that provides a high-fidelity physics environment to simulate robots and their interactions with the physical world. It focuses on helping agents learn through physical interaction with environments, supporting tasks like manipulation, locomotion, and real-world decision-making.\nTaichi. Taichi [103] is a platform combining the Taichi programming framework to enhance the efficiency and accuracy of physical system simulations. By integrating optimization and learning algorithms, it enables adaptive modeling, real-time simulation adjustments, and data-driven insights for robotics, virtual environments, and scientific simulation."}, {"title": "PHYSICAL UNDERSTANDING", "content": "Physical understanding in machine learning aims to infer underlying physical models, laws, or parameters from observational data such as images and videos. This capability is crucial for tasks that require modeling real-world dynamics, including object motion, material behavior, and environmental interactions. While extensive research has been dedicated to physical reasoning and model discovery [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], this area is largely orthogonal to physics-aware generative modeling, which is thus out of the core focus of this survey. Nonetheless, accurately estimating physical parameters remains a foundational component in enabling physically consistent generation. In the context of physics-aware generative AI, determining the physical parameters that drive simulations is essential for producing realistic and physically plausible outputs. We identify three major approaches to obtain physical parameters as below.\nManually-Set Physical Parameters. In this approach, domain experts explicitly define the physical properties and constraints used in simulations [83], [91], [95], [96], [100], [103], [104], [105], [108], [109], [110], [111], [112], [123], [156]. These parameters may include material properties (e.g., mass, friction, elasticity), environmental conditions (e.g., gravity, fluid dynamics), and initial configurations. While this method is practical and widely used, it often lacks scalability and adaptability to complex or diverse scenarios.\nAutomatically-Learned Physical Parameters. Data-driven models can automatically infer physical parameters by learning from visual observations [85], [92], [93], [94], [97], [98], [99], [101] to avoid inflexible pre-defined parameters. These parameters can be estimated either separately in a standalone stage or optimized jointly with the parameters of the generative model.\nLLM-Reasoned Physical Parameters. Recent multimodal large language models (LLMs) enable reasoning about physical systems using both textual and visual information [84], [86], [87], [88], [89], [90], [102], [106], [107], [157]. By leveraging contextual knowledge and commonsense reasoning, LLMs prompted with descriptions of objects can infer their physical materials and plausible physical configurations for simulation."}, {"title": "PHYSICS-Aware GENERATION", "content": "In this section, we first provide an overview of the primary classes of generative models employed in physics-aware generation. We then introduce the main paradigms for incorporating physics-based constraints and knowledge into these generative models."}, {"title": "Generative Models", "content": "We begin by introducing widely used models including Generative Adversarial Networks (GANs), Diffusion Models (DMs), and neural rendering methods-including Neural Radiance Field (NeRF) and Gaussian Splatting (GS) which, although traditionally viewed as rendering or reconstruction frameworks, are interpreted as generative models in a broader sense in this survey. These generative models can handle a variety of data modalities"}, {"title": "Generative Adversarial Networks", "content": "A generative adversarial network [3] consists of two neural networks-a generator and a discriminator-engaged in a competitive process. The generator seeks to create synthetic data that closely resembles real-world data, while the discriminator attempts to distinguish between real and generated data. This adversarial dynamic drives both networks to improve iteratively, resulting in a generator capable of producing highly realistic data samples. Specifically, the generator G and the discriminator D play a two-player minimax game with the following loss function:\n$\\min _G \\max _D L(G, D) =E_{x \\sim p_{data}(x)} [\\log D(x)]+\nE_{z \\sim p_{z}(z)} [\\log(1 - D(G(z)))],\nwhere x is a real sample from the data distribution and G(z) is a sample generated using the latent code z. StyleGAN [158] can provide an automatically learned, unsupervised separation of high-level attributes in latent codes, and later versions further fix characteristic artifacts in high-resolution generation [159]. GAN has been the dominant generative model before the popularity of diffusion models."}, {"title": "Diffusion Models", "content": "Diffusion models are a class of generative models that have gained significant attention in recent years for their ability to produce high-quality synthetic data [77]. Compared to GANs, DMs are known for their stability during training but lower efficiency during sampling [5], [6]. Diffusion models leverage a sequential process of transforming simple noise distributions into complex data distributions through a series of learned denoising steps. Diffusion models simulate a gradual corruption of data by adding Gaussian noise over multiple steps in a forward process:\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 \u2013 \\beta_t}x_{t-1}, \\beta_tI)$,\nwhere t is the diffusion step, xt is the noisy data at step t, \u03b2 is a schedule parameter. This process is then reversed using a neural network parameterized to denoise the data step-by-step:\n$P_\\theta(X_{t-1}|X_{t}) = N(x_{t-1}; \\mu_\\theta(x_{t}, t), \\sigma_\\theta^2 I)$,\nwhere \u03c3 is the variance schedule. This denoising process can be parameterized in several ways, including mean prediction, noise prediction, and clean data prediction [76]. This iterative refinement allows diffusion models to generate detailed and diverse outputs while maintaining strong theoretical foundations. Score-based models are of an equivalent formulation as diffusion models but from the differential equation perspective [160], [161], [162]. The denoising process can be accelerated by samplers such as DDIM [4] and DPM-Solvers [163]. Latent diffusion models [164]"}, {"title": "Neural Radiance Field", "content": "Neural radiance field [7] is an approach to scene representation and novel view synthesis, which leverages neural networks as implicit representation to model the volumetric properties of a scene. NeRF utilizes a multi-layer perceptron (MLP) to encode a continuous volumetric scene as a function mapping 3D spatial coordinates x, y, z and viewing directions \u03b8, \u03c6 to radiance density \u03c3 and color c = (r, g, b) values:\n$c, \\sigma = F(x, y, z, \\theta, \\phi)$.\nBy optimizing the network to minimize the error in rendering the observed views of a scene, NeRF can synthesize photorealistic novel views by querying the MLP along camera rays and using volume rendering techniques to project the output colors and densities into an image. PixelNeRF [173] enables neural scene representation from only one single observed image. MIP-NeRF [174] reduces aliasing artifacts and significantly improves the ability to represent fine details. DNeRF [175] extends neural radiance fields to dynamic objects with motions. These advancements have propelled the neural radiance field into widespread applications in physics-informed scene representations for dynamic and large-scale environments."}, {"title": "Gaussian Splatting", "content": "Gaussian splatting [8] is an approach to scene reconstruction and novel view synthesis that represents a scene as a set of dense, overlapping Gaussian \u201cblobs\u201d in 3D space with learned parameters including the mean \u00b5i, covariance \u03a3i, opacity \u03b1i, and view-dependent color c\u1d62(\u03b8, \u03c6):\n$G(x, y, z, \\mu_i, \\Sigma_i, \\alpha_i) \\cdot C_i (\\theta, \\phi)$.\nInstead of relying on volumetric grids or implicit neural fields, Gaussian splatting is an explicit radiance field-based scene representation. During rendering, the Gaussians are \"splatted\" onto the image plane, with their contributions composited together to form a final image. By jointly optimizing the positions, shapes, and colors of the Gaussians, Gaussian splatting achieves photorealistic renderings with compelling speed and accuracy, making it an attractive alternative or complement to grid-based and neural implicit methods like NeRF. 4D Gaussian Splatting (4D-GS) [176] goes beyond static scenes and extends to dynamic scenes as a 4D representation. SplatterImage [177] maps a single input image to one 3D Gaussian per pixel, enabling fast scene reconstruction from a single view. LGM [178] is proposed for 3D generation, which generates 3D Gaussians from multi-view images synthesized by external models at inference time from text or single-image inputs"}, {"title": "Physics-Aware Generation w/ Explicit Simulation", "content": "In the following, we review papers working on physics-aware generation with explicit physical simulation (PAG-E), which are grouped into six categories according to the paradigm of how physics simulation (Sim) is integrated with the generative model (Gen). Note that one paper can usually couple multiple paradigms, but is classified based on the most evident one."}, {"title": "Paradigm 1: Gen-to-Sim (GtS)", "content": "This category of papers usually appends physical properties to a generative representation in a post-processing manner to make it simulatable and interactable.\nSimulation Elements in NeRF Field. PIE-NeRF [100] uses Poisson disk sampling to distribute \"particles\" in the NeRF density field, which form simulation elements through Voronoi grouping. Quadratic generalized moving least square (Q-GMLS) strategy and Lagrangian dynamics are then applied to the elements for simulation, enabling users to interact with the NeRF scene using external forces. Video2Game [90] automatically converts a single video of a real-world scene into an interactive virtual environment. To achieve this, a NeRF-generated scene is segmented into individual objects. Each segmented object is then assigned physical parameters such as mass, friction, and collision geometry (e.g., sphere, box, convex polygon) to be simulated with rigid-body physics in real time using WebGL-based game engine.\nGaussian Blobs as Simulation Elements. PhysGaussian [96] first builds a Gaussian Splatting with anisotropic regularization, and then imparts physically grounded behavior to the Gaussian kernels through Material Point Method (MPM). The method applies continuum mechanics to evolve 3D Gaussian kernels, where the kernels are treated as discrete particle clouds for discretizing the continuum. This allows the system to model deformation realistically by tracking physical quantities like stress and strain. Similarly, GASP [103] also leverages the Material Point Method (MPM) to simulate physical behaviors. GASP converts Gaussian components into triangle-based meshes, on which the MPM is applied, and the results are re-parameterized back into Gaussian components for rendering. Spring-Gau [99] first reconstructs static Gaussian, from which anchor points are sampled and are connected via \"springs\" to model elastic behavior. The model uses differentiable simulation to optimize learnable stiffness and damping parameters from video observations, to transform the Gaussian representation to be dynamic and simulatable. Phy124 [112] generates physics-consistent 4D content from a single image, by evolving dynamic 3D content adhering to natural physical laws. Specifically, 3D Gaussians are generated from a single image using diffusion prior and then animated by attaching an MPM simulator.\nPhysical Feature Field. Feature Splatting [84] extends Gaussian Splatting by embedding semantic features extracted from vision-language models. Such feature-carrying 3D Gaussians bridge the gap between static 3D scene representations and dynamic physical behaviors. The physics engine simulates physical interactions by treating Gaussian centroids as particles assigned with material properties identified by natural language semantics. Phys4DGen [107] utilizes the Segment Anything model and large language models to infer the material composition and physical properties of different object components in a scene represented by Gaussian Splatting. This allows for more accurate physical simulation by assigning realistic material behaviors to different object parts. SimAnything [86] employs a multi-modal large language model to predict the mean physical properties at the object level, which further helps the estimation of the probability distribution of physical properties at a particle level for MPM simulation."}, {"title": "Paradigm 2: Sim-in-Gen (SiG)", "content": "This paradigm features physics simulation integrated directly into the generative model, functioning as a core sub-module.\nConditions from Simulation. In GPT4Motion [89], the Blender-based physics simulation is integrated into the video generation pipeline. Specifically, GPT-4 translates user prompts into Python scripts that control Blender's physics engine to simulate realistic motion. The output of Blender-rendered as edge and depth maps is then fed into ControlNet-modified Stable Diffusion to generate videos that are both visually consistent and physically plausible. MotionCraft [95] enhances pretrained image diffusion models and introduces motions by warping the noise latent space with optical flow derived from physics simulation. This process allows the model to generate temporally consistent frames that evolve according to physically accurate dynamics such as fluid dynamics, rigid body motion, and multi-agent interactions. Similarly, by using image-based warping with simulated motion dynamics, PhysGen [87] generates physically plausible video outputs given an input image and user-defined forces and torques. The simulation is based on rigid-body dynamics governed by Newton's Laws, using physical parameters inferred by large foundation models.\nSimulation as Optimization. PhyCAGE [156] treats MPM simulation as a physics-aware optimizer by evolving the particle system based on physical laws. The gradient of the loss function is interpreted as the initial velocity of the particles derived from 3D Gaussians, and this velocity is passed into the MPM simulation to optimize the system over sub-steps before each gradient descent step. PhysDiff [85] generates physically plausible human motions by integrating physics-based constraints directly into the sampling process of diffusion models. At each denoising step, the intermediate motion is passed through a physics simulator for correction. The physically-corrected motion then feeds back into the sampling process and guides subsequent denoising steps, steering the generation toward physically plausible motions.\nGenerative Programs for Simulation. AutoVFX [106] works on photorealistic and physically plausible video editing through natural language instructions, which combines LLM-based code generation and physics-based simulation. User instructions are transformed into executable code, enabling edits such as object insertion, material changes, dynamic interactions, and particle effects. The code is executed in the Blender engine to render the scene with photorealistic lighting and materials to produce the edited video. Similarly, GPT4Motion [89] uses large language models to automate the generation of Blender Python scripts that drive physical simulations, integrating semantic understanding with procedural generation to create complex and physics-based animations to guide video generation."}, {"title": "Paradigm 3: Gen-and-Sim (GnS)", "content": "This paradigm involves simultaneous or interconnected operation of generation and simulation processes with tight coupling.\nJoint Geometry and Physics Learning. PAC-NeRF [93] addresses the challenge of inferring both the geometric and physical parameters of objects solely from multi-view video data, using a hybrid Eulerian-Lagrangian representation of the scene. Concretely, the Eulerian grid representation is used for NeRF to learn the geometry, while the Lagrangian particle representation is used for simulation to learn the physical parameters. iPAC-NeRF [94] further proposes Lagrangian Particle Optimization to directly optimize the positions and features of particles in the Lagrangian space, leading to dynamic refinement of the geometric structure across the entire video sequence while adhering to physical constraints.\nAlternating Simulation and Generation. PhysMotion [88] follows a generation-simulation-generation process for image-to-video generation. Firstly, the foreground object in the input image is converted into a coarse 3D Gaussian Splatting representation. Then, the model undergoes simulation using MPM, applying physical laws to simulate how the object would behave under forces, to produce a coarse video depicting physics-grounded dynamics. Finally, the coarse video is further refined using diffusion-based video enhancement to improve visual realism."}, {"title": "Paradigm 4: Sim-Constrained Gen (SCG)", "content": "This is a paradigm where the simulation imposes constraints or guidance on the training of generative models to improve physical awareness.\nLoss Functions from Simulation. PhysComp [110] creates 3D models from single images while ensuring physical compatibility. It is constrained by a simulation-based physical model\u2014specifically, the static equilibrium constraint\u2014which ensures the generated 3D shapes behave realistically under physical forces. PhyRecon [109] incorporates simulation as loss functions to constrain and guide the generation process toward physically plausible 3D scenes. This is achieved through the integration of a differentiable particle-based physical simulator, which directly influences the neural implicit surface representation to improve stability and model physical uncertainty. Atlas3D [83] generates 3D models that are self-supporting, incorporating a standability loss to penalize rotational instability to ensure the generated model maintains its upright orientation during simulation, and a stable equilibrium loss to encourage resilience against minor perturbations. Mezghanni et al. [108] introduce two differentiable physical loss functions, i.e., a connectivity loss to regularize that the generated 3D shapes are single connected components, and a stability loss to promote physical stability under gravity.\nSimulation for Data Filtering DiffuseBot [92] employs simulation as a component for data filtering during its embedding optimization phase. Specifically, 3D robot models generated from diffusion models are evaluated using a differentiable physics simulation to assess the performance of each robot based on task-specific metrics. A filtering mechanism then selects the top-performing designs to be retained, gradually skewing the sampling distribution of the generative model toward successful robot designs.\nReinforcement Learning with Physical Feedback. Furuta et al. [157] addresses the challenge of generating realistic dynamic object interactions in text-to-video models, by using reinforcement learning (RL) fine-tuning from external feedback-particularly"}, {"title": "Paradigm 5: Gen-Constrained Sim (GcS)", "content": "In this paradigm, the generation model acts as a guidance or prior knowledge for the simulation process.\nScore Distillation Sampling. Physics3D [97", "98": "further proposes Motion Distillation Sampling (MDS) to better capture motion-specific priors and reduce color bias in the optimization.\nLearning Physical Parameters from Generated Data. Phys-Dreamer [101"}]}