{"title": "Motion Dreamer: Realizing Physically Coherent Video Generation through Scene-Aware Motion Reasoning", "authors": ["Tianshuo Xu", "Zhifei Chen", "Leyi Wu", "Hao Lu", "Yuying Chen", "Lihui Jiang", "Bingbing Liu", "Yingcong Chen"], "abstract": "Recent numerous video generation models, also known as world models, have demonstrated the ability to generate plausible real-world videos. However, many studies have shown that these models often produce motion results lacking logical or physical coherence. In this paper, we revisit video generation models and find that single-stage approaches struggle to produce high-quality results while maintaining coherent motion reasoning. To address this issue, we propose Motion Dreamer, a two-stage video generation framework. In Stage I, the model generates an intermediate motion representation\u2014such as a segmentation map or depth map-based on the input image and motion conditions, focusing solely on the motion itself. In Stage II, the model uses this intermediate motion representation as a condition to generate a high-detail video. By decoupling motion reasoning from high-fidelity video synthesis, our approach allows for more accurate and physically plausible motion generation. We validate the effectiveness of our approach on the Physion dataset and in autonomous driving scenarios. For example, given a single push, our model can synthesize the sequential toppling of a set of dominoes. Similarly, by varying the movements of ego-cars, our model can produce different effects on other vehicles. Our work opens new avenues in creating models that can reason about physical interactions in a more coherent and realistic manner.", "sections": [{"title": "1. Introduction", "content": "Video generation [3, 8, 15, 36, 37] has become a pivotal area of research in computer vision and artificial intelligence, with applications ranging from entertainment [17, 38, 44] and virtual reality [5, 22, 41] to autonomous driving [16, 21, 32, 35, 42] and robotics [10, 20, 34]. Recent advancements in deep learning have led to the development of powerful video generation models, often referred to as \"world models\" [16, 21, 32, 33, 35]. These models aim to generate future scenarios based on current observations and desired conditions, holding the potential to enhance decision-making processes in dynamic environments by predicting likely sequences of events.\nDespite significant progress, studies [1, 18, 24] have highlighted substantial limitations in existing video generation models, particularly in generating motion that maintains logical and physical coherence. For example, Kang et al. [18] found that models perform well on in-distribution data but frequently fail to maintain physical coherence in out-of-distribution scenarios. Similarly, Bansal et al. [1] demonstrated that recent text-to-video models, despite generating visually realistic content, often violate fundamental physical laws like Newtonian mechanics, especially in interactions involving different material types.\nThese challenges raise a critical question: Why do current video generation approaches struggle to produce logically coherent videos? Insights from cognitive psychology [13, 25] into human working memory and visual attention suggest that humans tend to focus on the movement and spatial relationships of objects rather than intricate details. This implies that simultaneously processing both fine-grained visual details and coherent motion is inherently complex for models.\nTo address this issue and achieve both rich detail and motion coherence, we propose a novel two-stage video generation framework, termed Motion Dreamer. In the first stage, we produce a motion-coherent representation-an intermediate representation that emphasizes the movement and spatial relationships of objects. In the second stage, we utilize this representation to generate the final videos. By decoupling these challenging tasks into two separate stages, we simplify the generation process, making it more feasible to achieve both high-quality visuals and coherent motion.\nIn the domain of controllable video generation, developing a control modality that is both intuitive for human input and effective as a conditioning signal for video synthesis remains a challenging problem [27, 29]. Motivated by the work of MOFA-Video [27], we propose a novel sparse-to-dense motion modality, termed instance flow, to address this challenge. During training, the instance flow captures the average optical flow vectors for each object and propagates these vectors across the corresponding object mask. At inference time, users can provide sparse motion cues, such as directional arrows representing the average optical flow, and leverage advanced models like the Segment Anything Model (SAM) [19] to obtain instance segmentation masks. This enables a pixel-aligned conditioning mechanism that facilitates the generation of dense motion representations. By effectively bridging the gap between sparse human input and dense motion fields, our approach supports the generation of temporally coherent videos.\nTo further improve the model's ability to generate motion in a reasoning-driven manner, we introduce a novel training strategy. In this approach, we randomly mask portions of the instance flow during training and require the model to reconstruct the full dense motion representation. This encourages the model to infer missing motion cues, thereby enhancing its generalization and reasoning capabilities. By training the model to handle incomplete motion information, we enable it to reason about object interactions and predict plausible motion trajectories, even when provided with sparse inputs.\nWe validate our framework on the Physion [2] dataset and a large autonomous driving dataset collected from \"YouTube\", which contains over 8,000 clips of interactive driving scenes totaling more than 200 hours. We perform extensive experiments to assess our model's performance. In comparisons with state-of-the-art video editing models like Mofa-Video [27] and leading driving generation model Vista [11], our model achieves more reasonable and coherent results in both qualitative and quantitative evaluations. These findings show that our framework not only generates high-quality videos but also significantly improves motion coherence and physical plausibility in complex scenarios. In summary, our contributions are threefold:"}, {"title": "2. Related Work", "content": "Video generation models predominantly extended Unet-based latent diffusion models (LDMs) from text-to-image frameworks like Stable Diffusion [28] to accommodate video applications. For instance, AnimateDiff [12] introduced a temporal attention module to enhance temporal consistency across frames. Building upon this, subsequent video generation models [6-8, 31, 39, 40] adopted alternating attention mechanisms that combine 2D spatial attention with 1D temporal attention. Notable examples include ModelScope, VideoCrafter, Moonshot, and Show-1, which have demonstrated significant improvements in video generation tasks."}, {"title": "2.2. Controllable Motion Generation", "content": "Controllable motion generation in video synthesis aims to produce videos that not only exhibit high visual quality but also adhere to specified motion patterns and dynamics. [37, 43] allow users to specify motion trajectories directly, enabling fine-grained control over the path an object takes in the video. Others utilize keypoints or motion fields to guide animations, translating abstract motion representations into realistic movements [9, 27]. By integrating these control mechanisms, these approaches aim to bridge the gap between user intent and the generated content.\nRecently, a novel category of video generation models, referred to as \"world models\" [16, 21, 32, 33, 35, 42]. Pioneering models like DriveDreamer [32] and DriveDiffusion [21] employ diffusion models conditioned on layout and ego-action to generate controllable driving videos. GAIA-I [16] further expands this paradigm by incorporating multiple conditioning inputs, such as video, text, layouts, and actions, enabling the generation of realistic and diverse driving scenarios with fine-grained control over vehicle behavior and scene features. GenAD [35] advances these efforts by scaling both the video prediction model and the dataset, thereby effectively managing complex driving scene dynamics and demonstrating superior zero-shot generalization across a range of unseen data.\nDespite these advancements, we observe a common limitation persists: While they enhance controllability by introducing motion guidance, they do not fully resolve the issue of logical motion coherence, particularly in complex scenes involving multiple interacting objects or scenarios requiring adherence to physical laws. For instance, while a trajectory might dictate where an object should move, it doesn't ensure that the movement adheres to principles like inertia or collision dynamics. Studies have shown that although models can generate visually appealing content, they frequently fail to maintain consistency with fundamental physical principles such as object permanence, conservation of mass, or Newtonian mechanics [1, 18]. To address this limitation, we propose decoupling the tasks of motion reasoning and high-fidelity visual generation. By separating these two aspects, we simplify the generation process, making it more feasible to achieve both objectives."}, {"title": "3. Method", "content": "In this section, we present a comprehensive overview of our proposed two-stage framework, Motion Dreamer. The overall pipeline is depicted in Fig. 2. In Section 3.1, we define the problem and introduce the fundamental notations used throughout the framework. Section 3.2 introduces the concept of instance flow, a novel motion representation."}, {"title": "3.1. Preliminaries", "content": "Let \\({I_t}\\)_{t=0}^T denote a sequence of video frames, where t indexes the time steps, and T is the total number of frames. Our objective is twofold: first, to generate future intermediate motion representations \\({R_t}\\)_{t=1}^T conditioned on the initial frame \\(I_0\\) and user-provided motion prompts; and second, to generate the future video frames \\({I_t}\\)_{t=1}^T conditioned on the initial frame \\(I_0\\) and the generated multi-frame intermediate motion representations \\({R_t}\\)_{t=1}^T.\nIn this case, the intermediate motion representation \\(R_t\\) is composed of three key components: optical flow \\(O_t \\in \\mathbb{R}^{2\\times H \\times W}\\), instance segmentation map \\(S_t \\in \\mathbb{R}^{H\\times W}\\), and depth map \\(D_t \\in \\mathbb{R}^{1\\times H \\times W}\\),\n\\[R_t = (O_t, S_t, D_t).\\]\nThe choice of these intermediate motion representations was deliberate, this unified representation R offers a comprehensive description of scene dynamics by combining optical flow, which captures the overall motion of the scene and camera movements; the instance segmentation map, which isolates and represents movements of individual objects; and the depth map, which encodes spatial relationships and distances between objects and the camera. By integrating these components, R facilitates a detailed understanding of both object-specific movements and their spatial interactions within the scene.\nThe effectiveness of this unified intermediate motion representation and the two-stage generation process is further demonstrated through experiments presented in subsequent sections, showcasing their ability to capture and reproduce complex scene dynamics accurately."}, {"title": "3.2. Instance Flow", "content": "To more effectively represent object motion, we propose a novel sparse-to-dense motion modality termed instance flow, which is well-suited for both human-in-the-loop input and video generation tasks. In the following, we provide a detailed description of the instance flow computation during both training and inference.\nTraining. During training, we assume that all intermediate motion representations are available within the dataset. Specifically, given optical flow fields \\({O_t\\}_{t=0}^T\\) and the instance segmentation map \\(S_0\\), we calculate the instance flow for each instance \\(i \\in \\mathcal{I}\\). The instance mask for instance i is defined as follows:\n\\[M^{(i)} (x, y) = \\begin{cases} 1, & \\text{if } S_0 (x, y) = i, \\\\ 0, & \\text{otherwise.} \\end{cases}\\]\nThe instance flow \\(F^{(i)} \\in \\mathbb{R}^{2\\times H \\times W}\\) for instance i is obtained by averaging the optical flow vectors within the instance mask over the temporal window T:\n\\[F^{(i)} = \\frac{1}{T} \\sum_{t=0}^{T-1} (M^{(i)} \\odot O_t),\\]\nwhere \\(\\odot\\) denotes element-wise multiplication, with the instance mask \\(M^{(i)}\\) broadcasted over the flow dimensions. The overall instance flow field \\(F\\in \\mathbb{R}^{2\\times H \\times W}\\) is then constructed by aggregating the instance flows for all instances:\n\\[F(x, y) = \\sum_{i \\in \\mathcal{I}} M^{(i)} (x, y) \\cdot F^{(i)} (x, y).\\]\nInference. During inference, users can provide sparse motion cues, such as arrows representing the desired average motion vectors for specific objects. Instance segmentation masks can be generated using advanced models like the Segment Anything Model (SAM) [19]. These inputs are combined to generate a sparse instance flow \\(F_{\\text{user}}\\):\n\\[F_{\\text{user}} (x, y) = \\sum_{i \\in \\mathcal{Z}_{\\text{user}}} M^{(i)} (x, y) \\cdot v^{(i)} \\cdot \\delta,\\]\nwhere \\(\\mathcal{Z}_{\\text{user}}\\) denotes the set of instances for which the user provides motion cues, \\(v^{(i)}\\) is the user-specified motion vector (e.g., an arrow) for instance i, \\(\\delta\\) is a scaling factor, and \\(M^{(i)}\\) is the instance mask for object i.\nThis formulation enables effective user guidance for instance-specific motion, offering an intuitive framework for both video generation and downstream tasks."}, {"title": "3.3. Stage I: Reasoning Motion Generation", "content": "In Stage I, Motion Dreamer, we employ a diffusion-based video generation model built upon the pre-trained SVD model [3]. To emphasize low-frequency motion representations and improve temporal coherence, we adopt the \\(x_0\\)-prediction parameterization in the diffusion process.\nAs discussed in many diffusion approaches [14, 28], the training objective can be written as\n\\[\\mathcal{L} = \\mathbb{E}_{x_0, c, t} [|| x_0 - x_{\\theta}(x_t, t, c) ||^2],\\]\nwhere \\(x_{\\theta}(x_t, t, c)\\) is the model's prediction of the original intermediate motion representation \\(x_0\\), composed of the optical flow \\({O_t\\}_{t=0}^T\\), instance segmentation map S_t, and depth map \\(D_t\\), conditioned on the noisy input \\(x_t\\), timestep t, and conditioning information c (which includes the instance flow F).\nConditions Incorporation. To effectively integrate the instance flow into the model, we prepare multi-scale versions of the instance flow F to align with the feature maps at different scales within the network. Specifically, for each scale \\(s \\in \\{8, 16, 32, 64\\}\\) used in the network, we resize the instance flow to match the spatial dimensions of the corresponding feature maps:\n\\[F^{(s)} = \\frac{1}{s} \\cdot \\text{Resize} \\left( F, \\frac{H}{s}, \\frac{W}{s} \\right),\\]\nwhere the division by s scales the flow vectors appropriately for the new resolution.\nNext, we warp the feature maps \\(C^{(s)}\\) of the first-frame conditions (RGB image, instance segmentation, and depth) at each scale using the scaled instance flows. Following the approaches of DragNUWA [37] and MOFA-Video [27], we apply the Softmax Splatting function [26]:\n\\[W^{(s)} = \\text{Softsplat} \\left( C^{(s)}, F^{(s)} \\right),\\]\nwhich warps the feature map according to the flow field by performing a differentiable splatting operation that aggregates input features onto the output grid based on the flow vectors. The Softmax Splatting function effectively distributes the features of \\(C^{(s)}\\) to new positions dictated by \\(F^{(s)}\\), allowing for seamless integration of motion information while maintaining differentiability for end-to-end training.\nThese warped features \\(W^{(s)}\\) are then integrated into the network by adding them to the corresponding feature maps at each scale during the encoding process:\n\\[X^{(s)} = X^{(s)} + W^{(s)}.\\]\nThis fusion strategy allows the network to incorporate explicit motion cues at multiple scales, enhancing its ability to generate motion-coherent intermediate representations and improving temporal consistency across frames.\nRandom Masking To enhance the model's capability for reasoning-based motion generation, we introduce a simple yet effective training strategy where we randomly mask out portions of the instance flow and require the model to reconstruct the same dense motion representation. Specifically, during training, we apply a random mask to the instance flow field F to create a partially observed flow F:\n\\[F' = F \\odot M_{\\text{mask}}\\]\nwhere \\(M_{\\text{mask}} \\in \\{0, 1\\}^{2 \\times H \\times W}\\) is a binary mask with each element independently set to zero with probability p (the masking ratio) and one otherwise, and \\(\\odot\\) denotes element-wise multiplication.\nThe model is trained to reconstruct the target intermediate motion representation \\(x_0\\) using the masked instance flow F, minimizing the loss:\n\\[\\mathcal{L} = \\mathbb{E}_{x_0, c, t} [|| x_0 - x_{\\theta}(x_t, t, F', c) ||^2],\\]"}, {"title": "3.4. Stage II: Video Decoder", "content": "In Stage II, Video Decoder, we transform the intermediate motion representations generated by the Motion Dreamer into high-quality RGB video frames. Building upon the pre-trained SVD model [3], the Video Decoder performs conditioned image-to-video generation by leveraging the initial frame and the generated intermediate motion representations.\nFormally, let \\(x_0\\) denote the intermediate motion representation produced by the Motion Dreamer, which includes the optical flow, instance segmentation map, and depth map. The Video Decoder synthesizes the final RGB video \\({I_t\\}_{t=1}^T\\) by conditioning on \\(x_0\\) and the initial frame \\(I_0\\):\n\\[\\{I_t\\}_{t=1}^T = \\text{SVD}(I_0, x_0).\\]\nThe Video Decoder utilizes the instance flow to guide the generation process, ensuring that the synthesized frames exhibit coherent object movements and maintain spatial consistency with the initial frame. By conditioning on both the initial frame and the intermediate motion representations, the Video Decoder effectively renders high-quality videos that align with the specified motion cues.\nThis two-stage approach decouples motion reasoning from high-fidelity video synthesis, simplifying the generation process and making it more feasible to achieve both rich visual details and coherent motion. The integration of the pre-trained SVD model facilitates efficient and effective video generation, leveraging existing powerful image-to-video capabilities while enhancing them with our novel motion reasoning framework."}, {"title": "4. Experiments", "content": "Implementation Details. We evaluate the performance of the proposed Motion Dreamer on two datasets: Physion [2] and a large-scale driving dataset collected from YouTube. The driving dataset consists of over 8,000 clips of interactive driving scenarios, totaling more than 200 hours of video footage. Representative examples from this dataset are shown in Figure 5. The dataset will be made publicly available.\nThe model leverages a two-stage training approach: Stage I employs \\(x_0\\)-parametrization, focusing on low-frequency motion representations to capture global dynamics. Stage II adopts \\(v\\)-parametrization, which enhances the generation of fine-grained video details. Each stage is trained on 8 NVIDIA A800 GPUs for approximately one week, ensuring robust convergence and high-quality results.\nEvaluation Metrics. To assess the performance of Motion Dreamer in generating physically coherent videos, we utilize 100 samples from the test set of Physion [2] and calculate two widely recognized metrics: (1) Frechet Video Distance (FVD) [30], measures the overall quality of generated videos by evaluating the statistical similarity between generated and real video distributions. (2) Frechet Video Motion Distance (FVMD) [23], evaluates motion coherence by comparing the temporal consistency of generated frames with that of ground-truth videos.\nFor real-world driving video generation, we use 100 test videos from our collected driving dataset and compute the same two metrics. These evaluations provide a comprehensive analysis of our model's ability to generate high-quality, temporally consistent, and physically plausible videos across diverse scenarios."}, {"title": "4.1. Comparions with the State-of-the-art Methods", "content": "Physical Coherent Video Generation. We evaluate the proposed Motion Dreamer on the Physion [2] test set, shown in Figure 3, comparing it with state-of-the-art video editing models, including SG-I2V [9] and Mofa-Video [27], for generating physically coherent motion videos. Additionally, we implement a simplified \u201cone-stage\" version of Motion Dreamer, which shares the same input and output structure but omits the generation of intermediate motion representations. This ablation study allows us to validate the effectiveness and superiority of our two-stage pipeline."}, {"title": "4.2. Ablation Study", "content": "We conduct an ablation study to systematically evaluate the contribution of each component and functional part within our framework, as detailed in Table 3. Specifically, we assess how the removal or modification of intermediate motion representations (IMR) and functional parts such as excluding the segmentation map or omitting the motion enhancement loss-impacts overall performance. Additionally, we examine the effect of substituting the control signal with sparse optical flow (as implemented in Mofa-Video) in place of our instance flow. This study aims to elucidate the individual and collective roles of these components in enhancing reasoning motion generation and robustness."}, {"title": "5. Limitation", "content": "Despite our model demonstrating strong performance in reasoning motion generation, it still exhibits limitations on some more complex cases, such as the tower data in Physion, where a ball strikes a tower composed of multiple blocks, causing the tower to collapse, as well as in driving scenarios with a high density of non-motorized vehicles. These issues will be discussed in detail in the appendix."}, {"title": "6. Conclusion", "content": "In conclusion, we have introduced Motion Dreamer, a novel two-stage video generation framework that addresses the critical challenge of producing logically coherent videos by decoupling motion reasoning and high-fidelity video synthesis. By incorporating instance flow, a sparse-to-dense motion modality, our approach enables intuitive human control while maintaining dense motion coherence. Additionally, we propose a unique training strategy that encourages the model to infer missing motion cues, enhancing its ability to reason about object interactions and predict plausible motion trajectories. We believe that this work will advance the field of controllable video generation, particularly in dynamic and complex environments. To foster further research and development in this area, we will be open-sourcing our training and inference code as well as the driving dataset we collect."}, {"title": "7. Implementation Details of Camera Motion", "content": "In this section, we detail the processing of camera motion and its incorporation into the model. We represent the camera motion for each sample in the batch as a vector \\(c\\in \\mathbb{R}^{B\\times 2}\\), where B is the batch size. The camera motion vectors are encoded using a motion encoder:\n\\[e_{\\text{cam}} = \\text{MotionEncoder}(c) \\in \\mathbb{R}^{B\\times 1\\times D},\\]\nwhere D is the dimensionality of the cross-attention features, this encoding captures the global camera movements affecting all frames in a sequence. MotionEncoder is a multi-layer perception (MLP) block, that projects these parameters into the cross-attention feature space.\nThe encoded camera motion \\(e_{\\text{cam}}\\) is then added to the encoder hidden states H, which are repeated across frames to match the temporal dimension:\n\\[H' = H + e_{\\text{cam}},\\]\nwhere \\(H'\\in \\mathbb{R}^{(B \\cdot L)\\times 1\\times D}\\) and L is the number of frames per sample.\nTemporal Attention Integration. We employ temporal attention layers to fuse the camera motion encoding into the model. The temporal attention mechanism captures dependencies across frames, allowing the network to consider temporal dynamics influenced by camera motion.\nIn each temporal attention layer, the camera motion encoding adjusts the attention weights, enhancing the model's ability to focus on relevant temporal features. This is achieved by incorporating \\(e_{\\text{cam}}\\) into the query or key projections of the attention mechanism."}, {"title": "8. Collected Highly Interactive Driving Data", "content": "Highly interactive driving data encompasses scenarios in which the movements of the ego vehicle or other vehicles significantly influence the behavior of surrounding vehicles and pedestrians. Existing publicly available driving datasets, such as nuScenes [4], are limited in their representation of highly interactive driving scenarios. To address this limitation, we have meticulously curated a comprehensive subset of highly interactive driving data sourced from YouTube. This curated dataset comprises over 8,000 video clips, totaling nearly 200 hours of footage. It encompasses a wide range of weather conditions, diverse geographical and urban scenes, and, most critically, varying levels of vehicular and pedestrian interactions. An illustrative example of the collected data is presented in Figure 7. The corresponding videos are made available in the supplemental material to facilitate further analysis and validation. The dataset will be made publicly available soon."}, {"title": "9. Unconditional Image-to-Video Generation", "content": "In this section, we present an expanded set of visual results for unconditional image-to-video generation within driving scenarios, providing a comprehensive comparison with the Vista [11]. These results are illustrated in Figure 8, demonstrating the effectiveness of our approach in generating realistic and temporally coherent video sequences from single images. Our method exhibits enhanced visual fidelity and consistency across diverse driving environments compared to Vista. To facilitate a thorough evaluation, the corresponding video sequences are included in the supplemental material, allowing for an in-depth assessment of the generated videos' temporal dynamics and overall quality."}, {"title": "10. Limitations", "content": "Despite the effectiveness of our model in generating coherent motion sequences, it exhibits limitations when confronted with highly complex dynamic scenes involving intricate physical interactions or dense multi-agent environments. Failure cases are illustrated in the supplementary material.\nSpecifically, in scenarios like the \"tower\u201d data from the Physion dataset\u2014where a ball impacts a tower composed of multiple blocks causing it to collapse our model struggles to accurately capture the resulting motion. The complex interactions among numerous blocks, involving simultaneous collisions, rotations, and translations, are difficult to predict without explicit physical modeling. As a result, the generated motion sequences lack the chaotic yet physically plausible behaviors observed in real tower collapses. This suggests that while the model handles simple object motions and interactions, it struggles to generalize to scenarios requiring detailed understanding of physics and object interdependencies (shown in Figure 9).\nIn driving environments characterized by a high density of non-motorized vehicles, such as bicycles and pedestrians, the model's performance diminishes. The unpredictable and highly variable movements of these agents, along with frequent interactions and occlusions, pose significant challenges. Consequently, the model sometimes fails to produce realistic motion trajectories for all agents, leading to inconsistencies in crowded scenes.\nThese limitations highlight the difficulty of modeling complex multi-agent dynamics, where each agent's behavior is influenced by numerous factors, including other agents' actions and environmental constraints. The absence of explicit mechanisms to capture social interactions and collision avoidance behaviors contributes to the model's shortcomings in these scenarios (shown in Figure 10)."}]}