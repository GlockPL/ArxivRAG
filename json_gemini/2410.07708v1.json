{"title": "Learning Tree Pattern Transformations", "authors": ["Daniel Neider", "Leif Sabellek", "Johannes Schmidt", "Fabian Vehlken", "Thomas Zeume"], "abstract": "Explaining why and how a tree t structurally differs from another tree t* is a question that is encountered throughout computer science, including in understanding tree-structured data such as XML or JSON data. In this article, we explore how to learn explanations for structural differences between pairs of trees from sample data: suppose we are given a set {(t1,t\u2081),..., (tn, t)} of pairs of labelled, ordered trees; is there a small set of rules that explains the structural differences between all pairs (ti,t)? This raises two research questions: (i) what is a good notion of \"rule\" in this context?; and (ii) how can sets of rules explaining a data set be learnt algorithmically?\nWe explore these questions from the perspective of database theory by (1) introducing a pattern- based specification language for tree transformations; (2) exploring the computational complexity of variants of the above algorithmic problem, e.g. showing NP-hardness for very restricted variants; and (3) discussing how to solve the problem for data from CS education research using SAT solvers.", "sections": [{"title": "Introduction", "content": "Explaining why and how a tree t structurally differs from another tree t* is a question that is encountered throughout computer science, including in understanding tree-structured data such as XML or JSON data, analysis of formal syntax trees, or code optimization.\nIn this article, we explore how to learn explanations for structural differences between pairs of trees from sample data: suppose we are given a set {(t1,t\u2081),..., (tn, t)} of pairs of labelled, ordered trees; is there a small set I of rules that explains the structural differences between all pairs (ti, t)? This raises two research questions:\n(i) What is a good notion of \"rule\" in this context?\n(ii) How can sets of rules explaining a data set be learnt algorithmically?"}, {"title": "Pattern-Based Tree Transformations", "content": "The goal of our tree transformation language is to explain structural differences between trees. Before formalizing our language, we discuss some of our design choices and fix notations.\nTowards explaining structural differences between trees In many contexts, structural differences between trees t and t* are mostly local in the following sense: t can be transformed into t* by \"reshuffling\" a small subtree \\(t_v\\) rooted at some node v of t and keeping the rest of t as is. This basic idea is already illustrated in Fig. 1. Formally, a tree pattern transformation will be of the form \\(\\sigma \\leftrightarrow \\sigma^*\\) for two tree patterns \\(\\sigma\\) and \\(\\sigma^*\\) called body and head, respectively. A transformation is applied to a tree t by matching its body into t and manipulating the \"covered\" subtree as indicated by the head. To this end, body and head may use explicit labels, node variables to \"copy\" labels of single nodes, and tree variables to \"copy\" subtrees."}, {"title": "Learning Tree Pattern Transformations", "content": "In this section we study algorithmic properties of the learning problem for tree pattern transformations. Formally, we consider the following algorithmic problem:\nProblem: LEARNINGTREETRANSFORMATIONS\nInput: Pairs (t1, t\u2081),..., (tn,t) of labelled, ordered trees, and s,r \u2208 N.\nFind: A set \\(\\Gamma\\) of at most r tree pattern transformations such that ti can be transformed into t with transformations from \\(\\Gamma\\) in at most s steps, for all i.\nIt turns out that this algorithmic problem is already NP-hard, even for very restricted inputs. For instance, we show that it is hard when fixing s = 1 as well as when fixing s = 3 and r = 2. Due to the hardness, we discuss how to attack the problem by encoding it into SAT and employing a SAT solver (see Section 3.2). We apply this approach exemplarily to educational data (see Appendix B.1.2)."}, {"title": "Learning Tree Pattern Transformations Is Hard", "content": "It is easy to see that LEARNINGTREETRANSFORMATIONS is NP-hard in general. For this reason, we take a closer look at fragments obtained by restricting s and r. In practical applications, such as for the data from CS education research sketched in the introduction, s and r can be chosen to be very small. For instance, solving the above problem even when fixing the number of steps to s = 1 would help to identify common types of mistakes. Unfortunately, the learning problem remains NP-hard even for small numbers s of steps and small numbers r of transformations to learn.\nTheorem 4. LEARNINGTREETRANSFORMATIONS is NP-complete when\n1. fixing s = 1; or\n2. fixing both s = 3 and r = 2.\nMembership in NP is straightforward for these cases. We note that membership in NP for the general problem, i.e. for arbitrary s and r, is less clear because intermediate trees can become exponentially large. Since s and r are small in our applications, we leave the general question for future work.\nWe prove the hardness of the two cases of Theorem 4 in Propositions 5 and 7.\nProposition 5. LEARNINGTREETRANSFORMATIONS is NP-hard, even when all examples are binary trees with labels from a binary alphabet and when s = 1 is fixed.\nProof sketch. We start by proving hardness for a large label alphabet \\(\\Sigma\\) and defer a sketch of how to adapt the proof to binary alphabets to Appendix A.1.\nWe reduce from the NP-complete problem VERTEXCOVER where, given a graph G with nodes V = {v1, ..., vn} and edges E = {e1,...,em} as well as a natural number k, one asks for a set of at most k nodes from V such that each edge from E is incident to at least one of these nodes (cf. [8]). Such a set of k nodes is called a k-vertex cover.\nConstruction. The idea for our reduction f is to construct from a VERTEXCOVER instance (G, k), an instance of LEARNINGTREETRANSFORMATIONS with s = 1, r = k, and one example pair (tu,v, tv) of trees for each edge e = (u, v). The goal is that each k-vertex cover S of G corresponds to a solution set \\(\\Gamma_S\\) of transformations of size k that allows to transform each tu,v into tv in one step.\nLet \\(l\\) be minimal such that \\(n \\leq 2^l\\). For each edge \\(e = (u, v)\\), construct the pair (tu,v, tu,v) such that tu,v is the full binary tree of depth \\(l\\) and tv is a single node. We associate the first n leaves (from left to right) of tu,v with the nodes v1, ..., vn of the graph. In tu,v, the leaves corresponding to u and v are labelled with \\(l_{u,v}\\) and all other nodes are labelled with b. In tv, the single node is labelled with \\(l_{u,v}\\). This construction is demonstrated in Example 6. The reduction f is clearly computable in polynomial time."}, {"title": "Learning Tree Pattern Transformations: A Pragmatic Approach", "content": "One approach for dealing with the NP-hardness of learning tree pattern transformations for small parameters is to employ the power of SAT solvers. We next sketch how the problem can be encoded into propositional formulas. We defer details and an application to data sets from CS education research to Appendix B.\nWe point out that the approach discussed in the following is rather straightforward. Yet, it is also useful: while theory suggests that learning of tree pattern transformations is computationally hard, the SAT solving approach works well enough for our actual data.\nSuppose we are given pairs of trees D = {(t1,t\u2081),..., (tn, t)} and numbers s,r \u2208 N and are searching for a set of r transformations that explains each pair in D in at most s steps. Our encoding closely follows a straightforward non-deterministic algorithm, which proceeds in two steps: first, it guesses (1) a set of r transformations and (2) a sequence of these transformations for each pair (ti,t) and how they are applied; second, it verifies that this information indeed represents a solution."}, {"title": "Extending Tree Pattern Transformations", "content": "A tree pattern transformation language for explaining structural differences between trees has to be sufficiently expressive to express typical differences. Yet, it should not be too expressive for two reasons: (1) algorithmic problems such as whether a transformation explains differences betweens two pairs of trees become harder; and (2) learnt transformations may overfit the data.\nIn this section, we discuss one extension of our transformation language which is natural, but too powerful for our application. One shortcoming of our language is that patterns need to exactly adhere to the structure of the (sub)tree they are matching. A potential extension to circumvent this is to include, besides node and tree variables, also interval variables which can match contiguous intervals of children of a node. Such variables loosen the strict adherence to the structure while still yielding good explanations of differences by requiring to \"capture\" everything that can be manipulated.\nConsider the transformation \\(p:  \\frac{Z_1 \\quad Z_2}{ \\downarrow }  \\leftrightarrow \\frac{Z_1}{ \\downarrow }\\) with interval variables Z1 and Z2. This transformation encodes the equivalence transformation for propositional formulas which replaces conjunctions containing a \"false\" by a sole \"false\", e.g. it can be used to transform the syntax trees of \\(A \\wedge I \\wedge B\\) and \\(A\\wedge B\\wedge I \\wedge C\\) into \\(\\perp\\).\nWe formally introduce tree patterns with interval variables before briefly discussing their algorithmic properties and why they are too powerful for learning tree transformations.\nA tree pattern with interval variables A tree pattern with interval variables \u03c3 is a \\(\\Sigma \\cup \\mathbb{N} \\cup T\\cup I\\)-labelled tree, where T- and I-labelled nodes must be leaves. The components \\(\\Sigma\\), \\(\\mathbb{N}\\), and T are as in the definition of tree patterns. The elements of I are called interval variables. A match \\(\\mu\\) of a tree pattern \\(\\sigma = (V_\\sigma, E_\\sigma, l_\\sigma)\\) with interval variables in a tree \\(t = (V_t, E_t, l_t)\\) is a mapping \\(\\mu: V_\\sigma \\rightarrow 2^{V_t}\\) from nodes of \\(\\sigma\\) to sets of nodes of t such that (1) the image sets are disjoint, and (2) \\(\\Sigma \\cup \\mathbb{N} \\cup T\\)-labelled nodes of \\(\\sigma\\) are mapped to singletons. Also, additionally to the conditions on tree pattern matches, we have\n(iv) interval variables I are mapped to (possibly empty) contiguous intervals, i.e., \\(\\mu(I) = \\{u_e,..., u_{e+k-1}\\} \\) for a contiguous sequence ue, ..., ue+k\u22121 of children of some node v (called interval); and\n(v) interval variables are mapped consistently, i.e., if u, v \u2208 V are labelled with the same interval variable, then \\(|\\mu(u)| = |\\mu(v)|\\) and the subtree sequences of \\((t_{u_1},...,t_{u_k})\\) and \\((t_{v_1},...,t_{v_k})\\) are isomorphic, where u is mapped to the interval u1,..., uk and v is mapped to the interval v1,..., Vk by \\(\\mu\\)."}, {"title": "Summary and Perspectives", "content": "This paper takes a first step towards understanding how to algorithmically learn explanations for structural differences between trees. A language for specifying tree transformations based on tree patterns was introduced. It was shown that learning such transformations from examples is computationally hard, even for restricted cases. Towards practical application, we proposed an encoding of the problem as a propositional satisfiability problem. We validated the usefulness of the specification language and the effectiveness of the encoding exemplarily on a dataset for educational tasks for logical modelling obtained with the educational support system ILTIS.\nIn the future, we plan to address open theoretical questions as well as explore further applications.\nSeveral theoretical questions remain open regarding properties of (variants of) our tree transformation language. While we saw that already severe restrictions of the learning problem are NP-hard, it is unclear whether the general problem is even in NP. The main issue is that the size of intermediate trees can become exponential in the number of steps, since a transformation with m nodes can introduce O(m \u2013 1) copies of the tree to which it is applied, potentially resulting in trees of size \\(O(m^s)\\) after s steps. We conjecture that it is never necessary to construct large intermediate trees and that, therefore, the answer to the following question is positive.\nOpen problem. Is LEARNINGTREETRANSFORMATIONS in NP?\nAll restrictions studied here remain NP-hard. We leave open the complexity of the restriction asking whether there exists a single transformation explaining all given pairs in one step (i.e. the case s = 1 and r = 1).\nOpen problem. Is LEARNINGTREETRANSFORMATIONS in PTime when fixing s = 1 and r = 1?\nAs a first step, we can prove that this case is solvable in polynomial time if all transform- ations must be applied at the root node of a tree (see Appendix D).\nWe made several design choices, including injective semantics and patterns that must match the shapes of trees very closely. Studying variants where different choices are made may be insightful as well.\nOpen problem. How do slight changes in the definition of tree pattern transformations (e.g. injective vs. non-injective semantics) impact algorithmic properties and expressive power?\nBesides addressing the theoretical questions, we also plan to explore further applications, among other things, (1) the (empirical) application of this approach to identify typical mistakes in modelling for other formalisms (including further logics, query languages, etc.), (2) the extension of the approach to other educational tasks, such as the identification of mistakes in equivalence transformation tasks, and (3) similar approaches for educational tasks in other domains, such as the formal language domain. These applications are at the borderline of CS education research and theoretical computer science, in particular database theory, but we expect that they raise further interesting theoretical questions."}]}