{"title": "Script-Strategy Aligned Generation: Aligning LLMs with Expert-Crafted Dialogue Scripts and Therapeutic Strategies for Psychotherapy", "authors": ["XIN SUN", "JAN DE WIT", "ZHUYING LI", "JIAHUAN PEI", "ABDALLAH EL ALI", "JOS A. BOSCH"], "abstract": "Chatbots or conversational agents (CAs) are increasingly used to improve access to digital psychotherapy. Many current systems rely on rigid, rule-based designs, heavily dependent on expert-crafted dialogue scripts for guiding therapeutic conversations. Although recent advances in large language models (LLMs) offer the potential for more flexible interactions, their lack of controllability and transparency poses significant challenges in sensitive areas like psychotherapy. In this work, we explored how aligning LLMs with expert-crafted scripts can enhance psychotherapeutic chatbot performance. Our comparative study showed that LLMs aligned with expert-crafted scripts through prompting and fine-tuning significantly outperformed both pure LLMs and rule-based chatbots, achieving a more effective balance between dialogue flexibility and adherence to therapeutic principles. Building on findings, we proposed \"Script-Strategy Aligned Generation (SSAG)\", a flexible alignment approach that reduces reliance on fully scripted content while enhancing LLMs' therapeutic adherence and controllability. In a 10-day field study, SSAG demonstrated performance comparable to full script alignment and outperformed rule-based chatbots, empirically supporting SSAG as an efficient approach for aligning LLMs with domain expertise. Our work advances LLM applications in psychotherapy by providing a controllable, adaptable, and scalable solution for digital interventions, reducing reliance on expert effort. It also provides a collaborative framework for domain experts and developers to efficiently build expertise-aligned chatbots, broadening access to psychotherapy and behavioral interventions.", "sections": [{"title": "1 INTRODUCTION", "content": "The integration of chatbots, or conversational agents (CAs), into psychotherapy and behavioral interventions [5, 39, 40, 59, 72, 85, 89] has transformed the way mental health services are delivered, providing around-the-clock accessibility and support. Traditionally, these chatbots have relied heavily on rule-based systems [51] where expert-written scripts [70, 78] were needed to facilitate therapeutic dialogues, ensuring controlled and therapeutically precise interactions. expert-written scripts [70, 78] and While this approach is reliable, it often leads to rigid conversations that lack flexibility and diversity, largely due to the need for extensive domain expertise in designing and crafting dialogues. The advent of Natural Language Generation (NLG) [23] has marked a shift away from these strictly predefined interactions, with systems being developed to generate more dynamic conversations. For example, Motivational Interviewing (MI) [52] chatbots have employed rephrasing techniques and template-based [22] dialogue generation to improve user engagement [5, 39, 54]. However, despite these advancements, such NLG systems are still limited by their reliance on domain-specific data, which is especially scarce and sensitive in the field of psychotherapy.\nThe rise of large language models (LLMs) [13] in psychotherapy has shown promising potential [18], offering new pathways for personalized support and engaging interactions in therapeutic settings. Trained on vast amounts of human dialogue, LLMs can handle flexible conversations and simulate conversational empathy and understanding without additional training [68, 76], a critical foundation for emotional intelligence in mental health care. However, significant challenges remain, particularly the non-transparent and uncontrollable nature of these models [24, 25], which raises quality and safety concerns in sensitive psychotherapy contexts. In addition, LLMs also face two critical challenges when applied to the specific domains of psychotherapy: (1) LLMs often lack domain-specific knowledge required to initiate conversations on specific psychotherapeutic topics, such as increasing intrinsic motivation or cognitive behavioral practices [9] like \u201cmindfulness\u201d [29], \"should statements\" [2], as shown in Fig 2, and (2) LLMs struggle to generate therapeutic questions (see Fig 2) and reflections that are essential for guiding a meaningful therapeutic dialogue, particularly in delivering psychotherapy-embedded intervention techniques such as Motivational Interviewing (MI). As a result, domain experts in clinical and health psychology still tend to favor rule-based systems, as they ensure precision and relevance through expert-crafted scripts, although the problems of rigid and unengaging conversations remain. However, this reliance on predefined dialogue scripts introduces another challenge: (3) creating these expert-crafted dialogues with domain expertise is highly time-consuming and costly, limiting scalability."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Chatbots and Conversational Design for Digital Psychotherapy and Behavioral Intervention", "content": "The application of chatbots or conversational agents (CAs) in delivering psychotherapy and behavioral interventions has gained significant attention as a means of enhancing access to psychotherapeutic services like behavioral intervention [5, 39, 72] and mental health support [59, 85]. Rule-based chatbots [51], which operate on predefined dialogue scripts, are widely used in digital psychotherapy for their high controllability and transparency-essential qualities in sensitive fields like mental health. These systems ensure precision in addressing therapeutic needs and prevent deviations from clinically validated pathways. For example, studies have used rule-based chatbots for Motivational Interviewing (MI) [49, 52] and Cognitive Behavioral Therapy (CBT) [9] to support interventions like smoking cessation [5, 39] and physical activity promotion [48, 72], where controlled, structured responses are crucial.\nDespite the benefits of transparency and controllability, rule-based chatbots face limitations in empathy and flexibility. Their interactions tend to be rigid, lacking the dynamic adaptability of human therapists, which is particularly essential in psychotherapy, where a nuanced understanding of user queries and empathetic responses is critical [86]. For instance, a core principle of MI is reflective listening [53], where the therapist mirrors the client's expressions to promote self-reflection and insight. However, rule-based chatbots struggle to emulate such responsive empathy [40], as their responses are confined to predefined scripts that may not fully capture the depth of user needs or emotional nuances. More importantly, rule-based chatbots typically rely on intent-based dialogue systems [19, 51], where domain experts design expert-crafted scripts to define the chatbot's response to various user intents, ensuring consistent therapeutic guidance and simulating therapeutic conversations with a high degree of accuracy and reliability. However, this dependence on expert-written scripts makes rule-based chatbots highly resource-intensive, as they require extensive input from experts to develop domain-specific dialogue scripts [17, 70, 78]. Implementing these expert-crafted dialogues often involves encoding intricate conversation designs that incorporate strategies from psychotherapy such as MI or CBT, making the development process time-consuming and costly. Therefore, our research has developed extensive pre-scripted dialogues by domain experts tailored for physical activity interventions using MI and CBT."}, {"title": "2.2 Generative Language Models for Psychotherapy and Mental Healthcare", "content": "The integration of generative language models [22, 64, 69] into digital psychotherapy shows promise in enhancing conversational flexibility and depth in chatbot-driven therapeutic interventions. Initial efforts to improve rigid, rule-based chatbots led to hybrid systems from prior work by [7] that combined Natural Language Generation (NLG) [23] with predefined dialogue scripts. These systems aimed to improve conversational fluidity by incorporating generative elements to create more empathetic, reflective responses, a core principle of MI [49, 52]. For example, hybrid models generate reflective statements or paraphrases to encourage clients to explore their thoughts while staying aligned with therapeutic goals [73]. With advancements in large language models (LLMs) [13, 32], the potential for more adaptive and"}, {"title": "2.3 Alignment of LLMs with Domain Expertise and Human Instruction", "content": "Aligning LLMs for dialogue generation has evolved to focus on more than linguistic fluency, emphasizing alignment with specific dialogue objectives and domain knowledge [44]. This approach, also known as instructed dialogue generation [35] or the approach called reinforcement learning from human feedback (RLHF) [55, 57], represents a shift from traditional pure probabilistic generative models [10] toward more balanced and adaptive generative models. These models are capable of tailoring responses based on context [88], intent [19], and conversational strategy [66, 84], which enables more effective \"mixed-initiative\" dialogues [20, 77] where both the model and user can guide the conversations with specific dialogue objectives.\nIn psychotherapeutic settings, aligned dialogue generation can support models to embed psychological and empathetic principles within responses [31, 62, 66, 75, 86], aiming to align output with therapeutic objectives while enhancing client engagement. This approach helps ensure that the dialogue remains relevant to therapeutic goals and enhances engagement [68], crucial elements in psychotherapeutic interactions. The need to balance flexibility with adherence to therapeutic goals has spurred research into alignment techniques that guide LLM responses while maintaining domain relevance. Approaches like prompting [46], in-contextual learning [81, 83, 87, 88, 91], and domain-specific fine-tuning [93] have emerged to embed expert knowledge [71] into LLMs for high-stakes applications. Prompting, for instance, allows models to leverage instructions without extensive retraining, making it a scalable solution for aligning LLM outputs with specific therapeutic frameworks. Fine-tuning, on the other hand, offers more precise control but requires substantial domain-specific data, which may not always be available, particularly in specialized fields like psychotherapy. For digital psychotherapy and behavioral interventions, alignment techniques provide a critical safeguard, ensuring that responses remain within therapeutic guidelines while accommodating the adaptability of generative models. These structured approaches allow LLMs to effectively emulate the precision of rule-based systems"}, {"title": "3 CREATING DATASET WITH EXPERTS-CRAFTED DIALOGUE SCRIPTS", "content": "A team of fifteen clinical and health psychology experts, each holding a master's degree or higher, collaboratively developed a comprehensive dataset of pre-scripted, tree-structured dialogues as Fig 2 designed to support physical activity interventions. Based on the psychotherapeutic techniques of Motivational Interviewing (MI) [49, 52] and Cognitive Behavioral Therapy (CBT) [9], these dialogue scripts target therapeutic scenarios common in behavioral interventions. Examples of crafted dialogue scripts are shown in Figure 3, covering topics such as \"rating confidence of change\" in MI and \"Supportive Social Environment\" in CBT.\nThe creation process began with a brainstorming session, where experts identified key psychotherapeutic topics related to behavioral interventions, particularly those focused on promoting physical activity. Topics such as overcoming barriers to exercise, enhancing sleep hygiene, and incorporating mindfulness practices were selected to encompass diverse therapeutic situations. Each expert individually crafted dialogue scripts for specific topics, ensuring that the dataset represented the depth and variability of real therapeutic interactions and reflected both MI and CBT frameworks. To ensure the scripts were clear, coherent, and authentic to therapeutic settings, a think-aloud protocol [30] was implemented. Three participants, independent of the dataset development, reviewed the scripts, verbalizing their"}, {"title": "4 STUDY ONE: CONCEPT OF ALIGNING LLM WITH EXPERT-CRAFTED DIALOGUE SCRIPTS", "content": "Study One proposed a concept of aligning LLMs with expert-crafted dialogue scripts for delivering psychotherapy. This alignment aims to balance adherence to therapeutic principles and expert knowledge with conversational flexibility and engagement unique to LLMs. Thus, the study compares (1) a rule-based chatbot strictly following expert-crafted scripts, (2) an LLM-powered chatbot aligned to expert-crafted dialogue scripts, and (3) a pure LLM without expert-crafted dialogues. By evaluating these chatbot types, we aim to explore the potential of aligned LLMs to advance the capabilities of chatbots in delivering flexible, engaging, and expert-guided psychotherapy."}, {"title": "4.1 Concept: Script-Aligned Generation (SAG) through Fine-tuning and Prompting", "content": "We proposed two approaches for the proposed concept of strictly aligning LLMs with expert-crafted, tree-structured dialogue scripts designed for healthier lifestyle interventions (shown in Fig 2): fine-tuning (LLM-SAG (FT)) [93] and prompting (LLM-SAG (Prompt)) [46]. For the fine-tuning approach, we fine-tuned a GPT-4 model [27] on our developed dataset of expert-crafted dialogue scripts. This approach ensures the LLM closely follows the structure, content, and therapeutic strategies embedded in the expert dialogues, making the chatbot's responses consistent with crafted dialogues. For the prompting approach we called \"LLM-SAG (Prompt)\", we employed the 'Tree-of-Thoughts' technique [87], as depicted in Figure 4. This technique utilizes a Breadth-First Search (BFS) algorithm [43] to guide the LLM through the tree-structured dialogue scripts, ensuring the model follows the correct dialogue pathways without extensive retraining. By prompting the GPT-4 model to navigate through predefined dialogue branches, \u201cLLM-SAG (Prompt)\" offers an efficient alternative to the resource-intensive process of fine-tuning [56]. The 'Tree-of-Thoughts' technique is particularly well-suited for our case, as it effectively instructs LLMs on tree-structured data, allowing the chatbot to maintain the integrity of the expert-crafted dialogues while minimizing computational costs."}, {"title": "4.2 Study Methods", "content": ""}, {"title": "4.2.1 Study design and procedure.", "content": "We conducted a human evaluation study employing a within-subjects design to compare the performance of four types of chatbots: 1) a rule-based chatbot strictly followed the expert-crafted dialogues (\"rule-based\"); 2) a pure LLM (\"pure LLM\"); and LLM-powered chatbot aligned with expert-crafted scripts through either 3) prompting (\u201cLLM-SAG (Prompt)\") or 4) fine-tuning (\u201cLLM-SAG (FT)\").\nBefore the study, each participant received an information letter and provided informed consent. After, participants interacted with each chatbot in a randomized order and completed a survey immediately after interaction to assess their experience with that specific chatbot. This within-subjects design allowed for a direct comparison of user assessments across the different chatbot types under consistent conditions, aiming to identify which type of chatbot best performs in facilitating therapeutic interactions. A comprehensive overview of the study procedure is illustrated in Figure 1."}, {"title": "4.2.2 Measures.", "content": "We assessed the four chatbot conditions using several measures as follows:\nLinguistic Quality: Evaluates the fluency, relevance, naturalness, coherence, and the human-likeness of the chatbot's responses, five assessing items are adopted from [15, 34]. Example items are \"The conversations of the chatbot are fluent.\" and \"The conversation of the chatbot is more like a person or a bot.\"\nDialogue Relevance: Inspired by work [12], two self-constructed items are used to specifically assess the alignment of expert-crafted therapeutical dialogues. Example items are \u201cThe chatbot asks an appropriate amount of questions.\" and \"The chatbot stays on the specific topic of the conversation.\"\nEmpathy and Engagement: Gauges perceived empathy of the chatbot's conversations, which is crucial for therapeutic effectiveness. We adopted the questionnaire from [40, 65] with two items: \"The chatbot seems to know how I was feeling.\" and \"The chatbot seems to understand me.\u201d To assess the chatbot's ability to maintain user interest and interaction as the engagement level, a questionnaire from [58] was adopted with four items. Example items are \"I lost myself in the interaction with this chatbot.\" and \"The chatbot is enjoyable to talk to.\"\nPerceived Motivational Interviewing (MI) Adherence: Evaluates how well the chatbot simulates an MI session and adheres to the MI principles. We adopted a questionnaire from [40, 47] with four items. Example items are \"The chatbot helped me talk about changing my behavior.\" and \"The chatbot helped me feel hopeful about changing my behavior.\"\nMotivation for Physical Activity: Measures the chatbot's impact on participants' motivation to change, a key factor in therapeutic outcomes. We self-constructed an item \"I am motivated to make changes in the behavior after I talk with the chatbot.\"\nTherapeutic Alliance: Explores the rapport and connection between the chatbot and the user. We adopted the questionnaire from [41] with six items. Example items are \"I believe this chatbot can help me to address my problem.\" and \"This chatbot encourages me to accomplish tasks and make progress for the change we discussed.\"\nUsability: Measures the ease of interaction with the chatbots. We adopted the questionnaire called Bot Usability Scale from [11] with seven items. Example items are \"Communicating with the chatbot was clear.\" and \"The chatbot's responses were easy to understand.\"\nTwo open-ended questions were additionally included to capture participants' detailed feelings and opinions about the chatbots: \"What have you enjoyed most or least about interacting with this chatbot?\" and \"What could be improved about this chatbot?\"\nAutomatic Evaluation Metrics: We also developed two self-defined, tailored metrics to objectively evaluate the chatbots' performance in delivering expert-crafted psychotherapeutic topics and therapeutic questions (examples are"}, {"title": "4.2.3 Participants.", "content": "A power analysis conducted using G*Power [28] determined that a minimum of 30 participants was necessary to detect a medium effect size (Cohen's d=0.25) and a power of 90%. To ensure a more robust sample, we recruited 43 participants (N=43) through institutional recruitment channels. The participants represented a diverse demographic profile, including variations in age, gender, and educational background. Eligibility criteria required participants to be at least 18 years old and fluent in English. Participation was voluntary and received monetary compensation in accordance with institutional guidelines for completing a 30-minute online session. This study was reviewed and approved by the institute's ethical committee. Detailed participant demographics are presented in Table 1."}, {"title": "4.2.4 Web-based interfaces for chatbot interaction and evaluation.", "content": "The study was conducted using a self-developed web-based application that seamlessly integrated both the chatbot interaction and the evaluation survey within a single interface as shown in Figure 5. This integrated design allowed participants to interact with the chatbot and provide immediate feedback in one continuous session, streamlining the user experience. The interface was carefully optimized for simplicity to ensure that participants could engage with the chatbot smoothly and without encountering navigational challenges. This user-friendly design can minimize distractions, enabling participants to focus on their interactions and evaluations."}, {"title": "4.2.5 Data analysis.", "content": "To examine how different types of chatbots influenced participants' perceptions in the context of psychotherapy for behavioral intervention, we first assessed the suitability of the data for statistical analysis. Normality was evaluated using the Shapiro-Wilk test [67], and homogeneity of variance was assessed with Bartlett's test [6]. As the data did not follow a normal distribution, we employed a Generalized Estimation Equation model (GEE) [37] for further analysis. The GEE regression was used to compare the mean scores across different measures between the rule-based chatbot condition and the LLM-powered chatbot conditions. Besides, post-hoc pairwise comparisons"}, {"title": "4.3 Quantitative Findings", "content": ""}, {"title": "4.3.1 Automatic evaluation metrics.", "content": "Table 2 shows the results of the automatic evaluation metrics, as demonstrated in 4.2.2. For \"Auto-Metric 1\" (psychotherapeutic topic completion), the rule-based CA scores the highest, followed by LLM-SAG (Prompt) and the pure LLM, which have similar scores. In \"Auto-Metric 2\" (therapeutic questions asked), the rule-based CA again leads, with the LLM-SAG (Prompt) close behind. The pure LLM scores the lowest, indicating a significant deviation from the expert-crafted dialogue scripts."}, {"title": "4.3.2 Descriptive statistics.", "content": "Participants were surveyed on their chatbot's prior experience and familiarity using the 5-Likert scale. The average frequency of chatbot usage was moderate at 3.14 (SD=0.83), and prior experience was rated"}, {"title": "4.3.3 Rule-based vs. LLM-powered chatbots.", "content": "After checking the statistical reliability, we conducted the analysis to compare different chatbots for behavioral intervention. As depicted in Table 3 and Figure 6, our study reveals distinct differences in user perceptions across the four chatbot types. To answer RQ1, the LLM-powered chatbot aligned with expert-crafted dialogue scripts employing the prompting approach (LLM-SAG (Prompt)) significantly outperforms the rule-based chatbots in linguistic quality (p=0.02), empathy (p=0.01), engagement (p=0.03), perceived MI (p=0.01), and motivation for physical activity (p=0.01). The pure LLM only surpasses the rule-based chatbot in empathy (p=0.04),"}, {"title": "4.3.4 Prompting (LLM-SAG (Prompt)) vs. Fine-tuning (LLM-SAG (FT)).", "content": "To identify the most effective approach for realizing SAG that align LLMs with expert-crafted dialogue scripts in psychotherapy, we compare LLM-powered chatbots using either prompting or fine-tuning with expert-crafted dialogue scripts. As shown in Table 4 and Figure 7, the prompted LLM-powered chatbot (LLM-SAG (Prompt)) significantly outperforms the fine-tuned ones (LLM-SAG (FT)) in measures such as linguistic quality (p=0.01), engagement (p=0.02), perceived MI (p=0.01), therapeutic alliance (p=0.08), and motivation for physical activity (p=0.2). These results indicate that the alignment approach has a significant impact on the effectiveness of LLMs in psychotherapeutic settings. The pairwise fine-tuning [93] might not be good at capturing the hierarchy tree structure of expert-crafted dialogue scripts."}, {"title": "4.3.5 Correlation analysis.", "content": "As shown in Fig 8, the Spearman correlation analysis revealed key relationships between evaluation dimensions across the four chatbot types: Rule-based, Pure LLM, LLM-SAG (FT), and LLM-SAG (Prompt). Across all chatbot types, Linguistic Quality was consistently correlated with Usability, Therapeutical Alliance, Perceived Empathy, Engagement and MI level, as well as the Motivation to change. This suggests that higher linguistic clarity and coherence led to better user interaction across the board. Engagement was another key measure, strongly"}, {"title": "4.4 Qualitative Findings: Open-ended Questions", "content": "We collected a total of 344 free-text responses (43 participants \u00d7 4 chatbot types \u00d7 2 open-ended questions). Our analysis identified three key themes shaping participants' perceptions of chatbots in psychotherapy for behavioral intervention. First, participants emphasized the importance of effective communication (Section 4.4.1). Second, the tone of the chatbot's language style influenced their perceptions (Section 4.4.2). Lastly, personalization and context awareness were highly valued, contributing to more engaging interactions for psychotherapy (Section 4.4.3)."}, {"title": "4.4.1 Effective communication in chatbots through clarity, coherence, and relevance.", "content": "The quantitative analysis of interactions with both rule-based chatbots and LLM-powered chatbots reveals significant differences in their ability to provide clarity, coherence, and relevance in communication. Participants also reported that LLM-powered chatbots generally offered more coherent and contextually relevant responses compared to the rule-based chatbots. Although participants appreciated the rule-based chatbot that gave more straightforward pre-written responses, they preferred LLM-powered chatbots, which were able to provide clear and concise answers to their specific free inquiries. One participant noted, \"I liked the fact that the chatbot was really clear in providing information, leading the users into questions\" This clarity is crucial in therapeutic dialogues, where participants seek guidance and support. The ability of LLMs to understand and respond to nuanced questions allows for more relevant advice, enhancing the overall user experience. In contrast, rule-based chatbots often struggled with relevance, as their pre-set responses could lead to generic or unrelated answers. A participant expressed frustration, stating, \"I do not like chatbots with pre-set responses. There was no flexibility in what I could talk about with the chatbot\" This lack of adaptability can hinder effective communication, making users feel as though their specific needs are not being addressed.\nBesides, LLM-powered chatbots demonstrated a superior ability to maintain coherence throughout conversations. Participants reported that these chatbots could follow the dialogue context more effectively, providing responses that were not only relevant but also logically connected to the previous dialogues. One participant remarked, \"The chatbot seems to stick to the context and provide decent responses to the questions asked\" This coherence is particularly important in therapeutic settings, where continuity and understanding are vital for effective support. In contrast, rule-based chatbots often produce disjointed responses that could cause confusion. A participant noted, \"The chatbot jumps to other topics, and sometimes gives me not relevant responses\" This inconsistency can disrupt the flow of conversation and diminish user trust in the chatbot's capabilities.\nTo enhance the effectiveness of both types of chatbots, participants provided several suggestions: They first suggested that rule-based chatbots could benefit from integrating LLM features to improve their ability to understand context and provide relevant responses. One participant stated, \"It could be connected to ChatGPT as well\". Participants also expressed a desire for chatbots to allow more open-ended interactions rather than relying solely on pre-set responses of the rule-based chatbot. A participant mentioned, \"It would be good if the chatbot can understand free text input other than the given buttons (of the rule-based chatbot).\" Additionally, participants indicated that responses should be concise and focused, avoiding overly verbose or repetitive answers. A participant suggested, \"If each response can be a bit shorter, that would be good.\""}, {"title": "4.4.2 Tones of language shape user perception and their relationships with the chatbots.", "content": "The tone of language used by chatbots can influence user perceptions and their relationships with the chatbots, particularly when comparing rule-based chatbots to LLM-powered chatbots. Participants reported that LLM-powered chatbots can foster a more relatable interaction compared to rule-based chatbots. Participants expressed a preference for the conversational tone of LLM-powered chatbots, which often felt more flexible and human-like. One participant noted, \"The tone of this chatbot is interesting and appealing. It talks like a human being.\" This human-like quality enhances user engagement, making interactions feel more personal and less mechanical. In contrast, rule-based chatbots often rely on pre-set responses that could come across as robotic or superficial. A participant remarked, \"I do not like chatbots with pre-set responses. There was no flexibility in what I could talk about with the chatbot.\" This rigidity can create a barrier to meaningful interaction, leading users to feel disconnected from the chatbot."}, {"title": "4.4.3 Personalization and context awareness build engaging interactions.", "content": "The findings reveal that personalization and context awareness are critical components in creating engaging interactions between users and chatbots. Participants highlighted differences between rule-based chatbots and LLM-powered chatbots, as well as between pure LLMs and those aligned with expert-crafted dialogues. Participants reported that LLM-powered chatbots excelled in providing personalized interactions that felt more relevant to their individual queries and needs. One participant noted, \"I liked the fact that the (LLM-powered) chatbot was really straightforward and clear in providing information, leading the users into questions.\" This ability to tailor responses based on user input fosters a sense of connection and being understood. In contrast, rule-based chatbots often relied on pre-set responses, which participants found limiting and rigid. A participant expressed frustration, stating, \"I do not like chatbots with pre-set responses. There was no flexibility in what I could talk about with the chatbot.\" This rigidity can lead to a superficial interaction, where users feel their specific concerns are not adequately addressed. Additionally, the distinction between pure LLM chatbots and those aligned with expert-crafted dialogues further emphasizes the importance of context awareness. While pure LLMs can generate contextually relevant responses, participants noted that those aligned with expert-crafted dialogues provided a more structured and supportive interaction. One participant remarked, \"It (LLM-SAG (Prompt)) guides my behavior change step by step in human-like conversations. And it is not overwhelming.\" This structured approach not only enhances the clarity of communication but also fosters a sense of trust and reliability in the chatbot's responses. Conversely, some participants found that pure LLM chatbots could sometimes lack depth in their responses. A participant commented, \"The conversation is brief and not overwhelming, but it can give me more specific information about behavioral change.\" This suggests that while LLMs can provide clear and quick answers, they may not always delve deeply into the user's context or needs, which is essential for therapeutic engagement."}, {"title": "5 STUDY TWO: APPROACH FOR EFFICIENTLY ALIGNING LLM WITH EXPERT-CRAFTED DIALOGUES", "content": "Building on the findings from Study One, which underscored the importance of expert-crafted dialogue scripts in aligning LLMs for effective psychotherapy, we recognized the need for a more efficient and flexible method that leverages core domain expertise without heavy reliance on extensive, time-consuming, and costly expert-crafted scripts. To address this, we proposed a new alignment approach called \u201cScript-Strategy Aligned Generation (SSAG)\u201d, designed to flexibly integrate domain expertise by using only essential components of expert-crafted content rather than full dialogue scripts. Inspired by prior work [14, 75], this method incorporates Motivational Interviewing (MI) strategies [53] to guide the LLM in generating responses through structured, adaptable MI techniques, aiming to balance therapeutic adherence with conversational flexibility. In Study Two, we conducted a long-term evaluation comparing SSAG's performance to the general SAG approach via prompting, to assess whether SSAG could achieve comparable or even superior efficacy in delivering effective psychotherapy for behavioral interventions."}, {"title": "5.1 Approach: Script-Strategy Aligned Generation (SSAG)", "content": "Script-Strategy Aligned Generation (SSAG) leverages key components of expert-crafted dialogue scripts to guide LLM responses while maintaining flexibility in generating conversational content. SSAG combines partially pre-scripted expert content with MI strategies to create a more efficient alignment method that reduces the reliance on fully structured dialogue scripts, focusing only on the most essential elements needed for therapeutic effectiveness.\nAs shown in Figure 3, expert-crafted dialogue scripts typically include four critical components: psychotherapeutic topics, sequence of therapeutic questions, reflections, and general information or advice. SSAG requires only three of these elements-psychotherapeutic topics, essential therapeutic questions, and general information or advice-as expert input. The reflective responses, which are essential for empathetic and context-sensitive dialogue, are generated dynamically by the LLM based on the conversational flow, thus avoiding the need for fully pre-structured scripts. The SSAG process operates in two main steps:\nStep 1: Predicting the MI Strategy. SSAG begins by predicting the next MI strategy based on context, selecting from core therapist behaviors such as \u201casking questions\u201d, \u201creflective listening\u201d, and \u201cgiving information\". By anchoring each response to a specific MI behavior, SSAG aligns the conversation with established therapeutic strategies [53, 66], providing structure while allowing flexibility in response generation.\nStep 2: Generating the Response. Once the MI strategy is identified, LLM generates the response according to the predicted strategy. If the strategy is \"asking questions\" or \"giving information\", the LLM retrieves expert-crafted content"}, {"title": "5.2 Study Methods", "content": ""}, {"title": "5.2.1 Study design and procedure.", "content": "We conducted a 10-day field study employing the mixed design to compare the performance of three chatbot types: a rule-based chatbot (used as the control condition) and two LLM-powered chatbots aligned with expert-crafted dialogues using either SAG (Prompt) or SSAG (more flexible alignment). Participants were assigned into two groups. One group interacted with the rule-based chatbot and the LLM-powered chatbot using SAG (\"LLM-SAG (Prompt)\"), while the other group interacted with the rule-based chatbot and the LLM-powered chatbot using SSAG (\"LLM-SSAG\"). The study followed a counterbalanced design, where participants in each group engaged with either the rule-based or LLM-powered chatbot for five consecutive days. After five days, they switched to the alternate chatbot condition, resulting in a 10-day study period for each participant. Evaluation surveys were completed immediately after each interaction to assess user experience and allow direct comparison of the chatbots under consistent conditions. This design aimed to explore if the proposed SSAG can align LLMs in delivering psychotherapy for behavioral interventions as effectively as the SAG using the prompting approach. A comprehensive depiction of the study procedure is provided in Figure 1."}, {"title": "5.2.2 Measures.", "content": "The measures in Study Two were designed to align with those used in Study One for consistency, including assessments of linguistic quality, engagement level, perceived empathy and MI level, therapeutic alliance, and usability for each chatbot. These chatbot-related measures were collected daily after participants interacted with the chatbots over five days. Participants' intrinsic motivation to change was measured every two days (i.e., pre-intervention, day 2, day 4, and post-intervention) using a questionnaire adapted from [50]. Additionally, we tracked daily steps and calorie consumption to gauge the effectiveness of the behavioral intervention aimed at promoting physical activity, with activity data collected continuously throughout the field study."}, {"title": "5.2.3 Self-developed chatbot application.", "content": "For the field study, we developed a mobile application as shown in Fig 10, designed to facilitate ongoing interactions between participants and the chatbot, while also tracking physical activity levels. The application features three main interfaces: the Menu Interface, which allows users to navigate between key functions; the Chat Interface, where participants engage in conversations with the chatbot; and the Activity Tracking Interface, which monitors key physical activity metrics such as daily steps, walking distance, and calories consumed. This integrated application ensured that participants could seamlessly interact with the chatbot and do assessments, creating a dynamic and engaging environment for behavioral intervention."}, {"title": "5.2.4 Participants.", "content": "A power analysis conducted using G*Power [28] determined that a minimum of 16 participants was necessary to detect a medium effect size (Cohen's d=0.25) with an alpha level of 0.05 and a power of 80%. To ensure a more robust sample, we recruited 22 participants (N=21) through institutional recruitment channels. Eligibility criteria required participants to be at least 18 years old and fluent in English. Participation was voluntary, and participants received monetary compensation in accordance with platform guidelines for completing a total 60-minute study session across 10 days. This study was reviewed and approved by the institute's ethical committee. Detailed participant demographics are presented in Table 5."}, {"title": "5.2.5 Data analysis.", "content": "To evaluate how participants perceive various chatbots differently, we first assessed the data's suitability for statistical analysis. The Shapiro-Wilk test [67] was used to assess normality, and Bartlett's test [6] checked for homogeneity of variance. Due to the data distribution not being fully normal, we utilized a Generalized Estimation Equation (GEE) model [37], which is robust to non-normal data, to compare the effectiveness of the rule-based chatbot, and the LLM-powered chatbot using SAG (Prompt) and SSAG."}, {"title": "5.3 Findings", "content": ""}, {"title": "5.3.1 Pre-validation of MI strategy prediction in SSAG.", "content": "We first pre-validate how effectively LLMs can predict MI strategies which are then used in the \"Step 2\" of SSAG. Building on work [14, 61, 75], we benchmark LLMs for the MI strategy prediction task on two open-sourced MI datasets, AnnoMI [94] and BiMISC [74]."}, {"title": "5.3.2 Rule-based vs. LLM-powered chatbots.", "content": "Study Two investigated each chatbot using the same measures", "conditions": "rule-based, LLM-SAG (Prompt), and LLM"}]}