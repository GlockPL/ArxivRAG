{"title": "Planning Transformer: Long-Horizon Offline Reinforcement Learning with Planning Tokens", "authors": ["Joseph Clinton", "Robert Lieck"], "abstract": "Supervised learning approaches to offline reinforcement learning, particularly those utilizing the Decision Transformer, have shown effectiveness in continuous environments and for sparse rewards. However, they often struggle with long-horizon tasks due to the high compounding error of auto-regressive models. To overcome this limitation, we go beyond next-token prediction and introduce Planning Tokens, which contain high-level, long time-scale information about the agent's future. Predicting dual time-scale tokens at regular intervals enables our model to use these long-horizon Planning Tokens as a form of implicit planning to guide its low-level policy and reduce compounding error. This architectural modification significantly enhances performance on long-horizon tasks, establishing a new state-of-the-art in complex D4RL environments. Additionally, we demonstrate that Planning Tokens improve the interpretability of the model's policy through the interpretable plan visualisations and attention map.", "sections": [{"title": "Introduction", "content": "Offline reinforcement learning (Offline RL) has emerged as a powerful paradigm, enabling agents to learn effective policies from a fixed dataset without requiring interaction with the environment, making them particularly beneficial in scenarios where data collection is costly or unsafe (Levine et al. 2020).\n\nA recent paradigm shift in Offline RL has come in the form of Reinforcement learning via supervised learning (RvS) which approaches Offline RL as a sequence modelling problem, with one notable implementation of this framework being the Decision Transformer (Chen et al. 2021). Unlike with traditional temporal difference based Offline RL methods, Decision Transformer (DT)s perform credit assignment directly, making them highly sample efficient. They are resilient to distractor signals, excelling in sparse reward environments, and can successfully model multimodal distributions, enabling better generalization and transfer across different tasks (Chen et al. 2021; Janner, Li, and Levine 2021; Micheli, Alonso, and Fleuret 2023).\n\nDespite these advantages, RvS methods, including DTs, still face two significant challenges:\n\n1.  Compounding Error: Auto-regressive token prediction suffers from compounding error in long, complex, or multi-task environments. RL environments, unlike language, are very sensitive to small errors as without opportunities for \"checkpoints\" (i.e., points where the agent can re-calibrate), even small errors will accumulate along the trajectory. (Janner, Li, and Levine 2021; Asadi et al. 2019).\n\n2.  Credit Assignment: Whilst DTs can handle credit assignment directly to an extent, they still require long contexts to assign credit effectively over long horizons. Without these contexts, which can lead to increased model complexity and slower training, their ability to reinforce an optimal long-term policy is limited (Badrinath et al. 2023).\n\nHierarchical reinforcement learning (HRL) offers a solution by decomposing long tasks into a series of shorter, more manageable sub-tasks. HRL employs a high-level controller to select sub-tasks or subgoals for a low-level worker policy, facilitating more efficient learning and reducing the impact of compounding error. However, HRL models introduce greater complexity, making them harder to train and extend. They are also task-specific and can even worsen the credit assignment problem due to their distinct separation between the low and high level tasks. (Pateria et al. 2021).\n\nIn this paper, we introduce a novel agent architecture that combines the strengths of RvS and HRL. We extend the DT framework by incorporating high-level Plans that the agent can learn to generate and use, implicitly guiding its long-horizon decision-making. This hybrid approach leverages the hierarchical decomposition of tasks to manage compounding errors, whilst maintaining the simplicity and efficiency of RvS. By incorporating the Plans seamlessly with the RvS trajectories and using a unified model, we additionally overcome HRL's credit assignment limitation as there is no explicit distinction between the high and low level policies.\n\nOur main contributions are as follows:\n\n1.  Dual-Timescale Token Prediction (Planning Tokens): We introduce the Planning Transformer model, that uses the novel concept of extending RvS methods with High-Level Planning Tokens.\n\n2.  State-of-the-Art Offline-RL Performance: We demonstrate our approach is competitive with, or exceeds state-of-the-art offline-RL methods in both long and short horizon tasks,"}, {"title": "Related works", "content": "Our paper integrates Offline RL, Hierarchical RL, and Model-based planning. We'll briefly explore each domain and relevant literature."}, {"title": "Offline RL via Supervised Learning (RvS)", "content": "Offline RL learns action policies from fixed datasets, valuable when online data collection is costly or unsafe. Challenges include sparse rewards and out-of-distribution states. RvS, introduced with Trajectory Transformer (Janner, Li, and Levine 2021) and Decision Transformer (Chen et al. 2021), addresses these by modeling Offline RL as a sequence problem. This approach naturally adheres to in-distribution actions and performs well in sparse reward environments.\n\nRvS excels in sparse reward settings but underperforms Temporal difference methods like CQL (Kumar et al. 2020) and IQL (Kostrikov, Nair, and Levine 2021) in dense reward environments. It struggles with long-horizon tasks due to compounding error (Asadi et al. 2019) and difficulty in assigning value to states (Kostrikov, Nair, and Levine 2021; Badrinath et al. 2023). While possible with a two-layer FNN (Emmons et al. 2022), RvS commonly uses transformers, which are expensive to train (Padalkar et al. 2023; Fournier, Caron, and Aloise 2023).\n\nOur model's Planning Tokens enables it to directly address the compounding error and credit assignment problem, enhance overall performance even in dense reward environments, and achieve strong results with few parameters making training faster and cheaper."}, {"title": "Hierarchical RL", "content": "HRL methods tackle long-horizon decision-making by learning high-level policies for selecting subgoals or sub-tasks, while low-level policies execute these actions (Bacon, Harb, and Precup 2017; Vezhnevets et al. 2017; Nachum et al. 2018; Mendonca et al. 2021; Sharma et al. 2019; Vezhnevets et al. 2016). Key models include the option-critic and feudal network architectures.\n\nRecent advancements include automated skill discovery (Mendonca et al. 2021; Sharma et al. 2019; Vezhnevets et al. 2016) and using latent space world-models for goal sampling (Mandlekar et al. 2020; Jiang et al. 2023). Large language models (LLMs) have been explored as high-level controllers for proposing subgoals (Wang et al. 2023; Huang et al. 2022) or subtasks (Ahn et al. 2022; Brohan et al. 2022; Driess et al. 2023).\n\nDespite progress, HRL methods face challenges in skill discovery and maintaining coherence between high and low-level policies. Our method addresses these issues, integrating strengths from both feudal and option-critic frameworks."}, {"title": "Model-Based Planning", "content": "Model-based Planning (MDP) uses forward dynamics models to predict future states, which in turn are used to improve state-value estimates for Offline-RL. This approach excels in high-dimensional environments with sparse rewards, where direct learning is challenging.\n\nMBP has been applied to discrete long-horizon tasks using Monte Carlo Tree Search (MCTS) methods (Sutton 1990; Silver et al. 2016, 2017; Schrittwieser et al. 2020) where the model performs rollouts in perfect information environments with small action spaces. MBP has also been extended to continuous search spaces by combining cross-entropy methods with probabilistic forward dynamics models often within latent observation spaces (Chua et al. 2018; Sekar et al. 2020; Hafner et al. 2019a; Ha and Schmidhuber 2018). Recent research has explored folding the forward-dynamics planning model into the action policy through iterative denoising diffusion policies (Janner et al. 2022; Chi et al. 2023; Wang, Hunt, and Zhou 2022), as-well as learning latent-temporal spaces for efficient compact planning (Jiang et al. 2023; Co-Reyes et al. 2018; Ozair et al. 2021; Hafner et al. 2019b)\n\nOur method is similar to MDP in that our Plan Generator is a forwards dynamics model that guides our action policy, however notably we do not perform a search within this space it is only used for guidance to the action policy. Our Plans are also in a latent planning space, however our method does not require an autoencoder to learn this space. Like diffusion policy methods our planning and action policy are unified however our method uses a transformer backbone allowing it to remain auto regressive reducing latency and allowing it to adapt to environmental changes."}, {"title": "Hierarchical Decision Transformers", "content": "Three principal models address long-horizon challenges in DT through Hierarchical RL: HDT (Correia and Alexandre 2022), WT (Badrinath et al. 2023), and ADT (Ma et al. 2023). Each uses a high-level policy to propose goal prompts for a decision-transformer acting as a low-level policy.\n\nHDT uses a hierarchical sub-goal policy with independent high and low-level DTs. WT employs a simple FNN for goal prediction, combining state and goal tokens. ADT uses HIQL (Park et al. 2024) for goal prediction, achieving SOTA results on the D4RL benchmark.\n\nOur model introduces Multi-token Plans, flexible conditioning targets, fixed-interval re-planning, and a unified model. This approach surpasses prior works in accuracy, flexibility, efficiency, and simplicity, while offering interpretability."}, {"title": "Method", "content": "The original DT (Chen et al. 2021) predicts the next action using a GPT-style Transformer (Vaswani et al. 2023; Radford et al. 2019) that takes as input the previous 7 returns-to-go (RTGs), states, and actions. We denote RTGs with r, states with s and actions with a:\n\n$(r_0, s_0, a_0, r_1, s_1, a_1,\\dots, r_{\\tau}, s_{\\tau}, a_{\\tau})$"}, {"title": "Plan Representation", "content": "Plan Sampling A Plan is a temporally high level representation of a trajectory, as such prior works have generated Plans by temporally compressing trajectories within the dataset using Variational Autoencoders (Jiang et al. 2023; Hafner et al. 2019b; Co-Reyes et al. 2018). We opt for a simpler method of just sparsely selecting timesteps from throughout the trajectory and concatenate them together.\n\nThis would seem to lose necessary information about the trajectory and one would expect it to be ineffective but we find quite the opposite. We believe that the reason our method remains effective despite its simplicity is that our unified training method means that naturally the model learns to optimize for Plans that contain information that is beneficial to the action prediction policy. This means that despite the Plans being simple they remain effective for guidance to the action policy.\n\nWe explored various methods for how to sample these timesteps, which we explore in detail in section 4.1.\n\nPlanning Token representation A Planning Token is a subgoal containing information about the agent's long-horizon future trajectory. Each Planning Token:\n\n\u2022 Maps to one future time step\n\n\u2022 Contains the full observation feature space or subset of it\n\n\u2022 Contains RTGS targets for reward conditioned environments\n\n\u2022 May contain the corresponding Action for that observation.\n\nWe explore various design choices for this representation in Section 4.1"}, {"title": "Input Sequence Construction", "content": "Once we have constructed the Plans we pair them with our trajectories during the dataset batch loader process.\n\nOur Sequence Construction is described as follows:\n\n1.  Plans are inserted after the first state and first return-to-go and before the first action\n\n2.  We subtract the first state of the plan during training or current state observation during evaluation to make the Plans relative rather than absolute."}, {"title": "Unified Training and Inference Pipeline", "content": "Unified training The architecture of our model remains largely consistent with the original DT. The primary modification is the addition of a planning head, which takes as input K consecutive tokens from the input sequence starting from $s_0$ and outputs K corresponding planning tokens.\n\nTo train this modified model, we employ a combined loss function:\n\n$\\mathcal{L} = \\alpha \\cdot \\mathcal{L}_{action} + \\beta \\cdot \\mathcal{L}_{plan}$\n\nHere, $\\mathcal{L}_{action}$ represents the L2 norm action loss, and $\\mathcal{L}_{plan}$ is a newly introduced L2 norm plan deviation loss.\n\nThe value of $\\alpha$ and $\\beta$ depends on the feature size of the Plans and the actions. In most cases we use $\\alpha = \\beta = 0.5$ to balance the two, but it may be necessary to bias the optimizer towards one of the other if one policy is significantly harder to learn than the other."}, {"title": "Evaluation", "content": "In our experiments, we evaluated our methods across various environments to comprehensively assess their performance under different conditions.\n\nFor goal-conditioned environments, we tested our methods on both short and long-horizon tasks to evaluate the model's ability to achieve specified objectives in diverse scenarios. The AntMaze environment, where an 8-DOF quadruped (\"Ant\u201d) navigates mazes of varying sizes (umaze, medium, large, ultra), was particularly useful for assessing the model's \"trajectory stitching\u201d capabilities-its ability to connect suboptimal trajectories to reach the desired goals. Additionally, the FrankaKitchen environment featured a 9-DOF Franka robot performing a sequence of goal-conditioned tasks in a kitchen setting.\n\nFor reward-conditioned tasks, we used the Gym-Mujoco suite, focusing on locomotion and control tasks with varying trajectory quality. We tested in medium-replay (medium + low-quality trajectories) and medium-expert (medium + high-quality trajectories) settings. The environments are HalfCheetah (a multi-jointed robot running forward), Hopper (a one-legged robot hopping without falling), and Walker2d (a bipedal robot balancing and walking).\n\nWe maintain mostly consistent hyperparameters across all environments and tasks, however we vary whether the model uses timestep embedding, the sequence length and embedding dropout probability for certain environments. We detail our hyperparameters in more detail in the supplementary material."}, {"title": "Ablation Study", "content": "Plan Sampling Method We tested four different methods for how to sample Plans: fixed-timestep width, fixed-distance width, and logarithmic-distance sampling. Fixed step sampling refers to sampling at equal gaps of either timesteps or distance, whilst logarithmic sampling, samples more timesteps from early on in the trajectory than later.\n\nWe found that distance-based sampling often produced better performance which we hypothesize is because it provides Plans which are more information-rich. We found that for ant-maze log sampling was more effective, for hopper replay it was a little less effective, but surprisingly it was much less effective for kitchen. Fixed-distance seems to be most overall effective.\n\nPlan Use Relative States Using relative states consistently improved performance across all environments compared to absolute states. We hypothesize that planning in relative space, helps the model generalize its planning policy.\n\nGoal Representation We tested goals being absolute observation space, goals being in relative observation space. Aswell as two more experimental ideas, of projecting the first state to the goal space and concatenating it with the goal in either absolute or relative observation space.\n\nGenerally, the experimental approach of projecting the first state to goal space and concatenating it to the goal, outperformed more naive methods, with the absolute goal version doing best. Our observations indicate that relative goals, helped the model generalize but decreased the accuracy of the Plans, and that absolute goals did the opposite. So we hypothesize that this goal representation lets the model generalize whilst keeping the goals accurate.\n\nUse actions in plan We find that whether or not to use actions in the policy is not clear, as it benefits some environments like Kitchen but in others it hurts performance. We hypothesize that actions can make the planning policy harder to learn, and may provide little additional information in some environments, but in environments with complex action spaces like Kitchen, inlcuding actions can give the Planning Tokens sub-task qualities aswell as sub-goal qualities which might be beneficial."}, {"title": "Comparison with Prior Methods", "content": "Our evaluation benchmarks the performance of PT against various state-of-the-art (SOTA) Offline RL methodologies. We include the following:\n\n\u2022  CQL (Kumar et al. 2020) and IQL (Kostrikov, Nair, and Levine 2021): Examples of SOTA Offline RL methods.\n\n\u2022  HIQL (Park et al. 2024): An example of a SOTA goal-conditioned Offline RL method.\n\n\u2022  RvS-R/G (Emmons et al. 2022) and DT (Chen et al. 2021): Baselines for RvS methods, with DT being the model we extended.\n\n\u2022  WT (Badrinath et al. 2023) and G/V-ADT (Ma et al. 2023): Examples of SOTA goal-conditioned DT variants.\n\nFor all methods, we reference reported results from previous works (Ma et al. 2023; Badrinath et al. 2023; Zeng"}, {"title": "The Utility and Interpretability of Planning with PT", "content": "Utility of the Plans Due to our addition of goal-conditioning and improved hyper-parameter tuning, it may be unclear to what degree our results are due to the Planning Tokens addition on their own. To demonstrate the efficacy of our planning approach, in Figure 3 we compare the normalized score of the model with Plans and with Plans disabled. Without Plans, the goal-conditioned transformer outperforms DT on many environments, for example on AntMaze-Medium-Diverse it achieves 41 which is much higher than DT's score of 0.0. However, for all environments, using Plans increases performance, over the No-Plan variation. The degree of improvement in the models performance appears dependent on the complexity of the environment as it significantly enhances performance in long-horizon in long-horizon environments like Kitchen-Partial"}, {"title": "Future Work", "content": "The Planning Transformer (PT) introduces a robust framework that opens several promising avenues for future research, aiming to expand its applicability and efficacy.\n\nAn immediate area of interest is the expansion of PT into online learning environments. Building on the groundwork laid by the Online DT research (Zheng, Zhang, and Grover 2022), PT's algorithmic Plan generation is well-suited for online adaptation. As new data is collected through exploration, the Plan generation policy can be dynamically refined.\n\nAddressing the limitations in non-Markov environments represents another critical direction for enhancing PT. Whilst sparse Plans have been surprisingly effective in the environments we benchmarked, it is possible there are environments where this simple method of generating Plans would not suffice, for example environments where decisions depend on intricate historical contexts. Incorporating temporal encoders, such as convolutional autoencoders or leveraging the Transformer architecture itself for dense temporal encoding, could significantly improve the representation and utility of Plans.\n\nFinally, since PT is just a single transformer model, there is little preventing this framework from being applied to other domains that use transformers, such as NLP. Recently, it has been shown that multitoken prediction can actually improve an LLM's coding performance (Gloeckle et al. 2024), as such it would be particularly interesting to see if PT applied to LLMs would provide similar enhancements particularly in their long-horizon and complex reasoning abilities."}, {"title": "Conclusion", "content": "We introduce the Planning Transformer (PT), a novel framework that integrates multi-token planning with transformer-based reinforcement learning via supervised learning methods. By prepending multi-token Plans to trajectory inputs, PT enhances long-horizon decision-making capabilities in Offline RL settings. Our approach achieves performance competitive or surpassing SOTA across a variety of challenging benchmarks in the D4RL Offline RL suite, while also being simpler and more flexible. The explicit incorporation of hierarchical planning enables better trajectory stitching and strategic reasoning over long time horizons. Furthermore, PT provides interpretability benefits by visualising the generated Plans and attention maps, promoting transparency in the model's decision-making process. Overall, our work represents a transformative integration of planning and transformers for Offline RL, opening new avenues for model-free hierarchical reinforcement learning. Future directions include extending PT to online adaptation, handling non-Markovian environments through temporal encoders, and incorporating the framework within large language models. The paradigm established by PT underscores the potential for further breakthroughs at the intersection of planning and powerful sequence modeling architectures like transformers."}, {"title": "Experiment details", "content": "We describe below each evaluation environment that we used to benchmark our model's performance:\n\n1.  Gym-Mujoco The Gym-Mujoco tasks involve a series of locomotion and control challenges with varying degrees of data optimality. We benchmark the medium-replay and expert categories, where medium-replay includes both medium and low-quality trajectories, while medium-expert includes medium and high-quality trajectories.\n\nThere are three environments for each of these data types:\n\n(a) HalfCheetah: A simulated robot resembling a cheetah with the task of running forward as fast as possible. The robot has multiple joints and the control input includes forces applied at these joints.\n\n(b) Hopper: A one-legged robot with the goal of hopping forward as far as possible without falling over.\n\n(c) Walker2d: A bipedal robot that needs to maintain balance and walk forward as effectively as possible.\n\nAs this is a purely reward-conditioned environment, we disable goal-conditioning by setting the goal to a 0-length vector.\n\n2.  AntMaze\n\nThe AntMaze tasks involve an 8-DOF quadruped (\u201cAnt\u201d) robot, which must navigate various simulated mazes from a start to a goal. This environment is designed to test an RL agent's \"trajectory stitching\u201d abilities. It comes in four sizes: umaze, medium, large, ultra, where umaze is a U-shaped maze. Ultra is a larger and more challenging version of AntMaze proposed by (Jiang et al. 2023)\n\nAntmaze comes in two dataset qualities: play and diverse, where play is a handpicked selection of starts and goals, while diverse randomly picks starts and goals. We choose to benchmark only the more challenging diverse environments due to computational constraints. We expect that performance on Diverse is also reflective of performance on Play.\n\nWhilst Antmaze-Diverse is trained on random starts and goals trajectories. The evaluation is always with the start at the bottom left of the maze and the goal at the top right. We have found that this evaluation configuration results in performance superior than if the goal was randomly chosen in the maze due to a bias of the ant to reach the top right corner, but we maintain this evaluation procedure for consistency with other works.\n\nGoals are provided as a single x,y location. In training we use the first two indices of the observation to extract goals from trajectories.\n\n3.  FrankaKitchen\n\nThe FrankaKitchen environment involves a 9-DOF Franka robot performing various goal-conditioned kitchen tasks with a large observation space, with many objects that can be interacted with. The ultimate goal is to complete four tasks in any order. The tasks to complete depend on the specific environment:\n\n(a) Partial: (1) opening the microwave, (2) relocating the kettle, (3) toggling the light switch, and (4) initiating the sliding action of the cabinet door.\n\n(b) Mixed: (1) opening the microwave, (2) relocating the kettle, (3) rotating the bottom burner knob, and (4) toggling the light switch.\n\nThis is the most challenging environment due to its high observation space, complex series of tasks, and sparse rewards.\n\nWe select the indices 11-29 from the State-Based observation space to use as the goal space. During evaluation, we use the observations with the goal indices selected and set the indices specific to the current tasks of the evaluation environment to their goal locations."}, {"title": "Evaluation Methodology", "content": "To evaluate our models, we trained our model on the environment for a sufficient number of update steps, and then performed rollouts. The methodology used by (Emmons et al. 2022) is to train 5 random seeds and then perform 100 rollouts to get a mean normalized score. The average of these normalized scores and the standard deviation is reported. Ours is the same except we use 3 seeds instead of 5, due to computational limitations. This should not affect results significantly. It was not clear whether (Emmons et al. 2022) used the last checkpoint or the best checkpoint when scoring, but in interest of fairness and adhering to standard testing procedures in the field, we use last checkpoint."}, {"title": "Selection of hyperparameters", "content": "For the FrankaKitchen and AntMaze experiments, we implement goal-conditioning with PT. For AntMaze, we restrict the Plans to only the first two features in the observation space, which contain the ant's body, as this provides a clear high-level observation space that can be used to make the Plans as effective as possible. However, for FrankaKitchen, there is no obvious subset of features to use as a high-level set, so we use all the features. For FrankaKitchen, we also included the actions.\n\nWe removed the timestep embedding for all environments except for FrankaKitchen, as we hypothesised they may make policy generalisation more difficult to learn. For example, why should the route an ant takes be different whether it begins taking it after 0 steps or 500?\n\nFor the Gym-MuJoCo tasks, which involve locomotion and have dense rewards, we apply reward-conditioning based on a target return. Unlike prior work, we don't constrain our high-level targets (the Plans) to only reward targets and instead use the full state, aswell as rewards to maximize the available information contained in the Plans.\n\nWe found that target returns needed to be set to the maximum reward or slightly above it. We used 1.0 for AntMaze, 4.0 for Kitchen, and a normalised score of 110 for MuJoCo.\n\nFor the AntMaze environment specifically, we found by visualizing the paths that a primary cause of failure was that the model would freeze up when the state went out-of-distribution. As a simple remedy, we added a small amount of noise to the action values, which was surprisingly effective."}]}