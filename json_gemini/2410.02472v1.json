{"title": "Meta-Models: An Architecture for Decoding LLM Behaviors Through Interpreted Embeddings and Natural Language", "authors": ["Anthony Costarelli", "Mat Allen", "Severin Field", "Joshua Clymer"], "abstract": "As Large Language Models (LLMs) become increasingly integrated into our daily lives, the potential harms from deceptive behavior underlie the need for faithfully interpreting their decision-making. While traditional probing methods have shown some effectiveness, they remain best for narrowly scoped tasks while more comprehensive explanations are still necessary. To this end, we investigate meta-models-an architecture using a \u201cmeta-model\u201d that takes activations from an \"input-model\" and answers natural language questions about the input-model's behaviors. We evaluate the meta-model's ability to generalize by training them on selected task types and assessing their out-of-distribution performance in deceptive scenarios. Our findings show that meta-models generalize well to out-of-distribution tasks and point towards opportunities for future research in this area.", "sections": [{"title": "Introduction", "content": "Al systems continue to increase in capabilities and complexity, and are becoming more widely deployed in real-world use cases. As these systems continue to become responsible for more tasks in our own lives, the need becomes paramount to verify that a model will do what we intend it to do. Further, if an issue were to arise with that model, we would need to be able to see its inner workings to find out what went wrong in that system.\nCurrently, the human ability to understand decision-making within AI systems such as LLMs is at best limited, but showing promise [Templeton, 2024]. Techniques such as probes allow us to look inside a model and see what it is \"thinking\u201d about when given a task [Gurnee and Tegmark, 2024], and has shown efficacy in various domains [Hern\u00e1ndez L\u00f3pez et al., 2022, Troshin and Chirkova, 2022]. Though powerful in some contexts, probes are not perfect [Levinstein and Herrmann, 2024]. In [Kumar et al., 2022] the authors mention the unavoidable inaccuracies of probing classifiers' representations when they use \u201cnon-concept features\" and [Zhang et al., 2022] points at the varying effectiveness when a model's input prompt changes. [Zhong et al., 2021] points at the impact that a base model's pretraining dataset has which allows for exploitation of probing accuracy. Most importantly, probes are limited in that they are trained for a specific task and offer little promise interpreting outside of their trained task set [Zhao et al., 2024].\nBuilding off the lack of flexibility of probe-based interpretation methods, we extend research on the meta-model architecture [Langosco et al., 2024] which utilizes an existing LLM already trained to process to a wide variety of situations and fine-tune it to interpret the internal activations of another model. The setup takes the activations of one \"input-model\u201d along with a question about said activations and are fed to an \"interpreting\" model (referred here on as the \u201cmeta-model\"). The meta-model will then interpret and answer questions on the input-model's activations. This allows humans to interact with natural language questions about behaviors of the input-model with a \u201cnatural language probe\" of sorts that has the potential to interpret a wide variety of behaviors.\nAn intended benefit of this approach is the ability to interpret a model's internal processing and receive verification about decision-making that humans would not be able to make sense of based on output text alone. In other words, we would have a \u201cmind-reading\" AI model. For example, if a model is lying about a statement of which humans don't have access to the ground truth of that statement, we wouldn't be able to verify the model's answer. But, if we were to instead rely on what's inside the \"brain\" of an LLM, we should receive more faithful results.\nIn this work, we specifically test the general interpretation abilities of the meta-model to extend to behaviors outside of its initial fine-tuning set. After fine-tuning the meta-model on initial task sets (e.g. detecting emotions, languages, etc.) we then evaluate its ability to interpret behaviors outside of that task set. In particular, we focus on interpreting a lying input-model.\nWe find that meta-models perform well in generalizing to interpreting lying in another model based solely on the input-model's internal activations. We also find that using models from different families still allows for interpreting behaviors with this architecture."}, {"title": "Contributions", "content": "In this work, we present several contributions to the field of AI Interpretability. Our contributions are as follows:\n\u2022 Generalization of Meta-Models in Out-of-Distribution Tasks: We further expand the meta-model architecture [Langosco et al., 2024] designed to interpret the internal activations of another LLM. We show an advantage over traditional probe techniques in the ability to generalize to new behaviors.\n\u2022 Generalization Across Model Families: We demonstrate that the meta-model architecture is capable of generalizing across different model families. Specifically, we show that a meta-model trained on microsoft/phi-2 can effectively interpret behaviors in an unrelated input-model, meta-llama/Llama-3.1-8B-Instruct.\n\u2022 Evaluation of Deception Detection: We test the meta-model's ability to generalize to detecting deceptive behavior in an input-model, even when trained on unrelated tasks. This"}, {"title": "Related Works", "content": "Mechanistic Interpretability seeks to make sense of the inner workings of the 'black box' of neural networks, with research spanning various levels of abstraction. Early work [Rumelhart et al., 1986] recognized that hidden units in neural networks could represent important features. The field has examined neural networks through different lenses: scrutinizing individual neurons [Alain and Bengio, 2016], mapping connections between neurons in circuits [Olah et al., 2020], analyzing weights to determine model characteristics [Krizhevsky, 2009, Ha et al., 2016], and uncovering high-level model behaviors and representations [Gurnee and Tegmark, 2024, Li et al., 2022, Nanda et al., 2023]. While much work has been done in this area, Mechanistic Interpretability is a young field with many unsolved problems [Bereska and Gavves, 2024, Elhage et al., 2022]. Given the limitations of the field, we explore a more flexible approach that leverages the power of LLMs to interpret neural activations. With the meta-model architecture, we can interpret any prompt from one model and ask any number of questions about it to receive natural language descriptions of said model.\nAutomated Interpretability arises in response to the complexities inherent in manual interpretability methods even when applied to smaller, simplified models [Elhage et al., 2021, 2022]. While some automated techniques like Sparse Autoencoders [Bricken et al., 2023] or automated interpretability agents [Shaham et al., 2024, Schwettmann et al., 2023] have eased efforts and found interesting results [Templeton, 2024, Cunningham et al., 2023, Gurnee et al., 2023], these methods differ from our work in that they focus on interpreting specific neurons where we focus on higher abstractions of behavior.\nMeta-models, a subfield of Automated Interpretability, leverages one model's decoding capabilities to interpret high level details of another [Langosco et al., 2024]. An early exploration of this concept was conducted in [Bills et al., 2023], where a GPT-2 neuron's activations were formatted as a prompt for GPT-4 to generate an explanation. One significant meta-model approach uses classifiers to determine \"internal\" states of models, as demonstrated in [Kolouri et al., 2020, Xu et al., 2020, Langosco et al., 2024]. Another example is the use of an autoencoder trained on the weights of many models to infer behaviors [Sch\u00fcrholt et al., 2022]. In this work, we aim to interpret one model's activations using another model's decoding abilities, without needing to train a model from scratch.\nActivation Patching, a recent technique for analyzing and manipulating neural networks, involves reading or selectively modifying (or \u201cpatching\u201d) specific activations within a model to either interpret or modify its behavior. Decoding the intermediate representations allows us to understand the progression of decision-making in a model [nostalgebraist, 2020]. Building on this paradigm, by interpreting representations in a network we can also edit specific representations to alter factual knowledge [Meng et al., 2023], aid in safety tasks [Turner et al., 2024, Zou et al., 2023], or even fine-tune models [Chen et al., 2024] based on the model's internal representations. Our work diverges from these approaches by focusing solely on interpreting existing behaviors without modifying the observed model. The closest related studies to our tasks are [Ghandeharioun et al., 2024, Chen et al., 2024], who similarly take one model's activations and patch them into another for interpretation. Though we build off of [Chen et al., 2024] in this study (see subsection 4.2 for implementation details), we instead explore the ability of a meta-model to generalize its ability to interpret another model's behaviors from its activations. To the best of our knowledge, no comprehensive study has been conducted on the generalizability of a meta-model architecture in interpreting and reporting behaviors in out-of-distribution tasks, which is the primary focus of our research."}, {"title": "Methodology", "content": "In this section, we detail the architecture and implementation of our experiments. Our setup is based on Chen et al. [2024] and involves conditioning an input-model to exhibit specific behaviors, capturing its hidden activations, and using the meta-model to interpret the input-model's activations to determine those specific behaviors."}, {"title": "Dataset Utilization", "content": "Each dataset is employed to elicit specific behaviors from the input-model by crafting tailored prompts (e.g. Listing 1). Each dataset is balanced such that P(Yes|question) = 0.5 for every question.\n\u2022 Sentiment Analysis (SST-2) Wang et al. [2019], Socher et al. [2013]: 67.3k movie reviews labeled as positive or negative, used for binary sentiment classification.\n\u2022 Emotion Detection (GoEmotions) Demszky et al. [2020]: Contains 58k Reddit comments annotated with 28 distinct emotions, used to classify emotional behaviors.\n\u2022 Language Identification Keung et al. [2020]: This dataset includes text samples in six languages-English, Japanese, German, French, Spanish, and Chinese.\n\u2022 Multilingual Sentiment Analysis Keung et al. [2020]: We make use of the same Language Identification dataset as above and incorporate its sentiment labels to test on sentiment classification across multiple languages.\n\u2022 Lying Detection Pacchiardi et al. [2024]: A set of 380 \"questions to which the model has not encountered the answer during pre-training (e.g., a statement about a fictional character) but for which we provide the correct answer in the prompt.\" We similarly provide the correct answer in the input-model's prompt and also use the author's definition of what it means for a model to lie; that it is \u201coutputting false statements despite \u2018knowing' the truth in a demonstrable sense\" Pacchiardi et al. [2024]."}, {"title": "Meta-Model Architecture", "content": "The Meta-Model Architecture relies on two models: the input-model and the meta-model. For the input-model we use meta-llama/Llama-3.1-8B-Instruct [Dubey et al., 2024] and for the meta-model we use microsoft/phi-2 [Javaheripi et al., 2023]. Both are accessed via the Transformers Hugging Face API [Wolf et al., 2020].\nThe meta-model is trained to classify the input-model behaviors by being given a \u201cYes\u201d or \u201cNo\u201d question and the captured internal activations of the input-model. To generate the meta-model's prediction it is prompted with a question and a set of placeholder tokens (e.g. Listing 2). The placeholder tokens are swapped with the internal activations immediately after the embedding layer. We perform no finetuning of the input-model.\nOur methodology consists of four main steps:"}, {"title": "Conditioning the Input-Model", "content": "We employ few-shot prompting to elicit specific behaviors from the input-model. For instance, to condition the model to exhibit negative sentiment, we structure the prompt as follows:"}, {"title": "Capturing Internal Activations", "content": "During a single forward pass of the conditioned input-model, we capture the internal activations from selected layers L1, L2,..., Ln. These activations are stored as vectors A1, A2,..., An."}, {"title": "Prompting the Meta-Model", "content": "The meta-model is prompted with a \"Yes\u201d or \u201cNo\u201d question regarding the input-model's behavior, alongside placeholder tokens that will be replaced by the captured activations after the embedding layer (as seen in Listing 2):"}, {"title": "Intercepting and Replacing Placeholders", "content": "During the meta-model's forward pass, we intercept the placeholder tokens and replace them with the captured activations A1, A2,..., An. The meta-model then continues operating as normal."}, {"title": "Empirical Findings", "content": "Our main results are located in Figure 1. The results strongly suggest that the meta-model architecture effectively promotes generalization to our designated out-of-distribution task of detecting lying. The steady improvement in performance as different dataset combinations are incorporated indicates that the model can learn to interpret information about a given input-model to enhance its predictive capabilities. This finding opens up numerous research opportunities which are listed in section 6."}, {"title": "Generalization to lie-detection", "content": "Our main results are located in Figure 1. The results strongly suggest that the meta-model architecture effectively promotes generalization to our designated out-of-distribution task of detecting lying. The steady improvement in performance as different dataset combinations are incorporated indicates that the model can learn to interpret information about a given input-model to enhance its predictive capabilities. This finding opens up numerous research opportunities which are listed in section 6."}, {"title": "Best and worst performance", "content": "The poorest performance is observed with the untrained meta-model, achieving an average evaluation accuracy of about 0.2, showing a drastic improvement of any training at all. In contrast, the highest accuracies were from models trained on the Sentiment and/or Multilingual dataset alone. Performing more experiments may show different performance, but \"more\" didn't necessarily mean \u201cbetter\u201d; possibly suggesting destructive interference from different datasets. For example, the model trained on Sentiment alone scored second highest with 0.75 but the model trained on Sentiment, Emotions, and Language identification was just above chance at 0.5."}, {"title": "Impact on performance from datasets", "content": "The Multilingual dataset consistently showed a positive impact on performance when included in training combinations. Conversely, while the Emotions and Language datasets sometimes contributed to improved performance, in other cases it appeared to have a neutral or slightly negative effect.\nThere's a general trend of improved performance as more datasets are combined, but this is not strictly linear. Some combinations perform better than others, suggesting complex interactions between"}, {"title": "Performance on other models", "content": "We extended our experiments to the internlm/internlm2_5-7b-chat [Cai et al., 2024] model to further assess the generalization to other models. The results listed in Figure 2, show several similarities with the meta-llama/Llama-3.1-8B-Instruct experiments while also showing unique results.\nAs in the main experiment, the Multilingual dataset continued to show a strong positive effect on model performance. Configurations that included the Multilingual dataset consistently outperformed those that did not. Most notably, the two highest and lowest performing (outside of 'no training') configurations are a difference of Multilingual being added or not. Language and Language + Sentiment scored lowest but with Multilingual added they were the highest scores.\nSimilar to the trends observed in the meta-llama/Llama-3.1-8B-Instruct experiments, we saw a general increase in accuracy as more datasets were incorporated into the training. Again, however, the improvement was not strictly linear. Strikingly different from the main results is how internlm/internlm2_5-7b-chat saw an increase from the Language dataset, where the main experiments showed worse performance when trained on this dataset. This further shows that datasets have a more complex relationship than \u201cmore is better\".\nIn general, the findings from the internlm/internlm2_5-7b-chat experiments strongly align with the results observed in the meta-llama/Llama-3.1-8B-Instruct model, demonstrating further robustness of the meta-model architecture."}, {"title": "Future Work", "content": "Our findings open up many future research directions. We outline some areas that warrant further investigation to expand on the meta-model approach.\n\u2022 Investigating the optimal combination and sequencing of datasets for training\n\u2022 Experimenting with different parameter sizes, layer sampling, number of layers, and spread of sampling points in the input-model. We take samples of one token every four layers, would wider spreads reveal different information?\n\u2022 Testing on further out-of-distribution safety relevant tasks (in addition to lying, detecting malware generation, sycophancy, backdoor detection, etc.)\n\u2022 Explore the causal effect of datasets on generalization abilities and training on new datasets, noting if there's a constructive or destructive influence from a dataset on a model's performance\n\u2022 Generating and training on new types of datasets\n\u2022 The effect of datasets on different types of models. For example, Language performs poorly with meta-llama/Llama-3.1-8B-Instruct but performs well (Figure 2) with internlm/internlm2_5-7b-chat\n\u2022 Influence the effect of training meta-models to output \"yes\" or \"no\" before any other training\n\u2022 Training an instruct-meta-model to give a greater depth of description of the input-model\n\u2022 Extend to multi-modal models and determine generality\n\u2022 Interpret a model during its pretraining phase\n\u2022 Identify adversarial attacks against meta-models\n\u2022 Training input-models to be more interpretable, possibly through LoRA layers"}, {"title": "Conclusion", "content": "Our study demonstrates the effectiveness of meta-models in generalizing to out-of-distribution model interpretation, focusing particularly on detecting deception without needing a given model's output text. We also demonstrate the efficacy across different model families using microsoft/phi-2 as the meta-model and meta-llama/Llama-3.1-8B-Instruct as the input-model. We find that meta-models offer superior generalization to out-of-distribution tasks compared to traditional probing methods. Though it remains unclear the exact effect that training on different combinations of datasets provides, training the models on sentiment, emotion, language identification, and multi-lingual sentiment tasks does increase performance in detecting deception.\nThis method of interpreting model behavior without relying on output text also opens up new possibilities for real-time monitoring and auditing of AI systems when we're not certain we can trust a model's output text. This research takes a step towards demystifying AI decision-making processes and extends new directions for researchers to explore more advanced meta-model architectures and training strategies."}]}