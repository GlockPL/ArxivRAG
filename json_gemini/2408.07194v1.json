{"title": "Massive Dimensions Reduction and Hybridization with Meta-heuristics in Deep Learning", "authors": ["Rasa Khosrowshahli", "Shahryar Rahnamayan", "Beatrice Ombuki-Berman"], "abstract": "Deep learning is mainly based on utilizing gradient-based optimization for training Deep Neural Network (DNN) models. Although robust and widely used, gradient-based optimization algorithms are prone to getting stuck in local minima. In this modern deep learning era, the state-of-the-art DNN models have millions and billions of parameters, including weights and biases, making them huge-scale optimization problems in terms of search space. Tuning a huge number of parameters is a challenging task that causes vanishing/exploding gradients and overfitting; likewise, utilized loss functions do not exactly represent our targeted performance metrics. A practical solution to exploring large and complex solution space is meta-heuristic algorithms. Since DNNs exceed thousands and millions of parameters, even robust meta-heuristic algorithms, such as Differential Evolution, struggle to efficiently explore and converge in such huge-dimensional search spaces, leading to very slow convergence and high memory demand. To tackle the mentioned curse of dimensionality, the concept of blocking was recently proposed as a technique that reduces the search space dimensions by grouping them into blocks. In this study, we aim to introduce Histogram-based Blocking Differential Evolution (HBDE), a novel approach that hybridizes gradient-based and gradient-free algorithms to optimize parameters. Experimental results demonstrated that the HBDE could reduce the parameters in the ResNet-18 model from 11M to 3K during the training/optimizing phase by meta-heuristics, namely, the proposed HBDE, which outperforms baseline gradient-based and parent gradient-free DE algorithms evaluated on CIFAR-10 and CIFAR-100 datasets showcasing its effectiveness with reduced computational demands for the very first time.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, researchers have invested their time and efforts in proposing variants of deep neural networks (DNNs) to solve real-world problems effectively. Training DNNs is commonly done using gradient-based optimizers such as Adam [1], [2]. Gradient-based optimization is a conventional method in training neural networks that necessitates the utilization of differentiable activation and loss functions. This approach involves the computation of the gradient of the objective function with respect to the model parameters, which is subsequently employed to update such parameters.\nThere is a strong relation between the choice of differentiable loss function and overfitting and a weak relation between the differentiable loss function and evaluation metric. In this way, the model becomes more prone to outliers in the training set if the objective function penalizes significant differences between the predicted and target labels because of a huge gap between data distributions. However, the ultimate objective is to achieve the expected performance on evaluation metrics such as F1-score on the unseen (test) data.\nAn alternative approach is using gradient-free optimization algorithms. These methods do not require calculating gradients and are helpful when computing gradients is impractical and complex. Thus, the objective function could be any, such as an evaluation metric (e.g., F1-score, Precision, Recall, etc.). These methods explore the behavior of objective function by iteratively evaluating different points in the parameter space and adjusting the search direction based on the function values observed. One of the well-known families of gradient-free optimization algorithms is meta-heuristic algorithms, designed to be versatile and can be applied to various optimization prob-lems. In the literature, meta-heuristic algorithms are applied to optimize DNNs for different purposes, such as weights and biases, the number of layers and neurons in the layers, and hyper-parameter settings. Based on a recent comprehensive review by Kaveh et al. [3], just 20% of studies worked on optimizing small-scale weights and biases in various neural networks, showing an open research direction. Hybridization of gradient-based optimization and meta-heuristic algorithms has been demonstrated to boost accuracy [4]\u2013[8]. These algorithms are inspired by metaphors from nature, such as evolution, swarm intelligence, or annealing. To increase the accuracy of Convolutional Neural Networks, Rere et al. [9] presented the techniques of Differential Evolution, Harmony Search, and Simulated Annealing algorithms, which fall under meta-heuristics [10]. Recently, Rokhsatyazdi et al. [11] proposed a two-extreme-point Coordinate Search (CS) algorithm to optimize a relatively small fully connected network with two hidden layers consisting of 266K parameters evaluated on the MNIST handwritten digits dataset. They reported that the learning convergence rate of CS as a meta-heuristic algorithm is faster than the Stochastic Gradient Descent (SGD)"}, {"title": "II. BACKGROUD REVIEW", "content": "One of the evolutionary population-based algorithms that gained popularity and drew attention from numerous opti-mization researchers is Differential Evolution (DE) [17]. The method should begin with a produced initial population, as with other evolutionary algorithms. The DE uses the crossover and mutation operators to create new trial vectors. The muta-tion operator calculates a vector \u0177i, or a mutant vector, using a linear combination function and randomly selected candidate solutions of the current population. The following is the linear combination function:\n\u0177i = Xi1 + F * (Xi2 - Xi3). \nwhere Xi1, Xi2, and Xi3 are three randomly selected member from the population and F is the scaling factor to control the differentiation. By exchanging the mutant vector Xi with its corresponding parent (target) vector xi, the crossover operator generates a new vector yi, also referred to as the trial vector. When calculating yi, one of the most well-known crossovers is binomial formulated as follows:\nYi,j= {Yi,j  randi,j\u2264 CR or j == jrand \nXi,j   otherwise\nwhere j = 1,..., D, an integer random number jrand in 1,..., D, a real random number randi,j \u2208 [0,1], and the crossover probability CR \u2208 [0,1] are all present. Finally, the selection operator compares the associated target vector Xi with trial vector vi and, after computing fitness values, chooses the most fit among them to represent the people of the following generation.\nRecently, a novel optimization algorithm called Block Dif-ferential Evolution (BDE) has been proposed by Khosrow-shahli and Rahnamayan [18] that addresses spatial issues in huge-scale problems and aims to reduce the convergence time of the Differential Evolution (DE) algorithm. The block scheme is a one-time blind clustering of dimensions in a search space, which was motivated by Block Coordinate Descent (BCD) and Discrete Coordinate Descent (DCD) algorithms [19]. The primary motivation behind BDE is to overcome chal-lenges associated with the memory requirements of embedded systems through a huge dimension reduction.\nIn conventional DE, the algorithm operates on the entire dimensionality of the problem, which can be computation-ally demanding for large-scale problems. BDE proposes a dimension reduction-based approach where specific grouped or blocked parts of the dimensions are saved and optimized"}, {"title": "III. PROPOSED ALGORITHM", "content": "This section describes the elements of the proposed hybrid of gradient-based and gradient-free DNN optimization process.\nIn the pursuit of optimizing Deep Neural Networks (DNNs), one can employ gradient-free optimization algorithms falling within the realm of meta-heuristic algorithms. The parameter space of DNNs comprises the weights and biases associated with the connections between neurons, and a high dimen-sionality typically characterizes it. Meta-heuristic algorithms may struggle to explore and exploit such large search spaces efficiently, leading to slow convergence or getting stuck in local optima. Meta-heuristic algorithms commonly depict the complete parameter set of a neural network as a singular vector. This vectorization simplifies the optimization task, adjusting all parameters uniformly and facilitating the use of conventional optimization methods. One solution involves de-creasing the weights and biases within the solution vector. This allows the application of meta-heuristic algorithms, such as evolutionary algorithms, which function on compact candidate solution populations, enhancing exploration and exploitation within extensive search spaces.\nWe claim that gradient-free optimization has many advan-tages in training deep neural networks. Two elements need to be defined for the gradient-free optimization procedure to optimize a DNN\u2019s parameters effectively: the search space and the objective function.\nA DNN architecture includes various layers, such as the convolutional layers and fully connected layers, which collectively enable the network to learn intricate patterns and representations from input data during the training process. The connections between each layer have trainable parameters called weight and bias, which are adjusted dur-ing the training process through optimization algorithms like gradient descent. These parameters play a crucial role in fine-tuning the model\u2019s ability to make accurate predictions by adjusting the strength and direction of the connections between neurons. In Gradient-Free Optimization (GFO), the set of all weights and biases is organized into a 1-D vector, commonly referred to as the parameter vector or parameter set. In this search space, the D is defined as the size of the dimensions. The GFO algorithms work directly with this parameter vector to explore and search the high-dimensional space of possible configurations. The optimization algorithm does not rely on gradient information, which is the key distinction from gradient-based optimization methods. Instead, it explores the parameter space based on the evaluations of the objective function at various points, adapting its search strategy to improve the model\u2019s performance iteratively. It is worth noting that using a 1-D parameter vector is a conve-nient representation. GFO algorithms treat the neural network parameters as a flat, continuous vector, and the optimization process is guided by the observed performance of different configurations in this parameter space.\nIn conventional deep learning, a deriv-able function calculates the loss of DNNs on a given task, and a gradient-based optimization algorithm is used to update parameters on backward propagation during training. However, the final performance is assessed by an evaluation metric to find the effectiveness or accuracy of the trained (DNN) model on the given task. Since the evaluation metrics such as accuracy, precision, recall, and F1-score are non-differentiable functions, the constraints related to gradient computation are impossible to use in the gradient-based optimization approach. This motivated us to choose a meta-heuristic algorithm to solve the optimization as it is robust to solve large-scale global optimization problems where the nonlinear and non-differentiable continuous space and the size are large and, therefore, considered widely used gradient-free methods. In order to handle the multi-class classification, we calculated the F1-score, which takes all classes to be equally important. The F1-score is calculated according to the following equation:\nf(x) = Fl-score= 2 \u00d7 Precision \u00d7 Recall/Precision + Recall\nwhere Precision can be calculated as follows:\nPrecision= True Positive/True Positive + False Positive\nSimilarly, Recall can be given as follows"}, {"title": "B. Training Stages by Gradient-free Optimization", "content": "A common problem in training a deep neural network is facing overfitting when the model learns to perform well on the training data but fails to generalize effectively on unseen data. It is more expected to happen when the model becomes too complex, or the number of adjustable parameters is large relative to the size and complexity of the data. In this section, we adopt two approaches to reduce the overfitting and increase the convergence rate of meta-heuristic algorithms by utilizing pre-training and random super-batches as follows:\nPre-training. A simple step to accelerate the optimization is to pre-train the model. The parameters in a pre-trained model are fitted to the vast majority of the training dataset, but the model cannot predict the minority of data. For only the pre-training stage of a DNN model, we aim to use Adam, which is a gradient-based optimization algorithm. Adam [1] is a well-known optimization algorithm in training DNN models, and it performs effectively for problems with a large number of data and/or parameters. As a matter of fact, Adam helps us accelerate the optimization to fit the most training data but not overfitting. However, there are some issues associated with using Adam, as follows: 1) Despite the fact that Adam has few parameters to be set, its sensitivity to the learning rate is deniable, 2) Utilizing an adaptive learning rate is good but causes a lack of orthogonality in model parameters, leading to poor generalization on unseen data. and, 3) Adam might not perform on noisy or non-stationary loss functions resulting in premature convergence. Because of this, pre-training model by Adam is halted after a few training epochs to avoid dropping on the overfitting problem. Gradient-free optimization is the next component that takes the position of the gradient-based optimizer, Adam, to fine-tune the model by exploring the search space.\nAs a part of gradient-free op-timization, we designed a data sampling system to replace super-batches randomly. In other words, the objective eval-uation in each iteration is calculated based on randomly selected super-batch data for all the NP population without overlapping with super-batches in the previous iterations. To this end, the candidate solutions in the population are fairly evaluated by cycles of the training dataset after iterations. Bodner et al. [20] argued that using moving (changing) super-batches could reduce the variance in the objective function, leading to an acceleration in the convergence rate of the meta-heuristic algorithm."}, {"title": "C. Histogram-based Blocking Differential Evolution (HBDE)", "content": "In this context, to reduce the search space complexity, we propose a parameter-reduction technique for deep neural networks inspired by the Block approach proposed by Khos-rowshahli et al. [18]. Our proposed algorithm has two leading steps: pre-training the DNN model with a gradient-based optimizer and fine-tuning with a gradient-free optimizer. The pre-training allows the model to learn useful representations from a large dataset quickly. Once the pre-training is complete, the model is ready to transfer the knowledge gained during the pre-training stage to the target task by fine-tuning the model on the dataset using a meta-heuristic algorithm such as DE.\nA histogram is commonly utilized in the pro-cess of data summarization, providing an approximate value, p(x) for the initial measurement of a variable x, as p(x), particularly in the context of computing efficiency [21]. The algorithm for creating a histogram of 1-D dimensional vectors divides each dimension into a set of equitized bins with bin width as h typically involves the following formula:\np(x) =1/N \u00d7 h\nWhere N is the total number of variables in the 1-D vector. The bin width is a crucial histogram parameter since it manages the trade-off between under- and over-smoothing the actual distribution. The histogram begins with calculating bin width, which determines the width of each bin by dividing the range of min and max weights, lw = min(W),uw = max(W), by the number of bins Nbins as follows:\nh = (uw - lw)/Nbin.\nWhere each bin represents a specific range of [lb\u2081, ub] for bini which i indicates the dimension. By having the lower and upper bounds for each wi, uniform random distribution is used to initialize the candidate solutions population. We discard the empty bins and reduce the number of blocks from Nbins to Nblocks, and in this way, the number of parameters in candidate solutions could be even less. In other words, the Nbins represents the expected search space dimension for DE to explore; however, the real number of blocks Nblocks optimally is smaller than Nbins.\nThe starting point of the DE algorithm is population initialization, and here, we designed a heuristic blocked and compressed population initialization based on Histogram-based Block (HBB) on pre-trained pa-rameters. In fact, the distance between parameters is very close enough to tie the weights. Therefore, we can divide the weights into bins using the histogram algorithm. Let us assume the weights in DNNs are W = [W1, W2, ..., WD] where D is the total number of trainable parameters in the model. By a given Nbins, parameters are placed into Nbins bins, and, after removing empty bins, the indexes of parameters placed in bini are stored in into Nblocks blocks. The proposed approach divides parameters into equal sizes of Nbins where the contents of each bin are blocked, and an average of dimensions is calculated and replaced into the blocked vector of parameters. The sparsity"}, {"title": "IV. EXPERIMENTAL ANALYSIS", "content": "Our experiments study the performance of HBDE in opti-mizing the ResNet-18 model on two well-known benchmark datasets. The ResNet is a deep learning architecture that was introduced by He et al. [12]. The total number of trainable parameters in ResNet-18 is ~11M without a fully connected network.\nThe CIFAR-10 and CIFAR-100 are image classification benchmarks that have 10 and 100 distinct classes, respectively. They include 50K 32 \u00d7 32 color images for train and 10K images for test [22]. The ResNet-18 model w is pre-trained on the ImageNet 2012 dataset [23]. Then, the ResNet-18 model w is fine-tuned, evaluated on circular super-batches of 10K training images, and finally evaluated on test images, reported by the F1-score metric.\nThe training process includes the following steps:\n1) Loading the pre-trained model w, which is trained by the ImageNet dataset. [24].\n2) Fine-tuning model w by a gradient-based optimizer, to get w'. We use Adam with a learning rate of lr = 10-3, a weight decay of zero, and a cross-entropy loss function formulated as follows:\nLcross-entropy (Y, Y) =- \u03a3y(k) log(y(k))\nWhere K is the number of classes in the dataset.\nThe parameters setting is reviewed in Table I where F is the mutation rate, and Cr is crossover probability. A comparison is made between the places where the model is pre-trained with Adam and fine-tuned with classic DE and HBDE with the parameter settings in Table I. For a fair competition, each gradient-free optimization algorithm is terminated by reaching MaXNFE = 100,000 number of objective function evaluations.\nTable II presents the findings of the CIFAR-10 dataset. The pre-trained model by Adam results in 80.76% test F1-score. In the next step, DE and the proposed HBDE fine-tune the model based on maximizing the F1-score. The classic DE resulted in 81.34% with an improvement of 0.82% in the performance of the pre-trained model. On the other hand, our proposed HBDE could reach 81.83% F1-score, which is 1.07% better performance compared to the pre-trained model and ultimately 0.5% better than its parent, classic DE algorithm.\nTable III displays the findings from the CIFAR-100 dataset. In more detail, the model is pre-trained by Adam and resulted in 52.61% and stopped by reaching the maximum number of epochs; then, in a separate optimization, DE optimized and is evaluated as 52.69% F1-score a similar performance to F1-score, whereas HBDE is much more successful with 53.30% in terms of F1-score metric.\nFrom Tables II and III, one can realize that the proposed HBDE has a dramatic convergence rate in the initial steps as the reduced and blocked population is randomly initiated within the blocks, showcasing the effectiveness of block uti-lization for gradient-free optimization algorithms. It is worth mentioning that the proposed method is explored in a 3K times smaller space, and tied weights located in blocks reconstruct the model with a single parameter."}, {"title": "V. CONCLUSION REMARKS", "content": "This paper presents a novel massive dimension reduction approach to tackle the sparsity in deep neural networks during gradient-free optimization. Our method blocks large numbers of parameters and reduces the search space to thousands of times smaller, allowing for straightforward utilization of meta-heuristics to train deep neural networks.\nOptimizing parameters in the ResNet-18 model with 11M trainable parameters by an evolutionary algorithm is inefficient regarding resources and ineffective in a huge-scale search space. We demonstrate that the accuracy does not drop by blocking weights in a deep neural network.\nUsing our proposed blocking approach, training, fine-tuning, and storing deep neural networks is much more efficient during inference time. Similar to other weight-sharing techniques in the literature, this approach stays with the same architecture and hardly shares the weights across the whole network.\nA major advantage of our approach is that huge dimension reduction enables meta-heuristic algorithms to take a step into optimizing huge-scale neural networks for different tasks with-out the limitations of traditional differentiable loss functions. In other words, it guarantees the applicability of gradient-free optimization, which has many benefits over traditional gradient-based optimization, such as trapping into local optima and stagnation. In addition, it opens the door for multi-objective training, with the benefit of multi-modal, multi-task, and multi-loss training. The experiments prove that the proposed HBDE outperforms Adam and its parent, classic DE, algorithm, which is evaluated on CIFAR-10 and CIFAR-100 datasets.\nIn future works, we aim to find the optimized number of bins Nbin and larger blocks to reduce the search space dimension further to result in a tiny trainable DNN. Given the successful approval of the block concept in the hybridiza-tion of gradient-based and gradient-free optimization, we are ambitious to eliminate reliance on pre-trained parameters by gradient-based optimization and train any deep neural network architecture from scratch only with gradient-free optimization in an efficient strategy. Moreover, we are able to break weighted sum (average) single-objective functions to multi-objective functions, for instance, in our work, convert Fl-score (see Eq. 3) to Precision (see Eq. 4) and Recall (see Eq. 5) and optimize by any multi-objective gradient-free optimization algorithms such as NSGA-II [25], efficiently and effectively."}]}