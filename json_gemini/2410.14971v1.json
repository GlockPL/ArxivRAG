{"title": "BrainECHO: Semantic Brain Signal Decoding through Vector-Quantized Spectrogram Reconstruction for Whisper-Enhanced Text Generation", "authors": ["Jilong Li", "Zhenxi Song", "Jiaqi Wang", "Min Zhang", "Zhiguo Zhang"], "abstract": "Recent advances in decoding language from brain signals (EEG and MEG) have been significantly driven by pretrained language models, leading to remarkable progress on publicly available non-invasive EEG/MEG datasets. However, previous works predominantly utilize teacher forcing during text generation, leading to significant performance drops without its use. A fundamental issue is the inability to establish a unified feature space correlating textual data with the corresponding evoked brain signals. Although some recent studies attempt to mitigate this gap using an audio-text pre-trained model, Whisper, which is favored for its signal input modality, they still largely overlook the inherent differences between audio signals and brain signals in directly applying Whisper to decode brain signals. To address these limitations, we propose a new multi-stage strategy for semantic brain signal decoding via vEctor-quantized speCtrogram reconstruction for WHisper-enhanced text generation, termed BrainECHO. Specifically, BrainECHO successively conducts: 1) Discrete autoencoding of the audio spectrogram; 2) Brain-audio latent space alignment; and 3) Semantic text generation via Whisper finetuning. Through this autoencoding\u2014alignment-finetuning process, BrainECHO outperforms state-of-the-art methods under the same data split settings on two widely accepted resources: the EEG dataset (Brennan) and the MEG dataset (GWilliams). The innovation of BrainECHO, coupled with its robustness and superiority at the sentence, session, and subject-independent levels across public datasets, underscores its significance for language-based brain-computer interfaces.", "sections": [{"title": "Introduction", "content": "Decoding text from brain activity, such as electroencephalography (EEG) and magnetoencephalography (MEG), is a critical and frontier research topic, that can provide a foundation for language-based brain-computer interfaces (BCI) by enabling direct text input through brain signals. In the long term, accurate real-time translation of human brain signals can promote the widespread application of BCI technology in medicine, assistive technology, and entertainment, bringing new possibilities to human life.\nWith the rapid developments in natural language processing (NLP), automatic speech recognition (ASR), and other fields, researchers have leveraged the powerful language understanding and generating capabilities of pretrained large language models (LLMs) for neural decoding tasks (Wang and Ji 2022; Duan et al. 2024; Yang et al. 2024a,b), making it possible to accurately decode text stimuli from non-invasive signals. EEG-to-Text (Wang and Ji 2022) is the first work to decode open-vocabulary tokens from encoded word-level EEG rhythm features with the pretrained large model BART (Lewis et al. 2020). Furthermore, DeWave (Duan et al. 2024) used sentence-level raw EEG signals to perform EEG-to-text decoding without eye movement event markers.\nLater on, several BART-based methods (Xi et al. 2023; Feng et al. 2023; Amrani, Micucci, and Napoletano 2024) were introduced, predominantly employing a pretraining-finetuning paradigm. These methods first align EEG representations with pretrained text embeddings before feeding them into BART for finetuning. Although these approaches have yielded impressive results, they rely on a teacher-forcing generation strategy, wherein the model depends on the ground truth preceding text during each token prediction. This setting does not accurately reflect the model's performance in real-world scenarios. These methods show poor decoding performance without teacher forcing.\nTo address this limitation, NeuSpeech (Yang et al. 2024a) and MAD (Yang et al. 2024b) treat raw MEG signals as a specialized form of speech, transforming MEG signals and feeding them into a pre-trained Whisper model (Radford et al. 2023), which is trained on large-scale audio-text pairs, for end-to-end text decoding without teacher forcing. However, these approaches primarily focus on mapping continuous brain signals to discrete text without compressing the signals into discrete representations, thereby limiting the model's decoding accuracy and generalization capabilities.\nExtensive researches in speech recognition (Zhang et al. 2023a; Puvvada et al. 2024) demonstrate that discrete representations preserve more semantic information for translation compared to conventional speech features like Fbank, thanks to their carefully designed self-supervised learning paradigms. While DeWave (Duan et al. 2024) aligns discrete representations of input EEG signals and text, it assumes a chronological order for the discrete token sequence, requiring a highly capable feature extractor. Considering the natural temporal alignment between audio-evoked brain signals and audio stimuli, aligning raw signals and speech within a discrete space leverages implicit temporal properties, thereby reducing the difficulty of training."}, {"title": "Related Works", "content": "Therefore, we propose a novel multi-stage semantic decoding framework for EEG/MEG brain signals, aurally evoked by semantic audio, through vector-quantized speCtrogram reconstruction for WHisper-enhanced text generation, termed BrainECHO. Specifically, BrainECHO executes the following steps: 1) Discrete autoencoding of the audio spectrogram, particularly employing codebook-based vector quantization, to establish a pre-warmed representation space that facilitates Mel spectrogram reconstruction; 2) Brain-audio latent space alignment, utilizing a brain encoder and pre-warmed quantizer and decoder to reconstruct the evoked brain signal's Mel spectrogram; 3) Semantic text generation, achieved through AdaLoRA-based finetuning of the pre-trained Whisper model, with the reconstructed Mel spectrogram as input. The overall three-stage (autoencoding, alignment, finetuning) training process of the proposed BrainECHO is illustrated in Figure 1. We validate the performance of BrainECHO using two different public audio-evoked brain signal datasets: Brennan, which contains EEG data, and GWilliams, which contains MEG data. The principal contributions of our work are summarized below:\n\u2022 The proposed BrainECHO framework overcomes the current flaw in EEG/MEG-to-text approaches that mistakenly rely on the teacher-forcing strategy. It achieves semantic decoding with significantly improved results compared to using Gaussian noise as the input.\n\u2022 We propose breaking down the EEG/MEG-to-text task into a multi-stage strategy to mitigate the biases induced by the overwhelming capabilities of large language models, while still leveraging their pre-trained knowledge, specifically utilizing Whisper in our work.\n\u2022 We introduce vector-quantized discrete representations to enhance the model's efficiency, achieving state-of-the-art (SOTA) performance on EEG and MEG datasets. Specifically, we evaluate BrainECHO across various data split scenarios, which are neglected in prior research.\nNon-invasive brain signals such as EEG and MEG offer significant advantages over invasive alternatives, particularly in terms of safety and cost-effectiveness. Significant progress has been made in decoding text from non-invasive signals.\nClosed-Vocabulary Neural Decoding Ghazaryan et al. (Ghazaryan et al. 2023) utilized Word2vec to decode 60 nouns from MEG recordings. Meta (D\u00e9fossez et al. 2023) developed a model employing wav2vec 2.0 (Baevski et al. 2020) and contrastive learning to decode speech from 3-second EEG/MEG signals. However, these methods are constrained to closed-vocabulary tasks, restricting their applicability in open-vocabulary text generation.\nDecoder-Only Architectures for Open-Vocabulary Brain-to-Text Decoding Recent advancements have leveraged the powerful understanding and generation capabilities of pretrained models, particularly LLMs, to extend vocabulary from closed to open. In decoder-only architectures, some researchers have aligned brain signals with text to guide pretrained generative models like GPT in text generation. For example, Zhao et al.(Zhao et al. 2024) mapped fMRI data to text embeddings to iteratively guide GPT-2 in generating text. Similarly, Chen et al.(Chen et al. 2024) used text-aligned fMRI representations as prompts for GPT-2 to decode language information.\nSeq2seq Architectures for Open-Vocabulary Brain-to-Text Decoding Wang et al.(Wang and Ji 2022) fed transformed word-level EEG rhythm features into a pretrained BART model to decode open-vocabulary tokens. Duan et al.(Duan et al. 2024) integrated discrete EEG encodings with text-EEG contrastive alignment to mitigate individual variability in brain activity. However, these BART-based methods rely on teacher forcing during inference. Furthermore, as Jo et al. (Jo et al. 2024) demonstrated, their performance on noisy data is comparable to that on EEG data, suggesting that these models may simply memorize the training data. Recently, NeuSpeech (Yang et al. 2024a) directly fed raw MEG signals into a modified, pretrained Whisper model for text decoding without teacher forcing. Furthermore, MAD (Yang et al. 2024b) introduced MEG-speech alignment loss to decode sentences not present in the training data. However, these Whisper-based methods do not utilize discrete representations of the original signals to enhance the model's generalization capabilities. Our work integrates brain-audio discretization and alignment, aiming to predict high-quality Mel spectrograms from brain signals that align with Whisper's input format. Leveraging Whisper's advanced speech recognition abilities, our approach generates sentences that closely mirror the original text."}, {"title": "Task Definition", "content": "Given the raw EEG/MEG $E$, text content $T$, and corresponding audio stimuli $A$, the experimental data can be divided into a series of sentence-level EEG/MEG-text-speech pairs $(e, t, a)$ according to $T$. Our goal is to decode the corresponding open-vocabulary tokens $t$ from the brain signal $e$, with $a$ serving as auxiliary information."}, {"title": "Model Architecture", "content": "Unlike the multi-task joint training employed in MAD (Yang et al. 2024b), BrainECHO adopts a three-stage training process. This method reduces resource consumption at each training step and facilitates the prediction of high-quality, high-resolution Mel spectrograms from brain signals. Specifically, we extend the spectrogram duration from 3 seconds, as used in (D\u00e9fossez et al. 2023; Yang et al. 2024b), to over 10 seconds, enabling sentence-level rather than segment-level brain-to-text translation, thereby preserving the semantics of the original sentences. The details of the model are shown in Figure 2. The following sections will detail each training stage.\nDiscrete Autoencoding of Audio Spectrogram Van den Oord et al. introduced the Vector Quantized-Variational AutoEncoder (VQ-VAE) (Van Den Oord, Vinyals et al. 2017) to learn discrete latent representations of audio, video, and other data types. Building on this approach, several studies (Li, Jia, and Chiu 2023; Sadok, Leglaive, and S\u00e9guier 2023; Yang et al. 2023) have explored representing Mel spectrograms using discrete tokens to capture phoneme-like information. Inspired by these methods, our first stage involves autoencoding Mel spectrograms, with the purpose of obtaining a discrete representation space that is conducive to Mel reconstruction. Specifically, given a spectrogram $m \\in \\mathbb{R}^{T_m \\times F_m}$, the audio encoder $Enc$ first converts it into a feature map $z_m = Enc(m) \\in \\mathbb{R}^{t_m\\times f_m \\times D}$, where $T_m$, $F_m$ and $D$ denote the number of time frames, frequency bins and latent channels, respectively. The spectrogram is generated by the Whisper Processor, enabling text decoding from the reconstructed spectrogram using Whisper's encoder-decoder architecture. Then, $z_m$ is processed by a vector quantizer $Q$. Specifically, each latent embedding $z_m^i \\in \\mathbb{R}^{D} (1 \\leq i \\leq t_m, 1 \\leq j \\leq f_m)$ is replaced by the nearset vector $z_q^i$ from a codebook $C\\in \\mathbb{R}^{N\\times D}$, which consists of $N$ learnable $D$-dimensional vectors. Formally, this process is expressed as follows:\n$Q(z_m^i) = z_q^i = c_k, where k = \\underset{k\\in\\{1,2,...,N\\}}{arg \\space min} \\space ||z_m^i - C_k||_2$. (1)\nThe reconstructed spectrogram is then obtained by the audio decoder $Dec$ as: $\\hat{m} = Dec(z_q)$. The encoder and decoder are both composed of ResUNet blocks (Kong et al. 2021). The training objective at this stage is defined as follows:\n$L_1 = ||m - \\hat{m}||_2 + \\alpha ||sg(z_m) - z_q||_2 + \\beta_1 ||z_m - sg(z_q)||_2$, (2)\nwhere $sg()$ is a function for stopping gradients, and $\\alpha$, $\\beta_1$ are hyperparameters for the quantization loss and commitment loss weights, respectively.\nBrain-Audio Latent Space Alignment In the second stage, we freeze all the modules pre-trained in the previous"}, {"title": "Experiments", "content": "stage and train a brain encoder to convert raw EEG/MEG signals $\\varepsilon$ into latent features $z_{\\varepsilon}$. The brain encoder utilizes a Conformer-based architecture (Song et al. 2022), which begins with Spatio-Temporal Convolutional Networks to process the input signals into a one-dimensional embedding sequence. The spatial convolutional layer reduces the number of input signal channels to one, while the temporal convolutional layers downsample the time dimension. This sequence is then added to learnable position embeddings and fed into a stack of Transformer encoder blocks. Linear layers and 2D convolutional networks subsequently transform the EEG/MEG features into representations matching the shape of $\\hat{m}$. Similarly, $z_{\\varepsilon}$ is input into the frozen quantizer $Q$ and audio decoder $Dec$ to predict the corresponding Mel spectrogram $\\hat{m}$. Additionally, we align the representations of the Mel spectrogram and raw signals in the latent space. Notably, we employ a unified codebook to leverage pre-warmed discrete acoustic tokens for representing brain activity. The introduction of vector quantization enhances the stability and generalization of the Mel spectrogram reconstruction from brain signals, thereby improving the performance of subsequent text decoding. Formally, the loss for stage 2 is designed as follows:\n$L_2 = ||\\hat{m} - Dec(Q(z_{\\varepsilon}))||_2 + \\gamma ||z_m - z_{\\varepsilon}||_2 + \\beta_2 ||\\varepsilon - sg(Q(z_{\\varepsilon}))||_2$, (3)\nwhere $\\gamma$ and $\\beta_2$ are used to scale the latent alignment loss and the commitment loss, respectively. The intermediate representations of the codebook and speech provide additional supervisory signals to guide the generation of Mel spectrograms. We employ L2 loss rather than CLIP loss (D\u00e9fossez et al. 2023; Yang et al. 2024b) to generate highly restored spectrograms that match Whisper's input.\nWhisper Finetuning After obtaining the predicted Mel spectrogram, it is fed into the pretrained Whisper-base model to decode tokens. To adequately leverage the pre-trained knowledge embedded in Whisper, we utilize AdaLoRA (Zhang et al. 2023b), as employed in NeuSpeech (Yang et al. 2024a) and MAD (Yang et al. 2024b), to fine-tune its encoder while keeping the remaining parameters frozen. The objective in this final stage is to minimize the cross-entropy loss between the predicted sentence and the ground truth $t$.\nDataset\nThe Brennan dataset (Brennan and Hale 2019) comprises 49 human EEG recordings, of which 33 remained after screening. Participants passively listened to a 12.4-minute audiobook recording while their EEG signals were recorded. The GWilliams (Gwilliams et al. 2023) dataset contains raw MEG recordings from 27 English speakers who listened to naturalistic stories for 2 hours. More details of the datasets are provided in the Appendix.\nPreprocess\nBrain signals in both datasets are preprocessed similarly. The EEG signals are notch-filtered at 60 Hz and bandpass-filtered between 0.5 and 99 Hz, and then resampled to 200 Hz. The MEG signals are notched at 50 Hz, filtered with 1~58 Hz and resampled to 100 Hz. Both datasets are normalized to a range of -1 to 1 using robust scalar.\nAll audio is resampled to 16,000 Hz to align with Whisper's pretraining configuration. To assess the robustness of our proposed method, we employ different approaches to generate samples. For the Brennan dataset, we utilize WhisperX (Bain et al. 2023), a time-accurate speech recognition system, to segment the audio into chunks of up to 12 seconds. For the GWilliams dataset, we split the audio according to the original annotations, resulting in segments no longer than 24 seconds. This process generates a series of EEG/MEG-text-speech pairs.\nThe Whisper processor then converts the speech into an 80-channel Mel spectrogram $m$ using 25-ms windows with a stride of 10 ms. To standardize settings and reduce memory usage, the length of the Mel spectrograms in GWilliams is downsampled to half its original value, resulting in $m$ having a consistent shape of (80, 1200). Finally, we obtain 140 and 661 unique sentences from the two datasets, respectively.\nDataset Splitting and Validation Strategies\nIndividual differences and attention levels of subjects can affect EEG signals, making it difficult for models to generalize across subjects and trials. To explore the model's generalization ability, we design different dataset splitting and validation strategies: random shuffling, session-based, sentence-based, and subject-based splittings. More details are provided in the Appendix. Unless otherwise specified, the Brennan and GWilliams datasets are partitioned by subject-based splittings and random shuffling, respectively, in the following presented results.\nImplementation Details\nThe models are trained on Nvidia 3090 GPUs (24GB). Training on the Brennan and GWilliams datasets take approximately 4 and 24 hours, respectively, using a single GPU. The hyperparameters are set as follows: $\\alpha = 0.5$, $\\beta_1 = \\beta_2 = 0.1$, $\\gamma = 1$, $N = 2048$, $d = 256$, and $D = 8$. The audio encoder is configured with a downsampling rate of 4. We use vanilla Transformer encoder with 4 layers and 8 heads. All EEG/MEG samples are zero-padded to 2400 in the time dimension. Input spectrograms are padded uniformly to a length of 3000 with -1 following Whisper's configuration. For the GWilliams dataset, the length of the predicted Mel spectrogram is upsampled by a factor of 2. When generating with Whisper, we set the number of beams to 5 for beam search and apply a repetition penalty of 5.0 with a no-repeat n-gram size of 2. Further details on the training configuration are provided in the Appendix.\nExperimental Results\nOverall Comparison We use BLEU (Papineni et al. 2002), ROUGE-1 (Lin 2004) and Word Error Rate (WER) to evaluate decoding performance. BLEU and ROUGE-1 are used to evaluate the quality of text generation, while WER is used to calculate error rate based on edit distance. As shown"}, {"title": "Ablation Study and Analysis", "content": "rate autoencoding when splitting Brennan by subject. Additionally, all metrics improve when splitting by sentence. This suggests that incorporating Mel spectrograms from other datasets during autoencoding enhances the model's ability to extract richer discrete speech representations, thereby enhancing its generalizability.\nPrediction of Mel Spectrogram Examples of Mel spectrograms predicted by BrainECHO are presented in the Appendix. Since reconstructing a 12-second Mel spectrogram from brain signals in a single pass can significantly impact training efficiency and memory usage, we opt to divide the raw signals into n-second segments to balance decoding accuracy and resource consumption. The corresponding Mel spectrogram for each segment is predicted and then concatenated to form a complete one. We experiment with window"}, {"title": "Conclusion", "content": "Autoencoding We compare the decoding performance when the autoencoding task (stage 1) is applied separately to the Mel spectrograms from individual datasets versus both datasets combined. The results, presented in Table 3, indicate that joint autoencoding results in either stable or slightly improved metrics (except for BLEU-3) compared to separate autoencoding.\nThree Training Stages To verify the effectiveness of our proposed three-stage training, we incrementally remove each stage and observe the corresponding changes in performance. As presented in Table 6, when the autoencoding stage is removed-where the quantizer and audio decoder are randomly initialized\u2013BLEU-4 drops to 85.74 (-3.17%). Further removal of the brain-audio alignment stage leads to an abnormal increase in BLEU, highlighting the challenge of directly constructing a representation space from the brain signals to the Mel spectrogram. However, by pre-warming a discrete representation space, the reconstruction quality and stability are enhanced. The most significant decline in performance occurs when the fine-tuning phase is omitted. Notably, even without fine-tuning, BrainECHO achieves impressive performance based solely on the predicted Mel spectrogram, suggesting that excessive fine-tuning of Whisper is unnecessary, as it could result in overfitting.\nThis paper introduces a novel three-stage brain-to-text framework, BrainECHO, that addresses the shortcomings of prior methods. These methods relied on teacher forcing and failed to compare model performance against pure noise inputs. BrainECHO bridges the latent spaces of text and corresponding aurally evoked brain signals through vector-quantized spectrogram reconstruction and fine-tuned use of the Whisper model. It achieves state-of-the-art (SOTA) performance on public EEG and MEG datasets across various experimental settings. By extracting deep semantic information from brain signals, BrainECHO provides valuable insights for future research in the brain-to-text decoding paradigm in the BCI field."}]}