{"title": "A Bayesian framework for active object recognition, pose estimation and shape transfer learning through touch", "authors": ["Haodong Zheng", "Andrei Jalba", "Raymond H. Cuijpers", "Wijnand IJsselsteijn", "Sanne Schoenmakers"], "abstract": "As humans can explore and understand the world through the sense of touch, tactile sensing is also an important aspect of robotic perception. In unstructured environments, robots can encounter both known and novel objects, this calls for a method to address both known and novel objects. In this study, we combine a particle filter (PF) and Gaussian process implicit surface (GPIS) in a unified Bayesian framework. The framework can differentiate between known and novel objects, perform object recognition, estimate pose for known objects, and reconstruct shapes for unknown objects, in an active learning fashion. By grounding the selection of the GPIS prior with the maximum-likelihood-estimation (MLE) shape from the PF, the knowledge about known objects' shapes can be transferred to learn novel shapes. An exploration procedure with global shape estimation is proposed to guide active data acquisition and conclude the exploration when sufficient information is obtained. The performance of the proposed Bayesian framework is evaluated through simulations on known and novel objects, initialized with random poses and is compared with a rapidly explore random tree (RRT).The results show that the proposed exploration procedure, utilizing global shape estimation, achieves faster exploration than the RRT-based local exploration pro- cedure. Overall, results indicate that the proposed framework is effective and efficient in object recognition, pose estimation and shape reconstruction. Moreover, we show that a learned shape can be included as a new prior and used effectively for future object recognition and pose estimation of novel objects.", "sections": [{"title": "I. INTRODUCTION", "content": "Tactile sensing is an important aspect of robot sensing, and it is useful both to complement vision, but also as a standalone sensing modality. Visual information is not always available and reliable, especially under severe occlusions and poor light- ing conditions. Therefore, methods that can work regardless of the visual information availability are desired, to have robots function well in natural or unstructured environments. In this study, we focus on localizing and recognizing objects based on the shape of the object, solely through touch. Knowing the object's shape and pose is important for locating and manipulating objects, and therefore, it is useful for a variety of applications.\nIn unstructured environments, both known and novel ob- jects can be encountered, nonetheless, how to address both known and novel objects under an unified framework remains an open question. Identifying known and unknown objects, and addressing them accordingly is therefore of paramount importance. For known objects, the object class and pose should be estimated, whereas, for a novel object, its shape should be explored and learned. Previous work tends to separate the processes of object recognition, pose estimation, and shape reconstruction. In this study, we combine these goals. Moreover, we explore the possibility of transferring knowledge from known object shapes to learn novel shapes by addressing these novel objects within the same framework that has obtained prior knowledge about objects.\nCompared to robotic vision, robotic tactile sensors provide sparse data that are often insufficient for resolving uncertainty in the object class/shape and pose estimation. Especially when the tactile sensors are small compared to the size of the object in contact, only local information is available. Therefore, tactile exploration needs to be performed in an active manner to reduce the uncertainty in object class, pose and shape.\nWith the recent advancement of tactile sensing technology, vision-based tactile sensors [1], [2] have gained popularity, as they provide accurate and detailed information on the local geometry of the contact area, although they are still less accessible due to their relatively high cost and complicated manufacturing procedure. In this study, we assume the input to the system to be a contact mask with the sensor location and a surface normal vector when in contact. The sensor's location and orientation can be extracted from robot's proprioception through forward kinematics, while the normal vector can be estimated based on the sensor's orientation.\nA Bayesian-based approach is a good choice for the problem introduced in this study, because it allows for straightforward integration of prior knowledge with sensor data, and it has an explicit uncertainty treatment. Therefore, we use a Bayesian framework to jointly estimate the object class and object pose with active exploration from first contact to conclusion. Moreover, the proposed framework can identify and learn the shape of novel objects through Bayesian evidence tracking and transfer learning.\nAn outline of the proposed framework is shown in Fig. 1. The particle filter (PF) is in charge of online updating the joint distribution of object class and pose, while more data are collected. Taking inspiration from the Manifold particle filter [3] and point-pair features proposed in [4], we modify a tradi- tional particle filter [5] to progressively sample new particles based on newly observed data points. Therefore, our particle filter can deal with arbitrary initial uncertainty. In addition, this approach keeps the computational cost tractable at each time step and can achieve low pose estimation error as more data are collected despite using coarse discretized approximation in the PF. The Bayesian evidence of the maximum likelihood estimation (MLE) obtained from the PF is used to determine if the object is a known or novel object. Once a novel object is detected, the MLE from the PF is used as a prior for Gaussian process implicit surface (GPIS) reconstruction. The target point selection and contact enforcement modules take care of the active data acquisition using information from global shape estimation from either the PF or GPIS. A termination criterion is used, based on the Directed Hausdorff Distance(DHD), to determine when the object has been sufficiently explored and a conclusion can be drawn.\nThe main contributions of this paper are as follows:\n\u2022\n\u2022\nA Bayesian framework is proposed to perform simulta- neous object recognition and pose estimation for known objects. In case of novel objects shape transfer learning is applied in an active setting to learn the new shapes.\nThe proposed Bayesian framework can capitalize on the knowledge of a learned novel object shape for faster object recognition and pose estimation when the object is later encountered."}, {"title": "II. RELATED WORK", "content": "Our work is closely related to Bayesian-based object recog- nition, pose estimation, shape reconstruction, and active tactile exploration.\nPrevious work approached the object recognition problem based on material properties. Kaboli et al. and Xu et al. [6]\u2013[8] used properties such as stiffness, texture, and thermal conductivity to distinguish between different objects. Our method instead focuses on object recognition with geometric information, which can be obtained through most tactile sensors.\nSome researchers used methods more in line with our work with touch-based pose estimation of objects. For instance, particle filters (PF) can be used to approximate arbitrary distributions, which makes them suited for capturing the uncer- tainty of the object pose presented during tactile exploration. However, using a PF comes with a computational challenge as a substantial amount of particles is required to cover the state space. Koval et al. [3] proposed the manifold particle filter by adaptively sampling particles that reside on the contact manifold to increase sampling efficiency. Petrovskaya et al. [9] proposed the Scaling Series algorithm, combining the Bayesian Monte Carlo technique coupled with annealing, to refine the posterior distribution of the 6-degree-of-freedom (DOF) pose through multiple stages with a small number of particles at each stage. Vezzani et al. [10] proposed to use a memory unscented Kalman filter (MUKF) to cover the 6-DOF pose space with a small number of particles where each particle is treated as a Gaussian distribution instead of a discrete sample. Although the methods are quite efficient, the work by [3], [9], [10] was confined to pose estimation. As they limited their scope to the pose estimation of a known object, multi-class classification was not addressed in their work. Vezzani et al. [11] extended their own work on pose estimation [10] to also address object recognition by applying the localization scheme to multiple object classes and picking the object class with the smallest error in localization.\nDiffering from the aforementioned methods, our proposed framework estimates the joint distribution of object class and object pose by progressively sampling new particles based on newly observed point pairs. The motivation behind this design choice is to avoid tracking all possibilities from the beginning of the exploration. Instead of using all data, the particle filter samples new particles that fit a new point pair generated from the new contact point and a previous observed contact point. Due to the discretized nature of the PF, there is no guarantee that a small number of initial particles can cover the region near the ground truth. By sampling new particles at each time step, the PF can consider more particles throughout active tactile exploration while keeping the number of particles small at each time step through resampling. More importantly, we combine the PF and GPIS to distinguish between known and novel objects and transfer knowledge on known object shapes to learn novel object shapes. Another distinction is that our proposed framework uses an active exploration procedure utilizing the global estimated shape at each time step for active data acquisition."}, {"title": "B. Bayesian Touch-Only Shape Reconstruction", "content": "Gaussian processes (GPs) are widely used for the task of shape reconstruction as they provide uncertainty measurement of the reconstructed surface, which enables Bayesian optimization for selecting actions during exploration. Sun et al. and Jamali et al. [12], [13] used the Gaussian process explicit surface for shape reconstruction, but the choice of representation cannot represent arbitrary shapes. Gaussian process implicit surface (GPIS) [14] was introduced to address this limitation. Several researchers, e.g. [15]\u2013[20], adopted GPIS for active shape reconstruction based on touch. These methods used the same prior for all test objects, which did not explore the possibility of shape transfer learning. Martens et al. [21] proposed to fit a parametric ellipsoid according to the observed point cloud as prior for GPIS reconstruction, whereas our approach updates the maximum likelihood prior continuously to take on the shape of objects in the prior at each time step based on new observations."}, {"title": "C. Active Tactile Sensing", "content": "Due to the local nature of tactile data, robots often need to acquire data actively to obtain sufficient information for object recognition and shape reconstruction. Therefore, good strate- gies are necessary to perform active exploration efficiently.\nActive learning procedures have been proposed for efficient object recognition by selecting the action with the least ex- pected entropy at the next time step [22]. Other researchers suggested selecting actions that minimize their proposed ex- pected confusion metrics [6]\u2013[8]. In this study, we instead aim to increase data coverage on the estimated object surface and get global context, which leads to the proposed directed Haus- dorff distance (DHD) based target point selection procedure.\nSeveral studies used GP-based approaches. Jamali et al. [13] and Yi et al. [23] proposed active learning procedures that select the next point of interest to be the point with the most uncertainty. Yang et al. [24] proposed a greedy target point selection procedure based on mutual information of the GPIS. Using graph-based planning, Matsubara et al. [25] took into account both surface uncertainty and travel cost to improve the efficiency of tactile exploration. Instead of considering the uncertainty of each point, Driess et al. [18] proposed to generate smooth trajectories with the most uncertainty on the object's surface for more human-like tactile exploration. Their subsequent study [19] introduced a new loss function based on the differential entropy of the GP to enable multi-finger exploration simultaneously. One problem with GP-based continuous path exploration is that it requires many points to get the global context of the object. In our study, motivated by obtaining global context as soon as possible, the most uncertain point on the GPIS will be selected as the target point when a novel object is detected.\nFor both known and novel objects, a contact enforcement procedure is performed to attempt to establish contact from the point of interest on the estimated shape. It will resort to local exploration if it fails to establish contact based on the estimated shape."}, {"title": "D. Deep-Learning-Based Approaches for shape reconstruc- tion", "content": "Deep learning methods were developed to address active shape reconstructions. Comi et al. [26] learned a compact vector representation from large object datasets, and created a DeepSDF network conditioned on the learned representation. Based on contact patches from a vision-based tactile sensor, they retrieved the reconstructed shape by finding the shape embedding that yields the most consistent results with the contact. Their work does not consider an active learning setup.\nWang et al. [27] and Rustler et al. [28] developed methods to perform active shape completion based on touch data, but their methods relied on information from RGB/RGB-D camera for initialization. Smith et al. [29], [30] proposed graph-neural-network-based methods (GNNs) to reconstruct 3D shapes using vision and touch. Though aiming at visuo- tactile information integration, their methods also work un- der touch-only settings. In addition, Smith et al. [30] used reinforcement learning to train an active exploration policy for efficient shape reconstruction. However, in contrast to our work, they used a fixed spherical prior for shape reconstruc- tion. The aforementioned methods did not provide the joint distribution of the object class and object pose nor addressed the uncertainty throughout tactile exploration explicitly. In addition, they require a significant amount of training data."}, {"title": "III. METHODS", "content": "In this section, we start by deriving the Bayesian formu- lation of the object recognition and pose estimation problem, then expand the formulation to include shape reconstruction for novel objects."}, {"title": "A. Problem Formulation", "content": "To estimate the object class and object pose through tactile contact points and surface normal vectors at these points, we consider the joint distribution of the object class and object pose, given the tactile observations., since both object class and pose determine whether, or not, contact with the object can be observed at a certain location. Often an object is held stationary by one hand and explored by the other hand, therefore, for simplicity, we assume that the object's pose is stationary during tactile exploration of the robotic arm.\nThe object class $C$ and the object pose $p \\leftarrow [p_x, p_y, p_z, \\alpha, \\beta, \\gamma]$ are concatenated into a latent variable vector $z$, where $[p_x, p_y, p_z]$ represents the origin coordinates of the object frame in the world frame and $[\\u03b1, \\beta, \\gamma]$ denote the relative yaw, pitch, and roll angles of the object frame in the world frame. Each observed data point coordinate $[u_x, u_y, u_z]^T$ is denoted as $x$. Likewise, the observed signed distance value $d_s$ and surface normal vector $[n_x, n_y, n_z]$ at each data point $x$, is concatenated into a vector $d$, i.e.,\n$z \\leftarrow [c, p]$\n$d \\leftarrow [d_s, n]$\n$d_s = 0$ on surface\n$d_s > 0$ outside object\n$d_s < 0$ inside object\nGiven an object class $c$ and object pose $p$, the predicted Euclidean signed distance value $d_s$ from a point $x$ to the object surface is given by its signed distance function\n$d_s := f(x, z)$,\nwhere $f$ denotes the Euclidean signed distance function. The predicted surface normal $\\hat{n}$ at a contact point $x$ could be calculated by differentiation, i.e.,\n$\\hat{n} = \\frac{\\nabla_x f(x, z)}{||\\nabla_x f(x, z)||}.$\nThe predicted value of $d$ is denoted as $\\hat{d}$, where\n$\\hat{d} \\leftarrow [\\hat{d_s}, \\hat{n}].$\nB. Bayesian Inference for Simultaneous Object Recognition and Pose Estimation and Shape Reconstruction through Touch\nLet $D$ denote the set of all observed tactile signals $d$ and $X$ be the set of all locations $x$ where $D$ are observed. Given tactile sensory data $D$ at locations $X$, the aim is to estimate the latent variables $z$. Based on Bayes' theorem, the posterior distribution on $z$ can be calculated as follows,\n$p(z|D, X) = \\frac{p(D|z, X)p(z)}{\\sum_z p(D, z|X)} = \\frac{p(D|z, X)p(z)}{\\sum_z p(D|z, X)p(z)}$\nwhere $p(z)$ is the prior joint distribution of object class and object pose, $p(D|z, X)$ denotes the measurement likelihood function, which represents the likelihood of observing tactile sensory data $D$ at locations $X$ assuming the object class is $c$ and the pose is $p$; $p(d|z,x)$ is the observation likelihood function for a single data point. In this paper, $p(z)$ is chosen to be a uninformative uniform distribution $U$. For simplicity, in our formulation, each observation $d$ at location $x$ is assumed to be independent from other observations, given the latent variables $z$ is known. This simplifies equation (7) into,\n$p(D|z, X) = \\prod_{i=1}^{O} p(d_i|z, x_i)$,\nwhere the subscript $i$ denotes the index of each observed data point, and $O$ denotes the total number of observed data points.\nAs the proposed framework actively acquires data, at each time step $t$, new tactile data $D_t$ are observed at locations $X_t$. Let $T$ denote the total time steps passed, $D_{1:T}$ represent all observed tactile data from time step 1 to $T$ and $X_{1:T}$ denote the locations where $D_{1:T}$ are observed; then equation (7) can be rewritten as\n$p(z|D_{1:T}, X_{1:T}) = \\frac{p(z, D_{1:T} | X_{1:T})}{\\sum_z p(z, D_{1:T}|X_{1:T})}$\nAs we assume independence between different observations given $z$, the joint distribution can be calculated as\n$p(z, D_{1:T} | X_{1:T}) = p(z) p(D_{1:T-1}|z, X_{1:T-1})p(D_T | z, X_T)$\n$= p(z, D_{1:T-1}|X_{1:T-1}) p(D_T | z, X_T).$\nOne can obtain the MLE $z^*$ of the object pose and object class given observed data as follows,\n$z^* := argmax_z (p(z|D_{1:T}, X_{1:T}))$\nTo decide if the object is known or unknown, one can track the consistency between the $z^*$ and the observed data $D_{1:T}$ by calculating $p(D_{1:T}|z^*, X_{1:T})$ using equation (8). In the following sections, $p(D_{1:T}|z^*, X_{1:T})$ is referred to as MLE model evidence. A low value for $p(D_{1:T}|z^*, X_{1:T})$, when the object surface is fully explored, would imply the test object is unknown."}, {"title": "C. Measurement Likelihood Function", "content": "The measurement likelihood function is defined based on the difference between the predictive signed distance values, normal vectors and the observed values. The observed signed distance value $d_s$ at a contact location $x$ is always regarded as zero.\nIf we assume that each tactile observation $d$ at location $x$ follows a Gaussian distribution,\n$d \\sim N(\\hat{d}, \\Sigma)$\n$\\Sigma = diag (\\frac{1}{2\\sigma_1^2}, \\frac{1}{2\\sigma_2^2}, \\frac{1}{2\\sigma_3^2}, \\frac{1}{2\\sigma_n^2})$\nIn other words, the measurement likelihood function satisfies:\n$p(d|z, x) = \\frac{1}{Z} exp(-(d - \\hat{d})^T \\Sigma^{-1} (d - \\hat{d}))$\n$Z = (2\\pi)^2 \\sqrt{det(\\Sigma)}$\n$-ln p(d|z, x) \\propto (d - \\hat{d})^T \\Sigma^{-1} (d - \\hat{d}),$"}, {"title": "D. Negative Information Update", "content": "Equation (14) assumes the observed data takes on precise values. However, when the tactile sensor is not in contact with the object, it implies that, at the sensor location $x$, $d_s \\in (0, +\\infty)$, and though the normal vector can still be predicted at x, it cannot be observed from tactile sensors as no contact force would be sensed in a real setup.\n$p(d|z, x) = p(d_s > 0|z, x) = \\int_{0}^{\\infty} p(d_s|z, x) dd_s.$\nFor the Gaussian likelihood function, the likelihood reads\n$p(d_s > 0 | z, x) = \\frac{1}{2} (1 - erf(\\frac{f(x,z)}{\\sqrt{2\\sigma_a}}))$,\nwhere $erf$ denotes the Gauss error function. By substituting equation (17) and equation (18) into equation (8) the frame- work can update it's belief when no contact is observed at a location $x$."}, {"title": "E. Particle Filter (PF) for Object Recognition and Pose Esti- mation", "content": "A particle filter (PF) [5] is adopted to estimate the joint distribution of the object's class and pose online based on equations (11)-(16). The core idea behind particle filters is to use weighted discrete samples (particles) to approximate the posterior distribution of interest based on importance sampling, i.e.,\n$p(z|D_{1:t}, X_{1:t}) = \\sum_{j=1}^{N} w_{j,t} \\delta (z - z_{j,t})$,\nwhere the subscript $j,t$ denotes the index of a particle in the PF at time step $t$, and $\\delta$ denotes the Dirac delta function, $N$ denotes total number of particles. The unnormalized weight of a particle $w_{j,t}$ can be calculated as\n$w_{j,t} \\propto p(z_{j,t} | D_{1:t}, X_{1:t})$\n$\\propto p(z_{j,t}, D_t | D_{1:t-1}, X_{1:t})$\n$= p(z_{j,t} | D_{1:t-1}, X_{1:t-1}) p(D_t | z_{j,t}, X_t)$\n$= \\sum_{z_{j,t-1}} (w_{j,t-1} p(z_{j,t} | z_{j,t-1}, D_{1:t-1}, X_{1:t-1})) \\times p(D_t | z_{j,t}, X_t).$\nSince we assume the object stays static during the exploration, thus\n$p(z_{j,t} | z_{j,t-1}, D_{1:t-1}, X_{1:t-1}) = \\delta(z_{j,t} - z_{j,t-1})$,\n$z_{j,t} \\leftrightarrow z_{j,t-1}.$\nNoteworthy, $p(z_{j,t} | z_{j,t-1}, D_{1:t-1}, X_{1:t-1})$ can be replaced by a motion estimator if a motion model is available. With equation (21) and equation (22), it follows\n$w_{j,t} = \\sum_{z_{j,t-1}} (w_{j,t-1} \\delta(z_{j,t} - z_{j,t-1})) p(D_t | z_{j,t}, X_t)$\n$= w_{j,t-1} p(D_t | z_{j,t}, X_t)$\n$\\overline{w_{j,t}} = \\frac{w_{j,t}}{\\sum_{j=1}^{N} w_{j,t}}$\nwhere $\\overline{w_{j,t}}$ is the normalized weight of particle $z_{j,t}$. Note that the recursive formula of the particle weight update is consistent with equation(10).\nAs the particle filter needs to estimate the joint distribution of the object's class and its 6D pose, the number of particles required to cover the space sufficiently can be calculated as $n \\times m^6$, where $n$ denotes the number of known object classes, and $m$ denotes the resolution of the discretization at each continuous dimension.\nTo enhance the sample efficiency of the particle filter and have better coverage on the high-density region of the posterior distribution, we propose to sample new particles based on newly observed data at each time step and mix them with existing particles. The rotation and translation invariant point-pair feature proposed in [4] is adopted for this purpose. For every two data points, the point-pair feature can be calculated. For more details about the alignment process, please refer to [4]."}, {"title": "F. Discriminating Known and Unknown Objects", "content": "To discriminate between known and unknown objects, a threshold needs to be defined for the MLE model evidence $p(D|z^*, X)$. In this study, two types of data points are present, namely, contact points and non-contact points. The criterion for an object to be classified as a known object is defined as follows\n$\\frac{1}{N} p(D|z^*, X) \\ge (0.90)^{n_{pos}} * (0.50)^{n_{neg}}$,\nwhere $n_{pos}$ denotes the number of observed contact points, $n_{neg}$ denotes the number of observed non-contact points."}, {"title": "G. Gaussian Process Implicit Surface for Shape Reconstruc- tion", "content": "When a novel object is detected, the Gaussian Process Implicit Surface (GPIS) [14] is used to reconstruct the shape given a prior function $\\mu$ and observed data $D$. The aim is to learn a signed distance function that fits the data $D$ while taking into account the prior $\\mu$. Similar to $d$ in equation (6), $\\mu: R^3 \\rightarrow R^4$ maps a point $x$ in the Euclidean space to a predictive signed distance value and a gradient vector.\nUnder the GP assumption, given an unexplored location $x^*$, observed contact points $X$ and observed tactile data $D$, the predictive observation $d^*$ at $x^*$ satisfies,\n$d^*(x^*) \\sim N(\\mu^*(x^*), \\Sigma^*(x^*)).$\nThe mean $\\mu^*(x^*)$ and the covariance matrix $\\Sigma^*(x^*)$ are given by\n$\\mu^*(x^*) = \\mu(x^*) + k_*^T (K + \\sigma^2 I)^{-1} (D - \\mu(X)),$\n$\\Sigma^*(x^*) = k_{**} - k_*^T(K + \\sigma^2 I)^{-1}k_*$\nwhere $k_{**}$, $k_*$ and $K$ is the covariance matrix between $(d^*, d^*)$, $(d^*, D)$ and $(D, D)$ respectively; $\\sigma$ denotes the sensory noise level; $D$ represents a flattened vector that con- tains observed data $D$ observed at $X$, while $\\mu(X)$ represents a flattened vector that contains the prior predictive observed values at $X$. The covariance matrix between $d$ at $x$ and $d'$ at $x'$ can be calculated as\n$cov(d, d') = \\begin{bmatrix} k_f(x, x') & \\nabla_x k_f(x, x') \\\\ \\nabla_{x'} k_f(x, x') & \\nabla_x \\nabla_{x'} k_f(x, x') \\end{bmatrix}.$\nwhere $k_f$ is the kernel function of choice. Following (32), $k_{**}$, $k_*$ and $K$ can be calculated.\nIn this study, the thin-plate kernel [14] [21] is used. The thin-plate kernel function is defined as follows,\n$k_f(x, x') = k_{TP}(x, x') = a(2d^3 - 3Rd^2 + R^3)$\n$d = ||x - x'||_2$\nThe first and second derivatives of $k_{TP}(x, x')$ can be calcu- lated as\n$\\frac{\\partial k_{TP}(x, x')}{\\partial x_i} = -6a (x_i - x'_i)(d - R),$\n$\\frac{\\partial^2 k_{TP}(x, x')}{\\partial x_i \\partial x_j} = -6a (-\\frac{(x_i - x'_i)(x_j - x'_j)}{d} + \\delta_{ij} (d - R))$\nwhere $\\delta_{ij}$ denotes the Dirac delta function $(i - j)$. Differing from the definition in [14] [21], a scaling coefficient $a$ is introduced to allow the GP to adapt to different novel objects. The higher $a$ is, the larger the assumed process noise is, which corresponds to assuming a larger difference between the prior and the actual object. The kernel parameter $a$ can be updated online using a gradient-based optimizer to maximize the data likelihood of the GP [32]. Last but not least, only contact points are used to update the GP since the signed distance values at contact points are precise, while it is not the case for non-contact points."}, {"title": "H. Combining PF and GPIS", "content": "For GPIS reconstruction, given a fixed prior function $\\mu$, the negative log-likelihood of observing data $D$ at a set of points $X$ satisfies the following relation\n$-ln p(D|X) \\propto (\\mu(X) - D)^T K^{-1} (\\mu(X) - D)$"}, {"title": "I. Active Data Acquisition", "content": "As tactile data points at each time step only cover a fraction of the object surface, active data acquisition is required to address the uncertainty and ambiguity due to incomplete information. The process is divided into two steps: target point selection and contact enforcement. The idea is to first deter- mine a target point, and start exploration from the candidate target point until a contact point is found.\nOne way to select the target point is to calculate the posterior variance for each vertex on the reconstructed surface based on equation (31), then the point with the highest posterior variance will be selected as the target point $X_{t+1}$ for the next time step, i.e.,\n$var(x^*) = \\Sigma_{0,0}$\n$X_{t+1} = argmax_{x^* \\in S^*} {var(x^*)}$\n$S^* \\subset \\{x^* \\in R^3 | \\mu^*(x^*) = 0\\},$\nwith $S^*$ a finite subset of the zero-level set of the posterior GPIS, which can be obtained through the marching cubes algorithm [33].\nHowever, this exploration procedure requires the framework to update the GPIS every time step, which is computationally expensive. To alleviate this issue, the GPIS will not be updated and the framework will use the MLE shape from PF as estimated shape when the object is considered to be a known object. An exploration based on the directed Hausdorff distance (DHD) is used to guide the exploration as variance from the GPIS is not available. The point of interest for the next time step $X_{t+1}$ can be selected on the MLE surface by finding the point that yields the largest DHD, i.e.,\n$X_{t+1} = argmax_{X \\in M} min_{X \\in X} ||x_m - x_c||_2,$\nwhere $M$ represents the set of all vertices of the MLE shape, and $X$ denotes the set of observed contact points. The point of interest obtained through equation (42) is the point on the MLE shape with the largest distance to its nearest data point. And DHD is defined as follows,\n$d_H(A, B) = max_{a \\in A} min_{b \\in B}||a - b||_2$\nwhere A and B are two point sets.\nBoth GPIS-based and DHD-based target point selection procedures share the same idea, prioritizing target points that are far away from existing data points in the hope of captur- ing the global context efficiently. In the following sections, the aforementioned target point selection procedure will be referred to as the GPIS-DHD exploration procedure.\nWith equations (40) and (42), a candidate target point is obtained at each time step, but there is no guarantee that it would lead to contact. On the contrary, since the target point is near the furthest point on the estimated surface from the existing data point, it often fails to establish contact at the proposed target location. A simple solution to guarantee contact at each time step is to perform contour following, namely, the tactile sensor will move towards the target point locally on the tangent plane at each time step. However, this approach might take a long time to reach the target location and fail to capture the global context efficiently.\nAs the selected target point is either on the GPIS or the MLE shape, confirming the occupancy near the target point can provide information that could greatly impact the belief of the framework. Negative information (non-contact points) around the target point implies the current estimation is likely to be wrong, whereas positive information (contact) provides evidence of the current estimation."}, {"title": "J. Termination criterion", "content": "In practice, the termination of the algorithm can be based on a time limit, e.g. the limit of registered data points. In this study, we define a termination criterion based on the DHD to ensure the framework terminates if, and only if, enough information is obtained on the object's surface. The incentive behind this choice of criterion is to make the framework decide when to terminate based on the data point's coverage on the object's surface. For known objects, the termination criterion can be written as\n$d_H(M, X_c) < \\epsilon$\nThe algorithm terminates if $d_H(M, X_c)$ is lower than a small value $\\epsilon$, where $M$ represents the set of all vertices of the MLE shape, and $X_c$ denotes the set of observed contact points. Intuitively, if any point on the estimated surface is at most $\\epsilon$ away from its closest existing contact point, the program terminates. In other words, to terminate the exploration, the contact points have to cover the estimated object surface with a certain density related to $\\epsilon$. The threshold $\\epsilon$ can be interpreted as a level of detail parameter: lower $\\epsilon$ results in more data being collected, albeit with longer exploration time. Similarly, for novel objects, the termination criterion is\n$d_H(S^*, X_c) < \\epsilon,$"}, {"title": "IV. EXPERIMENTS", "content": "To show that the framework can address both known and un- known objects, we selected ten 3D models from the Princeton Shape Benchmark [34", "35": "as known objects, and ten comparable, but distinct, other objects from the same databases as novel objects. They are all scaled to fit in a 6 \u00d7 6 \u00d7 6 bounding box. This should make it less likely for the framework to identify the correct object class merely by the length of the object. For this reason, the distance unit in this study does not correspond to real world measurements. An overview of the objects used in the experiments are shown in Fig. 4 and 5. Both known and novel objects are used as test objects in the experiment. For each trial in the experiment"}]}