{"title": "SODor: Long-Term EEG Partitioning for Seizure Onset Detection", "authors": ["Zheng Chen", "Yasuko Matsubara", "Yasushi Sakurai", "Jimeng Sun"], "abstract": "Deep learning models have recently shown great success in classifying epileptic patients using EEG recordings. Unfortunately, classification-based methods lack a sound mechanism to detect the onset of seizure events. In this work, we propose a two-stage framework, SODor, that explicitly models seizure onset through a novel task formulation of subsequence clustering. Given an EEG sequence, the framework first learns a set of second-level embeddings with label supervision. It then employs model-based clustering to explicitly capture long-term temporal dependencies in EEG sequences and identify meaningful subsequences. Epochs within a subsequence share a common cluster assignment (normal or seizure), with cluster or state transitions representing successful onset detections. Extensive experiments on three datasets demonstrate that our method can correct misclassifications, achieving 5%-11% classification improvements over other baselines and accurately detecting seizure onsets.", "sections": [{"title": "Introduction", "content": "Epilepsy affects 60 million of the population worldwide, and approximately 40% of patients have drug-resistant epilepsy with recurrent seizures that cannot be controlled by available medications (WHO 2024). This problem leads to an increased risk of sudden death. Deep learning models have demonstrated impressive success in automating seizure detection using electroencephalogram (EEG) data (Yang et al. 2023; Ho and Armanfard 2023; Chen et al. 2023). Successful methods typically form a classification task. They divide EEG recordings into a sequence of second-level epochs and aim to classify them accurately. While state-of-the-art (SOTA) performance has been demonstrated, these methods cannot provide inherent information of seizure detection research, i.e., the seizure onset (SO). Clinically, many epileptic patients benefit from accurate SO detection as it helps localize and surgically remove the onset zone in the brain, which exhibits the earliest electrophysiological changes during a seizure event. Also, successful detection provides optimal timing to adjust abnormal electrical activities in the brain by neuromodulatory devices (Conrad et al. 2019). Despite great importance, the objective of classification-based methods is to determine whether a seizure exists within an EEG, without explicitly modeling and showing SO position over a long sequence. Several abrupt misclassifications randomly appear within a state-consistent sequence, as shown in Figure 1. These misclassifications inevitably increase false alarms of SO detection and lead to unexplainable outcomes.\nA few studies propose a two-stage method (Burrello et al. 2020; Li et al. 2021b; Boo et al. 2021; Batista et al. 2024). They employ post-processing to smooth abrupt changes and reduce the false detection rate. Samples within a sliding window are re-assigned a consistent label based on a majority vote. However, several limitations remain unsolved.\n\u2022 Still lacking explicit SO detection modeling. The objective of post-processing is to smooth misclassifications. No existing works effectively formulate SO detection as a learning task and directly output the SO information. They are required to manually set parameters, e.g., voting threshold and window size, which hinder scalability.\n\u2022 Insufficient feature utilization. Existing works estimate a simple statistical observation of label assignments. They ignore the underlying features that characterize an EEG sample and its relation to a seizure event. Such empirical observations fail to offer any explanation for detection.\n\u2022 Lacking long-sequence dependencies modeling. Either classification-based methods or voting operations treat all second-level epochs equally and uniformly. They fail to take into context information or long-term dependencies within a sequence. This often yields suboptimal results.\nTo tackle these, we propose a new seizure onset detector, SODor, with the following contributions:\n\u2022 Subsequence clustering formulation for explicit SO detection. We propose a deep clustering method to explicitly model the SO detection task, which is formulated as a subsequence clustering problem to find and segment the subsequences that characterize a consistent state (normal vs. seizure) automatically. The segment points show state transitions that are identified as SO timestamps.\n\u2022 Channel logits representations. We propose a channel correlation representation and successfully formulate it as the clustering objective. This approach benefits from the classification model by leveraging logits of label assignments from EEG channels, rather than smoothing label assignments. By learning the correlations between these logits, the method provides insights into how multi-channel interactions relate to seizures.\n\u2022 Temporal consistency modeling. We propose modeling time-invariant interactions within epochs and long-sequence consistency through a clustering constraint. This approach helps mitigate abrupt changes and encourages neighboring epochs to be assigned to the same cluster.\nTo our knowledge, we are the first to formulate SO detection as a clustering task and explicitly output SO information."}, {"title": "Related Works", "content": "Existing SO detection methods can be categorized into end-to-end and two-stage approaches. End-to-end methods frame SO detection as a classification task, labeling second-level epochs as either normal or seizure. Various deep learning models such as CNNs (Eldele et al. 2021; Feizbakhsh and Omranpour 2023), Transformers (Chen et al. 2022a,b; Yang et al. 2023; Kotoge et al. 2024), and Graph models (Tang et al. 2022; Ho and Armanfard 2023; Cai et al. 2023) are employed to automate feature extraction and classification. While an accurate model may provide some SO information, these methods do not explicitly model and detect SOs. Some studies set up onset EEG fragments as a separate class for three-way classification (Rasheed et al. 2021; Dissanayake and Fookes 2021; Chen et al. 2023). Since they focus only on classification accuracy, end-to-end methods often result in abrupt misclassifications at random timestamps, increasing false SO detections. Two-stage methods involve post-processing the outputs of classification models. They re-assign consistent state labels to sequences of epochs within a window based on majority voting (Burrello et al. 2020; Li et al. 2021b; Batista et al. 2024) or thresholding (Khan et al. 2018; Boo et al. 2021; Feizbakhsh and Omranpour 2023). However, these works lack explicit learning for SO detection and oversimplify label smoothing, resulting in suboptimal performance that often requires data-specific manual adjustments. Different from existing methods, we propose a novel formulation of SO detection as a time series subsequence clustering task."}, {"title": "Time series subsequence clustering", "content": "Subsequence clustering is an important task in time series data mining. The objective is to distinct states in sequences of time series, without relying on known labels and segments (Matsubara et al. 2014; Obata et al. 2024). For example, in a dance routine, a multivariate time series captures transitions between motion states such as \"walk,\" \"run,\" \"jump,\" and \"kick.\" Subsequence clustering segments these time observations into concise segments and assigns each segment a motion label. Model-based methods are commonly used, where each cluster is represented as a model, and sequences are fitted to these models (Hallac et al. 2017; Kawabata et al. 2021; Nakamura et al. 2023; Koki et al. 2023). Recent researchers propose some deep clustering involving two stages (Nagano et al. 2019; Wang et al. 2023; Lai et al. 2024). The first stage learns a latent embedding by deep learning models. Then, they view a sequence of embeddings with multiple dimensions as a multivariate time series, and a clustering stage identifies the state of embeddings. SODor follows this line but differs from these methods, which use unexplainable embeddings for clustering. Inspired by (Conrad et al. 2019; Li et al. 2021a), we propose an explainable method that treats the learned channel-independent logits as multivariate time series and models their state-specific structures."}, {"title": "Problem Formulation", "content": "This section presents the problem formulation for SO detection using EEG data. We first introduce preliminaries.\nDefinition 1 (Multi-Channel EEG recordings). EEG data captures neuronal activities from different brain regions over time. Let $X := \\{X^{(n)}\\}_{n=1}^N$ denote a longitudinal EEG set of N patients. For n-th patient, $X^{(n)} \\in \\mathbb{R}^{C \\times T}$ represents several recordings from C channels over a duration of T time points.\nTo prepare for detecting the SO timestamps, each patient recording is segmented into a sequence of second-level epochs by a sliding window, denoted as $X = [X_1, X_2, ..., X_P]$, each $X_p \\in \\mathbb{R}^{C \\times L}$ and L is the window size. Each $X_p$ is associated with a label $y_p \\in \\{0,1\\}$.\nDefinition 2 (Subsequence Clustering). Suppose $X = [X_1,X_2,..., X_P]$ is a set of multivariate time series. Subsequence clustering wants to group these P time steps or epochs into K clusters by:\n\u2022 finding a set of M non-overlapping subsequences of X, i.e., $\\check{X} = [\\check{X}_1,..., \\check{X}_M]$, where $M \\ll P$.\n\u2022 estimate the model representations of K clusters, i.e.,$\\Theta = \\{\\Theta_1, ..., \\Theta_K\\}$.\n\u2022 assign each subsequence to one cluster, $\\Theta_k$ i.e.,$\\tilde{y} = \\{\\tilde{y}_1,..., \\tilde{y}_K\\}$ where $\\tilde{y}_k \\subset \\{1, ..., P\\}$.\nEpochs in a subsequence $\\check{X}_m$ are consecutive and each subsequence is dependent on its neighbors. That is, $\\check{X}_m = \\{t_s, t_e\\}$ represents the starting and ending timestamps, serving SO detection. Moreover, model-based methods define each cluster by statistical models, such as Markov chains (Nakamura et al. 2023) or Gaussian (Matsubara et al. 2014)."}, {"title": "Proposed Method", "content": "This section presents our SODor framework, as shown in Figure 2. Specifically, it comprises two phases: second-level representation learning and sequence-level clustering aimed at explicitly detecting SO in EEG recordings.\nSeizure is fundamentally a network disease (Burns et al. 2014). The goal of this graph model is to characterize this network and show how it is relevant to a seizure event. A network constructed from EEG data $X = [X_1,...,X_P]$ can be represented as a graph $G = (V,E)$ spanning all P epochs. V denotes the set of vertices, corresponding to the EEG channels, and E represents the set of edges, which capture correlations between channels. An edge is defined as $e_{ij} = (v_i, v_j)$, where $i, j \\in C$.\nChannel Correlation for Graph Construction To construct a graph, we extract frequency features in each channel as the node embeddings. We apply the Fast Fourier Transform (FFT) to each normalized epoch $X_p$ to decompose the signal into its frequency spectrum. The absolute values, $x_p = \\{x_p^{(1)},...,x_p^{(|L|)}\\}$, $X_p$ are then extracted to represent the node set V. For edge initialization, we employ a dynamic connectivity modeling method (Tang et al. 2022). We compute the absolute value of the normalized cross-correlation between embeddings $(x_p^{(i)}, x_p^{(j)})$ of each node pair $(v_i, v_j)$. Specifically, we calculate each edge weight $e_{ij}$ as follows:\n$e_{ij} := \\begin{cases} o\\left(x_p^{(i)}, x_p^{(j)}\\right) & \\text{if } v_j \\in V_{\\text{top}}(v_i) \\\\ 0 & \\text{otherwise} \\end{cases}$ (1)\nwhere o denotes the normalized cross-correlation, and $V_{\\text{top}}$ represents the set of neighbor nodes with the highest correlation scores. This results in a sparse graph representing each second-level EEG epoch for subsequent feature encoding.\nChannel Logits Representation Learning While SODor follows the deep clustering paradigm, unlike existing works, it generates explainable feature embeddings as the target of subsequent modeling. Specifically, we propose learning a set of channel-wise logits. These logits encode the information from the original data and represent the probabilities of seizures in various channels. That is, we shift the observation from a Boolean value (in post-process methods) and unexplainable embeddings to probabilities, offering insights into \"how multi-channel interactions in an EEG epoch relate to a seizure event.\u201d To learn the logit representations, $F(\\cdot)$ is designed to capture the spatial and temporal dependencies within each epoch. Given $X_p$ with a processed graph structure, it first learns the spatial information by:\n$Z_{\\text{spatial}}^p = \\text{ReLU}\\left(f_{\\text{conv}}\\left(\\sum_{k=0}^K \\theta_k \\cdot (D^{-1}V_p)^k*X_p\\right)\\right),$ (2)\nwhere $V_p$ denotes the adjacency matrix defined by Eq. (1) and D is the diagonal degree matrix. $f_{\\text{conv}}(\\cdot)$ denotes a diffusion convolution with a ReLU activation function, referring to (Tang et al. 2022). Hence $(D^{-1}V_p)^k$ represents the diffusion process across k steps. $\\theta_k$ are trainable parameters. $Z_{\\text{spatial}} \\in \\mathbb{R}^{C \\times S}$ where $S \\ll L$, is a set of channel-wise feature embeddings. For each channel, we aggregate frequency messages $x_p^{(i)}$ across edges by $\\left[\\sum_{v_j \\in V_{\\text{top}}(i)} e_{ij} \\cdot x_p^{(j)}\\right]$ where $V_{\\text{top}}(i)$ represents the set of neighbors of the node i defined by normalized cross-correlations. Afterward, we use a recurrent neural network to model temporal dependencies along with S time steps:\n$\\{z_c^{(1)},...,z_c^{(S)}\\} = \\sigma(\\text{GRU}(Z_{\\text{spatial}})),$ (3)\nThe c-th channel $z_c$ contains a pair of logits for normal or seizure, obtained by using a softmax $\\sigma(\\cdot)$ to the hidden state of the last time step. A max-pooling layer selects the channel with the maximum logit for loss calculation:\n$L_{\\text{BCE}} := -\\left[y \\log(z_{\\text{max}}) + (1 - y) \\log(1 - z_{\\text{max}})\\right]$ (4)\nwhere $z_{\\text{max}}$ represents the maximum logit value across all channels. Pooling operations are commonly used to aggregate node features, with each method making different assumptions about the graph structure. We assume that max-pooling retains only the most prominent signals, which may lead to better performance, particularly in tasks where certain strong features are indicative of a seizure event. This assumption aligns with findings suggesting that a few unique, abnormal connections across EEG channels can serve as SO markers (Li et al. 2021a; Boo et al. 2021). A comprehensive evaluation is provided in the Experiment section.\nFormulating SO Detection as a Subsequence Clustering Given second-level representations $Z = \\{z_1, ..., z_P\\}$ with C multivariate sequences, our goal is to cluster and segment them into subsequences $\\check{Z} = \\{\\check{Z}_1, \\ldots, \\check{Z}_M\\}$. As described in Definition 2, each subsequence contains several EEG epochs with consistent clustering, and a pair $\\{\\check{Z}_m, \\check{Z}_{m+1}\\}$ represents a cluster transition, which facilitates SO detection. In this work, $H(\\cdot)$ is designed on top of a novel Toeplitz Inverse Covariance-based Clustering (Hallac et al. 2017). This method employs a graphical lasso to estimate sparse Gaussian inverse covariance matrices, also known as precision matrices (Obata et al. 2024), to represent cluster models $\\{\\Theta_{kk} = \\text{normal, seizure}\\}$. Each matrix provides insights into pairwise conditional independencies among EEG channels, determining which correlations contribute most significantly to cluster assignments. The formulation is:\n$\\begin{aligned} \\underset{\\Theta,\\tilde{y}}{\\text{argmin}} & \\sum_{k=1}^K \\sum_{\\tilde{y}=1}^{|\\mathcal{Y}|} \\sum_{z_p \\in \\mathcal{Y}_k} \\left(-ll(z_p, \\Theta_k) \\+\\beta \\mathbb{I}\\{z_{p-1} \\notin \\mathcal{Y}_k\\}\\right) + ||\\lambda \\odot \\Theta_k ||_1 \\end{aligned}$ (5)\nwhere the log-likelihood term measures the probability that the p-th epoch belongs to cluster k by observing its representation $z_p$. The temporal consistency term models long-term dependencies, encouraging neighboring epochs to be assigned to the same cluster. $||\\lambda \\odot \\Theta_k ||_1$ is an $l_1$-norm penalty to control the sparseness of $\\Theta_k$. This enforces the preservation of cluster-specific correlations between EEG channels, enabling the learning of time-invariant representations.\nLimitation. Notably, when $|\\mathcal{Y}| = 1$, Eq. (5) denotes the original clustering algorithm (Hallac et al. 2017). However, while the original clustering algorithm operates on multivariate time series as input, our pairwise logit representation is structured as a two-dimensional tensor time series. Next, we address how to define and infer the model $\\Theta_k$.\nLogits Toeplitz Matrices Instead of clustering each epoch independently, we assume neighboring epochs should be consecutive, so we redefine the \"epoch\" by a sliding window $w \\ll P$, represented as $Z_p := \\{z_{p-w+1},...,z_p\\}$. Thus, we cluster these short-duration matrices and then fit all variables into $\\Theta$, characterized by block Toeplitz inverse covariance matrices. These block-wise constraints are designed to capture time-invariant structural patterns within $Z_p$, helping to smooth abrupt changes. A matrix can be expressed as:\n$\\Theta_k := \\begin{pmatrix} A^{(0)} & A^{(1)} & A^{(2)} & ... & A^{(w-1)} \\\\ (A^{(1)})^P & A^{(0)} & A^{(1)} & ... & A^{(w-2)} \\\\ (A^{(2)})^P & (A^{(1)})^P & A^{(0)} & A^{(1)} & ... \\\\ : & : & : & : & : \\\\ (A^{(w-1)})^P & (A^{(w-2)})^P & ... & (A^{(1)})^P & A^{(0)} \\end{pmatrix}$\nwhere each A represents logit correlations among C channels within w time observations. An element $a_{i,j} \\in A$ refers to the relationship between the i-th and j-th channels at the same w-th epoch or between the (w \u2013 1)-th and w-th epochs. However, the original graphical lasso estimates $\\Theta_k$ based on single observations T. Instead, we formulate a pair of logits as a two-dimensional tensor, treating it as a single observation. Such estimations can become too high-dimensional. (Koki et al. 2023) proposes separating tensor $\\Theta_k$ into multi-mode, where $a_{i,j}^{(n)} \\in A^{(n)}$ refers to the relationship between the i-th and j-th variables in mode-n. This may lead to over-representation, since the logit for \"normal\" already implies the probability of \"seizure\". Since $\\Theta$ are covariance matrics, we solve this by the linearity property of covariance (Wackerly, Mendenhall, and Scheaffer 2008).\nProposition 1. Given a pair of logits $\\{Z_{nor}, Z_{sei}\\}$, denoted as $\\{A, \\bar{A}\\}$, under the constraint $z_{nor} + z_{sei} = 1$, computing $\\text{Cov}(A, \\bar{A})$, as it fully captures the covariance relationship between A and $\\bar{A}$, due to the linearity property and $\\text{Cov}(A, 1) = 0$, as denoted by $\\text{Cov}(A, \\bar{A}) = \\text{Cov}\\{A, \\bar{A}\\}$.\nThe proof is provided in the appendix. Thus, we focus on analyzing the logit representation of the seizure state, which captures pairwise logit correlations. This proof allows us to estimate $\\Theta$. Furthermore, this single logit representation can seamlessly serve as the optimization objective for the BCE loss in Eq. (4).\nClustering for SO detection After estimating the cluster representation $\\Theta$, we identify the optimal $\\Theta_k$ for each $Z_p$. More importantly, we incorporate temporal consistency by ensuring that consecutive EEG epochs are aligned to a consistent representation, thereby further modeling long-term dependencies in a state-consistent sequence, as denoted by:\n$\\underset{k=1}{\\text{minimize }} \\sum_{k=1}^K \\sum_{z_p\\in \\mathcal{Y}_k}-ll(Z_p, \\Theta_k) + \\beta \\mathbb{I}\\{Z_{p-1} \\notin \\mathcal{Y}_k\\}.$ (6)\nThis formulation jointly maximizes the log-likelihood and maintains temporal consistency. The balance between these objectives is controlled by $\\beta$. This is an indicator function: when the same cluster $\\tilde{y}$ is made between neighboring epochs, there is no penalty, but $\\mathbb{I}\\{t-1 \\notin \\mathcal{Y}_k\\}$is 1, if $Z_{p-1}$ does not belong to the same cluster as $Z_p$.\nMinimizing this constraint ensures that neighboring EEG epochs are assigned to the same clusters. Any assignment that deviates from this constraint indicates a state transition. The corresponding p-index marks the onset of the next state, identifying SO when transitioning from a normal state to a seizure state in the EEG sequence.\nOptimization The subsequence clustering is optimized by the expectation-maximization (EM) algorithm to iteratively learn the cluster assignments $\\tilde{y}$ and the structural patterns $\\Theta$ until convergence. Specifically, in Eq. (5), the log-likelihood term and the sparsity term, which can be considered as a typical graphical lasso problem, have a solution guaranteed to converge to the global optimum using the alternating direction method of multipliers (ADMM) (Boyd et al. 2011). The clustering is formed by a dynamic programming optimization that finds the minimum cost Viterbi path for a sequence (Hallac et al. 2017). More detailed methods, implementations, and optimizations can be found in the appendix."}, {"title": "Experiments and Results", "content": "We evaluate SODor to determine if it addresses the following questions:\n\u2022 Can SODor filter out the false detections, e.g., abrupt misclassifications, and can automatically detect the SO.\n\u2022 Are the channel logit correlation representations robust and beneficial for SO detection?\n\u2022 Does SODor potentially provide explainable clusters?\nWe evaluated SODor for the seizure onset (SO) detection task on three real-world datasets. In addition, we assessed a more challenging task: seizure prediction, which aims to identify the preictal state preceding seizures. This task is critical in clinical settings, and we provided onset information for it, referred to as PSO (Preictal Seizure Onset).\ncomprises 844 hours of continuous scalp EEG data from 22 patients, recorded across 22 channels, with a total of 163 seizure episodes. For the PSO detection task, we preprocessed this dataset by defining the pre-seizure state as the 5 minutes preceding seizure onset.\nis collected from University of Helsinki, Finland. It consists of scalp 21-channel EEG data of 79 patients, serving the seizure detection task.\ndataset is part of the Temple University Hospital EEG Seizure Corpus. It comprises 5,612 EEG recordings with 3,050 clinically annotated seizures. We utilized 19 EEG channels, following the standard 10-20 system.\nWe divided each dataset into 70%/20%/10% for training, testing, and validation. We stored the IDs of all epochs in the patient recordings, enabling recall in long-term recordings to verify detection accuracy.\nWe compared SODor with three post-process (PP) SO detection baselines, two classification-based (Cls) seizure detection baselines, and two deep clustering (Clu) methods.\n1. (Burrello et al. 2020) proposed a post-processing that uses a sliding 5-second window to re-assign labels based on a patient-specific voting threshold.\n2. (Boo et al. 2021) proposed a two-step method involving a deep model to classify second-level epochs first and a weighting phase to score the probabilities of SO.\n3. (Li et al. 2021b) proposed an ensemble learning post-process based on four machine learning models and used majority voting to determine the SO threshold.\n4. (Tang et al. 2022) proposed a GNN-based method for seizure detection and classification tasks.\n5. (Ho and Armanfard 2023) proposed a GNN model incorporating contrastive learning for seizure classification.\n6. Time2State (Wang et al. 2023) is a deep clustering method tailored for multivariate time series.\n7. E2Usd (Lai et al. 2024) formulates each dimension of feature embeddings as a multivariate time series and conducts a subsequence clustering on them.\nWe compared selected baselines to SODor. For a fair comparison, with (Burrello et al. 2020), (Boo et al. 2021), and (Li et al. 2021b), we maintained our first-stage second-level learning and compared their post-processing methods. For one-step classification methods (Tang et al. 2022) and (Ho and Armanfard 2023), we used their learning models as the backbone and added our subsequence clustering. For Time2State and E2Usd, since they are tailored for time series, we used our classification model and incorporated it into their model-based clustering, the Dirichlet Process, Gaussian Mixture Model.\nMetrics. We evaluated performance using two clustering metrics: normalized mutual information (NMI) and adjusted Rand index (ARI), along with accuracy (ACC) for assignment analysis.\nResults (main). presents the main SO detection results across three datasets. SODor outperforms all baseline methods. Specifically, for CHB-MIT, SODor achieved an NMI of 0.979, an ARI of 0.964, and an ACC of 0.981, significantly surpassing the performance of other approaches. The post-processing method (Burrello et al. 2020) performed well on this dataset, achieving an NMI of 0.897, an ARI of 0.875, and an ACC of 0.882; however, SODor clearly demonstrated superior performance. One-step classification methods outperformed the two deep subsequence clustering methods (Time2State and E2Usd), as these clustering methods lacked a temporal consistency term. Post-processing and deep clustering methods performed worse on the TUH dataset, likely due to the presence of diverse seizure types and rapid transitions (Tang et al. 2022). These transitions or preictal phases often contain numerous misclassifications that dominate sequences (Daoud and Bayoumi 2019), presenting significant challenges for majority voting.\nalthough SODor exhibits some mismatches between the ground truth and detection labels, it avoids abrupt changes and operates in an automated manner. In contrast, while post-processing reduces false detections, it relies on data-specific manual parameter tuning. also shows significantly fewer mismatches, with minor discrepancies highlighted in the red box.\nTo maintain consistency in the analysis, we further visualized the TUH case used in , focusing on the logits from different channels after training. As discussed in the \"Proposed Method\" section, pooling operations play a critical role in our framework, particularly in summarizing and extracting significant features from channels. To evaluate their impact, we replaced the proposed max pooling with weighted pooling (using an MLP) and mean pooling, keeping the same parameter settings. Interestingly, the final classification accuracy remained unchanged at 0.816.\nResults. illustrates the impact of different pooling methods on channel-wise logits. It is clear that the representation generated by SODor has distinctive characteristics between normal and seizure states. Max pooling exhibits superior robustness compared to other methods. Even though the classification accuracy is the same, weighted pooling distorts the representations, and mean pooling averages the features of graph nodes, which leads to some channels having high probabilities of seizure in normal samples. Moreover, we empirically observe an unstable representation issue during different training settings, but max pooling remains robust across different settings, as shown in the last subfigure.\nSince $\\Theta$ represents inverse covariance matrices and the sparsity term in Eq. (5) controls the sparsity of each cluster model, we extracted the sparse matrices as the adjacency matrix after training. Because our representation preserves the channel index across all learning and clustering stages, we mapped the channels back to the EEG electrodes and graph node positions, following the method outlined in (Tang et al. 2022), to visualize channel correlations in normal and seizure cluster models. For CHB-MIT, which contains more than 19 EEG channels, we used the standard 19-channel system for visualization, consistent with the settings for TUH. Each connection in the visualization corresponds to a \"1\" that persists in $\\Theta$.\ncompares the visualizations of normal and seizure states in two different cases. The results show a consistent pattern: from sparse to dense connections, indicating that more channels are connected and the brain becomes more active during seizures. The central connection of \"C3-CZ-C4\" in the normal state still appears during seizures, as shown in , but more channels are activated. shows that the Occipital lobe (O1 and O2) and Parietal lobe (PZ) remain stable across the transitions, suggesting that the seizure may be related to other areas.\nSODor significantly outperforms both MDL and DPGMM, achieving the highest values with the lowest standard deviation. The reason may be that the MDL-based method (Matsubara et al. 2014) focuses on model compression loss without explicitly considering temporal consistency. DPGMM (Lai et al. 2024) focuses more on estimating the number of clusters. Our method can also use Bayesian information criterion (BIC) to estimate the number of clusters. Since the seizure states are predetermined, in this work, we specify it to two clusters for SO detection and three for SPO detection."}, {"title": "Conclusion", "content": "Actually, several powerful seizure detection models have been proposed, yet no existing works explicitly model the seizure onset, often resulting in unexplainable labeling in long-sequence EEG recordings. We aimed to provide a robust SO detection framework that successfully formulates this task as subsequence clustering, identifying the state (normal or seizure) transition as the SO timestamp. One advantage of this framework is its two-stage learning process, allowing us to fully leverage the high capabilities of existing deep learning methods. Experimental results confirmed that our framework has a strong capacity for SO detection and may have potential for clinical applications."}]}