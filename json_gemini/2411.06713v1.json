{"title": "Ambient Al Scribing Support: Comparing the Performance of Specialized Al Agentic Architecture to Leading Foundational Models", "authors": ["Chanseo Lee BS", "Sonu Kumar MTech", "Kimon A. Vogt MS", "Sam Meraj MBBS"], "abstract": "Background: Integration of Al-powered tools into healthcare workflows necessitates a comparative understanding of Large Language Models (LLMs) for clinical documentation. This study evaluates Sporo Health's Al Scribe-a proprietary, multi-agentic model fine-tuned for medical scribing-against both open-source and closed-source LLMs, including GPT-40, GPT-3.5, Gemma-9B, and Llama 3.2-3B.\n\nMethods: We collected de-identified patient conversation transcripts from partner clinics using Sporo Health's proprietary transcription Al. Clinician-provided Subjective, Objective, Assessment, and Plan (SOAP) notes served as the ground truth. Each Al model was tasked with generating SOAP-formatted summaries from the transcripts using zero-shot prompting. The generated summaries were evaluated against the ground truth using recall, precision, and F1 score metrics. Clinical user satisfaction was assessed using the modified Physician Documentation Quality Instrument revision 9 (PDQI-9).\n\nResults: Sporo's Al Scribe demonstrated superior performance, achieving the highest recall (73.3%), precision (78.6%), and F1 score (75.3%) among all models tested, with the lowest variance in performance. One-way ANOVA indicated statistically significant differences among the models (n = 10, p < 0.05). Post-hoc t-tests revealed that Sporo significantly outperformed GPT-3.5, Gemma-9B, and Llama 3.2-3B (p < 0.05). Sporo surpassed GPT-40 across all metrics by up to 10%, but the difference was not statistically significant (p = 0.25). PDQI-9 surveys showed that Sporo was significantly preferred. Evaluations of Sporo's outputs were notably more comprehensive and relevant, particularly in synthesizing medical information.\n\nConclusion: Sporo Health's Al Scribe outperforms both open-source and closed-source LLMs in accurately summarizing and reasoning through clinical data, as evidenced by objective metrics and clinician satisfaction. This underscores the high potential of Sporo's multi-agentic architecture and proprietary training methodologies to enhance clinical practice.", "sections": [{"title": "Introduction", "content": "Following our initial case study, which compared the performance of Sporo Al Scribe against GPT-40 mini, we continue our exploration of Al-driven medical scribing tools by expanding our analysis to a broader range of models. In this second case study, we compare the performance of several open-source and closed-source Large Language Models (LLMs) in real-world clinical documentation tasks. Specifically, we assess GPT-40,[1] GPT-3.5, [2] Gemma-9B,[3] and Llama 3.2-3B, [4] alongside Sporo's multi-agentic architecture, which has been specifically designed and fine-tuned for medical scribing.\n\nAs Al-powered tools become increasingly integrated into healthcare workflows, understanding the comparative strengths and weaknesses of these models is crucial. While the earlier case study highlighted the promise of Al scribing in improving clinician efficiency, accurate documentation remains a key challenge-especially when dealing with the complexity and nuance of medical language. A recent study by Sporo Health in the current state of Al adoption in clinical workflow reveals these crucial challenges in engineering and clinician satisfaction on the basis of clinical utility \u2013 namely in accuracy, comprehensiveness, and clinician-specific tailoring. [5]\n\nSporo Health's research-oriented approach to healthcare innovation produces key models and a new paradigm in Al applications \u2013 namely in its multi-agentic approach and pioneering training methodologies. In order to prove the effectiveness of our proprietary approach to healthcare Al, this study delves deeper into comparing Sporo's models against a broader range of available, closed-source or open-source models, evaluating how each model handles accuracy, contextual understanding, and the ability to capture all relevant details from patient encounters. With the growing adoption of Al scribing technologies and increasing traction for incorporating Al into clinical workflow, these comparisons offer insights into how different LLMs stack up in real-world medical environments, and the implications for future deployments in clinical settings. It is our hope to prove not only that Sporo Health is at the forefront of innovation, but also that Al is the next step to better healthcare."}, {"title": "Methods", "content": "The following methods were also utilized in the first case study but expanded to larger samples. We collected a dataset of de-identified patient conversation transcripts from one of our partner clinics, generated using Sporo Health's proprietary speaker labeling and transcription Al. For each patient encounter, the piloting clinician provided Subjective, Objective, Assessment, and Plan (SOAP) notes through the scribe platform, which we designated as the ground truth. Sporo Health's Al agentic workflow, along with GPT-40 and GPT-3.5 hosted within Azure Playground, and natively hosted Gemma-9B and Llama 3.2-3B, were then tasked using zero-shot prompting with generating SOAP-formatted summaries from the same transcripts. These summaries were subsequently compared to the clinician's ground truth notes using various quantitative evaluation metrics.\n\nClinical content recall (sensitivity) was defined as the proportion of relevant clinical information from the clinician's ground truth summary that was accurately captured in the Al-generated summaries. To evaluate recall, salient clinical items were manually extracted from each conversation into an inventory. Recall was then calculated by dividing the number of correctly included items from the inventory by the total number of relevant items identified in the inventory.\n\nClinical content precision (positive predictive value) was defined as the proportion of information in the Al-generated summary that was both accurate and relevant when compared to the clinician's ground truth. Precision was calculated by dividing the number of correctly included items in the Al-generated summary by the total number of items in the Al summary, including any additional or incorrect items. This metric reflects the accuracy and relevance of the Al-generated content without introducing extraneous or inaccurate details.\n\nThe F1 score is used as a balanced metric to combine both clinical content precision and recall, providing a single measure of the Al-generated summaries' performance.[6] It represents the harmonic mean of precision and recall, ensuring that both the accuracy of relevant information captured (precision) and the completeness of that information (recall) are taken into account. The F1 score was calculated using the formula:\n\nF1 = 2 x \\frac{Precision \u00d7 Recall}{Precision + Recall}\n\nThis metric is particularly useful for evaluating the overall effectiveness of the Al model when there is a need to balance precision and recall in the generated summaries.\n\nIn addition to the objective accuracy metrics, clinical user satisfaction was evaluated in conjunction with accuracy using the modified Physician Documentation Quality Instrument revision 9 (PDQI-9). The original PDQI-9 employs a 5-point Likert scale across nine attributes to assess the"}, {"title": "Results", "content": "Patient conversation transcripts (n = 10, avg. length = 3862 words) and their respective physician-generated notes (serving as the ground truth) were manually analyzed by an expert for salient clinical content items. The content included but was not limited to items relevant to the chief complaint (ex. \u201cXX presents today for a follow-up appointment regarding their substance use disorder and to discuss treatment options\u201d), social history (ex. social support, living situation, diet, occupation, exercise, relationships), patient emotions such as stress and anxiety, portions of the physical exam or discussions of lab results, and follow-up items such as scheduling an MRI. Each Al summary's averaged metrics across the summaries are shown in Table 2, with the best performing metric bolded.\n\nSporo demonstrated superior accuracy metrics compared to other models, exhibiting the lowest performance variance as indicated by the standard deviation of the F1 score. A one-way ANOVA analysis revealed statistically significant differences in performance among the various models (n =\n10, p < 0.05). Post-hoc two-tailed t-tests indicated that Sporo consistently outperformed GPT-3.5, Gemma-9B, and Llama 3.2-3B, with p-values below 0.05. Although Sporo surpassed GPT-40 across all evaluated metrics by a factor of 10%, the observed differences did not reach statistical significance (p\n= 0.25), likely due to the limited sample size.\n\nIn addition to objective accuracy metrics, clinical utility and satisfaction was assessed using the modified PDQI-9 inventory and the averaged results from a clinician and medical student is shown in Table 3. Results showed that across the both evaluators, Sporo was significantly more preferred across accuracy metrics and overall medical utility. The results showed that there is a tradeoff between succinctness and thoroughness/accuracy \u2013 no models were limited in output size, but there is a preference for notes that can contain the most salient information in the least space."}, {"title": "Discussion", "content": "We present this case study to highlight Sporo's dedication to the highest quality of medical reasoning Al. Through this study, we were able to show using a comparative statistical model, applicable to real clinical practice, that Sporo can outperform both open-source (Gemma-9B, Llama 3.2-3B) and closed-source (GPT-40, GPT-3.5) models in their ability to summarize and reason through real-world clinical data. This was proven through both objective accuracy metrics through manual clinical information inventories and clinician satisfaction surveys using the modified PDQI-9. This is one of many ways of proving that multi-agentic workflow and Sporo's proprietary methodologies in healthcare LLM training and deployment have the potential to transform clinical practice. It reflects Sporo Health's philosophy that Al should be more than a tool that a clinician must chaperone for full demonstration of utility \u2013 it should be an ambient assistant that ameliorates clinical outcomes and lessens burdens, not replace them.\n\nOf particular interest to future studies are Meditron-7B and Mistral-7B, two open-source models trained for the purpose of medical applications. Recent evaluations of medical language models have highlighted notable differences in performance across various medical benchmarks. The Meditron-7B, a domain-specific model fine-tuned for healthcare tasks, has demonstrated strong results, achieving an average accuracy of 57.5% across key medical benchmarks, including MMLU-Medical (54.2%), PubMedQA (74.4%), MedMCQA (59.2%), MedQA (47.9%), and MedQA-4-Option (52.0%). In comparison, the Mistral-7B (including the Mistral-7B-instruct variant) performed relatively lower, with an average accuracy of 38.3% across the same tasks. Specifically, Mistral-7B showed strong results on MMLU-Medical (60.0%) but struggled significantly on domain-specific tasks such as PubMedQA (17.8%) and MedQA (32.4%). Meditron-7B showed a relatively stronger performance on domain-specific tasks, and also outperforms PMC-Llama-7B, a medically fine-tuned version of the popular LLAMA model, showcasing its advantage in specialized healthcare applications. These results underscore the effectiveness of Meditron-7B in handling complex medical queries and its superior performance in comparison to other general-purpose or fine-tuned models in the healthcare domain. [10]\n\nWe also hope to engage in future studies in patient chart review automation (pre-charting), conversion of language models to other languages, and studies with larger sample sizes and diverse ranges of specialties, which were currently limited due to the sheer quantity of work required for analyses like the ones presented in this case study."}]}