{"title": "ACQ: A Unified Framework for Automated Programmatic Creativity in Online Advertising", "authors": ["Ruizhi Wang", "Kai Liu", "Bingjie Li", "Yu Rong", "Qingpeng Cai", "Fei Pan", "Peng Jiang"], "abstract": "In online advertising, the demand-side platform (a.k.a. DSP) enables advertisers to create different ad creatives for real-time bidding. Intuitively, advertisers tend to create more ad creatives for a single photo to increase the probability of participating in bidding, further enhancing their ad cost. From the perspective of DSP, the following are two overlooked issues. On the one hand, the number of ad creatives cannot grow indefinitely. On the other hand, the marginal effects of ad cost diminish as the number of ad creatives increases. To this end, this paper proposes a two-stage framework named Automated Creatives Quota (ACQ) to achieve the automatic creation and deactivation of ad creatives. ACQ dynamically allocates the creative quota across multiple advertisers to maximize the revenue of the ad platform. ACQ comprises two components: a prediction module to estimate the cost of a photo under different numbers of ad creatives, and an allocation module to decide the quota for photos considering their estimated costs in the prediction module. Specifically, in the prediction module, we develop a multi-task learning model based on an unbalanced binary tree to effectively mitigate the target variable imbalance problem. In the allocation module, we formulate the quota allocation problem as a multiple-choice knapsack problem (MCKP) and develop an efficient solver to solve such large-scale problems involving tens of millions of ads. We performed extensive offline and online experiments to validate the superiority of our proposed framework, which increased cost by 9.34%.", "sections": [{"title": "1 Introduction", "content": "Online advertising generates substantial revenue for many Internet companies [24, 44]. Prominent Internet companies, including Google, ByteDance, Kuaishou, Tencent, and Baidu, have developed sophisticated demand-side platforms (DSPs). These platforms are instrumental in automating advertising strategy formulation, optimizing ad delivery, and facilitating real-time bidding. These highly automated and user-friendly platforms substantially enhance advertising efficacy [8, 43]. As a short video platform with nearly 400 million daily active users (DAU), Kuaishou has implemented an advanced automated advertising delivery system. In advertising systems, advertisers must set the quota ratio for three hierarchical levels within their accounts-campaigns, units, and creatives-based on their experience, as well as determine the number of creatives created from each photo. Figure 1 provides additional details.\nAdvertisers tend to create multiple creatives for a single photo, with each creative potentially appealing to different audiences, in order to increase ad cost[26, 27]. Assuming the cost of each photo used to create a single creative is an independent and identically distributed (iid) random variable X with an expected value E(X), the total cost of a photo used to create n creatives is represented by F(x, n), with an expected value E(F(x, n)) = n\u00b7 E(X). This indicates that increasing the number of creatives created from a single photo correlates positively with total ad cost.\nHowever, two critical issues are often overlooked from the perspective of DSPs. First, the number of creatives cannot expand indefinitely due to platform capacity limitations. Additionally, an excessive number of new creatives can result in significant data sparsity issues, as the majority may remain unconsumed. Second, in practice, the total cost F(x, n) is likely a submodular function, indicating the marginal contribution of each additional creative diminishes as the number of creatives increases. This phenomenon is attributed to audience overlap among creatives, as well as the relevance and competitiveness of advertisements. This paper focuses on the automated dynamic creation and cessation of creatives with the objective of maximizing platform revenue. To the best of our knowledge, no comparable research has been conducted in the industry.\nIn this paper, we define the infrastructure allocation problem as optimizing the number of creatives created from a single photo to maximize total cost. However, several challenges arise in modeling infrastructure allocation. First, the actual cost metrics for advertising photos exhibit a significant imbalance, with over 98% of photos experiencing no cost on a given day. This severe imbalance presents a substantial challenge for predictive modeling techniques, as conventional methods for addressing data imbalance are inadequate for such distributions. Second, traditional regression models lack interpretability and fail to capture the functional relationship between photo cost and the number of creatives, resulting in unreliable predictions in real-world scenarios. From a logical perspective, this relationship is likely to be monotonic, submodular, and smooth. Third, the total number of photos is approximately 10 million. Traditional solvers struggle with issues related to computational resources, memory requirements, and processing time when addressing ultra-large-scale optimization problems.\nThis paper proposes a new framework named Automated Creatives Quota (ACQ) for addressing the infrastructure allocation problem, inspired by the successful two-stage \"prediction + allocation\" approach employed in industries such as budget allocation[7, 12], bonus and discount allocation [6, 11], and insurance claims[10]. ACQ consists of two steps: learning the cost model of photos utilized for creating varying numbers of creatives from historical data and allocating the number of creatives to each photo based on business constraints. The prediction module models the data using an unbalanced binary tree and designs a new loss function based on multi-task learning to address the imbalance of the target variable. Cost prediction is performed through a multi-head structure, enabling the model to capture properties that align with logical deductions. The allocation module reformulates the problem as an equivalent convex optimization problem, applies the Lagrange multiplier method to find the optimal solution, and accelerates the process using the bisection method. The time complexity of each iteration is O(n) [7], making it suitable for large-scale problems. Our contributions can be summarized as follows:\n\u2022 We proposed a solution framework named ACQ for a new fundamental problem with significant commercial value, which has not been studied by the industry.\n\u2022 We developed a multi-task learning model based on an unbalanced binary tree to effectively address the imbalance in the target variable. Furthermore, we verified the properties of network monotonicity, submodularity, and smoothness during the modeling process, enhancing the model's interpretability.\n\u2022 We utilized the Lagrange dual method with bisection to calculate the Lagrange multiplier, compress the strategy space, and solve the large-scale allocation problem involving tens of millions with high efficiency.\n\u2022 We conducted extensive offline experiments on real-world datasets and online A/B experiments in real business scenarios. The results demonstrated the superiority of ACQ.\nThe rest of our paper is organized as follows. We introduce related work in Section 2 and briefly formulates infrastructure allocation problem in Section 3. In Section 4, we introduce the proposed framework. We presents the dataset, offline experiments, and online experiments in Section 5. The conclusions are given in Section 6."}, {"title": "2 Related Work", "content": "Programmatic Creativity Commercial advertising generates substantial profits for enterprises and has become an integral component of corporate marketing strategies[1]. The advertising platform's delivery process is divided into the production and editing of campaigns, units, and creatives. The integration of AI in the advertising sector enhances the personalization, precision, and intelligence of advertisements [2]. In the creative dimension, AI applications primarily encompass two key areas: dynamic creative optimization (DCO) and programmatic advertising creation (PAC)[1]. DCO is a technology that leverages data and algorithms to create and optimize advertising creative in real time. By analyzing data such as user behavior, interests, and environmental factors, DCO can automatically adjust advertising content to better align with the needs and preferences of the target audience [3, 4]. PAC involves the automatic creation of ad creativity through programmatic methods. It utilizes technologies such as machine learning and natural language processing to select and combine the most appropriate elements from an extensive library of materials to produce high-quality advertising content[5].\nDeep Imbalanced Regression Learning from imbalanced data using neural networks remains a challenging research problem [13, 32]. Single-stage methods typically involve oversampling, undersampling, or reweighting the loss function during training. Multistage methods enhance long-tail predictions by decoupling the learning of representations from the learning of classifier heads. However, these methods lack generalizability and require extensive hyperparameter tuning [14]. Long-tail data is a significant characteristic in short video recommendation business scenarios. In this domain, certain models based on multi-task learning [15] have partially mitigated the challenges posed by data distribution. For instance, employing interval quantiles as labels [16] and modeling multiple binary classification problems [17] have proven beneficial for addressing imbalanced data. Significantly, several approaches integrating neural networks with tree-based models have demonstrated remarkable efficacy. TPM[18] employs viewing time prediction as an estimate of the expected ordinal rank and proposes a decomposition tree search to replace the traditional linear search.\nNetwork Property Modeling Modeling network properties within neural network models is essential across various practical domains. For example, in finance, the conversion rate for the same user under varying privileges increases as the privileges increase [11, 12]. Ensuring the monotonicity of neural networks involves either function modeling or the design of loss functions and neural network architectures. Function modeling entails establishing a priori linear [6] or nonlinear monotonicity [7, 8] functions based on data distribution and employing models to estimate the parameters corresponding to these functions. The design of loss functions involves the common pair-wise loss in ranking tasks, which is based on the relative order between data point pairs [9]. In model design, selecting an appropriate activation function is crucial for maintaining monotonicity in feature encoding and transformation. Additionally, the activation function of the estimation layer must be monotonic, non-negative, and concave [10]. A direct approach entails designing a neural network with multiple output heads, where each head predicts the increment relative to its predecessor. Activation functions such as square or exponential can be employed to ensure that the increment is positive[11, 12].\nLagrangian dual method One of the most effective methods for decision-making in the second stage is the Lagrangian dual method. Li et al. [23] and Ai et al. [19] modeled the decision-making problem as a multiple choice knapsack problem (MCKP) [45], constructed a Lagrangian relaxation function, and utilized a dual gradient descent method. Zhao et al. [7] and Wang et al. [11] implemented an efficient algorithm based on the bisection method for solving the MCKP. Wu et al.[10] approached the online constrained submodular welfare maximization (SWM) problem, aiming to maximize fraud detection accuracy. The PDA-SP algorithm was proposed to expedite the solution of the dual problem by leveraging submodularity and piecewise linear properties."}, {"title": "3 Problem Formulation", "content": "In this section, we formulate the infrastructure allocation problem as a multiple choice knapsack problem (MCKP). Specifically, the task involves determining the optimal number of creatives to be created from each photo. This task constitutes a constrained optimization problem. The objective is to maximize the estimated cost of all photos, subject to a constraint on the total number of creatives. We formulate this optimization problem as follows:\n$$\\begin{aligned}\n&\\max \\quad \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{k=1}^{K} r_{i j k} x_{i j k} \\\\\n& \\text { s.t. } \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{k=1}^{K} c_{k} x_{i j k} \\leq C \\\\\n& \\sum_{k=1}^{K} x_{i j k}=1, \\quad \\forall i, j \\\\\n& x_{i j k} \\in\\{0,1\\}, \\quad \\forall i, j, k \\\\\n& c_{k}=1,2, \\ldots, 200\n\\end{aligned}$$\nwhere for a given account-photo ID pair (i, j), k denotes the number of creative candidates for the account-photo ID pair, and $c_k$ represents the number of photos used to create candidate k. In practical applications, a photo can be used to create up to 200 creatives. $x_{ijk}$ represents the account-photo ID pair created by k creatives, and $r_{ijk}$ denotes the cost of the pair. $r_{ijk}$ is calculated as shown in Equation 2, where $pvalue_{ijk}$ is the output of the first stage of the ACQ, representing the estimated cost of the account-photo ID pair used for creating k creatives.\n$r_{ijk} = pvalue_{ijk} + \\lambda \\cdot explore\\_score_{ijk}$         (2)\nwhere explore_scoreijk represents the exploration score, based on the classic UCB algorithm in reinforcement learning[41]. It scores each photo based on past statistical experience and uncertainty. In actual business, it plays a significant role in balancing exploration and utilization."}, {"title": "4 Methodology", "content": "In this section, we present the details of ACQ. In the prediction module, we address the challenge of unbalanced target variables and examine network properties to ensure monotonicity, submodularity, and smoothness. In the allocation module, we tackle the large-scale allocation problem using a Lagrangian dual optimization method with bisection."}, {"title": "4.1 Prediction Module", "content": "4.1.1 Modeling based on unbalanced binary tree. Given a training set (X, n, C), where X denotes the features of the photo, n represents the number of creatives created from the photo, and C indicates the actual cost. The objective of the first stage is to construct a model M that accurately fits the data distribution of the training set. Additionally, this stage aims to mitigate challenges associated with the model's prediction results M(X\u2758c), which may exhibit bias towards high-frequency values or display imbalanced evaluation metrics due to the inherent imbalance in target variables[32].\nDatasets in recommendation systems often exhibit a long-tail distribution, where tree-based neural networks like TDM[31] and TPM[18] perform strongly. Inspired by these models, we propose a modeling approach called Unbalanced Binary Tree Model (UBTM), which is based on an unbalanced binary tree. For the sample set, we construct an unbalanced binary tree T consisting of N nodes, each representing an interval or point set of cost, as illustrated in Figure 2. Assuming there are m training samples, with the cost of each sample xi denoted as yi, the cost should satisfy {yo \u2264 y1, . . ., Yk, ..., \u2264 Ym} after being sorted in ascending order. The root node nodeo then represents the full interval of the cost, i.e., nodeo \u2208 [yo, ym]. The left child node1 of the root node represents the {0} set, i.e., the sample with yi = 0; these two sets remain unchanged regardless of the data distribution of the training dataset, whereas the right subtree of the root node adapts accordingly. In the right subtree of the root node, each leaf node represents a data interval formed by equally frequent grouping of samples with yi > 0, and the subspace of the parent node is the union of the subspaces of its child nodes. In the model, each non-leaf node is equipped with a binary classifier. If the unbalanced binary tree has Nf leaf nodes, then it contains N - Nf binary classifiers. The output of a non-leaf node represents the conditional probability that the cost belongs to the interval or point set associated with the child node, based on the output probability of its parent node. In other words, for any node in T, we must determine the path $p_e$ from the root node to it and multiply the probability outputs of multiple classifiers. Consequently, given the node node at the d(k)-th layer, its cost follows the subsequent multinomial distribution:\n$$\np(C \\in node \\mid X, T)=\\left\\{\n\\begin{array}{ll}\np(C=0 \\mid X, T) & \\text { if } node=\\{0\\} \\\\\np(C \\in p_{d(k)}(k) \\mid X, T) & \\text { otherwise }\n\\end{array}\n\\right.\n$$                (3)\nwhere $p_{i(k)}$ denotes the node at the i-th level (i \u2208 {0, 1, 2, ...}) on the path from the root node to $node_k$ (d(k) \u00bb 1). Then, p(C \u0454 $P_{d(k) (k)} | X, T$) represents the conditional probability of $p_{i(k)}$. Consequently, we can infer:\n$$\np(C \\in node \\mid X, T)=p(C \\in p_{d(k)}(k) \\mid X, T)=\\prod_{i=1}^{d(k)} p(C \\in p_{i(k)} \\mid X, T, C \\in p_{i-1(k)})\n$$             (4)\nFor a leaf node $node_k$, its conditional probability represents the likelihood of the cost falling within the corresponding interval. Certain approaches estimate the metric by accumulating the products of each leaf node interval's midpoint value and its associated conditional probability. However, if the range of a leaf node interval is too large (e.g., (46.37, 20000]), it can significantly affect the regression value. Therefore, the unbalanced binary tree we constructed will be used for auxiliary tasks in multi-task learning.\n4.1.2 Multi-task learning. Multi-task learning [33] effectively leverages shared information among related tasks to enhance the model's generalization capability. The model trains N \u2013 Nf binary classification tasks, where N - Nf represents the number of non-leaf nodes in an unbalanced binary tree. Each binary classification task computes the conditional probability of a cost falling within the interval represented by the corresponding child node, conditioned on the parent node's output. The classification error component aims to maximize the likelihood of a sample belonging to the leaf node along its traversal path. To quantify the discrepancy between predicted and actual probability distributions, we employ the cross-entropy loss function:\n$$\nl_{1}=\\sum_{i=\\mid \\text { node }_{i} \\notin S_{\\text { leaf }}}^{N-N_{f}} w_{i}\\left(-y_{i} \\log \\hat{y}_{i}-\\left(1-y_{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)\\right)\n$$              (5)\nwhere Sleaf denotes the set of leaf nodes. \u011di denotes the estimated value of the model classifier, while yi represents the actual binary label of the sample, determined by the sample's cost and the range of each interval in the unbalanced binary tree T. Before training commences, each sample is assigned to a leaf node of T. The traversal path from the root node to the leaf node determines the yi value of the sample. The classifier weight wi along this path is set to 1, serving as a mask, while it is set to 0 for all other paths. Figure 2 illustrates two examples of this process.\nAn additional auxiliary task involves quantifying uncertainty. By integrating the standard deviation into the loss function, the model can emphasize not only prediction accuracy but also confidence, thereby enhancing the overall reliability of the predictions[18]. Note that C represents the total estimated cost:\n$$\nl_{2}=\\sqrt{E\\left(C^{2} \\mid X, T\\right)-E(C \\mid X, T)^{2}}$$\nFor the primary task, the model incorporates a regression output head to compute the regression loss, which quantifies the discrepancy between the final predicted cost \u0177i and the actual value yi:\n$$l_3 = (y_i - \\hat{y}_i)^2$$\nFinally, by integrating these three training tasks, we derive the loss function of UBTM:\n$$l = a_1 l_1 + a_2 \\cdot l_2 + a_3 \\cdot l_3$$\nThe parameters used in our study are a\u2081 = 1, a2 = 1, a3 = 0.2 during training. While methods exist capable of automatically learning the parameter values for each component of the loss function[34], extensive offline experiments have demonstrated that the model exhibits robustness and is not sensitive to parameter variations.\n4.1.3 Modeling Network Properties. In this section, we analyze the functional relationship between the number of creatives created from a single photo and the corresponding cost. Figure 3 depicts the overall structure of the proposed model. For a sample (x, bin, c), x comprises both sparse and dense features. The feasible range of creative numbers is discretized into multiple bins, where each bin denotes the specific interval containing the sample. After feature processing, the model learns complex patterns through multiple hidden layers. In the following sections, we first introduce the definition of the properties to be explored, followed by a detailed description of the implementation.\nSuppose a photo P can be used for creating a set of creatives S = {S1, S2,..., Sn}, with the cost for each creative is R = {r1, r2,...,rn | r\u2081 \u2265 2 \u2265 ... \u2265 rn}.\nASSUMPTION: We hypothesize that the cost of a photo is equivalent to the cumulative cost of all its associated creatives. This hypothesis is based on the understanding that ad ranking and recall are evaluated at the individual creative level.\nThe cumulative cost for n creatives is given by:\n$$\ncc_{k}=\\sum_{i=1}^{n} c_{i}\n$$\nMonotonicity. DEFINITION: $N_{1}<N_{2} \\Rightarrow c c_{n_{1}}<c c_{n_{2}}$\nDifferent creatives appeal to diverse audiences. The cost of a photo increases with its frequency of use in creating various creatives. To ensure monotonicity, we developed a model with multiple output heads. The first output head predicts the initial cost, while each subsequent kth (k > 1) output head predicts the increment relative to its preceding (k \u2013 1)th output head. The final regression prediction is derived from the appropriate output head, determined by the bin corresponding to the number of creatives created from the sample. Consequently, the prediction formula that ensures monotonicity is as follows:\n$$\n\\hat{y}_{k}=\\left\\{\n\\begin{array}{ll}\n\\hat{y} & \\text { if } k=1 \\\\\n\\hat{y}_{1}+\\sum_{i=2}^{k} 2(\\hat{y}^{\\prime}\\right)^{2}, & \\text { if } k>1\n\\end{array}\n\\right.\n$$                   (9)\nwhere squaring the value of the kth (k > 1) output head serves to ensure that the cumulative increment is non-negative.\nSubmodularity. DEFINITION: As the number of creatives increases, the marginal benefit decreases: $n_{1}<n_{2} \\Rightarrow \\delta n_{1}=c c_{n_{1}}-c c_{n_{1-1}}=c_{n_{1}}>\\delta n_{2}=c c_{n_{2}}-c c_{n_{2-1}}=c_{n_{2}}$\nThe audiences for different creatives exhibit overlap, and due to ad fatigue, users' interest in advertisements diminishes as the number of impressions increases [35]. To satisfy submodularity constraints, we must ensure that the effect increment remains nonnegative and its slope decreases. We employ reverse accumulation to guarantee the monotonic decrease of the effect increment's slope. Let $N_{bin}$ denote the number of interval bins for creatives, and yi represent the average rate of change of the increment slope in the i-th interval. Then:\n$$\ny_{k}^{\\prime\\prime}=\\left\\{\n\\begin{array}{ll}\ny_{k}^{\\prime\\prime} & \\text { if } k=N_{\\text {bin }} \\\\\ny_{k}^{\\prime\\prime} N_{b i n}+\\sum_{i=k}^{N_{b i n}}\\left(y_{i}^{\\prime\\prime}\\right)^{2}, & \\text { if } 1<k<N_{b i n} \\\\\ny_{k}^{\\prime\\prime}, & \\text { if } k=1\n\\end{array}\n\\right.\n$$             (10)\n$$y_{k}^{\\prime}=y_{k}^{\\prime\\prime} \\cdot \\delta_{i}$$\nwhere di denotes the interval length of the i-th creative bin, defined as the difference between the right and left boundaries of the interval.\nLet f(x) represent the cost function with respect to the number of creatives, where x denotes the number of creatives. Consequently, over the interval [$a_{i-1}$, $a_i$], the effect increment can be expressed as a definite integral:\n$$ \\Delta f=\\int_{a_{i-1}}^{a_{i}} f^{\\prime}(x) d x $$\nAccording to the Mean Value Theorem in calculus, for a continuous function f(x) on the closed interval [$a_{i-1}$, $a_i$], there exists a point c\u2208 ($a_{i-1}$, $a_i$) such that:\n$$\\int_{a_{i-1}}^{a_{i}} f^{\\prime}(x) d x=f^{\\prime}(c) \\cdot\\left(a_{i}-a_{i-1}\\right)$$\nIn our model, the effect increment slope $y_i''$; can be interpreted as the average rate of change over the interval, equivalent to $f'(c)$. This reasoning enables us to derive the form of Equation 11.\nSmoothness. DEFINITION: The change in total cost should be smooth as the number of creatives increases. This can be ensured by Lipschitz continuity: Vn1, n2 \u2208 {1, 2, ..., 200}, |CCn\u2081-CCn2| \u2264 L|n1-n2|, where L is a Lipschitz constant ensuring the smoothness of the cost function.\nSmoothness is a crucial attribute that ensures the function does not exhibit abrupt changes, which would be impractical for real-world applications [12]. To incorporate smoothness into our model, we estimate the Lipschitz constant $z_j$ for the j-th layer by calculating the spectral norm (i.e., the largest singular value) of the weight matrix of that layer. Subsequently, the Lipschitz constant of the entire network can be approximated by the product of the Lipschitz constants of each layer[36, 37]. Finally, this overall Lipschitz constant is incorporated into the loss function as a regularization term. The Lipschitz-regularized loss function llip and the total loss of the network $l_{smoothness}$ are expressed as follows:\n$$L_{l i p}=\\prod_{j=1}^{L} \\mid \\text { softplus }\\left(z_{j}\\right)\n$$               (14)\n$$l_{\\text {smoothness }}=1+\\lambda l_{l i p}$$\nwhere softplus($z_j$) = ln(1+ezi) is utilized to ensure the non-negativity of $z_j$, L denotes the number of fully connected layers, and A is a weighting coefficient that modulates the intensity of the smoothness regularization constraint."}, {"title": "4.2 Allocation Module", "content": "Integer programming problems, especially large-scale ones, are inherently complex and computationally expensive. In contrast, linear programming problems can be solved in polynomial time, with optimal solutions readily obtainable through existing efficient algorithms[38]. Previously, we formulated the infrastructure allocation problem as an optimization problem, as shown in Equation 1. To expedite the solution process, we relax this problem into a linear programming problem by allowing the variable xijk to take values in the continuous interval [0, 1] instead of the discrete set {0, 1}. The corresponding relaxed problem is formulated as follows:\n$$\n\\begin{aligned}\n&\\max \\quad \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{k=1}^{K} r_{i j k} x_{i j k} \\\\\n& \\text { s.t. } \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{k=1}^{K} c_{k} x_{i j k} \\leq C \\\\\n& \\sum_{k=1}^{K} x_{i j k}=1, \\quad \\forall i, j \\\\\n& c_{k}=1,2, \\ldots, 200\n\\end{aligned}\n$$\nThe relaxed problem is a linear programming problem that satisfies both strong duality and the Karush-Kuhn-Tucker (KKT) conditions. By introducing the constraint $\\sum_{i,j} \\sum_{k} c_k x_{ijk} \\leq C$ with its corresponding Lagrange multiplier \u03bb > 0, we derive the Lagrangian form of Problem 16 as follows:\n$$\n\\max L(X, \\lambda)=\\sum_{i j k} r_{i j k} x_{i j k}+\\lambda\\left(C-\\sum_{i j k} c_{k} x_{i j k}\\right)\n$$\nThe dual problem of the original problem entails minimizing the Lagrangian function. Formally, it can be expressed as follows:\n$$\n\\min _{\\lambda} \\max _{X} L(X)=\\sum_{i j k} r_{i j k} x_{i j k}+\\lambda\\left(C-\\sum_{i j k} c_{k} x_{i j k}\\right)\n$$\nOne of the KKT conditions for obtaining the optimal solution is complementary slackness, which states:\n$$\n\\lambda\\left(C-\\sum_{i j k} c_{k} x_{i j k}\\right)=0\n$$\nPROPOSITION 1: Given the dual variable \u03bb, for each i and j, the optimal $k^*$ is determined by $k^*= \\arg \\max _k (r_{ijk} - \\lambda c_k)$. Consequently, $x_{ijk^*} = 1$.\nBased on Proposition 1, the problem is reformulated to focus on determining the dual variable \u03bb. We denote the partial expression on the right-hand side of Equation 19 by g(\u03bb):\n$$g(\\lambda)=\\sum_{i j k} c_k * x_{ijk} - C$$\nPROPOSITION 2: The function $g(\\lambda)=\\sum_{ij} \\sum_k C_k \\cdot X_{ijk} - C$ is a monotone non-increasing function of \u03bb.\nAccording to Proposition 2, the function g(1) is monotonic. As stated in Equation 19, the corresponding A is optimal when g(1) = 0. Therefore, we employ a method called Dual based Binary Search Solver (DBSSolver), which utilizes a binary search approach to determine the optimal value of [7]. Once this optimal A is found, we solve the optimization problem presented in Equation 17. This conclusion is substantiated in Proposition 1."}, {"title": "5 Experiments", "content": "5.1 Dataset\nThe real-world dataset used in this study is sourced from Kuaishou's advertising intelligent delivery platform. A detailed description is provided in Table 1."}, {"title": "5.2 Offline Experiments", "content": "To demonstrate the effectiveness of the ACQ, experiments were conducted separately on the prediction and allocation modules. The training and validation data were obtained from two consecutive days within the real-world dataset.\n5.2.1 Prediction.\nComparison Methods.\n\u2022 UBTM: The model we proposed is a multi-task learning framework based on an unbalanced binary tree, as shown in Figure 5.\n\u2022 DNN: The basic regression model utilizes a deep neural network for prediction, employing the Mean Squared Error (MSE) loss function, as illustrated by structure 13 in Figure 3.\n\u2022 ZILN[39]: This approach employs a deep neural network in conjunction with the Zero-Inflated Log-Normal (ZILN) loss function for lifetime value (LTV) prediction. The ZILN loss function combines a zero-point mass with a log-normal distribution to address challenges related to zero-value occurrences and long-tail distributions in LTV prediction.\n\u2022 CREAD [17]: The CREAD framework initially discretizes the continuous label into multiple intervals. Subsequently, it trains multiple binary classifiers to predict whether the viewing time exceeds each threshold. Finally, the framework reconstructs the final label using the predicted values from these classifiers.\nEvaluation Metrics.\n\u2022 Area Under the Curve (AUC): AUC indicates the probability that a randomly selected positive instance is ranked higher than a randomly selected negative instance. A higher AUC signifies better model performance and serves as the core indicator in our offline experiments.\n\u2022 Mean Squared Error (MSE): MSE is a standard metric for regression tasks that measures the average of the squared errors between predicted and actual values. A lower MSE signifies better model performance.\n\u2022 Positive AUC (PAUC): PAUC is a variant of AUC focusing solely on ranking positive instances. It measures the probability that a randomly selected positive instance is ranked higher than another positive instance. A higher PAUC signifies better model performance in ranking positive instances.\n\u2022 Cost-weighted AUC (GAUC): GAUC extends the AUC metric by weighting it according to the total cost within each account. This approach enhances the influence of high-value accounts, as these accounts significantly contribute to online performance.\nExperimental conclusion. Table 2 presents the effect of the prediction module on the real-world dataset, where \"mon,\" \"sub,\" and \"smo\" denote the monotonicity, submodularity, and smoothness structures of the model, respectively, and \"control\" indicates the absence of special function design. The results reveal the following observations: (1) UBTM demonstrates excellent performance in core indicators and ranks well in other metrics, proving the effectiveness of the proposed method. (2) A horizontal comparison between UBTM and DNN methods reveals that UBTM achieves significant advantages, demonstrating the benefits of modeling data distribution based on unbalanced binary trees. (3) UBTM integrates monotonicity and submodularity in model design. Ablation experiments indicate that these special designs enhance the modeling of properties and improve interpretability. However, incorporating smoothness into UBTM does not enhance performance. We hypothesize that this is due to the discretization of the number of creatives into a relatively small number of bins (9 in the actual business), resulting in mutation points in the function, which aligns with expectations.\nParameter sensitivity. As illustrated in Figure 5, multiple offline experiments were conducted by varying the weight factor of multitask learning and modifying the number of leaf nodes in the construction of the unbalanced binary tree.\n5.2.2 Allocation. We demonstrate the efficiency of the DBSSolver in providing solutions. GLPK (GNU Linear Programming Kit) is an open-source software package designed for solving large-scale linear programming, mixed integer programming, and other related problems [40]. Compared to GLPK, DBSSolver significantly enhances solution efficiency while maintaining acceptable solution quality across varying dataset sizes. The comparison results are presented in Table 3. The results further demonstrate that the dual problem can be solved by sampling large-scale instances, allowing decisions to be made on all samples based on the solved dual value to obtain the original solution, facilitated by the efficient dichotomy method."}, {"title": "5.3 Online Experiments", "content": "5.3.1 Online framework. The entire online framework is shown in Figure 4.\n\u2022 Two-stage algorithm: This approach employs a two-stage solution of \"prediction + allocation\" specifically designed for the infrastructure problem. The material creation rules at the output account granularity are stored in Redis.\n\u2022 Engine service: On the engine side, the service retrieves the creation rules from Redis in real-time for existing accounts, ensuring that the online infrastructure volume of account materials aligns with the synchronized rules.\n\u2022 Advertising platform: The ad platform determines the optimal number of creatives that should be created for each photo based on the algorithm module. Subsequently, the platform allocates all creatives from campaigns to units, then constructs a complete three-level index tree for each account.\n\u2022 Ad Display: The process of advertisement display involves recall, rough sorting, and fine sorting, and is completed after matching with user-side features, resulting in an increased cost of photos[42].\n5.3.2 A/B test. ACQ has been implemented on the Kuaishou advertising platform, where we conducted A/B testing over a period of 6 days, focusing on actual cost and the number of creatives. Compared to Kuaishou's existing rule-based baseline, ACQ increases cost by an average of 9.34% and the total number of creatives by an average of 1.42%, indicating a significant improvement. This demonstrates that a more intelligent infrastructure allocation is achieved while adhering to the specified constraints."}, {"title": "6 Conclusion", "content": "In this paper", "challenge": "optimizing the number of creatives created from a single photo to maximize total cost. The initial stage employs UBTM, a cost prediction model leveraging unbalanced binary trees for multi-task learning in data modeling. UBTM incorporates network components ensuring monotonicity and submodularity, thereby guaranteeing accurate and interpretable outputs. The subsequent stage utilizes a Lagrangian root-finding module coupled with DBSSolver, which employs a dichotomy method. DBSSolver substantially expedites the solution process compared to conventional solvers, rendering it appropriate"}]}