{"title": "Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment", "authors": ["Jiaqi Wu", "Simin Chen", "Zehua Wang", "Wei Chen", "Zijian Tian", "F. Richard Yu", "Victor C. M. Leung"], "abstract": "Abstract-As the volume of collected video data continues to grow, data-oriented cloud computing in Internet of Video Things (IoVT) systems faces increasing challenges, such as system latency and bandwidth pressure. Task-oriented edge computing, by shifting data analysis to the edge, enables real-time visual inference. However, the limited computational power of edge devices presents challenges for effectively executing computationally intensive visual tasks. Existing methods struggle with the trade-off between high model performance and low resource consumption. For instance, lightweight neural networks often deliver limited performance, while Neural Architecture Search (NAS)-based model design incurs high computational and commercial costs at training Inspired by hardware/software co-design principles, we propose, for the first time, a co-design framework at both the model and system levels to optimize neural network architecture and deployment strategies during inference. This framework maximizes the computational efficiency of edge devices to achieve high throughput, enabling real-time edge inference. Specifically, we implement a dynamic model structure based on the re-parameterization principle, coupled with a Roofline-based model partitioning strategy to synergistically enhance the computational performance of heterogeneous devices. Furthermore, we employ a multi-objective co-optimization approach for the above strategies to balance throughput and accuracy. We also conduct a thorough analysis of the mathematical consistency between the partitioned and original models, in addition to deriving the convergence of the partitioned model. Experimental results show that, compared to baseline algorithms, our method significantly improves throughput (12.05% on MNIST, 18.83% on ImageNet) while also achieving higher classification accuracy. Additionally, it ensures stable performance across devices with varying computational capacities, highlighting its generalizability and practical application value. Simulated experiments further", "sections": [{"title": "I. INTRODUCTION", "content": "Real-time visual task inference is a core function of the In-ternet of Visual Things (IoVT) systems [22], [23], particularly for time-sensitive tasks such as object detection [25] and object tracking [26]. Real-time inference can effectively prevent task failures caused by latency. Currently, many IoVT systems rely on a centralized cloud computing architecture, as shown in Fig. 1, where data collected by terminal sensing devices is transmit-ted to cloud servers for centralized inference. However, as the number of terminal devices increases and the volume of visual data grows exponentially, the long-distance transmission of large-scale data creates immense bandwidth pressure, severely compromising the system's real-time performance [24], [27]. Thus, this centralized cloud computing design principle is not well-suited for machine vision applications.\nEdge computing [28], as an emerging computational paradigm, as illustrated in Fig. 1, shifts data processing and analysis tasks closer to the data source, avoiding the need for massive data transmission, and thus holds greater potential for enhancing real-time system performance. [29] For example, studies [27] and [30] deploy lightweight object detection mod-els on edge devices (e.g., drones) to achieve real-time power inspection, while study [31] deploys deep learning models on edge servers to avoid the latency issues associated with transmitting visual data to the cloud. However, a key challenge is limited computational resources of each edge device, which are insufficient to meet high-performance demands of complex visual tasks [32]. Although the computational power of edge devices has improved, it still lags significantly behind cloud servers, making it difficult to fully satisfy the computational requirements of visual inference tasks [33], [34].\nDespite neural network edge deployment solutions being proposed to address this issue [39], [40], there remain signif-icant limitations in achieving real-time inference, where bal-ancing performance and throughput proves challenging [38].\nTo explore this, we introduce several key edge deployment approaches and analyze their drawbacks:\n\u2022 General lightweight models (e.g., the Mobilenet fam-ily [42] and GhostNet [43]): These models focus on"}, {"title": "II. RELATED WORK", "content": "A. Co-design mechanism for computing efficiency\nCo-design mechanism [58] is widely applied to reduce computation latency, focusing on joint design across differ-ent layers, such as hardware and software levels, to fully leverage hardware capabilities and minimize system delays. This approach enables a more efficient balance between al-gorithmic complexity and hardware performance, making it a key strategy in many modern computational tasks [59]. Below are examples of various co-design algorithms that illustrate these principles: Neo [2] is a co-designed software-hardware system that efficiently trains large-scale deep learning rec-ommendation models (DLRMs) by using 4D parallelism to optimize embedding computations. Paired with the ZionEX"}, {"title": "III. METHODOLOGY", "content": "A. Overview\nWe propose a co-design method for model dynamic struc-ture and edge partition deployment tailored for real-time infer-ence on resource-constrained edge devices in IoVT systems.\u00b9\nThe architecture of the method is shown in Fig. 3. At the sys-tem level, we utilize Roofline analysis (see Fig. 4) to determine the partition point by matching the partitioned model with the corresponding device's capabilities, thereby maximizing the device's computational performance. At the model level, we adopt a dynamic model structure based on re-parameterization, as depicted in Fig. 2, and employ adaptive network structure to further optimize model-device alignment. This approach uses dynamic network structures to facilitate model partitioning,\nComputational Intensity Requirements. To ensure that the partitioned sub-models operate efficiently on their respective hardware platforms, they must satisfy the computational in-tensity requirements of each sub-model. The Roofline analysis [56] helps to fully utilize hardware resources, preventing sys-tem bottlenecks caused by limitations in memory bandwidth or computational capacity. Computational intensity (Operational Intensity, $I$) is defined as the ratio of floating-point operations (FLOPs) to data transfer volume (in bytes), measured in FLOP/byte. The concept of computational intensity reflects the balance between the system's computational tasks and its data transfer requirements. Each hardware platform has its own computational intensity threshold: $I_m = \\frac{\\pi}{\\beta}$, where $\\pi$ represents the peak computational capability of the hardware (in FLOP/s), and $\\beta$ represents the memory bandwidth (in bytes/s). The computational intensity threshold (in FLOP/byte) is used to evaluate the hardware's ability to execute tasks efficiently.\nThe computational intensity of the complete model is ex-pressed as $I_o = \\frac{\\sum_{i \\epsilon C_i}{M_i}}$. $C_i$ represents the floating-point oper-ations (FLOPs) of sub-model i, while $M_i$ denotes the memory size in bytes required for a single feed-forward propagation. In this study, we partition the model into two sub-models, where the computational intensity of each sub-model is defined as $I_i = \\frac{C_i}{M_i}$, with $i \\in \\{1, 2\\}$. The Roofline results of model partitioning processing as shown in Fig. 4. The computational intensity of sub-model 1, namely $I_1$, representing the intensity of the sub-model on the computing terminal, is given by the following equation:\n$I_1 = \\frac{C_1}{M_1} = \\frac{\\lambda_c}{\\lambda_m} \\frac{C_o}{M_o} = \\frac{\\lambda_c}{\\lambda_\\mu} I_o, \\qquad(1)$\nwhere the parameter $\\lambda$ represents the partition point, where $\\lambda_c$ and $\\lambda_M$ indicate the proportion of the model partitioned in terms of floating-point operations and memory size, respec-tively.\nTo ensure that sub-model 1 operates efficiently, the com-putational intensity $I_1$ should ideally be as large as possible to maximize the computational performance of the device. If $I_1$ reaches or exceeds the computational intensity threshold"}, {"title": "IV. NEURAL NETWORK/EDGE DEPLOYMENT CO-DESIGN METHOD", "content": "We propose a neural network / edge deployment strategies co-design approach. Next, we provide a detailed introduction to the system-level edge partitioning deployment strategy, the neural network-level dynamic model structure, and the multi-objective co-optimization approach. We propose a neural network / edge deployment strategies co-design approach. Next, we provide a detailed introduction to the system-level edge partitioning deployment strategy, the neural network-level dynamic model structure, and the multi-objective co-optimization approach.\nComputational Intensity Requirements. To ensure that the partitioned sub-models operate efficiently on their respective hardware platforms, they must satisfy the computational in-tensity requirements of each sub-model. The Roofline analysis [56] helps to fully utilize hardware resources, preventing sys-tem bottlenecks caused by limitations in memory bandwidth or computational capacity. Computational intensity (Operational Intensity, $I$) is defined as the ratio of floating-point operations (FLOPs) to data transfer volume (in bytes), measured in FLOP/byte. The concept of computational intensity reflects the balance between the system's computational tasks and its data transfer requirements. Each hardware platform has its own computational intensity threshold: $I_m = \\frac{\\pi}{\\beta}$, where $\\pi$ represents the peak computational capability of the hardware (in FLOP/s), and $\\beta$ represents the memory bandwidth (in bytes/s). The computational intensity threshold (in FLOP/byte) is used to evaluate the hardware's ability to execute tasks efficiently.\nThe computational intensity of the complete model is ex-pressed as $I_o = \\frac{\\sum_{i \\epsilon C_i}{M_i}$. $C_i$ represents the floating-point oper-ations (FLOPs) of sub-model i, while $M_i$ denotes the memory size in bytes required for a single feed-forward propagation. In this study, we partition the model into two sub-models, where the computational intensity of each sub-model is defined as $I_i = \\frac{C_i}{M_i}$, with $i \\in \\{1, 2\\}$. The Roofline results of model partitioning processing as shown in Fig. 4. The computational intensity of sub-model 1, namely $I_1$, representing the intensity of the sub-model on the computing terminal, is given by the following equation:\n$I_1 = \\frac{C_1}{M_1} = \\frac{\\lambda_c}{\\lambda_m} \\frac{C_o}{M_o} = \\frac{\\lambda_c}{\\lambda_\\mu} I_o, \\qquad(1)$\nwhere the parameter $\\lambda$ represents the partition point, where $\\lambda_c$ and $\\lambda_M$ indicate the proportion of the model partitioned in terms of floating-point operations and memory size, respec-tively.\nTo ensure that sub-model 1 operates efficiently, the com-putational intensity $I_1$ should ideally be as large as possible to maximize the computational performance of the device. If $I_1$ reaches or exceeds the computational intensity threshold"}, {"title": "V. CONVERGENCE ANALYSIS OF THE PARTITIONED MODEL", "content": "In this section, we conduct a convergence analysis for the optimization of the base model at partitioned deployment. The optimized model will be input into the proposed co-design method, where it serves as the initial state for co-optimization.\nFirst, we analyze the mathematical consistency of parameter updates between the partitioned model and the original full model. Furthermore, we provide a detailed convergence proof for the partitioned model.\nConsistency Analysis in terms of gradient computation and parameter updates. By analyzing the consistency of gradi-ent computation and parameter updates, we ensure that the split model remains mathematically equivalent to the original model. This analysis is crucial because if the splitting pro-cess introduces discrepancies, such as incorrect gradients or parameter updates, several issues may arise: 1) the model's optimization process could deviate from the desired behavior; 2) the theoretical guarantees of convergence (e.g., achieving global or local optima) may no longer hold; and 3) the convergence proof derived for the non-split model would no longer apply to the split model, necessitating additional complex modifications.\nThe detailed analysis is as follows: in the split model, the gradients are computed independently by Device 1 and Device 2:\n$g_1 = \\nabla_{w_1} L(w_1, w_2), \\quad g_2 = \\nabla_{w_2} L(w_1, w_2), \\quad n = 1,2\\qquad(12)$\nThat is, Device 1 computes the partial derivative of the loss function $L(w_1, w_2)$ with respect to the parameter $w_1$, and Device 2 computes the partial derivative with respect to the parameter $w_2$. Since the forward and backward propagation processes in the split model compute all gradient information completely, combining the gradients from Device 1 and Device 2 yields the overall gradient:\n$\\nabla L(w) = [g_1, g_2],\\qquad (13)$\nthis is consistent with the overall gradient computation in the original model. Thus, the split model maintains consistency with the non-split model in terms of gradient computation.\nMoreover, parameter updates are performed independently on Device 1 and Device 2:\n$w_n^{(k+1)} = w_n^{(k)} - \\eta g_n, \\qquad n = 1,2\\qquad(14)$\nThat is, Device 1 updates the parameter $w_1$ using the computed gradient $g_1$ and the learning rate $\\eta$, while Device 2 updates the parameter $w_2$ using the computed gradient $g_2$ and the learning rate $\\eta$. This update rule is also consistent with the overall parameter update rule in the original model. Therefore, the split model also maintains consistency with the full model in terms of parameter updates.\nConvergence Proof for the Split Model. Based on the above analysis of gradient computation and parameter update consistency, we ensure that the split model is mathematically equivalent to the original non-split model. Next, we will prove the convergence of the split model.\n1) Parameter Update Rules.: In the split model, Devices 1 and 2 update parameters $w_1$ and $w_2$ independently, following the update rule:\n$w_n^{(k+1)} = w_n^{(k)} - \\eta g_n = w_n^{(k)} - \\eta \\nabla_{w_n} L(w_1^{(k)}, w_2^{(k)}), \\qquad n = 1,2\\qquad(15)$\nWe aim to derive the change in the loss function $L(w_1, w_2)$ after each iteration, thereby proving the convergence of the split model.\n2) Loss Function Reduction Using Strong Convexity as-sumption: Based on the mathematical consistency between the complete model and the split model, we employ the assumption of strong convexity:\n$L(w') \\geq L(w) + \\nabla L(w)^T (w'-w) + \\frac{\\mu}{2}||w' - w||^2.\\qquad (16)$\nFor the split model, assuming $L(w_1, w_2)$ is strongly convex with respect to $w_1$ and $w_2$, for any $w^{(k+1)}, w_1^{(k+1)}, w_2^{(k+1)}$ and $w^{(k)}, w_1^{(k)}, w_2^{(k)}$, we have Eq. (17) where $\\mu > 0$ is the strong convexity constant, and this formula describes the decrease in the loss function during each iteration."}, {"title": "VI. EXPERIMENTS & ANALYSIS", "content": "A. Experimental setting\nDevices. We selected four commonly used resource-constrained embedded devices [47], namely NVIDIA Jetson Nano (Nano), NVIDIA Jetson TX1 (TX1), NVIDIA Jetson TX2 (TX2) and NVIDIA Jetson Xavier NX (NX) as experi-mental devices. The system transmission bandwidth is set to 100 Mbps. In each experiment, two of these devices were selected to serve as the computational terminal (Device 1) and the edge server (Device 2) in the IoVT system, arranged in ascending order of computational intensity. Moreover, we use TensorRT, an NVIDIA deep learning acceleration tool, to enhance the inference speed.\nDatasets and models. The experimental datasets include MNIST [48], CIFAR-100 [49], and ImageNet [50]. We utilize the default training and test set splits of these datasets, respectively. During the testing phase, we simulated real-world random event intervals by driving edge inference tasks with user requests following an exponential distribution [51]. This approach allows for a more realistic simulation of system behavior under uncertain conditions, assessing how edge de-vices handle fluctuating request frequencies and enabling us to evaluate system performance and reliability. To verify the gen-eralizability of the proposed method across different models, we selected RepVGG, Rep-ResNet, and Rep-GoogLeNet as the base models, as listed in TABLE II. The following TABLE III compares the computational intensity of the models and devices.\nEvaluate metrics and goals. We employed three key per-formance metrics to evaluate the real-time performance and accuracy of the IoVT system. Throughput: This refers to the number of requests the system can process within a unit of time. A higher throughput indicates that the system can handle more tasks in a shorter period, demonstrating"}, {"title": "VII. CONCLUSION & FUTURE WORK", "content": "We propose a Co-design Framework of Neural Networks and Edge Deployment to realize real-time visual inference for IoVT systems. Specifically, we utilize Roofline analysis to perform partitioned model deployment, and we introduce a re-parameterization-based dynamic model structure to further enhance model-device alignment, maximizing computational performance to increase throughput. Additionally, we em-ploy a multi-objective co-optimization approach to balance throughput and accuracy in the partitioned deployment and network fusion. Extensive experiments demonstrate that, com-pared to benchmark algorithms, our method achieves higher throughput and superior classification accuracy. Furthermore, it has the advantage of generalizability, as it leverages the co-optimization approach to adapt pre-trained models across different devices. This applies well to practical IoVT systems with varying device registration and deregistration scenarios. Additional detection simulation results show that our method enables real-time and accurate detection at the edge of IoVT systems in real-world industrial scenarios, especially for small-scale objects.\nDespite the proposed co-design method's ability to achieve high throughput and accuracy, along with generalizability and practical application value, some limitations remain:\n\u2022 Dependency on Specific Network Structures: The dy-namic model structure relies on the re-parameterization principle for channel fusion with different convolutional kernels. While many neural networks use multi-channel structures, this approach is not universally compatible with all network architectures, significantly restricting its application scope.\n\u2022 Limited Throughput Improvement: Traditional Roofline analysis primarily focuses on the relationship between compute intensity and memory bandwidth, often overlooking dynamic factors such as resource allocation and workload balancing during model-device adaptation."}]}