{"title": "Can LLMs Design Good Questions Based on Context?", "authors": ["Yueheng Zhang", "Xiaoyuan Liu", "Yiyou Sun", "Atheer Alharbi", "Hend Alzahrani", "Basel Alomair", "Dawn Song"], "abstract": "This paper evaluates questions generated by LLMs from context, comparing them to human-generated questions across six dimensions. We introduce an automated LLM-based evaluation method, focusing on aspects like question length, type, context coverage, and answerability. Our findings highlight unique characteristics of LLM-generated questions, contributing insights that can support further research in question quality and downstream applications.", "sections": [{"title": "Introduction", "content": "People have long designed questions with known answers for various purposes, such as education, dialog systems, and model evaluation (Guo et al., 2024). As an automated approach, Question Generation (QG) is a generation task focused on creating relevant questions based on given facts. Numerous automatic QG methods have been proposed in the literature (Duan et al., 2017; Pan et al., 2019). Recent advancements in Large Language Models (LLMs) have greatly improved the performance of NLP tasks, including QG (Es et al., 2023).\nDespite the widespread use of LLMs in QG, few in-depth studies have explored the characteristics of LLM-generated questions. Do LLMs, without additional prompt constraints, prefer longer or shorter questions? What types of questions do they tend to ask? How do LLM-generated questions differ from those created by humans? While existing research (Es et al., 2023) evaluates human alignment using empirical methods, there is a lack of studies comparing the quality criteria of LLM-generated questions to human-generated ones. A deeper understanding of LLM behavior in QG would not only help prevent misuse in inappropriate contexts but also provide valuable insights for optimizing prompt engineering."}, {"title": "Methodology", "content": ""}, {"title": "Question Generation from Context", "content": "Given paragraph string as the context C and a QG instruction prompt P that asks to generate N questions, a LLM M output N questions Q that can be answered with the facts from the context.\nIn this work, we explore LLM-based QG. A direct generation prompt often produces questions that assume the reader understands the context's limited scope or lacks standard answers. Additionally, some generated questions refer directly to the context, making them unanswerable without access to the context. To generate self-contained, independently answerable questions, we use the following system prompt in our setup:\nTo construct the context C, we adapt the Wiki-Text dataset for our setting by splitting it into 860k paragraphs, while preserving section structure as metadata. After filtering out overly short paragraphs and cleaning special characters, we complete the context by incorporating the paragraph text and appending the relevant section titles. A complete example is provided in Figure 1.\nThere are existing datasets that contain human-generated questions also use Wikipedia as context. HotpotQA (Yang et al., 2018) is a human-labeled multi-hop QA dataset, where workers on Mechanical Turk generate questions based on multiple evidence paragraphs from Wikipedia. In contrast, TriviaQA (Joshi et al., 2017) is a QA dataset compiled by trivia enthusiasts, where the questions are created first, and relevant evidence from Wikipedia is then identified to support the answers. Although both datasets incorporate Wikipedia knowledge and human-generated questions, they represent different workflows for introducing evidence: HotpotQA presents context first, with humans generating questions based on it, while TriviaQA starts with the questions and then seeks evidence.\nIn our work, we adopt an automatic, context-dependent approach to QG, similar to HotpotQA's context-first methodology."}, {"title": "Answer-irrelevant Metrics", "content": "The use of LLMs presents new opportunities for more comprehensive evaluation of the text of the question in terms of characteristics and quality. While previous work (Es et al., 2023) has addressed aspects such as faithfulness, answer relevance, and context relevance, we extend this by introducing finer-grained statistical measurements and new evaluation methods. Specifically, we focus on two groups of criteria, totaling six. The first three assess the questions themselves, while the second involves evaluating potential answers that reflect question qualities. We first introduce answer-irrelevant criteria.\nFor humans, the choice of which question to ask is subjective. Here, we explore the types of questions LLMs can generate without additional constraints and compare them with human preferences. Previous work (Yang et al., 2018) introduced heuristic rules, categorizing questions based on their starting words. In this work, we extend this by incorporating a LLM-based classification to analyze question types across ten manually defined categories. These categories were developed by observing a mix of questions from HotpotQA, TriviaQA, and our dataset, followed by automatic balanced grouping using LLMs for insights, and further refined by human review. Appendix A provides detailed definitions for these question types.\nLength is the most straightforward statistical measure of generated questions. In this work, we mainly measure word count numbers. In addition to directly comparing question lengths between human-generated and LLM-generated datasets, we also examine the relationship between question length and question types."}, {"title": "Answer-relevant Metrics", "content": "The quality about the questions are often reflected in their answers. In addition to asking LLM to generate the question, we also use LLM to generate answers according to the context and rate the quality of the answer. Specifically, we adopt the star-rating approach from the previous work (Wang et al., 2023) and use prompts in Appendix B.\nAnswerability. A key quality criterion for a question is whether it can be precisely answered given specific knowledge. In concrete terms, a generated question should be answerable when the context is provided. To assess the answerability of a question, we prompt the LLM to generate an answer using the given context as input. Since answer correctness is also evaluated based on the same context, in most cases, the generated questions are answerable.\nLLMs are trained on widely available common knowledge from the internet. As a result, even when the context is not explicitly provided, the LLM may still answer the question. Evaluating the answer quality without giving the LLM the context thus becomes a way to assess the question's uncommonness relative to its pretraining data. Compared to the answerability evaluation, the key difference is that the context is omitted during answer generation, while other factors remain unchanged. As we demonstrate later, removing the context significantly reduces the answer quality. This also suggests that the generated questions are valuable for evaluating RAG systems or for automated hallucination testing.\nIn addition to question length, the required answer length serves as a more effective measure of how much information is asked. However, due to the nature of generative models, generated answers tend to be longer and include more details. To filter out unnecessary information from the answers generated with context, we implement two strategies to measure the essential answer length. First, we explicitly prompt the model to provide a shortest text answer. Second, we set a word limit and instruct the model to generate answers within that limit. We then evaluate these new answers to determine if they can achieve the same quality rating with fewer words. Our approach significantly reduces answer length, with the second strategy generally performing better. This provides a direct measure of the information needed to answer the question."}, {"title": "Experiments", "content": "We present the evaluation results using two representative LLMs: the closed-source GPT-40 and the open-source LLaMA-3.1-70b-Instruct (Dubey et al., 2024), each generating 1,024 questions with the same set of 256 sampled Wiki contexts (N=4). The complete evaluation uses around 50k chat completion calls. All prompt-based metrics were evaluated using GPT-40 for better alignment with human judgment. For proposed prompt-based evaluation strategies, human annotators are provided with same instructions for alignment checking and the agreement ratio exceeded 80% in general. In terms of answer-rating, we observed an average Pearson correlation of 0.77 between human annotators and the LLM, while the correlation between human annotators themselves was 0.85, indicating a strong positive linear correlation.\nClassified by the predefined question types, Table 1 compares LLM-generated questions with those from human datasets. Based on the comparison, we categorize the question types into three groups and discuss the findings. The first group (T1-T5) focuses on factual checking, with both human and LLM-generated questions heavily represented. We observe a strong preference for asking about specific facts and figures in both LLaMA and GPT models, likely due to the frequent occurrence of numbers and entities in their training data. Questions in the second group (T6-T7) require reasoning across multiple facts from the context. HotpotQA, designed for multi-hop questions, contains a higher number of questions in this category. The third group (T8-T10) involves questions that require more descriptive answers. We observe a strong preference for asking descriptive questions in LLMs. As we discuss later, this preference also leads to longer answers.\nFrom Table 2, we observe that while overall question lengths are quite similar to be around 20 words, different LLMs tend to exhibit distinct preferences for length. Additionally, the length of human-generated questions shows greater variation.\nTable 3 presents the context coverage analysis. We observe that human-generated questions tend to cover more of the context, with consistent results across both sentence-level and word-level measurements. In Figure 2 further investigates which regions of the context LLMs prefer to focus on when generating questions. We observe that while human-generated questions tend to concentrate on the beginning of the context, likely due to the labeling process, LLM-generated questions exhibit a more balanced distribution, with a slight decrease in focus at both ends. This suggests that LLM-based QG displays an almost opposite positional focus compared to QA (Saito et al., 2024).\nBy combining answer generation and rating, Figure 3 shows the rating distribution. We observe that with context, the LLM typically generates satisfactory answers, meeting expectations. Also, performance declines when context is not provided. Notably, about one-fourth of the generated questions cannot be properly answered. Questions generated by GPT-40 show an even higher rate of uncommon questions compared to the human-constructed HotpotQA dataset, highlighting an opportunity for automatic testing in RAG systems or detecting model hallucinations.\nAs shown in Table 4, LLM-generated answers are generally much longer than human-labeled golden answers, likely due to the nature of generative models. To more accurately measure the amount of information required, we compress the LLM-generated answers to obtain a minimum-length version that maintains the same rating. While this compression does reduce answer length, LLM-generated questions still require significantly longer answers. Further analysis, breaking down answer length by question type, reveals that descriptive questions are the primary reason for longer answers."}, {"title": "Limitations", "content": "This paper investigates LLM-generated questions using representative models in default settings. However, many potential variations remain unexplored. Future research could expand to include additional models and employ more sophisticated sampling algorithms to improve robustness in specific generated instances.\nWhile we utilized a single set of prompts sufficient for our observations, in practice, users may adjust prompts for more targeted question generation. The goal of this work is to highlight that, similar to LLM behavior analysis in QA, there are significant differences between human-generated and LLM-generated questions in QG tasks. The findings in this paper serve as a case study and provide insights for future prompt engineering.\nDue to resource constraints, we only evaluated human alignment on a sampled dataset and did not incorporate large-scale human annotation using public resources such as Mechanical Turk. Future studies that integrate more extensive human annotations and cross-checking could provide more accurate quantitative measurements of human-LLM differences.\nAlthough this paper focuses on general QG settings, it would also be valuable to explore LLM-based QG for specific downstream tasks, such as RAG system evaluation (Es et al., 2023) or hallucination detection. We expect future research to provide more detailed application-specific analyses. Similarly, exploring QG using context from specialized domains, such as finance or medical texts, could enhance the application of QG in domain-specific education."}, {"title": "APPENDIX", "content": ""}, {"title": "Question type definitions", "content": "1. Verification/Affirmation Questions: These questions ask for confirmation about the equivalence or relationship between two or more entities. They often use formats like \"Are...?\"or \"Which...?\"\n2. Specific Fact and Figure Questions: These questions request a specific quantitative or qualitative fact. They are straightforward and seek concrete data or a precise answer, often involving numbers or specific details.\n3. Identity and Attribution Questions: These inquiries focus on identifying a person or entity responsible for an action or associated with a work. They tend to ask \"Who...?\" or refer to persons or origins related to a context.\n4. Which/What-Based General Knowledge Questions: This group contains questions that start with \"Which\" or \"What\" and inquire about general knowledge, often requiring a selection from a set or identification of a type/category.\n5. Event/Outcome Questions: These questions inquire about the outcome of specific events or actions, focusing on consequences or results. They often address changes, damages, or effects.\n6. Sequential/Ordering/Causation Questions: These questions require identifying a sequence, comparison, or causation among entities, often using terms like \"first,\" \"before,\" \"between,\" etc.\n7. Location-Based Questions: These questions focus on identifying a geographic location or specific place where something is based or occurs.\n8. Descriptive/Characterization Questions: These questions seek an explanation or characterization of entities, often requiring a description of how or why something is the way it is, involving traits or actions.\n9. Comparison and Selection Questions: Questions in this group involve comparing two entities to determine which one holds a particular status or characteristic, often using formats like \"Between X and Y, who/which is...?\"\n10. Classification and Categorization Questions: These inquiries request the classification or categorical identity of entities or things, often seeking to place an item within a broader group or category."}, {"title": "Answer generation and rating system prompt", "content": "Answer Generation. You are to generate a short answer based on the following question and an optional supporting fact.\nAnswer Rating. You are to rate the following answer to a question, taking into account any optional supporting facts provided. Assign a rating from 0 to 5 based on the criteria below: 0: No answer or completely irrelevant 1: Significantly incorrect or incomplete 2: Partially correct; major inaccuracies or omissions 3: Correct but lacks depth; minimal detail 4: Mostly correct; minor errors; includes relevant details 5: Fully accurate and detailed; clear and comprehensive Your response should consist of two lines: The rating from 0 to 5. A brief justification for your rating."}, {"title": "Code Availability", "content": "The source code supporting the findings of this study is publicly available at our GitHub repository:\nhttps://github.com/CoLearn-Dev/1lmqg"}]}