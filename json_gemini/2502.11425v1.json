{"title": "Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models", "authors": ["Jongho Kim", "Seung-won Hwang"], "abstract": "Despite the advanced capabilities of large language models (LLMs), their temporal reasoning ability remains underdeveloped. Prior works have highlighted this limitation, particularly in maintaining temporal consistency when understanding events. For example, models often confuse mutually exclusive temporal relations like \"before\" and \"after\" between events and make inconsistent predictions. In this work, we tackle the issue of temporal inconsistency in LLMs by proposing a novel counterfactual prompting approach. Our method generates counterfactual questions and enforces collective constraints, enhancing the model's consistency. We evaluate our method on multiple datasets, demonstrating significant improvements in event ordering for explicit and implicit events and temporal commonsense understanding by effectively addressing temporal inconsistencies.", "sections": [{"title": "1 Introduction", "content": "Despite the impressive capabilities of LLMs, a line of research (Jain et al., 2023; Chu et al., 2023) has highlighted that these models often lack temporal reasoning abilities. This is especially true for relative event understanding, where the goal is to infer temporal relationships between events or properties within an event in the passage, without depending on absolute time indicators (e.g., specific dates). The primary challenge is that LLMs lack temporal consistency in their responses (Qiu et al., 2024; Chen et al., 2024). Temporal consistency is defined as the model's ability to ensure that conflicting timelines do not co-exist. For instance in Figure 1-(a), if the model is temporally inconsistent, mutually exclusive temporal relations like \u201cbefore\" and \"after\" are sometimes confused when ordering events, leading to contradictory predictions\u2014such as stating that Event A happens both before and after Event B in the same context. Considering temporal consistency is fundamental in temporal reasoning, its absence in LLM can undermine key tasks like planning (Sakaguchi et al., 2021; Zhang et al., 2024). These observations highlight the need for alternative reasoning skills to achieve temporal consistency. This study answers the following research question: Can we prompt LLMs to elicit the ability to mitigate temporal inconsistency? Inspired by counterfactual augmentation, where models are exposed with lexically similar, but typically label-flipping pairs in training (Kaushik et al., 2020), we extend it to LLMs to generate temporally counterfactual questions: We introduce lexically small interventions to the original input (e.g. before to after, years to centuries) that drastically affect its temporal semantics. By providing these questions and self-generated answers alongside the original input, the model would rely less on lexical similarities and better understand the semantics. To this end, we propose a novel counterfactual-consistency prompting (CCP), designed to enhance the temporal consistency of LLMs, as described in Figure 1-(b). CCP first generates temporal counterfactual exemplars and then applies the insights gained to address the original temporal question. This method is particularly effective in relative event understanding because the counterfactual exemplars not only encourage the model to understand different temporal semantics but also directly impose temporal constraints. For instance, if the model states that \"Event A happens after Event B\" and also recognizes that \u201cEvent A happens before Event B\", the conflict forces the model to collectively re-weight the validity of these two statements. We show performance gain of CCP across multiple relative event understanding tasks. Our effectiveness in mitigating temporal inconsistencies is further demonstrated by our inconsistency metric."}, {"title": "2 Method", "content": "Our idea is to make the model approximate the temporal constraints using counterfactuals. Because temporal reasoning imposes unique interdependencies, where one temporal aspect affects another (Han et al., 2019; Kim et al., 2024), counterfactuals enable us to capture these constraints. For example, if the model establishes from a counterfactual exemplar that \"Event e\u2081 happens [r2: before] Event e2\", it is constrained to predict the original question that \u201cEvent e\u2081 cannot happen [r1: after] Event e2\":\n$$r_2(e_1, e_2) \\in V \\Rightarrow r_1(e_1, e_2) \\notin V$$\nwhere r(ea, eb) represents the temporal relation r between events ea and eb, and V represents the set of coherent temporal relations with the context."}, {"title": "2.1 Generating Temporally Counterfactual Questions", "content": "Given a context C, our task is to provide an accurate answer to question Q while maintaining temporal consistency. We start by creating temporally counterfactual questions, $Q_{c_1}...Q_{c_i}$. Counterfactual augmentation conventionally aims to generate instances with lexically minimal edits while keeping others unchanged (Huang et al., 2019; Kaushik et al., 2020; Wang and Culotta, 2020). This strategy discourages models from relying too much on superficial similarity. However, previous works (Kaushik et al., 2020) require arbitrary label-flipping edits, which are unsuitable for LLM inference with unknown test labels. In contrast, we focus on temporally counterfactual questions that specifically edit the temporal semantics of the original sentence. Our approach ensures that the model adheres to the \"temporal constraints\", yet retains the effect of label-flipping that emphasizes temporal cues over superficial similarity. Specifically, we set the types of counterfactual questions based on the temporal semantics each dataset aims to capture. The types of temporal counterfactuals are listed in Table 1. For sentences representing temporal relation between two events (r1(e1, e2)), we substitute the relation r\u2081 to r2, or event e2 to e3. For those representing an event's temporal properties (r1(e\u2081)) such as duration or stationarity, we substitute the property to r\u2082 or negate it to -r1. To generate counterfactual questions, we design our model to dynamically create them rather than rely on a predefined rule-based template. While rule-based approaches like Chen et al. (2024) in logical reasoning constrain answers using a predefined question set, they limit flexibility to cover the broad range of temporal expressions. By comparison, our dynamic generation of constraints provides a more adaptable solution. We specify various counterfactual types through in-context learning (ICL) to control the relevance of these generated questions. The full prompts are in Appendix G.1."}, {"title": "2.2 Counterfactual-Consistency Prompting", "content": "After generating the counterfactual questions, we prompt the model again to produce predictions for counterfactual $Y_{c_1}, ..., Y_{c_n}$. However, there is a risk when LLMs may fail to answer the counterfactual questions correctly. In this case, their direct use propagates errors to the original question. As a proxy for determining whether the generated prediction can be trusted, existing works aggregate multiple predictions of the same question (Wang et al., 2023; Du et al., 2024). Formally, the refined prediction Y is derived by re-weighting the probability distribution P of previous predictions Y1, ..., Yn from the same question as: P(Y) = f(P(Y1), ..., P(Yn)) where f is an aggregation function such as majority voting or LLM itself. Though, they can lead to errors as they solely rely on feedback from a single question. Our distinction is to aggregate predictions from both the original and counterfactual questions. We design the model to re-weight the counterfactual answer distributions across the questions.\n$$P(Y) = f(P(Q, Y), P(Q_{c_1}, Y^{c_1}), ..., P(Q_{c_n}, Y^{c_n}))$$\nFor instance, even if the model wrongly predicts the relation as 'after' in a counterfactual, collectively considering the possibility of the relation 'before' can re-weight the effect of the constraint. The prompts are provided in Appendix G.2. This re-evaluation approach improves robustness against potential errors in generated answers. The second analysis in Subsection 3.5 shows such self-correction outperforms a baseline directly leveraging counterfactuals without aggregation."}, {"title": "3 Experiments", "content": "Among publicly available datasets, we selected three based on two criteria: (1) the task focuses on relative event understanding without absolute time indicators, and (2) the temporal inconsistency on the dataset can be evaluated. TempEvalQA-Bi (Qiu et al., 2024) involves ordering two explicit events in time, assessing temporal consistency in mutually exclusive question pairs. TRACIE (Zhou et al., 2021) expands the event ordering to implicit events, testing if the hypothesis logically follows the story. We finally added MCTACO (Zhou et al., 2019) considering the diverse event-related temporal properties. The dataset covers broader aspects like event duration or frequency. We modified the multiple-choice setting of MCTACO into a binary question-answering task for consistency evaluation, presenting each answer candidate separately to determine if it fits the context. Dataset statistics and examples are in Appendix A."}, {"title": "3.2 Metrics", "content": "Along with accuracy (ACC) and F1 scores to assess overall performance, we introduce the inconsistency metric (INC) as a main evaluation measure for temporal inconsistency. We define the INC as the percentage of inconsistent predictions. An inconsistency is counted when at least one incorrect answer is found within a group of minimally dissimilar questions with slight modifications in their temporal semantics, while all other aspects remain unchanged. TempEvalQA-Bi directly provides this metric. For TRACIE, we manually group questions that are counterfactual to each other. We adapt INC in MCTACO by grouping original multiple-choice candidates by question."}, {"title": "3.3 Evaluation Settings and Baselines", "content": "For models, we used open-source LLM Llama-3 8B and 70B (AI@Meta, 2024), and API-based LLM GPT-40-mini and GPT-40 (OpenAI et al., 2024). For baselines, we first compare CCP with standard prompting (SP) that directly answers the question without intermediate steps, and CoT, which incorporates step-by-step reasoning to derive the answer. Next, we consider methods that aggregate multiple predictions of the same question. Self-Consistency (Wang et al., 2023) predicts one question multiple times and performs majority voting. Self-Reflect methods (Madaan et al., 2024; Shinn et al., 2024) iteratively refine own predictions. Multi-agent Debate (Du et al., 2024) leverages both majority vote and reflection. We employ a 3-shot setting across all configurations. More details on evaluation settings are in Appendix B."}, {"title": "3.4 Main results", "content": "Table 2 highlights the performance of our method compared to baseline methods on relative event understanding tasks. Compared to SP, the CoT baseline is not usually effective and often worsens performance. Advanced baselines, Consistency, Reflect, and Debate, also fail to consistently reduce inconsistencies or achieve competitive accuracy. In contrast, CCP steadily outperforms these baselines, significantly reducing temporal inconsistencies across all datasets and achieving notable improvements in ACC and F1 scores. The full results on other models are available in Table 5."}, {"title": "3.5 Analysis", "content": "Creating counterfactual questions by generation handles diverse temporal aspects. We compared our generative setting with the Retrieved Questions (Ret.Q) approach, where counterfactual questions were retrieved from other questions within the same question group. We evaluated the methods on MCTACO, which covers various aspects of event reasoning. Figure 2 shows that generating counterfactual questions proved more effective for all temporal types. These results suggest that our method performs better in event understanding with diverse relations, where the dataset cannot often provide high-quality counterfactual questions. Notably, CCP outperforms the Ret.Q baseline even though our method may produce incorrect questions. Also, CCP is more practical since Ret.Q assumes the questions in the test set are observed. CCP is robust against wrong counterfactual exemplars. We conducted a comparative analysis of two methods: Direct Answering (Dir.A), which involves answering directly from counterfactual exemplars, versus CCP which leverages the aggregation step to re-evaluate them. We conducted experiments on TempEvalQA-Bi and TRACIE, where before-after relations ensure that identifying a counterfactual answer is sufficient to determine the original. We excluded MCTACO since its counterfactual answers do not always determine the validity of the original answer. In the Dir.A implementation, the answer to the counterfactual question is flipped and directly used as the response to the original question. The results in Figure 3 demonstrate that CCP consistently outperforms Dir.A, supporting our robustness by the collective evaluation."}, {"title": "4 Conclusion", "content": "We targeted the temporal inconsistency in relative event understanding with LLMs by proposing a prompting approach using counterfactual questions. This encourages the model to focus more on the temporal aspects and collectively evaluate its answer with imposed constraints. Experiments with the INC metric show that our approach mitigates inconsistency and improves overall performance."}, {"title": "5 Limitation", "content": "Our method showed limited performance improvement when time indicators, such as specific years (e.g., 1980), are involved in temporal understanding. This is implied from our evaluations on event-time ordering and time-time ordering tasks, as shown in Appendix E.1. The findings suggest that arithmetic reasoning is essential for grounding timelines with absolute time indicators, as emphasized in prior studies (Su et al., 2024; Zhu et al., 2023). Another limitation is that we focused on pointwise and pairwise event reasoning to highlight the model's struggles with basic temporal reasoning due to consistency issues. We anticipate future work expanding our approach to more complex list-wise ordering like event schema prediction (Zhang et al., 2024). Finally, the effectiveness of LLMs can be impacted by the specific wording of prompts (Jiang et al., 2020). For example, Appendix E.2 shows that, although our revised prompts generally yield better results than CoT, CCP experienced a slight decrease in performance."}, {"title": "Appendices", "content": "Table 3 summarizes the dataset statistics used in this study. The numbers of official test samples are reported. Due to the budget, we evaluated Llama-3-8B on the full test set, GPT-40-mini and Llama-3-70B on a random sample of up to 2,000 test set instances, and GPT-40 on 1,000 test set instances. Additionally, the number of temporal relations considered in each dataset is included in Table 3. TempEvalQA-Bi and TRACIE focus mainly on the before-after relation. MCTACO includes diverse temporal relations, and the number of annotated candidates is reported. The questions in MCTACO are categorized into 5 question types, and examples for each type are provided in Figure 4.\nTable 4 demonstrates counterfactual types and examples addressed across the targeted temporal aspects in our datasets. Among the five temporal aspects in the MCTACO dataset, we generate counterfactuals for duration, frequency, and typical time in the same way by intervening in the temporal property, modifying r\u2081(el) to r\u2082(el)."}, {"title": "B Details of Evaluation Settings", "content": "This section outlines the detailed evaluation settings, including hyperparameters, resources, efficiency, and parsing methods. We use greedy decoding for SP, CoT, and CCP. For Consistency, Reflect, and Debate, we adopt the approach from Wang et al. (2023), employing top-k sampling with k = 40 and a temperature of 0.5 for the LLaMA model. For GPT-based models, we set the temperature to 0.7. Consistency samples 40 outputs from the decoder. Reflect refines the output iteratively for two iterations, including the initial output. In Debate, three agents engage in a debate over two rounds(Du et al., 2024). The implementations of the latter two baselines (Reflect, Debate) are based on the GitHub repository 1 from Du et al. (2024). Single-run performances are reported. We note that our method prompts 3 times: for counterfactual question generation, counterfactual answer generation, and original question's answer generation, whose efficiency is compatible with or even more efficient than the three baselines. We also note that the Consistency baseline of Llama-3-70B cannot be reported due to its computation inefficiency. For resources, we used the Transformers library (Wolf et al., 2020) and vLLM (Kwon et al., 2023) with 4 RTX A6000 GPUs for Llama-3 models. We used Openai API 2 for GPT models. For output parsing, the models generate the final answer after the phrase \"Final answer:\". Counterfactual exemplars are generated by modifying each dataset's questions, hypotheses, and candidate answers."}, {"title": "C Details of Main Results", "content": "Table 5 shows the performance of our method compared with baseline methods on relative event understanding tasks. The results show that our method outperforms the baselines across the board."}, {"title": "C.2 Task generalizability", "content": "To demonstrate that our solution extends beyond binary question answering to multiple-choice question answering (MCQA), we evaluated the performance of GPT models using the original MCTACO evaluation setting (Zhou et al., 2019). While our primary evaluation decomposed the multiple-choice format into binary questions to measure inconsistency, it can be reconstructed for multiple-choice evaluation. We additionally introduced a baseline for MCQA (MCQA-CoT) that provides the context, question, and all candidate answers, generating one or more correct answers step-by-step. The results in Table 6 indicate that our method (CCP) outperforms the MCQA-CoT baseline on multiple-choice tasks, demonstrating its effectiveness in the MCQA setting."}, {"title": "D Further Analysis", "content": "We tested whether our claim in Figure 2 can be generalized to other models. Figure 5 consistently confirms that creating counterfactual questions by generation handles diverse temporal relations better than retrieving questions across different models."}, {"title": "D.2 Number of In-context Learning Examples", "content": "Our approach inevitably introduces additional counterfactual examples during in-context learning (ICL), leading to a higher total number of shots compared to the baseline. To ensure a more competitive baseline, we increased the total number of shots in the baseline. In the MCTACO dataset and with the Llama model, we additionally experimented with the 12-shot CoT, which includes 12 passage (P)-question (Q)-candidate (C) pairs, and compared them with our 3-shot. We note that our 3-shot examples include 3 passage-question pairs and 11 candidates."}, {"title": "E Details of Limitations", "content": "Table 8 shows the experimental results for the tasks requiring the understanding of temporal indicators. We evaluated our method on TimeQA (Chen et al., 2021), the event-time ordering task, and TimexNLI-T1 (Thukral et al., 2021), the time-time ordering task, where CCP showed limited performance gains."}, {"title": "E.2 Prompt Sensitivity", "content": "Our key decision in the prompt design was to separate the counterfactual question generation prompt (CCP), described in Appendix G.1, from the prompt that answers counterfactual and original questions, detailed in Appendix G.2. If we generate counterfactual questions and answers in an end-to-end manner using only the prompt in Appendix G.2 (CCP(e2e)), while it still outperforms CoT, the performance slightly decreases, as shown in Table 9."}, {"title": "F Usage of AI Assistants", "content": "ChatGPT was employed to generate answers in the prompt examples."}, {"title": "G Prompt Templates", "content": "We list the prompts that we used."}, {"title": "G.1 Prompt Templates for Generating Counterfactual Questions", "content": "To generate the counterfactual questions, we use the prompts provided below. We control question aspects by tailoring ICL examples and prompting the model to follow few-shot examples with the prompt: (\"following previous examples\")."}, {"title": "H Scientific Artifacts", "content": "We used existing scientific artifacts for research purposes, and the use of existing artifacts was consistent with their intended applications."}]}