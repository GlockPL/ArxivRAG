{"title": "ATMOSSCI-BENCH: Evaluating the Recent\nAdvance of Large Language Model for Atmospheric Science", "authors": ["Chenyue Li", "Wen Deng", "Mengqian Lu", "Binhang Yuan"], "abstract": "The rapid advancements in large language models (LLMs), particularly in their reasoning capabilities,\nhold transformative potential for addressing complex challenges in atmospheric science. However,\nleveraging LLMs effectively in this domain requires a robust and comprehensive evaluation bench-\nmark. To address this need, we present ATMOSSCI-BENCH, a novel benchmark designed to systemati-\ncally assess LLM performance across five core categories of atmospheric science problems: hydrology,\natmospheric dynamics, atmospheric physics, geophysics, and physical oceanography. We employ a\ntemplate-based question generation framework, enabling scalable and diverse multiple-choice ques-\ntions curated from graduate-level atmospheric science problems. We conduct a comprehensive eval-\nuation of representative LLMs, categorized into four groups: instruction-tuned models, advanced\nreasoning models, math-augmented models, and domain-specific climate models. Our analysis\nprovides some interesting insights into the reasoning and problem-solving capabilities of LLMs in\natmospheric science. We believe ATMOSSCI-BENCH can serve as a critical step toward advancing LLM\napplications in climate service by offering a standard and rigorous evaluation framework. Our source\ncodes are currently available at [https://github.com/Relaxed-System-Lab/AtmosSci-Bench].", "sections": [{"title": "Introduction", "content": "The rapid advancement of large language models (LLMs) [1], especially in their reasoning capabilities, offers\ntransformative potential for addressing complex challenges in atmospheric science [2, 3, 4, 5]. However, the\ndevelopment of reliable and effective LLM-based applications for climate-related tasks requires a robust and\ncomprehensive evaluation framework. Such a benchmark is essential to systematically assess the performance of\nLLMs across a diverse array of atmospheric science problems, ensuring their utility, accuracy, and robustness in\nthis critical domain.\nConstructing a comprehensive benchmark for atmospheric science is crucial to harness the recent advancements\nin large language models (LLMs) for diverse climate service applications [3]. Note that atmospheric science\npresents unique and complex challenges, ranging from micro-scale processes like cloud dynamics to global-scale\nclimate systems. To ensure that LLMs can effectively contribute to solving these real-world problems, it is\nessential to establish a benchmark that evaluates their performance, especially their reasoning and interpretative\nabilities\nSuch a well-designed benchmark will not only foster innovation but also provide a standardized\nframework for assessing the utility, accuracy, and robustness of LLMs in this field.\nAtmospheric science problems differ significantly from the mathematical and physical problems commonly found\nin existing LLM benchmarks [6, 7]. This field is inherently interdisciplinary, requiring the integration of theoret-\nical knowledge with real-world phenomena. Atmospheric science involves analyzing and synthesizing heteroge-\nneous data types, such as spatial coordinates, temperatures, wind patterns, and empirical estimates, which are\noften presented in varied formats and units. Furthermore, solving these problems necessitates the selection of"}, {"title": "Related Work", "content": "LLM advances. LLMs, such as OPT [8], LLAMA [9], GPT [10], GEMINI [11], CLAUDE [12], and MIXTRAL[13],\nhave demonstrated remarkable performance across a wide range of applications. While general-purpose LLMs\nexhibit strong adaptability, domain-specific models have also been developed to enhance performance in\nspecialized fields. In the context of atmospheric science, climate-focused LLMs such as CLIMATEBERT [14],\nCLIMATEGPT [4], and CLIMAX [15] are designed to address the unique challenges of climate modeling\nand analysis, which illustrates a promising paradigm different from traditional approaches that designing a\nspecific model for some particular task [16, 17, 18, 19, 20]. More recently, reasoning models, including GPT-\n01 [21], GEMINI-2.0-FLASH-THINKING [22], QwQ [23], and DEEPSEEK-R1 [24], have emerged, highlighting\nadvancements in mathematical and scientific problem-solving. These models leverage sophisticated reasoning\ntechniques, presenting exciting opportunities for tackling complex challenges in atmospheric science.\nLLM benchmarks. Assessing LLMs is crucial for ensuring their effectiveness in deployment across various\ndomains [25]. Traditional benchmarks like GSM8K [26] and MATH [6] have become less effective as state-of-\nthe-art models achieve near-perfect scores, necessitating more challenging benchmarks to evaluate reasoning\ncapabilities accurately. Recent benchmarks target specialized fields, such as GPQA-Diamond [27] for expert-level\nscience, AIME2024 [28] for advanced mathematics, and SCIBENCH [7] for collegiate science problems. However,\na comprehensive LLM benchmark for atmospheric science remains underrepresented, where CLIMAQA [29]\nonly offers basic definition-based assessments, lacking depth in evaluating complex problem-solving abilities.\nDesigning a good LLM benchmark requires principled guidance to ensure robust, accurate, and meaningful"}, {"title": "Benchmark Construction", "content": "We introduce a comprehensive multiple-choice question (MCQ) benchmark, AtmosScI-BENCH, specifically\ndesigned for atmospheric science to enable more effective evaluation of LLMs. Unlike traditional metrics such\nas exact match, BLEU, or F1 scores, which primarily assess superficial similarity, MCQs offer well-defined\nanswer choices, reducing ambiguity and enabling a more precise assessment of model comprehension and\nlogical inference [35]. This structured format ensures a more robust evaluation of LLMs' capabilities in tackling\natmospheric science challenges.\nDesign principles: To ensure a rigorous evaluation of LLMs in atmospheric science, we adhere to a set of\nwell-defined principles that emphasize reasoning and interpretative abilities:\nAtmospheric science is governed by fundamental physical\nequations, and a meaningful evaluation requires that LLMs not only recall these principles but also apply\nthem appropriately in the corresponding contexts. Thus, the questions should be designed to assess both\nconceptual comprehension and the ability to use these equations in problem-solving, ensuring the benchmark\nmeasures true scientific reasoning rather than mere memorization.\nMany real-world atmospheric problems require synthesizing informa-\ntion from multiple sources, integrating equations, and applying multi-step logical reasoning. To reflect these\nchallenges, benchmark questions should be crafted to go beyond simple recall, testing the model's ability\nto handle intricate reasoning and dynamic problem-solving scenarios inherent to the field.\nAccurate numerical computation is essential for scientific dis-\nciplines, where correct reasoning leads to fixed, verifiable answers. By incorporating numerical problems,\nwe provide a structured and objective evaluation framework, eliminating ambiguities in assessment. This\napproach also enables seamless integration of reasoning tasks, extending the benchmark's scope to evaluate\nmathematical intuition and computational fluency."}, {"title": "Data Source and Preprocessing", "content": "To ensure the rigor and relevance of the benchmark, we curated questions from authoritative graduate-level\ntextbooks and exam materials widely recognized in atmospheric science education. These sources provide\nhigh-quality, well-established content that aligns with the complexity and depth required for evaluating LLMs\nin this domain.\nWe leverage Mathpix OCR [36] to extract both questions and their corresponding explanations from the collected\nmaterials. For multi-part problems or sequential questions where solving one step is necessary to proceed to the\nnext, we consolidated them into single questions to enhance the complexity and depth of reasoning required.\nThis approach preserves the logical progression of problem-solving, ensuring a comprehensive assessment of\nmodel capabilities. The benchmark covers distinct sub-fields of atmospheric science, each representing a key\nsubject:"}, {"title": "Question Generation Framework", "content": "To rigorously evaluate the reasoning and problem-solving capabilities of LLMs in atmospheric science, we\nemploy symbolic MCQ generation techniques inspired by the GSM-Symbolic framework [30], enhanced with a\nrule-based mechanism. This approach enables the creation of scalable and diverse question sets while ensuring\nlogical coherence and alignment with real-world physical laws. Instead of fixed numerical values, we also design\na template-based question perturbation mechanism with placeholder variables, which can be systematically\ninstantiated through symbolic extensions. This ensures that models are tested on genuine reasoning ability\nrather than pattern matching from the potentially contaminated training data. \nQuestion template construction: We invite domain experts in atmospheric science to systematically transform\nselected questions (OCR extracted) into reusable templates. The experts manually identify numerical values\nwithin each question and replace them with variable placeholders, ensuring flexibility for symbolic instan-\ntiation. These variable placeholders, highlighted in different colors in Figure 1, allow for systematic variation\nwhile preserving the original scientific integrity of the problem.\nNumerical assignment in question template: We design a rule-based mechanism for valid numerical assignments\nin each question template. Note that many variables in atmospheric science problems are interdependent,\nmeaning that the inappropriate assignment of some value(s) could lead to unrealistic or invalid physical sce-\nnarios. To fulfill this requirement, we ask the experts for each question template to define: (i) a valid numerical\nrange (min, max) for each variable to ensure scientifically plausible values; (ii) a granularity parameter (i.e.,\nthe smallest step size between values) to control precision, allowing for variation in significant digits - this\nvariation affects numerical representation, potentially influencing LLM arithmetic performance [33, 32, 34];\nand (iii) a set of rule-based constraints that are manually implemented to enforce logical dependencies (e.g.,\nin Figure 1, ensuring $t_1<t_2$). We believe these manual configurations ensure that all generated instances\nremain scientifically valid while allowing systematic variation in numerical representation.\nAutomatic problem solver to support value perturbation: For each question, we utilize GPT-40 to generate an ini-\ntial Python implementation based on the corresponding explanatory materials (e.g., textbook solutions). This\nsynthesized solution is then manually reviewed, verified, and refined by experts to ensure correctness and ad-\nherence to the intended problem-solving methodology. Once validated, the solver can automatically compute\nthe correct answer for any given set of valid input variables, ensuring consistency and scalability in question\ngeneration. Note that to ensure consistency, accuracy, and alignment with real-world scientific standards, we\nalso manually assign appropriate units and define significant digits for rounding the final answer in each au-\ntomatic problem solver. This standardization maintains numerical precision while preventing inconsistencies\nin representation, ensuring that generated answers adhere to established atmospheric science conventions.\nIncorrect option generation: To effectively assess LLM reasoning, multiple-choice questions require plausible\nbut incorrect distracting options that challenge the model's understanding while avoiding trivial elimination\nstrategies [37]. We design the following mechanisms to generate incorrect options: (i) producing an incor-\nrect answer by randomly swapping two variables in the computation; (ii) altering a single variable in the\nequation to generate a close but incorrect result; (iii) randomly assigning all variables within their predefined"}, {"title": "Evaluation Setup", "content": "We design three main experiments to assess LLM performance on our benchmark, focusing on comprehensive\nperformance comparison among various LLMs (Q1), reasoning ability variations for the tasks (Q2), and robust-\nness of the benchmark results for real-world deployment (Q3). We enumerate these concrete questions below:\nQ1. How do various state-of-the-art LLMs (i.e., falling into different categories of instruction, math, reasoning,"}, {"title": "Constructed Benchmark Dataset", "content": "To answer the above three questions and systematically evaluate LLMs on atmospheric science tasks, we leverage\nour question generation framework consisting of 67 question templates to construct a concrete benchmark\ndataset. As we mentioned in Section 3.3, each template supports multiple variations, ensuring a diverse and\nscalable question set. We consider three levels of significant digits\u2014Low, Standard, and High\u2014to analyze\nthe impact of numerical precision on LLM performance. To answer Q1 (assessing the overall performance of\nvarious LLM categories) and Q2 (inference scaling of reasoning models), we construct ATMOSSCI-BENCH10,\nwhere 10 question test sets are generated, with each test set being constructed from all question templates\nwhile maintaining predefined significant digits (Standard). To investigate model robustness Q3, we construct\nadditional test sets: (i) to investigate the influence of scientific numerical precision, we generate two addi-\ntional ATMOSSCI-BENCH10, each varying the level of significant digits (Low, High) along with Standard level\nto measure the effect of numerical precision; (ii) to evaluate the robustness under symbolic variation, we\ngenerate ATMOSSCI-BENCH30, which consists of 30 test sets for each question template, with controlled symbolic\nvariations to analyze sensitivity to numerical perturbations."}, {"title": "Benchmark Models", "content": "To comprehensively assess LLM performance in atmospheric science, we include state-of-the-art LLMs falling into\nfour categories: (i) instruction models, (ii) reasoning models, (iii) math models, and (iv) domain-specific mod-\nels. This categorization enables a structured comparison of general-purpose, specialized, and domain-adapted\nmodels.\nInstruction models. Instruction-tuned models serve as strong general-purpose baselines, optimized for\nfollowing prompts and single-step inference tasks, where we include:\nOpenAI's instruction-tuned models.\nInstruction-tuned Qwen models with enhanced abilities.\nGoogle's open-weight instruction models; along with Gemini-2.0-\nthe powerful Gemini model optimized for efficiency.\nMeta's widely used instruction models.\nDeepseek's latest MoE-based instruction model for general tasks.\nMath models. Mathematical LLMs specialize in problem-solving, computational reasoning, and theorem\nproving\nsuch ability is essential for atmospheric problems. Towards this end, we include:\nDeepseek's math-focused models trained\nfor theorem proving.\nQwen's recent models optimized for mathematics.\nReasoning models. Reasoning ability is the core technique to improve LLMs' performance over complicated\ntasks. We include the recent advanced reasoning models focus on deep logical reasoning and multi-step\nproblem-solving:\nOpenAI's reasoning-optimized model.\nReasoning model based on Qwen2.5-32B.\nExtended Gemini-2.0-Flash-Exp for enhanced reasoning.\nDeepseek's RL-trained model for complex problem-solving."}, {"title": "Evaluation Results and Discussion", "content": "Experimental setup. To comprehensively evaluate the performance of four categories of LLMs on atmospheric\nscience tasks and assess whether ATMOSSCI-BENCH provides a sufficiently challenging and discriminative eval-\nuation framework, we conduct a systematic performance comparison using our ATMOSSCI-BENCH10 benchmark\nacross four representative LLM categories introduced in Section 4. We standardize experimental settings for each\ncategory as: (i) Reasoning models use 32K max context length, including the reasoning tokens; (ii) Instruction\nand math models use 8K max output tokens, balancing response quality and efficiency; (iii) Domain-specific\nmodels are set to 4K context length, the maximum capacity they support. By controlling these variables, we\nensure that performance differences reflect genuine capability gaps rather than confounding factors, allowing\nus to validate whether ATMOSSCI-BENCH effectively differentiates model performance and highlights reasoning\nproficiency.\nResults and analysis. We present accuracy across different atmospheric science tasks, along with an overall\nperformance comparison in Table 1 with three key observations:"}, {"title": "Inference Scaling for Reasoning Models", "content": "Experimental setup. To answer Q2, i.e., whether increasing the length of reasoning tokens improves the\nperformance of reasoning models, we conduct an inference time scaling evaluation on ATMOSSCI-BENCH10\nusing the QwQ-32B-PREVIEW model, varying its reasoning token limits from 4K up to 32K. By systematically\nincreasing the token limit, we aim to determine whether a longer inference process leads to higher accuracy\nand whether there exists an optimal threshold beyond which additional tokens provide minimal benefit.\nResults and analysis. As shown in Figure 2, increasing the reasoning token limit generally improves model\naccuracy, but the gains diminish beyond a certain threshold. Across all evaluated metrics, including overall\naccuracy, performance is consistently lower at 4K tokens, improves significantly at 8K and 16K tokens, and then\nplateaus beyond 16K tokens, with 32K tokens offering only marginal improvement. This trend suggests that\nwhile extending reasoning length enhances model performance up to a certain point, it further increases yield,\ndiminishing returns without proportional accuracy gains. Thus, our answer to Q2 is that increasing the length of\nreasoning tokens improves model accuracy up to 16K tokens, beyond which performance gains diminish, indicating\nan optimal threshold for inference time scaling."}, {"title": "Robustness of ATMOSSCI-BENCH", "content": "To evaluate the robustness of AtmosSCI-BENCH (Q3), we conduct experiments to assess: (i) robustness against\nvariations in numerical precision and (ii) robustness to different degrees of perturbation introduced by symbolic\nvariation."}, {"title": "Conclusion", "content": "In this paper, we introduced AtmosSCI-BENCH, a novel benchmark designed to systematically evaluate the\nreasoning and problem-solving capabilities of LLMs in atmospheric science. Our benchmark covers five core\ncategories-hydrology, atmospheric dynamics, atmospheric physics, geophysics, and physical oceanogra-\nphy-through a scalable, template-based question generation framework that ensures diversity and complexity\nin multiple-choice question assessments. By conducting a comprehensive evaluation across four distinct model\ncategories-instruction-tuned models, advanced reasoning models, math-augmented models, and domain-\nspecific climate models we provide key insights into the strengths and limitations of LLMs in addressing\natmospheric science problems. Our findings highlight that reasoning models outperform other categories,\ndemonstrating stronger problem-solving and reasoning capabilities in the domain of atmospheric science. This\nalso underscores the benchmark's effectiveness in differentiating models. We believe that ATMOSSCI-BENCH\n(where all the implementations are fully open-sourced) can serve as an essential step toward advancing the\napplication of LLMs in climate-related decision-making by offering a standardized and rigorous evaluation\nframework for future research."}]}