{"title": "The Extrapolation Power of Implicit Models", "authors": ["Juliette Decugis", "Alicia Y. Tsai", "Max Emerling", "Ashwin Ganesh", "Laurent El Ghaoui"], "abstract": "In this paper, we investigate the extrapolation capabilities of implicit deep learning models in handling unobserved data, where traditional deep neural networks may falter. Implicit models, distinguished by their adaptability in layer depth and incorporation of feedback within their computational graph, are put to the test across various extrapolation scenarios: out-of-distribution, geographical, and temporal shifts. Our experiments consistently demonstrate significant performance advantage with implicit models. Unlike their non-implicit counterparts, which often rely on meticulous architectural design for each task, implicit models demonstrate the ability to learn complex model structures without the need for task-specific design, highlighting their robustness in handling unseen data.", "sections": [{"title": "1 Introduction", "content": "Learning to extrapolate, which involves estimating unknown values beyond observed data, is a fundamental aspect of human intelligence and a significant step toward achieving general machine intelligence. Despite the remarkable success of modern neural networks across diverse domains, they often struggle when faced with unobserved data outside their training distribution. In this work, we investigate a general class of implicit deep learning models that encompasses classical layered-wise neural networks as special cases. Implicit deep learning models allow information to propagate both forwardly and backwardly through closed-form feedback loops, offering a flexible and powerful approach to learning data representations.\nVarious formulations have been proposed, including deep equilibrium models (DEQs), Neural ODEs, and implicit models. These models don't have explicitly defined layers and instead employ state vectors defined via an \u201cequilibrium\" (fixed-point) equation. In this framework, outputs are determined implicitly by the same equation. Recent results have highlighted the utility of implicit models. There has also been emerging work where the equilibrium state is interpreted as a closed-loop feedback system from a neuroscience perspective. Moreover, emerging work interprets the equilibrium state as a closed-loop feedback system from a neuroscience perspective. Unlike traditional feed-forward models that traverse nodes in a single direction in the computational graph, inputs of implicit models can revisit or reverse directions between nodes through closed-loop feedback.\nIn this paper, we investigate whether implicit deep learning models exhibit higher extrapolation capabilities, a fundamental skill in human intelligence, compared to similarly sized non-implicit neural network models. Our contributions are summarized in the following:\n\u2022 We demonstrate the extrapolation power of implicit deep learning models across three distinct domains: well-defined mathematical arithmetic, real-world earthquake location data, and volatile time series forecasting.\n\u2022 We conduct analyses and ablation studies on depth adaptability and closed-loop feedback, revealing that features learned by implicit models are more generalizable compared to their non-implicit counterparts."}, {"title": "1.1 Related Work.", "content": "Mathematical tasks. Previous research has primarily focused on developing specialized neural network models capable of learning algorithms. For instance, Neural Arithmetic Logic Units (NALUs) were specifically designed to represent mathematical relationships within their architecture, with the goal of improving arithmetic extrapolation. However, these models were later found to be highly unstable during training. Studies by Nogueira et al. demonstrated that transformers performed effectively for addition and subtraction tasks, achieving high accuracy in interpolation experiments. However, challenges arose with other Transformer-based architectures such as BART and large language models (LLMs), which struggled to accurately reproduce functions when dealing with inputs from a wide distribution range. On the contrary,\nCharton showed that Transformers could provide \"roughly correct\" solutions for matrix inversion and eigenvalue decomposition tasks, even on out-of-distribution (OOD) inputs, indicating a notable level of mathematical understanding.\nOut-of-distribution generalization. Only a handful of studies have begun to explore out-of-distribution (OOD) generalization for implicit deep learning models. These works showcase the capabilities of implicit deep learning models on tasks like Blurry MNIST, sequential tasks , matrix inversion, and graph regression. Researchers such as Liang et al. and Anil et al. highlight a characteristic of DEQs known as path independence, where they converge to a similar fixed point regardless of initialization. This property suggests that if DEQs iterate for longer before converging on OOD inputs, they could gather more information about the data and potentially outperform other models. [Ramzi et al., 2023] theorized that this property exists only when testing DEQs on more complex data than in their training distribution. They demonstrate that increasing the number of inner iterations can lead to overfitting for interpolation tasks. We contribute further evidence supporting both hypotheses by examining well-defined functions and real-world data with out-of-distribution characteristics.\nFunction extrapolation. Xu et al. studied how ReLU multi-layer perception (MLPs) and graph neural networks extrapolate on quadratic, cosine, and linear functions. They identified specific architectural choices that enhance extrapolation, such as encoding task-specific non-linearities in model features. Similarly, in the vision domain, Webb et al. introduced context normalization for more generalized features. Additionally, Wu et al. demonstrated the advantages of neural networks with Hadamard products (NNs-Hp) and polynomial networks (PNNs) for arithmetic extrapolation. In this paper, we hypothesize that implicit models can \"adapt\u201d to distribution changes, thereby eliminating the need for specific feature transformations for extrapolation."}, {"title": "1.2 Background", "content": "Given a data input u, DEQs are defined as \\(f_w (u, x)\\) where f is a function that depends on model parameters w, and x represents the hidden features. The objective is to find a fixed point \\(x^*\\) by iteratively applying f to x until convergence, such that \\(x^* = f_w(u,x^*)\\). In this paper, we adopt the more generalized formulation presented in El Ghaoui et al.. This formulation, proven to converge linearly, includes DEQs as a special case. Formally, for a given data point \\(u \\in \\mathbb{R}^P\\), an implicit model solves the equilibrium equation \\(x = \\phi(Ax+Bu)\\), where x represents the equilibrium state for input u, \\(\\phi\\) denotes a non-linear activation such as ReLU, and matrices A and B are model parameters. Prediction is obtained by passing the equilibrium state x through an affine transformation, yielding \\(\\hat{y}(u) \\in \\mathbb{R}^O\\), where matrices C and D are also model parameters:\n\n\n\\(\\hat{y}(u) = Cx + Du, \\text{ where } x = \\phi(Ax + Bu)\\). \\(\\) (1)\n\nThe \"state\" vector \\(x \\in \\mathbb{R}^n\\) contains the \"hidden features\" of the model. These features are not expressible in closed form but are implicitly defined via the equilibrium equation \\(x = \\phi(Ax + Bu)\\).\nIn this paper, we define convergence as \\(||x_t - x_{t-1}||_{\\infty} < \\epsilon\\), where \\(x_t\\) and \\(x_{t-1}\\) represent the current and previous implicit hidden states, respectively. We set \\(\\epsilon = 3 \\times 10^{-6}\\). As demonstrated by El Ghaoui et al., implicit models will always converge under certain conditions, such as projecting the A matrix onto an infinite norm ball, which we ensure in our experiments. We train implicit models using stochastic gradient descent and back-propagation. During the forward pass, we aim to approximate the fixed point \\(x^*\\) given a set of weights A, B, C, and D, along with a threshold \\(\\epsilon\\). We denote \\(X_N\\) as the approximation of the fixed point. During the backward pass, implicit differentiation is employed to compute the Jacobian of \\(X_N\\) with respect to the model weights. We define \\(\\Phi := \\frac{\\partial \\phi(x_t)}{\\partial x_t}\\) as a block diagonal matrix of gradients with \\(t \\in [0, N]\\). As demonstrated by El Ghaoui et al. in Appendix G, the gradient equations for each weight matrix are unique and rely on the inversion of \\((I - \\Phi A)^{-1}\\), which can be accelerated through approximate inverse Jacobian methods."}, {"title": "2 Problem Setup", "content": "We explore two types of implicit models in this paper: standard implicit models as defined by Eq. (1), and a variant we term implicitRNN.\nImplicitRNN. The implicitRNN functions similarly to a vanilla RNN, processing sequence inputs one by one. For each time step i, where \\(s_i \\in \\mathbb{R}^P\\) represents the i-th element in a sequence \\((s_1, s_2, \u2026\u2026\u2026, s_t)\\). The model input u for implicitRNN is a concatenation of \\(s_i\\) and the previous hidden state \\(h_{i-1}\\), akin to a vanilla RNN. The implicit prediction rule for implicitRNN is as follows:\n\n\n\\(h_0 = 0; x_0 = 0;\\)\n\\(u_i = \\begin{pmatrix} s_i \\\\ h_{i-1} \\end{pmatrix}\\)\n\\(x = \\phi(Ax + Bu_i)\\) (equilibrium equation)\n\\(\\hat{y}_i(u_i) = Cx + Du_i\\) (prediction equation)\n\\(h_i = \\hat{y}_i(u_i)\\).\n\n\nIn this setup, the recurrent layer is replaced with an implicit structure comprising the equilibrium and prediction equations. We introduce the implicitRNN to contrast implicit with explicit sequential models, both retaining representations of data timestep by timestep."}, {"title": "2.1 Extrapolate on Mathematical Tasks", "content": "We explore three types of functions, ranging from simple to complex: 1) identity function, 2) arithmetic operations, and 3) rolling functions over sequential data. To evaluate the extrapolation performance, we use two sets of data: \\(U_{train} \\sim \\mathcal{P}(\\mu; \\sigma)\\) and \\(U_{test} \\sim \\mathcal{P}(\\mu + \\kappa;\\sigma + \\kappa)\\), where \\(\\mathcal{P}\\) represents a known distribution, \\(\\mu\\) and \\(\\sigma\\) are mean and the standard-deviation, and \\(\\kappa\\) denote the hyper-parameters for distribution shift. Our code, models, dataset, and experiment setup are available on GitHub\u00b9.\nIdentify function. Recent work has shown that neural networks struggle to learn the basic task of identity mapping, \\(f(u) = u\\), where models should return the exact input as given.\nArithmetic operations We also focus on two arithmetic operations: addition and subtraction with additional data transformation. Following the task proposed by Trask et al., we randomly select four numbers i, j, k, l from 1 to 50, ensuring that i < j and k < l. For each sample, we construct two new numbers a and b from the input \\(\\tilde{u} := (u_1, u_2, ..., u_{50})\\) as follows: \\(a = \\sum_{a=i}^{j} u_a\\), \\(b = \\sum_{b=k}^{l} u_b\\). Finally, we predict \\(y = a + b\\) for addition and \\(y = a - b\\) for subtraction.\nRolling functions. Lastly, we explore two rolling functions over a sequence: average and argmax. In the rolling average task, we predict the average of the sequence up to the current time step j for each timestep, calculated as \\(\\sum_{i=1}^{j} u_i/j\\). For the rolling argmax task, we predict the index of the maximum value seen by the model so far for each time step. We convert this into a classification problem by outputting a one-hot vector of length \\(L = 10\\), representing the index of the predicted maximum input seen so far, where L is the length of the input sequence. Our evaluation focuses on predictions made for the final element of the sequence.\nWe compare implicit models with neural networks specially designed to excel on each task: MLPs for simple functions, LSTMs for sequential data, NALU for OOD arithmetic task, and various Transformers-based models. The details of each task and the model architectures are provided in the Appendix. We optimize all models using grid search and 5-fold cross-validation on in-distribution inputs. To ensure a fair comparison, we set the model dimension of the implicit models to have a similar number of parameters as their non-implicit counterparts."}, {"title": "2.2 Extrapolate on Noisy Real-world Data", "content": "In addition to mathematical extrapolation where data is generated from a known underlying function, we consider extrapolation on real-world problems where data is typically noisy and often lacks known data generation functions. We focus on two real-world applications known for predicting unseen events: oscillating time series forecasting and earthquake location prediction\nOscillating time series forecasting. We first consider a simpler synthetic version known as spiky time series forecasting, where spikes are inserted into a time series derived from a combination of sine functions at random intervals (see Appendix A.3). In the second case, we forecast AMC stock data volatility, particularly focusing on the drastic increase in average volatility observed at the beginning of 2021 as illustrated in Figure 1. Volatility, in this context, refers to the expected amount by which a security's price might change suddenly, serving as a measure of financial risk for an asset. Our prediction task involves forecasting AMC's volatility over the next 10 minutes based on its volume-weighted average trade price (VWAP) for each of the past 60 minutes. Volatility is calculated as the variance of the VWAP prices\u00b2 during the forecast period. Our training data goes from 2/1/2015 to 12/31/2020 and our validation set from 1/1/2021 to 12/31/2021. Our training data spans from 2/1/2015 to 12/31/2020, with the validation set covering 1/1/2021 to 12/31/2021. We deliberately refrain from making our data stationary through differencing or examining returns, as we aim to evaluate our models' ability to adapt to changes in price distribution. We compare the performance of implicit models against simple linear regression and non-implicit neural network baselines, with details of the baseline models included in Table 4 in the Appendix.\nEarthquake location prediction. The earthquake location prediction is a well-established problem in seismology. It involves predicting the location (X, Y, and Z coordinates) and the seismic wave travel time (T) of an earthquake based on seismographs recorded from nearby seismometers. Successfully solving this problem can have significant humanitarian implications, allowing for early warnings before the potentially destructive second wave(s) of an earthquake. However, the problem remains challenging due to the sparsity of the event observations [Chuang et al., 2023].\nWe follow the methodology outlined by Chuang et al., generating seismograph samples recorded by five stations with one anchor station designated as the reference for all seismic wave arrival times. As illustrated in the left panel of Figure 2, our input features include a station's coordinates (x, y, z), event-station back-azimuths (\\(\\theta\\)), and relative wave travel times (p) with respect to the anchor station. Training data is synthetically generated within a range of 90\u00b0E and -90\u00b0E, roughly corresponding to the Pacific Ring of Fire, as shown in Figure 2 (further details are provided in Appendix A.4). Testing is conducted on regions shifted from 10\u00b0E to 90\u00b0E beyond this Ring of Fire. As a comparison baseline, we use EikoNet, a deep learning model introduced by Smith et al. specifically designed for earthquake location prediction. While earthquakes primarily occur in certain active tectonic boundaries, an extrapolated earthquake location prediction system can aid in detecting earthquakes in new areas, whether natural or human-made (e.g., caused by mining, oil, and gas activities). Such a system is also valuable for explosion monitoring, providing universal mapping capabilities."}, {"title": "3 Experimental Results", "content": ""}, {"title": "3.1 Mathematical Extrapolation", "content": "Identify function. The test mean squared error (MSE) for the identify function task is shown in Figure 3. The implicit model maintains the lowest test MSE (below 5) for test data with a distribution shift from 0 to 25. Even for very large distribution shifts of up to 200, where \\(U_{test} \\in \\mathbb{R}^{10} \\sim \\mathcal{U}(-205, 205)\\), the implicit model outperforms the Transformers encoder model by a factor of \\(10^5\\) and the MLP by a factor of \\(10^3\\). This disparity highlights the implicit model's robustness in handling distribution shifts compared to non-implicit models, which tend to overfit to the training distribution and exhibit increased error with larger distribution shifts. The identity function task exposes the spurious features learned by both the MLP and Transformers models. Our implicit model reaches equilibrium after only 4 training iterations. In contrast, similar-sized MLPs and transformer encoders struggle with overfitting, highlighting the implicit model's ability to limit overfitting through faster convergence when dealing with simpler functions like identity mapping.\nArithmetic operations. The results for the addition and subtraction arithmetic tasks are depicted in Figure 4. The implicit model not only outperforms various Transformer encoders but also NALU, a model specifically designed for mathematical operations. As shown by Wei et al., Transformers models require much larger model sizes (\\(10^{23}\\)) to perform an arithmetic task effectively. In contrast, an implicit model with only 7, 650 parameters successfully learns these operations with a testing loss of < 1 for shifts < 100. Throughout the experiment, we consistently observed that implicit models outperform non-implicit models on extrapolation tasks, especially with limited training samples. Surprisingly, the specialized NALU model achieves the worst testing loss, exceeding \\(10^{10}\\) for an extrapolation shift of only 10, as shown in Figure 4. Across our experiments, we couldn't produce robust out-of-distribution predictions with the NALU model.\nRolling functions. In Figure 5, we present the performance of the two sequence modeling tasks. In the rolling average task (left plot of Figure 5), the LSTM and Transformers exhibit similar behavior while the implicit model maintains close to constant loss regardless of the test distribution. For the rolling argmax, we use a Transformer encoder-decoder architecture, where the target sequence is the right-shifted argmax labels. This architecture has an inherent advantage, as simply outputting the final element in this right-shifted sequence will provide the argmax of the input sequence up to but not including the current element. Given that argmax labels of a sequence are uniformly distributed, this setup implies an expected accuracy of 90% or of \\(1 - \\frac{1}{L}\\), where L = 10 is the sequence length. Surprisingly, we observe that the standard implicit model and implicitRNN both outperform the LSTM and Transformer models. Additionally, the implicitRNN outperforms the standard implicit model, indicating the potential benefits of a rolling latent representation in tasks requiring awareness of relative positions within a sequence.\nComparing different Transformers variants highlights the potential for small Transformers to overfit to their positional encodings (PE) on simpler tasks. The Transformer without positional encodings, lacking the ability to differentiate between different positions in the sequence, outperforms the other two architectures, consistently hovering around the 90% accuracy benchmark. This performance suggests that it effectively learns to always output the final value of its target sequence. Overall, these results demonstrate a very competitive performance of the standard implicit model and implicitRNN over Transformers and LSTMs in extrapolation tasks, particularly in scenarios where an understanding of relative positions within a sequence is crucial.\nIt is worth emphasizing that the implicitRNN structure resembles that of a vanilla RNN, with the standard recurrent cell replaced by an implicit layer. The substantial performance improvement over the LSTM suggests an exciting potential of the implicit layer to learn effective memory-gating mechanisms. However, further experimentation, specifically with longer sequence lengths, would be required to verify this conjecture. For further insights and experimental findings, please refer to Appendix A.2."}, {"title": "3.2 Oscillating Time Series Forecasting", "content": "In our prior experiments, we tested implicit models on clearly defined mathematical operations. Now, we shift focus to real-world noisy data, where the underlying function is unknown. This transition allows us to explore whether the extrapolation benefits of implicit models extend to such complex, real-world scenarios.\nIn the spiky time series forecasting task, our objective is for models to effectively capture sudden and short-lived distribution changes in our generated data. Such extreme events, resembling those seen in stock prices, sales volumes, or storage capacity predictions, often challenge forecasting models that struggle to extrapolate to these unprecedented patterns. The ability to accurately adapt to synthetic spikes in this task carries significant real-life implications. Table 1 demonstrates that implicitRNN achieves a threefold reduction in test MSE compared to non-implicit models in the spiky time series task. Furthermore, Figure 6 showcases the implicit model's capacity to accurately capture the locations and magnitudes of these spikes. In contrast, the transformer decoder and LSTM models often output the average of the time series. This tendency of standard models to revert to the mean also hints at their potential weakness in simulating time-series data effectively. Encouraged by these positive results, we applied implicit models to predict the sharp rise in AMC stock volatility observed in 2021. We report the mean absolute percentage error (MAPE)\u00b3 in Table 1, where the implicit model outperforms other baselines by a factor of 1.67."}, {"title": "Earthquake Location Prediction", "content": "In the earthquake location prediction task, our implicit model demonstrates an improvement of 0.25e-3 in the in-distribution test loss compared to EikoNet, a model specifically designed for seismic data. Notably, in the left panel of Figure 7, as \\(\\kappa\\) increases, we observe a progressive improvement in the performance of the implicit model in terms of the extrapolated test MSE. By the time \\(\\kappa\\) reaches 2, the implicit model has surpassed EikoNet, and at \\(\\kappa = 9\\), the implicit model's test loss is 1.59e-2 better than that of EikoNet. This improvement translates to an average enhancement of 11\u00b0 in longitude and 2\u00b0 in latitude. Further refinement could be achieved by limiting the source latitude along with longitude during training, potentially leading to a more substantial improvement in latitude extrapolation. However, as depicted in the right panel of Figure 7, the implicit model encounters greater difficulty in predicting time and depth, exhibiting deteriorating performance with increasing \\(\\kappa\\). Specifically, at \\(\\kappa = 9\\), the implicit model's average performance worsens by 9.2 seconds and 409 km. Future research should explore whether training an implicit model exclusively on time and depth labels can enhance its performance in these aspects. This two-pass approach resembles traditional location prediction software such as HYPOINVERSE (USGS), where constraints on depth and time are notably challenging."}, {"title": "4 Analysis", "content": "In our analysis, we pinpoint two key properties that contribute to the effective extrapolation capabilities of implicit models, even with limited datasets. The first property is \"depth adaptability,\" meaning these models are not constrained by a fixed number of layers. The second property, \u201cclosed-loop feedback\", allows inputs to revisit the same node within a single pass through the model, enhancing its learning and adaptability.\nDepth adaptability. During a forward pass through an implicit model, the process concludes either upon convergence to a fixed point \\(x^*\\) or upon reaching a predetermined iteration limit. We investigated whether the complexity of the input affects the number of iterations required for the model to achieve equilibrium. Across our experiments, we noticed that as our model familiarized itself with parameter matrices, the iteration count stabilized for inputs within the distribution. On average, implicit models converged in about 15 iterations for learning the addition operation, 30 iterations for subtraction, and 175 iterations for the rolling argmax task. We can interpret the number of iterations required for a given task as an indication of the model's perceived difficulty with that task. Consistent with the findings of Liang et al., we observed that as our input deviated further from the training distribution, the number of iterations needed to converge increased. This suggests that the input U underwent more transformations by the model parameters (as shown in the bottom two panels of Figure 8). Therefore, we interpret that implicit models dynamically \"grow\" in depth to adapt their feature space to extrapolated inputs, iterating longer compared to in-distribution inputs. This adaptability allows them to benefit from low depth for in-distribution inputs (reducing overfitting) and higher depth for out-of-distribution inputs (reducing underfitting).\nClosed-loop feedback. Introduced by Wiener, refers to a system's ability to self-correct based on its outputs. This concept, elaborated by Patten and Odum, involves returning a portion of the output, which partly influences the input at the next step. In neural networks, feedback is encoded through the backward pass. Implicit models, as depicted in Figure 9, inherently incorporate feedback within a single iteration via the feedback connection in the parameter matrix A (lower-triangular part). We observe that inputs in implicit models do not only move forward from one set of nodes to the next, as in a standard feed-forward neural network. Instead, they may revisit the same nodes or even move backward, allowing the model to correct itself from one layer to the next within a single iteration. This is in contrast to non-implicit models, which correct themselves after a pass through all layers. Ma et al. highlighted closed-loop feedback as a strength of implicit models and noted its similarity to human neural networks. Therefore, we evaluated the utility of closed-loop feedback in extrapolation through ablation studies on the lower triangular part of the model's A matrix. We illustrate the absence of feedback in an implicit model by considering a 3 \u00d7 3 example with a strictly upper triangular A matrix. At time t + 1, the iteration equation simplifies to \\(x_{t+1} = \\phi(Ax_t + Bu)\\). Here, we can disregard the activation \\(\\phi\\) and Bu terms, focusing solely on the \\(x_t\\) term as it carries the outputs from the previous iteration. This setup allows us to demonstrate how an implicit model without feedback operates, contrasting it with the typical behavior of a truly implicit models.\n\n\n\\(x_{t+1} \\approx Ax_t = \\begin{pmatrix} * & w_0 & w_1 \\\\ * & * & w_2 \\\\ * & * & * \\end{pmatrix} \\begin{pmatrix} x_0^t \\\\ x_1^t \\\\ x_2^t \\end{pmatrix} = \\begin{pmatrix} *x_0^t + w_0 x_1^t + w_1 x_2^t \\\\ *x_0^t + *x_1^t + w_2 x_2^t \\\\ *x_0^t + *x_1^t + *x_2^t \\end{pmatrix}\\) (2)\n\n\nClosed-loop feedback corresponds to \\(x_i^t\\) being used to generate \\(x_i^{t+1}\\). Equation 2 illustrates how the * weights encode this feedback mechanism. Specifically, the i-th state at time step t + 1, denoted as \\(x_i^{t+1}\\), depends not only on its past output \\(x_i^t\\) directly through weights on the diagonal but also indirectly through weights on the lower triangular matrix. As an example of this indirect dependence, consider \\(x_2^{t+1}\\), which depends not only on \\(x_2^t\\) but also on \\(x_0^t\\) due to the lower triangular * weights. Furthermore, both \\(x_1^t\\) and \\(x_2^t\\) depend on \\(x_0^{t-1}\\), highlighting the chain of dependencies facilitated by the feedback mechanism.\nWe compared a standard implicit model (with feedback loops) against an ablated implicit model (without feedback loops) where the upper-triangularity of A was enforced during training. The standard implicit models correspond to those used in our experiments, as described in Section 2. The results of the mathematical tasks for implicit models with and without feedback are presented in Table 2. It is evident that the feedback loops play a crucial role in helping the models achieve significantly lower testing loss, especially on inputs with distribution shifts. Figure 10 presents the ablation results for both models across distribution shifts in arithmetic and rolling sequential tasks. Ablating the feedback harms model performance for subtraction but not for addition. The regular model, with its feedback loops, has twice as many weights, which could potentially lead to overfitting on simpler tasks. Feedback loops appear to increase stability to distribution shifts for rolling average tasks, whereas they only provide better performance in calculating the rolling argmax of a sequence. We also note that besides superior overall performance, the presence of closed-loop feedback seems to make the model more resistant to distribution shifts: notably, we observe that its loss increases slower in the subtraction and rolling average experiments. Our analysis demonstrates that implicit models possess the capability to adapt their architecture by learning the necessary depth, node connections, and correcting themselves based on past predictions within a single training iteration, thanks to their closed-loop feedback mechanism.\nImplicit models represent a significant advancement over standard feed-forward neural network layers by relying on the convergence of an equilibrium equation to extract features from input data. In this paper, we have demonstrated their remarkable ability to learn useful function representations for extrapolation tasks, even in scenarios with limited training data. Our experiments consistently show that implicit models outperform non-implicit baselines when handling out-of-distribution inputs, temporal and geographical shifts. The adaptive nature of training implicit models allows them to explore and identify optimal architectures with minimal hand-engineering, resulting in a robust inner function representation for extrapolation tasks involving out-of-distribution data. These results highlight the potential of implicit models and motivate further research into their use as more interpretable solutions for wider scenarios with limited data set."}, {"title": "A.3 Spiky Data Generation", "content": "Both the LSTM and the implicit model were trained on 7000 data points and tested on 3000 data points. The training regime featured 20 spiky regions of 100 data points each. The testing regime featured a proportionate amount of spiky regions. The data points in the spiky regions were sampled from \\(y = 5 \\times (sin(2x) + sin(23x) + sin(78x) + sin(100x))\\). We arbitrarily choose frequencies in [0, 100] to generate a sufficiently spiky pattern. The magnitude of the spiky regions is at most 20. For the non-spiky regimes, the data points were sampled from \\(y = sin(x)\\) with added noise \\(\\epsilon \\sim \\mathcal{N}(0, 0.25)\\)."}, {"title": "A.4 Earthquake Data Generation", "content": "To generate samples of seismic waves between specific longitudes, based on the methods presented by Chuang et al., we used a 1D velocity model called Ak135 from the Python library obspy.taup. Obspy is a Python framework used to process seismological data. Kennett et al. demonstrate the accuracy of this model compared to real-world data (see specifically Figure 6). A 1D velocity model assumes the P-wave travel time (the duration the P-wave takes to travel from point A to point B) only depends on two attributes: the distance between the source and the receiver station and the depth of the source. We use this model to create a travel time lookup table based on these two attributes. We then generate source locations from a mesh that spans the entire globe while adding perturbation to each latitude and longitude pair. We generate station locations using the source-station distances we have from the lookup table and place the stations in random orientations (azimuths) from the source."}]}