{"title": "Split Learning in Computer Vision for Semantic Segmentation Delay Minimization", "authors": ["Nikos G. Evgenidis", "Nikos A. Mitsiou", "Sotiris A. Tegos", "Panagiotis D. Diamantoulakis", "George K. Karagiannidis"], "abstract": "In this paper, we propose a novel approach to minimize the inference delay in semantic segmentation using split learning (SL), tailored to the needs of real-time computer vision (CV) applications for resource-constrained devices. Semantic segmentation is essential for applications such as autonomous vehicles and smart city infrastructure, but faces significant latency challenges due to high computational and communication loads. Traditional centralized processing methods are inefficient for such scenarios, often resulting in unacceptable inference delays. SL offers a promising alternative by partitioning deep neural networks (DNNs) between edge devices and a central server, enabling localized data processing and reducing the amount of data required for transmission. Our contribution includes the joint optimization of bandwidth allocation, cut layer selection of the edge devices' DNN, and the central server's processing resource allocation. We investigate both parallel and serial data processing scenarios and propose low-complexity heuristic solutions that maintain near-optimal performance while reducing computational requirements. Numerical results show that our approach effectively reduces inference delay, demonstrating the potential of SL for improving real-time CV applications in dynamic, resource-constrained environments.", "sections": [{"title": "I. INTRODUCTION", "content": "Real-time computer vision (CV) has become essential for a variety of emerging applications, including digital manufacturing, advanced transportation, remote sensing, robotics, virtual/augmented reality (VR/AR), and the metaverse [1]. Semantic segmentation, a specific CV technique, plays a crucial role in these areas by labeling each pixel in an image with a class label to partition the image into semantically meaningful segments, providing a highly detailed understanding of scenes [2]. Such granular segmentation is critical for accelerating decision making in dynamic environments, making it invaluable for real-time CV-based applications. To this end, efficient deep neural networks (DNNs) have accelerated real-time processing, making semantic segmentation a viable option for latency-sensitive applications. Convolutional neural network (CNN) architectures such as U-Net [3], ENet [4], and DeepLab [5] have been developed to optimize both accuracy and speed, allowing them to process and analyze complex scenes with high accuracy. Moreover, recent innovations in efficient DNNs focus on reducing computational complexity and inference time, which are key to deployment in resource-constrained environments such as edge devices and internet-of-things (IoT) networks.\nDespite these advances, real-time CV also faces significant challenges due to limited communication resources, as large amounts of data must be transmitted across distributed entities. Therefore, joint optimization of communication and computational resources is essential to enable latency-efficient CV-based decisions [6]. Emerging machine learning (ML) paradigms that integrate distributed computing and edge intelligence, such as split learning (SL) [7], are a promising option for such a holistic real-time CV solution [8]. Instead of transmitting entire datasets or high-resolution images to a central server, SL divides a DNN model between edge devices and a central server, allowing each segment of the model to process its portion of the data locally before sharing only the necessary intermediate outputs [9]. This approach significantly reduces the amount of data transmitted, while allowing the slicing layer to be dynamically adjusted based on channel conditions, device power, and computing resources."}, {"title": "A. State of the Art", "content": "For real-time CV applications on resource-constrained devices, efficient DNN models have been developed. In [10], fully convolutional networks replaced fully connected layers, allowing end-to-end training for pixel-wise classification and supporting inputs of different sizes. This innovation set the stage for subsequent models, such as U-Net [3], which used a symmetric encoder-decoder structure with skip connections to combine low and high-level features, improving detail representation and contextual understanding. Furthermore, SegNet [11] utilized encoder pooling indices to minimize additional parameters, while ENet [4] employed early downsampling and factorized convolutions to enable faster inference with minimal loss of accuracy. Improvements in segmentation accuracy were also made in DeepLab [5], which used atrous convolutions and conditional random fields to capture multi-scale context and improve boundary precision. DeepLab evolved into DeepLabv3+ for further refinement [12]. Other models, such as RefineNet [13] and ERFNet [14], emphasize efficiency and precision, with RefineNet preserving high-resolution details and ERFNet integrating residual connections and factorized convolutions for real-time performance, making it suitable for autonomous driving.\nMoreover, to further optimize the performance of CV-based applications, their integration with distributed ML architectures has been investigated. In particular, the integration of federated learning (FL) has been proposed to accelerate distributed collaborative learning of CV tasks. For example, [15] developed FedCV, a versatile FL framework addressing data heterogeneity and privacy across different tasks, while [16] highlighted the suitability of FL for segmentation with minimal communication requirements. In addition, [17] demonstrated FedVision, an FL-based object detection platform that operates without centralizing sensitive data, illustrating practical industrial applications of FL, while [18] applied FL to vision-language tasks, improving model collaboration for tasks such as image captioning. However, FL cannot be used to accelerate the inference delay of an ML model once it has been trained.\nSL is a distributed ML paradigm that can address this challenge. Its growing adoption in both academia and industry includes implementation in open-source applications [19], [20] and services from startups [21]. Academic research encompasses empirical studies in various fields, e.g., [22] applied SL to depth-image-based millimeter-wave power prediction and achieved significant latency reductions, while other studies explored SL in medical imaging [23]. Comparative studies also highlighted the reduced communication overhead of SL compared to FL [24]. Research on SL privacy showed that it provides higher protection than FL since devices do not have access to server-side models, as noted in [25]. Further research optimizes SL by introducing variants with multiple cut layers, allowing devices to retain initial and final layers to protect data and labels [26]. Furthermore, joint optimization of communication and computation resources for SL was studied in [27]\u2013[29], showing significant latency gains. However, the integration of SL with CV-based tasks has not yet been investigated in the literature."}, {"title": "B. Motivation and Contribution", "content": "The interest in exploring the SL paradigm in semantic segmentation and CV lies in its potential to address the high computational and communication burden associated with pixel-level predictions, especially in resource-constrained devices. Semantic segmentation, while critical for applications such as autonomous vehicles and smart city infrastructure, requires intensive computation due to the need to make dense, per-pixel predictions. As a result, traditional centralized approaches, where an entire model runs on either an edge device or a central server, can result in significant communication and computation latency, often making real-time processing infeasible. Therefore, to fully realize the potential of SL in CV-based applications, a joint optimization of computational and communication resources is required. This includes strategies for partitioning the CV models between the edge devices and the central server of the network according to their computational capabilities and the data load of the central server, and optimizing the data transmission between the devices and the server through optimal bandwidth allocation.\nTo address these challenges, this paper aims to minimize the inference delay in semantic segmentation by adopting a SL approach. Unlike SL for feedforward neural networks (FNNs) in [27]\u2013[29], SL for CV tasks must accommodate the complexity of multi-layer CNN architectures and their reliance on bottleneck modules (BMs). These modules, which are integral to CNNs, do not allow splitting within a BM, unlike the simpler successive layers in FNNs. In addition, the data size to be transmitted when splitting such a DNN at a given layer is significantly different from that in FNNs, as will be explained in the next section. Furthermore, we consider two different use cases. First, the case where the central server processes all incoming tasks in parallel, and second, the case where tasks are processed sequentially. Each scenario leads to different optimal layer-slicing policies and resource allocation strategies. Moreover, to facilitate real-time implementation, we propose heuristic, sub-optimal layer-cutting and resource allocation policies that offer significantly lower complexity compared to the optimal delay minimization scheme. The contribution of the paper is given below:\n\u2022 For the first time in the literature, we propose the use of SL to minimize the inference delay in CV. To achieve this, we take into account the special features of CV, such as multi-layer CNNs and BM modules, which complicate optimal layer splitting.\n\u2022 For two cases of interest, the case of data processed in parallel by the central server and the case of serial data processing, the problem of joint bandwidth allocation, cut layer selection of the edge devices' DNN, and central server processing resource allocation is tackled. The initial problem is non-convex, but by using alternating optimization, it is reduced to a convex optimization sub-problem and a binary programming sub-problem, which can be solved efficiently.\n\u2022 To support real-time CV, low-complexity algorithms are provided by optimizing only a subset of the variables. Specifically, for the parallel data processing case, by fixing the bandwidth allocation, an equivalent formulation of the problem with a closed-form solution is given, while a scheme that uses the cutting layer with the least amount of data to be transmitted is also studied. For the case of serial data processing, a strategy based on the simultaneous arrival of data at the central server is investigated. Moreover, a low-complexity variant is studied, which reallocates the bandwidth among the devices appropriately, aiming to minimize their maximum waiting time in the queue upon arrival.\n\u2022 Numerical results provide useful insights into the effectiveness of SL and all considered schemes. It is shown that SL can reduce the inference delay of CV applications, while trade-offs between all proposed schemes are revealed, which can be used to select the most appropriate algorithm given the available communication and computational resources and the number of devices."}, {"title": "C. Structure", "content": "The rest of the paper is organized as follows. Section II presents the system model. Section III introduces the proposed slicing policy. In Section IV, we formulate and solve the inference delay minimization problem. Finally, Section V presents the numerical results and Section VI concludes the paper."}, {"title": "II. SYSTEM MODEL", "content": "Many current CV applications, including semantic segmentation, rely heavily on complex multi-layer CNN architectures. Although the cornerstone of these frameworks are convolutional layers, in order to enhance feature extraction, the specific frameworks implement the so-called BM instead of simple consecutive layers. The main characteristic of the BM is that it acts on two separate branches, which are added just before the output of the BM, as shown in Fig. 1. In addition, each BM inner structure can be different from others, depending on its type. This means that some of its layers can have different parameters, which are given in Table I. This variety of inner structures ensures the capture of different characteristics, since each parameter affects the effective area of the included elements as the kernel slides along the data. Therefore, each BM can consist of multiple layers on each branch, which increases the workload of the CNNs and does not allow splitting while inside a BM. Obviously, the floating point operations per second (FLOPS) workload of each BM consists of that of multiple CNNs and pooling workloads and can be calculated using the formulas in Table II. Although BMs are quite common in CNN architectures, semantic segmentation CV frameworks have unique characteristics that differentiate them from others.\nA distinctive feature of semantic segmentation frameworks is downsampling and upsampling of image dimensions, which is achieved by changing the number of filters for better feature extraction. The downsampling process can be performed in two ways, either by using pooling layers or convolutional layers with stride, and requires a specific inner layer structure in its BM as shown in Fig. 1. On the other hand, to obtain the classification of each pixel of the original image, these architectures also require upsampling, which is performed in a similar way to the downsampling part, as shown in Fig. 2. This means that if a pooling layer was used, an unpooling layer must be used for upsampling, and if a convolutional strided layer was used, a corresponding convolutional transpose layer should be used for upsampling. Note that downsmapling/upsampling is generally performed over MaxPooling/MaxUnpooling layers, except for the first and last layers, which use convolutional layers depicted as \u201cConv\u201d and \u201cFullConv\u201d in Fig. 2. Also, since upsampling is the inverse of downsampling, the number of upsampling layers must be equal to the number of downsampling layers to ensure that the output of the architecture has the same dimensional size with respect to the original height and width of the image. Regarding the data output of each BM in a stack, it is important to emphasize that it remains the same compared to the input of the BM, since the number of input and output filters, as well as the height and width, are preserved, unless a BM is used for downsampling or upsampling, in which case it is decreased or increased, respectively.\nIn addition, unpooling layers require not only the data to perform upsampling, but also the corresponding pooling layer index. Thus, as shown in Fig. 2, an upsampling BM requires the processed data and the pooling layer index, which indicates where in the unpooling kernel the output-element, denoted as e in Fig. 2, of the upsampling BM will be placed. For example, in Fig. 2 a MaxPooling is performed with $K_h=K_w=2$, meaning that the maximum of each $K_hK_w$ elements is forwarded to the BMs for further processing. Moreover, the index of this element is used for its MaxUnpooling counterpart to create a frame of $K_hK_w$ elements, in which the BM's output-element is placed, while the rest of the elements are filled using an interpolation technique between neighboring elements. Therefore, some BMs may require an additional amount of data to be generated for later use when downsampling is performed. Note that this data can be relatively small compared to the processed data, e.g., a downsampling BM with MaxPooling layer with $K_h=K_w=2$ requires only 2 bits to keep the index information of each kernel, as shown in Fig. 2. Apart from this, the index data is necessary for correct upsampling and thus provides some protection against potential leakage of the processed data of the framework.\nIn this paper, we are interested in the timely inference of all devices in a communication network. For this purpose, we consider a network consisting of $K$ devices communicating with a single base station (BS) where the central server is also located. For the communication of each device, we assume that the transmit power of the $k$-th device is constant and equal to $P_k$, while the available bandwidth of the $k$-th device is denoted as $B_k$. Considering the small-scale fading coefficient $h_k$ of the channel between each device and the BS, the maximum achievable rate is given by\n$R_k = B_k \\log_2 \\left(1+ \\frac{P_kG_tG_rL_p(d)|h_k|^2}{N_0B_k}\\right)$.\nwhere $N_0$ is the noise power spectral density, $G_t$ and $G_r$ are the transmitter and receiver gains, respectively, while $L_p(d) = (4 \\pi d / \\lambda)^{-n}$ is the path loss at a transmit frequency corresponding to wavelength $\\lambda$ at distance $d$ from the BS, and $n$ is the path loss exponent, which varies with the environment. With respect to the computational capabilities of the transmitters, let $f_k$ be the computational resources of the $k$-th device."}, {"title": "III. SLICING POLICY FOR SEMANTIC SEGMENTATION", "content": "One of the main requirements of many real-time applications in modern communication networks is a low inference delay to ensure normal functionality. Consequently, in CV applications such as semantic segmentation, it is critical for the system to minimize the total inference time of all participating devices. With this in mind, we propose an SL-inspired slicing policy that allows all devices to \"slice\" their used architecture at any BM and transmit the necessary data for further processing at the central server, thus fully exploiting the processing capabilities of both communication ends and aiming to decrease inference time.\nThe rationale behind the proposed policy is that the two extreme cases, i.e., transmitting raw pixel data and transmitting the pixel-wise prediction output of the last layer of the semantic segmentation architecture, can both lead to increased delay. In the first case, the central server of the BS must simultaneously process the entirety of the generated data in parallel, which increases the inference delay, while in the second case, the inference delay is increased due to the limited computational resources at each device. Furthermore, unlike other CNN frameworks, where the amount of data per layer is usually consistently reduced, a typical semantic segmentation architecture, such as the one shown in Fig. 2, may have more data to process than previous BMs when upsampling takes place. Therefore, there is a trade-off between processing and communication delay that can be addressed by the proposed SL-based slicing policy.\nIn this context, we assume that each device implements its own semantic segmentation architecture consisting of $L$ BMs. Then, the processing time of the $l$-th BM module at the $k$-th device is denoted as $W_{k,l}$ and the total processing time of a framework results from the summation of all individual workloads. Except for the workload $W_{k,l}$ of each BM module, each of the latter generates data that must be passed to the next module, which is denoted as $D_{k,l}$ for the $k$-th device at the $l$-th BM. Then, if the $k$-th device decides that it is advantageous to execute the slicing policy at the $l$-th BM, $D_{k,l}$ must be transmitted at rate $R_k$. Unique to this type of architecture, an additional amount of required data, denoted as $T_{k,l}$, for the $k$-th device slicing at the $l$-th BM, may also need to be transmitted if upsampling must be performed at the BS, as explained in Section II. It should be noted that this amount will be different depending on the sliced BM. For example, as shown in Fig. 2, if a device chooses to slice at the black dashed line, then it must transmit index-related data from two downsampling BMs, while if it slices at one of the two green dashed lines, index-related information from only one BM is required.\nIt is important to emphasize that, as explained in Section II, the amount of $T_{k,l}$ is small compared to $D_{k,l}$, but still affects the overall communication delay and is required to complete processing at the BS. It is also important to emphasize that even if leakage of $D_{k,l}$ occurs, without knowledge of the corresponding $T_{k,l}$, a potential attacker will not be able to make correct inferences from the processed data. Therefore, the proposed slicing policy also provides some real-time protection against data leakage due to the nature of the utilized MaxUnpooling layers in upsampling [4], when the devices choose to transmit data before the final output. Moreover, for the central server to successfully implement the proposed policy, we assume that it contains all possible instances of the devices' DNN architectures and can process data from any BM. Let $f_{k,s}$ denote the computational resources of the central server associated with the $k$-th device. For completeness, we consider two resource allocation schemes at the BS side, one where the resources are distributed among all devices and cannot reach values greater than a maximum computational limit, $f_{max}$, and another where full resources are given to each device."}, {"title": "IV. INFERENCE DELAY MINIMIZATION", "content": "The goal of this section is to minimize the inference delay of all devices, which contains the local processing delay, the transmission delay and the processing delay at the server. To this end, each device chooses to slice its DNN at a specific BM. To model the slicing layer selection, binary variables $\\alpha_{k,l}$ are introduced for the $k$-th device and its $l$-th BM layer. Obviously, only one slicing layer can be selected for each device, thus $\\Sigma_{l=1}^L \\alpha_{k,l} = 1, \\forall k \\in K$. Also, each device can control its bandwidth by dynamically allocating its spectrum from a total available bandwidth $B_{tot}$, leading to $\\Sigma_{k=1}^K B_k = B_{tot}$. Next, we investigate the delay minimization problem for two possible scenarios, related to the way that the server processes the received data."}, {"title": "A. Parallel Processing", "content": "From the above constraints, if the server chooses to process data from different devices in parallel, the inference delay of the $k$-th device sliced at the $l$-th BM is given as\n$J_{k,1} = \\frac{D_{k,l}}{R_k} + \\frac{T_{k,l}}{R_k} + \\frac{\\Sigma_{j=1}^l W_{k,j}}{f_k} + \\frac{\\Sigma_{j=l+1}^L W_{k,j}}{f_{k,s}}$\nwhere the last part of (2) indicates a dynamic resource allocation at the central server, which means that parallel processing of data is performed. Taking (2) into account, we can now formulate the following general optimization problem\n$\\min_{\\alpha_{k,1},f_{k,s}, B_k} \\{\\max_1 \\{\\alpha_{k,l}J_{k,1}\\} \\}$\ns.t.\n$C1: \\Sigma_{k=1}^K f_{k,s} = f_{max},$\n$C2: \\Sigma_{k=1}^K B_k = B_{tot},$\n$C3 : \\Sigma_{l=1}^L \\alpha_{k,l} = 1, \\forall l \\in \\{1,\\ldots, L\\}.$\nObviously, (P1) cannot be solved directly because it involves binary and continuous variables. However, it can be observed that $J_{k,l}$ in (2) is convex in both $f_{k,s}$ and $B_k$. Therefore, for fixed $\\alpha_{k,l}$, the objective of (P1) is also convex as the maximum of convex functions. As a result, alternating optimization can be used to solve (P1) by iteratively solving a convex and a binary programming problem. For each device, an initial random slicing layer selection is made and a convex optimization problem is solved to extract the optimal $f_{k,s}, B_k$. The obtained solution is then used to solve a binary programming problem in terms of $\\alpha_{k,l}$. Although integer programming techniques can effectively solve this subproblem, the required complexity can sometimes be large due to the large number of variables. Observing that for fixed $f_{k,s}, B_k$ also fixes $J_{k.l}$, we can decompose the binary programming problem into $K$ independent minimization problems, each of which is equivalent to finding the minimum value of $J_{k,1}, \\forall k \\in K$, whose overall complexity is very small. Repeating this process until the iterative procedure converges yields a solution to the original general problem.\nSpecial Case of Fixed Bandwidth: Although (P1) can be efficiently solved, its complexity may be high for real time applications, especially as the number of devices in the system increases. Consequently, a simpler version of this problem that yields a near-optimal solution, but with considerably lower complexity, is of great interest. As such, by fixing the bandwidth allocation for all devices, the following optimization problem is formulated\n$\\min_{\\alpha_{k,l}, f_{k,s}} \\{\\max_1 \\{\\alpha_{k,l}J_{k,l}\\} \\}$\ns.t.\n$C1: \\Sigma_{k=1}^K f_{k,s} = f_{max},$\n$C2: \\Sigma_{l=1}^L \\alpha_{k,l} = 1,\\forall l \\in \\{1,\\ldots,L\\},$\nwhich can be solved by the same method. However, instead of solving a convex optimization problem, we can equivalently transform the problem into a simpler form with much lower complexity. For any realization of $\\alpha_{k,1}$, the local processing and transmission delay of the device can be written as\n$C_k = \\frac{D_{k,l} + T_{k,l}}{R_k} + \\frac{\\Sigma_{j=1}^l W_{k,j}}{f_k}$"}, {"title": "B. Serial Processing", "content": "In contrast to parallel processing, in this case the central server uses the maximum computational resources for each device and processes the incoming data serially one by one. For this reason, the central server utilizes a queue of $K$ positions. Assuming that a particular BM has been selected for the slicing policy, the delay of each device upon arrival at the queue is given by $C_k$ in (3) plus a $F_k/f_{max}$ term corresponding to the task processing delay from the server resulting from (4). However, a device may need to wait for previous tasks in the queue to finish before entering its processing phase. Note that tasks enter the queue in order of arrival, so devices are ordered in the queue according to their delay $C_k$, with shorter delays placing data earlier in the queue.\nLet $K'$ represent a permutation of the devices in $K$ that reflects the arrival order of their data. We define the total inference delay $I_k$ for a device $k \\in K'$ as follows\n$I_k = \\begin{cases} C_1 + \\frac{F_1}{f_{max}}, & k = 1\\\\ max \\{I_{k-1}, C_k\\} + \\frac{F_k}{f_{max}}, & 1 < k < K, \\end{cases}$\nMinimizing the overall inference delay for all devices reduces to minimizing $I_K$, the delay of the last device in the queue. However, since a particular arrival order in the queue, $K'$, depends on the delay of the devices, this makes it extremely difficult to optimize bandwidth allocation, since a different bandwidth allocation results in a different queue, and thus a different delay per device according to (10). Furthermore, with $K$ devices in the queue, there are $K!$ possible queue configurations, making the problem highly complex, especially for real-time applications. This motivates the need for simpler, low-complexity approaches that can still perform effectively. Accordingly, we investigate two alternative schemes that are more tractable."}, {"title": "1) Local Processing and Transmission Delay Minimization:", "content": "To avoid a potentially long transmission delay that would result in an increased total inference time, a useful scheme is to use bandwidth allocation to achieve simultaneous arrival of all tasks at the BS, resulting in a unique queue order of the devices. In this case, the inference delay minimization problem can be formulated as follows\n$\\min_{\\alpha_{k,l}, B_k} \\{\\max_l \\{\\alpha_{k,1}C_{k,1}\\} + \\frac{\\Sigma_{k=1}^K F_{k,l}}{f_{max}}\\}$\ns.t.\n$C1: \\Sigma_{k=1}^K B_k = B_{tot},$\n$C2: \\Sigma_{l=1}^L \\alpha_{k,l} = 1,\\forall l \\in \\{1,\\ldots, L\\},$\nwhere $C_{k,l}, F_{k.l}$ are the extended definitions of $C_k, F_k$ in (3) and (4) to include the BM index $l$, which now changes. Note that the choice of $\\alpha_{k,l}$ affects $F_{k,l}$. Using alternating optimization between $\\alpha_{k,l}, B_k$, we can derive an optimal solution to (P3) by iteratively solving a convex optimization problem and searching for the $l^*$-th layer that minimizes all sequences $C_{k,l^*}, \\forall k \\in K$."}, {"title": "2) Equal Bandwidth Allocation:", "content": "In this scheme, we aim to leverage the different arrival rates of device data. If a device's data arrives while the previous data is still being processed, no additional delay is introduced, as described in (10). However, if a device's data arrives after all previous data has been fully processed, the central server sits idle, increasing the total inference delay. To address this problem, we propose a heuristic algorithm designed to reduce delays when such idle gaps are likely to occur. We begin by introducing some key definitions about the queuing system and the inference delay, taking into account the arrival order of the data.\nDefinition 1: A \"break\" between the data arrivals of devices occurs when a device k experiences a delay, i.e., its data arrives after the processing of the previous device has been completed. This can be expressed as $C_k = max\\{I_{k-1}, C_k\\}$, for some $k \\in K'$. If there are no breaks, i.e., each device's data arrives as soon as the previous one finishes processing, the queue is called \u201cunbroken\", and this is represented by the condition $I_{k-1} = max\\{I_{k-1}, C_k\\}, \\forall k \\in K'$. A queue is called \u201c(M, M)-broken\u201d if it contains M consecutive unbroken sub-queues, where the set M = $\\{k | I_{k\u22121} < C_k, 1 < k < K\\}$ specifies the indices where M \u2212 1 breaks occur in ascending order within the queue.\nAccording to definition 1, for each device in an unbroken queue consisting of Q devices, it holds that\n$I_q = I_{q-1} + \\frac{F_k}{f_{max}}, \\forall q \\in Q, I_{q1} = C_{q1} + F_{91}/f_{max}$,\nwhere Q is the set containing all Q devices in the investigated unbroken queue, and $q_1$ is the index of the first device in this queue. Thus, the total inference delay for all devices in this queue is given by\n$I_Q = I_Q = C_{q1} + \\Sigma_{k \\in Q} \\frac{F_k}{f_{max}}$.\nThen, by definition, the total inference delay of the (M, M)-broken queue, denoted as $T_{K}^{(M,M)}$, is given by\n$T_K^{(M,M)} = I_K = C_{max\\{M\\}} + \\Sigma_{k=max\\{M\\}}^K \\frac{F_k}{f_{max}}$,\nbecause all devices before the device where the last break occurs have completed their tasks. From (13) we see that if the local processing and transmission delay of the (M, M)-broken queue $C_{max\\{M\\}}$ is reduced, the total inference delay of all devices will also be reduced. Driven by this observation, we will prove that by reallocating bandwidth from the device associated with the first break to the device associated with the last break, the total inference delay time is reduced. Note that such a technique is feasible because the (min{M} \u2212 1)-th device of the initial queue can allow an increase in its delay, so that\n$C'_{min\\{M\\}-1} = C_{min\\{M\\}} - \\frac{F_{min\\{M\\}-1}}{f_{max}}$,\nbecause this ensures that $I'_{min\\{M\\}-1} = C'_{min\\{M\\}}$, which simply eliminates the first break without changing $T_K^{(M,M)}$, since the latter depends only on the last break. The following lemma proves that by reallocating the bandwidth between the device corresponding to the first break and the device corresponding to the last break and using (14), the queuing inference delay is reduced.\nLemma 2: Let M > 3 and let $M_1 = \\{\\{M, min\\{M\\} - 1\\}\\{\\max\\{M\\}, min\\{M\\}\\}\\}$ be the set of the intermediate M \u2212 3 breaks of the (M, M)-broken queue. Let also $M_2 = \\{\\{M, min\\{M\\}\u22121,max\\{M\\}+1\\}\\{\\max\\{M\\}, min\\{M\\}\\}\\}$ denote the set of the last M \u2212 2 breaks of the (M, M)-broken queue, but with the last break occurring at the next position. Finally, let $M_3 = \\{\\{M, min\\{M\\} \u2212 1, max\\{M\\}+1\\}\\{min\\{M\\}\\}\\}$ be the set of the same breaks as the (M, M)-broken queue, but with an additional occurrence at the max{M} + 1 position. All $M_1, M_2, M_3$ contain an additional break at the min{M}\u22121 position. Then, the (M, M)-broken queue has a worse inference delay time than the (M \u2013 1, $M_1$)-broken, (M, $M_2$)-broken and (M + 1, $M_3$)-broken queues."}, {"title": "Algorithm 1: Alternating Optimization implementing Heuristic Algorithm for Serial Processing", "content": "1 Fix number of iterations, iters.\n2 For each device perform the slicing policy $\\alpha_{\\kappa\\iota}, \\theta_{\\kappa} \\in K$.\n3 for j = 1: iters do\n4 Fix bandwidth for each device equal to $B_k = B_{tot}/K$.\n5 Sort the devices in terms of $C_k$ and calculate $I_k$, characterizing the queue.\n6 while M> 2 do\n7 Find the number of \"breaks\" and create M.\n8 Solve (14) to get $B'_{min\\{M\\}-1}$\n9 Calculate bandwidth excess, $\\Delta B = B_{min\\{M\\}\u22121} - B'_{min\\{M\\}\u22121}\u00b7$\n10 Change bandwidth of the max{M}-th element of the queue to $B'_{max\\{M\\}} = B_{max\\{M\\}} + \\Delta B$.\n11 Find optimal slicing policy $\\alpha_{k,1}, \\forall k \\in K$ as in (P3).\n12 Keep minimum achieving slicing policy $\\alpha_{\\kappa\\iota}, \\forall k \\in K$ and bandwidth allocation vector."}, {"title": "C. Complexity Analysis", "content": "In this subsection, we compare the proposed schemes in terms of complexity. For the parallel processing schemes, (P2) can significantly reduce the complexity compared to (P1). Unlike many convex optimization techniques, the proposed one does not need more iterations, since only the first root of the equation in (6) is computed, which is in a small interval. If (P2) is solved with (6), it is easy to see that the complexity is given as O(2|F|) \u2248 O(2K), since only the minimum of the two sequences described by (7) is required. On the other hand, (P1) may perform better, but it has twice as many variables and its complexity can be derived as O ((2K)3.5) [30", "30": "."}]}