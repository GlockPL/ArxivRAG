{"title": "Deep Fusion Model for Brain Tumor Classification Using Fine-Grained Gradient Preservation", "authors": ["Niful Islam", "Mohaiminul Islam Bhuiyan", "Jarin Tasnim Raya", "Nur Shazwani Kamarudin", "Khan Md Hasib", "M. F. Mridha", "Dewan Md. Farid"], "abstract": "Brain tumors are one of the most common diseases that lead to early death if not diagnosed at an early stage. Traditional diagnostic approaches are extremely time-consuming and prone to errors. In this context, computer vision-based approaches have emerged as an effective tool for accurate brain tumor classification. While some of the existing solutions demonstrate noteworthy accuracy, the models become infeasible to deploy in areas where computational resources are limited. This research addresses the need for accurate and fast classification of brain tumors with a priority of deploying the model in technologically underdeveloped regions. The research presents a novel architecture for precise brain tumor classification fusing pretrained ResNet152V2 and modified VGG16 models. The proposed architecture undergoes a diligent fine-tuning process that ensures fine gradients are preserved in deep neural networks, which are essential for effective brain tumor classification. The proposed solution incorporates various image processing techniques to improve image quality and achieves an astounding accuracy of 98.36% and 98.04% in Figshare and Kaggle datasets respectively. This architecture stands out for having a streamlined profile, with only 2.8 million trainable parameters. We have leveraged 8-bit quantization to produce a model of size 73.881 MB, significantly reducing it from the previous size of 289.45 MB, ensuring smooth deployment in edge devices even in resource-constrained areas. Additionally, the use of Grad-CAM improves the interpretability of the model, offering insightful information regarding its decision-making process. Owing to its high discriminative ability, this model can be a reliable option for accurate brain tumor classification.", "sections": [{"title": "1 Introduction", "content": "Brain tumors are abnormal formations of cells that may occur in the brain or in the tissues surrounding the central nervous system (CNS). They may be classified as either cancerous (malignant) or noncancerous (benign). Malignant tumors exhibit fast growth and have the ability to spread to surrounding tissues, whereas benign tumors develop at a slower pace. According to statistical data, almost 70% of brain tumors are classified as benign, while the remaining 30% are categorized as malignant [1]. Researchers have identified over 120 distinct forms of brain tumors, including prominent examples such as Gliomas, Meningiomas, and Pituitary Tumors. Gliomas, the predominant kind of brain tumor, arise from glial cells that support and nourish neurons. They constitute 30% of CNS and 80% of Glioma tumors are malignant. Meningiomas are benign tumors that grow slowly and originate in the meninges, which are the membranes that surround the brain and spinal cord inside the skull [2]. Pituitary tumors arise inside the pituitary glands, which are responsible for hormone regulation and bodily functions and can range from benign to potentially invasive or cancerous. Complications from pituitary tumors can lead to long-term hormonal imbalances and vision problems [3]. Brain tumors can cause various neurological symptoms, including migraines, epileptic seizures, and cognitive decline. In addition to affecting physical health, they can also affect emotional and psychological well-being. Children with brain tumors may experience delays in social and academic development [4]. Tumors near the sensory processing areas can lead to vision and hearing abnormalities. Medical professionals typically use medical history, physical examinations, and imaging tests such as computed tomography (CT) or MRI scans to diagnose brain tumors [5]. Biopsies, which include extracting tiny tissue samples for microscopic analysis, are often required to ascertain the characteristics and severity of the tumor. MRI and CT scans help categorize brain tumors based on their appearance in imaging studies [6]. Precise categorization of brain tumors is essential for developing personalized treatment plans, assessing prognosis, and conducting research to improve treatment outcomes. Neurosurgeons, neurologists, and oncologists combine these classification techniques to determine the most appropriate treatment approach [7].\nConvolutional Neural Networks (CNNs) have become a potent tool for classifying brain tumors in the area of medical image analysis, particularly when working with images from MRI or CT scans. CNNs are a specific kind of deep learning algorithms that are highly regarded for their ability to effectively identify and categorize patterns and classes present in images. These networks operate through multiple layers, including specialized convolutional layers that excel at extracting critical features such as edges and textures from images. Subsequent pooling operations refine these features and preserve essential details while reducing spatial dimensions. CNNs' prowess in image processing"}, {"title": "2 Related Works", "content": "In recent times, researchers have suggested various techniques for categorizing images of brain tumors. Among them, CNN-based approaches are the most commonly used strategies due to their effectiveness. Noreen et al. [10] conducted an experiment where they fine-tuned the pre-trained Inception-v3 and DenseNet201 models to assess their performance in classifying brain tumors. Their objective was to discover which model outperformed the other. During the fine-tuning phase, the authors eliminated some blocks (i.e. the inception block and dense block) from the lower levels of both models. The features generated from the intermediate inception and dense blocks were then sent through a succession of average pooling, dropout, and fully connected layers before being concatenated and categorized using softmax. According to the experiment, the fine-tuned DenseNet outperformed Inception-v3 by a small margin. High-resolution medical images can play a crucial role in the classification of various diseases. Nevertheless, due to the high expense of acquisition and storage, their widespread utilization may be limited. Mohsen et al. [11] addressed this issue by employing a Generative Adversarial Network (GAN). The generator produced a 256\u00d7256 image using a 64\u00d764 size image as input. The high-resolution image was then classified with ResNext and VGG networks. Asif et al. [12] experimented with four pre-trained deep neural networks: Xception, NasNet, DenseNet, and InceptionResNet. After undergoing various data pre-processing and augmentation techniques, Xception outperformed the other three models in the comparison. Shah et al. [13] performed fine-tuning on EffecientNetB0 to accurately identify three distinct categories of brain cancers. Efficient NetB0 is a compact image classifier that provides cutting-edge performance by using multi-branch convolution. The authors conducted a comparison between EffecientNetB0 and five state-of-the-art CNN architectures. The experiment suggested the superiority of the proposed solution. Kang et al. [14] conducted experiments using several pre-trained deep neural networks to identify the most effective feature extractors. In order to identify the optimal feature extractor, the authors subjected the features to nine machine learning algorithms and evaluated their average performance. Subsequently, the top three best feature extractors (DenseNet, Inception, and ResNeXt) were considered to construct the final ensemble classifier. Khan et al. [6] proposed two CNN architectures for classifying brain tumor images from two different datasets. On the Havard Medical Dataset, the model employed a pre-trained VGG16 and for the Fighshare dataset, the authors proposed a 23-layer CNN architecture. Owing to the high resolution of the input image, the model consumes more than twice the computation time compared to its similar research works.\nResearchers have proposed many shallow CNN architectures for brain tumor classification because of the significant computational cost of CNN models. Classifiers with four blocks of CNN provided the highest level of efficiency. Montaha et al. [15] introduced a specialized CNN architecture consisting of four blocks, in which the convolutional and pooling layers were combined with Long Short-Term Memory (LSTM). Through a comprehensive ablation analysis, the suggested model attained an accuracy of 98.90% on a merged dataset, resembling the prior approach. Additionally, Musallam et al. [16] introduced a customized CNN model with a feature extractor consisting of four blocks. The first two blocks consisted of two convolutional layers, whereas the latter two blocks were composed of three convolutional layers apiece. Batch normalization was used after each convolutional layer to accelerate the convergence. Additionally, a 2\u00d72 max-pooling layer was included at the end of the block to decrease the spatial dimension. Amin et al. [17], introduced a novel CNN model consisting of four layers. The model incorporated a Discrete Wavelet Transform (DWT) to merge four different kinds of MRI data into a single image. After the fusion procedure, the authors used a global thresholding strategy to partition the tumor area. The segmented area was then designated as the area of Interest (ROI), which was used to precisely identify the tumor location in the original image. Subsequently, the annotated images were passed through the CNN model for classification. The authors assessed the proposed architecture using five publicly accessible brain tumor datasets and determined that the fused images demonstrated superior performance compared to classification based on a single images. Khairandish et al. [18] introduced a convolutional neural network (CNN) consisting of only two convolutional layers. The dataset was first segmented by the system using threshold-based segmentation. The segmented image was then passed through the CNN feature extractor, which consisted of two 5\u00d75 convolutional layers and two 2\u00d72 pooling layers. The features obtained from the Convolutional Neural Network (CNN) were ultimately categorized using a Support Vector Machine (SVM) classifier. The experiment illustrated that the integration of the SVM increased CNN's classification performance by 1%. Diaz et al. [19] proposed a three-stream CNN architecture that extracted multi-scale features using only two convolutional layers. The first stream extracted large features with 11\u00d711 convolutional blocks, and the second stream incorporated 7\u00d77 convolutional blocks to capture median scale features. Finally, the last stream consisted of 3\u00d73 convolutional layers to detect small features. The features extracted from all streams were concatenated and classified using a Multi-Layer Perceptron (MLP) head.\nTransformers, initially designed for natural language processing, have now been widely used for image recognition tasks. Some well-known transformer architectures have gained attention due to their superiority over traditional CNN architectures in various image classification challenges [20, 21]. Due to their high classification performance, researchers have integrated vision transformers for brain tumor classification. Tummala et al. [22] ensembled four vision transformer (ViT) models for effective brain tumor classification. Among the four ViT models, two of them were ViT Base (B/16 and B/32), and two others were ViT Large (L/16 and L/32). Traditionally ViT models have a huge number of parameters, especially ViT Large models, which boast over three times as many parameters as ViT Base. Furthermore, due to the self-attention mechanism, where each patch must attain every other patch, the computation becomes quadratic, making ViT relatively slow. Therefore, the proposed ensemble approach faces challenges where computational power is moderate. Ferdous et al. [23] presented a solution to the quadratic time consumption of self-attention by replacing the attention mechanism with external attention, which is of linearly complex. The authors trained the modified transformer through knowledge distillation where a CNN model was considered as the teacher model and the proposed transformer as the student model. Since the feature extraction mechanisms of CNN and transformers were different, some researchers have combined CNN and transformers to attain better accuracy. Aloraini et al. [24] presented a two-stream architecture where the first stream was comprised of a transformer and the second stream was a CNN. For the CNN stream, the authors employed a pre-trained DneseNet201. Due to the local and global feature extraction with the CNN and transformer, respectively, the proposed solution achieved high classification performance."}, {"title": "3 Methodology", "content": "The proposed solution comprises three major steps: data pre-processing, model construction, and model quantization. Section 3.1, 3.2, and 3.3 illustrate these steps in detail."}, {"title": "3.1 Data Pre-processing", "content": "We have leveraged two publicly available datasets for conducting the study. The first dataset is the Figshare dataset [25] that comprises 3064 T1-weighted brain tumor images from three classes. Collected from 233 patients, the slightly imbalanced dataset consisted of 708, 1426, and 930 images of meningiomas, gliomas, and pituitary tumors respectively. The images of that dataset are collected from three distinct viewpoints. The second dataset employed is commonly known as the Kaggle dataset [26] which consists of 253 MRI images. 98 of those images are categorized as non tumorous while the rest of the images are tumorous. Unlike the Figshare dataset, the images of the Kaggle dataset are collected from one viewpoint. In the data preprocessing stage, the dataset was first split into three non-overlapping training(70%), validation(10%), and test(20%) sets. The subsequent four steps are the region of interest (ROI) selection, adaptive histogram equalization, data augmentation, and resizing. The details of these processes are presented in Section 3.1.1, 3.1.2, 3.1.3, and 3.1.4 respectively."}, {"title": "3.1.1 ROI Selection", "content": "As presented in Figure la and 1b, the images have a noticeable dark background that does not contribute to the classification process. Consequently, eliminating these portions enhances the accuracy of classification. In order to choose the region of interest (ROI), the images were first segmented using Otsu's thresholding technique. This image segmentation approach utilizes a histogram to calculate the optimal threshold value by minimizing the intra-class variance. This technique often utilizes grayscale images and produces a binary image [27]. The threshold value is determined using Equation 1. Here, $k_{otsu}$ represents the predicted threshold value for Otsu's thresholding, whereas k denotes the possible threshold candidates. The value of k lies from 0 to 255. The variables $q_1(k)$ and $q_2(k)$ represent the probabilities of the background and foreground pixels, respectively. Similarly,"}, {"title": "3.1.2 Adaptive Histogram Equalization", "content": "Due to various factors such as variations in tissue thickness, illumination during imaging, or shadows caused by tumors, images in medical imaging, including images of brain tumors, frequently have uneven lighting. Important details in an image may be difficult to visualize because of these variations. Contrast Limited Adaptive Histogram Equalization (CLAHE) can be employed to address this issue. This image processing algorithm enhances the contrast and visibility of details in an image. Standard histogram equalization operates on the entire image which can degrade the quality of a brain tumor image because the tumor portion is a small area of the image. Therefore CLAHE is leveraged to divide the image into small tiles. Histogram equalization was applied within each tile. To prevent over-enhancement and noise amplification, the contrast enhancement of each tile is limited. To prevent noticeable artifacts at the tile boundaries, CLAHE employs a sliding window approach with overlapping tiles. As the tumor lesion contributes a small portion and CLAHE operates with smaller portions of the image, it greatly improves the image quality highlighting the tumor region. In the proposed solution, the tiles are of size 8\u00d78 in size and the contrast limit for localized changes is 2.0."}, {"title": "3.1.3 Augmentation", "content": "Since the dataset had a limited number of samples, the training set was augmented to increase the robustness and reduce the overfitting of the model. Six real-time data augmentation techniques were applied as illustrated below.\n\u2022 Random Rotation: A 40-degree clockwise or counterclockwise rotation is applied at random.\n\u2022 Height Shift: Random height shift is applied by a maximum of 20%.\n\u2022 Width Shift: Random width shift is applied by a maximum of 20%.\n\u2022 Shear Transform: A shear transformation was employed on the input images, utilizing a maximum shear angle of 0.2 degrees.\n\u2022 Zooming: Some random images are zoomed with a zooming range of 20%.\n\u2022 Random Flipping: Some images are randomly flipped horizontally or vertically."}, {"title": "3.1.4 Resizing", "content": "Resizing is a commonly used data preprocessing technique, particularly in the context of transfer learning. Generally, the state-of-the-art models employ ImageNet [29] for training the model. Although the images in ImageNet are of size 469\u00d7387, they are mostly resized to 255\u00d7255 or 224\u00d7244 for architectural compatibility and resource efficiency [30]. In order to fully utilize the advantage of transfer learning, we have also resized the images to 224\u00d7244 which makes the model compatible with the pretrainded classifier. We have leveraged the nearest neighbor interpolation algorithm for resizing. The algorithm resizes images by selecting the nearest pixel value from the original image to determine the color of each pixel in the resized image."}, {"title": "3.2 Model Construction", "content": "The fusion model, presented in Figure 3, consists of two advanced image classifiers, namely ResNet152V2 and VGG16. The features obtained from the pre-trained feature extractors are combined and then sent to an attention module to prioritize the important features. Upon completion of the fine-tuning process, the model was quantized and prepared for classification. The following sections provide an elaborate synopsis of the proposed classifier."}, {"title": "3.2.1 ResNet152V2", "content": "ResNet152V2 is an improvement over the original ResNet architecture. Traditional sequential CNNs architectures face the vanishing gradient problem, where the gradients of the loss function with respect to the network parameters become extremely small as they are backpropagated through numerous layers. This problem can make it difficult to train deep neural networks, which can cause learning to converge slowly or even stop altogether. ResNets provided a solution to this issue by incorporating skip connections, also known as residual connections [31]. In this network, the features extracted from the previous block are passed on to the next block. This addresses the vanishing gradient problem while ensuring feature reuse, thereby making the construction of deep neural networks possible. Several versions of ResNet have been developed since its introduction in 2016. ResNet152V2, which is composed of 152 layers, is an improvement over the base ResNet architecture. This model integrates batch normalization which accelerates up the training process [32]. Moreover, a deeper bottleneck was integrated to extract more complex features while maintaining the complexity manageable."}, {"title": "3.2.2 Modified VGG16", "content": "Visual Geometry Group 16, commonly known as VGG16, is a 16-layer convolutional neural network used for various image recognition tasks. Developed in 2014, this deep learning architecture has gained popularity due to its simplistic architecture and high performance. With a stack of convolutional layers followed by max-pooling layers, VGG16 achieved state-of-the-art performance. However, since VGG16 architecture does not integrate any residual connections, some small or fine-grained features tend to be oversmoothed or removed. This characteristic becomes especially important when processing images of brain tumors because the tumor region typically makes up a small portion of the overall image. Accurate tumor detection and characterization depend heavily on recognizing and maintaining minute details. Therefore, we present a novel architecture that addressed this challenge. With minimal fine-tuning, our method passes the features extracted from the third, fourth, and fifth convolutional blocks directly for classification purposes. This method enables us to extract and store essential fine-grained data, thereby enhancing the performance of the model in the accurate localization of tumor regions within brain images."}, {"title": "3.2.3 Fine Tuning", "content": "The fine-tuning process involved fusing the features from ResNet152V2 and modified VGG16. Instead of relying on a single classifier, combining different feature extractors with a heterogeneous architecture produces a significantly better performance. Because the pretrained image classifiers are trained on ImageNet, which represents a different domain, adopting an attention module after the feature extraction stage can help generalize better in the medical domain, which differs in terms of image characteristics, textures, and structures [34]. Therefore, the fusion model integrates a dual attention module after the feature fusion stage. Dual attention is a complex attention module that integrates both channel and spatial attention [35]. Let the input features be denoted by $X$ and the height and width of the features be $H$ and $W$ respectively. The channel attention mechanism can be explained using Equation 3. To obtain the global description $U$, the global average pooling of each layer was performed. Subsequently, the global description is multiplied by $W_1$, the weight matrix of the first dense layer which is then passed through the ReLU activation function. Similarly, the feature vector is multiplied by $W_2$, the second weight matrix and passed through the sigmoid activation function to obtain the channel attention weight $A_c$. Because the range of the sigmoid funciton is [0, 1], each channel is prioritized based on its importance to the output, with 0 being irrelevant and 1 being fully relevant. Finally, the attention weight was reshaped to match the input feature size and multiplied with it to obtain the channel prioritized output."}, {"title": "3.3 Model Quantization", "content": "In TensorFlow, deep learning models typically have 32-bit precision, which indicates that the weights, biases, and activation values are stored in 32-bit floating-point numbers. While higher bit precision ensures greater precision, lowering the number of bits allocated to each value results in a more compact model, which is a major concern in resource-constrained applications. Model quantization is a technique that systematically converts the weights, biases, and activation values of a neural network from its original floating-point format to a lower-precision format, ensuring minimum or no performance degradation. For compressing the proposed model without compromising the performance, we have leveraged 8-bit quantization that compresses the feature extractor by a factor of 4. The careful compression prevented over-quantization, which resulted in no performance loss. We compressed the feature extractor, which initially had a size of 289.45 MB. After quantization, the feature extractor was reduced to 73.88 MB. Following the feature extractor, the XGBoost classifier remained at a size of 0.72 KB, undergoing no compression. Overall, the full classifier is reduced to a size of less than 73.881 MB through quantization."}, {"title": "4 Experiment", "content": "This section presents the outcomes derived from the proposed model, along with a thorough comparison with the existing research."}, {"title": "4.1 Experimental Setup", "content": "The experiment was carried out with Kaggle. The programming language used was Python. The configuration included a GPU P100 to expedite the training process of the deep learning model. This experiment used many Python libraries, including numpy (used for numerical and scientific computing), pandas (for data manipulation and analysis), os (for data read/write), tensorflow (for developing deep learning models), sklearn (used for XGBoost and performance evaluation), TensorFlow Lite (for model quantization), and matplotlib (for visualization)."}, {"title": "4.2 Result Analysis", "content": "We performed a thorough assessment of the proposed solution for the classification of brain tumors. We employed multiple evaluation metrics for assessing the classifier, including accuracy, precision, recall, F1-score, and Matthews Correlation Coefficient (MCC). Among the four matrices, MCC is considered the most important one for measuring the model's completeness as it considers all the coordinates for the result calculation [36]. These metrics serve as a foundation for our assessment and provide comprehensive insight into the capabilities of our model. We conducted a side-by-side comparison of MLP and XGBoost, two classification heads of the model. Moreover, we included confusion matrices and Receiver operating characteristic (ROC) curve to provide detailed insight into the performance of our model.\nOur initial classification head, the MLP, displayed remarkable outcomes in all metrics. It achieved a notable accuracy of 92.17% and 92.16% in the Figshare and the Kaggle dataset respectively, indicating that the predictions of our model were mostly accurate. The model's recall of 91.41% on the Figshare and 92% on the Kaggle dataset indicates its excellent performance in identifying true positive instances, while its precision of 91.40% and 93.33% on the Figshare and Kaggle dataset showcases its excellent performance in minimizing false positives. Impressively, the model achieved an Fl-score of 91.20% on the Figshare dataset and 92.08% on the Kaggle dataset indicating a high level of balance between precision and recall. In addition, the proposed solution attained an MCC of 87.88% and 85.32% on the Figshare and the Kaggle datasets, respectively, suggesting a comprehensive solution that considers both true and false positives as well as true and false negatives."}, {"title": "4.3 Model Attention", "content": "This work employed Gradient-weighted Class Activation Mapping (Grad-CAM) [37] to investigate the model's priority regions during decision-making. Grad-CAM is an effective approach that enhances the transparency of the decision-making process in deep learning models, specifically in the domain of computer vision. It offers a heatmap of the specific regions of input images that have a major contribution on a model's predictions. Grad-CAM utilizes the gradient values of the target class with respect to the feature maps in the last convolutional layer of a neural network. This technique generates a heatmap that visually represents the relative importance of various regions within an input image in relation to the model's decision. This research used Grad-CAM to render the preferences of our intricate neural network.\nFigure 9 showcases a side-by-side comparison of three preprocessed images along with their corresponding heatmaps on the Fighsare dataset. The attention map illustrates that the model predominantly focuses on the tumor regions during the decision-making process. While our approach primarily caters to critical regions, our investigation also reveals cases where it incorrectly concentrates on unimportant regions."}, {"title": "4.4 Comparison with Existing Works", "content": "Over the past few years, several studies have been conducted to classify brain tumors. Most studies have employed CNN as the classifier. The authors predominantly employed various pretrained feature extractors that were slightly fine-tuned for the final classification. Custom shallow CNN architectures without transfer learning have also achieved acceptable performance. Additionally, some researchers have localized the tumor portion leveraging R-CNN and employed various deep learning algorithms for final classification. Notably, a combination of deep learning and traditional machine learning is also seen which has produced an outstanding performance. A comparison with recent studies on the Figshare dataset is presented in Table 2.\nLikewise, Table 3 holds a direct comparison with the recent research on the Kaggle dataset. Since the dataset has only 253 instances, lightweight neural networks, such as ResNet50, often produce an acceptable accuracy. The comparison shows that the proposed model outperforms most existing methods by a noticeable margin.\nThe existing solutions have three major drawbacks that the research addresses. Firstly, the medical images typically consist of noises and uneven lighting which significantly influences the classification performance. Therefore, the proposed system includes a variety of data preparation techniques to improve the image quality, addressing a common shortcoming of recent efforts in the field. Moreover, some deep neural networks such as VGG16 drop performance due to the vanishing gradient problem. We also presented a solution that addresses this issue without compromising performance and resource consumption. Additionally, deep learning architectures tend to produce relatively low performance if the network is not pre-trained. In the transfer learning method, the top layer is typically trained from scratch which may lead to a decrease in the performance if the training data is limited. To address this shortcoming, we replaced the top MLP layer with XGBoost, a high-performing machine learning classifier. With all the improvements, the proposed solution achieved significant performance in brain tumor detection."}, {"title": "4.5 Discussion", "content": "The study on a computer vision based diagnosis of brain tumors addresses the critical need for accurate and fast diagnosis of the disease. While existing studies offer solution to the problem, they primarily focus on designing classifiers with high accuracy. However, since deep learning models typically have a large size, they are infeasible to integrate in remote areas, particularly where high-configuration devices are unavailable. We have addressed this limitation by carefully designing the classifier to be lightweight without compromising the performance. Furthermore, we have leveraged quantization to effectively reduce the model size to 73.881 MB which makes the model easily deployable into edge devices. Additionally, some research employs only one dataset for evaluating the classifier. This increases the threat of the model being biased towards a particular dataset. To address the censorious issue, we have employed Grad-CAM to visualize the attention heatmap of the model. According to the analysis, the model predominantly focuses on the tumor regions for making the decision. The experiment enhances the reliability of the classifier. The proposed system, however, lacks additional studies that can be addressed in the future. Firstly, due to the variety in the image acquisition process of different medical devices, the model needs to be domain-adaptive for its widespread utilization. Future studies may include additional datasets, acquired from different devices to evaluate the model's efficiency. Additionally, no ablation study is performed to understand the impact of the hyperparameters. A thorough ablation study will provide further insights into the model which may result in an advancement in the classification accuracy. Nevertheless, considering the high classification accuracy and resource efficiency, the solution can be integrated into diagnostic centers located in remote regions with poor internet connectivity and unavailability of high computing devices."}, {"title": "5 Conclusion", "content": "This study introduces a novel method designed to classify brain tumors in an efficient manner. The classification process encompasses a range of image preprocessing methods aimed at enhancing image quality. For classification, we have proposed a deep learning fusion model comprising ResNet152V2 and VGG16. Due to the over-smoothing problem of VGG16, we have altered the architecture to preserve the fine gradients. Following a small fine-tuning, the retrieved features are concatenated and finally classified using XGBoost. The suggested lightweight system is designed to enhance both performance and resource efficiency, attaining an accuracy of over 98% while using merely 2.8 million trainable parameters and undergoing 50 training epochs. The model is further compressed through 8-bit quantization, making the model easily deployable in edge devices. The model is thoroughly evaluated on the Figshare and the Kaggle datasets, and the findings demonstrated its superiority over existing solutions. The solution, however, has two major drawbacks. Firstly, the solution is evaluated on only two datasets. Medical images may exhibit slight variations due to the differences in the capturing devices. Domain adaptation can serve as a potential solution to this problem. Moreover, this research lacks an ablation study that may have presented more insights into this research. Further studies can be conducted to address these limitations."}, {"title": "Data Availability", "content": "The Figshare dataset is available at https://figshare.com/articles/dataset/brain_tumor_dataset/1512427 (accessed on 16 May 2024) and the Kaggle dataset is available at: https://www.kaggle.com/datasets/navoneel/brain-mri-images-for-brain-tumor-detection/ (accessed on 16 May 2024)."}, {"title": "Conflicts of Interest", "content": "The authors declare no conflict of interest."}]}