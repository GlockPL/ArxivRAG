{"title": "C2: Scalable Auto-Feedback for LLM-based Chart Generation", "authors": ["Woosung Koh", "Jang Han Yoon", "MinHyung Lee", "Youngjin Song", "Jaegwan Cho", "Jaehyun Kang", "Taehyeon Kim", "Se-young Yun", "Youngjae Yu", "Bongshin Lee"], "abstract": "Generating high-quality charts with Large Language Models presents significant challenges due to limited data and the high cost of scaling through human curation. (instruction, data, code) triplets are scarce and expensive to manually curate as their creation demands technical expertise. To address this scalability issue, we introduce a reference-free automatic feedback generator, which eliminates the need for costly human intervention. Our novel framework, C2, consists of (1) an automatic feedback provider (CHARTAF) and (2) a diverse, reference-free dataset (CHARTUIE-8K). Quantitative results are compelling: in our first experiment, 74% of respondents strongly preferred, and 10% preferred, the results after feedback. The second post-feedback experiment demonstrates that CHARTAF outperform nine baselines. Moreover, CHARTUIE-8K significantly improves data diversity by increasing queries, datasets, and chart types by 5982%, 1936%, and 91%, respectively, over benchmarks. Finally, an LLM user study revealed that 94% of participants preferred CHARTUIE-8K's queries, with 93% deeming them aligned with real-world use cases. Core contributions are available as open-source at an anonymized project site, with ample qualitative examples.", "sections": [{"title": "1 Introduction", "content": "Charts are a powerful means to convey information in diverse fields, including journalism, business, and scientific research. With the success of foundation models, there has been an increasing demand for generating charts using Large Language Models (LLMs). For instance, Dibia uses LLM-generated charts for infographic curation, and Maddigan and Susnjak incorporate LLM-generated charts in their software interface. Moreover, LLM-generated charts empower humanity by allowing non-experts to generate high-quality charts and improving accesibility to those with special needs. Despite the rising interest, two key challenges persist: (i) the difficulty in evaluating LLM-generated charts, which hampers their quality, and (ii) the limited availability of training data.\n(i) Chart generation lacks straightforward evaluation methods, making it difficult to assess and improve the quality of LLM-generated charts. Unlike tasks with clear-cut answers, such as mathematical problem-solving where verifiers can automatically assess correctness, chart evaluation is inherently subjective. Multiple correct designs may exist for one task (or goal), and quality often aligns with human aesthetic and functional preferences. Consequently, current evaluation systems rely on reference-based approaches, necessitating labor-intensive (instruction, data, code) triplets as a gold reference for evaluation and thus limiting their scalability.\n(ii) Furthermore, in contrast to image generation, which typically requires only (instruction, image) pairs, chart generation demands more complex (instruction, data, code) triplets. This complexity significantly increases the costs associated with data collection and annotation. The scarcity and limited diversity of available data restrict the variety of charts users can generate, making chart generation expensive and labor-intensive even for common real-world applications"}, {"title": "1.1 Related Work", "content": "Chart Generation. Maddigan and Susnjak offer prompt-engineered, LLM-based chart generation-however, they only provide qualitative case studies to verify their contribution. Dibia presents an LLM-based infographic visualization tool, which includes interactive charts. However, Dibia does not include a human study, making it challenging to assess its effectiveness. Sah et al. propose a natural language-to-chart recommendation approach based on the visualization language Vega-Lite. Therefore, their task deviates from the LLM-based chart generation we tackle. Tian et al. recently proposes a chart generation work that generates charts from \"abstract user utterances\" (as stated in the paper), which diverges from the instruction-based queries we address. An example of an \"abstract user utterance\" they provide is \"What kind of movies earn the most recently?\"-this diverges from the example instructions provided in our LLM user study. This difference is understandable as their chart generation is based on proprietary user-interaction software, not a common LLM chatbot.\nTuning with Feedback. LLMs are tuned via three methods: parameter tuning, in-context tuning, and test-time scaling (TTS; OpenAI (2024)). Parameter tuning occurs through (implicit or explicit) rewards, requiring large amounts of reward data. For instance, Kirstain et al. and Xu et al. collected over 500,000 and 137,000 annotations, respectively. In-context tuning allows LLMs to improve via n-shot generation, which requires n additional prompts. TTS refers to the case where a verifier with an ordinal output (typically of scalar value) can help improve the final generation. Each tuning method requires an external feedback-provider, such as a reward model, additional prompt, or verifier."}, {"title": "2 C2: CHARTAF", "content": "In C2, a feedback-provider, CHARTAF enables TTS and in-context tuning. We first describe the shortcomings of existing approaches, and introduce two versions of CHARTAF: CHARTAF-S."}, {"title": "2.1 Towards High-performing Feedback", "content": "Two feedback types have been explored in the LLM research community: (1) scalar score-based $(s_n \\in \\mathbb{N}_{\\leq n_0})$ and (2) natural language-based $(s_n \\in \\mathbb{N}$, where $\\mathbb{N}$ is the natural language set space) feedback. Rather than (1) and (2) being competing approaches, they serve different purposes. (1) is suited for parameter tuning with rewards, and TTS, while (2) is suited for in-context tuning. We introduce prior attempts of (1) and (2) in the chart generation domain below.\n(1) Yang et al. (2024), Wu et al. (2024), and Xia et al. provide [0,100], [1,10], [0,5], scores, respectively. However, as these methods were developed under a reference-based regime, their performance sharply deprecates when applied reference-free. While there are other similar works, they are closed-source without clear details for replication. For (2), a na\u00efve approach investigated is using a zero-shot LLM to provide feedback , which we term Na\u00efve Feedback (NF). This method, as our experiments later show, is ineffective. Furthermore, Yang et al. (2024) does not provide human studies of their NF against baselines so there is no evidence on its efficacy.\nTo overcome the limitations of past works, we present CHARTAF. As CHARTAF-S is a subset of CHARTAF-G, we first introduce CHARTAF-S and then the additional component corresponding to CHARTAF-G. Fine-grained pseudocode and prompts are provided in App. A."}, {"title": "2.2 CHARTAF-S (fAF)", "content": "Module 1. The user query, $q \\in Q \\in \\mathbb{N}$, is first decomposed into three essential factors in a chart generation query: Task, Purpose, and Audience. By explicitly decomposing the $q$ into Task, Purpose, and Audience, CHARTAF can better infer the user intention. Since user queries are often brief, the intention may not always be clearly stated. However, CHARTAF utilizes the $q$ to induce underlying intentions.\nThis information is then fused with Basic Criteria\u2014a general, high-level criteria applicable to all chart evaluations. The Basic Criteria ensures that CHARTAF comprehensively considers chart elements: Chart Type, Visual Embellishment, Text, Color, Annotation, Aesthetics, and Visual Clutter. This criteria is inspired by the rich literature in visualization research. We document the research that corresponds to each element in App. A.1.\nThis first module is the domain grounding module. CHARTAF replaces gathering costly human annotations with existing scholarly research. Not only is this cheaper, this approach closely aligns with how human-made chart generations would be evaluated. We would grade human students' chart generations with a domain expert (lecturer) that has learned the principles of chart generation, grounded in scholarly literature"}, {"title": "2.3 CHARTAF-G (fAF)", "content": "Module 3. For granular feedback, the process continues by associating each answered question with Retain, Edit, Discard, and Add. This association helps to decompose the evaluation result of each criterion to actionable feedback. Based on this association, code-centric text feedback is provided: the LLM is prompted to provide fine-grained feedback considering the downstream application, i.e., chart generation via code generation. Code-centric feedback encourages the LLM to provide explicit feedback that can be directly applied to the downstream f."}, {"title": "3 C2: CHARTUIE-8K", "content": "Leveraging the reference-free nature of CHARTAF, we curate CHARTUIE-8K, a comprehensive chart generation evaluation set. Since no gold references are required, we can significantly scale the dataset. Qualitative examples of CHARTUIE-8K can be viewed in Figs. 1, 2, App. C, and our project site.\nCuration Method. An overview of CHARTUIE-8K's curation process is illustrated in Fig. 3. First, for diverse chart topics, we semi-automatically crawl diverse datasets online with appropriate licenses. We include only the datasets that have been used by humans for chart generation purposes, as not all datasets are suitable for visualization. Next, to ensure diverse chart types and annotations, we adopt a comprehensive list of chart types and annotations. Then, we emulate two types of users: [U1] lay users and [U2] detailed users.\nTo this end, we synthetically (LLM-assisted) generate initial instructions with two configurations: approx. [U1] <50, and [U2] <100 words. Considering the salience of multi-turn benchmarking , we emulate a single QA cycle. I.e., the LLM asks for clarifying questions to the user, then, the user emulator responds to [U1] 25% or [U2] 50% of the questions. The pseudocode is provided in App. B.1.\nStatistical Summary. To systematically understand the evaluation set, we compare key statistics against relevant benchmarks . We exclude chart topic counts for benchmarks, as they are not provided in MatPlotBench and Plot2Code. We do not manually count their chart topics as this a subjective task. On the other hand, we count MatPlotBench's and Plot2Code's chart types using the same taxonomy used for CHARTUIE-8K (App. B.1). We leave the original number of coarse chart types provided by Plot2Code in parentheses for documentation. Next, the distribution of chart topics and types of CHARTUIE-8K are presented in Fig. 4. As we only scrape datasets that have been used for chart generation, the distributions in Fig. 4 is a reflection of real-world chart generation use-cases."}, {"title": "4 Empirical Study", "content": "4.1 Preliminary and Notations\nLet a feedback-provider take the output of f, then $f_{AF}: O \\leftrightarrow S_{\\mathbb{N}} \\in \\mathbb{N}^{uo}$, $f_{aF} : O \\leftrightarrow S_{\\mathbb{N}} \\in \\mathbb{N}$"}, {"title": "4.2 Test-time Scaling with CHARTAF", "content": "Experiment Set-up. We demonstrate that CHARTAF-S is an effective verifier for TTS. Following Snell et al.'s parallel best-of-N, we generate N $\\in \\mathbb{N}$ independent samples and choose the one with the highest score ($S_N \\in [0,100]:=S_N$). Here, the unit of inference budget is $N_s$. Let memory set $M := {o_1,\\ldots,o_N}$, $o \\in O$, hold the $N$ independent outputs of f. Then, we experimentally show that\n$\\hat{o} := \\underset{o \\in M}{arg \\, max} \\, f_{AF}(o)$,\nif $\\hat{o}_i > \\hat{o}_j, i \\neq j \\in \\mathbb{N}$,\n$\\Rightarrow h(\\hat{o}_{j:=1} := f(q), \\hat{o}_i) > 0$,\nacross $q \\in Q$.\nWe employ a double-blind 120 human study (of which 77 pass the rigorous sanity check) to compare pre-TTS ($N := 1$) and post-TTS ($N := 4$) preference scores. Each participant is presented with a random TTS sample. For this experiment we set both f and $f_{AF}$ backbone as GPT 40 to emphasize that $f_{AF}$ does not have to be a superior model for CHARTAF-S to be useful.\nResults. First, the scaling curve is depicted in Fig. 5. The positive slope highlights that CHARTAF can be effectively used as a TTS verifier. To rigorously verify this claim, the distribution of the human study is presented in Tab. 2. Going from a median $s_n$ of $40.47\\rightarrow62.75$ (pre$\\rightarrow$post-TTS) leads to 74% strongly preferring the post-TTS and 10.4% preferring post-TTS. This is a strong indication that $s_n$ is closely proxying human preferences."}, {"title": "4.3 In-context Tuning with CHARTAF", "content": "Experiment Set-up. For a comprehensive empirical study we consider four LLMs: (i) GPT 40, (ii) Claude 3.5 Sonnet, (iii) Llama 3.1 70b , and (iv) Gemma 2 27B of which (i) and (ii) are closed-source and (iii) and (iv) are open-source. The two closed-source models are used as $f_{AF}$ backbones, while all four models are used as f. Detailed LLM configurations are provided in App. E.\nOur empirical study employs a double-blind human study of 60 queries with 120 participants, 77 of whom pass the rigorous sanity check. 120 individuals are required for 60 queries as we test the two $f_{AF}$ independently. Within the 60 queries, 15 are designated for each of the four f. For representative sampling, we random sample the 15 queries with two constraints: (1) each of the 15 queries ask for different chart types, (2) 1:1 ratio of [U1] and [U2] queries. Concretely, our experiments show\n$h(f(q), f(q \\oplus f_{AF}(q))) >\nh(f(q), f(q \\oplus f_{baseline} (q)))$\nacross $q \\in Q$, where $\\oplus$ denotes string concatenation.\nBaselines. We identify four baselines: (1) ChartX , (2) MatPlotBench , (3) Plot2Code , and (4) ChatEval that can provide feedback for chart generation. (1) (2) (3) are chart generation specific feedback providers, while (4) is a generalist. One advantage of (1) (2) (3) is its token cost light nature. Therefore, for a fair comparison vis-\u00e0-vis token cost, we enhance these baselines with Auto-Chain-of-Thought (A-CoT) and Self-Consistency (SC). Beforehand, we ran preliminary studies with A-CoT, SC, and Self-Refine , and found that A-CoT+SC performed best. To solidify the utility of CHARTAF we include two additional baselines. The first is Na\u00efve Feedback (NF) which is zero-shot asking another LLM to give feedback . The second is skipping the feedback stage, and instead directly adding A-CoT to f (Skip+A-CoT)."}, {"title": "4.4 CHARTUIE-8K Experiments", "content": "Experiment Set-up. We ensure that our evaluation set, Q, is an improvement from existing benchmarks, i.e.,\n$D_{KL}(\\Delta(Q^*) || \\Delta(Q)) <\nD_{KL}(\\Delta(Q^*) || \\Delta(Q_{baseline}))$,\nwhere $Q^*$ denotes real user query set, and $D_{KL}(\\cdot, \\cdot)$ denotes KL divergence. We employ a double-blind human study of 130 participants (89 of whom pass the rigorous sanity check). Each participant is given a randomized chart image they are imagining, and asked to prompt an LLM their initial instructions. To quantitatively illustrate Eq. (4), we compare the word count distributions. Additional qualitative demonstrations further provide evidence for Eq. (4).\nLastly, the participant is asked whether the extra QA cycle is desirable and realistic when interacting with an LLM. Details of the human study is provided in App. D.4.\nResults. We visualize the results in Fig. 6. In terms of word count, CHARTUIE-8K (green) most closely matches the ground truth distribution of the user study (purple). Please see some qualitative examples of the instructions the users gave in App. B.2. Notably, 94% of respondents prefer the extra QA cycle, and 93% believe CHARTUIE's user emulation is realistic. We also qualitatively observe, as presented in App. C, existing evaluation sets'"}, {"title": "5 Discussion and Impact of C2", "content": "5.1 CHARTAF Enables Scalable Feedback\nScalability. As described in Sec. 1, the lack of training data, (instruction, data, code), can be mitigated by an effective reference-free feedback provider that only requires (instruction, data). However, as indicated by the predominance of red in Tab. 3, existing feedback providers perform poorly under this reference-free regime. Furthermore, increasing token usage via current methods fails to resolve this issue. CHARTAF addresses this scalability barrier, as demonstrated by the experimental results in in-context tuning and TTS.\nIn-context Tuning with CHARTAF. CHARTAF performs best among existing methods for nearly all f and $f_{AF}$ . This is particularly notable, as in-context tuning is often challenging for smaller models. Even when the feedback is helpful, the intrinsic limitations of f\u2014such as small model size or insufficient tuning to follow instructions\u2014may result in f inaccurately reflecting the feedback. Despite these challenges, the granular feedback of CHARTAF effectively improves smaller models (Llama 3.1 70b, Gemma 2 27b). In fact, Llama 3.1 70b and Gemma 2 27b on average experience greatest performance improvements after in-context tuning with CHARTAF, likely due to their weaker baseline performance that allows for larger post-tuning gains.\nTowards Complex Task Verification. Additionally, CHARTAF's reference-free TTS performance shows significant strength. TTS has been a paradigm-shifting development as it introduces a novel neural scaling axis. While previous scaling laws focused on data and training compute, recent works show that similar scaling laws apply to the inference compute axis. However, current TTS is limited to tasks with reliable and cheap verifiers , emphasizing the salience of fast, reliable, and cost-effective verifiers. CHARTAF is the first effective demonstration of a chart generation verifier within the TTS framework.\nScaling Frontier Models with CHARTAF. Notably, CHARTAF is not a distillation method transferring knowledge from larger smaller models. In Tab. 3, CHARTAF remains effective even for <f, $f_{AF}$> pairs where f and $f_{AF}$ are similarly performing models. Furthermore, TTS is demonstrated using the same backbone LLM. Such self-improving LLMs are indispensable for advancing frontier models."}, {"title": "5.2 Unlocking Large-scale Data with C2", "content": "Comprehensive Evaluation and Generation. The strong reference-free performance of CHARTAF enables the curation of a cost-effective and diverse query set, CHARTUIE-8K. This approach contrasts with the reference-based nature of existing query sets, which suffer from limited diversity . By pairing CHARTAF with CHARTUIE-8K, the combined framework of C2 supports a broadly inclusive evaluation set . This enables researchers to evaluate models on a more comprehensive and diverse set of queries. Moreover, through CHARTAF, C2 can generate large-scale, high-quality outputs that significantly improve over previous methods.\nRealistic Evaluation. Lastly, it is crucial to create evaluation sets that closely reflect real-world use-cases. As shown by the distributions in Fig. 6, existing query sets often diverge significantly from common user queries, reducing their practical utility. We recommend that future work proposing evaluation sets include rigorous studies (e.g. Sec. 4.4) to ensure their assets are pragmatically aligned with real-world use-cases."}, {"title": "6 Limitations", "content": "We discuss the limitations of this work to ensure full transparency. To our knowledge, we disclose all reasonable information throughout the paper, project site, and github repository.\nCode Execution Error. The feedbacks, $s_n$, $\\hat{s}_n$, presented in this paper is conditioned on the fact that the chart image has been successfully generated from the code. Occasionally, the code fails to execute or executes with an error. Under such circumstances, we allow a maximum of 5 regenerations for the initial f inference, and a maximum of 3 re-generations for post-feedback f inference. We document the initial f inference error rate for each f in App. F.\nCoverage. The coverage of CHARTUIE-8K and CHARTAF is limited to the English language. Additionally, this study is not conducted with smaller models, e.g. 8B parameter size LLMs. We leave investigating expanded coverage to future works."}, {"title": "7 Human Study Ethical Consideration", "content": "To our knowledge, we follow best practices in computer science human studies. First, to guarantee the privacy of both researchers and surveyors the study is double-blind, and do not collect any unnecessary data. Second, we clearly indicate at the very start that this is an academic survey for an academic paper. We do not upload the raw data on the public domain to avoid any potential unethical usage. Third, surveyors voluntarily conduct the surveys and can choose to leave at any time. Fourth, we ensure that the surveyors are compensated fairly. While we do not include any geographic restrictions, we pay $14 per hour, well above the U.S. federal minimum wage of $7.25 as of the surveys. We pay surveyors within 48 hours of completing the survey. Finally, our surveys do not contain explicit or triggering content."}, {"title": "A CHARTAF Details", "content": "A.1 Basic Criteria References\nSee Table 4.\nA.2 CHARTAF Pseudocode\nWe present the pseudocode of CHARTAF in Alg. 1, along with the prompts used within it. These prompts can be accessed by clicking on the highlighted phrases in the pseudocode.\nWe first introduce the notations. Let inst represent the initial instruction, qst the follow-up questions, and ans the answers to qst. Let d represent the dataset, and dattr its attributes. fAF denotes the backbone LLM for feedback generation. Let mode be one of two values: \"Scalar\" or \"Granular.\" Finally, let codegen be the code that generates a chart. This code is produced by the chart-generating LLM, using the prompt Generate.\nThe arguments in the prompt\u2014data_path, data, file_index, initial_instruction, questions, and answers\u2014should be set to the data path for the dataset, d, the index for the resulting image, inst, qst, and ans, respectively. Denote img as the chart generated by executing codegen. The procedure returns either a single scalar value, Sn, or fine-grained natural language feedback, sw, depending on the value of mode."}, {"title": "B CHARTUIE-8K Details", "content": "B.1 CHARTUIE-8K Pseudocode\nWe present the pseudocode of CHARTUIE-8K in Alg. 2, along with the prompts used within it. These prompts can be accessed by clicking on the highlighted phrases in the pseudocode.\nWe first introduce the notations. Let d represent the underlying dataset. If it exists, let dtti denote the title of d; otherwise, set dtti to \"unknown.\" Similarly, if the topic of d is provided, let dtpc represent it. Finally, let fuie refer to the backbone LLM for CHARTUIE-8K. Here, fuie :=GPT 40.\nThe procedure returns a list of queries, Q, generated by the algorithm. Each tuple in Q includes the initial instruction (inst), column labels for visualization (data attributes, dattr), a selected task (tsk), a visualization purpose (prps), a text description of the target audience (aud), follow-up questions (qst), and answers (ans). The model fuie infers dattr, tsk, prps, and aud from inst and follow-up questions to clarify user preferences, of which only q% are answered."}, {"title": "C Chart Generation User Query Examples", "content": "Plot2Code Example\nThe figure generated by the provided Python code consists of four subplots, each displaying a different transformation of the same image. The image is created using a mathematical function that generates a grid of values.\nThe first subplot shows the image rotated by 30 degrees. The second subplot shows the image skewed by 30 degrees along the x-axis and 15 degrees along the y-axis. The third subplot shows the image reflected along the x-axis and scaled by 0.5 along the y-axis. The fourth subplot combines all these transformations: rotation, skewing, scaling, and translation.\nEach subplot has a yellow dashed line indicating the intended extent of the image. The x-axis limits are set from -5 to 5, and the y-axis limits are set from -4 to 4 for all subplots.\nTo recreate this figure, you would need to generate the same image and apply the same transformations in each subplot. The transformations are applied using an affine transformation matrix, which allows for rotation, skewing, scaling, and translation. The specific parameters for each transformation are mentioned above.\nThe figure is displayed using the matplotlib library in Python, which would need to be installed and imported to recreate the figure. The numpy library is also used to generate the image and would need to be installed and imported as well.\nThe image is generated using a mathematical function that creates a grid of values between -3.0 and 3.0 with a step of 0.25. This function uses the numpy exp function to calculate the exponential of the negative square of each value in the grid. The result is a 2D array of values, which is used as the image in the subplots.\nThe transformations applied to the image in each subplot are done using the Affine2D function from the matplotlib.transforms module. This function creates a 2D affine transformation matrix that can be used to apply various transformations to the image. The specific transformations and their parameters are as follows:\n\u2022 Rotation: 30 degrees\n\u2022 Skew: 30 degrees along the x-axis and 15 degrees along the y-axis\n\u2022 Scale: -1 along the x-axis (which reflects the image) and 0.5 along the y-axis\n\u2022 Translation: 0.5 along the x-axis and -1 along the y-axis\nThe extent of the image in each subplot is set to [-2, 4, -3, 2], and the image is clipped to these bounds. The yellow dashed line indicating the intended extent of the image is drawn using the plot function with these bounds.\nFinally, the figure is displayed using the show function from the matplotlib.pyplot module.\nMatPlotBench's Simple Instruction Example\nCreate a Python script using matplotlib to generate a specific plot with the following detailed parameters:\nInitialize a figure with a custom size of 7.5 by 7.5 inches. Add a single axis to the figure with a custom aspect ratio and specified position. Define X as a linear space from 0.5 to 3.5 with 100 elements. Calculate Y1 as 3 plus the cosine of X, Y2 as 1 plus the cosine of 1+X/0.75 divided by 2, and Y3 as random values uniformly distributed between Y1 and Y2.\nSet major and minor locators for both x and y axes with major intervals of 1 and minor intervals of 4. Set minor formatter for the x-axis to display values with two decimal places. Limit the x and y axes to a range from 0 to 4.\nFor major ticks, set the width to 1.0, length to 10, and label size to 14. For minor ticks, set the width to 1.0, length to 5, label size to 10, and label color to '0.25'. Add a grid with these specific attributes:\n\u2022 Linestyle set to \"-\" (dashed)."}, {"title": "D Human Study Details", "content": "D.1 Compensation and Qualification\nWe use paid crowdsource surveyors to conduct our human study. We use the same platform as Gudibande et al. (2024). To ensure quality responses, all voluntary participants are paid $14 per hour, well above the U.S. federal minimum wage of $7.25 . Moreover, we enforce five requirements to participate in our study: (i) Masters\u00b2, (ii) > 80% Approval Rate, (iii) > 50 Approved, (iv) Casual experience using LLMs, (v) Survey with a large screen (> 10 inches, $\\geq$ 25.4 cm). For privacy, we do not gather any other unnecessary data.\nD.2 Sanity Check\nFollowing Yang et al. (2023) and Xu et al. (2024a), we include multiple sanity check questions within the survey for reliability. These sanity checks are solidified a priori, and are never changed.\nCHARTUIE-8K. In CHARTUIE-8K, we manually remove LLM initial query that clearly suggests that the surveyor did not understand the survey instructions or put in virtually zero time. For transparency, we report all the sanity check fail cases on our open-source github repository.\nCHARTAF. For the CHARTAF human study, two sanity checks filter very poor quality surveyor responses. First, we remove surveys that were completed in 50% of the lower-bound completion time. We time the duration it takes to complete a survey, and create a lower- and upper-bound to signal to potential surveys how long our study takes. For example, 10 to 16 minutes. In this case, surveys that were completed within 5 minutes would be removed as this would indicate that very low effort was put into the survey. Second, a sanity check question was included, presenting an image option intentionally unrelated to the user instructions. Survey responses that prefer (or strongly prefer) this option is removed as it indicates that the surveyor does not understand the task or is putting in very little effort.\nD.3 CHARTAF Human Study\nWe provide the precise instructions provided to the surveyors below. We further provide exact screenshots in Fig. 7 and 8."}, {"title": "D.4 CHARTUIE-8K Human Study", "content": "We provide the precise instructions provided to the surveyors below. We further provide screenshots in Fig. 9 and 10."}, {"title": "E LLM Configurations", "content": "Table 5 shows the LLM configurations we used. These configurations are determined a priori to all experiments, and never changed."}, {"title": "F Code Error Rate", "content": "Fig. 11 shows the mean execution error rates of the four tested LLMs."}, {"title": "G Icon Attribution", "content": "All icons are from flaticon.com except Retain(cycle-100) is from icons8.com. The icons and their respective authors are in Table 6. The license to these icons belongs to their respective owners. We use them for non-profit academic use, with attribution as requested by the distributors."}]}