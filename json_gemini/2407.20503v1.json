{"title": "A Federated Large Language Model for Long-Term Time Series Forecasting", "authors": ["Raed Abdel-Sater", "A. Ben Hamza"], "abstract": "Long-term time series forecasting in centralized environments poses unique challenges regarding data privacy, communication overhead, and scalability. To address these challenges, we propose FedTime, a federated large language model (LLM) tailored for long-range time series prediction. Specifically, we introduce a federated pre-trained LLM with fine-tuning and alignment strategies. Prior to the learning process, we employ K-means clustering to partition edge devices or clients into distinct clusters, thereby facilitating more focused model training. We also incorporate channel independence and patching to better preserve local semantic information, ensuring that important contextual details are retained while minimizing the risk of information loss. We demonstrate the effectiveness of our FedTime model through extensive experiments on various real-world forecasting benchmarks, showcasing substantial improvements over recent approaches. In addition, we demonstrate the efficiency of FedTime in streamlining resource usage, resulting in reduced communication overhead.", "sections": [{"title": "Introduction", "content": "Long-term time series forecasting involves predicting future values based on historical data over an extended period. The primary goal is to provide reliable and accurate predictions for multiple timestamps ahead, which can be crucial for planning, decision-making, risk management, and resource allocation in various domains such as finance, energy management, transportation, and environmental monitoring.\nRecent advances in deep learning, particularly within Transformer- and LLM-based models [14, 32, 4, 33, 18, 3], have showcased significant progress in long-term time series forecasting. These models capitalize on the self-attention mechanism and its variants, providing a distinct advantage in capturing and modeling long-range dependencies within sequential data. While LLMs are adept at processing discrete tokens, time series data presents a unique challenge due to its continuous nature. Also, LLMs do not inherently possess the capability to interpret time series patterns, as this knowledge is not typically included in their pre-training. In addition, centralized learning models, including LLM-based methods, often require collecting and storing vast amounts of data in a central server, raising concerns about user privacy and data security. Thus, effectively leveraging LLMs for long-term time series forecasting, while ensuring accuracy and data privacy, highlights the challenges associated with centralized learning models, emphasizing the need for federated learning approaches [9]. Federated learning is an emerging machine learning paradigm that enables edge devices to utilize their local datasets to collaboratively train a global model with the help of a central server, while preserving data privacy by keeping the data on the respective devices. At each iteration, the server broadcasts the current global model to the devices for local training, and aggregates the local model updates from the devices to update the global model. Federated learning offers the potential to enhance long-term prediction accuracy [13]. However, certain challenges arise from skewed data distributions that may impact the training quality [12]. Another challenge in federated learning is systems heterogeneity, which arises from the variation in storage, computational, and communication capabilities influenced by differences in hardware specifications, and power constraints [9]. This leads to heightened concerns around stragglers and fault tolerance. Several strategies have been proposed to address this issue involving adaptive optimization techniques that cater to the varying capabilities of devices, optimizing resource usage based on each device's characteristics [9]. Additionally, employing techniques like asynchronous updates and gradient compression can reduce communication overhead and accommodate devices with limited connectivity or power [15]. These strategies collectively aim to enhance the efficiency and reliability of federated learning systems in heterogeneous environments. However, these techniques do not significantly enhance model accuracy in scenarios involving optimization and transfer learning, particularly in contexts where data heterogeneity arises from collective settings [27].\nIn this paper, we introduce a federated learning framework, dubbed FedTime, for long-term time series forecasting, while preserving data privacy. Local data are used for training individual predictive LLM models on edge devices, allowing for rapid, localized predictions while reducing the load on cloud or central servers. The cloud server plays a dual role in initializing the system and aggregating local LLMs for global updates, a key aspect of federated learning. To optimize the use of limited computational resources on edge devices, we employ parameter-efficient tuning techniques [8, 7], which are crucial in minimizing computational and communication overhead, facilitating local training. Our learning framework encompasses training procedures on local edge devices (e.g., EV charging stations) and subsequent aggregation processes on the central server side by leveraging the pre-trained Large Language Model Meta AI (LLaMA-2) [26]. This LLaMA model is a group of foundational pre-trained LLMs with various sizes (7B, 13B, 33B, and 65B parameters), offering a range of capabilities suited to different needs and computational resources. The key contributions of this paper can be summarized as follows: (i) We introduce a federated large language framework, called FedTime, allowing for collaborative model train-"}, {"title": "Related Work", "content": "A sizable body of research has been developed to design Transformer-based methods for long-term time series forecasting [14, 32, 28, 33, 18]. Pyraformer [14] presents a pyramidal attention module, leveraging inter-scale and intra-scale connections, which enhances model capability to capture temporal patterns. Informer [32] design a ProbSparse self-attention mechanism to improve model prediction capacity. Autoformer [28] adapts decomposition blocks and auto-correlation mechanisms to help separate the long-term trend information from predicted hidden variables. FEDformer [33] employs fast Fourier transform to decompose sequences, enhancing the extraction of long-term information from the data. PatchTST [18] segments time series data into subseries-level patches, which serve as input tokens to a Transformer-based architecture. It also employs channel-independence, where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all series, allowing efficient processing of multiple time series. More recently, various pre-trained LLMs, such as the generative pre-trained Transformer (GPT) [19] and LLaMA [26] models rooted in the Transformer architecture, have emerged as powerful methods in generating high-quality outputs in tasks spanning natural language processing [25, 17, 24, 31, 30] and time series forecasting [3], leveraging their ability to discern and comprehend intricate dependencies within data. Chang et al. introduce an LLM-based method, called LLM4TS, by adapting the GPT model as a foundational backbone to time series data and then fine-tuning it for time series forecasting tasks. Although our work also involves a two-stage fine-tuning strategy, it differs from LLM4TS in that our FedTime method not only focuses on efficient fine-tuning, but is also tailored for federated learning setups, aiming at long-term time series forecasting. The key strength of FedTime, compared to Transformer- and LLM-based approaches, lies in its ability to allow edge devices to collaborate in model training while keeping data decentralized. This preserves data privacy, a crucial aspect, especially in sensitive domains like healthcare. In addition, FedTime leverages parameter-efficient fine-tuning techniques, enabling it to update only a small number of local model parameters. This efficiency in training and model updates is crucial for real-world applications, especially in resource-constrained edge devices such as EV charging stations."}, {"title": "Method", "content": "In this section, we formulate the time series forecasting problem and provide an approach overview. Then, we present the main building blocks of the proposed federated learning framework."}, {"title": "Problem Statement and Approach Overview", "content": "Time series forecasting refers to the process of predicting future values over a period of time using historical data. Let $X_{1:L} = (x_1,\u2026\u2026,x_L) \\in R^{L \\times M}$ be a history sequence of $L$ multivariate time series, where for any time step $t$, each row $x_t = (X_{t1},...,X_{tM}) \\in R^{1 \\times M}$ is a multivariate vector consisting of $M$ variables or channels.\nGiven a history sequence $X_{1:L}$ with look-back window $L$, the goal of multivariate time series forecasting is to predict a sequence $X_{L+1:L+T} = (X_{L+1},..., X_{L+T}) \\in R^{T \\times M}$ for the future $T$ timesteps. To this end, we introduce a federated pre-trained large language model by leveraging channel independence and patching [18], as well as supervised and downstream fine-tuning strategies [3]."}, {"title": "Proposed Learning Framework", "content": "We present federated learning framework, designed to overcome the classical challenges within a centralized training environment, such as network congestion, low bandwidth, latency, and privacy. The proposed federated architecture is built on the LLaMA structure, enabling parallel feature learning across various time series forecasting benchmarks. Figure 1 depicts the main building blocks of our learning framework, including time series data normalization, patching and positional encodings, LLM encoder and robust fine-tuning strategies. Our two-phase approach consists of supervised fine-tuning, followed by forecasting fine-tuning.\nChannel Independence. Channel independence refers to a strategy where the features or variables of a multivariate time series are treated separately [18], emphasizing independence rather than mixing or amalgamating information from different channels or variables. This approach aims to retain the distinct characteristics of each"}, {"title": "Model Fine-Tuning", "content": "Parameter efficient fine-tuning (PEFT) methods offer an efficient way to adapt pre-trained LLMs to diverse downstream tasks without the need to fine-tune the entire set of model parameters. To fine-tune our pre-trained LLM, we employ quantized and low-rank adaptation (QLORA) [7], a PEFT technique that aims to minimize the number of parameters updated during fine-tuning while preserving or improving performance on downstream tasks. QLoRA builds upon LoRA [8], which freezes the weight matrices of the linear projection layers within the self-attention mechanism of the Transformer encoder and represents each of their update matrices as a product of two trainable low-rank matrices. This low-rank matrix decomposition significantly reduces the number of trainable parameters, making the model more efficient for downstream tasks. In addition to low-rank decomposition, QLoRA also incorporates quantization techniques that reduce the precision of the weight values, thereby further decreasing the model's memory footprint and computational requirements without substantially sacrificing performance. With QLoRA, only 1.2% of the model's parameters are considered trainable, whereas using LoRA increases this percentage to 1.5% [3]."}, {"title": "Experiments", "content": "In this section, we present experimental evaluation of the proposed federated learning approach by comparing it with recent state-of-the-art methods."}, {"title": "Experimental Setup", "content": "We conduct a comprehensive evaluation of our FedTime model on various benchmark datasets: Weather\u00b9, Traffic\u00b2, Electricity\u00b3, and four ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2) [28]. Dataset statistics are summarized in Table 1: (1) Weather is recorded at 10-minute intervals for the entire year of 2020, and contains 21 meteorological indicators, such as air temperature and humidity. (2)"}, {"title": "Communication Overhead", "content": "Evaluating the communication overhead is crucial in assessing the feasibility of a model's deployment in federated environments. We use the real-world ACN dataset [11], which is an adaptive charging network dataset containing information about an electric vehicle (EV) single charging session at two distinct sites: California Institute of Technology (Caltech 540 stations) and Jet Propulsion Laboratory (JPL 400 stations). An exploratory data analysis of the dataset shows that the data distribution has an increasing trend at both Caltech and JPL sites. These upward trends in the distribution of energy demand at both sites underscores the need for enhanced capacity to accommodate these surges and mitigate escalated demand charges.\nThe ACN dataset consists of 1.5 million charging sessions with a window time of 19 months. Both Caltech and JPL sites show a repetitive usage pattern with much higher utilization during weekdays than on weekends, as shown in Figure 4, which depicts the distribution of the energy delivered.\nOne of the key metrics of communication overhead is the data volume transferred, measured in Megabytes. As shown in Figure 5, our FedTime model yields reduced communication overhead compared to the baseline methods. This reduction is even more pronounced when considering metrics like message count and communication time. A key factor contributing to FedTime's efficient performance is the strategic engagement of edge devices in the learning process. This decentralized approach, leveraging the inherent distributed nature of IoT-enabled devices, facilitates a system where updating model"}, {"title": "Ablation Study", "content": "In Figure 6, we evaluate the performance of various FedTime variants on the ACN dataset for Caltech site. In the following, we introduce these variants and analyze their respective performances:\n\u2022 FedTime without Clustering: This model aligns relatively well with the actual energy consumption in general up to the 20-hour"}, {"title": "Conclusion", "content": "We introduced FedTime, a federated large language model for long-term time series forecasting. FedTime leverages federated learning, ensuring decentralized model training while upholding data privacy across edge devices. Our empirical evaluations underscore FedTime's superiority over both centralized and federated baselines in long-term time series forecasting, demonstrating significant performance enhancements across diverse datasets. Moreover, FedTime addresses communication overhead challenges by optimizing the transmission of model updates, thereby minimizing resource consumption while enhancing forecasting accuracy. In summary, FedTime offers a versatile and efficient solution that strikes a balance between privacy preservation, communication efficiency, and forecasting accuracy in decentralized settings. Looking ahead, our future research endeavors will explore synergies between federated frameworks and other distributed systems, with a keen focus on integrating blockchain technology."}]}