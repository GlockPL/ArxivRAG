{"title": "WSI-VQA: Interpreting Whole Slide Images by Generative Visual Question Answering", "authors": ["Pingyi Chen", "Chenglu Zhu", "Sunyi Zheng", "Honglin Li", "Lin Yang"], "abstract": "Whole slide imaging is routinely adopted for carcinoma diagnosis and prognosis. Abundant experience is required for pathologists to achieve accurate and reliable diagnostic results of whole slide images (WSI). The huge size and heterogeneous features of WSIs make the workflow of pathological reading extremely time-consuming. In this paper, we propose a novel framework (WSI-VQA) to interpret WSIs by generative visual question answering. WSI-VQA shows universality by reframing various kinds of slide-level tasks in a question-answering pattern, in which pathologists can achieve immunohistochemical grading, survival prediction, and tumor subtyping following human-machine interaction. Furthermore, we establish a WSI-VQA dataset which contains 8672 slide-level question-answering pairs with 977 WSIs. Besides the ability to deal with different slide-level tasks, our generative model which is named Wsi2Text Transformer (W2T) outperforms existing discriminative models in medical correctness, which reveals the potential of our model to be applied in the clinical scenario. Additionally, we also visualize the co-attention mapping between word embeddings and WSIs as an intuitive explanation for diagnostic results. The dataset and related code are available at https://github.com/cpystan/WSI-VQA.", "sections": [{"title": "1 Introduction", "content": "Multimodal large language models (MLLM), such as GPT-4V [37], LLaVa [24], Qwen-VL [2], have demonstrated remarkable superiority over a wide range of visual-language tasks. Even in medical domains requiring specialized knowledge and where a large visual gap exists, MLLMs continue to exhibit exceptional performance [20, 22, 33, 37, 39]. Despite their impressive performance in the medical area, these MLLMs can not be easily applied to achieve visual understanding for WSIs which are characterized by a tremendous size (the maximum can be 100,000\u00d7100,000 pixels at 20\u00d7 magnification). Recent years have also witnessed the emergence of pathological MLLMS (MI-Zero [26], PLIP [16], CONCH [25]).\nAlthough a large number of pathological images are seen by the MLLMs, the patch-level pre-training makes it challenging to achieve promising performance on slide-level tasks which requires an understanding of the whole WSI like carcinoma subtyping.\nIn VQA for natural images, the input with the approximate size of 256 \u00d7 256 is worth 16 \u00d7 16 words with visually descriptive labels. In the pathology field, the slide-level prediction for WSIs is a knotty problem that is usually formulated in a multiple-instance learning (MIL) framework [9,17,19,35,38]. Patches are cropped from the whole image followed by reducing dimensions to make computation feasible. Patch embeddings act as individual instances which are then aggregated to generate the final result in a weakly-supervised manner. The whole image acts as a bag exhibiting enormous heterogeneity with labels that are visually hard to distinguish. Differentiating tumors, as a basic slide-level task, depends on the fine-grained visual features that localize in a very small region. Survival outcome prediction is a challenging regression task where the field of view needs to be expanded to the entire WSI and to understand the interactions of different components (stroma, lymphocyte, microenvironment, etc).\nVisual-language MLLMs unify a variety of visual tasks such as object segmentation, scene understanding, and optical character recognition by elaborately building appropriate prompts and obtaining textual descriptions [2, 24, 37]. In the pathology field, previous task-specialized MIL models can only solve certain problems like tumor grading despite their remarkable performance. To promote the development of visual language learning for whole slide images, we propose a novel paradigm of WSI-VQA in which the input is the whole slide image and the output is the text. In WSI-VQA, a visual encoder is applied to extract and aggregate patch-level embeddings, and a text encoder gets word embeddings of the questions. Patch embeddings and word embeddings are aligned by a co-attention mapping in the decoder to generate the answers. In contrast to those VQA approaches [8, 23, 28, 31,40] which introduce a classifier to select the right answer given a finite set of choices, our generative model can predict the answer word by word in free form, which enables our model to deal with diverse problems especially when contrastive or classification candidate answers are not available in clinical practice.\nIn the field of pathology, visual-language models lag behind due to insufficient training data for MLLMs. The large resolution and visually challenging features make it especially difficult to annotate WSIs. In addition, the concern about data privacy and security also prevents enough slide-level data from being widely accessible. We are glad to see that some researchers collect pathological text from a wide range of resources like books [13] and social media (Twitter) [16]. However, these textual descriptions are constrained to narrating pathological patches instead of the whole slide image, which makes models struggle to achieve promising slide-level performance like tumor subtyping [26].\nTo effectively train our proposed WSI-VQA models, we propose an automatic and scalable pipeline to curate a slide-level question-answering dataset, termed WSI-VQA. It contains 977 WSIs equipped with 8672 question-answering pairs."}, {"title": "2 Dataset Construction", "content": "In this section, we will provide more details about the construction process of our proposed dataset WSI-VQA, as shown in Fig. 1. We will first give a brief introduction to the source data and comprehensively explain the scalable generation process of question-answering pairs, followed by analyzing and visualizing data statistics.\n\n2.1 Data Generation\nTCGA establishes an infrastructure which collects and characterizes a large number of cancer samples. It contains diagnostic WSIs and relevant clinical information of various modalities. We resort to TCGA-BRCA (the largest subset of TCGA about breast cancer) to curate our slide-level VQA dataset because of its reliability and security. From the captions of TCGA, as shown in Fig. 1, we generate question-answering pairs equipped with multiple choices with the aid of LLMs. The other question-answering pairs are obtained from the clinical files in TCGA by extracting keywords to fit templates.\nClose-ended Subset. Based on the largest WSI caption dataset which summarized pathological reports from TCGA [13], we can automatically generate several questions accompanied with candidate answers for each caption with the aid of large language models (LLM) like GPT [4]. Following the work [41], we use the following prompt to generate WSI-VQA pairs:\nAsk 6 questions about the content and generate four options for each question.\nThe output should use the following template: i:\u2018the question index' question:\u2018the generate question' choice: 'A: option content B: option content C: option content\nD: option content' answer: The correct option.\nAlthough this approach makes the automation of VQA pair generation feasible, the problem of hallucination brought by LLMs can not be ignored. The hallucination means that LLMs may generate content that is contradictory to the common knowledge or unfaithful to the provided content. Therefore, we adopt data filtering to discard those pairs that own significant defects. In addition, the answers that can not be inferred from WSIs are also removed from our collected dataset. Finally, we obtained 4535 close-ended VQA pairs which not only contain questions and answers but also provide candidate choices. Since the close-ended subset has choices for each question, we can compute the medical accuracy to measure the performance of our generative model.\nOpen-ended Subset. TCGA-BRCA includes various clinical indexes: subtyping, immunohistochemical testing, and survival prediction. By matching the keywords and their corresponding values in the question-answering templates, we can obtain high-quality VQA pairs which contain abundant clinical information. We designed several question-answering templates and one of the templates is shown below:\nQ: What is the result of [KEY]? A: [VALUE].\nDuring the training process, we randomly select a template for each pair. To make our generative model reasonable, we discard those clinical indexes that can not be inferred from diagnostic WSIs. As a result, the open-ended subset contains 4137 VQA pairs.\nData Statistics. We provide a statistical overview of our proposed dataset. The dataset contains 977 WSIs and 8672 QA pairs where each WSI is associated with an average of 8.9 QA pairs. Among these QA pairs, the close-ended subset has 4535 QA pairs while the open-ended one has 4137 pairs. As shown in Fig. 2 (a), the questions in the dataset demonstrate a diversity ranging from the questions that require spatial information such as identifying the margin status to challenging questions like immunohistochemical grading of the WSI. Fig. 2 (b) shows the distribution of different categories of questions. The dataset is predominantly characterized by 'what' questions, accounting for 80% of its content. There are also other types of questions such as 13.2% 'Yes/No' questions, 1.4% 'where' questions, and 3% 'which' questions. Since the open-ended subset is collected from the clinical files in TCGA, we calculate the coverage ratio of different entities in Fig. 2 (c). For example, the coverage ratio of 'her2'"}, {"title": "3 Method", "content": "In this section, we elaborate on our WSI-VQA model, the Wsi2Text Transformer (W2T) the structure of which is visualized in Fig. 3. Firstly, we explain the theoretical base of our VQA method in Section 3.1. We introduce our way of pre-processing for WSIs and the formulation of patch embeddings and word embeddings in Section 3.2. In Section 3.3, we present the interaction between visual and textual features showing how word embeddings attend to relevant patches when responding to a question.\n\n3.1 Problem Formulation\nThe inputs (Xi, Qi, Yi) are the whole slide image X\u2081 and the question Qi, predicting Yi as the output. The large resolution necessitates the pre-processing of WSIs. The WSI can be treated as the aggregation of a group of instances which are patches with a much smaller resolution. It is denoted as X\u2081 = {x}M\u2081 where Xi is the i th WSI and Mi is the sequence length which is usually larger than 10000. A visual extractor h is adopted to extract patch embeddings, denoted as h({x}) \u2208 RM\u2081\u00d7l where l is the embedding size.\nThe question can be seen as a sequence of tokens after tokenization, which is formulated as Qi = {q}11 where Ti is the number of tokens. We use a text extractor g to transform the tokens into word embeddings, denoted as g({q}) \u2208 RTik where k is the embedding size for word embeddings.\nA generative model is incorporated to generate the answer Yi = {y}1 where Ni is the length of target sequence. The model parameterized by o is trained with language model loss objectives that seek the maximization of the\nsum of conditional possibilities of words in the sequence. The negative log-likelihood (NLL) loss can be formulated as:\nL = -$\\sum_{it=1}^{N}log(p(y_{it}|h(\\{x\\}), g(\\{q\\}), \\{y\\}_{j<t};\\phi))$.\nGiven the patch embeddings, word embeddings, and the previous sequence {y}j<t, the probability of the next word to be generated can be calculated.\n\n3.2 Preprocessing for Bag Construction\nVisual Embeddings. In natural VQA, it is GPU-feasible to directly feed the image so that the visual embeddings are trainable. In the pathology field, we follow the curriculum of previous MIL approaches where the large image is cut down into small individual patches. We first perform preprocessing to remove"}, {"title": "3.3 Interaction between WSI and Text", "content": "Current multimodal approaches in the pathology field devote efforts to linking mutually independent patches with corresponding textual descriptions [16, 26], which limits their ability on slide-level tasks since the WSI is the aggregation of a lot of mutually relevant patches and their collected descriptions are focused on patch-level narration. In our framework, by formulating the WSI and the text as bag representations Ex and EQ, we can not only model the interaction among\npatches cropped from the same WSI but also establish fine-grained interactions between patch tokens and word tokens. After obtaining the visual embeddings Ex \u2208 RMixl, a transformer encoder is adopted to capture the mutual instance relationship, which is formulated as below:\nSelf Att(X) = softmax($\\frac{QK^T}{\\sqrt{d}}$)V\n= softmax($\\frac{W_Q E_X E_X^T W_K^T}{\\sqrt{d}}$)W_V E_X\nwhere WQ, WK, Wv \u2208 R\u00b9\u00d7l are trainable parameters and d is a scaling factor. We denote the transformer's [36] query, key, and value as Q, K, and V respectively. In the transformer encoder Te, our model learns to allocate attention to the instances and builds the mutual-instance correlations for the subsequent decoding stage. It is also a mimic of clinical workflow where pathologists usually observe by switching the region of interest and yield the diagnosis based on both cellular-level features and the structural information of tissues and organs.\nThe co-attention mechanism in the standard transformer decoder is adopted to capture the relations between visual and text concepts, which can be formulated as:\nCoAtt(X, Q) = softmax($\\frac{QK^T}{\\sqrt{d}}$)V=softmax($\\frac{W_Q E_Q E_X^T W_K^T}{\\sqrt{d}}$)W_V E_X= AWvEx,\nwhere WQ, WK,Wv \u2208 Rl\u00d7l are trainable matrices multiplied by the visual and word embeddings. The word embeddings need to be aligned to transform their size from k to l by the linear mapping Ti before being fed to the transformer decoder. The co-attention module A \u2208 RTi\u00d7Mi learns fine-grained patch-word similarity showing how much word tokens {q} attend to patches {x}. For a single word embedding qn, the according weight matrice is An \u2208 R1\u00d7Mi which reflects how attentions are allocated to the enormous patches based on the n-th word token. For each WSI-question pair, we choose the keyword (biological entity in the question) to guide the generation of attention maps as a visual explanation for the answer. For example, since the keyword 'her2' is the 4th token in the question as shown in Fig. 5, we choose A4 \u2208 R1\u00d7Mi to be visualized as heatmaps. The highlighted regions prove to be more relevant to the answer."}, {"title": "4 Experiments", "content": "In this section, we first provide more details about our proposed dataset WSI-VQA. The backbones we choose to extract visual and word embeddings are also introduced. Finally, we elaborate on the setting of our proposed model W2T.\n\n4.1 WSI-VQA Dataset\nOur proposed WSI-VQA dataset contains a total of 977 WSIs with 8671 question-answering pairs. We split the dataset into a training set, a validation set, and a test set. The training set includes 804 WSIs with 7139 question-answering pairs. The validation set contains 87 WSIs and 798 pairs. And the test set contains 86 WSIs and 735 question-answering pairs. The right choices in the close-ended subset when testing include 151 for A, 107 for B, 86 for C, and 46 for D. We use both language metrics and clinical metrics to evaluate our model. For the closed-ended subset, we also adopt accuracy (ACC) by comparing the sentence similarity to measure the performance of the model.\n\n4.2 Visual Extractors\nHere we present different visual extractors in our generative model. ResNet [15] is a classic vision model, pre-trained by a lot of natural images and their classification label in ImageNet. The version we use is ResNet-50 which is composed of 50 convolutional layers with 25.6M parameters. ViT [11] is the vision transformer VIT-S which has 12 transformer layers with approximately 22M parameters. DINO [5] is a strategy of self-supervised pre-training with a standard ViT model. It is adopted to pre-train the backbone with our cropped patches from TCGA-BRCA. We choose VIT-S as the backbone. HIPT [7] which conducts hierarchical self-supervised learning with a pyramid transformer is a WSI-specialized self-supervised approach of pre-training. It exploits the hierarchical structure inherent in WSIs from the whole TCGA to learn high-resolution image representations.\n\n4.3 Text Extractors\nIn this section, we present different text extractors which are employed in W2T. BioClinicalBert [1] is a BERT-like model which is trained on all notes from MIMIC III [18], a database (text size 0.5B words / 3.7GB) which has abundant electronic health records from the hospitals. It is initialized with BERT-Base which has 12 transformer layers with around 110M parameters. PubMedBert [14] is trained with abstracts and articles from PubMed and PubMedCentral [34] with the size of 3.1B words / 21G based on BERT-base structure. Scratch means that we use an embedding mapping to extract the text features without freezing the parameters. Since the length of word tokens is much smaller than image tokens, it is feasible to train a text extractor from scratch while the visual extractor can not be trainable.\n\n4.4 Training Setting\nThe transformer encoder Te and decoder Ta include three layers and the hidden size is 512. The multi-head self-attention has 4 heads and the embedding size is also 512. Adam is adopted as the optimizer to train our model with a learning"}, {"title": "5 Results", "content": "5.1 Benchmark\nAs illustrated in Table 1, we introduce our WSI-VQA benchmark on our proposed dataset. Different visual extractors and text extractors are included for comparison. To measure the performance of the baselines, we consider two kinds of metrics: 1) natural language generation (NLG) metrics such as BLEU [32], METEOR [3], and ROUGE [21] which are widely used in natural language processing tasks, and 2) factual consistency and completeness (FCC) metrics to measure the clinical performance of the model. It is worth noting that ACC is only adopted to evaluate the close-ended subset which contains multiple candidate choices one of which is the right answer. And Factent [30] is used to measure how much the generated entities in the pathology domain are consistent with the reference.\nWe find that the best result for NLG Metrics is achieved when the text extractor is training from scratch. In specific, the increases of +2.4 Bleu-1, +0.6 Bleu-4, +1.2 METEOR, and +3.0 ROUGE are observed on WSI-VQA when compared against the model with PubMedBert given visual embeddings from\nResNet. Training the text extractor from scratch shows its superiority over generating fluent and readable answers while those task-agnostic word embeddings lag in terms of the NLG metrics. It may be because of the domain gap between the corpus for pre-training and the pathological corpus that is used in our experiment. However, we find it interesting that the 'scratch' strategy does not win consistently regards the FCC metrics. For example, an increase of +4.1 ACC and +2.1 Factent when BioClinicalBert is adopted given the visual embeddings from ViT is observed, which reveals the potential of those well-established text extractors in recognizing and understanding the clinical concepts.\nIn terms of the visual extractor, we observe that DINO and HIPT (in-domain pre-training) consistently beat ResNet and ViT (out-of-domain pre-training) in medical correctness. In specific, a large increase of +2.4, +1.4, and +5.4 ACC are present respectively when DINO pre-training extractor is adopted compared to ViT which is pre-trained with ImageNet. It is reasonable that the visual extractors pre-trained with in-domain data are more capable of extracting relevant features in the pathological patches, which contributes much to the diagnosis of WSIs. For all the baselines, the ACC metrics are nearly 50, and the Factent metrics are over 90, which shows the potential of WSI-VQA in clinical scenarios."}, {"title": "5.2 Slide-level Tasks", "content": "Recent years have witnessed the development of AI-aided digital pathology and some deep-learning models can even achieve human-level performance in terms of slide-level tasks [42]. As a multimodal WSI-VQA method, our method can deal with various kinds of slide-level tasks if given appropriate prompts. Therefore, we evaluated the performance of our model on certain clinical tasks and reproduced several multiple instance learning (MIL) methods which are widely adopted in WSI classification for comparison, as shown in Table 2. The MIL"}, {"title": "5.3 Visualization", "content": "The co-attention heatmap which reflects the interaction between visual concepts and word embeddings can act as an intuitive explanation for the decision-making procedure and provide clinical clues for pathologists. We overlay the co-attention weights which attend to the keyword in the question with the thumbnail of the corresponding WSI. The image patches that have high attention weights are highlighted in the heatmap with red color."}, {"title": "6 Limitations and Future Work", "content": "As a generative framework, our model may suffer from hallucination which is frequently observed in large language models especially when we scale up our model. The black-box property of AI-aided methods and the possibility of generating unreasonable answers may be the major obstacles to being applied in the clinical. Although we have managed to improve the interpretability like visualizing the co-attention map, there is still a long way to go before we earn the trust of pathologists. In addition, since the current framework relies on the pre-trained visual extractor, such task-agnostic embeddings may hinder the performance of our model. In the future, we are going to scale up our pipeline to build a more powerful and general MLLM for computational pathology."}, {"title": "7 Conclusion", "content": "In this paper, we propose interpreting gigapixel whole slide images by generative visual question answering (WSI-VQA). Our approach demonstrates significant superiority in managing a broad range of slide-level tasks and achieving remarkable performance on these tasks, which reveals the potential of our method as the foundation model for computation pathology due to its scalability. The pathologists can know what they want to know or have difficulty with by giving appropriate prompts to the model. The explanation of the decision-making can be visualized in the format of heatmaps which makes our model more transparent and convincing. On the data end, we curate the first WSI-VQA dataset which is going to be public. The dataset will promote the development of MLLMs in the pathology field. In addition, other fields that involve large-resolution modalities can also be inspired by our work."}]}