{"title": "Towards Fair Graph Representation Learning in Social Networks", "authors": ["Guixian Zhang", "Guan Yuan", "Debo Cheng", "Lin Liu", "Jiuyong Li", "Shichao Zhang"], "abstract": "With the widespread use of Graph Neural Networks (GNNs) for representation learning from network data, the fairness of GNN models has raised great attention lately. Fair GNNs aim to ensure that node representations can be accurately classified, but not easily associated with a specific group. Existing advanced approaches essentially enhance the generalisation of node representation in combination with data augmentation strategy, and do not directly impose constraints on the fairness of GNNs. In this work, we identify that a fundamental reason for the unfairness of GNNs in social network learning is the phenomenon of social homophily, i.e., users in the same group are more inclined to congregate. The message-passing mechanism of GNNs can cause users in the same group to have similar representations due to social homophily, leading model predictions to establish spurious correlations with sensitive attributes. Inspired by this reason, we propose a method called Equity-Aware GNN (EAGNN) towards fair graph representation learning. Specifically, to ensure that model predictions are independent of sensitive attributes while maintaining prediction performance, we introduce constraints for fair representation learning based on three principles: sufficiency, independence, and separation. We theoretically demonstrate that our EAGNN method can effectively achieve group fairness. Extensive experiments on three datasets with varying levels of social homophily illustrate that our EAGNN method achieves the state-of-the-art performance across two fairness metrics and offers competitive effectiveness.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) have emerged as a powerful class of machine learning models, particularly suited for capturing complex relationships and interactions in a wide range of real-world systems [15, 35, 37]. GNNs update node representations by aggregating and transforming information from neighbouring nodes, a process commonly referred to as message-passing [38]. The message-passing mechanism allows the model to capture both the characteristics of individual nodes and their connectivity patterns within the graph, thus giving GNNs a powerful performance on various downstream tasks. Despite their strong performance, GNNs are often criticized for issues related to fairness and trustworthiness. Specifically, GNNs may inadvertently learn and amplify biases in the training data [27], meaning that any inherent biases in the data will be reflected in the model's predictions, potentially leading to unfair decisions for certain groups. These biased predictions raise significant ethical and social concerns, particularly in real-world applications like recommender systems [39], rumour detection [47], and social bot detection [46], where fairness is critical.\nFairness challenges in GNNs differ from those in other machine learning models because graph data involves not only node features but also the structure of the graph itself [38]. However, in social networks, users' interactions are influenced by sensitive attributes, such as gender, age and race, which can introduce biases into the graph structure. This phenomenon, referred to as \"social homophily\", describes the tendency of individuals to form connections with others who are similar to them [16, 23, 32]. This scenario is also summarised by the phrase \"similarity breeds connection\" [26]. For example, Stoica et al. [33] found that social media users are more likely to connect with others in the same age group, with male users displaying stronger homophily than female users. In social recommender systems, if users in a same group are frequently observed connecting with each other, the model may record and amplify this behaviour, ultimately recommending friends only within the same group, thereby causing bias [35]. To the best of our knowledge, no prior work has explicitly defined the problem of social homophily in fair graph representation learning, nor provided a practical solution for addressing its impact on fairness."}, {"title": "2 Preliminary", "content": "In this section, we collate the notations used in this paper and then define social homophily and fairness metrics."}, {"title": "2.1 Notations", "content": "In our work, we use $G = (V, E, A, X)$ to represent an attribute graph, where $V = {v_1, ..., v_n }$ is the set of nodes, $E$ is the set of edges, $A \u2208 R^{n\u00d7n}$ is the adjacency matrix, and $X \u2208 R^{n\u00d7d}$ is the nodes-attributes matrix, where n is the number of nodes and d is the dimension of attributes. Each node $v_i$ has a label y and a sensitive attribute $s_i$, where $s_i \u2208 {0, 1}$ and $y_i \u2208 {0, 1}$. The goal of fairness learning is to enable the model to make accurate predictions independent of sensitive attributes. In a binary classification task, with a classifier C and a representation $h_i$ for each node $v_i$, we obtain the predicted target $\u0177_i = C(h_i)$. We evaluate the effectiveness and fairness of the model by calculating the association between $\u0177_i$, the target labels $y_i$ and the sensitive attribute $s_i$."}, {"title": "2.2 Causal View", "content": "In this paper, we propose use the Structure Causal Model (SCM) [29] as shown in Figure 2 to represent the underlying process of the graph data generation and illustrate the motivation behind our work. During the generation of graph data, the sensitive (bias) variable S is typically unobserved, and affects the observed node attributes X and topology A. For instance, people of different races had varying positivity rates during COVID-19 because healthcare access may have been influenced by their economic status [2, 25]. In such cases, training a GNN can easily establish spurious correlations S and Y. Therefore, in our work, we design constraints based on the principles of sufficiency, independence, and separation to prevent"}, {"title": "2.3 Social Homophily", "content": "In this work, we focus on improving the fairness of GNNs by reducing the effect of social homophily. Similar to the definition of homophily [24, 48], we define social homophily in the graph based on whether connected node pairs belong to the same group:\nDEFINITION 1 (SOCIAL HOMOPHILY). Given a graph $G = (V, E, A, X)$ and the node group labels (sensitive attribute) S, the social homophily ratio is defined as the fraction of edges that connect nodes with the same groups. Formally, we have:\n$SH (G, {s_i; i \u2208 V}) = \\frac{1}{|E|} \u03a3_{(j,k) \u2208 E} 1 (s_j = s_k)$,\nwhere $|E|$ is the number of edges in the graph and 1 is the indicator function.\nA graph is considered to have a high degree of social homophily when SH(\u00b7) is large (typically, 0.5 \u2264 SH(\u00b7) \u2264 1). Note that in our experiments, to better validate the effectiveness of our proposed EAGNN method, we conduct experimental validation on two datasets with high social homophily and one with relatively low social homophily."}, {"title": "2.4 Fairness Metric", "content": "In this paper, we use two group-specific fairness metrics to evaluate the fairness of GNNs.\nDEFINITION 2 (STATISTICAL PARITY [6]). Statistical parity stipulates that the proportion of individuals receiving positive classifications should be approximately equal across demographic groups, i.e., S\u22a5 \u0176. Formally, statistical parity is defined as:\n$P(\u0177 | s = 0) = P(\u0177 | s = 1)$,\nwhere \u0177 is the prediction, and s is the sensitive attribute value representing a specific group.\nDEFINITION 3 (EQUAL OPPORTUNITY [12]). Equal opportunity stipulates that the true positive rate should be approximately equal across demographic groups, i.e. S 1 \u0176 | Y. Formally, equal opportunity is defined as:\n$P(\u0177 = 1 | y = 1, s = 0) = P(\u0177 = 1 | y = 1, s = 1)$,\nwhere \u0177 is the prediction, and s is the sensitive attribute value representing a specific group.\nWe use Asp and \u0394EO to measure the statistical parity differences between two groups and the equalised opportunity differences, respectively. Based on Definition 1 and Definition 2, we calculate these values as follows:\n$\u0394_{SP} = |P(\u0177 = 1 | s = 0) \u2013 P(\u0177 = 1 | s = 1)|$,\n$\u0394_{EO} = |P(\u0177 = 1 | y = 1, s = 0) \u2013 P(\u0177 = 1 | y = 1, s = 1)|$.\nNote that lower values for both metrics indicate greater fairness in the model predictions across groups."}, {"title": "3 The Proposed EAGNN Method", "content": "In this section, we start by by describing how a GNN model makes predictions, and the fairness issue from three perspective: sufficiency, independence and separation. We then design constraints for fair predictions from the three perspectives."}, {"title": "3.1 Encoding and Classification", "content": "GNNs operate on graph data by propagating information between neighbour nodes. In a k-layer GNN, the representation vector $h_i$ of node $v_i \u2208 V$ at the k-th layer captures the structural information within the k-hop subgraph surrounding $v_i$. The update process for the k-th layer of a GNN is formally defined as:\n$h_i^{(k)} = Update (h_i^{(k-1)}, Aggregate (\\{h_u^{(k-1)} | u \u2208 N(v_i)\\}))$,\nwhere $h_i^{(t)}$ is the representation vector of node $v_i \u2208 V$ at the t-th layer and $N(v_i)$ is the set of neighbours of $v_i$.\nAfter obtaining the node representation H, a MultiLayer Perceptron (MLP) is used to serve as a classifier C for predicting \u00dd:\n$\u0176 = C(H)$.\nSpecifically, the training loss function of the classifier is expressed as follows:\n$L_c = -E_{v_i\u223cv} (y_i log (\u0177_i) \u2013 (1 \u2013 y_i) log (1 \u2013 \u0177_i))$,\nwhere $y_i$ is the label of the node $v_i$."}, {"title": "3.2 Sufficiency", "content": "We assume the features of node $v_i$ are sampled from the feature distribution $F_{s_i}$, i.e., $x_i \u223c F_{s_i}$, with $\u00b5(F_{s_i})$ denoting the mean of $F_{s_i}$. The features are independent and the magnitude of each feature in X does not exceed a predefined scalar bound B, i.e., $max_{i,j} |X[i, j]| < B$. Based on these assumptions, we derive Theorem 1. The proof of the Theorem 1 is provided in Appendix A.1.\nTHEOREM 1. Let G be a graph defined by V, E. Each node $v_i$ in G is characterised by a feature vector $x_i \u2208 R^l$ and a sensitive attribute $s_i$. For any node $v_i \u2208 V$ belonging to group b, the expectation of the pre-activation output of a single Graph Convolutional Network (GCN) operation is given by:\n$E [h_i] = W (E_{b\u223cD_{s_i},x\u223cF} [x])$,                                                             (9)\nwhere W is the parameter matrix in the GCN and $D_{s_i}$ is the neighbour distribution.\nMoreover, for any positive scalar t, the likelihood that the Euclidean distance between the actual output $h_i$, and this expected output exceeds t is upper-bounded by:\n$P (||h_i - E [h_i] ||_2 \u2265 t) \u2264 2 \u22c5 l \u22c5 exp (\\frac{deg(v_i)t^2}{2p^2 (W) B^2 l})$,  (10)\nwhere l denotes the feature dimensionality, and $\u03c1(W)$ denotes the largest singular value of W.\nBy Theorem 1, we observe that the GNN model will map nodes with the same sensitive attribute to an expectation-centred area in the embedding space, with a small distance. This implies that the node representations in this case have a strong correlation with the"}, {"title": "3.3 Independence", "content": "The independence condition requires that S be independent of C(H), i.e., S\u22a5 C(H). In this section, we choose statistical parity as the criterion for independence and randomly generate S' to quantify the fairness level using the discriminator D which is constructed by a MLP. This design ensures that the predictive output of the model remains consistent under different values of the sensitive attribute, thereby reducing prediction bias with respect to the the sensitive attribute. Specifically, to achieve independence, we introduce an independence penalty $L_{in}$ for classifier C:\n$L_{in} = E_{v_i\u223cv} (log D(\u0177_i, s_i) + log(1 \u2013 D(\u0177_i, s')))$,                               (13)\nwhere $D_c$ is modeled by a MLP and $s' \u2208 S'$ is the randomly generated sensitivity value.\nThrough Theorem 2, we can obtain the optimal $D^*$. The proof of the Theorem 2 is offered in Appendix A.2.\nTHEOREM 2. Let $p_s$ and $p_\u0177$ represents the marginal density functions of the random variable S and \u0176, respectively. $p_{\u0177|s}$ is the conditional density function of \u0176 given S, and $p_{\u0177,s}$ is the joint density function of \u0176 and S. Now, we introduce the discriminator D to determine whether the model outputs \u0176 = C(H) are independent of S. The optimal discriminator $D^*$, which maximises the objective function $L_{in}$ over all possible discriminators D, can be expressed as:\n$P_{\u0177,s}(\u0177, s) = p_\u0177 (\u0177)p_s(s)$,\nwhere \u0177 \u2208 \u0176 and s \u2208 S.\nThe Independence constraint requires that the marginal distribution of the model output \u00dd does not change given the sensitive attribute S. Theorem 2 theoretically verifies that $L_{in}$ captures the difference between P(\u0176 | S) and P(\u00dd), providing a theoretical justification for using $L_{in}$ to achieve statistical parity, i.e., $P(\u0177 = 1 | s = 1) = P(\u0177 = 1 | s = 0)$. We can optimise C in a fairness-conscious way by incorporating the additional penalty $L_{in}$ into the fair risk minimisation problem. The proof of the Theorem 2 is provided in Appendix A.2."}, {"title": "3.4 Separation", "content": "The goal of separation is to ensure S \u22a5 C(H) | Y, i.e., S \u22a5 \u0176 | Y, but in real-world applications, the joint distribution of the sensitive attribute S and the label Y may not be uniform, leading to certain combinations S and Y occurring more frequently in the data. To address this, we introduce an $\u03f5$ function as a density ratio estimator in the separation constraint. This function adjusts for the non-uniformity of the distribution, ensuring that each combination is fairly considered when estimating the conditional probabilities. Specifically, the separation constraint is designed as follows:\n$R_{se} = E_{v_i\u223cv} (log D(\u0177_i, s_i, y_i) + \u03f5(s', y)log(1 \u2013 D(\u0177_i, s_i, y_i)))$, (15)\nwhere $s' \u2208 S'$ is the randomly generated sensitivity value.\nTHEOREM 3. Let $p_{\u0176|Y}$ be the conditional density function of \u0176 given Y and $p_{\u0176|S,Y}$ be of given Y and S. We are interested in finding the optimal discriminator $D^*$ that maximizes a certain objective function $R_{se}$ over all possible discriminators D:\n$D^\u2217(\u0177, s, y) = \\frac{p_{\u0176|S,Y}(\u0177 | s, y)}{p_{\u0176|S,Y}(\u0177 | s, y) + \u03f5(s, y)p_{\u0176|Y} (\u0177 | y) \\frac{p_{S,Y}(s,y)}{p_{S',Y}(s',y)}}$                                                                                                                          (16)\nfor all \u0177 \u2208 \u0176, s \u2208 S, and y \u2208 Y, where $p_{s',y}$ and $p_{s,y}$ are the joint density functions of S' and Y and of S and Y, respectively.\nIf $\u03f5(s, y)p_{s',y} (s, y) = p_{s,y} (s, y)$, then from Theorem 1 of [9], we can infer that $R_{se}$ can be explained by the Jensen-Shannon divergence JSD(\u00b7, \u00b7). Thus, we have:\n$R_{se} = 2 \u2217 JSD (P(\u0176, Y, S), P(\u0176, Y)P(S)) \u2013 log 4$,                                         (17)\nIf JSD = 0, it implies P(\u0176 | Y, S) = P(\u0176 | Y), i.e., $P(\u0177 | y = 1, s = 0) = P(\u0177 = 1 | y = 1, s = 1)$. Thus, we fit a density-ratio estimator \u03f5 by maximising:\n$R_\u03f5 = E_{s,y} [log D_\u03f5 (S, Y)] + E_{s,y} [log(1 \u2013 D_\u03f5 (S', Y))]$,\nwhere $D_\u03f5$ is modeled by a MLP.\nThe optimal decision function $D_\u03f5$ is defined as:\n$D = arg max R_\u03f5 (D_\u03f5 )$,\nwhere $R_\u03f5 (D_\u03f5 )$ represents the reward associated with the decision function $D_\u03f5$ parameteris ed by \u03f5. Under this optimal decision function, it holds that:\n$\\frac{D_\u03f5 (s, y)}{1 - D_\u03f5 (s, y)} = \\frac{p_{S,Y} (s, y)}{p_{S',Y} (s', y)}$,                             (20)\nwhere $D_\u03f5 (s, y)$ is the decision function output for a given s and y, and $p_{s,y} (s, y)$ and $p_{s',y} (s', y)$ represent the joint probability distributions of the sensitive attribute and outcome under different scenarios S and S', respectively. This ensures that the decision-making process is balanced in terms of opportunities across different scenarios.\nBy applying Theorem 3, we ensure the implementation of equal opportunity in our proposed EAGNN method. The proof of the Theorem 3 is provided in Appendix A.3. Thus, based on $R_\u03b2$ and $R_{se}$, the final separation constraint is given by:\n$L_{se} = R_\u03f5 + R_{se}$.                                                                      (21)"}, {"title": "3.5 Model Training", "content": "The proposed EAGNN method improves model fairness by integrating multiple fairness constraints while mitigating the social homophily present in the graph. The core of our EAGNN is to combine the classification loss ($L_C$) with three key fairness constraints during model training, resulting in a weighted composite loss function:\n$L = L_c + \u03b1 \u2217 L_{suff} + \u03b2 \u2217 L_{in} + \u03b3 \u2217 L_{se}$                                                  (22)\nwhere \u03b1, \u03b2 and \u03b3 are the weights assigned to each fairness constraint. Specifically, we balance the model's predictive performance and fairness by adjusting the three weights of each loss term, aiming to reduce the model's bias toward specific groups without sacrificing too much effectiveness."}, {"title": "4 Experiments", "content": "In this section, we conduct extensive experiments to evaluate the effectiveness of the EAGNN method and to assess the importance of each component."}, {"title": "4.1 Experiment setup", "content": ""}, {"title": "4.1.1 Real-world datasets", "content": "We employed three well-known datasets, namely the Recidivism, Credit, and German datasets [1, 3, 4, 36]. The details of these datasets are summarised in Table 1.\n\u2022 Credit [41]. Each node in the dataset represents a client, with 13 attributes such as marital status, age, and maximum payment amount. We use age as the sensitive attribute in our experiments.\n\u2022 German [5]. Each node represents a credit card user, and the dataset includes 27 attributes such as employment status, gender, and income. We use gender as the sensitive attribute in our experiments.\n\u2022 Bail [17]. The nodes in this dataset represent defendants on bail, each with 18 attributes such as type of case, race, and case duration. Race is used as the sensitive attribute in our experiments.\nThe social homophily of each dataset is calculated according to Definition 1. To better analyse the differences between the datasets, we define density as the ratio of the number of edges to the number of nodes. It is worth noting that although the sensitive attributes in the three real-world datasets we have chosen are discrete, our EAGNN method can be directly applied to continuous sensitive attributes. In terms of generality, our method is superior to others."}, {"title": "4.1.2 Baseline", "content": "In our experiments, we compare the proposed EAGNN method with nine state-of-the-art algorithms. Specifically, these methods can be divided into two categories: (1) Vanilla GNNs and (2) Fair GNNs. The following three methods belong to the category of Vanilla GNNs: GCN [19] captures local graph structure features by aggregating information from neighbouring nodes through convolution operations. GIN [38] employs a fine-grained feature aggregation mechanism to effectively distinguish nodes across different graphs, enhancing graph isomorphism discrimination, and making it suitable for complex graph structure analysis. SAGE [11] utilises sampling and aggregation strategies, enabling efficient training on large-scale and dynamic graphs while flexibly accommodating changes in node features.\nThe following six methods belong to the category of Fair GNNs:\nFairGNN [3] addresses bias and discrimination in GNN predictions by leveraging limited sensitive attributes and graph structures. NIFTY [1] establishes a novel framework that connects counterfactual fairness with stability in GNNs, facilitating the learning of fair and stable representations. EDITS [4] creates fairer GNNs from both feature and structural perspectives, mitigating biases present in the input graph. FVGNN [36] targets discriminatory bias by effectively addressing variations in feature correlations during propagation through feature masking strategies. FairMILE [13] is a multi-level framework designed to learn fair graph representations while incorporating fairness constraints. FairGB [22] achieves re-balancing across groups through counterfactual data augmentation and contribution alignment loss."}, {"title": "4.1.3 Evaluation metrics and implementation details", "content": "We consider F1 scores and accuracy as effectiveness metrics. For the fairness metrics, we use Statistical Parity (SP) $\u0394_{SP}$ and Equal Opportunity (\u0395\u039f) $\u0394_{\u0395\u039f}$, with smaller values for these fairness metrics indicating fairer model decisions. Following the setup of previous work [1, 22], the dataset is divided into three phases: training, validation, and testing. All FairGNNs use SAGE as the encoder, and the Adam optimisation algorithm is applied across all models. Hyperparameters were tuned in our experiments using a grid search method, and a detailed hyperparameter analysis is presented in Section 5.3."}, {"title": "4.2 Performance comparison", "content": "To gain a comprehensive understanding of EAGNN, we perform node classification tasks on three widely used real-world datasets, comparing EAGNN with other methods. From Table 2, we have four key observations:\n\u2022 EAGNN achieves satisfactory results in terms of both effectiveness and fairness across all three datasets, often obtaining competitive or even better performance compared to well-designed fairness GNN models. Notably, in some cases, fairness GNN models outperform vanilla GNNs in terms of validity, suggesting that the inherent bias in the dataset reduces the validity of the GNN, making it unreliable. Therefore, it is necessary to debias GNNs not only to improve fairness but also to enhance their overall validity.\n\u2022 EAGNN consistently achieves the best performance in terms of fairness on all datasets. This validates our hypothesis that mitigating social homophily can help GNNs learn fair representations. The three constraints-sufficiency, independence,"}, {"title": "4.3 Ablation study", "content": "To verify the necessity of each component in EAGNN, we constructed three variants of EAGNN by removing the sufficiency"}, {"title": "4.4 Hyperparameter sensitive analysis", "content": "Moreover, we conduct experiments to analyse the hyperparameters of the three constraints. We first control the independence constraint weight \u03b2 and the separation constraint weight \u03b3 unchanged, while varying the sufficiency constraint weight \u03b1 to analyse its impact. The results on the Credit and German datasets, which have high social homophily, are shown in Figure 3, and the results for the Bail dataset, which has low social homophily, are presented in Figure 5.\nAs observed in Figure 3, optimal validity and fairness are achieved when the weight of $L_{suff}$ is set to 0.15 for the Credit and German datasets, whereas for the Bail dataset, it needs to be set to 0.35. This is because low social homophily implies that individuals from different groups may differ significantly in many characteristics. In this case, increasing the sufficiency weight helps the model better learn and understand the features of non-sensitive attributes, reducing misclassification and bias toward these nodes. The sufficiency constraint encourages the model to learn shared features, even when the sensitive attributes differ, resulting in more accurate predictions. Additionally, we observe that when the sufficiency constraint is properly balanced, both fairness and model effectiveness improve. This demonstrates that the sufficiency constraint not only promotes fairness but also enhances classification accuracy by enabling sufficient learning across group boundaries.\nFor the experiments balancing the independence constraint weight \u03b2 and the separation constraint weight \u03b3, the results are displayed in Figure 4. As shown in Figure 4, the model tends to achieve better fairness when \u03b2 and \u03b3 are set to the same value. Theoretically, both the independence and separation constraints align with fairness principles, and assigning them equal weight reflects a balanced respect for these principles. Overemphasising one constraint could cause the model to overlook another important fairness considerations. Treating both constraints equally helps avoid sacrificing one fairness requirement in favour of another. As observed in Figure 4, advanced effectiveness and optimal fairness are obtained when the weight of $L_{suff}$ is set to 0.15. This suggests that sufficiency not only contributes to fairness but also leads to accurate classification through adequate learning of cross-group nodes."}, {"title": "5 Related work", "content": "In this section, we review related work on fairness in GNNs and data augmentation, which are most relevant to our EAGNN method."}, {"title": "5.1 Fairness in GNNS", "content": "There has been a wide variety of work attempting to improve the fairness of GNNs, which can be broadly categorised into pre-processing and in-processing methods. Pre-processing methods modify the original training data with the assumption that fair data produces fair models. Fairwalk [30] and Crosswalk [18] cross group boundaries by selecting each set of neighbouring nodes with probabilistic dropping or biased random walks. EDITS [4] de-configures attribute and structural information to enhance the fairness of the model. In-processing methods aim to mitigate unfairness during training by directly modifying the learning algorithm. FairGNN [3]"}, {"title": "5.2 Data Augmentation in GNNs", "content": "GNN belongs to data-driven deep neural networks, which makes its training results dependent on the quality of data. Some researchers have proposed improving the training results of the model through data augmentation, which can be specifically classified into two categories. (1) Generating new training instances by artificially introducing perturbations on the training graph, thus expanding the size of the training dataset and enhancing the model's ability to generalize to different graph structures, which is known as data augmentation. Specifically, it includes 1) subgraph sampling [34, 44], which induces subgraphs by randomly selecting nodes and their neighbours from the original graph; 2) edge modification [20, 40], which randomly removes or adds edges to the graph with a certain probability; and 3) feature masking [7, 14], which partially masks node features. These operations enhance the robustness of the model by perturbing features to form new training instances while maintaining the main topology and patterns of the original graph. (2) Structural or category imbalance is ameliorated by rebalancing ideas to avoid model bias. BLC [46] devises strategies to enhance the long tail for the imbalance problem in the structure. GRAPHENS [28] discovers the phenomenon of neighbour memory in the classification of imbalanced nodes and synthesizes self-networks to generate a few nodes based on similarity. GraphSHA [21] synthesizes only harder training samples and generates connected edges from subgraphs to stop messages from propagating from a few nodes to neighbouring classes. IA-FSNC [37] achieves effective node classification through support augmentation and shot augmentation. HyperIMBA [8] improves structural imbalance from the perspective of hyperbolic geometry.\nHowever, it is important to note that our EAGNN method differs from previous data augmentation approaches. While EAGNN randomly generates sensitive attribute values for each node, it does not modify the data itself; node representations for both training and prediction are based on the original data. We theoretically demonstrate that EAGNN can achieve group fairness on independence and separation."}, {"title": "6 Conclusion", "content": "In this paper, we provide a novel perspective on addressing the fairness issue in GNNs. We identify social homophily as a significant factor contributing to unfairness in GNNs. We demonstrate that the message-passing mechanism of GNNs tends to reinforce group-based biases due to social homophily, resulting in spurious associations between sensitive attributes and model predictions. To mitigate these effects, we propose the EAGNN method, which enhances fairness through constraints on three key aspects: sufficiency, independence, and separation. Our theoretical analysis confirms that these constraints effectively reduce bias and promote group fairness in GNN predictions. Additionally, the EAGNN method is broadly applicable to various fairness scenarios, regardless of whether sensitive attributes are continuous or discrete. Extensive experiments on three real-world datasets with varying degrees of social homophily demonstrate that our EAGNN achieves the state-of-the-art performance across two fairness metrics and offers competitive effectiveness."}, {"title": "A Theoretical proof", "content": ""}, {"title": "A.1 Social homophily effects", "content": "THEOREM 1. Let G be a graph defined by V, E. Each node $v_i$ in G is characterized by a feature vector $x_i \u2208 R^l$ and a sensitive attribute $s_i$. For any node $v_i \u2208 V$ of group b, the expectation of the pre-activation output of a single GCN operation is given by:\n$E [h_i] = W (E_{b\u223cD_{s_i},x\u223cF} [x])$,                                                             (1)\nwhere W is the parameter matrix in the GCN and $D_{s_i}$ is the neighbour distribution.\nMoreover, for any positive scalart, the likelihood that the Euclidean distance between the actual output $h_i$ and this expected output exceeds t is upper-bounded by:\n$P (||h_i - E [h_i] ||_2 \u2265 t) \u2264 2 \u22c5 l \u22c5 exp (\\frac{deg(v_i)t^2}{2p^2 (W) B^2 l})$,                                 (2)\nwhere l denotes the feature dimensionality and $\u03c1(W)$ denotes the largest singular value of W.\nPROOF. A single GCN operation is defined by H' = $D^{\u22121}AHW$, where H represents the input features and H' represents the output features of a given layer. W is a parameter matrix of size l \u00d7 l that is responsible for the transformation of the features. Additionally, D is a diagonal matrix, with its diagonal elements D[i, i] equal to deg(i), which represents the degree of node $v_i$.\nFocusing on a specific node $v_i$, the expectation of $h_i$ can be derived as follows:"}, {"title": "A.2 Independence", "content": "THEOREM 2. Let $p_s$ and $p_\u0177$ represents the marginal density function of the random variable S and \u0176. $p_{\u0177|s}$ is the conditional density function of \u0176 given S, and $p_{\u0177,s}$ is the joint density function of the random variables \u0176 and S. Now, we introduce a discriminator D which will discriminate between the model outputs \u0176 = C(H) and whether the sensitive attribute S is independent or not. The optimal discriminator $D^*$ that maximizes a certain objective function $L_{in}$ over all possible discriminators D can be expressed as:\n$P_{\u0177,s}(\u0177, s) = p_\u0177 (\u0177)p_s(s)$.\nwhere \u0177 \u2208 \u0176 and s \u2208 S.\nPROOF. Let \u0177 = C(h) where h is a specific node representation and s is its sensitive attribute. The loss function $L_{in}$ can be written:\n$\\int log D(\u0177, s)p(h, s) d\u0177 ds + \\int log (1 \u2013 D (\u0177, s')) p (\u0177, s') d\u0177ds'$,          (20)\n= $\\int log D(\u0177, s)p(\u0177 | s)p(s) + log(1 \u2013 D(\u0177, s))p(\u0177)p(s) d\u0177 ds$.                             (21)\nBy the proof of Proposition 1 in [9], $L_{in}$ is maximized at:\n$D^* (\u0177, s) = \\frac{p(\u0177)p(s)}{p(\u0177 | s)p(s) + p(\u0177)p(s)} = \\frac{p(\u0177s)}{p(\u0177 | s) + p(\u0177)}$,\nfor any \u0177 \u2208 \u0176 and s \u2208 S.\nAccording to the argument in Theorem 1 of [9], $L_{in}$ can be explained by the Jensen-Shannon divergence JSD(\u00b7, \u00b7), i.e:\n$L_{in} (C; D^*) = 2J (P(\u0176, S), P(Y)P(S)) \u2013 log 4$.\nIf JSD = 0, it implies $p_{\u0177,s}(\u0177, s) = p_\u0177 (\u0177)p_s(s)$ for \u0176 and S."}, {"title": "A.3 Separation", "content": "THEOREM 3. Let $p_{\u0176|Y"}, "be the conditional density function of H given Y and $p_{C(H)|S,Y}$ be of given Y and S. Now, we introduce a discriminator D which discriminates whether the model output \u0176 = \u0176 is independent of the sensitive attribute S, given Y.The optimal discriminator $D^*$ that maximizes a certain objective function $L_{se}$ over all possible discriminators D can be expressed as:\n$D^\u2217(\u0177, s, y) = \\frac{p_{\u0176|S,Y}(\u0177 | s, y)}{p_{\u0176|S,Y}(\u0177 | s, y) + \u03f5(s, y)p_{\u0176|Y} (\u0177 | y) \\frac{p_{S,Y}(s,y)}{p_{S',Y}(s',y)}}$                                                                                                                          (24)\nfor all \u0177 \u2208 \u0176, s \u2208 S, and y \u2208 Y, where $p_{s',y}$ and $p_{s,y}$ are the joint density functions of"]}