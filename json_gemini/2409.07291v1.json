{"title": "Exploring User-level Gradient Inversion with a Diffusion Prior", "authors": ["Zhuohang Li", "Andrew Lowy", "Jing Liu", "Toshiaki Koike-Akino", "Bradley Malin", "Kieran Parsons", "Ye Wang"], "abstract": "We explore user-level gradient inversion as a new attack surface in distributed learning. We first investigate existing attacks on their ability to make inferences about private information beyond training data reconstruction. Motivated by the low reconstruction quality of existing methods, we propose a novel gradient inversion attack that applies a denoising diffusion model as a strong image prior in order to enhance recovery in the large batch setting. Unlike traditional attacks, which aim to reconstruct individual samples and suffer at large batch and image sizes, our approach instead aims to recover a representative image that captures the sensitive shared semantic information corresponding to the underlying user. Our experiments with face images demonstrate the ability of our methods to recover realistic facial images along with private user attributes.", "sections": [{"title": "Introduction", "content": "Computing and storing gradients is essential for training most types of modern deep-learning models, ranging from compact neural networks [1] designed for edge applications to massive foundational models [2] that are fine-tuned using gradient descent. In circumstances such as distributed training and federated learning [3], gradients are directly exchanged in place of raw training data to facilitate joint learning from large-scale distributed data.\nPlainly sharing gradient information was long presumed to be safe. However, gradient inversion, a new type of privacy threat was recently demonstrated that calls this assumption into question. Zhu et al. [4] demonstrated that an adversary can recover the underlying private training data via an iterative gradient-matching optimization procedure. While plausible, this optimization problem becomes ill-posed and difficult to solve when the data is high-dimensional [5] and from a large batch size [6], or if local defenses are applied to obfuscate the gradients [7]. As a result, existing solutions based on total variation [5] or generative adversarial network (GAN) based image priors [8, 7] are prone to degradation in their reconstructed image quality with decreasing amounts of details as batch size and image resolution increase. Further, GAN-based approaches are also known to suffer from mode collapse [9], which can result in limited sample diversity and degrade overall attack performance.\nIn this work, we explore user-level gradient inversion as a new attack surface in distributed learning, beyond the risks of training data recovery. This is motivated by collaborative learning settings, where data across users may be neither independent nor identically distributed, and particularly when the data from each user may consistently belong to a distinct individual. Thus, while recovering individual data samples may be difficult, the potential inference of sensitive user-level characteristics may still pose a risk.\nWe empirically investigate the user-level privacy risks of existing gradient inversion methods, re- revealing their abilities to recover some level of private information albeit with low image quality. To improve reconstructed image quality and enable attacks against large batch sizes, we also propose a new type of user-level gradient inversion attack, utilizing a pre-trained diffusion model prior to assist the reconstruction of meaningful images. Specifically, in contrast to existing sample-level attacks that attempt to fully reconstruct every image in the target batch, we instead aim to synthesize a single representative image that captures the overall semantics of the private image batch, which makes the search space dimension invariant to the target batch size and thereby significantly reduces computational overheads and improves convergence stability.\nOur method is motivated by the recent success in the research domain of using diffusion priors to solve inverse problems [10, 11, 12, 13]. However, most existing research focuses on typical image inverse problems, such as denoising and inpainting, which have well-defined linear forward operators. By contrast, the forward process in our problem involves the calculation of model gradients, which involves a complex series of non-linear operations. Moreover, the measurements in gradient inversion are the observed gradients, which contrasts with traditional inverse problems, where both the original and measured signals are in the image space, rendering many existing methods inapplicable. To address these challenges, we apply a plug-and-play diffusion prior technique [14] that formulates the inference of the posterior distribution as an optimization problem and propose a dynamic optimization scheme that focuses on recovering the high-level semantics at early stages and then fine-tune image details at the later stage. To the best of our knowledge, this is the first study to investigate the application of a diffusion model prior for improving gradient inversion.\nDistinct from the prior literature, our evaluation extends beyond the conventional performance metrics for gradient inversion which measure only image-level similarity to inspect the recovery of certain private attributes that are more semantically meaningful and interpretable. Through experiments on the CelebA facial image dataset [15], we show that the proposed user-level gradient inversion method can effectively recover high-level semantics of large-batches of images that carry important private information about the person, including gender, race, age, and facial identity, without relying on strong adversarial assumptions such as the BatchNorm statistics [16]."}, {"title": "User-level Gradient Inversion with Diffusion Prior", "content": ""}, {"title": "Problem Formulation", "content": "We consider the typical supervised distributed learning setting with a set of clients $\\mathcal{C}$, where the goal is to find the optimal model parameters $w$ of a neural network $f_w$ such that the empirical risk is minimized, i.e., $\\min_w \\sum_{c \\in \\mathcal{C}} \\sum_{(x^{(i)}, y^{(i)}) \\in \\mathcal{D}_c} \\mathcal{L}(f_w(x^{(i)}), y^{(i)})$, where $x^{(i)}$, $y^{(i)}$ denotes the $i$-th image and corresponding label, respectively, $\\mathcal{D}_c$ represents the client's local training dataset, and $\\mathcal{L}$ is the loss function (e.g., cross-entropy). The most common practice is to optimize $w$ through stochastic gradient descent. Specifically, at each training step, the client node samples a batch of $B$ images $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^B$ to compute the averaged gradients $\\nabla w = \\frac{1}{B} \\sum_{i=1}^B \\nabla_w \\mathcal{L}(f_w(x^{(i)}), y^{(i)})$, which are then transferred over the communication channel to the server (in the parameter server setting) or its peers (in the peer-to-peer setting). In the gradient inversion attack, the adversary observes the user's gradients $\\nabla w$ and global model weights $w$ and tries to recover the original images and labels. When the adversary's access is limited to aggregated gradients (e.g., under secure aggregation), they may first initiate a disaggregation attack [17] to obtain gradients at the user level. Following prior work [18, 5, 6, 8, 7], we assume the labels can be analytically recovered from the gradient of the last layer. Therefore, the images can be reconstructed by solving the following optimization problem:\n$X^{(1)*}, ..., X^{(B)*} = \\arg \\min_{X^{(1)},...,X^{(B)}} d(\\mathcal{F}(X^{(1)}, ..., X^{(B)}), \\nabla w)$,\nwhere $d$ is a distance metric (e.g., Euclidean squared distance [4] or cosine distance [5]), $X^{(1)}, ..., X^{(B)}$ is a batch of synthetic data, and $\\mathcal{F}(X^{(1)}, ..., X^{(B)}) = \\frac{1}{B} \\sum_{i=1}^B \\nabla_w \\mathcal{L}(f_w(X^{(i)}),y^{(i)})$ is the correspond- ing gradient. This technique is referred to as gradient matching [4] in the literature and it has been empirically shown that as the distance in the gradient space reduces, the synthetic images will gradually recover the original images."}, {"title": "User-Level Gradient Inversion", "content": ""}, {"title": "Challenge of Sample-level Reconstruction", "content": "The gradient inversion attack in its canonical form (Eq. 1) suffers poor scalability in multiple aspects. First, due to the ill-posedness of the inverse problem, the returned synthetic image through naively minimizing the gradient matching loss may not be a natural image. These effects become more significant on deeper networks and higher-resolution images. One mitigation is to instead solve:\n$X^{(1)*}, ..., X^{(B)*} = \\arg \\min_{X^{(1)},...,X^{(B)}} d(\\mathcal{F}(X^{(1)}, ..., X^{(B)}), \\nabla w) + \\mathcal{R}_{prior}(X^{(1)}, \u2026\u2026\u2026, X^{(B)})$,\nwhere $\\mathcal{R}_{prior}$ is an additional prior term (e.g., total variation [5]) introduced to regularize the synthetic image. A more recent line of work considers using the generator $G$ from pre-trained GANs [8, 7] as a regularization and solve for low-dimensional latent vectors $Z_1, Z_2, ..., Z_B$ that minimizes the gradient matching loss, i.e.,\n$Z^{(1)*}, ..., Z^{(B)*} = \\arg \\min_{Z^{(1)},...,Z^{(B)}} d(\\mathcal{F}(G(Z^{(1)}), ..., G(Z^{(B)})), \\nabla w)$.\nAlthough this form enables the gradient inversion to generalize to larger images, it still does not address the other challenge that the search space scales linearly with the batch size $B$ and the attack performance quickly degrades as $B$ increases. As such, most existing gradient inversion attacks are confined to small-sized images with relatively small batch sizes (e.g., [8] considered a batch of 4 images rescaled to the size of 64 \u00d7 64px). The only exception is the work by Yin et al. [6], which is based on the strong assumption that the clients share additional BatchNorm statistics beyond regular gradients [16]. Another common assumption by prior work is that images in the batch are uncorrelated [19] or from different classes [6], which may not hold when learning across non-IID data. Extending gradient inversion attacks to practical batch sizes without such strong assumptions remains a challenging research problem [16]."}, {"title": "From Sample-level to User-level Inversion", "content": "In the sample-level gradient inversion attacks as described above, the adversary observes the gradients $\\nabla w$ computed from a batch of $B$ images, and the goal is to reconstruct every sample in the original batch, which is equivalent to estimating images to maximize $p(x_1, x_2, ..., x_B|\\nabla w)$ [20]. However, in many practical distributed learning scenarios, such as cross-device federated learning on mobile devices [21, 22], data may often have strong local correlation (and may not be identical between users), i.e., local data samples are from the same user and thus have similar semantics. In these settings, the ultimate goal of a practical attack may not be image reconstruction, but rather recovery of private information about the user, such as demographics and identity. To account for this semantic consistency across local batches, we extend the Bayesian model of the data to introduce a latent encoding $h_c$ that captures the user demographics that the adversary wishes to recover (e.g., gender, age, race, etc.). Joint recovery of the images and latent encoding, given a fixed $\\nabla w$, is guided by the probabilistic model\n$p(x^{(1)}, ..., X^{(B)}, h_c|\\nabla w) \\propto p(x^{(1)}, ..., X^{(B)}, h_c, \\nabla w)$\n$= p(\\nabla w|x^{(1)}, ..., X^{(B)})p(h_c|X^{(1)}, ..., X^{(B)})p(X^{(1)}, ..., X^{(B)})$, where the equality is due to $h_c \\rightarrow (X^{(1)}, ..., X^{(B)}) \\rightarrow \\nabla w$ forming a Markov chain, and which suggests the reconstruction optimization\n$\\arg \\max_{X^{(1)},...,x^{(B)}, h_c} \\log p(\\nabla w|x^{(1)}, ..., X^{(B)}) + \\log p(h_c|X^{(1)}, ..., X^{(B)}) + \\log p(X^{(1)}, ..., X^{(B)})$.\nHence, the adversary should consider three terms, where the third term is the image prior, which will be discussed in the next section. The first term implies the recovered images should produce the observed gradient, which is achievable through gradient matching. The second term implies that the images should be consistent with the latent encoding, i.e., sharing semantics due to being from the same user.\nExploiting this semantic similarity, while obtaining computational advantages, the adversary can instead aim to recover a single representative image $x$ that captures the semantics of the original batch $\\{x^{(i)}\\}_{i=1}^B$ by solving\n$x^* = \\arg \\min_{x} - \\log p(\\nabla w|\\mathcal{A}(x)) \u2013 \\log p(x)$,\nwhere $\\mathcal{A}(x) = \\{T(x)\\}_{i=1}^B$ is a batch of $B$ augmented images produced from $x$ with ran- dom semantics-preserving image transformation(s) $T$ (i.e., $p(h_c|T(x)) = p(h_c|x)$), and use $\\arg \\max_{h_c} p(h_c|x)$ as the predicted value for $h_c$. This approach has the benefits of simplifying the reconstruction of a potentially large batch to a single image, which substantially reduces the computational cost and memory consumption, accelerates the optimization, and thereby enables the attack to be extended to larger batch sizes. Despite this, in practice optimizing the first term in Eq. 7 drives the synthetic image $x$ towards the direction where its produced gradient approximates the observed gradient computed from a batch of images, which is likely to result in $x$ converging to an unrealistic noisy image. To obtain a natural image, we need to employ a strong prior to regularize $x$ during the optimization process, which we introduce next."}, {"title": "Diffusion Prior for Gradient Inversion", "content": ""}, {"title": "Denoising Diffusion Probabilistic Model as Prior", "content": "The denoising diffusion probabilistic model (DDPM) [23] is a generative model based on a Markov chain capturing a forward diffusion process that progressively adds Gaussian noise, given by\n$q(x_{1:T}|x_0) = \\prod_{t=1}^T q(x_t|x_{t-1}),  q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)$,\nwhere $x_0 := x \\sim q$ represents a data sample from the distribution being modelled, and $\\beta_t \\in (0, 1)$ are fixed noise schedule hyper-parameters. This forward process can be summarized as\n$q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I)$,"}, {"title": "Optimization", "content": "In our implementation, we employ cosine distance [5] as a proxy for the gradient matching term in Eq. 14, which allows the overall optimization to be solved with standard gradient descent. However, due to the stochasticity of the reverse diffusion process, in practice the optimization becomes stochastic and may produce different estimates from different runs that could potentially deviate from the observed gradient. To ensure the production of reliable results that satisfy both the diffusion prior and the gradient constraint, ideally, the early stage of the optimization should coarsely explore the search space, while the later stage of the optimization should carefully fine-tune the estimated $x_0$ to steadily converge to a local maximum [14].\nTo this end, we make the following refinements to the optimization process. First, we observe that to achieve the best recovery, the gradient matching loss should be adjusted according to the phase of the optimization: in the early stage, we should first focus on recovering the correct high- level semantics such as gender and race; then the later stage we can extend to reconstruct detailed facial attributes. Therefore, instead of assigning equal weights to all gradients, we apply a sliding asymmetric Hamming window function to dynamically adjust the weight of the gradient of each layer. Second, instead of simultaneously optimizing for all $t$, we optimize according to a schedule that gradually anneals from $T$ to a small value $t_{min}$. Third, similar to [23], we use constant weights $w_t$ to simplify the optimization objective. To balance the two terms, we instead clip the gradient norm of the gradient matching loss dynamically according to the gradient norm of the diffusion prior loss term. The full algorithm and a detailed description of the procedure are included in the Appendix."}, {"title": "Experiments", "content": "Setup. We consider the binary classification task of attractiveness on the CelebA dataset [15]. The images are resized to 64 \u00d7 64 px and gradients are computed on a randomly initialized ResNet-18 with a default batch size of 30. To simulate cross-device FL settings, images are grouped according to user ID. The experiments use the first 50 users with \u2265 30 images as the evaluation set."}, {"title": "Alternative Formulation", "content": "There are several alternative ways to implement the user-level inversion as described in Eq. 6. For instance, apart from the single-image-based formulation explored in this paper, one may modify the sample-level approach to enforce the consistency of the latent encoding in the reconstructed images through the guidance of external classifiers. Alternatively, an adversary with access to the private data distribution may learn a model to directly predict $h_c$ and further leverage the recovered $h_c$ as a condition for synthesizing private images, i.e., model $p(h_c|\\nabla w)$ and then sample from $p(x|h_c) \\propto p_{\\theta}(x)p(h_c|x)$. Regarding the utilization of the diffusion prior, an alternative approach is to use guided sampling from diffusion with modified conditions [33, 13]. We leave explorations in these spaces as future work."}]}