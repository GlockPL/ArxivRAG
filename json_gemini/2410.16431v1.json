{"title": "Conjuring Semantic Similarity", "authors": ["Tian Yu Liu", "Stefano Soatto"], "abstract": "The semantic similarity between sample expressions measures the distance between their latent 'meaning'. Such meanings are themselves typically represented by textual expressions, often insufficient to differentiate concepts at fine granularity. We propose a novel approach whereby the semantic similarity among textual expressions is based not on other expressions they can be rephrased as, but rather based on the imagery they evoke. While this is not possible with humans, generative models allow us to easily visualize and compare generated images, or their distribution, evoked by a textual prompt. Therefore, we characterize the semantic similarity between two textual expressions simply as the distance between image distributions they induce, or 'conjure.' We show that by choosing the Jensen-Shannon divergence between the reverse-time diffusion stochastic differential equations (SDEs) induced by each textual expression, this can be directly computed via Monte-Carlo sampling. Our method contributes a novel perspective on semantic similarity that not only aligns with human-annotated scores, but also opens up new avenues for the evaluation of text-conditioned generative models while offering better interpretability of their learnt representations.", "sections": [{"title": "Introduction", "content": "Semantic similarity is about comparing data not directly, but based on their underlying 'concepts' or 'meanings'. Since meanings are most commonly expressed through natural language, various methods have attempted to compute them in this space. Words have been often compared based on the occurrences of other words that surround them, and images have likewise been compared 'semantically' by using the text captions that describe them.\nWhile measuring semantic similarity comes natural to humans who share significant knowledge and experience, defining semantic similarity for trained models is non-trivial since text can often be ambiguous or open to multiple (subjective) interpretations. On the other hand, visual elements often transcend communication barriers and allow for comparison at finer granularity. Comparing images is also relatively simple - unlike words, pixel values do not depend on distant knowledge or context.\nHence, instead of comparing images by the captions which describe them, we propose the converse: comparing textual expressions in terms of the images they conjure. In other words, we propose an expanded notion of meaning that is purely \"visually-grounded\". This would be hard if not impossible for humans, since the process requires visualizing and comparing 'mental images' each individual can conceive, but it is straightforward for trained models.\nWe propose to leverage modern image generative models, in particular the class of text-conditioned diffusion models, for this purpose. By doing so, the semantic similarity between two text passages can simply be measured by the similarity of image distributions generated by a model conditioned on those passages.\nThere is a technical nugget that needs to be developed for the method to be viable, which is how to compare diffusions in the space of images. To address this, we propose to leverage the Jensen Shannon Divergence between the stochastic differential equations (SDEs) that govern the flow of the diffusion model, which we will show to be computable using a Monte-Carlo sampling approach."}, {"title": "Related Works", "content": "We begin by briefly surveying the litera-ture on text-conditioned image generation. Goodfellow et al. (2020) proposed the Generative Adversarial Network (GAN), a deep learning-based approach for image generation. Mirza & Osindero (2014) proposed a method for conditioning GANs based on specified labels. Works such as VQ-VAE (Van Den Oord et al., 2017) and VQ-VAE-2 (Razavi et al., 2019) also built upon the foundational works of Variational Auto Encoders (VAEs) (Kingma, 2013) to learn discrete representations used to generate high quality images, among other outputs, when paired with an autoregressive prior. DALL-E (Ramesh et al., 2021) has also been developed as an effective text-to-image generator leveraging autoregressive Transformers (Vaswani, 2017).\nOur work focuses on diffusion models (Sohl-Dickstein et al., 2015), which have achieved state-of-the-art results among modern image generation models. These models can be viewed"}, {"title": "Semantic Space of Generative Models", "content": "The Distributional Hypothesis (Harris, 1954) forms the basis for statistical semantics, characterizing the meaning of linguistic items based on their usage distributions. This is also closely related to Wittgenstein's use theory of meaning (Wittgenstein, 1953), often popularized as \"meaning is use\". Many methods have been developed in machine learning and Natural Language Processing (NLP) literature to compute these semantic spaces, including Word2Vec (Mikolov et al., 2013). In light of modern Large Language Models (LLMs), Liu et al. (2023) defined the space of meanings for autoregressive models to be the distribution over model continuations for any input sequence. This has been used to define notions of semantic distances and semantic containment between textual inputs. Achille et al. (2024) further defined conceptual similarity between images by projecting them into the space of distributions over complexity-constrained captions, producing similarity scores that strongly correlate with human annotations. Soatto et al. (2023) generalizes these definitions by considering meanings as equivalence classes, where partitions can be induced by either an external agent or the model itself. Vector-based representations such as those obtained from CLIP Radford et al. (2021), or in general any sentence embedding model (Devlin et al., 2018; Opitz & Frank, 2022) have also been defined specifically for the computation of semantic distances. Such representations, however, are often difficult to interpret.\nIn contrast to these works, Bender & Koller (2020) argues that training on language alone is insufficient to capture semantics, which they argue requires a notion of \"communicative intents\" that are external to language. In this paper, we explore a notion of meaning that is grounded in the distribution of evoked imageries, and present a simple algorithm to compute interpretable distances in this space for the class of text-conditioned diffusion models."}, {"title": "Evaluation and Interpretation of Diffusion Models", "content": "Common metrics used to evaluate diffusion models are, among many others, the widely-used FID (Heusel et al., 2017) score, Kernel Inception Distance (Bi\u0144kowski et al., 2018), and the CLIP score (Hessel et al., 2021). Despite these choices, Stein et al. (2024) recently discovered that no existing metric used to evaluate diffusion models correlates strongly with human evaluations. Several techniques have also been developed to interpret the generation of diffusion models. Kwon et al. (2022); Park et al. (2023) edit the bottleneck representations within the U-Net architecture of diffusion models for semantic image manipulation. Gandikota et al. (2023) identifies low-rank directions corresponding to various semantic concepts, and Li et al. (2023) shows that diffusion models can be used for image classification. Kong et al. (2023a,b) frame diffusion models using information theory, improving interpretability of their learnt semantic relations. Orthogonal to these works, our method enables visualizing the semantic relations between textual prompts in natural language learnt by diffusion models via the distributions over their generated imageries."}, {"title": "Method", "content": "We will present a short preliminary on (conditional) diffusion models in Section 3.1, and derive our algorithm for computing semantic similarity in Section 3.2."}, {"title": "Preliminary", "content": "Our derivations will leverage Song et al. (2020b)'s the SDE formulation of diffusion models when viewed from the lens of score-based generaive modeling. In particular, a (forward) diffusion process {$x(t)$}$_{t=0}^{T}$ can be modeled as the solution to the following SDE:\n$dx = f(x,t)dt + g(t)dw_{t}$\nwith drift coefficient $f: R^{d} \\times R \\rightarrow R^{d}$ and (scalar) diffusion coefficient $g: R \\rightarrow R$, where $w_{t}$ is standard Brownian motion. We constrain the timesteps t such that $t \\in [0,T]$, where $x(0)$ represents the distribution of \"fully-denoised\" images generated by the diffusion model. We also assume that by construction, the prior at time T is known and distributed according to $x(T) \\sim \\pi$, where often $\\pi = N(0, I)$.\nOnce trained, we can view text-conditioned diffusion models as a map $s_{\\theta}(x, t|y)$ parameterized by $\\theta$ and conditioned on a textual prompt $y \\in Y$ that is used to approximate the score function (Song et al., 2020b), where $s_{\\theta}(\\cdot, t|y): R^{d} \\rightarrow R^{d}$ and $Y$ is the set of textual expressions. As such, each conditional model $s_{\\theta}(x,t|y)$ defines a reverse-time SDE given by:\n$dx = [f(x, t) - g(t)^{2}s_{\\theta}(x,t|y)]dt + g(t)d\\bar{w}_{t}$ (1)\nwhere $\\bar{w}_{t}$ is the Brownian motion running backwards in time from $t = T$ to $t = 0$. For ease of notation, we will denote $\\mu_{\\theta}(x, t, y) := [f(x,t) - g(t)^{2}s_{\\theta}(x,t|y)]$"}, {"title": "Construction", "content": "Given two textual prompts $y_{1}$ and $y_{2}$, we obtain two separate diffusion SDEs in the space of images using eq. (1), which are given by\n$dx_{1} = \\mu_{\\theta} (x_{1},t,y_{1})dt + g(t)d\\bar{w}_{t}$ (2)\n$dx_{2} = \\mu_{\\theta} (x_{2}, t, y_{2})dt + g(t)d\\bar{w}_{t}$ (3)\nWe assume the standard conditions for existence and uniqueness of their solutions, in par-ticular for all $t \\in [0,T]$ we have $E [||x(t)||^{2}] < \\infty$, and for all $y \\in Y$ and $x, x' \\in R^{d}$, there exists constants C, D such that $|\\mu_{\\theta}(x, t,y)||^{2}+||g(t)||^{2} \\le C(1+ ||x||^{2})$ and $|\\mu_{\\theta}(x, t, y) - \\mu_{\\theta}(x',t,y)||^{2} < D||x - x'||^{2}$. We further assume Novikov's Condition holds for all pairs $y_{1}, y_{2} \\in V$, in particular we are guaranteed the following: $E[exp(\\int_{0}^{T} ||\\frac{1}{g(t)}[\\mu_{\\theta}(x, t, y_{2}) - \\mu_{\\theta}(x, t, y_{1})]||^{2}dt)]< \\infty$.\nSince our goal is to define a semantic distance between textual prompts $y_{1}$ and $y_{2}$ by com-paring the distributions over images that they conjure, we can achieve this by computing a"}, {"title": "Experiments", "content": "We describe implementation details in Section 4.1, empirical validation for our definitions in Section 4.2, and ablations in Section 4.3."}, {"title": "Implementation Details", "content": "We use Stable Diffusion v1.4 (Rombach et al., 2022), a text-conditioned diffusion model, for all our experiments. For sampling, we use classifier-free guidance (Ho & Salimans, 2022) with guidance scale of 7.5, and sample using the LMS Scheduler (Karras et al., 2022). We specify image sizes to be 512 \u00d7 512, but note that Stable Diffusion v1.4 uses latent diffusion, as such model predictions are in practice of dimension 64 \u00d7 64. We compute the Euclidean distance directly in this space. However, for visualization experiments such as in Figure 1, we decode the"}, {"title": "Empirical Validation", "content": "While our work defines a notion of semantic distance grounded in evoked imagery, the validity of this definition hinges on its use as a measure of similarity that aligns with humans. In particular, we should expect our definition to produce measurements of similarity that agree often with human annotators (which can be viewed as \u201cground-truth\u201d).\nTo quantify this, we use the Semantic Textual Similarity (STS) (Agirre et al., 2012, 2013, 2014, 2015, 2016; Cer et al., 2017) and Sentences Involving Compositional Knowledge (SICK-R) (Marelli et al., 2014) datasets, containing pairs of sentences each labelled by human annotators with a semantic similarity score ranging from 0-5. We then use our method to compute the image-grounded similarity score, and measure their resulting Spearman Correlation with the annotations.\nInterestingly, our experiments in Table 1 show that our visually-grounded similarity scores exhibit significant degrees of correlation with that annotated by humans. While, expectedly, our method presently lags behind embedding models trained specifically for semantic comparison tasks, we show that our visually-grounded similarity scores can rival that produced by existing large language models up to 33B in size. As a remark, we note the large standard deviations in"}, {"title": "Ablations", "content": "In this section, we run ablation studies using the STS-B dataset as a benchmark to explore the design space of our method, and improve its computational efficiency.\nIn our algorithm, we placed a uniform prior over timesteps {1,...,T}. In Figure 3, we show that this simple choice works best when evaluated on align-ment with human annotators on STS-B, as compared to other choices such as considering only a uniform prior over the subset {T',...,T} (cumulative from T' to T) where T' < T, or a Direc delta on any particular timestep \u03a4' \u2208 {1, ...,T} (pointwise).\n\n\nThe computational feasibility of our method depends on the number of Monte-Carlo steps (i.e. k in Algorithm 1) required to produce a reliable approxi-mate of the desired distance. We ablate over choices of k \u2208 {1, ..., 5} in Figure 3 and show that the deviation of scores when evaluated on the STS-B dataset is small (\u00b10.77) across different choices of k. This finding is promising, as it implies that our method can be computationally efficient, requiring only a small number of Monte-Carlo iterations to converge.\nOn the right of Figure 3, we also show that our results are relatively consistent across several versions of Stable Diffusion models, including Stable Diffusion XL and Stable Diffusion 3 Medium."}, {"title": "Discussion and Limitations", "content": "Our method has several limitations. First, imageries might indeed not be sufficient to fully capture the meaning of certain expressions, such as mathematical abstractions (like 'imaginary numbers') and meta-physical concepts (like 'conscience'). Furthermore, many modern diffu-sion models use a pre-trained text-encoder to pre-process textual prompts. This means that representation structures obtained from the diffusion model outputs would be bottle-necked by those learnt by the text encoder. However, this limitation can be mitigated by the development of better encoders, such as those based on LLMs (BehnamGhader et al., 2024). Additionally, these encoder-based (vector) representations are often difficult to interpret, while our proposed method offers a way to visualize and interpret the learnt semantic similarities between tex-tual expressions. Lastly, computation costs also remain a key limitation of our method, since it requires several inference passes through the diffusion model to compute a single semantic similarity score, mitigated only partially by the conclusions of our ablation study on required number of iterations.\nNevertheless, our method is the first to show that textual representations can be meaningfully"}]}