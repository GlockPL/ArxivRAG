{"title": "Denoising Fisher Training For Neural Implicit Samplers", "authors": ["Weijian Luo", "Wei Deng"], "abstract": "Efficient sampling from un-normalized target distributions is pivotal in scientific computing and machine learning. While neural samplers have demonstrated potential with a special emphasis on sampling efficiency, existing neural implicit samplers still have issues such as poor mode covering behavior, unstable training dynamics, and suboptimal performances. To tackle these issues, in this paper, we introduce Denoising Fisher Training (DFT), a novel training approach for neural implicit samplers with theoretical guarantees. We frame the training problem as an objective of minimizing the Fisher divergence by deriving a tractable yet equivalent loss function, which marks a unique theoretical contribution to assessing the intractable Fisher divergences. DFT is empirically validated across diverse sampling benchmarks, including two-dimensional synthetic distribution, Bayesian logistic regression, and high-dimensional energy-based models (EBMs). Notably, in experiments with high-dimensional EBMs, our best one-step DFT neural sampler achieves results on par with MCMC methods with up to 200 sampling steps, leading to a substantially greater efficiency over 100 times higher. This result not only demonstrates the superior performance of DFT in handling complex high-dimensional sampling but also sheds light on efficient sampling methodologies across broader applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Efficiently drawing samples from un-normalized distributions is a fundamental issue in various domains of research, including Bayesian inference (Green, 1995), simulations in biology and physics (Sch\u00fctte et al., 1999; Olsson, 1995), and the fields of generative modeling and machine learning (Xie et al., 2016; Andrieu et al., 2003). The goal is to generate sets of samples from a target distribution defined by a differentiable, but possibly not normalized, potential function, denoted as log q(x). The main challenge lies in ensuring the accuracy of the target distribution with minimal sampling costs, especially in simulations involving multi-modal distributions.\nThe sampling challenge can be tackled using two main methodologies. The first involves a range of Markov Chain Monte Carlo (MCMC) techniques (Hastings, 1970; Roberts and Rosenthal, 1998; Xifara et al., 2014; Neal, 2011). These techniques create Markov Chains that are designed to have the desired target distribution as their stationary distribution. By simulating these chains, random noises can be iteratively adjusted to stationary distributions following the target distribution. While MCMC methods are renowned for generating asymptotically unbiased samples, they often face computational inefficiencies, particularly when dealing with barriers in high-dimensional multi-modal distributions (Huang et al., 2024). This is largely due to the large number of iterations necessary to produce a batch of samples that accurately represent the target distribution.\nThe second category of methods is known as learning to sample (L2S) models (Levy et al., 2018; Hu et al., 2018; Wu et al., 2020; di Langosco et al., 2021; Arbel et al., 2021; Zhang and Chen, 2021; Matthews et al., 2022; Vargas et al., 2022; Lahlou et al., 2023). These models leverage modern neural networks for sampling, aiming to improve both the quality and efficiency of sample generation compared to traditional non-learning-based methods such as MCMCs."}, {"title": "2 RELATED WORKS", "content": "Neural networks have emerged as powerful tools in various domains, demonstrating their ability to produce diverse, high-quality samples. They have been successfully applied in tasks such as text-to-image generation (Brock et al., 2018; Karras et al., 2019, 2020, 2021; Nichol and Dhariwal, 2021; Dhariwal and Nichol, 2021; Ramesh et al.; Saharia et al., 2022; Rombach et al., 2022), audio generation (Huang et al., 2023), video and 3D creation (Clark et al., 2019; Ho et al., 2022a; Molad et al., 2023; Poole et al., 2022), and even molecule design (Nichol et al., 2021; Ho et al., 2022b). Recently, there has been a growing interest in leveraging neural networks for sampling from target distributions.\nThree primary classes of neural networks have been"}, {"title": "2.1 Neural Samplers", "content": "Neural networks have emerged as powerful tools in various domains, demonstrating their ability to produce diverse, high-quality samples. They have been successfully applied in tasks such as text-to-image generation (Brock et al., 2018; Karras et al., 2019, 2020, 2021; Nichol and Dhariwal, 2021; Dhariwal and Nichol, 2021; Ramesh et al.; Saharia et al., 2022; Rombach et al., 2022), audio generation (Huang et al., 2023), video and 3D creation (Clark et al., 2019; Ho et al., 2022a; Molad et al., 2023; Poole et al., 2022), and even molecule design (Nichol et al., 2021; Ho et al., 2022b). Recently, there has been a growing interest in leveraging neural networks for sampling from target distributions.\nThree primary classes of neural networks have been extensively studied for sampling tasks. The first class comprises normalizing flows (NFs) (Rezende and Mohamed, 2015), while the second class consists of diffusion models (DMs) (Song et al., 2021). NFs employ invertible neural transformations to map Gaussian latent vectors z to obtain samples x. The strict invertibility of NF transformations enables the availability of likelihood values for generated samples, which are differentiable with respect to the model's parameters. Training NFs often involves minimizing the KL divergence between the NF and the target distribution (Wu et al., 2020). On the other hand, DMs employ neural score networks to model the marginal score functions of a data-initialized diffusion process. DMs have been successfully employed to enhance the sampler performance of the annealed importance sampling algorithm (Neal, 2001), a widely recognized MCMC method for various sampling benchmarks. Despite the successes of NFs and DMs, both models have their limitations. The invertibility of NFs restricts their expressiveness, which can hinder their ability to effectively model high-dimensional targets. Moreover, DMs still require a considerable number of iterations for sample generation, resulting in computational inefficiency."}, {"title": "2.2 Implicit Neural Samplers", "content": "Different from other neural samplers, neural implicit samplers are favored for their high efficiency and flexible modeling ability. Typically, an implicit generative model uses a flexible neural transform ge(.) to push forward easy-to-sample noises z ~ p\u2082 to obtain samples x = go(z). Though implicit samplers have multiple advantages, training them is not easy.\nResearch has explored various techniques for efficiently training neural implicit samplers by focusing on minimizing divergence. For example, the study by Hu et al. (2018) examines the training process through the reduction of the Stein discrepancy, while another study looks into practical algorithms designed to decrease the Kullback-Leibler divergence and the Fisher divergence. Although these approaches have shown promising results, they are not without their shortcomings. In particular, the methods intended to minimize Fisher divergence, despite their theoretical validity, have struggled to perform effectively with target distributions that exhibit high multi-modality(Luo et al., 2024). Besides, the training approach that minimizes the KL divergence also encounters sub-optimal performances (Luo et al., 2024). On the contrary, our introduced Denoising Fisher Training overcame the above issues by introducing a new stable and equivalent loss. Before we introduce the DFT, we give some background knowledge in Section 3."}, {"title": "3 BACKGROUND", "content": "Standard Score Matching. Score matching (Hyv\u00e4rinen and Dayan, 2005) provided practical approaches to estimating score functions. Assume one only has available samples x ~ p and wants to use a parametric approximated distribution q(x) to approximate p. Such an approximation can be made by minimizing the Fisher Divergence between p and q with the definition\n$D_{FD}(p,q) := E_x {|\\nabla_x log p(x)||^2 + |\\nabla_x log q(x)||^2 - 2\\langle \\nabla_x log p(x), \\nabla_x log q(x) \\rangle\\}.$\nUnder certain mild conditions, the objective equals to\n$\\mathcal{L}(\\phi) = E_p {|\\nabla_x log q(x)||^2} + 2\\Delta_x log q(x) \\tag{1}$\nThis objective can be estimated only through samples from p, thus is tractable when q is well-defined. Moreover, one only needs to define a score network $s_\\phi(x)$: $\\mathbb{R}^D \\rightarrow \\mathbb{R}^D$ instead of a density model to represent the parametric score function. This technique was named after Score Matching. Other variants of score matching were also studied (Song et al., 2019; Pang et al., 2020; Meng et al., 2020; Lu et al., 2022; Bao et al., 2020).\nDenoising Score Matching. Notice that the standard score-matching objective has a term $\\Delta_x log q(x)$ that is computationally expensive. Denoising score matching (Vincent, 2011) is introduced to remedy this issue. The concept of denoising score matching is to perturb the sampling distribution by adding small Gaussian noises and then letting the score network match the conditional score functions. Specifically, let x ~ p be drawn from the sampling distribution. And let $x_\\sigma = x + \\sigma\\epsilon, \\epsilon \\sim N(0, I)$. When $\\sigma$ is small, the distribution $p_\\sigma$ of $x_\\sigma$ is a good approximation of the sampling distribution p of x. We use p(x|x\u03c3) to denote the conditional distribution. If we want to use a parametric distribution q (or a neural score function s(.)) to approximate the distribution of The matching objective of denoising score matching is to minimize\n$\\mathcal{L}_{DSM}(\\phi) = E_{x_0 \\sim p, x_\\sigma|x \\sim p(x_0)} { - ||\\nabla_{x_\\sigma} log q(x_\\sigma) - \\nabla_{x_\\sigma} log p(x_\\sigma|x_\\sigma)||^2} \\tag{2}$"}, {"title": "4 DENOISING FISHER TRAINING", "content": "In this section, we introduce the denoising Fisher training (DFT) method and show its equivalence to minimizing the Fisher divergence. The DFT incorporates a noise-injection and denoising mechanism into the training of implicit sampler and shows significant stability than the previous Fisher training method in both theoretical and empirical aspects."}, {"title": "4.1 The Training Objective", "content": "The Problem Setup. Let $g_\\theta(\u00b7)$: $\\mathbb{R}^{D_z} \\rightarrow \\mathbb{R}^{D_x}$ be an implicit sampler (i.e., a neural network transform), $p_z$ the latent distribution, $p_\\theta$ the sampler induced distribution $x = g_\\theta(z)$, and q(x) the un-normalized target. Our goal in this section is to train the implicit sampler $g_\\theta$ such that $p_\\theta(x)$ equals q(x). Instead of directly minimizing the Fisher divergence between $p_\\theta(x)$ and q(x), we add some little noise to the implicit sampler to tweak the sampler distribution with\n$x_\\sigma := x + \\sigma \\epsilon, x_0 = g_\\theta(z), \\epsilon \\sim N(0, I) \\tag{3}$\nTherefore the conditional distribution p(x|x\u03c3) = N(x\u03bf, \u03c3\u00b2I) has an explicit form. When o is set to be small, the distribution $p_{\\theta,\\sigma}$, which represents the distribution of x is a sufficiently good approximation of the implicit distribution $p_\\theta$ of xo. We denote $s_q(x_\\sigma) := \\nabla_{x_\\sigma}log q(x)$ and $s_{\\theta,\\sigma}(x) := \\nabla_{x_\\sigma} log p_{\\theta,\\sigma}(x_\\sigma)$. Our goal is to minimize the Fisher divergence between $p_{\\theta,\\sigma}(x)$ and q(x) which writes\n$D_{FD}(\\theta) := E_{x_0 \\sim p_{\\theta,\\sigma}} {||s_q(x_\\sigma) \u2013 s_{\\theta,\\sigma}(x_\\sigma)||^2} \\tag{4}$\nThe Intractabe Objective To make the derivation neat, we may use the notion x (0) and x\u03c3 interchange-ably to emphasize the parameter dependence of x\u03c3 and \u03b8 if necessary. In order to minimize the objective equation 4, we take the \u03b8 gradient for such an objective, which writes\n$\\frac{\\partial}{\\partial \\theta} D_{FD}(\\theta) = E_{x_0 \\sim p_{\\theta,\\sigma}} {||s_q(x_\\sigma) \u2013 s_{\\theta,\\sigma}(x_\\sigma)||^2} \\frac{\\partial}{\\partial \\theta}\n= E_{x_0 \\sim p_{\\theta,\\sigma}} {\\frac{\\partial}{\\partial \\theta} {||s_q(x_\\sigma) \u2013 s_{\\theta,\\sigma}(x_\\sigma)||^2}} x_\\sigma(\\theta)\n- 2 \\langle s_q(x_\\sigma) \u2013 s_{\\theta,\\sigma}(x_\\sigma), \\frac{\\partial}{\\partial \\theta} s_{\\theta,\\sigma}(x_\\sigma) \\rangle \\frac{\\partial}{\\partial \\theta} x_\\sigma(\\theta)\n= Grad_1(\\theta) + Grad_2(\\theta). \\tag{5}$\nWhere Grad\u2081 (\u03b8) and Grad2 (\u03b8) are defined with\n$Grad_1(\\theta) = E_{x_0 \\sim p_{\\theta,\\sigma}} {\\frac{\\partial}{\\partial \\theta} {||s_q(x_\\sigma) \u2013 s_{\\theta,\\sigma}(x_\\sigma)||^2}} \\frac{\\partial}{\\partial \\theta} x_\\sigma(\\theta)$\n$Grad_2(\\theta) = E_{x_0 \\sim p_{\\theta,\\sigma}} {-2 \\langle s_q(x_\\sigma) \u2013 s_{\\theta,\\sigma}(x_\\sigma), \\frac{\\partial}{\\partial \\theta} s_{\\theta,\\sigma}(x_\\sigma) \\rangle \\frac{\\partial}{\\partial \\theta} x_\\sigma(\\theta)}$\nThe gradient formula of equation 5 considers all path derivatives concerning parameter \u03b8. We put a detailed derivation in the Appendix.\nNotice that the first gradient term Grad\u2081(\u03b8) can be obtained if we stop the \u03b8 gradient for $s_{\\theta,\\sigma}(.)$, i.e. $s_{sg[\\theta],\u03c3}(.)$ and minimize a corresponding loss function\n$\\mathcal{L}_1(\\theta) = E_{x_0 \\sim p_{\\theta,\\sigma}} {||s_q(x_\\sigma) \u2013 s_{sg[\\theta],\u03c3}(x_\\sigma)||^2}\n= E_{z~Pz, \\epsilon~N(0,1)} {||s_q(x_\\sigma) \u2013 s_{sg[\\theta],\u03c3}(x_\\sigma)||^2}|_{x_\\sigma=g_\\theta(z)+\\sigma\\epsilon} \\tag{6}$\nHowever, the second gradient Grad2(\u03b8) include the term $s_{\\theta,\\sigma}(.)$ which is unknown yet intractable. This is because, for the implicit sampler, we only have efficient samples from the implicit distribution, but the score function $s_{\\theta,\\sigma}(.)$ along with its \u03b8 derivative is unknown.\nThe Tractable yet Equivalent Objective. Though gradient Grad\u2082(\u03b8) is intractable, fortunately, in this paper, we can address such an issue by introducing theoretical tools in Theorem 1.\nTheorem 1. If distribution $p_{\\theta,\\sigma}$ satisfies some wild regularity conditions, then we have for all vector-valued score function $s_q(\u00b7)$, the equation holds for all parameter \u03b8:\n$\\frac{\\partial}{\\partial \\theta} {2[s_q(x_\\sigma) \u2013 s_{sg[\\theta],\u03c3}(x_\\sigma)]^T \\nabla_{x_\\sigma} s_{\\theta,\\sigma}(x_\\sigma)}\n= E_{x_0 \\sim p_z, x_0=g_\\theta(z), \\epsilon~N(0,1), x_\\sigma=x_0+\\sigma\\epsilon}  E_{p(x_\\sigma | x_0 )} [s_{sg[\\theta], (x_\\sigma) \u2013 \\nabla_{x_\\sigma} log p(x_\\sigma | x_0 )] \\tag{7}$\nWe put the detailed proof in the Appendix. Theorem 1 shows that the intractable gradient Grad\u2082(\u03b8) happens to be the same as the gradient of a tractable loss equation 7 if we can have a good pointwise approximation of the score network $s_{sg[\\theta],\u03c3}(\u00b7,\u00b7)$. In practice, such an approximation is not difficult due to mature score function estimation techniques as we will discuss in Section 3. Therefore, we can minimize a tractable loss function equation 8 using gradient-based algorithms to get the intractable gradient Grad2(\u03b8):\n$\\mathcal{L}_2(\\theta) = E_{z~Pz, x_0=g_\\theta(z), \\epsilon~N(0,1), x_\\sigma=x_0+\\sigma\\epsilon}  {2[s_q(x_\\sigma) \u2013 s_{sg[\\theta],\u03c3}(x_\\sigma)]^T [s_{sg[\\theta],\u03c3}(x_\\sigma) \u2013 \\nabla_{x_\\sigma} log p(x_\\sigma | x_0 )]}$\n$= E_{z~Pz, x_0=g_\\theta(z), \\epsilon~N(0,1), x_\\sigma=x_0+\\sigma\\epsilon} {2[s_q(x_\\sigma) \u2013 s_{sg[\\theta],\u03c3}(x_\\sigma)]^T [s_{sg[\\theta],\u03c3}(x_\\sigma) \u2013 \\nabla_{x_\\sigma} log p(x_\\sigma | x_0 )]}  \\tag{8}$\nCombining with equation 6 and equation 8 with equation 5, we can have the final equivalent loss function\n$\\mathcal{L}_{DFT}(\\theta) = \\mathcal{L}_1(\\theta) + \\mathcal{L}_2(\\theta) \\tag{9}$\nWhere $\\mathcal{L}_1(\\theta)$ and $\\mathcal{L}_2(\\theta)$ are defined in equation 6 and equation 8. We name our training objective the denoising Fisher training objective because it originates from minimizing the Fisher divergence between the slightly tweaked sampler and the target distribution."}, {"title": "5 EXPERIMENTS", "content": "In this section, we validate neural samplers trained with DFT across a range of three distinct sampling benchmarks following Luo et al. (2024). These benchmarks span a spectrum of complexity, from low-dimensional (2-dimensional targets) to high-dimensional (784-dimensional image targets) tasks."}, {"title": "5.1 2D Synthetic Target Sampling", "content": "Experiment Settings. We leverage the open-source implementation from Sharrock and Nemeth (2023)\u00b9 and adhere to the experimental configurations established by Luo et al. (2024) for the training of samplers on six 2D target distributions. Our comparison encompasses a range of methods, including 3 MCMC baselines: Stein variational gradient descent (SVGD) (Liu and Wang, 2016), Langevin dynamics (LD) (Welling and Teh, 2011), and Hamiltonian Monte Carlo (HMC) (Neal et al., 2011); one explicit baseline: coupling normalizing flow (Dinh et al., 2016); and four implicit samplers: KL training sampler (Luo et al., 2024), Fisher training sampler (Luo et al., 2024), KSD neural sampler (KSD-NS) (Hu et al., 2018), and SteinGan (Wang and Liu, 2016). All implicit samplers are designed with a uniform neural architecture, which consists of a four-layer multilayer perceptron (MLP) with 400 hidden units per layer, and employ ELU activation functions for both the sampler and the score network where applicable.\nWe evaluate sampling results with kernelized Stein's discrepancy (KSD) (Liu et al., 2016), which is a widely used metric for evaluating the quality of a batch of samples to a target distribution with score functions (Liu et al., 2016; Gorham and Mackey, 2015). In our evaluations, we utilize the KSD with the inverse multiquadric (IMQ) kernel, which is implemented through"}, {"title": "5.2 Bayesian Logistic Regression", "content": "Having established the effectiveness of our sampler on low-dimensional 2D target distributions in the previous section, we now turn our attention to more complex, real-world scenarios. Bayesian logistic regression offers a set of medium-dimensional target distributions (ranging from 10 to 100 dimensions) that are ideal for this purpose. In this section, we pit our DFT neural samplers on solving this problem following Hu et al. (2018) and Luo et al. (2024).\nThe primary objective of this experiment is to assess the performance of our proposed sampler in terms of test accuracy when applied to Bayesian logistic regression tasks. To provide a comprehensive evaluation, we also compare our sampler's performance against that of various MCMC samplers. This comparison will shed light on how our sampler stacks up against existing methods in a real-world application, allowing us to"}, {"title": "5.3 Sampling from Energy-based Models", "content": "A pivotal benefit of employing an implicit sampler is its superior inference efficiency, which is particularly advantageous for applications where traditional sampling methods fall short, such as in the context of high-dimensional energy-based models (EBMs). The inefficiency of default sampling methods in these scenarios often stems from the complex and intricate nature of the models, which can make sampling a computationally demanding task."}, {"title": "6 CONCLUSION AND LIMITATIONS", "content": "In this paper, we've introduced denoising implicit training (DFT), an innovative technique for training implicit samplers to draw samples from distributions with un-normalized density. We've shown theoretically that DFT is equivalent to minimizing the Fisher divergence between a slightly tweaked neural sampler distribution and the target distribution. Through intensive quantitative experiments on scales of small, medium, and high dimensions, we show that neural samplers trained with DFT have better performances than other neural samplers and MCMC samplers with a decent number of sampling steps. Besides, our theoretical assessments on deriving the tractable loss may shed light on broader applications when handling intractable Fisher divergences.\nNevertheless, there are certain limitations to our proposed methods. First, the process of score estimation is not computationally inexpensive, making it a crucial avenue for research to develop a more efficient training algorithm that could eliminate the score estimation phase. Furthermore, currently, our sampler is confined to sampling tasks. Exploring how to extend this methodology to other applications, such as generative modeling, presents another intriguing area for future investigation."}, {"title": "B Theory", "content": "In this section, we introduce the denoising Fisher training (DFT) method and show its equivalence to minimizing the Fisher divergence. The DFT incorporates a noise-injection and denoising mechanism into the training of implicit sampler and shows significant stability than the previous Fisher training method in both theoretical and empirical aspects."}, {"title": "B.1 Proof of equation 5", "content": "Proof.\n$\\frac{\\partial}{\\partial \\theta} D_{FD}(\\theta) = E_{x_0 \\sim p_{\\theta,\\sigma}} {||s_q(x_\\sigma) \u2013 s_{\\theta,\\sigma}(x_\\sigma)||^2} \\frac{\\partial}{\\partial \\theta}\n= E_{x_0 \\sim p_{\\theta,\\sigma}} {\\frac{\\partial}{\\partial \\theta} {||s_q(x_\\sigma) \u2013 s_{\\theta,\\sigma}(x_\\sigma)||^2}} x_\\sigma(\\theta)\n= E_{\\epsilon~N(0,1), z~Pz} {\\frac{\\partial}{\\partial \\theta} {||s_q(g_\\theta(z) + \\sigma \\epsilon) \u2013 s_{\\theta,\\sigma}(g_\\theta(z) + \\sigma \\epsilon)||^2}} \\frac{\\partial g_\\theta(z) + \\sigma \\epsilon}{\\partial \\theta} \\tag{10}\n- 2 \\langle s_q(x_\\sigma) \u2013 s_{\\theta,\\sigma}(x_\\sigma), \\frac{\\partial}{\\partial \\theta} s_{\\theta,\\sigma}(x_\\sigma) \\rangle \\frac{\\partial}{\\partial \\theta} x_\\sigma(\\theta)\n= E_{x_0 \\sim p_{\\theta,\\sigma}} {\\frac{\\partial}{\\partial \\theta} {||s_q(x_\\sigma) \u2013 s_{\\theta,\\sigma}(x_\\sigma)||^2}} \\frac{\\partial}{\\partial \\theta} x_\\sigma(\\theta) -2 \\langle s_q(x_\\sigma) \u2013 s_{\\theta,\\sigma}(x_\\sigma), \\frac{\\partial}{\\partial \\theta} s_{\\theta,\\sigma}(x_\\sigma) \\rangle \\frac{\\partial}{\\partial \\theta} x_\\sigma(\\theta)\n= Grad_1(\\theta) + Grad_2(\\theta). \\tag{12}"}, {"title": "B.2 Proof of Theorem 1", "content": "Proof. First, we prove a Lemma 2. This lemma has been used for proving the equivalence of Denoising Score Matching (Vincent, 2011; Zhou et al., 2024) and Standard Score Matching (Hyv\u00e4rinen and Dayan, 2005) as we introduced in Section 3.\nLemma 2. Let u(.) be a vector-valued function, using the notations of Theorem 1. Let $q_\\sigma (x|x_\\sigma) =N(x;x_\\sigma,\\sigma^2I)$, then under mild conditions, the identity holds:\n$E_{x_0 \\sim p_{\\theta,\\sigma}}  E_{x|x_\\sigma \\sim q_\\sigma (x|x_\\sigma)} u(x_\\sigma, \\theta)^T {s_{\\theta,\\sigma}(x_\\sigma) \u2013 \\nabla_{x_\\sigma} log q_\\sigma (x|x_\\sigma)} = 0, \\forall \\theta. \\tag{13}$\nProof of Lemma 2. Recall the definition of $p_{\\theta,\\sigma}$ and $s_{\\theta,\\sigma}$:\n$\\rho_{\\sigma,\\theta}(x) = \\int q_\\sigma (x|x_\\sigma)p_{\\theta,\\sigma}(x)dx_\\sigma \\tag{14}$\n$s_{\\theta,\\sigma}(x) = \\int \\nabla_x log q_\\sigma (x|x_\\sigma) \\frac{q_\\sigma (x|x_\\sigma)p_{\\theta,\\sigma}(x)}{ \\rho_{\\sigma,\\theta}(x)} dx_\\sigma \\tag{15}$\nWe may use u for short of $u(x, \\theta)$. We have\n$E_{x_\\sigma \\sim p_{\\theta,\\sigma}} u^T s_{\\theta,\\sigma}(x) = E_{x_\\sigma \\sim p_{\\theta,\\sigma}}  E_{x|x_\\sigma \\sim q_\\sigma (x|x_\\sigma)} \\nabla_x log q_\\sigma (x|x_\\sigma) \\frac{q_\\sigma (x|x_\\sigma)p_{\\theta,\\sigma}(x)}{ \\rho_{\\sigma,\\theta}(x)} dx_\\sigma \\tag{16}$\n$= \\int  \\nabla_x log q_\\sigma (x|x_\\sigma) \\frac{q_\\sigma (x|x_\\sigma)p_{\\theta,\\sigma}(x)}{ \\rho_{\\sigma,\\theta}(x)} dx_\\sigma p_{\\theta,\\sigma}(x_\\sigma) dx_\\sigma \\tag{17}$\n$= \\int \\int u \\nabla_x log q_\\sigma (x|x_\\sigma) q_\\sigma (x|x_\\sigma)p_{\\theta,\\sigma}(x)dx dx_\\sigma \\tag{18}$\n$= E_{x|x_\\sigma \\sim q_\\sigma (x|x_\\sigma)} \\int u^T \\nabla_x log q_\\sigma (x|x_\\sigma) \\tag{19}$"}, {"title": "C Experiments", "content": "In this section, we validate neural samplers trained with DFT across a range of three distinct sam- pling benchmarks following Luo et al. (2024). These benchmarks span a spectrum of complexity, from low-dimensional (2-dimensional targets) to high-dimensional (784-dimensional image targets) tasks."}, {"title": "C.1 Experiment Details on 2D Synthetic Sampling", "content": "Model architectures. In our 2D synthetic data experiments, a 4-layer Multi-Layer Perceptron (MLP) neural network, equipped with 200 neurons in each layer, serves as our sampler. This network employs a LeakyReLU activation function with a leakage parameter of 0.2. Concurrently, the score network is also an MLP with 4 layers and 200 neurons per layer, utilizing GELU activation."}, {"title": "C.2 Experiment Details on Bayesian Regression", "content": "Experiment settings. Following the setup in Hu et al. (2018), we define the prior of the weights as p(w | \u03b1) = N (w; 0, a\u22121) and p(a) = Gamma(a;1,0.01). The Covertype dataset Blackard and Dean (1999), comprising 581,012 samples with 54 features, is split into a training set (80%) and a testing set (20%). Our methods are benchmarked against Stein GAN, SVGD, SGLD, DSVI Titsias and L\u00e1zaro-Gredilla (2014), KSD-NS, and Fisher-NS Hu et al. (2018). While SGLD, DSVI, and SVGD are trained for 3 epochs (approximately 15k iterations), neural network-based methods (Fisher-NS, KSD-NS, Stein GAN, KL-NS, and DFT-NS) are trained until convergence. Given the equivalence of Fisher-NS Hu et al. (2018), we train the implicit sampler using the denoising Fisher training method. The learning rate for both the score network and the sampler is set to 0.0002. The target distribution is approximated using 500 random data samples, with the score estimation phase set to 2. The logistic regression model's test accuracy is evaluated using 100 samples from the trained sampler.\nFor Bayesian inference, we adopt the same configuration as in Hu et al. (2018). The neural samplers are constructed with a 4-layer MLP containing 1024 hidden units per layer and GELU activations. The sampler's output dimension is 55, with an input dimension of 550, aligning with the setup in Hu et al. (2018). The score network mirrors the sampler's architecture but with an input dimension of 55. Adam optimizers with a learning rate of 0.0002 and default beta values are used for both networks. A batch size of 100 is employed for training the sampler, with the score network updated twice for every sampler update. Standard score matching is used to train the score network. The sampler undergoes 10k iterations per repetition, with 30 independent repetitions to determine the mean and standard deviation of the test accuracy. For SGLD, the learning rate follows 0.1/(t+1)0.55 as recommended in Welling and Teh (2011), with the last 100 points averaged for evaluation. DSVI uses a learning rate of 1e-07 with 100 iterations per stage. SVGD employs an RBF kernel with bandwidth determined by the median trick Liu and Wang (2016), utilizing 100 particles with a step size of 0.05 for evaluation.\nModel architectures. Our model employs 4-layer MLP neural networks for both the sampler and the score network. The GELU Hendrycks and Gimpel (2016) activation function is utilized for both, with a hidden dimension of 1024. The sampler's input dimension is set to 128."}, {"title": "C.3 Experiment Details on Sampling from EBMS", "content": "Datasets and model architectures. We pre-train a deep (multi-scale) Energy-Based Model (EBM) Li et al. (2019), denoted as Ed(.), with 12 residual layers He et al. (2015), following the approach in DeepEBM (Li et al., 2019). The noise levels are initialized at omin = 0.3 and capped at omax = 3.0. The energy for varying noise levels is modeled as Er(x) = f(x)/\u03c3. With a learning rate of 0.001, the EBM is pre-trained for 200k iterations. Samples are extracted from the deep EBM using an annealed Langevin dynamics algorithm similar to that in Li et al. (2019). Our implicit sampler is a neural network comprising four inverse convolutional layers with hidden dimensions of 1024, 512, 256, and 3. Each layer incorporates 2D BatchNormalization and a LeakyReLU activation with a leak parameter of 0.2. The implicit generator's prior is a standard Multivariate Gaussian distribution. The score network adheres to a UNet architecture adapted from the repository\u00b3. To align with the multi-scale EBM design, the score function is parameterized as So(x) := S(x)/\u03c3."}]}