{"title": "Text-to-3D Gaussian Splatting with Physics-Grounded Motion Generation", "authors": ["Wenqing Wang", "Yun Fu"], "abstract": "Text-to-3D generation is a valuable technology in virtual reality and digital content creation. While recent works have pushed the boundaries of text-to-3D generation, producing high-fidelity 3D objects with inefficient prompts and simulating their physics-grounded motion accurately still remain unsolved challenges. To address these challenges, we present an innovative framework that utilizes the Large Language Model (LLM)-refined prompts and diffusion priors-guided Gaussian Splatting (GS) for generating 3D models with accurate appearances and geometric structures. We also incorporate a continuum mechanics-based deformation map and color regularization to synthesize vivid physics-grounded motion for the generated 3D Gaussians, adhering to the conservation of mass and momentum. By integrating text-to-3D generation with physics-grounded motion synthesis, our framework renders photo-realistic 3D objects that exhibit physics-aware motion, accurately reflecting the behaviors of the objects under various forces and constraints across different materials. Extensive experiments demonstrate that our approach achieves high-quality 3D generations with realistic physics-grounded motion.", "sections": [{"title": "1. Introduction", "content": "Text-to-3D modeling has demonstrated remarkable achievements in creating highly realistic 3D representations of objects. Recently, several works have made great progress in generating delicate 3D objects using text-to-image priors [8, 14, 51, 55]. Additionally, other works have strides in producing the motion of the given 3D objects [9, 20, 27, 33, 57]. Despite these advancements, current methods face challenges in synthesizing realistic 3D objects from inefficient text prompts and accurately simulating their physics-grounded motion.\n3D Gaussian Splatting [21] has become a prominent technique in the domain of neural rendering, due to its remarkable ability to render delicate details, point-based representation, and rapid rendering speed. Several works have leveraged 3D GS to generate photo-realistic 3D models from the text prompts [5, 21, 28, 30, 56, 60]. A notable work GSGEN [6] integrates 3D GS with diffusion priors to produce 3D objects with highly realistic structures and visual fidelity. Other works adopt 3D Gaussian representations to model dynamic motion [10, 17, 23, 62]. Xie et al. introduces a remarkable framework PhysGaussian [52] that utilizes the physics models that describe the materials' behaviors to guide the 3D GS to simulate the object motion. These works have laid a robust foundation for the integration of text-to-3D generation and 3D-to-motion simulations.\nHowever, current works have not fully explored techniques for producing high-quality 3D models with realistic, physics-grounded motion from text prompts. In addition, existing text-to-3D frameworks are often guided by text-to-2D image generation models, which have limited text-"}, {"title": "2. Related Work", "content": "understanding ability. This limitation can lead to unsatisfied 3D generations when given poorly written text prompts. To overcome these challenges, we introduce a new framework that enables text-to-3D generation of physics-grounded motion with the aid of LLM-based prompt refinement. To achieve this, we utilize an LLM to refine the input text prompts. Then, we adopt 3D Gaussians as our 3D object representations and use the 3D (shape) diffusion prior and 2D (image) diffusion prior to guide the 3D GS to create photorealistic 3D models with reasonable geometric shapes and realistic appearances. Furthermore, we simulate physics-grounded motion on the generated 3D Gaussians by using a continuum mechanics-based deformation map to deform the Gaussian kernels. Additionally, we introduce a color regularization technique to ensure that the rendered objects maintain accurate and consistent colors. As a result, our framework generates high-quality 3D objects that exhibit physics-grounded motion. In conclusion, our main contributions include:\n\u2022 We present an innovative framework for synthesizing high-quality 3D objects with realistic, physics-based motion derived from text prompts.\n\u2022 We leverage an LLM to refine text prompts and diffusion priors to guide the generation of geometrically accurate and visually appealing 3D models.\n\u2022 We utilize a continuum mechanics-based deformation map combined with a color regularization technique to produce realistic 3D object motion with accurate colors."}, {"title": "2.1. Neural Rendering", "content": "Recent breakthroughs in neural rendering have significantly impacted novel view synthesis. Rendering with radiance fields gained considerable interest due to their remarkable ability to synthesize novel views and their significant promise for advancing 3D generative tasks. Building on this foundation, Neural Radiance Fields (NeRF) [32] revolutionizes volumetric rendering by leveraging neural networks to encode 3D scenes, achieving impressive rendering results. Subsequent works have emerged to improve NeRF in tasks such as 3D scene reconstruction [3, 41, 53, 61], in-the-wild scene handling [4, 31, 45], training speed optimization [7, 25, 49], and rendering quality improvement [11, 16, 48]. However, NeRF poses a computational challenge because of the extensive sampling required along each ray, leading to slow rendering speed and high memory consumption. To overcome these challenges, a point-based rendering method 3D Gaussian Splatting [21] is introduced to represent scenes with 3D Gaussians and render with a fast rasterization method. This enables it to achieve both rapid rendering and high-quality generation. In our proposed framework, we leverage 3D Gaussians to represent"}, {"title": "2.2. Text-to-3D Generation", "content": "As a groundbreaking approach in generative AI, text-to-3D generation enables synthesizing 3D models directly from the input text prompts. With the recent advancement in diffusion models, a wave of advancements in text-to-3D generation utilize diffusion priors to guide the 3D generation to be aligned with the text prompt description [6, 8, 14, 26, 40, 46, 54]. DreamBooth3D [40] proposes an efficient optimization strategy that leverages the 3D consistency of NeRF with the 2D diffusion prior. DreamFusion [37] utilizes a score distillation sampling loss to align 2D diffusion prior with the generated images during optimization. Magic3D [29] employs a coarse-to-fine optimization process, incorporating diffusion priors to accelerate NeRF's optimization and improve the quality of generated results. Building on these foundational works, our approach also utilizes diffusion priors to guide 3D generation, focusing on producing high-quality 3D objects with realistic appearances and well-defined shapes."}, {"title": "2.3. The Material Point Method", "content": "The Material Point Method (MPM) is a computational framework designed to simulate material behaviors by integrating both particle and grid-based approaches [44]. The MPM represents materials as a set of particles that are mapped onto a grid to compute and simulate the material deformations, which enables the simulation of diverse material properties [12, 24, 58, 59]. Owing to these advantages, we utilize the MPM to generate the physics-grounded motion for our 3D objects."}, {"title": "3. Method", "content": "We introduce a framework for synthesizing 3D models with physics-grounded motion using LLM-refined prompts, diffusion priors, and a deformation map (Figure 2). We initially employ an LLM to refine the prompt into a more explicit, detailed, and logically coherent form. For efficient and high-quality 3D model generation, we utilize 3D Gaussian splatting as our object representation. To address challenges such as the Janus problem and to improve the accuracy of the generated 3D object's shape and appearance, we incorporate guidance from both a 3D shape diffusion prior and a 2D image diffusion prior. Subsequently, a deformation map grounded in continuum mechanics is applied to the 3D Gaussian kernels, enabling realistic motion rendering that adheres to the principles of mass and momentum conservation. This section presents an in-depth explanation of the proposed framework."}, {"title": "3.1. 3D Gaussian Splatting", "content": "3D Gaussian Splatting achieves high-quality scene reconstruction with fast training and rendering speeds [21]. As a point-based rendering approach, it represents a scene using 3D Gaussians, defined by their position (mean) $x_i$, covariance matrix $\\sigma_i$, opacity $\\alpha_i$, and spherical harmonic coefficients $c_i$ as $G(x) = e^{-(x-x_i)^T\\Sigma_i^{-1}(x-x_i)}$. To render a scene, GS first projects the 3D Gaussians into 2D space. To achieve fast rendering, GS employs a tile-based rasterization strategy, which sorts the projected 2D Gaussians based on their depth in view space. Each screen tile is processed by a thread block that loads the Gaussians into shared memory and computes the final pixel colors via alpha-blending:\n$C = \\sum_{i \\in N} c_i\\alpha_i \\prod_{j=1}^{i-1}(1 - \\alpha_j)$,\nwhere $\\alpha$ indicates the opacity, $c_i$ represents the color of each point $i$, and $N$ denotes the total number of tile Gaussians. To produce high-quality radiance field representation, GS conducts optimization using $L_1$ and $D - SSIM$ loss: $L_{GS} = (1 - \\lambda)L_1+ \\lambda L_{D-SSIM}$, and it adaptively controls the density of the 3D Gaussians through pruning and densifying processes [21]. To leverage its fast rendering speed and high-quality rendering ability, we integrate 3D Gaussian Splatting into our framework to generate 3D Gaussians as our object representation. We further extend the GS kernel to incorporate time-dependency in $x_i$ and $\\sigma_i$, enabling"}, {"title": "3.2. LLM-Prompt Refinement", "content": "Text-to-3D generation often produces suboptimal results when the input prompt is vague, overly complex, or involves intricate logical relationships. This limitation arises primarily from the constrained text comprehension capabilities of the guidance models used in the process. Typically, 3D generation models rely on 2D content generation frameworks, including methods like diffusion models [35, 42]. These 2D generation models in turn depend on classifier guidance models like CLIP's text encoder [39]. These classifier guidance models lack advanced natural language understanding capabilities and are trained on datasets with simple textual descriptions that do not contain complex logic or detailed relational information. Hence, the visual concepts they encode are limited, restricting text-to-3D models to perform effectively only with simple prompts. Furthermore, when the prompts are too vague or brief, the 2D generation models may not have enough context to provide accurate or detailed guidance images for the 3D generation models.\nHowever, we notice that Large Language Models have showcased exceptional abilities in text comprehension, processing, and refinement, owing to their Transformer-based architecture and mechanisms like self-attention and contextual embeddings [2, 34, 47]. Therefore, we leverage an LLM, ChatGPT-4, to refine text prompts for improved text-"}, {"title": "3.3. Text-to-3D GS", "content": "to-3D generation. Following the LLM prompt engineering practices in the community [50], the revision instruction prompts that we give to the LLM are composed of the context and task components."}, {"title": "3.4. Physics-Grounded Deformation Map", "content": "nal text prompts that needs refinement. As mentioned above, some of the issues regarding the original prompts include vagueness, description complexity, and logical complicity. For example, a prompt might simply describe an object with minimal details, such as \"a red rose\" (Figure 3). However, this prompt lacks sufficient context and fails to specify details about the rose beyond its color, which can result in generated outputs that are vague or insufficiently detailed. Furthermore, prompts might also be overly lengthy and intricate (Figure 4), making it challenging for models to accurately interpret and generate the intended output.\nTask component. The task component provides instructions for refining the original prompt. Specifically, it guides the LLM to revise prompts in a comma-separated format, detailing the characteristics of the target object to improve clarity and interpretability for guidance models. For short and under-described prompts, the revision should include additional details and elaborations to provide richer context. However, if the original prompt is lengthy with complex descriptions, it should simplify the description while preserving the core information. Overall, the revised prompt should address any vagueness and logical inconsistencies. In addition, to emphasize the target object for text-to-3D generation, refined prompts should omit background descriptions.\nTo generate physics-grounded motion in 3D Gaussians, we incorporate continuum mechanics into our framework. Inspired by the work of [52], we employ a deformation map $\\phi(X,t)$ to describe the motion of a particle's position $x_i$ at the time $t$. Local transformations, such as rotation and stretch at any position, are defined using the deformation map gradient as $F(X,t) = \\nabla x \\phi(X,t)$. This can be decomposed as $F = F_E F_P$, where $F_E$ is the elastic part and $F_P$ is the plastic part. To be grounded in continuum mechanics, the updates of the deformation map $\\phi$ conform to the principles of mass and momentum conservation[44].\nUnder the mass conservation principle, the material mass should remain constant over time, regardless of how the region deforms:\n$\\int_R \\rho(x,t) d\\Omega = \\int_{R_0} \\rho(\\phi^{-1}(x,t),0) d\\Omega$,\nwhere $\\rho(x,t)$ denotes the the material density field and $R = \\phi(R_0,t)$ is the region within the undeformed material space. According to the conservation of momentum principle, the momentum of any material region should remain unchanged before and after the deformation:\n$\\rho(x,t)v(x,t) = \\nabla \\cdot \\tau(x,t) + f^{ext}$,\nwhere $\\tau = det(F)K(F_E)(F_E)^T$ with the Kirchhoff stress tensor $K = \\frac{\\partial \\Psi}{\\partial F}$ with a strain energy density $\\Psi(F)$, and $K$ depends on the materials' elasticity models. The term $\\nabla \\cdot \\tau(x,t)$ represents the internal forces within the material, while $f^{ext}$ denotes the external force.\nTo obtain the deformation map $\\phi(X,t)$ while satisfying the mass and momentum conservation, we leverage the MPM [44]. This transforms the continuum into discrete Lagrangian particles carrying quantities such as velocity $v_i$, deformation gradient $F_i$, and position $x_i$. To achieve the two-way transfer of information between these Lagrangian particles and Eulerian grids, we utilize B-spline kernels with $C^1$ degree of continuity [44]. During the time step $t^n$ to $t^{n+1}$, the mass of Lagrangian particles remains constant under the"}, {"title": "4. Experiments", "content": "mass conservation. With momentum conservation, the momentum of the particles also remains unchanged:\n$m^i \\frac{v_i^{n+1} - v_i^n}{\\Delta t} = - \\sum_{j} \\frac{\\partial \\Psi}{\\partial F_i^n} \\nabla \\beta_i^n V + f^{ext}$,\nwhere $i$ denotes the Lagrangian particles and $j$ represents the Eulerian grid; $m = \\rho V$ is mass; $\\beta$ is the B-spline kernel function; $V$ is volume. To update the Lagargian particles' positions, the updated Eulerian grid velocity $v_j^{n+1}$ is transferred onto $v_i^{n+1}$, then the particles' positions are updated as $x_i^{n+1} = x_i^n + \\Delta t v_i^{n+1}$. The elastic deformation gradients of the particles are updated as $F_E^{n+1} = (I + \\Delta t \\sum_j v_j^{n+1} \\nabla \\beta_i^n) F F_i^n$. Depending on the material-specific plasticity model, $F_E^{n+1}$ is adjusted by a mapping as $M: F_E^{n+1} \\rightarrow F_E^{n+1}$. Please refer to the supplementary document for detailed information on the plasticity mapping functions.\nTo apply the deformation map $\\phi(X,t)$ to generate physics-grounded motion, we employ 3D Gaussians to represent the discrete particles. Under the assumption that particles undergo local affine transformations, which ensures the deformed Gaussian kernel remains Gaussian in the world space, the deformation map is approximated with the first-order Taylor expansion as $\\phi_i(X,t) = x_i + F_i(X - X_i)$. The deformed Gaussian kernel then becomes:\n$G_i(x,t) = e^{-(\\phi^{-1}(x,t) - X_i)^T \\Sigma_i^{-1}(\\phi^{-1}(x,t)-X_i)} = e^{-(x-x_i)^T (F_i \\Sigma_i F_i^T)^{-1}(x-x_i)}$\nGiven the 3D Gaussians with {$X_i, \\Sigma_i, \\alpha_i, c_i$}, the deformation map $\\phi(X,t)$ deforms them to {$x_i(t), \\sigma_i(t), \\alpha_i, c_i$}, where $x_i(t) = \\phi(X_i,t)$ and $\\sigma_i(t) = F_i(t)\\Sigma_i F_i(t)^T$.\nTo ensure consistent and accurate RGB color values during rendering, we regularize the RGB values converted from spherical harmonics $c_i$ through a color regularization process that includes the normalization and clamping steps. Normalization. The normalization step rescales the RGB values to fit within the [0, 1] range. This is achieved by subtracting the minimum value of the original RGB tensor and dividing by the difference between the maximum and minimum values, as follows:\n$s_{norm} = \\frac{s - s_{min}}{s_{max} - s_{min}}$\nwhere $s$ represents the original RGB tensor, and $s_{min}, s_{max}$ are the minimum and maximum values in $s$, and $s_{norm}$ is the normalized RGB tensor.\nClamping. While the normalization step adjusts the RGB values to [0, 1], numerical precision issues can sometimes cause minor deviations, leading to values outside this range. For instance, these issues might arise when rounding errors accumulate to allow some values to exceed the [0, 1]"}, {"title": "4.1. Implementation Details", "content": "In this section, we evaluate the effectiveness of our proposed framework through comprehensive experiments. We provide both qualitative and quantitative evaluations, along with our detailed ablation study results.\nSetup. We utilize Pytorch to implement the 3D Gaussian Splatting, adhering to the optimization pipeline from [21]. To generate physics-grounded motion, we build upon the MPM [44, 52]. Our experiments are conducted on an Nvidia RTX 3090 GPU.\nMetrics. We utilize the LAION aesthetic score [17], which evaluates the aesthetic quality of a video on a scale from 0 to 10. Furthermore, we employ the CLIP score [39] to measure the prompt consistency of a video, which is the average cosine similarity between input prompt and all video frames. In addition, we adopt the Mean Opinion Score (MOS) for the human study evaluations on the generated videos.\nMethod comparison. As the first approach to utilize text for generating 3D objects with physics-grounded motion, there is no existing method for direct comparison. Due to the generative nature of our framework, the ground truth of the deformed scenes is also unavailable. To evaluate our method, we provide both qualitative and quantitative results of our framework and compare them against the results of the relevant 3D-to-motion methods [15, 38], using the 3D models provided by our approach."}, {"title": "4.2. Qualitative Evaluation", "content": "range, and when normalization does not precisely yield values within [0, 1]. To address this, a clamping step is applied to enforce strict adherence to [0, 1]. The clamping operation is defined as:\n$s_{clamp} = min(max(s_{norm}, 0), 1)$.\nWith the RGB values regularized, a GS rasterizer is used to render the deformed Gaussian kernels. This process produces high-quality 3D objects with realistic physics-grounded motion."}, {"title": "4.3. Quantitative Evaluation", "content": "shape remains unchanged despite undergoing deformation. Metal dynamics cause the object to undergo permanent deformation when its stress reaches a certain threshold, governed by von Mises plasticity model [43]. Fracture dynamics simulate particle separation into clusters under significant deformation. Sand dynamics, modeled by Druker-Prager plasticity model [22], captures granular level frictional effects among particles. As illustrated in Figure 5, our results demonstrate the capability of our framework to synthesize 3D objects with high-quality appearances, accurate shapes, and realistic physics-grounded motion for the given text prompts and material types."}, {"title": "4.4. Ablation Studies", "content": "We showcase the qualitative performance of our framework by creating high-quality 3D objects featuring diverse physics-based dynamics. For each material dynamics type, we present an example illustrating the deformation motion of a 3D object, as shown in Figure 5. Please review the supplementary document for the material-specific elasticity and plasticity dynamics models.\nOur framework demonstrates the following dynamics: elastic, fracture, jelly, metal, and sand. Elastic and jelly dynamics refer to the behavior of objects where the rest"}, {"title": "5. Discussion", "content": "Limitation. Our framework does not currently support rendering the interaction of 3D object surfaces with light, so it"}, {"title": "6. Theoretical Details", "content": "Ablation studies are conducted on our proposed framework to demonstrate the necessity of 1) LLM-prompt refinement, 2) 3D diffusion prior guidance, 3) RGB color regularization. The presented ablation results demonstrate the contribution of each of these components in improving the overall performance of our system. From Table 3, we observe that each proposed component plays an important role in ensuring both high aesthetic quality and strong prompt-video consistency. Overall, it shows that 3D diffusion prior guidance produces a larger impact on the LAION score, which indicates its importance in obtaining a high aesthetic quality. On the other hand, LLM-prompt refinement has a greater effect on the CLIP score, reflecting its ability to achieve semantically aligned generation results.\nLLM-prompt refinement. Figure 7 illustrates that without LLM-prompt refinement (LLM-PR), the generated object suffers from inaccuracies in object details. For instance, in the result generated without LLM-PR, the coloration of the salmon is inconsistent with its natural appearance, and the rice grains are inaccurately positioned on top of the salmon,"}, {"title": "6.1. The Material Point Method", "content": "cannot generate the effects such as reflections or shadows. Additionally, our framework only supports the motion simulation of limited material types. Future work could explore integrating advanced relighting techniques and expanding the range of material types to enhance the framework's versatility and realism.\nConclusion. In this paper, we present an innovative framework for text-to-3D motion generation based on physics, facilitating the creation of high-quality 3D objects with realistic, physics-aware movements, effectively integrating generative modeling with physics-driven motion simulation. Our framework integrates four innovative components: 1) LLM-prompt refinement to ensure the accurate 3D generation for the prompt; 2) diffusion prior guidance for steering the generative process toward the result with accurate shape and high-quality visual appearance; 3) continuum mechanics-based deformation mapping to to model realistic physical interactions and deformations of the generated 3D object; 4) color regularization for consistent and accurate color rendering. This unified pipeline integrates natural language processing, generative modeling, and physics simulation to redefine the boundaries of 3D content creation, which paves the way for transformative applications across diverse industries such as filmmaking, virtual/augmented reality, gaming, and beyond.\nThe MPM is a computational physics method to simulate the material behaviors under different physical forces and deformations [44]. The MPM discretizes a material body into a collection of Lagrangian particles, and each particle has a set of quantities such as position $x_i$, mass $m_i$, velocity $v_i$, Kirchhoff stress tensor $K$, deformation gradient $F$, and affine momentum $A$ on particle i at time $t^n$. At time $t^n$, let $x_j$, $m_j$, and $v_j$ represent the position, mass, and velocity on grid node $j$. These grid nodes facilitate the calculation of the deformations and the applied forces on the material body. Due to the conservation of mass, particle mass is invariant. At each time step, the MPM conducts a two-way transfer: 1) Particle-to-Grid; 2) Grid-to-Particle."}, {"title": "6.2. Physics Models", "content": "Particle-to-Grid Transfer. In this process, the mass and particle momentum are transferred to the grids [52]. The mass $m^n_j$ at a grid node $j$ is computed as:\n$m^n_j = \\sum_i w_i^n m_i$,\nwhere $w_i^n$ is the interpolation weight obtained from a B-spline kernel. Using the APIC momentum transfer method [18], the momentum at the grid node $j$ is updated as:\n$(m^n v^n)^j = \\sum_i w^n_i(m_i v^n_i + A_i^n (x_j - x_i)^n)$.\nBased on the particles' internal and external forces, the grid velocity $v_j^{n+1}$ at the next time step is updated as:\n$v_j^{n+1} = v_j^n - \\frac{\\Delta t}{m^n_j} \\sum_i K\\nabla w_i^n V + \\Delta t g$,\nwhere $g$ is the gravity acceleration.\nGrid-to-Particle Transfer. In this stage, the grid nodes' updated velocities and momentum are transferred back to the particles [19, 52]. The velocity $v_i^{n+1}$, position $x_i^{n+1}$, affine momentum $A_i^{n+1}$, and deformation gradient $F_i^{n+1}$ of particle i at the new time step are updated as:\n$v_i^{n+1} = \\sum_j v_j^{n+1} w_j^i$,\n$x_i^{n+1} = x_i^n + \\Delta t v_i^{n+1}$,\n$A_i^{n+1} = \\frac{1}{12} \\sum_j \\frac{m_j}{m_j +1}(x_j^n - x_i^n)^T \\otimes (x_j^n - x_i^n)$,\n$\\nabla v_i^{n+1} = \\sum_j v_j^{n+1} (\\nabla w_i^n)^T$,\n$F_i^{n+1} = M((I + \\nabla v_i^{n+1}) F_i^n)$.", "content_notes": "The equation `$G_i(x,t) = e^{-(x-x_i)^T (F_i \\Sigma_i F_i^T)^{-1}(x-x_i)}$` was missed when extracting equation environment from the document. Please add this expression to the document."}, {"title": "6.3. Score Distillation Sampling", "content": "In this section, we provide the physics model details for the paper, and we show the relevant material parameters in Table 4. The employed physics models are adopted from [52, 63]. For the given material, the Kirchhoff stress tensor K is mapped by the material's corresponding elasticity model, and the deformation gradient $F_E$ is mapped by the material's specific plasticity model.\nFixed Corotated Elasticity. The Fixed Corotated Elasticity model describes the behaviors of materials that undergo deformations with rotations and small elastic strains [18]:\n$K = 2 \\mu (F_E - R) (F_E)^T + \\frac{\\lambda}{1}(J - 1)J$,\nwhere $R = UV^T$ and $F_E = U\\Sigma V^T$, and J is the determinant of $F_E$.\nSt. Venant-Kirchhoff Elasticity. St. Venant-Kirchhoff models materials that return to their original shapes after large deformations [22]:\n$K = U (2 \\mu \\epsilon + \\lambda sum(\\epsilon)1)V^T$,\nwhere $\\epsilon = log(\\Sigma)$ and $F_E = U \\Sigma V^T$.\nDrucker-Prager Plasticity. Drucker-Prager Plasticity describes the behaviors of the materials that do not exhibit purely ductile behavior [22]:\n$F_E = U M(\\Sigma) V^T$,\n$M(\\Sigma) = \\begin{cases} 1 & sum(\\epsilon) > 0 \\\\ exp(-\\frac{\\delta \\gamma}{K_Y}) & \\delta y \\leq 0 \\text{ and } sum(\\epsilon) \\leq 0  \\\\ 0 & otherwise. \\end{cases}$", "content_notes": "The equation `$\\int_R \\rho(x,t) d\\Omega = \\int_{R_0} \\rho(\\phi^{-1}(x,t),0) d\\Omega$` was missed when extracting equation environment from the document. Please add this expression to the document. Additionally, equation `$F_i^{n+1} = \\sum_j v_j^{n+1} (\\nabla w_i^n)^T$` was missed when extracting equation environment from the document. Please add this expression to the document."}, {"title": "7. More Results", "content": "Here, M is the deformation adjustment mapping, $\\delta \\gamma = ||\\epsilon||_F - \\frac{K_Y}{2\\mu}$, and $K_Y$ is the yield stress.\nvon Mises Plasticity. von Mises Plasticity models the materials that will permanently deform when the stress reaches a certain threshold value [43]:\n$F_E =UM(\\Sigma)V^T$,\n$M(\\Sigma) = \\begin{cases} 1 & \\delta y \\leq 0  \\\\ (exp(-\\frac{\\delta \\gamma}{\\epsilon})) & otherwise, \\end{cases}$\nwhere $\\delta y = ||\\epsilon||_F - \\frac{K_Y}{2\\mu}$, and $KY$ is the yield stress.\nScore Distillation Sampling is a technique proposed in DreamFusion [37] that utilizes the 2D diffusion prior to optimize an image generator based on the probability density distillation. To achieve this, an image generator parameterized by parameters $\\theta$ is represented as g($\\theta$). To optimize over parameters $\\theta$ such that the generated image x = g($\\theta$) resembles a sampling from the pre-trained frozen 2D diffusion model, the SDS loss gradient for optimizing $\\theta$ is formulated as:\n$\\nabla_\\theta L_{SDS}(y, x = g(\\theta)) = E_{t, \\epsilon} [w(t) (\\epsilon - \\epsilon_\\theta(x_t; y, t)) \\frac{\\partial x}{\\partial \\theta}]$,\nwhere $\\epsilon_\\theta(x_t; y, t)$ is the predicted noise by the pre-trained 2D diffusion model with the text prompt y at the time step t, and $\\epsilon$ is the true noise at the time step. $\\frac{\\partial x}{\\partial \\theta}$ is the derivative of the image generator's generated image with respect to its parameters $\\theta$, and w(t) is a weighting function from DDPM [13]. This loss function aligns the scores (or gradients) of the image generator and the 2D diffusion model by optimizing the loss gradients with respect to $\\theta$, which can enable the use of the 2D diffusion prior to guide the generation of 3D models efficiently."}, {"title": "7.1. More Text-to-3D Physics Motion Results", "content": "We present additional text-to-3D physics-grounded motion results in Figure 10. These results demonstrate that our framework effectively generates 3D objects with high-quality appearances, accurate shapes, and realistic physics-driven motion for the given text prompts and material types. We also include the generated videos in the supplementary materials, please refer to the MP4 files in the videos folder or view them through the videos.html file."}, {"title": "7.2. More Qualitative Comparisons", "content": "moves to the same direction while also having light reflection. The results also show a good surface detail on the object in addition to a good visual appeal."}, {"title": "7.3. More Quantitative Comparisons", "content": "The additional results of the compared methods, DreamPhysics [15] and Feature-Splatting [38], are presented in Figure 10 and Figure 11, respectively. Compared to our findings, we notice that DreamPhysics generates undersaturated colors, displaying muted and less convincing"}, {"title": "8. More Experiments", "content": "The additional quantitative comparison results are shown in Table 5 for the object videos in Figure 10. The table shows that our framework achieves a mean LAION score of 3.88, outperforming the other methods. This indicates that our framework produces videos with higher visual quality compared to the other methods."}, {"title": "8.1. LLM-Prompt Refinement", "content": "Figure 12 demonstrates that the absence of LLM-Prompt Refinement (LLM-PR) can lead to uneven and oversaturated colors, as well as the lack of shadows, fine textures, and intricate details in the flower petals."}, {"title": "8.2. 3D Diffusion Prior Guidance", "content": "The results in Figure 13 illustrate that incorporating the 3D diffusion prior as guidance (3D Guidance) significantly improves the Gaussian Splatting process, enabling it to produce 3D objects with more precise geometrical shapes compared to those generated without this guidance."}, {"title": "8.3. Color Regularization", "content": "Our results in Figure 14 indicate that the absence of the color regularization (CR) on RGB values results in inaccurately rendered colors in the generated 3D object, as opposed to the more accurate results achieved with color regularization."}, {"title": "6.4 Supplementary video", "content": "Please see more supplementary video with different materials in videos.html and videos directory."}]}