{"title": "Reward-Guided Controlled Generation for Inference-Time Alignment in Diffusion Models: Tutorial and Review", "authors": ["Masatoshi Uehara", "Yulai Zhao", "Chenyu Wang", "Xiner Li", "Aviv Regev", "Sergey Levine", "Tommaso Biancalani"], "abstract": "This tutorial provides an in-depth guide on inference-time guidance and alignment methods for optimizing downstream reward functions in diffusion models. While diffusion models are renowned for their generative modeling capabilities, practical applications in fields such as biology often require sample generation that maximizes specific metrics (e.g., stability, affinity in proteins, closeness to target structures). In these scenarios, diffusion models can be adapted not only to generate realistic samples but also to explicitly maximize desired measures at inference time without fine-tuning. This tutorial explores the foundational aspects of such inference-time algorithms. We review these methods from a unified perspective, demonstrating that current techniques such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling, and classifier guidance\u2014aim to approximate soft optimal denoising processes (a.k.a. policies in RL) that combine pre-trained denoising processes with value functions serving as look-ahead functions that predict from intermediate states to terminal rewards. Within this framework, we present several novel algorithms not yet covered in the literature. Furthermore, we discuss (1) fine-tuning methods combined with inference-time techniques, (2) inference-time algorithms based on search algorithms such as Monte Carlo tree search, which have received limited attention in current research, and (3) connections between inference-time algorithms in language models and diffusion models. The code of this tutorial on protein design is available at https://github.com/masa-ue/AlignInversePro.", "sections": [{"title": "Introduction", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) have demonstrated remarkable success in computer vision, particularly as generative models for continuous domains such as images (Rombach et al., 2022). This success has been further extended to scientific areas such as the generation of protein 3D structures (Yim et al., 2023; Watson et al., 2023; Chu et al., 2024; Abramson et al., 2024) and small molecule 3D structures (Xu et al., 2022; Jing et al., 2022; Corso et al., 2022). Furthermore, recent works (Shi et al., 2024; Sahoo et al., 2024; Lou et al., 2023) have shown promising results with diffusion over traditional autoregressive models in discrete domains. Building on this progress in natural language processing (NLP), the use of diffusion models has also been explored for generating biological sequences (proteins, RNA, and DNA), which are inherently non-causal, because these sequences fold into complex tertiary (3D) structures (Campbell et al., 2024; Sarkar et al., 2024; Winnifrith et al., 2024; Wang et al., 2024).\nControlled generation is a pivotal topic in the study of diffusion models. In the context of \u201cfoundational models\u201d, the process typically begins with training conditional diffusion models on large datasets to generate natural designs (e.g., biologically plausible protein sequences) conditioned on fundamental functionalities. Following this pre-training stage, the focus often shifts to optimizing specific downstream reward functions, commonly referred to as \u201calignment\u201d problems in AI. By guiding generation to maximize a given reward during inference (e.g., binding affinity or stability in protein sequences), diffusion models can be effectively utilized as robust computational design frameworks. Similarly, conditioning on target properties during inference is treated as a reward maximization task, where rewards are frequently defined using classifiers.\nIn this tutorial, we aim to explore inference-time techniques for controlled generation in diffusion models, along with their foundational properties. These techniques aim to seamlessly integrate pre-trained generative models trained on large-scale datasets with reward models, as illustrated in Figure 1. Specifically, at each generation step in pre-trained diffusion models, certain modifications are introduced to optimize downstream reward functions as summarized in Figure 2. A significant advantage of these methods is that they don't require post-training of the diffusion models, which can be computationally demanding. The simplest such approach is best-of-N sampling in Figure 2a, which involves generating multiple designs (N samples) from a pre-trained diffusion model and selecting the best one based on reward functions (e.g., Nakano et al. (2021)). However, this method can be highly inefficient when the reward functions are difficult to optimize. More efficient sophisticated strategies include classifier guidance in Figure 2b and its variants (Dhariwal and Nichol, 2021; Song et al., 2021), sequential Monte Carlo-based methods in Figure 2c (Wu et al., 2024; Dou and Song, 2024; Cardoso et al., 2023; Phillips et al., 2024), and value-based sampling methods in Figure 2d (Li et al., 2024)."}, {"title": "Inference-Time Techniques vs. Post-Training", "content": "After pre-trainig, there are two main approaches for controlled generation: inference-time techniques (i.e., without fine-tuning diffusion models) and post-training methods such as RL-based fine-tuning (Black et al., 2023; Fan et al., 2023; Clark et al., 2023; Uehara et al., 2024) or classifier-free guidance-based fine-tuning (Ho and Salimans, 2022; Zhang et al., 2023). In this work, we focus on reviewing the former. For a comprehensive overview of the latter approach, we refer readers to Uehara et al. (2024). While both approaches are important, inference-time techniques generally offer several advantages:\n\u2022 Inference-time techniques are particularly straightforward to implement, as many of these methods are not only fine-tuning-free but also training-free, given access to reward functions. Despite their simplicity, they could deliver competitive performance compared to RL-based fine-tuning approaches.\n\u2022 Inference-time techniques can support post-training. For example, they can be employed as data augmentation methods within classifier-free guidance or as teacher policies in policy distillation-based post-training. Further details are provided in Section 9.4.\n\u2022 Even after obtaining fine-tuned models through post-training techniques, applying inference-time methods to fine-tune models can be advantageous for further improving the functionality of the generated outputs. This is particularly relevant when downstream reward feedback is highly accurate. Post-training may not fully exploit the information provided by the reward feedback, as it involves converting this feedback into data, a process that can result in information loss. In contrast, inference-time techniques can directly utilize reward feedback without the need for such conversion, enabling more effective optimization."}, {"title": "Critical Considerations for Choosing Inference-Time Techniques", "content": "In this article, we categorize current inference-time techniques according to the following features:\n1. Computational and Memory Efficiency: In general, even when utilizing the same algorithm, increased computational or memory resources during inference can result in higher-quality designs. Therefore, for fair comparisons, it is essential to evaluate the performance of generated samples within the same computational or memory budget at inference time. Additionally, the ease of parallel computation is an important practical consideration.\n2. What Rewards We Want to Optimize: It is relevant to consider whether the attributes we aim to optimize (referred to as reward models in this draft) function as classifiers, as seen in the standard guidance literature, or as regressors, as is common in the literature on alignment. In this draft, the former task is often called conditioning, while the latter is called alignment.\n3. Differentiability of Reward Feedback: In computer vision and NLP, many useful reward feedback is differentiable. However, in scientific domains such as molecular design, much of the useful reward feedback, such as physics-based simulations (Salomon-Ferrer et al., 2013; Chaudhury et al., 2010; Trott and Olson, 2010)), is non-differentiable. Additionally, when utilizing learned reward models as feedback, they are often non-differentiable due to their reliance on non-differentiable features such as molecular fingerprints or biophysical descriptors (Stanton and Jurs, 1990; Yap, 2011; Li et al., 2015)."}, {"title": "Summary", "content": "Considering these aspects, we provide a unified categorization of current inference-time techniques in diffusion models, while also highlighting novel perspectives. The key message of this tutorial is summarized as follows.\nAll methods introduced here (summarized in Figure 2) generally aim to approximately sample from specific target distributions. Denoting the reward function as $r : \\mathcal{X} \\rightarrow \\mathbb{R}$ (e.g., classifiers or regressors) and $p_{\\text{pre}}(\\cdot)$ as the distribution induced by policies (i.e., denoising processes) from the pre-trained model, the target distribution is defined as\n$p^{(\\alpha)}(.) := \\frac{\\exp(r(.)/\\alpha)p_{\\text{pre}}(.)}{\\int_{\\mathcal{X}} \\exp(r(x)/\\alpha)p_{\\text{pre}}(x)dx} = \\underset{p: \\mathcal{X} \\rightarrow \\Delta(\\mathcal{X})}{\\text{argmax}} \\mathbb{E}_{x \\sim p(\\cdot)}[r(x)] - \\alpha \\text{KL}(p(\\cdot)||p_{\\text{pre}}(\\cdot)).$\nwhere $C$ is the normalizing constant, and $\\alpha$ is a hyperparameter. This distribution is desirable because the generated outputs exhibit both naturalness and high functionality.\nTo enable sampling from this distribution, by denoting pre-trained denoising process as $\\{p^{\\text{pre}}_{t-1}(\\cdot|X_{t+1})\\}_{t=T}^{0}$ (from t = T to t = 0), all methods presented here (methods in Figure 2) employ the following distribution as the denoising process (i.e., optimal policies in RL) at each step during inference:\n$p_{t-1}^{\\alpha}(x_{t-1}):= p_{t-1}^{\\text{pre}}(x_t) \\times \\exp(v_{t-1}(\\cdot)/\\alpha),$\nwhere soft value functions act as look-ahead functions that predict the reward at the terminal state $x_{0}$ from intermediate state $x_t$ (formalized later in Section 2). The primary distinction among inference-time algorithms lies in how this approximation is achieved, and the effectiveness of each method depends on the specific scenario.\nAdditionally, we explore more advanced aspects of inference-time methods in diffusion models, including their integration with fine-tuning, search algorithms, editing, and applications to masked language models beyond diffusion frameworks. The remainder of this tutorial is organized as follows.\n\u2022 Section 2: We start by outlining the foundational principles of inference-time techniques. Specifically, we introduce the soft optimal policy defined in (1), which represents the denoising process targeted during inference. All methods discussed in this tutorial aim to approximate this optimal policy.\n\u2022 Section 3: We review inference-time techniques that do not require differentiable reward feedback, particularly useful in molecular design. These methods are roughly divided into two main categories: the SMC-based approach (Wu et al., 2024; Dou and Song, 2024; Cardoso et al., 2023; Phillips et al., 2024) and the value-based importance sampling approach (Li et al., 2024). Additionally, we explain how to integrate these two approaches."}, {"title": "Derivative-Free Guidance", "content": "We begin by outlining two primary derivative-free approaches that do not rely on differentiable models. As discussed in the introduction, constructing differentiable models in molecular design can be challenging due to the non-differentiable nature of reward feedback for the following reasons.\nTherefore, these derivative-free methods are particularly useful in such scenarios. In this section, we first introduce sequential Monte Carlo (SMC)-based guidance, followed by value-based importance sampling approach."}, {"title": "SMC-Based Guidance", "content": "We first provide an intuitive explanation of SMC-based guidance proposed in Wu et al. (2024); Dou and Song (2024); Cardoso et al. (2023); Phillips et al. (2024), which combine SMC (a.k.a. particle filter) (Gordon et al., 1993; Kitagawa, 1993; Del Moral et al., 2006) with diffusion models. While there are several variants, we focus here on the simplest version. Since SMC-based guidance is an iterative method, let us consider the process at t. At this stage, we assume there are N samples (particles), $\\{x_t^{[i]}\\}_{i=1}^{N}$, each with uniform weights: $1/N \\sum_{i=1}^{N} \\delta_{x_t^{[i]}}$. Given this distribution, our goal is to sample from the optimal policy $p_{t-1}^\\alpha(\\cdot|\\cdot)$.\nFor this purpose, using a proposal distribution $q_{t-1}(x_{t-1}|x_t): \\mathcal{X} \\rightarrow \\triangle(\\mathcal{X})$, such as policies from pre-trained models (we will discuss this choice further in Section 3.4), we generate new samples $\\{x_{t-1}^{[j]}\\}_{j=1}^{N}$. Ideally, we aim to sample from the optimal policy in (6). To approximate this, based on importance sampling, we consider the following weighted empirical distribution:\n$\\sum_{i=1}^{N} \\frac{w_{t-1}^{[i]}}{\\sum_{j=1}^{N} w_{t-1}^{[j]}} \\delta_{x_{t-1}^{[j]}}, w_{t-1}= \\frac{p_{t-1}^\\alpha(x_{t-1}|x_t^{[i]})}{q_{t-1}(x_{t-1}|x_t^{[i]})} \\propto \\frac{p_{t-1}^{\\text{pre}}(x_{t-1}|x_t^{[i]}) \\exp(v_{t-1}(x_{t-1})/\\alpha)}{q_{t-1}(x_{t-1}|x_t^{[i]}) \\exp(v_{t}(x_t)/\\alpha)}$\nHowever, as the weights become increasingly non-uniform, the approximation quality deteriorates. To mitigate this, SMC performs resampling with replacement, generating an equally weighted Dirac delta distribution:\n$\\frac{1}{N} \\sum_{i=1}^{N} \\delta_{x_{t-1}^{[\\xi_i]}}, \\xi_i \\sim Cat(\\frac{w_{t-1}^{[j]}}{\\sum_{j=1}^{N} w_{t-1}^{[j]}})_{i=1}^{N}$"}, {"title": "Value-Based Importance Sampling", "content": "Next, we explain a simple value-based importance sampling approach called SVDD, proposed by Li et al. (2024). For this purpose, we first provide an intuitive overview. This method is iterative in nature. Hence, suppose we are at time t and we have N samples, $\\{x_t^{[i]}\\}_{i=1}^{N}$, with uniform weights:\n$1/N \\sum_{i=1}^{N} \\delta_{x_t}$. Following a proposal distribution $q_{t-1}(\\cdot|x_t^{[i]})$ (e.g., pre-trained models), we generate M samples $\\{x_{t-1}^{[i,j]}\\}_{j=1}^{M}$ for each $x_t^{[i]}$. Ideally, we want to sample from the optimal policy (6). To approximate this, based on importance sampling, we consider the following weighted empirical distribution:\n$\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\frac{w_{t-1}^{[i,j]}}{\\sum_{j=1}^{M} w_{t-1}^{[i,j]}} \\delta_{x_{t-1}^{[i,j]}}, w_{t-1}= \\frac{p_{t-1}^\\alpha(x_{t-1}|x_t^{[i,j]})}{q_{t-1}(x_{t-1}|x_t^{[i]})} \\propto \\frac{p_{t-1}^{\\text{pre}}(x_{t-1}|x_t^{[i,j]}) \\exp(v_{t-1}(x_{t-1})/\\alpha)}{q_{t-1}(x_{t-1}|x_t^{[i]}) \\exp(v_{t}(x_t)/\\alpha)}$\nSince $\\exp(v_{t-1}(x)/\\alpha)$ in the above remains constant for all $j \\in [1, ..., M]$, the weight simplifies to\n$\\frac{p_{t-1}^{\\text{pre}}(x_{t-1}|x_t^{[i,j]}) \\exp(v_{t-1}(x_{t-1})/\\alpha)}{q_{t-1}(x_{t-1}|x_t^{[i]}) \\delta_4^4}$\nHowever, repeatedly using this empirical distribution increases the particle size to $O(N^M)$, making it computationally prohibitive. Therefore, we sample to maintain a fixed batch size:\n$ \\frac{1}{N} \\sum_{i=1}^{N} \\delta_{x_{t-1}^{[\\xi_i]}}, \\xi_i \\sim Cat(\\frac{w_{t-1}^{[k]}}{\\sum_{k=1}^{M} w_{t-1}^{[i,k]}})_{j=1}^{M}$"}, {"title": "Nested-SMC-Based Guidance", "content": "As discussed above, the SMC-based approach and the value-based importance sampling approach each have distinct advantages. A hybrid method, referred to as nested-SMC (or nested-IS) in computational statistics (Naesseth et al., 2019, Algorithm 5), may combine the strengths of both approaches. This method is outlined in Algorithm 6. Each step involves two processes: the first is local sampling, resembling value-based importance sampling, and the second is global resampling, characteristic of SMC-based guidance. By incorporating these two elements, nested-IS-based guidance can be more effectively tailored for optimization, allowing the elimination of suboptimal samples through global resampling."}, {"title": "Beam Search for Reward Maximization", "content": "Now, consider a special case of reward maximization where it is natural to set $\\alpha = 0$. In this scenario, the SVDD algorithm (Algorithm 5) simplifies to Algorithm 7, where the selected index corresponds to the one that maximizes the soft value functions.\nThis algorithm can also be viewed as a beam search guided by soft value functions. Specifically, multiple nodes are expanded according to the proposal distributions, and the best node is selected based on its value function. While this process may seem greedy, it is not, as soft value functions theoretically serve as look-ahead mechanisms, predicting future rewards from intermediate states.\nHowever, the theoretical foundation of this approach relies on the assumption of perfect soft value function access. In practice, approximation errors may arise, and in certain cases, a deeper search might yield additional benefits. We will later explore deeper search algorithms, such as Monte Carlo Tree Search (MCTS), in Section 6."}, {"title": "Selecting Proposal Distributions", "content": "Selecting the appropriate proposal distributions is an important decision for the methods introduced so far from Algorithm 4 to Algorithm 7. We outline three fundamental options below.\nPre-Trained Diffusion Polices. The simplest option is to use the policy from the pre-trained diffusion model."}, {"title": "Derivative-Based Guidance in Continuous Diffusion Models", "content": "We have introduced the derivative-free inference-time technique. In this section, we focus on classifier guidance (Dhariwal and Nichol, 2021; Song et al., 2021), a standard derivative-based method in continuous diffusion models. We first provide the intuition underlying the algorithm's derivation, followed by its formalization within a continuous-time framework. Finally, we propose an algorithm designed for Riemannian diffusion models, which are extensively used in protein strcuture generation.\nIn this subsection, we derive classifier guidance as a Gaussian policy that approximates the soft-optimal policy.\nFirst, recalling the form of pre-trained policies in diffusion models over Euclidean space, specifically (3) with score parametrization, let us denote the pre-trained model to be\n$p_{t-1}^{\\text{pre}}(x_{t-1}|x_t) = \\mathcal{N}(p^{\\text{pre}}(x_t, t); \\sigma_t^2I), p^{\\text{pre}}(x_t, t) := x_t + (\\delta t)g(x_t, t)$\n$\\mu(x_t, t) := 0.5x_t + \\nabla_{x_t} \\log p(x_t; \\theta_{\\text{pre}}), \\sigma_t^2 = (\\delta t).$\nHere, $(\\delta t)$ is assumed to be small. Substituting this expression into the optimal policy form in (8) yields\n$\\exp(\\frac{v_{t-1}(x_{t-1}) + \\frac{\\Vert x_{t-1} - p^{\\text{pre}}(x_t, t)\\Vert^2}{2\\sigma_t^2}}{\\alpha})$\nup to normalizing constant. However, directly sampling from this policy is challenging; therefore, we consider approximating it.\nA natural approach is to approximate this policy with a Gaussian distribution. To achieve this, we apply a Taylor expansion:\n$\\exp(\\frac{(v_t(x_t) + \\nabla v_t(x_t) \\cdot (x_{t-1} - x_t) + O(\\Vert x_{t-1} - x_t\\Vert^2)}{\\alpha}) \\times \\exp(\\frac{\\Vert x_{t-1} - x_t - (\\delta t)g(x_t, t)\\Vert^2}{2\\sigma_t^2}}).$\nSince $\\sigma_t^2$ is much smaller than $\\alpha$ (as $\\sigma_t^2$ scales with $(\\delta t)$), we can ignore the term $O(\\Vert x_{t-1} - x_t\\Vert^2/\\alpha)$. Therefore, the expression simplifies to\n$\\exp(\\frac{-\\Vert x_{t-1} - x_t - \\frac{\\sigma_t^2 \\nabla v_t(x_t)}{\\alpha} \\Vert^2}{2\\sigma_t^2}); \\mu(x_t,t) = p^{\\text{pre}}(x_t, t) + \\frac{\\sigma_t^2 \\nabla v_t(x_t)}{\\alpha}$"}, {"title": "Derivative-Free Guidance Versus Classifier Guidance", "content": "The critical assumption in classifier guidance is that accurate differentiable value function models can be constructed with respect to the inputs. A straightforward scenario for building such models is an inpainting task, where classifier guidance performs effectively. However, in many molecular design tasks, this assumption may not hold, as discussed in the introduction and Section 3. In such cases, derivative-free guidance becomes advantageous, as these methods do not require differentiable rewards or value function models.\nIt is also worthwhile to note these two approaches (derivative-free guidance and classifier-guidance) can still be combined by employing classifier guidance as a proposal distribution in derivative-free methods, potentially with different value functions. Specifically, even if the differentiable models used in classifier guidance are not fully accurate, they can serve as proposal distributions while SMC-based guidance or value-based sampling leverages more precise non-differentiable value function models, as we discuss in Section 3.4."}, {"title": "Continuous-Time Formalization via Doob transform", "content": "In this section, we formalize the intuitive derivation of classifier guidance in Section 4.1. For this purpose, we explain the continuous-time formulation of diffusion models first."}, {"title": "Preparation", "content": "We begin by outlining the fundamentals of diffusion models within the continuous-time framework. For further details, refer to Uehara et al. (2024, Section 1.1.1) or many other reviews (Tang and Zhao, 2024). The training process can be summarized as follows: (1) defining the forward SDE and (2) learning the time-reversal SDE by estimating the score functions."}, {"title": "Forward and Time-Reversal SDE", "content": "We first introduce a forward stochastic differential equation (SDE) from $t \\in [0,T]$. A widely used example is the variance-preserving (VP) process:\n$t \\in [0,T]; dx_t = -0.5x_tdt + dw_t, x_0 \\sim p^{\\text{pre}}(x),$\nwhere $dw_t$ denotes standard Brownian motion. Two key observations are:\n\u2022 As T approaches \u221e, the limiting distribution converges to $\\mathcal{N}(0, I)$.\n\u2022 The time-reversal SDE (Anderson, 1982), which preserves the marginal distribution, is given by:\n$t \\in [0, T]; dz_t = [0.5z_t + \\nabla \\log q_{T-t}(z_t)]dt + dw_t.$\nHere, $q_t \\in \\Delta(\\mathbb{R}^d)$ denotes the marginal distribution at time t induced by the forward SDE (12). Notably, the marginal distribution of $z_{T-t}$ is the same as that of $x_t$ induced by the forward SDE. Note the notation t is reversed relative to the forward SDE in (12).\nThese observations suggest that with sufficiently large T, starting from $\\mathcal{N}(0, I)$ and following the time-reversal SDE (13), we can sample from the data distribution (i.e., $p^{\\text{pre}}$) at terminal time T. A key challenge remains in learning the score function $\\nabla \\log q_{T-t}(z_t)$. In diffusion models, the primary objective is to estimate this score function. For such training methods, refer to Example 1. Our work assumes the availability of a pre-trained model $s(z_t, T \u2013 t; \\theta_{\\text{pre}})$, that predicts $\\nabla \\log q_{T-t}(z_t)$, fixed after pre-training."}, {"title": "Doob Transform", "content": "We now proceed to derive classifier guidance more rigorously. Consider a pre-trained model represented by\n$t \\in [0, T]; dz_t = [0.5z_t + s(z_t, T \u2013 t; \\theta_{\\text{pre}})]dt + dw_t, z_0 \\sim \\delta_{z_{\\text{ini}}}.$\nWe denote the resulting distribution as $p^{\\text{pre}}$. The following theorem is instrumental in deriving classifier guidance.\nTheorem 2 (Doob Transform). For a value function: $v_t(\u00b7) = \\log \\mathbb{E}_{\\theta^{\\text{pre}}}[\\exp(r(z_t))|z_t = \u00b7]$\nwhere the expectation $\\mathbb{E}_{\\theta^{\\text{pre}}}[\u00b7]$ is induced by the pre-trained model (14), the distribution induced by the SDE:\n$t \\in [0, T]; dz_t = [0.5z_t + \\{s(z_t, T \u2013 t; \\theta_{\\text{pre}}) + \\nabla \\log v_t(z_t)\\}]dt + dw_t$\nis a target distribution, proportional to $\\exp(r(x))p^{\\text{pre}}(x)$.\nThis theorem implies that, with standard Euler-Maruyama discretization, classifier guidance can be formally derived as in Algorithm 8 (when \u03b1 = 1) such that we can sample from the target distribution $\\exp(r(x))p^{\\text{pre}}(x)$. Here, we remark that $v_t(\u00b7) = \\log \\mathbb{E}_{\\theta^{\\text{pre}}}[\\exp(r(z_t))|z_t = \u00b7]$ in the theorem serves as the continuous-time analogue of the value function, which we introduce in (6).\nThe Doob transform is a celebrated result in stochastic processes (e.g., Chetrite and Touchette (2015, Chapter 3)). The connection between classifier guidance and the Doob transform has been"}, {"title": "Guidance in Riemannian Diffusion Models", "content": "We now extend the discussion from Euclidean spaces to Riemannian manifolds (De Bortoli et al., 2022; Yim et al., 2023; Chen and Lipman, 2023). To do so, we first provide a concise introduction to Riemannian manifolds, focusing specifically on the special orthogonal group (SO(3)). This is because SE(3) (i.e., SO(3) \u2297 R\u00b3) is commonly employed in protein conformation generation to efficiently model the 3D coordinates of the protein backbone (Yim et al., 2023; Watson et al., 2023). Subsequently, we describe the corresponding classifier guidance method. For a more detailed introduction to Riemannian manifolds, we refer readers to (Lee, 2018)."}, {"title": "Primer: Riemannian Manifolds", "content": "We denote a d-dimensional Riemannian submanifold embedded in Rm by M. The manifold is a space that locally resembles Euclidean space, and is formally characterized by a local coordinate chart $\u03c6 : \\mathcal{M} \\rightarrow \\mathbb{R}^d$ and its inverse $\u03c8$. A key concept in a manifold is the tangent space, which represents the space of possible velocities for a particle moving along the manifold. For each point $x \u2208 \\mathcal{M}$, the tangent space $T_x\\mathcal{M}$ is formally defined as the space spanned by the column vectors of the Jacobian $d\u03c8/dx\\vert_{x=\u03c6(x)}$. A Riemannian manifold is then defined as a manifold equipped with a specific metric $g : T_x\\mathcal{M} \u00d7 T_x\\mathcal{M} \\rightarrow \\mathbb{R}$, often denoted by $\\langle \u00b7, \u00b7\\rangle_g$. An important example in protein design is the well-known SO(3) group."}, {"title": "Classifier Guidance in Riemannian Diffusion Models", "content": "Now, with the above preparation, the pre-trained model is defined as an SDE on a manifold $M$:\n$t \u2208 [0;T]; dx_t = s(x_t, T - t; \\theta_{\\text{pre}})dt + dw^\\mathcal{M},$\nwhere $dw^\\mathcal{M}$ denotes Brownian motion on a Riemannian manifold M. The discretization is given by:\n$x_{t\u22121} = exp_{x_t}[v_{\\text{elt}}], v_{\\text{elt}} = (dt)s(x_t, T \u2013 t; \\theta_{\\text{pre}}) + \\sqrt{(dt)}\u03b5_t, \u03b5_t \u223c N(0, I_d)$,\nwhere $\u03b5_t$ is a normal distribution on the manifold M. Each step thus consists of two operations: (1) calculating the tangent (velocity) $v_{\\text{elt}}$ in the tangent space at $x_t$ and (2) moving along the geodesic induced by the velocity $v_{\\text{elt}}$, starting at $x_t$. In the Euclidean case, the second step reduces to $x_t + v_{\\text{elt}}$."}, {"title": "Derivative-Based Guidance in Discrete Diffusion Models", "content": "We now focus on inference-time techniques specifically designed for discrete diffusion models. In Section 2.1, we highlighted that exact sampling from the optimal policy is feasible within discrete diffusion models under certain limited scenarios. Here, we revisit this point by demonstrating that exact sampling from the optimal policy can be achieved through polynomial-time computation of value functions. Building on this point, we explain derivative-based guidance following the approach of Nisonoff et al. (2024). Finally, we formalize the discussion within a continuous-time framework (Wang et al., 2024)."}, {"title": "Exact Sampling in Discrete Diffusion Models", "content": "Our objective in this subsection is to show that sampling from the optimal policy can be achieved with polynomial-time computation of value functions in discrete diffusion models. As a preliminary step", "as": "n$\\prod_{l=1"}, {"p_{t-1}^{\\text{pre}}(x_{t-1}^{1": "L"}, "x_t^{1:L}), \\text{where} \\,  p_{t-1}^{\\text{pre}}(x_{t-1}^{1:L}|x_t^{1:L}) := \\mathbb{I}(x^l = x_{t-1}^l) + \\mathcal{Q}_{x_{t-1}^{1:L}|x_t^{1:L}}^{\\text{pre}}(\\cdot)(\\delta t),$ \nwhere $(\\delta t)$ is a discretization step. For example, in maskd diffusion models mentioned (Example 2),\n$\\mathcal{Q}_{x_{t-1}^{1:L}|x_t^{1:L}}^{\\text{pre}}(t) := \\begin{cases} \\frac{\\overline{a}_t - \\overline{a}_{t-1}}{1-\\overline{a}_t}\\tau_0(x_t;\\theta^{\\text{pre}}) ,& \\text{when}\\  x_{t-1}^{l} \\neq x^l, x^l = Mask \\\\ 1- \\mathcal{Q}_{x_{t-1}^{1:L}|x_t^{1:L}}^{\\text{pre}}(t) ,& \\text{when}\\  x_{t-1}^{l} = x^l, x^l = Mask \\\\ 0 ,& \\text{when}\\ x \\neq Mask. \\end{cases} $\nAt first glance, evaluating the optimal policy based on the pre-trained policy (18), i.e.,\n$\\exp(v(x)) \\prod_{l=1}^{L} p_{t-1}^{\\text{pre}}(x_{t-1}^{1:L}|x_t^{1:L})$\nmight seem computationally prohibitive, as it appears to require $O(L\\mathcal{K}^L)$ evaluations of the value functions. However, it can actually be computed using only $O(L\\mathcal{K})$ operations. The key idea lies in identifying an asymptotically equivalent policy, as described below.\n\u2022 Case 1: A single"]}