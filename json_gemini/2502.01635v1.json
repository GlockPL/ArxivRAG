{"title": "The AI Agent Index", "authors": ["Stephen Casper", "Luke Bailey", "Rosco Hunter", "Carson Ezell", "Emma Cabal\u00e9", "Michael Gerovitch", "Stewart Slocum", "Kevin Wei", "Nikola Jurkovic", "Ariba Khan", "Phillip Christoffersen", "A. Pinar Ozisik", "Rakshit Trivedi", "Dylan Hadfield-Menell", "Noam Kolt"], "abstract": "Leading AI developers and startups are increas-ingly deploying agentic AI systems that can plan and execute complex tasks with limited human involvement. However, there is currently no structured framework for documenting the technical components, intended uses, and safety features of agentic systems. To fill this gap, we introduce the AI Agent Index, the first public database to document information about currently deployed agentic AI systems. For each system that meets the criteria for inclusion in the index, we document the system's components (e.g., base model, reasoning implementation, tool use), application domains (e.g., computer use, software engineering), and risk management practices (e.g., evaluation results, guardrails), based on publicly available information and correspondence with developers. We find that while developers generally provide ample information regarding the capabilities and applications of agentic systems, they currently provide limited information regarding safety and risk management practices. The AI Agent Index is available online at https://aiagentindex.mit.edu/, with raw data at this link.", "sections": [{"title": "1. Introduction", "content": "'Agentic' AI systems that can be instructed to plan and directly execute complex tasks with only limited human in-volvement (Xi et al., 2023; Wang et al., 2024; Durante et al., 2024; Sager et al., 2025) are transitioning from research prototypes to real-world products (e.g., Devin, h2oGPTe, Simple AI, XBOW). These systems\u2014which are generally comprised of foundation models augmented with scaffold-ing for reasoning, planning, memory, and tool use (Sumerset al., 2023; Zaharia et al., 2024; Yao, 2024; Su et al., 2024)\u2014are being deployed in a growing number of domains (see Figure 7).\nThe performance of agentic systems is steadily improving on benchmarks (Mialon et al., 2023b; Xie et al., 2024b; Zhou et al., 2023; Koh et al., 2024; Yoran et al., 2024; Xu et al., 2024), and these systems are being integrated into broader swathes of economic activity (Wang et al., 2024; Durante et al., 2024; Sager et al., 2025). As a result, their real-world impacts are mounting (Chan et al., 2023; Gabriel et al., 2024; Anwar et al., 2024; Kolt, 2025). Alongside the significant opportunities presented by agentic systems, researchers have also raised noteworthy concerns, including cybersecurity risks (Fang et al., 2024a;b), loss of control (Cohen et al., 2024; Bengio et al., 2025), and physical harm where agents operate robotic systems (Ruan et al., 2023).\nDespite growing efforts to study trends in the development of agentic AI systems, including evaluating their perfor-mance and cost (Kapoor et al., 2024; Stroebl et al., 2025), assessing their potential harms (Andriushchenko et al., 2024; Kumar et al., 2024; U.S. AI Safety Institute, 2025), and in-creasing visibility into their operation (Shavit et al., 2023; Chan et al., 2024a;b; 2025; Kolt, 2025), many practical questions remain unanswered:\n\u2022 Which organizations are developing agentic systems?\n\u2022 In which domains are they being deployed?\n\u2022 What infrastructure do agentic systems require?\n\u2022 How is their performance and safety evaluated?\n\u2022 What guardrails are used to mitigate risks?\nTo empirically answer these questions and improve pub-lic understanding of agentic AI systems, we introduce and release the AI Agent Index, a comprehensive sample of deployed agentic AI systems (n = 67). The index, which is constructed from a combination of publicly available data and correspondence with developers, documents publicly-available information on the intended uses of agentic sys-tems, their technical components (including reasoning, plan-ning, and memory implementation, base models, observa-"}, {"title": "2. Background", "content": "There is no widely accepted definition of \"AI agent\". The notion of artificial agency has a long and contentious history, spanning multiple decades and diverse disciplines. These include cybernetics (Rosenblueth et al., 1943; Ashby, 1956;"}, {"title": "2.1. Agentic Architectures, Applications, and\nOpportunities", "content": "Contemporary AI agents are generally compound systems (Zaharia et al., 2024) comprised of a foundation model augmented by external resources, known as \"scaffolding\", which enable effective planning, memory, and tool use (Wang et al., 2024; Xi et al., 2023; Durante et al., 2024). Planning of complex series of actions is typically facilitated through chain-of-thought-based reasoning processes (Wei et al., 2022; Yao et al., 2022c; 2023; Shinn et al., 2023; OpenAI, 2024). Memory relies on information stored in"}, {"title": "2.2. Safety Risks and Ethical Concerns", "content": "Given that agentic AI systems are built on foundation mod-els, they are susceptible to many of the risks associated with such models, including harms arising from hallucinations, biased outputs, and leakage of private data (Bender et al., 2021; Weidinger et al., 2022; Solaiman et al., 2023). Agentic systems, however, also present new risks that stem specifi-cally from their agentic properties, i.e., underspecification, directness of impact, goal-directedness, and long-term plan-ning (Chan et al., 2023; Cohen et al., 2024; Ruan et al., 2023; Andriushchenko et al., 2024; Bengio et al., 2025). For example, while chatbots often cause harm by human users acting upon model outputs (e.g., deploying model-generated malicious code) (Phuong et al., 2024), agentic AI systems can directly cause harm (e.g., autonomously hacking websites) (Fang et al., 2024a; Jaech et al., 2024).\nAdditionally, as agentic AI systems undertake more complex and long-horizon tasks, with limited human oversight, users are likely to repose greater trust in those systems, poten-tially developing asymmetric relationships of dependence (Gabriel et al., 2024; Manzini et al., 2024b;a; Bengio et al., 2025). Moreover, agentic systems developed and operated by large platform companies could enable those compa-nies to exert greater influence and control over users and third parties with whom they interact (e.g., vendors accessed through platform-controlled agents) (Lazar, 2024)."}, {"title": "2.3. Documentation Frameworks", "content": "Many frameworks have been developed to document the features of AI systems, the resources used to build them, and the contexts in which they are deployed. These in-"}, {"title": "3. Methods", "content": "What does the index include? As discussed in Section 2, there is no widely-accepted definition of \u201cAI agent.\" We do not propose one here. Given our focus on the societal impacts of agentic AI systems, we draw on the four char-acteristics introduced by Chan et al. (2023) discussed in Section 2. Importantly, to address the practical questions outlined in Section 1, we primarily document the features of agentic AI systems that are either deployed as products or available open source.\nThe full decision graph we used to determine inclusion in the index is shown in Figure 3. Notably, we restricted the in-dex to agentic systems and did not include language models themselves, or agent development frameworks (unless the framework was built around a qualifying flagship system, in which case we indexed that system). We also created a single index entry per named and versioned system. Differ-ent releases (e.g., \"HelpfulAgent1.1\u201d vs \u201cHelpfulAgent1.2\")"}, {"title": "4. Agent Card Components", "content": "Each agent card contains 33 fields of information, divided into 6 categories:\n1. Basic information\n\u2022 Website\n\u2022 Short description\n\u2022 Intended uses: What does the developer state that the system is intended for?\n\u2022 Date(s) deployed\n2. Developer\n\u2022 Website\n\u2022 Legal name\n\u2022 Entity type\n\u2022 Country (location of developer or first author's first affiliation)\n\u2022 Safety policies: What safety and/or responsibility policies are in place?\n3. System components\n\u2022 Backend model: What model(s) are used to power the system?\n\u2022 Publicly available model specification: Is there formal documentation on the system's intended uses and how it is designed to behave in them?\n\u2022 Reasoning, planning, and memory implementation: How does the system 'think'?\n\u2022 Observation space: What is the system able to observe while 'thinking'?\n\u2022 Action space/tools: What direct actions can the system take?\n\u2022 User interface: How do users interact with the system?\n\u2022 Development cost and compute: What is known about the development costs?\n4. Guardrails and oversight\n\u2022 Accessibility of components:\nWeights: Are model parameters available?\nData: Is data available?\nCode: Is code available?\nScaffolding: Is system scaffolding available?\nDocumentation: Is documentation available?\n\u2022 Controls and guardrails: What notable methods are used to protect against harmful actions?\n\u2022 Customer and usage restrictions: Are there know-your-customer measures or other restrictions on customers?\n\u2022 Monitoring and shutdown procedures: Are there any notable methods or protocols that allow for the system to be shut down if it is observed to behave harmfully?\n5. Evaluations\n\u2022 Notable benchmark evaluations (e.g., on SWE-Bench Verified)\n\u2022 Bespoke testing (e.g., demos)\n\u2022 Safety: Have safety evaluations been conducted by the developers? What were the results?\n\u2022 Publicly reported external red-teaming or comparable auditing:\nPersonnel: Who were the red-teamers/auditors?\nScope, scale, access, and methods: What access did red-teamers/auditors have and what actions did they take?\nFindings: What did the red-teamers/auditors conclude?\n6. Ecosystem\n\u2022 Interoperability with other systems: What tools or integrations are available?\n\u2022 Usage statistics and patterns: Are there any notable observations about usage?\n7. Additional notes: If any\nWe populated each field in each card with written notes based on publicly available information. When no informa-tion was available, we recorded \"None\" or \"Unknown.\""}, {"title": "5. Findings", "content": "In addition to compiling specific information regarding each of the 67 indexed systems, the AI Agent Index offers a high-level perspective of this emerging field. Noting the limitations and biases discussed next (in Section 6), here, we offer a bird's eye view of the state of the art for AI agents.\nAgentic systems are being deployed at a steadily increas-ing rate. Systems that meet our criteria for inclusion in the index have had (initial) deployments dating back to early"}, {"title": "6. Limitations and Concerns", "content": "Defining agentic systems. The term \"AI agent\" is con-tentious, as discussed in Section 2. In particular, the term has been criticized for inappropriately anthropomorphiz-ing certain Al systems (Weidinger et al., 2022; Mitchell, 2021), which could potentially lead to unrealistic expecta-tions from, or over-reliance on, such systems (Gabriel et al., 2024; Manzini et al., 2024b). Recognizing this concern, we do not weigh in on this debate, advocate a particular definition of \"AI agent\", or propose alternative terminology. Instead, we focus on empirically documenting a growing class of deployed AI systems that exhibit \u201cagentic\" char-acteristics (as described in Chan et al. (2023)) and have a potential for significant impact. Through the index, we communicate our findings as plainly and openly as possible.\nScope and timing of index. The index is not a compre-hensive or exhaustive database of all agentic systems or related resources, such as language models and develop-ment frameworks for building agentic systems. The field of agentic AI is highly decentralized and poorly documented. Accordingly, there may also be systems that meet the se-lection criteria specified in Section 3 but do not appear in the index. In particular, the index is likely to disproportion-ately document agentic systems that are publicly available or publicly released, compared with systems used internally within organizations (which, by definition, are not publicly accessible). In addition, the index only includes systems described in the English language and includes relatively few systems from non-western developers. The index rep-resents a snapshot in time on December 31, 2024 and does not include systems that were obsolete by this date or were released thereafter. Moreover, while the agent cards in the index collect 33 fields of information, these are not exhaus-tive and exclude, for example, records of real-world safety incidents (to the extent such incidents have occurred).\nIncomplete or inaccurate information. In total, the in-dex contains over 2,200 fields of information reviewed by multiple authors. Nonetheless, despite our best efforts to manually verify the completeness and accuracy of all agent cards, mistakes may have occurred. In addition, the response rate of developers to our requests for feedback was 36%. Accordingly, it is possible that some developers may, for ex-ample, have in place internal safety documents or practices that we could not discover from publicly available documen-tation, or were not informed about through correspondence. Recognizing these concerns, we have established a struc-tured process for facilitating further corrections to the index. These can be submitted at this link."}, {"title": "7. Discussion and Future Work", "content": "The agentic AI ecosystem is difficult to document. The extensive data collection process undertaken for the current paper (see Section 3) sheds light on the significant chal-lenges involved in documenting agentic AI systems. During this process, we encountered a diverse range of AI systems, across multiple domains, in different places in the research-product spectrum, and accompanied by varying levels of information and documentation. The differences were often most stark when comparing systems developed in industry and systems developed in academia, the latter of which are typically simpler and more open. On occasion, these fea-tures of the agentic AI ecosystem made it challenging to determine whether a particular system meets our criteria for inclusion in the index. Most importantly, the fact that we ultimately produced an \u201cAI Agent Index\u201d should not be taken to suggest that this ecosystem lends itself to clean taxonomization and indexing (it does not). We expect these documentation challenges to persist for the foreseeable fu-ture.\nFuture documentation work should be appropriately scoped. Our research design-including both the selection of information fields to be collected and the methods for collecting data-offers lessons for future attempts to docu-ment the agentic AI ecosystem. From the outset, we sought to collect information on agentic systems that had been gen-erally overlooked by previous survey papers and overviews of the field, such as the accessibility of documentation and code, information regarding red-teaming and safety poli-cies, and the country of developers (see Section 4). Future documentation work can build on this approach, examin-ing a broader range of technical, safety, and policy-relevant features of agentic AI systems. To ensure tractability, we recommend that future work surveying the agent ecosystem be appropriately scoped either in breadth or depth. For ex-ample, selection criteria could be revised to demand a high threshold for \"agency\u201d or anticipated societal impact.\nDocumentation can inform governance and policy. Our findings (discussed in Section 5) may inform the scope and methods of AI governance and policymaking:\n\u2022 The majority of indexed agentic systems were devel-oped in industry, suggesting that governance interven-tions should consider the incentives of corporate devel-opers (distinct from those of academic labs).\n\u2022 Most indexed systems were developed by US-based or-ganizations, indicating that governance efforts focused on US contexts could have more leverage than efforts in other countries or regions.\n\u2022 The prominence of software engineering and computer-use agents suggests that policy researchers and practi-tioners should prioritize these domains when designing governance frameworks.\n\u2022 Very few developers disclose information about safety or risk management, underscoring the importance of establishing transparency and disclosure mechanisms as a key first step in the governance of agentic systems.\nTo address knowledge and accountability gaps uncovered by our findings, policymakers could consider:\n\u2022 Structured bug bounties: Incentivizing external red-teaming promotes the proactive discovery of vulnera-bilities, adapting approaches used in cybersecurity.\n\u2022 Systematic testing of agents: Governance bodies and academic labs could coordinate risk assessments of agentic systems.\n\u2022 Centralized oversight of indices: Regulatory or standard-setting institutions could establish and main-tain indices of agentic systems like this one.\n\u2022 Integration with model registries: Incorporate indices of agentic systems into broader registry frameworks (McKernon et al., 2024), ensuring unified reporting of agentic systems, common safety benchmarks, and clearer accountability mechanisms.\nImpact Statement\nThis work was undertaken to improve our collective un-derstanding of the emerging field of agentic AI. Its con-tributions revolve around the compilation and analysis of publicly available information, supplemented by correspon-dence with developers. In Section 6, we discuss how trans-parency standards can be 'gamed,' and note that this was one reason that we did not score developers using the index. Taken together, we hope the methodology and findings intro-duced by the AI Agent Index inform progress toward better risk management practices and governance frameworks for agentic AI systems."}, {"title": "A. Sample Agent Card", "content": "Here, we provide a sample agent card for Microsoft's Magentic One (Fourney et al., 2024). We selected it based on its recency, degree of documentation, openness, generality, and noteworthy performance. No authors have conflicts of interest related to Microsoft or Magentic One, and this example selection was made without correspondence with Microsoft. Including Magentic One's agent card as an example is not an endorsement of the system or developer.\nMagentic One\n1. Basic information\n\u2022 Website: https://www.microsoft.com/en-us/research/publication/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/\n\u2022 Short description: A multiagent system introduced by Microsoft with general capabilities.\n\u2022 Intended uses: What does the developer state that the system is intended for? It is used for \"ad-hoc, open-ended tasks such as browsing the web and interacting with web-based applications, handling files, and writing and executing Python code\" [source].\n\u2022 Date(s) deployed: Announced November 4, 2023 [source].\n2. Developer\n\u2022 Website: https://web.archive.org/web/20241231232226/https://www.microsoft.com/en-us/\n\u2022 Legal name: Microsoft Corporation [source].\n\u2022 Entity type: Corporation [source].\n\u2022 Country (location of developer or first author's first affiliation): Incorporation: Washington, USA (Microsoft Corporation (2357303)) [source]. Registration: Delaware, USA. HQ: Washington, USA [source].\n\u2022 Safety policies: What safety and/or responsibility policies are in place? Model evaluations and red teaming; model reporting and information sharing; security controls [source]. Microsoft's safety policies are described online [source].\n3. System components\n\u2022 Backend model: What model(s) are used to power the system? The default model used is gpt-4o-2024-05-13, but they also experiment with using OpenAI 01 [source].\n\u2022 Publicly available model specification: Is there formal documentation on the system's intended uses and how it is designed to behave in them? Available [source].\n\u2022 Reasoning, planning, and memory implementation: How does the system \u2018think'? The system contains multiple subagents that work together to solve problems. Things are controlled at a high level by the \u201cOrchestrator\u201d agent and executed by the \u201cWebSurfer,\" FileSurfer,\u201d \u201cCoder,\u201d and \u201cComputerTerminal", "space": "What is the system able to observe while 'thinking'? It has full access to a filesystem and web browser.\n\u2022 Action space/tools: What direct actions can the system take? It is able to surf (including posting) on the web, execute file system commands, and write/execute code.\n\u2022 User interface: How do users interact with the system? Users can configure and experiment with it using the AutoGen package [source].\n\u2022 Development cost and compute: What is known about the development costs? Unknown.\n4. Guardrails and oversight\n\u2022 Accessibility of components:\nWeights: Are model parameters available? N/A; backends various models.\nData: Is data available? N/A; backends various models.\nCode: Is code available? Available on GitHub as part of Microsoft's AutoGen project [source].\nScaffolding: Is system scaffolding available? Available [source].\nDocumentation: Is documentation available? Available on GitHub [source], see also the technical report [source]."}]}