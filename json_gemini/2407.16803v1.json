{"title": "Fusion and Cross-Modal Transfer for Zero-Shot Human Action Recognition", "authors": ["Abhi Kamboj", "Anh Duy Nguyen", "Minh Do"], "abstract": "Despite living in a multi-sensory world, most AI models are limited to textual and visual interpretations of human motion and behavior. Inertial measurement units (IMUs) provide a salient signal to understand human motion; however, they are challenging to use due to their uninterpretability and scarcity of their data. We investigate a method to transfer knowledge between visual and inertial modalities using the structure of an informative joint representation space designed for human action recognition (HAR). We apply the resulting Fusion and Cross-modal Transfer (FACT) method to a novel setup, where the model does not have access to labeled IMU data during training and is able to perform HAR with only IMU data during testing. Extensive experiments on a wide range of RGB-IMU datasets demonstrate that FACT significantly outperforms existing methods in zero-shot cross-modal transfer.", "sections": [{"title": "1 Introduction", "content": "Humans naturally can actuate a motion they have only seen before; however, transferring motion knowledge across sensors for machine learning models is nontrivial. Our interaction with computing has historically been centered around visual and textual modalities, which has provided these models an abundance of data. Thus, deep learning based human action recognition (HAR) systems often collapse 3D motion into related but imprecise modalities such as visual data [16, 31, 20, 36] or language models [28, 37, 33, 26, 9]. However, continuous monitoring by cameras or constant input to text models is impractical, limiting the applicability of these models in real-world scenarios.\nInertial Measurement Units (IMUs), which typically provide 3-axis acceleration and 3-axis gyroscopic information on a wearable device, emerge as strong candidates for understanding human motion in a nonintrusive fashion. Smartwatches, smartphones, earbuds and other wearables have enabled the seamless integration of IMUs into daily life [22]. Unfortunately, IMUs remain underutilized within current machine-learning approaches due to numerous challenges, such as the lack of abundant data and the difficulty in interpreting and labeling the data.\nBeyond IMUs, various sensing modalities are gaining popularity in wearables (e.g. surface electro-cardiogram, electromyography) and ambient monitoring systems (e.g. wifi signals, millimetewave\nRadar). This raises the critical question of how to integrate new sensors with existing ones in the absence of labeled data. One promising solution is leverage a well-documented modality to transfer knowledge to another modality, a process known as cross-modal transfer. Ideally, this source modality would be able to teach the new target modality without any human annotation effort. Existing cross-modal learning techniques assume a semi-supervised or fully superivsed setup where there exists some labels for each modality during training. Cross-modal learning has not thoroughly been inevestigated in a zero shot-setting.\nWe hypothesize that there exists some inherent latent space that can capture the alignment between multiple sensing modalities for human action recognition. We empirically investigate various methods to construct this space and leverage it to perform zero shot transfer between modalities. Specifically, given labeled data from one sensor and unlabeled data from another, we explore whether an intermediate latent structure can be used to infer human actions from new data obtained solely from the second sensor.\nOur method to perform Fusion and Cross-modal Transfer (FACT) through the hidden correlation between modalities, was tested on RGB and IMU data from 4 datasets against 4 baselines. We train FACT to perform action recognition with labeled RGB data, while simultaneously aligning a different set of unlabeled RGB data with synchronous IMU data. The resulting model can perform test-time inference on IMU data, showcasing cross-modal zero-shot transfer capabilities. This achievement lies at the intersection of transfer learning, multimodal representation learning, and sensor fusion and holds significant implications for the applicability of machine learning in more diverse, underexplored, modalities. Furthermore, to emphasize the importance of caputuring time in sensing modalities we provide an time-continuous extension of FACT, referred to as T-FACT, that aligns modalitles across extracted chunks of time, with a more powerful capability to perform zero-shot transfer in difficult scenarios. Our contributions are as follows:\n\u2022 A novel motivation and setup for zero-shot cross-modal transfer learning with sensing modalities for human action recognition.\n\u2022 The FACT and T-FACT infrastructures to perform zero-shot transfer to a new modality, with strong empirical results for alignment between two distinct sensing modalities.\nThe organization of the paper is as follows: We begin with the formulation of our setup in Section 2. We further provide an explanation of our model and methods in Section 3. Section 4 describes our results and baselines. We describe additional experiments which verify the integrity of FACT in Section 5 Then we briefly review relevant literature in Section 6. Finally, we discuss the implications and limitations of our work and conclude with Section 7."}, {"title": "2 Background", "content": "We investigate the creation of a robust multi-modal latent space for human action recognition, denoted as Z. We refer to this latent space as Z, and assume there exists a learnable mapping h: Z \u2192 Y, where y is the label space of human actions. We further assume that there exists a learnable projection from every modality x(k) \u2208 X(k),k \u2208 {1 . . . M} to this latent space f(k) : X(k) \u2192 Z.\nThe critical intuition that drives our method is that for a point zi \u2208 Z, any zj \"near\" zi maps to the same class as zi, thus we can leverage the structure of Z to classify any two neighboring vectors in Z regardless of which modality they are generated from. In our experiments, we attempt to quantify nearby in terms of cosine similarity (and also perform some ablations with L2 distance metric).\nFor simplicity, we begin with 2 modalities M = 2 and assume n data points are split into 4 disjoint index sets I\u2081 \u222a I2 \u222a I3 \u222a I\u2084 \u2208 {1... n}. Under our cross-modal transfer setting, during training the model has access to 2 of these datasets. One contains labeled data for one modality DHAR = {(x,yi)}111 and the other contains pairs of data between the modalities but these points are unlabeled: DAlign = {(x,x)}=1. This is analogous to having an existing sensor with labeled data or a trained model, and introducing a new sensor in which data can be synchronously collected, but there is no additional annotation effort. The third and fourth sets are used for validation and testing and contain only labeled data from the second modality, i.e. Dval = {(x,y)}=1 and DTest = {(x,yi)}1"}, {"title": "3 Methods", "content": "We present our Fusion and Cross-modal Transfer method to transfer knowledge to a new sensing modality without having seen labels in that modality. Given our motivation, we experiment with RGB videos as the source of labeled data x(1) = x(RGB) and IMU data as the target data x(2) = x(IMU).\nFACT training occurs in 2 phases. Phase b) trains the HAR module on labeled camera data x(1) from DHAR and phase a) involves aligning the representations of x(1) with x(2) on Dalign. The\nfinal results are reported on the test set where the model only has access to IMU data, DTest. The final results in table Table 1, show that existing works adapted to our settting heavily struggle in this zero-shot cross-modal transfer scenario. We experiment with different techniques to construct FACT and report the results in Section 5."}, {"title": "3.1 Model Architecture", "content": "3.1.1 FACT:\nThe architecture of FACT is fully modular allowing the different components to be used during the two steps of training. This also allows for scalability and interoperability of different sensing modalities, types of encoders, and output task heads. The full architecture is given in Figure 1 and the individual modules are as follows.\nVideo Feature Encoder f(1) : X(1) \u2192 Z: This module applies a pretrained Resnet18 to every frame a video and then performs a single 3D convolution and a simple 2-layer feed forward network (FFN) with ReLU activations.\nIMU Feature Encoder f(2) : X(2) \u2192 Z: This module consists of a 1D CNN followed by a FFN. HAR Task Decoder h : Z \u2192 Y: This is a simple FFN."}, {"title": "3.1.2 T-FACT:", "content": "We further propose a Time-continuous FACT model that leverages the temporal information of sensing modalities when aligning and fusing their representations. In T-FACT, we remove the FFN from the feature encoders and use the output of the temporal convolutions directly. This temporal receptive field would have extracted the salient features of neighboring time steps of the data. We use each of these time steps as the a z latent vector. Then during the alignment we align each of these time vectors with the same ones from the other modality. The during phase b) when training the HAR model, we use self attention with a learned class token to predict the action. The intuition is that the encoder will learn which tokens over time are the most informative for the action class and predict accordingly. This is a common method to perform classification with transformers The updated modules are as follows:\nVideo Feature Encoder f(1) : X(1) \u2192 Ztrec: This module applies a pretrained Resnet18 to every\nframe a video and then performs a single 3D convolution. The resulting output is trec vectors in z: 2(1) = ((1) ... (1) ).\nIMU Feature Encoder f(2) : X(2) \u2192 Ztrec: This is a 1D CNN that decreases the time dimension\nto trec, resulting in an output of 2(2) = (2(2)...(2)).\nHAR Task Decoder h: Ztrec \u2192 Y: This module is like a transformer encoder that uses self-attention on an input sequence of length trec vectors appended with a learned class token. The output class token of the self attention layer is then passed through a FFN and outputs a single action label Yi."}, {"title": "3.2 Training and testing:", "content": "As shown in Figure 1, training iterates through two phases: training a HAR model with labeled data, and aligning the modalities' representations using unlabeled data. For training the RGB and HAR model on DHAR we use the standard cross-entropy loss:\n$L_{CE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} 1_{i=j} \\log(\\frac{\\exp \\hat{y}_{i, j}}{\\Sigma_{j=1}^{M} \\exp \\hat{y}_{i,j}})$\nwhere \u0177\u2081 = h(f(1)(x1))) is the output of the ith sample in the batch of N samples. \u0177i,j is the score for the jth class out of C classes.\nTo align different modalities in the feature space on Dalign we use a symmetric contrastive loss formulation\nLCL [28, 23, 11] with temperature parameter \u0442:\n$L_{CL} = \\frac{1}{N} \\sum_{i=1}^{N} - \\log(\\frac{\\exp((\\frac{z_i^{(1)} z_i^{(2)}}{\\tau})}{\\Sigma_{j=1}^{N} \\exp((\\frac{z_i^{(1)} z_j^{(2)}}{\\tau})})$, where $z^{(k)} = \\frac{f^{(k)}(x^{(k)})}{|| f^{(k)}(x^{(k)}) ||},k\\in {1,2}$\nThe symmetric contrastive loss will cluster representations in Z by cosine similarity, which brings about the desired property of the latent space that vectors of the same class are 'near' each other."}, {"title": "4 Results", "content": "4.1 Datasets\nWe present results on small yet structure dataset (UTD-MHAD), one larger dataset captured in a controlled environment (CZU-MHAD), one very large dataset with various challenges (MMACT), and one egocentric camera dataset (MMEA-CL). For each of these datasets we create an approximately 40-40-10-10 percent datasplit for the Dalign, DHAR, Dval, and DTest splits respectively as shown in Table 5.\nUTD-MHAD Most of the development and experiments of FACT was performed on the UTD-Multi-modal Human Action Dataset (UTD-MHAD) [7]. This dataset consists of roughly 861 sequences of RGB, skeletal, depth and an inertial sensor, with 27 different labeled action classes performed by 8 subjects 4 times. The inertial sensor provided 3-axis acceleration and 3-axis gyroscopic information, and all 6 channels were used for in our model as the IMU input. Given our motivation, we only use the video and inertial data; however, FACT can easily be extended to multiple modalities.\nCZU-MHAD The Changzhhou MHAD [6] dataset provides about 1,170 sequences and includes depth infor-mation from a Kinect camera synchronized with 10 IMU sensors, each 6 channels, in a very controlled setting with a user directly facing the camera. We concatenate the IMU data to provide a 60-channel input as the IMU modality and use depth as the input modality. Given the controlled environment and dense IMU streams, the models performed the best on this dataset.\nMMACT The MMAct dataset [18] is a large scale dataset containing about 1,900 sequences of 35 action classes from 40 subjects on 7 modalities. This data is challenging because it provides data from 5 different scenes, including sitting a desk, or performing an action that is partially occluded by an object. Furthermore, the data was collected with the user facing random angles at random times. The dataset contains 4 different cameras at 4 corners of the room, and it measures acceleration on the user's watch and acceleration, gyroscope and orientation data from a user's phone in their pocket. We only use the cross-view camera 1 data, and again we concatenate the 4 3-axis inertial sensors into one 12 channel IMU modality.\nMMEA-CL The multimodal egocentric activity recognition dataset for continual learning (MMEA-CL) is a recent dataset motivated by learning strong visual-IMU based representations that can be used for continual learning. It provides about 6,4000 samples of synchronized first-person video clips and 6-channel accelerometer and gyroscope data from a wrist worn IMU. The dataset's labels features realisite daily actions in the wild, as opposed to recorded sequences in a lab. Due to issues with the data and technical constraints, we downsize the data proportionally and use about 1,000 samples. Nonethless, FACT's superior performance shows how this method can generalize to a different camera view, and different types of activities."}, {"title": "4.2 Baselines", "content": "Many works deal with robustness to missing sensor data during training or testing, however, few works deal with zero-labeled training data from one modality. As a result, constructing baselines was tricky and most methods had to be modified or adopted to fit our our approach."}, {"title": "4.2.1 Student Teacher Baseline", "content": "Various student teacher models have been proposed [18, 35, 3]. However, these models often assume the availability of student-teacher labeled modality pairs during training to distill knowledge from the teacher to the student when updating the corresponding losses. Thus most of these architectures are not directly applicable. Nonetheless, we borrow the basic concept of a teacher modality distilling knowledge to the student modality through psuedo labels, most similar to [32].\nWe denote the teacher network as g(t) : X(1) \u2192 Y and the student network as g(s) : X(2) \u2192 Y. Our designated student-teacher baseline uses DHAR to train g(t) on the RGB data. Next, in order to train g(s) on Dalign, we first use g(t) (x1)) = \u01771 to generate psuedo-labels for every datapoint i \u2208 I2. Then we use the augmented dataset Dalign = {(x,x,y)}21 to train g(s). We note that g(t) and g(s) have similar architectures to h(f(1)(.)) and h(f(2) (\u00b7)), respectively for a fair performance comparison.\nThis student teacher baseline presents a strong solution for our setup, and gives a competitive performance in Table 1. However, the only method it outperforms FACT was was in the relatively easy CZU-MHAD dataset. One drawback, with this model is that it requires labeled data from both modalities to improve it's performance, whereas FACT can use unlabeled data to learn some correlation between modalities in Z. This representation space can be used for other purposes as well, such as dynamically adding modalities or task specific heads."}, {"title": "4.2.2 Sensor Fusion Baselines", "content": "Many IMU-RGB based sensor fusion models have the ability to train on partially available or corrupted data and are robust to missing modalities during inference [15, 14]. No works have attempted the extreme case where one modality is completely unlabeled during training. Existing esnsor fusion methods can be adapted to our setup using a psuedo- labeling technique, similar to the student-teacher model above. The difference here is that the model learns a joint distribution between the two modalities so hopefully it may be able to learn some correlation between the models. Nonetheless, we show that these methods cannot generalize to the scenario where there is zero-labeled training data for one modality.\nLet g(\u00b7, \u00b7) : (x(1), X(2)) \u2192 Y. Our approach uses DHAR, to train by passing in zeros for one modality, e.g. we train g(, 0) : X(1) \u2192 Y. Then, with Dalign we use g(\u00b7, 0) to generated psuedo-labels and then train g(0, \u00b7, ) with those labels.\nWe reproduced the conventional sensor fusion models (early, feature, and late) from [38] and indicate the performance of the top model on 1. We further reproduce a self-attention based sensor fusion appraoch (HAMLET [14]) and tested it on our setup. We selected these model due to their state-of-the-art performance on the UTD-MHAD dataset, making them ideal benchmarks for comparison with our model."}, {"title": "4.2.3 Contrastive Learning Baseline", "content": "ImageBind [11] learns encoders for 6 modalities, (Images/Videos, Text, Audio, Depth, Thermal and IMU) by performing CLIP's training method [28] between each of those encoders and the Image/Video encoder. It was well tested for image, text and audio based alignment, retrieval and latent space generation tasks, however was not well test with IMU data and not used for specific tasks, such as HAR. In addition, one fundamental difference between Imagebind and FACT is that Imagebind constructs a latent space amongst the sensing modalities and text and aligns between them. We hypothesize that this is vector space is more difficult and unnecessary to construct, for human action recognitoin using sensing modalities. The text modality, although sequential in nature, does not have a time dimension, thus it cannot leverage correlations between modalities in time like T-FACT.\n(3)\nLet's denote the video, IMU and text encoders as g(1) : X(1) \u2192 Z, g(2) : X(2) \u2192 Z, and g(3) : X(3) \u2192 Z respectively. We perform two conventional task-specific adaptations for CLIP models. First, we attempt zero-shot transfer, in which we pass all the action labels through the text encoder. For a dataset with C classes, we have\n2(3) = (23) ...)). Finally, for a given IMU sample (x2), yi) \u2208 DTest, we pass x (2) through the IMU encoder g(2) and retrieve \ufffd(2). Finally, we classify the point by looking at which points gives the highest cosine similarity score in the latent space, e.g. \u0177i = argmaxi $\\frac{\\langle z_i^{(2)}, z_i^{(3)} \\rangle}{||z_i^{(2)}|| ||z_i^{(3)}||}$"}, {"title": "5 Additonal Experiments and Ablations", "content": "FACT is modular and scalable and well suited for time series sensor data. Here we perform ablations and experiments to show how our design is uniquely structured to perform cross-modal transfer for zero-shot transfer well. We conduct two primary sets of experiments: one investigates various training mechanisms of FACT, and the other examines the deployment of the model in different testing scenarios."}, {"title": "5.1 Training Experiments", "content": "We conducted four primary experiments to identify the optimal training method for the FACT infrastructure, using the UTD-MHAD dataset. FACT employs two distinct losses, each corresponding to a different part of the training process as illustrated in Figure 1. We experimented with various ways to combine these two losses and determined the most effective approach, as shown in Table 4.\n1) Align First: The first experiment first aligns the representations generated by the RGB and IMU encoders, f(1) and f(2) RGB on Dalign (phase a) of training). Then the weights for both encoders are frozen and the HAR module h is trained on the latent representation (1) generated from the RGB encoder.\n2) HAR First: In this method, we first perform phase b) and the RGB HAR model completely with the DAlign split of data. Then we freeze the encoder f(1) and align the modalities with the DHAR split of data (phase a).\n3) Interspersed Training: Here the model intermittently learns from DAlign and DHAR. The model learns an epoch from b) and updates it's weights to train the RGB HAR model, then learns an epoch from a) and updates it's weights to align the encoders. The model continues to iterate between the two losses. We also experimented with training within each batch but using different losses and optimizers, but this yielded instability in the training.\n4) Combined Loss: This method trains both a) and b) but within the same loss iteration. The loss from a) on a batch of data from DAlign is added to the loss from b) on a batch of data from DHAR and the total loss is then"}, {"title": "5.2 Testing Experiments", "content": "We have shown that the FACT performs well on a new modality, however, the question remains whether it can still retain performance on the original modality it was trained on. Furthermore, if it is given multiple modalities during inference, can it leverage information from all of them? Through FACT, for any data sample i given for inference, regardless of whether the sample contains data from X(1), X(2) or both (X(1), (2)), we can estimate the latent vector \u0109i \u2208 Z and thus predict the action using the HAR module, \u0177 = h(i)\n1. RGB (Supervised Learning) The typical supervised machine learning paradigm tests the model on different samples of the same distribution. In our case, this is testing FACT on RGB data. Thus the estimated latent vector is given by f(1) (x1)) = zi.\n2. IMU (Zero-Shot Transfer) The cross-modal zero-shot transfer method is the main result of this paper and described above in Section 3. Here the estimated latent vector is given by f(2) (x(2)) = zi.\n=  3. Fusion (Sensor Fusion) When both modalities are present, the model estimates the latent vector 2 from the outputs of modality-specific encoders assuming each estimate is equally as good as the other. Zi = $\\frac{E[z_i\\vert x_i^{(1)}, x_i^{(2)}] = E[z_i\\vert x_i^{(1)}] + E[z_i\\vert x_i^{(2)}]}{2} = \\frac{f_1(x_i^{(1)}+f_2(x_i^{(2)}}{2}$ Thus, for sensor fusion we average the outputs of each of the encoders. We show the results in 1 and 2."}, {"title": "6 Related Works", "content": "6.1 Sensor Fusion\nCross-modal transfer learning is a method to transfer knowledge from a modality with abundant training data to one with limited data [24]. Domain adaptation can be seen a s subset of cross-modal transfer Domain adaptation allows a machine learning model trained in one domain to efficiently adapt to another related domain for the same output task with fewer data labels [25, 8]. Given this focus on scarcely labeled domains, adaptation is often performed through unsupervised [5] or semi-supervised [1] methods. In terms of human activity recognition, different data domains can imply adapting between different sensor inputs [2], different positions of wearables on the human body [34, 5, 27], different users [13, 10] or IMU device type [17, 41, 4]. In the context of this cross-modal learning our work may be considered domain adaptation involving different sensor inputs.\nIMU Virtualization: To overcome the difficulties associated with limited IMU data, many recent works have leveraged the ability to simulate IMU data from videos such as IMUTube [19] or ChromoSim [12]. This allows them to train an IMU model from camera data and perform zero-shot classification on IMU data with a model that has never seen real IMU data. However, these models are time and data-intensive and cannot easily be extended to other modalities.\nContrastive Learning Methods: Contrastive learning-based multimodal models such as ImageBind [11] or IMU2CLIP [23] are relatively easy to extend to new modalities, however, they fail to fuse the sensor modalities when present and are designed for cross-modal retrieval or generation and perform poorly on specific tasks such as human action recognition.\nCross-Modal Knowledge Distillation Knowledge distillation methods typically use an extra auxiliary modality during training to increase single modal performance during testing, however, they assume labeled training data from both modalities during training [40, 18, 35, 3]. Notably, [32] attempts to perform without knowledge distillation without labels for one modality using a student-teacher frameowrk which FACT can out-perform. Furthermore, they test transferring acrosss visual modalities which is likely more correlated that visual and inertial modalities."}, {"title": "7 Conclusion", "content": "Limitations: Our experiments were run in a very controlled predefined environment and SF-CMT is a research prototype. Future extensions of this work may want to consider the more realistic application of continuous HAR, where the system also must localize the action temporally and spatially before being able to recognize it. We anticipate that T-FACT would work well in this instance. Another relevant extension would be to increase the generalizability of the model, by adding a dimension of inductive transfer, i.e. transfer across output distributions. This means creating a model that could transfer knowledge across different input modalities and different output tasks. Currently, the method is not well tested with more than two modalities, and thus the proposed fusion mechanism in Section 5.2 is under-explored. Even with two modalities, the model performs better using just RGB data than Fusing both; however, we posit that there may be some sort of weighted fusion mechanism that can leverage the IMU data to improve performance with both modalities.\nConclusion: Achieving a good performance with such a compact network demonstrates how this work is essential to the development of cross-modal transfer learning. FACT has a unique theoretical intuition, guiding its design to construct a joint latent space between modalities, that can leverage correlations between them while performing human action recogntion. This framework allows the model to train with zero labels for one modality and still performance inference with that modality alone. Our result has shown promising empirical results on the novel setting of zero shot cross-modal transfer between sensor representations. Especially for biometric or smart IoT applications where the physical form factor is constrained, training data is scarce, and FACT is a viable method for cross-modal transfer."}, {"title": "A.1 UTD-MHAD Dataset", "content": "The UTD-MHAD dataset has no predefined splits and benchmarks, and little to no works have open sourced their code on it. As such, to ensure we were using the data correctly implemented a few sensor fusion models and compared to state-of-the-art reported methods and showed similar performance results. The results are given in Table 6 These experiments provide a standard of comparison for our results with other methods on the UTD-MHAD dataset, and these models are available in the code for this paper."}, {"title": "A.2 Baselines", "content": "A.2.1 HAMLET\nGiven that there is no open source implementation for the HAMLET attention based sensor fusion method [14], we reproduce it from scratch. We follow a very similar architecture; however, extract spatio-temporal results using 3D convolution in the video as opposed to an LSTM and show similar results on the standard sensor fusion problem."}, {"title": "A.2.2 Sensor Fusion", "content": "Sensor fusion is often broken down into the following 3 methods based on where the data are combined [29, 21, 30], also shown in Figure 4: 1) Early or data-level fusion combines the raw sensor outputs before any processing. 2) Middle/intermediate or feature-level fusion combines each sensor modality after some preprocessing or feature extraction. 3) Late or decision-level fusion combines the raw output, essentially ensembling separate models."}]}