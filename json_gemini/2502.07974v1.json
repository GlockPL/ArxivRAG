{"title": "From Hazard Identification to Controller Design: Proactive and LLM-Supported Safety Engineering for ML-Powered Systems", "authors": ["Yining Hong", "Christopher S. Timperley", "Christian K\u00e4stner"], "abstract": "Abstract-Machine learning (ML) components are increasingly integrated into software products, yet their complexity and inherent uncertainty often lead to unintended and hazardous consequences, both for individuals and society at large. Despite these risks, practitioners seldom adopt proactive approaches to anticipate and mitigate hazards before they occur. Traditional safety engineering approaches, such as Failure Mode and Effects Analysis (FMEA) and System Theoretic Process Analysis (STPA), offer systematic frameworks for early risk identification but are rarely adopted. This position paper advocates for integrat-ing hazard analysis into the development of any ML-powered software product and calls for greater support to make this process accessible to developers. By using large language models (LLMs) to partially automate a modified STPA process with human oversight at critical steps, we expect to address two key challenges: the heavy dependency on highly experienced safety engineering experts, and the time-consuming, labor-intensive nature of traditional hazard analysis, which often impedes its integration into real-world development workflows. We illustrate our approach with a running example, demonstrating that many seemingly unanticipated issues can, in fact, be anticipated.", "sections": [{"title": "I. INTRODUCTION", "content": "Phrases like \"this was an unintended consequence\" or \u201cnobody could have anticipated this new problem\" often arise when software products face issues, such as mistakes or biases in ML models within the software. For example, applications may unintentionally amplify biases [1] or introduce privacy risks [2]. However, unintended consequences merely implies that these issues were not anticipated, not that they were inherently unpredictable; had they been anticipated, develop-ers could, in many cases, have mitigated them before they occurred. This position paper argues that (1) systematic hazard analysis methods from safety engineering are well suited to anticipate a wide range of harms in ML-powered applications beyond traditional safety risks, and (2) LLM-supported au-tomation can make hazard analysis more accessible and man-ageable in terms of effort. We believe that hazard analysis is particularly effective in the early development stages, helping identify and design controllers to mitigate harm.\nRecent research has explored more or less structured strate-gies to anticipate potential harms, primarily bias and fairness issues, in the context of ML impact assessment [3]\u2013[7]. How-ever, these approaches are generally model-centric and applied ad-hoc [8], overlooking controllers beyond the model such as safeguards, trend monitoring, human oversight, and user interface modifications areas where software engineers can contribute to responsible engineering of ML-powered systems, extending beyond model-focused considerations.\nMore recent research has adopted traditional safety engi-neering, especially hazard analysis methods such as Failure Mode and Effects Analysis (FMEA) and System Theoretic Process Analysis (STPA), to proactively identify a broad range of ethical and social risks in ML models and ML-powered software systems [8]-[15]. However, outside traditional safety-critical domains such as autonomous vehicles [16], [17], existing studies largely discuss only the potential application of hazard analysis, with several emerging challenges: First, traditional hazard analysis methods are limited by predefined system boundaries, restricting the consideration of a broader scope of risks and solutions. Second, these methods require substantial time and effort, making them costly and chal-lenging to integrate into fast-paced, continuous development workflows outside traditional safety-critical systems [11], [12]. Third, the responsibility for identifying and mitigating risks often falls to software developers or ML model creators, who may lack expertise in safety engineering, potentially reducing the effectiveness and thoroughness of these analyses [11].\nIn this paper, we provide a concrete illustration of how hazard analysis, with minor modifications, can proactively an-ticipate harms and, more importantly, guide the design of con-trollers to mitigate those harms before they occur at both the model and system levels. Furthermore, we show how LLMs can support this process by offering guidance to software engineers and data scientists with limited safety-engineering expertise, requiring only moderate efforts that align with the fast-paced development practices of ML-powered applications. In summary, this position paper offers a fresh and critical perspective on using hazard analysis to broadly anticipate harms in ML-powered applications and to design system-level controllers, illustrated with a concrete example. We further contribute a discussion and demonstration of the potential of how LLMs can support developers in the hazard analy-sis process. We envision that a lightweight, LLM-supported hazard analysis process will become a routine step in the responsible engineering of ML-powered applications, enabling the mitigation of many harms well before they occur."}, {"title": "II. SCOPE AND RELATED WORK", "content": "Though there are various definitions for AI and ML systems, in this paper, we use the term \"ML-powered system\" to denote any software system that incorporates ML models as a component, aiming to emphasize the system as a whole with ML model as an inherently \u2018unreliable' module within it. Practitioners and researchers have acknowledged the poten-tial social risks posed by ML-powered systems [19], [20], such as bias and insufficient oversight. Technical approaches typi-cally focus on measuring and addressing model-level issues, focusing on aspects such as fairness, shortcut reasoning, or privacy [1], [2], [21], [22], often neglecting broader system or environmental considerations. Recent research has introduced structured approaches and tools to help practitioners anticipate potential harms of ML-powered systems, ranging from impact assessment templates [23] to tools that facilitate more or less structured brainstroming [5], [6], [24], [25]. While these meth-ods effectively identify relatively explicit harms or hazards linked to the system's overarching purpose, they fall short in analyzing the detailed components or control structures within these systems and are often used in an ad-hoc manner [11].\nSafety engineering has a long history [26], with traditional methods having been applied in various safety-critical do-mains, such as aviation systems [27], [28] and autonomous vehicles [16], [17]. Recent research has applied these safety engineering methods, particularly the hazard analysis frame-works FMEA [13], [15] and STPA [8], [12]-[14], to identify potential hazards and assess control structures in ML-powered systems. Findings suggest that such systematic approaches are not only effective in identifying a wider spectrum of hazards but are also potentially applicable throughout all development stages. However, integrating safety engineering frameworks into the development cycles of ML systems is often impractical due to the extensive time and paperwork required [11], [12]. Many practitioners lack in-depth safety engineering exper-tise, which can reduce the effectiveness of these frameworks [11]. Furthermore, most existing studies still focus on the model and its development process, with limited attention to non-ML components and the social environment. This paper demonstrates that hazard analysis is effective in anticipating a wide range of hazards and aids in proactive controller design. Furthermore, it shows that LLMs are promising in supporting humans conducting STPA by reducing effort and encouraging broader, more comprehensive thinking beyond the model."}, {"title": "III. A BRIEF INTRODUCTION TO STPA", "content": "While safety engineering techniques date back to the 1950s [29], [30], our approach builds on System-Theoretic Process Analysis (STPA), a state-of-the-art hazard analysis framework grounded in the System-Theoretic Accident Model and Pro-cesses (STAMP) [31]. We use STPA because it is designed for analyzing highly complex systems, can be applied from the early development stages, and considers both technical and human factors [31] \u2013 all crucial for ML-powered software. Traditionally, STPA follows four steps, outlined in Figure 1:\n1) Define the Purpose of the Analysis: We list stakeholders and their values, then identify important losses (e.g., loss of life). Based on the losses, we determine system-level hazards, which are conditions that may lead to a loss (e.g., aircraft too close to other objects). Hazards are then inverted into constraints, which are system conditions that need to be satisfied to prevent hazards. This way, STPA anticipates harms and establishes safety requirements to mitigate them.\n2) Model the Control Structure: We outline the control structure designed to ensure the safety constraints are met. Each previously identified constraint targeted for resolution should have at least one associated controller (e.g., an au-tomated safeguard or a human supervisor). New controllers can be envisioned for constraints without sufficient controllers. STPA particularly focuses on feedback-control loops between controllers and the controlled processes.\n3) Identify Unsafe Control Actions: We analyze each controller's potential failure modes by considering whether a hazard may arise if each control action is absent, incorrect, mistimed, executed in the wrong sequence, prolonged, or stopped prematurely. Control action errors that can lead to unsafe outcomes are then analyzed in the final step.\n4) Identify Loss Scenarios: We examine why unsafe control actions may occur, potentially leading to revising existing con-trollers or introducing additional controllers (e.g., pilots may be unreliable in distance checking, hence we may introduce an automated warning system). The STPA process is then repeated with the modified control structure.\nIn a nutshell, STPA is a structured approach that works backward from harms (losses) to safety requirements (con-straints) and then to mitigation strategies (controllers) and their potential failures. It encourages broad consideration of the control structure, including non-technical controllers like training, human oversight, and government regulations."}, {"title": "IV. LLM-SUPPORTED HAZARD ANALYSIS TO ANTICIPATE HARMS IN ML-POWERED APPLICATIONS", "content": "We argue that hazard analysis is effective in anticipating a wide range of harms in ML-powered applications, such as fairness, usability issues, and societal concerns like deskilling and polarization, extending beyond the traditional focus on severe harms such as loss of life in safety-critical systems. Specifically, hazard analysis aids in anticipating problems and designing mitigation strategies in systems that might otherwise be released without any. In this section, we walk through the four STPA steps using a running example of a simple web system designed to recommend outdoor trails.\nEvidence suggests that routinely applying hazard analysis to everyday software applications as part of responsible engi-neering practices is challenging due to high costs and skill re-quirements. Therefore, we also present how LLMs, specifically GPT-40 by OpenAI [32], can assist software engineers and data scientists in conducting such analysis without requiring extensive training and excessive paperwork.\nOur running example is inspired by the existing customized GPT assistant AllTrails [33]. The application operates within the ChatGPT chat interface and is guided by system prompts from developers. It accesses the predefined alltrails.com API to fetch trail information. Upon receiving user prompts (e.g., \"Recommend trails near [district name] that are dog-friendly.\"), it calls the API for information and provides a response. To our knowledge, it lacks additional controllers beyond its system prompt instructions and ChatGPT's built-in safeguards. As is often the case, the LLM should be considered an unreliable component, as it may fail to follow instructions, hallucinate, or produce factual errors. We assume this appli-cation will be extended beyond its current form and expect it to gain significant popularity. Note that we explicitly chose a running example of an everyday ML-powered application, rather than a traditional safety-critical system, to illustrate how hazard analysis can proactively anticipate issues worth mitigating, even for such seemingly harmless applications."}, {"title": "A. Anticipating Losses and Hazards", "content": "Stakeholders: Following traditional STPA, we begin with identifying stakeholders and their values to assess system losses. To go beyond identifying only the most severe losses, we encourage a comprehensive exploration of stakeholders, including indirect ones, which extends the scope typically practiced in STPA. In our running example, we identified a list of 20 potential stakeholders. This includes directly involved entities such as end users, app developers, and API providers, as well as indirectly affected ones like trail management organizations and local businesses. Our experiments show that LLMs can generate a list of stakeholders based on an application description. We have designed prompts to encour-age LLMs to consider indirect and less common stakeholders, thereby fostering a broad exploration. Developers can then use this list as inspiration for selecting stakeholders to analyze.\nLosses: For each stakeholder, we consider their values and convert them into corresponding losses. In our running example, we identified 145 losses, averaging 6-8 losses per stakeholder; a selection of these is shown in Table II. Again, we found LLMs helpful for suggesting values and losses, given the broad scope and the large number of losses they identified.\nHazards and Constraints: For each loss, we identify hazards states or conditions that can lead to the loss under certain circumstances. We consider hazards beyond traditional STPA system boundaries. For example, the hazard \"system does not support user customization\" may lead to the loss \"lack of personalization\". Given a large number of losses and to avoid prematurely focusing on only a few, we find that LLMs"}, {"title": "B. Proactive Design of Control Structure", "content": "Since the existing AllTrails application had few controllers, as is common in many ML-powered applications, there was not much to analyze regarding the existing control structures in STPA steps 2-4. However, the STPA process is also effective for proactively envisioning new controllers to mitigate the identified hazards, aiding practitioners in modeling the control structure, which is the second step in STP\u0391.\nWe consider potential controllers or design structures that (a) prevent the hazard, (b) reduce its likelihood, (c) decrease the probability of it causing a loss, and (d) minimize the severity of the loss if it occurs. In our running example, aided by LLMs, we identified 10 to 15 possible designs per hazard. While some controllers may be impractical, costly, or insufficiently effective, our systematic approach revealed some easy-to-implement and potentially effective ideas we might not have otherwise considered, such as connecting social media or fitness tracking accounts for personalized recommendations. Among the identified hazards, we chose to explore controllers for three hazards: H4, H39, and H48. These pertain to the lack of personalization, absence of system monitors, and inability for users to withdraw consent, respectively. Some potential controllers are listed in Table IV. Having assessed the risks and costs, we adopted, modified, and merged several controller designs suggested by the LLMs. The resulting control struc-ture incorporates human feedback and hierarchical monitoring mechanisms, which would be challenging to develop without this process. We then performed the second step of STPA and visually modeled the control structure of the system, as illustrated in Figure 2.\nAgain, we found that LLMs can help consider potential designs, especially when given instructions to explore differ-ent designs. However, developers remain crucial in making judgments about relevance and importance."}, {"title": "C. Analyzing and Revising Controllers", "content": "After designing and modeling the control structure, we now return to steps 3 and 4 of the STPA process to analyze whether the controllers are sufficient or if they introduce new problems.\nWe identified unsafe control actions by examining each con-trol action against a checklist for potential constraint violations if the action was (a) absent, (b) incorrect, (c) mistimed, (d) executed in the wrong sequence, (e) prolonged, or (f) stopped early. We then identify potential loss scenarios, which are states or events that may lead to the unsafe control action.\nIn our running example, we analyze the control action where data scientists modify the LLMs that call APIs, as depicted by the bold arrow in Figure 2. Some identified unsafe control actions and corresponding loss scenarios are listed in Table V. We found LLMs effective in systematically exploring different problems by following the checklist.\nPractitioners can now review these unsafe control actions and loss scenarios to decide whether and how to revise certain controllers. For example, our initial monitoring approach was found to be naive, lacking an explicit structure to ensure that an operator regularly checks the dashboard, effectively detects issues, and does not cease monitoring due to notification fa-tigue. This encourages establishing explicit procedures for (a) setting up automated alerts, (b) implementing an engineering process for tracking and eliminating false positive alerts, and (c) designating someone responsible for monthly check-ins with the operator to ensure accountability. This approach goes far beyond simple engineering interventions such as exception handling, redundancy, or using an LLM to review the output of another LLM [34], instead considering the entire socio-technical system and its environment."}, {"title": "V. DISCUSSIONS AND CONCLUSION", "content": "While others have advocated adopting safety engineering to anticipate harms in ML-powered applications [8]\u2013[15], this paper reinforces this idea and provides a concrete example of the process, demonstrating its potential to anticipate harms beyond traditional safety concerns like bodily harm and mis-sion loss. Although our running example appears simple merely combining a system prompt with an API on OpenAI's customized GPTs platform \u2013 the analysis identified potential harms ranging from minor and far-fetched to worth address-ing. This encouraged comprehensive early system design to mitigate these harms. We advocate that developers of novel ML-powered applications should undertake this practice, even if the application appears harmless. There is no justification for not attempting to anticipate the \"unintended consequences,\" given the uncertainty, potential bias, and possible shortcuts associated with ML components in software systems.\nHowever, we also experienced firsthand how quickly the analysis can become unmanageable, especially when expand-ing beyond severe losses in traditional safety-critical systems. We analyzed dozens of stakeholders, hundreds of potential losses, and thousands of hazards. This scope can easily grow to include even more potential controllers and associated issues. Without technical support, this approach is overwhelming and can feel like a bureaucratic paper-heavy compliance activity, raising questions about how to encourage its routine adoption as part of everyday responsible engineering practices.\nAssisting Developers vs. Automating Compliance Activ-ity: LLMs effectively assist in navigating STPA steps and extensively exploring stakeholders, losses, hazards, and con-trollers, helping to scale the process beyond just the severe losses. However, there is a risk of over-reliance on LLMs for fully automating hazard analysis. We envision LLMs as support tools, not replacements for developers' critical judgment. While LLMs can produce ideas and explore broadly by following checklists, developers are essential for thinking beyond suggestions, assessing severity, and setting the analysis focus. Balancing LLM support while preserving human cre-ativity and judgment is a crucial question for future research.\nExpertise Requirements: Automated assistance in hazard analysis can enhance accessibility by offering step-by-step guidance and examples to developers lacking safety engineer-ing training. However, there is a risk that these developers may perform a more superficial analysis. Ideally, increased accessibility enables selective engagement of safety experts, allowing their expertise to scale across more projects. Balanc-ing this requires further research, potentially drawing insights from similar discussions in software security [35], [36] and democratizing data science [37], [38].\nCataloging Common Controllers: ML research and much software engineering research on ML primarily focus on the model to improve accuracy, measure fairness, explain model internals, or rectify shortcuts. In contrast, system-level mitiga-tions such as safeguards, user interface design, and human oversight receive less attention, despite their effectiveness as controllers [39]. Hazard analysis promotes broader system thinking, and we believe a pattern catalog of common system-level interventions could foster discussions and be embedded into the process of identifying and designing controllers.\nFostering Adoption: Interview studies indicate practi-tioners hesitate to integrate hazard analysis into their agile and exploratory workflows, citing the need for organizational changes such as increased incentives, managerial confidence, support, understanding, and resource investment [11], [13]. Further research is required to embed hazard analysis into routine engineering, even in non-safety-critical contexts where success is a lack of issues. Lowering effort and demonstrating value, while appealing to developers' responsibility and mas-tery, may effectively change practices and culture. While much work remains, we can learn from past efforts such as DevOps [40], establishing a quality or security culture [41], adopting fairness audits [42], promoting social responsibility through education [43], and broadly altering organizational culture [44]. Technological innovations, education, and activism can each promote more responsible engineering practices."}]}