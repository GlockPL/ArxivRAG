{"title": "Causal Interventions on Causal Paths: Mapping GPT-2's Reasoning From Syntax to Semantics", "authors": ["Isabelle Lee", "Joshua Lum", "Ziyi Liu", "Dani Yogatama"], "abstract": "While interpretability research has shed light on some internal algorithms utilized by transformer-based LLMs, reasoning in natural language, with its deep contextuality and ambiguity, defies easy categorization. As a result, formulating clear and motivating questions for circuit analysis that rely on well-defined in-domain and out-of-domain examples required for causal interventions is challenging. Although significant work has investigated circuits for specific tasks, such as indirect object identification (IOI), deciphering natural language reasoning through circuits remains difficult due to its inherent complexity. In this work, we take initial steps to characterize causal reasoning in LLMs by analyzing clear-cut cause-and-effect sentences like \"I opened an umbrella because it started raining,\" where causal interventions may be possible through carefully crafted scenarios using GPT-2 small. Our findings indicate that causal syntax is localized within the first 2-3 layers, while certain heads in later layers exhibit heightened sensitivity to nonsensical variations of causal sentences. This suggests that models may infer reasoning by (1) detecting syntactic cues and (2) isolating distinct heads in the final layers that focus on semantic relationships.", "sections": [{"title": "1 Introduction", "content": "As transformer-based large language models (LLMs) scale up, their performance on diverse downstream tasks has shown remarkable improvement [Wei et al., 2022a, Srivastava et al., 2022]. These models demonstrate remarkable capabilities across various tasks, from reasoning tasks such as math problem solving and commonsense reasoning to question-answering that require knowledge synthesis Kojima et al. [2022], Zellers et al. [2018], Wei et al. [2022b], Brown et al. [2020]. Understanding and benchmarking these capabilities has become a prolific research area, as both technical communities and the general public uncover new ways to harness LLMs. Despite these impressive abilities, however, the mechanisms driving these capabilities remain largely opaque.\nAs model scales increase, interpreting their associated capabilities becomes increasingly challenging. Nevertheless, notable advancements in interpretability have improved our understanding of these models. Recent work in mechanistic interpretability takes a microscopic approach to analyze models [Olah et al., 2020]. Many studies derive interpretable features and behaviors from attention mechanisms using simplified toy models of transformers, revealing concepts like induction heads and in-context learning [Olsson et al., 2022, Elhage et al., 2021]. Although these insights shed light on interpretable, microscopic mechanisms like feature recognition and copying, they fall short in explaining complex, high-level behaviors in realistic tasks. A major reason for this is that circuits rely on causal interventions, which require clear distinctions between in-domain and out-of-domain examples. However, many natural language tasks are complex and inherently ambiguous; for instance,"}, {"title": "2 Overview", "content": "We explore how LLMs understand reasoning by examining their responses to sentences with straightforward reasoning structures. We conduct our experiments with GPT-2 small, a 12-layer model with decoder blocks containing self-attention layers with 12 attention heads and multilayer perceptrons (MLPs) [Radford et al., 2019]. We recognize that humans comprehend reasoning in natural language in two steps. First, by identifying syntactic cues associated with reasoning, such as phrases connected by words like \"because\" and \"so\", we assess whether a sentence likely contains reasoning relations. Next, we consider the semantic relationships within cause-and-effect phrases. Our experiments are designed to reflect this two-step reasoning process. For syntactic analysis, we use a dataset of diverse sentence structures (see Table 1). For semantic analysis, we modify cause-and-effect phrases in templated sentences (see Table 2) to make the reasoning relations either coherent or nonsensical."}, {"title": "3 Where Is Syntax in a Transformer?", "content": "To locate syntactical knowledge in GPT-2, we analyze the model responses to a curated synthetic dataset of causal sentences with varying syntax. We generated the dataset by prompting the language models with multiple templates, as summarized in Table 1. We assess attention patterns based on the causal phrases and delimiters, following an approach similar to the syntactical analysis performed by Vig and Belinkov [2019] on BERT.\nSetup and Methods The templates used to generate the syntactical dataset in Table 1 show the syntactical structure of the sentences in the form of $[e_1,..., e_n, d, c_1,... c_m]$ or $[c_1,\u2026\u2026 c_m, d, e_1,\u2026\u2026,e_n]$ where $c_i$ = tokens of a cause phrase, $d$ = causal delimiter token, and $e_j$ = tokens of an effect phrase. Respectively, the first template refers to \"because\" sentences and the second template refers to \"so\" sentences. An example of such causal sentence is \"Alice went to the craft fair because she wants to buy handmade gifts.\" Then, we specifically analyze the attention maps by calculating 1) how much attention is paid to the causal delimiters and 2) how much effect token attends to cause tokens. We calculate 1) as\n$P_d = \\frac{\\sum_{j=1}^{m} a_{d,j}}{\\sum_{j=1}^{n+m+1} \\sum_{i=1}^{n+m+1} A_{i,j}}$\nwhere $a_{i,j} = [softmax (QK^T/\\sqrt{dK}) V]_{i}$, with query $Q$, key $K$, and value $V$ matrices calculated from the input tokens with attention weights with $1/\\sqrt{dk}$ as a scaling factor calculated from the dimension of the key matrix. We then calculate 2) proportion of cause-to-effect or effect-to-cause attention similarly. As described in Figure 1a, we isolate the cause-to-effect or effect-to-cause attention patterns by masking. The proportion of causal attention pattern can be expressed as\n$P_c = \\frac{\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\alpha_{i,j}}{\\sum_{j=1}^{n+m+1} \\sum_{i=1}^{n+m+1} A_{i,j}}$\nWith isolated causal attention map, we perform statistical analyses per head per layers."}, {"title": "3.1 Results", "content": "In order to analyze syntactical understanding of GPT-2, we first compute the proportion of attention paid to causal delimiters, $P_d$, such as \u201cbecause\" and \"so.\" Figure 4 summarizes the results, which shows that the heads that pay attention to delimiters are spread across the layers with some concentrations in the earlier layers of a transformer. On the other hand, Figure 2 shows that the heads that pay particular causal attention, $P_c$, tend to be most concentrated in the first 2-3 layers."}, {"title": "4 Locating Semantics: Where Does GPT-2 Figure Out a Sandwich Is for Eating, not Singing?", "content": "We also consider logit analysis at each layer of the model to analyze model behavior with causal sentences. From the residual stream, we calculate the per token loss at each layer, which we define"}, {"title": "4.1 Activation Patching Results", "content": "We apply activation patching to contrastive pairs of causal sentences. As outlined in 1b, we first run our model using an original causal sentence. Next, we introduce a semantic perturbation by replacing the sentence with its contrastive pair and rerun the model. By tracking the activation differences that result in changes to the final logit predictions, we pinpoint specific model components responsible for distinguishing causal semantics from random semantics.\nAs shown in Figure 3, few distinct attention heads in the middle to last few layers contribute most to the logit difference, especially layer 11 head 2, layer 10 head 0, and layer 8 head 8, light up in most templates. We also note that in the residual stream, the \u201cPERTUBRED\u201d token significantly influences predictions in the earlier layers, as shown in Figures 6, 7, 8, 9, and 10."}, {"title": "5 Conclusion", "content": "Our investigation suggests that the model demonstrates a syntactic focus in its initial layers, with attention mechanisms primarily engaging at this stage. As processing deepens, a shift occurs, and the model begins to handle reasoning tasks in a more semantic manner, particularly in the later layers. These findings are evident in cases of clear-cut reasoning, where causal relationships can"}, {"title": "6 Related Work", "content": "Reasoning in LLMs LLMs have demonstrated remarkable \"emergent\" abilities for which they were not explicitly trained, though mechanisms behind them are not well understood [Wei et al., 2022a, Schaeffer et al., 2023, Lu et al., 2023]. Among them are LLMs' ability to reason in many domains from informal, commonsense reasoning [Kojima et al., 2022, Bhagavatula et al., 2019, Zellers et al., 2018] to more formal domains such as scientific reasoning [Lu et al., 2022, Birhane et al., 2023] and mathematical reasoning [Cobbe et al., 2021, Yuan et al., 2023]. Behavioral studies have focused significant recent efforts in characterizing and benchmarking model capabilities [Srivastava et al., 2022, Huang et al., 2023], but they are not well connected to the intermediate representations and internal responses of a model. Our work provides first steps in connecting behavioral observations to internal and mechanical model responses with curated tasks.\nAttention Analysis and Mechanistic Interpretability Attention maps have been used for interpreting intermediate representations and behaviors of transformers since the transformer architectures took off in language modeling [Jain and Wallace, 2019, Wiegreffe and Pinter, 2019, Clark et al., 2019, Rogers et al., 2020]. Analyzing what the language models pay attention to when making predictions can elucidate relevant features for particular labels. Recently, work in mechanistic interpretability largely approximated transformers with simplified attention matrix multiplications to describe \"circuits\" [Elhage et al., 2021]. \u201cCircuits\" in LLMs can be thought of as information flow through a transformer that make certain decisions or perform a particular task."}, {"title": "Causal Tracing (Activation Patching) and Causal Intervention", "content": "While many recent behavioral characterizations of LLMs rely on post-hoc benchmarking, some interpretability methods actively engage with model responses. For instance, counterfactual perturbations on input data have been used to study subject-verb agreements in BERT by tracing model responses to particular input representations [Ravfogel et al., 2021, Elazar et al., 2022]. First introduced by Meng et al. [2022], activation patching causally traces the effect of perturbed input token on the activations throughout the layers and eventually on the predicted output token. Activation patching has been used to locate factual information in a transformer in the case of Meng et al. [2022], and it is frequently used for identifying circuits in LLMs. Wang et al. [2022] used activation patching to identify a circuit that performs the \"indirect object identification task,\" in which a model predicts the name as object of an action given the previous context."}, {"title": "A Dataset", "content": "The datasets for syntactical and semantic analysis are generated using templates which are detailed in Table 1 and in Table 2 respectively."}, {"title": "B Proportion of Attention Paid to Delimiters", "content": "Heatmap of the proportion of attention paid to causal delimiters such as \"because\" and \"so\" in GPT-2."}, {"title": "C Logit Analysis with Semantic Perturbation", "content": "Per token loss on sentence: 'we went shopping because we were bored.'"}, {"title": "D Activation Patching By Model Components", "content": "Activation Patching Per Block"}]}