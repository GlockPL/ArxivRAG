{"title": "Deploying Ten Thousand Robots: Scalable Imitation Learning for Lifelong Multi-Agent Path Finding", "authors": ["He Jiang", "Yutong Wang", "Rishi Veerapaneni", "Tanishq Duhan", "Guillaume Sartoretti", "Jiaoyang Li"], "abstract": "Lifelong Multi-Agent Path Finding (LMAPF) is a variant of MAPF where agents are continually assigned new goals, necessitating frequent re-planning to accommodate these dynamic changes. Recently, this field has embraced learning-based methods, which reactively generate single-step actions based on individual local observations. However, it is still challenging for them to match the performance of the best search-based algorithms, especially in large-scale settings. This work proposes an imitation-learning-based LMAPF solver that introduces a novel communication module and systematic single-step collision resolution and global guidance techniques. Our proposed solver, Scalable Imitation Learning for LMAPF (SILLM), inherits the fast reasoning speed of learning-based methods and the high solution quality of search-based methods with the help of modern GPUs. Across six large-scale maps with up to 10,000 agents and varying obstacle structures, SILLM surpasses the best learning- and search-based baselines, achieving average throughput improvements of 137.7% and 16.0%, respectively. Furthermore, SILLM also beats the winning solution of the 2023 League of Robot Runners, an international LMAPF competition sponsored by Amazon Robotics. Finally, we validated SILLM with 10 real robots and 100 virtual robots in a mockup warehouse environment.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-Agent Path Finding (MAPF) [1] is the problem of finding collision-free paths on a given graph for a set of agents, each assigned a start and goal location. Lifelong MAPF (LMAPF) [2] extends MAPF by continually assigning new goals to agents that reach their current ones. The main target of LMAPF is to maximize the throughput, which is defined as the average number of goals reached by all agents per time step. LMAPF has wide applications in the real world, such as automated warehouses, traffic systems, and virtual games. For example, Amazon fulfillment centers were reported to have more than 4,000 robots deployed in 2022 [3]. With growing demands in our daily life, larger autonomous systems are expected to be deployed in the near future. Therefore, more and more scalable search-based solvers have been developed for LMAPF in recent years [4], [5], [6], [7]. Existing search-based solvers [6], [7] can nowadays scale to thousands of agents.\nSince learning-based LMAPF solvers\u00b9 are expected to be more decentralized and scalable, numerous studies on"}, {"title": "II. BACKGROUND", "content": "The LMAPF problem includes a 4-connected grid graph G = (V, E) and a set of n agents A = {a\u2081, ..., an}, each with a unique start location. The vertices V of the graph G correspond to locations (namely unblocked grid cells), and the edges E correspond to connections between neighboring locations. Time is discretized into timesteps. At each timestep, an agent can move to an adjacent location by one of four move actions (up, down, left, right) or wait at its current location. During execution, we disallow vertex collisions and edge collisions. A vertex collision occurs when two agents occupy the same location at the same timestep. An edge collision occurs when two agents traverse the same edge in the opposite directions at the same timestep.\nLMAPF requires real-time planning as agents are con- tinuously assigned new goals. It assumes that an external task assigner handles goal assignments, with each agent knowing its next goal only upon reaching its current one. An LMAPF instance terminates after a predetermined number of timesteps. The objective is to plan collision-free paths for all agents while maximizing throughput, i.e., the average number of goals reached per timestep."}, {"title": "III. RELATED WORK", "content": "PRIMAL [19] is the pioneering work that adopts learning to solve the MAPF problem, which exploits the homogeneity in MAPF to train a decentralized policy shared by all agents. Subsequent research has focused on improving it from four main directions: enabling communication between agents [20], [10], incorporating global guidance into agents' field of view (FoV) [21], [12], enhancing collision resolution efficiency [10], and imitating search-based MAPF algorithms [8], [22]. In Table I, we summarize the learning-based solvers proposed in recent years that are commonly used as baselines, and in each subsection of Section IV, we explain the improvements we made to each technique."}, {"title": "IV. METHODS", "content": "In this section, we first describe the network structure of SILLM, including the Spatially Sensitive Communication (SSC) module, in Section IV-A. Then, we discuss three types of global guidance in Section IV-B. Finally, we introduce the inference and training procedure in Sections IV-C and IV-D."}, {"title": "A. Neural Policy with Spatially Sensitive Communication", "content": "Communication is an important aspect of neural network design in multi-agent systems. As shown in the second row of table I, existing works mostly adopted attention- based communication (ABC) to aggregate information from neighboring agents, which only implicitly reason with the spatial information. However, we argue that precise spatial information benefits local collision avoidance, as similar designs, such as the conflict avoidance table [27], are frequently applied in search-based solvers. Therefore, we propose a spatial sensitive communication (SSC) module that explicitly preserves the spatial relationship between agents when aggregating information.\nFigure 2 illustrates the core structure of our neural net- work. The neural network comprises two Convolutional Neural Network (CNN) modules and a Spatially Sensitive Communication (SSC) module. Since all agents are homo- geneous in LMAPF, they share the same network weights."}, {"title": "B. Providing Global Guidance with Heuristics", "content": "Many learning-based works incorporate global guidance to help agents move toward their goals more easily, as summarised in the third row of Table I. Some works try to follow a specific shortest path [21], [23]. However, since the shortest path might not be unique, other works encode the shortest distance from every location in FoV to the goal, computed by running a Dijkstra's algorithm backward from the goal [20], [10], [26], [25], [11]. A recent paper, Follower [12], tries to follow a shortest path that considers local traffic and is replanned at each timestep. However, no existing work reduces global traffic, which is important for systems with large agent numbers.\nWe systematically study three types of global guidance, represented as heuristics. The first type of heuristics is the"}, {"title": "C. Safeguarding Single-Step Execution with CS-PIBT", "content": "Our neural policy has one remaining issue during in- ference: the generated actions may still contain collisions as they are independently sampled by agents. Most earlier works address it by \"freezing\" the agents' movements that potentially lead to collisions by replacing their original actions with wait actions [19], as shown in the fourth row of Table I. However, this approach is inefficient as many unnecessary wait actions are introduced. It can also lead to deadlocks sometimes when agents repeatedly select the same conflicting actions. To mitigate this issue, SCRIMP [10] allows agents to reselect actions based on their action probabilities to avoid the original conflicts, but this has trouble scaling to many agents. Collision Sheild PIBT (CS- PIBT) [17] proposes to use PIBT [5] to resolve collisions.\nIn PIBT, each agent is assigned a priority. At each step, every agent ranks its actions in ascending order of the shortest distance from the resulting location to its goal (i.e., the BD heuristic). An agent always takes its highest-ranked action that does not collide with any higher-priority agent. If no collision-free actions exist for a low-priority agent, PIBT triggers a backtracking process, forcing the higher-priority agent to take its next best action until all agents can take collision-free actions. Given an agent's action probabilities from the neural network, CS-PIBT converts them into a ranking (by biased sampling) and runs PIBT with this action preference to get 1-step collision-free actions.\nWe directly applied this idea in our work with slight modifications; unlike CS-PIBT, which safeguards the in- ference of a policy trained by other methods, this work holistically considers training and inference with it. We adopt a simplified variant of CS-PIBT, which always prioritizes the learned action output by the neural policy and then ranks other actions as the original PIBT. If the learned actions are collision-free, then CS-PIBT will not change them. We still call this safeguarding procedure CS-PIBT but name the combination of the neural policy and CS-PIBT as Learnable PIBT (L-PIBT) from the perspective of learning."}, {"title": "D. Imitation Learning From A Scalable Search-Based Solver", "content": "We choose imitation learning because it is generally easier to learn team cooperation from mature search-based solvers, while reinforcement learning needs to explore the vast joint action space of multiple agents. A few earlier works have applied imitation learning, and they primarily focus on mimicking weak bounded-suboptimal algorithms, such as ODrM* [14] or ECBS [30], as shown in the last row of Table I. As a result, they are typically constrained to small-scale instances due to the heavy computation in data collection. For instance, PRIMAL2 [8] and SCRIMP [10] were trained on instances with 8 agents, while MAGAT [22] was trained on instances with 10 agents. When applying these solvers to large-scale instances, the significant differences between the original training setup and the actual application scenarios often result in a substantial decline in performance.\nTo address this issue, we directly imitate an anytime search-based algorithm, Windowed MAPF-LNS (W-MAPF- LNS), the central part of the winning solution WPPL [7]. It is unbounded suboptimal but scales well to large instances due to its planning window and anytime behavior.\nThe data collection procedure for imitation learning is shown in Figure 3. Given the observation at the current step T, Learnable PIBT is called w times to generate the initial w-step paths for all agents. Then, we apply W-MAPF-LNS to refine their w-step paths for k iterations. At each iteration, W-MAPF-LNS selects a small group of agents heuristically and tries to improve their w-step paths. Specifically, W- MAPF-LNS tries to optimize agents' w-step paths for an approximate objective:\n$Obj = \\sum_{i=1}^{n}(\\sum_{t=T}^{T+w-1}Cost(v_t^i,v_{t+1}^i) + h(v_{T+w}^i)),$\nwhere $v_t^i$ is agent $a_i$'s location at step t. The Cost function records the edge cost from one location to a neighbor, and $h(v_{T+w}^i)$ is the heuristic value in Section IV-B, which esti- mates the future cost from $v_{T+w}^i$ to agent $i$'s goal. Notably, the heuristics are consistently used in the observation and objective to make policy learning easy. Empirically, we set w = 15 and k = 5,000 so that the planning time at each step is less than 1 second during data collection.\nThen, we collect the first actions in the refined w- step paths with the current observation for later supervised training of Learnable PIBT. Notably, the imitation learning procedure can be repeated iteratively in a self-bootstrapping manner. After an iteration of supervised training, we ob-"}, {"title": "V. EXPERIMENTS", "content": "Applying imitation learning directly in large-scale settings is possible, but complex engineering is required to deal with the large memory consumption during training. Therefore, we first downscaled the large maps to small ones but kept the original obstacle patterns. We trained the neural policy with 600 agents on each small map (500 timesteps for all maps) but evaluated it with 10,000 agents on the corresponding large map (3200 timesteps for Sortation and Warehouse and 2500 timesteps for others). More details of each map are covered in Table II. We provide more experiments, includ- ing the evaluation of different agent numbers and another benchmark used by Follower [12] in the project webpage4. The code will be released upon acceptance."}, {"title": "A. Main Results", "content": "This subsection compares different variants of our solver, SILLM, with other state-of-the-art search- and learning- based solvers. Specifically, we compare with SCRIMP [10] and Follower [12] for learning and with PIBT [5], RHCR [4], and TrafficFlow [6] for search. In addition, we also compare"}, {"title": "B. Ablation Study", "content": "We show ablation results in Figure 5. We first compare our spatially sensitive communication (SSC) with attention- based communication (ABC) and no communication (None) trained by imitation learning (IL). SSC consistently outper- forms ABC and None, indicating the importance of precise spatial reasoning in LMAPF. We further compare our IL with a reinforcement learning (RL) implementation based on MAPPO [31]. Interestingly, we find our IL outperforms the simple MAPPO. We also tried to apply MAPPO after IL but did not notice any improvement. We believe more sophisticated RL methods are needed, especially those can properly distribute team-level rewards to individuals, which is left for future work."}, {"title": "C. Real-World Mini Example", "content": "Due to hardware and software limitations, we validate our solver with 10 real robots and 100 virtual robots in challeng- ing mini warehouse environments with multiple corridors. Details can be found in the video demo."}, {"title": "VI. CONCLUSION", "content": "In this work, we show how to scale learning-based solvers to manage a large number of agents within reasonable plan- ning times. Specifically, we design a unique communication module, incorporate efficient single-step collision resolution and different types of global guidance, and use scalable imitation learning with a state-of-the-art LMAPF solver. As a result, our proposed learning-based solver, SILLM, can effectively plan for 10,000 agents in under 1 second for various maps. It outperforms previously best learning- and search-based solvers and validates the potential of applying learning for large-scale LMAPF instances. Future work will explore how to further improve SILLM using RL."}, {"title": "APPENDIX", "content": "A. Visualization of all maps\nWe visualize all the large maps for evaluation in fig. 6 and all the down-scaled small maps for training in fig. 7, which keep the obstacle patterns in the corresponding large maps."}, {"title": "B. Evaluation with Different Numbers of Agents", "content": "In this section, we compare Learnable PIBT and PIBT with different global guidance and different numbers of agents. The conclusions are similar to the ones in section V. With the same global guidance, Learnable PIBT consistently outperforms PIBT, proving the effect of learning. Also, different global guidance excels in different scenarios. An"}, {"title": "C. Evaluation on Learn-to-Follow Benchmark", "content": "This section compares different decentralized methods with the Learn-to-Follow Benchmark [12] to validate the superiority of our SILLM (Learnable PIBT). Specifically, we compare Learnable PIBT trained with imitation learning and with reinforcement learning, Follower [12], SCRIMP [10], and PIBT [5]. For a simple comparison, we only use Back- ward Dijstraj as global guidance. All the training settings are the same as the ones in the Follower paper [12]. Notably, Learnable PIBT and Follower are trained on 40 Mazes maps and tested on 10 test maps and other maps. Our Learnable PIBT trained with imitation learning performs the best consistently across 4 different kinds of maps. Notably, Follower actually only outperforms PIBT in Mazes maps but may fail to outperform PIBT in other maps, which means the"}, {"title": "D. Real-World Mini Example", "content": "Since during the planning process, our algorithm assumes the position of all agents to be perfectly known at all times, we use ground truth positions for our virtual robots and use external localization (here, the Optitrack Motion Capture System) to obtain accurate position information for our real robots. However, the planned path may not be executed accurately due to disturbances and control inaccuracies. To eliminate these errors, we implement an Action Dependency Graph (ADG) [32], [33].\nThe video demo is available in the supplementary material. From our experiment with 10 real agents, we observe that agents can reach their goals quickly without collisions, and"}, {"title": "E. Computation Resources", "content": "Our models are trained on servers with 72 vCPU AMD EPYC 9754 128-Core Processor, 4 RTX 4090D (24GB), and 240 GB memory. Training on each map takes less than 12 hours."}, {"title": "F. Reinforcement Learning Implementation", "content": "In the ablation study (Section V-B), we show that Imitation Learning is better than simple MAPPO-based Reinforcement Learning. Specifically, we use the following reward function.\n$r(v, v') = h(v) \u2013 h(v') \u2013 1$                                                        (1)\nwhere h is the heuristic function defined in Section IV-B, v and v' are the current and next locations of an agent. Take Backward Dijkstra heuristics as an example. If v' is 1-step closer to the goal, the reward will be 0. Otherwise, the reward will be a negative penalty. If no other agents act as obstacles, the agent should follow its shortest path given this reward function after learning. Reward design is crucial to the performance of RL and worth further study."}, {"title": "G. Detailed Comparison for Different Guidance", "content": "We report detailed throughput and average running time for comparing PIBT and L-PIBT with different guidance in Table IV. Notably, in contrast to the conclusion in the large map, Static Guidance works the best among the three heuristics in the small map for training. Overall, since all the heuristics are manually designed, the best heuristic is instance-dependent and should be evaluated empirically. Also, these heuristics only define the framework, but there could be a lot of hyperparameters that affect the performance. Automatic hyperparameter tuning can be helpful but doesn't necessarily remove the structural bias in the framework."}]}