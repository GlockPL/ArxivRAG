{"title": "Synergistic Dual Spatial-aware Generation of Image-to-Text and Text-to-Image", "authors": ["Yu Zhao", "Hao Fei", "Xiangtai Li", "Libo Qin", "Jiayi Ji", "Hongyuan Zhu", "Meishan Zhang", "Min Zhang", "Jianguo Wei"], "abstract": "In the visual spatial understanding (VSU) area, spatial image-to-text (SI2T) and spatial text-to-image (ST2I) are two fundamental tasks that appear in dual form. Existing methods for standalone SI2T or ST2I perform imperfectly in spatial understanding, due to the difficulty of 3D-wise spatial feature modeling. In this work, we consider modeling the SI2T and ST2I together under a dual learning framework. During the dual framework, we then propose to represent the 3D spatial scene features with a novel 3D scene graph (3DSG) representation that can be shared and beneficial to both tasks. Further, inspired by the intuition that the easier 3D image and 3D text processes also exist symmetrically in the ST2I and SI2T, respectively, we propose the Spatial Dual Discrete Diffusion (SD\u00b3) framework, which utilizes the intermediate features of the 3D\u2192X processes to guide the hard X\u21923D processes, such that the overall ST2I and SI2T will benefit each other. On the visual spatial understanding dataset VSD, our system outperforms the mainstream T2I and I2T methods significantly. Further in-depth analysis reveals how our dual learning strategy advances.", "sections": [{"title": "Introduction", "content": "Within the research topic of Visual Spatial Understanding (VSU) [42, 94, 74, 89, 8], Spatial Image-to-Text (SI2T) [94, 96] and Spatial Text-to-Image (ST2I) [58] are two representative task forms across vision and language. SI2T aims to understand the spatial relationships of objects in the given image, while ST2I focuses on synthesizing a spatial-faithful image based on the input text prompts. Existing efforts mostly formalize SI2T and ST2I tasks as the normal I2T and T2I problems, applying general-purpose I2T and T2I models for task solutions [94, 96, 58, 37, 38]. Technically, researchers widely employ the vision-language generative architecture [79, 10, 83] for I2T tasks, i.e., generally, there is a visual encoder and a text decoder. For the T2I task, recent diffusion-based methods have shown the extraordinary capability of image generation and achieved state-of-the-art (SoTA) performance on a mount of benchmarks [4, 12, 28, 71, 63, 86, 18, 11].\nUnfortunately, these strong-performing I2T and T2I methods, without the deliberate spatial semantics modeling in VSU tasks, largely fall short in precisely extracting spatial features from visual or textual inputs. For instance, in the SI2T process, the spatial relationships are often incorrectly recognized due to layout overlap and perspective illusion [96]. This is due to the inherent characteristics of 2D images, which, lacking 3D feature modeling, inevitably fail to understand spatial relations. On the other hand, in ST2I, synthetic images frequently fail to match strictly the spatial constraints specified in the prompts, such as the position, pose, and perspective [58]. This is because language's abstract nature allows only for a general description of the content rather than detailed depictions of spatial scenes. Moreover, unlike general image generation, ST2I should place greater emphasis on 3D spatial modeling, while the input sequential language tokens intrinsically do not portray the kind of specific spatial scene.\nLet us revisit the human process in solving SI2T and ST2I tasks. We typically process the input image or text within our minds by constructing a 3D spatial scene, which serves as the center for further spatial textual descriptions or image generation. Specifically for SI2T, we intuitively project a 2D image into a reasonable 3D scene based on common sense before describing that scene in words. In contrast, for ST2I tasks, we start by imaginatively converting the text of user instruction into a 3D conceptual scene which is then rendered into a 2D image. Interestingly, we can actually find that SI2T and ST2I are dual processes, with each task's input and output being the reverse of the other, as illustrated in Figure 1(a). More importantly, there are two important observations in these dual tasks. First, [intermediate processing sharing], they can complement and benefit each other. For SI2T, the 'Image 3D' reasoning process is challenging in acquiring necessary 3D features, whereas the descriptive '3D\u2192Text' process is relatively easier. Conversely, for ST2I, the 'Text\u21923D' process requires complex reasoning of the 3D scene feature, while rendering \u20183D\u2192Image' is much more straightforward. Ideally, if complementing the information during each learning process, i.e., letting the easy part aid the hard part, it should enhance the performance of both tasks. Second, [3D scene feature sharing], both dual tasks urgently require modeling of the respective 3D features, where such stereospecific insights in 3D perspective can be essentially shared and also complementary between each other. Intuitively, vision offers very concrete clues describing spatial scenes, e.g., objects and attributes, while the 3D features derived from texts are more likely to define the semantics-oriented relations or constraints, which are often abstract and rough.\nInspired by such intuition, in this paper, we introduce a novel synergistic dual framework for SI2T and ST2I. To start with, we propose a spatial-aware 3D scene graph (namely 3DSG), where the spatial objects, their relations and the layouts within the 3D scene are formulated with a semantically structured representation. The 3DSG representation effectively depicts the stereospecific attributes of all objects, and meanwhile models the spatial relations between them, from which both the SI2T and ST2I processes can be beneficial. Technically, 3DSG is obtained from a shared graph diffusion model for both SI2T and ST2I processes, as shown in Figure 1(b). Trained with our \u20182DSG-3DSG' pair data, the graph diffusion is learned to propagate and evolve the initial 2D visual SG (parsed from input image at SI2T side) or the textual SG (parsed from input text at ST2I side) into the final 3DSG, i.e., by adding all necessary and reasonable spatial details.\nWith 3DSG, we next implement the whole dual framework of SI2T and ST2I generation. Instead of directly employing the SoTA generative I2T models or diffusion-based T2I methods, we consider a solution fully based on discrete diffusions [3], due to several key rationales. Primarily, for VSU, the most crucial spatial information that determines a scene consists of objects and their relationships, which presents the characteristic of discretization and combination in the spatial layout, while other background and spatial unrelated information would be noisy. Thus, the discrete representation is more appropriate to model pure spatial semantics in our scenario. Besides, in our scenario both the textual token and SG representations possess discrete characteristics [54], which can be perfectly modeled by discrete diffusion. Moreover, the discrete diffusion works on the limited index space"}, {"title": "Related Work", "content": "Visual Spatial Understanding. VSU is an important topic within the research of multimodal learning [51, 17, 19, 21]. VSU aims to extract spatial information from a given scene, developed within the forms of reasoning [48], relation extraction [56], role labeling [42], question answering [44, 61, 52], image-to-text generation [94, 96], image synthesis [58], 3D reconstruction [74, 69], etc. With the VSU capability, many downstream applications can achieve, such as robotics [24, 53], navigation [33, 80, 6], and language grounding [49]. Among various VSU tasks, the SI2T and ST2I generation attract significant attention due to their fundamental positions in vision-language cross-modal tasks. Current efforts mostly study ST2I or SI2T separately. For the text-to-image generation, the diffusion models have emerged as the SoTA approaches [15]. For image-to-text generation, i.e., image captioning, it has been a long-standing task and has achieved great progress. Recent advances can be largely attributed to vision-language pre-training (VLP) [79, 10]. Besides conventional VLP methods, [98] are the first to use diffusion models for image captioning (and more specifically, for text generation). In this work, we consider the two tasks together under a dual learning framework, via which we aim to achieve mutual benefits of the spatial feature modeling from each other.\nDiscrete Diffusion Models. The diffusion model is first proposed by [32], and has achieved impressive performance for text-to-image generation [63, 12, 71, 65, 86]. Original diffusion models are parameterized Markov chains trained to translate simple distributions to more sophisticated target distributions in a finite set of steps on the continuous data or its latent representations. In this work we adopt the SoTA diffusion-based model as our T2I backbone. Recently, diffusion models on discrete space are introduced to markedly reduce the computing cost [28, 3, 34, 35]. Discrete diffusion methods are also used in text generation [34] and structure generation [54] due to its natural adaptability for the data with discrete nature. This work follows the line and takes the SoTA discrete diffusion model as the backbone for image, text and scene graph generation.\nScene Graph Representations. This work is also closely related to scene graph (SG)-based representation learning. Scene graphs have advanced in depicting the intrinsic semantic structures of scenes in images or texts [43, 81]. In SGs, key object and attribute nodes are connected via pairwise relations to describe semantic contexts, which have been shown to be useful as auxiliary features that carry rich contextual and semantic information for a wide range of downstream applications, such as image retrieval [39], image generation [40, 86], translation [17], image captioning [90, 83] and video modeling [95, 18]. However, research on utilizing 3D scene graph representations to enhance various downstream tasks can be currently very scarce. In this paper, we incorporate both visual and language scene graphs within a 3D scope to enhance cross-modal alignment learning for better spatial semantics understanding.\nDual Learning. The dual learning method is proposed to enhance the coupled tasks that have the same exact input and output but in reverse [88, 91]. With dual bidirectional learning, the model could capture potential mutual information from the primary and dual tasks, improving their performance."}, {"title": "Preliminaries", "content": "3.1 Task Formulation\nWe process the dual visual spatial understanding tasks, ST2I and SI2T. Given a textual prompt Y, ST2I aims to generate an image \u00ce that semantically matches the spatial constraints with Y. Its dual task is SI2T, which is also known as the visual spatial description (VSD), aiming to generate a piece of textual description Y based on an input image I. In this paper, we process the two tasks parallelly.\n3.2 Discrete Diffusion\nDiffusion models [32] are generative models characterized by a forward and reverse Markov process. In the forward process, the given data $x_0$ with distribution $q(x_0)$ is corrupted into a Gaussian distribution variable $x_T$ in T steps, formulated as $q(x_{1:T}|x_0) = \\prod_{t=1}^T q(x_t|x_{t-1})$. In the reverse process, the model learns to recover the original data $x_0$ from $x_T$, denoted as $p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1}|x_t)$. In order to optimize the generative model $p_\\theta(x_0)$ to fit the data distribution $q(x_0)$, one typically optimizes a variational upper bound on the negative log-likelihood:\n$L_{vlb} = E_{q(x_0)} D_{KL} [q(x_T|x_0)||p(x_T)] + \\sum_{t=1}^T E_{q(x_t|x_0)} [D_{KL} [q(x_{t-1}|x_t, x_0)||p_\\theta(x_{t-1}|x_t)]]$\nVanilla diffusion models are defined on continuous space. Recently, the discrete diffusion model has been introduced where a transition probability matrix $Q_t$ is defined to indicate how $x_0$ transits to $x_t$ for each step of the forward process, where $x_t \\in \\mathbb{Z}^N$ is defined in discrete space. The matrices $[Q_t]_{ij} = q(x_t = i|x_{t-1} = j)$ defines the probabilities that $x_{t-1}$ transits to $x_t$. Then the forward and reverse process could be rewritten as $q(x_t|x_{t-1}) = v(x_t)Q_t v(x_{t-1})$ and $q(x_{t-1}|x_t, x_0) = q(x_t|x_{t-1}, x_0)q(x_{t-1}|x_0)/q(x_t|x_0)$, where $v(x)$ means the one-hot representation of $x$. Appendix A.1 gives more technical details about the discrete diffusion models.\n3.3 Scene Graph Representation\nThe scene graph presents a scene via the objects with their attributes and relationships in the form of a graph, which can be constructed from either text (TSG) [68] or image (VSG) [92, 46, 45]. We represent a scene graph as G = {V, E}, where V is the node set, and E is the edge set. There are three types of nodes in the scene graph: object nodes, attribute nodes, and relationship nodes. Each type of node has ts own unique tag vocabulary.\nFor SI2T and ST2I, we focus on spatial information extraction, especially the object relationships in the 3D space. However, due to the limited information presented by text or image, a 2D TSG or VSG hardly fully models these 3D spatial semantics. Here, we introduce the spatial-aware 3D scene graph (3DSG) [41], which thoroughly depicts 3D scenes. The 3DSG is formally equivalent to the 2D scene graph, i.e. the object, attribute, and relationship, but organized"}, {"title": "Methodology", "content": "4.1 Spatial Dual Discrete Diffusion Framework\nIn Figure 2, we illustrate the overall framework of the proposed Spatial Dual Discrete Diffusion (SD\u00b3), consisting of three separate discrete context diffusion models, i.e., the 3DSG diffusion model for the X 3D process, the ST2I diffusion model for the 3D Image process, and the SI2T diffusion model for the 3D Text process.\nX\u21923D: 3DSG Diffusion Model. We set a graph diffusion model to convert the initial TSG (for ST2I) and VSG (for SI2T) to the 3DSG. We first acquire the initial VSG and TSG from the input image and textual description through the off-the-shelf VSG [68] and TSG [92] parsers. Then the initial VSG and TSG nodes (represented as {$e_{obj}, e_{attr}, e_{rel}$}) are encoded to the latent quantized representations by a discrete graph auto-encoder (DGAE) [54], on which the discrete diffusion process on discrete graph representations is conducted to generate the 3DSG. Concretely, we denote the quantized gold 3DSG as $z_f^G$. Then, following conventional discrete diffusion, we can calculate $p^{Graph}(z_{t-1}|z_t^G)$. Moreover, on our final dual training, the 3DSG diffusion model leverages the intermediate feature of the following 3D\u2192X process to aid the TSG\u21923DSG and VSG\u2192 3DSG generation process. For TSG 3DSG, the model takes global text features $c^Y$ and the intermediate feature of the dual SI2T diffusion, i.e., $p^{Graph} (z_{t-1}^G|z_t, c^Y)$, $p^{Graph}(z_{t-1}|z_t^G, z_t^\\ddagger)$, and for VSG\u21923DSG it will be $p^{Graph} (z_{t-1}^G|z_t, c^I)$, $p^{Graph}(z_{t-1}|z_t^G, z_t^\\ddagger)$, where the $z_t^\\ddagger$ and $z_t^\\ddagger$ are the t step representations of SI2T and ST2I diffusion respectively. Then we align them by:\n$L_{X-T23D} = DKL(p^{Graph} (z_{t-1}^G|z_t, c^Y)||p^{Graph}(z_{t-1}|z_t^G, z_t^\\ddagger))$,\n$L_{X-I23D} = DKL(p^{Graph} (z_{t-1}^G|z_t, c^I)||p^{Graph}(z_{t-1}|z_t^G, z_t^\\ddagger))$.\nTo enable the training for this 2DSG\u21923DSG process, we should construct the paired '2DSG-3DSG' data. We follow previous work [72, 78, 27] to construct a gold 3DSG dataset. After that, we adopt a strong captioning model to generate text descriptions from view images. With all the inputs and\noutputs ready, we can train the graph diffusion model on aligned 3DSG-Image-Text data, after which the model will acquire the 3D estimation capability.\n3D Image: ST2I Diffusion Model. The ST2I diffusion model is used to generate image \u00ce with the condition of input textual prompts Y. In this task, we adopt a vector quantized variational autoencoder (VQ-VAE) [14] as the quantized model to encode image data to embedding vectors. We denote the quantized image as $z_f^I$ and we could also calculate $q(z_t^I|z_{t-1})$ and $q(z_{t-1}|z_t^I, z_f^I)$. On the other hand, the ST2I diffusion model takes two conditions, i.e., the global text feature $c^Y$ extracted via a CLIP model [60], and the shared 3DSG feature $c^G$. Then $c^Y$ and $c^G$ are fused and fed to the diffusion model to calculate $p^{ST2I} (z_{t-1}^I|z_t^I, c^Y \\oplus c^G)$. At last, the decoder from VQ-VAE is used to generate images from latent codes.\n3D Text: SI2T Diffusion Model. In the SI2T model, the inputs and outputs are reversed. The diffusion is applied on the text representation $z_t^Y$ with a similar process as ST2I. The latent text representation is the word embedding, which is functionally equivalent to the visual codebook. We use another denoising network $p^{SI2T}(z_{t-1}|z_t^Y, c^I + c^G)$, while the input condition becomes the visual tokens $c^I$ and also the 3DSG feature $c^G$. Afterward, the language model decoder generates textual results based on the latent representations.\nImage and Text Decoding. Finally, through three diffusion models, we acquire the predicted latent representations of image, text, and 3DSG, denoted as $z_t^I$, $z_t^Y$, $z_f^G$. For ST2I, we fuse the generated graph vectors $z_f^G$ to visual vectors $z_t^I$ by adopting an attention mechanism:\n$Attn_{i,m} = softmax(z_t^I[m] \\cdot z_f^G[i]), z_t^{+}[i] = z_t^I[i]) + \\sum_{m} Attn_{i,m} \\cdot z_f^G[m]$,\nwhere $\\oplus$ means the concatenation operation. Afterwards, the image decoder $G_V$ is used to reconstruct image from the scene graph enhanced codes $z_t^{I+}$:\n$\\hat{I} = G_V (I^+)$.\nFor SI2T, we append the flatten $z_f^G$ after generated textual codes $z_t^Y$ and use a language model decoder $G_T$ to generate the description:\n$\\hat{Y} = G_T([z_t^Y; z_f^G])$.\n4.2 Mutual Spatial Synergistic Dual Learning\nTo exploit the complementarity of the dual processes, we elaborate a dual training strategy for our framework. To conduct the strategy effectively, we introduce the essential training objectives.\nDual Learning Objective. Our dual learning framework contains two tasks, i.e. ST2I and SI2T, where their inputs and outputs are just reversed. We denote ST2I as $f_\\Theta : i \\rightarrow y, i \\in I, y \\in Y$ and SI2T as $f_\\Phi: y \\rightarrow i, i \\in I, y \\in Y$. Their learning objectives should be:\n$L_{ST2I} = E_{i,y} log p_\\Theta (y|i), L_{SI2T} = E_{i,y} log p_\\Phi (y|i)$.\nBased on the dual supervising learning [88], given the duality of the two tasks, if the learned ST2I and SI2T models are perfect, we should have the probabilistic duality:\n$p(i)p_\\Theta(y|i) = p(y)p_\\Phi(i|y) = p(i, y), \\forall i, y,$\nwhere $p(i)$ and $p(y)$ are the marginal distributions. Then we add this constraint to the dual learning target as an equivalent regularization term:\n$L_{dual} = L_{ST2I} + L_{SI2T} + || log\\hat{p}(i) + log p_\\Theta (y|i) - log \\hat{p}(y) - log p_\\Phi(i|y)||$,\nwhere $\\hat{p}(i)$ and log\u00f4(y) are the estimated marginal distribution by a pre-trained vision-language model, which is illustrated detailly in Appendix \u00a7A.4.\nLoss for Mutual 3D Feature Learning. The 3DSG diffusion model is used to estimating 3D scene information from TSG or VSG, denoted as $f_\\Theta: i\\rightarrow g$ and $f_\\Phi: y \\rightarrow g$, where $i \\in I, y \\in Y, g \\in G$. Based \u00a74.1 and Eq. 1, the graph diffusion loss can be acquired, denoted as $L_{I23D}$ and $L_{T23D}$\n$L_{I23D} = E_{i,y} log p_\\Phi (g|i), L_{T23D} = E_{i,y} log p_\\Theta (g|y),$"}, {"title": "Experiment", "content": "5.1 Experiment Settings\nDataset. To demonstrate the capability of our proposed method for both ST2I and SI2T generation, we conduct experiments on the VSD [94, 96] dataset, which is constructed for visual spatial understanding. The VSD dataset contains about 30K images from SpatialSense [89] and Visual Genome"}, {"title": "Quantitative Results", "content": "Main Comparisons. The main results are shown in Table 2 We first compare some strong baselines of I2T and T2I methods. Overall, the ST2I results on VSDv2 are consistently better, and the SI2T results on VSDv1 are better. This is because the VSDv2 contains more complicated and detailed textual descriptions, which is beneficial to ST2I but challenging for ST2I. We see that the proposed SD\u00b3 substantially outperforms the compared baselines on both VSDv1 and VSDv2. For the ST2I task, we outperform Frido by 1.82% FID, 3.28% IS. and 3.66% CLIP score on VSDv1 and 1.32% FID, 3.74% IS and 4.09% CLIP score on VSDv2 For the SI2T task, we surpass 3DVSD by 1.38% BLEU4 and 0.74% SPICE on VSDv1 and 1.23% BLEU4 and 1.06% SPICE on VSDv2. The results directly demonstrate the efficacy of our method.\nWe further show the key module ablation studies on the last four lines in Table 2. First, removing the 3DSG integration (\u201cVanilla Dual Learning\"), i.e., train the SI2T and ST2I diffusion with loss of Eq. 9, the performance drops much on main metrics, indicating the critical influence of the 3DSG guidance. Also, the drops of \"Singleton+3D\" reveal the effectiveness of our dual learning strategy.\""}, {"title": "In-depth Analyses and Discussions", "content": "Previously, we have verified the effectiveness of our dual learning by thorough numerical evaluations. Now, we explore how our methods advance via the following research questions.\nHow does 3DSG guidance aid the generation of spatial-faithful image and text? First, after removing the 3DSG integration, comparing the last two lines in Table 2, we can see that the 3DSG feature contributes great influence. Further, we follow GLIP [47] to assess how well the objects-"}, {"title": "Conclusion", "content": "In this work, we study the ST2I and SI2T tasks under a dual learning framework. We first propose the spatial-aware 3D scene graph to model the 3D feature which is essentially shared by ST2I and SI2T. The 3DSG is constructed by being initialized with a 2D TSG/VSG and then evolved to the 3DSG by a graph diffusion model. We then leverage the dual learning to enhance the 2DSG\u21923DSG evolving by sharing clues of the dual 3DSG\u2192Image/Text process, through which the 3DSG diffusion model is adequately guided and then facilitates the whole ST2I and SI2T. On the VSD dataset, our method shows great superiority in both SI2T and ST2I. Further analyses demonstrate how our dual learning method could capture 3D spatial structures and then help generate spatial-faithful images and texts.\nLooking forward, there can be quite rich explorations in future research. First, the graph diffusion model highly relies on the quality of 3DSG training data, which has significant impact to the final performance of our method. Correspondingly, we in the future will explore the data augmentation methods to optimize the training. Second, the evaluation for spatial understanding of ST2I and SI2T has not been fully explored in this work, i.e., with human evaluation instead. Also, one promising direction for VSU is constructing multimodal large language models (MLLMs) [22, 85, 20, 84, 93, 57, 82], especially for 3D spatial understanding [9]."}, {"title": "Extension of Technical Details", "content": "In this section, we introduce the specific details of our method which we cannot present in the main article due to the space limit.\nA.1 Discrete Diffusion\nWe first introduce the discrete representation of continuous data. Given data \u00e6 in a continuous space, a vector quantized model (VQM) is employed to encode x to embedding vectors. A VQM contains an encoder E, a decoder G and a pre-trained codebook Z = {$c_k$}$_{K=1} \\in \\mathbb{R}^{K\\times d}$, where Z has a finite number of embedding vectors, K is the size of the codebook and d is the code dimension. Given input x, we obtain a sequence of tokens $z_q$ with the encoder e = E(x) and a quantizer that maps e to its closet codebook entry $c_k$:\n$z_q = Q(e) = \\underset{k \\in \\mathbb{Z}}{argmin} ||e \u2013 c_k ||$\nCorrespondingly, given a quantified tokens $z_q$, the decoder could faithfully reconstruct the data x = G($z_q$).\nWith this discrete representation, the discrete diffusion model can be applied to it. Given the quantized data $z_0$, the forward diffusion process gradually corrupts $z_0$ through the Markov chain q($z_t$|$z_{t\u22121}$), randomly replacing some tokens in $z_{t\u22121}$. After a fixed number of T steps, the model outputs a sequence of increasingly noisy latent variables $z_1$, ..., $z_T$, where $z_t$ is the pure noise tokens. In the reverse process, the model gradually denoise from $z_T$ and reconstruct $z_0$, by sampling from the distribution q($z_{t\u22121}$|$Z_t$, $z_0$) sequentially. Specifically, for a token $z_0$ of $z_0$, $z_0$ takes the index of one entry of codebook, i.e., $z_0$ \u2208 {1, ..., K}. The probabilities of the transition from $z_{t\u22121}$ to $z_t$ can be represented by a matrix $Q_t$(mn) = q($z_t$ = m|$z_{t-1}$ = n) \u2208 $\\mathbb{R}^{K\u00d7K}$. Then the forward diffusion process for $z_t$ is:\nq($Z_t$$Z_{t-1}$) = $v^T$($z_t$)$Q_t v$($z_{t\u22121}$),\nwhere $v$(z) means the one-hot representation of x in K categories. $Q_t v$($z_{t\u22121}$) means the categorical distribution over $z_t$. Due to its Markov property, the q($z_t$|$z_0$) could be written as:\nq($z_t$|$z_0$) = $v^T$ ($z_t$) $(\\prod_{t=1}^T Q_t)$ v($z_0$) \n= $v^T$ ($z_t$)$Q_tv$($z_0$).\nBy applying Bayes' rule, we can compute the posterior q($Z_{t-1}$|$Z_t$, $z_0$) as:\nq($Z_{t-1}$|$Z_t$, $z_0$) = $\\frac{q(z_t|z_{t-1}, z_0)q(z_{t-1}|z_0)}{q(z_t z_0)} \\\\\n= \\frac{(v^T(z_t)Q_{t-1}v(z_{t-1})) (v^T(z_{t-1})Q_{t-1}v(z_0))}{v^T(z_t)Q_tv(z_0)}$\n$Q_t$ is usually defined as the a small amount of uniform noises and it can be formulated as:\n$Q_t = \begin{bmatrix}\n a_t + \u03b2_t & \u03b2_t & ... & \u03b2_t \\\\\n \u03b2_t & a_t + \u03b2_t & ... & \u03b2_t \\\\\n ... & ... & ... & ... \\\\\n \u03b2_t & \u03b2_t & ... & a_t + \u03b2_t\n\\end{bmatrix}$\nwhere $a_t$ \u2208 [0, 1] and $\u03b2_t$ = (1 \u2212 $a_t$)/K, which means each token has a probability of $a_t$ + $\u03b2_t$ to remain the previous value at one step and has a probability of K$\u03b2_t$ to be sampled uniformly over all the K categories.\nThen a noise estimating network $p_\u03b8$($z_{t-1}$|$z_t$, c) is trained to approximate the conditional transit distribution q($z_{t-1}$|$Z_t$, $x_0$) with condition c. The network is trained to minimize the variational lower bound (VLB) [70].\n$L = D_{KL}(q(z_{t-1}|z_t, x_0)||p_\u03b8(z_{t-1}|z_t, c))$"}, {"title": "Discrete Representation for 3DSG", "content": "Following [54], given the 3DSG G, we use a a GNN encoder to encode G to a set of node embeddings $\\mathbb{Z} = \\{z_i\\}$. We subsequently apply a quantization operator, $\\mathbb{F}$, on the continuous node embeddings and map them to fixed points within the same space. The quantization operates independently on each dimension of the latent space, and the quantization of the $j^{th}$ dimension of $z_i$ embedding is given by:\n$z^{L_i}_{ij} = \\mathbb{F}(z_{ij}, L_j) = \\mathbb{R}(\\frac{L_j}{2}tanh \\frac{z_{ij}}{L_j}), 1 \\le j \\le d_z$\nwhere $\\mathbb{R}$ is the rounding operator and $L_j$ is the number of quantization levels used for the $j^{th}$ dimension of the node embeddings. The quantisation operator maps any point in the original continuous latent space to a point from the set $\\mathbb{Z} = \\{z_i\\}$. The discrete latent representation of graph G is the set $\\mathbb{Z}^G = \\{z_i\\}, z^{l_i} \\in \\mathbb{Z}$.\nThe quantization operator is permutation equivariant and so is the mapping from the initial graph G to its discrete representation in the latent space as long as the graph encoder $\\mathbb{D}$ is permutation equivariant. Thus for any $\\mathbb{P}$ permutation matrix the following equivariance relation holds:\n$\\mathbb{P}^T \\mathbb{Z}^G = \\mathbb{F}(\\mathbb{P}^T \\mathbb{Z}) = \\mathbb{F}(\\mathbb{D}(\\mathbb{P}^T E_P, \\mathbb{P}^T X))$\nUsing an equivariant decoder $\\mathbb{D}$($\\mathbb{Z}^G$), which will operate on the assumption of a fully-connected graph over the node embeddings $\\mathbb{Z}^G$, results in reconstructed graphs that are node permutation equivariant with the original input graphs."}, {"title": "Aligned Image-Text-3DSG Data Construction.", "content": "We follow [72] to take the 3D datasets Matterport3D (MP3D) [7] 3DSSG [78] and CURB-SG [27], using the Hydra [36] parser to generate the 3D scene graphs and then leverage large language models to refine it as the ground-truth. Then we use ChatGPT to generate alignede spatial descriptions for the RGB images."}, {"title": "Marginal Distribution Estimation for the Dual Learning Target", "content": "Based on Eq. 9, the marginal distributions p(x) and p(y) can not be aqcired directly. Thus we estimate these marginal distribution p("}]}