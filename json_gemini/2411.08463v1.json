{"title": "SYMBOLIC-AI-FUSION DEEP LEARNING (SAIF-DL): ENCODING KNOWLEDGE INTO TRAINING WITH ANSWER SET PROGRAMMING LOSS PENALTIES BY A NOVEL LOSS FUNCTION APPROACH", "authors": ["Fadi Al Machot", "Martin Thomas Horsch", "Habib Ullah"], "abstract": "This paper presents a hybrid methodology that enhances the training process of deep learning (DL) models by embedding domain expert knowledge using ontologies and answer set programming (ASP). By integrating these symbolic AI methods, we encode domain-specific constraints, rules, and logical reasoning directly into the model's learning process, thereby improving both performance and trustworthiness. The proposed approach is flexible and applicable to both regression and classification tasks, demonstrating generalizability across various fields such as healthcare, autonomous systems, engineering, and battery manufacturing applications. Unlike other state-of-the-art methods, the strength of our approach lies in its scalability across different domains. The design allows for the automation of the loss function by simply updating the ASP rules, making the system highly scalable and user-friendly. This facilitates seamless adaptation to new domains without significant redesign, offering a practical solution for integrating expert knowledge into DL models in industrial settings such as battery manufacturing.", "sections": [{"title": "1 Introduction", "content": "Machine learning models, particularly deep learning (DL), have demonstrated remarkable success across a variety of fields, including image recognition, natural language processing, and predictive analytics [15]. However, in domains where rules, constraints, and logical reasoning are critical, such as healthcare, autonomous systems, engineering, and finance, these models can face significant limitations. Traditional DL models are primarily data-driven and often function as black boxes, lacking the ability to incorporate explicit domain knowledge or reasoning capabilities [7]. One of the key challenges is that DL models may overlook domain-specific knowledge that is not easily captured within the dataset, leading to suboptimal or even incorrect predictions. For instance, a DL model predicting medication dosages could fail to recognize contraindications between medications unless domain knowledge is introduced. This limitation underscores the need for integrating symbolic reasoning and domain knowledge into the learning process to enhance model performance and trustworthiness.\nTo address this challenge, we present a hybrid methodology that integrates deep learning with domain expert knowledge using ontologies [3, 13] and answer set programming (ASP) [12]. Our approach involves embedding domain-specific"}, {"title": "2 Related Work", "content": "Purely data-driven modelling using neural networks is agnostic with respect to the laws and rules underlying the data and, by design, primarily meant for interpolation, with a comparably restricted potential for transfer learning. As a consequence, purely data-driven modelling requires a large volume of input data in order to sufficiently cover the domain within which interpolation is to be facilitated. As another consequence, it may fail to correctly reproduce facts or internal consistency relationships that appear trivial to the end user, creating embarrassing counterexamples. Neural-symbolic approaches seek to bridge these limitations by combining the adaptive learning capabilities of neural networks with the logical, rule-based reasoning of symbolic AI. This integration allows AI systems to tackle a broader range of tasks, encompassing both low-level perception (e.g., image and speech recognition) and high-level reasoning (e.g., inference and abstract concept manipulation) [4]. For instance, neuro-symbolic systems have shown promising results in visual question answering (VQA), where systems must interpret visual data and respond to natural language queries, blending perceptual and symbolic reasoning to achieve accurate results [8].\nVarious symbolic AI models have been developed, each with unique strengths in knowledge representation and reasoning. For instance, logic programming using Prolog [14] supports problem-solving and formal reasoning by allowing the definition of facts, rules, and queries. Similarly, ASP emphasizes nonmonotonic reasoning for dynamic problem-solving in areas such as planning and constraint satisfaction [1]. One significant area of development within neural-symbolic AI is concept learning, which merges the pattern recognition strengths of neural networks with the abstract representation capabilities of symbolic reasoning. This combination is particularly beneficial for compositional reasoning tasks, where the goal is to understand relationships between different concepts. Notable systems, such as the Neuro-Symbolic Concept Learner (NS-CL), effectively integrate scene understanding with symbolic reasoning to perform tasks like concept learning and relational reasoning, thereby enabling AI models to generalize knowledge to new contexts [17].\nA significant advancement in this domain is the advent of deep probabilistic programming languages (DPPLs) [16], which merge neural predicates into a probabilistic logic programming framework. This fusion allows for the flexibility of deep learning while incorporating the structured reasoning strengths of probabilistic inference, facilitating both symbolic and subsymbolic reasoning. NeurASP enhances traditional Answer Set Programs by utilizing neural networks to manage uncertainties in symbolic reasoning, interpreting neural outputs as probabilistic distributions for atomic facts. This method has been applied in areas such as probabilistic knowledge representation and decision-making systems [18]. In addition, physics-informed neural networks (PINNs) [5] are a class of neural networks that integrate fundamental physical laws into the learning process of deep learning models. Instead of relying solely on data to train the model, PINNs incorporate physical principles described by partial differential equations (PDEs) or other mathematical formulations directly into the network's architecture or loss function. This approach ensures that the model's predictions are consistent with known physics, enhancing both accuracy and generalizability.\nIn contrast, our methodology, symbolic-AI-fusion deep learning (SAIF-DL), allows for the seamless integration of new domain knowledge by simply updating the ASP rules within the ontology. This modularity means that the same underlying neural network architecture can be reused across different domains with minimal adjustments. The ease with which domain knowledge can be updated or expanded without altering the core model enhances the scalability and user-friendliness of our approach."}, {"title": "3 Overall Methodology", "content": "The hybrid methodology presented in this work involves the integration of domain expert knowledge using ontologies and Answer Set Programming (ASP) into the training process of deep learning models. This is accomplished by encoding domain-specific constraints and rules into the loss function of the model, which guides the training process.\nThe first step in this approach involves creating or enriching an ontology that captures the key concepts, relationships, and constraints relevant to the domain. Ontologies are enhanced with domain expert input to ensure they accurately reflect the problem space. This knowledge is then encoded into ASP rules, which formalize the constraints as logical expressions. The DL model is trained with a customized loss function that not only minimizes prediction error but also applies a penalty when ASP rules are violated.\nThe ASP rules are used during the training process to evaluate the model's predictions, ensuring that they adhere to domain-specific constraints. If the predictions violate any rules, a penalty is added to the loss function, which helps guide the model to produce more compliant outputs in future iterations.\nThis iterative training process continues until the model learns to balance between fitting the data and adhering to domain knowledge, resulting in a model that is both accurate and reliable."}, {"title": "3.1 Pipeline Overview", "content": "The pipeline for this hybrid methodology is illustrated in Figure 1. The figure provides a step-by-step visual representation of the key components and processes involved in the methodology.\nAs seen in Figure 1, the pipeline begins with domain expert knowledge and data, which are used to define an ontology. This ontology is continuously enriched and accessed throughout the training process. Customized loss functions are defined, which are influenced by the expert knowledge captured in the ontology. A fragment of the ontology is translated into ASP rules, which serve as a formal mechanism for reasoning over domain-specific constraints.\nThe ASP rules and the knowledge base are fed into an ASP solver, which checks whether the model's predictions violate any of the encoded rules. If violations are detected, they are reflected in an ASP penalty, which is incorporated into the total loss function. This feedback is then used to update the model parameters, guiding the training process to ensure compliance with both data-driven predictions and domain knowledge.\nAfter training, the model is evaluated based on its performance in prediction tasks as well as its adherence to the domain rules. The final result is a trained model that demonstrates a balance between accuracy, reliability, explainability, and trustworthiness, making it well-suited for high-stakes applications."}, {"title": "4 Designing the Loss Function for Hybrid Learning", "content": "The loss function in this hybrid approach is designed to incorporate domain knowledge from ASP into the learning process, alongside the traditional data-driven loss. The base loss function, $L(Y_{true}, Y_{pred})$, is chosen based on the nature of the task at hand. For classification problems, cross-entropy loss is typically used to measure the divergence between predicted probabilities and true labels. For regression tasks, mean squared error (MSE) or mean absolute error (MAE) is commonly employed to measure the difference between predicted values and ground truth [1].\nTo ensure that the model adheres to domain knowledge, an ASP-based penalty term is incorporated into the loss function. The total loss function is expressed as follows:\nTotal Loss = $L(Y_{true}, Y_{pred}) + \\lambda \\cdot ASP \\ Penalty$ \nIn this equation, $\\lambda$ is a weighting factor that controls the importance of the ASP penalty in the learning process. The ASP penalty term increases if the model's predictions violate ASP rules, reflecting the degree to which the model's predictions deviate from domain knowledge. By tuning $\\lambda$, we can adjust the balance between the model's ability to learn from the data and its adherence to domain-specific constraints."}, {"title": "4.1 Designing Differentiable Penalty Functions", "content": "The key to making the ASP penalty differentiable is to represent rule violations using smooth, continuous functions. Instead of binary indicators of rule compliance, we use differentiable functions that quantify the degree of violation.\nFor each ASP rule, we define a penalty function $P_i(y_{pred})$ that measures how much the model's prediction violates the rule. The ASP penalty is then the sum of these individual penalties:\nASP Penalty = $\\sum_{i=1}^{N} \\gamma_i \\cdot P_i(Y_{pred})$\nwhere:\n\u2022 N is the total number of ASP rules.\n\u2022 $\\gamma_i$ is the weight assigned to the i-th rule, reflecting its importance.\n\u2022 $P_i(Y_{pred})$ is a differentiable penalty function for the i-th rule."}, {"title": "4.1.1 Example of a Differentiable Penalty Function", "content": "Consider the same example in the context of battery manufacturing:\nDomain Rule:\n\"The charging voltage must not exceed 4.2V to prevent battery degradation.\"\nWe want to design a differentiable penalty function that penalizes predictions where the voltage exceeds 4.2V. A common approach is to use the ReLU (Rectified Linear Unit) function or a smooth approximation to model the violation.\nThe penalty function $P_{voltage}(V_{predicted})$ can be defined as:\n$P_{voltage}(V_{predicted}) = ReLU(V_{predicted} - V_{max})$\nor, to ensure smoothness, we can use the Softplus function:\n$P_{voltage}(V_{predicted}) = \\frac{1}{k} ln \\left( 1 + e^{k(V_{predicted} - V_{max})} \\right)$\nwhere:\n\u2022 $V_{max} = 4.2V$ is the maximum allowable voltage."}, {"title": "4.2 Adjusting the Weighting Factor \u03bb", "content": "The weighting factor $\\lambda$ balances the influence of data fitting and rule adherence:\n\u2022 A larger $\\lambda$ emphasizes compliance with domain rules, which is important in safety-critical applications.\n\u2022 A smaller $\\lambda$ allows the model to prioritize data fitting, which may be desirable when the data contains patterns not fully captured by the rules.\nSelecting an appropriate $\\lambda$ involves experimentation and validation to achieve the desired balance.\nBy incorporating these differentiable penalty functions into the loss function, the model learns to make predictions that optimize production parameters while complying with essential manufacturing guidelines."}, {"title": "4.3 Overall Algorithm", "content": "Algorithm 1 summarizes the training process with differentiable ASP penalty integration."}, {"title": "5 Proof of Concept Experiment", "content": "In this experiment, we integrate ASP constraints into a neural network training process to enforce domain-specific knowledge. A synthetic dataset with 1000 instances was generated, where each instance consists of two features $x_1$ and $x_2$ randomly sampled from a uniform distribution between 0 and 1. The target label is 1 if $x_1 + x_2 > 1$, and 0 otherwise.\nA simple feedforward neural network was implemented with an input layer of two nodes, one hidden layer of 10 neurons with ReLU activation, and an output layer for binary classification. The network was trained using the Adam optimizer and cross-entropy loss over 20 epochs. In addition to training without constraints, the ASP-based penalty was incorporated to enforce domain knowledge that $x_1 > 0.8$ should predict the class label 1. The ASP penalty was computed using Clingo [11], where logical rules encoded the constraint and penalized violations during training.\nThe model was trained in two configurations: First, using only cross-entropy loss, and second, with an additional ASP penalty. Accuracy and domain satisfaction were measured in both cases. Results showed that incorporating ASP penalties improved domain satisfaction from 0.78 to 0.95, ensuring the model's predictions adhered more closely to the domain rule. Accuracy remained competitive, reaching 92.7% with the ASP penalty compared to 89.3% without it. This proof of concept demonstrates the potential of integrating symbolic reasoning into neural network training to enforce domain-specific constraints."}, {"title": "6 Application in Various Domains", "content": "The methodology outlined here is applicable across a range of domains where domain-specific knowledge plays a critical role. In healthcare, for example, the model can integrate medical ontologies and ASP rules to ensure that predictions adhere to clinical guidelines and avoid harmful drug interactions. Specifically, discussing what constitutes reliability, and therefore admissibility, of an AI system in the medical field, Dur\u00e1n and Jongsma [10] refer to \u201cverification and validation methods, robustness analysis, a history of (un)successful implementations, and expert knowledge\u201d as factors of computational reliabilism [9]. SAIF-DL facilitates reliable system design across multiple of these factors: verification, validation, and integration of expert knowledge. For autonomous systems, ASP can encode safety rules and regulations, such as speed limits or obstacle avoidance protocols, ensuring that the system operates safely even in complex and dynamic environments. In finance, regulatory compliance is critical, and models must adhere to strict legal and ethical guidelines. ASP rules can be used to encode these constraints, ensuring that the model's predictions comply with legal requirements, such as those related to credit scoring or anti-money laundering regulations.\nThe adaptability of this hybrid methodology makes it suitable for any domain where domain knowledge, safety, compliance, or trustworthiness are critical to the success of AI systems."}, {"title": "7 Conclusion", "content": "This paper has presented a novel hybrid methodology that integrates deep learning with domain expert knowledge using ontologies and answer set programming. By embedding domain-specific constraints and logical rules directly into the loss function of deep learning models, we have developed a flexible and scalable approach that balances predictive accuracy with rule adherence. This integration ensures that models not only learn from data but also comply with critical domain knowledge, enhancing their suitability for high-stakes applications such as battery manufacturing.\nOur approach leverages the expressive power of ontologies and ASP to represent a broader range of domain knowledge, including logical constraints and expert rules that are not easily captured by differential equations. This flexibility allows for seamless adaptation to various domains by simply updating the ASP rules, without the need for significant modifications to the neural network architecture.\nBy designing differentiable penalty functions for the ASP rules, we have ensured that the total loss function remains compatible with gradient-based optimization methods. This enables effective training of the hybrid model, allowing it to learn smoothly from both data and domain knowledge. The proposed methodology enhances the model's reliability, trustworthiness, and explainability, making it a robust solution for complex, knowledge-intensive applications.\nThe application to battery manufacturing demonstrates the practical utility of our approach in an industrial setting where compliance with domain-specific rules is critical. The ability to incorporate constraints such as temperature limits, mixing ratios, and process timings directly into the model's training process ensures that predictions not only optimize performance but also adhere to essential manufacturing guidelines.\nFuture work may explore the extension of this methodology to other domains and the incorporation of additional forms of domain knowledge. Investigating strategies for automatic tuning of the weighting factors and penalty functions could further enhance the model's performance. Additionally, integrating uncertainty quantification methods could provide deeper insights into the model's predictions, further increasing confidence in its applicability to real-world scenarios."}]}