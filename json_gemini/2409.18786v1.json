{"title": "A Survey on the Honesty of Large Language Models", "authors": ["Siheng Li", "Cheng Yang", "Taiqiang Wu", "Chufan Shi", "Yuji Zhang", "Xinyu Zhu", "Zesen Cheng", "Deng Cai", "Mo Yu", "Lemao Liu", "Jie Zhou", "Yujiu Yang", "Ngai Wong", "Xixin Wu", "Wai Lam"], "abstract": "Honesty is a fundamental principle for aligning large language models (LLMs) with human\nvalues, requiring these models to recognize what they know and don't know and be able to\nfaithfully express their knowledge. Despite promising, current LLMs still exhibit significant\ndishonest behaviors, such as confidently presenting wrong answers or failing to express what\nthey know. In addition, research on the honesty of LLMs also faces challenges, including\nvarying definitions of honesty, difficulties in distinguishing between known and unknown\nknowledge, and a lack of comprehensive understanding of related research. To address these\nissues, we provide a survey on the honesty of LLMs, covering its clarification, evaluation\napproaches, and strategies for improvement. Moreover, we offer insights for future research,\naiming to inspire further exploration in this important area.", "sections": [{"title": "Introduction", "content": "Honesty has become a prominent and frequently discussed topic in the development of large language models\n(LLMs) and is recognized as one of the key principles for aligning LLMs with human preferences and values. Specifically, an honest LLM should acknowledge its limitations when it encounters queries beyond\nits capabilities, rather than providing misleading information. This is particularly important in high-stakes\ndomains such as medicine law, and finance. Moreover, an honest LLM should faithfully express its knowledge, either parametric or in-context\nknowledge, which is crucial in knowledge-intensive scenarios.\nThough promising, current models still frequently exhibit dishonest behaviors. They almost always speak\nwith a confident tone, even when they make errors; they might \"know\" the answer internally but fail to\n\"say\" it accordingly; and they may provide biased information influenced by human input. These dishonest behaviors can mislead humans and undermine their trust, highlighting\nthe need for further research on improving the honesty of LLMs.\nHowever, research on the honesty of LLMs also faces several challenges. First, the many different definitions\nof honesty in LLMs cause confusion in studies. Additionally, the connection between honesty and various\nrelated issues remains unclear. Second, honesty is specific to each model, as it requires identifying the\nmodel's known and unknown knowledge, making both its evaluation and improvement challenging. Last but\nnot least, although many studies address related aspects of honesty, such as recognizing known and unknown"}, {"title": "Honesty in LLMs", "content": "Conceptually, being honest is described as being \"free of deception, morally upright or virtuous, among\nother traits\". In the realm of LLMs, there has been a long-standing pursuit of developing\nhonest models, with various definitions of honesty emerging over time. Askell et al. (2021) describe honesty\nas providing accurate information, expressing uncertainty without misleading, and being aware of knowledge\nand internal state. Kadavath et al. (2022) indicate honesty as an umbrella concept including truthfulness,\ncalibration, self-knowledge, explainability, and non-deceptiveness. In simpler terms, researchers consider a\nmodel as honest if it refrains from making statements it doesn't believe or if it is able\nto express everything represented in its internal states through natural language (Lin et al., 2022a). Recent\nstudies suggest that an honest model should accurately express its knowledge and humbly acknowledge its\nlimitations without deception or being inconsistent.\nTo summarize, the most widely accepted definitions for an honest LLM are self-knowledge and self-expression.\nSelf-knowledge involves the model being aware of its own capabilities, recognizing what it knows and what it\ndoesn't, allowing it to acknowledge limitations or convey uncertainty when necessary. Self-expression refers\nto the model's ability to faithfully express its knowledge, leading to reliable outputs. In this paper, we\nconsider an LLM to be honest if it fulfills these two widely accepted criteria: possessing both self-knowledge\nand self-expression."}, {"title": "Self-knowledge", "content": "The concept of self-knowledge is crucial in both the philosophy of mind and epistemology, referring to one's\nunderstanding of their own mental states, such as experience, thoughts, beliefs, and desires. Within the context of LLMs, research on self-knowledge has also emerged as a prominent and rapidly growing\nfield of interest. Specifically, the self-knowledge capacity of LLMs hinges on their ability to recognize what they know\nand what they don't know. This enables them to explicitly state \"I don't know\u201d when lacking necessary"}, {"title": "Self-expression", "content": "In human society, self-expression involves conveying one's thoughts and feelings through languages, decisions,\nor actions, and it is regarded as a highly respected value in Western civilization. In the\nfield of LLMs, we refer to self-expression as the model's ability to express its knowledge faithfully, either\nparametric knowledge acquired through training or in-context knowledge. This enables the model to ground\nits responses in its knowledge rather than fabricating information.\nAlthough seemingly straightforward, recent studies have revealed significant challenges in achieving reliable\nself-expression in LLMs. For example, Zhu & Li (2023); Zhang et al. (2024b); Li et al. (2024a) highlight that\neven when LLMS possess the knowledge internally, they may not be able to express it accurately. Additionally,\nthese models often struggle to accurately convey in-context knowledge, such as relevant articles or figures\n(Liu et al., 2024d; Shi et al., 2024; Bai et al., 2024; Zhang et al., 2024d). Both these issues can contribute to\nthe occurrence of hallucinations. Moreover, current LLMs frequently exhibits inconsistent behaviors due to\nshortcomings in self-expression. For instance, slight changes in prompts may lead to substantial performance\ndegradation (Gu et al., 2023; Mizrahi et al., 2023; Wang et al., 2023b; Sclar et al., 2024; Cao et al., 2024),\nor the model might provide biased information to cater to the user's views. These challenges underscore the critical importance of\nself-expression in the development of LLMs."}, {"title": "Evaluation of LLM Honesty", "content": "In this section, we review previous research on the evaluation of honesty and consolidate these efforts into\ntwo categories: evaluations of self-knowledge (\u00a73.1) and self-expression (\u00a73.2)."}, {"title": "Self-knowledge", "content": "An LLM with self-knowledge has the ability to recognize its own strengths and limitations. There are\ngenerally two approaches for evaluating self-knowledge. The first is a binary judgement regarding the capacity\nof LLMs on recognition of known/unknown. The second involves continuous confidence scoring, where the\nLLM assigns varying levels of confidence to its answers. This evaluation includes calibration and selective\nprediction."}, {"title": "Recognition of Known/Unknown.", "content": "LLMs should be capable of discerning what they know and what\nthey don't, in order to avoid misleading users when they lack relevant information. Current evaluation of this\nability can be broadly categorized into two types: model-agnostic and model-specific depending on\nwhether the approach is tailored to a particular LLM.\nModel-agnostic approach applies the same set of known and unknown questions across all LLMs. Repre-\nsentative benchmarks in this category include SelfAware KUQ UnknownBench HoneSet and BeHonest. These\nbenchmarks generally assume that the model's pre-training corpus forms its knowledge base. For example,\nYin et al. (2023) consider Wikipedia as part of the model's known knowledge as it is othen included in pre-\ntraining data. Consequently, questions sourced from Wikipedia (e.g., SQUAD (Rajpurkar et al., 2016)) can\nbe treated as known questions. For unknown questions, a heuristic annotation process is often used. This\ntypically involves defining various categories of unknown questions and curating corresponding questions.\nFor instance, HoneSet (Gao et al., 2024) identifies five categories (e.g., \"Latest information with external\nservices\") and then compiles questions that align with these categories (e.g., \"Show the current most-watched\nmovies on Netflix\").\nModel-specific approach tailors question sets for each LLM. A notable benchmark for this is Idk (Cheng\net al., 2024), which distinguishes between known and unknown questions based on the model's performance.\nSpecifically, it samples multiple outputs for each question, and if the accuracy of these outputs surpasses a\ncertain threshold, the question is identified as known; otherwise, it is considered unknown.\nThe evaluation process involves presenting a question to the\nLLM, obtaining its output, and then assessing whether the out-\nput indicates recognition of the unknown, such as responding\nwith \u201cI don't know\u201d. In\nterms of evaluation metrics, the F1 score and refusal rate are commonly employed. We formalize\nthem based on the confusion matrix in Tab. 2. The F1 score\ntypically treats unknown as the positive class and known as\nthe negative class, and is calculated as follows:\nF1 = 2 x \\frac{Precision \u00d7 Recall}{Precision + Recall},\nwhere Precision = \\frac{N4}{N3 + N4}, Recall = \\frac{N4}{N2 + N4}.\nMeanwhile, the refusal rate, also known as honesty rate in Gao et al. (2024), emphasizes the model's ability\nto recognize unknowns, measuring the percentage of cases in which the model correctly refuses to respond.\nIt is calculated as follows:\nRefusal Rate = \\frac{N4}{N2 + N4}, or Refusal Rate = \\frac{N3}{N1 + N3}."}, {"title": "Calibration.", "content": "Another area of research aims for LLMs to provide more precise confidence levels in their\nresponses. A standard metric for assessing this is calibration (Guo et al., 2017; Tian et al., 2023; Zhu et al.,\n2023; Geng et al., 2024), which determines whether the confidence score assigned to a prediction accurately\nreflects the likelihood that the prediction is correct. In a well-calibrated model, predictions with an 80%\nconfidence level are expected to, on average, have an actual accuracy of 80%. Formally, let x represent the\ninput, y the ground truth, \u0177 the model's prediction, and $conf(x, \\hat{y})$ the confidence score derived through\nspecific confidence elicitation methods. A model is\nconsidered well-calibrated if the following condition holds:\nP(\\hat{y} = y | conf(x, \\hat{y}) = p) = p, \\forall p \\in [0,1].\nGiven the evaluation set $D = \\{(x_i, y_i)\\}_{i=1}^N$, two widely adopted metrics for assessing calibration performance\nare the Brier score (Brier, 1950) and the expected calibration error (ECE) (Naeini et al., 2015), with lower\nvalues indicating better calibration. The Brier score measures the difference between the actual correctness\nand the confidence score through pointwise mean squared error:\nBrier Score = \\frac{1}{N} \\sum_{i=1}^{N} (acc(y_i, \\hat{y_i}) \u2013 conf(x_i, \\hat{y_i}))^2.\nECE measures the discrepancy between a model's confidence and its actual correctness using a bucketing\nstrategy. The confidence range [0, 1] is divided into M buckets of equal width, with each bucket having a\nlength of $\\frac{1}{M}$. Test examples are assigned to these buckets according to their confidence scores. The ECE is"}, {"title": "", "content": "then mathematically defined as:\nECE = \\frac{1}{N} \\sum_{m=1}^{M} |B_m| |acc(B_m) - conf(B_m)|,\nwhere $B_m$ represents the bucket for confidence scores within the interval $(\\frac{m-1}{M}, \\frac{m}{M}]$, $|B_m|$ is the number of\ntest examples in bucket $B_m$, $acc(B_m)$ is the average accuracy, and $conf(B_m)$ the average confidence in that\nbucket. However, the original ECE metric has some limitations, such as sensitivity to the choice of bin\nwidth and imbalanced bin size. To address these issues, several variants have been proposed, including the\nstatic calibration error (SCE) (Nixon et al., 2019), adaptive calibration error (ACE) (Nixon et al., 2019)\nand classwise ECE (Kull et al., 2019). In addition, the reliability diagram (DeGroot & Fienberg, 1983)\nvisually represents ECE by plotting the average confidence score against the corresponding average accuracy.\nDeviations from the diagonal line in the diagram indicate miscalibration. Typically, current research uses\nbenchmarks from knowledge-intensive question answering tasks and reasoning tasks to assess calibration."}, {"title": "Selective Prediction.", "content": "Another representative approach for evaluating confidence expression is selective\nprediction where predictions are ranked based\non their confidence scores and those below a certain threshold are discarded. For successful performance in\nselective prediction, the model needs to assign higher confidence scores to correct predictions and lower scores\nto incorrect ones. Unlike calibration, which focuses on matching confidence scores to actual accuracy, selective\nprediction measures how well the confidence scores differentiate between correct and incorrect predictions.\nFor example, a model that produces incorrect answers with low confidence might be well-calibrated but still\nperform poorly in selective prediction. Below are some commonly used metrics for selective prediction, along\nwith their characteristics:\n\u2022 AUROC (Area Under Receiver Operating Characteristic curve) (Tomani et al., 2024; Xu et al., 2024b;\nXiong et al., 2024; Chen et al., 2023a): The ROC curve plots the true positive rate (TPR) against the\nfalse positive rate (FPR) at various confidence thresholds, illustrating how well confidence scores can\ndistinguish between correct and incorrect predictions. AUROC quantifies this capability by calculating\nthe area under the ROC curve.\n\u2022 AUPRC (Area Under Precision Recall Curve): The precision recall\ncurve plots precision against recall at different confidence thresholds, capturing the model's effectiveness\nin balancing high precision and recall. AUPRC measures this effectiveness by computing the area under\nthe precision recall curve. This metric is particularly useful for imbalanced benchmarks, where it better\nreflects performance on the minority class than AUROC.\n\u2022 AUARC (Area Under Accuracy Rejection Curve): The accuracy rejection curve depicts the change in accuracy as a proportion\nof responses are progressively rejected based on different confidence thresholds. This metric reflects the\nmodel's ability to improve its performance by abstaining from uncertain predictions. AUARC is calculated\nas the area under the accuracy rejection curve.\n\u2022 AURCC (Area Under Risk Coverage Curve): The risk coverage curve illustrates how\nrisk (e.g., error rate) changes as coverage (the proportion of accepted prediction) increases based on\ndifferent confidence thresholds. AURCC measures the area under the risk coverage curve, where a lower\nvalue indicates better selective prediction performance.\nAs with calibration, current research applies benchmark from knowledge-intensive question answering and reasoning tasks to selective prediction."}, {"title": "Summary & Discussion.", "content": "In this section, we review current research on evaluating the honesty of LLMs in\nrelation to their self-knowledge capabilities. The task of recognizing known and unknown requires the model\nto identify what it knows and what it doesn't. Generally, two approaches are employed. The model-agnostic"}, {"title": "Self-expression", "content": "Self-expression refers to the ability of LLMs to faithfully express their knowledge. Research on evaluating\nthis ability can be broadly categorized into two approaches, based on whether knowledge identification is\nrequired: identification-based evaluation and identification-free evaluation.\nIdentification-based Evaluation. This approach involves identifying what the LLM knows and con-\nstructing a question-answering benchmark based on the identified knowledge. It then assesses whether the\nLLM can accurately express the correct answer when presented with questions. The process of identifying\nwhat LLM knows is similar to that of \"recognition of known\" described in \u00a73.1. Therefore, benchmarks for\nidentification-based evaluation also include model-agnostic benchmarks and\nmodel-specific benchmarks. The primary distinction between these two lies\nin the objectives: while recognition of known requires LLMs to merely classify what is known, identification-\nbased evaluation assess whether the model's provided answers are correct. Accordingly, accuracy is the most\ncommonly used metric in this context, calculates as:\nAccuracy = \\frac{Ncorrect}{Ntotal},\nwhere $Ncorrect$ denotes the number of correctly answered questions and $Ntotal$ represents the total number\nof questions."}, {"title": "Identification-free Evaluation.", "content": "Another approach indirectly evaluates self-expression capacity by mea-\nsuring the consistency across multiple outputs. The key principle is that an LLM with strong self-expression\nshould produce consistent outputs when given different prompts that refer to the same underlying knowledge.\nTypically, this approach begins by selecting a meta-example from existing datasets, and applying various"}, {"title": "", "content": "augmentation strategies to create multiple views of the same example. The consistency across these views\nthen serves as an indicator of the model's self-expression ability. Tab. 3 provides illustrated examples of the\ncommonly used augmentation strategies, with further details explained below.\n\u2022 Format Adjustment: This strategy involves making slight adjust-\nments to the meta-example, e.g., changing separators, adjusting spacing and modifying letter casing.\n\u2022 Query Rephrasing: This strategy rephrases the meta-example in\nmultiple ways while preserving its meaning, simulating the diverse expressions of real-world users.\n\u2022 Sycophancy Revision: This strategy incorporates\nhuman perspectives, such as personal opinions or profiles, into the contexts to assess whether the model\ncan maintain consistency in its outputs.\n\u2022 Generation-Validation (GV) Transformation: This strategy assesses\nthe consistency between the LLM's generation and validation capabilities. Specifically, the LLM first\nfunctions as a generator to produce an output based on a given instruction, and then it acts as a validator\nto assess whether it agrees with the output it generated.\nThe principle underlying identification-free evaluation dictates the evaluation metrics should emphasize the\nconsistency of the LLM's responses among the augmented examples rather than merely reporting absolute\nperformance. Accordingly, three representative metrics are used:\n(1) Performance Spread: This\nmetric measures the variation in performance among the augmented examples and is mainly used in the\ncontext of the format adjustment and instruction rephrasing strategy. It can be defined as:\nPerformance Spread = maxP(X) \u2013 minP(X), or Performance Spread = maxP(X) \u2013 avgP(X)\nwhere X represents the augmented dataset, while maxP(\u00b7), minP(\u00b7) and avgP(\u00b7) denote the operation to get\nthe maximum, minimum and average performance by selecting augmented examples for each meta-example.\n(2) Sycophancy Rate: This metric quantifies the frequency with\nwhich the model's responses changes after encountering human perspective information and is primarily\napplied in the context of the sycophancy revision strategy. It is defined as:\nSycophancy Rate = \\frac{Nchanged}{Ntotal},\nwhere Nchanged is the number of responses that changed due to the introduction of human perspective\ninformation, and Ntotal is the total number of meta-examples."}, {"title": "", "content": "(3) Agreement Rate: This metric assesses the degree of agreement between the response\nof the LLM as the generator and its response as a validator, and is primarily employed in the context of the\nGV transformation strategy. It is defined as:\nAgreement Rate = \\frac{Nagree}{Ntotal}\nwhere Nagree is the number of instances where the model's responses as a generator and validator are\nin agreement, and Ntotal is the total number of meta-examples."}, {"title": "Summary & Discussion.", "content": "In this section, we review both identification-based and identification-free ap-\nproaches to evaluating the self-expression capabilities of LLMs. Identification-based evaluation begins by\ndetermining what the LLM knows and doesn't know, followed by assessing the alignment between its knowl-\nedge and how it is expressed. On the other hand, identification-free evaluation uses various strategies to\ncreate diverse views of a meta-example, then assesses the consistency across these views, indirectly measur-\ning the model's self-expression capabilities. Future research could investigate alternative strategies to create\ndiverse views, such as translating original queries into different languages to evaluate the model's expression\nability across cross-lingual settings. Additionally, these evaluations predominantly focus on single-turn sce-\nnarios, where the model is expected to remain consistent in responding to the same query. Future studies\ncould extend this to multi-turn scenarios, where the model should maintain consistency with the conversation\nhistory over time."}, {"title": "Improvement of Self-knowledge", "content": "Many studies aim to improve the self-knowledge capabilities of LLMs. One line of research teaches them to\narticulate \"I don't know\u201d. Another line of research elicits calibrated confidence or uncertainty in response,\nwhich indicates the probability that the responses are correct. We categorize existing methods into two\nbroad groups: training-free approaches, which include Predictive Probability, Prompting, and Sampling and\nAggregation, and training-based approaches, such as Supervised Fine-tuning, Reinforcement Learning, and\nProbing."}, {"title": "Training-free Approaches", "content": "Predictive Probability. A straightforward approach to providing confidence is computing predictive\nprobability, which has been extensively explored in NLP classification tasks with masked language models\n(Xiao et al., 2022). In the era of LLMs, the predictive probability of an output is formalized as\nlog p(y x) = \\sum_{t=1}^{T} log(y*_t|y1:t-1, x),\nwhere x and y represent prompt and output respectively. As this measure is biased towards output length T\n(Wu et al., 2016), the length-normalized version is frequently used by dividing logp(y|x) with T (Adiwardana\net al., 2020; Malinin & Gales, 2021; Si et al., 2023a; Kuhn et al., 2023). Kadavath et al. (2022) indicate that\nthe predictive probability of LLM is well-calibrated on multiple-choice tasks (T = 1) and the calibration\nimproves with the capability of LLM. However, empirical experiments show that predictive probability is\nless suitable for free-form generation tasks (T > 1). Inspired by\nthis observation, Ren et al. (2023b) convert free-form generation into multiple-choice selection by sampling\nmultiple candidate answers and forming them into a multiple-choice format. One potential reason for the\nweakness of predictive probability in free-form generation is that token probability captures lexical confidence\ninstead of semantic confidence (Kuhn et al., 2023), which is more desired in applications. To better capture\nsemantics, Duan et al. (2024) reweight the token probability with a relevance score, which represents the\nsemantic change before and after the token is removed. A fundamental limitation of predictive probability\nis the requirement of token-likelihood, which might be inaccessible for closed-source LLMs, such as GPT-3.5\nand GPT-4 (Achiam et al., 2023)."}, {"title": "Prompting.", "content": "A set of research investigates prompting strategies to elicit self-knowledge from LLMs. We\nprovide an overview of these strategies in Table 4. In earlier studies, Kadavath et al. (2022) propose a\nself-evaluation approach P(True), which converts confidence estimation into a discrimination problem. In\nparticular, they prompt LLM to identify whether its answer is true or false given the question as a context,\nthen the probability of \u201ctrue\u201d serves as its confidence in this answer. The empirical results indicate that\nP(True) with multiple sampled answers in the context exhibit promising performance. Drawing from psy-\nchological and cognitive research, Zhao et al. (2024) propose a fact-and-reflection strategy. This strategy\nprompts LLMs to first provide relevant facts, then engage in reasoning, deliver an answer, and finally use\nP(True) or predictive token probability for confidence estimation. A major limitation of the self-evaluation\napproach is the additional inference required for assessment, which hampers efficiency. Moreover, recent\nstudies indicate that LLMs may struggle to accurately distinguish their own responses.\nAnother line of research prompts LLMs to verbalize self-knowledge. Yin et al. (2023) employ instructions\nor in-context demonstrations to facilitate LLMs to acknowledge limitations for unknown questions. Tian\net al. (2023) introduce various prompting strategies to elicit verbalized confidence, including chain-of-thought\nprompting (Wei et al., 2022), top-k prompting, where the model provides k guesses along with their respective\nconfidences, and linguistic prompting, which requires the model to express confidence using a set of predefined\nlinguistic terms rather than numerical values. Their experiments demonstrate that verbalized confidence can\nbe better-calibrated than conditional probabilities estimated through multiple sampling for RLHF models\nInspired by human conversations, Xiong et al. (2024) develop two novel prompting\nstrategies: self-probing, which estimates the confidence of an answer in an additional chat session, based on\nthe human tendency to more easily recognize others' errors; and a multi-step strategy, which prompts LLMS\nto break down the problem and provide confidence for each step. Despite significant progress, Xiong et al."}, {"title": "Sampling and Aggregation.", "content": "Numerous studies investigate the consistency among multiple outputs to\nestimate confidence. Typically, they use temperature sampling to obtain diverse outputs based on the same\nprompt, with the temperature controlling the randomness (Zhou et al., 2022; Kuhn et al., 2023; Lyu et al.,\n2024). Alternatively, Xiong et al. (2024); Yang et al. (2024) improve diversity by rephrasing the original"}, {"title": "Summary & Discussion.", "content": "The variance observed across multiple generations provides valuable insights for\nestimating confidence or uncertainty. However, this approach is computationally expensive, as it necessitates\ngenerating multiple outputs for each query, and typically relies on an additional model to aggregate these\noutputs (e.g., NLI model). To address this issue, recent research has focused on constructing training data\nthrough sampling and aggregation, then fine-tuning a model to directly predict confidence, thereby removing\nthe need for multiple sampling."}, {"title": "Training-based Approaches", "content": "Supervised Fine-tuning. Despite significant progress with training-free approaches, they may underper-\nform in free-form generation tasks (Kapoor et al., 2024). A straightforward approach to optimize LLMs for\nbetter self-knowledge is supervised fine-tuning. One line of research fine-tunes LLMs to verbalize \"I don't\nknow\" when they lack relevant knowledge. The primary challenge in this approach is developing effective\nmethods to distinguish between known and unknown questions. Yang et al. (2023); Zhang et al. (2024a);\nCheng et al. (2024) sample multiple candidate answers for each question and compare them with the ground-\ntruth answer, classifying a question as known if the accuracy exceeds a certain threshold. In contrast, Chen\net al. (2024b) use an unsupervised approach by leveraging the model's predictive probability in its predic-\ntions to discern between known and unknown information. The primary limitation of these methods is the\ndifficulty in evaluating long-form generations in instruction-following scenarios. To address this problem,\nWan et al. (2024a) create multiple-choice questions based on the required knowledge of the instruction, if\nthe model can not provide an accurate answer, they classify the question as unknown. Differing from the\naforementioned research focusing on questions, Kapoor et al. (2024) fine-tune LLMs to predict the likelihood\nof the model's answer being correct. They explore LoRA and probe for\noptimizing LLMs and find that using 1000 training examples can lead to promising performance.\nAnother line of research fine-tunes LLMs to provide confidence estimates for responses. Lin et al. (2022a)\nfine-tune GPT-3 to verbalize confidence on arithmetic questions, where the target confidence is the empirical\nprecision of GPT-3 on that type of question. Similarly, Ulmer et al. (2024) cluster questions based on sentence"}, {"title": "Summary & Discussion.", "content": "Supervised fine-tuning is an effective approach for improving the self-knowledge\ncapacity of LLMs. The primary challenge of this strategy lies in the data curation process, which requires\ndistinguishing between known and unknown questions or estimating the confidence in responses. Though\ncurrent methods perform well in short-form question answering, they struggle to generalize to long-form\nsettings. Future research should focus more on long-form scenarios, such as instruction following."}, {"title": "Reinforcement Learning.", "content": "Numerous studies have highlighted the great potential of reinforcement learn-\ning to improve self-knowledge. Cheng et al. (2024); Xu et al. (2024a) teach LLMs to abstain from responding\nto questions they do not know and apply DPO or PPO for\noptimization. They construct preference data based on the inherent knowledge of LLMs. Specifically, if the\nmodel correctly answers a question, the preferred response is the correct answer, and the rejected response is\n\u201cI don't know", "I don't know\", while\nthe rejected response is the incorrect answer. More simply, Gao et al. (2024) create preference pairs by using\nLLMs to judge both honesty and helpfulness, then use DPO for optimization. To provide more fine-grained\ninformation, Xu et al. (2024b) teach LLMs to verbalize numerical confidence scores alongside rationales\nexplaining the sources of their uncertainty. For optimization, they utilize PPO and design a reward function\nthat encourages high confidence in correct responses and low confidence in incorrect ones.\nRecent studies explicitly model the human-AI interaction process by simulating a \\\"listener\\\" using an LLM,\nwho makes decisions based on the response from a \\\"speaker\\\" LLM. They fine-tune the speaker to either\nrefuse to answer unknown questions or express well-calibrated confidence in its response, so that the listener\ncould make proper decisions accordingly, such as accepting or rejecting the response. Specifically, Stengel-\nEskin et al. (2024) train LLMs to articulate appropriate implicit (e.g., hedges) or explicit confidence markers\n(e.g., numeric confidence) using DPO. In this approach, a correct response accepted by the listener is valued\nequally with an incorrect response that is rejected by the listener, with both being better than an incorrect\nresponse that is accepted by the listener. Additionally, Band et al. (2024) allow the listener to answer\nsubsequent questions based on the speaker's long-form response. They then use the predictive log-likelihood\nof the listener on the ground-truth answer as a reward and employ PPO for optimizing the speaker.\"\n    },\n    {\n      \"title\"": "Summary & Discussion."}, {"content": "Reinforcement learning methods have demonstrated great potential for improving\nthe self-knowledge capabilities of LLMs. However, these methods also have limitations, particularly in\ntheir reliance on ground-truth labels to assess the correctness of responses for constructing preference pairs.\nAdditionally, current PPO-based strategies provide rewards based on the ground-truth labels (Xu et al.,\n2024b; Band et al., 2024), thereby limiting the exploration space during training. Future research could focus\non developing unsupervised methods to provide supervision for reinforcement learning, such as Predictive\nProbability, Prompting and Sampling and Aggregation."}, {"title": "Probing.", "content": "Instead of investigating the outputs of LLMs for insights into self-knowledge, another line of\nresearch delves into the internal representations of these models. Typically, this is achieved through a\nprobing strategy, where a simple network on the hidden states of a frozen LLM is trained to perform specific\nclassification tasks. In an earlier study, Kadavath et al. (2022) train\na value head to predict whether the LLM knows the answer to a given free-form question, demonstrating\npromising results. Similarly, Azaria & Mitchell (2023) find that a probing network based on the hidden states\nof LLMs can distinguish between true and false statements with an average accuracy ranging from 71% to\n83%, suggesting that the internal states of an LLM can recognize when it is providing false information. To\nfurther investigate this, Marks & Tegmark (2023) visualize the representations of true and false statements\nwithin LLMs and discover a clear linear structure. Moreover, Ji et al. (2024) demonstrate that a probing\nnetwork on the hidden states of query tokens could even predict the likelihood of hallucinations before\nresponses are generated. Despite the promise of these findings, one notable challenge with probing is its\nlimited ability to generalize to out-of-distribution scenarios. To address this,\nLiu et al. (2024c) scale the training data to 40 datasets, leading to improved generalization performance."}, {"title": "Summary & Discussion.", "content": "The strong performance of probing suggests that LLMs inherently possess self-\nknowledge, and our challenge lies in effectively extracting it with the appropriate methods. Moreover,\nprobing is efficient for both training and inference, as it only requires a simple additional network. However,\na significant concern is that most research in this area has primarily focused on short-form question answering,\nleaving its effectiveness in instruction-following scenarios largely unexplored."}, {"title": "Improvement of Self-expression", "content": "Numerous studies aim to improve the ability of LLMs to faithfully express their knowledge in responses.\nTo provide a clearer understanding of current research", "cate-\ngories": "training-free approaches, which include Prompting, decoding-time intervention, sampling and ag-\ngregation, and post-generation revision, and training-based approaches including self-aware fine"}]}