{"title": "IDEA: ENHANCING THE RULE LEARNING ABILITY OF\nLANGUAGE AGENT THROUGH INDUCTION, DEUCTION,\nAND ABDUCTION", "authors": ["Kaiyu He", "Zhiyu Chen"], "abstract": "While large language models (LLMs) have been thoroughly evaluated for deduc-\ntive and inductive reasoning, their proficiency in abductive reasoning and holis-\ntic rule learning in interactive environments remains less explored. This work\nintroduces RULEARN, a novel benchmark specifically designed to assess the\nrule-learning ability of LLMs in interactive settings. In RULEARN, agents in-\nteract with the environment to gather observations and discern patterns, using\nthese insights to solve problems. To further enhance the rule-learning capabili-\nties of LLM agents within this benchmark, we propose IDEA agent, which in-\ntegrates Induction, DEduction, and Abduction processes. IDEA agent refines\nthis approach by leveraging a structured reasoning sequence: generating hypothe-\nses through abduction, testing them via deduction, and refining them based on\ninduction feedback. This sequence enables agents to dynamically establish and\napply rules, mimicking human-like reasoning processes. Our evaluation of five\nrepresentative LLMs indicates that while these models can generate plausible ini-\ntial hypotheses, they often struggle with strategic interaction within the environ-\nment, effective incorporation of feedback, and adaptive refinement of their hy-\npotheses. IDEA agent demonstrates significantly improved performance on the\nRULEARN benchmark, offering valuable insights for the development of agents\ncapable of human-like rule-learning in real-world scenarios. We will release our\ncode and data.", "sections": [{"title": "1 INTRODUCTION", "content": "One major pillar of human intelligence is the ability to discern rules and apply them. We iden-\ntify patterns, formulate hypotheses, and refine them by interacting with the environment. This\nexploratory process traditionally involves three stages: abduction, deduction, and induction. Ac-\ncording to Charles Peirce's definition (Frankfurt (1958); Peirce (1974)), the rule-learning loop typi-\ncally begins with an explanatory hypothesis that arises from abduction. This is followed by iterative\nexperiments guided by the hypothesis, known as deduction, which leads to the modification and\nrefinement of the hypothesis through induction. Whether the hypothesis emerges from rigorous log-\nical reasoning or a more creative process is debated, but the cyclic nature of this reasoning is pivotal\nin advancing our understanding and problem-solving capabilities in various disciplines. This estab-\nlishes the foundation of how humans learn the rules of the world and apply them to effect change.\nRecent studies have explored the capabilities of large language models (LLMs) across various rea-\nsoning tasks, such as abduction, induction, and deduction, but often these tasks are examined in\nisolation, testing only one aspect of reasoning at a time (Bowen et al. (2024), Wang et al. (2023),\nSaparov et al. (2024)). Additionally, most benchmarks are non-interactive and rely on a fixed set\nof data, requiring LLMs to identify patterns from a predetermined number of samples (Liu et al.\n(2024)). However, real-world rule-learning does not typically begin with a complete set of obser-\nvations. Instead, it involves starting in an unknown environment, actively gathering observations,"}, {"title": "2 RELATED WORK", "content": "Recent studies have increasingly focused on evaluating the rule-learning capabilities of large lan-\nguage models. However, most research has addressed only partial aspects of the rule-learning loop,\nresulting in two distinct approaches within the literature.\nIsolated Studies: These studies assess abduction, deduction, and induction separately. For example,\nBowen et al. (2024) conducted an extensive evaluation of rule learning and leveraging in an ablation\nmanner. During deduction tests, agents were provided with the ground truth rules, whereas for\ninduction tests, agents received complete information. Wang et al. (2023) used executable code to\nrepresent rules, focusing solely on testing abduction capabilities. Similarly, Saparov et al. (2024)\nemployed first-order logic to represent rules in deduction evaluations but limited the assessments\nstrictly to deduction. Such isolated evaluations of reasoning power do not fully reflect real-world\ncomplexities.\nComprehensive Studies: While some tasks aim to evaluate the entire rule-learning loop, they still\nexhibit significant limitations. Xu et al. (2024) established their study in a novel open-world environ-\nment. However, distinct agents handled different phases of reasoning: a pretrained model conducted\nabduction only after a Reinforcement Learning (RL) agent, trained to maximize data collection, had\ncompleted its task. This setup mimicked isolated studies because the static data used in abduction\ndid not guide subsequent exploration, missing the iterative dynamic essential in reasoning processes.\nSimilarly, Liu et al. (2024) evaluated the abduction, induction, and deduction capabilities within a\nholistic rule-learning framework but relied on a fixed sample dataset. All existing works lacked the"}, {"title": "3 THE RULEARN BENCHMARK", "content": "We develop three puzzle sets, the Function Operator (\u00a73.1), Escape room (\u00a73.2), and Reactor\n(\u00a73.3), each comprising 20 environments of varying complexity. These environments are tailored to\nevaluate the rule-learning capabilities of language agents in diverse scenarios."}, {"title": "3.1 FUNCTION OPERATOR", "content": "This puzzle type simulates scenarios where systemic theories or established knowledge are appli-\ncable. The agent's challenge is to identify the necessary parameters of an existing rule in order to\nexplain the observations and solve the problem.\nIn this puzzle type, the agent interacts with a set of univariate multi-term equations involving the\nsame set of integer constants from [0,9] and subfunctions of the variable x, selected from f(x) \u2208\n{x0,x1, x2, sin(x),, |x|, -x}. An example Function Operator environment have the following\nfunctions: F1(x) = ax + b, F2(x) = csin(x) + b, F3(x) = ax2 and with the constant value\n{a = 3, b = 2, c = 7}\nThe goal of the agent is to determine the values of parameters, which in this specific case are a=3,\nb=2, c=7. To solve the puzzle, the agent must first identify the rule governing the environment,\nwhich involves deducing the exact formulation of each function based on limited information. The\nagent knows only the number of terms and the constants each function contains. For instance, F1(x)\nis known to include two terms associated with the constants a and b. With a hypothesized structure\nof each function in mind, the agent then decides which values to input into the function to determine\nthe parameters. For example, assigning values 1 and 2 to F3 reveals a quadratic increase in output,\nindicating the presence of x\u00b2 in F3. Similarly, assigning a value of 1 to F2 results in a floating-point\noutput, rather than an integer, suggesting the inclusion of trigonometric components; this confirms\nthat sin(x) is a component of F2.\nThis setting differs from standard equation solving because the agent's deductions are primarily\nbased on the outputs produced by various inputs, rather than on eliminating equations. The crucial\nstep involves deciding which function to interact with and what value to assign to the variable x in\norder to obtain the most informative output."}, {"title": "3.2 ROOM ESCAPE", "content": "This environment simulates scenarios where no established systemic knowledge is applicable, re-\nquiring the discovery of rules that adequately explain the observations. We create a fictitious en-\nvironment: an agent is in an art gallery tasked with deciphering a 3-digit password to unlock a\ncode-secured door and escape. The gallery features three types of paintings-oil, acrylic, and wa-\ntercolor each potentially differing in main color.\nIn this puzzle type, the rule is that 'the password is derived from the number of paintings of a given\ncolor, with each digit corresponding to a different painting type.' The agent knows only that the\npassword is a 3-digit number and receives a hint to focus on color. At each attempt of inputting the\npassword, agents receive feedback that indicates the correctness of each digit. By identifying the\ncorrectness of each digit, the agent needs to form a hypothesis on how the password is constructed"}, {"title": "3.3 REACTOR", "content": "This environment type challenges the agent to first discover a rule and then apply it to achieve the\ngoal. Unlike the Room Escape, where the password is directly revealed once the agent discerns the\nrelationship between the paintings and the password, Reactor require the agent to execute a series\nof actions in accordance with the discovered rule to achieve the objective. This adds a layer of\ncomplexity, challenging the agent to identify and apply a more robust rule.\nIn this environment type, the agent's task is to synthesize required materials using a mysterious\nreactor machine. This machine processes raw materials, represented by alphabetic letters such as A,\nB, AABB, and CAB. The reactor allows the agent to input two materials to initiate a reaction. The\nnewly generated material is then added and can be used in subsequent reactions. The critical task for\nthe agent is to determine the rule governing how the reactor synthesizes materials, as each reactor\noperates according to one specific rule. We have designed four types of rules, which are detailed in\nAppendix 3. A sample example, illustrating our IDEA agent as well as this type of environment,\ncan be seen in Fig 4)."}, {"title": "4 IDEA AGENT", "content": "We propose IDEA agent, a novel language agent designed to enhance the performance of rule-\nlearning tasks. IDEA agent mimics the real human rule-learning process, consisting of abduction,\ndeduction, and induction. Compared with conventional language agents, IDEA agent has additional\nabductive/inductive actions to choose from. At each step, if an abductive/inductive action is chosen,\nthe agent will reflect on all current observations, generate an initial hypothesis if none exists, refine\nits high-level hypothesis with the most recent experiments, and devise a plan to guide further ex-\nploration more effectively. If an interactive action is chosen, IDEA agent interacts with objects in\nthe environment, leveraging the generated hypothesis and plan to navigate and explore efficiently.\nBelow, we provide simplified pseudocode and a flowchart demonstrating how IDEA agent operates\nwithin the RULEARN environment. More detailed implementations and pseudocode can be found\nin Appendix A.3.1."}, {"title": "5 EXPERIMENT RESULTS", "content": ""}, {"title": "5.1 EXPERIMENT SETTINGS", "content": "To elucidate the challenges posed by the RULEARN benchmark and demonstrate how IDEA agent\naugments rule-learning performance, we conducted evaluations using five representative LLMs.\nThese include GPT-3.5-turbo, GPT-40 (OpenAI et al. (2024)), Gemma-7B (Team et al. (2024)),\nLlama3-7B, and Llama3-70B (AI@Meta (2024)). Additionally, we compared the performance of\nthe IDEA agent against two variants to assess its effectiveness.\nThe Baseline agent: The primary distinction between IDEA agent and the Baseline agent lies in\nthe absence of abductive/inductive actions within the latter's action space, which is central to IDEA\nagent. This absence prevents the Baseline agent from formulating explicit hypotheses or future\nplans to guide its explorations; therefore, in the Baseline, the agent's decisions are entirely depen-\ndent on historical observations. By comparing performance differences between the Baseline agent\nand IDEA agent, we can directly assess how the abduction, deduction, and induction reasoning\ninfluences rule-learning tasks.\nThe Deduction-only setting: In this setting, the ground truth rule is explicitly provided to LLM\nagents, eliminating the need for them to discover the rule independently. This setup primarily tests\nthe agents' deductive reasoning abilities by requiring them to apply these known rules to solve the\ntasks. Since the agents do not need to expend effort in rule discovery, their performance in the\nDeduction-only generally represents an optimal scenario. If a language agent is optimal in rule\ndiscovery (always finding the ground truth rule), its performance for IDEA agent and Baseline agent\nshould approximate but not exceed its performance in the Deduction-only setting. By comparing the\nperformance of the Deduction-only setting, IDEA agent, and the Baseline agent, we can assess the\nagents' proficiency in rule-learning and eliminate the influence of rule leveraging.\nFor each variant, we evaluate all three types of puzzles-totaling 60 puzzles. We conduct five trials\nfor each puzzle, resulting in 300 puzzles tested per variant. To facilitate hypothesis generation, which\nrequires a degree of creativity, we set the temperature of the LLMs to 1 and limit the maximum steps\nfor each puzzle to 15."}, {"title": "5.2 MAIN RESULTS", "content": "We calculated the Wilson Confidence Interval for each model's success rate across different variants,\nas outlined by Brown et al. (2001). The detailed results are displayed in Table 1.\nIn the Deduction-Only setting, GPT-40 consistently outperforms the other models when provided\nwith the ground truth rules, achieving the highest success rate in solving puzzles. This significant\nadvantage over the second-place model, Llama3-70B, demonstrates GPT-40's superior deductive\nabilities among the five LLM models. It excels in understanding given rules and executing efficient\nactions to solve puzzles. However, despite these advantages and the puzzles being simplified for the\nenvironment, the success rate for even advanced models like GPT-40 is only around 50%. This low\nrate highlights the significant challenges that rule-learning tasks in unfamiliar environments present\nto current LLMs. In specific scenarios like the Room Escape puzzles, the agent reaches an 80%\nsuccess rate, as it can directly derive the password by counting the paintings once the ground truth\nrule is known. However, in the Function Operator and Reactor puzzles, knowledge of the ground\ntruth rule alone is insufficient. Agents must still leverage this rule effectively and perform a series of\nstrategic actions to gather essential evidence for achieving their goals. Our experiments reveal that\nLLM agents often struggle in these more complex scenarios. They may repeat previous actions or\nfail to fully utilize current observations, collecting excessive and unnecessary data, which leads to\nsuboptimal outcomes and a low success rate in these puzzles.\nAs for Baseline agent, GPT-40 continues to outperform the other models, although Llama3-70B nar-\nrows the gap, particularly in Function Operator puzzles where it demonstrates strong performance.\nNotably, despite the intuitiveness of the rule associated with Room Escape puzzles\u2014which involves\nusing the number of paintings of different colors as the digits of the password-the success rate\ndeclines significantly. This decrease is attributed to the tendency of LLM agents to hallucinate when\nfaced with fictitious rules, often resorting to random guesses if not guided by a hypothesis. These\nagents reference relevant context information but end up generating a random password. In contrast,\nthe Function Operator puzzles, governed by mathematical rules, provide more concrete guidance\ngents. Detailed instances of incorrect guesses and their analysis are provided in the appendix, as\nshown in Figure 12.\nIDEA Agent Significantly boosts Success Rates: The implementation of our proposed IDEA\nagent leads to an approximate 10% increase in the overall success rates of Llama3-70B, GPT-3.5,\nand GPT-40, compared to the baseline. This improvement demonstrates that incorporating an ab-\nduction reasoning loop substantially boosts the performance of LLM agents when addressing prob-\nlems governed by unseen rules in unfamiliar environments. Moreover, while the performance gap\nbetween GPT-40 and Llama3-70B slightly increases, particularly in Room Escape puzzles, Llama3-\n70B outperforms GPT-40 in the other two puzzle types. This suggests that GPT-40 is better at\nforming simple and intuitive hypotheses. Conversely, models like Llama3-8B and Gemma-7B ex-\nperience a slight decline in performance compared to the baseline. This decrease stems from IDEA\nagent's increased complexity, which demands that agents manage larger contexts\u2014a challenge that"}, {"title": "5.3 ANALYSIS", "content": "LLM agents frequently repeat previous actions instead of exploring new ones. The IDEA\nagent, however, effectively reduces this tendency, improving their exploratory behavior: In the\ncontrolled environment of our puzzles, where each interaction yields deterministic results, repeating\nthe same action is generally unproductive. We calculated the average number of duplicated actions\nperformed while solving each puzzle, with detailed statistics shown in Appendix 2. We observed that\nmost LLMs commonly repeat actions. For models like GPT-4o and Llama3-70B, this issue is less\npronounced, but even these advanced models cannot completely avoid retrying actions that yield no\nnew information. Additionally, we noticed that IDEA agent significantly reduces action repetition.\nIDEA agent better discerns whether current observations are sufficient or if further specific evidence\nis necessary to reveal the underlying rule and achieve the goal. For example, in the Room Escape\npuzzle, IDEA agent avoids unnecessary assignments to a simple function when the already gathered\nevidence suffices to determine the needed value. Examples are provided in the Table 9.\nLLMs fall short in both generating and leveraging hypotheses in unfamiliar environments:\nThe Function Operator puzzles are based on established mathematical rules and Room Escape puz-\nzles rely on fictional rules. When rules are provided (Deduction-only), these models achieve sig-\nnificantly higher success rates on Room Escape puzzles compared to Function Operator puzzles.\nHowever, when the rules are hidden (IDEA), the gap between the success rates in Function Opera-\ntor and Room Escape puzzles narrows dramatically. For models like GPT-3.5 and Llama3-70B, the\nsuccess rate even surpasses that of Room Escape puzzles. This suggests that LLMs are capable of\ndeducing specific function forms from input and output data but struggle to uncover intuitive and\nsimple rules in the Room Escape context.\nIn the case of Reactor puzzles, we found that the rules are comparatively hard to describe in natural\nlanguage, and agents also struggle with deducing the reaction formula from the rule. The hypotheses\ngenerated by the agents are often incorrect, and they do not adhere to these hypotheses effectively,\nwhich ultimately decreases the success rate.\nInability to Refine Current Assumptions with New Findings In our experiments with IDEA\nagent, some puzzles were solved without any refinement to the initial hypotheses. The number of\npuzzles solved with rule refinement is usually less than 200 (60%). However, in our design, only\na few puzzles can be successfully resolved using the initial hypotheses formed with insufficient\nobservations. This problem is especially pronounced in Reactor puzzles, where agents often fail"}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce RULEARN, a new benchmark to evaluate the rule-learning abilities\nof language agents in interactive environments. We propose IDEA agent, a novel language agent\nencompassing the full reasoning loop of abduction, deduction, and induction, mimicking the human\nrule-learning process. Through comprehensive experiments with five popular LLMs, we demon-\nstrate that IDEA significantly enhances the rule-learning capability compared to strong baselines.\nDespite these advancements, several challenges persist. These include the generation of valid hy-\npotheses, the avoidance of repetitive actions, and the adaptation of strategies when faced with new\ninformation. We believe that RULEARN offers a valuable benchmark for future research into de-\nveloping language agents capable of human-like rule-learning in novel environments."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 FIGURES", "content": ""}, {"title": "A.2 TABLES", "content": ""}, {"title": "A.3 IDEA AGENT DETAIL", "content": ""}, {"title": "A.3.1 ENVIRONMENT ENTITIES", "content": "\u2022 Agent(A): Represents the entity focused on rule-learning and problem-solving, comprising\nthe following components:"}, {"title": "A.3.2 INTERACTIVE FUNCTIONS", "content": "\u2022 Perceputal Action:= \u00e2(O): An action automatically added to the agent's action space for\nall objects within the same scope. Upon perceiving an object, the agent gains the ability to\ninteract more detailedly with it, adding its interactive actions to the S.\n\u2022 Interactive Action:= \u00e3(D\uff61, G, H, P, I, M, M): A callable action within On that, once\nexecuted, triggers a pre-coded effect based on the agent's input I. For example, in using a\nreactor, the agent decides the materials and their order of addition, and the reactor processes\nthese inputs based on pre-coded rules to synthesize new materials.\n\u2022 Abductive Action:= \u0101(G, H, P, M, M): An action based on the current observations,\ngoals, prior hypotheses, and previous plans, allowing the agent to formulate or refine hy-\npotheses and generate new plans.\n\u2022 Action select:=F\u2084(G, H, P,M, M, S) \u2192 a: A function where the agent selects an action\nfrom the action space, considering all gathered information."}, {"title": "A.3.3 PSEUDOCODE OF INTERACTIVE RULE LEARNING PROCEDURE", "content": ""}, {"title": "A.4 PROMPT EXAMPLE", "content": ""}, {"title": "A.4.1 FUNCTION OPERATOR PUZZLES", "content": ""}, {"title": "A.4.2 ROOM ESCAPE", "content": ""}, {"title": "A.4.3 REACTOR PUZZLES", "content": ""}]}