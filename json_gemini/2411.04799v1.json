{"title": "Kwai-STaR: Transform LLMs into State-Transition Reasoners", "authors": ["Xingyu Lu", "Yuhang Hu", "Changyi Liu", "Tianke Zhang", "Zhenyu Yang", "Zhixiang Ding", "Shengsheng Qian", "Meng Du", "Ruiwen Kang", "Kaiyu Tang", "Fan Yang", "Tingting Gao", "Di Zhang", "Hai-Tao Zheng", "Bin Wen"], "abstract": "Mathematical reasoning presents a significant challenge to the cognitive capabilities of LLMs. Various methods have been proposed to enhance the mathematical ability of LLMs. However, few recognize the value of state transition for LLM reasoning. In this work, we define mathematical problem-solving as a process of transiting from an initial unsolved state to the final resolved state, and propose Kwai-STaR framework, which transforms LLMs into State-Transition Reasoners to improve their intuitive reasoning capabilities. Our approach comprises three main steps: (1) Define the state space tailored to the mathematical reasoning. (2) Generate state-transition data based on the state space. (3) Convert original LLMS into State-Transition Reasoners via a curricular training strategy. Our experiments validate the effectiveness of Kwai-STaR in enhancing mathematical reasoning: After training on the small-scale Kwai-STaR dataset, general LLMs, including Mistral-7B and LLaMA-3, achieve considerable performance gain on the GSM8K and GSM-Hard dataset. Additionally, the state transition-based design endows Kwai-STaR with remarkable training and inference efficiency. Further experiments are underway to establish the generality of Kwai-STaR.", "sections": [{"title": "1 Introduction", "content": "As an intricate problem for artificial intelligence(Ahn et al., 2024), mathematical reasoning poses a significant challenge to the cognitive abilities of large language models (LLMs). Recently, various efforts have been dedicated to enhancing LLMs' mathematical capabilities, achieving significant progress.\nA range of work focuses on enhancing the inherent mathematical abilities of LLMs through training: GPT (Achiam et al., 2023) and LLaMA (Dubey et al., 2024) incorporate massive mathematical corpus to improve general intelligence, Open-Math (Toshniwal et al., 2024) and MetaMath (Yu et al., 2023) specialize general LLMs into math-tailored models with large-scale in-domain datasets. For these methods, organizing mathematical corpus into structured data instances becomes an essential step. Another line of research, including CoT (Wei et al., 2022) and Best-of-N (Stiennon et al., 2020), explores how to fully harness the potential of LLMs during inference to boost mathematical performance. Recently, Monte Carlo Tree Search (Qi et al., 2024; Zhang et al., 2024) and Process Reward Model (Wang et al., 2024) have been applied; they achieve remarkable results by decomposing problem-solving process into multiple steps and providing intermediate rewards.\nWhether for training or inference paradigm, the modeling of mathematical reasoning process is playing an increasingly significant role, which determines how to appropriately organize the corpus and decompose the mathematical problem. In this paper, we provide a novel perspective to organize the reasoning process of LLMs: the solution of a mathematical problem can be intuitively viewed as an ordered sequence of transition from an initial unsolved state (original question) to a final resolved state (correct answer).\nBased on this observation, we manage to design a state transition paradigm to assist mathematical reasoning of LLM. We present Kwai-STaR, a framework to transform general LLMs into state-transition reasoners, which systematically solve problems by performing state transition.\nKwai-STaR takes three steps to incorporate state transition with LLM reasoning: (1) State space definition: We develop an action set and constrain the LLM to solve problems as a State-Transition Reasoner (STaR): The LLM takes one action from the action set at a time to transition the current state into a new state. (2) State-transition data construction. Although advanced LLMs possess remarkable instruction-following capabilities (Table 2), we still take appropriate training to help general language models master state-by-state reasoning. Hence, we construct a small-scale state-transition dataset with meticulous generation instructions and pipeline. (3) Curricular Training Strategy. Our dataset contains two types of instances: a majority of correct cases and a minority of wrong-then-verified cases from the data generator and trained reasoner. To maximize learning efficiency, our training strategy consists of two stages: a fundamental stage and an advanced stage. The former trains the model with the majority right cases, enabling it to grasp the state-transition manner and to solve relatively simple problems. The latter leverages pairs of wrong and verified cases to further strengthen the model's proficiency.\nUsing the above method, we construct a small-scale state-transition dataset from the GSM8K training data, our dataset contains 20K right instances and approximately 3K wrong-then-verified instances. We test Kwai-STaR's effect on various general LLMs. Our experiment indicates that Kwai-STaR framework significantly improves the performance of all tested general LLMs on two benchmarks. Moreover, compared to other data augmentation methods, the state-transition-based Kwai-STaR dataset offers higher data efficiency. In contrast to inference-time enhancement methods, Kwai-STaR achieves single-pass accuracy comparable to those methods' multi-pass accuracy, without complex inference paradigm.\nWe summarize our contributions as follows:\n\u2022 We provide the novel perspective of state transition to model the mathematical reasoning of LLMs and construct a state-transition dataset.\n\u2022 We propose Kwai-STaR framework to enhance LLM reasoning through state transitions. Kwai-STaR effectively transforms models of various scales into STaRs, significantly improving their mathematical performance.\n\u2022 Kwai-STaR achieves remarkable performance and efficiency, revealing the great potential of state-space strategies in enhancing LLM reasoning. We are actively extending our state-space strategies to broader scenarios."}, {"title": "2 The Kwai-STaR Framework", "content": "In the following sections, we first introduce the relevant concepts of the State-Transition Reasoner, then describe three steps of Kwai-STaR to transform LLMs into state-transition reasoner: 1) Define the mathematical state space. 2) Construct high-quality state-transition data. 3) Teach LLMs to solve problems by state transition."}, {"title": "2.1 Relevant Concepts", "content": "We adopt RL concepts to formalize the mathematical problem solving as a state transition process.\n\u2022 State: A specific point in the problem-solving process ranging from the initial state (original question) to the final state (correct answer).\n\u2022 State Transition Reasoner: A LLM that systematically solves problems by executing actions in the action set to transition from the initial state to the final state.\n\u2022 Action: Operations performed by the reasoner to transit states. Kwai-STaR's action set includes 7 kinds of actions"}, {"title": "2.2 Kwai-STaR State Space", "content": "The design of Kwai-STaR's state space is guided by the divide-and-conquer principle. First, the reasoner formalizes the original problem into mathematical expressions, listing the variables and their relationships. Then, the reasoner decomposes the complex problem into multiple subquestions and solves them individually. Finally, the reasoner combines subquestions' answers to solve the original problem. If an incorrect intermediate result is generated, the reasoner is expected to perform a Verify action and backtrack to the last correct state.\nThrough the above definitions, the reasoning process of the LLM is formalized as a series of state transitions within the state space."}, {"title": "2.3 State-Transition Data Construction", "content": "We initially validate the effectiveness of our state space in enhancing LLMs' reasoning capabilities by prompt engineering: We compare the accuracy of models prompted by CoT and Kwai-STaR instructions on GSM8K and MATH benchmark. As shown in Table 2, Kwai-STaR instruction significantly improve the accuracy of GPT-4o and LLaMA-3.1. This indicate that our state space contributes the reasoning process of LLMs.\nAlthough the pre-experiment has shown that state-of-art LLMs can follow instructions to perform state transition, the ability of smaller and weaker models may be not guaranteed. Thus, to improve reasoning ability of various LLMs broadly, we construct state-transition reasoning data with small scale and high quality to teach LLMs how to perform state transition.\nWe construct Kwai-STaR dataset with training sets of several math benchmarks. The data construction process of Kwai-STaR involves two stages: In Stage I, the action list in the instructions excludes Verify and Backtrack, the data generator act as students to solve problems with the provided actions. In Stage II, for the wrong cases from the data-generator and fine-tuned models on train set, we supply reference answers and instruct the data generator to correct the erroneous cases as a teacher, using the complete action list. The state transition paths in incorrect and verified cases naturally form accept-reject pairs, which serve as reinforcement learning samples in the Advanced Refinement stage of training."}, {"title": "2.4 Transform LLMs into State-Transition Reasoner", "content": "Corresponding to the data construction process, Kwai-STaR's training process also consists of two progressive stages: Fundamental Training and Advanced Refinement, each stage uses the data from the respective data construction stage for training. Kwai-STaR's training strategy can also be viewed as a state-transition process: As shown in Figure 2, the Fundamental Training stage first transforms standard LLMs into basic State-Transition Reasoners, then the Advanced Refinement stage further equip the basic reasoner with superior mathematical proficiency.\nIn the Fundamental Training stage, the correct cases from Stage I are adopted to train the model with commonly used next-token prediction loss:\n$L_{NTP} = -\\sum_{t=1}^{T} log P(y_t | Y_{<t}; \\theta)$\nHere's the translation:\nThe correct cases constitute the majority of the training set and are relatively less challenging, hence they are assigned to help the model master the process of solving mathematical problems through state transitions and learning solution methods for most problems.\nIn the Advanced Refinement stage, we take accept-reject pairs from more challenging wrong cases as data for DPO training.\n$L_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = -E_{(x,y_a,y_r)~D}[\\beta log \\sigma(\\frac{\\pi_{\\theta}(y_a|x)}{\\pi_{ref}(y_a|x)} - log \\sigma(\\frac{\\pi_{\\theta}(y_r|x)}{\\pi_{ref}(y_r|x)} )]$\nIn this formula, $\\pi_{ref}$ refers to the language model trained in the fundamental stage, $\\pi_{\\theta}$ denotes the language model updated through DPO, $y_a$ represents the verified accepted solution, $y_r$ is the rejected wrong solution, and x stands for the input problem. Since the wrong cases stem from the mistakes made by the generator and the trained model, they are challenging enough to teach the model how to tackle hard problems and further addressing the shortcomings left by Fundamental Training.\nAfter the two-stage training, the model acquires outstanding capability to solve problems by state transitions. During inference, the model can explore the state space, reasoning from the input problem to achieve the final answer."}, {"title": "3 Experiments", "content": "3.1 Experiment Setup\nDatasets. We choose GSM8K benchmark for experiment. On its training split, we adopt advanced language models including LLaMA-3.1-Insturct-70B and GPT-4o as generator to construct Kwai-STaR dataset, while strictly limiting the data scope.\nModels. We employ state-of-the-art LLMs including Mistral-7B(Jiang et al., 2023), LLaMA3-Instruct series, Phi3-mini-4k(Abdin et al., 2024) as base models, ranging from 4B to 8B.\nCompared methods. We select three types of powerful methods as baselines: (1) Training refinement: including MetaMathQA. (2) Inference enhancement: such as Self-Consistency and LLaMA-Berry. (3) Regular methods: including CoT, few-shot and SFT.\nDetails. For methods requires training, we follow their reported configurations. We run all experiments with LLaMA-Factory framework on same device for consistency. To maintain same parameter count, our experiment does not include reward models in some methods. For Kwai-STaR, all models are fine-tuned with LoRA for efficiency."}, {"title": "3.2 Primary Results", "content": "In Table 5, we present the accuracy of Kwai-STaR compared to other baselines on the GSM8K and GSM-Hard Benchmark. As can be seen, four Kwai-STaR models achieves the highest accuracy on the GSM8K dataset. On the GSM-Hard dataset, Kwai-STaR also demonstrates either optimal or sub-optimal performance. Since the current training data only includes the training set of GSM8K, the success on GSM-Hard demonstrates that Kwai-STaR's enhancement of LLM reasoning can adapt to more complex mathematical scenarios.\nBeyond performance gain, Kwai-STaR also offers efficiency advantages compared to other baselines: (1) Training: Compared with data-augmentation methods such as MetaMATH, Kwai-STaR achieves greater performance improvements with a smaller data scale and fewer trainable parameters. We attribute this to the high quality and structured format of Kwai-STaR's data, demonstrating the feasibility of state transitions in mathematical problem-solving. (2) Inference: Compared to inference-time enhancement methods like CoT, Self-Consistency, and MCTS-based approaches, Kwai-STaR's majority@1 accuracy is comparable to their majority@n accuracy, yet without complex inference paradigms and expensive inference cost. These advantages imply that Kwai-STaR effectively enhances the LLMs' intuitive reasoning ability with minimal training costs, we are convinced that the significant advantages of Kwai-STaR demonstrate the feasibility and great potential of state-transition paradigms for enhancing the reasoning capabilities of LLMs. We are managing to provide more experiment results and analysis to further show Kwai-STaR's ability."}, {"title": "4 Conclusion and Future Work", "content": "In this paper, we introduce the Kwai-STaR approach: By defining the state space, constructing state-transition data, and applying progressive training strategy, we transform a general LLM into a state-transition reasoner, thereby enhancing its reasoning capabilities for tackling mathematical problems. Our experiments show that Kwai-STaR significantly enhances LLMS' mathematical performance, demonstrating the crucial role and potential of state spaces in supporting LLM reasoning.\nDue to time constraints, currently we have not finished testing Kwai-STaR's feasibility in other scenarios. We are actively working on it to provide additional experimental results in more diverse and general settings to further demonstrate the generalizability of the Kwai-STaR approach."}, {"title": "5 Limitations", "content": "The current Kwai-STaR has only validated its effectiveness in the field of mathematics. While the mathematical domain is both challenging and representative, the potential of state-space for enhancing LLM reasoning in general scenarios remains unverified, which limits the generalizability of the Kwai-STaR. We are working on releasing updated versions of Kwai-STaR to address this limitation. Currently, the design of the state space is primarily manual. Although this approach has yielded good results, it lacks completeness and automation, which are also directions for our future work. Currently, we lack a theoretical explanation of how state space improves LLM reasoning."}]}