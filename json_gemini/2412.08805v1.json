{"title": "Autoformalizing and Simulating Game-Theoretic Scenarios using LLM-augmented Agents", "authors": ["Agnieszka Mensfelt", "Kostas Stathis", "Vince Trencsenyi"], "abstract": "Game-theoretic simulations are a versatile tool for exploring interactions of both natural and artificial agents. However, modelling real-world scenarios and developing simulations often require substantial human expertise and effort. To streamline this process, we present a framework that enables the autoformalization of game-theoretic scenarios using agents augmented by large language models (LLMs). In this approach, LLM-augmented agents translate natural language scenario descriptions into executable logic programs that define the rules of each game, validating these programs for syntactic accuracy. A tournament simulation is then conducted, during which the agents test the functionality of the generated games by playing them. When a ground truth payoff matrix is available, an exact semantic validation can also be performed. The validated games can then be used in further simulations to assess the effectiveness of different strategies. We evaluate our approach on a diverse set of 55 natural language descriptions across five well-known 2 \u00d7 2 simultaneous-move games, demonstrating 96% syntactic and 87% semantic correctness in the generated game rules. Additionally, we assess the LLM-augmented agents' capability to autoformalize strategies for gameplay.", "sections": [{"title": "1 INTRODUCTION", "content": "Game-theoretic simulations are a versatile tool for exploring diverse phenomena, ranging from individual human interactions and organisational dynamics to biological processes, robotics, and interactions among artificial agents. However, modelling real-world scenarios and implementing corresponding simulations typically require substantial human expertise and effort. Consider, for instance, the problem of autonomous vehicles coordination: designing a simulation model involves specifying the rules of interaction, the objectives of each agent, the information available to them, and the strategies they may employ, all while accounting for uncertainties and external factors such as traffic laws and pedestrian behaviour; that model subsequently has to be implemented. Recent advancements in Large Language Models (LLMs) enable a new approach to streamline these tasks.\nWhile LLMs have been evaluated as agents in game-theoretic contexts [2, 6, 14, 20], inherent limitations such as hallucinations [1] and logical and arithmetical errors [17] restrict their utility as direct rational decision-makers. An alternative approach is to leverage LLMs' capabilities in format translation, including the translation of natural language into formal representations. This process, known as autoformalization [38], has shown success in creating formal representations for mathematics [16, 18, 38] and logic [4, 5, 8, 28, 39]. In this work, we apply autoformalization to automatically generate formal representations of game-theoretic scenarios.\nThis work builds on our previous research [25], where we introduced a basic framework for autoformalizing natural language scenarios that can be modelled using 2 \u00d7 2 simultaneous-move games. In the earlier work, a formal solver was used to validate the syntactic correctness of the generated formal game representations, while semantic correctness was evaluated manually.\nIn this work, we integrate the previously introduced autoformalization module into a modular agent model and propose a framework for simulating tournaments with these agents. This framework serves two main purposes. First, by running the tournament, we can automatically verify both the functionality of the generated code and its semantic correctness. This verification can be performed approximately, by comparing the agent's total payoff with the target payoff, or exactly-when the ground truth payoff matrix is available-by checking whether each combination of players' moves produces the intended payoff. Second, the framework's modular agent design supports the autoformalization of strategies, enabling the exploration of how different strategies-whether autoformalized or predefined-perform in autoformalized game scenarios. This approach facilitates the evaluation of strategy efficiency across diverse game-theoretic contexts. The main contributions of this work are as follows:\n\u2022 An agent model with formal game and strategy representation: We introduce an agent model that incorporates a formal representation of both games and strategies, along with an autoformalization module, developed in the previous work.\n\u2022 Three-level agent validation: We provide a mechanism for validating agents for syntactic correctness (through formal solvers), functionality (by employing the code in gameplay), and semantic correctness (by comparing target and achieved payoffs).\n\u2022 A simulation framework for autoformalized scenarios: A framework is introduced for simulating autoformalized game scenarios, enabling the investigation and comparison of different strategies - both autoformalized and predefined.\n\u2022 Experimental evaluation: We evaluate the framework, demonstrating high syntactic and semantic correctness in the context of 2 \u00d7 2 simultaneous-move games. We also test the capability of the agents to autoformalize 5 strategies."}, {"title": "2 PRELIMINARIES", "content": "Game theory is a versatile and popular mathematical modelling framework that captures interaction scenarios pf varying complexity across various disciplines. Rasmusen [30] composes games using four main components:\n\u2022 Players: players consistently and willingly act towards maximising their utility, in full awareness of the situation they are involved in;\n\u2022 Actions: mappings between states and outcomes are actions carried out by the player [27];\n\u2022 Payoffs: payoffs are quantitative measures over the consequent outcomes of action tuples chosen by players [12];\n\u2022 Information: game information describes a player's knowledge about the available actions, historical moves, and the player's image of its opponents [27].\nWe formally define games in this context as the function of a set of n players N, a non-empty set of actions $A_i, \\forall i \\in N$, and for each player a utility function $u_i : A \\rightarrow R$ mapping a player i's action to the corresponding payoff $r_i$.\nIn the case of symmetric games, the payoffs solely depend on the chosen strategies, while with asymmetric games, the outcomes yield different payoffs based on the player's identity. In our experiments, the Prisoner's Dilemma, Hawk-Dove, and Stag-Hunt games are symmetric, and we classify actions U and L as \"Cooperate\" denoted by C and actions D and R as \"Defect\" denoted by D. This allows us to use the well-known terminology from Axelrod's tournaments to define these games in terms of the four outcomes: T: (D, C), R : (C, C), P : (D, D), S : (C, D).\n\u2022 Prisoner's Dilemma is characterised by a dominant temptation to defect, represented by the payoff relation T > R > P > S,\n\u2022 Hawk-Dove entails switched relative payoffs between the punishment for mutual defection and the sucker's payoff T > R > S > P,\n\u2022 Stag Hunt involves a relatively high reward for mutual cooperation over the temptation to defect - R > T > P > S.\nThe Battle of the Sexes and Matching Pennies are asymmetric games. As the outcomes mean different payoffs for row and column, we cannot use the generalization from above and have to define the games from both players' perspectives:\n\u2022 Battle of the Sexes is a coordination game, where both players prefer an agreement but have different preferences over the coordinated outcomes:\nrow: W > Z > {X, Y},\ncolumn: Z > W > {X, Y}.\n\u2022 Matching Pennies: coordinated outcomes reward the row player, while mismatched actions benefit the column player:\nrow: {W, Z} > {X, Y},\ncolumn: {X, Y} > {W, Z}.\n2.2 Large Language Models\nThe rapid development of natural language processing (NLP) through the advancements of transformer architectures [13] and pre-trained models [29] led to the emergence of Large Language Models. State-of-the-art (SOTA) LLMs are evolving at a swift rate, largely due to the progressive increase in model parameters and training dataset size [40]. While SOTA LLMs are improving with general use cases, there are approaches that further improve response accuracy and quality. Fine-tuning is the process of training LLMs on a specific custom dataset, increasing the model's performance in the specific application context captured by the custom data [41]. Chain-of-thought prompting guides LLMs to break down their reasoning into simpler steps, often allowing models to deal with more complex requests or specific contexts more accurately [37].\nOpenAI's GPT-4 Omni [26], employed in this work, is a multimodal SOTA model, that supports textual, visual and audio data as well. GPT4-0 has been shown to perform exceptionally well on reasoning benchmarks [36].\n2.3 General Game Playing\nGeneral game playing [10] focuses on developing intelligent systems that explicitly represent the rules of arbitrary new games and learn to play them autonomously, without human intervention. The Game Description Language (GDL) has been proposed as a formal, machine-processable language for describing the rules of arbitrary games [22]. GDL focused on information games only, so it was extended in GDL-II [34] to cover n-player games with incomplete information and games in extensive normal form [35]. GDL-II is based on the standard syntax and semantics of logic programming and characterised by the special keywords shown in Table 2.\nA challenge with GDL systems is that learning without human guidance demands complex reasoning. Players must infer the possible actions of others, effectively analysing hypothetical game scenarios before making decisions. Action formalisms like the classical Situation Calculus [24] have been developed for precisely this purpose. Formal schemes and inference methods are readily available for Situation Calculus [11, 19], while their deployment in general game playing presupposes a translation from GDL into existing, suitably expressive action languages. One such scheme [33] shows how to fully embed GDL-II into a version of the Situation"}, {"title": "3 LLM-AUGMENTED AUTOFORMALIZING AGENTS", "content": "3.1 Overview\nThe framework introduced in this work enables the creation of modular autoformalizing agents and supports tournament play using these agents. An overview of the agent model is presented in Fig. 1. A logic programming component implemented in Prolog consists of two modules: a Game module containing game-independent and game-dependent rules, and a Strategy module describing which action to select against an opponent. Together, these modules form a solver (detailed in Section 3.2) that encapsulates the game logic and determines the agent's next move. A Python wrapper maintains the agent's interaction with the external environment, manages the game history (tracking each agent's moves, opponent moves, and payoffs), and updates the solver's state. This modular design allows for easy substitution of game-dependent rules and strategies, either through predefined configurations or by autoformalizing natural language descriptions. Agents can also be saved to and reloaded from files. The source code and log files generated during the evaluation are available at\u00b9.\n3.2 Solver\nOur solver is based on logic programming, as described in [25], but repeated here for completeness. It comprises of a game-independent part representing the rules for any extensive-form game, a game-dependent part defining the rules of a particular game using the predicates from the game-independent part, and a set of auxiliary predicates completing the definition of that particular game. We follow standard Prolog conventions to interpret a game: variables are indicated by uppercase letters, while predicates and function symbols by lowercase. The symbol: - is interpreted as if, and the symbol \\+ signifies not (negation by failure). An underscore '_' represents a variable whose value is ignored within a definition. The game state is then a situation, initially denoted by a constant (e.g. s0). The binary function do(M, S) denotes the situation resulting from the executing move M in situation S, consistent with the Situation Calculus.\n3.2.1 Game-independent part. We define all legal transitions of an extended-form game from an initial situation S to a final situation F as follows:\ngame (F, F): -\nfinal (F).\ngame (S, F): -\n\\+ final(S),\nlegal (M, S),\ngame (do (M, S), F).\nThe game terminates when the final game situation F is reached. Otherwise, in a non-final situation S, the game accepts a legal move M, and the game continues in the next do (M, S) situation, until the final situation F is reached. To determine what holds in each legal situation, we use Situation Calculus, represented as:\nholds (F, S):-\ninitially (F, S).\nholds (F, do (M, S)): -\neffect (F, M, S).\nholds (F, do (M, S)): -\nholds (F, S),\n\\+ abnormal (F, M, S).\nholds/2 here is similar to true/1 in GDL, but with an additional parameter of the situation in which the fluent is true. It states that a fluent F holds in the initial situation (init/1 in GDL), a new fluent F is initiated by the effects of a move M executed in a situation S (next/1 in GDL), and a fluent F persists after a move is made, provided it is not abnormal; abnormal fluents do not persist (implicit in GDL). We also use rules of the form:\nfinally (F, S): Conditions.\nto return derived fluents F describing the result of the game, when the Conditions hold in the final situation S.\n3.2.2 Game-dependent part. To represent a specific game we need to define game-dependent predicates for the initial state initial/1, the legal moves legal/2, what holds in the initial game situation via initially/2, the effects of a move on a situation via effect/3, what stops persisting in a situation after the execution of a move via abnormal/3, the final situation final/1, and the result of the game via finally/2 definitions. To exemplify these definitions, we show how to describe a PD game to our solver. The initial situation s0 is defined as:\ninitial (s0).\nWhat holds in this initial situation we specify it as:\ninitially (player(p1), s0).\ninitially (player(p2), s0).\ninitially (role (p1, row), s0).\ninitially (role(p2,col), s0).\ninitially (control (p1), s0).\ninitially (control(p2), s0).\nThese assertions define first the player names represented by unique identifiers (p1 and p2), their roles (p1 is the row player, while p2 is the column player), and then the fact that initially either of them can the game next (via the control/1 fluent, as in GDL). What holds in the initial situation changes as a result of move being made in the game. We represent moves as terms of the form move (P, M), where P is a player, and M is a move. As in PD it is possible for any player to defect ('D') or cooperate ('C'), we express this as:\npossible (move(P, 'D'),S):-holds(player(P),S).\npossible (move(P, 'C'),S): - holds (player(P),S).\nIt is then legal for a player to make a possible move if they have the control to execute it in the current situation:\nlegal (move(P,M), S):-\npossible (move(P,M), S),\nholds (control(P), S).\nAs an effect of a legal move M being made by a player P, we record in the next situation that the player did that move (effect/3 is like next/1 in GDL):\neffect (did (P, M), move(P, M), S).\nOnce a legal move is executed, the player loses control, which we specify in our framework as:\nabnormal (control (P), move (P, M), S).\nWhen a player loses control cannot play a move until control is passed back. When each player makes a move from the initial situation, the resulting final situation can be determined as:\nfinal (do (M2, do (M1, S))):-\nground (M2),\nground (M1),\ninitial (S).\nAssuming the payoff matrix following the structure presented in Table 1 defined as:\npayoff('D', 'D', 1, 1).\npayoff('C', 'D', 0,5).\npayoff('D', 'C', 5,0).\npayoff('C', 'C', 3, 3).\nthe outcome of the game holds information about the actual moves made by the players, and their payoffs.\nfinally (outcome (P1, M1, U1, P2, M2, U2), S):-\nholds (role (P1, row), S),\nholds (did (P1, M1), S),\nholds (role (P2, col), S),\nholds (did (P2, M2), S),\npayoff (M1, M2, U1, U2).\nWe can then extract specific outcome information, e.g. the player's utility, through a goal/2 fluent (as in GDL) e.g.:\nfinally (goal(P1, U1), S):-\nfinally (outcome (P1,, U1, _, _, _ ), S).\nfinally (goal(P2, U2), S):-\nfinally (outcome(_,_,_,P2,,U2), S).\nThis finalises the definition of PD, enabling us to use the game description above to reason about the game's dynamics. For instance, if player p1 wanted to analyse the game and identify the actions required to achieve a utility of 5, then it asks the query:\n?- game (s0,F), finally(goal(p1,5),F).\nApplying the PD payoff matrix, our solver produces two solutions for F, separated by semicolons (;). The result false indicates \"no further answers\":\nF = do (move (p2, 'C'), do(move (p1, 'D'),0));\nF = do (move (p1, 'D'), do (move (p2, 'C'),0));\nfalse.\nIn the first solution, p1 acts first and p2 second, while in the second solution, the order is reversed. This outcome is expected, as both players have initial control in our game definition, resulting in both action order combinations being shown.\n3.2.3 Strategy. Strategies are submitted separately from the game-dependent component. In the current implementation, they may rely on specific predicates - default_move/2, opposite_move/2, possible/2, and finally/2 - which are required from the game-dependent solver. The select/4 predicate is used to determine the next move M of player P against opponent 0 in game situation S, as illustrated in the tit-for-tat strategy example below:\nselect (P, O, S, M):-\n\\+ holds (last_move (O, _LMo), S),\nholds (default_move (P, M), S).\nselect (P, O, S, Mo): -\nholds (last_move (O, Mo), S).\nHere, initially P selects the default move (typically to cooperate), otherwise it mirrors the opponent's move in the last round. More sophisticated strategies can be expressed. For example, below we show how to express the best response of a player for a game as:\nselect (P, O, S, M):-\n\\+ holds (last_move (O, _), S),\nholds (default_move (P, M),S).\nselect (P, O, S, M):-\nholds (last_move (O, LMo), S),\nfindall (Ui-Mi, \n(game (S, F),\nfinally (outcome (P, Mi, Ui, O, Mo, Uo),F),\nUi >= Uo),\nOptions),\nsort(0,@>, Options, Ranked),\nhighest (Ranked, M).\nThis second strategy differs from the previous one on tit-for-tat in that its second clause illustrates how to use our representation of games to perform complex reasoning with the game rules. This is done by ranking all legal future outcomes in a given situation according to he utility these have for the player and the opponent, and select the move that gives the highest utility for the player. This strategy also demonstrates the complexity of the autoformalization task.\n3.2.4 Constraint checking. We check that the auto-formalized payoff matrix satisfies the structural and content constraints of the specified game (as outlined in Sect. 2.1), by defining validity constraints. The listing below specifies the payoff matrix validity constraints for the Prisoner's Dilemma.\nvalid_pd_payoffs (T, R, P,S,C,D): -\npayoff (C,C,R,R),\npayoff (C, D,S,T),\nT>R,\npayoff (D, C,T,S),\npayoff (D,D,P,P),\nR>P,\nP>S.\nSince the auto-formalization module does not impose constraints on the names of actions, this predicate also identifies which actions correspond to specific roles (e.g., determining that 'coop' = C).\n3.3 Autoformalization Module\nTo perform autoformalization, a Large Language Model is employed. The framework supports the use of any language model by providing an abstract interface. Autoformalization of a natural language description of a game or strategy is achieved through one-shot prompting, using game-dependent predicates for the Prisoner's Dilemma and the tit-for-tat strategy as examples. After generating the code, its syntactic correctness is validated. If errors are detected, the LLM receives feedback, including the erroneous lines, error messages, and instructions for correcting the predicates. The max_attempts parameter specifies the maximum number of correction attempts. The overview of autoformalization algorithm is presented in Listing 1.\n3.4 Simulation\nIn our previous work [25], we manually validated the semantic correctness of generated programs. In this study, we introduce a tournament-based mechanism to automate the semantic correctness validation for scenarios and strategies where a target total payoff or a payoff matrix is available. In this approach, once a scenario is autoformalized, a copy of an agent is created and supplied with a specified strategy. The agent and its copy then play a specified number of rounds (clones mode). Comparing the agent's total payoff to the target total payoff provides an approximate validation. When a ground truth payoff matrix is available, it enables exact semantic validation by verifying whether the payoff received for each unique combination of moves matches the target payoff for that combination. Additionally, we perform constraint checking to ensure that the payoff matrix satisfies the specified constraints for a specified game.\nThe tournament can also evaluate which strategy is most successful, on average, within a given game scenario across a specified number of rounds. This is achieved by supplying agent copies with different strategies and having them compete in a round-robin mode, where each agent plays against every other.\nThe main parameters of a tournament are as follows:\n\u2022 game: can be provided by predefined rules or natural language description to autoformalize,\n\u2022 strategies: can be provided as a list of predefined strategy rules or a list of natural language descriptions to autoformalize,\n\u2022 mode: playing against a single clone or round-robin,\n\u2022 agents number,\n\u2022 rounds number."}, {"title": "4 METHODS", "content": "4.1 Parameters\nTo evaluate the framework, we conducted three experiments: Experiment 1 tested the autoformalization of game scenarios, Experiment 2 employed the autoformalized game rules in tournament's round-robin mode, and Experiment 3 examined the autoformalization of strategies. Table 3 provides the experimental parameters shared across all experiments, while Table 4 details the parameters unique to each specific experiment.\n4.2 Exp. 1: Autoformalization of Game Descriptions\nIn the first experiment, we used a dataset of 55 natural language game-theoretic scenarios to evaluate the syntactic and semantic accuracy of logic programs generated by agents. The dataset included 5 scenarios that employed common metaphors (e.g., two prisoners in the Prisoner's Dilemma) for the five canonical games introduced in Section 2.1. The remaining 50 scenarios - 10 for each game - used alternative descriptions that avoided these typical metaphors (e.g., two politicians competing in a campaign rather than two prisoners in the Prisoner's Dilemma). For each scenario, five agents were generated with autoformalized game rules and then engaged in tournament, using the tit-for-tat strategy against a clone employing the anti-tit-for-tat strategy, testing all four possible pairs of moves in four rounds. Each agent had a maximum of five attempts to produce syntactically correct code; agents failing after five attempts were labelled as syntactically incorrect. Post-tournament, each agent's individual and total payoffs were compared to the target, with matches indicating semantic correctness.\n4.3 \u0415\u0445\u0440. 2: Axelrod's Tournament\nIn the second experiment, we selected five agents created in Experiment 1, one for each of the five games. For each game, we conducted a tournament based on the design of Axelrod's tournament [3]. In this setup, six copies of each agent were created and assigned one of six strategies; each agent then played against every other agent, including itself. The strategies used are listed in Table 5.\nTo adapt the strategies to games without explicit cooperation and defection concepts, we defined a \"default move\" and its \"opposite\" as the basis for certain strategies. The default moves were: \"Cooperate\" for the Prisoner's Dilemma, \"Dove\" for Hawk-Dove, \"Stag\" for Stag Hunt, \"Opera\" for the Battle of the Sexes, and \"Heads\" for Matching Pennies.\nThe main metric for evaluating strategy performance was the total payoff achieved by an agent using a given strategy in each game. To allow for cross-game comparisons of strategy effectiveness, total payoffs were normalized with the following formula:\nnorm_payoff = $\\frac{total\\_payoff - min\\_total\\_payoff}{max\\_total\\_payoff - min\\_total\\_payoff}$\nTotal_payoff refers to the sum of the payoffs an agent collects across all rounds and min_total_payoff and max_total_payoff represent the minimum and maximum total payoffs achievable, respectively.\n4.4 Exp. 3: Autoformalization of Strategies\nWe then evaluated the semantic correctness of the autoformalized strategies. To achieve this, we used the natural language description of the tit-for-tat strategy, along with its Prolog implementation, as a reference example. For the other five strategies, the autoformalization module was provided with their natural language descriptions, similar to those shown in Table 5.\nEach autoformalized strategy was then assigned to an agent with predefined Prisoner's Dilemma game rules. This agent played four rounds against a clone utilizing the anti-tit-for-tat strategy. The agent's total payoff was compared to a target payoff specific to each strategy to assess correctness. For the random strategy, correctness was also manually validated. This tournament setup was repeated five times for each strategy to ensure consistency in results."}, {"title": "5 RESULTS AND DISCUSSION", "content": "5.1 Exp. 1: Autoformalization of Game Descriptions\nTable 6 shows the distribution of attempts needed to generate syntactically correct code for the 275 agents created in the first experiment. For 93% of agents, syntactically correct code was generated on the first attempt. When the correct code was not generated initially, 7% of agents succeeded on the second attempt, while 12% could not produce correct code within the allowed attempts. This latter case highlights a need for further refinement of the feedback loop to improve success rates. However, the examination of the non-aggregated distribution of the invalid agents showed that an agent without syntactically valid game program was generated in no more than 1 in 5 instances.\nThe overall syntactic correctness rate was 96%, while semantic correctness reached 87%, closely aligning with the results of our previous work (98% and 88%, respectively). Figure 2 depicts the distribution of syntactic and semantic correctness across various games. The Battle of the Sexes and Stag Hunt emerged as the most error-prone scenarios, exhibiting the lowest semantic correctness rates, though still exceeding 80%.\nThe total-payoff-based approximate validation demonstrated 88% semantic correctness, while the exact individual-payoff-based validation achieved 87%. Five instances were incorrectly labelled as semantically correct by the approximate method, indicating that the results of the approximate and exact methods were closely aligned. A comparison of constraint checking and exact semantic validation revealed a few cases where the autoformalized payoff matrix satisfied the constraints, but errors in other parts of the program - such as role assignment - resulted in incorrect payoff assignments. This demonstrates that exact validation is feasible only when the ground truth payoff matrix is available.\n5.2 Exp. 2: Axelrod's Tournament\nFigure 3 presents the normalized total payoffs for each strategy across all games, while Table 7 provides the average normalized total payoff for each strategy across games. Overall, the best-response strategy proved to be the most effective on average, followed closely by tit-for-tat. Best-response was most effective in all games except the Battle of the Sexes and the Prisoner's Dilemma. In the Prisoner's Dilemma, best-response achieved a lower payoff than the anti-default-move strategy (analogous to \"defect\"), as it initiated with the \"cooperate\" move. The results demonstrate that agents with autoformalized rules can effectively simulate and assess the performance of various strategies within different game-theoretic scenarios.\n5.3 Exp. 3: Autoformalization of Strategies\nTable 8 presents the percentage of correctly autoformalized strategies. The simpler strategies-default-move, anti-default-move, and anti-tit-for-tat-were always correctly autoformalized. In contrast, the random strategy achieved correct autoformalization in 60% of cases, and best-response, which requires selecting the move that maximizes payoff in response to the opponent's previous move, was correctly autoformalized in 40% of cases. Since all strategies were generated using only the tit-for-tat implementation as an example, increasing the diversity of examples provided to the autoformalization module could potentially improve its accuracy in correctly formalising more complex strategies."}, {"title": "6 RELATED WORK", "content": "Originally proposed for agents in game scenarios, the Game Description Language (GDL) offers a promising approach for formalising complex strategic interactions in multi-agent environments by providing a declarative framework to define game rules, legal moves, and outcome conditions [10]. Agents that employ GDL are often used to systematically explore game states and strategic options, supporting rigorous analysis across both cooperative and competitive games, as suggested by previous approaches (e.g., see [32]). However, these approaches typically rely on human-driven formalisation of specific games, a limitation addressed by the LLM-augmented agents and the GDL representation used here, which enable autoformalization and enhance flexibility.\nThere is a growing body of research exploring the use of LLMs in game theory [21], systematically examining their potential as player agents in game-theoretic scenarios [7], utilising their inference abilities to make decisions in games with imperfect information [15], comparing their behaviour with human participants [9], and deploying them in simulation frameworks [23]. While we share with these works an interest in the broad capabilities and value of LLMs in game theoretic settings, our focus diverges by concentrating on how to use LLMs to generate GDL-like executable game specifications. These formal specifications enable agents to rigorously analyse game situations and reason about possible outcomes, providing a structured and robust foundation for decision-making within game-theoretic frameworks. Our work builds on previous research discussed in [25], significantly extending it by embedding the autoformalization module within an agent model, incorporating the autoformalization of player strategies, implementing a distinct correctness validation process for the autoformalization through tournament play, and conducting experiments to validate the results.\nAutoformalization [38], has shown promising results in translating natural language into mathematical formalisms, enabling the integration of large language models with formal tools for solving reasoning tasks [16, 18, 38]. This approach has also been successfully extended to creating logical representations from natural language descriptions. For instance, Yang et al. [39] applied GPT-3 with few-shot prompting to translate natural language sentences into formal representations, which were then inputted into answer set programs. While effective on NLP benchmarks, this method faced occasional translation errors. To further enhance performance, Pan et al. [28] combined in-context learning with self-refinement based on solver evaluations, surpassing standard LLM and chain-of-thought prompting approaches on logical reasoning benchmarks. In the context of temporal logic, Cosler et al.[5] addressed the problem of translation errors with an interactive system allowing users to refine sub-translations. Fine-tuning LLMs has also proven effective for boosting accuracy in translations to temporal logics[4] and deductive datasets [8]."}, {"title": "7 CONCLUSIONS AND FUTURE WORK", "content": "We have presented a framework for the autoformalization of game-theoretic scenarios, specifically those modelled by 2\u00d72 simultaneous-move games. The framework also enables autoformalization of strategies for these games and the simulation of tournaments based on the autoformalized rules. The generated programs undergo three levels of validation: syntactic validation by consulting a solver, functional testing through gameplay, and semantic validation \u2013 either approximate or exact - by comparing achieved payoffs with target outcomes. Evaluation of the framework demonstrated high syntactic and semantic accuracy of the code generated by agents across various interaction scenarios.\nFuture work will focus on extending the framework to model games beyond 2 \u00d7 2 simultaneous-move scenarios. Our previous work [25] demonstrated that our sample solver for the Prisoner's Dilemma could successfully serve to generate code for structurally similar games, such as rock-paper-scissors (with a different number of moves) and sequential versions of the Prisoner's Dilemma. However, achieving successful generalization to a broader class of games will require assembling a more diverse set of in-context learning examples and developing methods to retrieve the most relevant examples for autoformalization in specific scenarios. The flexibility and generalisability of the introduced framework also make it applicable to games and game-like interactions beyond game theory.\nAnother direction for future work involves enhancing the feedback loop to more effectively leverage a large language model. At present, the LLM receives feedback solely from the trace generated when consulting the solver with the produced code. In the next phase, we plan to expand this feedback mechanism by incorporating runtime errors, thereby creating a more robust autoformalization module. Finally, in cases where unsupervised autoformalization proves infeasible for complex scenarios, we plan to introduce an interactive mode. This mode will allow users to iteratively refine the generated code alongside the autoformalization module. These interactively generated programs will then be added to our existing program library, expanding the resources available for in-context learning and enabling the autoformalization of increasingly complex scenarios."}]}