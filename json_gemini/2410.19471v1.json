{"title": "IMPROVING INVERSE FOLDING FOR PEPTIDE DESIGN WITH DIVERSITY-REGULARIZED DIRECT PREFERENCE OPTIMIZATION", "authors": ["Ryan Park", "Darren J. Hsu", "C. Brian Roland", "Chen Tessler", "Maria Korshunova", "Shie Mannor", "Olivia Viessmann", "Bruno Trentini"], "abstract": "Inverse folding models play an important role in structure-based design by predicting amino acid sequences that fold into desired reference structures. Models like ProteinMPNN, a message-passing encoder-decoder model, are trained to reliably produce new sequences from a reference structure. However, when applied to peptides, these models are prone to generating repetitive sequences that do not fold into the reference structure. To address this, we fine-tune ProteinMPNN to produce diverse and structurally consistent peptide sequences via Direct Preference Optimization (DPO). We derive two enhancements to DPO: online diversity regularization and domain-specific priors. Additionally, we develop a new understanding on improving diversity in decoder models. When conditioned on OpenFold generated structures, our fine-tuned models achieve state-of-the-art structural similarity scores, improving base ProteinMPNN by at least 8%. Compared to standard DPO, our regularized method achieves up to 20% higher sequence diversity with no loss in structural similarity score.", "sections": [{"title": "1 INTRODUCTION", "content": "Engineering biopolymers that fold into desired 3D structures, a computational challenge known as inverse protein folding problem, has broad applications in drug discovery and material science (Yang et al., 2023; Dill et al., 2008; Abascal & Regan, 2018). Several approaches for inverse folding have been adopted over the past decades, from molecular dynamics simulations to machine learning approaches (Dauparas et al., 2022b; Shanker et al., 2023; Hsu et al., 2022a; Yi et al., 2023; Correa, 1990). In the standard machine learning approach, a molecular backbone chain serves as input, and a model generates sequences that adopt folding topologies compatible with the reference backbone. Sequences do not necessarily share sequence homology, as multiple diverse sequences can fold into similar structures (Hsu et al., 2022a; Yue & Dill, 1992; Godzik et al., 1993).\nPeptides, which are small biopolymers comprising 2-50 residues, are interesting targets for inverse folding given their role in diverse biological functions, acting as hormones, neurotransmitters, signalling molecules, or nanostructures assemblers (Chockalingam et al., 2007; Torres et al., 2019; Copolovici et al., 2014; Ulijn & Smith, 2008). Only about 225,000 protein structures have been experimentally determined\u00b9 and made available via the Protein Data Bank (PDB). Training inverse-folding machine learning models in a supervised fashion is a challenging task, due to the complexity of the problem and the limited amount of experimental data. The challenge is aggravated in the peptide domain as fewer than 3.5% PDB structures contain 50 residues or less. In fact, applying the SCOP classification filter in the PDB to display structures labelled as \"Peptide\" reveals only 509 entries, circa 0.2% of all experimentally determined structures available."}, {"title": "2 PRELIMINARIES", "content": "Inverse folding is the problem of inferring the sequence of amino acids y that fold into a given protein structure. This protein structure is represented by a set of coordinates x = (xi, Yi, zi) | i = 1,...,n \u2208 R\u00b3n,where each (xi, Yi, zi) represents the 3D position of a backbone atom and n the number of atoms. The inverse folding problem is underconstrained; there may be many solutions y that fold into structures similar to x (Koehl & Levitt, 1999). Prior research is primarily concerned with recovering the \"ground-truth\u201d sequence yx from the experimental (reference) structure (Dauparas et al., 2022a; Hsu et al., 2022b; Jing et al., 2021b;a). Recently, forward folding models like AlphaFold (Jumper et al., 2021), ESMFold (Lin et al., 2022), and OpenFold (Ahdritz et al., 2022), have made it possible to estimate the structural similarity between generated sequences and the reference structure. In this work, we focus on measuring structural similarity of generated sequences to the reference structure via the self-consistency TM-score (sc-TM) (Gao et al., 2023b).\nProteinMPNN (Dauparas et al., 2022a) is a popular inverse-folding method that produces full protein sequences from backbone features (distances and orientations between backbone atoms, backbone dihedral angles, accounting for a virtual C\u1e9e atom). ProteinMPNN is a 6-layer encoder-decoder message-passing neural network based on the 'Structured Transformer' (Ingraham et al., 2019). Unlike autoregressive methods like ESM-IF1 (Hsu et al., 2022b), ProteinMPNN decodes residues in a random decoding order, as opposed to a fixed left-to-right order from N-terminus to the C-terminus. ProteinMPNN is trained on examples from the Protein Data Bank (PDB) to determine the most likely residues for a given protein backbone. On native protein backbones, it achieves a sequence recovery of 52.4%, compared to 32.9% for Rosetta (Leman et al., 2020). In this work we build upon ProteinMPNN by proposing methods for adapting it to peptide design.\nDPO for inverse folding. Direct Preference Optimization (DPO) is a popular method for aligning Large Language Models to a dataset of human-produced preference assignments, that discriminate amongst the responses grouped by prompt (Rafailov et al., 2023). We adapt DPO to fine-tune inverse-folding models by replacing human preference labels on generated sentences with TM-score rankings on generated sequences. Specifically, we generate a dataset of sequences conditioned on a set of reference structures, and score each sequence with the TM-score computed between its predicted structure and the reference structure. These scores define preference pairs over the generated sequences, for each reference structure."}, {"title": "3 PREFERENCE OPTIMIZATION FOR PEPTIDE DESIGN", "content": "In this section, we consider designing fine-tuning methods well-suited for peptide design. While DPO in its original formulation is useful for fine-tuning in biology (Park et al., 2023; Widatalla et al., 2024; Mistani & Mysore, 2024), we consider how it may be improved to tackle specific problems arising in inverse-folding for peptides (i.e., poor generation diversity and lack of peptide data in initial training). First, we derive a diversity-optimized DPO loss by incorporating an online diversity penalty directly into the top-level Reinforcement Learning objective. Next, we propose an ad-hoc modification to the DPO reward that incorporates scalar TM-scores instead of solely preference pairs, and address the distribution shift between peptides and longer-length proteins."}, {"title": "3.1 ONLINE DIVERSITY OPTIMIZATION", "content": "To encourage diversity in generated sequences while simultaneously maximizing reward (TM-score), we consider a modified DPO objective incorporating an auxiliary diversity reward:\n$\\max_{\\pi_{\\theta}} \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta}(y|x)}[r(x, y)] \u2013 \\beta D_{KL}[\\pi_{\\theta}(y | x) || \\pi_{ref}(y|x)] + \\alpha \\Gamma_{\\gamma}(\\pi_{\\theta})$\n$=\\max_{\\pi_{\\theta}} \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta}(y|x)} \\bigg[ r(x,y) \u2013 \\beta \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)} \u2013 \\alpha \\mathbb{E}_{y'\\sim\\pi_{\\theta}(y' | x)} [\\gamma(y, y')] \\bigg]$\nwhere \u03b1 controls the strength of diversity regularization, \u03b3 is a pairwise distance between sequences, and \u0393(\u03c0\u03b8) is the diversity of policy \u03c0\u03b8 under the distance \u03b3, i.e. \u0393(\u03c0) = Ey,y'\u223c\u03c0 [\u03b3(y, y')]. In practice, we let \u03b3 be the fraction of pairwise different tokens in equal-length sequences y and y'. This is equivalent to the diversity metric defined in (Gao et al., 2023b)."}, {"title": "3.2 LEVERAGING DOMAIN-SPECIFIC PRIORS", "content": "Aligning train set and base model. Supervised fine-tuning (SFT) is a standard part of the DPO pipeline (Rafailov et al., 2023), where a gradient-based optimizer maximizes the log-probability of a target sequence prior to applying DPO. The left part of Fig. 1 shows that the distribution of tokens sampled from base ProteinMPNN and the peptide training dataset differs significantly. SFT assuages the token distribution problem by improving alignment with training distribution, and ensures consistency with previous research, where DPO has been applied to related biological tasks (Park et al., 2023; Widatalla et al., 2024; Mistani & Mysore, 2024).\nIncorporating scalar rewards. Given multiple responses from a single prompt, DPO is derived assuming access to pairwise preferences. However, since we measure the reward of a sequence generation by the TM-score between the original structure and the generated sequence's predicted structure, we have access to a total ordering over generations through scalar rewards for each response. Widatalla et al. (2024) derives weighted DPO to incorporate scalar rewards, but it does not substantially outperform standard DPO.\nWe consider a simple ad-hoc method to incorporate these scalar scores. Given a structure x, K generated sequences yk | x, and K corresponding TM-scores rk | x, we scale the log-probabilities of Tref by the average of rk | x. To see the effect of this scaling, consider the modified implicit reward:"}, {"title": "4 RESULTS", "content": "Here we present results on benchmarks across a suite of inverse folding models evaluated on peptide design tasks, as well as an exploration into the behavior of the two proposed algorithm enhancements (diversity regularization and reward scaling). We find that diversity regularization is effective in improving sampling diversity, and provide justification as to how this improvement happens. Additionally, we find reward scaling produces a small improvement in TM-score, enabling fine-tuned ProteinMPNN to reach SOTA performance in some situations."}, {"title": "4.1 EXPERIMENTAL OVERVIEW", "content": "Datasets. We fine-tune trained ProteinMPNN from Dauparas et al. (2022a) on a set of 211,402 deduplicated peptide structures with length up to 50 amino acids, derived from the ColabFold database (Mirdita et al., 2022) by filtering for predicted local distance difference test (pLDDT) > 80. Each structure was used to generate 4 candidate sequences using pretrained ProteinMPNN with T = 0.1. Each sequence, including the true reference sequence from the structure, was folded with OpenFold (Ahdritz et al., 2022). TM-scores were computed to create a ranking over generated sequences for every structure prompt. Each structure therefore contributed $\\binom{5}{2}$ = 6 chosen-rejected pairs for DPO, for a total of 1,268,412 training pairs. Details of the folding process can be found in Appendix A.1.\nBenchmarks. We consider two non-overlapping benchmarks. First, we take 50 sequences from the OpenFold set, enforcing a sequence identity threshold of 0.4 from the train set and filtering for"}, {"title": "4.2 BENCHMARK SWEEPS", "content": "We benchmark our trained models against standard inverse-folding methods (Jing et al., 2021a;b; Hsu et al., 2022b; Dauparas et al., 2022a) across both the OpenFold and CATH benchmarks, filtered for peptide-length proteins. We consider TM-score to be the most important metric, as unlike sequence recovery, it can more accurately reflect the quality of generated sequences at high diversities. As shown in Table 1, on OpenFold structures, all DPO methods outperform base ProteinMPNN and other models by at least 8%. Diversity-regularized DPO (\u03b1 = 0.1) achieves state-of-the-art (SOTA) diversity, improving base DPO by 20% and even exceeding the diversity of base ProteinMPNN. This is notable, as fine-tuning tend to decrease generation diversity (Wang et al., 2023). Combining reward scaling with diversity regularization does not have a strong beneficial effect, with similar performance compared to the regularized method."}, {"title": "4.3 PARETO FRONT WITH DIVERSITY REGULARIZATION", "content": "In this section, we consider the impact of diversity regularization on DPO. We train four DPO models on top of fine-tuned ProteinMPNN (\u03b1 = {0.0,0.1, 0.2, 0.5}), and generate sequences across a range of temperatures on the OpenFold test split. As temperature increases, the diversity of generated sequences increases, but the average TM-score (reward) decreases. We consider this reward-diversity tradeoff on the left side of Figure 3. At sufficiently low \u03b1 values, diversity-regularized DPO produces a new Pareto frontier. With \u03b1 = 0.1, regularized DPO consistently achieves higher reward at the same diversity, indicating this regularization provides a strictly favorable reward-diversity tradeoff compared to simply increasing temperature. At temperature 0.0, \u03b1 = 0.1 yields a 20% relative improvement in diversity along with a small increase in TM-score (1.5%) over standard DPO.\nHowever, at high \u03b1 values, diversity regularization hurts both diversity and reward. We see symptoms of this in the KL divergences between trained DPO policies and initial fine-tuned ProteinMPNN (middle of Fig. 3). While for \u03b1 = {0.0, 0.1, 0.2}, KL divergence trajectories are similar during training, DPO with \u03b1 = 0.5 produces much higher divergences (i.e., the trained policy deviates significantly more from base ProteinMPNN). In this case, aggressive diversity optimization led to a collapse in model capacity via excessive deviation from the initial policy."}, {"title": "4.4 DIVERSITY REGULARIZATION TARGETS DIFFERENTIAL ENTROPY", "content": "We consider the mechanism behind diversity regularization, showing that diversity improves due to randomness in the token decoding order during sampling. Naively, we would expect diversity regularization to improve the entropy of the sampling distribution $H(\\pi) = -\\mathbb{E}_{x \\sim D, y \\sim \\pi} [\\log \\pi(y|x)]$. In the middle part of Fig. 2, we show that this is not the case. Apart from \u03b2 = 0.5 (an outlier as discussed in Section 4.3), increasing diversity does not increase entropy, which seems contradictory."}, {"title": "4.5 SEQUENCE RECOVERY WITH DIVERSITY-REGULARIZED MODELS", "content": "In this section, we consider how diversity optimization affects ProteinMPNN's ability to recover native sequences from structures, and accurately predict the quality of generated sequences.\nFirst, we explore whether sequence recovery (i.e., the ability to recover the conditioning structure's sequence) is still possible with diversity-regularized fine-tuning. Intuitively, it may seem that models with higher sampling diversity have lower native sequence recovery rate. In the left half of Fig. 4, it seems like this is the case, with sequence recovery rates decreasing as \u03b1 increases across all temperatures. However, for small alpha (\u03b1 = 0.1), the drop in sequence recovery compared to standard DPO is small. Additionally, this recovery gap disappears if we allow for more compute during inference time. In the right side of Fig. 4, we take N samples from each model and compute the best-of-N sequence recovery rate. As N grows, the gap between standard DPO recovery rate and \u03b1 = 0.1 recovery rate shrinks to nearly zero. Therefore, optimizing for diversity does not significantly impact sequence recovery rate, particularly as more sequences are sampled.\nGao et al. (2023b) claims that log-probabilities may correlate with sequence quality, e.g. TM-score. Given that diversity regularization increases log-probability entropy as shown in Section 4.4, one might expect this correlation to be less strong under more diverse models. As shown in the middle of Fig. 4, this is not the case. Indeed, across all models, log-probabilities do not correlate with TM-score."}, {"title": "4.6 REWARD SCALING REINFORCES BIOLOGICAL PRIORS", "content": "Next, we consider the effect of reward scaling on DPO's behavior. On the left side of Fig. 5, the reward-diversity curves for DPO and reward-scaled DPO are computed across T = {0.0, 0.1, 0.2, . . .,1.0}. Reward-scaled DPO achieves consistently higher reward at the same diversity, indicating it is a Pareto improvement over standard DPO. Moreover, as in the right side of Fig. 3, reward-scaled DPO operates on a slightly smaller KL divergence budget compared to standard DPO. Despite maintaining a smaller deviation from pretrained ProteinMPNN, the policy trained with reward-scaled DPO is still strictly better than the policy trained with standard DPO.\nThe motivation for reward scaling, as presented in Eq. 9, is dynamic \u03b2 selection based on the strength of the initial policy (or equivalently, the difficulty of the prompt), where \u03b2 controls the KL divergence"}, {"title": "5 LIMITATIONS", "content": "While we illustrate an empirical connection between diversity optimization and differential entropy, we do not establish a mathematical framework for how this optimization happens. Furthermore, during the derivation of diversity-regularized DPO, we approximate \u03c0* with the latest iteration of gradient descent. Exploring the bounds on this approximation, and establishing a theory-first perspective for the differential entropy framework, are promising directions for future research.\nSince DPO and the variants proposed here are model-agnostic, they can be used to fine-tune any inverse-folding model. Applying DPO to other inverse-folding models may produce even better results compared to ProteinMPNN Gao et al. (2023b). For example, it may be possible to further push the frontier of peptide sequence design by fine-tuning stronger base models like PiFold (Gao et al., 2022) or KW-Design (Gao et al., 2023a)."}, {"title": "6 CONCLUSION", "content": "We fine-tuned ProteinMPNN, a widely adopted inverse folding model, for diverse and structurally consistent peptide sequence generation and proposed diversity-regularized DPO with an online sampling term to accurately estimate and encourage diversity. Domain-specific priors were also incorporated into our methodology to account for peptides' residue distribution and dynamically control the strength of KL divergence regularization. Our approach results in improvements on sampling diversity, sequence recovery and structural similarity of the generated peptide sequences. Furthermore, we give additional intuition on the impact of diversity regularization on differential and discrete entropy. While our results are reported using ProteinMPNN as a base model for fine-tuning, our proposed methods are agnostic to the inverse folding model, setting the grounds for future research in peptide design via fine-tuning."}, {"title": "A APPENDIX", "content": "A.1 ADDITIONAL EXPERIMENTAL DETAILS\nFolding peptide sequences. Folding of peptides was done with the OpenFold module in NVIDIA BioNeMo Framework2, version 1.8, with default settings. We used mmseqs2(Steinegger & Soeding, 2017) to generate multiple sequence alignments (MSAs), referencing against UniRef90, Small BFD and MGnify datasets, as the input. For template searches, hhsearch was used with the PDB70 database. OpenFold inferencing was performed with a single set of weights, converted from an AlphaFold2 (Jumper et al., 2021) model checkpoint params-model-4; typically AlphaFold2 is run with five checkpoints. Folding was run on 8 to 32 NVIDIA A100-SXM4-80GB GPUs, with the overall folding throughput around 1.4 seconds per sequence per GPU. We did not perform structural relaxation after folding. For model checkpoint download scripts, and database download scripts, see github.com/aqlaboratory/openfold.\nChoosing \u03b2 fairly. Since \u03b2 is a proxy for specifying the amount of allowable deviation from the base (reference) policy (Rafailov et al., 2023), we ensure fair comparison between DPO and its variants by modifying \u03b2 so that all methods operate on a similar same KL divergence budget. For the diversity-regularized and reward-scaled methods, we choose \u03b2 = 0.1; for base DPO, we choose \u03b2 = 0.5. In the middle and left parts of Fig. 3, we show that these choices allow all models to deviate from the base policy by around the same amount, with standard DPO still dominating the other model's KL divergences. DPO with \u03b1 = 0.5 is an exception, but we consider this model to be an outlier as described in Section 4.3.\nMethod evaluations. While recent inverse-folding methods like PiFold and KW-Design show strong performance compared to ProteinMPNN (Gao et al., 2023b) and ESM-IF1 Hsu et al. (2022b), we were unable to get their implementation working on the necessary timeline. As a result, we did not include them in our benchmarks."}, {"title": "A.2 DETAILS OF ONLINE DIVERSITY REGULARIZATION", "content": "In Algorithm 1 we present the full online diversity-regularized DPO algorithm. Note that \u03b3 computes the pairwise diversity between a sequence y and N other sequences y', so it returns an N-length vector. is updated only once every K epochs, so in practice we only have to sample once every K epochs. Between these updates, samples are cached and take up only a few megabytes of GPU memory. The modifications compared to base DPO (Rafailov et al., 2023) are highlighted in blue."}, {"title": "A.3 ENTROPY-REGULARIZED DPO", "content": "Here, we consider optimizing for discrete entropy over the sequences sampled from ProteinMPNN. The derivation is the same as in Section 3.1, but with the entropy $H(\\pi) = -\\mathbb{E}_{y \\sim \\pi} [\\log \\pi(y)]$ as the diversity penalty instead of \u0393(\u03c0). The objective is:\n$\\max_{\\pi_{\\theta}} \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta}(y|x)}[r(x, y)] \u2013 \\beta D_{KL}[\\pi_{\\theta}(y | x) || \\pi_{ref}(y|x)] + \\alpha H(\\pi_{\\theta})$\n$=\\max_{\\pi_{\\theta}} \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta}(y|x)} \\bigg[ r(x,y) \u2013 \\beta \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)} \u2013 \\alpha \\log \\pi_{\\theta}(y|x) \\bigg]$\nWith the same approximation $\\tilde{\\pi}$ and modified reward $\\tilde{r}(x, y) = r(x, y) \u2013 \\alpha \\log \\tilde{\\pi}(y | x)$:\n\n\nThe full algorithm is described in Algorithm 2, with deviations from base DPO (Rafailov et al., 2023) highlighted in blue. We find that this algorithm does not produce diversity gains (Table 4) or TM-score gains (Table 3). After entropy regularization, sample entropy does increase by around 25% compared to standard DPO; however, diversity does not improve at temperature 0. This supports the differential entropy theory from Section 4.4, since explicitly increasing discrete entropy does not help improve diversity.\nInterestingly, as shown in Table 4, at higher temperatures, entropy-regularized DPO does slightly increase diversity. This again aligns with the analysis in Section 4.4, as at higher temperatures, the"}, {"title": "A.4 SAMPLED SEQUENCE EXAMPLE STRUCTURES", "content": "In Figures 7 and 6, we present some sequence samples conditioned on structures from both the OpenFold and CATH 4.3 benchmarks. We select pairs where reward scaled DPO and diversity-regularized DPO outperform base ProteinMPNN for visualization."}]}