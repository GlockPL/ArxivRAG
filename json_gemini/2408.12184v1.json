{"title": "RANDOMNESS CONTROL AND REPRODUCIBILITY STUDY OF RANDOM FOREST ALGORITHM IN R AND PYTHON", "authors": ["Louisa Camadini", "Yanis Bouzid", "Maeva Merlet", "L\u00e9opold Carron"], "abstract": "When it comes to the safety of cosmetic products, compliance with regulatory standards is crucial to guarantee consumer protection against the risks of skin irritation. Toxicologists must therefore be fully conversant with all risks. This applies not only to their day-to-day work, but also to all the algorithms they integrate into their routines. Recognizing this, ensuring the reproducibility of algorithms becomes one of the most crucial aspects to address.\nHowever, how can we prove the robustness of an algorithm such as the random forest, that relies heavily on randomness? In this report, we will discuss the strategy of integrating random forest into ocular tolerance assessment for toxicologists.\nWe will compare four packages: randomForest and Ranger (R packages), adapted in Python via the SKRanger package, and the widely used Scikit-Learn with the RandomForestClassifier() function. Our goal is to investigate the parameters and sources of randomness affecting the outcomes of Random Forest algorithms.\nBy setting comparable parameters and using the same Pseudo-Random Number Generator (PRNG), we expect to reproduce results consistently across the various available implementations of the random forest algorithm. Nevertheless, this exploration will unveil hidden layers of randomness and guide our understanding of the critical parameters necessary to ensure reproducibility across all four implementations of the random forest algorithm.", "sections": [{"title": "Introduction", "content": "The role of cosmetics toxicological evaluation is to guarantee the safety of consumer products. This involves substantial investments in time and financial resources for product evaluation. To reduce these investments, machine learning is one of the keys to help toxicologists in decision-making. Over recent years, several models have been developed to compute risk assessment metrics.\nIn a safety context, machine learning is actively used to develop predictive models that can replace animal testing. Various methods have been developed over the past four decades to address increasingly complex needs such as general systemic toxicology or skin sensitization, as mentioned in Burbank, Gautier, and Hewitt 2023 and retrieved from EU. (2009) Regulation (EC) No 1223/2009 of the European Parliament and of the Council of 30 November 2009 on cosmetic products. N.d. and EU. (2010) Directive 2010/63/EU of the European Parliament and of the Council of 22 September 2010 on the protection of animals used for scientific purposes (Text with EEA relevance) n.d.\nTo ensure that a new tool can be integrated into risk assessment strategy, the first and crucial step is the adoption by the community. Given the significant responsibility inherent in every cosmetic product safety report submitted to health authorities, this adoption process necessitates thorough testing of any new model that may be developed. If any doubts regarding the reproducibility of the algorithm's implementation arise, a toxicologist may request an evaluation of its impact on risk assessment.\nUpon initial observation, the use of random forests may seem unsettling. As of now, there is a lack of evidence regarding the reproducibility of this method across the most popular programming languages \u2013 such as R and Python \u2013 despite widespread discussions on performance comparisons as exemplified in Marchese Robinson et al. 2017. To bridge this gap, our study aims to compare four random forest packages in R and Python, thereby assessing the reproducibility of their outcomes.\nThe article will begin with a brief section on random forests, as understanding how they work is essential for revealing sources of randomness. This will be followed by a review of existing packages, then the methods for deciphering the differences between them and the associated results. At the end of the article, a clear procedure will enable us to achieve perfect reproducibility between the studied packages."}, {"title": "Random forest algorithm", "content": "The random forest algorithm combines the concepts of CART (Classification and Regression Trees) and bagging (bootstrap aggregation), whose concepts are fully detailed in Biau and Scornet 2015.\nBagging uses the bootstrap method to train decision trees on different samples of the dataset, then aggregates their predictions to make a final decision. This approach improves model stability and robustness by reducing variance and over-fitting. However, it adds an extra layer of randomness to the construction of each tree, making model reproducibility challenging.\nIn addition, the random forest introduces randomness by selecting only a subset of candidates features (m candidates among the total number of features available p) at each node split, instead of considering all features. This random selection helps decorrelate the individual trees, making them more diverse and less prone to overfitting. This split is typically determined using a criterion, such as the Gini index to minimize:\n$Gini = 1 - \\sum_{i=1}^{C} p_i^2$ \nc is the number of classes in the classification problem and each pi represents the proportion of samples that belong to class i (see Daniya, Geetha, and Kumar 2020).\nThen the algorithm constructs a forest of decision trees, where each tree is trained on a bootstrap data sample and selects the best split among a random subset of features at each node. By averaging the predictions of multiple trees, random forests provide robust and accurate predictions. If necessary, Fig. ?? in appendix provides a clear summary of this approach.\nTo summarize, up to now we have identified two main sources of randomness in the random forest algorithm, making model reproducibility challenging:\n1. Bootstrap sampling: involves randomly sampling observations from the dataset."}, {"title": "Existing Packages", "content": "We are going to study the characteristics of four packages implementing the random forest algorithm, two from R and two from Python, then compare them.\nThe reproducibility of random forest models represents a significant challenge due to the variations between different implementations in R and Python. This discrepancy can be attributed to differences in the algorithms, randomization processes and optimization techniques used by the different software packages. Addressing this issue is essential to ensure the reliability and applicability of random forest models in the context of studies requiring compliance validations.\nR packages\n\u2022 randomForest: a standard R package that provides an implementation of the random forest algorithm, written in C and Fortran.\nVersion: 4.7.1.1, function: \"randomForest()\"\n\u2022 Ranger: an R package written in C++, providing a faster implementation.\nVersion: 0.12.1, function: \"ranger()\"\nPython packages\n\u2022 SKRanger: a Python compatible version of the Ranger R package, offering a Scikit-Learn interface and built on top of the Ranger library, which is written in Python/Cython and C++. Ranger and SKRanger share the same C++ code, the difference being in the way it is called.\nVersion: 0.8.0, function: \"RangerForestClassifier()\"\n\u2022 Scikit-Learn: a package primarily written in Python with some parts optimized using Cython.\nVersion: 1.1.2, function: \"RangerForestClassifier()\""}, {"title": "Methods", "content": "To establish the common features and differences between Scikit-Learn, SKRanger, Ranger, and randomForest, we will implement a random forest model using each package and compare the outputs of the four models.\nSKRanger and Ranger share the same underlying C++ code but interact with it differently. SKRanger utilizes Cython for C++ function calls instead of R's. As they both rely on the same Pseudo-Random Number Generator (PRNG), with a fixed random seed their outputs should be identical."}, {"title": "Results", "content": "All four models were pre-trained on the dedicated data, as explained above. This section presents the results of models' predictions on the previously defined test dataset."}, {"title": "First implementation", "content": "Table 4 shows the number of divergent classifications between each package on the test set, containing 920 observations (formulas).\nThese results and the similarities in the source code allow us to conjecture that the algorithm implementations of the randomForest, Scikit Learn and Ranger packages are broadly similar. However, discrepancies are observed in the outputs obtained with SKRanger. The reasons are not obvious, and the aim is to clarify them in order to achieve perfect reproducibility between packages."}, {"title": "Eliminating sources of randomness", "content": "In this test, no bootstrap is performed, and all variables are considered when searching for the best division. Table 5 presents the number of divergent classifications.\nVariations in classification remain between the four models. To better understand them, next section (5.3) is intended to analyse the trees returned by the models. In theory, only one tree is required in this context."}, {"title": "Analysis of generated trees", "content": "In the following, we will impose a strict limit of 5 on the depth of trees, to ensure clear graphical representation. However, all observations concerning these trees have also been validated using trees without depth restrictions. Except for the depth, trees are generated as in 5.2 where sources of randomness are eliminated. Four fundamental results emerge from the tree-by-tree analysis and are presented in the following subsections."}, {"title": "1st finding: Randomness in splitting variables drawing", "content": "For the packages randomForest, Scikit-Learn and Ranger, generated trees are almost identical, but there are a few nodes for which the splitting variables are not. Indeed, by running the codes of these three packages several times, some splitting variables change. This is not the case for SKRanger, which uses a random seed set at 42 by default.\nHowever, utilizing the Scikit-Learn package, which records tree details, we observe that while the splitting variables of specific nodes vary across executions, the Gini criterion of the node remains the same.\nThis phenomenon is simply due to the random drawing of variables: although all variables are candidates in a node cut, they are drawn in random order. Thus, even if an equivalent cut is possible, it could be ignored, as a cut variable is only chosen if it strictly reduces the impurity criterion.\nSo, tree analysis reveals another source of randomness: for equal impurity criteria, the choice of variable splitting is random. Finally, by using the value of the Gini impurity criterion to compare the trees, the ones generated by the Scikit-Learn, randomForest and Ranger packages are identical."}, {"title": "2nd finding: min_node_size, a key parameter to set to ensure reproducibility with SKRanger", "content": "The tree generated by SKRanger has terminal node higher than the three other packages. Both Ranger and SKRanger use Ranger's C++ code to build the trees, and with identical PRNG, the outputs should be perfectly similar.\nWith SKRanger, if a node contains less than 10 observations, it is not split. Research revealed that this was due to an unexpected overwrite of the min_node_size default value (not fixed in 3), which is the minimum number of observations required in a node before it can be split. Even though both R and Python packages rely on the same underlying C++ library, apparently, they can independently adjust default parameter values. This flexibility leads to variations in the default min_node_size parameter between implementations (set to 10 by SKRanger, 1 by others), resulting in discrepancies in output trees.\nTo ensure efficient reproducibility between R and Python, it is advisable to set min_node_size to 1. Doing this, the trees generated by the four studied packages are now identical."}, {"title": "3rd finding: Parameter nodesize (randomForest) corresponds to the minimum node size to split at", "content": "To ensure the robustness of the results, we systematically varied the model parameters, leading to another crucial discovery.\nAccording to the Scikit-Learn and randomForest packages documentation, the min_samples_leaf and nodesize parameters are defined as the \u201cminimum size of leaf node\": a split point at any depth will only be considered if it leaves at least the number of samples in each of the left and right branches.\nWe have then compared the trees obtained by varying the parameters nodesize of randomForest (Fig. 1), as well as min_samples_leaf and min_samples_split of Scikit-Learn (Fig. 2), with the reproducibility procedures explained so far (5.2 and 5.3.2).\nClearly, nodesize acts as a min_samples_split, contradicting what is found in the documentation. Considering this, it is now possible to obtain similar trees for all four studied packages."}, {"title": "4rd finding: Differences in aggregation methods (Ranger, SKRanger)", "content": "Preceding steps make it possible to obtain identical trees by generating just one. This time, the aim is to extend the tree's reproducibility to the whole forest and compare the irritation score classifications on the test dataset containing 920 rows.\nNevertheless, when implementing random forests with both SKRanger and Ranger to predict the irritation class of the formulas \u2013 taking care to set the parameters just as before \u2013 this leads to 897/920 equal classifications.\nIt turned out that SKRanger employs a different approach than Ranger: SKRanger does not rely on majority class voting mechanism to derive final predictions. Instead, it relies on predicting probabilities for each class through the predict() function. These probabilities represent the averaged values obtained by aggregating probabilities assigned to each class across all individual trees. In contrast, the majority class voting involves each tree voting for a class, with the class receiving the most votes being chosen as the final prediction. Classifications may vary slightly between these two approaches, as discussed in greater detail in Breiman 2001.\nTo match the predictions of both forests, it is possible to manually select the class associated with the highest probability for each Ranger prediction. When doing this, reproducibility is finally achieved by obtaining 920/920 consistent classifications.\nIt is worth noting that SKRanger (v0.8.0) uses Ranger (v0.12.1) code, suggesting possible implementation differences in newer Ranger versions (v0.15.1)."}, {"title": "Discussion", "content": "The thorough analysis conducted confirmed the ability to generate identical decision trees across various implementations. However, locking in hyperparameters for reproducibility might not always be necessary in practical applications. Reproducibility is primarily a regulatory requirement, ensuring methodological integrity from input to output."}, {"title": "Conclusions", "content": "In conclusion, achieving perfect reproducibility between Scikit-Learn, SKRanger, Ranger and randomForest packages involves several key steps.\nFirstly, common elements such as the random seed generator and environment parameters need to be standardized.\nAdditionally, parameters with differing default values, such as the number of trees, should be set uniformly. Furthermore, meticulous adjustments are necessary to manage sources of randomness. This includes implementing models without bootstrapping and avoiding random variable selection during the search for the best split. Ensuring a consistent approach to sampling training and testing data is also essential.\nStudying consistency at the individual level has revealed some crucial mechanisms. The choice of the splitting variable is random with equal impurity criterion, so based on the Gini criterion it is possible to obtain identical trees in all packages when taking special care to set min_node_size to 1 in SKRanger.\nFuthermore, the nodesize randomForest parameter does not act as in the documentation, but as the minimum node size to split at.\nFinally, the reproducibility of the entire forest is studied, by evaluating the number of matching classifications. This highlighted the differences between aggregation methods, providing further insights on why overall classifications may still slightly vary between packages."}]}