{"title": "RANDOMNESS CONTROL AND REPRODUCIBILITY STUDY OF RANDOM FOREST ALGORITHM IN R AND PYTHON", "authors": ["Louisa Camadini", "Yanis Bouzid", "Maeva Merlet", "L\u00e9opold Carron"], "abstract": "When it comes to the safety of cosmetic products, compliance with regulatory standards is crucial to guarantee consumer protection against the risks of skin irritation. Toxicologists must therefore be fully conversant with all risks. This applies not only to their day-to-day work, but also to all the algorithms they integrate into their routines. Recognizing this, ensuring the reproducibility of algorithms becomes one of the most crucial aspects to address.\n\nHowever, how can we prove the robustness of an algorithm such as the random forest, that relies heavily on randomness? In this report, we will discuss the strategy of integrating random forest into ocular tolerance assessment for toxicologists.\n\nWe will compare four packages: randomForest and Ranger (R packages), adapted in Python via the SKRanger package, and the widely used Scikit-Learn with the RandomForestClassifier() function. Our goal is to investigate the parameters and sources of randomness affecting the outcomes of Random Forest algorithms.\n\nBy setting comparable parameters and using the same Pseudo-Random Number Generator (PRNG), we expect to reproduce results consistently across the various available implementations of the random forest algorithm. Nevertheless, this exploration will unveil hidden layers of randomness and guide our understanding of the critical parameters necessary to ensure reproducibility across all four implementations of the random forest algorithm.", "sections": [{"title": "Introduction", "content": "The role of cosmetics toxicological evaluation is to guarantee the safety of consumer products. This involves substantial investments in time and financial resources for product evaluation. To reduce these investments, machine learning is one of the keys to help toxicologists in decision-making. Over recent years, several models have been developed to compute risk assessment metrics.\n\nIn a safety context, machine learning is actively used to develop predictive models that can replace animal testing. Various methods have been developed over the past four decades to address increasingly complex needs such as general systemic toxicology or skin sensitization, as mentioned in Burbank, Gautier, and Hewitt 2023 and retrieved from EU. (2009) Regulation (EC) No 1223/2009 of the European Parliament and of the Council of 30 November 2009 on cosmetic products. N.d. and EU. (2010) Directive 2010/63/EU of the European Parliament and of the Council of 22 September 2010 on the protection of animals used for scientific purposes (Text with EEA relevance) n.d.\n\nTo ensure that a new tool can be integrated into risk assessment strategy, the first and crucial step is the adoption by the community. Given the significant responsibility inherent in every cosmetic product safety report submitted to health authorities, this adoption process necessitates thorough testing of any new model that may be developed. If any doubts\nregarding the reproducibility of the algorithm's implementation arise, a toxicologist may request an evaluation of its impact on risk assessment.\n\nUpon initial observation, the use of random forests may seem unsettling. As of now, there is a lack of evidence regarding the reproducibility of this method across the most popular programming languages \u2013 such as R and Python \u2013 despite widespread discussions on performance comparisons as exemplified in Marchese Robinson et al. 2017. To bridge this gap, our study aims to compare four random forest packages in R and Python, thereby assessing the reproducibility of their outcomes.\n\nThe article will begin with a brief section on random forests, as understanding how they work is essential for revealing sources of randomness. This will be followed by a review of existing packages, then the methods for deciphering the differences between them and the associated results. At the end of the article, a clear procedure will enable us to achieve perfect reproducibility between the studied packages."}, {"title": "Random forest algorithm", "content": "The random forest algorithm combines the concepts of CART (Classification and Regression Trees) and bagging (bootstrap aggregation), whose concepts are fully detailed in Biau and Scornet 2015.\n\nBagging uses the bootstrap method to train decision trees on different samples of the dataset, then aggregates their predictions to make a final decision. This approach improves model stability and robustness by reducing variance and over-fitting. However, it adds an extra layer of randomness to the construction of each tree, making model reproducibility challenging.\n\nIn addition, the random forest introduces randomness by selecting only a subset of candidates features (m candidates among the total number of features available p) at each node split, instead of considering all features. This random selection helps decorrelate the individual trees, making them more diverse and less prone to overfitting. This split is typically determined using a criterion, such as the Gini index to minimize:\n\n$Gini = 1- \\sum_{i=1}^{C} Pi^2$\n\nc is the number of classes in the classification problem and each pi represents the proportion of samples that belong to class i (see Daniya, Geetha, and Kumar 2020).\n\nThen the algorithm constructs a forest of decision trees, where each tree is trained on a bootstrap data sample and selects the best split among a random subset of features at each node. By averaging the predictions of multiple trees, random forests provide robust and accurate predictions. If necessary, Fig. ?? in appendix provides a clear summary of this approach.\n\nAlgorithm 1 describes how random forests work:\n\nTo summarize, up to now we have identified two main sources of randomness in the random forest algorithm, making model reproducibility challenging:\n\n1. Bootstrap sampling: involves randomly sampling observations from the dataset."}, {"title": "Existing Packages", "content": "We are going to study the characteristics of four packages implementing the random forest algorithm, two from R and two from Python, then compare them.\n\nThe reproducibility of random forest models represents a significant challenge due to the variations between different implementations in R and Python. This discrepancy can be attributed to differences in the algorithms, randomization processes and optimization techniques used by the different software packages. Addressing this issue is essential to ensure the reliability and applicability of random forest models in the context of studies requiring compliance validations.\n\nR packages\n\n\u2022 randomForest: a standard R package that provides an implementation of the random forest algorithm, written in C and Fortran.\nVersion: 4.7.1.1, function: \"randomForest()\"\n\u2022 Ranger: an R package written in C++, providing a faster implementation.\nVersion: 0.12.1, function: \"ranger()\"\n\nPython packages\n\n\u2022 SKRanger: a Python compatible version of the Ranger R package, offering a Scikit-Learn interface and built on top of the Ranger library, which is written in Python/Cython and C++. Ranger and SKRanger share the same C++ code, the difference being in the way it is called.\nVersion: 0.8.0, function: \"RangerForestClassifier()\"\n\u2022 Scikit-Learn: a package primarily written in Python with some parts optimized using Cython.\nVersion: 1.1.2, function: \"RangerForestClassifier()\""}, {"title": "Methods", "content": "To establish the common features and differences between Scikit-Learn, SKRanger, Ranger, and randomForest, we will implement a random forest model using each package and compare the outputs of the four models.\n\nSKRanger and Ranger share the same underlying C++ code but interact with it differently. SKRanger utilizes Cython for C++ function calls instead of R's. As they both rely on the same Pseudo-Random Number Generator (PRNG), with a fixed random seed their outputs should be identical.\nHowever, to ensure consistency in our experiments, we employed the rpy2 package, enabling the execution of R code directly within a Python environment and facilitating comparisons. It is noteworthy that rpy2 utilizes the R kernel installed on the machine, meaning that R-based code executed through rpy2 will utilize R's random number generator rather than Python's."}, {"title": "Data", "content": "Based on over a century of knowledge in cosmetics, L'Or\u00e9al has a huge history in assessing the risk of his product for local tolerance. To avoid any hazard to the consumer without using animal tests (which are not allowed since 2013), we have a wide variety of in vitro tests, giving each product an associated irritation score.\n\nAmong this palette of in vitro tests, there is a method for assessing ocular irritation, which returns the irritation class of a formula. In this test, each formula is classified as non-irritant (encoded 0), moderately irritant (1) or severely irritant (2).\n\nWe consider a dataset containing N = 4598 rows corresponding to different formulas (tested product). A formula is the sum of ingredient concentrations, with each of the p = 87 columns corresponding to a single ingredient contributing to the composition. The values indicate the percentage concentration in the formula, meaning that the sum of each row equals 100.\n\nThe random forest procedure is used to predict the irritation class from the dataset. First, this dataset is split into training and testing sets, following the common practice of allocating 80% of the dataset to the training set and reserving the remaining 20% for testing, using the train_test_split() Python function. Results will be displayed on this test dataset, composed of 920 rows corresponding to formulas and 87 columns which are the ingredients. Train and test datasets are now fixed for the rest of the testing process."}, {"title": "First implementation", "content": "For this first random forest implementation, we leave the default values of parameters to all four packages and run the random forest on 5000 trees. To do so, only the parameters in Table 2 are fixed.\n\nAll other parameters are not specified meaning that default values apply, which may vary between packages. This results (in 5.1) will serve as a basis for comparing metrics and highlighting the need to control the randomness of algorithms, in addition to parameters."}, {"title": "Eliminating sources of randomness", "content": "As mentioned in 2, randomness is introduced at two stages in the model: if using bootstrap sampling, and at each node split when we randomly draw m candidate variables from the p variables in our data. For the four packages, these phenomena can be controlled by setting model parameters as in Table 3.\n\nIn this test, we will not perform bagging. Instead, models are trained on the entire train set defined beforehand, thus limiting the randomness of data allocation. All p = 87 variables are considered at each node division (meaning all available variables).\n\nNote: Only one tree is needed, because considering all variables when dividing each node, and not doing bootstrap sampling would mean making the same decision tree every time."}, {"title": "Results", "content": "All four models were pre-trained on the dedicated data, as explained above. This section presents the results of models' predictions on the previously defined test dataset."}, {"title": "First implementation", "content": "Table 4 shows the number of divergent classifications between each package on the test set, containing 920 observations (formulas).\n\nThese results and the similarities in the source code allow us to conjecture that the algorithm implementations of the randomForest, Scikit Learn and Ranger packages are broadly similar. However, discrepancies are observed in the outputs obtained with SKRanger. The reasons are not obvious, and the aim is to clarify them in order to achieve perfect reproducibility between packages."}, {"title": "Eliminating sources of randomness", "content": "In this test, no bootstrap is performed, and all variables are considered when searching for the best division. Table 5 presents the number of divergent classifications.\n\nVariations in classification remain between the four models. To better understand them, next section (5.3) is intended to analyse the trees returned by the models. In theory, only one tree is required in this context."}, {"title": "Analysis of generated trees", "content": "In the following, we will impose a strict limit of 5 on the depth of trees, to ensure clear graphical representation. However, all observations concerning these trees have also been validated using trees without depth restrictions. Except for the depth, trees are generated as in 5.2 where sources of randomness are eliminated. Four fundamental results emerge from the tree-by-tree analysis and are presented in the following subsections."}, {"title": "1st finding: Randomness in splitting variables drawing", "content": "For the packages randomForest, Scikit-Learn and Ranger, generated trees are almost identical, but there are a few nodes for which the splitting variables are not. Indeed, by running the codes of these three packages several times, some splitting variables change. This is not the case for SKRanger, which uses a random seed set at 42 by default.\n\nHowever, utilizing the Scikit-Learn package, which records tree details, we observe that while the splitting variables of specific nodes vary across executions, the Gini criterion of the node remains the same.\n\nThis phenomenon is simply due to the random drawing of variables: although all variables are candidates in a node cut, they are drawn in random order. Thus, even if an equivalent cut is possible, it could be ignored, as a cut variable is only chosen if it strictly reduces the impurity criterion.\n\nSo, tree analysis reveals another source of randomness: for equal impurity criteria, the choice of variable splitting is random. Finally, by using the value of the Gini impurity criterion to compare the trees, the ones generated by the Scikit-Learn, randomForest and Ranger packages are identical."}, {"title": "2nd finding: min_node_size, a key parameter to set to ensure reproducibility with SKRanger", "content": "The tree generated by SKRanger has terminal node higher than the three other packages. Both Ranger and SKRanger use Ranger's C++ code to build the trees, and with identical PRNG, the outputs should be perfectly similar.\n\nWith SKRanger, if a node contains less than 10 observations, it is not split. Research revealed that this was due to an unexpected overwrite of the min_node_size default value (not fixed in 3), which is the minimum number of observations required in a node before it can be split. Even though both R and Python packages rely on the same underlying C++ library, apparently, they can independently adjust default parameter values. This flexibility leads to variations in the default min_node_size parameter between implementations (set to 10 by SKRanger, 1 by others), resulting in discrepancies in output trees.\n\nTo ensure efficient reproducibility between R and Python, it is advisable to set min_node_size to 1. Doing this, the trees generated by the four studied packages are now identical."}, {"title": "3rd finding: Parameter nodesize (randomForest) corresponds to the minimum node size to split at", "content": "To ensure the robustness of the results, we systematically varied the model parameters, leading to another crucial discovery.\n\nAccording to the Scikit-Learn and randomForest packages documentation, the min_samples_leaf and nodesize parameters are defined as the \u201cminimum size of leaf node\": a split point at any depth will only be considered if it leaves at least the number of samples in each of the left and right branches.\n\nWe have then compared the trees obtained by varying the parameters nodesize of randomForest , as well as min_samples_leaf and min_samples_split of Scikit-Learn , with the reproducibility procedures explained so far (5.2 and 5.3.2).\n\nClearly, nodesize acts as a min_samples_split, contradicting what is found in the documentation. Considering this, it is now possible to obtain similar trees for all four studied packages.\""}, {"title": "4rd finding: Differences in aggregation methods (Ranger, SKRanger)", "content": "Preceding steps make it possible to obtain identical trees by generating just one. This time, the aim is to extend the tree's reproducibility to the whole forest and compare the irritation score classifications on the test dataset containing 920 rows.\n\nNevertheless, when implementing random forests with both SKRanger and Ranger to predict the irritation class of the formulas \u2013 taking care to set the parameters just as before \u2013 this leads to 897/920 equal classifications.\n\nIt turned out that SKRanger employs a different approach than Ranger: SKRanger does not rely on majority class voting mechanism to derive final predictions. Instead, it relies on predicting probabilities for each class through the predict() function. These probabilities represent the averaged values obtained by aggregating probabilities assigned to each class across all individual trees. In contrast, the majority class voting involves each tree voting for a class, with the class receiving the most votes being chosen as the final prediction. Classifications may vary slightly between these two approaches, as discussed in greater detail in Breiman 2001.\n\nTo match the predictions of both forests, it is possible to manually select the class associated with the highest probability for each Ranger prediction. When doing this, reproducibility is finally achieved by obtaining 920/920 consistent classifications.\n\nIt is worth noting that SKRanger (v0.8.0) uses Ranger (v0.12.1) code, suggesting possible implementation differences in newer Ranger versions (v0.15.1)."}, {"title": "Discussion", "content": "The thorough analysis conducted confirmed the ability to generate identical decision trees across various implementations. However, locking in hyperparameters for reproducibility might not always be necessary in practical applications. Reproducibility is primarily a regulatory requirement, ensuring methodological integrity from input to output.\n\nAlthough our approach is based on several tests on a single tree, all observations can be extended to a forest. While this starting point may not represent the most optimized implementation, the aim was to highlight the divergences between the four packages in a comprehensive way.\n\nSimilarly, some demonstrations have been made with a limited tree depth, in order to better understand the side effects that can modify the result between the two languages. As the difference in reproducibility can be observed at different depths, this choice does not affect our overall observation.\n\nTherefore, we deliberately chose this comprehensive testing methodology to ensure the robustness and reliability of our analyses.\n\nIn this article, we have delved into the R and Python implementations of random forests, and should emphasize two points. First, only R and Python packages are studied, whereas most articles comparing random forest performance usually include a third programming language. This choice stems from the fact that these are the two most widely used languages in data science.\n\nSecond, we demonstrate how an internal effect of randomness can affect the overall robustness of an algorithm. Dealing with the reproducibility and robustness of randomness is a common problem in computer science, widely discussed in L'Ecuyer 2006.\n\nDespite the subtle differences we have found, they will have no impact on the use that most users have of random forests. This analysis focuses on reproducibility between several implementations and not on the internal reproducibility of a code, which can be dealt with by conventional guidelines Peng 2011.\n\nFrom a broad perspective, it is clear that this small effect of variability will not be detected by a classical optimization of the model hyperparameter, and that most of the community has no need to evaluate the safety of their algorithm. With this analysis, our confidence in random forests is guaranteed for use in safety regulation, which was the objective of our research."}, {"title": "Conclusions", "content": "In conclusion, achieving perfect reproducibility between Scikit-Learn, SKRanger, Ranger and randomForest packages involves several key steps.\n\nFirstly, common elements such as the random seed generator and environment parameters need to be standardized.\n\nAdditionally, parameters with differing default values, such as the number of trees, should be set uniformly. Furthermore, meticulous adjustments are necessary to manage sources of randomness. This includes implementing models without bootstrapping and avoiding random variable selection during the search for the best split. Ensuring a consistent approach to sampling training and testing data is also essential.\n\nStudying consistency at the individual level has revealed some crucial mechanisms. The choice of the splitting variable is random with equal impurity criterion, so based on the Gini criterion it is possible to obtain identical trees in all packages when taking special care to set min_node_size to 1 in SKRanger.\n\nFuthermore, the nodesize randomForest parameter does not act as in the documentation, but as the minimum node size to split at.\n\nFinally, the reproducibility of the entire forest is studied, by evaluating the number of matching classifications. This highlighted the differences between aggregation methods, providing further insights on why overall classifications may still slightly vary between packages."}]}