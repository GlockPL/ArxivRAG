{"title": "LIBEVOLUTIONEVAL: A Benchmark and Study for Version-Specific Code Generation", "authors": ["Sachit Kuhar", "Wasi Uddin Ahmad", "Zijian Wang", "Nihal Jain", "Haifeng Qian", "Baishakhi Ray", "Murali Krishna Ramanathan", "Xiaofei Ma", "Anoop Deoras"], "abstract": "Recent advancements in code completion models have primarily focused on local file contexts (Ding et al., 2023b; Jimenez et al., 2024). However, these studies do not fully capture the complexity of real-world software development, which often requires the use of rapidly-evolving public libraries. To fill the gap, we introduce LIBEVOLUTIONEVAL, a detailed study requiring an understanding of library evolution to perform in-line code completion accurately. LIBEVOLUTIONEVAL provides a version-specific code-completion task comprised of eight libraries (torch, torchvision, scipy, pil, tqdm, pyyaml, matplotlib, and pandas) as they evolve over the year along with a detailed analysis of the evolution of two popular and well-maintained public libraries: PyTorch and Matplotlib. We evaluate popular public models and find that public library evolution significantly influences model performance. We explored mitigation methods by studying how retrieved version-specific library documentation and prompting can improve the model's capability in handling these fast-evolving packages, paving a promising future path in better handling fast-evolving libraries.", "sections": [{"title": "Introduction", "content": "Large Language Models for code (a.k.a. code LLMs) (Li et al., 2023; Lozhkov et al., 2024; Roziere et al., 2023) have significantly advanced developer productivity through improved code completion tasks. These models are pivotal not only in code completion, but also in debugging, code summarization, and language translation for software development (Yan et al., 2023; Roziere et al., 2020, 2022; Min et al., 2024). These models are usually evaluated either with code contest dataset (Li et al., 2022) or with a focus on local files for context to enhance the completion of the function (Chen et al., 2021; Ding et al., 2023a; Athiwaratkun et al., 2023; Ding et al., 2023b; Jimenez et al., 2024). However, these studies do not fully encompass the complexities of real-world software development, which requires public libraries. Complexity of code completion with public library APIs increases, as the APIs often evolve-some APIs change their signature, some gets deprecated, while many new APIs surfaced in this evaluation process (McDonnell et al., 2013). While some works perform code completion involving public libraries (Liao et al., 2023; Zan et al., 2022), use documentation of the library for prediction (Qin et al., 2024), and show that zero shot code completions suffer from hallucinations (Patil et al., 2023), these works do not focus on the rapidly evolving nature of public libraries.\nLarge Language Models are trained on extensive corpora of open-source code, which likely includes public libraries. Consequently, while LLM-generated code may appear reasonable, it might not be accurate for the specific version of the library being used, leading to version-dependent performance issues. This variability is significant since developers often work with different library versions newest versions for current projects and older ones for legacy code maintenance. Therefore, the developer's experience with coding assistants depending on LLMs for code completion can vary greatly depending on their specific use case.\nExisting benchmarks and studies do not fully capture evolution, revealing a gap in our current evaluation and understanding of code LLMs. This work focuses on the following research questions: (1) Does the performance of code LLMs change as the library evolves? (2) If yes, can retrieving version-specific meta-data like library documentation mitigate the impact of library evolution on code completion? (3) With the evolution of libraries, new"}, {"title": "LIBEVOLUTIONEVAL: Version-Specific Code Completions", "content": "Each code completion example in LIBEVOLUTIONEVAL consists of code prompts ending at a position where the LLM is tasked to complete the missing expression, typically involving one or more API calls to the public library under consideration as shown in Figure 1. The uniqueness of this dataset lies in its emphasis on version-specific API usage, reflecting scenarios where developers use LLMs to perform code completions for different versions of the same library. We evaluate code completions under two scenarios: realistic (GitHub based) and controlled (documentation based)."}, {"title": "Version-Specific Evaluation Creation", "content": "API Usage Collection For a realistic scenario, we focus on data written by real-world developers, specifically from permissively licensed GitHub repositories. This allows us to understand if the impact of API evolution is significant with unknown confounding variables present in real-world code.\nFor detailed ablations on the other hand, we also simulate a controlled scenario by creating synthetic data for Matplotlib and PyTorch by taking API documentation and converting it to evaluation examples using a template. The template is designed to make the code LLM predict the API name given its description, service name, and mandatory arguments in the left context. This allows us to isolate the impact of API evolution without the confounding variables found in real-world code, such as variations in coding styles.\nVersioning of API Usage For a realistic setting, the GitHub repositories have a \u2018requirements.txt'"}, {"title": "API Data Classification", "content": "This classification assesses an LLM's ability to track the evolving relationships between APIs as a library changes. It does so by comparing completions based on import-driven prefixes with those using open-vocabulary prefixes. These are:\n\u2022 Direct Code Completions: These completions are driven by import statements, with prefixes derived directly from the public library's import statements. As an example, nn.ReLU() is a direct API completion from import torch.nn as nn.\n\u2022 Indirect Code Completions: These completions lack a well-defined prefix which originates from referenced objects instantiated through direct API calls. Figure 3 shows that variable X is defined by X = nn.linspace and is later used in X. round. These completions test a model's deeper contextual understanding, requiring it to identify the corresponding direct API call and comprehend the library's version-specific relationships between APIs to achieve accurate code completion."}, {"title": "LLM Context Classification", "content": "We assess the models' code completion capabilities using only the context available within the current file, replicating a typical development environment scenario. The import statements, the right context, and the left context extracted are given to the model. This methodology ensures that our evaluation accurately reflects the practical conditions faced by developers that use version-unaware code completions and is aimed to serve as a baseline.\nWhile the in-file context mimics a realistic code-completion setting, there still exists ambiguity for the LLM to perform code completion. For example, there might be two valid responses based on the in-file context corresponding to two different versions of the library, as shown in Figure 1. To mitigate this issue, we add a comment before left context that tells the LLM the version of the library under consideration.\nDevelopment on the success of retrieve-and-generate frameworks for repository-level code completions, we adapt this retrieve-and-generate approach for the"}, {"title": "Experimental Setup", "content": "We benchmark public code LLMs: Mistral, StarCoder2, GPT-40-mini and CodeGen 1.0. We benchmark version-specific retrieval tasks using CodeSage and OpenAI-ada-002. Lastly, we conducted scaling experiments with Star-Coder (1B, 3B, 7B), StarCoder2 (3B, 7B, 15B), and CodeSage (Small, Large).\nFor code completion, we concentrate on the correctness of APIs called by calculating the F1 score. For documentation retrieval, to evaluate the performance of embedding models by using Mean Reciprocal Rank (MRR), assessing how well they retrieve version-specific documentation and whether their performance varies with library evolution.\nWe maintain uniform hyperparameters across all models. The maximum sequence length is 8K tokens, with each context trimmed to include the nearest 4K tokens from the API expression. A maximum generation length is 128 tokens. We report the results of the greedy search. During the post-processing phase we check if the source code following target code completion is being generated; if so, the generation is truncated accord-"}, {"title": "Results", "content": "We evaluate how the performance of code LLMs changes as libraries evolve by performing code completions for eight libraries: torch, torchvision, scipy, pandas, pillow, pyyaml, and tqdm. This evaluation uses Star-Coder2, Mistral, and GPT40-mini. As shown in the first two columns of Table 3, these models employ different completion strategies: StarCoder2 uses fill-in-the-middle, Mistral utilizes left-context only, while GPT40-mini follows an instruction-based approach with a one-shot example. All eight libraries are benchmarked in realistic scenarios. Figure 5 shows that the developer experience can vary significantly across all models and libraries as public libraries evolve, highlighting the need for better model adaptation to API changes.\nTable 3 demonstrates a clear improvement in model performance as additional contextual information is provided during the code completion task. In the baseline In-File setting, where the models rely solely on the code context within a file, the performance is the lowest across all models. Introducing version awareness significantly enhances accuracy, as models can better disambiguate API usage across different library versions. The most notable improvements occur in the Version-Aware RAG setting, where documentation relevant to the specific library version is retrieved and used to further refine API completions. This enriched context enables models to generate more precise completions by taking into account the evolving API landscape. These results emphasize the critical role that version-specific and dependency-aware contexts play in improving the accuracy and reliability of code completions.\nFurthermore, Figure 6c visualizes the impact of using version-aware RAG compared to in-file context settings for the StarCoder2 model, focusing on the evolution of PyTorch and Matplotlib. Although version-aware RAG consistently improves the per-"}, {"title": "Related Works", "content": "Large language models for code excel in various software development tasks, facilitating the developments of coding assistants. Similarly, developments in code embedding models used for retrieval have further enhanced LLMs' capabilities. In this journey, evaluation benchmarks have played a pivotal role with numerous works developing benchmarks to evaluate code LLMs. These studies typically assess code completion abilities given local file contexts, both in-file and repository-level. However, they do not fully encompass the complexities of real-world software development, which requires extensive use of public libraries. Some works have explored code completion involving public libraries, but they do not address the rapidly evolving nature of these libraries. To fill this gap, we introduce LIBEVOLUTIONEVAL that evaluates the performance of LLMs on code completion across multiple versions of public libraries, capturing their evolution and reflecting real-world scenarios where developers interact with different versions of the same library."}, {"title": "Conclusion", "content": "In this paper, we introduced LIBEVOLUTIONEVAL, a comprehensive benchmark specifically designed to assess the performance of Code Large Language Models (code LLMs) in code completion tasks as public libraries evolve. Our results demonstrate significant variability in LLM performance based on the API version, highlighting the challenges of handling library evolution. The findings underscore the necessity for future advancements in code completion technologies to consider the dynamic nature of public libraries, aiming to improve developer productivity and accuracy in real-world settings."}, {"title": "Supplementary Material: Appendices", "content": "This study involves a zero-shot approach to evaluate the impact of the evolution of public libraries on Code LLMs. Pre-training a model exclusively with version-specific data from public libraries might help to reduce the version-dependent discrepancies observed in zero-shot settings. Additionally, it is important to acknowledge that CodeLMs are trained on vast repositories of unlabeled code, raising the possibility that the model might have previously encountered some of the evaluation data. This potential overlap should be carefully considered when interpreting the results of this study."}, {"title": "LIBEVOLUTIONEVAL Generation", "content": "Python files using the library are processed to extract library usage patterns by parsing the files into Abstract Syntax Trees (ASTs) using the tree-sitter library. This comprehensive approach allows us to identify syntactic elements such as function calls and import statements specific to the library. The AST is systematically traversed to detect both direct and indirect API calls to the library and its submodules. Direct API calls are identified by their explicit invocation in the source code, typically involving function calls directly on the library modules (e.g., torch.nn.Linear). Indirect API calls are recognized through variables or objects that are assigned to library functions or classes and used later in the code, which requires tracking variable scopes and aliases across the codebase. The broader structural context for a direct API call is determined by locating the closest enclosing syntactic structure, such as a function or a class method, in the AST. This enclosing structure is regarded as the scope of the API call. The entire block of code constituting this scope is extracted as a context. This context includes parameter lists, internal variable declarations, and other code elements within the same block, providing a comprehensive view of how the API is integrated into the function. The context for indirect API calls includes not only the block where the variable is used but also poten-"}, {"title": "Generating Natural Language Instructions Using Claude", "content": null}, {"title": "Retrieval Performance vs Model Size on LIBEVOLUTIONEVAL", "content": null}, {"title": "Code Completions Performance vs Model Size on LIBEVOLUTIONEVAL", "content": null}, {"title": "Visualizing prompts given to CodeLMs in LIBEVOLUTIONEVAL", "content": null}]}