{"title": "A Geometric Approach to Personalized Recommendation with Set-Theoretic Constraints Using Box Embeddings", "authors": ["Shib Dasgupta", "Michael Boratko", "Andrew McCallum"], "abstract": "Personalized item recommendation typically suffers from data sparsity, which is most often addressed by learning vector representations of users and items via low-rank matrix factorization. While this effectively densifies the matrix by assuming users and movies can be represented by linearly dependent latent features, it does not capture more complicated interactions. For example, vector representations struggle with set-theoretic relationships, such as negation and intersection, e.g. recommending a movie that is \"comedy and action, but not romance\". In this work, we formulate the problem of personalized item recommendation as matrix completion where rows are set-theoretically dependent. To capture this set-theoretic dependence we represent each user and attribute by a hyper-rectangle or box (i.e. a Cartesian product of intervals). Box embeddings can intuitively be understood as trainable Venn diagrams, and thus not only inherently represent similarity (via the Jaccard index), but also naturally and faithfully support arbitrary set-theoretic relationships. Queries involving set-theoretic constraints can be efficiently computed directly on the embedding space by performing geometric operations on the representations. We empirically demonstrate the superiority of box embeddings over vector-based neural methods on both simple and complex item recommendation queries by up to 30% overall.", "sections": [{"title": "1. Introduction", "content": "Recommendation systems are a standard component of most online platforms, providing personalized suggestions for products, movies, articles, and more. In addition to generic recommendation, these platforms often present the option for the user to search for items, either via natural language or structured queries. While collaborative filtering methods like matrix factorization have proven successful in addressing data sparsity for unconditional generation, they often fall short when attempting to combine them with more complicated queries. This is not unexpected, as vector embeddings, while effectively capturing linear relationships, are ill-equipped to handle the complex set-theoretic relationships. Even advanced neural network-based approaches, which are designed to capture intricate relationships, have been shown to struggle with set-theoretic compositionally that underlie many real-world preferences.\nLet us consider an example where a user named Bob wants to watch a comedy which is not a romantic comedy. Assuming we have a prior watch history for users, standard collaborative filtering techniques (e.g. low-rank matrix factorization) would yield a learned score function $score(m, Bob)$ for each movie $m$. If we also have movie-attribute annotations, we could form the set of comedies $C$ and set of romance movies $R$ and simply filter to those movies in $C \\ R$, however this assumes that the movie-attribute annotations are complete, which is rarely the case.\nA standard approach in a setting with sparse data is to learn a low-rank approximation for the attribute \u00d7 movie matrix $A$, yielding a dense matrix $\\hat{A}$. We can then form sets of movies based on this dense matrix using an (attribute-specific) threshold, e.g. $\\hat{C} := \\{m | \\hat{A}_{comedy,m} > T_{comedy}\\}$ and $R := \\{m | \\hat{A}_{romance,m} > T_{romance}\\}$, and then rank movies $m \\in \\hat{C} \\backslash R$ according to $score(m, Bob)$. While this approach does allow for performing the sort of queries we are after, it suffers from three fundamental issues:\n1. Limited user-attribute interaction: Since the attribute classification is done independently from the user, any latent relationships between the user and attribute cannot be taken into account.\n2. Error compounding: Errors in the completion of attribute sets accumulate as the number of sets involved in the query increase.\n3. Mismatched inductive-bias: Our queries can be viewed as set-theoretic combinations of the rows, not linear"}, {"title": "3. Method", "content": "Our proposed solution to address these issues starts by defining the sets of movies which comprise the queries of interest. Let, $P(M)$ be the power set of movies $M$. Specifically, for each user $u$ we can define the set $M_u = \\{m | (u,m) \\in D_u\\}$, and for each attribute $a$ we can define the set $M_a = \\{m | (a,m) \\in D_a\\}$. If we let $M \\subseteq P(M)$ be the collection of all such sets, then the set of movies corresponding to a given query $q$ are direct set-theoretic combinations of elements in $M$. Hence, the reasonable underlying assumption, in this case, is to model the elements of $M$ as sets via a map $f : M \\rightarrow R$ where $R$ is also a set of sets, and the map $f$ respects set-theoretic operations, i.e. $f(S \\cap T) = f(S) \\cap f(T)$ and $f (S\\backslash T) = f(S) \\backslash f(T)$, etc. Such a map is referred to as a homomorphism of Boolean algebras, and the problem of learning such a function was explored in general in (Boratko et al., 2022). In our work, we propose box embeddings as the function $f$ which can be trained to obey the homomorphism constraints. As a result, user-attribute-item representations based on box embeddings could serve as an optimal inductive bias for the proposed set-theoretic matrix completion task."}, {"title": "3.1. Set-theoretic Representation Box Embeddings", "content": "As introduced in Vilnis et al. (2018), box embeddings represent entities by a hyperrectangle in $R^D$, i.e. a Cartesian product of intervals. Let the box embedding for user $u$ be:\n$Box(u) = \\prod_{d=1}^{D}[u_{d}, u_{d}] = [u_{1}, \\bar{u}_{1}] \\times ... \\times [u_{D}, \\bar{u}_{D}] \\subseteq R^{D}$,\nwhere $[u_{d}, \\bar{u}_{d}]$ is the interval for $d$-th dimension, $u_{d} < \\bar{u}_{d}$ for $d \\in \\{1, ...,D\\}$.\nThe volume of an interval is defined as the length of the interval $Vol((u_{d}, \\bar{u}_{d})) = max(\\bar{u}_{d} - u_{d}, 0)$.\nLet, $Box(m) = \\prod_{d=1}^{D}[m_{d}, \\bar{m}_{d}]$ be the box embeddings for a movie $m$. At dimension $d$, the volume of intersection between user $u$ and movie $m$ is defined as\n$VolInt((u_{d}, \\bar{u}_{d}), (m_{d}, \\bar{m}_{d})) = max (min(\\bar{u}_{d}, \\bar{m}_{d}) - max(u_{d}, m_{d}), 0)$."}, {"title": "3.2. Training", "content": "We model each user, attribute, and movie as a box in $R^D$, and denote the map from these entities to their associated box parameters as $\\theta$, i.e., the trainable box embedding for user $u$ is $\\theta(u) := Box(u)$. Our goal is to train these box representations to represent certain sets of movies which allow us to perform the sort of queries we are interested in. As motivated above, for a given user $u$, we train $Box(u)$ to approximate the set $M_u$ via a noise-contrastive estimation objective. Namely, for each $(u, m) \\in D_u$, we have a loss term\n$l_{(u,m)} (\\theta) := E_{GB} (u, m; \\theta) - E_{m \\sim M} [log (1 - exp(-E_{GB} (u, m; \\theta))))]$.\nThe first term is minimized when $Box(u)$ contains $Box(m)$. We approximate the second term via sampling, which encourages $Box(u)$ to be disjoint from $Box(m)$ for a uniformly randomly sampled movie $m$. We define an analogous loss function $l_{(a,m)} (\\theta)$ for attribute-movie interactions, which trains $Box(a)$ to contain the box $Box(m)$ for each $m$ such that $(u, m) \\in D_u$.\nThe overall loss function is a convex combination of these loss terms:\n$L(\\theta; D_u,D_A) := \\omega * \\sum_{(u,m) \\in D_U}l_{(u,m)} (\\theta) + (1-\\omega) * \\sum_{(a,m) \\in D_A}l_{(a,m)} (\\theta)$.\nfor a hyperparameter $\\omega \\in [0, 1]$. This optimization ensures that the movie boxes are contained within the corresponding user and attribute boxes, thereby establishing a set-theoretic inductive bias. Both numbers of negative samples and $\\omega$ are hyperparameters for training (Please Refer to Section 4, Appendix B.2) for further details."}, {"title": "3.3. Inference", "content": "During inference, given the trained embedding model $\\theta$ and a user $u$ we determine the user's preference for the movie"}, {"title": "4. Task Formulation", "content": "For example, for the ground-truth data for comedies which are\nnot romance movies which Bob likes would be the vector\n$x \u2208 {0,1}|M|$, where xm = 1 if and only if $U_{Bob,m} = 1$\nand $A_{comedy,m} = 1$ and $A_{romance,m} = 0$. Note that this is\nnot a linear combination of the previous rows, and so while\nthe inductive bias of low-rank factorization has proven im-\nmensely effective for collaborative filtering we should not\nexpect it to be directly applicable in this setting.\nInstead, we propose to learn representations for the users\nand attributes that are consistent with specific set-theoretic\naxioms. These representations must also be compactly pa-\nrameterizable in a lower-dimensional space, differentiable\nwith respect to some appropriate score function, and allow\nfor efficient computation of various set operations. Box Em-\nbeddings (Vilnis et al., 2018; Dasgupta et al., 2020), which\nare axis-parallel n-dimensional hyperrectangles, meet these\ncriteria (see Figure 2). The volume of a box is easily calcu-\nlated as the product of its side-lengths. Furthermore, box\nembeddings are closed under intersection (i.e. the intersec-\ntion of two boxes is another box). Inclusion-exclusion thus\nallows us to calculate the volume of arbitrary set-theoretic\ncombinations of boxes."}, {"title": "2.1. Background", "content": "Matrix completion is a fundamental problem in machine learning, and arises in a wide array of tasks, from recommender systems to image reconstruction. Formally, this problem is typically modeled as follows: Given a matrix $X \\in R^{m \\times n}$ where only a subset of the entries are observed, find a complete matrix $\\hat{X} \\in R^{m \\times n}$ which closely approximates $X$ on the observed entries. For the task of recommendation, this involves predicting user interactions with items they have not previously interacted with, and a common assumption is that the preferences of users and characteristics of the items can be expressed by a small number of latent factors, with the alignment of these latent factors captured via dot-product. This justifies the search for a low-rank approximation $\\hat{X} = BC$, where $B \\in R^{m \\times D}$ and $C \\in R^{D \\times n}$. In the case where the original matrix is binary, $X \\in \\{0,1\\}^{m \\times n}$, it is common to perform logistic matrix factorization, where an elementwise sigmoid is applied after the dot-product of latent factors, which we denote (with slight abuse of notation) as $\\hat{X} = \\sigma(BC)$."}, {"title": "2.2. Set-Theoretic Matrix Completion", "content": "We will describe the task of set-theoretic matrix completion on the setting of movies, users, and attributes, though the formulation and our proposed model can be generalized to arbitrary domains. We are given a set $D_u \\subseteq U \\times M$ of user-movie interactions, and a set $D_A \\subseteq A \\times M$ of attribute-movie pairs. We assume both of these sets are incomplete.\nOur goal is to eventually be able to recommend movies based on some query, for example \"comedy and not romance\". Such a query for a particular user can be represented as $u \\land a_1 \\land \\neg a_2$, where $u$ is the user, $a_1 = comedy$ and $a_2 = romance$. We let $Q$ be the set of all queries of interest, which depends on which queries we anticipate evaluating at inference time. In this work, we will take $Q$ to be queries of the form $u, a_1, u \\land a_1, u \\land a_1 \\land a_2$, and $\\land a_1 \\land \\neg a_2$, where $u \\in U$ and $a_1, a_2 \\in A$.\nWith this formulation, we can view our task as matrix completion for a matrix $X \\in \\{0,1\\}^{|Q| \\times |M|}$, where the rows are derived by applying bitwise operators on the rows of user and attribute data. While we could, in theory, proceed directly with logistic matrix factorization on this matrix, there are both practical and theoretical reasons to search"}, {"title": "3. Method", "content": "Essentially replacing the hard min and max operators with\na smooth approximation, $\\mathbb{LSE}_t(x) := t \\log(\\sum_i e^{x_i/t})$. Expected intersection volume in higher dimensions is just\na product of the preceding equation, as the random vari-\nables are independent. We use this GUMBELBOX (ab-\nbrev GB) formulation in our work changing the notations\n$F_{Box}, Vol, VolInt$ to $F_{GB}, Vol_{GB}, VolInt_{GB}$. We modify\nthe per-dimension score function $F_{Box}$ in (2) by replacing\nthe ratio of hard volume calculations with the approximation"}, {"title": "3.2. Training", "content": "The overall loss function is a convex combination of these\nloss terms:\n$L(\\theta; D_u,D_A) := \\omega * \\sum_{(u,m) \\in D_U}l_{(u,m)} (\\theta) + (1-\\omega) * \\sum_{(a,m) \\in D_A}l_{(a,m)} (\\theta)$.\nfor a hyperparameter $\\omega \u2208 [0, 1]$. This optimization ensures\nthat the movie boxes are contained within the corresponding\nuser and attribute boxes, thereby establishing a set-theoretic\ninductive bias. Both numbers of negative samples and $\\omega$\nare hyperparameters for training (Please Refer to Section 4,\nAppendix B.2) for further details."}, {"title": "3.3. Inference", "content": "During inference, given the trained embedding model $\\theta$ and\na user u we determine the user\u2019s preference for the movie"}, {"title": "4. Experiments", "content": "In our experiments, we evaluate all the models on item rec-ommendation across three domains: movies, songs, and restaurants. (4.1). We systematically generate queries ofvarying complexity from these datasets to evaluate perfor-mance on set-theoretic tasks (4.2.1, 4.2.2). We train and"}, {"title": "4.1. Dataset", "content": "The datasets used in our study must contain two pri-mary components: Item-User interactions $D_U$ and Item-Attribute interactions $D_A$. We select datasets that offerrich ground truth annotations for both components. We uti-lize the MovieLens 1M and 20M datasets for personalizedmovie recommendations (Harper & Konstan, 2015). For thesong domain, we employ a subset of the Last-FM dataset,which is the official song tag dataset of the Million SongDataset (Bertin-Mahieux et al., 2011). In the restaurant do-main, we use the NYC-R dataset introduced by (Wang et al.,2018).\nWe utilize the data curated by Dasgupta et al. (2023) toconstruct $D_A$ for the Movielens data. This dataset employsWikidata (Vrande\u010di\u0107 & Kr\u00f6tzsch, 2014) to generate groundtruth attribute labels for movies. For the Last-FM dataset,the authors use the Last.fm API (\u2019getTopTags\u2019) to createattribute tags. Likewise, the authors in (Wang et al., 2018)crawl restaurant review data from TripAdvisor to curatetags and ratings for restaurants in NYC. The sparsity of$D_A$ and $D_U$ is comparable in the Movielens datasets. Incontrast, the Last.fm and NYC-R datasets, designed with tagannotations in mind, exhibit much denser attribute-movieinteraction. Thus, the selection of these three datasets notonly encompasses diverse domains but also offers varyingground-truth distributions for our experiments.\nWe use the binarized implicit feedback data (Hu et al., 2008),indicating whether the user or the attribute has been associ-ated with the specific item. To ensure the quality of the data,we retain users/items with 5 or more interactions and at-tributes with frequency 20 or more in all the datasets. Referto Table 1 for a detailed description of the dataset statistics."}, {"title": "4.2. Dataset Splits & Query Generation", "content": "To select models for each method, we train on a datasetsplit $D^{train}_U$ & $D^{train}_A$ while evaluating on a held-out set $D^{eval}_U$& $D^{eval}_A$. However, we use these eval set pairs to constructcompositional queries. Simple random sampling or leave-one-out data splits do not ensure a substantial number ofthese queries. Therefore, we devise a data splitting tech-nique closely linked to query generation, which we discussnext."}, {"title": "4.2.1. PERSONALIZED SIMPLE QUERY", "content": "This type of query corresponds to a single attribute for aparticular user, e.g. Bob wants to watch a comedy movie.More formally, given a user $u$ and an attribute $a$, the querytype would be - $u \\cap a$. Note that, these simple queriesare set-theoretic combinations between the item sets corre-sponding to the users and the attributes. Let us denote thedata corresponding to these queries as $Q_{U \\cap A}$.\nWhile constructing the $Q_{u \\cap a}$ pairs we need to ensurethat if an item is held out for evaluation for a sim-ple query, the individual user-item and attribute-item pairshould belong to the evaluation set as well. More formally,$(u,a,i) \\in Q_{u \\cap a} \\Leftrightarrow (u, i) \\in D^{eval}_U \\land (a, i) \\in D^{eval}_A$.To ensure this train/test isolation, we use the sampling al-gorithm 1 that takes in $D_U$ and $D_A$ and outputs $Q_{u \\cap a}$,$D^{train}_U, D^{train}_A, D^{eval}_U, D^{eval}_A$ (Refer to Appendix B.1 for moredetails). The detailed statistics for the splits are providedin Table 1. Also, the statistics for the $Q_{U \\cap A}$ are present inTable 2"}, {"title": "4.2.2. PERSONALIZED COMPLEX QUERY", "content": "The set-theoretic compositions that we consider here are theintersection and negation of attributes for a particular user.Given a user u and attributes a1 and a2, we consider thequery types- $u \\cap a_1 \\cap a_2$ and $u \\cap a_1 \\cap \\neg a_2$, e.g, Bob wantsto watch an Action Comedy movie, Alice want to watcha Children but not Monster movie. Creating meaningfulattribute compositions requires careful consideration, asnot all combinations make sense. For instance, \u2019Sci-Fi\u2019& \u2019Documentary\u2019 might not be a meaningful combination,whereas \u2019Sci-Fi\u2019 & \u2019Time-Travel\u2019 is. Similarly, \u2019Sci-Fi\u2019$\\\\$ \u2019Fiction\u2019 doesn\u2019t make sense, but \u2019Fiction\u2019 - \u2019Sci-Fi\u2019 does.Sometimes, even if the intersection is valid, it could betrivial and non-interesting, e.g., \u2019Fiction\u2019 & \u2019Sci-Fi\u2019.Intuitively, for two attributes a1 & a2, their intersection isinteresting if $\\left|a_1 \\cap a_2\\right|$ is greater than combining any tworandom items set. Also, for their intersection to be non-trivial the size of the intersection $\\left|a_1 \\cap a_2\\right|$ must be less thanthe individual sizes of the attributes i.e., $\\alpha |a_1|$ and $\\alpha |a_2|$.Here,|.| denotes the size of the item set corresponding tothe attributes. $\\alpha \\in [0, 1]$ is a design parameter, dedicatedafter manual inspection of the quality of the item sets for the"}, {"title": "4.3. Training Details & Evaluation Criteria", "content": "We train all the methods on users and attributes jointly using$D^{train"}, "U \\cup D^{train}_A \\cup D^{train}_I$. We use dimensions $d = 128$ forvector-based models, and $d = 64$ for box models so thatthe number of parameters per user, attribute, and movieis equal. We perform extensive hyperparameter tuningfor the learning rate, batch size, volume and intersectiontemperature of boxes, loss combination constant, etc. Pleaserefer to the Appendix B.2 for details. We follow the standardsampled evaluation procedure described in Rendle et al.(2020), only for model selection purpose. For each user-item tuple (u, m) in $D^{eval}_U$, the model ranks m amongst aset of items consisting of the m together with 100 othertrue negative items w.r.t the user. Then we report on twodifferent evaluation metrics namely Hit Ratio@k (HR@k)and NDCG. (a) HitRatio@k: If the rank of m is less thanor equals to k then the value of HR@k is 1 or 0 otherwise.(2) NDCG: if r is the rank of m, then 1/log(r + 1) is theNDCG.\nThe model is selected based on the best-performing modelon NDCG for the item prediction over the user-item valida-tion set $D^{eval}_U$, with the best-performing checkpoint saved forfurther evaluation on compositional queries. We follow thesame evaluation protocol for the compositional queries as"]}