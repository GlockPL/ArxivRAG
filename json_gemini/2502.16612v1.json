{"title": "MemeIntel: Explainable Detection of Propagandistic and Hateful Memes", "authors": ["Mohamed Bayan Kmainasi", "Abul Hasnat", "Md Arid Hasan", "Ali Ezzat Shahroor", "Firoj Alam"], "abstract": "The proliferation of multimodal content on social media presents significant challenges in understanding and moderating complex, context-dependent issues such as misinformation, hate speech, and propaganda. While efforts have been made to develop resources and propose new methods for automatic detection, limited attention has been given to label detection and the generation of explanation-based rationales for predicted labels. To address this challenge, we introduce MemeXplain, an explanation-enhanced dataset for propaganda memes in Arabic and hateful memes in English, making it the first large-scale resource for these tasks. To solve these tasks, we propose a multi-stage optimization approach and train Vision-Language Models (VLMs). Our results demonstrate that this approach significantly improves performance over the base model for both label detection and explanation generation, outperforming the current state-of-the-art with an absolute improvement of ~3% on ArMeme and ~ 7% on Hateful Memes. For reproducibility and future research, we aim to make the MemeXplain dataset and experimental resources publicly available.", "sections": [{"title": "1 Introduction", "content": "Despite the rapid growth of multimodal content\u2014integrating images, text, and sometimes video\u2014the automated detection of harmful and false information on online news and social media platforms has become increasingly critical. In particular, identifying propaganda and hate in memes is essential for combating misinformation and minimizing online harm. While most research has focused on textual analysis, multimodal approaches have received comparatively less attention. In propaganda detection, text-based methods have evolved from monolingual to multilingual setups (Piskorski et al., 2023; Hasanain et al., 2023), initially through binary classification and later via multilabel and fine-grained span-level tasks (Barr\u00f3n-Cedeno et al., 2019; Habernal et al., 2017, 2018; Da San Martino et al., 2019). Hate speech detection has similarly progressed from text-based to multimodal approaches that integrate both textual and visual elements. Recent methods have shifted from transformer-based text detection (Fortuna and Nunes, 2018) toward techniques that incorporate visual context (Kiela et al., 2020) by leveraging fusion strategies, attention mechanisms, and contrastive learning to boost accuracy, especially when hateful intent is conveyed through text-image interplay (Alam et al., 2022). The emergence of LLMs has demonstrated significant capabilities across various disciplines. Consequently, efforts have been made to leverage Vision-Language Models (VLMs) (Zhang et al., 2024) and prompting techniques to enhance the detection and classification of harmful and propagandistic memes (Cao et al., 2023). LLM-based models utilize prompt-based learning (Cao et al., 2022), contrastive learning techniques such as CLIP (Kumar and Nandakumar, 2022), and cross-modal attention mechanisms to better capture implicit hate and propaganda. Despite significant progress, challenges remain in detecting implicit hatefulness, particularly when sarcasm or an ironic dissonance exists between text and images. Propagandistic memes further complicate detection by employing emotional appeals, humor, cultural references, manipulative language, and other rhetorical strategies. To address these nuances, it is crucial for a system to provide not only accurate predictions but also interpretable explanations that reveal the underlying reasoning behind its decisions (Hee et al., 2023; Yang et al., 2023; Huang et al., 2023; Sun et al., 2023). Such explanations enhance classifier reliability by helping end users understand the decision, provided they maintain a natural tone and relate closely to the visual elements of the meme. An explanation-based approach offers numerous advantages and enhances performance across various tasks (Li et al., 2022; Magister et al., 2022; Nandi et al., 2024; Kumari et al., 2024). While most studies have focused on textual content (Li et al., 2022; Magister et al., 2022), a few recent approaches (Nandi et al., 2024; Kumari et al., 2024) have applied explainability to images. However, these methods rely on QA-based explanations that lack naturalness, use multiple inference calls with custom models\u2014thereby increasing computational complexity-and employ explanations only during training rather than as an inference output. These limitations motivated us to explore a simplified procedure for meme classification and explanation generation. To overcome the above limitations, we propose a novel procedure that achieves state-of-the-art performance on the target tasks across two distinct datasets. Our contributions are briefly outlined below:\n\u2022 We developed explanation-enhanced datasets, MemeXplain, using a rapid and low-cost annotation procedure;\n\u2022 We investigated state-of-the-art VLMs to identify an appropriate model for meme classification and explanation generation;\n\u2022 We proposed an efficient multi-stage optimization procedure that significantly improves performance;\n\u2022 With the experiments we achieved state-of-the-art performance on two types of datasets related to propaganda and hateful content detection. Our findings are as follows: (a) A higher human evaluation score suggests that explanations from stronger models (e.g., GPT-40) are reliable and can serve as gold-standard explanations for training smaller models. (b) Task-specific fine-tuning improves performance over the base model. (c) Our multi-stage optimization approach benefits both label detection and explanation generation. Overall, our work is the first to enhance VLMs for simultaneous propaganda and hateful content detection while providing natural reasoning to end users."}, {"title": "2 Related Work", "content": "The widespread use of social networks has become a major channel for spreading misinformation, propaganda, and harmful content. Significant research efforts have been directed toward addressing these challenges, particularly in multimodal disinformation detection (Alam et al., 2022), harmful memes (Sharma et al., 2022), and propagandistic content (Dimitrov et al., 2021a). However, most studies have focused on detection, while less attention has been given to generating natural explanations/reasons behind the predicted labels."}, {"title": "2.1 Multimodal Propagandistic Content", "content": "Following the previous research for propaganda detection using textual content (Da San Martino et al., 2019), Dimitrov et al. (2021b) introduced SemEval-2021 Task 6 focusing on persuasion techniques detection in both textual and visual memes. Subsequently, the focus has extended to the detection of multilingual and multimodal propagandistic memes (Dimitrov et al., 2024). Glenski et al. (2019) studied multimodal disinformation content on social media platforms in multilingual settings. Similar multimodal work on Arabic involves the development of datasets and shared task for propaganda detection (Alam et al., 2024b; Hasanain et al., 2024). For the detection problem, typical approaches include a fusion of textual and visual embedding and a classification head on top of them (Hasanain et al., 2024; Shah et al., 2024), graph attention network based approach for multimodal visual-textual objects Chen et al. (2024)."}, {"title": "2.2 Multimodal Hate speech", "content": "Similarly, there has been growing interest in detecting multimodal hate speech (Kiela et al., 2020; Velioglu and Rose, 2020; Hee et al., 2022). Due to the lack of resources, Kiela et al. (2020) developed a large-scale dataset for multimodal hate identification. This study advanced research in this area and emphasized the importance of integrating textual and visual features for effective detection. The issue has also been explored through a multi-task learning framework for identifying hate speech in memes using multimodal features (Sharma et al., 2022). To further progress in this field, efforts have been made to develop resources for multiple languages, including Arabic (Alam et al., 2024a), Bangla (Hossain et al., 2022), and English (Hee et al., 2023). A more detailed summary of these earlier efforts can be found in Sharma et al. (2022), which also highlights key challenges and outlines future research directions."}, {"title": "2.3 Training with Explanations", "content": "Integrating reasoning or explainability capabilities to enhance LLM/VLM performance has been shown to be highly beneficial for various tasks across multiple domains (Plaat et al., 2024). This approach has also proven effective for knowledge distillation and model compression (Li et al., 2022; Magister et al., 2022), where explanations generated by large LLMs improve the performance and capabilities of smaller LLMs. In the context of hateful speech, toxicity detection, and sentiment analysis, it has led to significant advancements (Yang et al., 2023; Huang et al., 2023; Sun et al., 2023). For example, in the hateful speech detection task, Hare (Yang et al., 2023) employs Chain-of-Thought (CoT) reasoning, while (Huang et al., 2023) utilizes Chain of Explanation (CoE). Their aim is to improve the effectiveness of LLM-based sentiment classifiers by leveraging reasoning capabilities. Likewise, Sun et al. (2023) introduced a technique called Clue and Reasoning Prompting (CARP), which incorporates both reasoning and keywords as clues to support the reasoning process. In the following subsections, we specifically examine approaches closely related to our task, focusing on those that have applied VLM-based methods for analyzing hateful or propagandistic memes. CoT is a widely recognized prompting technique that generates a chain of reasoning to derive answers. A recent comprehensive CoT-based meme analysis study is presented in (Kumari et al., 2024), which proposed a framework based on text- and image-based entity-object relationships using a scene graph. They applied a hierarchical three-step CoT-based prompting strategy to guide the LLM in identifying Emotion, Target, and Context, using these elements to build a model for meme analysis. Another recent work, called SAFE-MEME (Nandi et al., 2024), proposed two multimodal datasets and introduced a structured reasoning framework for hate speech detection in memes. They developed a CoT prompting-based framework that incorporates Q&A-style reasoning and hierarchical categorization to classify memes as hateful (explicit or implicit) or benign. However, they did not evaluate their approach using the popular Hateful Memes dataset, preventing a direct numerical comparison with their results. One drawback of these CoT-based approaches is that they rely on multi-step reasoning, requiring multiple inferences with VLMs. Our approach differs from these CoT-based methods in the following ways: (a) we do not employ a complex multistep CoT approach, eliminating the need for multiple LLM inferences, which significantly improves computational efficiency and reduces costs. (b) we focus on providing explanations alongside classification, helping the end-users better understand the reasoning behind classification decisions, thereby increasing reliability. (Hee et al., 2023) constructed a dataset providing explanations for hateful memes. However, unlike us, they focused solely on evaluating explanation generation and did not perform classification tasks. Despite the availability of their data, we do not use it due to the lack of naturalness. In particular, their explanations do not fully account for image content or image-centric contextual perspectives."}, {"title": "3 Dataset", "content": ""}, {"title": "3.1 ArMeme", "content": "The ArMeme dataset aimed to address the scarcity of Arabic-language datasets for multimodal propaganda detection. It comprises approximately ~6k Arabic memes collected from various social media platforms, each manually annotated to identify propagandistic content (Alam et al., 2024b). This dataset has been collected from different social media platforms, filtered, cleaned and manually annotated with four labels such as Not propaganda, Propaganda, Not-meme and Other. Table 1 provides the distribution of the data splits. The memes with \"Not propaganda\" category covers over half of the dataset (~66%), followed by \"Propaganda\" and the distribution of \"Not-meme\" and \"Other\" classes are significantly smaller. This distribution highlights a substantial class imbalance, particularly between \"Not propaganda\" and the other categories."}, {"title": "3.2 Hateful Meme", "content": "The Hateful Memes dataset (Kiela et al., 2020), is a benchmark designed to evaluate multimodal hate speech detection. It consists of ~12k memes, combining both text and images, carefully curated to ensure that effective classification requires an understanding of both modalities. The dataset was created using a mix of synthetically generated memes and real-world examples, sourced from social media, while ensuring a balanced distribution of hateful and non-hateful content. A key feature of this dataset is the inclusion of benign confounders, where individual elements of a hateful meme\u2014either the image or the text\u2014are altered to make it non-hateful. This approach prevents unimodal models (which rely only on text or images) from achieving high performance, reinforcing the need for true multimodal understanding. In Table 2, we report the distribution of hateful meme dataset used for this study. Note that hateful meme dataset consists of two other splits (dev-seen and test-seen), here, we used unseen versions."}, {"title": "4 MemeXplain: Explanation Generation", "content": "The outcomes of an automatic system become more reliable for users if it provides decisions with adequate and interpretable natural explanations, which help users better understand the underlying reason behind the system's decision (Hee et al., 2023; Yang et al., 2023; Huang et al., 2023; Sun et al., 2023). Technically, this approach provides numerous advantages in terms of knowledge distillation, model compression, and enhancing the performance of target tasks in different domains (Li et al., 2022; Magister et al., 2022; Nandi et al., 2024; Kumari et al., 2024). This motivates us to adopt the explanation-based approach in our research. However, we also aim to improve its efficiency, particularly with respect to dataset generation, model training, and system inference procedures. In this research, we generate explanations for two different stages: (a) during existing dataset enhancement, which leverages an expert VLM (such as GPT) to generate high-quality explanations and (b) during training/inference with a smaller VLM (such as Llama-3.2 11b). Figure 1 illustrates these different stages. Mathematically, these two stages can be described by the functions \\(f(i, l) = e\\) and \\(g(i) = (l, e)\\), where \\(e\\) denotes the explanation, \\(l\\) is the label, and \\(i\\) is the input image or meme. Specifically, \\(f(i, l)\\) returns an explanation \\(e\\) given both \\(i\\) and \\(l\\), whereas \\(g(i)\\) generates both the label \\(l\\) and the explanation \\(e\\) from only the input \\(i\\). This research enhances two existing datasets with explanations, see Section 3 and Table 3 for the details and statistics. For the explanation generation task, it first uses a VLM for \\(f(i,l)\\) and then involves human experts, which significantly accelerates high-quality explanation generation and lowers the overall cost and time. The following subsections provide step-by-step details."}, {"title": "4.1 VLMs for Explanation Generation", "content": "Figure 1 illustrates an example of an Arabic meme along with its explanation-generation process using a VLM. We leverage GPT-40 (version 2024-11-20) for automated explanation generation. The choice of this model is motivated by prior studies Wang et al. (2023), which show that advanced GPT models can produce fluent, informative, persuasive, and logically sound explanations when properly prompted. In Listing 1, we present the prompts used for generating explanations for ArMeme and Hateful Memes. To refine the prompt, we iteratively tested several memes in both English and Arabic, selecting the one that produced the most reasonable explanations. For Arabic memes, we generate two sets of explanations-one in English and one in Arabic. The motivation behind this approach is to assess the multilingual capability and quality of smaller VLMs, such as Llama-3.2 11b, in generating explanations and labels in both languages. Size of the Explanation Determining the optimal length for explanations is important for balancing informativeness and cognitive load (Herm, 2023). Shen et al. (2022) explored the relationship between explanation length and human understanding, finding that the shortest rationales are often ineffective. Recently, Wang et al. (2023) also studied the effect of explanation size and found that human evaluators are reluctant to read longer explanations. To achieve an optimal balance, we iteratively tested various explanation lengths and ultimately set a limit of 100 words. Model and Its Parameters To utilize GPT-40 (OpenAI, 2023), we accessed the OpenAI API via Azure services. Though recently released ol models have shown promising directions for complex reasoning, they were not accessible to us. For explanation generation, we employed zero-shot learning. To ensure reproducibility, we set the temperature value to zero."}, {"title": "4.2 Human Evaluation", "content": "Given that our idea is to use the generated explanation as gold data for further training and evaluation, therefore, we intended to go through human evaluation process. Following the prior studies (Wang et al., 2023; Huang et al., 2023; Agarwal et al., 2024) we adopted four metrics discussed below. For each metric we use 5-point Likert scale. Informativeness. Measures the extent to which the explanation provides relevant and meaningful information for understanding the reasoning behind the label. A highly informative explanation offers detailed insights that directly contribute to the justification, while a low-informative explanation may be vague, incomplete, or lacking key details. Clarity. Assesses how clearly the explanation conveys its meaning. A clear explanation is well-structured, concise, and easy to understand without requiring additional effort. It should be free from ambiguity, overly complex language, or poor phrasing that might hinder comprehension. Plausibility. Refers to the extent to which an explanation logically supports the assigned label and appears reasonable given the meme's content. A plausible explanation should be coherent, factually consistent, and align with the expected reasoning behind the label. Faithfulness. Measures how accurately an explanation reflects the reasoning behind the assigned label. A faithful explanation correctly represents the key factors and logical steps that justify the label, without adding misleading or unrelated details. For manual annotation, we first prepared an annotation guideline for the annotators. Additionally, we developed annotation guidelines and a platform (see Appendix B and A, respectively). Evaluation Setting. For the Arabic meme task, we recruited annotators who are native Arabic speakers and fluent in English, all holding at least a bachelor's degree. Because of their fluency, they also handled the hateful meme task. We provided necessary training and consultation, and all had prior experience with similar tasks. A total of six annotators participated in the evaluation. In line with institutional requirements, each signed a Non-Disclosure Agreement (NDA), and a third-party company managed their compensation at standard hourly rates based on location. Quality Assessment In Table 4, we summarize the quality assessment of the explanations. We used 5-point Likert scale for various human evaluation metrics, including informativeness, clarity, plausibility, and faithfulness. We compute the average of the Likert scale value for all evaluation metrics. We manually evaluated 359 and 202 random samples for ArMeme Arabic and English explanations while 200 random examples were evaluated for the Hateful meme dataset. The average agreement scores for the ArMeme dataset with Arabic explanations are 4.23, 4.38, 4.24, and 4.16 for faithfulness, clarity, plausibility, and informativeness, respectively, indicating high agreement across all evaluation metrics. However, for the English explanations of ArMeme, the faithfulness and plausibility scores are relatively. To better understand this issue, we plan to conduct further evaluations on another set of explanations. For the Hateful Memes dataset, the average Likert scale agreement scores range from 4.562 to 4.682."}, {"title": "4.3 Basic Statistics", "content": "Table 3 presents the basic statistics for both datasets. The average explanation length is 94 words for Arabic and 85 words for English. Notably, we instructed GPT-40 to generate explanations with fewer than 100 words. Based on manual evaluation (Table 4), we conclude that both the quality and length of the explanations are appropriate."}, {"title": "5 Methodology", "content": ""}, {"title": "5.1 Instructions Dataset", "content": "Our approach follows the standard pipeline for aligning LLMs with user intentions and specific tasks through fine-tuning on representative data (Zhang et al., 2023; Kmainasi et al., 2024). This process typically involves curating and constructing instruction datasets that guide the model's behavior, ensuring it generates responses that align with the desired objectives. For our study, the responses include label and explanation. Hence, we created instruction format for both datasets. For the ArMeme dataset, we replicated the experiments for both Arabic and English explanations."}, {"title": "5.2 Model Selection", "content": "As shown in Figure 1, our first experimental phase involves model selection among several recent VLMs, including Llama-3.2 (11b) (Dubey et al., 2024), Paligemma 2 (3b) (Steiner et al., 2024), Qwen2-vl (Wang et al., 2024), and Pixtral (12b) (Agrawal et al., 2024). We evaluate the base models in a zero-shot setting and fine-tune them using an instruction-following paradigm. The instructions prompt the model to generate responses in the format \u201cLabel: \". We use and a regex-based function to extract the predicted labels. Note that this stage fine-tunes the models to predict class labels only, allowing us to verify whether they can handle multilingual inputs\u2014especially in understanding Arabic text, cultural nuances, and image context. We do not ask the model to generate explanations here, as that is a more complex task and could affect their performance. Based on the results reported in Tables 5 and 6, we selected Llama-3.2-vision-instruct (11b) for further training with explanations.\""}, {"title": "5.3 Multi-Stage (MS) Optimization Procedure", "content": "To emphasize our novel contribution, we introduce a dedicated optimization procedure to train VLM with MemeXplain, which decouples the classification and explanation generation tasks. This approach is designed to first endow the model with strong task-specific representations through classification-only fine-tuning, and then refine its ability to generate coherent, natural explanations. Stage 1: Classification Fine-Tuning In this stage, the model is fine-tuned solely on the classification task. The training objective is restricted to predicting the correct class label. This focused objective encourages the model to develop robust, task-specific representations. We use the QLORA setup described later with a learning rate of \\(2 \times 10^{-4}\\) to optimize the model. Stage 2: Explanation Enhancement In this stage, the model is further fine-tuned on a combined label-with-explanation dataset. Here, we employ a reduced learning rate (\\(1 \times 10^{-5}\\)) to gently adapt the model's parameters for generating the explanations while preserving the classification performance achieved in Stage 1. To validate the effectiveness of the multi-stage procedure, we compare it against a single-stage (SS) fine-tuning baseline where the model is directly trained on the label-with-explanation dataset. Our ablation studies (detailed in Section 6) demonstrate that the proposed multi-stage approach significantly outperforms the single-stage strategy."}, {"title": "5.4 Training Setup", "content": "Our fine-tuning experiments utilize QLoRA (Dettmers et al., 2023), which combines INT4 quantization with parameter-efficient fine-tuning through Low-Rank Adaptation (LoRA) (Hu et al., 2022). In our setup, the base model is quantized to 4-bit precision, with LoRA updates applied to a subset of the model parameters. This approach was selected to address computational resource constraints. Furthermore, deploying models for inference incurs significant costs. Therefore, we focus on quantized models and assessing their performance accordingly. For all experiments, we fine-tuned the models using the QLoRA approach with 4-bit quantization. This approach was chosen due to its efficiency in reducing memory usage while maintaining model performance. We adapted all relevant submodules (vision, language, attention, and MLP layers) with a LoRA rank of 16, an alpha of 16, and no dropout. For training, we used a per-device batch size of 2 with gradient accumulation over 4 steps and optimized using AdamW with a learning rate of \\(2 \times 10^{-4}\\), a weight decay of 0.01, and a linear scheduler with 5 warmup steps. For the second stage experiments (label-with-explanation), the learning rate was reduced to \\(1 \times 10^{-5}\\)."}, {"title": "5.5 Evaluation Setup and Metrics", "content": "We train the models using the training set, fine-tune the parameters with the development set, and evaluate their performance on the test set as reported in Tables 5 and 6. For performance measurement across different experimental settings, we compute accuracy, weighted F1 score, and macro-F1 score. We evaluate the model's explanation performance on the test set using semantic similarity-based metric, measured by the F1 score within BERTScore (Zhang et al., 2020). This score is computed using contextual embeddings extracted from pre-trained BERT models. To enhance accuracy, we utilize language-specific transformer models for embedding extraction. For Arabic we use AraBERT (v2) (Antoun et al., 2020) model and for English we use bert-base-uncased model (Devlin et al., 2019). Although metrics such as BLEU and ROUGE are commonly used, studies have reported their limitations (Xu et al., 2023; Krishna et al., 2021). Therefore, we rely solely on BERTScore."}, {"title": "6 Experimental Results and Discussion", "content": "This section first presents competitive results among our proposed method and the state-of-the-art approaches. Next, it briefly analyzes and investigates the proposed method to validate and highlight the core contributions of this research. Table 7 compares our proposed models with state-of-the-art approaches. On the ArMeme dataset, our method achieves the best accuracy at 72.1% and the best weighted F1 at 0.699, with Qarib and mBERT following behind. Although Qarib attains the highest macro F1 (0.551), our model remains competitive with a macro F1 of 0.536. Importantly, our method stands out because it provides explanations that add significant value. On the Hateful Meme dataset, our approach clearly outperforms the state-of-the-art by achieving the best performance with an accuracy of 79.9%, a weighted F1 of 0.802, and a macro F1 of 0.792. These results clearly highlight the advantages of our explainability-enhanced dataset and the proposed multi-stage optimization procedure for both classification and explanation-generation tasks. Table 8 provides classification and explanation-generation results on the ArMeme and Hateful Meme datasets. It briefly presents these results from several perspectives: (a) Base vs. FT: demonstrates the performance difference between the same model with and without fine-tuning (FT); - FT with Llama MS Ar-Exp. The results show that fine-tuning using explanations generated in both languages yields comparable outcomes. This validates the multilingual capability of our empirically chosen VLM for the target task and enables users to understand multilingual content even if they are not fluent in that language. For example, our model allows an English speaker to analyze Arabic memes and receive explanations in English. (b) Single-stage (SS) vs. Multi-stage (MS): highlights the necessity and benefits of the proposed optimization procedure and (c) Eng-Exp vs. Ar-Exp: showcases the multilingual capability of the selected VLM. Next, we provide a brief analysis of the results based on these perspectives. First, we compare the Base vs. FT setup, from which it is evident that the FT model significantly outperforms the baseline. For example, on the ArMeme dataset, while the baseline achieves an accuracy of 12.7%, the proposed fine-tuning boosts it to 72.1%. Similarly, on the Hateful Meme dataset, fine-tuning improves the base accuracy from 65.2% to 79.9%. We observe similar improvements in the F1 metrics for classification and BERTScore for explanation quality. These significant performance gains validate our approach of fine-tuning the base models with the explainability enhanced dataset, demonstrating its efficacy for the meme classification and explanation generation tasks. Next, we compare the SS vs. MS setup, which reveals that multi-stage (MS) fine-tuning further enhances performance over the single-stage (SS) approach. For example, on the ArMeme dataset, the accuracy increased from 68.2% to 72.1%, the weighted F1 increased from 0.584 to 0.699, the macro F1 increased significantly from 0.257 to 0.536, and the BERTScore for Arabic explanation increased significantly from 0.58 to 0.72. A similar trend is observed on the Hateful Meme dataset, where additional fine-tuning iterations yield more robust classification (approximately 4% improvement) and enhanced explanation quality. These performance gains validate our proposed multi-stage optimization procedure to further refine the VLMs. Finally, we assess the model's multilingual capability by comparing the performance of Llama MS"}, {"title": "7 Conclusions and Future Work", "content": "In this study, we introduce a MemeXplain dataset for propagandistic and hateful meme detection and natural explanation generation, making it the first resource of its kind. To address both detection and explanation generation tasks and ensure efficient VLMs model training on this dataset, we also propose a multi-stage optimization procedure. To evaluate the multilingual capability of the model, we developed Arabic and English explanations for Arabic memes. The inclusion of English explanations benefits non-Arabic speakers, whereas providing explanations in the native language ensures that cultural nuances are accurately conveyed. With our multi-stage training procedure, we demonstrate improved detection performance for both ArMeme and hateful memes. The higher performance of explanation generation further demonstrates the efficacy of our multi-stage training approach. We foresee several future directions to extend this research and explore the following: (a) training the model with additional data through data augmentation, which could help it become an instruction-generalized model and potentially enhance its performance further; (b) incorporating pseudo and self-labeled data using an active learning procedure to incrementally improve the model's capabilities; and (c) developing a task-generalized model that addresses multiple tasks."}, {"title": "8 Limitations", "content": "Due to the complex nature of manual explanation creation, we have relied on GPT-40 for explanation generation. To ensure the reliability of the explanation we have manually evaluated in four criteria such as informativeness, clarity, plausibility, and faithfulness on a small sample for each set of explanation. The preliminary evaluation scores suggest that we can rely on the gold explanation as the reference. As a part of ongoing work we plan to conduct manual evaluation on a larger set. An important aspect of the ArMeme dataset is that it is highly imbalanced, which affects overall performance. One possible approach to address this issue is to increase the number of memes labeled as propaganda, other, and not-meme. This can be achieved through data augmentation or by collecting additional memes."}, {"title": "Ethics and Broader Impact", "content": "We extended existing datasets by adding explanations. To the best of our knowledge, the dataset does not contain any personally identifiable information, making privacy risks nonexistent. Regarding the explanations, we provided clear annotation instructions and cautioned annotators that some memes might be offensive. It is important to note that annotations are inherently subjective, which can introduce biases into the overall evaluation results. We encourage researchers and users of this dataset to remain critical when developing models or conducting further research. Models built using this dataset could be highly valuable for fact-checkers, journalists, and social media platforms."}, {"title": "A.1 Metrics", "content": ""}, {"title": "A.1.1 Informativeness", "content": "Measures the extent to which the explanation provides relevant and meaningful information for understanding the reasoning behind the label. A highly informative explanation offers detailed insights that directly contribute to the justification, while a low-informative explanation may be vague, incomplete, or lacking key details. As an annotator, you are judging if the explanation provides enough information to explain the label assigned to the meme.\n\u2022 1 = Not informative: The explanation lacks relevant details and does not help understand why the meme is labeled as such.\n\u2022 2 = Slightly informative: The explanation provides minimal information, but key details are missing or unclear.\n\u2022 3 = Moderately informative: The explanation contains some useful details but lacks depth or supporting reasoning.\n\u2022 4 = Informative: The explanation is well-detailed, providing a clear and meaningful justification for the label.\n\u2022 5 = Very informative: The explanation is thorough, insightful, and fully justifies the label with strong supporting details."}, {"title": "A.1.2 Clarity", "content": "Assesses how clearly the explanation conveys its meaning. A clear explanation is well-structured, concise, and easy to understand without requiring additional effort. It should be free from ambiguity, overly complex language, or poor phrasing that might hinder comprehension. As an annotator, you are judging the language and structure of the explanation. Spelling mistakes, awkward use of language, and incorrect translations will negatively impact this metric.\n\u2022 1 = Very unclear: The explanation is confusing, vague, or difficult to understand.\n\u2022 2 = Somewhat unclear: The explanation has some clarity but includes ambiguous or poorly structured statements.\n\u2022 3 = Neutral: The explanation is somewhat clear but may require effort to fully grasp.\n\u2022 4 = Clear: The explanation is well-structured and easy to understand with minimal ambiguity.\n\u2022 5 = Very clear: The explanation is highly readable, precise, and effortlessly understandable."}, {"title": "A.1.3 Plausibility", "content": "Refers to the extent to which an explanation logically supports the assigned label and appears reasonable given the meme's content. A plausible explanation should be coherent, factually consistent, and align with the expected reasoning behind the label. While it does not require absolute correctness, it should not contain obvious contradictions or illogical claims. As an annotator, you are judging if the explanation actually supports the label assigned to the meme. For example, if a meme is labeled as Not Propaganda, the explanation given should justify that label.\n\u2022 1 = Not plausible at all: The explanation does not align with the label and seems completely incorrect.\n\u2022 2 = Weakly plausible: The explanation has some relevance but lacks strong justification or contains logical inconsistencies.\n\u2022 3 = Moderately plausible: The explanation somewhat supports the label but may be incomplete or partially flawed.\n\u2022 4 = Plausible: The explanation logically supports the label and is mostly reasonable.\n\u2022 5 = Highly plausible: The explanation is fully aligned with the label and presents a strong, logical justification."}, {"title": "A.1.4 Faithfulness", "content": "Measures how accurately an explanation reflects the reasoning behind the assigned label. A faithful explanation correctly represents the key factors and logical steps that justify the label", "all": "The explanation is completely"}]}