{"title": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural Framework for Al Safety with Challenges and Mitigations", "authors": ["CHEN CHEN", "ZIYAO LIU", "WEIFENG JIANG", "GOH SI QI", "KWOK-YAN LAM"], "abstract": "AI Safety is an emerging area of critical importance to the safe adoption and deployment of AI systems. The adoption of AI as an enabler in digital transformation comes with risks that can negatively impact individuals, communities, society, and the environment. Specifically, AI introduces new ethical, legal and governance challenges, these include risks of unintended discrimination potentially leading to unfair outcomes, robustness, privacy and security, explainability, transparency, and algorithmic fairness. While AI has significant potential to support digitalization, economic growth, and advancement of sciences that benefit people and the world, with the rapid proliferation of AI and especially with the recent development of Generative AI (or GAI), the technology ecosystem behind the design, development, adoption, and deployment of AI systems has drastically changed. Nowadays, AI systems are highly interdependent or at least heavily dependent on third-party models (or even open-source models), whose failure may propagate down the AI technology supply chain and result in an unmanageable scale of negative safety impacts on society. With the new risks of GAI, failure of AI systems at one organization, or Al risks undertaken by one organization, may affect the entire AI ecosystem, potentially lead to collective failures, and cause large-scale harm to society, the economy, and the environment. AI Safety aims to address the pressing needs of developing the science and tools for specifying, testing, and evaluating AI models and AI systems to maintain a trusted supply chain of AI technologies and models; hence the safety of societies and communities that are supported by AI systems. Safe, responsible, and trustworthy deployment of AI systems are the key requirements in digitalization and digital transformation. This paper presents a novel architectural framework of AI Safety, supported by three key pillars: Trustworthy AI, Responsible AI, and Safe AI. Trustworthy AI focuses on the technical aspect, requiring AI systems' hardware, software, and system environment to behave as specified. Responsible AI considers ethical and organizational aspects, including fairness, transparency, accountability, and respect for privacy. Safe AI addresses the impacts of AI risks at the ecosystem system, community, society, and national level. AI Safety is an interdisciplinary area that aims to develop a risk management framework, best practices, and scientific tools to support the governance of the rigorous design, development, and deployment processes of AI models and AI systems that interact and impact people's daily lives. In this paper, we provide an extensive review of current research and developments in these characteristics, highlighting key challenges and vulnerabilities. Mitigation strategies are discussed to enhance AI Safety, incorporating technical, ethical, and governance measures. Through examples from state-of-the-art AI technologies, particularly Large Language Models (LLMs), we present innovative mechanism, methodologies, and techniques for designing and testing AI safety. Our goal is to promote advancement in AI safety research, and ultimately enhance people's trust in digital transformation.", "sections": [{"title": "1 Introduction", "content": "AI Safety is an emerging area of critical importance to the safe adoption and deployment of Al systems. While these systems enable digital transformation, they also pose risks that can negatively impact individuals, communities, society, and the environment [9, 147, 329]. Specifically, AI introduces new ethical, legal, and governance challenges, which include unintended discrimination potentially leading to unfair outcomes, robustness issues, privacy and security concerns, explainability, transparency, and algorithmic fairness. To address these challenges, the concepts of Trustworthy AI and Responsible AI have been proposed to ensure that Al systems comply with organization policies, and align societal norms and values [47, 90, 92, 145, 250, 298, 393, 756]. With the rapid proliferation of AI, particularly the recent development of Generative AI (or GAI), the technology ecosystem behind the design, development, adoption, and deployment of AI systems has drastically changed. This shift introduces new challenges for Trustworthy AI and Responsible AI and raises emergent forms of risks. Firstly, frontier Al systems are highly interdependent or at least heavily dependent on third-party models (or even open-source models), whose failure may propagate down the Al technology supply chain and result in an unmanageable scale of negative safety impacts on society [194, 295]. Secondly, with the new risks of GAI, failure of AI systems at one organization, or Al risks undertaken by one organization, can affect the entire Al ecosystem, and potentially lead to collective failures and cause large-scale harm to society, the economy and the environment [194]. For instance, if a generative Al model used by a major news agency hallucinates and generates invalid content, the impacts of this false information can be amplified by other media channels, inadvertently leading to widespread disinformation, which undermines public trust in the media industry. Given these emergent challenges, it is imperative to broaden the scope of AI Safety to fully cover the complexities introduced by advanced GAI technologies. This expansion not only involves enhancing the concepts of Trustworthy AI and Responsible AI but also demands the introduction of a new requirement: Safe AI, a critical AI Safety characteristic that arises at a broader ecosystem level. Therefore, AI Safety aims to address the pressing need of developing the science and tools for specifying, testing, and evaluating AI models and Al systems to maintain a trusted supply chain of AI technologies and models, ensuring the safety of societies and communities that rely on these Al systems. Safe, responsible, and trustworthy deployment of Al systems are the key requirements in digitalization and digital transformation. This paper presents a novel architectural framework of AI Safety, supported by three key pillars: Trustworthy AI, Responsible AI, and Safe AI. Trustworthy AI focuses on the technical aspect, requiring AI systems' hardware, software, and system environment to behave as specified. Responsible AI considers ethical and organizational aspects, including fairness, transparency, accountability, and respect for privacy. Safe AI addresses the impacts of AI risks at the ecosystem system, community, society, and national level. These concepts are demonstrated in Fig. 1."}, {"title": "2 Background", "content": "In this section, we provide the background information for the subsequent discussions. First, we introduce the concept of AI foundation models and their instances, e.g., LLMs, in Section 2.1. Second, we review the lifecycle of AI foundation models in Section 2.2, from their development to deployment. Finally, Section 2.3 defines Al systems and AI Safety, along with related notions such as Trustworthy AI, Responsible AI, and Safe AI."}, {"title": "2.1 Al Foundation Model", "content": null}, {"title": "2.1.1 Language Model", "content": "Language Model (LM) [136, 304, 700] is a probabilistic model that predict a probability distribution P(y) of a sequence of tokens y = y1y2...yT, where T is the sequence length. Using the product rule of probability (a.k.a. the chain rule), this joint probability is decomposed into: P(y) = P(y1) \\cdot P(y2|y1)...P(yT|y1,..., y_T-1) = \\prod_{t=1}^T P(y_t|y_{<t}) Typically, Language models obtain P(y) by autoregressively predicting the conditional probabilities P(yt|y<t), i.e., the probability distribution of yt given the preceding context y<t. In the generation process, the next token yt at each step is determined by the model's prediction P(yt|y<t). To enhance output performance, multiple decoding strategies are explored to improve the output performance [214, 307, 594, 628]. More decoding details are discussed in Section 2.2.3. Transformer architecture [700] has become the de facto standard for language modelling. The architecture follows an encoder-decoder design, where the encoder and decoder modules consist of a stack of transformer blocks, each comprising a Multi-Head Attention layer and a feedforward layer, connected by layer normalization [31] and residual connection modules [296]. In practice, the architectures are implemented to be encoder-only [174, 297, 439], decoder-only [585, 586] and encoder-decoder [391, 588] models, depending on their use cases. These Transformer-based Pre-trained Language Models (PLMs) have been applied in a wide range of downstream tasks, such as information"}, {"title": "2.1.2 Large Language Models", "content": "Large Language Models (LLMs) extend from PLMs but contain many more parameters, usually billions (or more) of parameters, which are trained on massive amounts of diverse text data. Recent advanced LLMs usually adopt decoder-only architectures [690, 806]. With their immense capacity, LLMs exhibit remarkable \"emergent abilities\" [738] that are not present in smaller-scale PLMs, i.e., in-context learning (ICL) [81], chain-of-thought (CoT) reasoning [739], and instruction following [564]. These emergent abilities make LLMs exceptionally capable and versatile, enabling them to perform a variety of tasks with notable performance. Examples of LLMs include proprietary models, e.g., ChatGPT [541, 542] and PaLM [21, 138] families, as well as open-source models like LLaMA2 [690] and ChatGLM [806], which serve as foundations in LLM research and development. Multi-modal Large Language Models (MLLMs) often build upon the capabilities of text-based LLMs by incorporating visual information, enabling them to process and generate both textual and visual content. These models typically consist of three key components: an LLM backbone, one or more visual encoders, and vision-to-language adapter modules. The LLM backbone, often from the open-source LLMs, such as LLaMa family [690] or their derivatives like Alpaca [680] and Vicuna [134], serves as the primary interface with the user. The visual encoders are specifically designed to extract relevant features from visual inputs and provide them to the LLMs [321, 399]. These signals are often encoded separately, with the vision-to-language adapters ensuring seamless interoperability between the visual and textual domains [30, 114, 227]. This design enables MLLMs to effectively integrate information from both modalities, allowing them to handle tasks such as visual question answering [227], image captioning [429], and visual dialogue [251]."}, {"title": "2.1.3 Other Al Foundation Models", "content": "Alongside LMs and LLMs, there are other prevalent types of AI foundation models. One popular class is Diffusion Models (DMs), which are developed for image and video generation [303, 530, 654-657]. DMs operate by gradually adding noise to the input data in a series of steps (forward diffusion process), and then learning to reverse this process (reverse diffusion process) to generate new samples. Notable examples include DALL-E [592, 593] and Stable Diffusion [603] for generating high-quality images from textual prompts, and Sora [79, 442] for video generation. Despite the popularity of these models, this paper primarily focuses on LLMs to maintain a concentrated and coherent scope. For research on the safety perspectives of DMs, please refer to [416, 582, 609, 725, 843, 852]."}, {"title": "2.2 Al Foundation Model Life-cycle", "content": "The AI foundation model life-cycle comprises multiple key stages, i.e., pre-training, alignment, and inference. Risks and safeguards are presented throughout these stages, and understanding them is essential for safeguarding the development and deployment of AI foundation models."}, {"title": "2.2.1 Pre-training", "content": "Data Preparation. Data preparation refers to collecting a large amount of high-quality data from various sources, including general data like webpages [150], books [224], and dialogue text [54, 601], as well as specialized data such as multilingual text [775], scientific publications [681], and code [29, 533]. Before pre-training, the collected data undergoes extensive preprocessing to remove low-quality, duplicate, and privacy-sensitive content [81, 138, 614]. The preprocessed data is then carefully scheduled for pre-training, considering factors such as the proportion of each data source, known as data mixture [457, 766], and the order in which different types of data are presented to the model, i.e., data curriculum [123, 768]. According to scaling laws [305, 358], it is essential to align the volume of pre-training"}, {"title": "2.2.2 Alignment", "content": "Supervised Fine-tuning. Supervised fine-tuning is an effective strategy for aligning AI foundation models with human values and desired behaviors. Unlike pre-training, which involves training on large-scale unsupervised data, supervised fine-tuning focuses on adapting these models using smaller annotated datasets. In the realm of LLM, supervised fine-tuning is also known as instruction tuning [405, 680, 729], where models are refined to understand and process complex instructions. Research indicates that the diversity and quality of the fine-tuning dataset are crucial factors for successful fine-tuning [846]. Exposing the model to such a well-curated dataset enhances its ability to generalize on previously unseen tasks and achieve better alignment [144, 737]. Alignment Tuning. Another line of alignment approaches is alignment tuning, e.g., reinforcement learning from human feedback (RLHF) [545]. This technique starts by training a reward model to evaluate the quality of model outputs based on human preferences. After optimizing the reward model, a reinforcement learning algorithm, typically Proximal Policy Optimization (PPO) [617], is employed to fine-tune the AI foundation model using the reward model's feedback. RLHF has shown effectiveness in Al foundation model alignment and safety enhancement [159], however, its implementation is complex and potentially unstable due to intricate training procedures. To address these challenges, recent efforts have explored alternative approaches, such as learning human preferences through ranking objectives [587, 653, 840] or in a supervised manner [431, 433]. Recently, the concept of Reinforcement Learning from AI Feedback (RLAIF) [39, 385] and Reinforcement Learning from Human and AI Feedback (RLHAIF) [567, 613] are introduced to reduce human involvement."}, {"title": "2.2.3 Inference", "content": "The inference for AI foundation models involves choosing the optimal decoding strategies to generate coherent and context-aware output. Greedy search selects the most likely token at each step [628], while sampling-based methods choose the next token based on its probability distribution [307, 594]. However, these basic methods may lead to suboptimal or repetitive outputs. To alleviate these issues, advanced decoding strategies have been developed for greedy search, such as beam search, length penalty, and diverse beam search [214, 560, 706]. Similarly, for sampling-based methods, temperature sampling and contrastive decoding are introduced to further control randomness [407]. Additionally, researchers have made efforts to improve decoding efficiency. Data transfer reduction aims to optimize GPU memory access and minimize memory fragmentation [163] while decoding strategies optimization is designed to enhance the sequential auto-regressive generation process [108, 149, 390]."}, {"title": "2.3 Formulation of Al Safety", "content": "In this section, we start with defining the AI system and its variant Al pipeline (Section 2.3.1). Based on these concepts, we provide the principles of AI Safety and its formulation (Section 2.3.2)."}, {"title": "2.3.1 Definition of Al system", "content": "Despite the term \"AI system\" being widely used in academic publications and public discourse [192, 480, 632], the literature has yet to converge on a single, universally accepted definition that precisely delineates such a system. Some endeavors focus on developing foundational models [690, 806], while recent efforts have emphasized the development of complex systems that integrate various Al modules, such as traditional machine learning, LLMs, and Agent-based AI [480, 563]. These modules serve specific purposes within the system. Here we attempt to provide a comprehensive conceptualization of AI systems. One notable example of AI foundation model is LLMs, which can process instructions and provide decision-making capabilities in textual form. Al foundation models often serve as core components within larger systems, enabling other components to function effectively. DEFINITION 1 (AI SYSTEM). An Al system S involves a collection of interconnected Al or non-Al modules Mi \\in M, each parameterized by \\theta_i. The interconnections are represented by the topology R, where a specific connection r_{i,j} indicates the information flow from module M_i to module M_j. Formally, S = \\{M_i(\\theta_i)\\}_{i=1}^n | r_{i,j} \\in R where n = |M|. The collection of parameters for the entire system can be denoted as:  \\Theta = \\bigcup_{i=1}^n \\theta_i It is noteworthy that the modules Mi can be Al-powered, such as AI foundation models, or non-AI-powered, e.g., frontend, database and API. Generally, an Al system contains at least one AI-powered module. Fig. 2 demonstrate the relations between Al foundation model and Al systems While the topology within an Al system can be considerably intricate, real-world AI applications often exhibit less complexity. Typically, the modules within an Al system are arranged sequentially, such that the output of module Mi"}, {"title": "2.3.2 Definition of Al Safety", "content": "The field of AI Safety refers to theories, methodologies and practices that ensure safe AI foundation models and AI systems. When contemplating them as a black-box operations, they can be expressed as a function S: X \u2192 Y, where X and Y represent input and output space respectively. We consider an Al system to satisfy AI Safety if it adheres to key principles and constraints on y and S during runtime. We conceptualize these guiding principles as follows. DEFINITION 3 (AI SAFETY PRINCIPLE I \u2013 OUTPUT CONSTRAINT). An Al system S is considered to comply with AI Safety Principle I if its output space Y is disjoint from a set of prohibited outputs Z, i.e., Y \u2229 Zi = 0 and Zi \u2286 Z for all i, where Zi is the unsafe output according to certain criteria. DEFINITION 4 (AI SAFETY PRINCIPLE II - RUNTIME CONSTRAINT). An Al system S adheres to AI Safety Principle II if it is capable of operating under a collection of predefined requirements Ri \u2208 R. Principle I and Principle II establish essential controls on AI systems, focusing on output and runtime operation, respectively. Principle I mandates that an AI system must avoid generating prohibited outputs. For instance, LLM systems must prevent producing harmful content, including biased or offensive language. Principle II requires AI systems to operate within certain requirements, such as maintaining transparency and explainability. These detailed constraints Z and R may slightly vary between systems, depending on the specific safety needs of the design. DEFINITION 5 (Trustworthy AI). Trustworthy AI requires an AI system ST to function as intended, be resilient against dangerous modifications and operate securely. Specifically, Trustworthy AI follows AI Safety Principle I where prohibited output set ZT in Trustworthy AI represents failure cases of the normal function. Definition 6 (Responsible AI). Responsible AI highlights an AI system SR to align with ethical principles and values. Responsible AI includes the scope of Trustworthy AI and requires additional AI Safety Principle I and II where prohibited output set ZR denotes the outputs misaligned with ethical norms and the requirements RR are transparency and explainability of the Al system. DEFINITION 7 (SAFE AI). Safe Al refers to the objective of an AI system SS to ensure its harmlessness to the entire AI ecosystems. Safe AI includes the scope of Responsible AI and further mandates AI Safety Principle I where prohibited output set ZS denotes the outcomes that are harmful to AI ecosystems. Building upon these principles, we proceed to a formal definition of AI Safety. This definition establishes the scope for our discussion, identifying the specific safety considerations that fall within this paper. DEFINITION 8 (AI SAFETY). AI Safety involves the science, techniques, and tools ensuring that Al systems S satisfy Trustworthy AI, Responsible AI, and Safe AI."}, {"title": "3 Challenges to Trustworthy Al", "content": "In this section, we review the spectrum of risks associated with AI trustworthiness, focusing on how these risks can hinder the effectiveness and reliability of LLMs and their defence mechanisms. We start with an extensive literature review of safety issues induced by input modifications and manipulations in Section 3.1. This research examines whether LLMs could function as intended under various input conditions. We then delve into threats from adversarial attacks, including jailbreak and prompt injection in Section 3.2, which aim to bypass and undermine security measures. Additionally, we explore the safety concerns in different contexts, including vulnerabilities of multi-modal LLMs and system-level security, which are discussed in Section 3.4 and Section 3.3 respectively."}, {"title": "3.1 Challenges of Input Modifications and Manipulations", "content": "In real-world applications, user input to an Al system may not always align with what is initially anticipated. This variability underscores the importance of the robustness of LLMs, which refers to their ability to maintain performance levels under a variety of circumstances [9]. In this section, we will review input robustness testing on traditional PLMs in Section 3.1.1 and introduce how this testing is extended to LLM systems in Section 3.1.2."}, {"title": "3.1.1 Input Robustness Testing on PLMs", "content": "The concerns of robustness in Al systems were first emphasized by [65] and [673], which demonstrated that these applications are vulnerable to deliberately engineered adversarial perturbations. To identify adversarial examples in image classification, gradient-based techniques such as the Fast Gradient Sign Method (FGSM) [260] and Projected Gradient Descent (PGD) [472] were developed by adding trained perturbation. However, the discrete nature of text tokens prevents the direct application of these methods to NLP tasks. Consequently, attacks on NLP models generally involve a discrete perturbation scheme. This scheme aims to identify the textual elements that significantly impact model output and then implements targeted perturbation operations, such as adding, deleting, flipping, or swapping, on them. The perturbation methods can broadly be organized into three principal types: character-level, word-level, and sentence-level. Character-level perturbation implies the manipulation of texts by introducing deliberate typos or errors in words, such as misspellings or the addition of extra characters [223, 309, 398]. On the other hand, word-level perturbation focuses on substituting words with synonyms or contextually similar terms to mislead models [17, 346, 402, 415, 611]. This technique aims to maintain the overall meaning of the text while using alternative vocabulary. The selection of substituted words may be determined by their gradient [415, 611] or attention scores [346], while the similarity is usually measured using the metrics in the word embedding space [17], such as GloVe [565]. Lastly, sentence-level perturbation entails suffixing irrelevant or extraneous sentences to the end of prompts, with the intention of distracting models from the main context [517, 600]. An alternative methodology is to generate paraphrased adversaries using techniques such as Generative Adversarial Networks (GAN) or encoder-decoder PLM [132, 334, 823]. It is noteworthy that these perturbation strategies are not mutually exclusive; thus, a multi-level perturbation approach can be implemented in a single adversarial example as long as the perturbations are imperceptible to humans [398, 415]."}, {"title": "3.1.2 Input Robustness Testing on LLMs", "content": "Similar to PLMs, LLMs are also sensitive to the variability of prompts. For instance, researchers recognize that semantically similar prompts can yield drastically different performance [786]. This observation raises questions about whether perturbations designed for PLMs might also be effective for LLMs. Initial studies have focused on evaluating ChatGPT's robustness against adversarial samples [531, 715] using traditional benchmarks [720]. Furthermore, Zhao et al. [854] specifically examine the robustness of LLMs for the task of semantic"}, {"title": "3.2 Threats from Adversarial Attacks", "content": "Al systems are designed to maintain normal, safe behavior and benign outputs, typically ensured through various safety measures [9]. These safety mechanisms are integral to the functionality of AI systems and are expected to perform effectively. However, adversarial attacks, such as jailbreak and prompt injection, aim to strategically undermine the effectiveness of these safeguards. This can lead to unexpected events, such as the generation of toxic content, dissemination of harmful information, or outputs that violate social norms and ethics [171, 234, 381, 637]. For LLMs, malicious actors may attempt to deliberately exploit vulnerabilities in LLMs to elicit such undesirable responses through techniques such as jailbreaking (section 3.2.1) and prompt injection attacks (section 3.2.2)."}, {"title": "3.2.1 Jailbreak", "content": "LLMs are typically equipped with built-in safety and moderation features to prevent them from generating harmful or inappropriate content. However, malicious users may develop \"jailbreaking\" techniques, such as deliberately crafting manipulative jailbreak prompts, to penetrate or bypass these safeguards. [274, 635, 743] By exploiting their vulnerabilities, a jailbroken LLM can be made to perform almost any requested task, regardless of potential dangers or ethical considerations. As LLMs become increasingly capable and knowledgeable, the risks associated with jailbreaking grow more severe, because greater amounts of harmful information become accessible for misuse by malicious users [437]."}, {"title": "3.2.2 Prompt Injection", "content": "Prompt injection draws inspiration from traditional injection attacks, such as SQL injection [74, 286, 515] and cross-site scripting (XSS) [275, 328, 744] attacks, where the payload is intentionally manipulated to inject malicious code into a program and consequently mislead its normal behavior. Similarly, in the context of LLMs, prompt injection leverages user input to inject information into elements outside the input field, such as the system prompt. The key difference between jailbreaking and prompt injection lies in their strategies for deceiving the system. Jailbreaking disguises malicious prompts as benign ones, while prompt injection camouflages user input as system-level instructions. Prompt injection can take two forms: goal hijacking and prompt leaking [569]. Goal hijacking aims to shift the original goal of an LLM prompt to a new goal, and prompt leaking seeks to disclose the system prompt, which is not intended to be exposed. We provide examples of goal hijacking and prompt leaking in Table 3. These prompt injection strategies can be introduced through user input, i.e., direct injection, or other external sources, i.e., indirect injection."}, {"title": "3.3 Vulnerabilities in Multi-modal LLMs", "content": "MLLMs enhance the abilities of LLMs by seamlessly incorporating multi-modal information. This integration allows them to process and understand various channels, such as text, images, and audio, simultaneously [41, 247, 525, 526]. However, this multi-modal capability also introduces additional vulnerabilities that attackers can exploit for malicious purposes [633]. A straightforward method to deceive MLLMs involves using deceptive prompts [20, 160, 430, 792, 849], where the model is manipulated to respond to non-existing objects in the image [578, 730], leading to hallucination [409, 834]. These prompt-based attack strategies are extensions of those used against LLMs. Recently, new forms of attacks unique to MLLMs have been explored. One notable type of attack is structure-based, which manipulates the format and presentation of text within images to mislead MLLMs. A prevalent strategy in this category, particularly for vision-language models like Contrastive Language-Image Pre-training (CLIP) models [584], is the typographic attack. This method aims to induce misclassification of images by intentionally overlaying misleading text onto them [247, 257]. These typographic attacks could affect the performance on various tasks, including object recognition, enumeration, visual attribute detection, and commonsense reasoning [131]. For instance, attackers might introduce the text \"YELLOW\" onto an image, guiding MLLMs to misclassify green clothing as yellow, as demonstrated in Fig. 3 (a). Noever et al. [535] demonstrate that even when the overlay text is misspelled, the model can still be successfully misled into incorrect conclusions. Another method to perform typographic attacks involves the use of \"image-prompt,\" which is textual content represented in image form. This technique is to conceal sensitive or harmful information within an image, thereby bypassing MLLM defense mechanisms on the text channel [257, 633]. Alarmingly, MLLMs can autonomously generate and refine typographic attacks, thereby improving their attack success rate [581]. Another form of attack is the perturbation-based attack [534, 635, 743] (see Fig. 3 (b)). These attacks introduce perturbations to the model's input across various modalities. The perturbations are designed to be trainable and imperceptible to humans, yet they significantly influence the behavior of MLLMs, causing them to follow predefined malicious instructions [40, 291, 576, 634, 693, 815, 841]. Some studies have found that these perturbations are highly transferable across different models [534, 576, 856]. In white-box scenarios, visual components combined with harmful textual requests are encoded into the model's text embedding space, and optimized to produce positive affirmation [534, 634] using techniques like Projected Gradient Decent (PGD) [472]. These perturbation strategies can be extended to audio or video content, either by deceiving sound source visual localization models [684] or generating incorrect sequences for video-based LLMs [397]. To further improve the attack success rate, the Multi-modal Cross-Optimization Method (MCM) is proposed. This advanced jailbreak attack method potentially introduces perturbations on both text and image input channels while dynamically selecting optimization channels based on performance [323]. AnyDoor [458] presented a test-time backdoor attack that does not require access to training data. It applies universal perturbations to images, creating a backdoor in the textual modality that can activate harmful effects with fixed triggers. In black-box scenarios, where attackers have access only to APIs, Li et al. [410] employ an iterative process of prompt optimization to progressively amplify the harmfulness of images generated by an image generation model. These optimized images are used to conceal the malicious intent within the text input, facilitating successful MLLM attacks. Building on this trend, Wu et al. [763] target bypassing defensive system prompt of MLLMs and identify effective jailbreak prompts through iterative search. Under grey-box settings, transfer attack strategies are commonly used. Researchers [181, 841] utilize white-box surrogate models, such as CLIP [584, 666] and BLIP [400], to craft targeted adversarial examples and then transfers these examples to larger MLLMs. To enhance their efficacy, OT-Attack [35] introduces Optimal Transport theory to balance the effects of data augmentation and modality interactions. Additionally, MLLMs are susceptible to data poisoning where attackers tamper with a portion of the training data to influence models' behavior during inference (see Fig. 3(c)). Shadowcast [773] initiates the data poisoning attack on MLLMs from two angles: label attack and persuasion attack. Label attack tricks MLLMs into misidentifying class labels of input image content, while persuasion attack induces MLLMs to craft harmful yet persuasive narratives, such as convincing people that junk food is healthy. ImgTrojan [678] contaminates the training dataset by injecting poisoned (image, text) pairs, where the text is replaced with Malicious Jailbreak Prompts (JBP). These data are strategically crafted to teach MLLMs the associations between harmful instructions and corresponding images, enhancing the success rate and stealthiness of the jailbreak attacks. Unlike previous work that targets only a single modality, Yang et"}, {"title": "3.4 Challenges to System-Level Security", "content": "As defined by Definition 1 and demonstrated in Fig. 2, AI systems may incorporate various modules working closely together to achieve the goal. However, the potential for systemic failures escalates if they are not properly managed. One critical issue is the propagation of errors within or across multiple modules [331, 757, 829]. The risks to system-level safety are presented from two perspectives. In section 3.4.1, we present the incompatibility of safety measures of AI and non-Al modules within the system. In section 3.4.2, we discuss the possible safety issues arising from the interaction of multiple Al foundation models or agents."}, {"title": "3.4.1 Vulnerability from Al and non-Al Modules", "content": "Real-world tasks are often too complicated to be solved by a single AI foundation model, requiring the use of advanced systematic solutions. Developers and system architects increasingly rely on multiple modules, either AI or non-AI, to streamline and enhance their operations. For instance, applications like Langchain [107], AutoGPT [787], and ChatGPT [541], enhanced with various plugins [542], stand out for their ability to tackle complex sub-tasks through a network of interconnected components (Definition 1). These applications can also be incorporated as a middleware [107, 432] in larger platforms, offering scalable solutions for diverse development needs. Within these applications, each module typically specializes in particular functionalities such as user interaction and data transmission, and is often developed to meet high safety standards. However, despite the robust security of individual modules, the overall system may still be vulnerable due to potential weaknesses in the integration and interaction between them. The vulnerability of current LLM systems is often exposed through system-level indirect prompt injections. An innovative study by [757] evaluates the robustness of the GPT-4 system, examining its interactions with other system components such as sandboxes, web tools, and frontend interfaces. This research provides numerous examples of the manipulation of the GPT-4 system to generate private and unethical content. Furthermore, it introduces an end-to-end attack framework that allows an adversary to illicitly acquire a user's chat history by exploiting system plugins. This method not only bypasses security constraints but also maintains stealth, even when handling long data sequences. Similarly, Iqbal et al. [331] investigate the vulnerabilities in ChatGPT's third-party plugin by analyzing 268 plugins hosted on OpenAI's plugin store. The study examines unsafe information flow between plugins and users, plugin and LLM systems, and among different plugins. Additionally, Abdelnabi et al. [3] highlight the risk associated with retrieval components, which are usually used to fetch external information to augment LLM prompts. The retrieval of malicious data from an adversary can poison the user's prompt and deliberately modify the behavior of LLMs in"}, {"title": "3.4.2 Vulnerability from Multiple Al Agents", "content": "Al systems generally comprise at least one AI agent, and achieving intricate objectives often requires the use of multiple agents. In the domain of LLMs, multi-agent systems present a complex architecture where multiple LLM-based agents can interact within an environment [289, 661"}]}