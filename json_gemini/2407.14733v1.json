{"title": "Hard Prompts Made Interpretable:\nSparse Entropy Regularization for Prompt Tuning with RL", "authors": ["Yunseon Choi", "Sangmin Bae", "Seonghyun Ban", "Minchan Jeong", "Chuheng Zhang", "Lei Song", "Li Zhao", "Jiang Bian", "Kee-Eung Kim"], "abstract": "With the advent of foundation models, prompt\ntuning has positioned itself as an important\ntechnique for directing model behaviors and\neliciting desired responses. Prompt tuning re-\ngards selecting appropriate keywords included\ninto the input, thereby adapting to the down-\nstream task without adjusting or fine-tuning\nthe model parameters. There is a wide range of\nwork in prompt tuning, from approaches that di-\nrectly harness the backpropagated gradient sig-\nnals from the model, to those employing black-\nbox optimization such as reinforcement learn-\ning (RL) methods. Our primary focus is on\nRLPrompt, which aims to find optimal prompt\ntokens leveraging soft Q-learning. While the\nresults show promise, we have observed that\nthe prompts frequently appear unnatural, which\nimpedes their interpretability. We address this\nlimitation by using sparse Tsallis entropy reg-\nularization, a principled approach to filtering\nout unlikely tokens from consideration. We\nextensively evaluate our approach across vari-\nous tasks, including few-shot text classification,\nunsupervised text style transfer, and textual in-\nversion from images. The results indicate a\nnotable improvement over baselines, highlight-\ning the efficacy of our approach in addressing\nthe challenges of prompt tuning. Moreover,\nwe show that the prompts discovered using our\nmethod are more natural and interpretable com-\npared to those from other baselines (Deng et al.,\n2022)", "sections": [{"title": "1 Introduction", "content": "While the use of large-scale language models\n(LMs) and vision-language models (VLMs), pre-\ntrained on a massive amount of data, is becoming a\ndominant paradigm in machine learning (Rombach\net al., 2022; Touvron et al., 2023; Radford et al.,\n2021), fine-tuning the model parameters for adap-\ntation to downstream tasks requires a vast amount\nof computational time and resources. In this sense,\nprompt tuning has emerged as a promising low-cost\nsolution (Brown et al., 2020; Lester et al., 2021),\ndiscovering input prompts that effectively guide\nthe pre-trained models to generate the desired out-\nputs, while keeping the model parameters frozen.\nPrompt tuning is generally categorized into two ap-\nproaches, soft and hard prompting methods, based\non their representation of prompts.\nSoft prompt methods (Lester et al., 2021; Li and\nLiang, 2021) primarily focus on learning continu-\nous embedding vectors at the token level, which\nare called as soft prompts. They usually perform\nthe gradient descent to optimize these continuous\nembedding vectors (Wu et al., 2023; Zhu et al.,\n2023). However, the prompts learned through soft\ntuning are opaque to human interpretation and are\nnot compatible with other pre-trained models that\ndo not share the same embedding spaces. More-\nover, computing internal gradients for models is\nhighly resource-intensive, especially as the num-\nber of model parameters increases, or even infea-\nsible in cases where models are accessible only\nthrough APIs (OpenAI, 2023). These limitations\nnecessitate an alternative approach: discovery of\nthe prompts composed of human-readable discrete\ntokens, referred to as hard prompts (Prasad et al.,\n2023; Shin et al., 2020; Deng et al., 2022).\nHard prompts offer numerous advantages over\nsoft prompts: they are transferable from one pre-\ntrained model to another since they are agnostic to\nthe embedding. Moreover, they confer composi-\ntional control by facilitating manual merging and\nmodification (Wen et al., 2023). Despite the bene-\nfits, they require large-scale discrete optimization\nin principle. Approaches for finding optimal hard\nprompts often employ backpropagated gradients\nfrom models as with soft prompt methods, while\ncircumventing the discrete optimization by, roughly\nspeaking, mapping the soft prompt to the similar\ndiscrete tokens in the embedding space (Wen et al.,"}, {"title": "2 Related Works", "content": "Prompting in Language Models Pioneering\nwork by Brown et al. (2020) highlighted the ef-\nficacy of using prompts for task adaptation in pre-\ntrained language models, a technique now com-\nmonly referred to as instruction tuning. This\napproach has become standard in enhancing the\nability of large models to execute complex, task-\nspecific instructions. Despite its success, the auto-\nmated generation of effective text prompts, particu-\nlarly hard prompts, remains a challenge. The work\nby Lester et al. (2021) to simplify prefix tuning\nled to the establishment of standard soft prompt\ntuning, which optimizes continuous embeddings\nthat are appended to embeddings of input tokens.\nHowever, Khashabi et al. (2022) pinpointed a lim-\nitation of this approach: the resulting embedding\nsequences often lack clear semantic interpretation.\nTo address these limitations, our work focuses on\nthe hard prompts optimization within a selected\nset of tokens, thereby generating task-specific and\ninterpretable tokens.\nDiscrete Optimization for Hard Prompts Au-\ntoPrompt (Shin et al., 2020) is an initial framework\nfor discrete prompt optimization in transformer-\nbased language models, inspiring a range of diverse\nmethods. These include a gradient-free phrase\nediting method (Prasad et al., 2023), a reinforce-\nment learning-based approach (Deng et al., 2022),\nand an embedding optimization approach based\non Langevin dynamics (Shi et al., 2023). In our\nwork, we first benchmark gradient-based methods\nlike AutoPrompt (Shin et al., 2020) and PEZ (Wen\net al., 2023) for hard prompt tuning. AutoPrompt\nemploys the HotFlip algorithm (Ebrahimi et al.,\n2018) to greedily identifies optimal tokens for each\nposition based on model gradients, while PEZ per-\nforms gradient re-projection during its continuous\noptimization within the embedding space. How-\never, both methods require substantial computa-\ntional cost in calculating gradients and are unsuit-\nable for black-box models. On the other hand, RL-\nPrompt (Deng et al., 2022) employs a gradient-free\nRL-based method, serving as our primary baseline.\nRLPrompt introduces an efficiently parameterized\nnetwork that maps continuous embedding vectors\nto adaptive vectors within the same space. Despite\nits simplicity, RLPrompt struggles with accurately\nrepresenting Q-values across all tokens, potentially\nleading to sub-optimal prompts as we discuss in\nSection 4.1."}, {"title": "3 Preliminaries", "content": "Hard Prompt Tuning with RL Hard prompt tun-\ning is the process of discovering an optimal prompt\nwithin the token space V, to efficiently tackle spe-\ncific downstream tasks. Assuming a fixed-length\nprompt of L tokens, this optimization can be for-\nmally defined as RL problem:\n$\\max_{z \\in V^L} R(y(z, x)).$ (1)\nHere, the objective is to find a discrete prompt\nz from the solution space of length-L token se-\nquences $V^L$, which maximizes a task-specific re-\nward function R when concatenated with input x."}, {"title": "4 Method", "content": "While the Q-network utilized by RLPrompt pro-\nvides efficient parameterization, it also possesses\na fundamental limitation. Training the Q-network\nis essentially solving for an extremely overdeter-\nmined linear system, where approximation error is\ninevitable. In order to observe this, we fix 20:t-1\nand rewrite Eq. (3) as the weighted least-squares\nproblem\n$\\min_{\\psi} || W_{LM} \\psi - q ||_2^2 \\qquad all (z_{0:t-1}),$ (6)\nwhere \u03c8 = (LM(20:t\u22121)) is the optimization\nvariable, and q = Q(20:t\u22121,\u00b7) is the right-hand-\nside vector. The coefficient matrix, which is the\nLM-head matrix from the policy LM, $W_{LM} \\in\nR^{|V| \\times dim(E)}$ has_many more rows than columns\nsince it is common that |V| \u226b dim(E) (e.g.,\n|V| = 50272 and dim(E) = 768 for OPT-125M),\ncorresponding to many more constraints than vari-\nables resulting in inevitable approximation error.\nTogether with the probabilities of tokens as\nweights in the least-squares formulation, the is-\nsue of approximation error becomes more critical.\nTokens with high probabilities, as estimated by \u03c0,\nreceive larger weights in the least-squares, leading\nto smaller approximation errors for them. On the\nother hand, the vast number of low-probability to-\nkens are assigned with smaller weights, resulting\nin relatively larger approximation errors. Conse-\nquently, the estimated action value could become\nunreasonably high for these low-probability tokens,\npromoting the RL algorithm to excessively try out\nthese improbable tokens. This results in an RL ap-\nproach that is overly biased towards exploration,\nspecifically favoring the selection of insignificant\nlow-probability tokens. The detrimental impact of\nthis unfortunate behavior is clearly observed by the\nunnatural prompts chosen by RLPrompt."}, {"content": "This reward function measures the appropriateness\nof the model output y(z, x), i.e., the output from\nLMs or VLMs when prompted with z for input\nx. For example, in a few-shot text classification\ntask utilizing masked LMs as task model, the re-\nward function can be defined as a binary signal\nthat indicate the correctness of model output based\non available few-shot data, where model output\nrefers to the predicted class for the [MASK] token\nposition\u00b2.\nEq. (1) can be approached using a bandit algo-\nrithm, which aims to identify a length-L token se-\nquence z that maximizes the reward R without\ngradient information. However, to cope with the\nexponentially large action space O(|V|L) for the\nbandit algorithm, we can treat the optimization as\na sequential decision-making process, as in RL-\nPrompt (Deng et al., 2022). More concretely, at\neach time step t, the algorithm chooses the token zt\nbased on the tokens 20:t\u22121 chosen at previous time\nsteps, denoted as policy \u03c0(zt|20:t-1). The calcula-\ntion of reward R is delayed until the completion of\nthe entire prompt sequence z to obtain the model\noutput. Thus, we optimize the policy \u3160 with the\nreformulation of Eq. (1), given by\n$\\max_{\\pi} E_{z \\sim \\Pi \\pi(z|z_{0:t-1})} [R(y(z, x))].$\nSoft Q-Learning (SQL) and RLPrompt RL-\nPrompt (Deng et al., 2022) employs SQL (Haarnoja\net al., 2017), an RL algorithm that incorporates\nentropy regularization. It aims to maximize the\nexpected cumulative reward with the bonus given\nby the entropy of the action distribution in order\nto balance the trade-off between exploitation and\nexploration. More formally, at each time step t,\nRLPrompt trains the policy with the objective:\n$\\max_{\\pi} E_{z \\sim \\pi(z|z_{0:t-1})} [Q(z_{0:t-1}, z) - \\alpha \\log \\pi(z|z_{0:t-1})],$ (2)\nwhere is the action-value function capturing\nlong-term reward effects as follows:\n$Q(z_{0:t-1}, z) E_{z_{t+1:L-1} \\sim \\pi}[R(y(z_{0:t-1}, z, z_{t+1:L-1}, x)],$\nand a is the regularization coefficient.\nEq. (2) yields an analytical solution for the opti-\nmal policy, expressed as\n$\\pi^*(z|z_{0:t-1}) = softmax(\\frac{Q(z_{0:t-1}, z)}{\\alpha}).$\nHowever, since the action-value function Q is not\nreadily available, it is estimated by a neural network\nparameterized by 0, referred to as Q-network.\nOne of the main contributions of RLPrompt is\nthe introduction of an efficient parameterization for\nthe Q-network. This involves integrating a frozen\nand pre-trained language model (LM), referred to\nas the policy LM, into the lower layers of the Q-\nnetwork. Besides, a trainable multi-layer percep-\ntron (MLP) layer is augmented at the upper level\nfor the adaptation to the downstream task. Thus,\nthe trainable parameter & of the Q-network is only\nthe parameters of the MLP layer.\nFormally, given the prefix of prompt 20:t-1, the\nencoding vector et \u2208 E obtained from the policy\nLM is passed through MLP layer Ve to compute an\nadapted embedding \u00eat \u2208 E, where & denote the real\nvector space of embeddings. Subsequently, this\nadapted embedding \u00eat is multiplied with the LM-head matrix of policy LM, $W^{LM} \\in R^{|V| \\times dim(E)}$, to\nget the next prompt token probabilities.\n$\\hat{e_t} = \\psi_{\\theta}(e_t) = \\psi_{\\theta}(LM(z_{0:t-1}))$\n$Q_{\\theta}(z|z_{0:t-1}) = {W^{LM}}^T \\hat{e_t}$\n$\\pi_{\\theta}(z|z_{0:t-1}) := softmax(\\frac{Q_{\\theta}(z|z_{0:t-1}, z)}{\\alpha})$\nwhere $W^{LM}$ is kept fixed. Thus, this particular\nparameterization makes ${W^{LM}}^T \\hat{e_t}$ collectively repre-\nsent the scaled Q-values for all tokens. In other\nwords, for token z \u2208 V and its index iz, its action\nvalue is calculated by ${w_i}^T \\psi_{\\theta}(e_t) = Q_{\\theta}(z|z_{0:t-1}, z)$,\nwhere wi denotes the i-th row vector of WLM.\nTo optimize the parameter 0, the objective is to\nminimize the temporal difference error,\n$E_{z \\sim \\pi_{\\theta}} [(Q_{\\theta}(z|z_{0:t-1}, z_t) - \\bar{Q}(z|z_{0:t-1}, z_t))^2].$ (3)\nHere, the target value $\\bar{Q}$ is the bootstrapped esti-\nmate of the action value, given by:\n$\\bar{Q}(z|z_{0:t-1}, z_t)$\n=\n\\begin{cases}\n\\gamma \\alpha \\log \\sum_{z \\in V} exp(\\frac{Q_{\\theta}(z|z_{0:t,z})}{\\alpha}) & \\text{ if t < L-1}\nR(y(z,x)) & \\text{ if t = L-1}\n\\end{cases}\nwhere \u03b3\u2208 (0, 1] denotes the discount factor.\nSparse Tsallis Entropy Regularized Q-Learning\nGiven a policy \u03c0(\u00b7|20:t\u22121) that represents a proba-\nbility distribution on z, the Tsallis entropy (Amari\nand Ohara, 2011) with entropic index q is defined as\n$S_q(\\pi) = k(1 - \\sum_{z} \\pi^q(z|z_{0:t-1}))$, where k is"}, {"title": "4.2 PIN (Prompts made INterpretable)", "content": "One of the straightforward solutions to mitigate\nthe challenge of the overdetermined linear system\nis to drop the constraints that are less important.\nTo achieve this, we introduce the ignorable token\nset comprised of tokens deemed improbable from\na general language model. The key concept is to\nsidestep evaluating the action values of these to-\nkens, as they may adversely impact the value esti-\nmations of high-probability tokens. The construc-\ntion of the ignorable token set entails utilizing the\nlogit of the predictive probability of tokens derived\nfrom the policy LM with the original embedding,\nand choosing those that score below the k-th largest\nlogit. Formally, given the prompt prefix 20:t-1, the\nignorable token set is defined by\n$I_{z_{0:t-1}} = \\{z \\in V | {w_i}^T e_t < {w_{i(k)}}^T e_t\\},$\nwhere et is the embedding vector of 20:t\u22121 obtained\nfrom the policy LM, wi is the i-th row vector of its\nLM-head matrix, and i(k) is the index of the token\nwith k-th largest logit.\nDeciding the number of k is important for learn-\ning effective hard prompts, as demonstrated by our\nexperimental results. If we set k aggressively (i.e.,\nsmall) to avoid constraint violation, we may end\nup ignoring strong candidate tokens. On the other\nhand, if we set k conservatively (i.e., large), we suf-\nfer from the original challenge of approximation\nerror. Therefore, we empirically chose k = 10000\nto ensure a sufficiently diverse set of tokens, disre-\ngarding about 80% of the vocabulary tokens. Nev-\nertheless, we observed that the ignorable token set\ndose not work well alone because k is significantly\nlarger than dim(E), which is 768, even in case of\nOPT-125M.\nNow, the remaining challenge is to address the\napproximation error still existent due to more con-\nstraints than variables for training the Q-network.\nIf we adopt SQL as the baseline RL method, we\nwill end up with the same undesirable behavior due\nto the dense probability over the tokens. We thus in-\nstead employ the sparse Tsallis entropy regularized\nQ-learning described in the previous section, which\nyields a sparse policy that naturally suppresses the\nprobability of choosing many unimportant tokens\nwith errors in the action value estimation.\nOur algorithm, PIN (Prompts made INter-\npretable), is presented in Algorithm 1. We remark\nthat the algorithm employs operator $F_I$ that sys-\ntematically filters out the action values of ignorable"}, {"title": "5 Experiments", "content": "The goal of these tasks is to find the optimal prompt\nthat assigns input text \u00e6 to the class label, given\na few examples in the context. We employ the\nsame experimental setting in Schick and Sch\u00fctze\n(2021); Deng et al. (2022), which addresses the\nclassification tasks via prompting the task LM and\nmapping the predicted token to the class label.\nExperiment Setup We employ ROBERTa-\nlarge (Liu et al., 2020) as the task LM and OPT-\n125M as our policy LM. We experiment with an\nextensive range of few-shot text classification task\nto assess the effectiveness of our approach. The\ndatasets encompass various domains, including\nsentiment classification, such as SST-2 (Socher\net al., 2013), Yelp Polarity (Zhang et al., 2015),\nMR (Pang and Lee, 2005), CR (Hu and Liu, 2004),\nand subjectivity classification like Subj (Pang and\nLee, 2004). Furthermore, we also extend to\ntopic classification, such as AG's News and Ya-\nhoo (Zhang et al., 2015).\nBaselines We compare our algorithm against var-"}, {"title": "5.2 Unsupervised Text Style Transfer", "content": "The text style transfer task (Jin et al., 2022) aims\nto rephrase an input text \u00e6 to match a desired style.\nFor example, in the sentiment transfer task, the\ngoal is to alter a negative sentence \u201cThe movie\nwas disappointing\u201d into its positive counterpart,\ne.g., \"The movie was awesome\". We focus on\nunsupervised text style transfer task, where there\nare no input-output pair examples for training.\nExperiment Setup We employ OPT-125M\nas our policy LM and OPT-125M/350M/\n1.3B (Zhang et al., 2022) as the task LM. We con-\nduct the experiments on Yelp (Shen et al., 2017)\nthat is a widely-used dataset for the sentiment\ntransfer task, focusing on negative-to-positive and\npositive-to-negative sentiment transfer. For the rest\nof the settings, we adhere to the experiment setup\noutlined in Deng et al. (2022).\nBaselines The output is evaluated by a combined\nmetric that measures content preservation as well\nas style alignment, which is given as the scalar\nreward feedback. Assuming that the metric is not\ndifferentiable, prompt tuning methods that directly\nrely on gradients are not applicable. Therefore,\nwe only compare our method against RLPrompt"}, {"title": "5.3 Textual Inversion From Images", "content": "Hard prompts are also useful in the vision-language\ndomains (Wen et al., 2023). A salient task in these\ndomain is textual inversion, which entails identify-\ning the caption that describe target images using"}, {"title": "5.4 Analysis on Hyperparameters", "content": "Length L of Prompts To further evaluate the\neffectiveness of our algorithm relative to PIN-no-\nfluency, we conducted experiments with varying\nlengths of prompts. The results, as illustrated in\nFigure 3(a), reveal that the performance gap be-\ntween our method and PIN-no-fluency widens with\nincreasing prompt length up to L = 25. There\nare two primary reasons for this observed trend.\nFirstly, our algorithm reduces the search action\nspace to a set of familiar tokens. As prompt length\nincreases, the search space expands exponentially;\nthus, our approach to narrowing the action space\nresults in enhanced performance compared to PIN-\nno-fluency. Secondly, PIN-no-fluency attempts to\nestimate the ground- truth Q-value across all to-\nkens at every state. With increasing prompt length,\nthe state space also expands, which can lead to\ninaccuracies in Q-value estimation at each state.\nOur method, by contrast, mitigates this challenge,\nleading to more reliable and effective prompt op-\ntimization, particularly in scenarios with longer\nprompts.\nk For Determining I An important determinant\nof the performance of our algorithm is the number\nof tokens ignored at each state. We conduct an\nanalysis focusing on the hyperparameter k, which\nrepresents the number of tokens considered for Q-\nvalues estimation. The findings are presented in\nFigure 3(b), where the last datapoint, indicating\n50k is the same with PIN-no-fluency. A smaller\naction space (|I| \u2248 V), can exclude tokens that\nare crucial for discovering optimal prompts. Such\nan exclusion risks limiting our algorithm's abil-\nity to identify the most effective prompts as po-\ntentially valuable tokens could be filtered out pre-\nmaturely. Conversely, an excessively large action\nspace (|I| < V) from fewer ignored tokens, dimin-\nished the impact of our filtering technique. In such\ncases, the benefit of reducing search space is lost,\nas the algorithm still needs to evaluate a vast num-\nber of tokens. Our empirical investigations across\nvarious tasks indicate that maintaining the number\nof tokens that are not to be ignored within the range\nof 10000 to 20000 yields the most favorable results,\nparticularly when the coefficient a = 1."}, {"title": "6 Limitations", "content": "PIN can discover prompts that are more inter-\npretable compared to baselines. However, we ac-\nknowledge a couple of inherent limitations. Firstly,\nthe algorithm exhibits a relatively higher time con-\nsumption for training when compared to gradient-\nbased methods. Secondly, despite its advanced ca-\npabilities in discovering interpretable prompts, our\nalgorithm does not guarantee the consistent discov-\nery of grammatically perfect sentences. We would\nleave discovering grammatically perfect prompts\nas future work."}, {"title": "7 Conclusion", "content": "In this paper, we firstly discuss the overdetermined\nissue encounterd in the efficiently parameterized\nnetwork. To address this issue, we propose PIN al-\ngorithm, which uses sparse Tsallis entropy regular-\nization to systematically exclude ignorable tokens\nfrom constraints. Prompts learned by PIN exhibit\nbetter performance in various tasks. Future work\ncould explore alternative strategies for identifying\nignorable tokens to improve interpretability further,\nlike leveraging task-specific domain knowledge."}, {"title": "A Hyperparameter Settings", "content": "We employ 2 MLP layers with 2048 hidden states\nfor the implementation of 4. During the learn-\ning of our PIN method, we sample a batch of 256\nsequences from the replay buffer to update the pa-\nrameters of Q-network. We use an Adam optimizer\nwith learning rate 5e-5. Note that we maintain con-\nsistency with PIN and other RL algorithms in all\nshared hyperparameters, such as the learning rate,\nexcept for the reward scale, which varies across\ntasks. Our experiments mainly follow the setting\nin Deng et al. (2022)."}, {"title": "B Few-Shot Text Classification", "content": "The classification process begin with integrating\nthe input text \u00e6 that needs to be classified and\nthe prompt z into a templated format: '[x] [z]\n[MASK]'. Subsequently, the classification decision\nrelies on selecting the predefined tokens, each rep-\nresenting a specific class, that has the highest prob-\nability of filling the [MASK] position."}, {"title": "B.1 Experiment Setup", "content": "We use OPT-125m (dim(E) = 768, |V| = 50272)\nas our backbone of policy-LM, and prompt length\nL is 5 over all experiments. During training, the\nprompt is learned from a training dataset containing\na few pairs of input text \u00e6 and their corresponding\nlabels c. Additionally, a validation dataset is used\nto assess the prompts during training. The accuracy\nof the predicted labels is evaluated on a test dataset\nusing the prompt that demonstrated the best perfor-\nmance on the validation dataset. For each dataset,\nwe sample 5 distinct sets for training and valida-\ntion, each includes 16 examples per class. We run 3\nexperiments with a different random seed for each\nset and we report the average accuracy. For our\nPIN algorithm, the reward scale (1/a) is 1 and k is\n10000 (|Z| is 40272)."}, {"title": "B.2 Rewards", "content": "The objective of the text classification task is to\naccurately assign input text a to its corresponding\nground truth label c. we employ a piecewise re-\nward function designed to incentivize the correct\nclassification of each example. We use a piecewise\nreward function designed to enhance prompts to\nclassify each example correctly as used in Deng\net al. (2022). For a given prompt z and a training\nexample (x, c), we compute the reward in a manner\nakin to hinge loss. This is achieved by measuring\nthe gap between the probability of the correct la-\nbel and the highest probability among the other\nclasses. we denote PLM(c|z, x) as the probability\nof label c, we can write the gap as Gap(c) :=\nPLM(c|z, x) - maxc\u2260c PLM(c'|z, x). This gap is\npositive when the prediction is correct and negative\notherwise. We define a binary indicator for correct\npredictions as Correct := 1[Gapz (c) > 0]. For\ncorrect predictions, we amplify the positive reward\nby a larger factor to emphasize its desirability. The\nreward function is thus formulated as follows:\n$R(x, c) = A^{1-Correct} \\lambda_1^{Correct} Gap_z (c)$\nand \u03bb\u2081 = 180, \u03bb2 = 200."}, {"title": "B.3 Baselines", "content": "We retrained PEZ (Wen et al., 2023) and RL-\nPrompt (Deng et al., 2022) using the official repos-\nitories34. For PEZ, we used 5 prompt tokens and\nconducted training with a batch size 16 with the\nfew-shot train dataset. For a fair evaluation, we\nutilized the same policy LM for training RLPrompt\nas was used in our method. For RLPrompt, we\nfollowed standard setting (Deng et al., 2022) by set-\nting the reward scale (1/a) to 5 and using top-256\nsampling from the prompt policy during training.\nNote that top-256 sampling is performed based on\nestimated Q-values over tokens. The performance\nresults of the other methods are presented based on\nthe reported in Deng et al. (2022)."}, {"title": "C Unsupervised Text Style Transfer", "content": "We use OPT-125m (dim(E) = 768, |V| = 50272)\nas our backbone of policy-LM, and prompt length\nis 5 over all experiments. The reward scale (1/a)\nis 2 and I is 40271. Our experiments follow the\nsuggested setting of text style transfer task in (Deng\net al., 2022). Dataset Statistics Yelp (Shen et al.,\n2017) contains 266K positive and 177K negative\nreviews for training, 38K and 25K for validation,"}]}