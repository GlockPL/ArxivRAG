{"title": "Hard Prompts Made Interpretable:\nSparse Entropy Regularization for Prompt Tuning with RL", "authors": ["Yunseon Choi", "Chuheng Zhang", "Sangmin Bae", "Lei Song", "Seonghyun Ban", "Minchan Jeong", "Li Zhao", "Jiang Bian", "Kee-Eung Kim"], "abstract": "With the advent of foundation models, prompt\ntuning has positioned itself as an important\ntechnique for directing model behaviors and\neliciting desired responses. Prompt tuning re-\ngards selecting appropriate keywords included\ninto the input, thereby adapting to the down-\nstream task without adjusting or fine-tuning\nthe model parameters. There is a wide range of\nwork in prompt tuning, from approaches that di-\nrectly harness the backpropagated gradient sig-\nnals from the model, to those employing black-\nbox optimization such as reinforcement learn-\ning (RL) methods. Our primary focus is on\nRLPrompt, which aims to find optimal prompt\ntokens leveraging soft Q-learning. While the\nresults show promise, we have observed that\nthe prompts frequently appear unnatural, which\nimpedes their interpretability. We address this\nlimitation by using sparse Tsallis entropy reg-\nularization, a principled approach to filtering\nout unlikely tokens from consideration. We\nextensively evaluate our approach across vari-\nous tasks, including few-shot text classification,\nunsupervised text style transfer, and textual in-\nversion from images. The results indicate a\nnotable improvement over baselines, highlight-\ning the efficacy of our approach in addressing\nthe challenges of prompt tuning. Moreover,\nwe show that the prompts discovered using our\nmethod are more natural and interpretable com-\npared to those from other baselines (Deng et al.,\n2022)", "sections": [{"title": "1 Introduction", "content": "While the use of large-scale language models\n(LMs) and vision-language models (VLMs), pre-\ntrained on a massive amount of data, is becoming a\ndominant paradigm in machine learning (Rombach\net al., 2022; Touvron et al., 2023; Radford et al.,\n2021), fine-tuning the model parameters for adap-\ntation to downstream tasks requires a vast amount\nof computational time and resources. In this sense,\nprompt tuning has emerged as a promising low-cost\nsolution (Brown et al., 2020; Lester et al., 2021),\ndiscovering input prompts that effectively guide\nthe pre-trained models to generate the desired out-\nputs, while keeping the model parameters frozen.\nPrompt tuning is generally categorized into two ap-\nproaches, soft and hard prompting methods, based\non their representation of prompts.\nSoft prompt methods (Lester et al., 2021; Li and\nLiang, 2021) primarily focus on learning continu-\nous embedding vectors at the token level, which\nare called as soft prompts. They usually perform\nthe gradient descent to optimize these continuous\nembedding vectors (Wu et al., 2023; Zhu et al.,\n2023). However, the prompts learned through soft\ntuning are opaque to human interpretation and are\nnot compatible with other pre-trained models that\ndo not share the same embedding spaces. More-\nover, computing internal gradients for models is\nhighly resource-intensive, especially as the num-\nber of model parameters increases, or even infea-\nsible in cases where models are accessible only\nthrough APIs (OpenAI, 2023). These limitations\nnecessitate an alternative approach: discovery of\nthe prompts composed of human-readable discrete\ntokens, referred to as hard prompts (Prasad et al.,\n2023; Shin et al., 2020; Deng et al., 2022).\nHard prompts offer numerous advantages over\nsoft prompts: they are transferable from one pre-trained model to another since they are agnostic to\nthe embedding. Moreover, they confer composi-\ntional control by facilitating manual merging and\nmodification (Wen et al., 2023). Despite the bene-\nfits, they require large-scale discrete optimization\nin principle. Approaches for finding optimal hard\nprompts often employ backpropagated gradients\nfrom models as with soft prompt methods, while\ncircumventing the discrete optimization by, roughly\nspeaking, mapping the soft prompt to the similar\ndiscrete tokens in the embedding space (Wen et al.,"}, {"title": "2 Related Works", "content": "Prompting in Language Models Pioneering\nwork by Brown et al. (2020) highlighted the ef-\nficacy of using prompts for task adaptation in pre-\ntrained language models, a technique now com-\nmonly referred to as instruction tuning. This\napproach has become standard in enhancing the\nability of large models to execute complex, task-specific instructions. Despite its success, the auto-\nmated generation of effective text prompts, particu-\nlarly hard prompts, remains a challenge. The work\nby Lester et al. (2021) to simplify prefix tuning\nled to the establishment of standard soft prompt\ntuning, which optimizes continuous embeddings\nthat are appended to embeddings of input tokens.\nHowever, Khashabi et al. (2022) pinpointed a lim-\nitation of this approach: the resulting embedding\nsequences often lack clear semantic interpretation.\nTo address these limitations, our work focuses on\nthe hard prompts optimization within a selected\nset of tokens, thereby generating task-specific and\ninterpretable tokens.\nDiscrete Optimization for Hard Prompts Au-\ntoPrompt (Shin et al., 2020) is an initial framework\nfor discrete prompt optimization in transformer-based language models, inspiring a range of diverse\nmethods. These include a gradient-free phrase\nediting method (Prasad et al., 2023), a reinforce-\nment learning-based approach (Deng et al., 2022),\nand an embedding optimization approach based\non Langevin dynamics (Shi et al., 2023). In our\nwork, we first benchmark gradient-based methods\nlike AutoPrompt (Shin et al., 2020) and PEZ (Wen\net al., 2023) for hard prompt tuning. AutoPrompt\nemploys the HotFlip algorithm (Ebrahimi et al.,\n2018) to greedily identifies optimal tokens for each\nposition based on model gradients, while PEZ per-\nforms gradient re-projection during its continuous\noptimization within the embedding space. How-\never, both methods require substantial computa-\ntional cost in calculating gradients and are unsuit-\nable for black-box models. On the other hand, RL-Prompt (Deng et al., 2022) employs a gradient-free\nRL-based method, serving as our primary baseline.\nRLPrompt introduces an efficiently parameterized\nnetwork that maps continuous embedding vectors\nto adaptive vectors within the same space. Despite\nits simplicity, RLPrompt struggles with accurately\nrepresenting Q-values across all tokens, potentially\nleading to sub-optimal prompts as we discuss in\nSection 4.1."}, {"title": "3 Preliminaries", "content": "Hard Prompt Tuning with RL Hard prompt tun-\ning is the process of discovering an optimal prompt\nwithin the token space V, to efficiently tackle spe-\ncific downstream tasks. Assuming a fixed-length\nprompt of L tokens, this optimization can be for-\nmally defined as RL problem:\n\\begin{equation}\n\\max_{z\\in V^L} R(y(z, x)).\n\\end{equation}\nHere, the objective is to find a discrete prompt\nz from the solution space of length-L token se-\nquences $V^L$, which maximizes a task-specific re-\nward function R when concatenated with input x."}, {"title": "5 Experiments", "content": "5.1\nFew-Shot Text Classification\nThe goal of these tasks is to find the optimal prompt\nthat assigns input text \u00e6 to the class label, given\na few examples in the context. We employ the\nsame experimental setting in Schick and Sch\u00fctze\n(2021); Deng et al. (2022), which addresses the\nclassification tasks via prompting the task LM and\nmapping the predicted token to the class label.\nExperiment Setup We employ ROBERTa-\nlarge (Liu et al., 2020) as the task LM and OPT-125M as our policy LM. We experiment with\nan extensive range of few-shot text classification task\nto assess the effectiveness of our approach. The\ndatasets encompass various domains, including\nsentiment classification, such as SST-2 (Socher\net al., 2013), Yelp Polarity (Zhang et al., 2015),\nMR (Pang and Lee, 2005), CR (Hu and Liu, 2004),\nand subjectivity classification like Subj (Pang and\nLee, 2004). Furthermore, we also extend to\ntopic classification, such as AG's News and Ya-\nhoo (Zhang et al., 2015).\nBaselines We compare our algorithm against var-\nious baselines, including heuristic methods such"}, {"title": "5.2 Unsupervised Text Style Transfer", "content": "The text style transfer task (Jin et al., 2022) aims\nto rephrase an input text \u00e6 to match a desired style.\nFor example, in the sentiment transfer task, the\ngoal is to alter a negative sentence \u201cThe movie\nwas disappointing\u201d into its positive counterpart,\ne.g., \"The movie was awesome\". We focus on\nunsupervised text style transfer task, where there\nare no input-output pair examples for training.\nExperiment Setup We employ OPT-125M\nas our policy LM and OPT-125M/350M/\n1.3B (Zhang et al., 2022) as the task LM. We con-\nduct the experiments on Yelp (Shen et al., 2017)\nthat is a widely-used dataset for the sentiment\ntransfer task, focusing on negative-to-positive and\npositive-to-negative sentiment transfer. For the rest\nof the settings, we adhere to the experiment setup\noutlined in Deng et al. (2022).\nBaselines The output is evaluated by a combined\nmetric that measures content preservation as well\nas style alignment, which is given as the scalar\nreward feedback. Assuming that the metric is not\ndifferentiable, prompt tuning methods that directly\nrely on gradients are not applicable. Therefore,\nwe only compare our method against RLPrompt"}, {"title": "5.3 Textual Inversion From Images", "content": "Hard prompts are also useful in the vision-language\ndomains (Wen et al., 2023). A salient task in these\ndomain is textual inversion, which entails identify-\ning the caption that describe target images using"}, {"title": "4 Method", "content": "4.1 Overdetermined Linear Systems\nWhile the Q-network utilized by RLPrompt pro-vides efficient parameterization, it also possesses\na fundamental limitation. Training the Q-network\nis essentially solving for an extremely overdeter-mined linear system, where approximation error is\ninevitable. In order to observe this, we fix $z_{0:t-1}$\nand rewrite Eq. (3) as the weighted least-squares\nproblem\n\\begin{equation}\nmin_\\psi || W_{LM} \\psi - q ||^2,\n\\end{equation}\nwhere $\\psi = \\psi_\\theta(LM(z_{0:t-1}))$ is the optimization\nvariable, and $q = Q_\\theta(z_{0:t-1}, \\cdot)$ is the right-hand-side vector. The coefficient matrix, which is the\nLM-head matrix from the policy LM, $W_{LM} \\in\nR^{|V| \\times dim(\\mathcal{E})}$ has many more rows than columns\nsince it is common that $|V| \\gg dim(\\mathcal{E})$ (e.g.,\n$|V| = 50272$ and $dim(\\mathcal{E}) = 768$ for OPT-125M),\ncorresponding to many more constraints than vari-\nables resulting in inevitable approximation error.\nTogether with the probabilities of tokens as\nweights in the least-squares formulation, the is-\nsue of approximation error becomes more critical.\nTokens with high probabilities, as estimated by $\\pi$,\nreceive larger weights in the least-squares, leading\nto smaller approximation errors for them. On the\nother hand, the vast number of low-probability to-\nkens are assigned with smaller weights, resulting\nin relatively larger approximation errors. Conse-\nquently, the estimated action value could become\nunreasonably high for these low-probability tokens,\npromoting the RL algorithm to excessively try out\nthese improbable tokens. This results in an RL ap-\nproach that is overly biased towards exploration,\nspecifically favoring the selection of insignificant\nlow-probability tokens. The detrimental impact of\nthis unfortunate behavior is clearly observed by the\nunnatural prompts chosen by RLPrompt."}, {"title": "4.2 PIN (Prompts made INterpretable)", "content": "One of the straightforward solutions to mitigate\nthe challenge of the overdetermined linear system\nis to drop the constraints that are less important.\nTo achieve this, we introduce the ignorable token\nset comprised of tokens deemed improbable from\na general language model. The key concept is to\nsidestep evaluating the action values of these to-kens, as they may adversely impact the value esti-mations of high-probability tokens. The construc-tion of the ignorable token set entails utilizing the\nlogit of the predictive probability of tokens derived\nfrom the policy LM with the original embedding,\nand choosing those that score below the k-th largest\nlogit. Formally, given the prompt prefix $z_{0:t-1}$, the\nignorable token set is defined by\n\\begin{equation}\n\\mathcal{I}_{z_{0:t-1}} = \\{z \\in V | W_{i_z}e_t < W_{i_{(k)}}e_t\\},\n\\end{equation}\nwhere $e_t$ is the embedding vector of $z_{0:t-1}$ obtained\nfrom the policy LM, $W_i$ is the i-th row vector of its\nLM-head matrix, and $i_{(k)}$ is the index of the token\nwith k-th largest logit.\nDeciding the number of k is important for learn-ing effective hard prompts, as demonstrated by our\nexperimental results. If we set k aggressively (i.e.,\nsmall) to avoid constraint violation, we may end\nup ignoring strong candidate tokens. On the other\nhand, if we set k conservatively (i.e., large), we suf-fer from the original challenge of approximation\nerror. Therefore, we empirically chose $k = 10000$\nto ensure a sufficiently diverse set of tokens, disre-garding about 80% of the vocabulary tokens. Nev-ertheless, we observed that the ignorable token set\ndose not work well alone because k is significantly\nlarger than $dim(\\mathcal{E})$, which is 768, even in case of\nOPT-125M.\nNow, the remaining challenge is to address the\napproximation error still existent due to more con-straints than variables for training the Q-network.\nIf we adopt SQL as the baseline RL method, we\nwill end up with the same undesirable behavior due\nto the dense probability over the tokens. We thus in-stead employ the sparse Tsallis entropy regularized\nQ-learning described in the previous section, which\nyields a sparse policy that naturally suppresses the\nprobability of choosing many unimportant tokens\nwith errors in the action value estimation.\nOur algorithm, PIN (Prompts made INter-pretable), is presented in Algorithm 1. We remark\nthat the algorithm employs operator $\\mathcal{F}_1$ that sys-tematically filters out the action values of ignorable"}, {"title": "5.4 Analysis on Hyperparameters", "content": "Length L of Prompts To further evaluate the\neffectiveness of our algorithm relative to PIN-no-fluency, we conducted experiments with varying\nlengths of prompts. The results, as illustrated in\nFigure 3(a), reveal that the performance gap be-tween our method and PIN-no-fluency widens with\nincreasing prompt length up to L = 25. There\nare two primary reasons for this observed trend.\nFirstly, our algorithm reduces the search action\nspace to a set of familiar tokens. As prompt length\nincreases, the search space expands exponentially;\nthus, our approach to narrowing the action space\nresults in enhanced performance compared to PIN-no-fluency. Secondly, PIN-no-fluency attempts to\nestimate the ground- truth Q-value across all to-kens at every state. With increasing prompt length,\nthe state space also expands, which can lead to\ninaccuracies in Q-value estimation at each state.\nOur method, by contrast, mitigates this challenge,\nleading to more reliable and effective prompt op-timization, particularly in scenarios with longer\nprompts.\nFor prompts of length L = 25, PIN achieves the\nhighest Clip Score. However, for L = 26, 27, the\nscore drops. We remark that longer prompts pose\nthe challenge of combinatorial search space of hard\nprompts for RL-based methods, and we believe that\nwe need more information-rich feedback other than\njust reward signals for making the methods more\neffective. This would be nontrivial for our setting\nwhere the LM is assumed to be a black-box model,\nbut it is a promising direction for future work.\nk For Determining $\\mathcal{I}$ An important determinant\nof the performance of our algorithm is the number\nof tokens ignored at each state. We conduct an\nanalysis focusing on the hyperparameter k, which\nrepresents the number of tokens considered for Q-values estimation. The findings are presented in\nFigure 3(b), where the last datapoint, indicating\n50k is the same with PIN-no-fluency. A smaller\naction space ($|\\mathcal{I}| \\approx V$), can exclude tokens that"}, {"title": "6 Limitations", "content": "PIN can discover prompts that are more inter-pretable compared to baselines. However, we ac-knowledge a couple of inherent limitations. Firstly,\nthe algorithm exhibits a relatively higher time con-sumption for training when compared to gradient-based methods. Secondly, despite its advanced ca-pabilities in discovering interpretable prompts, our\nalgorithm does not guarantee the consistent discov-ery of grammatically perfect sentences. We would\nleave discovering grammatically perfect prompts\nas future work."}, {"title": "7 Conclusion", "content": "In this paper, we firstly discuss the overdetermined\nissue encounterd in the efficiently parameterized\nnetwork. To address this issue, we propose PIN al-gorithm, which uses sparse Tsallis entropy regular-\nization to systematically exclude ignorable tokens\nfrom constraints. Prompts learned by PIN exhibit\nbetter performance in various tasks. Future work\ncould explore alternative strategies for identifying\nignorable tokens to improve interpretability further,\nlike leveraging task-specific domain knowledge."}]}