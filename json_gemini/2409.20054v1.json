{"title": "Evaluating and explaining training strategies for zero-shot cross-lingual news sentiment analysis", "authors": ["Luka Andren\u0161ek", "Boshko Koloski", "Andra\u017e Pelicon", "Nada Lavra\u010d", "Senja Pollak", "Matthew Purver"], "abstract": "We investigate zero-shot cross-lingual news sentiment detection, aiming to develop robust sentiment\nclassifiers that can be deployed across multiple languages without target-language training data. We\nintroduce novel evaluation datasets in several less-resourced languages, and experiment with a range of\napproaches including the use of machine translation; in-context learning with large language models; and\nvarious intermediate training regimes including a novel task objective, POA, that leverages paragraph-level\ninformation. Our results demonstrate significant improvements over the state of the art, with in-context\nlearning generally giving the best performance, but with the novel POA approach giving a competitive\nalternative with much lower computational overhead. We also show that language similarity is not in itself\nsufficient for predicting the success of cross-lingual transfer, but that similarity in semantic content and\nstructure can be equally important.", "sections": [{"title": "INTRODUCTION", "content": "Sentiment analysis (SA) \u2014 identifying the sentiment of textual documents \u2013 is one of the most popular\nnatural language processing (NLP) tasks Goddard (2011). SA can be formulated to analyze the sentiment\nfrom the readers' perspective, the sentiment of the writer towards particular subject and so on; has been\napplied to a variety of data sources, including social media posts Yue et al. (2019), news articles Pelicon\net al. (2020b), reviews Fang and Zhan (2015) and similar; and can be performed at the sentence, the\nparagraph or the document level Medhat et al. (2014). The methods can be either monolingual (training\nand testing on the same language) or cross-lingual (training on a source language and testing on a different\ntarget language), but in either case it is generally approached as a supervised learning problem; this\nimposes a need for labeled data, seen as one of the main obstacles in developing robust systems for NLP,\nand especially difficult with low-resource languages. For this reason, research has been focused lately on\nmodels that can work cross-lingually and preferably in a zero-shot setting (requiring no training in the\ntarget language). We are especially interested in news sentiment analysis, and in building cross-lingual\nmodels, allowing for SA of news document collections in several languages.\nWe recognise several variants of cross-lingual SA, based on the availability of target language data:\nzero-shot (when no target data is available for training), few-shot (where only about a dozen instances\nare available) and full-shot (where larger amounts of data are available). This paper focuses on the\nzero-shot cross-lingual setting, where models trained on available data in one language are applied to\npredict sentiment in other language without any additional training on the target language. By using\nthis approach, we can extend the benefits of advanced NLP techniques to a broader range of languages,\npromoting linguistic inclusivity and enhancing the understanding of sentiment across diverse linguistic"}, {"title": "1 RELATED WORK", "content": "While early approaches towards sentiment analysis were based on handcrafted sentiment lexicons like\nVADER Hutto and Gilbert (2014), most recent work uses machine learning due to its higher accuracy,\nrobustness and ability to transfer to new domains and languages without needing to create new resources.\nIn this section, we give an overview of the commonly used traditional machine learning approaches, deep\nlearning architectures and finally the more recent pre-trained large language model based approaches."}, {"title": "1.1 Machine learning", "content": "Machine learning-based approaches Zhang and Zheng (2016) for SA are often based on two constructs.\nThe first consists of featurizing the input text, usually constructing interpretable features from the bag-of-\nwords family: word and/or character n-gram counts, lexical features, sentiment lexicon-based features,\nparts of speech, or adjectives and adverbs, often weighting these via e.g. term frequency-inverse document\nfrequency Sammut and Webb (2010). The second part is based on learning a classifier on top of these\nfeatures. Mehra et al. (2002) adopted a maximum entropy classifier, while Gowda et al. (2022) utilised\na Naive Bayes classifier. A support vector machine (SVM) classifier for SA on tweets was explored\nby Firmino Alves et al. (2014), based on linear classifiers such as SVMs and logistic regression Pang\net al. (2002). These methods have established a robust baseline for the field. However, they suffer from\nsignificant drawbacks. First, they are based on counting and ignore any deep semantics or word order."}, {"title": "1.2 Deep learning approaches", "content": "The deep learning approaches are based on the idea that both the representation and the classification\nfunctions can be learned end-to-end in a differentiable fashion. Glorot et al. (2011) proposed a deep\nlearning-based approach towards sentiment analysis across different domains. The core foundation of\nthis work was that representations for the words and documents can be learned from the data in an\nunsupervised fashion, and then a classifier can exploit these semantically rich features. In a similar vein,\nSocher et al. (2011) proposed learning recursive autoencoders in a semi-supervised setting for SA, which\noutperformed the traditional machine learning approach. Variational autoencoders based on Long-Short\nTerm Memory (LSTM) networks Wu et al. (2019) have also been proposed to tackle this task successfully.\nThe shift towards deep learning for SA is marked by the wide adoption of convolutional neural networks\n(CNNs) and recurrent neural networks (RNNs). These models benefit from the deep architectures' ability\nto learn hierarchical representations of text data, significantly enhancing nuance and accuracy. Efforts\nto improve both monolingual and cross-lingual sentiment analysis have led to various flavours of RNNS\nand CNNs being explored Dong and De Melo (2018); Abdalla and Hirst (2017); Ghasemi et al. (2022);\nBaliyan et al. (2021); Kanclerz et al. (2020)."}, {"title": "1.3 Pretrained models", "content": "The use of pre-trained Transformer models, such as BERT (Bidirectional Encoder Representations from\nTransformers) and its multilingual derivatives, mBERT Devlin et al. (2019) and XLMR Conneau et al.\n(2020), has set new standards in text classification in both monolingual and cross-lingual settings. Despite\ntheir widespread adoption, these models face limitations Peng et al. (2024) like fixed input lengths and a\npotential lack of sensitivity to sentiment nuances due to their pre-training focus on linguistic structures\nrather than emotional content. To address the structural lack of emotional content, Yin et al. (2020)\nproposed the SentiBERT model, capable of capturing semantic composition by synergising contextualised\nrepresentations with binary constituency parse trees. In another study, Ke et al. (2020) enhanced pre-\ntrained transformer representations with word-level linguistic knowledge, such as part-of-speech tags and\nsentiment polarity, via label-aware masked-language modelling, further improving the representations.\nAlthough these improvements in pre-trained representations are significant, no general approach exists\nfor multilingual sentiment detection. Pelicon et al. (2020b) investigated how training a multilingual model\nto analyse sentiment in one language could be leveraged for zero-shot applications such as news sentiment\nanalysis. Similarly, Robnik-Sikonja et al. (2021) explored cross-lingual transfer. The introduction of cross-\nlingual pre-training Conneau and Lample (2019) enabled the creation of powerful multilingual models\ncapable of cross-lingual transfer. Models pre-trained with multilingual masked-language modelling\nConneau et al. (2020) exhibit strong cross-lingual understanding due to the compositional nature of\nlanguage, as demonstrated by Chai et al. (2022).\nCore challenges in the cross-lingual sentiment detection field are surveyed in Xu et al. (2022), which\nextends to maintaining sentiment accuracy across languages without extensive labelled data in each target\nlanguage. Early strategies often involved direct translation of text data followed by sentiment analysis\nusing a model trained in the source language Banea et al. (2008). The rise of neural machine translation\nhas facilitated more sophisticated approaches, such as training joint bilingual sentiment models that\nleverage shared representations across languages Chen and Cardie (2018). Various methodologies have\nbeen developed to leverage machine translation and transfer learning techniques to overcome language\nbarriers in NLP tasks.\nAmong these, the T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification\nintroduced by Unanue et al. is a notable contribution Unanue et al. (2023). T3L capitalises on a translate-\nand-test strategy that simplifies adapting models for multiple languages by enabling end-to-end fine-tuning\nwith soft parameter sharing. While their approach introduced heavier memory restrictions and slower\ninference speeds, it achieved competitive text classification results. Another significant approach is by\nSiddhant et al., who evaluated the cross-lingual effectiveness of massively multilingual neural machine\ntranslation Siddhant et al. (2020), which included 103 languages. Their findings underscore the potential"}, {"title": "2 DATA", "content": "In this Section 2.1, we provide preliminary statistics for the datasets used. The process of constructing\nthese datasets is detailed in Section 2.2. Upon acceptance, we will publish the new datasets."}, {"title": "2.1 Data description", "content": "We work with seven datasets, each containing general news articles in a different language. Each document\nis labelled as negative, neutral or positive. The languages, with document counts, are as follows: Slovenian\n(10,427 documents), Croatian (2,025), Bosnian (200), Macedonian (198), Estonian (100), Serbian (200),\nand Albanian (200); see Table 1."}, {"title": "2.2 Dataset creation", "content": "The Slovenian dataset was collected by Bu\u010dar et al. (2018) and results from an extensive annotation cam-\npaign by several annotators. Each document was annotated either as positive, neutral or negative from the\nreaders' perspective. The annotators were asked to answer, \u201cDid this news evoke positive/neutral/negative\nfeelings?\". Six annotators annotated the dataset; between 2 and 6 annotators annotated each article.\nThe articles were annotated using a five-level Likert scale (1-very negative, 2-negative, 3-neutral,\n4-positive, and 5-very positive). The final sentiment of an instance was defined as the average of the\nsentiment scores given by the different annotators. An instance was labelled as negative if the average of\ngiven scores was less than or equal to 2.4; neutral if the average was between 2.4 and 3.6; or positive if the\naverage of given scores was greater than or equal to 3.6. The inter-annotator agreement on the document\""}, {"title": "3 METHODOLOGY", "content": "In Section 1, we discussed that one can perform zero-shot cross-lingual sentiment classification in multiple\nways. Namely, one can do zero-shot cross-lingual classification by employing a pre-trained multilingual\nlanguage model; we use this method as a baseline, details in Section 3.2. Pelicon et al. (2020b) showed\nthat learning paragraph level sentiment-detection before learning document-level classification, as an\nintermediate task, enhanced the results and offered better classification. Motivated by this, we introduce\na novel intermediate-training task (see Section 3.3). On a parallel note, one of the established ways to\nsolve zero-shot cross-lingual classification consisted of translating the document from the target language\n(e.g. Slovenian) to the source language (usually English) and performing the classification in the source\nlanguage. In our experiments, therefore, we use XLMR Conneau et al. (2020) as a baseline model,\nin combination with different intermediate training strategies (including our new method) and with or\nwithout translation, and compare these methods to a strong Large Language Model, based on in-context\nlearning with Mistral7B Jiang et al. (2023)."}, {"title": "3.1 Formulations", "content": "Let X denote a set of documents which we learn to classify into three classes of sentiment y \u2208 {positive, neutral, negative}\nThroughout our experiments, we will use Slovenian as the source language on which we will learn, so we\nwill use X and XSlovenian interchangeably. Formally, we aim to learn a function f that maps the texts in X\nto the correct classes y. We characterize the function f with parameters a neural network. Following\nthe deep learning paradigm LeCun et al. (2015), we seek to learn the function f(0,X) such that we\nminimize the error L(0,X,y). In our case, we use XLM-Roberta as the base parameterization Obase of\nthis function. We define the loss function L(0, X, y) as categorical cross-entropy, given by:\nLsentiment (0,x,y) = \u2013 \u03a3\u03a3 Vic log(yi,c),\ni=0c=0\nwhere N is the number of samples, C is the number of classes, yi,c is a binary indicator (0 or 1) if class\nlabel c is the correct classification for sample i, and \u0177ic is the predicted probability that sample i belongs\nto class c. To learn the parameters 0, we aim to minimize the loss function L(0,X,y). This optimization\nproblem can be expressed as:\n0* = arg min L(0, X,y),\n\u03b8\nwhere * denotes the parameters after an optimizer of choice minimizes the loss, updating the initial 0. We\ndenote this process as fine-tuning the base XLMR model to the task of document-level sentiment prediction.\nSince we are interested in the zero-shot cross-lingual setting, our goal is to obtain a representation of 0*,\nsuch that we will be able to successfully classify on new X', to get predictions in that target language."}, {"title": "3.2 Base model-XLMR", "content": "This approach consists of fine-tuning a model on a dataset in a particular language (in our case Slovenian)\nand then using this model to predict sentiment on a different dataset, either on a test set in the same\nlanguage (monolingual evaluation) or a or on a dataset in a different language in zero-shot cross-lingual\nsetting. For this reason, we select the XLMR Conneau et al. (2020) model as a baseline, as it supports all\nof the languages in our dataset, enabling efficient transfer in a cross-lingual setting. In this scenario, we\nutilise the above defined approach of optimising the weights to solve the sentiment classification, where\nwe adapt the XLMR embeddings to the given task. XLMR's input is limited to 512 tokens; we follow the\noriginal approach on news sentiment classification by Pelicon et al. (2020b) for articles exceeding 512\ntokens, retaining only the first 256 tokens and last 256 tokens and concatenating them. In this case we\nlearn in Slovenian: 0* = argmine L(0, X, y) and use the learned weights to infer predictions in all other\nlanguages. This model, without any intermediate training (see methods below), is denoted in our tables 2\nand 3 as XLMR (no int. training)."}, {"title": "3.3 Intermediate training strategies", "content": "Intermediate training involves adapting the initial XLMR (Obase) weights on a given intermediate task,\nwhich are subsequently used to solve the main task-in our case, learning to classify the sentiment\nLsentiment. In this work, we focus on learning to solve paragraph-level sentiment prediction as the\nintermediate task. Following the methodology introduced in Pelicon et al. (2020b), we denote the\nparagraph dataset as P and their corresponding labels yp. We explore two different paragraph-level\nintermediate training paradigms: PSE proposed by Pelicon et al. (2020a) and a novel proposed method\nPOA."}, {"title": "3.3.1 PSE - Paragraph Sentiment Enrichment", "content": "We denote the method proposed by Pelicon et al. (2020a) as Paragraph Sentiment Enrichment (PSE). The\nidea is based on training the base weights of a model on the task of paragraph-level sentiment prediction\nand then transferring this knowledge to document-level prediction. Given that for each paragraph p we\nhave its corresponding label yp, the approach suggests learning to classify the paragraph-level sentiment\nas the first objective, denoted as Pparagraph-sentiment. Alongside this, the authors propose adapting to the\nlanguage via self-supervised masked language modeling of the paragraphs LMLM.\nThe loss for this intermediate sentiment learning is given by:\nLPSE(0, P, yp) =\nLparagraph-sentiment\nParagraph sentiment prediction loss\n+\nLMLM\nMasked language modeling loss\nwhere\nLparagraph-sentiment(0,P,yp) = \u2211\u2211ypp,clog(ypp,c),\nC\npePc=1\nis the task of predicting the sentiment at the paragraph level, and\nM\nLMLM(0, P, P*) = \u2013 \u03a3\u03a3 log P(pk|p*; 0),\npePk=1\nis the self-supervised task of masked language modeling, where we task the model to reconstruct the\ninput (in our case the paragraphs) after self-corruption. Here, N is the number of paragraphs, yp; is the\nsentiment label for paragraph pi, pi is the input paragraph, C is the number of sentiment classes, M is\nthe number of masked tokens in each paragraph, pj,k is the original token in paragraph j, and p is the\nmasked input tokens in paragraph j.\nOnce we have optimized this, the initial XLM-R weights (base) are transformed to the set of intermedi-\nate weights OPSE, which we later use to solve the main objective by fitting the model to the document-level\nsentiment prediction:\nOPSE* = arg min Sentiment (OPSE, X, y).\nOPSE"}, {"title": "3.3.2 POA - Part Of Article", "content": "Next, we present our novel approach to the intermediate training \u2013 Part Of Article (POA). As the name\ndenotes, we aim to learn where a paragraph is positioned within a document as an additional task.\nThe approach modifies the paragraph-level Slovenian dataset by adding an extra label indicating the\nparagraph's relative position within its article (document). First, we annotate each paragraph of the dataset\nwith its own unique identifier (ID) as a position of the paragraph in the document from which it originated.\nFor instance, the first paragraph contained an ID of 0, the second one received an ID of 1, and so on. We\nnow propose calculating the POA label as:\nPOA(paragraph) = 3[\nID(paragraph)\nL(D) \u2013 1\n] (1)\nwhere paragraph is in the article (document) D, L(D) denotes the number of paragraphs in the arti-\ncle(document) D, and [x] denotes the most extensive whole number, which is less than or equal to x.\nHowever, we make an exception for the final paragraph in a document, assigning a POA label of 2.\nIn summary, each paragraph will be labelled based on its position in the article's beginning, in-\ntermediate or last third. The intermediate training step of this approach extends the PSE approach of\nmasked language modelling and paragraph sentiment prediction by introducing the task of predicting\nthe paragraph's relative positional information. The additional information provided, combined with\nsentiment and the paragraph's position, provides additional information on news sentiment to the model.\nFor example, introductions to news articles typically show more sentiment information than the part\ndescribing the backstory of some event. This way, the model could consider these factors and output a\nmore robust sentiment classifier. In addition, POA does not require any additional manual labelling. This\nenables us to apply it to practically any long-form sentiment analysis approach by splitting our document\ninto smaller fragments, enumerating them, and then computing their POA with the above-mentioned\nformula.\nHaving defined POA, we next define the POA loss, which consists of the prediction of the correct\nPOA for each paragraph. Formally, for each paragraph \u2208 P, we end up with a class POA(paragraph).\nOne can quickly realize that this is a classification task that we will introduce to our model, and we end\nup with a loss:\nN C\n\u03a7\u03a1\u039f\u0391 (\u0398, P, POA) = -log(POA),\ni=1 c=1\ni.c\nwhere yPOA is the true label for the POA classification and \u0177POA is the predicted probability for the POA\nclassification.\nFinally, the final loss of this novel intermediate training task is denoted as:\nLparagraph-sentiment + LMLM+\nPSE loss (Section 3.2)\nLPOA\nnovel Part Of Article loss\nAfter optimizing this loss, the base @base weights converge to \u03b8POA*, which we utilize to initialize\nthe the main task of document-level sentiment classification. We show the diagram of fine-tuning based\napproaches in Figure 1."}, {"title": "3.4 Translation strategies \u2013 from source to English", "content": "While multilingual models, such as XLMR, support a large number of languages, their performance\nmight be lower for less-resourced languages compared to the dominant languages Bapna and Firat (2019);\nPfeiffer et al. (2020); Winata et al. (2022). The main reason for poor performance is the model's inability\nto equally represent all languages in its vocabulary and representation space. Translating the data from\nless-resourced language (like Macedonian or Albanian for example) to well-resourced one like English,\ncan benefit classification tasks Unanue et al. (2023). To assess the benefits from translation, we leverage\nthe recently proposed No Language Left Behind model (NLLB) Team et al. (2022) and compare the\nperformance of the cross-lingual transfer when using the original languages, compared to using all the\ndata in English. We consider evaluating the base and the two intermediate training strategies, both with\nand without translation. The setting where we train the XLMR model with translation is denoted as\nTranslation Yes or No."}, {"title": "3.5 In-context learning", "content": "The in-context learning (ICL) approach Dong et al. (2023) capitalizes on a large language model (LLM)'s\ncapacity for analogical reasoning. Distinct from many machine learning paradigms, ICL does not involve\na training phase where the model's parameters are updated through gradient back-propagation. Instead,\nthe pretrained model's weights are frozen, and a straightforward protocol is employed to elicit the model's\noutput. This process involves gathering data in its natural language form and restructuring it into a\nstandardized template. For example, the template for a single article example might be structured as"}, {"title": "4 EXPERIMENTAL SETTING", "content": "We are interested in evaluating the zero-shot transfer capabilities of the base approach (Section 3.2),\nas well as the intermediate-training PSE and novel POA approaches (Section 3.3.1 and Section 3.3.2)\nalongside the in-context learning on translated and non-translated data. The main research questions we\naddress are:\n\u2022 RQ1: How does the novel intermediate training POA approach perform in monolingual and\nzero-shot cross-lingual settings for document-level sentiment classification?\n\u2022 RQ2: What is the best method for cross-lingual sentiment classification in the era of in-context\nlearning?\n\u2022 RQ3: How does translating from a less-resourced (e.g. Slovenian) to high resourced language\n(English) affect the cross-lingual transfer performance across different methods?\n\u2022 RQ4: How can we explain potential discrepancies between source and target languages, with\noptimal-transport dataset distance and topic modeling?\nIn all settings we start from Slovenian as source language, and we transfer to Croatian, Bosnian,\nEstonian, Macedonian, Albanian and Serbian. We also experiment with monolingual (i.e. Slovenian)\nevaluation, to help investigate which strategy is optimal for training when auxiliary paragraph-level data"}, {"title": "4.1 Data splits", "content": "The intermediate training used the same training, validation and test splits as the base model approach."}, {"title": "4.1.1 Splitting the Slovenian dataset", "content": "The procedure for splitting the data in the Basic approach and Basic + Translation approach is the standard\ntrain, validation and test split. We employ 10 fold validation, so we hold out 10% of the data for testing\npurposes, while splitting the remaining 90% of the data into a train and validation split (80% and 20%).\nIn the approaches with intermediate training, namely PSE and POA, we are considering paragraph-\nlevel information. In these settings, we keep the test set (used for document level evaluation in the Basic\napproach) untouched, while paragraph-level pretraining uses 10-fold cross-validation on the training\ncorpus. This ensures that paragraphs used in the final evaluation step are excluded from pre-training, to\nprevent data leakage.\nIn the zero-shot in-context learning (ICL) approach, we infer the results for the entire dataset, but\nreport the F1-score results across 10 folds (as with the trained models) allowing to compute also standard\ndeviation.\nWe exclude the examples that appear in the prompt from the evaluation set. Because the number of\nthese examples (9 in total in the three-shot setting on a large Slovenian dataset of 10 427 articles) is so\nsmall in comparison to the dataset size, results are still comparable across methods."}, {"title": "4.1.2 Splitting other datasets", "content": "For other languages than Slovenian, no splitting was needed as we evaluated the models in a zero-shot\nsetting. We did however run 10 iterations in the basic, PSA and POA approaches. This is because in all of\nthese methods, the models were trained on different data splits because of the 10-fold cross validation\napproach. We report the average of these results for each approach respectively. In the ICL approach\nno splitting of data was needed. In the one-shot and three-shot ICL setting, no risk of data leakage was\npresent as we used examples from the Slovenian dataset."}, {"title": "4.2 Evaluation metrics", "content": "In all cases, we report the F1 score and the standard deviation respective of each setting."}, {"title": "4.3 Experiment specifications", "content": "Finding the optimal values for the 0 parameters is intractable, so we use the gradient-based AdamW\noptimizer Loshchilov and Hutter (2018) to adapt the weights to the data. The model is trained for 3\nepochs with a batch size of 8. We set the learning rate to 2 \u00d7 10-5, apply a weight decay of 0.01, and\nuse an Adam epsilon of 10\u20138. We implement the experiments in PyTorch Paszke et al. (2017) and use\nHuggingFace Transformers Wolf et al. (2020). All experiments are executed on a single 32GB V100\nGPU."}, {"title": "5 RESULTS", "content": "Here, we present the performance of the proposed intermediate learning method POA in monolingual\nsetting in Subsection 5.1, followed by the comparison of the selected zero-shot cross-lingual classification\nmethods in Subsection 5.2. Following that we provide a qualitative analysis on the predictability of the\nperformance of methods and qualitative analysis of the datasets."}, {"title": "5.1 Monolingual evaluation", "content": "Table 2 shows the results for the Slovenian dataset. We can see that all but the basic non-translation\napproach outperform ICL. Furthermore, we find that translation into English improved the baseline\napproach, while translation did not significantly affect the PSE and POA results. We note that the standard\ndeviation of the baseline approach without translation is very high compared to the other settings, which\nleads us to conclude that this may not be a reliable result. We perform a paired T-test across all strategies\nto assess the statistical significance of the effects of translation in the general setting. We find that there\nis no statistical significance in influencing the results (t-statistic=-1.15, p-value=0.33). We also find\nthat intermediate training strategies improve performance and robustness when used to initialise the"}, {"title": "5.2 Zero-shot evaluation", "content": "The results of the zero-shot evaluation on other datasets are shown in Table 3 and visualized in Figure\n2. The results show that in cross-lingual context the in-context learning outperforms most other models.\nThis coupled with the fact that the in-context approach does not require any pretraining or fine-tuning\nmakes it an efficient method for cross-lingual sentiment analysis. On the other hand we do note that for\nAlbanian and Macedonian better results are achieved with intermediate training."}, {"title": "5.3 On the predictability of performance of methods", "content": "In Figure 3, we analysed the distance between datasets based on the Optimal Transport Dataset Distance\n(OTDD) metric Alvarez-Melis and Fusi (2020). We compared the combined Slovene (train and test) with"}, {"title": "5.4 Semi-automatic qualitative analysis", "content": "Next, we conduct a semi-automatic qualitative analysis to examine whether the high OTDD distances can\nbe attributed to topical discrapancies between datasets. We generate topics on the Slovenian dataset using\nBERTtopic Grootendorst (2022) for generating topics on the Slovenian dataset. Next, we use this model\nto annotate the articles in the remaining documents. We use the translated documents from Section 3.4 for\nconsistency: related work shows that encoder-based LLMs place similar languages in similar clusters,\nwhich might disrupt our analysis. Following Koloski et al. Koloski et al. (2024), we consider using LLMs\nto devise more understandable human topics. We use GPT-4 OpenAI et al. (2024) for generating topics\nand the XLMR above sentence transformers variant for encoding the articles. We set the maximum\nnumber of topics to 30. Since we have discrete topics per language and language label distributions,\nwe consider the x-square test to assess any statistical differences. As the datasets are unbalanced, we\nconsider sampling within a similar scale (always down-sampling Slovenian) for fair comparison. The\nresulting 29 specific and one general (outlier) topic are shown in Figure ??. The most-prevalent (outlier\ntopic) was 'finance and governance' with around 30% (3192) articles. The specific topics were the topics\nof 'Slovenian Banking and Economy', 'Stock market Trends', and 'Finance and Banking' between 10%\nand 15%. These results do not surprise us, since Economy and Finance are amongs the most used topics\naccording to Lah and \u017dili\u010d-Fi\u0161er. Following were the topics on \u2018aviation\u201c, \u2018Slovenian Politics' and\n'unemployment'. The least represented topics were 'gender equality in business and technology' and the\n'corporate awards and mergers', which we entitled to their specificity and regularization of media Dr\u017eani\u010d\nand Fi\u0161er (2022). The results show that the dataset originates from general news, with few segments of it\nbeing fairly specialised. We hypothesize that due to the generality of the dataset, the learned knowledge\nshould transfer cross-lingually."}, {"title": "6 DISCUSSION AND LIMITATION", "content": "The two in-context learning variants (with and without translations) achieved comparable and in most\ncases the best scores. Given the significant differences in computational requirements between various\napproaches, in some cases, in-context learning provides a more flexible and high-performing alternative\nto traditional machine learning methods. It can also address common challenges such as the limitation of\nfixed input sizes.\nA notable limitation of our study is the uneven amount of data across languages. The Slovenian\nlanguage is by far the most widely represented in the corpus, followed by Croatian. Such limitations\nobscure the direct comparison of performances on these subdatasets, while still allowing us to explore the\neffectiveness of various approaches. A surprising result is the fact that the performance on the Estonian\ndataset is not obviously diminished as we had expected since it is the only non-Slavic language in our\nstudy. As expected, the results on the Macedonian dataset are lower, owing to the fact it is written in the\nCyrillic alphabet. An improvement on these limitations would be acquiring a dataset with a comparable\namount of labeled data across languages and expanding the evaluation phase to other European and even\nnon-European languages. The POA approach achieved the highest performance on the Slovenian dataset,\nwhile not showing direct improvements across all other languages. The explanation behind this may be the\nfact that news articles from other datasets had an inherently different document structure than the articles\nin the Slovenian dataset, the relative position information conveyed in particular document sections may\ndiffer across different media houses. This difference may be even more amplified in our case, as our data\ninvolves changes not only in media house, but across country and culture. Another shortcoming of the\nPOA approach may stem from an assumption that an article's structure may be described linearly with\nonly 1 parameter. This leads us to further research into the correlation between the surface structure of a\nnews article to its semantic information.\nWe conclude by returning to our research questions:\n\u2022 RQ1: How does the novel intermediate training POA approach perform in monolingual and zero-\nshot cross-lingual settings for document-level sentiment classification?\nANSWER: The novel POA approach outperforms all methods in the monolingual setting and\nperforms competitively against other approaches, including in-context learning."}, {"title": "7 CONCLUSION", "content": "In this paper, we explore various techniques for cross-lingual sentiment analysis. We introduce a novel\nintermediate step, POA, designed specifically to improve sentiment analysis on long documents. We\ncompare this approach with other standard techniques, such as zero-shot and translation. We demonstrate\nthe benefits and limitations of POA and its impact on the stability and robustness of the model across this\ndiverse linguistic environments using the XLMR model. Additionally we tested the in-context learning\napproach with LLMs and achieved pleasing results. We may benefit from gathering more data in the\nlanguages which are in our case less abundant"}]}