{"title": "Enhancing AI Assisted Writing with One-Shot Implicit Negative Feedback", "authors": ["Benjamin Towle", "Ke Zhou"], "abstract": "AI-mediated communication enables users to communicate more quickly and efficiently. Various systems have been proposed such as smart reply and AI-assisted writing. Yet, the heterogeneity of the forms of inputs and architectures often renders it challenging to combine insights from user behaviour in one system to improve performance in another. In this work, we consider the case where the user does not select any of the suggested replies from a smart reply system, and how this can be used as one-shot implicit negative feedback to enhance the accuracy of an AI writing model. We introduce NIFTY, an approach that uses classifier guidance to controllably integrate implicit user feedback into the text generation process. Empirically, we find up to 34% improvement in ROUGE-L, 89% improvement in generating the correct intent, and an 86% win-rate according to human evaluators compared to a vanilla Al writing system on the MultiWOZ and Schema-Guided Dialog datasets. The code is available at https://github.com/BenjaminTowle/NIFTY.", "sections": [{"title": "Introduction", "content": "The average worker reportedly spends around 23% of their time on reading and answering emails (Mark et al., 2012). To alleviate this burden, there is a growing demand for AI-mediated communication systems to draft and potentially fully automate replies for users. These facilitate faster communication by providing suggestions at different stages of the conversational pipeline. Various modes of interaction exist, each with differing trade-offs: smart reply systems \u2013 such as in Gmail (Henderson et al., 2017) or Outlook (Deb et al., 2019) \u2013 offer a low-latency solution to dealing with simple requests, using a retrieval-based model to present canned suggested replies to the user which can be clicked instead of requiring manual typing. Al writing models \u2013 such as used by Jasper or Grammarly employ generative architectures to produce more complex replies, but may require additional manual editing and / or prompting from a user to obtain the desired result. The smart reply system may also provide an initial skeleton reply, that an AI writing model can later improve upon (Chen et al., 2019).\nYet, the heterogeneity of the forms of interaction and architectures often renders it non-obvious how the information from one system can be leveraged to improve another: e.g., a smart reply system is only useful when the user clicks one of the suggestions. In practice however, there is often too much uncertainty surrounding the user's intent for any of the suggestions to be relevant (Figure 1) (Chakravarthi and Pasternack, 2017). Due to reading the suggestions, this increases the user's cognitive load, with no additional payoff. This contrasts with positive feedback, when selecting a suggestion reduces user typing time.\nTo address this problem, we conduct a pilot study into how one-shot implicit negative feedback can be used to integrate and improve the performance between two heterogeneous AI-mediated communication systems. In particular, we consider how feedback when a user clicks none of a smart reply system's suggestions can improve a downstream Al writing model at run-time. We concentrate on the one-shot setting, which has the advantage of enabling a single shared model for all users, as well as being more challenging due to the limited amount of interaction information per user. Future work may extend this to greater degrees of user personalisation, although this is currently out of scope due to lack of data access.\nIn this paper, we introduce Negative Implicit Feedback from Smart Reply (\"NIFTY\"). NIFTY employs classifier guidance (Yang and Klein, 2021) to controllably integrate one-shot implicit negative feedback into the generation process at run-time. In particular, given an unconditional AI writing model, we condition the model on a desired attribute c, via an application of Bayes' rule, using a classifier trained to predict c. We explore two possible settings for c: an intent-based approach that conditions on the most likely next intent not represented in the suggestions and a user action-based approach that conditions directly on the user rejecting the suggestions. Overall our method affords several key advantages: (i) it keeps the smart reply and AI writing systems decoupled, allowing it to be readily integrated into existing systems which can be optimised by separate teams; (ii) it enables additional forms of negative feedback to be introduced in the future, e.g., lingering on a suggestion without clicking it (Zhang et al., 2015), via a linear combination with a separate classifier; (iii) incorporating implicit negative feedback into the AI writing process reduces the cost to the user of being presented with irrelevant suggested replies.\nEmpirically, by evaluating on two publicly available datasets, we find up to 34% improvement in ROUGE-L and 89% improvement in generating the correct intent compared to a vanilla AI writing system, and an 86% win-rate according to human evaluators. In summary, our key contributions are: (1) We introduce the framework of implicit negative feedback to the smart reply and AI writing tasks; (2) We develop and open-source an approach that uses classifier guidance, considering both intent-based and user action-based attributes for conditioning; (3) We provide both quantitative and qualitative analysis of our model's superior performance using both automated and human evaluation."}, {"title": "Related Work", "content": "Early AI-mediated communication centred around smart reply systems, which provided canned replies to a user message (Kannan et al., 2016; Henderson et al., 2017). Work then expanded to various forms of heterogeneous interaction including Al writing systems, such as autocompletion (Chen et al., 2019), alternative word suggestions (Wang et al., 2023), or conditioning on: rough drafts (Ito et al., 2019), abbreviated sentences (Adhikary et al., 2021), or user provided intents (Sun et al., 2021). While these methods all require explicit user effort, our approach requires no interactive effort from the user, by leveraging implicit negative feedback.\nVarious types of implicit feedback have been explored in previous works, such as user lingering time (Zhang et al., 2015), skipping content (Pan et al., 2023; Gong and Zhu, 2022), or conversation length (Irvine et al., 2023). To the best of our knowledge, our work is the first to apply this to the smart reply setting.\nFinally, our work builds upon the growing literature on controllable text generation. Here, early work focused on conditioning generation on particular control 'codes' (Keskar et al., 2019). More recently, work focuses on classifier guidance, in which a separate classifier guides token-by-token generation (Krause et al., 2020; Yang and Klein, 2021; Shuster et al., 2021; Arora et al., 2022). Note that our focus is not to introduce a novel algorithm for classifier guidance, but rather to demonstrate how it can be combined with implicit negative feedback to solve a problem in AI-mediated communication - namely \u2013 the lack of easy integration between different modes of interaction. Therefore, as new techniques emerge from this field they may be used to enhance our own method."}, {"title": "Method", "content": "Filtering with User Simulator We run our user simulator using the dataset D = {(m,r,s)}{=1, where m is the incoming message, r is the ground-truth reply, and s = {81, ...SK} is the set of reply suggestions obtained from a black-box smart reply system. We are concerned with the scenario in which the user clicks none of these suggestions. To represent this concretely, let z represent the set of all possible intents, zs be the set of intents assigned to suggestions in s, and z be the intent of the ground-truth response r \u2013 e.g., 'Yes I can! Table for 1?' corresponds to the booking-inform intent. We simulate the user by having the user reject all suggestions when z \u2209 zs, i.e. none of the suggestions contain the intent of the ground-truth reply. By filtering according to this criterion, we obtain dataset D' which is used for downstream evaluation. Note that both our generator and classifier are trained on the full version of our dataset D.\nSmart Reply The smart reply system uses a vector retrieval model (Karpukhin et al., 2020), trained to jointly embed messages and replies into a shared latent space. At run-time, it retrieves the top K nearest neighbours as suggested replies. Following convention (Deb et al., 2019), we set K = 3 for all of our experiments. We choose this straightforward approach in order to demonstrate that even an out-of-the-box smart reply system can provide useful implicit negative feedback, without requiring any bespoke alterations.\nGenerator Following previous approaches (Sun et al., 2021; Faltings et al., 2023), the AI writing model is a transformer trained to generate tokens autoregressively. Given a message m, the probability of generating reply r can be factorised as:\n$P_{\\theta}(r|m) = \\prod_{t=1}^{T} p_{\\theta}(r_t|m, r_{<t})$\nThis model is trained only on the (message, reply) pairs from D, without requiring access to the smart reply system, or any implicit user feedback, using negative log likelihood.\nClassifier Guidance allows an unconditional text generation model $p_{\\theta}(r_t|m, r_{<t})$ to generate tokens conditioned on an attribute c, by performing a classification step over possible next tokens $p_{\\theta}(c|m, r_{<t})$. These can be combined through Bayesian decomposition (Yang and Klein, 2021).\n$p(r_t|m, r_{<t}, c) \\propto p_{\\theta}(r_t|m, r_{<t}) \\cdot p_{\\theta}(c|m, r_{<t})$\nNote, the classifier predicts whether the attribute will be obtained by the time the response is completed, not whether the attribute is currently present. By operating at the token level, rather than over completed generations (i.e. reranking), the model is able to explore a larger search space.\nFor our purposes, we consider two possible approaches for what attribute to condition the generator on. First, we condition on the desired intent of the response. In particular, we train a classifier to predict the final intent of the reply, given only the reply prefix $r_{<t}$ and message m. At run-time, we then condition on the most likely intent that is not present in the suggestions:\n$\\underset{j=1:J}{argmax} p_{\\theta} (z_j|m, r_{<t}) \\cdot \\mathbb{1}_{z_j \\notin z_s} (z_j)$\nwhere $z \\notin z_s$ is the set of intents not present in the suggestions, and $\\mathbb{1}$ is the indicator function that outputs 1 if $z_j \\in z\\setminus z_s$, 0 otherwise. The main limitation of this approach is that it requires access to a labelled dataset of intents. To remove this limitation, we therefore also consider conditioning the classifier directly on the user action. Specifically, we attempt to predict whether or not the user rejected the suggested replies as a binary classification task."}, {"title": "Experiment", "content": "Datasets We evaluate our method on two task-oriented datasets covering various domains: Multi-WOZ v2.2 (Budzianowski et al., 2018) and Schema-Guided Dialog (SGD) (Rastogi et al., 2020). Both have the advantage of being annotated with intents, and also feature the professional style of conversations that many AI writing applications aim to facilitate. We treat replies containing multiple intents as unique intents in their own right as it is unclear that a user would accept a suggestion that only partially contained the desired intents. As we are concerned with the assisted writing setting, rather than creating a consumer-facing task-oriented chatbot, we evaluate using standard AI writing metrics, rather than success-based task-oriented metrics. See Appendix A for dataset statistics.\nMetrics We employ both automatic and human evaluation. ROUGE-L, which has been used previously in Al writing tasks (Ito et al., 2019), measures the longest common subsequence within the prediction, capturing surface-level overlap with the ground-truth. However, there are often many ways of expressing the same meaning that lack term-overlap. Therefore, inspired by previous evaluation work on smart reply systems (Weng et al., 2019), we complement this metric with R@1, which measures the proportion of generated responses that contain the correct intent. As the responses are generated on-the-fly, we detect intents using a DistilBERT-based classifier trained to map utterances to the corresponding intent for each dataset (a separate classifier to the one used in NIFTY), which we then compare to the ground-truth intent. We conduct human evaluation using Amazon Mechanical Turk. As the purpose of the system is to reduce uncertainty about the user's intended reply, we apply a pairwise setup, in which the evaluator is asked to select which candidate reply is most similar to the target reply. For each model and dataset, we randomly select 100 data points from the test set and assign 3 annotators to each data point. We define a 'win' as when a system is achieves a majority vote for a given data point. Overall, the procedure was carried out by 268 unique annotators, across 400 data points. See Appendix C for further details.\nBaselines We compare our approach to: the standard Baseline AI writing model \u2013 i.e. without any classifier guidance; a Reranker approach which reranks the final output beams, without any token-level reranking, by selecting the beam with the highest score for the desired intent / action; an Unlikelihood decoding approach that downweights the probability for terms that occur in the rejected intents, encouraging the model to generate one of the non-rejected intents (Welleck et al., 2020); a Rules-based approach in which multiple candidate beams are generated, and are then filtered to remove beams containing rejected intents from the reply suggestions.\nModel Details We explore two different transformers for generation: BlenderBot-400M (BB) (Roller et al., 2020) and T5-220M (T5) (Raffel et al., 2020). At run-time, we generate responses using beam search (n = 5). For efficiency, we use the lightweight DistilBERT-66M for classification (Sanh et al., 2019) and only rescore the top 10 tokens with it, following (Shuster et al., 2021). All models are trained with a batch size of 32, learning rate of 5e-5 with linear decay until convergence using the AdamW optimiser. For the generative models, we train using low-rank adaptors (LoRA) (Hu et al., 2022), with an r value of 8, alpha of 32 and dropout of 0.1, see Appendix B.\nMain Results Table 1 presents the results for our overall system. We find our approach consistently increases the model's ability to generate the correct intent, with an improvement of up to 89% compared to the baseline. This further corresponds to an improvement in ROUGE-L of up to 34% compared to the baseline. We find that the Reranker and Rules-based approaches fail to improve much upon the Baseline approach, which is consistent with the findings of previous work (Shuster et al., 2021), as the resulting beams typically do not represent a broad range of intents. The Unlikelihood approach also struggles due to different intents often having significant term overlap. Choice of base model proved important for some methods, although ultimately NIFTY Intent remained the strongest approach in both cases. In terms of choice of classifier, we find the intent-based classifier performs significantly better, especially for MultiWOZ. This gap is much narrower in SGD however, which we hypothesise is due to the smaller number of intents making it easier for the Action classifier to implicitly learn them. Future work may investigate techniques such as unsupervised intent discovery, which has already been used in smart reply to replace the reliance of NIFTY Intent on labelled data (Kannan et al., 2016).\nWe note also that the classifier used for intent prediction given the message has an R@1 of 47.4% for MultiWOZ and 83.2% for SGD. This difference is due to MultiWOZ having significantly more possible intents (see Appendix A). Future work may explore techniques to improve accuracy such as increasing the context window to multi-turn, as we expect this to improve the overall task performance.\nVarying Degree of Guidance In Table 2, we further evaluate how performance for the strongest version of our method, T5 NIFTY Intent, varies under different levels of \u03b1, which determines the weighting of the classifier on the logits, i.e., the rightmost term of Equation 1. We find although the approach is broadly flexible to a range of values, higher ROUGE-L scores are associated with weaker levels of guidance, while stronger levels result in higher R@1. Overall, we find the middle-ground setting of 1.0 offers the best trade-off, as well as not requiring any additional hyperparameter search.\nHuman Evaluation Table 3 shows our human evaluation results. Across both datasets and models, NIFTY Intent is statistically significantly judged better in the pairwise evaluation, with up to an 86% win rate.\nCase Study Table 4 presents a qualitative example of model performance. By utilising the fact that the user simulator rejected the unsuccessful booking intent, NIFTY correctly surmises that a successful booking intent is instead required, in contrast to the baseline model, which refuses to make the booking. This example also supports our intuition in designing the user simulator on the assumption that the user would not select a suggestion that only partially overlapped in intent with the ground truth. In this case, although [general-reqmore] is present in both suggestions and target, the lack of the [booking-book] intent appears critical to appropriately responding to the message."}, {"title": "Conclusion", "content": "In this work we introduce NIFTY, an AI writing system that uses classifier guidance to account for implicit negative user feedback from an upstream smart reply system. Empirically, we find up to 34% improvement in relevance and 89% improvement in generating the correct intent compared to a vanilla AI writing system. Future work may explore applying these techniques to real-life data from deployed systems and / or may be extended to a broader range of types of feedback beyond click data from smart reply systems."}, {"title": "Limitations", "content": "We identify three main limitations of our work, which we address here. First, our work assumes the main driver of a user clicking a suggestion is whether it matches their intended intent. However, there may be additional factors that influence which suggestions a user clicks, such as formality of the utterance, the user's own preference for using AI generated content, or the user's own writing style. However, deployed systems may circumvent this issue by training directly on user click through data, in place of the user simulator used here. Second, the datasets used are somewhat artificial, being deliberately designed as dialogue benchmarks, rather than organic datasets created in an actual working environment, which are potentially more noisy. Third, the most effective version of our method, NIFTY Intent requires a dataset of intent annotations to train the classifier. We regard removing this limitation, such as via unsupervised intent detection, as a possible avenue for future work."}, {"title": "Ethical Considerations", "content": "Various ethical concerns have been raised around the potential for generative dialog systems to produce inappropriate content, particularly as their fluency increases and their content because less distinguishable from a human's. However, there is additional nuance in the context of AI-assisted writing in that humans have oversight over the content being generated, and may reject it if it is inappropriate. On the one hand, this may be seen as a mitigating factor, as the human may function as an implicit safety classifier. On the other hand, recent research indicates that suggestions may influence user behaviour to a degree; specifically, while smart reply systems have long been known to have a positivity bias (Kannan et al., 2016), recent work finds that this can influence the behaviour of the system's users. In particular, Wenker (2023) find that users of smart reply systems produced overly more positive sentiment replies than users without access to these systems. Further research is needed on understanding in what other ways assisted writing systems shift the distribution of user replies."}, {"title": "Disclosure", "content": "GitHub Copilot was used to a limited extent for boilerplate code autocompletion. All models and the dataset used in this paper are freely available for use in research."}, {"title": "Dataset Details", "content": "Table 5 presents the detail for the MultiWOZ and SGD datasets. We filter the datasets to include only those examples where the suggestions were rejected in the user simulator, i.e. none of the suggestions had a shared intent with the ground-truth reply. The datasets contain a large number of intents, as we treat replies containing multiple intents as standalone intents."}, {"title": "Training Hyperparameters", "content": "Table 6 summarises the training hyperparameters. To reduce training time for the generator, we use LORA (Hu et al., 2022). All classifiers used in the paper use the same hyperparameters. We train the classifier for more epochs than the generator as the classifier requires learning the weights for the individual classes in the classifier head from scratch. All remaining unstated hyperparameters are the default provided by the HuggingFace TrainingArguments class."}, {"title": "Human Evaluation", "content": "For human evaluation, workers were provided with the the following short instructions: You will be shown a target reply and two candidate replies. Select which candidate reply is most similar to the target reply. They were also provided with the following long instructions: You will be shown a target reply and two candidate replies. Select which candidate reply is most similar to the target reply. Similar means having the same semantic meaning."}]}