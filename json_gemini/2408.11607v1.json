{"title": "Networked Communication for Mean-Field Games with Function Approximation and Empirical Mean-Field Estimation", "authors": ["Patrick Benjamin Alessandro Abate"], "abstract": "Recent works have provided algorithms by which decentralised agents, which may be connected via a communication network, can learn equilibria in Mean-Field Games from a single, non-episodic run of the empirical system. However, these algorithms are given for tabular settings: this computationally limits the size of players' observation space, meaning that the algorithms are not able to handle anything but small state spaces, nor to generalise beyond policies depending on the ego player's state to so-called 'population-dependent' policies. We address this limitation by introducing function approximation to the existing setting, drawing on the Munchausen Online Mirror Descent method that has previously been employed only in finite-horizon, episodic, centralised settings. While this permits us to include the population's mean-field distribution in the observation for each player's policy, it is arguably unrealistic to assume that decentralised agents would have access to this global information: we therefore additionally provide new algorithms that allow agents to estimate the global empirical distribution based on a local neighbourhood, and to improve this estimate via communication over a given network. Our experiments show-case how the communication network allows decentralised agents to estimate the mean-field distribution for population-dependent policies, and that exchanging policy information helps networked agents to outperform both independent and even centralised agents in function-approximation settings, by an even greater margin than in tabular settings.", "sections": [{"title": "1 Introduction", "content": "The mean-field game (MFG) framework (Lasry and Lions 2007; Huang, Malham\u00e9, and Caines 2006) can be used to address the difficulty faced by multi-agent reinforcement learning (MARL) regarding computational scalability as the number of agents increases. It models a representative agent as interacting not with other individual agents in the population on a per-agent basis, but instead with a distribution over the other agents, known as the mean field. The MFG framework analyses the limiting case when the population consists of an infinite number of symmetric and anonymous agents, that is, they have identical reward and transition functions which depend on the mean-field distribution rather than on the actions of specific other players. The solution to this game is the mean-field Nash equilibrium (MFNE), which can be used as an approximation for the Nash equilibrium (NE) in a finite-agent game, with the error in the solution reducing as the number of agents N tends to infinity (Saldi, Ba\u015far, and Raginsky 2018; Anahtarci, Kariksiz, and Saldi 2023; Yardim, Goldman, and He 2024; Toumi, Malhame, and Le Ny 2024; Hu and Zhang 2024). MFGs have thus been applied to a wide variety of real-world problems: see Lauri\u00e8re et al. (2022) for applications and references.\nRecent works argue that classical algorithms for solving MFGs rely on assumptions and methods that are likely to be undesirable in real-world applications, emphasising that desirable qualities for practical MFG algorithms include: learning from the empirical distribution of N agents (without generation or manipulation of this distribution by the algorithm itself or an external oracle/simulator); learning from a single, continued system run that is not arbitrarily reset as in episodic learning; model-free learning; decentralisation; and fast practical convergence (Yardim et al. 2023; Benjamin and Abate 2024). While these works address these desiderata, they do so only in settings in which the state and action spaces are small enough that the Q-function can be represented by a table, limiting their approaches' scalability.\nMoreover, in those works, as in many others on MFGs, agents only observe their local state as input to their Q-function (which defines their policy). This is sufficient when the solved MFG is expected to have a stationary distribution ('stationary MFGs') (Lauri\u00e8re et al. 2022; Xie et al. 2021; Anahtarci, Kariksiz, and Saldi 2023; uz Zaman et al. 2023; Yardim et al. 2023; Benjamin and Abate 2024). However, in reality there are numerous reasons for which agents may benefit from being able to respond to the current distribution. Recent work has thus increasingly focused on these more general settings where it is necessary for agents to have so-called 'master policies' (a.k.a. population-dependent policies) which depend on both the mean-field distribution and their local state (Cardaliaguet et al. 2015; Carmona, Delarue, and Lacker 2016; Perrin et al. 2022; Wu et al. 2024; Lauri\u00e8re et al. 2022; Lauriere et al. 2022; Lauri\u00e8re et al. 2022).\nThe distribution is a high-dimensional (and thus large) observation object, and also takes a continuum of values. Therefore a population-dependent Q-function cannot be represented exactly in a table and must be approximated. To address these limitations while maintaining the desiderata for real-world applications given in recent works, we introduce function approximation to the MFG setting of decentralised"}, {"title": "2 Preliminaries", "content": "We use the following notation. N is the number of agents in a population, with S and A representing the finite state and common action spaces, respectively. The set of probability measures on a finite set X is denoted \\(\\Delta_x\\), and \\(e_x \\in \\Delta_x\\) for \\(x \\in X\\) is a one-hot vector with only the entry corresponding to x set to 1, and all others set to 0. For time \\(t \\ge 0\\), \\(\\mu_t = \\frac{1}{N}\\sum_{i=1}^{N}\\delta_{s^i_t}\\in \\Delta_S\\) is a vector of length |S| denoting the empirical categorical state distribution of the N agents at time t. For agent \\(i \\in 1 ... N\\), i's policy at time t depends on its observation \\(o^i_t\\). We explore three different forms that this observation object can take:\n\\begin{itemize}\n    \\item In the conventional setting, the observation is simply i's current local state \\(s^i_t\\), such that \\(\\pi^i_t(a|o^i_t) = \\pi^i_t(a|s^i_t)\\).\n    \\item When the policy is population-dependent, if we assume perfect observability of the global mean-field distribution then we have \\(o^i_t = (s^i_t, \\mu_t)\\).\n    \\item It is unrealistic to assume that decentralised agents with a possibly limited communication radius can observe the global mean-field distribution, so we allow agents to form a local estimate \\(\\hat{\\mu}_t^i\\) which can be improved by communication with neighbours. Here we have \\(o^i_t = (s^i_t, \\hat{\\mu}_t^i)\\).\n\\end{itemize}\nIn the following definitions we focus on the population-dependent case when \\(o^i_t = (s^i_t, \\mu_t)\\), and clarify afterwards the connection to the other observation cases. Thus the set of policies is \\(\\Pi = \\{\\pi : S \\times \\Delta_S \\rightarrow \\Delta_A\\}\\), and the set of Q-functions is denoted \\(Q = \\{q : S \\times \\Delta_S \\times A \\rightarrow \\mathbb{R}\\}\\).\nDefinition 1 (N-player symmetric anonymous games)\nAn N-player stochastic game with symmetric, anonymous agents is given by the tuple \\((N, S, A, P, R, \\gamma)\\), where A is the action space, identical for each agent; S is the identical state space of each agent, such that their initial states are \\(\\{s^i_0\\}^N_{i=1} \\in S^N\\) and their policies are \\(\\{\\pi^i\\}^N_{i=1} \\in \\Pi^N\\). \\(P : S \\times A \\times \\Delta_S \\rightarrow \\Delta_S\\) is the transition function and \\(R : S \\times A \\times \\Delta_S \\rightarrow [0,1]\\) is the reward function, which map each agent's local state and action and the population's empirical distribution to transition probabilities and bounded rewards, respectively, i.e. \\(\\forall i = 1, ..., N\\):\n\\[s^{i}_{t+1} \\sim P(\\cdot | s^{i}_{t}, a^{i}_{t}, \\mu_{t}), r^{i}_{t} = R(s^{i}_{t}, a^{i}_{t}, \\mu_{t}).\\]\nAt the limit as \\(N \\rightarrow \\infty\\), the infinite population of agents can be characterised as a limit distribution \\(\\mu \\in \\Delta_S\\); the"}, {"title": "2.2 (Munchausen) Online Mirror Descent", "content": "Instead of performing the computationally expensive process of finding a BR at each iteration, we can use a form of policy iteration for MFGs called Online Mirror Descent (OMD). This involves beginning with an initial policy \\(\\pi_0\\), and then at each iteration k, evaluating the current policy \\(\\pi_k\\) with respect to its induced mean-field flow \\(\\mu = I(\\pi_k)\\) to compute its Q-function \\(Q_{k+1}\\). To stabilise the learning process, we then use a weighted sum over this and past Q-functions, and set \\(\\pi_{k+1}\\) to be the softmax over this weighted sum, i.e. \\(\\pi_{k+1}(\\cdot | s, \\mu) = softmax(\\sum_{i=0}^{k}Q_{i}(s, \\mu, \\cdot )\\)\\.\n\\(\\tau_q\\) is a temperature parameter that scales the entropy in Mun-chausen RL (Vieillard, Pietquin, and Geist 2020); note that this is a different temperature to the one agents use when selecting which communicated parameters to adopt, denoted \\(\\tau^{comm}_k\\) and discussed in Sec. 3.2.\nIf the Q-function is approximated non-linearly using neural networks, it is difficult to compute this weighted sum. The so-called 'Munchausen trick' addresses this by computing a single Q-function that mimics the weighted sum using implicit regularisation based on the Kullback-Leibler (KL) divergence between \\(\\pi_k\\) and \\(\\pi_{k+1}\\) (Vieillard, Pietquin, and Geist 2020). Using this reparametrisation gives Munchausen OMD (MOMD), detailed further in Sec. 3.1 (Lauriere et al. 2022; Wu et al. 2024). MOMD does not bias the MFNE, and has the same convergence guarantees as OMD (Hadikhanloo 2017; P\u00e9rolat et al. 2022a; Wu et al. 2024)."}, {"title": "2.3 Networks", "content": "We conceive of the finite population as exhibiting two time-varying networks. The basic definition of such a network is:\nDefinition 6 (Time-varying network) The time-varying network \\(\\{G_t\\}_{t > 0}\\) is given by \\(G_t = (\\mathcal{N}, E_t)\\), where \\(\\mathcal{N}\\) is the set of vertices each representing an agent \\(i = 1, ..., N\\), and the edge set \\(E_t \\subseteq \\{(i,j) : i,j \\in \\mathcal{N}\\}\\) is the set of undirected links present at time t.\nOne of these networks \\(G^{comm}_t\\) defines which agents are able to communicate information to each other at time t. The second network \\(G^{obs}_t\\) is a graph defining which agents are able to observe each other's states, which we use in general settings for estimating the mean-field distribution from local information. The structure of the two networks may be identical (e.g. if embodied agents can both observe the position (state) of, and exchange information with, other agents within a certain physical distance from themselves), or different (e.g. if agents can observe the positions of nearby agents, but only exchange information with agents by which they are connected via a telephone line, which may connect agents over long distances).\nWe also define an alternative version of the observation graph that is useful in a specific subclass of environments, which can most intuitively be thought of as those where agents' states are positions in physical space. When this is the case, we usually think of agents' ability to observe each other as depending more abstractly on whether states are visible to each other. We define this visibility graph as follows:\nDefinition 7 (Time-varying state-visibility graph) The time-varying state visibility graph \\(\\{G^{vis}_t\\}_{t > 0}\\) is given by \\(G^{vis}_t = (S', E^{vis}_t)\\), where S' is the set of vertices representing the environment states S, and the edge set \\(E^{vis}_t \\subseteq \\{(m,n) : m,n \\in S'\\}\\) is the set of undirected links present at time t, indicating which states are visible to each other."}, {"title": "3 Learning and policy improvement", "content": "Lines 1-14 of our novel Alg. 1 contain the core Q-function/policy update method. Agent i maintains a neural network parametrised by \\(\\theta^i\\) to approximate its Q-function: \\(Q_{\\theta^i}(o, a)\\). The agent's policy is given by \\(\\pi^i_{\\theta^i}(a|o) = softmax(Q_{\\theta^i}(o, \\cdot))(a)\\). When appropriate, we denote the policy \\(\\pi^i_t(a|o)\\) for simplicity. Each agent maintains a buffer (with size M) of collected transitions of the form \\((o_t, a_t, r_t, o_{t+1})\\). At each iteration k, they empty their buffer (Line 3) before collecting M new transitions (Lines 4-7); each decentralised agent i then trains its Q-network \\(Q_{\\theta^i}\\) via L training updates as follows (Lines 8-12). For training purposes, i also maintains a target network \\(Q_{\\theta^{'i}}\\) with"}, {"title": "4 Mean-field estimation and communication", "content": "We first describe the most general version of our algorithm for decentralised estimation of the empirical categorical mean-field distribution, assuming the more general setting where \\(G^{obs}\\) applies (see discussion in Sec. 2.3). We subsequently detail how the algorithm can be made more efficient in environments where the more abstract visibility graph \\(G^{vis}\\) applies, as in our experimental settings. In both cases, the algorithm runs to generate the observation object when a step is taken in the main Alg. 1, i.e. to produce \\(o^i_t = (s^i_t, \\hat{\\mu}^i_t)\\) for the steps \\(a \\sim \\pi( \\cdot | o^i_t)\\) in Lines 5, 17 and 26. Both versions of the algorithm are subject to implicit assumptions, which we highlight and discuss methods for addressing in our 'Future Work' section in Appx. C.\nAlgorithm for the general setting The method for the general setting functions is given in Alg 2. In this setting, we assume that each agent is associated with a unique ID to avoid the same agents being counted multiple times. Each agent maintains a \u2018count' vector v of length |S| i.e. it is the same shape as the vector denoting the true empirical categorical distribution of agents. Each state position in the vector can hold a list of IDs. Before any actions are taken at each time step t, each agent's count vector \\(\\hat{v}^i_t\\) is initialised"}, {"title": "5 Experiments", "content": "We provide two sets of experiments. The first set showcases that our function-approximation algorithm (Alg. 1) can scale to large state spaces for population-independent policies, and that in such settings networked, communicating agents can outperform purely-independent and even centralised agents, and do so by an even greater margin than in the tabular settings from Benjamin and Abate (2024). The second set demonstrates that Alg. 1 can handle population-dependent policies, as well as the ability of Alg. 3 to practically estimate the mean-field distribution locally.\nFor the types of game used in our demonstrations we follow the gold standard in prior works on MFGs, namely grid-world environments where agents can move in the four cardinal directions or remain in place (uz Zaman et al. 2023; Lauriere et al. 2022; Algumaei et al. 2023; Lauri\u00e8re 2021; Cui, Fabian, and Koeppl 2023; Lauriere et al. 2022; Benjamin and Abate 2024; Wu et al. 2024). We present results from four coordination tasks defined by the reward/transition functions of the agents see Appx. A.1 for a full technical description. The first two tasks are those used with population-independent policies in Benjamin and Abate (2024), though while they show results for an 8x8 and a 'larger' 16x16 grid, our results for these tasks are for 100x100 and 50x50 grids:\n\\begin{itemize}\n    \\item Cluster. Agents are rewarded for gathering together. The agents are given no indication where they should cluster, agreeing this themselves over time.\n    \\item Target agreement. Agents are rewarded for visiting any of a given number of targets, but their reward is proportional to the number of other agents co-located at the target. Agents must thus coordinate on which single target they will all meet at to maximise their individual rewards.\n\\end{itemize}\nWe also showcase the ability of our algorithm to handle two more complex tasks, using population-dependent policies and estimated mean-field observations:\n\\begin{itemize}\n    \\item Evade shark in shoal. At each t, a 'shark' in the environment takes a step towards the grid point containing the\n\\end{itemize}"}, {"title": "5.1 Results", "content": "We evaluate our experiments via two metrics. Exploitability is the most common metric in works on MFGs, and is a measure of proximity to the MFNE. It quantifies how much a best-responding agent can benefit by deviating from the set of policies that generate the current mean-field distribution, with a decreasing exploitability meaning the population is closer to the MFNE. However, there are several issues with this metric in our setting, such that it may give limited or noisy information (see Appx. A.2 for a full discussion). We thus also give a second metric, as in Benjamin and Abate (2024): the population's average discounted return. This allows us to compare how much agents are learning policies that increase their returns, even when the exploitability metric gives us limited ability to distinguish between the desirability of the MFNEs to which populations are converging.\nPopulation-independent policies in large state-spaces Figs. 1 and 2 illustrate that introducing function approximation to algorithms in this setting allows them to converge within a practical number of iterations (k < 100), even for large state spaces (100x100 grids). By contrast, the tabular algorithms in Benjamin and Abate (2024) appear only just to converge by k = 200 for the same tasks for the larger of their two grids, which is only 16x16.\nIn Fig. 1 the networked agents generally have lower exploitability than both centralised and independent agents, while in Fig. 2 the networked agents' exploitability is similar to that of the other cases. However, the networked agents significantly outperform the other architectures in terms of average return on both tasks, indicating that the communication scheme helps agents to find substantially 'preferable' equilibria. Moreover, the margin by which the networked agents can outperform the centralised agents is much greater than in Benjamin and Abate (2024), showing that the benefits of the communication scheme are even greater in non-tabular settings. See Appx. A for further experiments with these large state spaces.\nPopulation-dependent policies in complex environments Figs. 3 and 4, where agents estimate the mean-field distribution via Alg. 3, differ minimally from Figs. 5 and 6 in Appx. A, where agents directly receive the global mean-field distribution. This shows that our estimation algorithm allows agents to appropriately estimate the distribution, even with only one round of communication for agents to help each other improve their local counts. Only in the \u2018push object' task in Fig. 3, and there only with the smaller broadcast radii, do agents slightly underperform the returns of agents in the global observability case in Fig. 5, as is reasonable.\nFor the reasons given in Appx. A.2, the exploitability metric gives limited information in the 'push object' task in Fig. 3. In the 'evade' task in Fig. 4, exploitability suggests that centralised learners outperform the other cases. However, all of the networked cases significantly outperform the independent learners in terms of the average return to which they converge in both tasks. In the 'push object' task networked learners also significantly outperform centralised learners in all but the case with the smallest broad communication radius, while in the 'evade' task all networked cases perform similarly to the centralised case. Recall though that in the real world a centralised architecture is a strong assumption, a computational bottleneck and single point of failure."}, {"title": "C Extensions and future work", "content": "Alg. 3, described in Sec. 4 assumes that if a state s' is connected to s on the visibility graph \\(G^{vis}\\), an agent in s is able to accurately count all the agents in s', i.e. it either counts the exact total or cannot observe the state at all. While we assume this for simplicity, this is not inherently the case, since a real-world agent may have only noisy observations even of others located nearby, due to imperfect sensors. We suggest two ways to deal with this case. Firstly, if agents share unique IDs as in Alg. 2, then when communicating their vectors of collected IDs with each other via \\(G^{comm}\\), agents would gain the most accurate picture possible of all the agents that have been observed in a given state. However, as we note in the main body of the paper, there are various reasons why sharing IDs might be undesirable, including privacy and scalability. If instead only counts are taken, and if the noise on each agents' count is assumed to be independent and, for example, subject to a Gaussian distribution, the algorithm can easily be updated such that communicating agents compute averages of their local and received counts to improve their accuracy, rather than simply using communication to fill in counts for previously unobserved states. (Note that we can also consider the original case without noise to involve averaging, since averaging identical values equates to using the original value). Since the algorithm is intended to aid in local estimation of the mean-field distribution, which is inherently approximate due to the uniform method for distributing the uncounted agents, we are not concerned with reaching exact consensus between agents on the communicated counts, such that we do not require repeated averaging to ensure asymptotic convergence.\nWe may also wish to consider more sophisticated methods for distributing the uncounted agents across states, in place of the uniform distribution we use now. Such choices may be domain-specific based on knowledge of a particular environment. For example, one might in fact use the counts to perform Bayesian updates on a specific prior, where this prior may relate to the estimated mean-field distribution at the previous time step t - 1. If agents seek to learn to predict the evolution of the mean field based on their own policy or by learning a model, the Bayesian prior may also be based on forward prediction from the estimated mean-field distribution at t - 1. Future work lies in conducting experiments in all of these more general settings.\nPerrin et al. (2022) notes that in grid-world settings such as those in our experiments, passing the (estimated or true global) mean-field distribution as a flat vector to the Q-network ignores the geometric structure of the problem. They therefore propose to create an embedding of the distribution by first passing the vector to a convolutional neural network, essentially treating the categorical distribution as an image. This technique is also followed in (Wu et al. 2024) (for their additional experiments, but not in the main body of their paper). As future work, we can test whether such a method improves the performance of our algorithms.\nAs in the closely-related work by Wu et al. (2024), we conduct extensive numerical experiments to demonstrate the benefits of our algorithm over baselines, and leave the theoretical analysis, such proof of convergence in the function approximation setting, for future work."}, {"title": "D Additional remarks", "content": "In our Algs. 2 and 3, agents share their local counts with neighbours on the communication network \\(G^{comm}\\), and only after the \\(C_e\\) communication rounds do they complete their estimated distribution by distributing the uncounted agents along their vectors. An alternative would be for each agent to immediately form a local estimate from their local count obtained via \\(G^{obs}\\) or \\(G^{vis}\\), which is only then communicated and updated via the communication network. However, we take the former approach to avoid poor local estimations spreading through the network and leading to widespread inaccuracies. Information that is certain (the count) is spread as widely as possible, before being locally converted into an estimate of the total mean field. The same would be the case in our proposed extension for averaging noisy counts i.e. only the counts would be averaged, with the estimates completed by distributing the remaining agents after the \\(C_e\\) communication rounds."}]}