{"title": "Networked Communication for Mean-Field Games\nwith Function Approximation and Empirical Mean-Field Estimation", "authors": ["Patrick Benjamin Alessandro Abate"], "abstract": "Recent works have provided algorithms by which decentralised agents, which may be connected via a communication network, can learn equilibria in Mean-Field Games from a single, non-episodic run of the empirical system. However, these algorithms are given for tabular settings: this computationally limits the size of players' observation space, meaning that the algorithms are not able to handle anything but small state spaces, nor to generalise beyond policies depending on the ego player's state to so-called 'population-dependent' policies. We address this limitation by introducing function approximation to the existing setting, drawing on the Munchausen Online Mirror Descent method that has previously been employed only in finite-horizon, episodic, centralised settings. While this permits us to include the population's mean-field distribution in the observation for each player's policy, it is arguably unrealistic to assume that decentralised agents would have access to this global information: we therefore additionally provide new algorithms that allow agents to estimate the global empirical distribution based on a local neighbourhood, and to improve this estimate via communication over a given network. Our experiments showcase how the communication network allows decentralised agents to estimate the mean-field distribution for population-dependent policies, and that exchanging policy information helps networked agents to outperform both independent and even centralised agents in function-approximation settings, by an even greater margin than in tabular settings.", "sections": [{"title": "1 Introduction", "content": "The mean-field game (MFG) framework (Lasry and Lions 2007; Huang, Malham\u00e9, and Caines 2006) can be used to address the difficulty faced by multi-agent reinforcement learning (MARL) regarding computational scalability as the number of agents increases. It models a representative agent as interacting not with other individual agents in the population on a per-agent basis, but instead with a distribution over the other agents, known as the mean field. The MFG framework analyses the limiting case when the population consists of an infinite number of symmetric and anonymous agents, that is, they have identical reward and transition functions which depend on the mean-field distribution rather than on the actions of specific other players. The solution to this game is the mean-field Nash equilibrium (MFNE), which can be used as an approximation for the Nash equilibrium (NE) in a finite-agent game, with the error in the solution reducing as the number of agents N tends to infinity (Saldi, Ba\u015far, and Raginsky 2018; Anahtarci, Kariksiz, and Saldi 2023; Yardim, Goldman, and He 2024; Toumi, Malhame, and Le Ny 2024; Hu and Zhang 2024). MFGs have thus been applied to a wide variety of real-world problems: see Lauri\u00e8re et al. (2022) for applications and references.\nRecent works argue that classical algorithms for solving MFGs rely on assumptions and methods that are likely to be undesirable in real-world applications, emphasising that desirable qualities for practical MFG algorithms include: learning from the empirical distribution of N agents (without generation or manipulation of this distribution by the algorithm itself or an external oracle/simulator); learning from a single, continued system run that is not arbitrarily reset as in episodic learning; model-free learning; decentralisation; and fast practical convergence (Yardim et al. 2023; Benjamin and Abate 2024). While these works address these desiderata, they do so only in settings in which the state and action spaces are small enough that the Q-function can be represented by a table, limiting their approaches' scalability.\nMoreover, in those works, as in many others on MFGs, agents only observe their local state as input to their Q-function (which defines their policy). This is sufficient when the solved MFG is expected to have a stationary distribution ('stationary MFGs') (Lauri\u00e8re et al. 2022; Xie et al. 2021; Anahtarci, Kariksiz, and Saldi 2023; uz Zaman et al. 2023; Yardim et al. 2023; Benjamin and Abate 2024). However, in reality there are numerous reasons for which agents may benefit from being able to respond to the current distribution. Recent work has thus increasingly focused on these more general settings where it is necessary for agents to have so-called 'master policies' (a.k.a. population-dependent policies) which depend on both the mean-field distribution and their local state (Cardaliaguet et al. 2015; Carmona, Delarue, and Lacker 2016; Perrin et al. 2022; Wu et al. 2024; Lauri\u00e8re et al. 2022; Lauriere et al. 2022; Lauri\u00e8re et al. 2022).\nThe distribution is a high-dimensional (and thus large) observation object, and also takes a continuum of values. Therefore a population-dependent Q-function cannot be represented exactly in a table and must be approximated. To address these limitations while maintaining the desiderata for real-world applications given in recent works, we introduce function approximation to the MFG setting of decentralised"}, {"title": "2 Preliminaries", "content": "We use the following notation. N is the number of agents in a population, with S and A representing the finite state and common action spaces, respectively. The set of probability measures on a finite set X is denoted \\( \\Delta_X \\), and \\( e_x \\in \\Delta_X \\) for \\( x \\in X \\) is a one-hot vector with only the entry corresponding to x set to 1, and all others set to 0. For time \\( t \\ge 0 \\), \\( \\mu_t = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{1}_{s_t^i = s} \\in \\Delta_S \\) is a vector of length |S| denoting the empirical categorical state distribution of the N agents at time t. For agent \\( i \\in 1 \\dots N \\), i's policy at time t depends on its observation \\( o_t^i \\). We explore three different forms that this observation object can take:\n\u2022 In the conventional setting, the observation is simply i's current local state \\( s_t^i \\), such that \\( \\pi_t^i(a | o_t^i) = \\pi_t^i(a | s_t^i) \\).\n\u2022 When the policy is population-dependent, if we assume perfect observability of the global mean-field distribution then we have \\( o_t^i = (s_t^i, \\mu_t) \\).\n\u2022 It is unrealistic to assume that decentralised agents with a possibly limited communication radius can observe the global mean-field distribution, so we allow agents to form a local estimate \\( \\hat{\\mu}_t^i \\) which can be improved by communication with neighbours. Here we have \\( o_t^i = (s_t^i, \\hat{\\mu}_t^i) \\).\nIn the following definitions we focus on the population-dependent case when \\( o_t^i = (s_t^i, \\mu_t) \\), and clarify afterwards the connection to the other observation cases. Thus the set of policies is \\( \\Pi = \\{\\pi : S \\times \\Delta_S \\rightarrow \\Delta_A\\} \\), and the set of Q-functions is denoted \\( Q = \\{q : S \\times \\Delta_S \\times A \\rightarrow \\mathbb{R}\\} \\).\n**Definition 1 (N-player symmetric anonymous games)**\nAn N-player stochastic game with symmetric, anonymous agents is given by the tuple \\( (N, S, A, P, R, \\gamma) \\), where A is the action space, identical for each agent; S is the identical state space of each agent, such that their initial states are \\( \\{s_0^i\\}_{i=1}^N \\in S^N \\) and their policies are \\( \\{\\pi^i\\}_{i=1}^N \\in \\Pi^N \\). \\( P : S \\times A \\times \\Delta_S \\rightarrow \\Delta_S \\) is the transition function and \\( R : S \\times A \\times \\Delta_S \\rightarrow [0,1] \\) is the reward function, which map each agent's local state and action and the population's empirical distribution to transition probabilities and bounded rewards, respectively, i.e. \\( \\forall i = 1, ..., N \\):\n\\[\ns_{t+1}^i \\sim P(\\cdot | s_t^i, a_t^i, \\mu_t), \\quad r_t^i = R(s_t^i, a_t^i, \\mu_t).\n\\]\nAt the limit as \\( N \\rightarrow \\infty \\), the infinite population of agents can be characterised as a limit distribution \\( \\mu \\in \\Delta_S \\); the"}, {"title": "2.1 Mean-field games", "content": "We use the following notation. N is the number of agents in a population, with S and A representing the finite state and common action spaces, respectively. The set of probability measures on a finite set X is denoted \\( \\Delta_x \\), and \\( e_x \u2208 \\Delta_x \\) for \\( x \u2208 X \\) is a one-hot vector with only the entry corresponding to x set to 1, and all others set to 0. For time t \u2265 0, \\( \\mu_t = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}_{s^i_t=s} \u2208 \\Delta_s \\) is a vector of length |S| denoting the empirical categorical state distribution of the N agents at time t. For agent i \u2208 1 . . . N, i's policy at time t depends on its observation o\u1fd2. We explore three different forms that this observation object can take:\n\u2022 In the conventional setting, the observation is simply i's current local state s\u012f, such that \u03c0\u00b2(a|o\u1fd2) = \u03c0\u00b2(a|s\u012f).\n\u2022 When the policy is population-dependent, if we assume perfect observability of the global mean-field distribution then we have o = (st, \u03bct).\n\u2022 It is unrealistic to assume that decentralised agents with a possibly limited communication radius can observe the global mean-field distribution, so we allow agents to form a local estimate \u00fb which can be improved by communication with neighbours. Here we have o = (s\u012f, \u012f).\nIn the following definitions we focus on the population-dependent case when o\u1f33 = (s\u2021, \u03bc\u2081), and clarify afterwards the connection to the other observation cases. Thus the set of policies is \u03a0 = {\u03c0 : S \u00d7 \u2206s \u2192 \u2206A}, and the set of Q-functions is denoted Q = {q : S \u00d7 \u2206s \u00d7 A \u2192 R}.\nDefinition 1 (N-player symmetric anonymous games)\nAn N-player stochastic game with symmetric, anonymous agents is given by the tuple (N, S, A, P, R, \u03b3), where A is the action space, identical for each agent; S is the identical state space of each agent, such that their initial states are {50}1 \u2208 SN and their policies are {\u03c0\u00b2}N\u2081 \u2208 \u03a0\u039d. P : S \u00d7 A \u00d7 As \u2192 As is the transition function and R : S \u00d7 A \u00d7 \u2206s \u2192 [0,1] is the reward function, which map each agent's local state and action and the population's empir-ical distribution to transition probabilities and bounded rewards, respectively, i.e. Vi = 1, ..., N:\nsi+1 ~ P(\\|s, \u03b1\u0390, \u03bct), r = R(s, \u03b1\u0390, \u03bc\u03b5).\nAt the limit as N \u2192 \u221e, the infinite population of agents can be characterised as a limit distribution \u03bc\u2208 \u0394s; the"}, {"title": "2.2 (Munchausen) Online Mirror Descent", "content": "Instead of performing the computationally expensive pro-cess of finding a BR at each iteration, we can use a formof policy iteration for MFGs called Online Mirror Descent(OMD). This involves beginning with an initial policy \u03c0\u03bf,and then at each iteration k, evaluating the current policy \u03c0\u03ba with respect to its induced mean-field flow \\( \\mu = I(\\pi_k) \\)to compute its Q-function \\( Q_{k+1} \\). To stabilise the learningprocess, we then use a weighted sum over this and past Q-functions, and set \\( \\pi_{k+1} \\) to be the softmax over this weightedsum, i.e. \\( \\pi_{k+1}(\\cdot | s, \\mu) = softmax(\\frac{1}{\\tau_q}\\sum_{i=0}^{k} Q_i(s, \\mu, \\cdot)) \\).\n\\( \\tau_q \\) is a temperature parameter that scales the entropy in Mun-chausen RL (Vieillard, Pietquin, and Geist 2020); note thatthis is a different temperature to the one agents use when se-lecting which communicated parameters to adopt, denoted\\( \\tau_{\\text{comm}} \\) and discussed in Sec. 3.2.\nIf the Q-function is approximated non-linearly using neu-ral networks, it is difficult to compute this weighted sum.The so-called 'Munchausen trick' addresses this by comput-ing a single Q-function that mimics the weighted sum usingimplicit regularisation based on the Kullback-Leibler (KL)divergence between \\( \\pi_k \\) and \\( \\pi_{k+1} \\) (Vieillard, Pietquin, andGeist 2020). Using this reparametrisation gives MunchausenOMD (MOMD), detailed further in Sec. 3.1 (Lauriere et al.2022; Wu et al. 2024). MOMD does not bias the MFNE, andhas the same convergence guarantees as OMD (Hadikhanloo2017; P\u00e9rolat et al. 2022a; Wu et al. 2024)."}, {"title": "2.3 Networks", "content": "We conceive of the finite population as exhibiting two time-varying networks. The basic definition of such a network is:\nDefinition 6 (Time-varying network) The time-varyingnetwork \\( \\{G_t\\}_{t \\ge 0} \\) is given by \\( G_t = (\\mathcal{N}, E_t) \\), where \\( \\mathcal{N} \\) is theset of vertices each representing an agent \\( i = 1, ..., N \\), andthe edge set \\( E_t \\subseteq \\{(i, j) : i, j \\in \\mathcal{N}\\} \\) is the set of undirectedlinks present at time t.\nOne of these networks \\( G_t^{\\text{comm}} \\) defines which agents areable to communicate information to each other at time t.The second network \\( G_t^{\\text{obs}} \\) is a graph defining which agentsare able to observe each other's states, which we use in gen-eral settings for estimating the mean-field distribution fromlocal information. The structure of the two networks may beidentical (e.g. if embodied agents can both observe the posi-tion (state) of, and exchange information with, other agentswithin a certain physical distance from themselves), or dif-ferent (e.g. if agents can observe the positions of nearbyagents, but only exchange information with agents by whichthey are connected via a telephone line, which may connectagents over long distances).\nWe also define an alternative version of the observationgraph that is useful in a specific subclass of environments,which can most intuitively be thought of as those whereagents' states are positions in physical space. When this isthe case, we usually think of agents' ability to observe eachother as depending more abstractly on whether states are vis-ible to each other. We define this visibility graph as follows:\nDefinition 7 (Time-varying state-visibility graph) Thetime-varying state visibility graph \\( \\{G_t^{\\text{vis}}\\}_{t \\ge 0} \\) is given by\\( G_t^{\\text{vis}} = (S', E_t^{\\text{vis}}) \\), where \\( S' \\) is the set of vertices representingthe environment states S, and the edge set \\( E_t^{\\text{vis}} \\subseteq \\{(m, n) : m, n \\in S'\\} \\) is the set of undirected links present at time t,indicating which states are visible to each other."}, {"title": "3 Learning and policy improvement", "content": "Lines 1-14 of our novel Alg. 1 contain the core Q-function/policy update method. Agent i maintains a neural network parametrised by 0 to approximate its Q-function: Qo (0\u0390, \u00b7). The agent's policy is given by \u03c0\u03bf (a|o) = softmax((,)) (a). When appropriate, we denote the policy \u03c0\u03af(\u03b1|\u03bf) for simplicity. Each agent maintains a buffer (with size M) of collected transitions of the form (ot, at, rt, 0+1). At each iteration k, they empty their buffer (Line 3) before collecting M new transitions (Lines 4-7); each decentralised agent i then trains its Q-network Qo via L training updates as follows (Lines 8-12). For training purposes, i also maintains a target network Q with\nthe same architecture but parameters \u03b8 \u03b8 copied from 0 less regularly than 0 themselves are updated, i.e. only every v learning iterations (Line 11). At each iteration l, the agent samples a random batch B of |B| transitions from its buffer (Line 9). It then trains its neural network using stochastic gradient descent to minimise the following empirical loss (Line 10):\nDefinition 8 (Empirical loss for Q-network) The empiri-cal loss is given by\nL(\u03b8, \u03b8') = [ \u03a3 (\u010c (0, \u03b1) \u2013 T2 ,\nwhere the target T is\nT = rt + [Tq1n \u03c0\u03bf' (at Ot)]\u03b5 + \u03a3\u03c0\u03bf \u03c0\u03c1. (alot+1) (\u010c (Ot+1, a) \u2013 Tq Ing. (alot+1) (a/Ot+1)),\nFor cl < 0, [] is a clipping function used in Munchausen RL to prevent numerical issues if the policy is too close to deterministic, as the log-policy term is otherwise unbounded (Vieillard, Pietquin, and Geist 2020; Wu et al. 2024)."}, {"title": "3.1 Q-network and update", "content": "Lines 1-14 of our novel Alg. 1 contain the core Q-function/policy update method. Agent i maintains a neural network parametrised by 0 to approximate its Q-function: Qo (0\u0390, \u00b7). The agent's policy is given by \u03c0\u03bf (a|o) = softmax((,)) (a). When appropriate, we denote the policy \u03c0\u03af(\u03b1|\u03bf) for simplicity. Each agent maintains a buffer (with size M) of collected transitions of the form (ot, at, rt, 0+1). At each iteration k, they empty their buffer (Line 3) before collecting M new transitions (Lines 4-7); each decentralised agent i then trains its Q-network Qo via L training updates as follows (Lines 8-12). For training purposes, i also maintains a target network Q with\nk,l\nk,l\nk,l\nthe same architecture but parameters \u03b8 \u03b8 copied from 0 less regularly than 01 themselves are updated, i.e. only ev-ery v learning iterations (Line 11). At each iteration l, theagent samples a random batch B of |B| transitions fromits buffer (Line 9). It then trains its neural network usingstochastic gradient descent to minimise the following em-pirical loss (Line 10):\nDefinition 8 (Empirical loss for Q-network) The empiri-cal loss is given by\n\u0108(0,0') = [ \u03a3 (\u010c (0, \u03b1) \u2013 T2,\ntransitionEB\nwhere the target T is\nT = rt + [Tq1n \u03c0\u03bf' (at Ot)]\u03b5\u03b9 +\n(a/Ot+1)).\n\u03a3\u03c0\u03bf \u03c0\u03c1. (alot+1) (\u010c (Ot+1, a) \u2013 Tq Ing.\n\u03b1\u0395\u0391\nk,l\nk,l\n-k,l\nFor cl < 0, [] is a clipping function used in MunchausenRL to prevent numerical issues if the policy is too close todeterministic, as the log-policy term is otherwise unbounded(Vieillard, Pietquin, and Geist 2020; Wu et al. 2024)."}, {"title": "3.2 Communication and adoption of parameters", "content": "We use the communication network Geomm to share two types of information at different points in Alg 1. One is usedto improve local estimates of the mean-field distribution (seeSec. 4). The other, described here, is used to privilege thespread of better performing policy updates through the pop-ulation, allowing faster convergence in this networked casethan in the independent and even centralised cases.\nWe extend the work in Benjamin and Abate (2024) tothe function-approximation case, where in our work agentsbroadcast the parameters of the Q-network that defines theirpolicy, rather than the Q-function table. At each iterationk, after independently updating their Q-network and policy(Lines 3-14), agents approximately evaluate their new poli-cies by collecting rewards for E steps, and assign the es-"}, {"title": "4 Mean-field estimation and communication", "content": "We first describe the most general version of our algo-rithm for decentralised estimation of the empirical categor-ical mean-field distribution, assuming the more general set-ting where Gobs applies (see discussion in Sec. 2.3). Wesubsequently detail how the algorithm can be made moreefficient in environments where the more abstract visibil-ity graph Gris applies, as in our experimental settings. Inboth cases, the algorithm runs to generate the observationobject when a step is taken in the main Alg. 1, i.e. to pro-duce of = (si, \u012f) for the steps a ~ \u03c0\u03af(\u00b7\u03bf\u0390) in Lines 5,17 and 26. Both versions of the algorithm are subject to im-plicit assumptions, which we highlight and discuss methodsfor addressing in our 'Future Work' section in Appx. C.\nAlgorithm for the general setting The method for thegeneral setting functions is given in Alg 2. In this setting,we assume that each agent is associated with a unique ID toavoid the same agents being counted multiple times. Eachagent maintains a \u2018count' vector v of length |S| i.e. it is thesame shape as the vector denoting the true empirical cate-gorical distribution of agents. Each state position in the vec-tor can hold a list of IDs. Before any actions are taken ateach time step t, each agent's count vector \u00ee is initialised"}, {"title": "5 Experiments", "content": "We provide two sets of experiments. The first set show-cases that our function-approximation algorithm (Alg. 1)can scale to large state spaces for population-independentpolicies, and that in such settings networked, communicat-ing agents can outperform purely-independent and even cen-tralised agents, and do so by an even greater margin than inthe tabular settings from Benjamin and Abate (2024). Thesecond set demonstrates that Alg. 1 can handle population-dependent policies, as well as the ability of Alg. 3 to practi-cally estimate the mean-field distribution locally.\nFor the types of game used in our demonstrations we fol-low the gold standard in prior works on MFGs, namely grid-world environments where agents can move in the four car-dinal directions or remain in place (uz Zaman et al. 2023;Lauriere et al. 2022; Algumaei et al. 2023; Lauri\u00e8re 2021;Cui, Fabian, and Koeppl 2023; Lauriere et al. 2022; Ben-jamin and Abate 2024; Wu et al. 2024). We present re-sults from four coordination tasks defined by the reward/-transition functions of the agents see Appx. A.1 for a"}, {"title": "5.1 Results", "content": "We evaluate our experiments via two metrics. Exploitabil-ity is the most common metric in works on MFGs, and is a measure of proximity to the MFNE. It quantifies how mucha best-responding agent can benefit by deviating from the setof policies that generate the current mean-field distribution,with a decreasing exploitability meaning the population iscloser to the MFNE. However, there are several issues withthis metric in our setting, such that it may give limited ornoisy information (see Appx. A.2 for a full discussion). Wethus also give a second metric, as in Benjamin and Abate(2024): the population's average discounted return. This al-lows us to compare how much agents are learning policiesthat increase their returns, even when the exploitability met-ric gives us limited ability to distinguish between the desir-ability of the MFNEs to which populations are converging.\nPopulation-independent policies in large state-spacesFigs. 1 and 2 illustrate that introducing function approx-imation to algorithms in this setting allows them to convergewithin a practical number of iterations (k < 100), even forlarge state spaces (100x100 grids). By contrast, the tabularalgorithms in Benjamin and Abate (2024) appear only justto converge by k = 200 for the same tasks for the larger oftheir two grids, which is only 16x16.\nIn Fig. 1 the networked agents generally have lower ex-ploitability than both centralised and independent agents,while in Fig. 2 the networked agents' exploitability is simi-lar to that of the other cases. However, the networked agentssignificantly outperform the other architectures in terms ofaverage return on both tasks, indicating that the communi-cation scheme helps agents to find substantially 'preferable'equilibria. Moreover, the margin by which the networkedagents can outperform the centralised agents is much greaterthan in Benjamin and Abate (2024), showing that the bene-fits of the communication scheme are even greater in non-tabular settings. See Appx. A for further experiments withthese large state spaces.\nPopulation-dependent policies in complex environmentsFigs. 3 and 4, where agents estimate the mean-field distribu-tion via Alg. 3, differ minimally from Figs. 5 and 6 in Appx.A, where agents directly receive the global mean-field dis-tribution. This shows that our estimation algorithm allowsagents to appropriately estimate the distribution, even withonly one round of communication for agents to help eachother improve their local counts. Only in the \u2018push object'task in Fig. 3, and there only with the smaller broadcast radii,do agents slightly underperform the returns of agents in theglobal observability case in Fig. 5, as is reasonable.\nFor the reasons given in Appx. A.2, the exploitability met-ric gives limited information in the 'push object' task in Fig.3. In the 'evade' task in Fig. 4, exploitability suggests thatcentralised learners outperform the other cases. However, allof the networked cases significantly outperform the indepen-dent learners in terms of the average return to which theyconverge in both tasks. In the 'push object' task networkedlearners also significantly outperform centralised learners inall but the case with the smallest broad communication ra-dius, while in the 'evade' task all networked cases performsimilarly to the centralised case. Recall though that in thereal world a centralised architecture is a strong assumption,a computational bottleneck and single point of failure."}, {"title": "B Related work", "content": "MFGs are a quickly growing research area, so we only dis-cuss the works most closely related to this present work, andinstead refer the reader to Benjamin and Abate (2024) fordetailed discussion around the setting of networked com-munication for MFGs, and to Lauri\u00e8re et al. (2022) for a broader survey of MFGs. Our work is most closely relatedto Benjamin and Abate (2024), which introduced networkedcommunication to the infinite-horizon MFG setting. How-ever, this work focuses only on tabular settings rather thanusing function approximation as in ours, and only addressespopulation-independent policies.\nLauriere et al. (2022) uses Munchausen Online MirrorDescent (MOMD), similar to our method for learning withneural networks, but there are numerous differences to oursetting: most relevantly, they study a finite-horizon episodicsetting, where the mean-field distribution is updated inan exact way and an oracle supplies a centralised learnerwith rewards and transitions for it to learn a population-independent policy. Wu et al. (2024) uses MOMD to learnpopulation-dependent policies, albeit also with a centralisedmethod that exactly updates the mean-field distribution ina finite-horizon episodic setting. Perrin et al. (2022) learnspopulation-dependent policies with function approximationin infinite-horizon settings like our own, but does so in a cen-tralised, two-timescale manner without using the empiricalmean-field distribution.\nYongacoglu, Arslan, and Y\u00fcksel (2022) addresses decen-tralised learning from a continuous, non-episodic run of theempirical system using either full or compressed informa-tion about the mean-field distribution, but agents are as-sumed to receive this information directly, rather than es-timating it locally as in the algorithm we now present. Theyalso do not consider function approximation or inter-agentcommunication in their algorithms. In the closely related butdistinct area of Mean-Field RL, Subramanian et al. (2020)does estimate the empirical mean-field distribution from thelocal neighbourhood, however agents are seeking to estimatethe mean action rather than the mean-field distribution overstates as in our MFG setting. Their agents also do not haveaccess to a communication network by which they can im-prove their estimates."}, {"title": "C Extensions and future work", "content": "Alg. 3, described in Sec. 4 assumes that if a state s' is con-nected to s on the visibility graph Gris, an agent in s is ableto accurately count all the agents in s', i.e. it either countsthe exact total or cannot observe the state at all. While weassume this for simplicity, this is not inherently the case,since a real-world agent may have only noisy observations even of others located nearby, due to imperfect sensors. Wesuggest two ways to deal with this case. Firstly, if agentsshare unique IDs as in Alg. 2, then when communicatingtheir vectors of collected IDs with each other via Gcomm ,agents would gain the most accurate picture possible of allthe agents that have been observed in a given state. How-ever, as we note in the main body of the paper, there are vari-ous reasons why sharing IDs might be undesirable, includingprivacy and scalability. If instead only counts are taken, andif the noise on each agents' count is assumed to be indepen-dent and, for example, subject to a Gaussian distribution, thealgorithm can easily be updated such that communicatingagents compute averages of their local and received countsto improve their accuracy, rather than simply using commu-nication to fill in counts for previously unobserved states.(Note that we can also consider the original case withoutnoise to involve averaging, since averaging identical valuesequates to using the original value). Since the algorithm isintended to aid in local estimation of the mean-field distri-bution, which is inherently approximate due to the uniformmethod for distributing the uncounted agents, we are notconcerned with reaching exact consensus between agents onthe communicated counts, such that we do not require re-peated averaging to ensure asymptotic convergence.\nWe may also wish to consider more sophisticated meth-ods for distributing the uncounted agents across states, inplace of the uniform distribution we use now. Such choicesmay be domain-specific based on knowledge of a particularenvironment. For example, one might in fact use the countsto perform Bayesian updates on a specific prior, where thisprior may relate to the estimated mean-field distribution atthe previous time step t - 1. If agents seek to learn to predictthe evolution of the mean field based on their own policy orby learning a model, the Bayesian prior may also be basedon forward prediction from the estimated mean-field distri-bution at t - 1. Future work lies in conducting experimentsin all of these more general settings.\nPerrin et al. (2022) notes that in grid-world settings suchas those in our experiments, passing the (estimated or trueglobal) mean-field distribution as a flat vector to the Q-network ignores the geometric structure of the problem.They therefore propose to create an embedding of the dis-tribution by first passing the vector to a convolutional neu-ral network, essentially treating the categorical distributionas an image. This technique is also followed in (Wu et al.2024) (for their additional experiments, but not in the mainbody of their paper). As future work, we can test whethersuch a method improves the performance of our algorithms.\nAs in the closely-related work by Wu et al. (2024), weconduct extensive numerical experiments to demonstrate thebenefits of our algorithm over baselines, and leave the the-oretical analysis, such proof of convergence in the function approximation setting, for future work."}, {"title": "D Additional remarks", "content": "In our Algs. 2 and 3, agents share their local counts withneighbours on the communication network Geomm, and onlyafter the Ce communication rounds do they complete theirestimated distribution by distributing the uncounted agentsalong their vectors. An alternative would be for each agentto immediately form a local estimate from their local countobtained via Gobs or Guis, which is only then communicatedand updated via the communication network. However, wetake the former approach to avoid poor local estimationsspreading through the network and leading to widespread in-accuracies. Information that is certain (the count) is spreadas widely as possible, before being locally converted intoan estimate of the total mean field. The same would be thecase in our proposed extension for averaging noisy countsi.e. only the counts would be averaged, with the estimatescompleted by distributing the remaining agents after the Cecommunication rounds."}]}