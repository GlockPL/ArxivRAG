{"title": "Invariance of deep image quality metrics to affine transformations", "authors": ["Nuria Alabau-Bosque", "Paula Daud\u00e9n-Oliver", "Jorge Vila-Tom\u00e1s", "Valero Laparra", "Jes\u00fas Malo"], "abstract": "Deep architectures are the current state-of-the-art in predicting subjective image quality. Usually, these models are evaluated according to their ability to correlate with human opinion in databases with a range of distortions that may appear in digital media. However, these oversee affine transformations which may represent better the changes in the images actually happening in natural conditions. Humans can be particularly invariant to these natural transformations, as opposed to the digital ones.\nIn this work, we evaluate state-of-the-art deep image quality metrics by assessing their invariance to affine transformations, specifically: rotation, translation, scaling, and changes in spectral illumination. We propose a methodology to assign invisibility thresholds for any perceptual metric. This methodology involves transforming the distance measured by an arbitrary metric to a common distance representation based on available subjectively rated databases. We psychophysically measure an absolute detection threshold in that common representation and express it in the physical units of each affine transform for each metric. By doing so, we allow the analyzed metrics to be directly comparable with actual human thresholds. We find that none of the state-of-the-art metrics shows human-like results under this strong test based on invisibility thresholds.\nThis means that tuning the models exclusively to predict the visibility of generic distortions may disregard other properties of human vision as for instance invariances or invisibility thresholds.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep architectures are the current state-of-the-art in predicting subjective image quality. These models, usually referred to as perceptual metrics, are often used as measures to optimize other models [1], [2]. This implies that assessing their performance can be critical in a wide range of applications. Usually, these models are evaluated (or even tuned) according to their ability to correlate with human opinion in databases including a wide range of generic distortions [3], [4]. In fundamental terms, this means trying to predict the visibility of generic distortions. However, focus on generic distortions may be a problem to take into account other relevant phenomena [5], and human vision may also be described in terms of perceptual constancies or invariances [6], [7], [8], [9], [10], [11].\nIn this regard, considering the structural analysis of distortions and high-level interpretation of human vision, people have suggested that humans are mainly invariant to transformations which do not change the structure of the scenes [12]. Affine transformations (rotations, translations, scalings, and changes in spectral illumination) are examples of distortions that do not change the structure of the scene. Therefore, humans should be relatively tolerant to them, and the corresponding models to assess image similarity should be invariant to these transformations too.\nIn fact, the spirit of the influential SSIM was focused on measuring changes of structure [13] to achieve invariances to irrelevant transformations [14]. Moreover, Wang and Simoncelli [12] decomposed generic distortions into structural and non-structural components so that the part not affecting the structure (e.g. affine transformations) could be processed, and weighted, differently. On the other hand, metrics with bio-inspired, explainable architecture [15], [16], [17], [18], [19], [20], [5], [4] work in multi-scale / multi-orientation domains where invariances can be introduced by means of appropriate poolings as in the scattering transformations [21]. This sort of poolings are thought to happen in the visual brain leading to invariances and texture metamers [22].\nHowever, current state-of-the-art deep-architectures for image quality [3] do not address the invariance problem in any way, while examples that try to apply the SSIM concept in deep-nets [23] do not use invariances in simple or explicit ways. As a result, the analysis of invariance in deep image quality metrics remains an open question.\nIn this work, we compare the ability of metrics to be invariant to affine transformations in the same way as humans are. In particular, we propose to evaluate the metrics from the point of view of human detection thresholds: by assessing the invariance of the metrics to image transformations that are irrelevant (or invisible) to human observers. For example, the classical literature on visual thresholds determines the intensity of certain affine transformations which is invisible for humans [24], [25], [26], [27]. The sizes of invisibility thresholds are related to the more general concept of invariance to transformations. By definition, transformations whose intensity is below the threshold are invisible to the observer. Then one can say that, in this region, the observer is invariant to the transform. Here we propose a methodology that allows us to assign specific invisibility thresholds per metric. This proposal uses transduction functions to a common internal representation based on a subjectively rated database and a psychophysically measured threshold in this representation. Then, we can (1) assess if the thresholds for the metrics are comparable to those found for human observers, and (2) assess if the sensitivities of a metric for the different distortions follow the same order as the human sensitivities (for instance humans are more sensible to rotation changes than to illumination changes). This proposed evaluation of invariance to distortions is a necessary complement to the conventional evaluation of the visibility of distortions because, as we will see, none of the studied metrics presents human-like thresholds nor sensitivities for all the transformations considered.\nThe structure of the paper is as follows: Section II describes the proposed methodology to assess the human-like behavior of metric invariances. The proposed methodology consists of comparing the detection thresholds for humans and metrics. This proposal depends on several concepts (transduction functions, psychophysical thresholds for humans, theoretical thresholds for metrics...) that will be detailed in this section too. Section III describes the experimental setting, and Section IV considers the results on the thresholds and the sensitivities. Finally, the discussion and conclusions are presented in Section V and Section VI, respectively."}, {"title": "II. PROPOSED METHODOLOGY: THRESHOLDS AND SENSITIVITIES", "content": "Given an original image, i, it can be distorted through certain transform whose intensity depends on a parameter \u03b8, $i' = T_\\theta(i)$. Image quality metrics are models that try to reproduce the subjective sensation of distance between the original image and the distorted image, $d(i, i')$. Human observers are unable to distinguish between i and $i'$, i.e. they are invariant to transform $T_\\theta$, if its intensity is below a (human) threshold, $ \\theta_H^T$.\nWhile invariance thresholds in human observers, $ \\theta_H^T$, are easy to understand and measure [24], [25], [26], [27], they are not obvious to define in artificial models of perceptual distance. The reason is simple: for the usual image quality models any non-zero image distortion leads to non-zero variations in the distance. In this situation where artificial distances are real-valued functions one should define a value, the threshold distance, $D_T$, below which the difference between the images could be disregarded (or taken as zero). Once this threshold distance is available (eventually in a common scale for all metrics) one can translate this threshold to the axis that measures the distortion intensity, \u03b8, and hence obtain the threshold of the metric, $ \\theta_M^T$, in the same units that has been measured for humans, $ \\theta_H^T$. \n\nThe diagram in Fig. 1 displays the relation between the physical description of the intensity of certain distortions in abscissas (the parameter \u03b8, e.g. the angle in a rotation transform), and the common internal description of the perceived distance in ordinates (the distance D, e.g. the normalized Mean Opinion Score units explained below). This relation between the physical description of the intensity and common perceptual distance is what we call transduction function. The example displays three transduction functions, $D_M = g_M(\\theta)$, with $M = 1,...,3$, in blue, red, and green, for the three corresponding metrics. Transduction functions are monotonic: the application of a progressively increasing distortion along the abscissas leads to monotonic increments in distances in ordinates. The threshold distance, $D_T$, in the internal representation is plotted in orange in Fig. 1. This threshold may be uncertain (as represented by the central value, solid line, and the quartile limits, in dashed lines), but the empirical transduction functions can be used to put this internal threshold back in the axis that describes the distortion intensity: $ \\theta_M^T = g_M^{-1}(D_T)$. In this way, one can check if the actual invisibility threshold measured for humans (black line) is consistent with the threshold interval deduced for each metric. In our illustration, the metric 2 (red line) is the only one compatible with human behavior.\nUsing the above, on the one hand, we propose to evaluate the alignment between the metric models and human observers by comparing $ \\theta_M^T$ versus $ \\theta_H^T$. This is a strict comparison that only depends on the experimental uncertainty of the thresholds. On the other hand, we can define an alternative (less strict) comparison by considering the order among the sensitivities for the different distortions in humans and models. In particular, one can define the sensitivity of a metric for a distortion as the variation of the transduction function in terms of the energy (or Mean Squared Error) of the distortion introduced by the transform. The human sensitivity in detection is classically defined to be proportional the inverse of the energy required to see the distortion [28].\nThe proposed comparisons of metric vs human thresholds and metric vs human sensitivities are general as long as one can address the following issues:\n(a) The transduction function (red, green, and blue curves in the illustration). For the n-th metric, $d_M(i,i')$, the relation between the physical description of the image transform and a common metric-independent distance domain has two components:\n(a.1) The (non-scaled) response function can be empirically computed by generating images distorted (transformed) with different intensities, \u03b8, and using the metric expression to compute the corresponding distances from the original image. This leads to the distances $d_M(\\theta) = d_M(i,i') = d_M(i, T_\\theta(i))$.\n(a.2) A metric equalization function transforming the previous (non-scaled) distance values into the common scale of the internal distance representation (what we called normalized DMOS units in the illustration). Here we propose to use auxiliary empirical data (e.g. certain subjectively rated databases) to scale the range of the different metrics: $D_M = f_M(d_M)$. This makes different $D_M$ comparable.\nThen, the final (scaled) transduction is the composition of response and equalization: $D(\\theta) = g_M(\\theta) = f_M(d_M(\\theta))$.\n(b) The human thresholds, that can be defined in different domains:\n(b.1) The human threshold in the common internal representation, $D_T$, orange line in the illustration of Fig 1. In principle, this value is unknown. Here we propose a standard measurement of this threshold through a psychometric function [29] using distorted images of the selected subjectively rated database.\n(b.2) The human threshold in the input physical representation, $ \\theta_H^T$, black line in the illustration of Fig 1. In this work, we explore two options: (i) take the values from the classical literature, which in general uses substantially different stimuli (synthetic as opposed to natural), and (ii) re-determine the thresholds in humans by using comparable natural stimuli and a separate psychometric function for each distortion.\n(c) The metric threshold, which can be expressed in intuitive physical units, or in the own units of the metric:\n(c.1) In physical units $ \\theta_M^T$. Here, the blue, red, and green points in the x-axis of the illustration of Fig 1, computed as $ \\theta_M^T = g_M^{-1}(D_T)$. These units are useful to compare with the equivalent human values.\n(c.2) In the units of the own metric, $d_M^T = f_M^{-1}(D_T)$. This value is particularly interesting, as it indicates a variation in the distance of each metric for which humans see no difference between the original and the distorted image. Distortions leading to distances below this value are invisible to humans and hence should be neglected.\n(d) The sensitivities of humans and metrics for the different distortions. The sensitivity for a small distortion is usually defined as the inverse of the energy required to be above the invisibility threshold [28]. In the case of metrics, this general definition reduces to the derivative of the transduction function with regard to the energy of the distortion [30].\nBelow, we elaborate on each of these factors in turn.\nA. Transduction: response and equalization\nThe response function of a metric, $d_M$, to a certain image transform, $T_\\theta$, is just the average over a set of images, ${i_s}_{s=1}^S$, of the distances for the distorted images for different transform intensities:\n$d_M(\\theta) = \\frac{1}{S} \\sum_{s=1}^S d(i, T_\\theta(i))$ (1)\nwhich can be empirically computed from controlled distortions of the images of the dataset. Of course, when considering M different metrics, their response functions, $d_M$, are not given in a common scale.\nIn this work, we propose to use auxiliary empirical data to determine a common distance scale, D, for any metric. In particular, subjectively rated image quality databases (e.g. TID [31]) consist of pairs of original and distorted images, ${(i_p, i_p')}_{p=1}^P$, with associated subjective scores, the so-called Mean Opinion Scores, ${DMOS_p}_{p=1}^P$. The databases contain a wide set of generic distortions of different intensities thus ranging from invisible distortions to highly noticeable distortions. In this setting, DMOS values in the database can be normalized to be in the [0,1] range. In this way, the extreme values of the normalized DMOS represent an invisible distortion and the biggest subjective distortion in the database respectively. Therefore, if the range of distortions in the database is wide, the variations induced by $T_\\theta$ will be within the limits of the normalized DMOS, and hence, it can be used to set the common distance scale D = norm DMOS \u2208 [0, 1].\nUsing the normalized DMOS values and the image pairs of a large subjectively rated database one can fit an equalization function, $f_M$, for each metric to transform the non-scaled response $d_M$ into the common scale D:\n$D = f_M(d_M(i,i')) = a_M \\cdot d_M(i, i')^{b_M}$ (2)\nwhere an exponential function with $a_M > 0$ and $0 < b_M \u2260 1$ is chosen because of the nature of the DMOS, which changes rapidly for low distortion intensities, low values of \u03b8, and saturates for bigger distortions, big values of \u03b8. This is due to the maximum number of comparisons performed when building the database.\nIn summary, (a.1) an application of the distance metric to a controlled set of transformed images, and (a.2) a fit of an equalization function to transform the arbitrary scale of the distance to a common scale given by the set of distortions in a wide subjectively rated database, gives us the transduction function of the metric made of Eqs. 1 and 2:\n$D = g_M(\\theta) = f_M d_M(\\theta)$ (3)\nIn our work, we compute these transduction functions for:\nB. Human thresholds\nIn this section, we measure the human thresholds from two different points of view. First, we use distorted images from a subjectively rated database to measure from the internal common representation. Then, we measure it from the internal physical representation using natural stimuli.\nB.1. Human thresholds in the common internal representation\nTo find out if the metrics behave similarly to humans we need to determine the human invisibility threshold, $D_T$. This is the distance in normalized DMOS units from which humans can't tell the difference between i and $i'$ for low distortion intensities \u03b8. For that, we need a database with ratings and opinions of observers, for which we can assign a threshold value of invisibility. In this case, we use the TID13 database. [31].\nThe value of $D_T$ could be roughly estimated by visual inspection of the images presented in Appendix A: images with low values of normalized DMOS (below 0.3) cannot be discriminated from the original, while images with big normalized DMOS (above 0.6) are clearly distinct from the original. However, the more accurate estimation of such a threshold is obtained from the psychometric function in a constant stimulus experiment [29] applied to other fields within computer vision, such as [36], [37], [38], [39], [40]\nIn this kind of experiments, given a set of distorted images one computes the probability that an observer sees some distorted image as different from the original, i.e. $P(D > D_T)$. This is done by using the two-alternative forced choice paradigm (2AFC): by randomly presenting the observer each distorted image together with the original so that the observer is forced to choose the distorted one. This is repeated R times, so the probability, $P(D > D_T)$, is given by the number of correct responses over R. Note that if $D < D_T$, the observer will not see the difference and the probability of correct answer will be 0.5. On the other extreme, if $D >> D_T$, the answer will be obvious for the observer, and the probability of a correct answer will be 1. As a result, the threshold can be defined as the point where $P(D > D_T) = 0.75$.\nWe look for the optimal threshold, $D_T$, and slope, k, that better fit the experimental data using the following sigmoid [29]:\n$p(x) = \\frac{1}{2} + \\frac{1}{2(1 + e^{-k(D-D_T)}}$ (4)\nNote that this expression enforces that for extremely distorted images $lim_{D \\rightarrow \\infty} p(D > D_T) = 1$, and that the probability at the threshold, $D = D_T$ is 0.75, as required.\nIn our experiment, the psychometric function has been evaluated in 20 different values of D, repeated 15 times for 5 observers, resulting in 20 \u00d7 15 \u00d7 5 = 1500 forced choices in total.\n$D_T = 0.44 \u00b1 0.05$\nwhere the corresponding quartiles give the uncertainty. This threshold corresponds to the orange lines in Figure 1.\nB.2. Human threshold in physical units\nHumans are not equally sensitive to all transformations or to different degrees of distortion, \u03b8. In fact, there are certain thresholds at which these distortions are not perceptible, $ \\theta_H^T$, known as the invisibility threshold. Initially, we were going to use the thresholds from the classical literature [24], [25], [26] but, considering that these thresholds are measured with synthetic stimuli and not with natural images, we decided to do the experiments ourselves. To derive the new thresholds with human data, we followed the psychometric function methodology, following the Equation 4. Contrary to the literature, these experiments have been done with natural images, to use the same type of images that the models use. In particular, images from ImageNet have been used to find the invisibility thresholds in a more realistic context. Regarding the geometric affine transformations (Section III-A), the procedure follows the classic psychometric function methodology (Section II-B).\nBecause when changing the spectral illuminant we move in a 2D space, illuminant transformations are treated slightly differently. On the one hand, the MacAdam ellipses [27] don't contain any ellipse centered in the true white, coordinates (1/3,1/3) in the spectral locus, but it is the starting point of the employed illuminants. This can be solved by fitting a new ellipse at this point considering the closest existing ellipses.\nFollowing the methodology in Figure 1, we can obtain the metric thresholds in physical units, $ \\theta_M^T$. For example, the rotation threshold is expressed in degrees. Through the function $g_M^{-1}(D_T)$, specific to each metric, a value in physical units is assigned to the threshold in the common internal representation, computed as $ \\theta_M^T = g_M^{-1}(D_T)$. These values can be compared numerically with the thresholds obtained for humans, as they are expressed in the same units because they were obtained with the psychometric functions, Section II-B. Therefore, a metric can be said to have human behavior if the invisibility threshold for a certain affine transformation coincides, $ \\theta_H^T == \\theta_M^T$. In particular, since $D_T$ has an associated uncertainty, $ \\theta_M^T$ also has a confidence interval. In this case, we will check whether the human threshold falls within the confidence interval of each metric.\nThis test is particularly demanding because it is strictly quantitative, i.e. numerical comparison. It is to be expected, that the metrics will not be able to reproduce these thresholds. This is why an alternative, less demanding, test is proposed, reflecting the qualitative behavior of the metrics in the face of these distortions.\nD. Sensitivities of metrics and humans for the different distortions\nTaking into account that the methodology proposed above is particularly demanding, we propose another test. Having calculated the thresholds for metrics and humans, we can define sensitivities for both and compare them. However, this comparison will be qualitative, not quantitative, so this test is less demanding for metrics.\nThe sensitivity for a small distortion is usually defined as the inverse of the energy required to be above the invisibility threshold (Eq. 5), i.e, expressing the distortion $ \\delta \\theta$ in RMSE units. In the case of metrics, this general definition reduces to the derivative of the transduction function concerning the energy of the distortion, i.e. its slope.\n$S = \\frac{1}{|i - T_{\\theta_{H,M}^T}(i)|^2}$ (5)\nIn this way, we can obtain the sensitivity of a metric for each affine transform, order them and check if the order matches the human ones.\nGiven two curves, the most sensitive will be the one with a higher slope.\nIn Figure 7, the human thresholds indicate that the human sensitivity order (indicated by the vertical lines) is: scale, translation, rotation, RG, and YB illuminants. However, the metric shown returns the following order (given by the slopes of the curves): scale, rotation, translation, YB, and RG illuminants. Even if the ordering doesn't strictly match, it is to be noted that this metric maintains more sensitivity to geometric than chromatic distortions."}, {"title": "III. EXPERIMENTAL SETTINGS", "content": "In this section, we review the selected affine transformations, models, and databases that we will use in the experiments. Code available to test other metrics publicly\nA. Affine Transformations\nAn affine transformation or affine application (also called affinity) between two affine spaces is a transformation that satisfies Equation 6.\n$F : v \\rightarrow Mv + b$ (6)\nWhere v can be any vector and the affine transformation is represented by a matrix M and a vector b satisfying the following properties: first, it maintains the collinearity (and coplanarity) relations between points and, second, it maintains the ratios between distances along a line.\nHere, we apply affine transformations in two cases: domain (Equation 7) and image samples (Equation 8), i.e. modifications within the image vector:\n$i'(x) = i(Mx + b)$ (7)\n$i'(x) = Mi(x) + b$ (8)\nSome examples of affine transformations are geometric contraction, expansion, dilation, reflection, rotation, or shear. In this work, we are going to focus specifically on translation, rotation scale, and illuminant changes. For each tested affine transformation, the original images are modified in the following ways:\n\n\nTranslation: Displacements on the vertical and horizontal axis (and the combination between them) with an amplitude of 0.3 degrees of translation in each direction (left, right, up, and down). Given the symmetry in both displacement directions, we calculate the average and show only the displacements to the right in the graphs.\nRotation: Rotations from -10 to 10 degrees, in steps of 0.1 degree. Again, given the symmetry in both displacement directions, only positive rotations will be shown in the graphs.\nScale: Scale factors range from 0.1 to 2, but do not have a fixed step. Only scale factors that return images of even size are used. This ensures that only scales that do not force a translation are applied. Only scale factors bigger than 1 will be shown.\n\nIlluminant changes: We have desaturated the original images and modified the illuminant for 20 hues in 8 saturations. Following the distribution at the locus as indicated in Figure 8.\n\nAs a summary, examples of all the affine transformations emulated in this work can be seen in Figure 9 and an example of all illuminant changes and different intensities in Figure 11.\nB. Datasets\nIn terms of dataset selection, we chose four different datasets covering a wide range of features. On the one hand, we have MNIST [33] for the black and white images. A simple set both to understand and to modify. On the other hand, for color images, we have selected CIFAR-10[34] for color images with low resolution, and ImageNet [35] and TID2013[31] for color images with high resolution.\nSpecifically, from each dataset we selected 250 images to reduce the computational burden of applying all affine transformations and comparing all metrics; and they were modified so that, when applying the transformations, the resulting images would not present new artifacts or the central element would disappear. For the MNIST set images -originally with 28x28 images-, we simply enlarged the images (56x56) by adding black pixels around the original images to give us more room to move them. For color images, to avoid the appearance of black borders when applying some transformations, we decided to modify them and generate a mosaic. In addition to generating the mosaic, the images have a mirror effect to make the transitions between the different images smoother. Once the mosaic is created, the transformation is applied and a patch is taken from the original size of the database, thus preserving the size while including the modification."}, {"title": "C. Metrics", "content": "We choose different metrics because of their relevance in the context of perceptual metrics, either because of their age and widespread use, or because of the good results obtained in other works and applied to other areas of study. In this work, the following will be used:\n\nMean Squared Error (MSE): Measures the average squared difference between corresponding pixels in two images. It's a basic metric for image quality assessment, often used in image denoising and restoration tasks. However, it doesn't always align well with human perception.\nStructural Similarity Index (SSIM) [13]: For two images - the original and the distorted image - it computes three different comparisons: Luminance, contrast and structure. SSIM provides a more perceptually meaningful measure than MSE.\nLearned Perceptual Image Patch Similarity (LPIPS) [3]: It uses a VGG trained with Imagenet to pass the images and compute distances in different feature spaces. Then, it performs a weighted sum so that the correlation with a perceptual database is maximized.\nDeep Image Structural Similarity (DISTS) [23]: As in LPIPS, it uses the VGG network, but in addition, it performs SSIM index at different intermediate layers. This new index combines sensitivity to structural distortions with tolerance to textures sampled elsewhere in the image.\nPerceptNet [4]: It proposes an architecture to reflect the structure and stages of the early human visual system by considering a succession of canonical linear and nonlinear operations. The network is trained to maximize correlation with a perceptual database.\nPerceptual Information Metric (PIM) [32]: Unlike the other metrics, its training is based on two principles: efficient coding and slowness. On the one hand, it is compressive and, on the other, it captures persistent temporal information. This is achieved by training the network with images extracted from videos over which very short periods of time have passed, which should make it more robust to small variations in the object's movements or subtle changes in lighting."}, {"title": "IV. RESULTS", "content": "Table II summarizes the numerical results of the experiments. For each geometric affine transformation, we present the discrimination thresholds per metric, $ \\theta_M^T$. In the case of illuminant changes, we show the errors made with respect to the interpolated MacAdam and the experimentally fitted ellipses. As the ellipses are very similar (Figure 6), the errors are very similar. In those where they did not coincide completely, the mean of the errors has been calculated and marked with an asterisk (*) in the Table.\nAs we are considering both the human thresholds extracted from the literature, $ \\theta_H$, and the threshold obtained by ourselves with natural images, $ \\theta_{Hn}$, a result is marked in bold if falls within $ \\theta_H$ and is underlined if it falls within $ \\theta_{HN}$. Note that a result can be marked both ways at the same time. A metric with many highlighted boxes will be a metric with a good performance from the point of view of human invariance.\nThe results from Table II show that following our strong invariance criteria, there is no clear winner. No model shows the required robustness to affine transformations nor type of stimuli.\nThe comparison of the order between the human thresholds and the sensitivities of the metrics can be seen in Table III, where even if no metric shows complete human behavior, most of them maintain the chromatic ordering."}, {"title": "V. DISCUSSION", "content": "As we have seen in Table II, bearing in mind that no model is highlighted for all related transformations, the conclusion is that there is no metric that behaves, in terms of invariance, like the human being. We can dissect the behavior for the different affine transformation as follows:\n\nTranslation: There is no particular model that shows human-like translation invariance but DISTS and PIM are the best in this regard.\nRotation: Perceptnet shows the most human-like behavior but it's very dependent on the database used.\nScale: There are isolated highlighted cells. In general, all the models seem less sensitive than humans except in the ImageNet database, where the threshold is an order of magnitude smaller than the human threshold.\nIlluminants: Two models rise above the rest. On one side, DISTS has a better performance with small images, on the other side, PIM performs better with big images. It caught our attention that PerceptNet's discrimination ellipse has a radically different orientation. We attribute this to the fact that the chromatic transformations it applies at its first layers, make all the images greener as shown in Figure 12.\nGetting into Table III, it is not surprising to see that the ordering of the geometric transformations does not depend on the dataset chosen for the human observers. Regarding the metrics' performance, PerceptNet is the only metric that almost maintains the ordering of being more sensitive to geometric over chromatic transformations. All the metrics are generally more sensitive to YB than RG, which matches human performance. All in all, no metric can be said to have human-like behavior even with the least strict test.\nIn addition, a collateral result that can be obtained from our work is specific invisibility thresholds for each metric. That is thresholds in units of each metric that indicate variations for which the distance is imperceptible to the human eye. These values are shown in Table IV."}, {"title": "VI. CONCLUSSION", "content": "As a complement to the usual reproduction of subjective quality ratings, we argue that the invisibility thresholds of perceptual metrics should also correspond to the invisibility thresholds of humans. This direct comparison (particularly interesting for affine transformations) is a strong test for metrics because human thresholds can be accurately measured with classical psychophysics experiments. This comparison requires (a) data of human thresholds and (b) metric thresholds.\nRegarding human thresholds, we used both classical and explicitly measured values of the thresholds obtained using the same kind of natural images employed in perceptual metrics. Whereas for metric thresholds we propose a methodology to assign invisibility regions for any particular metric, which can be expressed in units of the metric themselves or in physical units to facilitate direct comparison with human psychophysics results.\nWe also propose a less restrictive test: instead of reproducing the exact threshold values, we evaluate if the metric just matches the sensitivity ordering. That means, using sensitivity as the inverse of the threshold energy, one can check whether the metrics are more sensitive to one distortion or the other. This ordering can be then compared to the one in humans.\nMaking the demanding comparison between human and metric thresholds for a range of state-of-the-art deep image quality metrics shows that none of the studied metrics (both deep as well as RSMSE and SSIM) succeeds under these criteria: they do not reproduce all the human thresholds for the different affine transformations, and they do not reproduce the order of sensitivities in humans. No metric is capable of reproducing the thresholds of invisibility and human sensibilities.\nThis means that tuning the models exclusively to predict quality ratings may disregard other properties of human vision for instance invariances or invisibility thresholds."}]}