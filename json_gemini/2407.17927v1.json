{"title": "Invariance of deep image quality metrics to affine transformations", "authors": ["Nuria Alabau-Bosque", "Paula Daud\u00e9n-Oliver", "Jorge Vila-Tom\u00e1s", "Valero Laparra", "Jes\u00fas Malo"], "abstract": "Deep architectures are the current state-of-the-art\nin predicting subjective image quality. Usually, these models are\nevaluated according to their ability to correlate with human\nopinion in databases with a range of distortions that may appear\nin digital media. However, these oversee affine transformations\nwhich may represent better the changes in the images actually\nhappening in natural conditions. Humans can be particularly\ninvariant to these natural transformations, as opposed to the\ndigital ones.\nIn this work, we evaluate state-of-the-art deep image quality\nmetrics by assessing their invariance to affine transformations,\nspecifically: rotation, translation, scaling, and changes in spectral\nillumination. We propose a methodology to assign invisibility\nthresholds for any perceptual metric. This methodology involves\ntransforming the distance measured by an arbitrary metric to a\ncommon distance representation based on available subjectively\nrated databases. We psychophysically measure an absolute de-\ntection threshold in that common representation and express it\nin the physical units of each affine transform for each metric. By\ndoing so, we allow the analyzed metrics to be directly comparable\nwith actual human thresholds. We find that none of the state-of-\nthe-art metrics shows human-like results under this strong test\nbased on invisibility thresholds.\nThis means that tuning the models exclusively to predict the\nvisibility of generic distortions may disregard other properties of\nhuman vision as for instance invariances or invisibility thresholds.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep architectures are the current state-of-the-art in predict-\ning subjective image quality. These models, usually referred to\nas perceptual metrics, are often used as measures to optimize\nother models [1], [2]. This implies that assessing their perfor-\nmance can be critical in a wide range of applications. Usually,\nthese models are evaluated (or even tuned) according to their\nability to correlate with human opinion in databases including\na wide range of generic distortions [3], [4]. In fundamental\nterms, this means trying to predict the visibility of generic\ndistortions. However, focus on generic distortions may be a\nproblem to take into account other relevant phenomena [5],\nand human vision may also be described in terms of perceptual\nconstancies or invariances [6], [7], [8], [9], [10], [11].\nIn this regard, considering the structural analysis of dis-\ntortions and high-level interpretation of human vision, people\nhave suggested that humans are mainly invariant to transfor-\nmations which do not change the structure of the scenes [12].\nAffine transformations (rotations, translations, scalings, and\nchanges in spectral illumination) are examples of distortions\nthat do not change the structure of the scene. Therefore,\nhumans should be relatively tolerant to them, and the corre-\nsponding models to assess image similarity should be invariant\nto these transformations too.\nIn fact, the spirit of the influential SSIM was focused on\nmeasuring changes of structure [13] to achieve invariances to\nirrelevant transformations [14]. Moreover, Wang and Simon-\ncelli [12] decomposed generic distortions into structural and\nnon-structural components so that the part not affecting the\nstructure (e.g. affine transformations) could be processed, and\nweighted, differently. On the other hand, metrics with bio-\ninspired, explainable architecture [15], [16], [17], [18], [19],\n[20], [5], [4] work in multi-scale / multi-orientation domains\nwhere invariances can be introduced by means of appropriate\npoolings as in the scattering transformations [21]. This sort of\npoolings are thought to happen in the visual brain leading to\ninvariances and texture metamers [22].\nHowever, current state-of-the-art deep-architectures for im-\nage quality [3] do not address the invariance problem in any\nway, while examples that try to apply the SSIM concept in\ndeep-nets [23] do not use invariances in simple or explicit\nways. As a result, the analysis of invariance in deep image\nquality metrics remains an open question.\nIn this work, we compare the ability of metrics to be\ninvariant to affine transformations in the same way as humans\nare. In particular, we propose to evaluate the metrics from\nthe point of view of human detection thresholds: by assessing\nthe invariance of the metrics to image transformations that\nare irrelevant (or invisible) to human observers. For example,\nthe classical literature on visual thresholds determines the\nintensity of certain affine transformations which is invisible\nfor humans [24], [25], [26], [27]. The sizes of invisibility\nthresholds are related to the more general concept of invari-\nance to transformations. By definition, transformations whose\nintensity is below the threshold are invisible to the observer.\nThen one can say that, in this region, the observer is invariant\nto the transform. Here we propose a methodology that allows\nus to assign specific invisibility thresholds per metric. This\nproposal uses transduction functions to a common internal\nrepresentation based on a subjectively rated database and a\npsychophysically measured threshold in this representation.\nThen, we can (1) assess if the thresholds for the metrics\nare comparable to those found for human observers, and\n(2) assess if the sensitivities of a metric for the different\ndistortions follow the same order as the human sensitivities\n(for instance humans are more sensible to rotation changes\nthan to illumination changes). This proposed evaluation of"}, {"title": "II. PROPOSED METHODOLOGY: THRESHOLDS AND\nSENSITIVITIES", "content": "Given an original image, i, it can be distorted through\ncertain transform whose intensity depends on a parameter\n\u03b8, \u03af' = To(i). Image quality metrics are models that try\nto reproduce the subjective sensation of distance between\nthe original image and the distorted image, d(i, i'). Human\nobservers are unable to distinguish between i and i', i.e. they\nare invariant to transform To, if its intensity is below a (human)\nthreshold, $ \\theta_H $.\nWhile invariance thresholds in human observers, $ \\theta_H $, are\neasy to understand and measure [24], [25], [26], [27], they\nare not obvious to define in artificial models of perceptual dis-\ntance. The reason is simple: for the usual image quality models\nany non-zero image distortion leads to non-zero variations in\nthe distance. In this situation where artificial distances are\nreal-valued functions one should define a value, the threshold\ndistance, $D_T$, below which the difference between the images\ncould be disregarded (or taken as zero). Once this threshold\ndistance is available (eventually in a common scale for all\nmetrics) one can translate this threshold to the axis that\nmeasures the distortion intensity, 0, and hence obtain the\nthreshold of the metric, $ \\theta_M $, in the same units that has been\nmeasured for humans, $ \\theta_H $. See an illustration of this concept\nin Fig. 1.\nThe diagram in Fig. 1 displays the relation between the\nphysical description of the intensity of certain distortions\nin abscissas (the parameter 0, e.g. the angle in a rotation\ntransform), and the common internal description of the per-\nceived distance in ordinates (the distance D, e.g. the nor-\nmalized Mean Opinion Score units explained below). This\nrelation between the physical description of the intensity and\ncommon perceptual distance is what we call transduction\nfunction. The example displays three transduction functions,\n$D_M = g_M(\\theta)$, with M = 1,2,3, in blue, red, and green,\nfor the three corresponding metrics. Transduction functions\nare monotonic: the application of a progressively increasing\ndistortion along the abscissas leads to monotonic increments\nin distances in ordinates. The threshold distance, $D_T$, in the\ninternal representation is plotted in orange in Fig. 1. This\nthreshold may be uncertain (as represented by the central\nvalue, solid line, and the quartile limits, in dashed lines), but\nthe empirical transduction functions can be used to put this\ninternal threshold back in the axis that describes the distortion\nintensity: $ \\theta_M = g^{-1}_M(D_T) $. In this way, one can check if\nthe actual invisibility threshold measured for humans (black\nline) is consistent with the threshold interval deduced for each\nmetric. In our illustration, the metric 2 (red line) is the only\none compatible with human behavior.\nUsing the above, on the one hand, we propose to evaluate\nthe alignment between the metric models and human observers\nby comparing $ \\theta_M $ versus $ \\theta_H $. This is a strict comparison\nthat only depends on the experimental uncertainty of the"}, {"title": "A. Transduction: response and equalization", "content": "The response function of a metric, $d_M$, to a certain image\ntransform, $T_\\theta$, is just the average over a set of images, ${i_s}^S_{s=1}$,\nof the distances for the distorted images for different transform\nintensities:\n$ d_M(\\theta) = \\frac{1}{S} \\sum^S_{s=1} d_M (i, T_\\theta (i)) $\n(1)\nwhich can be empirically computed from controlled distortions\nof the images of the dataset. Of course, when considering M\ndifferent metrics, their response functions, $d_M$, are not given\nin a common scale.\nIn this work, we propose to use auxiliary empirical data\nto determine a common distance scale, D, for any metric.\nIn particular, subjectively rated image quality databases (e.g.\nTID [31]) consist of pairs of original and distorted images,\n${(i_p, i'_p)}^P_{p=1}$, with associated subjective scores, the so-called\nMean Opinion Scores, ${DMOS_p}^P_{p=1}$. The databases contain\na wide set of generic distortions of different intensities thus\nranging from invisible distortions to highly noticeable dis-\ntortions. In this setting, DMOS values in the database can\nbe normalized to be in the [0,1] range. In this way, the\nextreme values of the normalized DMOS represent an invisible\ndistortion and the biggest subjective distortion in the database\nrespectively. Therefore, if the range of distortions in the\ndatabase is wide, the variations induced by $T_\\theta$ will be within\nthe limits of the normalized DMOS, and hence, it can be used\nto set the common distance scale D = norm DMOS \u2208 [0, 1].\nUsing the normalized DMOS values and the image pairs of\na large subjectively rated database one can fit an equalization\nfunction, $f_M$, for each metric to transform the non-scaled\nresponse $d_M$ into the common scale D:\n$D = f_M(d_M(i,i')) = a_M . d_M (i, i')^{b_M} $\n(2)"}, {"title": "B. Human thresholds", "content": "In this section, we measure the human thresholds from two\ndifferent points of view. First, we use distorted images from\na subjectively rated database to measure from the internal\ncommon representation. Then, we measure it from the internal\nphysical representation using natural stimuli.\nThis is the distance in normalized DMOS units from which\nhumans can't tell the difference between i and i' for low\ndistortion intensities 0. For that, we need a database with\nratings and opinions of observers, for which we can assign a\nthreshold value of invisibility. In this case, we use the TID13\ndatabase. [31].\nThe value of Dr could be roughly estimated by visual\ninspection of the images presented in Appendix A: images\nwith low values of normalized DMOS (below 0.3) cannot\nbe discriminated from the original, while images with big\nnormalized DMOS (above 0.6) are clearly distinct from the\noriginal. However, the more accurate estimation of such a\nthreshold is obtained from the psychometric function in a\nconstant stimulus experiment [29] applied to other fields\nwithin computer vision, such as [36], [37], [38], [39], [40]\nIn this kind of experiments, given a set of distorted images\none computes the probability that an observer sees some\ndistorted image as different from the original, i.e. P(D >\n$D_T$). This is done by using the two-alternative forced choice\nparadigm (2AFC): by randomly presenting the observer each\ndistorted image together with the original so that the observer\nis forced to choose the distorted one. This is repeated R times,\nso the probability, P(D > $D_T$), is given by the number of\ncorrect responses over R. Note that if D < $D_T$, the observer\nwill not see the difference and the probability of correct answer\nwill be 0.5. On the other extreme, if D \u226b $D_T$, the answer will\nbe obvious for the observer, and the probability of a correct\nanswer will be 1. As a result, the threshold can be defined as\nthe point where P(D > $D_T$) = 0.75.\nWe look for the optimal threshold, $D_T$, and slope, k,\nthat better fit the experimental data using the following sig-\nmoid [29]:\n$p(x) = \\frac{1}{2} + \\frac{1}{2(1 + e^{-k(D-D_T)})} $\n(4)\nNote that this expression enforces that for extremely distorted\nimages $lim_{D \\to \\infty} p(D > D_T) $ = 1, and that the probability at the\nthreshold, D = $D_T$ is 0.75, as required.\nIn our experiment, the psychometric function has been\nevaluated in 20 different values of D, repeated 15 times for 5\nobservers, resulting in 20 \u00d7 15 \u00d7 5 = 1500 forced choices in\ntotal. The fitted psychometric function is shown in Figure 3\nand shows that the value of the threshold for humans in the\ncommon internal distance representation is\n$D_T$ = 0.44\u00b10.05\nwhere the corresponding quartiles give the uncertainty. This\nthreshold corresponds to the orange lines in Figure 1."}, {"title": "D. Sensitivities of metrics and humans for the different dis-\ntortions", "content": "Taking into account that the methodology proposed above\nis particularly demanding, we propose another test. Having\ncalculated the thresholds for metrics and humans, we can\ndefine sensitivities for both and compare them. However, this\ncomparison will be qualitative, not quantitative, so this test is\nless demanding for metrics.\nThe sensitivity for a small distortion is usually defined as\nthe inverse of the energy required to be above the invisibility\nthreshold (Eq. 5), i.e, expressing the distortion $\\delta$ in RMSE\nunits. In the case of metrics, this general definition reduces\nto the derivative of the transduction function concerning the\nenergy of the distortion, i.e. its slope.\n$S = \\frac{1}{|i - T_{\\theta_{H,M}} (i)|^2} $\n(5)\nIn this way, we can obtain the sensitivity of a metric for each\naffine transform, order them and check if the order matches\nthe human ones. A real example of this can be found in\nFigure 7 (the rest of the figures can be found in Appendix\n??), where the vertical lines represent the human thresholds\nexpressed in terms of energy and the different curves describe\nthe behavior of that metric for the various transformations.\nGiven two curves, the most sensitive will be the one with a\nhigher slope.\nIn Figure 7, the human thresholds indicate that the human\nsensitivity order (indicated by the vertical lines) is: scale, trans-\nlation, rotation, RG, and YB illuminants. However, the metric\nshown returns the following order (given by the slopes of the\ncurves): scale, rotation, translation, YB, and RG illuminants.\nEven if the ordering doesn't strictly match, it is to be noted\nthat this metric maintains more sensitivity to geometric than\nchromatic distortions."}, {"title": "III. EXPERIMENTAL SETTINGS", "content": "In this section, we review the selected affine transforma-\ntions, models, and databases that we will use in the experi-\nments. Code available to test other metrics publicly 2\nAn affine transformation or affine application (also called\naffinity) between two affine spaces is a transformation that\nsatisfies Equation 6.\nF : \u03c5 - \u039c\u03c5 + b\n(6)"}, {"title": "IV. RESULTS", "content": "Table II summarizes the numerical results of the experi-\nments. For each geometric affine transformation, we present\nthe discrimination thresholds per metric, $ \\theta_M $. In the case of\nilluminant changes, we show the errors made with respect to\nthe interpolated MacAdam and the experimentally fitted el-\nlipses. As the ellipses are very similar (Figure 6), the errors are\nvery similar. In those where they did not coincide completely,\nthe mean of the errors has been calculated and marked with\nan asterisk (*) in the Table. The Figures from which these\nnumerical results are derived are given in Appendix B and\nAppendix C to avoid cluttering the main text.\nAs we are considering both the human thresholds extracted\nfrom the literature, $ \\theta_H $, and the threshold obtained by our-\nselves with natural images, $ \\theta_H^N $, a result is marked in bold if\n$ \\theta_M $ falls within $ \\theta_H $ and is underlined if it falls within $ \\theta_H^N $.\nNote that a result can be marked both ways at the same time.\nA metric with many highlighted boxes will be a metric with a\ngood performance from the point of view of human invariance.\nThe results from Table II show that following our strong\ninvariance criteria, there is no clear winner. No model shows\nthe required robustness to affine transformations nor type of\nstimuli.\nThe comparison of the order between the human thresholds\nand the sensitivities of the metrics can be seen in Table III,\nwhere even if no metric shows complete human behavior, most\nof them maintain the chromatic ordering."}, {"title": "V. DISCUSSION", "content": "As we have seen in Table II, bearing in mind that no model\nis highlighted for all related transformations, the conclusion is\nthat there is no metric that behaves, in terms of invariance, like\nthe human being. We can dissect the behavior for the different\naffine transformation as follows:\nTranslation: There is no particular model that shows\nhuman-like translation invariance but DISTS and PIM are\nthe best in this regard."}, {"title": "VI. CONCLUSSION", "content": "As a complement to the usual reproduction of subjective\nquality ratings, we argue that the invisibility thresholds of\nperceptual metrics should also correspond to the invisibility\nthresholds of humans. This direct comparison (particularly in-\nteresting for affine transformations) is a strong test for metrics\nbecause human thresholds can be accurately measured with\nclassical psychophysics experiments. This comparison requires\n(a) data of human thresholds and (b) metric thresholds.\nRegarding human thresholds, we used both classical and\nexplicitly measured values of the thresholds obtained using the\nsame kind of natural images employed in perceptual metrics.\nWhereas for metric thresholds we propose a methodology to\nassign invisibility regions for any particular metric, which can\nbe expressed in units of the metric themselves or in physical\nunits to facilitate direct comparison with human psychophysics\nresults.\nWe also propose a less restrictive test: instead of reproduc-\ning the exact threshold values, we evaluate if the metric just\nmatches the sensitivity ordering. That means, using sensitivity\nas the inverse of the threshold energy, one can check whether\nthe metrics are more sensitive to one distortion or the other.\nThis ordering can be then compared to the one in humans.\nMaking the demanding comparison between human and\nmetric thresholds for a range of state-of-the-art deep image\nquality metrics shows that none of the studied metrics (both\ndeep as well as RSMSE and SSIM) succeeds under these\ncriteria: they do not reproduce all the human thresholds for\nthe different affine transformations, and they do not reproduce\nthe order of sensitivities in humans. No metric is capable of\nreproducing the thresholds of invisibility and human sensibil-\nities.\nThis means that tuning the models exclusively to predict\nquality ratings may disregard other properties of human vision\nfor instance invariances or invisibility thresholds."}]}