{"title": "A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models", "authors": ["Daking Rai", "Yilun Zhou", "Shi Feng", "Abulhair Saparov", "Ziyu Yao"], "abstract": "Mechanistic interpretability (MI) is an emerging sub-field of interpretability that seeks to understand a neural network model by reverse-engineering its internal computations. Recently, MI has garnered significant attention for interpreting transformer-based language models (LMs), resulting in many novel insights yet introducing new challenges. However, there has not been work that comprehensively reviews these insights and challenges, particularly as a guide for newcomers to this field. To fill this gap, we present a comprehensive survey outlining fundamental objects of study in MI, techniques that have been used for its investigation, approaches for evaluating MI results, and significant findings and applications stemming from the use of MI to understand LMs. In particular, we present a roadmap for beginners to navigate the field and leverage MI for their benefit. Finally, we also identify current gaps in the field and discuss potential future directions.", "sections": [{"title": "Introduction", "content": "In recent years, transformer-based language models (LMs) have achieved remarkable success in a wide range of tasks (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023; Bubeck et al., 2023). Alongside these advancements, there are growing concerns over the safety, alignment, reliability, and robustness of their usage and development (Chang et al., 2024; Yao et al., 2024; Weidinger et al., 2022), especially as they are increasingly implemented in real-world applications. These concerns primarily stem from our limited understanding of these LMs and the difficulty in interpreting their behavior.\nRecently, mechanistic interpretability (MI) has emerged as a promising technique that fills the gap in the research field of interpretability. This is a line of methods that interpret a model by reverse-engineering the underlying computation into human-understandable mechanisms (Olah et al., 2020; Elhage et al., 2021). It has shown promise in providing insights into the functions of LM components (e.g., neurons, attention heads), offering mechanistic explanations for various LM behaviors, and enabling users to leverage the explanations to enhance an LM's utilization (Wang et al., 2022a; Marks et al., 2024; Templeton et al., 2024).\nDespite the promise, however, there are concerns on the scalability and generalizability of MI and its helpfulness in addressing critical problems in AI safety (R\u00e4uker et al., 2023; Casper, 2023).\nObserving these promises and challenges, we aim to provide a comprehensive review of MI in its applications to interpret transformer-based LMs. There exist surveys on relevant topics, but they differ from ours in a number of key aspects. For example, while R\u00e4uker et al. (2023) discussed interpretability approaches broadly for all types of deep neural networks, we target an in-depth review of MI for transformer-based LMs. Similarly, Zhao et al. (2024) presented a broad survey of LM interpretability but did not focus on MI. Concurrent with our work, Ferrando et al. (2024) reviewed techniques developed for understanding the inner workings of transformer-based LMs but did not particularly focus on MI, and Bereska and Gavves (2024) emphasized MI advances for AI safety.\nOur paper is organized as follows. We first present the background knowledge needed to understand this survey (Section 2). Then, we review the key developments in MI for LMs, including the fundamental objects of study, techniques, and evaluation (Sections 3-5). We then particularly present a beginner's roadmap in Section 6, which summarizes actionable items for newcomers to use MI for their benefits. This is then followed by an overview of findings and applications of MI in Section 7. Finally, we conclude the survey by discussing challenges and future work in this field."}, {"title": "Background: Transformer-based LM", "content": "A transformer-based LM (Vaswani et al., 2017) \u041c takes input tokens X = (x1, ..., xn) and outputs a vector in R|V|, a probability distribution over the vocabulary V, to predict the next token Xn+1. The model refines the representation of each token xi layer by layer (Figure 2). In the first layer, h0i is an embedding vector of xi, resulting from a lookup operation in an embedding matrix WE \u2208 R|V|\u00d7d.\nThis representation is then updated layer-by-layer through the calculations of multi-head attention (MHA) and feed-forward (FF) sublayers in each layer, i.e.,\n$h^{l}_{i} = h^{l-1}_{i} + a^{l}_{i}$\n$a^{l}_{i} + f^{l}_{i},$\n(1)\nwhere hlidenotes the representation of token xi at layer l, alidenotes the attention output from the MHA sublayer, and flidenotes the output from the FF sublayer. The sequence of haacross the layers is also referred to as the residual stream (RS) of transformer in literature (Elhage et al., 2021).\nBriefly, the MHA sublayer with H attention heads is implemented via\n$a^{l}_{i} = concat(a^{l,0}_{i},.., a^{l,H}_{i}),$\n(2)\n$a^{l,h}_{i} = softmax\\Big((\\dfrac{(h^{l-1}_{i}W^{l,h}_{Q})(h^{l-1}_{i}W^{l,h}_{K})^T}{\\sqrt{d_{k}}}\\Big)(h^{l-1}_{i}W^{l,h}_{V}),$\n(3)"}, {"title": "Fundamental Objects of Study", "content": "MI is a bottom-up approach that interprets LMs by decomposing them into smaller components and more elementary computations. Following Olah et al. (2020), one of the earliest studies in MI, we categorize research of MI into three areas: the study of features, circuits, and their universality."}, {"title": "Features", "content": "A feature is a human-interpretable input property that is encoded in a model's activation. For instance, a neuron or a set of neurons that consistently activates for French text can be interpreted as a \"French text detector\" feature (Gurnee et al., 2023).\nMI aims to interpret LMs in terms of the independent features or components representing these features. To this end, Elhage et al. (2022b) proposed the linear representation hypothesis, which posits that neural networks have two properties - linearity, i.e. the network's activation space consists of meaningful (linear) vectors, each representing a feature, and decomposability, i.e., network activations can be decomposed and described in terms of these independent features. This hypothesis resonates with earlier research that also shows linearity in word embeddings (e.g. $woman \\approx king - man \\rightarrow queen$) (Mikolov et al., 2013). In Section 7.1, we will review some identified features in LMs and how they are represented."}, {"title": "Circuits", "content": "While the study of features helps us to understand what information is encoded in a model's activations, it does not inform us of how these features are extracted from the input and then processed to implement specific LM behaviors (e.g., reasoning). MI fills this gap by studying \"circuits\", which are meaningful computational pathways that connect features.\nMore formally, if we view an LM M as a computational graph with features as nodes and the weighted connections between them as edges, a circuit is a sub-graph of M responsible for implementing specific LM behaviors (Wang et al., 2022a). Additionally, although circuits were initially defined as connections between features (Olah et al., 2020), subsequent studies have generalized them as connections between the activation outputs of transformer components (Olsson et al., 2022; Wang et al., 2022a). Therefore, we include research on interpreting transformer components, both individually (e.g., interpreting RS in Eq 1 connecting the activations of MHA, FF, and the RS from earlier layers) and across multiple components (e.g., the induction circuit in Figure 3), as circuit study.\nAn example circuit discovered by Elhage et al. (2021) in a toy LM is shown in Figure 3. This is an induction circuit consisting of outputs of two attention heads (previous token head and induction head) as nodes and connection between them as edges of the circuit. The circuit implements the task of detecting and continuing repeated subsequences in the input (e.g., Mr D urs ley was thin and bold. Mr D -> urs), where the previous token head encodes the information \u201curs\u201d follows the \u201cD\u201d token in the RS, which is then read by the induction head to promote \u201curs\u201d as the next token prediction."}, {"title": "Universality", "content": "For any feature or circuit that we have identified in an LM in one task, the critical question arises: Do they similarly exist in other LMs or in other tasks? The investigation into this question has then given rise to the notion of universality, i.e., the extent to which similar features and circuits are formed across different LMs and tasks (Olah et al., 2020; Gurnee et al., 2024). The implications of universal features and circuits can be significant. For instance, many studies (Olsson et al., 2022; Elhage et al., 2022a,b) on features and circuits were"}, {"title": "Techniques", "content": "Next, we review the techniques that have been developed to study the fundamental objects described in Section 3 and to perform MI analysis on LMs."}, {"title": "Logit Lens", "content": "The logit lens (Figure 4) was first introduced by nostalgebraist (2020), which provides insights on how LMs refine their prediction across layers. This approach has been applied to interpret activations in both feature (Geva et al., 2021) and circuit discovery (Lieberum et al., 2023; Wang et al., 2022a). Specifically, the activation hlin each layer l can be projected to the vocabulary space by multiplying it with the unembedding matrix WU, yielding the logits over the vocabulary V. In doing so, tokens with the highest logits can be used to infer what information is encoded in hl.\nThe logit lens can also be used to interpret the weights of transformer components. For example, Geva et al. (2022) used it to understand the role of the FF sublayer in the LM's prediction by projecting the columns of W parameter matrix to the vocabulary space. Dar et al. (2023) applied the technique to project query-key $(W^{l,h}_{Q}W^{l,h}_{K})$ and value-output $(W^{l,h}_{V}W^{O})$ interaction parameter matrices to vocabulary space to study how attention heads transfer and mix information from source tokens to the target token.\nHowever, vocabulary space projection may yield seemingly nonsensical results for some models. For instance, the top-1 projected token for BLOOM (Scao et al., 2022) in many intermediate layers is often the input token (Belrose et al., 2023). The authors posited that this may be caused by the discrepancy between the intermediate layers and the last layer (where WU was trained to apply to). To address it, Belrose et al. (2023); Din et al. (2023); Pal et al. (2023) transformed the intermediate activations to align with the representation space of the final layer."}, {"title": "Probing", "content": "The probing technique (Figure 5) is used extensively in (and also before) MI to investigate whether specific information, such as the part-of-speech linguistic property, is encoded in given intermediate activations (a single neuron or set of neurons) (Conneau et al., 2018; Tenney et al., 2019a,b)."}, {"title": "Sparse Autoencoder (SAE)", "content": "SAEs (Figure 6) serve as an unsupervised technique for discovering features from activations, especially those that demonstrate superposition, a phenomenon in LMs where their d-dimensional representation encodes more than d features (Elhage et al., 2022b; Sharkey et al., 2023; Cunningham et al., 2023; Bricken et al., 2023; Yun et al., 2021). In contrast to dimensionality reduction techniques (e.g., Principal Component Analysis), SAEs seek to embed the activation vectors into a much higher-dimensional space, but with strong sparsity. Specifically, an encoder maps the d-dimensional input into an s-dimensional vector (s > d), which the decoder then maps back to the d-dimension. The encoder and decoder are jointly trained for input reconstruction and sparsity of the s-dimensional representation. This s-dimensional representation, owing to its sparsity, makes the discovery of independent (or monosemantic) features more easily."}, {"title": "Visualization", "content": "Visualization (Figure 7) is employed across various stages of an MI investigation, from generating initial hypotheses to refining them, conducting qualitative analyses, and validating results. For instance, attention patterns are often visualized to understand attention heads (Lieberum et al., 2023; Olsson et al., 2022); a neuron activation across the input text is visualized to identify its functionality (Elhage et al., 2022a; Bricken et al., 2023). While visualization can be highly useful, it requires human effort to interpret results and carries the risks of overgeneralization. Thus, any claims need to be substantiated with further experimentation and analysis."}, {"title": "Automated Feature Explanation", "content": "For feature discovery, human effort is required to annotate neurons with interpretable feature labels based on their activation patterns (Elhage et al., 2022a; Conmy et al., 2024) or the projected tokens when using logits lens (Geva et al., 2022). To reduce human labor, Bills et al. (2023) proposed using LLMs to generate feature labels automatically (Figure 8). Additionally, they also introduced a quantitative automatic explanation score to measure the quality of these explanations, using LLMs to simulate activations based on the automatically generated labels and comparing them with the ground-truth activations."}, {"title": "Knockout / Ablation", "content": "Knockout or ablation (Figure 9) is primarily used to identify components in a circuit that are important to a certain LM behavior. It removes specific components and analyzes the change in the LM behavior, where a significant change suggests importance. There are three main ablation methods: (1) zero: replacing the output of the component with a zero vector (Olsson et al., 2022); (2) mean: replacing the output with the mean value of randomly sampled inputs from the same input distribution (Wang et al., 2022a); (3) resampling: replacing the output with that of another random input (Chan et al., 2022).\nChan et al. (2022) introduced causal scrubbing which employs resampling ablation for hypothesis verification. Rather than a direct ablation, they formalize a hypothesis as (G, I, c), where G is the model's computational graph, I is an interpretation graph reflecting the hypothesis, and c is a function that maps between nodes of I to G. Resampling ablation is then performed on G to validate I. The model's behavior should be preserved when one ablates activations in G with random samples that follow the same hypothesis."}, {"title": "Causal Mediation Analysis (CMA)", "content": "Causal mediation analysis (CMA) is popular for circuit discovery, including two main patching approaches (Figure 10).\nActivation patching localizes important components in a circuit (Vig et al., 2020; Meng et al., 2022), which performs three rounds of model inference. First, the clean run uses a prompt that demonstrates the LM capability in investigation. Then, the corrupt run uses the same prompt, but with a few important tokens corrupted with noise such that the LM fails to demonstrate the capability. Finally, the patch run uses all activation values of the corrupt run, except those of the localized component, which uses the clean run values. The recovery of the lost capability indicates that the patched component is important.\nPath patching localizes important connections between components (Wang et al., 2022a; Goldowsky-Dill et al., 2023). To assess whether the connection between two components, C1and C2, is significant, path patching applies activation patching to the output of C1but only along paths serve as the input to C2. If a change in behavior is observed, then the connection between the two components is considered important.\nCircuit discovery via patching is often computationally inefficient, as one has to iteratively patch each component or connection in an LM to measure its importance. To address it, Conmy et al. (2024) proposed ACDC (Automatic Circuit Discovery) to automate the iterative localization process. Nanda et al. (2022); Kram\u00e1r et al. (2024) proposed attribution patching to approximate activation patching, which requires only two forward passes and one backward pass for measuring all model components. Syed et al. (2023) further extended it to edge attribution patching (EAP), which outperforms ACDC in circuit discovery."}, {"title": "Evaluation", "content": "Existing work has performed two types of evaluation, i.e., intrinsic evaluation that seeks to establish the quality of the explanations on their own, without the context of any particular application, and extrinsic evaluation, which measures the quality of the interpretation by applying the obtained insights to a relevant downstream task. This section will focus on the former, whereas the latter was driven by the applications which we will present in Section 7.6.\nFaithfulness Faithfulness measures the degree to which an MI interpretation accurately reflects the actual decision-making process underlying a model's behavior under study (Jacovi and Goldberg, 2020). For feature discovery, Elhage et al. (2022a) and Bricken et al. (2023) recruited human annotators to rate the interpretation of a feature based on its activations over texts, and Bills et al. (2023) automated it with GPT-4. For circuit discovery, localization is evaluated by comparing the full model vs. the partial model with the localized circuit alone (Olsson et al., 2022; Wang et al., 2022a; Marks et al., 2024), while the explanation of circuit components is often validated by knocking out the component and manually examining its effect."}, {"title": "Completeness and Minimality", "content": "Besides faithfulness, completeness and minimality are also desirable (Wang et al., 2022a). For example, when studying the model's ability to recognize French text, an explanation algorithm may identify a group of neurons N or a circuit C. A complete explanation would mean that no other neurons outside of N are responsible (e.g., highly activated), or no other circuits besides C implement this functionality. It can be measured by ablation-based techniques (Wang et al., 2022a; Marks et al., 2024). Specifically, if the model behavior remains unchanged after removing the neuron or circuit, then the explanation is incomplete. On the other hand, minimality measures whether all parts of the explanation (e.g., N or C) are necessary, often by ablating parts of the explanation and computing the change in model behavior (Wang et al., 2022a)."}, {"title": "Plausibility", "content": "Jacovi and Goldberg (2020) defined plausibility as \u201chow convincing the interpretation is to humans\u201d. In MI, for example, attributing a model behavior to polysemantic neurons may be less plausible as compared to monosemantic ones (Bricken et al., 2023; Marks et al., 2024), as humans may have trouble understanding the former during the evaluation."}, {"title": "A Beginner's Roadmap to MI", "content": "A key motivation for this survey is to provide a friendly guide for researchers and developers interested in MI to quickly pick up the field. To this end, we provide a beginner's roadmap in Figure 11."}, {"title": "Feature Study", "content": "The study of features can be broadly divided into two categories, i.e., targeted feature discovery, which aims to determine if a pre-defined feature (e.g., a \"French text\u201d feature) is encoded in the activations of LMs, and open-ended feature discovery, which aims to discover all interpretable features in the activations of LMs.\nProbing, logit lens, and visualization are common techniques used for targeted feature discovery. For probing, a labeled dataset indicating the presence or absence of the target feature is required to train the probe. If the trained probe achieves high accuracy on a held-out test set, it can be inferred that the activation encodes the target feature. On the other hand, the logit lens or visualization requires a human explainer to interpret the logit lens projection or the visualization of the highlighted text. Alternatively, LLMs (e.g., GPT-4) can also be employed to automatically interpret the logit lens projection or highlighted text to confirm if the feature is encoded in the activations. Open-ended feature discovery is typically conducted via SAEs and visualization. Interpretation results of these approaches are then passed to human explainers or LLMs (if going with automatic feature explanation) for annotating them with interpretable feature labels, similar to the annotation process in targeted feature discovery.\nFinally, the faithfulness of both categories can be measured by humans based on how interpretable the discovered features are (e.g., \"on a scale of 0-3, rate your confidence in this interpretation\u201d as executed by Bricken et al. (2023)) and using automatic explanation scores proposed by Bills et al. (2023)."}, {"title": "Circuit Study", "content": "Similarly, the study of circuits can be broadly divided into two categories, i.e., interpret an LM behavior (e.g., IOI) and interpret an LM component (e.g., attention head).\nThe first category involves finding a circuit that is responsible for the LM behavior. To do so, we start with (1) localization i.e. identifying all the important components and the edges connecting them via CMA and knockout. Subsequently, the second step is to (2) explain each component and its connection one by one. The functional interpretation of each component or edge can be further divided into two sub-steps \u2014 (2.1) Generating a hypothesis: The behavior of the component under study is analyzed under relevant input using various visualization techniques (e.g., attention visualization) and logit lens vocabulary projection. A human can then generate a plausible hypothesis based on the analysis (e.g., attention head X is hypothesized to function as a copy suppression head, when it was observed to decrease the logit of the token it attends to). (2.2) Validating the hypothesis: The hypothesis can be validated using techniques such as knockout, CMA, and logit lens. For instance, if the hypothesis suggests that attention head X is a copy suppression head, then knocking out attention head X should remove the suppression effect. If the hypothesis is validated, the model component is considered interpreted; otherwise, further analysis and new hypotheses are needed. Finally, the discovered circuit, with or without the second-step explanation, can be evaluated in terms of faithfulness, completeness, and minimality (Section 5). The second category of the circuit study, i.e., interpreting an LM component, follows similar steps but starts with the second step of the first category, given that the target component to interpret has been provided."}, {"title": "Findings and Applications", "content": ""}, {"title": "Findings on Features", "content": "Monosemantics vs. Polysemantics Earlier work studied neurons as a natural candidate for encoding features, leading to the discovery of \u201csentiment neurons\u201d (Radford et al., 2017), \u201cknowledge neurons\u201d (Dai et al., 2022), \u201cBase64 neurons\u201d (Elhage et al., 2022a), \u201cskill neurons\u201d (Wang et al., 2022b), \u201cpositional neurons\u201d (Voita et al., 2023), etc. However, many of these studies found that neurons within LMs are not monosemantic, i.e. they do not activate only for a single feature. In contrast, they are polysemantic, i.e. they activate in response to multiple unrelated features (Elhage et al., 2022a; Gurnee et al., 2023; Elhage et al., 2022b). For instance, a single neuron could activate for both French texts and texts encoded in Base64. Due to its polysemantic nature, mapping a neuron to a specific feature becomes challenging.\nSuperposition The discovery of polysemantic neurons has led to the hypothesis of superposition, i.e., a model can represent a greater number of independent features than the number of available neurons. Specifically, features represented in superposition are encoded in activations by a linear combination of multiple neurons, rather than by a single neuron, at the cost of some interference with each other. This hypothesis has been verified by Elhage et al. (2022b), where the authors showed that when features are sparse, the model tends to encode features in activation space using superposition. To extract features from such representations, SAEs have become a widely adopted approach (Bricken et al., 2023; Sharkey et al., 2023; Riggs, 2023; Cunningham et al., 2023), as discussed in Section 4.3. Early studies on SAEs have shown promising results, with the extracted features from SAEs appearing more interpretable than those from neurons, according to both human analysis and automatic explanation scores (Bricken et al., 2023)."}, {"title": "Findings on Circuits", "content": "Interpreting LM Behaviors Specific circuits have been identified for various LM behaviors"}, {"title": "Interpreting Transformer Components", "content": "The study of circuits has also yielded insights into the functionalities of transformer components.\nThe RS can be viewed as a one-way communication channel that transfers information from earlier to later layers. Furthermore, Elhage et al. (2021) hypothesized that MHA and FF in different layers write their output in different subspaces of the RS, which prevents interference of information. In addition, nostalgebraist (2020) proposed to view the RS as an LM's current \"guess\" for the output, which is iteratively refined layer-by-layer..\nThe MHA sublayers are responsible for moving information between tokens, which enables information from other tokens (i.e., context) to be incorporated into each token's representation. Elhage et al. (2021) showed that each attention head in a layer operates independently and can be interpreted independently. Several MI studies have shown that these attention heads seem to have specialized roles. For instance, \"negative heads\u201d discovered in GPT2-small by McDougall et al. (2023) are responsible for reducing the logit values of the tokens that have already appeared in the context. Other notably identified attention heads include previous token heads (Wang et al., 2022a), duplicate token heads (Wang et al., 2022a), copying heads (Elhage et al., 2021), induction heads (Olsson et al., 2022), and successor heads (Gould et al., 2023).\nThe FF sublayers are attributed for the majority"}, {"title": "Findings on Universality", "content": "Studies on universality have yielded mixed results. Early circuit analyses identified components such as induction heads (Olsson et al., 2022), successor heads (Gould et al., 2023), and duplication heads (Wang et al., 2022a) across multiple LMs. Similarly, Merullo et al. found that different circuits implementing different tasks (IOI and colored objects task) reuse the same components (e.g., induction head), demonstrating universality across tasks. However, Zhong et al. (2024) discovered that two LMs trained with different initialization can develop qualitatively different circuits for the modular addition task. Similarly, Chughtai et al. (2023) found that LMs trained to perform group composition on finite groups with different random weight initializations on the same task do not develop similar representations and circuits. Finally, Gurnee et al. (2024) found that only about 1-5% of neurons from GPT-2 models trained with random initialization exhibit universality. Understanding the degrees of feature and circuit universality and their dependency on various aspects of model training (e.g., initialization, model size, and loss function) remains a crucial open problem."}, {"title": "Findings on Model Capabilities", "content": "In-Context Learning (ICL) ICL is an emergent ability of LLMs that enables them to adapt to new tasks based solely on instructions or a few demonstrations at inference time (Wei et al., 2022). Elhage et al. (2021) studied a simplified case of ICL and discovered an induction circuit composed of attention heads with specialized roles (e.g., induction heads), which were then found to be crucial even for general cases of ICL (Olsson et al., 2022; Bansal et al., 2023). Ren et al. (2024) further investigated few-shot ICL and identified \u201csemantic induction heads\u201d, which, unlike prior induction"}, {"title": "Reasoning", "content": "Stolfo et al. (2023b) studied arithmetic reasoning and found that attention heads are responsible for transferring information from operand and operator tokens to the RS of the answer or output token, with FF modules subsequently calculating the answer token. Dutta et al. (2024) studied chain-of-thought (CoT) multi-step reasoning over fictional ontologies and found that LLMs seem to deploy multiple different neural pathways in parallel to compute the final answer. Concurrently, Rai and Yao (2024) investigated neuron activation (Geva et al., 2022) as a unified lens to explain how CoT elicits arithmetic reasoning of LLMs, including phenomena that were only empirically discussed in prior work (Wang et al., 2023; Ye et al., 2023; Madaan and Yazdanbakhsh, 2022). In particular, the authors found neurons representing various arithmetic reasoning concepts and showed that the activation of these neurons is necessary but not sufficient for an LLM's arithmetic reasoning. Finally, Brinkmann et al. (2024) discovered an interpretable algorithm in LM for the task of pathfinding in trees.\nOthers As listed in Section 7.2, prior work has also studied LM capabilities in tasks such as IOI (Wang et al., 2022a), modular addition (Nanda et al., 2023b), and greater-than operations (Conmy et al., 2024), leading to the discovery of circuits that implement these tasks. Compared with the interpretation of ICL and reasoning, these studies not only justified the rationale of a capability but also revealed its underlying algorithm through circuits."}, {"title": "Findings on Learning Dynamics", "content": "Phase Changes during LM Training Prior studies have observed sudden shifts in LMs' capabilities, called \"phase changes\u201d (Olsson et al., 2022; Power et al., 2022; Wei et al., 2022). These changes are considered key steps during the LM training. MI has been applied to examine the relationship between the emergence of features and circuits and these phase changes. For example, Olsson et al. (2022) found correlations between phase changes and the formation of induction circuits, suggesting that the development of these circuits underlies the phase change. In the task of symbol manipulation, Nanda et al. (2023a); Varma et al. (2023) discovered a similar correlation contributing to LM grokking (Power et al., 2022), a phenomenon of LM generalizing beyond memorization. Chen et al. (2024) found that sudden drops in the loss during training correspond to the acquisition of attention heads that recognize specific syntactic relations. Finally, Huang et al. (2024b) provided a unified explanation for grokking, double descent (Nakkiran et al., 2021), and emergent abilities (Wei et al., 2022) as a competition between memorization and generalization circuits.\nLearning Dynamics during LM Fine-Tuning Prakash et al. (2024) investigated the underlying changes in mechanisms (e.g., task-relevant circuits) to understand performance enhancements in fine-tuned LMs. The authors found that fine-tuning does not fundamentally change the mechanisms but enhances existing ones."}, {"title": "Applications of MI", "content": ""}, {"title": "Model Enhancement", "content": "Knowledge Editing LMs are known to store factual knowledge encountered during pre-training (Petroni et al., 2019; Cohen et al., 2023). For instance, when an LM is prompted with \"The space needle is in the city of\", it may retrieve the stored facts and correctly predict \"Seattle\". However, these stored facts may be incorrect or outdated over time, leading to factually incorrect generation (Cohen et al., 2024). MI has been found to be a helpful tool for addressing the problem, including understanding where and how facts are stored within LMs, how they are recalled during the inference time, and the approaches for knowledge editing (Meng et al., 2022, 2023; Geva et al., 2023; Sharma et al., 2024). For instance, Meng et al. (2022) used path patching to localize components that are responsible for storing factual knowledge, and then edited the fact (e.g., replacing \"Seattle\" with \"Paris\") by only updating the parameters of those components."}, {"title": "AI Safety", "content": "AI safety is an important concern that MI aims to address. At present, the exact role MI can play in addressing AI safety is unclear (Casper, 2023). Within MI, \"enumerative safety\" aims to address Al safety by enumerating all features in LMs and inspecting those related to dangerous capabilities or intentions (Elhage et al., 2022b; Olah and Jermyn, 2023). To this end, Templeton et al. (2024) identified several safety-relevant features that not only activate when the LM exhibits specific behaviors but also causally influence the LM's output; however, the specific circuits that use these features to implement the behavior have not yet been identified. As we have discussed, Geva et al. (2022) encouraged language safety by steering the LM's generation of non-toxic tokens. Finally, insights from circuit studies were used to detect prompt injection (Belrose et al., 2023) and find adversarial examples for the IOI task (Wang et al., 2022a)."}, {"title": "Automated Hypothesis Generation", "content": "At a high level, MI study is a process with two stages: generating hypotheses on the underlying mechanisms of LM behavior and validating them. Although various techniques have automated hypothesis validation (Section 4.5), the generation part is mainly left to humans, a potential scalability bottleneck to LLMs and complex behaviors. To address it, automated hypothesis generation, potentially with humans in the loop, is instrumental."}, {"title": "Studies on Complex Tasks and LLMs", "content": "Current MI studies are mostly performed on simpler tasks, often criticized as \u201cstreetlight interpretability\u201d (Casper, 2023; Wang, 2022). For instance, Wang (2022) intentionally selected the IOI task (Wang et al., 2022a) because it is a simple algorithmic task. Similarly, although a few studies were done on \"production-level\" LMs (Lieberum et al., 2023; Templeton et al., 2024), most still used small LMs, which may have limited generalizability, given the mixed results on universality."}, {"title": "Limitations", "content": "Our survey paper discusses MI studies conducted only on decoder-only transformer LMs, omitting studies on other transformer LM variants such as encoder-only and encoder-decoder transformer LMs. Additionally, due to space constraints, some techniques may not have been presented with full technical details (e.g., causal scrubbing)."}]}