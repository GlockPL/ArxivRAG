{"title": "A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models", "authors": ["Daking Rai", "Yilun Zhou", "Shi Feng", "Abulhair Saparov", "Ziyu Yao"], "abstract": "Mechanistic interpretability (MI) is an emerging sub-field of interpretability that seeks to understand a neural network model by reverse-engineering its internal computations. Recently, MI has garnered significant attention for interpreting transformer-based language models (LMs), resulting in many novel insights yet introducing new challenges. However, there has not been work that comprehensively reviews these insights and challenges, particularly as a guide for newcomers to this field. To fill this gap, we present a comprehensive survey outlining fundamental objects of study in MI, techniques that have been used for its investigation, approaches for evaluating MI results, and significant findings and applications stemming from the use of MI to understand LMs. In particular, we present a roadmap for beginners to navigate the field and leverage MI for their benefit. Finally, we also identify current gaps in the field and discuss potential future directions.", "sections": [{"title": "Introduction", "content": "In recent years, transformer-based language models (LMs) have achieved remarkable success in a wide range of tasks. Alongside these advancements, there are growing concerns over the safety, alignment, reliability, and robustness of their usage and development, especially as they are increasingly implemented in real-world applications. These concerns primarily stem from our limited understanding of these LMs and the difficulty in interpreting their behavior.\nRecently, mechanistic interpretability (MI) has emerged as a promising technique that fills the gap in the research field of interpretability. This is a line of methods that interpret a model by reverse-engineering the underlying computation into human-understandable mechanisms. It has shown promise in providing insights into the functions of LM components (e.g., neurons, attention heads), offering mechanistic explanations for various LM behaviors, and enabling users to leverage the explanations to enhance an LM's utilization. Despite the promise, however, there are concerns on the scalability and generalizability of MI and its helpfulness in addressing critical problems in AI safety.\nObserving these promises and challenges, we aim to provide a comprehensive review of MI in its applications to interpret transformer-based LMs. There exist surveys on relevant topics, but they differ from ours in a number of key aspects. For example, while R\u00e4uker et al. (2023) discussed interpretability approaches broadly for all types of deep neural networks, we target an in-depth review of MI for transformer-based LMs. Similarly, Zhao et al. (2024) presented a broad survey of LM interpretability but did not focus on MI. Concurrent with our work, Ferrando et al. (2024) reviewed techniques developed for understanding the inner workings of transformer-based LMs but did not particularly focus on MI, and Bereska and Gavves (2024) emphasized MI advances for AI safety.\nOur paper is organized as follows. We first present the background knowledge needed to understand this survey (Section 2). Then, we review the key developments in MI for LMs, including the fundamental objects of study, techniques, and evaluation (Sections 3-5). We then particularly present a beginner's roadmap in Section 6, which summarizes actionable items for newcomers to use MI for their benefits. This is then followed by an overview of findings and applications of MI in Section 7. Finally, we conclude the survey by discussing challenges and future work in this field."}, {"title": "Background: Transformer-based LM", "content": "A transformer-based LM  takes input tokens  and outputs a vector in , a probability distribution over the vocabulary , to predict the next token . The model refines the representation of each token layer by layer (Figure 2). In the first layer, is an embedding vector of , resulting from a lookup operation in an embedding matrix . This representation is then updated layer-by-layer through the calculations of multi-head attention (MHA) and feed-forward (FF) sublayers in each layer, i.e.,\n$h_i^l = h_i^{l-1} + a_i^l$\n$a_i^l + f_i^l,$\nwhere denotes the representation of token at layer , is the attention output from the MHA sublayer, and is the output from the FF sublayer. The sequence of across the layers is also referred to as the residual stream (RS) of transformer in literature.\nBriefly, the MHA sublayer with H attention heads is implemented via\n$a_i^l = concat(a_{i,1}^l,..., a_{i,H}^l),$\n$a_{i,h}^l = softmax\\left(\\frac{(h_i^{l-1} W_Q^{l,h})(W_K^{l,h})^T}{\\sqrt{d_k}}\\right) (h_i^{l-1} W_V^{l,h}),$"}, {"title": "Fundamental Objects of Study", "content": "MI is a bottom-up approach that interprets LMs by decomposing them into smaller components and more elementary computations. Following Olah et al. (2020), one of the earliest studies in MI, we categorize research of MI into three areas: the study of features, circuits, and their universality."}, {"title": "Features", "content": "A feature is a human-interpretable input property that is encoded in a model's activation. For instance, a neuron or a set of neurons that consistently activates for French text can be interpreted as a \"French text detector\" feature. MI aims to interpret LMs in terms of the independent features or components representing these features. To this end, Elhage et al. (2022b) proposed the linear representation hypothesis, which posits that neural networks have two properties - linearity, i.e. the network's activation space consists of meaningful (linear) vectors, each representing a feature, and decomposability, i.e., network activations can be decomposed and described in terms of these independent features. This hypothesis resonates with earlier research that also shows linearity in word embeddings (e.g. ).\nIn Section 7.1, we will review some identified features in LMs and how they are represented."}, {"title": "Circuits", "content": "While the study of features helps us to understand what information is encoded in a model's activations, it does not inform us of how these features are extracted from the input and then processed to implement specific LM behaviors (e.g., reasoning). MI fills this gap by studying \"circuits\", which are meaningful computational pathways that connect features.\nMore formally, if we view an LM M as a computational graph with features as nodes and the weighted connections between them as edges, a circuit is a sub-graph of M responsible for implementing specific LM behaviors. Additionally, although circuits were initially defined as connections between features, subsequent studies have generalized them as connections between the activation outputs of transformer components. Therefore, we include research on interpreting transformer components, both individually (e.g., interpreting RS in Eq 1 connecting the activations of MHA, FF, and the RS from earlier layers) and across multiple components (e.g., the induction circuit in Figure 3), as circuit study.\nAn example circuit discovered by Elhage et al. (2021) in a toy LM is shown in Figure 3. This is an induction circuit consisting of outputs of two attention heads (previous token head and induction head) as nodes and connection between them as edges of the circuit. The circuit implements the task of detecting and continuing repeated subsequences in the input (e.g., Mr D urs ley was thin and bold. Mr D -> urs), where the previous token head encodes the information \"urs\u201d follows the \u201cD\u201d token in the RS, which is then read by the induction head to promote \"urs\u201d as the next token prediction."}, {"title": "Universality", "content": "For any feature or circuit that we have identified in an LM in one task, the critical question arises: Do they similarly exist in other LMs or in other tasks? The investigation into this question has then given rise to the notion of universality, i.e., the extent to which similar features and circuits are formed across different LMs and tasks. The implications of universal features and circuits can be significant. For instance, many studies on features and circuits were"}, {"title": "Techniques", "content": "Next, we review the techniques that have been developed to study the fundamental objects described in Section 3 and to perform MI analysis on LMs."}, {"title": "Logit Lens", "content": "The logit lens (Figure 4) was first introduced by nostalgebraist (2020), which provides insights on how LMs refine their prediction across layers. This approach has been applied to interpret activations in both feature and circuit discovery. Specifically, the activation in each layer can be projected to the vocabulary space by multiplying it with the unembedding matrix , yielding the logits over the vocabulary V. In doing so, tokens with the highest logits can be used to infer what information is encoded in .\nThe logit lens can also be used to interpret the weights of transformer components. For example, Geva et al. (2022) used it to understand the role of the FF sublayer in the LM's prediction by projecting the columns of parameter matrix to the vocabulary space. Dar et al. (2023) applied the technique to project query-key and value-output interaction parameter matrices to vocabulary space to study how attention heads transfer and mix information from source tokens to the target token.\nHowever, vocabulary space projection may yield seemingly nonsensical results for some models. For instance, the top-1 projected token for BLOOM in many intermediate layers is often the input token. The authors posited that this may be caused by the discrepancy between the intermediate layers and the last layer (where was trained to apply to). To address it,"}, {"title": "Probing", "content": "The probing technique (Figure 5) is used extensively in (and also before) MI to investigate whether specific information, such as the part-of-speech linguistic property, is encoded in given intermediate activations (a single neuron or set of neurons)"}, {"title": "Sparse Autoencoder (SAE)", "content": "SAEs (Figure 6) serve as an unsupervised technique for discovering features from activations, especially those that demonstrate superposition, a phenomenon in LMs where their d-dimensional representation encodes more than d features. In contrast to dimensionality reduction techniques (e.g., Principal Component Analysis), SAEs seek to embed the activation vectors into a much higher-dimensional space, but with strong sparsity. Specifically, an encoder maps the d-dimensional input into an s-dimensional vector (s > d), which the decoder then maps back to the d-dimension. The encoder and decoder are jointly trained for input reconstruction and sparsity of the s-dimensional representation. This s-dimensional representation, owing to its sparsity, makes the discovery of independent (or monosemantic) features more easily."}, {"title": "Visualization", "content": "Visualization (Figure 7) is employed across various stages of an MI investigation, from generating initial hypotheses to refining them, conducting qualitative analyses, and validating results. For instance, attention patterns are often visualized to understand attention heads; a neuron activation across the input text is visualized to identify its functionality. While visualization can be highly useful, it requires human effort to interpret results and carries the risks of overgeneralization. Thus, any claims need to be substantiated with further experimentation and analysis."}, {"title": "Automated Feature Explanation", "content": "For feature discovery, human effort is required to annotate neurons with interpretable feature labels based on their activation patterns or the projected tokens when using logits lens. To reduce human labor, Bills et al. (2023) proposed using LLMs to generate feature labels automatically. Additionally, they also introduced a quantitative automatic explanation score to measure the quality of these explanations, using LLMs to simulate activations based on the automatically generated labels and comparing them with the ground-truth activations."}, {"title": "Knockout / Ablation", "content": "Knockout or ablation (Figure 9) is primarily used to identify components in a circuit that are impor-"}, {"title": "Causal Mediation Analysis (CMA)", "content": "Causal mediation analysis (CMA) is popular for circuit discovery, including two main patching approaches (Figure 10).\nActivation patching localizes important components in a circuit, which performs three rounds of model inference. First, the clean run uses a prompt that demonstrates the LM capability in investigation. Then, the corrupt run uses the same prompt, but with a few important tokens corrupted with noise such that the LM fails to demonstrate the capability. Finally, the patch run uses all activation values of the corrupt run, except those of the localized component, which uses the clean run values. The recovery of the lost capability indicates that the patched component is important.\nPath patching localizes important connections between components. To assess whether the connection between two components, and , is significant, path patching applies activation patching to the output of but only along paths serve as the input to . If a change in behavior is observed, then the connection between the two components is considered important.\nCircuit discovery via patching is often computationally inefficient, as one has to iteratively patch each component or connection in an LM to measure its importance. To address it, Conmy et al. (2024) proposed ACDC (Automatic Circuit Discovery) to automate the iterative localization process. Nanda et al. (2022); Kram\u00e1r et al. (2024) proposed attribution patching to approximate activation patching, which requires only two forward passes and one backward pass for measuring all model components. Syed et al. (2023) further extended it to edge attribution patching (EAP), which outperforms ACDC in circuit discovery."}, {"title": "Evaluation", "content": "Existing work has performed two types of evaluation, i.e., intrinsic evaluation that seeks to establish the quality of the explanations on their own, without the context of any particular application, and extrinsic evaluation, which measures the quality of the interpretation by applying the obtained insights to a relevant downstream task. This section will focus on the former, whereas the latter was driven by the applications which we will present in Section 7.6."}, {"title": "Faithfulness", "content": "Faithfulness measures the degree to which an MI interpretation accurately reflects the actual decision-making process underlying a model's behavior under study. For feature discovery, Elhage et al. (2022a) and Bricken et al. (2023) recruited human annotators to rate the interpretation of a feature based on its activations over texts, and Bills et al. (2023) automated it with GPT-4. For circuit discovery, localization is evaluated by comparing the full model vs. the partial model with the localized circuit alone, while the explanation of circuit components is often validated by knocking out the component and manually examining its effect."}, {"title": "Completeness and Minimality", "content": "Besides faithfulness, completeness and minimality are also desirable. For example, when studying the model's ability to recognize French text, an explanation algorithm may identify a group of neurons N or a circuit C. A complete explanation would mean that no other neurons outside of N are responsible (e.g., highly activated), or no other circuits besides C implement this functionality. It can be measured by ablation-based techniques. Specifically, if the model behavior remains unchanged after removing the neuron or circuit, then the explanation is incomplete. On the other hand, minimality measures whether all parts of the explanation (e.g., N or C) are necessary, often by ablating parts of the explanation and computing the change in model behavior."}, {"title": "Plausibility", "content": "Jacovi and Goldberg (2020) defined plausibility as \u201chow convincing the interpretation is to humans\". In MI, for example, attributing a model behavior to polysemantic neurons may be less plausible as compared to monosemantic ones, as humans may have trouble understanding the former during the evaluation."}, {"title": "A Beginner's Roadmap to MI", "content": "A key motivation for this survey is to provide a friendly guide for researchers and developers interested in MI to quickly pick up the field. To this end, we provide a beginner's roadmap in Figure 11."}, {"title": "Feature Study", "content": "The study of features can be broadly divided into two categories, i.e., targeted feature discovery, which aims to determine if a pre-defined feature (e.g., a \"French text\u201d feature) is encoded in the activations of LMs, and open-ended feature discovery, which aims to discover all interpretable features in the activations of LMs.\nProbing, logit lens, and visualization are common techniques used for targeted feature discovery. For probing, a labeled dataset indicating the presence or absence of the target feature is required to train the probe. If the trained probe achieves high accuracy on a held-out test set, it can be inferred that the activation encodes the target feature. On the other hand, the logit lens or visualization requires a human explainer to interpret the logit lens"}, {"title": "Circuit Study", "content": "Similarly, the study of circuits can be broadly divided into two categories, i.e., interpret an LM behavior (e.g., IOI) and interpret an LM component (e.g., attention head).\nThe first category involves finding a circuit that is responsible for the LM behavior. To do so, we start with (1) localization i.e. identifying all the important components and the edges connecting them via CMA and knockout. Subsequently, the second step is to (2) explain each component and its connection one by one. The functional interpretation of each component or edge can be further divided into two sub-steps \u2014 (2.1) Generating a hypothesis: The behavior of the component under study is analyzed under relevant input using various visualization techniques (e.g., attention visualization) and logit lens vocabulary projection. A human can then generate a plausible hypothesis based on the analysis (e.g., attention head X is hypothesized to function as a copy suppression head, when it was observed to decrease the logit of the token it attends to). (2.2) Validating the hypothesis: The hypothesis can be validated using techniques such as knockout, CMA, and logit lens. For instance, if the hypothesis suggests that attention head X is a copy suppression head, then knocking out attention head X should remove the suppression effect. If the hypothesis is validated, the model component is considered interpreted; otherwise, further analysis and new hypotheses are needed. Finally, the discovered circuit, with or without the second-step explanation, can be evaluated in terms of faithful-"}, {"title": "Findings and Applications", "content": ""}, {"title": "Findings on Features", "content": "Monosemantics vs. Polysemantics Earlier work studied neurons as a natural candidate for encoding features, leading to the discovery of \u201csentiment neurons\u201d, \u201cknowledge neurons\u201d, \u201cBase64 neurons\u201d, \u201cskill neurons\u201d, \u201cpositional neurons\u201d, etc. However, many of these studies found that neurons within LMs are not monosemantic, i.e. they do not activate only for a single feature. In contrast, they are polysemantic, i.e. they activate in response to multiple unrelated features. For instance, a single neuron could activate for both French texts and texts encoded in Base64. Due to its polysemantic nature, mapping a neuron to a specific feature becomes challenging."}, {"title": "Superposition", "content": "The discovery of polysemantic neurons has led to the hypothesis of superposition, i.e., a model can represent a greater number of independent features than the number of available neurons. Specifically, features represented in superposition are encoded in activations by a linear combination of multiple neurons, rather than by a single neuron, at the cost of some interference with each other. This hypothesis has been verified by Elhage et al. (2022b), where the authors showed that when features are sparse, the model tends to encode features in activation space using superposition. To extract features from such representations, SAEs have become a widely adopted approach as discussed in Section 4.3. Early studies on SAEs have shown promising results, with the extracted features from SAEs appearing more interpretable than those from neurons, according to both human analysis and automatic explanation scores."}, {"title": "Findings on Circuits", "content": "Interpreting LM Behaviors Specific circuits have been identified for various LM behaviors"}, {"title": "Interpreting Transformer Components", "content": "The study of circuits has also yielded insights into the functionalities of transformer components.\nThe RS can be viewed as a one-way communication channel that transfers information from earlier to later layers. Furthermore, Elhage et al. (2021) hypothesized that MHA and FF in different layers write their output in different subspaces of the RS, which prevents interference of information. In addition, nostalgebraist (2020) proposed to view the RS as an LM's current \"guess\" for the output, which is iteratively refined layer-by-layer..\nThe MHA sublayers are responsible for moving information between tokens, which enables information from other tokens (i.e., context) to be incorporated into each token's representation. Elhage et al. (2021) showed that each attention head in a layer operates independently and can be interpreted independently. Several MI studies have shown that these attention heads seem to have specialized roles. For instance, \"negative heads\u201d discovered in GPT2-small by McDougall et al. (2023) are responsible for reducing the logit values of the tokens that have already appeared in the context. Other notably identified attention heads include previous token heads, duplicate token heads copying heads, induction heads, and successor heads.\nThe FF sublayers are attributed for the majority"}, {"title": "Findings on Universality", "content": "Studies on universality have yielded mixed results. Early circuit analyses identified components such as induction heads, successor heads and duplication heads across multiple LMs. Similarly, Merullo et al. found that different circuits implementing different tasks (IOI and colored objects task) reuse the same components (e.g., induction head), demonstrating universality across tasks. However, Zhong et al. (2024) discovered that two LMs trained with different initialization can develop qualitatively different circuits for the modular addition task. Similarly, Chughtai et al. (2023) found that LMs trained to perform group composition on finite groups with different random weight initializations on the same task do not develop similar representations and circuits. Finally, Gurnee et al. (2024) found that only about 1-5% of neurons from GPT-2 models trained with random initialization exhibit universality. Understanding the degrees of feature and circuit universality and their dependency on various aspects of model training (e.g., initialization, model size, and loss function) remains a crucial open problem."}, {"title": "Findings on Model Capabilities", "content": "In-Context Learning (ICL) ICL is an emergent ability of LLMs that enables them to adapt to new tasks based solely on instructions or a few demonstrations at inference time. Elhage et al. (2021) studied a simplified case of ICL and discovered an induction circuit composed of attention heads with specialized roles (e.g., induction heads), which were then found to be crucial even for general cases of ICL. Ren et al. (2024) further investigated few-shot ICL and identified \u201csemantic induction heads\", which, unlike prior induction\""}, {"title": "Findings on Learning Dynamics", "content": "Phase Changes during LM Training Prior studies have observed sudden shifts in LMs' capabilities, called \"phase changes\u201d . These changes are considered key steps during the LM training. MI has been applied to examine the relationship between the emergence of features and circuits and these phase changes. For example, Olsson et al. (2022) found correlations between phase changes and the formation of induction circuits, suggesting that the development of these circuits underlies the phase change. In the task of symbol manipulation, Nanda et al. (2023a); Varma et al. (2023) discovered a similar correlation contributing to LM grokking (Power et al., 2022), a phenomenon of LM generalizing beyond memorization. Chen et al. (2024) found that sudden drops in the loss during training correspond to the acquisition of attention heads that recognize specific syntactic relations. Finally, Huang et al. (2024b) provided a unified explanation for grokking, double descent, and emergent abilities as a competition between memorization and generalization circuits.\nLearning Dynamics during LM Fine-Tuning Prakash et al. (2024) investigated the underlying changes in mechanisms (e.g., task-relevant circuits) to understand performance enhancements in fine-tuned LMs. The authors found that fine-tuning does not fundamentally change the mechanisms but enhances existing ones."}, {"title": "Applications of MI", "content": ""}, {"title": "Model Enhancement", "content": "Knowledge Editing LMs are known to store factual knowledge encountered during pre-training. For instance, when an LM is prompted with \"The space needle is in the city of\", it may retrieve the stored facts and correctly predict \"Seattle\". However, these stored facts may be incorrect or outdated over time, leading to factually incorrect generation. MI has been found to be a helpful tool for addressing the problem, including understanding where and how facts are stored within LMs, how they are recalled during the inference time, and the approaches for knowledge editing. For instance, Meng et al. (2022) used path patching to localize components that are responsible for storing factual knowledge, and then edited the fact (e.g., replacing \"Seattle\" with \"Paris\") by only updating the parameters of those components."}, {"title": "LM Generation Steering", "content": "LM generation steering involves controlling an LM's output by manipulating its activations at inference time. For instance, Geva et al. (2022) proposed a method to suppress toxic language generation by identifying and manually activating neurons in FF layers responsible for promoting non-toxic or safe words. Similarly, Templeton et al. (2024) identified safety-related features (e.g., unsafe code, gender bias) and manipulated their activations to steer the LM towards"}, {"title": "AI Safety", "content": "AI safety is an important concern that MI aims to address. At present, the exact role MI can play in addressing AI safety is unclear (Casper, 2023). Within MI, \"enumerative safety\" aims to address Al safety by enumerating all features in LMs and inspecting those related to dangerous capabilities or intentions. To this end, Templeton et al. (2024) identified several safety-relevant features that not only activate when the LM exhibits specific behaviors but also causally influence the LM's output; however, the specific circuits that use these features to implement the behavior have not yet been identified. As we have discussed, Geva et al. (2022) encouraged language safety by steering the LM's generation of non-toxic tokens. Finally, insights from circuit studies were used to detect prompt injection (Belrose et al., 2023) and find adversarial examples for the IOI task (Wang et al., 2022a)."}, {"title": "Others", "content": "Insights from MI have also been used for other downstream tasks. Marks et al. (2024) improved the generalization of classifiers by identifying and ablating spurious features that humans consider to be task-irrelevant. Geva et al. (2022) proposed self-supervised early exit prediction for efficient inference, drawing insights from their investigations of the FF sublayers' role in token prediction."}, {"title": "Discussion and Future Work", "content": "Automated Hypothesis Generation At a high level, MI study is a process with two stages: generating hypotheses on the underlying mechanisms of LM behavior and validating them. Although various techniques have automated hypothesis validation (Section 4.5), the generation part is mainly left to humans, a potential scalability bottleneck to LLMs and complex behaviors. To address it, automated hypothesis generation, potentially with humans in the loop, is instrumental."}, {"title": "Studies on Complex Tasks and LLMs", "content": "Current MI studies are mostly performed on simpler tasks, often criticized as \u201cstreetlight interpretability\u201d . For instance, Wang intentionally selected the IOI task because it is a simple algorithmic task. Similarly, although a few studies were done on \"production-level\" LMs, most still used small LMs, which may have limited generalizability, given the mixed results on universality."}, {"title": "Practical Utility", "content": "While people acknowledge the importance of actionable insights to downstream applications, MI studies that highlight such applications, such as adversarial example generation often lack the depth of investigation when they do not compare against alternative methods or baselines. As an exception, Marks et al. (2024) showed that their MI technique improves model generalization better than several baselines."}, {"title": "Standardized Benchmarks and Metrics", "content": "Evaluating interpretability results is inherently challenging due to the lack of ground truth and current MI studies also employ diverse ad hoc evaluation approaches, potentially leading to inconsistent comparisons (Zhang and Nanda, 2023). There are some proposals for standardized evaluation benchmarks. RAVEL evaluates techniques (e.g., SAEs) that disentangle polysemantic neurons into monosemantic features. Tracr converts human-readable RASP programs into transformers to enable ground-truth features and algorithms. However, these proposed benchmarks are still insufficient. For instance, success on synthetic transformers in the Tracr benchmark may not imply that on naturally trained transformers. Thus, more effort is needed to develop standard evaluation techniques and to ensure their wide adoption."}, {"title": "Limitations", "content": "Our survey paper discusses MI studies conducted only on decoder-only transformer LMs, omitting studies on other transformer LM variants such as encoder-only and encoder-decoder transformer LMs. Additionally, due to space constraints, some techniques may not have been presented with full technical details (e.g., causal scrubbing)."}, {"title": "Furthermore, given the fast-paced nature of the field, our survey may not reflect the latest advancements that were published close to or after the survey was conducted. Despite these limitations, however, our survey provides a comprehensive taxonomy of the current state of the MI field as well as a roadmap, specifically curated to help newcomers quickly pick up the field. Both the taxonomy and the roadmap can serve as guides for future researchers interested in encoder-only or encoder-decoder LMs to build up their survey. In addition, we have created a GitHub repository at to sustainably maintain this survey, and we welcome researchers sharing the same interest to add new papers and extend the taxonomy and the roadmap."}]}