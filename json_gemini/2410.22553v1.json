{"title": "ML RESEARCH BENCHMARK", "authors": ["Matthew Kenney"], "abstract": "Artificial intelligence agents are increasingly capable of performing complex tasks across various domains. As these agents advance, there is a growing need to accurately measure and benchmark their capabilities, particularly in accelerating AI research and development. Current benchmarks focus on general machine learning tasks, but lack comprehensive evaluation methods for assessing AI agents' abilities in tackling research-level problems and competition-level challenges in the field of AI. We present the ML Research Benchmark (MLRB), comprising 7 competition-level tasks derived from recent machine learning conference tracks. These tasks span activities typically undertaken by AI researchers, including model training efficiency, pretraining on limited data, domain specific fine-tuning, and model compression. This paper introduces a novel benchmark and evaluates it using agent scaffolds powered by frontier models, including Claude-3 and GPT-40. The results indicate that the Claude-3.5 Sonnet agent performs best across our benchmark, excelling in planning and developing machine learning models. However, both tested agents struggled to perform non-trivial research iterations. We observed significant performance variations across tasks, highlighting the complexity of AI development and the challenges in creating versatile agent scaffolds. While current AI agents can successfully navigate complex instructions and produce baseline results, they fall short of the capabilities required for advanced AI research. The ML Research Benchmark provides a valuable framework for assessing and comparing AI agents on tasks mirroring real-world AI research challenges.", "sections": [{"title": "Introduction", "content": "The rapid advancement of artificial intelligence has led to increasingly capable AI agents that can perform a wide range of tasks. However, accurately measuring and benchmarking the progress of these agents, particularly in the domain of Al research and development, remains a significant challenge. Within this broader context, there is a growing need to evaluate AI agents' ability to accelerate AI research itself [1] While some benchmarks exist for general machine learning tasks [2] [3] there is a lack of comprehensive evaluation methods specifically tailored to assess AI agents' capabilities in tackling research problems and challenges in the field of AI. To address this gap, we present the ML Research Benchmark, a novel benchmark designed to measure the capabilities of AI agents in AI research and development. Our benchmark focuses specifically on competition-level tasks that reflect the current frontiers of machine learning research. ML Research Benchmark is composed of 7 ML conference competition tasks that span the spectrum of activities typically undertaken by AI researchers including pretraining, finetuning, model pruning and compression, and model merging techniques. These tasks are derived from 2023-2024 machine learning conference competition tracks, where top researchers compete to develop state-of-the-art models and datasets. We evaluate our benchmark using agent scaffolds powered by frontier models, including Claude-3 [4] and GPT-40 [5]. Our results demonstrate that the Claude-3.5 Sonnet agent performs best across our benchmark across machine learning model development. This work contributes to the field by providing a robust framework for assessing AI agents' potential to accelerate AI research, offering insights into the current capabilities and limitations of AI models in tackling complex, research-oriented tasks.\nWe present the following contributions to agentic benchmarking:"}, {"title": "Related Works", "content": "There are currently several benchmarks for AI agents, including WebShop [6], Mind2Web [7], WebArena [8], and AgentBench [9] API-Bank [10] and ARC Evals [11]. While these benchmarks are general purpose, we propose a benchmark strictly focused on agent-based AI research. There are also several benchmarks for AI agents performing ML tasks, including HuggingGPT [12], MLAgentBench [3], and MLBench [2] benchmarks. These benchmarks mainly focus on tasks related to machine learning in general. For instance, MLAgentBench focuses on 13 tasks, including tasks on image classification and segmentation, time-series modeling, and text classification. Several of the MLAgentBench tasks focus on canonical machine learning tasks like CIFAR-10 image classification, and classic Kaggle competitions like the Parkinson's-disease regression challenge. While MLAgentBench excels at measuring general machine learning techniques, it does not extend to the day-to-day work of current capabilities researchers. AI Competition Benchmark builds on MLAgentBench with tasks that are considerably harder and focused exclusively on conference competition benchmarks, with the objective of benchmarking agentic progress in frontier model research."}, {"title": "AI Research and Development Agents", "content": "AI agents like AutoGPT [13], SWE-Bench agent [14], and Aider [15] have the capabilities to complete tasks in software development, showing continuous improvement across benchmarks such as HumanEval [16] and SWE-Bench [17]. With respect to AI research and development, machine-learning domain-specific agents have been developed, such as AutoML-GPT [18], MLcopilot [19], HuggingGPT [12], MLAgentBench agent [3], and AI Scientist [20]. These domain-specific agents are provided with additional tools to conduct AI research. We similarly develop a domain-specific AI agent, designed to tackle machine learning tasks by leveraging domain-specific tools for research and development. Our baseline agent is modular, allowing researchers to extend its capabilities."}, {"title": "ML Research Benchmark: Utilizing Conference Competitions to Benchmark Agent Performance", "content": "Machine learning competitions, often hosted at conferences such as Conference on Neural Information Processing Systems (NeurIPS), International Conference on Machine Learning (ICML), and The North American Chapter of the Association for Computational Linguistics (NAACL), have become integral to AI research. These competitions serve as platforms for researchers to demonstrate novel approaches and advance state-of-the-art performance in specific domains. Conference competition tracks offer several advantages for evaluating AI research capabilities:\n\u2022 They focus on emerging challenges in AI subfields.\n\u2022 They provide standardized evaluation frameworks.\n\u2022 They often bridge theoretical advancements with practical applications.\n\u2022 They are challenging tasks for machine learning practitioners.\n\u2022 Winning competitors often publish their results.\nThese characteristics make competition tracks suitable for assessing AI agents' research abilities. The tasks typically require deep knowledge of specific AI subfields, mirroring the expertise expected of human researchers. They often involve complex instructions and multi-faceted evaluation criteria, testing an agent's ability to interpret and execute detailed directives. The ML Research Benchmark leverages these competition-based tasks to evaluate AI agents' capabilities in advanced machine learning research. This approach builds on existing benchmarks that focus on general software development [17] and machine learning techniques [3], offering an assessment of an agent's potential to contribute to AI research and development. By developing this benchmark, we aim to provide a tool for measuring AI agents' potential to accelerate innovation in machine learning and related fields. This approach bridges the gap between general-purpose AI agent evaluations and the specific demands of AI research."}, {"title": "Agent Conference Competition Tasks", "content": "To prompt the agent on ML Research Benchmark tasks, we provide the agent with:"}, {"title": "Defining Tasks", "content": "An inherent limitation of binary benchmarks is that they are quickly saturated and offer no gradient of performance measurement. Competition benchmark tasks are structured to allow for indefinite improvement, aligning with the evolving capabilities of AI agents and models. Consequently, the benchmark mitigates the risk of rapid saturation. In practice, this translates to tasks that, while fundamentally straightforward to complete, become progressively more challenging as the baseline for success is raised, mirroring the dynamic nature of real-world research, where incremental improvements build upon previous advancements."}, {"title": "Tasks Constraints", "content": "We constrain the tasks that the agent has to perform across a number of dimensions. To ensure accessibility and practicality, we have established constraints on computational resources and task duration for our benchmark. Specifically, all training runs must be executable on a single A100 40GB GPU. This constraint reduces computational overhead and forces the agent to develop technical architectural approaches, and removes compute advantages. Further, we constrain the tasks to a 24 hour time window. As agents have been shown to increase performance with scaled inference [21], we add this constraint to time-bound the agents' ability to inference the model indefinitely. Because agents typically take multiple iterations to successfully complete the task, these constraints reduce the computational resources required by the agent."}, {"title": "Tasks Construction", "content": "The benchmark tasks were developed through a systematic process to cover key AI research activities while maintaining practical constraints. We reviewed recent machine learning conference competitions from NeurIPS, ICML, ICLR, and other conferences to identify tasks representing current research challenges. We selected tasks that require both theoretical understanding and practical implementation skills, reflecting typical demands in AI research. The agents are evaluated on the models that they produce. For ease of evaluation, we restrict the evaluation methods to those found in LM-Evaluation Harness. Where there are complementary evaluations, we use LM-Evaluation Harness to evaluate the model. Where there are no complementary evaluations, we provide methods in our Agent-Eval library, which contains methods to measure memory usage, throughput, and other evaluation protocols, as well as task-specific evaluations like BLEU [22] and ROUGE [23] for the math reasoning task."}, {"title": "Task Challenges", "content": "Below, we outline the tasks performed in the ML Research Benchmark:\n1. MiniPile Challenge\n2. LLM-Merging Competition\n3. Edge LLMs Challenge: Compression Track\n4. Edge LLMs Challenge: Training from Scratch Track\n5. ICML 2024 Challenges on Automated Math Reasoning: AutoFormalization Track\n6. 1LLM + 1GPU + 1Day: LLM Efficiency Challenge\n7. BabyLM Challenge"}, {"title": "The MiniPile Challenge for Data-Efficient Language Models [24]", "content": "8.1.1 MiniPile Goal\nThe MiniPile Challenge requires agents to pre-train the best possible language model using the MiniPile dataset. The goal is to create a model that performs well on the SuperGLUE benchmark within a 24-hour time limit. This challenge tests agents' ability to effectively pre-train a language model on a moderate-sized dataset and optimize it for performance on a diverse set of natural language understanding tasks.\n8.1.2 MiniPile Setup\nAgents have the flexibility to choose any model architecture they prefer. They are provided with the AlgorithmicRe- searchGroup/minipile dataset, which includes 1 million training samples, 500 validation samples, and 10,000 test samples. The objective is to produce a Hugging Face model that achieves the highest possible performance on the SuperGLUE benchmark.\n8.1.3 MiniPile Evaluation\nModels produced by the agent are evaluated on the SuperGLUE benchmark, where the average accuracy is taken across A = (ABoolQ + \u0410\u0441\u0432 + \u0410\u0441\u043e\u0440\u0430 + AMultiRC + AReCoRD + ARTE + Awic + Awsc), where each A term represents the accuracy for its respective task (BoolQ, CB, COPA, MultiRC, ReCoRD, RTE, WiC, and WSC)."}, {"title": "NeurIPS 2024 LLM Merging Competition: Building LLMs", "content": "8.2.1 LLM Merging Goal\nThe LLM-Merging Competition challenges agents to create a generalist model by merging expert models to perform optimally on the MMLU benchmark. Agents must use publicly available models up to 8GB in size and adhere to specific merging techniques provided in the example code. This competition tests agents' ability to effectively combine multiple expert models into a single, high-performing generalist model. It emphasizes innovative approaches to model merging and optimization for diverse language tasks.\n8.2.2 LLM Merging Setup\nAgents can choose from a variety of publicly available model weights, including the Llama 2 family, Llama 3 family, Mistral family, FLAN T5 family, and Gemma family, among others. The competition provides validation datasets for CosmosQA and XSUM tasks. The challenge imposes strict rules: no training on MMLU directly, merging/fine-tuning and evaluation must take less than 1 hour, and only open-source data can be used. Agents must use the provided example code for merging models, with specific instructions on code placement and usage.\n8.2.3 LLM Merging Evaluation\nModels that the agent produces are evaluated on average scores across MMLU subcategories."}, {"title": "NeurIPs 2024 Edge LLMs Challenge: Compression Track [25]", "content": "8.3.1 Edge LLM Compression Goal\nThe Edge LLMs Challenge: Compression track focuses on developing compression methods for pre-trained Large Language Models (LLMs) to run on memory-constrained devices. The goal is to compress the microsoft/phi-2 model to run on a device with 12 GB DRAM while maintaining high performance on the MMLU benchmark. The challenge emphasizes finding innovative ways to reduce model size while preserving performance, testing agents' ability to optimize LLMs for edge devices with limited memory.\n8.3.2 Edge LLM Compression Setup\nThe compressed model must be submitted in FP16 or FP32 format, with no quantization allowed. Agents may only perform compression; no training is permitted. Model distillation is not allowed.\n8.3.3 Edge LLM Compression Evaluation\nModels that the agent produces are evaluated on average scores across MMLU subcategories. The average accuracy for MMLU (\u0100MMLU) is calculated as \u0100MMLU = 1=1 Ai, where A\u017c represents the accuracy for each main category (humanities, other, social sciences, stem). Each A\u2081 is calculated as A\u2081 = avg(x1, x2, ..., xn), where x1, x2, ..., Xn are the accuracy scores for individual subjects within each main category"}, {"title": "NeurIPs 2024 Edge LLMs Challenge: Training from Scratch Track [25]", "content": "8.4.1 Edge LLM Training Goal\nThe Edge LLMs Challenge: Training from Scratch track requires agents to train a language model from scratch without using pre-trained LLMs. The goal is to create a model that can run on a device with 1 GB DRAM while performing well on the SuperGLUE benchmark. This challenge tests agents' ability to design and train efficient, high-performing language models from scratch under severe memory constraints, emphasizing innovative approaches to model architecture and training techniques.\n8.4.2 Edge LLM Training Setup\nThe model must be submitted in FP16 or FP32 format, with no quantization allowed. Only C4 and Alpaca datasets are allowed for training and fine-tuning. Agents may not use pre-trained LLMs or quantize the model.\n8.4.3 Edge LLM Training Evaluation\nModels produced by the agent are evaluated on the SuperGLUE benchmark, where the average accuracy is taken across A = (ABoolQ + \u0410\u0441\u0432 + \u0410\u0441\u043e\u0440\u0430 + AMultiRC + ARECORD + ARTE + Awic + Awsc)"}, {"title": "ICML 2024 Challenges on Automated Math Reasoning: AutoFormalization Track [26]", "content": "8.5.1 Math Reasoning Goal\nThis challenge focuses on training a model that can generate formal statements and proofs in Lean 3 given problem statements and proofs in natural language. It is part of the ICML 2024 Challenges on Automated Math Reasoning. This challenge tests agents' ability to bridge the gap between natural language mathematics and formal theorem proving, requiring deep understanding of both mathematical concepts and formal logic systems. It emphasizes the development of models that can accurately translate informal mathematical reasoning into rigorous, machine-verifiable proofs.\n8.5.2 Math Reasoning Setup\nAgents are provided with the AlgorithmicResearchGroup/math_reasoning_autoformalization_track dataset, which contains 3,963 training samples. Each sample includes a problem name, informal statement, informal proof, and formal proof. Agents are allowed to use other open-source datasets as well. The challenge allows the use of any open-source model and the Hugging Face Transformers library.\n8.5.3 Math Reasoning Evaluation\nModel produced by the agent are evaluated on BLEU, ROUGE-L and Passrate Compiled, where BLEU and ROUGE measure the similarity between between generated proof and ground truth on a held out test set, and Passrate measures the number of scripts that compile into working Lean 3 code out of 100."}, {"title": "NeurIPs 2023 1LLM + 1GPU + 1Day: LLM Efficiency Challenge [27]", "content": "This challenge tasks agents with fine-tuning a large language model (LLM) to maximize performance across various metrics within strict time and resource constraints. Agents must start with an approved base model and use only open-source data for fine-tuning on an A100 40GB GPU within 24 hours. Agents can choose from a wide range of base models, including BERT, GPT-2, LLaMA, T5, and many others. They are free to use any open-source dataset for fine-tuning, with suggestions including Databricks-Dolly-15, OpenAssistant Conversations Dataset, and The Flan Collection. The challenge emphasizes efficiency in model optimization, with strict rules prohibiting training on the MMLU benchmark (used for evaluation), using non-open-source data, or exceeding the 24-hour time limit. The goal is to produce a Hugging Face model that performs optimally on a subset of the MMLU benchmark. This competition tests agents' ability to balance performance improvements with resource constraints, encouraging innovative approaches to model optimization and data selection."}, {"title": "CONLL 2024 BabyLM Challenge [28]", "content": "8.7.1 BabyLM Goal\nThe BabyLM Challenge (Strict-Small) focuses on training a large language model from scratch using a small pretraining corpus of approximately 10 million words. Agents must use the provided AlgorithmicResearchGroup/babylm dataset and are not allowed to use pre-trained models or fine-tune on the evaluation set. This challenge aims to test the ability of language models to learn linguistic knowledge from a limited amount of data, focusing on fundamental language understanding rather than large-scale pretraining.\n8.7.2 BabyLM Setup\nThe challenge allows agents to choose any model architecture and use Hugging Face Transformers library for implementation. Key rules include: no training on BLiMP (used for evaluation), no fine-tuning of pre-trained models, and strict adherence to the provided dataset. The goal is to create a Hugging Face model that performs optimally on the BLIMP (Benchmark of Linguistic Minimal Pairs) evaluation suite.\n8.7.3 BabyLM Evaluation\nModels are evaluated on the BLiMP benchmark. \u0100BLiMP = 1/N \u2211i=1 Ai Where ABLIMP is the average accuracy for BLiMP; N is the total number of subtasks; A\u2081 is the accuracy for the i-th subtask."}, {"title": "Baseline Agent", "content": "To facilitate evaluation of the benchmark, we provide a baseline agent, compatible with both OpenAI and Anthropic models. Our baseline agent architecture comprises two main components: a supervisor and a worker agent. The supervisor manages task instructions and results, while the worker executes the tasks. This design allows for parallel execution of multiple worker agents, enhancing efficiency and scalability. We equip the worker agent with a modular set of tools, leveraging the function calling capabilities of current language models. These tools, located in the tools directory, include functionalities such as running Python and Bash scripts, managing code files, interacting with GitHub repositories, and searching academic papers. This modular approach facilitates easy modification and extension of agent capabilities. The baseline agent employs a ReAct-like thought process [29], recording intermediate steps and reasoning. Upon task completion, the agent outputs a comprehensive table including the task number, run ID, submission status, model path, total tokens used, number of actions, and time taken. This flexible framework allows researchers to design and implement diverse agent architectures while maintaining a standardized evaluation protocol."}, {"title": "Agent Tools", "content": "We equip the worker agent with a modular set of tools, leveraging the function calling capabilities of current language models. These tools, located in the tools directory, include functionalities such as running Python and Bash scripts, managing code files, interacting with GitHub repositories, and searching academic papers. This modular approach facilitates easy modification and extension of agent capabilities. The baseline agent employs a ReAct-like thought process, recording intermediate steps and reasoning. Upon task completion, the agent outputs a comprehensive table including the task number, run ID, submission status, model path, total tokens used, number of interaction turns, and time taken. This flexible framework allows researchers to design and implement diverse agent architectures while maintaining a standardized evaluation protocol. The baseline agent works with either OpenAI or Anthropic models and leverages their function calling capabilities to navigate through the tasks. We provide the following tools:"}, {"title": "Agent Environment", "content": "We provide the agent with an environment to produce its work. The environment is in a docker container, to ensure that each agent has the same initial environment. The environment includes pre-installed packages: numpy, torch, torchvision, and datasets, and transformers. We also install LM-Evaluation Harness for model evaluation, and our agent-eval library. All other libraries and packages must be installed by the agent. Each environment has access to an NVIDIA A100 40GB GPU, as well as 30 cores CPU, 214.7 GB RAM, and 549.8GB storage. We utilize Lambda Labs cloud GPU instances to ensure consistency."}, {"title": "Results and Discussion", "content": "We compare the performance of two AI agent scaffolds, one powered by OpenAI's GPT-40 and the other by Anthropic's Claude Sonnet 3.5, across the seven competition-level AI challenges. The results are compared against 0-shot baselines, in which we prompt the respective model with the task challenge, and manually fix issues with the code, without making substantial changes that will effect the metric score.\nThe Claude Sonnet 3.5 agent produced higher scores in most tasks, outperforming the GPT-40 agent in five out of seven challenges. Notable differences were observed in tasks such as LLM Merging, Edge LLM Compression, and LLM Efficiency. However, it's important to note that these results are based on a limited number of runs."}, {"title": "MiniPile Task", "content": "In the Mini Pile challenge, which used the SuperGLUE benchmark, the Claude Sonnet 3.5 agent achieved a higher average score than the GPT-4o agent (0.541 vs 0.457).\nThe GPT-4o agent faced several challenges during its attempts. Out of five runs, only one was successful, using the google/flan-t5-base [30] model and achieving a score of 0.4567375. The other attempts either ran out of time, encountered errors (such as a bad JSON response), or failed to produce a model. Notably, an attempt to use gpt2-large [31] ran out of time during training.\nIn contrast, the Claude Sonnet 3.5 agent showed more consistent performance. Four out of five runs were successful, all using the openai-community/gpt2 model. These runs achieved scores ranging from 0.5258125 to 0.5551375, with an average of approximately 0.541. One run using the RoBERTa model encountered a CUDA error and did not produce a score."}, {"title": "LLM Merging Task", "content": "A notable difference was observed in the LLM Merging task, evaluated using the MMLU benchmark. The Claude Sonnet 3.5 agent produced a score of 0.273, while the GPT-40 agent did not produce a valid result. The Claude agent successfully used various models, including facebook/opt-350[32], google/flan-t5-base [30] combined with google/flan-t5-large, and facebook/opt-1.3b[32]. Claude agent demonstrated a more sophisticated approach to model merging. The top solution implements a simple model merging technique using two instances of the FLAN-T5 Base model (a sequence-to-sequence model with approximately 250 million parameters). The merging process involves averaging the parameters of the two identical models, effectively creating a new model with the same architecture but potentially different behavior."}, {"title": "Edge LLM Compression", "content": "In the Edge LLM Compression challenge, also evaluated using MMLU, the Claude Sonnet 3.5 agent scored higher than the GPT-4o agent (0.532 vs 0.326). The Claude agent achieved scores between 0.2282 and 20.55122, while the GPT-40 agent's scores ranged from 0.22838 to 0.54212. Claude Sonnet 3.5 agent loaded a phi-2 model, converts it to FP16, applied and applied magnitude-based pruning to linear layers, reducing the Non-zero parameter count from 2,779,683,840 to 1,984,822,594 while maintaining accuracy across MMLU."}, {"title": "Edge LLM Training", "content": "The Edge LLM Training challenge, using the SuperGLUE benchmark, saw both agents produce similar scores, with the Claude agent slightly higher (0.494 vs 0.459). Both agents consistently used the openai-community/gpt2 [31] model, indicating that both agent scaffolds were able to produce comparable baselines when training models from scratch under the given constraints. The winning solution from Claude Sonnet 3.5 agent implements the training of a GPT-2 model on a combined dataset of C4 and Alpaca data. The model architecture is based on GPT-2 but with a smaller configuration: 6 layers, 12 attention heads, and an embedding dimension of 768, resulting in approximately 82 million parameters. It uses a maximum sequence length of 512 tokens. The training data consists of 500,000 examples sampled from the combined C4 and Alpaca datasets. The model is trained for 3 epochs using the AdamW optimizer with a learning rate of 5e-5 and a batch size of 16."}, {"title": "Math Reasoning", "content": "In the Math Reasoning task, evaluated using BLEU, ROUGE-L, and Passrate metrics, the GPT-40 agent achieved a higher average score (0.08725 vs 0.02842) across five runs. The GPT-40 agent primarily used google/flan-t5-small, while the Claude-Sonnet 3.5 agent experimented with meta-llama/Llama-2-7b-chat-hf, facebook/opt-350m, gpt2, and mistralai/Mistral-7B-Instruct-v0.2. Notably, all models from both agents failed to produce compilable code, indicating the significant challenge posed by this task.\nThe Claude-Sonnet 3.5 agent's approach involved fine-tuning the Mistral-7B-Instruct-v0.2 model (which has 7 billion parameters) on the dataset using Low-Rank Adaptation (LoRA) [33]. The model employs the Mistral tokenizer with a maximum sequence length of 512 tokens. The fine-tuning process uses the Hugging Face Trainer API with a LoRA configuration (rank 8, alpha 32, dropout 0.1) applied to the causal language modeling task. Training runs for 3 epochs with a per-device batch size of 4, gradient accumulation steps of 4, and a learning rate of 2e-5. The process includes 100 warmup steps, logging every 10 steps, and saving the model after each epoch. The training objective is to generate formal Lean 3 proofs from informal mathematical statements and proofs. This approach adapts the Mistral-7B model to the specific task of mathematical reasoning and formalization while maintaining efficiency through parameter-efficient fine-tuning (PEFT) [34] with LoRA.\nBoth agents experienced runs with zero scores. The GPT-4o agent had one failed run, while the Claude-Sonnet 3.5 agent had two failed runs, including one with a CUDA error for the Mistral-7B-Instruct-v0.2 model. The failure to produce compilable code across all models underscores the difficulty of translating informal mathematical reasoning into formal, compilable proofs."}, {"title": "LLM Efficiency", "content": "In the LLM Efficiency challenge, evaluated using MMLU, the Claude Sonnet 3.5 agent produced a higher score (0.310 vs 0.234). The Claude agent achieved its best results using meta-llama/Llama-2-7b-chat-hf, with scores up to 0.40776, while the GPT-40 agent's best performance came from EleutherAI/pythia-1.4b[35] with a score of 0.25036. Claude agent trained a meta-llama/Llama-2-7b-chat-hf [36] quantized version of the Llama-2-7b model (which has 7 billion parameters) on the OASST1 dataset using 8-bit quantization and Low-Rank Adaptation (LoRA). The model uses the Llama-2 tokenizer with a maximum sequence length of 256 tokens. The fine-tuning process employs the Hugging Face Trainer API with a LoRA configuration (rank 16, alpha 32, dropout 0.05) applied to the query and value projection layers. Training runs for 3 epochs or a maximum of 100,000 steps, with a per-device batch size of 8, gradient accumulation steps of 8, and a learning rate of 2e-4. The process includes 500 warmup steps, weight decay of 0.01, logging every 100 steps, and saving/evaluating every 5000 steps. It uses the 8-bit AdamW optimizer and mixed precision training (fp16). After training, the model is evaluated on the MMLU (Massive Multitask Language Understanding) benchmark. This approach adapts the Llama-2-7b model to the OASST1 dataset while maintaining efficiency through quantization and parameter-efficient fine-tuning (PEFT) with LoRA."}, {"title": "BabyLM", "content": "The Baby LM challenge, using the BLIMP benchmark, saw both agents produce similar scores, with the GPT-40 agent slightly higher (0.527 vs 0.535). Both agents predominantly used the openai-community/gpt2 model, with the Claude agent's scores ranging from 0.508 to 0.5527 and the GPT-40 agent's from 0.5198 to 0.5395. Both agent scaffolds were able to produce comparable baselines when working with the provided limited dataset. Claude Sonnet 3.5 agent produced a custom GPT-2 configuration with 6 layers, 12 attention heads, and an embedding dimension of 768, resulting in approximately 82 million parameters. It uses the standard GPT-2 tokenizer vocabulary and has a maximum sequence length of 512 tokens. The model was pretrained on the AlgorithmicResearchGroup/babylm dataset using the AdamW optimizer with a learning rate of 5e-5 and a batch size of 8. The training process ran for 3 epochs."}, {"title": "Discussion", "content": "ML Research Bench highlights the gap between an agent's ability to follow complex instructions and produce baselines, versus its capacity for non-trivial research and model development. Importantly, neither baseline agent that we developed demonstrated the ability to perform non-trivial model development or research. However, both agents were successful at producing baselines across the range of tasks.\nThe benchmark also revealed challenges in task completion and time management for both agents, particularly for one of them. Across several tasks, agents chose to train or fine-tune models that did not converge within the 24 hour window, and on several occasions, the agents did not checkpoint their models. This observation underscores the importance of efficiency and resource management in agent design.\nAcross the various tasks in this study, the Claude Sonnet 3.5 agent generally performed better compared to the GPT-4o agent."}, {"title": "Limitations and Future Work", "content": "This study has several limitations that should be considered. With only five runs per task, the statistical significance of the results is limited. Future work should include more runs to ensure the robustness of findings. Additionally, as these agent scaffolds and their underlying models are frequently updated, results may not reflect the most current versions.\nThe chosen tasks, while diverse, may not comprehensively represent all relevant AI capabilities. Expanding the range of tasks could provide a more complete picture of agent performance. Future research could include more complex real-world scenarios to further test the capabilities and limitations of these Al agents.\nThe study was conducted with specific hardware (A100 40GB GPU) and time constraints (24 hours per task). Varying these parameters could yield different results and insights into the performance of the agent scaffolds under different conditions. Furthermore, with an average cost of $42.89 per run and a total of $300.23 per agent for all tasks, cost- production tradeoff should be a consideration in future comparisons, especially for resource-intensive tasks.\nIn conclusion, this study provides initial insights into the performance of two AI agent scaffolds across a range of AI challenges. While the agents were able to produce baselines for many tasks, they did not demonstrate capabilities for non-trivial model development or research. These results highlight both the progress made in AI agents and the significant challenges that remain in developing more capable and versatile AI systems."}, {"title": "Conclusion", "content": "This study provides initial insights into the performance of two AI agent scaffolds across a range of AI challenges. While the agents were able to produce baselines for many tasks, they did not demonstrate capabilities for non-trivial model development or research. These results highlight both the progress made in AI agents and the significant challenges that remain in developing more capable and versatile AI systems. The ML Research Benchmark provides a framework for assessing and comparing the performance of AI agents on tasks that closely mirror real-world AI research and development challenges. Our findings suggest that while our AI agents can successfully navigate complex instructions and produce baseline results across a variety of tasks, they still fall short of the capabilities required for advanced AI research. This gap presents an important area for future development, as the ability of AI agents to conduct non-trivial research and model development could significantly accelerate progress in the field of AI. The performance differences observed between the GPT-40 and Claude Sonnet 3.5 agents across different tasks underscore the importance of developing versatile AI systems that can adapt to a wide range of challenges. It also highlights the need for comprehensive benchmarks like the ML Research Benchmark that can provide a nuanced evaluation of AI capabilities across diverse tasks. ML Research Benchmark represents a step forward in our ability to assess and understand the capabilities of AI agents in the context of AI research and development."}, {"title": "Agent Prompts", "content": "We provide the system and message prompts for both the supervisor and worker agent below. We use the same prompts for both Claude Sonnet 3.5 and GPT-40 agents."}, {"title": "Code Availability", "content": "The code for the ML Research Benchmark, including the baseline agent implementation and evaluation scripts, is available at [GitHub repository URL]. We encourage researchers to use, adapt, and build upon this framework for further research in AI agent capabilities."}, {"title": "Computational Resources", "content": "All experiments were conducted using NVIDIA A100 40GB GPUs on Lambda Labs [37]. The total computational resources used for this study amounted to approximately 1,295 GPU-hours."}, {"title": "Ethical Considerations", "content": "The development of AI agents capable of conducting AI research and development (R&D) raises significant ethical considerations that warrant examination. Our work on the ML Research Benchmark, while aimed at assessing current AI capabilities, could potentially contribute to the acceleration of AI development. This acceleration, while promising numerous benefits, also presents risks that require thoughtful consideration and mitigation strategies. Of particular concern is the potential for these advancements to lead to fast takeoff scenarios. As AI systems become increasingly capable of improving"}]}