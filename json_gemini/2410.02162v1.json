{"title": "Planning in Strawberry Fields:\nEvaluating and Improving the Planning and\nScheduling Capabilities of LRM 01", "authors": ["Karthik Valmeekam", "Kaya Stechly", "Subbarao Kambhampati", "Atharva Gundawar"], "abstract": "The ability to plan a course of action that achieves a desired state of affairs has long\nbeen considered a core competence of intelligent agents and has been an integral\npart of AI research since its inception. With the advent of large language models\n(LLMs), there has been considerable interest in the question of whether or not they\npossess such planning abilities, but-despite the slew of new private and open source\nLLMs since GPT3-progress has remained slow. OpenAI claims that their recent\no1 (Strawberry) model has been specifically constructed and trained to escape the\nnormal limitations of autoregressive LLMs-making it a new kind of model: a Large\nReasoning Model (LRM). In this paper, we evaluate the planning capabilities of two\nLRMS (01-preview and 01-mini) on both planning and scheduling benchmarks. We\nsee that while ol does seem to offer significant improvements over autoregressive\nLLMs, this comes at a steep inference cost, while still failing to provide any\nguarantees over what it generates. We also show that combining 01 models with\nexternal verifiers-in a so-called LRM-Modulo system-guarantees the correctness\nof the combined system's output while further improving performance.", "sections": [{"title": "Introduction", "content": "The recent release of OpenAI's ol (Strawberry) [26] brings with it the opportunity to freshly evaluate\nthe progress of large pre-trained AI models on planning and scheduling benchmarks. Unlike the\nLarge Language Models (LLMs) which came before it\u2013which can roughly be viewed as approximate\nretrievers-o1 seems to have been trained to be an approximate reasoner, capable of scaling the amount\nof compute it uses depending on the query.\u00b9 Following OpenAI2, we draw a distinction between\nprevious Large Language Models (LLMs) and 01, a Large Reasoning Model (or LRM), as its new\n(unknown) architecture, operation, and capabilities all seem to be fundamentally different from those\nof vanilla LLMs, likely during both the pre-training phase and at inference time. Our aim in this\npaper is twofold: to comprehensively evaluate the performance of o1 on established planning and\nscheduling benchmarks as well as more difficult extensions; and to demonstrate how to provide\ncorrectness guarantees and boost performance by embedding an LRM in a loop with a sound verifier,\nin a similar vein to the LLM-Modulo framework [16].\nTo properly evaluate this new kind of model and understand its abilities and limitations will require\nnew tools and evaluation methods, especially if details of the overall model structure are kept\nsecret and internal traces remain inaccessible to outside researchers. In this paper, we evaluate\nperformance on established benchmarks and compare to previous state-of-the-art results, extending\nthese benchmarks to more difficult problems when possible and necessary. For planning, we use\nPlanBench [35], which consists of both specific test sets and a suite of tools intended for evaluating\nlanguage models on arbitrary IPC planning domains. To evaluate scheduling capabilities, we test on\nTravelPlanner [37], on the three domains from Natural Plan [40], and on graph coloring problems [32].\nUsing these benchmarks as our basis for analysis, we investigate the performance jump that LRMs\nfrom the o1 family promise. We then demonstrate how these benchmarks can be elaborated on\nin order to remain relevant metrics for LRMs. We argue that, to be complete, new approaches to\nmeasuring LRM reasoning capabilities must take into account efficiency, cost, and guarantees. We\nalso note the steep inference cost of LRMs and discuss the tradeoffs between using LLMs vs LRMS,\narguing that in some cases an LLM-Modulo [16] approach may be significantly cheaper than ol\nmodels for comparative performance, and with guarantees. Our results also show that that same LLM-\nModulo approach can indeed be adapted to LRMs to further improve their performance and provide\nguarantees. In essence, LRMs can replace LLMs as significantly better-but still fallible-generators\nin the LLM-Modulo framework."}, {"title": "Background and Related Work", "content": "Though they are trained as text completion systems, Large Language Models (LLMs) have shown\nsome promise on many other tasks. Initial claims were wildly positive, claiming they are general\npurpose reasoning systems [4], especially when prompted in just the right way [19, 36], but later\nresults showed that their seeming reasoning capabilities are brittle and break down even in simple\ndomains [21, 7, 32, 27] and may be attributable to dataset contamination [28]. In planning, [35]\nshowed that LLMs fail even on problems as trivial as three block stacking.\nBased on what little has been revealed by OpenAI, o1 seems to be a new class of model (a Large\nReasoning Model or LRM), designed to combine the fuzzy language capabilities of LLMs with some\nimplementation of approximate reasoning. With this in mind, we believe it is time to bring up the\nsame questions that were asked about LLMs for these LRMs. We use benchmarks from the LLM\nliterature, extending them where possible and necessary to show how well and how robustly ol does\nor doesn't perform on various planning and scheduling tasks."}, {"title": "Domains: Planning", "content": "The LLM literature abounds with claims of the 'emergent' planning capabilities of LLMs [12].\nHowever, closer inspection reveals that many of the empirical results supporting these claims comes\nfrom evaluations on simpler, commonsense domains, such as ALFworld [39], BEHAVIOR [30],\nkitchen environments [1, 13], and virtual home [12]. Not only do the the instances tested on tend to\nhave fewer interactions, but many of them conflate reactive acting and deliberative planning [9].\nIn contrast, we focus on classical planning problems, or STRIPS planning problems, which are a\nformalism for automated planning in discrete, deterministic spaces. To define a planning problem, we\nspecify an initial state, a domain, and a goal. The domain contains all relevant information about the\ntypes of objects that may exist and the allowable actions from any given state, specified by defining\nthe preconditions and effects of each named action. Problems and domains are represented in the\nflexible PDDL (Planning Domain and Definition Language) framework [22]. Solutions to PDDL\nproblems are correct plans-sequences of actions executable from the initial state which arrive at a"}, {"title": "Domains: Scheduling", "content": "More recent text-based benchmarks have provided full, static descriptions of their domains, initial\nstates, and goals. However, many of these, despite the word \"planning\" in their titles, would be better\ncharacterized as testing scheduling abilities [9]. Classical planning problems are canonical graph\nsearch problems which are PSPACE-complete. Scheduling problems are only NP-Hard [5], and\nmainly revolve around resource allocation. These problems are equivalent to constraint satisfaction\nproblems, and thus easier than the planning problems we describe above.\nWe evaluate ol on three scheduling benchmarks on which LLMs have failed. [40]'s Natural Plan\nbenchmark consists of three scheduling domains: trip planning, calendar scheduling, and meeting\nplanning. [37]'s Travel Planning benchmark consists of a large dataset of travel information (flights,\naccommodations, restaurants, etc.) with prompts that ask the model to create a three to seven day\nitinerary based on natural language instructions. Finally, [32] translate graph coloring, a classical\nconstraint satisfaction problem, into natural language prompts and evaluates GPT-4's accuracy on\nthese problems. We take their test set and extend it to more difficult instances."}, {"title": "From Approximate Retrieval to Approximate Reasoning", "content": "Many researchers have argued that \"standard\" autoregressive LLMs generate outputs via approximate\nretrieval, and that, while they show impressive performance on a range of System 1 tasks, they\nare unlikely to achieve the more System 2-like approximate reasoning capabilities that are critical\nfor planning tasks (c.f. [15]). From our analysis, we believe that ol's architecture supplements an\nunderlying LLM with System 2-like abilities, allowing it to outperform previous models."}, {"title": "Planning", "content": "Evaluating LRMs on PlanBench: We test ol-preview and o1-mini on the original 600-instance\nPlanBench test set. The full results can be seen in Table 2. These 600 Blocksworld instances range\nfrom three to five blocks, and require plans of between 2 to 16 steps to solve. Far surpassing any LLM,\no1 correctly answers 97.8% of these instances. On Mystery Blocksworld, the model does not maintain\nthis level of performance, but it does far surpass all previous models (which barely managed a few\npercent), answering 52.8% correctly. To test whether the exact obfuscation might be compromised\nbecause of data contamination, we also generated a new obfuscation using completely random strings,\nand presented these problems in a new, semantically equivalent prompt format with fully specified\nand unambiguous PDDL descriptions of both the domain and problem. This is presented in the\ntable as Randomized Mystery Blocksworld. Exact prompts can be seen in the appendix. While\nperformance did dip further, 37.3% of instances are answered correctly, sharply contrasting the flat\nzeroes of previous models. The same pattern can be seen when evaluating Logistics and a freshly\ngenerated obfuscation of that domain. Despite the higher branching factor of the domain, o1-preview\nsolves 94% of all 200 problems tested and achieves 52% on the obfuscated variant.\nIncreasing Problem Size: Standard LLM chain-of-thought prompting approaches are brittle, do\nnot robustly scale with problem size, and fail to induce general algorithmic procedure-following [31].\nWe extend planbench to a set of 110 harder Blocksworld problems. Problems in this set range from\n6 to 20 blocks in length and require 20 to 40 step optimal plans. Without any obfuscation, we see\nperformance quickly degrade from the 97.8% reported earlier. In fact, over these 110 instances,"}, {"title": "LRM-Modulo to Improve o1 with Guarantees", "content": "We propose augmenting ol with external verifiers to endow the combined system with soundness\nguarantees. While o1 is a stride in the direction of general-purpose, expressive planning systems, our\nresults show that it cannot plan robustly when faced with harder instances, nor can it consistently\nrecognize when instances are unsolvable, still providing incoherent plans in a majority of such cases.\nIn other words, o1 is still fallible and without guarantees. Prior to the release of these models, the\nbest way to coax planning capabilities out of LLMs has been to pair them with sound external verifier\nin generate-test frameworks, in what are known as LLM-Modulo systems [16, 33]. This framework\nis broadly applicable even beyond LLMs, and-given a sound verifier for some domain-requires\nonly a generator expressive enough to provide guesses for that domain. Moreover, because of the\nbuilt-in verification, it guarantees that any answer output is correct. For safety-critical systems, this is\nessential! High accuracies are not sufficient, especially when the underlying system-as is the case\nfor both LLMs and even more so for LRMs-is an opaque black box. Therefore, we investigated\nintegrating LRMs into LRM-modulo systems to both boost their overall performance and to provide\nmuch-needed guarantees over their outputs.\nGenerate-test systems are limited by how good the generator is. A poor generator, such as one that\nproduces completely random strings, may be capable of eventually producing the correct answer, but\nbe so unlikely to do so at each iteration as to be useless, while an incomplete generator may never\noutput the correct answer at all. LLMs and LRMs can be backprompted\u2014that is, we can take feedback\nfrom the sound verifier and send it back to the model or modify the next prompt in some other way\nto increase the diversity of the responses generated-which may steer their next output towards the\ncorrect answer. Based on our results, o1 models are much better generators than anything that came\nbefore them, but, a priori, it is unclear if they are any more complete or capable of effectively utilizing\nbackprompts or advice.\nWe test LRM-modulo setups on our five hardest test sets: 20+ length plan Blocksworld, Sokoban,\n20 vertex graph coloring, OSU's Travel Planning, and 10 city trip planning. Due to cost constraints,\nwe limit the number of iterations to a maximum of ten, but we stop the system early once the\nperformance increase from round to round has become mostly flat. Even with so few iterations, we\nsee significant jumps in performance across almost all of our domains. 01-preview's performance on\nharder Blocksworld saturates within 7 iterations, with the combined system achieving 98.2%. Harder\ngraph coloring shows similar results, going up to 94%. Perhaps most surprising, our most difficult\ndomain, Sokoban, shows a significant jump from 12.7% to 43.6%. 01-mini-Modulo performance,"}, {"title": "Accuracy/Cost Tradeoffs and Guarantees", "content": "With LRMs showing better performance on planning and scheduling problems, our evaluations must\nexplicitly take into account the trade-offs that come from choosing general models over established\ndeep and narrow systems. While 01-preview may provide higher accuracy than LLMs, it still fails to\nprovide any correctness guarantees, and it is unclear that it is at all cost-effective. Unlike previous\nmodels, whose APIs only charge based on the number of input tokens and the number of output\ntokens (usually at a rate that is five times higher for the latter), o1's price-per-call includes a surcharge\nbased on the number of \"reasoning tokens\" it used\u2013tokens generated as part of inference and not\nrevealed to the user-which are charged at the significantly higher output token rate. Currently, end\nusers have no control over the number of these tokens generated, a number which is expanded or\nlimited by the model in its own opaque way. We have already run up a bill of over $4000 for just the\no1 model experiments reported in this evaluation!\nWithout exposing the ability to scale inference time to particular specifications, influence the internal\n'thinking' process in task-specific ways, or ensure that intermediate steps are evaluated by trusted\nor sound verifiers, the o1 models are a coarse-grained choice in the space of cost, inference time,\nguarantees, and performance trade-offs. They aren't, however, the only choices in that space, and\nreasonable LRM evaluations must take this into account (see similar arguments in [18, 17]).\nClassical planners like Fast Downward [10] achieve 100% on our dataset in a fraction of the time,\ncompute, and cost, while providing guarantees that their answers are correct. Running Fast Down-\nward on a personal computer was essentially free in dollar terms and averaged 0.12 seconds per\ninstance, which is many orders of magnitude faster than the average ol clock times listed in table 2.\nIt is also generally predictable, and can be scaled to harder instances very directly. Vanilla LLMs are\ntypically very good at translating problems between formats, and could be used to do so in concert\nwith a classical planner at a fraction of the cost of LRMs (e.g. [23, 20]). For problems which don't\nhave simple PDDL domain and instance specifications, LLM-Modulo systems may be a safer and\ncheaper approach: run a smaller, faster LLM in a loop with a sound verifier, so that the combined\nsystem will only output guaranteed correct solutions (e.g. [16, 29, 33]).\nThe correctness guarantees provided by these latter two methods are sorely lacking in LRMs like\no1. A general reasoning system cannot be deployed in safety critical and non-ergodic domains if it\ncontinues to confidently make incorrect plans. o1 is a fully black box system, even more so than\nprevious models, and OpenAI's decision to not only keep the architecture under wraps and hide the\nreasoning traces, but to warn away and even ban anyone who attempts to understand what is going on\ninside them [8], makes interpretability nearly impossible, and reduces trust in the system overall.7"}, {"title": "Conclusion", "content": "In this paper, we investigated the performance of ol-preview and o1-mini-the new so-called LRMs-\non a variety of planning and scheduling benchmarks. While LLMs have thus far failed to make much\nprogress on the obfuscated (or \"Mystery\") versions of PlanBench domains, o1 shows the first bit\nof real progress. In general, it seems to have made impressive headway on benchmarks that were\npreviously unassailable. However, when we evaluated the model on longer problems and on the\nquestion of determining solvability of potentially impossible instances, we found that these accuracy\ngains are not general nor robust. While 01 made some gains on scheduling problems, performing\nmuch better on graph coloring than previous models, these were not evenly distributed, only making\nsome progress on OSU's Travel Plan domain and the Natural Plan benchmark suite. We also discussed\nthe critical accuracy/efficiency tradeoffs that are brought up by the fact that o1 that uses (and charges\nfor) significant inference-time compute, as well as how it compares to other LLM-based approaches\n(such as LLM-Modulo [16]) and dedicated solvers. Future evaluations will have to maintain a focus\non these factors if they are to remain meaningful or relevant. Finally, we showed that approaches\nlike LLM-Modulo [16] can indeed be adapted to LRMs to further improve their performance and to\nprovide much-needed guarantees. In essence, LRMs can replace LLMs as significantly better-but\nstill fallible-generators in LLM-Modulo frameworks."}, {"title": "Appendix", "content": "A Further Discussion of LLM Planning Performance on Obfuscated Domains\nLLMs are highly capable at providing translations between equivalent representations [23]. This fact,\ncombined with their significantly higher performance on the unobfuscated version of the Blocksworld\ndomain, predicts that-if they are capable of composing reasoning operations-the performance gap\nbetween Mystery Blocksworld and classic Blocksworld should shrink substantially if the translation\nfrom Mystery Blocksworld back into Blocksworld is explicitly provided. However, when we provide\nthis in the prompt (see Appendix G), performance only improves a very small amount: GPT-4\nachieves 10%.\nB Speculations about o1's Internal Operations\nWhile our evaluation of o1 did not depend on any specific assumption about its operation, we did\nhave a working model of o1 based on the very skimpy description that was provided in the blog post\nthat accompanies ol's release [25]. Verifying our model is unfortunately made infeasible by the fact\nthat o1 doesn't actually provide any trace of its operations (even during the costly inference stage),\nand OpenAI warns that API access will be revoked if any attempts are made to surface its reasoning\ntokens.\nThere are two things-\u201creinforcement learning\" and \"Private Chain-of-Thought (CoT)\" that are\nmentioned in the writeup. So imagine you are trying to transplant a \u201cgeneralized AlphaGo\u201d\u2013let's call\nit GPTGo-onto the underlying LLM token prediction substrate.\nTo do this, you need to know\n1. What are the GPTGo moves? For AlphaGo, we had GO moves). What would be the right\nmoves when the task is just \"complete the prompt the right way\"?\n2. Where is it getting its external success/failure signal from? For AlphaGo, we had simula-\ntors/verifiers giving the success/failure signal. The most interesting question in transplanting\nthe self-play idea to a general AI agent is where is it getting this signal?\nOur guess is that the moves are auto-generated CoTs (thus the moves have a very high branching\nfactor). Let's assume-for simplification\u2013that we have a CoT-generating LLM, that generates these\nCoTs conditioned on the prompt. (It is not clear if the CoT's are domain independent of the \u201cthink\nstep by step\" variety [19] or domain/task specific, or a combination.)\nThe success signal is likely from massive amounts of synthetic training data with correct answers.\nWhen the completed prompt is seen to contain the correct answer (presumably judged by the LLM\nitself), then the episode is considered a success, and a failure otherwise.\nThe task for the reinforcement learner then is: Given the original problem prompt, generate and select\na CoT, and use it to continue to extend the prompt (possibly generating subgoal CoTs after every few\nstages). Get the final success/failure signal for the example (for which you do have answer).\nThe RL stage may involve training on a a huge number of training examples with answers. The\ntraining examples with answers can either be coming from benchmarks, or from synthetic data with\nproblems and their solutions-using external solvers. In this phase the RL part attempts to learn the\nq-values of the CoT moves (much like AlphaGo learns the q-values of the moves of the Go). (The\nq-values learning may be incorporated into the internal weights of the CoT generator LLM). At this\npoint, we have a CoT move generator that is better than the random one before the RL stage\nDuring the inference stage-which OpenAI says can be indefinitely long (although it is currently\ncapped internally by them, with no external control), like AlphaGo, 01 might be further improving its\nevaluation of the q-values of the CoT moves in the context of the current prompt. While AlphaGo\nused MCT-based rollouts, we obviously don't know the mechanism ol uses. The announcement\nonly says that at inference stage a long chain of thought is added to the original prompt (and o1 does\ncharge the end users for its \"reasoning tokens,\" which are never seen by the end user, at the same\nhigh rate as the output tokens). In this sense, our speculations seem to be consistent, even though it is\nnot clear whether the reasoning tokens are proportional to the entire inference-stage computation, or\njust represent the final sequence of CoT moves that get selected after the rollout-like inference stage."}]}