{"title": "RClicks: Realistic Click Simulation\nfor Benchmarking Interactive Segmentation", "authors": ["Anton Antonov", "Andrey Moskalenko", "Denis Shepelev", "Alexander Krapukhin", "Konstantin Soshin", "Anton Konushin", "Vlad Shakhuro"], "abstract": "The emergence of Segment Anything (SAM) sparked research interest in the field\nof interactive segmentation, especially in the context of image editing tasks and\nspeeding up data annotation. Unlike common semantic segmentation, interac-\ntive segmentation methods allow users to directly influence their output through\nprompts (e.g. clicks). However, click patterns in real-world interactive segmenta-\ntion scenarios remain largely unexplored. Most methods rely on the assumption\nthat users would click in the center of the largest erroneous area. Nevertheless,\nrecent studies show that this is not always the case. Thus, methods may have poor\nperformance in real-world deployment despite high metrics in a baseline bench-\nmark. To accurately simulate real-user clicks, we conducted a large crowdsourcing\nstudy of click patterns in an interactive segmentation scenario and collected 475K\nreal-user clicks. Drawing on ideas from saliency tasks, we develop a clickability\nmodel that enables sampling clicks, which closely resemble actual user inputs.\nUsing our model and dataset, we propose RClicks benchmark for a comprehensive\ncomparison of existing interactive segmentation methods on realistic clicks. Specif-\nically, we evaluate not only the average quality of methods, but also the robustness\nw.r.t. click patterns. According to our benchmark, in real-world usage interactive\nsegmentation models may perform worse than it has been reported in the baseline\nbenchmark, and most of the methods are not robust. We believe that RClicks is a\nsignificant step towards creating interactive segmentation methods that provide the\nbest user experience in real-world cases.", "sections": [{"title": "1 Introduction", "content": "The task of interactive segmentation involves providing additional hints or prompts to the method,\nallowing it to produce more precise annotations compared to conventional semantic segmentation.\nThe most famous member of interactive segmentation methods is Segment Anything (SAM) [1, 2].\nNowadays, SAM-like methods are applied in various fields, including the thin object segmentation [3,\n4], medical segmentation [5, 6, 7, 8, 9, 10], 3D segmentation [11, 12], tracking [13] and video [2].\nTypically, interactions occur in several rounds, where in each round the user corrects the prediction\nerrors of the previous one. Evaluation of such methods requires user inputs. However, collecting\nmany real-user inputs for multiple rounds is impractical since such a dataset needs to be rebuilt for\nevery method and every interaction round due to its iterative nature. Thus, researchers often resort to\na simple strategy to simulate user inputs. According to this strategy, a single click for each interaction\nround is generated as follows: (1) select the largest error region in the previous interaction round, and\n(2) click in the furthest point from the boundaries of this region (center point). Hereinafter, we refer"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 User Input Types in Interactive Segmentation", "content": "Various types of user inputs have been explored in the literature. In [15, 20] an initial selection is\nobtained using bounding boxes, and then refined with strokes. In [21] object selection is done with\nstrokes. [22] considers contours for selecting small objects, minor parts of an object, or a group of\nobjects of the same type. [23] proposes to use trimap, scribblemap or clickmap as an input. Segment\nAnything, or SAM [1], processes multiple types of user prompts, including a point, a box, a mask, or\na text.\nClicks-based approach selects objects of interest according to multiple user clicks (either positive\nor negative), and was first introduced in [24] and investigated in [19, 25, 26, 27, 1, 3, 28]. We\nfocus solely on the click-based approach, since it is well-explored and has an established evaluation\nprocedure in the field."}, {"title": "2.2 Benchmarking Interactive Segmentation", "content": "GrabCut [15] is the first dataset proposed for interactive segmentation task. Then [16] adapted\nBerkeley [29] segmentation dataset to evaluate interactive segmentation methods, but it required\nmanual testing. However, manual testing is a time-consuming and resource-intensive process.\nInteractive segmentation expects multiple rounds of interactions, when each interaction depends on\nprevious ones, and it is infeasible to apply manual procedure for larger scales. For these reasons, in\npractice, benchmarks generate user interactions automatically based on previous interactions.\nIn [24] authors proposed an automatic clicks generation strategy for evaluation on PAS-\nCAL VOC 2012 [30] and COCO [17] segmentation datasets. The subsequent work [31] used\nDAVIS [18] and SBD [32] datasets for interactive segmentation, applying the same baseline strategy.\nMost of the existing click-based methods [19, 25, 26, 27, 1, 3, 28] use the baseline strategy. However,\nit has not been validated in real-world usage scenarios until recently. [14] introduced TETRIS\nbenchmark and revealed that real users do not always click in the center of an area with the largest\nerror, as assumed in the baseline strategy. Using the adversarial attacks, the paper demonstrated that\nmethods have a tendency to overfit to the baseline strategy. Specifically, when the baseline clicks\nare used, the segmentation quality may be high, but even a slight change in the click position can\nresult in a significant drop in quality. Therefore, the baseline strategy may not accurately estimate the\nquality of the methods in real usage. We believe that to estimate the actual quality, each click should\nbe generated in accordance with human perception."}, {"title": "2.3 Saliency Prediction", "content": "The task of saliency prediction aims to model human perception by predicting probability maps [33,\n34] of user engagement in a free-view observation for a given media content. Reference data for this\ntask usually comes from a specialized device \u2013 an eye tracker \u2013 which records eye fixations [35, 36, 37].\nSubsequently, fixations from multiple viewers are aggregated into a probability distribution through\nGaussian at each fixation point, with sigma corresponding to the retinal angle of a human's field of\nview [35]. Since scaling expensive eye tracker experiments is too complex, several researchers [38, 39]\nproposed to use mouse movements as a proxy for saliency when training saliency models. However,\nsaliency fixations cannot be directly used in the interactive segmentation task because saliency\nobservers engage in free-viewing, while in our task, the user's goal is to make a click to highlight a\nspecific object or a part of it. Thus, for the interactive segmentation problem, real-user clicks should\nbe collected."}, {"title": "3 Users' Clicks Dataset", "content": "We propose a novel dataset of real-users clicks for interactive segmentation. Our dataset is based on\nthe existing image segmentation datasets. In total, we collected 475 544 user inputs for GrabCut [15],\nBerkeley [29], DAVIS [18], COCO-MVal [40], TETRIS [14]. To gather users' clicks, we developed\na specialized presentation tool. Specifically, in each task, we asked users to click on the target objects\nby displaying images and corresponding segmentation masks. We considered several display modes"}, {"title": "3.1 Collection Procedure", "content": "When collecting user clicks,\nwe executed the follow-\ning procedure (see Fig-\nure 2): (1) Show the en-\ntire image for 1.5 seconds.\n(2) Show segmentation tar-\nget using one of the Dis-\nplay Modes. (3) The en-\ntire image is shown again\nfor 1.5 seconds, during\nwhich clicking is not al-\nlowed. (4) The user makes a\nclick. Steps (1) and (3) sim-\nulate the user behavior dur-\ning real-world interactive\nsegmentation, when individ-\nuals initially view the image\nand then interact with it by\nclicking. Step (2) visualizes\nthe object that should be selected by the user."}, {"title": "3.2 Selecting Unbiased Task Display Mode", "content": "Text Description is considered to be unbiased because users do not see the target segmentation mask,\nas in real-world interactive segmentation. However, textual descriptions may be ambiguous for certain\ntypes of instances or areas (see Figure 3). In other display modes, the mask is presented, which could\npotentially distort the distribution of user clicks. To choose mask-based display mode with minimal\nbias, we compare all modes with Text Description mode."}, {"title": "3.3 Collected Interactions", "content": "We annotated each instance in all common interactive segmen-\ntation benchmark datasets \u2013 DAVIS [18], GrabCut [15], COCO-\nMVal [17], Berkeley [29], TETRIS [14] using PC and mobile\nclicks. Collected clicks were validated similarly to the ablation\nstage. When annotating subsequent interaction rounds, the user\nshould click in the area of the segmentation error. To obtain er-\nror masks for the subsequent rounds, we applied state-of-the-art\ninteractive segmentation methods \u2013 SAM [1], SimpleClick [25],\nand RITM [19] \u2013 to all images and all clicks corresponding to\nthose images from the first round. Then, for each image, we\nselected the mask with the highest quality up to a threshold of\n0.95 IoU. We motivate it by the fact that at such a high level of\nquality, the user is likely to stop annotating the instance as the"}, {"title": "4 Click Simulation", "content": "In this section, we explore models for predicting user clicks. Firstly, the baselines are described (4.1).\nSecondly, we introduce a clickability model used for click prediction (4.2) in our interactive segmen-\ntation benchmark. Thirdly, in (4.3) we describe the construction of training dataset for clickability\nmodel. Finally, we compare our clickability model with baselines (4.4)."}, {"title": "4.1 Baseline Models", "content": "As baselines for comparison, we considered uniform, distance, and saliency distribution models.\nThe uniform hypothesis postulates that the clickability of all pixels within the target area is equally\ndistributed (see Figure 4(b)). When the area of interest is relatively small, the uniform assumption is\nreasonable. However, according to this assumption, the click probability of object boundaries and\ntheir centers is equal, which is not necessarily true. The distance transform [14, 19] addresses this\nissue by assigning greater weight to pixels in the center of the object than to those on the boundary\n(see Figure 4(c)). Nevertheless, this transform considers only the shape of the object, neglecting\nhuman perception. To account for human perception, saliency distribution can be used. This is a\nreasonable baseline, as users look at the target area of interaction when clicking. For the saliency\nbaseline, we utilized a state-of-the-art model, TranSalNet [45]. The example of constructed saliency\ndistribution is presented in Figure 4(d), details of how such map is constructed can be found in the\nAppendix A.2. However, saliency models are trained for free-viewing task, and do not take into\nconsideration the setup of our task."}, {"title": "4.2 Clickability Prediction Model", "content": "Similar to the saliency prediction\ntask, we formulate the task of\nsimulating user clicks as a proba-\nbilistic problem. Given an image,\na ground truth object mask, and\na segmentation error mask (FP\nor FN), the model should predict\nat each pixel the probability of\nbeing clicked. We refer to this\nas a clickability map (see details\nin Section 4.3). The proposed\npipeline is shown in Figure 5. As\na base architecture for our model,\nwe adapted state-of-the-art SegNeXt segmentation network [46]. We input the original image into the\nnetwork and concatenate the ground truth mask with the error mask, feeding the resulting tensor as an\nadditional input to the network, a technique inspired by the Conv1S [19]. We use the Kullback-Leibler\ndivergence (KLD) loss function between the predicted and ground truth distributions."}, {"title": "4.3 Clickability Maps Dataset", "content": "We introduce the concept of a clickability map as a single-channel image, such that the value of each\npixel corresponds to the probability that the user will click on it during the interaction round. We\npropose to use such maps to train clickability models.\nGiven an image, error mask, and user clicks, the clickability map is constructed as follows: (1) ini-\ntialize the map as an image of zero values; (2) at each pixel position that was clicked, add one;\n(3) smooth the map by a Gaussian with some sigma, where sigma is a hyperparameter; (4) multiply\npixel values of the map by corresponding pixel values from a soft error mask, obtained by smoothing\nthe original error mask by a Gaussian; (5) normalize pixels by the map sum. The proposed method is\nanalogous to the construction of saliency maps from human eye fixations, except for step (4).\nUnlike saliency, we need to somehow constrain the most likely click positions within the boundaries\nof the mask. Moreover, recall that during the collection of clicks, we considered clicks as valid if\nthey were inside the mask or close to its border. For these reasons, in step (4) the clickability map\nis conditioned by multiplying it on the soft error mask \u2013 smoothing the error mask we consider the\nallowed radius of the border vicinity. We smooth the error mask by Gaussian blurring with a sigma\nequal to the radius of the click used in the user interface (i.e., 1% of image diagonal). The sigma in\nstep (3) simulates the probability density of clicks inside the mask.\nWe constructed train and validation datasets as follows. We split images of TETRIS dataset into\nnon-overlapping train and validation parts. Since we do not know the real click density, for model\ntraining and validation, several sets of clickability maps were constructed with varying magnitudes of\nsigma from step (3). To choose the best sigma, we conducted an ablation study, which can be found\nin Appendix A.3. Note that we constructed clickability maps using clicks from both smartphones and\nPCs. This was done to ensure that the model would predict clicks regardless of the device type."}, {"title": "4.4 Models Evaluation", "content": "To choose the best clickability model, we evaluated considered models on the real-user clicks of\nTETRIS validation part. Here, in addition to sample-based metrics considered above, we calculated\nadditional metrics, that were computed based on the ground-truth clicks positions and predicted\nclicks distribution: PDE \u2013 likelihood of ground-truth clicks, and NSS from saliency benchmarks [47].\nEvaluation results are presented in Table 3. Our clickability model shows the best performance. We\naddress the question of model generalizability in the Appendix A.2."}, {"title": "5 Benchmarking Interactive Segmentation", "content": "In this section, we introduce RClicks benchmark that evaluates the interactive\nsegmentation methods according to the proposed clickability model. Our\nevaluation protocol aims to estimate not only the average annotating time\nbut also the spread w.r.t. clicking groups. Our model returns a probability\ndensity for an instance. For every possible click, we have (x, y) coordinates\nand probability. We sort clicks according to their probabilities and split them\ninto 10 intervals (called clicking groups) {G\u00bf}101 s.t. every interval has 10%\nof total probability mass. We interpret these groups as different user clicking\npatterns, and evaluate methods for each group separately. Visualization of\nclicking groups for an instance may be seen in Figure 6. Note, that even the\nprobability mass of each group G\u00bf is equal, the average probability of clicks\nin each group increases with increase of i.\nWe modify a common evaluation protocol [19] by replacing the baseline sam-\npling strategy with sampling from different clicking groups, obtained through\nthe clickability model. Specifically, (1) interactive segmentation metrics (e.g.\nNoC) are calculated for every instance in a dataset and group G\u00bf by sampling click from G\u00bf (weighted\nby the clickability model) for every interaction round (in our experiments \u2013 20 rounds); (2) then for\neach instance statistics (e.g. mean and standard deviation) of sampled metrics over clicking groups\nare estimated; (3) finally, these statistics are averaged over all instances in the dataset."}, {"title": "6 Discussion", "content": "A review of Table 4, Figures 7 and 8 leads to the following conclusions. First, according to ASB,\nbaseline strategy underestimates the real-world annotation time from 5% up to 29%. This\nimplies that the baseline benchmark may significantly underestimate the real-world annotation costs.\nConsequently, our benchmark may be employed for a more accurate estimation of annotation costs.\nThen, according to AGR, annotation time of users from different clicking groups varies from 3%\nup to 79%. The observed variations in AGR indicate that segmentation methods are unstable w.r.t.\nclick positions in the image."}, {"title": "7 Conclusion", "content": "In this paper, we presented RClicks \u2013 a benchmark for interactive segmentation methods, that\nevaluates both real-world quality and robustness with respect to different clicking patterns. Using the\ndeveloped unbiased presentation strategy, we collected the multi-round real-user click dataset. We\ndeveloped the clickability model that can be utilized to estimate click probabilities and sample realistic\nuser clicks. By employing this model in our benchmark, we demonstrated that baseline strategy may\noverestimate methods' performance in the real world. Furthermore, our analysis showed that there\nis currently no interactive segmentation method that is optimal in terms of both performance and\nrobustness on all datasets. Additionally, we evaluated segmentation methods using real-user clicks\nof the first round and proposed a methodology to estimate the instance difficulty for state-of-the-art\nmethods. We hope RClicks will facilitate the advancement of interactive segmentation methods that\nprovide optimal user experiences in real-world scenarios."}]}