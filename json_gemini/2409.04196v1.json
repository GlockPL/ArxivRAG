{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting Transformers", "authors": ["Lorenza Prospero", "Abdullah Hamdi", "Joao F. Henriques", "Christian Rupprecht"], "abstract": "Reconstructing realistic 3D human models from monocular images has significant applications in creative industries, human-computer interfaces, and healthcare. We base our work on 3D Gaussian Splatting (3DGS), a scene representation composed of a mixture of Gaussians. Predicting such mixtures for a human from a single input image is challenging, as it is a non-uniform density (with a many-to-one relationship with input pixels) with strict physical constraints. At the same time, it needs to be flexible to accommodate a variety of clothes and poses. Our key observation is that the vertices of standardized human meshes (such as SMPL) can provide an adequate density and approximate initial position for Gaussians. We can then train a transformer model to jointly predict comparatively small adjustments to these positions, as well as the other Gaussians' attributes and the SMPL parameters. We show empirically that this combination (using only multi-view supervision) can achieve fast inference of 3D human models from a single image without test-time optimization, expensive diffusion models, or 3D points supervision. We also show that it can improve 3D pose estimation by better fitting human models that account for clothes and other variations. The code is available on the project website https://abdullahamdi.com/gst/.", "sections": [{"title": "1. Introduction", "content": "Reconstructing realistic 3D human models from monocular images is crucial for virtual reality and creative industries, as well as possibly improving human pose estimation for human-computer interfaces and health applications. It is also an integral component of \"3D spatial computing\" for mainstream consumer products incorporating 3D vision in VR and augmented reality (AR). To be practical in homes, offices, and workplaces, these products require precise 3D rendering, speed, compactness, flexibility, and realism. However, reconstructing 3D models from a single image remains challenging. Previous approaches that have shown success in this problem often utilize 2D priors in a multi-view setup [8, 14, 20, 32, 36, 38, 40, 45-47, 57]. Specifically, in human modeling, issues arise due to intricate 3D details such as facial features, clothing, and joints, which present challenges for deep learning methods. Early methods addressed these challenges using a learned Signed Distance Function (SDF) on a human template to predict detailed 3D meshes [53, 70]. Later works incorporated Neural Radiance Fields (NeRFs) to capture texture details [24, 61], or leveraged pre-trained diffusion models to generate dense views from a single frontal image, reducing prediction ambiguity [7, 16, 22, 61, 65, 71]. However, they typically suffer from low speed, hindering real-time deployment.\nIn this work, we present GST (Gaussian Splatting Transformer), illustrated in Fig. 1, a direct method that learns to predict 3D Gaussian Splatting [28] for 3D representation, allowing for fast rendering and flexible editing abilities compared to others. Our method does not rely on diffusion priors and is, therefore, capable of near real-time predictions. This is essential for downstream applications and ensures that the inference can be easily incorporated with prior-based approaches. GST leverages multi-view supervision instead of precise (and expensive) 3D point clouds. Despite this, it predicts accurate 3D joint and body poses while maintaining the perceptual quality of renderings from novel views. Table 1 summarizes the characteristics of prior work.\nGST is inspired by recent works on single view 3D reconstruction [56]. However, the complexity of human pose in 3D space poses significant challenges to the direct applications of methods that associate one 3D point (or Gaussian) to each pixel. Therefore, we augment our model also to predict the pose parameters of the SMPL [37] model. The SMPL model is used as the scaffolding on which the Gaussians are positioned and rendered. Each Gaussian is loosely tied to a vertex on the SMPL model by an offset. This has two advantages. First, it provides a good initialization of the density and pose of the Gaussians, including back faces, which are notoriously difficult for single-view methods. Second, we find that the joint optimization of pose and appearance also improves the SMPL pose prediction.\nTo the best of our knowledge, GST is the first work that efficiently combines accurate 3D human prediction with improved visual quality, utilizing only multi-view supervision and without relying on diffusion priors. In summary, our contributions are the following:\n1) We propose GST, a 3D human body model prediction method that does not rely on diffusion priors and performs novel view synthesis from a single image input. This makes it particularly amenable to real-time modeling tasks, where multiple views are uneconomical or impractical.\n2) We evaluate our method and compare it to other state-of-the-art models. Although prior methods only solve for 3D pose estimation or 3D reconstruction, our method still performs equally or better on perceptual and 3D pose estimation metrics without 3D supervision."}, {"title": "2. Related work", "content": "3D Reconstruction using Image Priors. In the area of prior-based 3D reconstruction, contemporary zero-shot text-to-image generators [2, 14, 48, 49, 51, 52] have shown significant improvement by leveraging enhanced synthesis priors [6, 9, 39, 45, 60]. DreamFusion [45] stands out as a pioneering work that distilled a pre-existing diffusion model [52] into a NeRF framework [3, 41] using text prompts. This innovation spurred further research in both text-to-3D synthesis [8, 32] and image-to-3D reconstruction [36, 38, 46, 54]. The latter approach leverages supplementary reconstruction losses focused on frontal camera perspectives [36] and subject-specific diffusion guidance [46, 47]. In addition, task-specific priors have been explored [23, 26, 50], as well as additional control mechanisms [40]. More recently, Gaussian-Splatting approaches [21, 28] have improved the efficiency of 3D generation optimization through rapid Gaussian Splatting rasterization [57, 58, 68]. In contrast, our method GST does not rely on diffusion priors, providing a simpler and more cost-effective framework specialized for human 3D reconstruction and jointly predicts the precise internal 3D human body joints.\n3D Human Pose Estimation. Many approaches in the literature focus on predicting 3D human pose and shape from a single image [10, 18, 27, 30, 33, 34]. Relevant for our work are the approaches that directly regress the body shape and pose from a single image. The first work to introduce this approach was HMR [27], which uses a CNN to regress SMPL [37] parameters. Dedicated designs have been proposed for the HMR architecture; HoloPose [19] suggests a pooling strategy based on the 2D locations of body joints, while HKMR [17] relies on SMPL hierarchical structure to make predictions. PARE [29] introduces a body-part-guided attention mechanism to handle occlusions better, and Py- MAF [66, 67] incorporates a mesh alignment module for"}, {"title": "3. Gaussian Splatting Transformers (GST)", "content": "This section presents our methodology for reconstructing 3D human models from a single image using Gaussian Splatting Transformers (GST), as illustrated in Fig. 2."}, {"title": "3.1. Architecture", "content": "Our model predicts 3D Gaussian splatting parameters from a single input image using a transformer architecture, including tokenization, processing through transformer blocks, and"}, {"title": "Image Encoder Architecture.", "content": "Our backbone follows HMR2 [18] and uses a ViT [11] to map an image to a series of visual tokens. The input is an RGB image $X \\in \\mathbb{R}^{H\\times W\\times 3}$, which is divided into non-overlapping patches $p_j \\in \\mathbb{R}^{p\\times p\\times 3}$, with $j\\in \\{1, . . ., HW/p^2\\}$. The patches are vectorized and affinely transformed into patch tokens $x_j \\in \\mathbb{R}^{d}$.\nThe patch tokens are processed through a series of Transformer blocks [59]. The final output is a set of tokens $y_j \\in \\mathbb{R}^{d}$ encapsulating the transformed image information."}, {"title": "Human Shape Representation.", "content": "The SMPL model [37] represents the 3D human mesh shape as a mesh. SMPL is a low-dimensional parametric model defined by pose parameters $\\theta \\in \\mathbb{R}^{24\\times 3\\times 3}$ and shape parameters $\\beta \\in \\mathbb{R}^{10}$, outputting mesh vertices' 3D positions $v = SMPL(\\theta, \\beta) \\in \\mathbb{R}^{6890\\times 3}$."}, {"title": "Decoder Architecture.", "content": "We build on HMR2 [18], which predicts the SMPL representation $(\\theta, \\beta)$ from the image representation $y$; through a cross-attention mechanism. Specifically, a single (fixed) token $t_{SMPL}$ attends to all image tokens $y_j$ through a series of cross-attention layers. An MLP decodes the token into the pose parameters $\\theta$ and $\\beta$.\nThis representation could be learned with image-pose pairs $(\\\u03a7, \\theta, \\beta)$. However, here we focus on multi-view supervision, as 3D supervision is costly and scarce.\nTo train with multi-view supervision, the model needs to generate an image. We use recent advances in fast neural rendering: Gaussian Splatting [28]. This scene representation is defined by a set of 3D Gaussians, each characterized by a mean position $\\mu \\in \\mathbb{R}^{3}$, a covariance matrix $\\Sigma \\in \\mathbb{R}^{3\\times 3}$, the opacity $\\alpha \\in \\mathbb{R}$ and a colour $c \\in \\mathbb{R}^{3}$.\nWe link the 3D body shape and pose with the Gaussian scene representation, such that each vertex $v_n$ in the mesh is assigned a Gaussian $G_n = (\\mu_n, \\Sigma_n, \\alpha_n, c_n)$. We allow the Gaussians to move away from the original vertex positions by a learned offset $\\delta_n$ to model clothes and other visual shape features that the SMPL model cannot capture.\n$\\mu_n = v_n + \\delta_n,$\nThis combination ensures the 3D model captures both shape and appearance, allowing more realistic reconstructions.\nSimilar to prior work [56], we factorize and simplify the covariance into the product of a rotation matrix and a diagonal matrix, enforcing a reduced number of degrees of freedom from 9 to 6: $G_n \\in \\mathbb{R}^{14}$.\nIt is theoretically possible to assign five tokens per Gaussian, one for each parameter: rotation, offset, scale, color, and opacity. However, this would result in over 34k tokens, which is computationally infeasible to decode with a standard Transformer. We thus group vertices into K groups, reducing the number of tokens to 5K + 1 (in practice, we set K = 26). As discussed before, the additional token is used to predict the SMPL shape parameters."}, {"title": "3.2. Loss Functions", "content": "We use a combination of losses to train our model to ensure accurate and visually realistic 3D reconstructions."}, {"title": "Image Reconstruction Loss.", "content": "We use a combination of Mean Squared Error (MSE) to measure the difference between the M multi-view ground truth images $\\hat{I}_i$ and rendered images $I_i$, a perceptual loss to capture high-level features and textures with LPIPS metric [69], and a masked loss on the rendered opacity $I_\\alpha$ to remove background splats [68]:\n$L_{img} = \\frac{1}{M} \\sum_{i=1}^{M} (\\| I_i - \\hat{I}_i \\|_2 + \\lambda_{perceptual} \\cdot LPIPS(\\hat{I}_i, I_i) + \\lambda_{\\alpha} \\cdot \\| M_i - I_{\\alpha} \\|_2),$\nwhere $M_i$ is the background mask of the ground truth images $\\hat{I}_i$, and $\\lambda_{perceptual}$ and $\\lambda_{\\alpha}$ are weighting hyperparameters for the perceptual and transparency losses respectively. The transparency loss is necessary to reduce floating Gaussians that do not contribute to the foreground object."}, {"title": "Gaussian Tightness Regularization.", "content": "To ensure that the predicted Gaussian Splats in Sec. 3.1 follow the SMPL parameters closely, we introduce a Gaussian tightness regularization that ensures the generated Gaussian splats [28] do not diverge and remain faithful to the underlying SMPL parameters as follows:\n$L_{tight} = \\frac{1}{\\V} \\sum_{n=1}^{V} \\|\\delta_n\\|^2,$\nwhere $\\delta_n$ is defined in (1) and V = 6890 is the number of Gaussian splats (number of vertices in SMPL).\nThe total loss function is a weighted sum of the image losses (MSE, perceptual, and alpha) and tightness:\n$L = L_{img} + \\lambda_{tight} L_{tight},$\nwhere $\\lambda_{tight}$ is the weighting hyperparameter for the tightness regularization. As we show later in Sec. 5.3, this regularization plays an important role in the precision of the 3D human body predicted by GST. By minimizing this combined loss, our GST model learns to generate accurate and visually pleasing 3D reconstructions from a single image."}, {"title": "4. Experiments", "content": "In this section we describe our evaluation setup and the baselines for our comparisons."}, {"title": "4.1. Datasets and Metrics", "content": "Datasets. Similar to previous works [24], we utilize four comprehensive human datasets for evaluation: THuman [72], RenderPeople [1], ZJU MoCap [43], and HuMMan [4]. For ZJU MoCap, the dataset is divided following the SHERF setup [24]. Similarly, for HuMMan, we adhere to the official split (HuMMan-Recon), using 317 sequences for training and 22 for testing, with 17 frames sampled per sequence. For THuman, we select 90 subjects for training and 10 for testing, and for the RenderPeople dataset, we randomly sample 450 subjects for training and 30 for testing. Those four datasets used for evaluation above are all small in terms of subject diversity. To showcase the capabilities of GST on a large dataset, we train our GST also on the TH21 dataset [63], which contains 2,500 3D scans, with diverse subject diversity. We randomly select 200 scans for evaluation.\nEvaluation Metrics. When the Ground Truth 3D SMPL parameters are available as in RenderPeople [1] and HuMMan [4], we adopt 3D Human Joints precision MPJPE as a metric [27]. MPJPE refers to Mean Per Joint Position Error: the average L2 error across all joints after aligning with the root node. To quantitatively assess the quality of rendered novel view and novel pose images, we report peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS)[69]. Consistently with prior works[15, 24, 72], we project the 3D human bounding box onto each camera plane to derive the bounding box mask, subsequently reporting these metrics based on the masked regions.\nBaselines. In addition to earlier works on Human NeRF with multi-view setting, NHP [31] and MPS-NeRF [15], we compare to recent single-image methods SHERF [24] for novel view synthesis and HMR2 [18] and TokenHMR [12] for 3D Human reconstruction precision. Different from SHERF, our method does not take as input ground truth SMPL parameters, therefore we adapt SHERF to use HMR2 [18] or TokenHMR [12] SMPL predictions for a fair comparison to our method. We also include a fast and salable Splatter Image [56], a state-of-the-art single image 3D reconstruction method for novel view synthesis tables."}, {"title": "4.2. Implementation Details of GST", "content": "Our model follows the implementation of HMR2 [18] for the predictions of SMPL parameters. We extend the HMR2 decoder implementation to process some additional learnable tokens for the predictions of the Gaussian parameters. The Gaussian parameters (color, rotation, scale, opacity, offset) are predicted for K = 26 groups of 265 Gaussians. The token output is passed through a linear layer to obtain the final parameters. We use pre-trained weights from HMR2"}, {"title": "5. Results", "content": "In this section, we discuss the results obtained on four datasets in various evaluation settings."}, {"title": "5.1. 3D Human Shape Results", "content": "The primary focus of this work is the ability to infer a precise 3D human body from a single image without explicit 3D supervision. We show quantitative results in Table 2 on RenderPeople [1] and HuMMan datasets [4]. We compute the MPJPE error with respect to the ground truth SMPL joints before and after training. The results show that without explicit 3D supervision, our training improves the quality of the pose estimation from the pretrained HMR2 [18] and TokenHMR [12]. Furthermore, Fig. 3 shows some examples of our predictions in comparison with the HMR2 initialization and the ground truth SMPL pose. Our poses visually appear better aligned to the ground truth, emphasizing the results in Table 2. During training, the shape of the SMPL models also changes, with the 3D human shape becoming thinner. SMPL models human body shape without clothing. Our method decouples the body shape from additional layers, such as clothing. We hypothesize that this leads the model to estimate the underlying body shape of the human, effectively using the offsets to model clothes and other deformations."}, {"title": "5.2. Novel View Synthesis Results", "content": "We evaluate our method in the task of novel view synthesis across 4 datasets and compare the results with SHERF [24]. For a fair comparison with our method, which does not assume ground truth SMPL parameters are available, we evaluate SHERF using the estimated HMR2/TokenHMR pose and shape parameters instead of the ground truth ones. Our results are in Tables 2 and 3. Visual results are shown in Figures 4, 5 and 6. Note that the underlying 3D body is consistent, despite a slight blurriness of the Gaussians, and follows precise 3D geometry.\nTo showcase the capabilities of GST on a large dataset, we train our GST on multi-view images rendered from the TH21 dataset [63], which contains 2,500 3D scans and shows the results on 200 randomly sampled test scans in Table 4 and Fig.7. It clearly shows less blurriness than the other datasets. For reference, we include Splatter Image [56] in Table 4, where our GST predicts precise 3D body pose and shape in addition to the renderable representation unlike Splatter Image."}, {"title": "5.3. Ablation and analysis", "content": "Ablation Study. We present an ablation study of different design choices and key elements in our architectures and losses and their effect on the 2D and 3D results of a single image to 3D of humans on the HuMMan dataset [4] in Table 5. For these experiments, we report PSNR, SSIM and LPIPS metrics computed on the entire image. It shows the importance of combining the LPIPS, tightness, and transparency loss on the final 3D precision while maintaining the"}, {"title": "6. Conclusions and Discussion", "content": "In this paper, we introduced GST, a novel approach for human 3D representation that predicts 3D Gaussian Splatting [28], enabling fast rendering with accurate poses. GST leverages multi-view supervision to accurately predict 3D joint and body poses while preserving the perceptual quality of novel view renderings. This dual capability combines precise pose estimation with high-quality rendering, bridging two research paradigms and showcasing the benefits of our approach (See Fig. 3).\nLimitations. The main limitation in our method is the requirement of multi-view datasets to train. Another issue is the slight blurriness that appears on some of the renderings"}, {"title": "A. Additional Results and Analysis", "content": "A.1. Additional Ablations\nIn addition to the ablations described in Table 5 in the main paper, we report here three variations to the GST model that did not result in a performance improvement. The ablations are provided in Table I.\nMore Gaussians. The first design change we tested is an increase in the number of Gaussians per vertex. We increase the number of splats by predicting two or three independent offsets per vertex. Because random initialization breaks the symmetry, the model can learn to move each splat independently even though all two/three are anchored to the same vertex. Contrary to our assumption, an increase in the number of splats did not result in a increased visual quality of the renderings.\nSetting Opacity to 1. Predicting opacity is not strictly necessary to render humans, therefore we tried simplifying the model by removing this parameter. We removed the opacity prediction during training and manually set the opacity to 1 for all the Gaussians.\nSingle-view + Multi-view Images. Next, to increase the subject diversity in the small datasets we use, we tried including some single view images in our training pipeline. For this experiment, we use crops of images containing humans from the MSCOCO dataset [35]. The single view images are used for training together with the multi-view images from the original dataset. For the single view images, the model predictions are supervised using the same input image. The results do not show any notable improvement."}, {"title": "A.2. Overfitting Example", "content": "To test that the number of Gaussians is sufficient to produce sharp details, we train our model to overfit a single data sample. We obtain an almost perfect reconstruction with PSNR of 41. Image I shows examples of the renderings we obtained. This result confirms our assumption that with a large enough dataset, our model would be able to learn sharper details than it currently learns on the small scale datasets."}, {"title": "A.3. Additional Details for TH21 Experiment", "content": "For the TH21 [63] experiment in Table 4 in the main report, we use 72 views rendered in a loop around the subject. We train both our method and Splatter Image [56] using 256x256 images. Despite our model performing worse than Splatter Image in terms of visual metrics, our model also predicts the SMPL paramters for 3D pose estimation. This is both useful for downstream tasks, but also ensures that the underlying 3D shape is plausible for a human. This difference can be noticed in the examples in Figure II, where GST can reconstruct a plausible human shape despite the uncommon input pose, while Splatter Image fails to reconstruct arms and legs."}, {"title": "A.4. HMR2 Finetuning Comparison", "content": "As we discussed in the main paper, we train GST starting from a pretrained version of HMR2 [18] and the resulting model almost halves the MPJPE error on the two datasets, compared to the original pretrained version. We now instead compare GST with a finetuned version of HMR2 to test the quality of the 3D pose estimation of our method.\nWe finetune HMR2 on the two datasets in Table 2 in the main paper: HuMMan [4] and RenderPeople [1]. The results are reported in Table II. To reproduce a similar training setup to our method (that does not require any 3D ground truth annotations), we finetune HMR2 using only 2D keypoints annotations. We use images from all views in the dataset, but restrict the supervision to only use the 2D keypoints loss. The results show that the 2D information alone is not enough for HMR2 to improve the quality of the 3D pose estimation on the two datasets, and the finetuned model MPJPE error is worse than the pretrained one.\nFor completeness, we also report the errors when finetuning HMR2 with additional 3D annotations: 3D keypoints and ground truth SMPL parameters. We would like to emphasize that we think this is an unfair comparison to our method, since our method does not use ground truth SMPL parameters or 3D keypoints for training. The MPJPE of the HMR2 version finetuned with 3D data is only 7mm better than ours on RenderPeople and 6mm better than ours on HuMMan."}]}