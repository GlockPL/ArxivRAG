{"title": "Human-Readable Programs as Actors of Reinforcement Learning Agents Using Critic-Moderated Evolution", "authors": ["Senne Deproost", "Denis Steckelmacher", "Ann Now\u00e9"], "abstract": "With Deep Reinforcement Learning (DRL) being increasingly considered for the control of real-world systems, the lack of transparency of the neural network at the core of RL becomes a concern. Programmatic Reinforcement Learning (PRL) is able to to create representations of this black-box in the form of source code, not only increasing the explainability of the controller but also allowing for user adaptations. However, these methods focus on distilling a black-box policy into a program and do so after learning using the Mean Squared Error between produced and wanted behaviour, discarding other elements of the RL algorithm. The distilled policy may therefore perform significantly worse than the black-box learned policy. In this paper, we propose to directly learn a program as the policy of an RL agent. We build on TD3 and use its critics as the basis of the objective function of a genetic algorithm that synthesizes the program. Our approach builds the program during training, as opposed to after the fact. This steers the program to actual high rewards, instead of a simple Mean Squared Error. Also, our approach leverages the TD3 critics to achieve high sample-efficiency, as opposed to pure genetic methods that rely on Monte-Carlo evaluations. Our experiments demonstrate the validity, explainability and sample-efficiency of our approach in a simple gridworld environment.", "sections": [{"title": "1 Introduction", "content": "While Deep Reinforcement Learning (DRL) becomes a viable method to generate system controllers in an automatic manner [1, 2], their broader adoption becomes hindered by the lack of transparency and explainability. Since the DRL agent behaviour is computed by a black box model, such as a neural network, the exact method used by the network to map a state to an action is often too complex and beyond human comprehension [3, 4]. For control engineers who require specific guarantees from the controller (stability, robustness, ...) this lack of transparency is unacceptable, leading to low adoption of DRL in their workflow [5]."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Reinforcement Learning", "content": "Reinforcement Learning (RL) is a machine learning paradigm used to solve sequential decision problems where each step is taken at a timestept \u2208 [0, tmax] [17] wit tmax as time horizon. These problems are modeled as a Markov Decision Process (MDP) [18], a control problem scheme represented by a tuple (S, A, Pa, Ra) with S the state space, A the action space, Pa the probability distribution of state transitions given action a and Ra the immediate reward given after performing a in the environment. The transition between states is described by Pas,. For the formulation of an MDP, we assume the transitions of states to be Markovian, with Prob{st+1 = s',rt+1 = r | St, at}. This means the transitions of the states are not influenced by past transitions.\nAt any timestep in the MDP a state st is observed. The policy at predicts an action at for that observation. A new state of the environment St+1 \u2208 S is observed together with reward signal rt+1 \u2208 R. This can consist of both positive and negative values and is provided by the user as reward function Ras, expressing the control objective. The objective of a Reinforcement Learning agent is to learn a policy that maximizes the discounted sum of rewards R(T) =\n\u2211r*rt+1, with \u03b3\u2208 [0,1[ as the discount factor."}, {"title": "2.2 Q-learning", "content": "A well-known approach to Reinforcement Learning, at the basis of our work, is Q-Learning. The quality of an action at under a policy is given by a Q-Value Q(\u03c2, \u03b1), the expected return obtained by executing the at in st, and then following policy \u03c0. The Reinforcement Learning agent iteratively refines estimates of the Q-Values. The update rule is given by:\n\u03b4 = rt+1 + YQ(St+1, at+1) \u2013 Q(St, at) (TD error)\nQ(st, at) \u2190 Q(st, at) + \u03b1\u03b4\nwith learning rate a \u2208 [0,1]. Selecting an action at under the optimal policy \u03c0*(st) is obtained by at = argmax Q* (st, a) with Q* the converged optimal Q-Value function."}, {"title": "2.3 Policy gradient", "content": "Q-Learning is a value-based RL method because it learns the value of actions (how good they are). Another approach to RL is Policy Gradient, a policy-based RL method, that directly learns the best parameters of a parametric policy \u03c0\u03bf, such that the agent achieves the highest-possible returns [19]:\nRt = \u2211rt'\nVe =  \u2211Rtrt'\nwith Tmax the time-step at which one episode finishes. Policy gradient using Ve is applied after the episode, as it needs the Monte-Carlo sum of rewards obtained"}, {"title": "2.4 Use of black-box models in RL", "content": "Traditionally, an RL policy is represented by a table of Q-values for discrete state and action spaces. To learn in continuous spaces, a discretization function needs to be applied. The resulting discretization error could be minimized by extending the table size at the cost of memory, which explodes for multi-variable observation spaces. Replacing the table with an artificial neural network addresses this issue since it is capable to generalize between encountered inputs and have been considered universal function approximators[22].\nHowever, due to their outputs being the result of a large number of simple operations (additions, multiplications, simple non-linear functions), using neural networks comes at the cost of losing model transparency, due to the sheer number of operations and seemingly-arbitrary numbers appearing in neural networks. To address this lack of explainability, the recent field of Explainable Reinforcement Learning (XRL) has prompted many researchers to come up novel methods of representing policies or critics [7]. Various algorithm generate explanations at different moments (from the beginning, during or after training), and consider either global or local explanations. In this paper, we focus on global explanations, with the aim of producing a program that can be \"copy-pasted\" in a"}, {"title": "2.5 Genetic Programming", "content": "Optimizing via evolution is performed by executing iterations of the Genetic Programming (GP) process or generation loop. The starting condition is a population of individuals with gi being the ith individual's genome. This genome represents model parameters to be optimized (polynomials, weights, ...) or an encoding of the model itself (program statements). It is up to the user to translate the problem into a representation that fits the GP process, and one contribution of our paper is the representation of programs as a sequence of real-valued genes. A fitness function is defined, encapsulating the objective to be optimized in order to solve the problem.\nAt the beginning of each generation, a number of individuals are selected in order to perform crossover. These parents will exchange genetic material with each other to produce offspring that share their characteristics. Afterwards, mutation is applied to the whole population to encourage random exploration in the genome. From this phase, advantageous behaviour can arise that is passed through the generations. It is often used to look beyond the peaks of local optima. In the selection phase, the fitness function is applied on each individual. Those that perform worst are eliminated. The best scoring individuals survive and will contribute to the next generational loop.\nThe optimization ends when the user-specified number of generations is met or some performance threshold. The individual achieving the highest fitness is considered as the best individual. However, in the context of generating programs, individuals with similar performances could also be considered if other criteria are considered (program length, limited nesting, ...)."}, {"title": "3 Related work", "content": ""}, {"title": "3.1 Programmatic Reinforcement Learning", "content": "In the recent past, several attempts have been made to synthesize programs from an RL agent. Verma et al. introduced a template-based search over a set of programmatic policies [11]. By using an oracle network from a trained DRL agent, they steered the search for fitting variable values. However, this method is not so flexible as the templates are user-defined at the start of training, which could be non-optimal. Nevertheless, they showed the approach is well suited for PID-like programs on both a racing game and a classic control environments. Trivedi et al. propose to learn program embeddings using a variational autoencoder (VAE) [12]. This model first learns program embedding by reconstructing source code using several loss functions. Afterwards, it can be used to directly learn a programmatic policy by interacting in the environment and suggesting candidate programs based on return maximization. Hein et al. incorporate"}, {"title": "3.2 Genetic Reinforcement Learning", "content": "Genetic programming, besides evolving surrogate models, has been used to optimize other components of the Reinforcement Learning process. Niekum et al. created new reward functions using PushGP, a stack-based programming language [23]. Using a set of simple operators, they could formulate a better reward for dynamic gridworld environments. This approach captures common features among the different environments, allowing for hierarchical decomposition. Finally, Sehgal et al. used GP to optimize the hyperparameters of a DDPG agent on MuJoCo robotic environments [24, 25]."}, {"title": "4 Evolving programs", "content": "Our contribution builds on TD3 and Genetic Programming to implement the actor of a Reinforcement Learning agent as a program, expressible in source code. Our method differs from related work in two key aspects:\n1. The program replaces the TD3 actor, and is optimized using gradients of actions produced by the critics. This allows the actor to directly optimize the returns obtained by the agent, as opposed to approximate some black-box policy, leading to high-quality programs being produced in terms of reward.\n2. The program is trained using gradients produced by the TD3 critics, not by direct interaction with the environment. This makes our method several orders of magnitude more sample-efficient than other programmatic RL approaches, that evaluate individuals in the Genetic Algorithm population by performing rollouts in the environment.\nWe now introduce how we represent programs as a list of real values (the genome), execute these programs, and integrate them as the actor of TD3."}, {"title": "4.1 Representing programs as sequences of real values", "content": "At the start of the training process, a population of genomes is initialized as a two-dimensional array of size num_individuals \u00d7 num_genes. Each ith genome gi E Gmax is a selection of num_genes random floats with values in the range of]-len(operators),max] from the gene space G which represents an encoding"}, {"title": "4.2 Optimization landscape", "content": "The representation of the programs has been designed with some specific aspects that improve learning:\n1. Abrupt changes in program behavior following a small mutation of a gene makes the optimization landscape rigid, with many plateaus. To address this issue, we add some stochastic aspects in the program 1.\n2. To prevent random programs (when the agent has not learned yet) from biasing the behavior of the agent, we ensure that random programs produce an expected value of 0. We do that by ensuring that literals have no definite sign (the sign is sampled at random at runtime), so they are not biased towards positive values. Functions that have an image biased towards positive or negative numbers also have a negated version. For instance, there exist sqrt and -sqrt.\nTo smooth the optimization landscape, we introduce stochasticity in the mapping from real value to operators. Instead of casting the real value to an integer, and using that integer to identify an operator, we identify the operator with:\no = [g + x ~ U(-0.5, 0.5)\u300d"}, {"title": "4.3 Execution", "content": "To execute a program, a simple stack-based execution is used. Given is the input observed state s, and a genome gi. The stack is pre-populated with [s] \u00d7 20, so input values are available many times on the stack. This encourages operators at the beginning of the genome to use state variables, leading to more state-dependent and reactive programs. Genes are then interpreted as described in the previous section: either as literals of random sign, or as operators sampled as described above.\nLiterals are pushed onto the stack. When an operator is encountered, as many values as required operands are popped from the stack. If a stack underflow happens, the program is considered invalid and given a low fitness. When the proper amount of operands is retrieved, the operator is applied and the result pushed back on the stack.\nAt the end of execution, the result of the program is the current top of stack value."}, {"title": "4.4 Critic-Moderated evolution", "content": "We provide an overview of our method in figure 3 together with pseudocode 1. To perform the evolutionary loop, we use the PyGad library [26]. Used hyperparameters during the evolutionary loop can be found in appendix 1.\nIn this paper, we assume both the state space and action spaces to be continuous. When the action space has several dimensions, we learn one program per action dimension.\nEvery policy_freq time-steps, we optimize the programs according to the following procedure. We first use both critics of TD3 to produce improved actions A* for a batch of states S. For this, we query the current programs for current actions \u00c2 for S. We then ask the two critics for Q-Value estimates QA(S) and QB(S). We compute an overall program quality metric by averaging the Q-Values over QA and QB, and over states. This produces a single real value. All these operations are performed with autograd enabled (using PyTorch in our case [27]) which automatically computes gradients when performing backward passes through the critics. We can then retrieve the gradient of that real value with regards to the actions \u00c2 produced by the programs, leading to VA. Slightly improved actions A* are then produced by computing A* = A + VA\nWe now set \u00c2\u2190 A* and repeat the process above 50 times. This forms a sort of 50-step gradient ascent algorithm that, starting from the current output of the programs, follows the TD3 critics to lead to better actions. This optimization process on a critic is akin to the CACLA method proposed in [28]. We stop that process if A* becomes too different from the actions produced by the programs (when the L1 norm is above 1), akin to the trust region of TRPO proposed in [29]."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Environment", "content": "To validate our approach, we used an environment called SimpleGoal (fig. 4). This navigation task is performed in a bounded continuous space of size 1 \u00d7 1 where the agent needs to take steps towards a goal area located at x < 0.1, y < 0.1. The initial starting position is random. The observation space is the current (x, y) coordinate of the agent. The action space is in the range [-1,1] and defines the change in x and y for the next time-step, with dx = 0.1a0 and dy = 0.1a1. At each timestep, the reward rt = 10*(old_distance-new_distance) is calculated based on progress in lowering Euclidean distance towards the goal. If the goal area is reached, an additional reward of 10 is given and the episode terminates. A forbidden area exists at the center of the environment, at coordinates 0.4 < x < 0.6,0.4 < y < 0.6. Entering this region terminates the episode with a reward of -10. Otherwise, episodes terminate after 50 time-steps."}, {"title": "5.2 Training", "content": "To evaluate our method, we trained on several runs using nodes on HPC infrastructure. We got allocated 40 cores out of a a 2x 32-core AMD EPYC 9384X node with 377 GB of memory. A run of 15.000 steps with our method took roughly 8 hours."}, {"title": "5.3 Produced programs", "content": "We took a selection of produced programs at the end and plotted their behaviour in an arrow plot (fig. 6). The full programs of each plot are listed in the appendix.\nAt first, we can see that the arrows tend to point towards the goal area in the left bottom corner. The distance of actions taken are quite small, leading to the agent taking a large amount of steps to get to the goal relative to the total distance it has to traverse. For almost all programs (except prog_1) the closer the agent is at the goal the larger the step it will take towards it. Since the environment is not strict on going out of range, the agent can take a big step towards the wall and just move along an angle at its edge. Where prog_2 and prog_4 tend to avoid the pitfall area, prog_1 and prog_3 both have the tendency to enter the area from the right. In the former ones, we also notice a tendency to get stuck in the bottom-right corner.\nWhen we examine the program notations, we can observe some patterns. First, programs tend to be quite complicated in their raw forms, with always-true conditions and a general tendency for producing constants. However, automated or manual constant propagation can be used to make the programs more readable. For all action variables we see the regular incorporation of sin and cos, probably to have a bending curve effect on the action. In general, we notice both action variables produce negative value most of the time, resulting in a direction towards (0,0)."}, {"title": "6 Conclusion", "content": "We introduced a new method, building on TD3 and Genetic Programming, for generating programs out of Reinforcement Learning agents based on a critic network. The programs are produced and tuned as part of the RL agent learning (they are its policy). Our experiments show that the learned policies are of comparable quality to black-box vanilla TD3 policies, with a sample-efficiency several orders of magnitude higher than Genetic Programming without TD3.\nOur current program representation is quite simple compared to other structures used in GP. Future research avenues include looking into tree-based or graph-based program representations and their dedicated operations. We also designed our algorithm such that the optimization landscape of the GP algorithm is smoothed and without too many local optima. Future program representations may further improve the search landscape, hopefully allowing for more readable yet more expressive policies to be learned in challenging environments. Finally, we note that the selection of operators is domain-dependent. Selecting a more suiting set of primitives would benefit the interpretability of the produced programs by the end user."}]}