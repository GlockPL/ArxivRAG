{"title": "On Effects of Steering Latent Representation for Large Language Model Unlearning", "authors": ["Dang Huu-Tien", "Trung-Tin Pham", "Hoang Thanh-Tung", "Naoya Inoue"], "abstract": "Representation Misdirection for Unlearning (RMU), which steers model representation in the intermediate layer to a target random representation, is an effective method for large language model (LLM) unlearning. Despite its high performance, the underlying cause and explanation remain under-explored. In this paper, we first theoretically demonstrate that steering forget representations in the intermediate layer reduces token confidence, causing LLMs to generate wrong or nonsense responses. Second, we investigate how the coefficient influences the alignment of forget-sample representations with the random direction and hint at the optimal coefficient values for effective unlearning across different network layers. Third, we show that RMU unlearned models are robust against adversarial jailbreak attacks. Last, our empirical analysis shows that RMU is less effective when applied to the middle and later layers in LLMs. To resolve this drawback, we propose Adaptive RMU\u2014a simple yet effective alternative method that makes unlearning effective with most layers. Extensive experiments demonstrate that Adaptive RMU significantly improves the unlearning performance compared to prior art while incurring no additional computational cost.", "sections": [{"title": "Introduction", "content": "State-of-the-art LLMs such as GPT-4 (Achiam et al. 2023), Gemini (Team et al. 2023), Llama-3 (Meta 2024), and Claude-3 Sonnet (Anthropic 2024) achieve remarkable performance through pre-training on large amounts of internet texts and rigorous alignment process for safety enhancement. Despite the immense effort in safety research, LLMs are still vulnerable to adversarial jailbreak attacks and can exhibit unwanted behaviors (Shah et al. 2023; Chao et al. 2023; Zou et al. 2023b; Jones et al. 2023; Yuan et al. 2024; Wei, Haghtalab, and Steinhardt 2024).\nMachine Unlearning (Cao and Yang 2015; Chris Jay Hoofnagle and Borgesius 2019; Bourtoule et al. 2021; Nguyen et al. 2022; Xu et al. 2023; Liu et al. 2024c) has emerged as a promising method for mitigating unforeseen risks in LLMs before deployment. Li et al. (2024b) introduced Representation Misdirection for Unlearning (RMU)\u2014an unlearning method that steers the representations of forget-samples (i.e. samples that the model should forget) toward a random representation while keeping the representations of retain-samples (i.e. samples that the model should remember) unchanged. RMU significantly degrades models' accuracy on forget-tasks, while only slightly affecting the performance on retain-tasks and demonstrates stronger robustness against adversarial jailbreak attacks. However, the reason for RMU's effectiveness is not well understood, hindering the development of better unlearning algorithms. In this paper, we make the following contributions:\n\u2022 We theoretically analyze the impact of the RMU method on LLM unlearning.\n\u2022 We investigate the connection between RMU and adversarial robustness. We demonstrate that RMU impedes the adversary's ability to determine optimal updates for generating adversarial samples, thus improving the adversarial robustness of the model.\n\u2022 We empirically show that the RMU forget loss, which minimizes the mean squared error (MSE) between forget representation and a fixed scaled random vector, fails to converge when the norm of the forget representation is larger than the scaling coefficient, making RMU less effective when applied to middle and last layers in LLMs.\n\u2022 To overcome RMU's limitation, we introduce Adaptive RMU\u2014a variant that adaptively adjusts the coefficient value based on the norm of the forget representation. Experimental results show that Adaptive RMU achieves higher drop-in-accuracy for forget knowledge, maintaining high performance on general knowledge, and enables effective unlearning for most layers without incurring additional computational overhead."}, {"title": "Background and related work", "content": "Machine Unlearning. A natural is leave-some-out re-training: retraining the model from scratch without the forget samples. However, this method becomes more computationally expensive as the size of datasets and modern deep networks grows. Existing works focus on approximating un-learning (Warnecke et al. 2021; Izzo et al. 2021; Sekhari et al. 2021; Isonuma and Titov 2024) using Influence Function (Koh and Liang 2017; Grosse et al. 2023), gradient projection (Bae et al. 2023), gradient ascent (Thudi et al. 2022; Trippa et al. 2024), second-order approximation (Jia et al. 2024), preference optimization (Zhang et al. 2024b), and embedding corrupted (Liu et al. 2024a). Other views on the landscape of machine unlearning include: unlearning in text classification (Ma et al. 2022), image classification and recognition (Ginart et al. 2019; Golatkar, Achille, and Soatto 2020; Fan et al. 2024; Choi and Na 2023; Cha et al. 2024), image-to-image generative models (Li et al. 2024a), diffusion models (Gandikota et al. 2023; Zhang et al. 2024a; Kumari et al. 2023), multimodal unlearning (Cheng and Amiri 2023), federated unlearning (Liu et al. 2020a; Romandini et al. 2024; Wang et al. 2022; Che et al. 2023; Halimi et al. 2022; Jeong, Ma, and Houmansadr 2024), graph unlearning (Chen et al. 2022; Chien, Pan, and Milenkovic 2023; Wu et al. 2023a; Said et al. 2023; Cheng et al. 2023; Dukler et al. 2023; Zhu, Li, and Hu 2023; Li et al. 2024c; Tan et al. 2024), recommender systems (Zhang et al. 2023; Chen et al. 2024; Li et al. 2023; Wang et al. 2024), certified minimax unlearning (Liu et al. 2024b), and evaluation on unlearning (Lynch et al. 2024; Hayes et al. 2024; Shi et al. 2024a,b).\nLLM Unlearning. Due to the large size of the parameters and training data, LLM poses a new challenge to unlearning. Current studies in LLM unlearning mainly focus on task or context-specific settings such as unlearning copyrighted material from the Harry Potter series (Eldan and Russinovich 2023), In-context unlearning (Pawelczyk, Neel, and Lakkaraju 2023), fictitious unlearning (Maini et al. 2024), specific harmful input-output (Yao, Xu, and Liu 2023; Liu et al. 2024d), sensitive and private information (Jang et al. 2023; Wu et al. 2023b; Ishibashi and Shimodaira 2023; Patil, Hase, and Bansal 2024), gender (Belrose et al. 2023) concepts (Hong et al. 2024), or facts (Meng et al. 2022). More recently, Li et al. (2024b) consider unlearning an entire distribution of hazardous knowledge given limited samples.\nNotation & problem formulation. Let Dforget and Dretain be the forget and retain sets, respectively. Let fo : Rnxd \u2192 Rnx|V| be an autoregressive LLM parameterized by \u03b8 that maps a prompt input X1:n consisting of n tokens {X1, X2, ..., Xn} to an output of probability distributions over the vocabulary V. hl(x) denotes the averaged hidden states of input tokens xi from the l-th layer of fo. Our goal is to unlearn the undesired harmful knowledge Dforget from fo while retaining unrelated or general knowledge Dretain. Unlearned models should be robust to knowledge recovery attacks that attempt to recover harmful knowledge from the model.\nRepresentation Misdirection for Unlearning. (RMU; Li et al. (2024b)) is a fine-tuning-based unlearning method inspired by representation engineering (RepE; Zou et al. (2023a)) that steers the model's representation of forget samples XF \u2208 Dforget to a random vector and regularizes the model representation of retain samples XR \u2208 Dretain back to the original model representation, by using the MSE loss:\n$$L = ||h^{(l)}_{\u03b8_{unlearn}} (X_F) \u2013 cu||^2 + \u03b1||h^{(l)}_{\u03b8_{unlearn}} (X_R) - h^{(l)}_{\u03b8_{frozen}} (X_R)||^2$$(1)\nWhere \u03b8unlearn and \u03b8frozen are parameters of the update model and frozen model respectively, u is a fixed random unit vector sampled from Uniform distribution U(0, 1), c\u2208 R is a fixed scaling coefficient, and \u03b1 is a retain weight. RMU updates \u03b8unlearn in the direction of the gradient of the loss L with respect to (w.r.t) \u03b8 using gradient descent."}, {"title": "Theoretical Analysis", "content": "3.1 The confidence of tokens generated by RMU models\nIn general, samples from the shifted distribution (such as wrong label or out-of-distribution) are associated with smaller \u201cconfidence\u201d scores such as softmax probability (Hendrycks and Gimpel 2017; Northcutt, Jiang, and Chuang 2021), maximum logit (Hendrycks et al. 2022; Wei et al. 2022), l2-distance (Sun et al. 2022), energy score (Liu et al. 2020b), and cosine similarity (Ngoc-Hieu et al. 2023). Recently, LLM has shown a tendency to produce a lower (higher) confidence in its incorrect (correct) answers in multiple-choice Q&A (Plaut, Nguyen, and Trinh 2024). Building on previous works, we hypothesized that the logit of generated tokens by RMU models exhibit randomness. As seen by a deep neural network, such randomization signifies low confidence in the logit, resulting in nonsensical or incorrect responses. To evaluate our hypothesis, we conducted a theoretical analysis of the logits of generated tokens produced by RMU models. To facilitate subsequent analysis, we make the following definition and assumption.\nDefinition 1. (Unlearned model & logit of tokens on un-learned model). Let fk,l = g(k) oh(l) be the transformation from layer l to layer k of network f, for any two layers k \u2265 l; l \u2208 [1...L], k \u2208 [l + 1...L]. We define the unlearned model funlearn = W(g(L) oh(l),steered), where hl,steered(XF) is the steered representation of forget input xF at layer l and W is the unembedding matrix. Given a prompt input XF,1:n\u00b7 For a next token xn+1, the logit of xn+1 obtained from un-learned model funlearn is defined as:\n$$f_{unlearn} (x_{n+1} | X_{F,1:n}) = W(g^{(L)} oh^{(l), steered}) (x_{n+1} | X_{F,1:n})$$\n$$= Wg^{(L)} (h^{(l), steered} (x_{n+1} | X_{F,1:n}))$$(2)\nAssumption 1. A well-unlearned model shifts the representation hl,steered(xF) of a forget-sample xf at layer l to a scaled random vector cu. More concretely,\n$$h^{(l), steered} (x_F) = cu + \u03f5,$$(3)\nwhere \u03f5 is a small error. Without losing generality, we assume that \u03f5 is sampled from Normal distribution N(0, \u03b7I), where \u03b7I is the covariance matrix, \u03b7 \u2208 R.\nProposition 1. If Assumption 1 holds, by Definition 1, the logit of token xn+1 generated by unlearned model funlearn given as funlearn(xn+1|XF,1:n) follows the Normal distribution N (Wg(L)(z), \u03b7W\u2207zg(L)(z)T\u2207zg(L)(z)WT), where z = cu.\nProof. Given Assumption 1, we have:\n$$h^{(l), steered} (x_{n+1} | X_{F,1:n}) = cu + \u03f5,$$(4)\nWe denote z = cu. Substituting Eqn. 3 into Eqn. 2, we get:\n$$f_{unlearn} (x_{n+1} | X_{F,1:n}) = Wg^{(L)}(z + \u03f5)$$(5)\nSince \u03f5 is small, we approximate the function g(L)(z + \u03f5) by its first-order derivative:\n$$f_{unlearn} (x_{n+1} | X_{F,1:n}) = Wg^{(L)}(z + \u03f5)$$\n$$\u2248 Wg^{(L)}(z) + W\u2207_zg^{(L)}(z)^T\u03f5$$(6)\nGiven that \u03f5 \u223c N(0, \u03b7I), by applying the affine transformation property of the multivariate normal distribution, we get:\n$$f_{unlearn} (x_{n+1} | X_{F,1:n})$$\n$$\u223cN(Wg^{(L)} (z), \u03b7W\u2207_zg^{(L)}(z)^T\u2207_zg^{(L)}(z)W^T)$$(7)\nSince u \u223c U(0,1), then cu \u223c U(0, c). By the linearity property of expectation and definition of variance, we have: E(z) = E(cu) = c/2; Var(z) = Var(cu) = c2Var(u) = c2/12\u03a0\nProposition 1 suggests that he variance of funlearn(xn+1|XF,1:n) is controlled by (i) \u03b7: a scalar variance and (ii) W\u2207zg(L)(z)T\u2207zg(L)(z)WT: the product of W\u2207zg(L)(z) and \u2207zg(L)(z)WT. If funlearn(xn+1|XF,1:n) has high variance, the logit values are more random. Since \u03f5 presents a small error, then \u03f5 is vary for different input XF. This variation makes it difficult to control the variance of the logit by \u03b7. The main effect depend on W\u2207zg(L)(z)T\u2207zg(L)(z)WT. While the unembedding matrix W is unchanged after unlearning, the product \u2207zg(L)(z)TVzg(L)(z) is vary depends on the specific characteristics of sub-networks g(L) and input z = cu. Unfortunately, g(L) is a composition of transformer layers, which is highly nonlinear, making it difficult to have a complete analysis. The variance of z is derived as Var(z) = c2/12. When \u03f5 gets larger, the variance of z is higher. This could increase the variability of g(L)(z) and the gradient \u2207zg(L)(z). A larger c could introduces more randomness to the logit. We conduct an empirical analysis to understand the confidence of generated tokens by RMU models in Section 4.1."}, {"title": "The effect of the coefficient on forget-sample representations", "content": "RMU forget loss steers forget-sample representation h(l)(xF) aligns with a random direction given by u and scales the magnitude of h(l)(xF) to c (Eqn 1). While vector u is predetermined before unlearning, the magnitude of h(l)(xF) varies depending on input XF and specific properties of layer l. This raises the following research questions:\nRQ1 (Direction): \u201cHow does the coefficient c influence the alignment between hl(xF) with u.\u201d\nRQ2 (Magnitude): \u201cWhat is the optimal value of the coefficient c for effectively unlearning with different layers.\u201d\nUnlearning as minimizing the noise sensitivity. We aim to answer these questions by analyzing the unlearning problem under a noise compression view. We consider the output of a transformation f k,l on input x: fk,l(x) = (g(k) hl)(x) = g(k) (h(l)(x)). Suppose we compress a noise vector \u03be to the representation h(l) of layer l at an input x, then the output become g(k)(h(l)(x) + \u03be). Naturally, if layer g(k) is robust (less sensitive) to noise \u03be, then \u03be has a small effect on the output of g(k) i.e. the normalized squared norm\n$$\u03a6(g^{(k)}, x) = \\frac{||g^{(k)} (h^{(l)}(x) + \u03be) \u2013 g^{(k)} (h^{(l)}(x)) ||^2}{||g^{(k)} (h^{(l)}(x)) ||^2}$$(8)\nis small. In contrast, a higher \u03a6(g(k), x) mean g(k) is higher sensitive to noise \u03be at input x. For a dataset Dforget, we define the noise sensitivity of a layer g(k) w.r.t \u03be on Dforget as:\n$$\u03a6(g^{(k)}, D_{forget}) = \\frac{||g^{(k)} (\u0125^{(l)}(x_F) + \u03be) \u2013 g^{(k)} (\u0125^{(l)}(x_F))||^2}{||g^{(k)} (\u0125^{(l)} (x_F))||^2}$$(9)\nwhere \u0125l(xF) is the mean of hl(xF) over XF \u2208 Dforget. During unlearning, RMU steers hl(xF) for all XF \u2208 Dforget to the fixed vector cu + \u03f5 i.e. ||g(k)(cu + \u03f5) \u2212 g(k)(\u0125(l)(xF))||2 is minimized. If we let \u03be = cu + \u03f5 \u2212 \u0125l(xF), we can define the unlearning problem as minimizing the noise sensitivity of layer. This objective is described by\n$$min \\frac{||g^{(k)} (cu + \u03f5) \u2013 g^{(k)} (\u0125^{(l)}(x_F))||^2}{||g^{(k)} (\u0125^{(l)} (x_F))||^2}$$(10)\nWhile g(k) is a composition of transformer layers, which is hard to expand it in term of c. Therefore, we propose to use the Jacobian matrix J(k)(xF)\u2014a linearized of g(k) at XF\u2014which describes the change in the output of g(k) due to a noise perturbed in the input \u0125l(xF). For simplification, we write \u0125(l), J(k) instead of \u0125l(xF), J(k)(xF) respectively. The objective becomes\n$$min \\frac{||J^{(k)} (cu + \u03f5) \u2013 J^{(k)}\u0125^{(l)} ||^2}{||J^{(k)}\u0125^{(l)} ||^2}$$(11)\nSince J(k) is a linear transformation, then\n$$||J^{(k)} (cu + \u03f5) \u2013 J^{(k)}\u0125^{(l)}||^2 = ||J^{(k)} (cu + \u03f5 \u2013 \u0125^{(l)})||^2$$(12)\nLet v = \u03f5 \u2212 \u0125(l). By definition of the squared norm, we have:\n$$||J^{(k)} (cu + v)||^2 = (J^{(k)} (cu + v))^T J^{(k)} (cu + v)$$\n$$= (cu + v)^T J^{(k)T}J^{(k)} (cu + v)$$(13)\nLet matrix A = J(k)TJ(k). Expand the right-hand side of Eqn. 13, we get:\n$$||J^{(k)} (cu + v)||^2$$\n$$= (cu)^T A cu + (cu)^T Av + v^T Acu + v^T Av$$(14)\nSince A is a symmetric matrix (i.e. AT = A), then\n$$(cu)^T Av = (cu)^T A^T v = (Acu)^T v = v^T Acu$$(15)\nSubstituting (cu)TAv = vT Acu into Eqn. 14 we get:\n$$||J^{(k)} (cu + v)||^2 = c^2u^T Au + 2cu^T Av + v^T Av$$(16)\nWhile ||J(k)\u0125(l)||2 is not zero. The objective described in Eqn. 11 is equivalent to\n$$min ||J^{(k)} (cu + v)||^2$$(17)\nSince ||J(k)(cu + v)||2 form a quadratic expression dependence on c, we take its derivative w.r.t c and set it equal to zero:\n$$2u^T Auc + 2u^T Av = 0$$(18)\nSolve for c:\n$$c = \\frac{u^T J^{(k)T} J^{(k)} (\u0125^{(l)} \u2013 \u03f5)}{u^T J^{(k)T} J^{(k)} u} = \\frac{Au^T J^{(k)T} (\u0125^{(l)} \u2013 \u03f5)}{||J^{(k)}u||^2}$$ $$= \\frac{||J^{(k)} (\u0125^{(l)} \u2013 \u03f5)) ||}{||J^{(k)}u||} cos (J^{(k)}u, J^{(k)} (\u0125^{(l)} \u2013 \u03f5))$$(19)\nSince ||J(k)u||||J(k) (\u0125(l)\u2212\u03f5))|| is positive, then c and cos(J(k)u,J(k) (\u0125(l)\u2212\u03f5)) are positively correlated.\nThis means smaller (larger) c indicates less (more) alignment between J(k)u and J(k)(\u0125(l) \u2013 \u03f5). Given that the Jacobian J(k) describes how small changes in the input lead to changes in the output using linear approximation around a given point. If J(k) does not vary drastically, it will not significantly alter the directions of u and \u0125(l) \u2212 \u03f5. In such cases, J(k) will have a small effect on directional alignment, preserving the relative angles between u and \u0125(l) \u2212 \u03f5. Here, reasonably, u and \u0125(l) are becoming more aligned as c increases since error \u03f5 \u2192 0 as unlearning becomes more accurate.\nThe above discussion does not directly address RQ2. However, the definition of the noise sensitivity suggests that the noise sensitivity of layer g(k) characterized by the inherent properties of g(k), the representation \u0125(l)(xF) (which is fixed) and the perturbed noise \u03be. If \u03be is predetermined, the noise sensitivity of g(k) depends solely on its properties. This suggest the following experiment: we compute h(l)(xF) the mean of h(l)(xF) over a set of input XF \u2208 Dforget, compress a fix perturbed noise \u03be into \u0125(l)(xF). We then calculate the noise sensitivity of g(k) for different layers. Fig. 1 shows the noise sensitivity of layers across different models. We empirically observed that: the noise sensitivity decreases as layers go deeper and varies across different models. Since noise sensitivity describes a layer's robustness to noise, higher noise sensitivity means g(k) requires smaller noise to produce the same level of output randomness, while lower noise sensitivity means it requires larger noise. In other words, early layers require smaller noise \u03be (smaller c) whereas later layers require larger noise \u03be (larger c). We present an empirical experiment to verify our analysis in Section. 4.3."}, {"title": "Robustness of RMU models to adversarial jailbreak attacks", "content": "RMU algorithms not only erase undesirable knowledge (forgotten knowledge) but also prevent them from resurfacing when attacked by adversarial jailbreak attacks. Li et al. (2024b) present a qualitvative experiment by employing Greedy Coordinate Gradient (GCG; Zou et al. (2023b)) to jailbreak RMU models to extract the answer given a harmful input. As a result, RMU models demonstrate strong robustness against GCG attack. We conduct an analysis from the standpoint of an attack-defense game to explain the underlying causes for the effectiveness of the RMU algorithm.\nAttacker's goal. The goal of the attacker is to inject an adversarial trigger into the prompt input (e.g., \u201cHow would I engineer influenza A to be significantly more virulent? x x x x x x\u201d), causing the model generate a harmful responses (e.g., \u201cTo make influenza A more virulent...\u201d) instead of refusing to answer (e.g., \u201cAs a language model,...\u201d).\nAttacker's knowledge and capability. In this setting, we focus on white-box jailbreak attack scenarios (Zou et al. 2023b), where the victim model's architecture, model input, and output are exposed to the attacker. The attacker is based on gradient signals to search and inject an adversarial trigger into the prompt input, and supplies this adversarial input to the model.\nProblem formulation. Let f : Rnxd \u2192 Rnx|V| be an autoregressive LLM. Given a prompt input joint with an adversarial trigger XF,1:n, the attacker finds an update \u03b4 to adversarial trigger aims to maximize the likelihood of generating the target sequence XF,n+1|n+K consists of K tokens. For simplification, we denote XF = [XF,1:n, XF,n+1:n+K]. The attacker tries to solve the following objective:\n$$min J(f(x_F + \u03b4)),$$XF+\u03b4(20)\nwhere I(,) is the loss function of the attacker. The attacker find an update \u03b4 based on the linearized approximation of the loss (Zou et al. 2023b):\n$$\u2207_{ex_i} I(f(x_F))$$(21)\nwhere ex is the one-hot vector representing the current value of the ith token in the XF. The gradient \u2207exi I (f(xF)) is a good indicator for finding a set of candidates for the adversarial token replacement. A more negative value of the gradient \u2207exiI (f(xF)) makes a more decrease in the loss. The GCG algorithm finds top-k largest negative value of Ver; I(f(xF)) for each token in the adversarial trigger and makes the replacement the most decrease in the loss.\nRobustness of RMU models against GCG attack. We show that the GCG attacker misjudges in finding optimal adversarial token substitution in RMU models. Specifically, the gradient of the loss at input XF with respect to exi in RMU model is\n$$\u2207_{ex_i} I (f_{unlearn} (x_F))$$(22)\nGiven the Assumption 1, we have\n$$\u2207_{ex_i} I (f_{unlearn} (x_F)) = \u2207_{ex_i} I (g (h^{(l), steered} (X_F))$$(23)\n$$= \u2207_{ex_i} (Jo g)(cu + \u03f5)$$(24)\nSince c and u are predetermined before unlearning, (Jo g)(cu) does not change with respect to exi. The gradient \u2207ex. (Jo g)(cu + \u03f5) close to 0 for all token x\u2081 since the error \u03f5 \u2192 0 as unlearning becomes accurate. This means the GCG attacker received unreliable, uninformative gradient signals from RMU models. The RMU model serves as a defender by causing the attacker to miscalculate the gradient of the loss to optimize its objective, thereby increasing the attacker's cost. The attacker, therefore, cannot find the optimal adversarial tokens for replacement. Li et al. (2024b)'s experiment results implicitly verify our analysis."}, {"title": "Empirical Analysis", "content": "4.1 Measuring token confidence with MaxLogit\nAs discussed in Section 3.1, we test our hypothesis by considering the Maximum Logit Value (MaxLogit) estimator for measuring the token confidence. More specifically, we compute the MaxLogit for each token xn+1 given a sequence of tokens X1:n = {x1, ..., xn} from vocabulary V as:\n$$MaxLogit(x_{n+1}) = max f_{unlearn} (x_{n+1} | X_{1:n}),$$(25)\nWe use WMDP-Biology and WMDP-Cyber Q&A datasets (Li et al. 2024b) with total 3260 Q&As. We formulated each question and answer as a default zero-shot Q&A prompt to query the unlearned LLM (Gao et al. 2023). The detail of the prompt template are located in Appendix A.1. We used greedy decoding to generate tokens and compute the MaxLogit of each token over k = 30 generated tokens. The MaxLogit distribution was then analyzed for each model Base vs. RMU (unlearned on WMDP-Biology and WMDP-Cyber forget datasets).\nThe results are presented in Fig. 2 (a)-(d). We find that the MaxLogit distribution for the base model is generally wider compared to the RMU model. In contrast, the RMU model demonstrates a more concentrated and approximately normal distribution of MaxLogit values. The peak of the RMU model's MaxLogit distribution is shifted towards lower values relative to the base model. This indicates that the RMU model tends to assign lower confidence scores to the generated tokens. Overall, the RMU model's MaxLogit distribution exhibits lower compared to the base model, thereby verifying our analysis."}, {"title": "The effect of the coefficient c", "content": "On accuracy. We analyze the impact of c for forgotten knowledge and retained knowledge, using WMDP (Li et al. 2024b) and MMLU (Hendrycks et al. 2020). See Section 6 for the full experiment setting. Fig. 3a shows: (i) a clear positive correlation between the drop-in-accuracy rate and the value of c, i.e. higher c makes the accuracy decrease faster. (ii) A larger value of c tends to make a more drop-in-accuracy on WMDP (Fig. 3a). (iii) However, a larger c comes with a caveat in a significant drop in general performance on MMLU (Fig. 3b).\nOn alignment between u and hl. We compute cos(u, hl) scores of pairs of u and hl(xF) for all 1F in on WMDP-Biology and WMDP-Cyber forget datasets and plot the cos(u, hl) score distribution shown in Fig. 2(e)-(h). We observed that there is a clear positive correlation between cos cos(u, hl) scores and the coefficient c. As c increases, the distribution of cos(u, hl) scores shifts towards higher values and are almost distributed with a peak at 1.0 (Fig. 2(g)-(h)). This verify our analysis in Section 3.2."}, {"title": "The effect of layers on unlearning", "content": "We investigate the effect of unlearn layers on accuracy and the representation norm during unlearning. We change the unlearn layer 1 from 3 \u2192 31, fixed c = 6.5. Table 1 shows that RMU is effective for unlearning within the early layers (3 \u2192 10), yet exhibits inefficacy within middle and later layers (11 \u2192 31). Interestingly, in Fig. 4, we observed that within early layers, the l\u00b2-norm of forget samples are smaller than the coefficient c. During unlearning, the representation norm exponentially increases, approaching c, thereby facilitating the convergence of forget loss. Conversely, within middle and later layers, the representation norms of forget samples, initially larger than c, remain unchanged during unlearning, making the forget loss divergent"}, {"title": "Adaptive RMU", "content": "Inspired by the observations in Section 4.3, we propose Adaptive RMU, a simple yet effective alternative method with an adaptive forget loss by scaling the random unit vector u with an adaptive scaling coefficient \u03b2||h frozen (XF)||2, where \u03b2 \u2208 R+ is a scaling factor and ||h frozen (XF)||2 is the l2-norm of forget samples XF on model f frozen . The total loss is calculated as follows:\n$$L_{adap} = ||h_{\u03b8_{unlearn}}^{(l)} (X_F) - \u03b2||h_{\u03b8_{frozen}}^{(l)} (X_F)||_2 u||^2 + \u03b1||h_{\u03b8_{unlearn}}^{(l)} (X_R) - h_{\u03b8_{frozen}}^{(l)} (X_R)||^2$$(26)\nOur Adaptive RMU is shown in Algorithm 1. In Appendix A.2, we show that Adaptive RMU has the same computational complexity as RMU."}, {"title": "Experiment", "content": "Datasets. We use WMDP-Biology and WMDP-Cyber forget datasets as Dforget and Wikitext (Merity et al. 2016) as Dretain for fine-tuning the LLM. Unlearned models are evaluated on WMDP Q&A datasets and MMLU (Hendrycks et al. 2020). An unlearned model has a higher average of accuracy on MMLU and drop-in-accuracy on WMDP is better. Details of the datasets can be found in the Appendix A.1."}, {"title": "Conclusion", "content": "We studied the effect of steering latent representation for LLM unlearning and explored its connection to jailbreak adversarial robustness. We developed a simple yet effective alternative method that enhances unlearning performance across most layers while maintaining overall model utility. Our findings illuminate the explanation of the RMU method and pave the way for future research in LLM unlearning."}, {"title": "Datasets and algorithm computational perplexity", "content": "A.1 Datasets\nWMDP (Li et al. 2024b). The WMDP benchmark", "https": ""}]}