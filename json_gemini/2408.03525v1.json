{"title": "Hierarchical learning control for autonomous robots inspired by central nervous system", "authors": ["Pei Zhang", "Zhaobo Hua", "Jinliang Ding"], "abstract": "Mammals can generate autonomous behaviors in various complex environments through the coordination and interaction of activities at different levels of their central nervous system. In this paper, we propose a novel hierarchical learning control framework by mimicking the hierarchical structure of the central nervous system along with their coordination and interaction behaviors. The framework combines the active and passive control systems to improve both the flexibility and reliability of the control system as well as to achieve more diverse autonomous behaviors of robots. Specifically, the framework has a backbone of independent neural network controllers at different levels and takes a three-level dual descending pathway structure, inspired from the functionality of the cerebral cortex, cerebellum, and spinal cord. We comprehensively validated the proposed approach through the simulation as well as the experiment of a hexapod robot in various complex environments, including obstacle crossing and rapid recovery after partial damage. This study reveals the principle that governs the autonomous behavior in the central nervous system and demonstrates the effectiveness of the hierarchical control approach with the salient features of the hierarchical learning control architecture and combination of active and passive control systems.", "sections": [{"title": "1 Introduction", "content": "Animals can develop a large number of motor skills to complete various complex tasks, which stem from the hierarchical structure of their central nervous system (CNS) and the complex interaction mechanisms between multiple regions [1, 2]. Studying the hierarchical structure and mechanisms of the central nervous system can help understand the principle of autonomous movement and promote the design of autonomous robot control systems[3, 4].\nRecently, approaches to developing robot autonomous control systems inspired by the central nervous system can be divided into two groups: hierarchical model active control approaches based on hierarchical structure and bionic passive control approaches based on neural mechanisms. The hierarchical model active control approaches aim to obtain a multi-level active control system with high flexibility and adaptability (consciously participated) by mimicking the hierarchical structure of the nervous system. Its internal controller continuously and actively provides control signals based on real-time feedback information to achieve specific goals [5\u201311]. These approaches generally include two stages: In the pre-training stage, the low-level controller learns low-level skills. In the task-training phase, the high-level controller learns to use these skills for autonomous decision-making[5\u20137]. However, finding effective reusable skills in complex robot systems is difficult. Learning low-level skills from data is a promising method, but the flexibility and versatility of skills are limited by the quality of data sets[8, 9]. Collecting high-quality motion data for some special robots (such as insect robots) is also particularly difficult. The unsupervised reinforcement learning methods encourage robots to learn diverse skills through intrinsic objectives and eliminate the dependence on data. However, in complex systems with high degrees of freedom, it is usually unable to find useful skills[10]. The goal-driven methods transform low-level skill learning in complex systems into target tracking tasks by designing the target space[11]. Still, the manually designed target space (such as plane coordinates and joint angles) will limit the full exploration in the learning process. In addition, due to the need for real-time data feedback for continuous decision-making, active control systems rely heavily on sensor information.\nThe bionic passive control approaches aim to obtain a structurally simple and highly reliable passive control system (unconsciously participated) by simulating various mechanisms within the central nervous system. Through preset intrinsic internal mechanisms, the robot can spontaneously generate various behaviors, thereby overcoming many problems related to hierarchical model approaches. For example, by simulating the conditional reflex neural circuits inside the spinal cord (SC) of vertebrates and the ventral nerve cord (VNC) of invertebrates, control methods based on reflex mechanisms can directly and quickly trigger a series of rhythmic reflex behaviors[12, 13] (such as crawling, etc.) based on sensory information, enabling robots to autonomously adapt to unpredictable irregular terrain[14, 15]. Central pattern generators (CPGs) are another type of neurons or neural circuits that exist within SC or VNC. They can generate periodic motor signals through internal oscillations without a bursting input"}, {"title": "1.1 Central nervous system inspired hierarchical learning control framework", "content": "Combined with related research, we analyze the mammalian central nervous system. As shown in Fig. 1a, the central nervous system of mammals (taking humans as an example) is a three-level control system from the cerebral cortex to the spinal cord[1, 2]. The highest level includes most cortical regions, such as the prefrontal cortex, premotor area (P\u039c\u0391), supplementary motor area (SMA), posterior parietal cortex, and basal ganglia. They are responsible for processing various information and generating high-level decisions. First, the ascending nerves of the spinal cord will project proprioception of the body to the primary somatosensory cortex (S1), and the visual nerves will project visual information of the retina to the visual cortex. Areas 5 and 7 of the posterior parietal cortex process proprioception from S1 and visual information from the visual cortex, respectively, and generate abstract perceptual information. Subsequently, the prefrontal cortex decides"}, {"title": "1.2 Results of the CPG module's motion generation", "content": "The purple box in Fig. 1b shows the structure of the proposed CPG module. We use the amplitude-phase oscillator to simulate the RG layer and the desired pose solver to simulate the PF layer, and the high-level and mid-level controllers can regulate them directly and indirectly.\nThe oscillator consists of six elements acting on the robot\u2019s six legs respectively. We refer to the stable gait mode of insects[31] (Fig. 2a,b) and design the independent tripod gait phase \u03b1, which is only affected by the maximum oscillation frequency wm, as the fixed component of the mixed phase \u03c6. The adjustable phase \u03b8 provides the other component of \u03c6. This mixed phase enhances the autonomy of the CPG module and retains flexibility. Even if the higher control information is missing, the independent phase component can ensure the CPG module produces a stable forward or backward gait (Fig. 2c). In this mode, the oscillator can generate a regular amplitude signal r, tripod gait phase \u03b1, and adjustable phase \u03b8 (Fig. 2e). Amplitude r and adjustable phase \u03b8 can be adjusted by \u03bc, \u03c9 generated by the mid-level controller based on sensory feedback, thus making the CPG module generate more complex gait (the range of \u03bcw in the figure is mapped to [-1,1]). Compared with the single-phase mode in refs [20\u201322], the introduction of the independent phase enables the oscillator to generate a stable phase signal when the higher"}, {"title": "1.3 Results of the mid-level controller's skill learning and control", "content": "The mid-level controller is a reinforcement learning policy composed of neural networks, responsible for the robot active skill regulation and gait control. It receives proprioception and skill vectors from the high-level controller. Then, it generates various differential parameters \u03bc,\u03c9 for the oscillator, thus adjusting the CPG module to generate gait signals of different modes. External feedback ||z|| is proportional to the maximum internal frequency wm, which can adjust the robot\u2019s speed. Combining the unsupervised reinforcement learning algorithm[32] and CPG module, we propose a skill pre-training method to learn the mid-level policy in simulation to find diversified motor skills (Fig. 3a). The CPG module can use low-dimensional parametric action space to generate high-dimensional joint control signals so that unsupervised learning can discover useful skills in robot systems with high degrees of freedom. (The learning process is reserved for the description of the Methods).\nFig. 3b shows the movement trajectories of the robot within 30 seconds of the XOY plane under the influence of different skills. To test the control effect of the mid-level"}, {"title": "1.4 Results of the high-level controller's multi-task learning and decision", "content": "The high-level controller is also a reinforcement learning policy composed of neural networks, which is responsible for processing the robot\u2019s proprioception and environmental information, respectively, and then giving the appropriate high-level decision ah = [dmp, z] to actively command the lower levels to produce the appropriate movement pattern to deal with obstacles. dmp directly changes the morphological parameters of the CPG module through the lateral loop. At the same time, z controls the mid-level controller to change the internal rhythm of the oscillator through the ventromedial loop. It is worth noting that ||z|| determines the maximum gait frequency wm of the robot, and this information is also transmitted directly from the lateral loop to the CPG module, which reflects the regulatory function of the basal ganglia on the willingness of the voluntary movement."}, {"title": "1.5 Results of rapid recovery of movement in case of limb damage", "content": "Animals exhibit rapid motor recovery after injury through various compensatory behaviors[33]. In the context of the bionic hierarchical structure, our novel method has the reliability of passive control systems, enables the robot to swiftly relearn its high-level controller in the event of limb damage, thereby coordinating different levels to restore its motion ability. The semi-autonomous CPG module further diminishes the control system\u2019s reliance on sensor information, enhancing its robustness and resilience.\nTo simulate the effect of limb damage, we randomly disable the control and feedback signals of a leg of the hexapod robot. Fig. 5a demonstrates the robot\u2019s adaptability in crossing a gap (difficulty 0.6) when the robot\u2019s front, middle, and hind legs are broken, respectively. Regardless of which leg fails, the controller effectively guides the robot to adapt and utilize the remaining healthy legs to cross the gap and maintain a stable posture during crawling. Fig. 5b further illustrates the gait changes of the robot in this task. After the failure of the middle leg, the other five legs of the robot adjust their gait and redistribute their workload to complete the obstacle crossing. Overall, the robot\u2019s gait frequency decreases, and the"}, {"title": "1.6 Results of unknown environment adaption", "content": "After entering the unknown environment, animals can recombine the learned knowledge and solve new tasks instead of learning from scratch. The high-level active control system of the proposed method also has this advantage. As shown in Fig. 6a, the hierarchical controller trained in the simulation environment is directly deployed to the robot system without modification, and its adaptability is tested in various unknown terrains (see Supplementary Video S5 for relevant videos). The robot can move independently on the outdoor dirt ground and cross the stone path to the opposite side through the high-level controller\u2019s autonomous inference and decision-making ability. It can climb the stone bridge steeper than the learned slope task. It can adjust its shape to drill into an irregularly shaped cave and quickly climb the curb. Moreover, it can navigate autonomously in multi-objective combined terrain without being affected by limb fracture (Supplementary Section 10 introduces the composition of the combined terrain and further analyzes the motion effect of the robot).\nFig. 6b shows the change of morphological parameters adjusted by the high-level controller over time while climbing the curb. After seeing the stone curb, the robot decided to reduce the height and increase the width to ensure stability during climbing. At the same time, increase gc so that the legs can be lifted higher to climb the stone platform. After climbing the stone platform, the maximum frequency wm decreases, maintaining low sports activity to save energy consumption. Fig. 6c shows the change of skill vector z generated by the high-level controller with the number of decision steps in this process. Under the effect of the hierarchical structure of the proposed method, the high-level controller will sample reusable skills from the skill space according to the environmental information at different times and combine them into continuous signals to guide the lower-level controllers to generate appropriate gait and trajectory to deal with the new environment."}, {"title": "2 Discussion", "content": "We propose a general hierarchical learning control framework based on the structure and function of the central nervous system, which theoretically integrates active and passive control systems, and demonstrates in practice how to use hierarchical semi-active control systems to enhance the autonomous movement ability of robots. This framework provide new ideas and methods for the design and application of future robot control systems. This framework refers to the structure and mechanisms of the mammalian central nervous system, including three interdependent hierarchical controllers, which can learn various motor skills and enable robots to use sensory feedback to produce autonomous motor behavior. Taking the"}, {"title": "3 Methods", "content": "This section describes in detail the composition and learning methods of each part of the proposed hierarchical learning control framework and the application process for the hexapod robot (see Supplementary Section 1 for details of robot platform and physical simulation). The CPG module includes the oscillator and the desired pose solver. We introduce the differential equations of the oscillator and the internal stable phase embedding method, then show the desired pose solver\u2019s signal adjustment process and the robot motor\u2019s control method. Then, we give a detailed description of the pre-training method of the mid-level controller. Using the learned middle controller, we show how to get a high-level controller with autonomous decision-making ability through the two-stage learning process."}, {"title": "3.1 Half-center rhythm generator layer", "content": "To generate the basic motion rhythm signal, we use the Hopf oscillation differential equations[20, 49] to implement the RG layer of CPGs. The following first-order differential equations give the dynamic system:\nri = vi\nv = (f(\u03bci) - ri)\n\u03b8i = f(\u03c9i)\nai = wm + \u03a3rjmij sin(\u03b1j - \u03b1i + \u03a8ij),\n\u2200i, j\n                                                                                      (1)\nwhere ri is the amplitude of the oscillator, vi is the differential of amplitude, \u03b8i is the adjustable phase, \u03b1i is the tripod gait phase, \u03b1 is a positive constant representing the convergence factor, \u03bc\u03b5 and wi are amplitude and phase adjustment factors, wm is the maximum oscillation frequency. The oscillator finally produces a mixed phase \u03c6i = (\u03b1i + \u03b8i) (i = 1,2,..., 6 is the serial number of each leg). The coupling weight and bias between oscillation elements are mij and \u03a8ij. They form an additive coupling term to generate an independent tripod gait for the robot, where mij = 1, and the bias matrix is shown in the following formula\n\u03a86\u22176 = 2\u03c0\n\u239b\n\u239c\n\u239c\n\u239c\n\u239c\n\u239c\n\u239d\n0 0.5 0 0.5 0 0.5\n-0.5 0 -0.5 0 -0.5 0\n0 0.5 0 0.5 0 0.5\n-0.5 0 -0.5 0 -0.5 0\n0 0.5 0 0.5 0 0.5\n-0.5 0 -0.5 0 -0.5 0\n\u239e\n\u239f\n\u239f\n\u239f\n\u239f\n\u239f\n\u23a0                                      (2)\nDue to the effect of the coupling term, the left front leg (LF), the left hind leg (LH) and the right middle leg (RM) of the robot are a group. Their \u03b1 is the same, while the other three legs are another group, and their \u03b1 lags \u03c03 rads. This setting makes the six legs form a tripod gait. On this basis, the mid-level controller can adjust the \u03bci, wi of each leg to directly change the amplitude ri and adjustable phase di of the oscillator, then adjust the mixed phase i to make the CPG module produce different gaits. f(\u03bci), f(wi) are used to calculate the internal natural amplitude and frequency, where f(\u03bc\u03b5) = \u03bcmin + \u00b5i+1(\u00b5max \u2212\u00b5min) and f(wi) =  Wmin + wi+1(wmax \u2013 Wmin), they map \u03bci \u2208 [-1,1],wi\u2208 [-1,1] to [\u00b5min = 1, \u00b5max = 4], [Wmin = 0, wmax = wm = u(||z||)\u03a9]. u is a linear mapping, which maps the ||z|| between 0 and 1 to [0.2, 1.0]. \u03a9 is a fixed value 8Hz, which can ensure that wm is always positive, thus ensuring that the independent tripod gait phase \u03b1 is not affected by any external factors, and can always produce periodic tripod gait signals. This is different from previous work 20-22]. These methods add the external feedback signal wi and the coupling term directly and take them as the differential of a single phase. When the feedback signal is boundary value (such as f (wi) = 0), the only coupling term can not make the phase oscillate periodically, which makes the oscillator invalid.\nWe use the following formula to solve the state of the differential equations:\nr = r + (ri+1 + ri) dt 2\nv = v + (vi+1 + vi) dt 2\n\u03b8t+1 = \u03b8 + (\u03b8i+1 + \u03b8i) dt 2\nai = ai + (\u03b1i+1 + \u03b1i) dt 2           (3)\nwhere dt = 0.005s."}, {"title": "3.2 Pattern formation layer", "content": "To reshape the rhythm signal, we use the desired pose solver to realize the PF layer function of CPGs. After the oscillator generates ri, \u00c7i, we calculate the desired pose of the end of each leg and then obtain the position under Cartesian Coordinates of the end of the leg, then convert it into the desired motor angles through the inverse kinematics, to generate the control signal of the motors. The end position of each leg is calculated as follows:\nPxi = -l(ri - 1) cos(i)\nPyi = Lwy\nPzi =\n{\n-h+ gcsin(pi), if sin(pi) > 0\n-h + gpsin(i), otherwise         (4)\nwhere i is the mixed phase, and {Pxi, Pyi, Pzi} is the position of the end of leg in the leg's local Cartesian Coordinates. I is the step length, L = 11 + 12, 11, 12, 13 are the lengths of the three links of the robot coxa, femur and tibia, wy is the width adjustment variable, h is the height of the robot, ge is the maximum ground clearance in the swing process, and gp is the maximum ground penetration in the support process. These parameters constitute the CPG morphological parameter set mp = {l,h,gp, gc, wy} (Fig1.bicy shows the relationship between the above parameters and the gait). The high-level controller can directly adjust the shape of the robot by providing the deviation value dmp = {\u03b4\u03b9, \u03b4\u03b7, \u03b4\u0434\u0440, \u0431\u0443\u0441, \u0431\u0448\u0443} \u0404 [-1,1]. The adjustment process is as follows:\nl  1 + g(\u03b41)\nhh + g(dh)\n\u0434\u0440 - \u0434\u0440 + \u0434(\u0431\u0434\u0440)\ngcgc + g(gc)\nwy \u2190 wy + g(\u03b4\u03c9y)\n         (5)\nwhere g(x) = 0.02(xmax - Xmin)dx maps the deviation to the range of specified parameters, which is given in Table.1. When 1 < 0, the foot trajectory rotates clockwise, and the robot moves backward. On the contrary, when 1 > 0, the robot moves forward."}, {"title": "3.3 Skill learning of the mid-level controller", "content": "The mid-level reinforcement learning control policy can combine with the CPG module to form many coordinated motor skills. To achieve this, we use the parameterized neural network as the mid-level policy, and output a = [\u03bc,\u03c9] \u2208 R12 to adjust the internal amplitude and frequency of the oscillation, i.e. at ~ \u03c0(at St, z), with a control frequency of 16.67Hz, according to the higher skill vector z and the robot\u2019s proprioception st (including 18 joint angles of the legs, rotational quaternions, angular velocities and linear accelerations information measured by the internal measurement unit (IMU), as well as the morphological parameters and maximum oscillation frequency of the CPG module).\nCombining the unsupervised reinforcement learning method of work [32] and the CPG module, we propose a new pre-training method, which can enable the mid-level policy to explore motor skills with excellent dynamic performance in the high degree of freedom system. The training process is shown in Fig. 3a. The mid-level policy, CPG module, and robot environment form a closed loop. The CPG module provides the control signal, while the robot environment feeds back the state st and motion reward rm to the mid-level policy. During the learning process, the skill vector z is randomly selected from the unit circle using polar coordinates, while the morphological parameters are uniformly sampled within the range in Table.1. By designing the parameterized state representation function \u03c6, we can map the homologous state st of the robot to the potential space and align it with the skills, then feed back the skill rewards rsd to the mid-level policy. By maximizing the sum of rewards through the reinforcement learning algorithm SAC[50], we can learn that applies to different morphological parameter sets. The algorithm maximizes the difference between the initial state and the final state under the condition of a certain skill, so that the policy can produce different trajectories under the effect of different skills, and its optimization objective is as follows:\nJ1(\u03c0) = Ez,\u03c4 [(\u03a6(sT ) \u2212 \u03a6(s0 ))T z]                                                                            \n= Ez,\u03c4 [T\u2211t=0(\u03a6(st+1) \u2212 \u03a6(st ))T z], s.t. \u2200x,y \u2208 Sd ||\u03c6(x) \u2212 \u03c6(y)|| \u2264 ||x \u2212 y||      (6)\nwhere z is the skill vector, \u03c4 is the trajectory generated by the policy under the effect of the skill, st and st are generated at the same time. However, different from st, s\u00e5 mainly contains the robot\u2019s XYZ coordinates, IMU information (roll, pitch, yaw angles, linear velocities, and angular velocities) and oscillator\u2019s internal state [r, r, \u03b8, \u03b8, \u03b1, \u03ac]. The state representation function \u03c6 is also a learn- able neural network responsible for mapping these information into the skill space and aligning skills with it. This asymmetric state structure makes the policy a respond to the skill vectors associated with different oscillator\u2019s internal states while considering the proprioception. To avoid infinite \u03c6(s\u0165), 1-Lipschitz constraint is used on \u03c6. On the other hand, during the robot\u2019s movement, it is necessary to maintain the body\u2019s stability, so it is necessary to reduce the vertical speed of the robot. Therefore, another optimization objective is as follows:\nJ2(\u03c0) = Ez,\u03c4 [T\u2211t=0 \u2212wzv2z],                                                                         (7)\nwhere wz = 0.1 is the speed penalty weight, and vz is the speed of the robot in the z-axis direction. Thus, the reward per-step transition (st, at, rt, st+1) can be written as follows:\nrt = rsd + r = (\u03c6(st+1) \u2212 \u03c6(st ))T z + (\u2212wzvz).                                                           (8)\nConsidering the discount factor \u03b3, the overall optimization objective of the mid-level policy is as follows:\nJ(\u03c0) = Ez,\u03c4 [\u221e\u2211t=0\u03b3trt],                                                                                      (9)\nwe use SAC to optimize \u03c0 and use Stochastic Gradient Descent (SGD) to optimize \u03c6 (see Supplementary Section 5 for details of the algorithm and pseudo code).\nInstead of sampling skills from Gaussian or von Mises distribution [32, 51] we uniformly sampled the skill vector z from the unit circle. Skills of different lengths can cover the whole space evenly. The mid-level policy can learn rich motion patterns under different oscillation frequencies. Another advantage is that the skill space can be conveniently used as the abstract action space of the high-level policy. In this regard, we design the following random skill generator:\nR \u223c U(0,1)\n\u03b2 \u223c U(0,2\u03c0)                                                                                \nzx = \u221aR cos(\u03b2)\nzy = \u221aR sin(\u03b2).                                                                                                (10)\nThrough polar coordinate transformation, we can get the vector z = (zx, zy) uniformly distributed in the unit circle (See Supplementary Section 7 for the proof of uniform sampling)."}, {"title": "3.4 Multi-task reinforcement learning of the high-level controller", "content": "To make the high-level reinforcement learning control policy have the ability of autonomous decision-making, we propose a two-stage multi-task reinforcement learning method. The first stage is carried out in the multi-task simulation environment (Fig. 4b). We use the parameterized neural network \u03b7 as the high-level policy. It accepts the robot\u2019s proprioception st and the environmental information st from the environment (including the height sampling points centered on the robot and the heading directions from the robot to the two target points). It generates the high-level decision action ah = [dmp, z] \u2208 R7, i.e. a ~ n(a|st, s\u00e5). Use the learned skills to control the robot movement, we can get the environmental reward rp. Due to the time abstraction of the hierarchical structure, the action execution frequency (1.67Hz) of the high-level policy is only 10 of that of the middle level, which saves computational resources and improves efficiency.\nThe learning process is shown in Fig. 4a phase 1. After learning the mid-level policy, fix it and only learn the high-level policy. Each time the high-level policy interacts with the environment, it will receive a reward re. Through the model-free reinforcement learning algorithm (SAC), the total reward of each task can be maximized, and the learning process can be completed in a short time (about 1/10 of the learning time of the mid-level policy). The overall optimization objective of \u03b7 can be written as:\nJ(n) = E[\u221e\u2211t=0 \u03b3trt],                                                                              (11)\nwhere is the trajectory generated in each episode. Due to the time abstraction, the number of transitions collected during high-level policy training will be reduced. To solve the problem of low sample efficiency, we use step-conditioned critical SAC structure [52], (see Supplementary Section 8 for details of the algorithm and pseudo code). The reward function uses the following form:\nrh = d1 \u00d7 (wv \u00d7 rv + wd \u00d7 rd + wb \u00d7 rb + ws \u00d7 rs + wT \u00d7 ry),                                      (12)\nwhere di is the difficulty level (d\u0131 = 1, 2, 3, 4, 5). The higher the difficulty level, the richer the reward. To enable the robot to adapt to obstacles quickly, we adopt the dynamic learning mode of course learning and divide the obstacle environment into different difficulty levels. When the robot reaches the final goal, it will be transferred to a more difficult environment. If the robot cannot achieve half of the goal, it will be transferred to a simpler environment. The total reward mainly includes five sub rewards: speed tracking reward rv, direction tracking reward rd, balance reward rb, collision reward rs and completion time reward ry. The weight set of sub rewards is {\u03c9\u03bd, wd, wb, \u03c9\u03c2, \u03c9\u03c4}.\nTo enable the robot to climb in any direction and overcome obstacles instead of moving around obstacles, we use the target points in the world coordinates to calculate the desired heading directions, which can be written as\nd =\ng \u2212 x\n||g \u2212 x||,                                                                                             (13)\nwhere g is the coordinates of the target points, and x is the coordinate of the robot in the world coordinates. The following speed tracking reward rv encourages the robot to move towards the target points\nrv = min(< d, v >, Umax),                                                                                  (14)\nwhere v is the speed of the robot in the world coordinates, um is the maximum running speed of the robot, here is 0.3m/s.\nUse the following direction tracking reward rd to prompt the robot to quickly adjust its motion direction\nrd = exp(\u2212||d \u2212 dc||),                                                                                      (15)\nwhere dc is the current heading directions vector of the robot in the world coordinates.\nUse the following balance reward rs to punish the robot for its vertical movement and avoid the robot shaking up and down during the movement\nrb = \u2212v2z,                                                                                                       (16)\nwhere vz is the speed of the robot in the vertical direction.\nUse the following collision reward rs to reduce the number of collisions between the foot and the obstacle during robot movement.\nrs =\n{\n-1, if fzy > 4|fz|\n10, otherwise                                                                                                      (17)\nwhere fzy is the resultant force in the XY direction and fz is the force in the Z direction.\nUse the following time reward ry to urge the robot to complete the task quickly\nrr = -0.1. (18)\nThe weight set differs for the four obstacle environments to improve the learning effect. For stair climbing, we focus on the robot\u2019s ability to climb quickly, so the weight of the balance reward is reduced, and its reward weight set is {\u03c9\u03bd = 1.0, wd = 1.0, w\u044c = 0.5, \u03c9\u03c2 = 1.0, \u03c9\u03c4 = 1.0}. For crossing the gap, we focus more on its balance ability, and the reward weight set is {\u03c9\u03bd = 1.0, wd = 1.0, wb = 2.0, ws = 1.0, \u03c9\u03c4 = 1.0}. For crossing the narrow alley, the robot should move quickly and avoid hitting the walls on both sides, so the weight of speed and collision reward has been improved, {\u03c9\u03bd = 2.0, wd = 1.0, wb = 2.0, \u03c9\u03c2 = 2.0,\u03c9\u03c4 = 1.0}. When the robot climbs over the slope, it needs to adjust its heading directions quickly on the slope to avoid falling, so its direction tracking reward has a higher weight, {\u03c9\u03bd = 1.0,wd = 1.5, \u0448\u044c = 1.0, \u03c9\u03c2 = 1.0, \u03c9\u03c4 = 1.0}."}, {"title": "3.5 Distillation learning of the high-level controller", "content": "In the first stage of the learning process, the robot uses the surrounding height field and target heading directions as the external environmental information in the simulation environment, which can accelerate the learning and sampling process. However, the real robot must perceive the external environment through the camera. Therefore, in the second stage, distillation learning is used in the multi-task visual simulation environment, and multiple high-level policies learned are extracted into a student policy so that it can independently infer the environmental features and target heading directions according to the depth image, and then give appropriate high-level decision instructions. We use the distillation learning method of teacher-student to collect virtual image information for training in the simulation.\nh The distillation learning process is shown in Fig. 4a phase 2. We use multiple high-level neural networks for different tasks as the teacher policies and a neural network that can receive image input as the student policy to learn in the most difficult obstacle environment (see Supplementary Section 9 for the structure of the student network). The student policy needs to predict the height information near the robot and the target heading directions vector \u00e2 from the depth image and generate the predicted action a through the predicted information. If the heading directions vector predicted by the student policy is inaccurate initially, the direct use of the predicted vector as the state may lead to catastrophic distribution drift. To deal with this problem, we refer to ref [38] and use Dagger[53] to train the student policy. Specifically, we use the mixed heading directions of the teacher and student as the target heading directions. The heading directions are calculated as follows:\nd =\n{\nfd, if |d \u2212 d| < 0.6\ns d, otherwise                                                                                  (19)\nwhere d is the heading directions vector predicted by the student policy, and d is the actual calculated heading directions vector. The predicted heading directions are adopted when the difference between the two is within the allowable range. Otherwise, the actual heading directions are adopted. so is the heading directions vector of observations obtained by the student policy.\nDuring distillation, supervised learning is used to train the student policy. The loss function is as follows:\nloss =\n1\nN\u2211\ni=1(|ld \u2212 d||+||ah \u2212 \u00e2||).                                                  (20)\nwhere ah is the high-level action predicted by the student policy, and ah is the guiding action generated by teacher policies. In the training process, we add random noise to the depth image obtained in the simulation environment to enhance the robustness of the policy to image input in the physical environment."}]}