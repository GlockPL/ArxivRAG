{"title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics", "authors": ["Ruilin Luo", "Zhuofan Zheng", "Yifan Wang", "Yiyao Yu", "Xinzhe Ni", "Zicheng Lin", "Jin Zeng", "Yujiu Yang"], "abstract": "Chain-of-thought (CoT) reasoning has been widely applied in the mathematical reasoning of Large Language Models (LLMs). Recently, the introduction of derivative process supervision on CoT trajectories has sparked discussions on enhancing scaling capabilities during test time, thereby boosting the potential of these models. However, in multimodal mathematical reasoning, the scarcity of high-quality CoT training data has hindered existing models from achieving high-precision CoT reasoning and has limited the realization of reasoning potential during test time. In this work, we propose a three-module synthesis strategy that integrates CoT distillation, trajectory-format rewriting, and format unification. It results in a high-quality CoT reasoning instruction fine-tuning dataset in multimodal mathematics, MMathCoT-1M. We comprehensively validate the state-of-the-art (SOTA) performance of the trained URSA-7B model on multiple multimodal mathematical benchmarks. For test-time scaling, we introduce a data synthesis strategy that automatically generates process annotation datasets, known as DualMath-1.1M, focusing on both interpretation and logic. By further training URSA-7B on DualMath-1.1M, we transition from CoT reasoning capabilities to robust supervision abilities. The trained URSA-RM-7B acts as a verifier, effectively enhancing the performance of URSA-7B at test time. URSA-RM-7B also demonstrates excellent out-of-distribution (OOD) verifying capabilities, showcasing its generalization. Model weights, training data and code will be open-sourced.", "sections": [{"title": "1. Introduction", "content": "Chain-of-thought (CoT) reasoning has proven to be highly effective in mathematical reasoning with Large Language Models (LLMs), serving various roles from multiple perspectives (Zhang et al., 2024e; Wei et al., 2022; Chu et al., 2023; Sprague et al., 2024; Zhang et al., 2022; An et al., 2023; Luo et al., 2024b; Mu et al., 2024). Recently, numerous studies have focused on supervising the quality of CoT-style reasoning during inference to investigate test-time scaling laws and push the performance boundaries of these models (Zhang et al., 2024b; Lightman et al., 2023; Setlur et al., 2024; Fu et al., 2024b). However, this pathway has not been fully explored in the context of mathematical reasoning with Multimodal Large Language Models (MLLMs).\nBefore considering the introduction of a CoT supervision paradigm for multimodal mathematical reasoning, we first identify the limitations of existing work: i) General MLLMs, which perform well across various leaderboards, are designed with a balanced development approach. The models not specifically focus on enhancing mathematical capabilities, particularly in high-difficulty scenarios such as complex geometry problems (Shi et al., 2023). ii) Meanwhile, current open-source models in the multimodal mathematics do not yet exhibit satisfactory CoT reasoning capabilities. This may be due to the lack of high-quality and sophisticated CoT-formatted reasoning data in the training datasets, restricting models to merely fitting short output patterns (Zhang et al., 2024e; Yang et al., 2024a; Zhang et al., 2023).\nTo address the need for enhanced CoT reasoning capabilities in the multimodal mathematical domain, we begin by selecting a LLM specifically designed for mathematical reasoning as our LLM backbone. During the instruction fine-tuning phase, we extensively collect existing open-source training data from the mathematical domain, including datasets such as Multimath (Peng et al., 2024), MAVIS (Zhang et al., 2024d), PUMA-VarsityTutors (Zhuang et al., 2024), and MathV-360K (Shi et al., 2024), which cover tasks like geometry problem solving, math word problems, and table QA. However, we observed that high-quality CoT-formatted reasoning data is either scarce or not sufficiently sophisticated. Recognizing the importance of CoT reasoning, we synthesize"}, {"title": "3. Model Training Process", "content": "In this section, we introduce the training process of URSA-7B, as illustrated in Figure 5. In Section 3.1, we introduce the model architecture and the composition of the vision-language alignment data. In Section 3.2, we describe the data synthesis strategy for math SFT, named MMathCoT-1M. In Section 3.3, we explain the data synthesis process for training PRM with DualMath-1.1M.\nTraditional MLLMs, which are largely trained on tasks like OCR and document understanding for general purposes, are not well-suited for direct transfer to mathematical training (Wang et al., 2024a). In contrast, mathematics-specific LLMs already possess substantial mathematical knowledge and foundational capabilities in mathematical CoT reasoning. Therefore, we choose a composite architecture. Specifically, we use the same hybrid vision encoder as Deepseek-VL, which combines the SAM-B and SigLIP-L encoders (Lu et al., 2024). On the language model side, we utilize the advanced Qwen2.5-Math-7B-Instruct (Yang et al., 2024a). Following previous work, we use an MLP projector as an aligner between the vision encoder and the LLM (Liu et al., 2024d; Bai et al., 2023).\nWe collect 960K vision-language alignment open-source data for training by combining and filtering open-source datasets, naming it URSA-alignment-960K. The details are listed in Figure 2."}, {"title": "3.2. CoT Augmentation in Multimodal Mathematics", "content": "To enhance the multimodal mathematical CoT reasoning capabilities of MLLMs, we collect and categorize existing open-source multimodal mathematical training data. Subsequently, we employ a robust generator, Gemini-1.5-Flash-002, denoted as G to execute the CoT data synthesis strategy as illustrated in Figure 3. Gemini-1.5-Flash-002 demonstrates performance in multimodal mathematical reasoning comparable to GPT-40 while being more cost-effective (Team et al., 2023).\nAnswer-only Answer-only data D\u2081 = {(xi, Yi)}1 includes MathV360k (Shi et al., 2024), each contains question xi and ground-truth answer yi. However, answer-only training restricts the model from fully capturing the problem-solving process. This approach may lead the model to rely on memory-based reasoning, hindering it provide direct answers to more complex geometric reasoning problems through fast thinking (Trinh et al., 2024; Li et al., 2024b). For this type of data, we use G for CoT path distillation. Specifically, given CoT distillation prompt Pc, we provide the problem and corresponding standard answer, then prompt the model to output the reasoning trajectory leading to the answer. We filter out responses that refuse to answer or indicate that more conditions are needed.\nSAo = G(Pc; {xi, Yi}i=1) \nAnalysis-formatted This category of data D2 = {(xi, Yi, Ai)}21 includes MAVIS-Geo, MAVIS-MetaGen (Zhang et al., 2024d), VarsityTutors (Zhuang et al., 2024), and Geo170k-QA, each sample contains question xi, answer yi and texual analysis ai. Given rewriting prompt Pr, we utilize G to transcribe solutions, enhancing step-by-step reasoning trajectories and linguistic diversity"}, {"title": "3.3. Dual-view Process Supervised Data Synthesis", "content": "However, even though URSA-7B possesses strong CoT reasoning capabilities, it still has a significant likelihood of producing incorrect reasoning trajectories. Inspired by the use of process supervision for test-time scaling in LLM reasoning (Zhang et al., 2024b; Setlur et al., 2024; Zelikman et al., 2022), we consider training a multimodal process supervision model to extract high-quality CoT trajectories. In multimodal scenarios, we propose a dual-view process reward data approach, focusing on both \u201clogically correct\u201d and \u201cvisually accurate\u201d aspects. The former is derived from error localization in the incorrect reasoning trajectories of URSA-7B, while the latter comes from manually inserted image misinterpretations.\nError Step Locating Engine Inspired by (Lightman et al., 2023; Luo et al., 2024a), the Monte Carlo Tree Search (MCTS) approach can help us identify erroneous steps in the trajectory of problem-solving. We use the binary labeling approach to mark erroneous steps, enabling automated generation of process reward signals."}, {"title": "4. Experiment", "content": "Benchmarks We use several benchmarks to evaluate the performance of URSA-7B. MathVista (Lu et al., 2023a) is a popular dataset for multimodal reasoning. Math-verse (Zhang et al., 2025) focuses on mathematical skills, covering areas such as plane geometry, solid geometry, and functions. It tests the model's reasoning abilities across different scenarios using both visual and language information. DYNAMATH (Zou et al., 2024) modifies problem elements to check the model's reasoning robustness. WE-MATH (Qiao et al., 2024) evaluates the end-to-end reason-"}, {"title": "5. Ablations", "content": "In this section, we conduct ablation experiments on the CoT augmentation strategy. The results in Figure 7 and Figure 8 demonstrate that data enhanced by CoT augmentation better assist the model in reasoning. Specifically, the tasks most affected are GPS and MWP, which are the most mathematically related. This indicates that CoT reasoning capabilities play a crucial role in the mathematical understanding and reasoning processes of MLLMs. Additionally, unrefined mixed data can cause the model to inconsistently choose between direct and CoT format reasoning, which somewhat contradicts the logical nature of System-2 reasoning.\nWe conduct an ablation study on synthetic data for URSA-RM-7B to verify the effectiveness of the misinterpreta-tion insertion engine. We validate this on MathVerse and"}, {"title": "6. Conclusion", "content": "In this work, we focus on enhancing the mathematical reasoning abilities of MLLMs and propose a training streamline from high-quality CoT fine-tuning to multimodal math test-time scaling. During the instruction fine-tuning phase, we address the gap in existing multimodal mathematical open-source datasets concerning high-quality reasoning processes. Through CoT distillation, rewriting, and style unifying, we contribute a high-quality mathematics CoT training dataset, URSA-1M. With only a 1M-size dataset, we train the open-source URSA-7B model, which achieves several SOTA results on major benchmarks. Furthermore, we evolve from improving CoT reasoning capabilities to enhancing CoT supervision capabilities by proposing dual-view supervised synthetic data and training URSA-RM-7B. As a verifier, it effectively boosts URSA-7B's reasoning abilities and demonstrates its capabilities in OOD scenarios. This work fills the gap from high-quality CoT instruction"}, {"title": "A. Hyperparameter and Time cost", "content": "In this section, we provide the specific parameter settings and time costs for the three stages. Unless otherwise specified, experiments are conducted on 32\u00d7 NVIDIA-H100-HBM3 GPUs by default. Additionally, we provide important parameters used in data construction. During the generation of positive and negative example pairs, we set the temperature to 1.0, n_return_sequences to 16, and top-p to 0.95. In the BinaryErrorLocating phase, we set the temperature to 0.3, n_return_sequences to 16, and top_p to 0.95.\nWe adapt the VLLM (Kwon et al., 2023) framework for the URSA-7B's architecture (hybrid vision tower + MLP + Qwen2.5-math-Instruct is not original supported by VLLM) and use it as an acceleration tool during the inference phase. During the data pair generation phase, we use 16\u00d7 NVIDIA-H100-HBM3 GPUs for inference, which takes approximately 28 hours. In the BinaryErrorLocating phase, we also use 16\u00d7 NVIDIA-H100-HBM3 GPUs for inference, taking about 20 hours."}, {"title": "B. Supplementary Results", "content": "We provide detailed test results based on various mathematical abilities on MathVista. The results in Figure 7 show that URSA-7B's greatest strengths are in algebra and geometry, while open-source general MLLMs perform better on scientific and statistical problems. When compared with open-source math MLLMs, URSA-7B maintains a consistently leading level except in logical problems."}, {"title": "C. Prompt", "content": "In this section, we provide a demonstration of the prompt used for MMathCoT-1M construction. The prompts used on analysis-formatted, answer-only and CoT-formatted source data are illustrated in Figure 9, 10, and 11. The response checking prompt is illustrated in Figure 12."}, {"title": "D. Detailed description of Benchmarks", "content": "In this section, we introduce the detailed subtasks and metrics of four used benchmarks to more precisely demonstrate the evaluation.\nMathVista MathVista (Lu et al., 2023a) comprises a total of 5 subtasks: Geometry Problem Solving (GPS), Math Word Problem (MWP), Figure Question Answering (FQA), Textbook Question Answering (TQA) and Visual Question Answering (VQA).\nAdditionally, MathVista evaluates the model's performance in different mathematical thinking areas by testing the skills or capabilities required by various problems. Specifically, it includes Arithmetic (ARI), Geometry (GEO), Logical (LOG), Numeric (NUM), Scientific (SCI) and Statistical (STA). The performance comparison on capability is demonstrated in Table 7."}, {"title": "E. Case Study", "content": "Some actual cases are presented in Figure 15 and Figure 16. An example demonstrating how URSA-RM-7B verifies step-level correctness is shown in Figure 17."}]}