{"title": "IntegrityAI at GenAI Detection Task 2: Detecting Machine-Generated Academic Essays in English and Arabic Using ELECTRA and Stylometry", "authors": ["Mohammad AL-Smadi"], "abstract": "Recent research has investigated the problem of detecting machine-generated essays for academic purposes. To address this challenge, this research utilizes pre-trained, transformer-based models fine-tuned on Arabic and English academic essays with stylometric features. Custom models based on ELECTRA for English and AraELECTRA for Arabic were trained and evaluated using a benchmark dataset. Proposed models achieved excellent results with an F1-score of 99.7%, ranking 2nd among of 26 teams in the English subtask, and 98.4%, finishing 1st out of 23 teams in the Arabic one.", "sections": [{"title": "Introduction", "content": "Since the launch of ChatGPT in November 2022, research on developing models for artificial intelligence (AI)-generated text detection has increased. This increase reflects growing concerns about maintaining academic integrity in the face of advanced generative AI (GenAI) tools capable of producing human-like text (Al-Smadi, 2023). Guo et al. (2023) were among the first working on this topic by developing a dataset named the \"Human ChatGPT Comparison Corpus (HC3)\" out of nearly 40,000 questions from different datasets along with their answers provided by humans. They also generated responses to these questions by ChatGPT and used the combined dataset to train detectors in both English and Chinese. The developed models included machine and deep learning based models like ROBERTa (Liu, 2019) and demonstrated decent performance across different scenarios.\nAnother paper focused on detecting ChatGPT-generated text written in English and French (Antoun et al., 2023a). The English model was trained using the HC3 dataset. The authors also translated some of its English content to French and included additional small French out-of-domain dataset of 113 French responses from ChatGPT and 116 from BingGPT. They fine-tuned two pre-trained models, CamemBERT (Martin et al., 2019) and CamemBERTa (Antoun et al., 2023b), using the French dataset, and RoBERTa (Liu, 2019) and ELECTRA (Clark, 2020) models using the English one. They also used XLM-R (Conneau, 2019) as multi-language model for the combined datasets of both languages. Research results showed that all models demonstrated good performance in identifying machine-generated content within the same domain, but when tested on out-of-domain content, their results dropped.\nAlshammari et al. (2024) used transformer-based models, namely AraELECTRA (Antoun et al., 2020) and XML-R (Conneau, 2019) to solve the challenges of machine-generated Arabic text identification. The authors focused on the influence of diacritics on detection model performance. Their method showed great accuracy on the AIRABIC benchmark dataset. Other research utilized stylometric features to detect machine-generated content. For instance, Kutbi et al. (2024) introduced a machine learning model with stylometry for identifying \"Contract cheating\", the act of students depending on others to complete academic assignments on their behalf, by detecting deviations from a learner's distinctive writing style, which achieved excellent accuracy in their research. Opara (2024) developed a data-driven model named \"StyloAI\" trained with 31 stylometric features to detect machine-generated content. \"StyloAI\" performance outperformed other models on the same dataset.\nWee and Reimer (2023) discovered that AI identification technologies classified human-written writings translated from non-English languages as AI-generated, which raised worries among non-native English speakers (Liang et al., 2023). Moreover, Weber-Wulff et al. (2023) tested many AI text identification systems and found that they were neither accurate nor dependable, especially when content masking techniques were used.\nThis research aims at addressing the challenge of AI-generated text. The rest of this paper is organized as follows: Section 3 discusses the research methodology, Section 4 presents the findings, and Section 5 concludes the study and highlights future directions."}, {"title": "Research Methodology", "content": "This research is based on our participation in the shared task \"GenAI Content Detection Task 2: \u0391\u0399 vs. Human \u2013 Academic Essay Authenticity Challenge\" (Chowdhury et al., 2025), which is organized as part of the \"Workshop on Detecting AI Generated Content at the 31st International Conference on Computational Linguistics (COLING 2025)\". The task aims at encouraging researchers to submit their research for detecting AI-generated academic essays. The task is designed to have three phases: (a) Models training and validation, (b) First evaluation phase, also referred as development phase, and (c) Models testing phase. Participated teams were ranked based on the results achieved in the final phase, i.e. models testing phase. The task covers content generated in two languages, Arabic and English. The next section explains in more detail the datasets provided for model training, validation, and testing."}, {"title": "Dataset", "content": "The datasets for this task consist of essays generated by generative AI models and human written ones. The essays authored by humans were curated from the \"ETS Corpus of Non-Native Written English\"\u00b9, whereas the AI-generated ones were generated using seven different models including,"}, {"title": "Baseline Model", "content": "The task organizers have implemented the following baseline model (Chowdhury et al., 2025). For each language, a baseline model is trained using an n-gram approach, specifically unigrams. The textual content of the essays is transformed into a Term Frequency-Inverse Document Frequency (TF-IDF) representation, with the features limited to a maximum of 10,000. Finally, the performance is evaluated by training a Support Vector Machine (SVM) classifier on this feature representation."}, {"title": "IntegrityAI Model", "content": "The proposed model is based on ELECTRA (Clark, 2020) and its implementation named AraELECTRA (Antoun et al., 2020), which is a model specifically tuned for the Arabic language. ELECTRA is an encoder only transformer that is designed to enhance the efficiency of implementing models for NLP tasks. Instead of implementing a masked language model (MLM), ELECTRA utilizes a unique training strategy known as \"replaced token detection\". While other encoder only transformers (such as BERT (Kenton and Toutanova, 2019)) implement MLM training strategy by predicting masked words in a sentence, ELECTRA relies on its generator component to generate plausible alternatives to replace some tokens in the input text. Then, uses the discriminator component to detect whether the token is replaced or original. The \"replaced token detection\" training strategy, requires the model to evaluate and learn all the input text tokens instead of the masked ones - as in BERT - which increases the model efficiency and minimizes the number of training epochs required to train the model. ELECTRA has three different pre-trained models that were used in this research, see Table 2 for differences between them\u00b2.\nAs depicted in Figure 1, the same model architecture was used for both the Arabic and English text classification. The ELECTRA model was trained on the English dataset, whereas its tuned version on Arabic, i.e., AraELECTRA was trained on the Arabic dataset. Both datasets went into a standard preprocessing phase, then stylometirc features were extracted and used with the text embeddings to train the pretrained models (see Table 3 for more information about extracted features). The following layers were added to enhance the models' performance:\n1. Dropout Layer: is a regularization technique where, during training, random neurons are temporarily ignored (\"dropped out\") to prevent overfitting and improve the model's generalization (Srivastava et al., 2014).\n2. Batch Normalization (\"BatchNorm1d\"): normalizes the features of the input vector, stabilizing learning, and aiding in faster and more stable training (Ioffe and Szegedy, 2015).\n3. Fully Connected (Linear) Layers: these layers are basic neural network layers where every input is connected to every output by a learned weight. These layers include: (a) \"numerical\": takes the batch-normalized numerical features and projects them onto a new space to learn a higher-level representation of these features. (b) \"text\": processes the [CLS] token embedding from the ELECTRA model output, allowing the model to further tailor this representation for the task at hand.\n4. Rectified Linear Unit (ReLU) activation Function: is a non-linear operation used after linear layers to introduce non-linear properties to the model, making it capable of learning more complex patterns (Glorot et al., 2011). This layer is used after each of the fully connected layers (numerical and text) to add non-linearity to the model, which helps in learning complex patterns in the data.\n5. Output Layer (Fully Connected (Linear)): after processing through their respective pathways, both text and numerical data features are combined (concatenated) to form a unified feature vector. This combined feature vector is then passed to a final fully connected layer (combined), which outputs the logits for the classification categories.\nThe models were trained for 10 epochs with the option of (early_stopping_patience=2) implemented to avoid model overfitting during training. Models participating in this task were evaluated and ranked based on their achieved F1-score."}, {"title": "Results and Findings", "content": "Table 4 presents the developed models results for Arabic and English datasets. Results show that models achieved high F1 scores of 99.8% for the Arabic dataset and 100% for the English dataset in the evaluation phase and maintained that high performance in the testing phase with (98.4% and 98.5%, for Arabic and English datasets respectively). This achievement demonstrates that the models are not only well-tuned to the training data but also maintain their discriminative power on new and unseen data. This finding is also represented by the confusion matrices on the validation datasets.\nThe trained model on the English dataset classified all 'ai' and 'human' labels accurately. Whereas, The trained model on the Arabic dataset had a near-perfect classification with only one instance of 'ai' being misclassified as 'human' (see Figure 2).\nTo evaluate the impact of the stylometric features on the model performance, we trained the models without features. The results demonstrate that excluding these features leads to a decrease in model performance, with a 1.5% and 2.4% drop in F1 score for AraELECTRA and ELECTRA models, respectively. This indicates that extracted features enhanced model predictions. Despite the modest decline, the impact underscores the importance of these features for better generalization.\nThe results of training vs. validation Loss values after each epoch of models training in Figure 3, show that the training loss rapidly declined from the first epoch and then quickly stabilized to run in parallel with the validation loss. Both values of training and validation loss kept decreasing smoothly until the end of models training epoch without any sign of overfitting, as the validation loss remains close to the training loss throughout the training process. This was also maintained by enabling the option of early_stopping during the models training. Moreover, this also indicates that both models generalizes very well when confronted by new unseen data.\nThe rapid stabilization of loss values may indicate that more complex model architectures might achieve even better results. Therefore, we trained the ELECTRA_large instead of the ELECTRA_small model for the english subtask for 10 epochs as well. As, expected the ELECTRA_large achieved better results with F1 score of 99.7%.\nFor more information on the results of other participating teams in the task, the reader is redirected to (Chowdhury et al., 2025)."}, {"title": "Conclusion and Future Work", "content": "This study demonstrates the efficacy of transformer-based models for identifying machine-generated academic articles. Using ELECTRA-Small for English and AraELECTRA-Base for Arabic, paired with stylometric characteristics, our models produced remarkable F1-scores of 98.5% and 98.4%, respectively. Experiments using ELECTRA-Large for English revealed the possibility of even better F1-score, reaching 99.7%, but at a larger computing cost.\nOur proposed models offer an adaptable solution that balances performance and efficiency and is appropriate for a variety of hardware setups. To improve robustness, future study might focus on real-time detection, expanding to new academic areas, and extending language coverage."}]}