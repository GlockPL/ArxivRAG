{"title": "HYBRINFOX at CheckThat! 2024 - Task 1: Enhancing\nLanguage Models with Structured Information for\nCheck-Worthiness Estimation", "authors": ["G\u00e9raud Faye", "Morgane Casanova", "Benjamin Icard", "Julien Chanson", "Guillaume Gadek", "Guillaume Gravier", "Paul \u00c9gr\u00e9"], "abstract": "This paper summarizes the experiments and results of the HYBRINFOX team for the CheckThat! 2024 - Task 1\ncompetition. We propose an approach enriching Language Models such as RoBERTa with embeddings produced\nby triples (subject; predicate; object) extracted from the text sentences. Our analysis of the developmental data\nshows that this method improves the performance of Language Models alone. On the evaluation data, its best\nperformance was in English, where it achieved an F1 score of 71.1 and ranked 12th out of 27 candidates. On the\nother languages (Dutch and Arabic), it obtained more mixed results. Future research tracks are identified toward\nadapting this processing pipeline to more recent Large Language Models.", "sections": [{"title": "1. Introduction", "content": "The recent democratisation of social media has given the users unprecedented access to information,\nwith the possibility to contribute knowledge as well as to share personal views and opinions. By the\nsame token, however, it has also offered misinformation new paths to propagate, sometimes with\nmassive impact. Because of that, automated misinformation detection and automated fact-checking\nhave become tasks of central interest in the data science community.\nThis paper deals with a specific aspect of fact-checking, namely with the problem of check-worthiness\nestimation, presented as Task 1 of the broader CheckThat! 2024 workshop [1], concerned with informa-\ntion quality evaluation. Various works dealing with fact-checking operate under the assumption that the\nentirety of the claims, sentences, or articles in a dataset are checkworthy [2, 3, 4]. But this approach can\nbe inefficient, and a useful preliminary step is the identification of which claims are check-worthy, and\nwhich are not. The notion of check-worthiness is complex. Some claims in a text are not check-worthy\nsimply because they are not declarative sentences and do not report even potential facts (viz. questions).\nOthers are not check-worthy because, while making declarative assertions, they make claims of no\nconsequence. Conversely, a declarative sentence with potentially harmful consequences is one that\nranks high on check-worthiness. Others, finally, may not be check-worthy when they simply report\nsubjective views that are not susceptible of verification proper. It is a non-trivial challenge, therefore, to\ndetermine which claims in a document are specifically check-worthy.\nCheck-worthiness is a recent task [5], mostly covered using Language Models [6]. In this paper, we\npropose an approach designed to leverage structured information from the text, in order to enhance the\nrepresentation obtained with a Language Model. Because the task of check-worthiness estimation is\nrelated to fact-checking, it seems appropriate to identify facts from the text to help the model predict\ncheck-worthiness. By using both structured facts and Language Models embeddings, we obtained better\nresults than when using Language Models alone, ranking 12th among 27 competing teams for English\n(with an F1 score of 71.1). Results were more mixed for the non-English languages represented in the\ntest set: in Dutch our method ranked 8th out of 16 candidates (F1 score of 58.9), and in Arabic it ranked\n10th out of 14 (F1 score of 51.9).\nIn Section 2, we open with a quick review of the state of the art on the task. In Section 3, we spell\nout the functioning of the proposed processing pipeline. Then, Section 4 discusses preliminary results\nobtained with the initial training data and presents the final submitted results. Finally, some elements\non the evolution and future use of the proposed hybrid system are presented in Section 5."}, {"title": "2. Related work", "content": "As explained in the previous section, check-worthiness is a fairly recent task, first mentioned in 2015 [5].\nSeveral datasets have been constructed, like the ClaimBuster dataset [7], or the datasets proposed at the\nCheckThat workshops since 2018 [8]. These datasets focus on two main types of contexts for the task:\n\u2022 Classifying sentences from a political debate. These could be used to ease fact-checking during\ntelevision political debates, on datasets such as [9].\n\u2022 Classifying tweets. Because they are easily and widely shared online, check-worthiness is an\nimportant task for online discussions to avoid information manipulation.\nBoth of these categories are important, and a commonality between them is their short format.\nHowever, the scope of this task of check-worthiness can be widened so as to also include online press,\nwith an eye to so-called \u201cpink slime\u201d news [10], encompassing longer texts whose truthfulness is\nquestionable.\nAmong pioneering approaches to the task, we find methods such as ClaimRank [11], using traditional\nNLP methods (e.g. lemmatization, TF-IDF) to identify check-worthy claims.\nMore recent approaches take advantage of the Transformer layer and of pretrained Language Models\nsuch as BERT [12], ROBERTa [13] or XLNet [14]. The fortune of these approaches can be seen in the\n2023 CheckThat! Task 1 overview paper [15], showing that nearly all teams used a transformer-based\nLanguage Model.\nWith the even more recent development of Large Language Models and of Generative AI, a natural\nshift has been made toward the use of LLMs, relying on prompt engineering [16] and in-context\nlearning [17] to achieve check-worthiness estimation. These approaches were used by the winners of\nthis year's competition, both in English [18] and in Dutch [19]."}, {"title": "3. Methodology", "content": "A straightforward approach for check-worthiness estimation would be to use a pretrained Language\nModel fine-tuned with the provided training data. However, these language models produce embeddings\nthat are opaque, even if they are sufficient most of the time. To increase the quality of the language\nmodel predictions, we propose to use them in conjunction with a small neural network able to leverage\nstructured information from the input text. A visual description of the architecture is given in Figure 1.\nThe processing pipeline is the following:"}, {"title": "3.1. Model", "content": "1. To begin with, the text is embedded using a Language Model. In our case, the ROBERTa model [13]\nis used, producing an embedding of dimension 768. ROBERTa was chosen for its ease of use, its\nhigh performance for classification tasks and its relatively small size when compared to recent\nLLMs.\n2. In parallel, the text is structured using an Open Information Extraction system. These systems\nextract information from the text in the form of triples (subject; predicate; object). We used\nOpenIE6 [20] to extract triples in the English language. Using triples allows us to produce\nstructured information from the text and to reduce syntactic complexity, with the aim of helping\nsentence classification. A maximum limit of 4 triples by text were extracted, which is enough to\nconsider all information triples for more than 90% of sentences. Each part of the triples is encoded\nusing fastText [21], producing 3 vectors of dimension 300 per triple. These vector representations\ngo through a dense layer with ReLU activation function. Then, they are averaged before being\ncombined in the last layer to produce an embedding of dimension 768 (the same dimension as the\nLanguage Model).\n3. Encodings from the previous two parts are concatenated and go through a dense layer with ReLU\nactivation function with a final output producing the probability of being checkworthy by means\nof a sigmoid activation function.\nThe described architecture can be transposed to other languages when an OpenIE system and an LM\nare available. In the context of Task 1 of the evaluation, for Spanish, Dutch and Arabic, ROBERTa was\nswapped with a multilingual BERT [12].1 The OpenIE6 system was replaced with Multi\u00b2OIE [22] in a\nzero-shot setting for non-English languages, providing worse performance than OpenIE6 on English,\nbut allowing us to test the architecture on other languages. In principle, the same architecture can be\nused for any language, and in practice it is applicable to the 98 languages currently supported by the\nmultilingual version of BERT."}, {"title": "3.2. Example", "content": "To better understand how this architecture works, we illustrate the pipeline with a simple example. We\ntook a sentence from the training English dataset: \"I must remind him the Democrats have controlled\nthe Congress for the last twenty-two years and they wrote all the tax bills.\" This sentence comes from a\ndebate between US presidential candidates Jimmy Carter and Gerald Ford on September 26th 1976. It is\ndeemed to be checkworthy, as it contains allegations on Jimmy Carter's party."}, {"title": "4. Results", "content": "This section is divided in three parts. The first presents the protocol of our model. The second part\nreports the evaluation of our proposed approach and of a standard Language Model, in order to measure\nhow the additional triple processing part impacts performance. The third part contains an analysis of\nour submitted results, as well as of the difficulties encountered."}, {"title": "4.1. Training procedure", "content": "Each model was trained over 5 epochs on the train set. After each epoch, the model was evaluated on\nthe dev set and the best model in terms of macro-F1 score was kept. The scores reported in Section 4.2\nare the macro-F1 score on the dev-test set.\nIn our procedures, only the train set was used for training the model, the dev and dev-test sets being\nused for model selection. Reported results were produced by the models with the best dev-test macro-F1\nscore, which were also used to make predictions on the final test set."}, {"title": "4.2. Preliminary results on the development data", "content": "The main goal of our approach was to evaluate how combining structured information from the text with\na standard Language Model is impacting performance. Results observed on the dev-test set provided\nbefore the competition are provided in Table 1."}, {"title": "4.3. Results on the evaluation data", "content": "The competition scores and ranking are shown in Table 2, with the scores of the best performing team\n(state-of-the-art for this dataset) and the baseline being also reported.\nFor the three languages, our approach outperformed the baseline by a substantial margin. Performance\nfor non-English languages was mixed. The Arabic dataset proved to be challenging for all teams, with\nmost candidate approaches getting scores between 50 and 55."}, {"title": "4.4. Discussion", "content": "While the proposed approach outperformed ROBERTa on the dev-test set, several upgrades could have\nbeen made to reduce possible errors in the processing pipeline. Firstly, it is well known that triples\nextracted with OpenIE are noisy and may not always contain useful facts for the task. This can be seen\nin the first triple of the example given in Section 3: (I; must remind; him the Democrats have controlled\nthe Congress for the last twenty-two years). One way would be to filter out the triples that do not contain\nnamed entities in the subject and object part. This approach would keep only the second triples in\nthe example. One additional step to increase the usefulness of triples would be to apply a coreference\nanalysis, changing pronouns by the objects they refer to. After coreference, (they; wrote; all the tax bills)\nwould become (the Democrats; wrote; all the tax bills), which is more descriptive.\nAnother way of improving this approach would be to use post-hoc explanation methods such as\nintegrated gradients [23], to identify which embeddings make the highest contribution to the prediction.\nThis could help identify the triples most relevant to the prediction, giving interpretability to the proposed\naddition, and further input for a fact-checking system."}, {"title": "5. Future work and conclusion", "content": "The HYBRINFOX team is interested in neurosymbolic architectures and our aim is generally to improve\nperformance of Language Models by adding structured information from the texts. This approach has\nto be adapted to misinformation detection or fact-checking settings. In general, we believe that all tasks\nthat are related to factual claims could benefit from adding structured information into their pipeline in\norder to increase performance.\nThe proposed approach uses Language Models such as BERT (for OpenIE) or ROBERTa, but could be\nupgraded by using most recent advances in Large Language Models such as Mistral or ChatGPT. An\nLLM prompted with instructions could easily perform a similar pipeline:\n1. Extract information triples from the text.\n2. Select factual triples.\n3. Identify if the factual triples are check-worthy.\nThis approach could help identify which part of the text contains check-worthy information with\nbetter accuracy.\nTo conclude, the proposed approach, enriching Language Models with a level of structured informa-\ntion, has shown promising results in comparison to the use of Language Models alone on the task of\ncheck-worthiness estimation. For check-worthiness, the extraction of factual triples from the text helps\nclassification. However, performance was mixed on non-English texts.\nFurther analyses need to be conducted with other expert systems to further improve performance.\nAs mentioned in the introduction, the definition of what counts as check-worthy is complex. One\napproach, which we have not tried, might be to consider as checkworthy first and foremost sentences\nmaking objective claims. For that purpose, we may piggyback on the methods used in Task 2 of the\nCheckThat! Lab [24] dealing with the classification of subjective vs objective sentences. We leave that\nexploration for further work."}]}