{"title": "Edge Delayed Deep Deterministic Policy Gradient: efficient continuous control for edge scenarios", "authors": ["Alberto Sinigaglia", "Niccol\u00f2 Turcato", "Ruggero Carli", "Gian Antonio Susto"], "abstract": "Deep Reinforcement Learning is gaining increasing attention thanks to its capability to learn complex policies in high-dimensional settings. Recent advancements utilize a dual-network architecture to learn optimal policies through the Q-learning algorithm. However, this approach has notable draw-backs, such as an overestimation bias that can disrupt the learning process and degrade the performance of the resulting policy. To address this, novel algorithms have been developed that mitigate overestimation bias by em-ploying multiple Q-functions.\nEdge scenarios, which prioritize privacy, have recently gained prominence. In these settings, limited computational resources pose a significant chal-lenge for complex Machine Learning approaches, making the efficiency of algorithms crucial for their performance.\nIn this work, we introduce a novel Reinforcement Learning algorithm tailored for edge scenarios, called Edge Delayed Deep Deterministic Policy Gradient (EdgeD3). EdgeD3 enhances the Deep Deterministic Policy Gra-dient (DDPG) algorithm, achieving significantly improved performance with 25% less Graphics Process Unit (GPU) time while maintaining the same memory usage. Additionally, EdgeD3 consistently matches or surpasses the performance of state-of-the-art methods across various benchmarks, all while using 30% fewer computational resources and requiring 30% less memory.", "sections": [{"title": "1. Introduction", "content": "Autonomous agents operating in dynamic environments require robust training methods, with Reinforcement Learning (RL) offering a potent framework for such continuous adaptation. Control strategies are of paramount importance in domains characterized by continuous action spaces, as discussed in recent literature [1]. Actor-critic methods, particularly those utilizing tem-poral difference learning [2], represent a foundational approach within this area. However, the integration of Q-learning with deep neural networks to form a critic function has set new benchmarks [3], enhancing policy opti-mization through advanced policy gradient techniques [4, 5, 6].\nThese state-of-the-art approaches, however, are not without their pitfalls, particularly the tendency of Q-learning to induce overestimation bias [7]. \u03a4\u03bf combat this, the Twin Delayed Deep Deterministic Policy Gradient (TD3) method was developed, innovatively applying Clipped Double Q-Learning (CDQ) to refine accuracy in action value estimations [5]. By deploying two independently trained neural networks, this method systematically lowers the risk of overestimating the Q-values by adopting the lesser of the two during the training process.\nDespite its benefits, CDQ can introduce underestimation bias, albeit with a lesser impact on policy modifications when compared to overestimation, as argued by the proponents of TD3 [5]. Furthermore, the introduced estimator leaves very little room for tuning the trade-off. Indeed, new developments [8] built on top of TD3 optimize for a convex combination of the minimum between the estimates, the CDQ estimate, and the maximum of the two.\nRecently, new algorithms further increased the pool of Q-networks in or-der to have a less noisy estimate of the true Q-learning target [9], getting to the point of using 10 or more networks. These new enlarged pools of independent estimates are usually combined with multiple steps of updates, overshadowing previous algorithms' performances. Instead, TD3 performs a single update per step. However, these enhancements introduce a very noticeable overhead, using up to 10x more memory and 10x more computational resources than the original algorithm they build on top of.\nThe increase in computational cost of these new algorithms is prohibitive in many low-resource scenarios, for example, in edge computing applications. Many edge computing applications can benefit from data-driven-based ap-proaches. For example, in the domain of autonomous driving, real-time sens-ing, and decision processing can be conducted by deploying edge computing nodes on self-driving vehicles; this advancement enables a reduction in re-sponse times and enhances driving safety [10]. Similarly, in the field of smart healthcare, edge computing nodes utilized on wearable and medical devices allow for the monitoring of patient's physiological parameters in real-time. The collected data is then transmitted to the cloud, where it is analyzed and used for diagnosis, facilitating the realization of telemedicine and personal-ized medicine [11].\nThe ability to deploy deep learning models on the edge is lately gaining always more attention due to the appealing properties of on-device learning, for its decentralized computation, for economic purposes, and for its privacy preserving nature. Such a problem can be tackled using quantization [12] or pruning [13].\nHowever, for the case of Reinforcement Learning, in order to allow the personalization of such models to the context in which the edge device is deployed, it is important to also tackle the algorithmic side. Indeed, differ-ently from most other areas of machine learning, Deep RL requires additional computation and additional models for the learning to be carried out, thus adding a non-negligible overhead.\nIn this paper, we propose an alternative to DDPG that tackles overesti-mation with a single Q estimate. This algorithm, called Edge Delayed Deep Deterministic Policy Gradient (EdgeD3), has a computational cost lower than DDPG by 25% while maintaining the same memory footprint. It does it with a new expectile loss, which induces an underestimation bias that evens the overestimation caused by Q-learning. Even though being cheaper, EdgeD3 has performances comparable or superior to the state-of-the-art in most cases, while using 30% less processing resources and having 30% less memory footprint compared to such more advanced methods. Thanks to its memory efficiency and requiring less computational resources, it is much more suited for low-resource settings, such as edge computing applications, where CPU computing time, energy savings, and memory usage are highly impactful. Furthermore, such an ability to require less computing and mem-ory can allow for in-device learning, preserving privacy, which is ever more important in such scenarios. In addition, the introduced loss formulation allows for more control over the estimation bias with respect to the CDQ approach.\nTo benchmark and compare the proposed algorithm, we use a selection of Mujoco [14] robotics environments from the OpenAI Gym suite [15].\nThe paper is structured as follows: in Section 2, we discuss recent contri-"}, {"title": "2. Related work", "content": "The problem of estimation bias in Value function approximation has been widely recognized and addressed in numerous studies. Notably, Q-learning has been identified to exhibit overestimation bias in discrete action spaces, as highlighted in [16]. A seminal response to this challenge was Double Q-learning, introduced by Van Hasselt [17], marking a foundational devel-opment in this field. Building on this, the Maxmin Q-learning approach [18] demonstrated that employing a broader ensemble of more than two Q estimates could substantially alleviate this bias and enhance the efficacy of Q-learning.\nIn the context of controlling continuous action spaces, recent contribu-tions have tackled the dual issues of underestimation and overestimation biases by ensembling Q function estimates [19, 20, 21, 9]. Truncated Quan-tile Critics (TQC) was proposed by Kuznetsov et al. [19], which extends the Soft Actor-Critic (SAC) by integrating an ensemble of five critic estimates. This model not only offers a distributional representation of the critic but also implements truncation during critic updates to reduce overestimation bias. Moreover, TQC refines the actor update mechanism by employing an averaged ensemble of Qs.\nMirroring some aspects of TQC, Randomized Ensembled Double Q-learning (REDQ) developed by Chen et al. [20] uses a larger ensemble of 10 networks. Unlike TQC, In REDQ, the critic estimates are updated multiple times for each step in the environment, 20 times in the presented results.\nFurther advancing the methodology, Quasi-Median Q-learning (QMQ) introduced by Wei et al. [21] employs four Q estimates and the quasi-median operator to compute the targets for critic updates. This method highlights a trade-off approach between overestimation and underestimation, while the policy gradient is computed relative to the mean of the Qs.\nAdditionally, the Realistic Actor-Critic (RAC) approach by Li et al. [9] seeks to strike a balance between value overestimation and underestimation, employing an ensemble of 10 Q networks. In this approach, the ensemble of Q functions is updated 20 times per environmental step using targets com-puted from the mean of the Qs minus one standard deviation, with the actor update maximizing the mean of the Q functions. This ensemble strategy has been applied successfully to both TD3 and SAC, achieving competitive performance and sample efficiency akin to Model-Based RL [22].\nRemark 1. The majority of contributions discussed in this section enhance"}, {"title": "3. Background", "content": "Reinforcement Learning (RL) is modeled as a Markov Decision Process (MDP), encapsulated by the tuple (S, A, P, R, \u03b3). The components S and A represent the continuous state and action spaces, respectively, necessitating a continuous transition density function P:S\u00d7A\u00d7S \u2192 [0,\u221e). The reward function is denoted as r : S \u00d7 A \u00d7 S \u2192 R, and the discount factor is given by \u03b3\u2208 [0,1]. Policy \u03bc, parameterized by 6, is a mapping at each time-step t from the current state st \u2208 S to an action at \u2208 A, defined by the conditional distribution \u00b5\u03c6(at|st). This policy is often realized through a Neural Network.\nThe principal objective in RL is to find a policy u that maximizes the expected discounted sum of rewards, mathematically expressed as Rt = \u0395\u03bc\u03bf [\u03a3\u03c4\u03bf rt]. To facilitate this, RL utilizes two primary constructs: the value function V and the action-value function Q, defined as:\n$V(s) = \u0395\u03bc[Rt | St = s]$ (1)\n$Q(s, a) = E\u00b5[Rt | St = s, At = a]$ (2)\nWithin the framework of Reinforcement Learning, the Q function can be recursively defined, leading to its essential role in both theoretical exploration and practical application:\n$Q(s, a) = E\u00b5[r(s, a, s') + yE[Q\u201c(s', a')]]$\nUnder the assumption that the policy at subsequent time t + 1 is optimal, Q-learning reformulates the Q function as:\n$Q\u2122(s, a) = E[r(s, a, s') + y max Q\" (s', a)]$\na\nThis off-policy characterization allows the Q function to depend solely on the environmental dynamics. If the greedy policy is modeled as a neural network \u03bc, parameterized by 6, the expression for the Q function refines to:\n$Q(s, a) = E[r(s, a, s') + yQ\u2122(s', \u03bc\u03c6(s'))]$\nThe Deterministic Policy Gradient (DPG) algorithm [23] employs the Q function to derive a policy update rule for a differentiable model Qe:\n\u25bdJ($) = \u2207[Qe(s,a)|s=st,a=\u00b5\u2084(s)], (6)\n= a[Qe(s, a)|s=st,a=\u03bc\u03c6(s)]\u2207\u00a2\u00b5\u00a2(S)|s=st (7)"}, {"title": "4. Overestimation and Underestimation in Q-Learning", "content": "When dealing with discrete action spaces, the Value function can be optimized with Q-learning with the greedy target y = r + maxaQ(s', a'). However, in [16], it has been proven that if this target has an error, then the maximum over the value biased by this error will be greater than the true maximum in expectation. Consequently, even when errors initially have mean zero, they probably lead to consistent overestimation biases in the up-dates of values, which are then carried through the Bellman equation. In [5], the authors have shown both analytically and experimentally that this overestimation bias is also present in actor-critic methods. While the over-estimation may seem minor with each value update, the authors express two concerns. First, if not addressed, the overestimation could accumulate into a more substantial bias over numerous updates. Second, an imprecise value estimate has the potential to result in suboptimal policy updates. This poses a significant issue, as it initiates a feedback loop where suboptimal actions, favored by the inaccurate critic, can be reinforced in subsequent policy up-dates. For these reasons, CDQ was introduced in [5] in the TD3 algorithm, showing significant improvements with respect to previous state-of-the-art, i.e., DDPG. However, CDQ has two main drawbacks: (i) it introduces an uncontrollable underestimation bias in the critic, and (ii) memory and com-putation consumption are doubled in the critic estimate due to the introduc-tion of a second Q network. This expensiveness is shared by SAC, too, and is exacerbated in newer, improved alternatives, as discussed previously.\nThe rest of this section proposes an extension to DDPG for the control of the overestimation bias with an alternative strategy to CDQ without the need to introduce a second Q network."}, {"title": "4.1. Tackling Overestimation with a single Q estimate", "content": "TD3 applies CDQ in the critic updates in order to favor underestima-tion over overestimation, hoping to counterbalance the bias introduced by Q-learning. Even though TD3 is an effective algorithm and theoretically sound, taking the minimum between the two estimates leaves very little room for adjusting this bias in case we have any evidence that it's hurting the per-formances. For this reason, we explore a method that allows more control over a possible underestimation bias to compensate for the overestimation induced by Q-learning, with a single Q function estimate, thus making it computationally cheaper and having a smaller memory footprint compared to the CDQ mechanism shared by TD3 and SAC. Specifically, we propose to change the CDQ mechanism with an Expectile Regression Loss for a single Q function.\nThe T expectile in probability theory for a cumulative density function F of the random variable X is the solution of the following equation [24]:\n$(1-7) \u222b_{-\u221e}^{t} (t - x)dF(x) = \u0442 \u222b_{t}^{+\u221e} (x - t)dF(x)$ (9)\nHowever, the value for t can be hard to interpret, so, for this reason, we will use the following equivalent definition using two hyperparameters \u03b1, \u03b2:\n$\\frac{\u03b1}{2 max(\u03b1, \u03b2)} \u222b_{-\u221e}^{t} (t-x)dF(x) = \\frac{\u03b2}{2 max(\u03b1, \u03b2)}\u222b_{t}^{+\u221e} (x - t)dF(x)$ (10)\nIndeed, it can be seen how if we set \u03c4 = 0.5, then t = E[X]. More specifically, 7 defines a monotonically increasing mapping with respect to t, thus allowing to control the distance to the mean.\nDefinition 1. We say that a function f : R \u2192 R is monotonic non decreasing if and only if, given x1, x2 \u2208 R and x1 < x2, then f(x1) \u2264 f(x2)\nTheorem 1. The function defined in eq. (9) is monotonic non-decreasing, thus, given 71 \u2264 T2, then t\u2081 < t2, with t\u2081 and t2 the respective expectiles solution of eq. (9) T1 \u2264 T2.\nProof. We first need to consider that the Equation (9) is actually a function. Such function is defined as:\nf(t) =ts.t. $(1-7) \u222b_{-\u221e}^{t} (t - x)dF(x) = \u0442 \u222b_{t}^{+\u221e} (x - t)dF(x)$\nWe need to show that such a function is monotonic non-decreasing; thus, if \u03c4 increases, then the corresponding f(t) cannot decrease.\nDividing both sides by (1 - T) and then by $\u222b_{t}^{+\u221e} (x - t)dF(x)$, we can rewrite eq. (9) in the following way:\n$\\frac{T}{1-T} = \\frac{\u222b_{-\u221e}^{t} (t-x)dF(x)}{\u222b_{t}^{+\u221e} (x - t)dF(x)}$\nWe can observe that the left-hand side is monotonic with respect to \u03c4. Indeed its derivative is g(r)' = $\\frac{1}{(1-7)^2}$ \u2265 0\u0474\u0442 \u0454 (0,1). Furthermore, we can observe that the integrands on the right-hand side are non-negative. Considering that:\ngiven f(x) \u2265 0, a \u2264 min(b1, b2) : $\u222b_{a}^{b1} f(x)dx \u2264 \u222b_{a}^{b2} f(x)dx \u2194 b1 <b2$,\ngiven f(x) \u2265 0, max(a1, a2) \u2264 b : $\u222b_{a1}^{b} f(x)dx \u2264 \u222b_{a2}^{b} f(x)dx \u2194 a1 a2$,\nwe can thus conclude that the right-hand side is monotonic with respect to t.\nTo conclude, since the left-hand side is monotonic with respect to \u03c4, that the right-hand side is monotonic with respect to t, and that the equality between the two sides has to be preserved, then we can conclude that t is monotonic with respect to \u0442and vice versa.\nThe same holds true for the formulation in eq. (10). In particular, given two hyperparameters a \u2208 R+, \u03b2\u2208 R+ Expectile Regression is the solution of an asymmetric loss, in particular, the Mean Squared Loss that relaxes one of the two sides of the function.\n$L^{\u03b1,\u03b2}(fo(x), y) = \\frac{1}{Z} \\begin{cases} \u03b1 (y - fo(x))^2 & if fo(x) < y\\\\ \u03b2 (y - fo(x))^2 & otherwise \\\\ \\end{cases}$ (11)\nwith Z = max(\u03b1, \u03b2). Thanks to theorem 1, we can prove that a and \u1e9e control the overestimation-underestimation bias, namely, for a generic function f:\n1. \u03b1 = \u03b2 reverts to Mean Squared Error (MSE), as the solution of Le for any c > 0 is exactly the MSE.\n2. \u03b1 < \u03b2 favors underestimation errors, as fa,\u1e9e(x) \u2264 fc,c(x), with fa,f(x) solution of La, and fc,c(x) solution of Lc for any c > 0\n3. \u03b1 > \u03b2 favors overestimation errors, as fa,f(x) \u2265 fc,c(x), with fa,f(x) solution of La, and fc,c(x) solution of Lc for any c > 0\nIn fig. 1, we show the function learned by a fourth-degree polynomial mini-mizing the expectile loss varying \u03b1, \u03b2 approximating the function y = 0.1x3+\n\u0454, \u0454 ~ N (0,8). Features were sampled uniformly from U(-10, 10), and both features and targets were normalized in the range [0, 1]. The parameters are"}, {"title": "5. Smoothing the optimization landscape", "content": "Actor-critic RL algorithms for continuous control algorithms are mainly composed of 2 components: the Q-function Qe and the policy network \u03bc\u03c6\u00b7 The optimization criterion of the latter one, can be seen as gradient ascent procedures on the former one. However, differently from the usual optimiza-tion settings where we assume the function to be stationary throughout the optimization procedure[27], in DDPG-like algorithms Qe changes over time. Due to this property, and due to the fact that is a conditional optimization, as it depends on the state s, it's very hard to take advantage of new optimiza-tion techniques [25], such as adaptive stepsizes and momentum. Furthermore, recently, adversarial attacks [28] have shown how the landscape of a neural network is far from being smooth, and that small changes in the input, such as the action computed by \u00b5\u00f8 fed to Qe(s, a), can lead to big changes in the output, in our case in the Q-estimate. For this reason, is very important to tackle this ill-conditioning of the Q-network Qe as also addressed in [5, 29]. One such way, applied in GANs, is to build and regularize the final network in such a way that it is almost 1-Lipschitz [30]. Another way, also used in GANs, is to penalize the gradient. In order to apply such a method in the DDPG case would lead to the following loss:\n$L(0) = (r + yQo' (s', \u03bc\u03c6\u03b9 (s')) \u2013 Qe(s,a))\u00b2 + \u00a7||\u2207aQ(s, a)||$ (14)\nEven though this formalization can work, it is very computationally expen-sive, as it requires first estimating the gradient of the Q-estimate with respect to the action, computing the norm of it, and then doing the full loss gradient update. However, [31] shows how penalizing the gradient of the output, in our case the Q-estimate, to the input, in our case the action, is equivalent to adding noise. Even though the two are theoretically similar, they have very different computational costs. With this technique, in our case, this implies solving the following optimization objective:\n$L(0) = (E\u20ac~p(x) [r + yQo' (s', \u03bc\u03c6' + e(s'))] \u2013 Qe(s, a))\u00b2.$ (15)\nThis new formulation, applied to the EdgeDDPG algorithm, equates to solv-ing the expectation over the Expectile loss:\n$L(0) = La,\u03b2 (Qe(s, a), E\u20ac~p(x) [r + Qo' (s', \u03bc\u03c6' + e(s'))]). $"}, {}, {"title": "6. Experiments", "content": "To evaluate our proposed algorithm, we benchmark its performance on the suite of Mujoco [14], a set of robotic environments aimed for continuous control, with no change to the environment itself or the reward to improve reproducibility.\nFor our implementation of all the algorithms, we used a feed-forward network composed of 3 layers of 256 neurons, optimized using Adam optimizer [25] with 3.10-4 stepsize for a fair comparison. For EdgeD3, TD3 and SAC we considered k = 2, thus the actor is updated every 2 updates of the Q-functions. For the target smoothing distribution, we use the proposed clipped Gaussian distribution also used in [5], and for the exploration policy, we used a Gaussian distribution N(0,0.1) for all the algorithms, apart from SAC [6] where we used the learned posterior distribution. More technical details for reproducibility can be found in appendix Appendix A. In the rest of this section, we firstly compare the memory usage of the proposed algorithm with state of the art, secondly, their GPU-time utilization, and thirdly, we compare learning performance on the Mujoco benchmarks."}, {"title": "6.1. Resource use comparison", "content": "The proposed algorithm EdgeD3 aims at being a step towards RL-algorithms that are suited for Edge Computing, which lately is gaining a lot of attention thanks to its natural ability to be scalable and highly privacy-preserving, as all the computation is done on-device. Such a setting, however, requires the use of the least amount of computational resources as well as memory resources. Indeed, edge computing algorithms aim at achieving the following characteristics[33, 34, 35]:\n\u2022 minimal CPU usage: the processing power of an edge device is limited in order to keep the cost of the device low;\n\u2022 minimal memory usage: as per the CPU usage, the memory is also limited for production cost;\n\u2022 minimal computation: many edge devices, such as smartphones, are powered by a battery, and having CPU-intensive algorithms leads to shorter battery duration but also shorter overall life of the battery due to overheating."}, {"title": "6.1.1. RAM usage", "content": "The most popular algorithms that build upon DDPG, implementing the DPG estimator[23], are SAC and TD3, which both utilize the CDQ mecha-nism, exploiting an ensemble of two Q functions to estimate the Q-learning target. However, having this additional function also implies having another additional target function, thus effectively having four networks in total to maintain in memory. Such additional cost is justified by the improvements in performance. However, if memory is a concern, such as for edge computing, this additional cost might not be worth it. For this reason, we will compare the algorithms by their memory consumption. This section, in conjunction with section 6, shows we can achieve state-of-the-art performances with much less memory required, thanks to the new loss formulation.\nSince all algorithms share the same Replay Buffer size, the only factor influencing the footprint is the number of networks that the algorithms re-quire. In table 1, we show the percentage of decrease in peak memory usage of 10.000 update steps of each algorithm using 10-dimensional fake Gaussian noise data generated at the beginning, thus removing the replay buffer from the memory consumption, and also removing the environment, which might cause some sharp increases in memory usage biasing the estimates. The test was carried out using the CPU for the computation so that the memory used by the process was not split across RAM and GPU memory."}, {"title": "6.1.2. GPU-time comparison", "content": "TD3 and SAC are popular algorithms that build upon DDPG. They both exploit an ensemble of two Q estimates for the calculation of the Q-learning target. During training however, we are required to train them indepen-dently, and we need to pay the computational expenses of the additional Q-function. Instead, EdgeD3 requires the same network as the original DDPG algorithm, avoiding such additional computational costs.\nIn order to have a better sense of the improvement brought by EdgeD3 for edge computing from a computational point of view, we compared the dif-ferent algorithms to showcase the various footprints. The hyper-parameters used for the comparison are the default ones used in the respective papers. The only exception to this is for Soft Actor-Critic, which originally did not use the delayed update. For a fair comparison, we report both the compu-tational time of SAC with the delayed update (SACd) and without (SAC\u00b0). For the benchmarks, however, we report the original SAC implementation, which updates the actor at every step, together with the Q-functions.\nSince all have a Replay Buffer B, and that all at inference time have roughly the same cost, as the state is forwarded through the policy network \u03bc\u03b5 and some Gaussian noise is added to the network prediction, the only component that can vary the computational cost, is the training loop. For this reason, we create random data from a 10-dimensional Gaussian distribu-"}, {"title": "6.1.3. Comparison to state-of-the-art", "content": "For the comparison with other algorithms, as reported at the beginning of the section, we will use the Mujoco suit. For reproducibility, we used the same criterion used by the authors TD3 [5], having a bootstrap phase at the beginning of the learning. Each task ran for 1 million steps and was evaluated every 5000 step on 10 different environment initializations per eval-uation. The reported results are also averaged over 10 different independent learning with a different seed each for different environment and network initializations."}, {"title": "7. Conclusions and future work", "content": "Edge computing is always gaining more attention thanks to its ability to allow for scalable deployment and preserving the privacy of the final user, handling the computation directly on-device.\nWe present Edge Delayed Deep Deterministic Policy Gradient (EdgeD3), which builds on top of Deep Deterministic Policy Gradient (DDPG) [4]. We introduce a new lightweight, easy-to-implement, highly tunable loss that trades off overestimation and underestimation by exploiting an unbalanced loss, described in eq. (11). Furthermore, we include new tricks to stabilize the training without additional costs. This algorithm aims at being a step to-wards scalable Deep Reinforcement Learning algorithms for edge computing, thus aiming at minimizing the computational cost and the memory footprint, while not hindering performances.\nAs done by previous works, such as TD3 and SAC, it achieves better perfor-mance by tackling the overestimation bias brought by the temporal differ-ence loss of Q-learning. However, instead of using an ensemble of estimators, EdgeD3 exploits a new Expectile loss to do so while avoiding adding compu-tational burden, thus keeping its property of being edge-friendly.\nThis new expectile loss, combined with additional blocks proposed by various other works in literature, such as the target smoothing and the de-layed policy update, allow us to create a method that is 30% computationally cheaper than the current state-of-the-art methods, preserves a memory foot-print on the same level as the original algorithm and 30% smaller that the state of the art, all by preserving the same performances of such more com-putationally demanding algorithms.\nPotential future research avenues involve investigating other unbalanced losses, such as quantile loss or unbalanced-huber loss, which have the same computational cost as the proposed expectile loss while being more robust to outlier values. Furthermore, the proposed method introduces a hyperparam-eter that controls the overestimation and the underestimation. Even though a priori good guess for such hyperparameters exists, there is the possibility to extend the current algorithm with online fine-tuning of such hyperparam-eters in order to tune it automatically. Finally, we aim to test the presented method in real-world scenarios, thus leveraging its ability to be very compu-tationally cheap and to have a small memory footprint to carry out learning directly on edge devices of several control tasks."}, {"title": "Appendix A. Reproducibility", "content": "Appendix A.1. Networks architectures\nFor TD3, DDPG, EdgeDDPG, and EdgeD3 the following network architecture have been used for the actor:\n(state dim -> 256)\nRelu\n(256 -> 256)\nRelu\n(256 -> action dim)\ntanh\nFor SAC, the following network architecture has been used for the actor:\n(state dim -> 256)\nRelu\n(256 -> 256)\nRelu\n(256 -> action dim * 2)\nFor all methods, the following network architecture has been used for the critic:\n(state dim + action dim-> 256)\nRelu\n(256 -> 256)\nRelu\n(256 -> 1)"}, {"title": "Appendix A.2. Hyper-parameters", "content": "In table A.4, we report the hyperparameters used for the simulations. For a fair comparison, the hyper-parameters that could lead to an unfair setting, such as the stepsize, have been kept constant throughout all the methods. For the hyper-parameters that were not common to all of them, we used the one reported in the respective original papers. Regarding the proposed methods, the only parameters that have been varied are the hyperparameters for controlling the trade-off between overestimation and underestimation, formerly \u03b1, \u03b2. However, to avoid cherrypicking of such parameters, only good guesses have been used, and are all reported in fig. B.6. Regarding the decay, we observed little improvement in using both a linear decay and an exponential decay during the execution. Since such decay would be part of non-trivially tunable hyper-parameters, and we wanted to keep the method as simple as possible, we decided to use X(t) = 1, so no decay has been applied during any of the training reported throughout the paper. Thus, all the curves report learning done with fixed \u03b1, \u03b2. Regarding the noise distribution used for the action in the target estimation, we used the clipped Normal distribution introduced in [5]."}, {"title": "Appendix B. Ablation over various tradeoffs", "content": "Indeed, the expectile loss allows for very simple and flexible control over the tradeoff between underestimation and overestimation compared to the CDQ mechanism. Thanks to such freedom, we can actually evaluate various values for \u03b1, \u03b2 in order to understand the problem we are trying to solve. For this reason, in fig. B.6, we compare different tradeoffs."}, {"title": "Appendix C. Comparison with environment steps", "content": "In fig. C.7 we report the comparison of the proposed algorithm and the baselines using environment timesteps as a unit of measure. However, we want to emphasize that this might be misleading, as gives no sense of the computational cheapness of the proposed method. Indeed, the aim of this paper is to present a new, computationally, and memory-cheap algorithm suited for edge scenarios. Therefore the focus was on speed and lightness, showing how, taking into consideration these properties, the performance of very established methods such as TD3 and SAC can be reached with much cheaper alternatives."}]}