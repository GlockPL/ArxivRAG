{"title": "PATCH-BASED DIFFUSION MODELS BEAT WHOLE-IMAGE MODELS FOR MISMATCHED DISTRIBUTION INVERSE PROBLEMS", "authors": ["Jason Hu", "Bowen Song", "Jeffrey A. Fessler", "Liyue Shen"], "abstract": "Diffusion models have achieved excellent success in solving inverse problems due to their ability to learn strong image priors, but existing approaches require a large training dataset of images that should come from the same distribution as the test dataset. When the training and test distributions are mismatched, artifacts and hallucinations can occur in reconstructed images due to the incorrect priors. In this work, we systematically study out of distribution (OOD) problems where a known training distribution is first provided. We first study the setting where only a single measurement obtained from the unknown test distribution is available. Next we study the setting where a very small sample of data belonging to the test distribution is available, and our goal is still to reconstruct an image from a measurement that came from the test distribution. In both settings, we use a patch-based diffusion prior that learns the image distribution solely from patches. Furthermore, in the first setting, we include a self-supervised loss that helps the network output maintain consistency with the measurement. Extensive experiments show that in both settings, the patch-based method can obtain high quality image reconstructions that can outperform whole-image models and can compete with methods that have access to large in-distribution training datasets. Furthermore, we show how whole-image models are prone to memorization and overfitting, leading to artifacts in the reconstructions, while a patch-based model can resolve these issues.", "sections": [{"title": "INTRODUCTION", "content": "In image processing, inverse problems are of paramount importance and consist of reconstructing a latent image x from a measurement y = A(x) + \u025b. Here, A represents a forward operator and \u025b represents random unknown noise. By Bayes' rule, log p(x|y) is proportional to log p(x) + log p(y|x), so obtaining a good prior p(x) is crucial for recovering x when y contains far less information than x. Diffusion models obtain state-of-the-art results for learning a strong prior and sampling from it, so similarly competitive results can be obtained when using them to solve inverse problems (Chung et al., 2022a; 2023a; Song et al., 2024; Wang et al., 2022; Kawar et al., 2021; Li et al., 2023a).\nHowever, training diffusion models well requires vast amounts of clean training data (Song et al., 2021; Ho et al., 2020), which is infeasible to collect in many applications such as medical imaging (Chung et al., 2022b; Song et al., 2022; Jalal et al., 2021), black hole imaging (Feng et al., 2023; 2024), and phase retrieval (Li et al., 2023a; Wu et al., 2019). In particular, for very challenging inverse problems such as black hole imaging (Feng et al., 2023) and Fresnel phase retrieval (Gureyev et al., 2004), no ground truth images are known and one only has a single measurement y available. In other applications such as dynamic CT reconstruction (Reed et al., 2021) and single photo emission CT (Li et al., 2023b), obtaining a high quality measurement that can lead to a reconstruction that closely approximates the ground truth can be slow or potentially harmful to the patient, so only a very small dataset of clean images are available. Thus, in this paper we consider two settings: the single measurement setting in which we are given one measurement y whose corresponding x be-\nlongs to a different distribution from the training dataset, and the small dataset setting in which we are only given a small number of samples x that belong to the same distribution as the test dataset.\nIn recent years, some works have aimed to address these problems by demonstrating that diffusion models have a stronger generalization ability than other deep learning methods (Jalal et al., 2021), so slight distribution mismatches between the training data and test data may not significantly degrade the reconstructed image quality. However, in cases of particularly compressed or noisy measurements, as well as when the test data is severely out of distribution (OOD) with a significant domain shift, an improper choice of training data leads to an incorrect prior that causes substantial image degradation and hallucinations (Feng et al., 2023; Barbano et al., 2023). To address these challenges in the single measurement case, recent works use each measurement y to adjust the weights of a diffusion network at reconstruction time Barbano et al. (2023); Chung & Ye (2024), aiming to shift the underlying prior learned by the network toward the appropriate prior for the latent image in the test case. However, as the networks have huge numbers of weights, an intricate and parameter-sensitive refining process of the network is required during reconstruction to avoid overfitting to the measurement. Furthermore, there is still a substantial gap in performance between methods using an OOD prior and methods using an in-distribution prior. Finally, these methods have only been tested in medical imaging applications (Barbano et al., 2023; Chung & Ye, 2024). On the other hand, in the small dataset case, various methods (Moon et al., 2022; Zhang et al., 2024) have been devised to fine-tune a diffusion model on an OOD dataset, but these methods still require several hundred images and have not used the fine-tuned network to solve inverse problems.\nPatch-based diffusion models have shown success both for image generation (Wang et al., 2023; Ding et al., 2023) and for inverse problem solving (Hu et al., 2024). In particular, the method of Hu et al. (2024) involves training networks that take in only patches of images at training and reconstruction time, learning priors of the entire images from only priors of patches. In cases of limited data, Hu et al. (2024) shows that patch-based diffusion models outperform whole image models for solving certain inverse problems. These works motivate our key insight that patch-based diffusion priors potentially obtain stronger generalizability than whole-image diffusion priors for both the single measurement setting and the small dataset setting due to a severe lack of data. Inspired by this, we propose to utilize patch-based diffusion models to tackle the challenges arising from mismatched distributions and lack of data in a unified way. We first develop a method to take a network trained on patches of a mismatched distribution and adjust it on the fly in the single measurement setting. We also show how in the small dataset setting, fine-tuning a patch-based network results in a much better prior than fine-tuning a whole-image network, leading to higher quality reconstructed images.\nIn summary, our contributions are as follows:\n\u2022 We integrate the patch-based diffusion model framework with the deep image prior (DIP) framework to correct for mismatched distributions in the single measurement setting. Experimentally, we find this approach beats using whole-image models in terms of quantitative metrics and visual image quality in image reconstruction tasks, as well as achieving competitive results with methods using in-distribution diffusion models.\n\u2022 We show that in the small dataset setting, fine-tuning patch-based diffusion models is much more robust than whole-image models and very little data is required to obtain a reasonable prior for solving inverse problems.\n\u2022 We demonstrate experimentally that when fine-tuning on very small datasets, whole image diffusion models are prone to overfitting and memorization, which severely degrades reconstructed images, while patch-based models are much less sensitive to this problem."}, {"title": "BACKGROUND AND RELATED WORK", "content": "Diffusion models and inverse problems. In a general framework, diffusion models involve the forward stochastic differential equation (SDE)\ndx = -\\frac{\\beta(t)}{2}xdt + \\sqrt{\\beta(t)} dw,\nwhere t \u2208 [0,T], x(t) \u2208 Rd, and \u03b2(t) is the noise variance schedule of the process. This process adds noise to a clean image and ends with an image indistinguishable from Gaussian noise (Song\net al., 2021). Thus, the distribution of x(0) is the data distribution and the distribution of x(T) is (approximately) a standard Gaussian. Then the reverse SDE has the form (Anderson, 1982):\ndx = (-\\frac{\\beta(t)}{2}x+\\nabla_{x_t} \\log p_t (x_t))dt + \\sqrt{\\beta(t)} dw.\nScore-based diffusion models involve training a neural network to learn the score function \u2207x log pt (xt), from which one can start with noise and run the reverse SDE to obtain samples from the learned data distribution.\nWhen solving inverse problems, it is necessary to instead sample from p(xr|y), so the reverse SDE becomes\ndx = (\\frac{\\beta(t)}{2}\\nabla_{x_t} \\log p_t (x_t|y)) dt + \\sqrt{\\beta(t)} dw.\nUnfortunately, the term log pt(xt|y) is difficult to compute from the unconditional score \u25bdxt log pt(xt) alone. Liu et al. (2023), Chung et al. (2023b), and Ozdenizci & Legenstein (2023) among others proposed directly learning this conditional score \u2207x log pt (xt|y) instead. However, this process requires paired data (x, y) between the image domain and measurement domain for training, instead of just clean image data. Furthermore, the learned conditional score function is suitable only for the particular inverse problem for which it was trained, limiting its flexibility.\nFor greater generalizability, it is desirable to apply the unconditional score \u2207xt log pt (xt) to be able to solve a wide variety of inverse problems. Thus, many works have been proposed to approximate the conditional score in terms of the unconditional one (Wang et al., 2022; Chung et al., 2023a; 2024; Kawar et al., 2022). Notably, Peng et al. (2024) unified various diffusion inverse solvers (DIS) into two categories: the first consists of direct approximations to pt(y|xt), and the second consists of first approximating E[xo|x, y] (typically through an optimization problem balancing the prior and measurement) and then applying Tweedie's formula (Efron, 2011) to obtain\n\u2207_{x_t} \\log p_t (x_t| y) = \\frac{E[x_0 | x_t, Y] - x_t}{\\sigma_t^2}\nwhere ot is the noise level of xt. All of these methods require a large quantity of clean training data that should come from the distribution p(x) whose score is to be learned, which may not be available in practice.\nMethods without clean training data. When no in-distribution data is available, one approach is to use traditional methods that do not require any training data, such as total variation (TV) (Li et al., 2019) or wavelet transform (Daubechies, 1992) regularizers that encourage image sparsity. More recently, plug and play (PnP) methods have risen in popularity (Sun et al., 2021; Sreehari et al., 2016; Hong et al., 2020; 2024b); these methods use a denoiser to solve general inverse problems. Although these methods often use a trained denoiser, Ryu et al. (2019) found that using an off-the-shelf denoiser such as block matching 3D (Dabov et al., 2006) can yield competitive results. Nevertheless, with the rise of deep learning in image processing applications, methods that harness the power of these tools may be desirable.\nThe deep image prior (DIP) is an extensively studied self-supervised method that is popular when no training data is available and reconstruction from a single meausurement y is desired. The method consists of training a network f\u03b8 using the loss function\nL(\\theta) = ||y \u2013 A(f_{\\theta}(z))||^2, z ~ N(0, I),\nso that f\u03b8(z) produces the reconstruction. Although the neural network acts as an implicit regular-izer whose output tends to lie in the manifold of clean images, DIP is prone to overfitting (Ulyanov et al., 2020). Various methods have been proposed involving early stopping, regularization, and net-work initialization (Liu et al., 2018; Jo et al., 2021; Barbano et al., 2022). Nevertheless, the method is very sensitive to parameter selection and implementation and can take a long time to train (Jo et al., 2021).\nMost DIS methods learn a prior from a large collection of clean in-distribution training images, but recently Barbano et al. (2023) and Chung & Ye (2024) proposed self-supervised diffusion model methods that are based off the DIP framework. These methods involve alternating between the usual"}, {"title": "METHODS", "content": "3.1 PATCH-BASED PRIOR\nWe adapt the patch-based diffusion model framework of Hu et al. (2024); we zero pad the image by an amount P on each side and analyze the distribution of the resulting image x. Then we assume the true underlying data distribution takes the form\np(x) = \\Pi_{i=1}^{M^2}p_{i,B}(x_{i,B}) \\Pi_{r=1}^{(k+1)^2} Pi,r(x_{i,r})/Z,\nwhere xi, B represents the aforementioned bordering region of x that depends on the specific value of i, pi,B is the probability distribution of that region, xir is the rth P \u00d7 P patch when using the partitioning scheme corresponding to the value of i, pi,r is the probability distribution of that region, and Z is an appropriate scaling factor.\nFor training, we use a neural network D\u03b8(x, \u03c3\u03c4) that accepts a noisy image x and the noise level \u03c3\u03c4. For each patch, we define the x positional array as the 2D array consisting of the x positions of each pixel of the image scaled between -1 and 1. To allow the network to learn different patch distributions at different locations in the image, we extract the corresponding patches of these positional arrays and concatenate them along the channel dimension of the noisy image patch and treat the entire array as the network input. Since we are using a patch-based prior, we perform denoising score matching on patches of an image instead of the whole image. Hence, the training loss is given by\n\\underset{\\theta}{\\text{arg min }} E_{t \\sim u(0,T)} E_{x \\sim p(x)}E_{\\varepsilon \\sim N(0,\\sigma_t^2I)} ||D_\\theta(x + \\varepsilon, \\sigma_t) - x||^2,\nwhere x ~ p(x) represents a patch drawn from a sample of the training dataset, \u03c3\u03b5 is a predetermined noise schedule, and U represents the uniform distribution.\n3.2 SINGLE MEASUREMENT SETTING\nConsider the first case where only the measurement y is given, and no in-sample training data is available. For each specific measurement y, the DIP framework optimizes the network parameters \u03b8 via the self-supervised loss (5) from the predicted reconstructed image. Diffusion models provide a prediction of the reconstructed image at each timestep: namely, the expectation of the clean image E[xo|xt] is approximated by the denoiser Do(xt) via Tweedie's formula. Then the expectation conditioned on the measurement E[xo xt, y] can be obtained through one of many methods of enforcing the data fidelity constraint.\nWe begin with the unconditional expectation by leveraging the patch-based prior. Following (8), we apply Tweedie's formula to express the denoiser of x in terms of solely the denoisers of the patches of x. Because the outermost product is computationally very expensive, in practice we approximate Do(x) using only a single randomly selected value of i for each denoiser evaluation:\nD_\\theta(x) \\approx D_{i,B}(x_{i,B}) + \\sum_{r=1}^{(k+1)^2} D_{i,r} (x_{i,r}).\nBy definition, Di,B(xi,B) = 0 and we compute each Dir(xi,r) with the network. Note that (10) provides an unconditional estimate of the clean image; to obtain a conditional estimate Do(xt|y) of the clean image, we run M iterations of the conjugate gradient descent algorithm for minimizing || Ax - y||2, initialized with the unconditional estimate (Chung et al., 2024).\nThe image that is being reconstructed might not come from the distribution of the training images. Hence, the estimate Do(xt|y) may be far from the true denoised image. Thus, we use y to update the parameters of the network in a way such that Do(xt|y) becomes more consistent with the measurement:\n\u03b8 \u2190 arg min\u03b8 || y \u2013 A Do(xt|y)||2.\nPreviously, additional LoRA parameters (Hu et al., 2021) have been used as an injection to the network to leave the original parameters unchanged during this process (Barbano et al., 2023; Chung & Ye, 2024). However, the effect of using different ranks for LoRA versus other methods of network fine-tuning on DIS has not been studied extensively, so we opt to update all the weights of the network in this step. Appendix A.3 shows results from using the LoRA module.\nCrucially, iterative usage of CG for computing the conditional denoiser allows for simple and efficient backpropagation through this loss function, a task that would be much more computationally challenging if another DIS such as Chung et al. (2023a) or Wang et al. (2022) were used. Furthermore, because the number of diffusion steps is large and the change in xt is small between consecutive timesteps, we apply this network refining step only for certain iterations of the diffusion process, reducing the computational burden.\n3.3 SMALL DATASET SETTING\nNow turn to the case where we have trained a diffusion model on OOD data, but we also have a very small dataset of in-distribution test data that we can use to fine-tune the model. When fine-tuning, we initialize the network with the checkpoint trained on OOD data and then use the denoising score matching loss function to fine-tune the network on in-distribution data. Wang et al. (2023) found that improved image generation performance can be obtained by training with varying patch sizes, as opposed to fixing the patch size to the one used during inference. Here, we apply a varying patch size scheme during fine-tuning also as a method of data augmentation. We use the UNet architecture in Ho et al. (2020) that can accept images of different sizes. Hence, the loss becomes\n\\underset{\\theta}{\\text{arg min }} E_{t \\sim u(0,T)} E_{x \\sim p_a(x)}E_{\\varepsilon \\sim N(0,\\sigma_t^2I)} ||D_\\theta(x + \\varepsilon, \\sigma_t) - x||^2,\nwhere x ~ pa(x) represents the drawing a randomly sized patch from an image belonging to the fine-tuning dataset. Appendix A.5 provides full details of the training process.\nAt reconstruction time, we assume that our network has been fine-tuned reasonably to our dataset. Thus, we remove the network refining step in Algorithm 1 and keep the weights fixed throughout the entire process. We still use the same CG descent algorithm to enforce data fidelity with the measurement."}, {"title": "EXPERIMENTS", "content": "Experimental setup. For the CT experiments, we used the AAPM 2016 CT challenge data from McCollough et al. (2017). We applied the same data processing methods as in Hu et al. (2024) with the exception that we used all the XY slices from the 9 training volumes to train the in distribution networks, yielding a total of approximately 5000 slices. For the deblurring and superresolution experiments, we used the CelebA-HQ dataset (Liu et al., 2015) with each image having size 256 \u00d7 256. The test data was a randomly selected subset of 10 of the images not used for training. In all cases, we report the average metrics across the test images: peak SNR (PSNR) in dB, and structural similarity metric (SSIM) (Wang et al., 2004). For the training data, we trained networks on generated phantom images consisting of randomly placed ellipses of different shapes and sizes. See Fig. 20 for examples. These phantoms can be generated on the fly in large quantities. We used networks trained on grayscale phantoms for the CT experiments and networks trained on RGB phantoms for the deblurring and superresolution experiments. Appendix A.4 contains precise specifications of the phantoms.\nWe trained the patch-based networks with 64 \u00d7 64 patches and used a zero padding value of 64, so that 5 patches in both directions were used to cover the target image. We used the network architecture in Karras et al. (2022) for both the patch-based networks and whole-image networks. All networks were trained on PyTorch using the Adam optimizer with 2 A40 GPUs.\nSingle measurement setting. In cases where no training data is available and we only have the measurement y, we applied Algorithm 1 to solve a variety of inverse problems: CT reconstruction, deblurring, and superresolution. For the forward and backward projectors in CT reconstruction, we used the implementation provided by the ODL Team (2022). We performed two sparse-view CT (SVCT) experiments: one using 20 projection views, and one using 60 projection views. Both of these were done using a parallel beam forward projector where the detector size was 512 pixels. For the deblurring experiments, we used a uniform blur kernel of size 9 \u00d7 9 and added white Gaussian noise with \u03c3 = 0.01 where the clean image was scaled between 0 and 1. For the superresolution ex-"}, {"title": "CONCLUSION", "content": "This paper presented a method of using patch-based diffusion models to solve inverse problems when the data distribution is mismatched from the trained network. In particular, we conducted experiments in the setting when only a single measurement is available as well as the setting when a very small subset of in-distribution data is available. In both settings, the proposed patch-based method outperformd whole-image methods in a variety of inverse problems. In the future, more work could be done on using acceleration methods for faster reconstruction, exploring other less computationally expensive methods of fine-tuning the network geared toward inverse problem solv-ing, and methods of refining the prior when a set of measurements are available (Yaman et al., 2020). Limitations of the work include a slow runtime for the self-supervised algorithm and a lack of theoretical guarantees for convergence of algorithms and dataset size requirements."}, {"title": "APPENDIX", "content": "A.1 ADDITIONAL INVERSE PROBLEM SOLVING FIGURES\nFigure 8 shows the results of various methods applied to superresolution in the single measurement setting.\nA.2 EFFECT OF SELF-SUPERVISION FOR DIFFERENT DISTRIBUTIONS\nRecall that in the single measurement setting, Algorithm 1 is used to adjust the underlying dis-tribution of the network away from the originally trained OOD data and toward the ground truth image. We investigated the effect of applying this method even when the network was trained on the\nA.4 PHANTOM DATASET DETAILS\nWe used two phantom datasets of 10000 images each: one consisting of grayscale phantoms and the other consisting of colored phantoms. The grayscale phantoms consisted of 20 ellipses with a random center within the image, each with minor and major axis having length equal to a random number chosen between 2 and 20 percent of the width of the image. The grayscale value of each ellipse was randomly chosen between 0.1 and 0.5; if two or more ellipses overlapped, the grayscale values were summed for the overlapped area with all values exceeding 1 set to 1. Finally, all ellipses were set to a random angle of rotation. The colored phantoms were generated in the same way, except the RGB values for each ellipse were set independently and then multiplied by 255 at the end. Figure 20 shows some of the sample phantoms.\nA.5 EXPERIMENT PARAMETERS\nWe applied the framework of Karras et al. (2022) to train the patch-based networks and whole image networks. Since images were scaled between 0 and 1 for both grayscale images and RGB channels, we chose a maximum noise level of \u03c3 = 40 and a minimum noise level of \u03c3 = 0.002 for training. We used the same UNet architecture for all the networks consisting of a base channel multiplier size of 128 and 2, 2, and 2 channels per resolution for the three layers. We also used dropout connections with a probability of 0.05 and exponential moving average for weight decay with a half life of 500K images to avoid overfitting.\nThe learning rate was chosen to be 2 \u00b7 10-4 when training networks from scratch and was 1.10-4 for the fine-tuning experiments. For the patch-based networks, the batch size for the main patch size (64 x 64) was 128, although batch sizes of 256 and 512 were used for the two smaller patch sizes of 32 \u00d7 32 and 16 \u00d7 16. The probabilities of using these three patch sizes were 0.5, 0.3, and 0.2 respectively. For the whole image model, we kept all the parameters the same, but used a batch size of 8.\nFor image generation and inverse problem solving, we used a geometrically spaced descending noise level that was fine tuned to optimize the performance for each type of problem. We used the same set of parameters for the patch-based model and whole image model. The values without the self-supervised loss are as follows:\n\u2022 CT with 20 and 60 views: \u03c3max = 10, \u03c3min = 0.005\n\u2022 Deblurring: omax = 40, \u03c3min = 0.005\n\u2022 Superresolution: \u03c3max = 40, \u03c3min = 0.01.\nThe values with the self-supervised loss are as follows:\n\u2022 CT with 20 and 60 views: \u03c3max = 10, \u03c3min = 0.01\n\u2022 Deblurring: \u03c3max = 1, \u03c3min = 0.01\n\u2022 Superresolution: \u03c3max = 1, \u03c3min = 0.01.\nFinally, for generating the CT images we used \u03c3max = 40, \u03c3min = 0.005.\nWhen running Algorithm 1, we set K = 10 for all experiments and M = 5 for CT reconstruction and M = 1 for deblurring and superresolution. We ran 5 iterations of network backpropagation with a learning rate of 10-5. When using the LoRA module as in the ablation studies (see Tables 7 and 6), we ran 10 iterations of network backpropagation with a learning rate of 10-3.\nThe ADMM-TV method for linear inverse problems consists of solving the optimization problem\n\\underset{x}{\\text{argmax }} \\frac{1}{2}||y - Ax|| + \\lambda TV(x),\nwhere TV(x) represents the L1 norm total variation of x, and the problem is solved with the alternating direction method of multipliers. For CT reconstruction, deblurring, and superresolution, we chose \u03bb to be 0.001, 0.002, and 0.006 respectively.\nThe PnP-ADMM method consists of solving the intermediate optimization problem\n\\underset{x}{\\text{argmax }} f(x) + (\u03c1/2)||x \u2212 (z \u2212 u)||^2,\nwhere \u03c1 is a constant. The values for \u03c1 we used for CT reconstruction, deblurring, and super-resolution were 0.05, 0.1, and 0.1 respectively. We used BM3D as the denoiser with a parameter representing the noise level: this parameter was set to 0.02 for 60 view CT and 0.05 for the other inverse problems. A maximum of 50 iterations of conjugate gradient descent was run per outer loop. The entire algorithm was run for 100 outer iterations at maximum and the PSNR was observed to decrease by less than 0.005dB per iteration by the end.\nThe PnP-RED method consists of the update step\nx \u2190 x + \u03bc(\u2207f \u2212 \u03bb(x \u2212 D(x))),\nwhere D(x) represents a denoiser. The stepsize \u03bc was set to 0.01 for the CT experiments and 1 for deblurring and superresolution. We set \u03bb to 0.01 for the CT experiments and 0.2 for deblurring and superresolution. Finally, the denoiser was kept the same as the PnP-ADMM experiments with the same denoising strength."}]}