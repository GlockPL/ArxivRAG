{"title": "AUTOREGRESSIVE ACTION SEQUENCE LEARNING FOR ROBOTIC MANIPULATION", "authors": ["Xinyu Zhang", "Yuhan Liu", "Haonan Chang", "Liam Schramm", "Abdeslam Boularias"], "abstract": "Autoregressive models have demonstrated remarkable success in natural language processing. In this work, we design a simple yet effective autoregressive architecture for robotic manipulation tasks. We propose the Chunking Causal Transformer (CCT), which extends the next-single-token prediction of causal transformers to support multi-token prediction in a single pass. Further, we design a novel attention interleaving strategy that allows CCT to be trained efficiently with teacher-forcing. Based on CCT, we propose the Autoregressive Policy (ARP) model, which learns to generate action sequences autoregressively. We find that action sequence learning enables better leverage of the underlying causal relationships in robotic tasks. We evaluate ARP across diverse robotic manipulation environments, including Push-T, ALOHA, and RLBench, and show that it outperforms the state-of-the-art methods in all tested environments, while being more efficient in computation and parameter sizes. Video demonstrations, our source code, and the models of ARP can be found at http://github.com/mlzxy/arp.", "sections": [{"title": "1 INTRODUCTION", "content": "Autoregressive models are the basis of recent breakthroughs in natural language processing (Min et al., 2023). These models predict the next token in a given sequence based on the previous tokens. Autoregressive models are implemented with causal transformers, where each token attends only to preceding ones, and they are trained with the single objective of maximizing the conditional likelihood of each token. Despite their simplicity, autoregressive models such as GPTs (Mann et al., 2020) are shown to demonstrate a reasoning ability that can capture causal dependencies (Prystawski et al., 2024). In this work, we explore the design of a simple autoregressive architecture that can be used for various robot manipulation tasks in diverse environments.\nDecision Transformer (DT) and Trajectory Transformer (TT) are the pioneering approaches that use autoregressive models to solve control tasks (Chen et al., 2021; Janner et al., 2021). These methods learn to generate trajectories as $(R_1, s_1, a_1, ..., R_T, s_T, a_T)$, where $R_t$, $s_t$, $a_t$ respectively denote the reward-to-go (Tamar et al., 2016), the state, and the action at time-step t. In contrast, we propose to predict only the future action sequence, and condition the prediction on the current state (or observation). Action sequence learning is more attainable as an objective in robotics applications, where the underlying reward functions are unknown or ill-defined, and observations, given as images or point clouds, are high-dimensional and cannot be predicted accurately (Kroemer et al., 2021). DT and TT are mainly used in tasks where low-dimensional state variables are fully observed. To deal with uncertainty, our model generates a new action sequence after every k time-steps, using an updated new observation and following the Model Predictive Control (MPC) approach. Action sequence modeling also enables a better leverage of the underlying causal dependencies in robotic tasks. Examples of such causal dependencies include: logical dependencies, where low-level actions depend on high-level plans, spatial dependencies, where orientation depends on the end-effector's position, and temporal dependencies, where latter actions depend on earlier ones. We showcase the action sequence designs of our real robot experiment, Push-T, ALOHA, and RLBench in Figure 3.\nWe propose the Chunking Causal Transformer (CCT), an auto-regressive model that is tailored for robotic tasks. CCT extends the next-token predictions of causal transformer to support multi-token predictions in a single pass. CCT predicts the future tokens, or a chunk of actions, from empty"}, {"title": "2 RELATED WORK", "content": "Learning robotic manipulation from demonstrations. Imitation learning enables robots to learn to perform tasks demonstrated by experts (Zare et al., 2024). Recently, various methods have been developed for manipulation learning with different task constraints and control modalities. Notably, Chi et al. (2023) proposed the Diffusion Policy (DP) method for solving the Push-T task. Zhao et al. (2023) proposed the Action Chunking Transformer (ACT) for bi-manual manipulation tasks in the ALOHA environment. Goyal et al. (2024) proposed RVT-2 for language-conditioned tasks in the RLBench environment (James et al., 2020). We outline these environments and the corresponding state-of-the-art (SoTA) solutions in Figure 2 and Figure 6, respectively. In contrast, our proposed"}, {"title": "3 METHOD", "content": "In this section, we present the Auto-regressive Policy (ARP), which predicts actions using the Chunking Causal Transformer (CCT). The architecture is summarized in Figure 1.\nAction sequence design. Unlike natural language, robot actions lack a universal vocabulary. As shown in Figure 2, different robot tasks may require drastically different types of actions. We therefore propose to represent actions as structured sequences that follow a format that is pre-defined for each family of tasks. Figure 3 showcases the formats of the action sequences generated in our real robot experiment, Push-T, ALOHA, and RLBench tasks.\nAction embedding and decoding. Language models map each word to a continuous vector called word embedding. The word embeddings of the input sentences are fed into a causal transformer. The distribution of the next word is decoded from the output embedding of the last word with a linear layer. Figure A2 and A3 illustrate our embedding and decoding methods for robot actions.\nChunking causal transformer. Figure Al illustrates the essential difference between a causal transformer and our CCT. A causal transformer modifies the token embedding with causal attention so that the last token becomes the next token. Our CCT modifies the token embedding with causal attention for the known tokens $a_i$ (past actions) and bidirectional attention for the empty tokens $e_i$ (future actions). The empty tokens become the next tokens. This allows the prediction of multiple next tokens at once in a single forward pass by adding empty tokens. The advantages are two-fold: (1) A better accuracy is achieved because error accumulation is reduced when actions are grouped in chunks and executed as one unit (Zhao et al., 2023). (2) A better efficiency is achieved with fewer forward passes because each forward pass predicts multiple tokens at once. We study the impacts of action chunking in detail in Section 4. In ARP, CCT alternates between self-attention within the input embeddings and cross-attention with vision features, as in Figure 1. We extract vision features from a standard backbone identical to the ones used in SoTA methods, as detailed in section 4.\nTrain-time attention interleaving. During training, a causal transformer is taught to predict each token in a given sequence by consuming all previous ground-truth tokens as input. This training strategy is named teacher-forcing (Williams & Zipser, 1989). As shown in Figure 4, only a single forward pass is required for training samples such as $a_1, a_2, a_3 \\rightarrow a_4$ (predict $a_4$ from $a_1, a_2, a_3$), $a_1, a_2 \\rightarrow a_3$, and $a_1 \\rightarrow a_2$. Causal transformers are therefore efficiently trained with teacher-forcing. We follow this teacher-forcing strategy. However, training CCT yields separate forward passes per chunk. For example, the prediction of $a_4$ depends on $a_2, a_3$, as in $a_1, a_2, a_3, e_4 \\rightarrow a_4$, but $a_2, a_3$ need to be replaced with $e_2, e_3$ to predict them from $a_1$, as in $a_1, e_2, e_3 \\rightarrow a_2, a_3$. This prohibits the use of a single forward pass for both $a_1, a_2, a_3, e_4 \\rightarrow a_4$ and $a_1, e_2, e_3 \\rightarrow a_2, a_3$. Note $a_i$ denotes the i-th action and $e_i$ denotes the empty token for i-th action. This issue increases the training cost and drastically complicates the training procedure.\nTo resolve this, we first divide the attention into three categories: (1) causal attention within the known tokens, (2) bidirectional attention within the empty tokens, and (3) causal attention between the empty and the known tokens, as marked in Figure 4. Figure A4 shows that the causal attention within known tokens is computed repeatedly. We avoid this redundancy by precomputing the causal"}, {"title": "4 EXPERIMENTS", "content": "In this section, we investigate how the Auto-regressive Policy (ARP) performs compared to the existing methods that were designed specifically for each environment. In addition, we examine"}, {"title": "4.1 COMPARISON WITH STATE-OF-THE-ART", "content": "Setup. We compare the auto-regressive policy (ARP) against the SoTA solutions in Push-T, ALOHA, and RLBench environments. Push-T is a single task. ALOHA consists of two tasks: insertion and cube transfer. RLBench includes 18 tasks, each with multiple language variants. These environments are illustrated in Figure 2 and Figure A5. For Push-T and ALOHA, we train a separate policy for each task. For RLBench, a single policy is trained for all 18 tasks. In Push-T, the policy observes the last two 96 \u00d7 96 RGB frames, and predicts a window of future 2-d pointer positions as actions. In ALOHA, the policy observes the current 480 \u00d7 640 RGB frame and predicts a window of future 14-dimensional joint positions. In RLBench, the policy observes four RGBD 128 \u00d7 128 images and predicts the next target end-effector pose and gripper states. Existing SoTA techniques in these environments are outlined in Figure 6. We use the same vision backbones as the SoTA solutions to extract vision tokens, namely ResNet50 (He et al., 2016) for Push-T and ALOHA, and Multi-View Transformer (Goyal et al., 2023) for RLBench. We use the same training data, number of episodes, optimizer configuration, and evaluation frequency as the SoTA solutions. We detail the full list of hyper-parameters, such as the number of layers, sequence lengths, chunk sizes, and optimizer setups in Appendix A.2. Success rates for Push-T and RLBench are averaged over three independent runs. ALOHA's results are averaged over five runs.\nResults. Figure 5 shows that our auto-regressive policy (ARP) outperforms environment-specific SoTAs while being more computationally efficient. Table A4 compares the per-task success rates of our ARP and RVT-2 (Goyal et al., 2024). In addition, we report the result of ARP+, which shares the same network definition with ARP but has more layers. The MACs / parameter sizes of RVT-2, ARP, ARP+ are 72.1M/57.1G, 71.9M/56.2G, and 73.8M/57.4G, respectively. Notably, ARP+ achieves an average success rate of 86% with a minor increase in computational cost. Note that the success rate of RVT-2 was originally reported as 81.6%. But this was achieved by using a logical time-step as an extra input, which indicates task progress (sub-task milestone). Many existing works have followed this convention (Shridhar et al., 2023; Goyal et al., 2023). However, this information is unavailable in real applications. Thus, we train all our RLBench models without the input time-step."}, {"title": "4.2 ANALYSIS", "content": "Does the performance gain come from auto-regression? Our action sequence design incorporates additional inputs for Push-T and ALOHA, as shown in Figure 3. These inputs are automatically extracted from the demonstration trajectories. In Push-T, the high-level waypoints are simply uniformly sampled down from the low-level trajectories and then discretized. In ALOHA, the pixel coordinates of the end-effector are computed from the joint values with the robot's forward kine-"}, {"title": "4.3 REAL ROBOT EXPERIMENT", "content": "Setup. We evaluate ARP on a challenging tight nut-screwing task using a real robot, which requires precise alignment between the nut and wrench with a tolerance of 2mm, as shown in Figure 8. In each episode, the bolt (blue) is randomly placed on a 20\u00d720 cm\u00b2 table, while the height of the nut (yellow) is randomized along the 6cm tall bolt. The orientations of both the bolt and nut are also"}, {"title": "4.4 QUALITATIVE VISUALIZATION", "content": "We showcase all the evaluation tasks in Figure A5. Video demonstrations of ARP in simulation and in the real world are available in the file Video/demo.mp4 in the supplementary material. In this section, we show two key advantages of ARP: (1) estimating the likelihood of given robot actions, (2) and predicting actions conditioned on human input.\nLikelihood inference. To generate the next token $a_n$, an auto-regressive model estimates the conditional probability $P(a_n|a_1, ..., a_{n-1})$. Using the product rule, the model can estimate the joint probability $P(a_1, ..., a_n) = \\Pi_{i=2}^n P(a_i|a_1, ..., A_{i-1})P(a_1)$ for any given sequences, a capability that more advanced generative frameworks such as VAE and diffusion lack. Figure 9 shows for different trajectories the likelihood estimated by ARP. All these trajectories are human demonstrations. ARP generally assigns higher likelihoods to effective trajectories and lower likelihoods to futile ones. For instance, in sub-figure (b), ARP assigns high likelihoods to two symmetrical trajectories around the T object, demonstrating its understanding of action multi-modality. However, some likelihood assignments are less intuitive. For example, trajectories, , and receive moderately high likelihoods, yet they may not bring the T-shape object closer to the green target, at least not better than the low-likelihood trajectories and 6. marks two similar trajectories, yet they have different likelihoods. We believe that this type of likelihood inference can help identify the model's weaknesses and eliminate defective demonstrations from the training data.\nPrediction with human guidance. Auto-regressive models generate future tokens conditioned on the previous sequence. In Figure 10, we illustrate examples of trajectories of ARP (blue) in Push-T, predicted conditionally on human-drawn initial trajectories (red). The first row (green) shows predictions under correct guidance, where the intention is to complete the task successfully. The"}, {"title": "5 DISCUSSION", "content": "We have shown that ARP is a strong and universal architecture that can be trained to perform diverse manipulation tasks. In the following, we discuss its limitations and potential future directions.\nLearning to plan. Planning is a key ability of intelligent agents. It requires the agent to reason not only about its actions but also their impacts on its environment (Garrett et al., 2021). Motivated by the reasoning capacity of auto-regressive models in NLP, a promising direction is to incorporate planning into ARP. One possible solution is to predict sequences of both states and actions. States in robotics are typically high-dimensional, such as images or point clouds. Therefore, it would be desirable to predict only key states instead of generating every frame in the future. To solve this problem, ARP can be extended to generate future states by using recent hybrid architectures of auto-regression and diffusion, such as Diffusion Forcing (Chen et al., 2024a),\nInteractive robot learning. Human-Robot collaboration improves efficiency by allowing the robot to recover from its errors (Mukherjee et al., 2022; Liu et al., 2023). One possible future direction is to integrate active learning techniques into ARP to learn from immediate human feedback. The auto-regressive mechanism naturally supports conditioning action prediction on human input. Moreover, ARP can estimate the likelihood of action sequences. Likelihood is a common measure for identifying the most informative samples in active learning (Taylor et al., 2021). This can be used, for example, to prioritize demonstrations of tasks where the robot encounters more difficulties.\nAdaptive action sequence learning. Despite ARP's impressive performance, it still requires a manual design of action sequence formats and chunk sizes for each environment. Developing a general approach to automatically determine the optimal chunk size would not only improve ARP's performance, but also deepen our understanding of the action chunking techniques for robot imitation learning in general. We discuss when and why action chunking matters in Appendix A.3. Additionally, unlike natural language, robot actions lack a universal vocabulary. A promising direction is to design a universal robot action language that is applicable across multiple environments, which would reduce the cost of defining new actions, unify training datasets, and improve generalization."}, {"title": "A APPENDIX", "content": "A.1 CODE AND PRETRAINED MODELS\nThe source code of our auto-regressive policy is included in the supplementary folder Code. Please check Code/README.md for instructions on installation, dataset setup, and downloading pre-trained models from an anonymous server.\nA.2 HYPER-PARAMETERS AND IMPLEMENTATION DETAILS\nIn this section, we provide a full list of hyper-parameters in Table A1, Table A2, and Table A3 for Push-T, ALOHA, and RLBench, respectively, along with comments on selected hyper-parameters to provide additional implementation details.\nModel. The mlp size denotes the hidden feature dimension of the MLP network within the standard multi-head attention operation. The number of latents refers to the number of Gaussians for the Gaussian mixture distribution used to decode continuous actions. The backbone denotes the network used to extract the vision features. We use the ResNet50 for Push-T and ALOHA, and Multi-View Transformer (MVT) (Goyal et al., 2023) for RLBench, identical to the ones used in Diffusion Policy, ACT, and RVT2.\nAction Sequence. The horizon refers to the number of actions predicted at each step, while the number of action steps indicates how many actions are actually executed, with the remainder discarded. We adopt the same horizon and action steps as state-of-the-art methods. In Push-T, the chunk size for both high- and low-level actions matches the horizon, meaning all high-level points are predicted in one chunk, followed by all low-level points. Yet, interestingly, as shown in Table 1, combining these two chunks into a single-step prediction degrades performance. For RLBench, which uses the next key end-effector pose as the control interface, there is no need for high-frequency actions, so neither the horizon nor action steps apply. Instead, low-level robot movements are generated using RLBench's built-in RRT planner. We use a chunk size of 2 for binary gripper states and a chunk size of 1 for end-effector positions and rotations. For example, ARP first predicts the roll, followed by pitch and yaw of the rotation Euler angle. We follow the strategy of RVT-2 to predict coarse positions and then refine them by zooming into the images (with updated vision features) to obtain more accurate positions. The end-effector positions are predicted in 2-d, and the 3-d positions are derived from the 2-d coordinates of each viewpoint.\nTrain& Eval. The observation 2 \u00d7 96 \u00d7 96 \u00d7 3 represents 2 frames of RGB images, each with a resolution of 96x96 pixels. For RLBench, the observation 4 \u00d7 128 \u00d7 128 \u00d7 4 refers to RGBD images (with one depth channel) at 128x128 resolution, captured from 4 cameras. In ALOHA, the maximum evaluation steps of 400 and control frequency of 50Hz indicate an evaluation time limit of 8 seconds. LAMB refers to the large batch optimizer proposed by You et al. (2019). We use the same number of training steps, evaluation frequency, optimizer, learning rate, and learning rate scheduler as used by the SoTA solutions.\nA.3 DISCUSSION ON ACTION CHUNKING\nAction chunking has a clear downside \u2013 when predicting multiple actions at a time, the agent doesn't receive information about what state was observed after the first action. This means that the agent is operating with less information than if a single-step prediction was used. At the same time, in a MDP the state is guaranteed to be a sufficient statistic for the optimal policy. Given this information, why should action chucking be useful?\nWe propose two main reasons. First, as has been explored in other imitation learning works, using expert data means that the dataset often lacks information on how to recover from errors, which means that predictions grow worse over time. Using longer action chunks effectively shortens the time horizon. However, we find that action chunking still has noticeable benefits even when the state is well-covered, such as in the Push-T environment. Additionally, this problem becomes less severe as the dataset grows when the prediction error goes to zero, so does the effect of error recovery Ross et al. (2011)."}]}