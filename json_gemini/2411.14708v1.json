{"title": "Understanding LLM Embeddings for Regression", "authors": ["Eric Tang", "Bangding Yang", "Xingyou Song"], "abstract": "With the rise of large language models (LLMs) for flexibly processing information as strings, a natural application is regression, specifically by preprocessing string representations into LLM embeddings as downstream features for metric prediction. In this paper, we provide one of the first comprehensive investigations into embedding-based regression and demonstrate that LLM embeddings as features can be better for high-dimensional regression tasks than using traditional feature engineering. This regression performance can be explained in part due to LLM embeddings over numeric data inherently preserving Lipschitz continuity over the feature space. Furthermore, we quantify the contribution of different model effects, most notably model size and language understanding, which we find surprisingly do not always improve regression performance.", "sections": [{"title": "1. Introduction and Related Work", "content": "Regression is a fundamental statistical tool used to model the relationship between a metric and a selected set of features, playing a crucial role in various fields, enabling predictions, forecasting, and the understanding of underlying relationships within data. Traditional regression techniques often rely on handcrafted features or domain-specific knowledge to represent input data. However, the advent of Large Language Models (LLMs) and their ability to instead process semantic representations of text has raised the question of whether regression can instead be performed over free-form text.\nPrevious works have predominantly examined the topic of LLM-based regression through decoding, i.e. generating floating point predictions using token-based sampling. For example, (Song et al., 2024) examines the case when the model is fully accessible and fine-tunable against data, while (Vacareanu et al., 2024) study the ability of service-based closed-source LLMs such as GPT-4 using in-context learning.\nOne understudied case however is the use of service-based LLM embeddings - fixed vector representations derived from pre-trained (but frozen) language models, which are ubiquitously offered among most LLM services (Google, 2024; OpenAI, 2023). Although they are used frequently in recent applications such as retrieval (Karpukhin et al., 2020), semantic similarity (Li et al., 2020), and a variety of other downstream language tasks (Liu et al., 2020), there has been very little research for their general and direct use in regression, outside of specific applications such as Bayesian Optimization (Nguyen et al., 2024)."}, {"title": "2. Problem and Methodology", "content": "A regression task $T = (f, X, D)$ consists of an underlying scalar-valued function $f : X \\rightarrow \\mathbb{R}$ over an input space $X$. Provided are offline training data $D_{train} = \\{(x_1, y_1), ..., (x_t, y_t)\\}$ collected from querying $f$ and an analogous test set $D_{test}$ for evaluation. Given access to training data $D_{train}$, the goal is to obtain accurate predictions over test points $(x, y) \\in D_{test}$, usually measured by an aggregate performance measure, e.g. mean squared error or Kendall-Tau ranking scores.\nRequired by nearly all learnable regression methods are features, which we assume come from an embedder $\\phi : X \\rightarrow \\mathbb{R}^d$ which takes an input x and returns a fixed-dimensional feature representation, of dimension d. Here, we use the terms \"features\" and \"embedding\" interchangeably, since traditional methods typically use a canonical, manually defined feature engineering method for tabular data, in which continuous values are normalized and categorical selections are one-hot encoded. This feature vector $\\phi(x)$ is then sent to a downstream predictor, e.g. MLP or random forest, which is trained using a loss function such as mean squared error.\nLanguage models also provide a canonical definition of embedding, which typically consists of, in order:\n1. Tokenizing a string representation x into L tokens.\n2. Obtaining a \"soft prompt\" $R^{L \\times v}$ via vocabulary look-up.\n3. Applying a forward pass of a Transformer to obtain an output $R^{L \\times f}$.\n4. Pooling down to a fixed dimension vector in $\\mathbb{R}^d$.\nAfterwards, one may also attach an MLP predictor head and apply an analogous training procedure as in the traditional case. Thus we can see that the only difference becomes the input representation $\\phi$, i.e. whether we used a traditional $\\phi_{trad}$ or LLM-based $\\phi_{LLM}$.\nWhile it is straightforward to assume that the whole process outlined for LLMs should constitute the definition of a language model embedding $\\phi_{LLM}$, it is not obvious how much each of these steps may contribute to the final regression result. For instance, one could simply skip applying a forward pass in step (3) and pool the soft prompt directly, or use a randomly initialized model as opposed to a pretrained one. We extensively study this case in Section 3.3."}, {"title": "2.1. Modeling", "content": "To minimize confounding factors and maintain fairness during comparisons, we use the exact same MLP prediction head (2 hidden layers, ReLU activation), loss (mean squared error), and y-normalization scheme (shifting by empirical mean and dividing by empirical deviation), regardless of using LLM and $\\phi_{trad}$. Note however, that the embedding dimensions of the two representations may be different, and so we distinguish them using notation $d_{llm}$ and $d_{trad}$ respectively, where typically $d_{llm} > d_{trad}$. Further details can be found in Appendix B.1.\nTo demonstrate consistent results over different families of language models, we benchmark over both the T5 (Raffel et al., 2020) and Gemini 1.0 (Google, 2024) families, which use different architectures (encoder-decoder and decoder-only), different vocabulary sizes (32K and 256K), and embedding dimensions (See Appendix B.2) respectively. However, to remain consistent with the definition of embedding, we follow previous literature (Li et al., 2020; Reimers and Gurevych, 2019) and use average-pooling as the canonical method of aggregating Transformer outputs, and thus the embedding dimension $d_{llm}$ is equivalent to the the output feature dimension $f$ following a forward pass.\nSimilar to previous work (Nguyen et al., 2024; Song et al., 2024), for string representations of x from any regression task, by default we use a key-value JSON format with consistent ordering of keys, i.e. {param1:value1,param2:value2,...},with specific examples shown in Appendix C."}, {"title": "2.2. Regression Tasks", "content": "For regression tasks, we first use synthetic, closed-form objective functions in order to produce controlled studies in which we may query any x from the input space. Our synthetic functions are defined from the standard Black-Box Optimization Benchmarking (BBOB) suite (Elhara et al., 2019). To avoid confounding terminology between embedding \"dimension\" d and the intrinsic \"dimension\" of an objective f, we denote the latter as \"degree-of-freedom\" (DOF), and thus f(\u00b7) is dependent on input coordinates $x_{(1)}, ..., x_{(DOF)}$, each of which is between [-5, 5]. This provides a comprehensive variety of both convex and non-convex objective landscapes to regress upon.\nWe further use real-world regression tasks representative of those encountered in the wild and in industry settings by benchmarking over offline objective evaluations found in Google Vizier (Golovin et al., 2017), which optimizes Google's largest production and research systems. These consist of four families, with each family containing at least 50 individual yet similar regression tasks. The families are:\n\u2022 AutoML (Google Cloud, 2023): Automated Machine Learning platform for automating TFX (Google, 2023) pipelines (e.g. batch size, activation, layer counts) over tabular or text data.\n\u2022 Init2Winit (Dahl et al., 2023): Learning rate scheduling parameters influencing common image classification tasks (e.g. ResNets on CIFAR-10 and ImageNet).\n\u2022 XLA (Phothilimthana et al., 2021): Tuning for the Accelerated Linear Algebra (XLA) compiler which affects LLM serving latencies.\n\u2022 L2DA (Yazdanbakhsh et al., 2021): \"Learning to Design Accelerators\", for improving accelerators such as TPUs and corresponding computer architectures to improve hardware performance.\nIn the real world regression tasks, each parameter may be continuous or categorical, and we define the DOF of such a task by its number of parameters. Note that for synthetic objectives, where all inputs are continuous, $d_{trad}$ = DOF. However, for real-world tasks with categorical parameters, $d_{trad} > DOF$ due to additional one-hot encodings.\nFor obtaining data, we may either sample (x, y) pairs (in the case of synthetic objectives where x are uniformly sampled from X), or use the given offline data (in the case of real-world tasks, where they"}, {"title": "3. Experimental Results", "content": "Due to the inherent differing of metric scales across tasks, it would be inappropriate to aggregate results based on scale-dependent metrics such as mean squared error (MSE). Furthermore, we found that the selection of the regression metric (e.g. Kendall-Tau, Pearson, mean squared error, mean absolute error) did not matter for comparisons, as they all strongly correlated with each other. Thus, by default we report the Kendall-Tau ranking correlation, which is always within [0, 1] and can also be aggregated across different tasks."}, {"title": "3.1. High Dimensional Regression", "content": "We begin by demonstrating cases in which LLM embeddings better represent inputs over high degree-of-freedom spaces than traditional representations. In Figure 2, we show that for a subset of functions, LLM embeddings possess surprising robustness, retaining the same performance for varying DOFs whereas traditional baselines such as XGBoost and MLPs significantly falter over higher DOFs.\nThis result is not universal however, as we show in Appendix A.1, this pattern does not apply for a few selected functions, but nonetheless it occurs in the majority of the BBOB functions. We further corroborate this observation over real-world tasks in Table 1. We see that in general, regressions on LLM embeddings outperform traditional methods more often for tasks with higher DOFs (AutoML and XLA)."}, {"title": "3.2. LLM Embedding Smoothness", "content": "Particularly due to the discrete nature of tokenization, it is non-obvious whether LLM embeddings possess a notion of continuity in embedding space. For example, assuming character-wise tokenization, 1.234 is not so numerically distant from 1.567, but is token-wise distant, as the majority of the tokens (234 and 567) are not shared.\nThe notion of continuity and smoothness is crucial for neural network generalization (Kalimeris et al., 2019; Neyshabur et al., 2018), robustness (Weng et al., 2018), vulnerability to adversarial examples (Goodfellow et al., 2015), and more. We can characterize smoothness in the regression case by the Lipschitz-continuity induced by a representation $\\phi$ in its latent space $\\mathbb{R}^d$.\nIntuitively, similar inputs should lead to similar objective values, which can be quantified inversely by the Lipschitz factor $L(x, x') = ||f(x) - f(x')||/||(\\phi(x) - \\phi(x')||$ with respect to a representation $\\phi$ and $||\\cdot||$ norm. We emphasize to the reader that the input space X does not actually have an explicit notion of distance on its own. Instead, traditionally it has always been assumed that the distance was defined canonically by Euclidean distance over the traditional embedding method, i.e. $||\\Phi_{trad}(x) - \\phi_{trad}(x')||_2$ as demonstrated by common use of Euclidean-based radial basis and Matern kernels (Genton, 2002) during regression modeling. However, as seen from the results previously, it may be the case that $\\phi_{trad}$ is suboptimal for some regression tasks."}, {"title": "Understanding LLM Embeddings for Regression", "content": "In order to analyze the continuity of an embedding $\\phi$ with respect to offline data D, we define a Normalized Lipschitz Factor Distribution (NLFD) as follows:\n1. Normalize each embedding vector $\\phi(x)$ coordinate-wise to have zero mean and unit variance across the dataset D.\n2. For each $x \\in D$, choose $x' \\in D$ such that $\\phi(x')$ is the nearest $l_2$ neighbor of $\\phi(x)$, and compute the Lipschitz factor $L(x, x')$.\n3. To assume an average embedding norm of 1 for different embedding dimensions d, we downscale all Lipschitz factors by $\\sqrt{d}$.\nWe see that there is a high inverse relationship between the skewedness of the NLFD and regression performance. Specifically, in Figure 3, when $\\phi_{LLM}$ outperforms $\\phi_{trad}$ for regression, $\\phi_{LLM}$'s distribution of Lipschitz factors also tends to skew relatively more to zero than $\\phi_{trad}$, and vice-versa.\nTo formally quantify comparisons between NLFDs from $\\phi_{LLM}$ and $\\phi_{trad}$, for a fixed regression task, we may thus compute the Z-score using the difference of the two distributions:\n$Z = \\frac{\\mu_{trad} - \\mu_{LLM}}{\\sqrt{\\frac{\\sigma^2_{trad}}{\\sigma^2_{PLLM}}}}$         (1)\nwhere $\\mu_{\\phi}$ and $\\sigma_{\\phi}$ are respectively mean and standard deviations of the NLFD of a representation $\\phi$. We may then observe the relationship between gaps in representation smoothness vs. regression performance. In Figure 4 with extended results in Appendix A.3, we see that for a given BBOB regression task, the Z-score (i.e. gap in embedding smoothness) is highly correlated with the gap in regression performance, regardless of the model used (T5 or Gemini) or the DOF of the underlying objective f.\nWe further visualize whether $\\phi_{LLM}$ is distance aware, i.e. whether $\\phi_{LLM}(x)$ are $\\phi_{LLM}(x')$ are close in embedding space if $\\phi_{trad}(x)$ and $\\phi_{trad}(x')$ are close. As mentioned before however, there is no ground truth notion of \"closeness\" - nonetheless, we use $\\phi_{trad}$ as a point of comparison. Since it is inappropriate"}, {"title": "Understanding LLM Embeddings for Regression", "content": "to simply sample x's uniformly in a high DOF space, as then average distances concentrate around $\\sqrt{DOF}$, we instead take a reference point and sample points from $l_2$-balls of increasing distance from the reference.\nIn Figure 5, we see that distances over the LLM embedding space are correlated with the traditional measure of distance, but may be non-linearly warped, which benefits LLM-based regression in certain cases as seen in Section 3.1."}, {"title": "3.3. Model Effects", "content": "In this subsection, we comprehensively investigate the impact of many common LLM factors on regression performance.\nAre Larger Models Always Better? Within the research community, the prevailing assumption is that there exists a direct correlation between language model size and performance improvement. However, with the rise of leaderboards such as LMSYS (LMS, 2023), smaller models have been shown to outperform larger competitors, due to differences in their \"recipe\", such as training data quality, pre-training and post-training techniques, and architecture."}, {"title": "Does Language Understanding Actually Help?", "content": "Recent works (Devlin et al., 2019; Li et al., 2020) have claimed that logit-based embeddings mostly measure the semantic similarity between string inputs, and thus it is unconfirmed whether they may be beneficial for numeric regression tasks. To resolve this, using the T5 family, we compare against using (1) a randomly initialized model for the forward pass, and (2) representing our features via vocabulary embeddings without applying a forward pass.\nIn Figure 7, we see that the default mode of applying a forward pass of a pre-trained model performs the best, as expected. However, it is worth noting that in some tasks such as AutoML and L2DA, the improvement is surprisingly quite minimal, suggesting that applying forward passes by pretrained models does not always help for regression.\nWe further ablate differences in string representation, i.e. whether by default to show feature names as {param1:value1, param2:value2,...} or omit them, only showing [value1,value2,...]. In Figure 8, for the majority of tasks, omitting feature names does not significantly affect performance, although specific tasks such as XLA do benefit from feature names. This is surprising, as presumably feature names in XLA tasks such as auto_cross_replica_sharding are not as common as names such as batch_size or learning_rate found in both AutoML and Init2winit.\nThe results of Figures 7 and 8 combined lead to additionally surprising conclusions, such as language-to-numeric transfer. For instance, inputs x from Init2Winit tasks only possess numeric values, and as expected, removing feature names does not significantly change regression results. Yet applying"}, {"title": "Understanding LLM Embeddings for Regression", "content": "forward passes by pre-trained T5 models still benefits regression, despite the fact that T5's pre-training data contains mostly web-corpus data which is unlikely to contain significant amounts of scientific or numeric information (Dodge et al., 2021).\nMore Training Data Reduces Baseline Gaps: Intuitively, as more samples are available in a task, the difference in inductive biases between regression methods should matter less, since predictions will be more influenced by training data. We verify this in Figure 9, where we see that for tasks with low numbers of (x, y) points, there is more variability in performance between using $\\phi_{LLM}$ and $\\phi_{trad}$, but additional training points decreases these differences."}, {"title": "4. Conclusion and Future Work", "content": "We thoroughly investigated multiple important aspects around the use of LLM embeddings for traditional regression. We found that LLM embeddings can be quite performant for input spaces with high degrees of freedom, and proposed the Lipschitz factor distribution to understand the embedding-to-objective landscape and its relationship to regression performance. We further investigated the nuanced conditions for which better language understanding does improve LLM-based regression.\nSince strings, and more generally tokens, can represent many varieties of data, it is worth further understanding the effects of LLM embeddings over non-tabular forms of inputs, including combinatorial objects such as graphs, and even other modalities such as images and videos."}, {"title": "Appendix", "content": ""}, {"title": "A. Extended Experiments", "content": ""}, {"title": "A.1. High Dimensional Regression", "content": "For full transparency, In Figure 10, we display BBOB functions where LLM-based regression was not consistently dimensionally robust against MLP and XGBoost baselines. Note that even in these cases, we still see certain cases where a language model outperforms at least one of the baselines, e.g. in the Discus and DifferentPowers functions, Gemini and T5 outperform MLP but not XGBoost."}, {"title": "B. Exact Modeling Details", "content": ""}, {"title": "B.1. Hyperparameters Used", "content": "The full list of hyperparameters and training details for MLP-based regression (using traditional and language model features):\n\u2022 Regression Head: MLP with 2 ReLU hidden layers of dimension 256.\n\u2022 y-Normalization: We compute the empirical mean \u00b5 and standard deviation o over all y-values in the task's training data, and apply y \u2190 (y \u2212 \u03bc)/\u03c3 as a preprocessing step.\n\u2022 Optimizer: AdamW with sweeped learning rates across {1e-4, 5e-4, 1e-3, 5e-3, 1e-2} and weight decay across {0, 1e-1, 1}.\n\u2022 Loss: Mean Squared Error.\n\u2022 Maximum Epochs: 300, with early stopping enabled.\nFor XGBoost, we additionally grid-searched over the following parameters for each task:\n\u2022 \"min_child_weight\": [1, 5, 10]\n\u2022 \"learning_rate\": [0.001, 0.01, 0.1]\n\u2022 \"gamma\": [0.0, 0.3, 0.5]\n\u2022 \"subsample\": [0.6, 0.8, 1.0]\n\u2022 \"colsample_bytree\": [0.6, 0.8, 1.0]\n\u2022 \"max_depth\": [3, 5, 7]"}, {"title": "B.2. Embedding Sizes", "content": "Table 3 displays the embedding dlm for each model used in our experiments. As mentioned in the main text, note that dllm is significantly larger than dtrad."}, {"title": "C. Example String Representations", "content": "Below are example string representations of x for different regression task families. '...' denotes that there are actually more parameters, but we omit them due to page limits."}]}