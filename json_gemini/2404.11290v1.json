{"title": "Inductive Cognitive Diagnosis for Fast Student Learning in Web-Based Online Intelligent Education Systems", "authors": ["Shuo Liu", "Junhao Shen", "Hong Qian", "Aimin Zhou"], "abstract": "Cognitive diagnosis aims to gauge students' mastery levels based on their response logs. Serving as a pivotal module in web-based online intelligent education systems (WOIESs), it plays an upstream and fundamental role in downstream tasks like learning item recommendation and computerized adaptive testing. WOIESs are open learning environment where numerous new students constantly register and complete exercises. In WOIESs, efficient cognitive diagnosis is crucial to fast feedback and accelerating student learning. However, the existing cognitive diagnosis methods always employ intrinsically transductive student-specific embeddings, which become slow and costly due to retraining when dealing with new students who are unseen during training. To this end, this paper proposes an inductive cognitive diagnosis model (ICDM) for fast new students' mastery levels inference in WOIESs. Specifically, in ICDM, we propose a novel student-centered graph (SCG). Rather than inferring mastery levels through updating student-specific embedding, we derive the inductive mastery levels as the aggregated outcomes of students' neighbors in SCG. Namely, SCG enables to shift the task from finding the most suitable student-specific embedding that fits the response logs to finding the most suitable representations for different node types in SCG, and the latter is more efficient since it no longer requires retraining. To obtain this representation, ICDM consists of a construction-aggregation-generation-transformation process to learn the final representation of students, exercises and concepts. Extensive experiments across real-world datasets show that, compared with the existing cognitive diagnosis methods that are always transductive, ICDM is much more faster while maintains the competitive inference performance for new students.", "sections": [{"title": "Introduction", "content": "With the proliferation of vast online learning resources and web-based online intelligent educational systems (e.g., KhanAcademy.org, junyiacademy.org), a growing number of students and learners increasingly turn to the web as a primary medium for education. Web-based online intelligent education systems (WOIDSs) [Kahraman et al., 2010; Yin et al., 2023] enhance personalized student learning through computer-assisted methods, offering a wealth of educational resources (e.g., courses, exercises). Cognitive diagnosis (CD), as the cornerstone of WOIDSs, plays an upstream and fundamental role in them, affecting downstream modules such as computer adaptive testing [Zhuang et al., 2022], course recommendation [Xu and Zhou, 2020] and learning path suggestions [Liu et al., 2019], etc. Specifically, by analyzing students' historical response logs, CD endeavors to infer students' underlying mastery levels (Mas) and shed light on attributes of exercises (e.g., difficulty, discrimination).\nIn recent years, a plenty of cognitive diagnosis models (CDMs) have come to the fore, like item response theory (IRT) [Haberman, 2005] and neural cognitive diagnosis odel (NCDM) [Wang et al., 2020a]. IRT employs a latent factor to represent Mas, using a simple logistic function as the interaction function (IF). NCDM replaces the traditionally IF with multi-layer perceptrons (MLPs) and adopts concept-specific (i.e., set the embedding dimension as the number of concepts) vectors to depict Mas. As embedding-based methods continue to advance rapidly and become a mainstream, researchers are showing a growing preference for converting both students and exercises into vectorized forms, further refining them with various techniques [Gao et al., 2021; Li et al., 2022; Ma et al., 2022; Wang et al., 2023; Liu et al., 2023; Shen et al., 2024]. Notably, most existing CDMs employ intrinsically transductive student-specific embeddings and thus have to access the response logs of all students during the training phase.\nExisting WOIDSs are open learning environment where a vast number of new students register and complete a multitude of exercises, as shown in the right part of Figure 1. It means that the number of students is uncertain and cannot be pre-defined. In WOIDSs, students expect to obtain immediate and timely feedback on their diagnostic results (i.e., mastery levels, Mas), which can fast aid in their self-improvement or assist teachers in providing tailored advice. Unfortunately, most existing CDMs are transductive and may struggle to provide such diagnostic results quickly due to their reliance on student-specific embedding and their neglect of the identifying binary response patterns. These two factors indicate that they need to be retrained to infer new students' Mas, which is time-consuming and unacceptable in practice. Thus, inductive cognitive diagnosis is more suitable for WOIDSs under open learning environment. In the inductive scenarios of CD, there is an urgent need for efficient and interpretable approach capable of inferring the Mas of new students without retraining model. For inductive cognitive diagnosis, the only related work is incremental CD (ICD) [Tong et al., 2022] which targets"}, {"title": "Related Work", "content": "Cognitive Diagnosis. CDMs are used to evaluate student profiles by employing either latent factor models, such as IRT [Lord, 1952] and multidimensional IRT (MIRT) [Sympson, 1978], or models based on patterns of concept mastery, such as deterministic input, noisy and gate model (DINA) [De La Torre, 2009]. For instance, DINA, a typical example of CDMs, utilizes binary independent variables to represent mastery states, where 0 indicates an unmastered state and 1 represents a mastered state. Favored by recent deep learning techniques, researchers achieve great success in large-scale interactions circumstances. NCDM [Wang et al., 2020a] employs MLPs as IF and represents mastery patterns as continuous variables within the range of [0, 1]. Various approaches have been employed to capture fruitful information in the response logs, such as MLP-based [Ma et al., 2022; Wang et al., 2023], graph attention network based [Gao et al., 2021], Bayesian network based [Li et al., 2022]. However, existing CDMs are tailored for the transductive scenario in CD and cannot be directly applied to the inductive scenario. The only related CDM is ICD [Tong et al., 2022] which targets streaming log data. Its goal is to update the Mas at the next moment without retraining. However, in WOIESs where new students often generate vast amounts of response data, using such an approach can become prohibitively costly due to frequent updates.\nInductive Matrix Completion for Collaborative Filtering. Given that contemporary CDMs do not possess inductive learning abilities, and factoring in that they are evaluated by predicting student outcomes on new, unattempted exercises (similar to filling out the rating matrix), and only having IDs as distinguishing features for students, exercises and concepts, we turn to the methods of Inductive Matrix Completion for Collaborative Filtering [Hartford et al., 2018; Zhang and Chen, 2020; Wu et al., 2021; Shen et al., 2021; Wu et al., 2022] to draw comparisons and encapsulate pertinent studies. Nevertheless, these approaches either demand significant computational resources [Hartford et al., 2018; Wu et al., 2021], potentially hinder-"}, {"title": "Preliminaries", "content": "In this section, we first introduce the fundamentals of cognitive diagnosis. Subsequently, we formalize both the transductive and inductive cognitive diagnosis tasks.\nCognitive Diagnosis Basis. Consider web-based online intelligent education systems (WOIDSs) which contain two sets: $E = \\{e_1,...,e_M\\}$, and $C = \\{c_1,...,c_Z\\}$. They symbolize exercises and knowledge concepts, with respective sizes of $M$ and $Z$. $Q$ represents the relationship between exercises and knowledge concepts, which can be regarded as a binary matrix $Q = (Q_{ij})_{M \\times Z}$, where $Q_{ij} \\in \\{0,1\\}$ means whether $e_i$ relates to $c_j$ or not. Here, we assume that both the exercises and concepts are static, implying that their quantities remain constant. Students in set $S = \\{s_1,...,s_N\\}$, driven by unique interests and requirements, select exercises from $E$. The results are documented as response logs. Specifically, these logs can be illustrated as triplets $T = \\{(s,e,r)|s \\in S,e \\in E,r_{se} \\in \\{0,1\\}\\}$. $r_{se} = 1$ represents correct and $r_{se} = 0$ represents wrong. In this paper, we treat response logs as rating matrix $R \\in \\mathbb{R}^{|S| \\times M}$ where $|S|$ denotes the size of set $S$. It contains three categorical values (1 means right, 0 means no interaction and -1 means wrong. Cognitive diagnosis is to infer $Mas \\in \\mathbb{R}^{|S| \\times Z}$ which denotes the latent Mas of students on each knowledge concept based on $R$.\nTransductive Cognitive Diagnosis. Current CDMs assess student performance within a transductive scenario as shown in the left part of Figure 1. Formally, given the students' set $|S|$, a rating matrix $R\\in \\mathbb{R}^{|S|\\times M}$, a binary matrix $Q$, our goal is to infer $Mas \\in \\mathbb{R}^{|S|\\times Z}$, which denotes the latent Mas of all students.\nInductive Cognitive Diagnosis. The frequent registration and participation of new students in the WOIDSs can be characterized as an inductive scenario. As illustrated in the right part of Figure 1, it expects the CDMs to accurately diagnose for newcomers without retraining the models. Formally, given the existing students' set $S^O$, unseen students' set $S^U$ where $S^O \\cap S^U = \\emptyset, S^O \\cup S^U = S, |S^O| = N^O, |S^U| = N^U$, rating matrices $R^O, R^U$ and a binary matrix $Q$. The"}, {"title": "METHODOLOGY: The Proposed ICDM", "content": "This section begins by presenting the innovative student-centered graph. Following that, we delve into the CAGT process, which allows us to derive representations for students, exercises, and concepts. Subsequently, the proposed global-level interactive function (GLIF) is introduced. We conclude the section by discussing the model's training. Notably, the strength of ICDM lies in addressing the inductive scenario in CD. Hence, all its underlying notions are derived from this scenario. Nevertheless, we claim that ICDM is versatile enough to be applied in the transductive scenario as well. The framework of ICDM is shown in Figure 2.\nStudent-Centered Graph. As illustrated in Figure 3(a), focusing on students, the student-"}, {"title": "CAGT Process", "content": "Construction. The sole features in CD is response logs and Q. Evidently, it is essential to decompose these intricate logs into their constituent elements: student, exercise with right pattern, exercise with wrong pattern, and concept. We encode them with trainable embeddings $H_s \\in \\mathbb{R}^{N^O \\times d}, H_r \\in \\mathbb{R}^{M \\times d}, H_w \\in \\mathbb{R}^{M \\times d}, H_c \\in \\mathbb{R}^{Z \\times d}$. For instance, $h_{s_i} \\in \\mathbb{R}^{1 \\times d}$ denotes the row vector of i-th student. Notably, we employ response-aware embeddings to capture the binary response patterns in exercises, patterns that were overlooked by previous CDMs, in order to infer the new students' Mas. Moreover, based on the phenomenon that the exercises chosen by students might potentially reveal their preferences for learning specific knowledge concepts, we construct the involvement matrix $I^O \\in \\mathbb{R}^{N^O \\times Z}$ with $Q$ and $R$. For example, if student $s_1$ practices exercise $e_2$ which is associated with concept $c_3$, then the value of $I^O_{13}$ is set to 1. Finally, we construct the SCG G based on $R^O, I^O$ and $Q$. Specifically, if $R^O_{ij} = 1$, $U_{s_i,r_j} \\in U$; if $R^O_{ij} = -1$, $U_{s_i,w_j} \\in U$; if $Q_{jz} = 1$, $U_{r_j,c_z} \\in U \\lor U_{w_j,c_z} \\in U$; if $I^O_{iz} = 1$, $U_{s_i,c_z} \\in U$.\nAggregation. As shown in Figure 3(b), the k-hop neighbors of a specific student convey rich information. For instance, the one-hop neighbors directly characterize Bob in WOIDSs, while the behaviors of the two-hop neighbors are somewhat similar to Bob's. They might have answered a particular exercise right or wrong. Clearly, aggregating the information from neighbors is instrumental in more adeptly deducing Mas. Analogously, this holds true for the attributes of exercises and concepts. During the Aggregation phase, we aim to aggregate information from different types of neighbors for each specific node type which can be expressed as\n$\\begin{aligned}\nh^k_{S_i}(R \\rightarrow S) &= AGG \\bigg(\\text{Drop}^{\\rho_1}\\big\\{\\text{h}^{k-1}_{r_j} | \\forall j, \\text{if } U_{s_i, r_j} \\in U\\big\\}\\bigg) ,\\\\\nh^k_{S_i}(W \\rightarrow S) &= AGG \\bigg(\\text{Drop}^{\\rho_1}\\big\\{\\text{h}^{k-1}_{w_j} | \\forall j, \\text{if } U_{s_i, w_j} \\in U\\big\\}\\bigg) ,\\\\\nh^k_{S_i}(C \\rightarrow S) &= AGG \\bigg(\\text{Drop}^{\\rho_1}\\big\\{\\text{h}^{k-1}_{c_z} | \\forall z, \\text{if } U_{s_i, c_z} \\in U\\big\\}\\bigg) . \n\\end{aligned}$\n(1)\nThe term $h^k_{S_i}(R \\rightarrow S), h^k_{S_i}(W \\rightarrow S), h^k_{S_i}(C \\rightarrow S)$ denotes the aggregated outcome from exercises with right pattern, exercises with wrong pattern and concepts at depth k respectively. AGG stands for the aggregator function, which consolidates the information from the provided vectors of neighbors. Here, the aggregator function can be of various types, such as mean aggregation [Hamilton et al., 2017] or graph attention [Velickovic et al., 2018; Brody et al., 2022]"}, {"title": "Interaction Functions", "content": "Interaction Functions (IFs) are devised to predict the likelihood of students correctly answering exercises, which can be formulated as\n$\\begin{equation}\n\\hat{y}_{ij} = \\sigma(F((Mass_i - Diff_{e_j}) \\odot Q_{e_j})),\n\\end{equation}$\n(7)\nwhere $\\hat{y}_{ij} \\in [0,1]$ represents the prediction outcome of i-th student practice j-th exercise, $\\sigma$ typically employs the Sigmoid, $Mass_i \\in \\mathbb{R}^{1\\times Z}$ denotes the inferred Mas of i-th student, $Diff_{e_j} \\in \\mathbb{R}^{1\\times Z}$ denotes the inferred difficulty of j-th exercise. $Q_{e_j} \\in \\mathbb{R}^{1\\times Z}$ signifies the concepts associated with the j-th exercise. ICDM is versatile and adaptable to various IFs. For instance, it is compatible with traditional IFs like MIRT that utilizes the logistic function for $F(\\cdot)$, where $Mass_i = h_{s_i}$, $Diff_{e_j} = h_{e_z}$. Additionally, ICDM can also support recent approaches like NCDM [Wang et al., 2020a], which employs MLPs for $F(\\cdot)$, where $Mass_i = h_{s_i}$, $Diff_{e_j} = h_{e_j}$, all the while ensuring the weights remain non-negative to uphold the monotonicity assumption. However, in practical learning environments, when assessing whether a student can solve a particular exercise, we frequently base our judgment on the difficulty of other exercises they have tackled previously or Mas of students who have attempted the same exercise (i.e., global-level information). Obviously, previous IFs have neglected this aspect."}, {"title": "Global-Level Interaction Function", "content": "The global-level information can be effectively captured by introducing layers of Graph Convolutional Network (GCN). Specifically, we adopt the propagation rule from LightGCN [He et al., 2020] due to its proven efficacy in scenarios dominated by ID-features. To facilitate this, we construct a bipartite graph $G_{se} = (V_{se}, E_{se})$ where $V_{se} = S \\cup E$ and $E_{se}$ involve all observed interactions between students and exercises. The proposed GLIF can be expressed as\n$\\begin{aligned}\nCone_{e_j} &= \\frac{1}{|RC_{e_j}|}\\Sigma_{e_j \\cdot c_z \\in RC_{e_j}}h_{c_z}, \\\\\nMass_i &= \\frac{1}{2}\\big(h_{s_i} + \\frac{1}{\\sqrt{|N_{s_i}|\\mid N_{e_j}|}} \\Sigma_{s_i\\in N_{S_i}, e_j\\in N_{e_j}} h_{e_j}\\big) \\odot Cone_{e_j}, \\\\\nDiff_{e_j} &= \\frac{1}{2}\\big(h_{e_j} + \\frac{1}{\\sqrt{|N_{e_j}|\\mid N_{S_i}|}} \\Sigma_{e_j\\in N_{E_i}, s_i\\in N_{S_i}} h_{s_i}\\big) \\odot Cone_{e_j},\n\\end{aligned}$\n(8)\nwhere $Cone_{e_j} \\in \\mathbb{R}^{1\\times d}$ denotes the average related concepts' information of the j-th exercise, $RCe_j$ represents the set of related concepts for the j-th exercise. $N_{s_i}$ denotes the neighbors of the i-th student in $G_{se}, Ne_j$ denotes the neighbors of the j-th exercise in $G_{se}$. In the end, akin to NCDM, we employ MLPs for $F(\\cdot)$ and use the ReLU to ensure non-negative weights, thereby fulfilling the monotonicity assumption. The prediction is calculated as Eq (7)."}, {"title": "Model Training", "content": "In the CD task, the main loss function utilized is the binary cross-entropy loss. This computes the discrepancy between the model's predicted outcomes and the actual response scores within a mini-batch. Additionally, we employ regularization term $\\Omega(\\cdot)$ to mitigate overfitting. The whole loss function can be formulated as\n$\\begin{aligned}\nL_{BCE} &= - \\Sigma_{i,j,r_{ij} \\in T} r_{ij} \\text{log } \\hat{y}_{ij} + (1 - r_{ij}) \\text{log}(1 - \\hat{y}_{ij}), \\\\\nH^{(0)} &= H_s \\oplus H_r \\oplus H_w \\oplus H_c, L = L_{BCE} + \\lambda_{reg}\\Omega(\\text{H}^{(0)}),\n\\end{aligned}$\n(9)\nwhere $\\lambda_{reg}$ is a hyperparameter to control the weight of regularzation loss in $L$. $\\text{H}^{(0)} \\in \\mathbb{R}^{(N^O + 2M + Z) \\times d}$ $\\oplus$ is a concatenation operator. The code is supplied in the supplementary material. $\\Omega(\\text{H}^{(0)}) = \\frac{1}{N^O+M}||\\text{H}^{(0)}||_{F}^{2}, where ||$\\text{H}^{(0)}$||_{F}^{2} = \\Sigma_u\\Sigma_v |\\text{H}^{(0)}|_{u,v}^2$ is an entry-wise matrix norm. We analyze the time complexity of ICDM, and the details are presented in Appendix A."}, {"title": "Experiments", "content": "In this section, we first delineate four real-world datasets and evaluation metrics. Then through comprehensive experiments, we aim to manifest the preeminence of ICDM in both transductive and inductive scenarios. To ensure reproducibility and robustness, all experiments are conducted ten times. Our code is available at https://github.com/lswhim/ICDM."}, {"title": "Experimental Settings", "content": "Datasets Description. The experiments are conducted using four real-world datasets: FrcSub, EdNet-1, Assist17, and NeurIPS20. For more detailed statistics on these four datasets, please refer to Table 1. Notably, \u201cSparsity\" refers to the sparsity of the dataset, which is calculated as $\\frac{|T|}{|S|E}$. \u201cAverage Correct Rate\u201d represents the average score of students on exercises, and \u201cQ Density\u201d indicates the average number of knowledge concepts per exercise. For the source of these datasets, please refer to Appendix B.\nEvaluation Metrics. To assess the efficacy of ICDM, we utilize both score prediction and interpretability metrics. This approach offers a holistic evaluation from both the predictive accuracy and interpretability standpoints.\nScore Prediction Metrics: Evaluating the efficacy of CDMs poses difficulties owing to the absence of the true Mas. A prevalent workaround is to appraise these models based on their capability to predict students' scores on exercises in the test data. In line with prior CDM studies [Wang et al., 2020a], in the transductive scenario shown in Figure 4(a), we partition the data into training and test data and assess the model's performance on the test data using classification and regression metrics such as AUC, Accuracy (ACC), and RMSE. The test size is set to 0.2, following the previous researches [Wang et al., 2020a; Li et al., 2022]. Crucially, we build the SCG solely based on the training data. In the inductive setting depicted in Figure 4(b), we retain the test data intact and partition the training data by students at a ratio $p_n = 0.2$. In this"}, {"title": "Transductive Cognitive Diagnosis", "content": "Baselines and State-of-the-Art Methods. We compare ICDM with various leading baselines and state-of-the-arts methods, including DINA [De La Torre, 2009], MIRT [Sympson, 1978], NCDM [Wang et al., 2020a], RCD [Gao et al., 2021], KSCD [Ma et al., 2022] and KANCD [Wang et al., 2023], and adopt the hyperparameter settings and IFs as described in their original papers. For more details of these methods, please refer to Appendix B. To further demonstrate the versatility of ICDM in accommodating various IFs, we not only employ the proposed GLIF but also utilize the traditional MIRT and the classic deep-learning based IF (i.e., NCDM). These variations are denoted as ICDM-MIRT, ICDM-NCDM, and ICDM-GLIF, respectively."}, {"title": "Inductive Cognitive Diagnosis", "content": "Baselines and Compared Methods. We conduct a comparison of ICDM against other baselines and utilize the hyperparameter settings and IFs described in their respective original publications.\n\u2022 Random: It predicts students' scores based on a uniform distribution ranging from 0 to 1.\n\u2022 KANCD-Mean: We incorporate the postprocessing mean strategy from IMCGAE into KANCD, as the original KANCD was designed solely for the transductive scenario. It assigns the embedding of new students to the average of the old students.\n\u2022 KANCD-Closest: For each new student in $S^U$, we assign their embedding based on the most similar student in $S^O$, who is selected based on the similarity of response logs.\n\u2022 IMCGAE [Shen et al., 2021]: It utilizes a graph autoencoder combined with a postprocessing strategy tailored for inductive rating prediction. We select IMCGAE as the representative because INMO cannot be applied in CD, and the performance of IMCGAE in [Wu et al., 2022; Shen et al., 2021] is superior to previous methods like IGMC [Zhang and Chen, 2020], IDCF [Wu et al., 2021]. We modify it accordingly to suit the inductive CD task.\n\u2022 KANCD-Re: By integrating the train data of $S^U$ into the training phase, we retrain KANCD.\n\u2022 ICDM-Re: By incorporating the train data of $S^U$ into the training phase, we retrain ICDM."}, {"title": "Hyperparameters Analysis", "content": "The Effect of Different Aggregators. We conduct experiments to study the impact of different aggregators on ICDM-GLIF, as shown in Figure 9(a) within Appendix C. Using GATV2 [Brody et al., 2022] results in out of memory for the last three datasets, hence it is not displayed in the figure. The mean and pool aggregators surpass other attention-based aggregators (e.g., GAT [Velickovic et al., 2018], GATV2). This can be attributed to the fact that CD's features are solely based on IDs. Employing complex non-linear transformations like attention might not lead to performance improvement and could even result in a decline, as also mentioned in [Wu et al., 2019; He et al., 2020]. The mean operator is recommended as it exhibits stable and relatively good performance across all four datasets."}, {"title": "Conclusion", "content": "This paper proposes an inductive cognitive diagnosis model (ICDM) for fast new students' mastery levels inference in WOIESs. ICDM mainly focuses on handling inductive scenario in CD which can provide immediate feedback for new students. A construction-aggregation-generation-transformation process is introduced to extract features effectively from the newly proposed student-centered graph. ICDM still has some limitations. It cannot infer the difficulty or discrimination of new incoming exercises. We look forward to finding a solution that can address this issue reasonably and effectively which is also of vital importance to WOIDSs."}, {"title": "A Time Complexity Analysis", "content": "In this section, we present a detailed time complexity analysis of our proposed model ICDM. We compare our time complexity with that of RCD, as RCD is the only CDM based on Graph Neural Networks.\nTime Complexity Analysis of ICDM. In ICDM, we construct a student-centered graph (SCG) G with four node and edge types based on $R^O, I^O$ and $Q$. We choose the mean operator as the aggregator in the CAGT process for illustration purposes. Given that we do not employ the non-linear activation and feature transformation usually found in GNNs, the time complexity can be straightforwardly computed as $O(2(||Q_B||_0 + ||R_B||_0 + ||I_B||_0)kd + 5d^2 + 3Zd)$ for the CAGT process, where B denotes the pre-defined batch size and $Q_B$ refers to the entries in Q that are related to the students, exercises, and concepts present in that batch. Similarly, $R_B$ and $I_B$ follow the same logic. $||Q_B||_0, ||R_B||_0, ||I_B||_0$ represents non-zero number of $Q_B, R_B, I_B$ respectively. k represents the k-hops. d stands for the size of the embeddings, Z denotes the number of knowledge concepts. The most time-consuming part is the aggregation step which takes $O(2(||Q_B||_0 + ||R_B||_0 + ||I_B||_0)kd)$.L denotes the number of GAT layers.\nTime Complexity Analysis of RCD. In RCD, an exercise-concept graph is constructed using Q and a student-exercise graph is formed using R. Given that RCD employs the graph attention network, which necessitates the computation of attention coefficients between every pair of connected nodes, its time complexity belongs to $O(2(||R|| + ||Q||)LZ^2)$. Herein, Z represents the number of concepts (d \u226a Z).\nICDM evidently takes less time compared to RCD due to two main reasons. First, in each batch during training, we only need to extract the relevant part from the constructed graph to perform graph convolution, while RCD needs to perform graph convolution on the entire graph. Second, in the CAGT process, the transformation phase reduces the embedding dimension to d, where d is much smaller than Z."}, {"title": "B Experimental Details", "content": "Datasets Source. FrcSub [DeCarlo, 2011; Tatsuoka, 1984] consists of middle school students' scores on fraction subtraction objective problems. EdNet-1 [Choi et al., 2020] is the dataset of all student-system interactions collected over 2 years by Santa, a multi-platform AI web-based tutoring service with more than 780K users in Korea. The Assist17 datasets is provided by the ASSISTment web-based online tutoring platforms [Feng et al., 2009] and are widely used for cognitive diagnosis tasks [Wang et al., 2020a]. The NeurIPS20 dataset is derived from a competition called The NeurIPS 2020 Education Challenge [Wang et al., 2020b]. It contains students' response logs to mathematics questions over two school years (2018-2020) from Eedi, a leading educational platform which millions of students interact with daily around the globe.\nImplementation Details This section delineates the detailed settings when comparing our method with the baselines and state-of-the-art methods in both transductive scenario and inductive scenario. All experiments are run on a Linux server with two 3.00GHz Intel Xeon Gold 6354 CPUs and one RTX3090 GPU. All the models are implemented by PyTorch [Paszke et al., 2019]. We employ cross-validation and grid-search techniques to select hyperparameters. Initially, we establish the range for each hyperparameter, with each combination of parameters representing a unique model, referred to as a grid. Then, we split the dataset, excluding the test data, into training and validation subsets. Finally, each model is evaluated by cross-validation, and the model that performs the best is selected.\nTransductive Scenario. In the following section, we will elaborate on some details regarding the utilization of compared methods.\n\u2022 DINA [De La Torre, 2009] is a representative CDM which models the mastery pattern with discrete variables (0 or 1).\n\u2022 MIRT [Sympson, 1978] is a representative model of latent factor CDMs, which uses multidimensional \u03b8 to model the latent abilities. We set the latent dimension as 16 which is the same as [Wang et al., 2020a]\n\u2022 NCDM [Wang et al., 2020a] is a deep learning based CDM which uses MLPs to replace the traditional interaction function (i.e., logistic function). We adopt the default parameters which are reported in that paper.\n\u2022 RCD [Gao et al., 2021] leverages GNN to explore the relations among students, exercises and knowledge concepts. Here, to ensure a fair comparison, we solely utilize the student-exercise-concept component of RCD, excluding the dependency on knowledge concepts.\n\u2022 KSCD [Ma et al., 2022] also explores the implicit association among knowledge concepts and leverages a knowledge-enhanced interaction function. Due to the absence of open-source code online, we have independently replicated KSCD.\n\u2022 KANCD [Wang et al., 2023] improves NCDM by exploring the implicit association"}]}