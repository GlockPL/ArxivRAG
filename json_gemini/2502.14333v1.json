{"title": "A Survey on Feedback-based Multi-step Reasoning for Large Language Models on Mathematics", "authors": ["Ting-Ruen Wei", "Haowei Liu", "Xuyang Wu", "Yi Fang"], "abstract": "Recent progress in large language models (LLM) found chain-of-thought prompting strategies to improve the reasoning ability of LLMs by encouraging problem solving through multiple steps. Therefore, subsequent research aimed to integrate the multi-step reasoning process into the LLM itself through process rewards as feedback and achieved improvements over prompting strategies. Due to the cost of step-level annotation, some turn to outcome rewards as feedback. Aside from these training-based approaches, training-free techniques leverage frozen LLMs or external tools for feedback at each step to enhance the reasoning process. With the abundance of work in mathematics due to its logical nature, we present a survey of strategies utilizing feedback at the step and outcome levels to enhance multi-step math reasoning for LLMs. As multi-step reasoning emerges a crucial component in scaling LLMs, we hope to establish its foundation for easier understanding and empower further research.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have made significant progress and demonstrated improvements on many benchmarks (Hendrycks et al., 2021). Traditional methods to improve the model performance include increasing the size of the dataset and enhancing its quality to approximate the population distribution. However, humans usually do not require as many practice problems as a training set to be able to solve the same type of problems. Based on this motivation, researchers have turned to enhancing LLMs through multi-step reasoning. Instead of learning from a vast amount of training data through optimizing a training objective, reasoning decomposes a complex problem into multiple simpler problems and makes decisions based on the context and their patterns. On the other hand, chain-of-thought (CoT) (Wei et al., 2022) as a pioneering work, has found great success through prompting LLMs to output step-by-step responses, establishing the paradigm of multi-step reasoning (Chu et al., 2024) and opening new avenues for training LLMs to produce such structured outputs, which showed better performance than prompting alone (Uesato et al., 2022). This is empowered by step-level feedback (process rewards) and outcome-level feedback (outcome rewards) which can be seen as a special case of step-level feedback with no intermediate rewards. Step-level feedback encourages correct reasoning steps and sets the solution on the right track, and such feedback can be provided by training-based models, training-free LLM prompting, or external tools, as shown in Figure 1. Due to the logical nature of mathematics, lots of research apply reasoning on LLMs to solve math problems, creating the need for a survey. The specific focus on multi-step reasoning is due to the trend of solving problems by guiding LLMs through multiple steps with feedback, which is different from traditional methods (CoT and supervised fine-tuning (SFT)) where there is no step-level feedback. In this paper, we present a"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Problem Statement", "content": "Given a question Q, an LLM generates the mth solution consisting of many reasoning steps denoted as $r^m_1, r^m_2,..., r^m_n$ and a final answer $a^m$, resulting in a set of solution paths {$r^m_1, ..., r^m_n, a^m$}$_{m=1}^M$ where n and M are the total number of steps and solutions. Each final answer $a^m$ has a correctness label $l_m$, and the final prediction a is selected from the set of final answer(s) {$a^m$}$_{m=1}^M$ and compared against the correct final answer A for correctness which is the objective to maximize."}, {"title": "2.2 Preliminaries", "content": "To integrate multi-step reasoning into LLMs, Uesato et al. (2022) conducted SFT on a pre-trained LLM by treating the question as input tokens and the multi-step solution as the target tokens. They annotated each step whether the mth solution has been correct so far, resulting in step-label pairs {(rm, sm)}=1, where s is a binary label. Process reward models (PRMs) utilize this additional supervision for each reasoning step and the training combines with the language model objective. In Cobbe et al. (2021), a scalar head implemented as a bias parameter and a gain parameter operates on the logit of the special token for binary classification. To save annotation cost, all reasoning steps can instead be labeled as whether the final answer is correct, resulting in outcome reward models (ORMs). Figure 2 illustrates the difference between process and outcome rewards with an example. Though Uesato et al. (2022) found comparable performance between PRMs and ORMs, Lightman et al. (2023) conducted a similar experiment and found PRMs to outperform ORMs by a large margin, attributing the success to the larger size of the dataset.\nWhile the LLM generation process typically follows greedy decoding to output tokens with the highest probability, Self-consistency (Wang et al., 2022) found success by sampling 40 solutions {$a^m$}$_{40=1}$ and selecting the most consistent answer as the final answer. Consistency can be measured through majority voting or weighted voting. Majority voting selects the most common answer:\n$a = \\arg \\max_a \\sum_{i=1}^m 1(a_i = a)$ (1)\nwhile weighted voting selects the final answer with the largest sum of weights:\n$a = \\arg \\max_a \\sum_{i=1}^m w_i 1(a_i = a)$  (2)\nwhere $w_i$ is the weight for $a_i$. Alternatively, best-of-N (Cobbe et al., 2021; Lightman et al., 2023) selects the solution with the highest weight:\n$a = \\arg \\max_{1<i<m} Wi$ (3)\nThese strategies can directly follow a trained ORM by setting $w_i$ as the model's scalar output in probability. For PRMs, the model predicts a probability score for each reasoning step, and these scores are"}, {"title": "3 Training-based Strategies", "content": "Before training on binary stepwise labels,\nMATH-Minos (Gao et al., 2024a) fine-tunes the\nLLM on stepwise natural language feedback that\nexplains how each step is correct or incorrect. In-\nstead of SFT, VerifierQ (Qi et al., 2024) trains the\nPRM through Q-learning. To avoid step-level an-\nnotation costs, Yuan et al. (2024) trains an implicit\nPRM with labels at the outcome level by param-\neterizing the reward as a ratio of log-likelihoods\nbetween the policy and the reference models."}, {"title": "3.1 Step-level Feedback", "content": "On top of SFT, some models enhance the reasoning process by training on stepwise labels to evaluate the quality of a reasoning step. Such trained models utilize the label predictions as scores to guide or select the best solution path. Specifically, some aggregate stepwise scores to represent the entire solution in a voting scheme, while some others utilize stepwise scores to guide tree search, and others refine or prompt refinement from LLMs."}, {"title": "3.1.1 Aggregation", "content": "It is common to jointly train the language model with a binary cross-entropy term on the correctness of intermediate steps. The aggregation of stepwise scores often follow majority voting, weighted voting, or best-of-N to select the final solution. The key distinction between the papers lies in their ways of defining, annotating, or training with stepwise labels. Based on the motivation that the reasoning steps are not equally wrong, DiVeRSe (Li et al., 2022) labels steps that do not occur in paths that lead to the correct final answer as incorrect. Math-Shepherd (Wang et al., 2024b) defines stepwise labels as the potential to arrive at the correct final answer and automates the labeling process through soft and hard estimation. For each step, an LLM generates multiple solutions, and soft estimation computes the percentage of paths reaching the correct final answer (also in MiPS (Wang et al., 2024d)), while hard estimation determines whether the correct answer is achieved at all. Similarly in Self-Explore (Hwang et al., 2024), for a solution path leading to an incorrect final answer, multiple solution paths are generated from each step. The first step that does not have one of its newly generated paths reaching the correct answer is labeled as the first error, forming stepwise labels. ER-PRM (Zhang et al., 2024b) proposes entropy-regularized PRMs to balance reward optimization with policy stability, preventing drastic changes from the original policy. In contrast to classification-based PRMS that assess each step independently, PQM (Li and Li, 2024) captures dependencies among reasoning steps by optimizing their ranking using Plackett-Luce loss (Plackett, 1975; Luce, 1959). This enables more precise stepwise reward distribution"}, {"title": "3.1.2 Search", "content": "Instead of generating multiple solution paths and selecting the final one through aggregation and voting, stepwise scores can also directly guide the generation of one solution path through search. DBS (Zhu et al., 2024) implements beam search on reasoning steps using stepwise scores based on their logical consistency with the previous steps. The PRM is specifically trained on margin ranking loss (Shashua and Levin, 2002) with synthetic data that simulate typical errors. TVM (Lee et al., 2024) trains a PRM with token-level values instead of step-level values to guide beam search"}, {"title": "3.1.3 Refinement", "content": "Besides serving as the score for aggregation or the heuristic for search, stepwise feedback can also directly train LLMs or prompt them for refinement. Jiao et al. (2024) annotates reasoning steps with a trained PRM and trains the language model on these labels through DPO (Rafailov et al., 2024b). Step-DPO (Lai et al., 2024) extends DPO by optimizing intermediate reasoning steps alongside final outputs, improving multi-step reasoning. Similarly, DAPO (Liu et al., 2024) applies preference learning at the step level instead of the outcome level, encouraging the language model to generate high-quality steps. BackMATH-LLM (Zhang and Xiong, 2025) utilizes a PRM and another PRM for backward reasoning to refine the model through PPO (Schulman et al., 2017). Gao et al. (2024c) stabilizes reinforcement learning by setting an upper bound on the reward to reduce incorrect steps and subtracting the rewards between neighboring steps to discourage repeating similar steps. On the other hand, ReST-MCTS* (Zhang et al., 2024a) integrates a PRM with MCTS to collect reasoning steps and their labels to enhance self-training of the language model as the policy model. Positive steps are used to train the language model, while all labels are used to train the PRM. LeMa (An et al., 2023) fine-tunes LLMs on correction data obtained by prompting the GPT-4 model to correct false solution paths, with difficult questions sampled to expand the original set of questions. SORM (Havrilla et al., 2024) designs stepwise outcome rewards as the probability of the reaching the correct final answer given an intermediate reasoning step. The pipeline incorporates additional models for global and local refinement, drafting an entirely different solution and modifying local parts of the solution.\nThe stepwise refinement can also be directly prompting an LLM. StepCo (Wu et al., 2024) generates one solution path and utilizes a trained PRM to rate each reasoning step. Steps lower than a threshold are considered erroneous and revised by the LLM until all steps exceed the threshold. Similarly, LM2 (Juneja et al., 2024) decomposes a problem into sub-questions and sequentially verify the answer to each sub-question, and an incorrect sub-answer prompts the re-generation of the sub-question. The verifier model is trained to identify nine types of incorrectness, facilitating stepwise re- finement. On the other hand, REFINER (Paul et al., 2024) trains a generator model to generate solution paths and a critic model to provide feedback to one of the generated paths in an iterative fashion."}, {"title": "3.2 Outcome-level Feedback", "content": "Instead of step-level feedback, some papers turn to utilize outcome-level feedback, potentially avoiding a higher cost at the expense of more fine-grained feedback. Outcome-level feedback can be obtained by discriminative or generative ORMS or determined by rules."}, {"title": "3.2.1 Discriminative ORM", "content": "As the pioneering work, Cobbe et al. (2021) trains an ORM to select the best generated solution. A solution generator is first trained to output solutions given a question, and multiple solutions are generated, along with their binary correctness labels, to train the ORM to output the correctness probability of a given solution. For inference, a large number of diverse solutions are generated, evaluated by the trained ORM, and ranked to select the best solution. Many papers follow the same framework with various enhancements. REPS (Kawabata and Sugawara, 2024) treats only the best solution path as positive with all others as negative in training the ORM. Pairs of solution paths are iteratively compared to yield the best one for each problem. V-STaR (Hosseini et al., 2024) utilizes the correct and incorrect solutions during each training iteration in STaR (Zelikman et al., 2022) to simultaneously train the ORM, taking advantage of the solution generator that is improving throughout the iterations, instead of a fixed generator. Similarly, TS-LLM (Feng et al., 2023) proposes a framework to iteratively guide inference with tree search and train the policy model and ORM with the collected trajectories. On the other hand, OVM (Yu et al., 2024) utilizes a trained ORM to guide beam search in inference by evaluating at the step level and leveraging outcome rewards as process rewards. GraphReason (Cao, 2023) utilizes an additional graph to aggregate the solution paths with the same final answer to train an answer classifier. At inference, multiple solution paths are sampled and processed in the same way, and the outcome with the highest predicted level of correctness is selected as the final answer. Moreover, TinyGSM (Liu et al., 2023a) finds an ORM to improve the performance of a small language model. With a comprehensive analysis, Brown et al. (2024) observes that increasing the number of samples generated boosts the chance of generating the correct solution. However, the chance of identifying that correct solution remains relatively unchanged despite the increase in samples."}, {"title": "3.2.2 Generative ORM", "content": "ORMS can also be trained through a generative objective, as GenRM (Zhang et al., 2024c) trains the verifier model by predicting the next token. Given the problem and a potential solution, the model answers the question: \"Is the answer correct (Yes/No)?\" The token probability of \u201cYes\u201d and \"No\" are extracted for ranking at test time. To enhance with CoT, GenRM-CoT adds the verification steps before answering \u201cYes\u201d or \u201cNo\u201d. At test time, multiple verification rationales are generated, and majority voting is applied to their token probabilities of \u201cYes\u201d. To gather training data, instead of prompting the LLM to verify step-by-step, reference-guided grading (Zheng et al., 2023) guides the step-by-step verification by providing a reference solution that arrives at the correct final answer. Similarly, STILL-1 (Jiang et al., 2024) trains a generative reward model and utilizes it to evaluate rollouts in MCTS at test time to obtain step-level values which are combined with Self-consistency (Wang et al., 2022) estimates."}, {"title": "3.2.3 Rule-based Rewards", "content": "Instead of training a neural network for an ORM, DeepSeekMath (Shao et al., 2024) and DeepSeek-R1 (Guo et al., 2025) use rules to obtain feedback at the outcome level and GRPO (Shao et al., 2024) to improve the language model with reinforcement learning. A group of solutions are generated and evaluated for correctness to obtain feedback which is further normalized within the group. This reduces the computational resources during training compared to PPO (Schulman et al., 2017) and neural ORMs."}, {"title": "3.3 Step- and Outcome-level Feedback", "content": "Instead of using only step- or outcome-level feedback, some utilize both to enhance reasoning. CoRe (Zhu et al., 2023) trains a PRM and an ORM to label reasoning paths in MCTS and adds high-quality paths back to the dataset for training the reward models. The iterative process assist MCTS to better evaluate each step and obtain the final answer. On the other hand, Setlur et al. (2024) measures the advantage of each step defined as the gain in likelihood of reaching the correct answer by taking that step, and combines with an ORM. Termed process advantage verifiers, the model achieves better reasoning performance and higher efficiency than PRMs and ORMs."}, {"title": "4 Training-free Approaches", "content": "Instead of training PRMs, ORMs, or fine-tuning the policy model, some papers utilize training-free approaches through frozen LLMs or external tools to obtain feedback in navigating multi-step reasoning."}, {"title": "4.1 Evaluate by LLM Response", "content": "At each step, the solution path can be evaluated by directing prompting the LLM. Self-evaluation (Kadavath et al., 2022) turns an initial response of the LLM into a true-or-false question and prompts the same LLM for confirmation, and Xie et al. (2024) utilizes this result to guide stochastic beam search to find the final solution path. Natural Program (Ling et al., 2024) verifies each reasoning step against the premises from the question. Multiple solution paths are sampled for the verification process and only those that have all the steps verified are applied majority voting to obtain the final answer. LoT (Zhao et al., 2024a) generates a solution path and verifies each step for revision. The verification prompts two explanations to why the step is correct and why it is incorrect, and the step is revised if the latter is preferred over the former. Similarly, SelfCheck (Miao et al., 2023) prompts the LLM to verify its own steps by re-generating each step and comparing against it. The results serve as the weight of the solution path in majority voting. On the other hand, CoT Rerailer (Wan et al., 2024) generates multiple solution paths and selects the least hallucinated one for an LLM agent to verify each step. A debate is initiated between multiple LLMs, and a new solution path can be re-generated. SSC-CoT (Zhao et al., 2024b) also generates multiple solutions and ensures their logical consistency by finding the overlapping steps with an LLM and querying a knowledge graph, increasing the likelihood of accurate and coherent solutions."}, {"title": "4.2 Evaluate by LLM Logits", "content": "Besides using the LLM's response, the token logits output by LLMs provide another way to evaluate each reasoning step. RAP (Hao et al., 2023) utilizes the token probability of \u201cYes\u201d in self-evaluation along with its confidence as the heuristic to find the solution through MCTS. UAG (Yin et al., 2024) detects uncertainty spikes in the solution path at the token level using LLM logits and adds relevant exemplars to the beginning of the prompt to leverage their insights. LeCo (Yao et al., 2024)"}, {"title": "4.3 Evaluate by External Tools", "content": "Besides using an LLM, external tools can also evaluate the steps. MathDivide (Srivastava and Gandhi, 2024) decomposes a problem into a series of sub-problems and converts them into Python programs for evaluating the input values. The outcome of a sub-problem serves as the input to the next sub-problem to obtain the final answer. On the other hand, DTV (Zhou et al., 2024) utilizes Isabelle (Nipkow et al., 2002) as an automated theorem prover to verify solutions. An informal solution is generated and converted to a formal solution for verification. Multiple solution paths are sampled for verification, and solutions with all reasoning steps verified participate in majority voting to select the final answer. It defaults to Self-consistency (Wang et al., 2022) if all solutions failed verification."}, {"title": "5 Datasets", "content": "While LLMs perform well on datasets with K-12 difficulty, we present more challenging datasets at the college and competition levels, which tend to be more recent with a lower risk of data contamination. As we summarize them in Table 1, additional details can be found in Appendix B.2, and K-12"}, {"title": "6 Challenges and Future Directions", "content": "Although multi-step reasoning improves LLMs in solving math problems, it also leads to longer output lengths, which is rather inefficient for easier math problems that can be answered in a short response. To optimize efficiency, an LLM should reason to solve difficult problems while providing direct answers to easier ones, though determining this distinction can be a challenge. Previous datasets typically focus on elementary problems involving basic math operations. As LLMs continue to excel on these benchmarks, it becomes crucial to evaluate them on more complex problems to provide a more comprehensive assessment of their mathematical abilities. Recent datasets address this gap by introducing problems at the college and competition levels (Section B.2), presenting a more substantial challenge for performance. The same goes for multilingualism. While previous datasets are mostly in English and Chinese, LLMs should generalize across languages. As most papers have yet experimented with complex and multilingual problems, it is important to start incorporating those problems into the benchmark, paving the way to more capable LLMs.\nTo further improve PRMs and ORMs in multi-step reasoning, it is essential to study the presence of reward hacking (Amodei et al., 2016), a phenomenon where the model learns unintended behavior that results in high rewards, such as repeating or reiterating a previous correct step. Such behavior does not make one step closer to the correct final answer, but it can be assigned a high reward due to its correctness to the original question. Another challenge is the inverse scaling law (Gao et al., 2023a; Zeng et al., 2024) where the policy model learns from feedback from the reward model and deviates away from the original distribution over the training iterations, yet the feedback is provided by the reward model trained on the original distribution, resulting in a distribution shift. This hinders the reward model from providing useful feedback as training prolongs, but the issue can potentially be mitigated by adapting the reward model correspondingly.\nMath problems have attracted most of the research efforts in reasoning due to the logical nature of mathematics. The inherent logical property applies to code (Ding et al., 2024) and plan (Valmeekam et al., 2023) generation problems, but as multi-step reasoning shows promise in these fields, it has the potential to expand to other domains. In fact, the technique has started to appear in many domains, including image caption-ing (Wang et al., 2024c), question answering (Xue et al., 2024), and cross-lingual tasks (Ranaldi et al., 2024). At the highest level, large world models (Xiang et al., 2024) can provide multi-faceted feedback from the entire environment to facilitate multi-step reasoning for various tasks beyond mathematical reasoning."}, {"title": "7 Conclusion", "content": "This survey presents the various usage of feedback to facilitate multi-step reasoning for LLMs to solve math problems, including step-level and outcome-level feedback. We propose a taxonomy to categorize training-free and training-based approaches and further discuss their subcategories in detail. We present recent datasets with challenging problems to encourage more robust benchmarking, and we discuss other challenges and future directions going forward. In this rapidly evolving field, we aim to establish a solid foundation for readers and make research more accessible and easier to navigate."}, {"title": "Limitations", "content": "While we make our best efforts to present a comprehensive survey, it is possible that we may have missed some references. Due to the page limit, we choose to describe each work in more relevant parts, so our content can omit further details. For the datasets, we focus on the recent challenging datasets and present other datasets in the Appendix."}, {"title": "A Survey Methodology", "content": "To systematically obtain relevant references, we first query keywords in Google Scholar, including process rewards, outcome rewards, verifier models, multi-step reasoning, and LLMs. We extend the coverage by adding additional references mentioned in their related work sections. To ensure relevance, we manually examine the methodology of each reference."}, {"title": "B Datasets", "content": ""}, {"title": "B.1 K-12 Difficulty", "content": "This section presents math datasets with K-12 difficulty.\nAddSub (Hosseini et al., 2014) collects 395 math problems in 1.5K sentences, involving only addition and subtraction at the elementary level.\nASDiv (Miao et al., 2021) contains 2.3K diverse math problems on basic arithmetic and aggregative operations at the elementary level.\nGSM8K (Cobbe et al., 2021) includes 8.5K grade-school math problems on basic arithmetic"}, {"title": "B.2 College Difficulty and Beyond", "content": "This section presents math datasets at the college and competition levels.\nAIME\u00b9 presents 90 problems from the American Invitational Mathematics Examination across three years. The exam helps select participants for the international Olympiad.\nAlphaGeometry (Trinh et al., 2024) collects 30 Olympiad-level questions on geometry.\nAQUA-RAT (Ling et al., 2017) involves 100K algebraic problems, each with five choices and a multi-step rationale. The problems are inspired and modified from graduate admission tests.\nCollegeMath (Tang et al., 2024) contains college-level questions on linear algebra and differential equations, with 1.2K and 2.8K samples in the training and test sets.\nFrontierMath (Glazer et al., 2024) consists of hundreds of extremely difficult problems crafted by experts, with the best current model reaching a low accuracy.\nGaoKao2023 (Liao et al., 2024) includes 385 challenging problems from the Chinese college entrance exam in 2023, translated into English.\nHumanityLastExam (Phan et al., 2025) presents a challenging benchmark of manually-reviewed questions from various subjects and contains 1.2K expert-level math problems.\nMathOdyssey (Fang et al., 2024) collects 101 and 148 college-level and Olympiad-level problems across 12 domains.\nMATH (Hendrycks et al., 2021) contains 12.5K challenging math problems at the competition level. Each problem has a step-by-step solution, which facilitates model training on step-level annotation."}]}