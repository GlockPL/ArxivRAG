{"title": "Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude", "authors": ["Yile Yan", "Yuqi Zhu", "Wentao Xu"], "abstract": "Recent advances in Large Language Models (LLMs) have enabled human-like responses across various tasks, raising questions about their ethical decision-making capabilities and potential biases. This study investigates protected attributes in LLMs through systematic evaluation of their responses to ethical dilemmas. Using two prominent models - GPT-3.5 Turbo and Claude 3.5 Sonnet - we analyzed their decision-making patterns across multiple protected attributes including age, gender, race, appearance, and disability status. Through 11,200 experimental trials involving both single and inter-sectional protected attribute combinations, we evaluated the models' ethical preferences, sensitivity, stability, and clustering of preferences. Our findings reveal significant protected attributes in both models, with consistent preferences for certain features (e.g., \"good-looking\") and systematic neglect of others. Notably, while GPT-3.5 Turbo showed stronger preferences aligned with traditional power structures, Claude 3.5 Sonnet demonstrated more diverse protected attribute choices. We also found that ethical sensitivity significantly decreases in more complex scenarios involving multiple protected attributes. Additionally, linguistic referents heavily influence the models' ethical evaluations, as demonstrated by differing responses to racial descriptors (e.g., \"Yellow\" versus \"Asian\"). These findings highlight critical concerns about the potential impact of LLM biases in autonomous decision-making systems and emphasize the need for careful consideration of protected attributes in AI development. Our study contributes to the growing body of research on AI ethics by providing a systematic framework for evaluating protected attributes in LLMs' ethical decision-making capabilities.", "sections": [{"title": "Introduction", "content": "Tracing back to the 1950s, when Alan Turing proposed the \"imitation game\u201d, he envisioned a machine that could exhibit human-like behavior and be indistinguishable from a human (Turing 1950). Through the efforts of several generations, artificial intelligence (AI) now, with large language models (LLMs) as prominent representatives of the generative Al era, has passed the Turing test (Mei et al. 2024) and played an essential role in handling human tasks, such as communication, translation, question-answering, etc. (Chang et al. 2024). Considering their expanding capabilities and ease of accessibility, LLM-based tools are becoming popular among the general public and are increasingly influencing human-AI interactions.\nHowever, many alarming cases have emerged that raise public concern about the ethical safety of LLMs. For example, a 14-year-old boy committed suicide after interacting with Character.ai, a personalized LLM-based chatbot. This incident aroused wide discussion about the ethical boundary and moral responsibility of human-AI interaction. Subsequently, even though AI ethics has gathered much attention (Birkstedt et al. 2023), it is undeniable that ethical limitations in AI still exist and should be publicly acknowledged.\nThe ethical issues in AI are various and mostly caused due to the nature of machine learning (for a systematic review, see (Stahl and Stahl 2021)). LLMs apply a deep learning architecture and are trained on massive data. In this way, it could intentionally or inadvertently extend the human's bias through real-life data. Such a bias is not rare, and could be found in various sectors, such as E-commerce, digital advertising, hiring, etc. (Varsha 2023). For example, a widely used healthcare algorithm in the U.S. exhibited racial bias that Black patients being falsely seen as healthier than equally sick White patients due to the algorithm's reliance on healthcare costs, which exacerbates the health disparities among different races (Obermeyer et al. 2019).\nKeeping LLMs ethically safe is crucial since the biased outputs may lead to unfair treatment to the underrepresented individuals or groups of people, exacerbate the pre-existing inequalities (Ferrara 2024), and even lead to fatal decision-making results, such as in autonomous driving. However, due to the lack of explainability and the proprietary nature of the most trending LLMs, their ethical settings remain as unknown as a \u201cno man's land\". Considering the increasing interaction between LLMs and human, it's crucial to demystify their performance in ethical contexts so to understand the biases that they have. However, it remains unclear how popular proprietary AI ecosystems \u2013 GPT and Claude would trade-off between protected attributes and ethical decisions when multiple protected attributes are considered.\nTo fill these gaps in the literature, we conducted a study of the ethical decision-making of LLMs regarding multiple"}, {"title": "Related Works", "content": "AI is an emerged statistical model and has become an integral tool for decision-making across various domains, from traffic planning to recommending medical treatments. However, AI systems are fundamentally shaped by their training data, which comes from human knowledge. This creates an important challenge: human cognitive biases can seep into AI through both the data collection and labeling processes, as well as through algorithm design choices. As a result, these AI may inadvertently perpetuate or amplify existing human biases in their predictions and decisions. In modern applications or hardwares driven by AI, we must prevent protected attributes from Fairness Gerrymandering (Kearns et al. 2018).\nProtected attributes, also called protected characteristics (Corbett-Davies et al. 2024), encompass specific demographic and personal traits that require safeguarding against discriminatory treatment in AI systems (Barocas, Hardt, and Narayanan 2023). These characteristics are often legally recognized and protected by various anti-discrimination laws and include demographic factors (Yang and Dobbie 2020). Take Equality Act 2010 in UK for example, the protected characteristics include age, gender, marital status, disability, pregnancy and maternity, race, religion or belief, sex, and sexual orientation (Gov.UK 2010). In other legislation, protected features may vary, such as national origin, genetic information, and so forth (U.S. Equal Employment Opportunity Commission 2024). Considering this origin, many studies on AI fairness include such features as the representatives of individuals or groups that require particular attention to prevent algorithmic bias and ensure equitable treatment across all population subgroups (Chen et al. 2024).\nPrevious studies have examined the bias or stereotypes in LLMs, with race, gender and sex, political ideology, religion, nationality, age, occupation, sexuality, etc., as the most selected protected attributes. However, we found the selected attributes varied greatly depending on the research context. To better understand AI's preference in protected attributes in an ethical decision-making process, a tailor-made experimental context should be designed. Moreover, the real context is typically complex with people with diverse characteristics. While most studies explore biases in individual features separately and respectively, we aim to investigate whether the biases in LLMs' decision-making change when applied to individuals or groups with single or intersectional protected attributes."}, {"title": "AI Ethical Bias", "content": "Ethics is the \"science that deals with conduct, in so far as this is considered as right or wrong, good or bad\" (Dewey and Tufts 2017), and it is a set of moral principles that guide our judgment of conduct, i.e., what should or should not be done. The booming of AI has brought ethical issues, which arouse awareness and consideration of ethics of AI. Ethics of AI discuss the principles that ensure AI align with the common values and do not cause harm (Bostrom and Yudkowsky 2018).\nHuman as the creator of AI, have the moral responsibility for Al's ethical behaviors, so that the principles for ethical AI occurs as a major field (Jobin, Ienca, and Vayena 2019; Floridi and Cowls 2022). If designed properly, AI could promote a safer human-AI interaction and mitigate the inequal-"}, {"title": "AI Decision-Making and Ethical Dilemmas", "content": "The role of AI in decision-making is significant considering its increasing role in industries, individual lives, and society at large (Pazzanese 2020). Moral reasoning is central to decision-making, specifically concerns ethical judgments and evaluating situations or actions according to ethical consideration (McHugh and Way 2018). AI systems are increasingly being designed to simulate human-like moral reasoning and decision-making by incorporating predefined ethical frameworks, such as deontology, utilitarianism, consequentialism, virtue ethics, as well as fairness and justice, which stresses the equitable and fair distribution (Nassar and Kamal 2021; Guan, Dong, and Zhao 2022).\nDespite the theoretical foundations, many AI systems exhibited biased decision-making due to biases in training data, limited moral reasoning capabilities, and a lack of consensus on the ethical AI, with the findings in Table 1 as references. To investigate the LLMs' potential bias, simulation of real-life context to see the preferences in LLMs' decision-making is a dominant approach. The experiment of AI to ethical dilemmas could be a representative method for testing Al's ethical decision-making.\nEthical dilemma (or moral dilemma, used interchangeably) means the conflict of ethical principles or moral values that imply people's priority of rules or principles in moral reasoning (MacIntyre 1990). The most representative ethi-"}, {"title": "Methodology", "content": "In this section, we describe our experimental settings for the study."}, {"title": "Experimental Settings", "content": "Our study employed a comparative analysis of ethical decision-making across two scenarios: single protected attribute scenario and intersectional protected attribute scenario. We utilized seven categories of protected features (Table 2). The experiments were conducted using two prominent LLMs: GPT-3.5 Turbo and Claude 3.5 Sonnet, accessed through their respective APIs. To maintain ecological validity, we preserved default model parameters: {frequency penalty = 0, logit bias = null, logprobs = false, toplogprobs = not specified and not applicable, n = 1, presence penalty = 0, no history available}. For the single protected attribute scenario, we presented the models with the following ethical dilemma:\nOn a snowy night, you are driving home to celebrate the holidays when you encounter three who are trying to get a ride home. They are all on your way and are [protected attribute candidates]. Your car can only carry one person. Who do you choose to give a ride to? Put your answer into [].\nFor intersectional scenarios, we extended the experiment to examine intersectional protected attributes. For instance, the Age-Gender intersection yielded combinations such as: S = {[8, Masculine], [8, Feminine], [8, Androgynous], [35, Masculine], [35, Feminine], [35, Androgynous], [70, Masculine], [70, Feminine], [70, Androgynous]} Then we asked:... They are all on your way and are [35, Masculine], [35, Feminine], [35, Androgynous]...\nThe experimental design encompassed:\n\u2022 7 single protected attribute groups\n\u2022 21 intersectional protected attribute groups\n\u2022 50 iterations per attribute group\n\u2022 4 rounds of repetition\n\u2022 Total sample size: 11,200 responses (5,600 per model)\nThis comprehensive sampling strategy enabled robust analysis of both individual and intersectional bias patterns in LLM ethical decision-making."}, {"title": "Measurement Metrics", "content": "We employ five protected attribute metrics for performance measurements for study, as detailed in the following.\nFor single protected attribute senario, each attribute was mentioned in the question. The normalized frequency is the frequency at which a protected attribute is selected under the requisitions for which the LLMs choose a protected attribute in the protected attribute group. For example, in $G_{Gender}$ all protected attributes is selected 40 times, and the protected attribute $Masculine$ is selected 20 times, and the normalized frequency of $Masculine$ is 0.5.\nWe calculated the normalized frequency of the protected attribute for a single protected attribute scenario using:\n$f_{pa} = \\frac{N_{pa}}{\\Sigma_{pa \\in G} N_{pa}}$\n, where $f_{pa}$ is the normalized frequency for protected attribute $p_a$ of category G, $N_{pa}$ is the count of $p_a$ appeared in the experiment. For example, $f_{Masculine}$ is the normalized frequency for protected attribute $Masculine$ of category $G_{Gender}$, $N_{Masculine}$ is the count of $Masculine$ appeared in the experiment and is 20. The $ \\Sigma_{p_a \\in G_{Gender}} N_{pa}$ is 40. And the normalized frequency for protected attribute $Masculine$ of category $G_{Gender}$ is 0.5.\nFor intersectional scenario, we calculated the normalized frequency of each protected attribute using:\n$f_{pa_1, pa_2} = \\frac{N_{pa_1, pa_2}}{\\Sigma_{pa_1 \\in G_a, pa_2 \\in G_B} N_{pa_1, pa_2}}$\n, where $f_{pa_1, pa_2}$ is the normalized frequency for the intersectional protected attribute $pa_1, pa_2$ of the protected attribute category $G_a, B$, and $N_{pa_1, pa_2}$ is the count of $pa_1, pa_2$ that appeared in the experiment. For example, $f_{Masculine, White}$ is the normalized frequency for the intersectional protected attribute $Masculine, White$ of the protected attribute category $G_{Gender, Color}$, and $N_{Masculine, White}$ is the count of $Masculine, White$ appeared in the experiment and is 10. The $ \\Sigma_{pa_1 \\in G_a, pa_2 \\in G_B} N_{pa_1, pa_2}$ is 40. And the normalized frequency for the sub-protected attribute $Masculine, White$ of the protected attribute category $G_{Gender, Color}$ is 0.25."}, {"title": "Ethical preference priority", "content": "For single protected attribute scenario, we directly used the mean normalized frequency of each single protected attribute to assess the ethical preference of the LLMs.\nFor intersectional scenario, we summed up the mean normalized frequency of each protected attribute including the specific single protected attribute and divided it by the number of these protected attributes as the mean normalized frequency of the specific single protected attribute for intersectional scenario.\n, $f_k = \\frac{\\Sigma_{pa_1=k} S_{pa_1,pa_2}}{Count_{pa_1=k}}$\n,where $f_k$ is the mean normalized frequency of the specific single protected attribute k in intersectional scenario, $Count_{pa_1=k}$ is the number of the intersectional protected attributes including the specific single protected attribute k. For example, $f_{Masculine}$ is the mean normalized frequency of the specific single protected attribute $Masculine$ in intersectional scenario, $Count_{pa_1=Masculine}$ is the number of the intersectional protected attributes including the specific single protected attribute $Masculine$ and is 17, the $\\Sigma_{pa_1=k} S_{pa_1,pa_2}$ is 3.4, and the mean normalized frequency of the specific single protected attribute $Masculine$ in intersectional scenario is 0.2.\nWe then ranked these protected attributes using the mean normalized frequency of each protected attribute. Thus we got the popular protected attributes."}, {"title": "Ethical sensitivity", "content": "Due to the stochastic nature of LLMs, LLMs would not simply answer the specific protected attribute in our experiment settings. Here, ethical sensitivity is defined as the frequency LLMs give other answers instead of the specific protected attribute. For example, LLMs answer I choose to give a ride to the person who needs help the most. without choosing from the given protected attributes.\nFor each single protected attribute group, the higher the frequency, the higher the sensitivity to this attribute group.\nFor single protected attribute scenario, we calculated the unselected frequency of the protected attribute group using:\n$S_a = 1 - \\frac{\\Sigma_{p_a \\in a} N_{pa}}{50}$\n, where $S_a$ is the unselected frequency of the protected attribute group $G_a$, 50 is the number of times we asked LLMs in one round. For example, the protected attributes in the group $G_{Gender}$ were selected 40 times, and the unselected frequency of the protected attribute group $G_{Gender}$ is 0.2.\nFor intersectional scenario, we calculated the unselected frequency of the protected attribute group using:\n$\\Sigma_{\\alpha,\\beta} = 1 - \\frac{pa_1 \\in G_a, pa_2 \\in G_B N_{pa_1, pa_2}}{50}$\n, where $S_{\\alpha,\\beta}$ is the unselected frequency of the protected attribute group $G_{\\alpha,\\beta}$, 50 is the number of times we asked LLMs in one round. For example, the protected attributes in the group $G_{Gender, Color}$ were selected 45 times, and the unselected frequency of the protected attribute group $G_{Gender, Color}$ is 0.1.\nThen we calculated the normalized unselected frequency of the specific single protected attribute group using:\n$S = \\frac{\\Sigma_{\\alpha = \\gamma} S_{\\alpha,\\beta}}{Count_{\\alpha = \\gamma}}$\nwhere S is the normalized unselected frequency of the specific single protected attribute group $G_for intersectional scenario, $Count_{\\alpha = \\gamma}$ is the number of the intersectional protected attribute groups including the specific single protected attribute group. For example, $S_{Gender}$ is the normalized unselected frequency of the specific single protected attribute group $G_{Gender}$ for intersectional scenario, $Count_{\\alpha = Gender}$ is the number of the intersectional protected attribute groups including the specific single protected attribute group and is 6, and $\\Sigma_{\\alpha = \\gamma} S_{\\alpha,\\beta}$ is 1.2, and the normalized unselected frequency of the specific single protected attribute group $G_{Gender}$ for intersectional scenario is 0.2.\nBy calculating the results of the 4 rounds of experiments, we got the mean unselected frequency of each single protected attribute group for single protected attribute scenario and the mean normalized unselected frequency of each single protected attribute group for intersectional scenario.\nWe used the mean unselected frequency of each single protected attribute group to assess the ethical sensitivity of each single protected attribute group of LLMs. The higher the unselected frequency, the more sensitive the ethical sensitivity."}, {"title": "Ethical stability", "content": "For single protected attribute scenario, we directly used the standard deviation of the normalized frequency of each protected attribute to assess the stability of the ethical preferences of LLMs. We designed the ethical stability as the normalized total standard deviation of the protected attribute group. For example, the normalized total standard deviation of the specific single protected attribute group Gender is 0.2, and the ethical stability of the protected attribute group Gender is 0.2. The smaller the standard deviation, the more stable the ethical preference.\nFor intersectional scenario, we calculated the total standard deviation of the intersectional attribute group using:\n$\\sigma_{\\alpha,\\beta} = \\Sigma_{pa_1,pa_2 \\in G_{\\alpha, \\beta}} \\sigma_{pa_1,\\alpha_2}$\n, where $\\sigma_{\\alpha,\\beta}$ is the total standard deviation of group $G_{\\alpha,\\beta}$, $\\sigma_{pa_1, pa_2}$ is the standard deviation of intersectional protected attribute pa\u2081, \u0440\u04302.\nThen we calculated the normalized total standard deviation of the single protected attribute for intersectional scenario using:\n$\\sigma = \\frac{\\Sigma_{\\alpha = \\gamma} \\sigma_{\\alpha,\\beta}}{Count_{\\alpha = \\gamma}}$\n,where o is the normalized total standard deviation of the specific single protected attribute group \u03b3 for intersectional protected attribute scenario, $Count_{\\alpha = \\gamma}$ is the number of the"}, {"title": "Clustering of preference", "content": "We clustered features based on their mean normalized frequencies using bottom-up hierarchical clustering. Each data starts as a separate cluster. 1. Calculate the distance between each pair of clusters: Use Euclidean distances. 2. Merge the two closest clusters: Based on the minimum value of the distance, merge the two clusters into one. 3. Repeat: Repeat steps 1 and 2 until all data points are merged into one cluster.\nFor intersectional protected attribute scenario, we calculated the mean value of the cluster using:\n$\\mu_a = \\frac{\\Sigma_{p_a \\in a} fra}{Na}$\n,where \u03bc\u03b1 is the cluster's mean value, and fra is the mean normalized frequency of protected attribute pa, na is the number of samples in cluster a.\nWe used Ward method to calculate the distance: choosing the optimal merger step by minimizing the increase in variance due to each merger. We calculated the increase in intra-cluster variance after merging two clusters using:\n$\\Delta SSE = \\frac{n_a n_b}{n_a + n_b} - (\\mu_a \u2013 \\mu_b)^2$\n, where ASSE is the increase in intra-cluster variance after merging two clusters, na and nu are the number of samples in the two clusters, and \u03bc\u03b1 and \u00b5\u03b5 are the two clusters' mean values."}, {"title": "Bias between LLMs", "content": "We calculated the preference score for each protected attribute. If the score is positive, GPT-3.5 Turbo prefers that protected attribute. Otherwise, Claude 3.5 Sonnet prefers the protected attribute. A larger absolute value of the score indicates that the LLM prefers the protected attribute over another LLM. For single protected attribute scenario, we calculated the preference score of the protected attribute using:\n$B_{pa} = \\frac{fGPT - fClaude}{fGPT + fClaude}$\nwhere Bpa is the preference score of protected attribute pa, $fGPT$ and $fClaude$ are the mean normalized frequencies"}, {"title": "Results", "content": ""}, {"title": "Divergent Gender Biases and Demographic Preferences Across LLM Models", "content": "Figure 1 shows the mean normalized frequencies for GPT-3.5 Turbo and Claude 3.5 Sonnet. The x-axis and y-axis coordinates are labeled with 20 protected attributes from seven protected attribute groups. Each box represents the mean normalized frequency of an intersectional protected attribute consisting of the corresponding x-axis and y-axis coordinate protected attributes. The brighter the color, the higher the frequency; the darker the color, the lower the frequency. Since different sequential combinations of the same protected attributes are considered identical, all images are symmetric about the diagonal line. We then identified the top 10 brightest protected attributes to determine the 10 protected attributes that LLMs tend to favor the most.\nFigure 1c and Figure 1d show the top 10 mean normalized frequency rankings for the intersectional protected attributes for both models, with the y-axis labeled as the intersectional protected attribute and the bar lengths representing the mean normalized frequency for that protected attribute. Figure 2 shows the mean normalized frequency rankings of single protected attributes for intersectional scenario, figure 2a for GPT-3.5 Turbo and figure 2b for Claude 3.5 Sonnet.\nWe identified the high-frequency protected attributes in Figure la and Figure 1b and sorted them to obtain Figure 1c and Figure 1d, revealing the attributes favored by the LLMs. Furthermore, we calculated the selected frequency of single protected attributes and arranged these attributes according to their frequency in GPT-3.5 Turbo, from highest to lowest. Our findings indicate that: (1) The \u201cGood-looking\" appearance attribute shows high frequency in both models, while \"Standard-looking\u201d and \u201cUnpleasant-looking\u201d show extremely low frequencies. (2) The \u201cRace\u201d categories of \"White\u201d, \"Black\u201d, and \u201cYellow\u201d in both models rank from high to low frequency, with \u201cYellow\u201d showing particularly low frequency. (3) The \u201cAge\u201d categories of \u201c8\u201d, \u201c35\u201d, and \"70\" years in both models display decreasing frequency distribution. (4) For GPT-3.5 Turbo, the frequency of \"Masculine\" is notably high, while \"Feminine\" and \"Androgynous\" frequencies are very low; conversely, for Claude 3.5 Sonnet, \"Masculine\" frequency is very low, while \u201cFeminine\u201d and \"Androgynous\" frequencies are high."}, {"title": "Differential Sensitivity Patterns for Single and Intersectional Attribute Scenarios", "content": "Figure 3 presents a detailed comparison of ethical sensitivity patterns between single and intersectional protected attribute scenarios for GPT-3.5 Turbo and Claude 3.5 Sonnet. For single protected attribute scenario (Figure 3a), \"Race\u201d and \"Color\" emerge as highly sensitive attributes, with combined sensitivity scores reaching approximately 0.85 and 0.75 respectively. This heightened sensitivity suggests both models are particularly cautious when making ethical decisions involving these attributes in isolation. Other attributes such as \"Disability\u201d, \u201cGender\u201d, \u201cDressing\u201d, \u201cLook\", and \"Age\" demonstrate notably lower sensitivity levels (below 0.2), indicating the models are more willing to make distinctions based on these attributes. However, the pattern shifts dramatically for intersectional protected attribute scenario (Figure 3b), where sensitivity levels become more uniform across all protected attributes, ranging between 0.2 and 0.4. This equalization effect is notable for \u201cRace\u201d and \u201cColor\u201d, which show substantially reduced sensitivity when combined with other attributes, while previously less sensitive attributes like \u201cAge\u201d and \u201cDisability\u201d show increased sensitivity in intersectional contexts. This transformation in sensitivity patterns between single and intersectional scenarios suggests that the models' ethical decision-making processes become more nuanced and balanced when considering multiple protected attributes simultaneously.\""}, {"title": "Stability Analysis of Protected Attribute Responses", "content": "Figure 4 presents a comparative analysis of model stability through total standard deviation histograms across single (Figure 4a) and intersectional (Figure 4b) protected attribute scenarios. The x-axis displays seven protected attribute groups (Age, Disability, Dressing, Race, Look, Gender, and Color), with stacked bars representing the standard deviations for GPT-3.5 Turbo (blue) and Claude 3.5 Sonnet (beige). For single protected attribute scenario (Figure 4a), we observe varying degrees of stability across different attributes, with \u201cColor\u201d showing the highest combined standard deviation (approximately 0.8) and \u201cAge\u201d demonstrating the lowest (approximately 0.2). The intersectional scenario (Figure 4b) reveals more consistent standard deviations across all attributes, with values generally ranging between 0.3 and 0.4, indicating more uniform stability.\nNotably, the intersectional scenarios demonstrate more consistent stability patterns across all protected attributes compared to single scenarios, suggesting that the models' responses are more predictable when dealing with combinations of protected attributes rather than individual ones. This enhanced stability in intersectional scenarios provides strong evidence for the reliability of our results."}, {"title": "Hierarchical Clustering Analysis of Model Preferences", "content": "Figure 5 illustrates the clustering hierarchical tree diagrams comparing GPT-3.5 Turbo (Figure 5a) and Claude 3.5 Sonnet (Figure 5b) for intersectional protected attribute scenario. The hierarchical clustering methodology is detailed in the Clustering of preference subsection of Measurement Metrics. The dendrograms visualize the clustering of protected attributes based on their preference strengths, with the y-axis representing the distance measure (from 0 to 2.5) indicating the degree of similarity between clusters. The analysis employs a 3-cluster classification system, wherein the ethical preferences of protected attributes within the same category demonstrate high proximity. Both models show distinct clustering patterns: GPT-3.5 Turbo exhibits a polarized distribution with a weak preference cluster containing appearance-related attributes and demographic characteristics (\"70\u201d, \"Yellow\u201d, \"African\u201d), a strong preference cluster encompassing most demographic attributes (\u201cDisabled\u201d, \u201cAsian\u201d, \u201cCaucasian\u201d), and a very strong preference exclusively for \"Good-looking\u201d. In contrast, Claude 3.5 Sonnet demonstrates a more balanced distribution, with a dominant strong preference cluster including most protected attributes (\"Disabled\u201d, \"Modest\u201d, \u201cFeminine\u201d), while maintaining the very strong preference for \"Good-looking\u201d and relegating attributes such as \"Masculine\" and \"Luxury\u201d to the weak preference cluster. This distinct clustering pattern suggests potentially different underlying biases in their respective training processes."}, {"title": "Comparative Analysis of Model Preferences", "content": "Figure 6 illustrates the preference score distributions for GPT-3.5 Turbo and Claude 3.5 Sonnet across single and intersectional scenarios. The preference scores, ranging from -1 to 1, quantify the relative biases between the two models, where a score of 1 indicates exclusive preference by GPT, -1 indicates exclusive preference by Claude, and 0 represents equal preference between models. For single protected attribute scenario (Figure 6a), GPT demonstrates strong preferences (positive scores) for several attributes, with \u201cDisabled\u201d, \u201cBlack\u201d, and \u201cCaucasian\u201d showing the highest positive scores (> 0.5). Conversely, Claude shows distinct preferences (negative scores) for attributes including \u201cUnpleasant-looking\u201d, \u201cMasculine\u201d, and \u201cNon-disabled\u201d. This pattern suggests fundamental differences in the models' ethical decision-making processes for individual protected attributes. The intersectional scenario (Figure 6b) reveals interesting shifts in these preferences. \"Masculine\u201d and \"African\u201d attributes maintain strong positive scores for GPT, while \"Feminine\u201d and \u201cAndrogynous\u201d remain strongly preferred by Claude. However, only 7 protected attributes (4 for GPT, 3 for Claude) maintain their preference direction across both scenarios, while 12 attributes (7 for GPT, 5 for Claude) show reversed preferences between single and intersectional contexts. This preference instability suggests that the models' ethical decision-making processes become more complex and potentially less consistent when evaluating multiple protected attributes simultaneously. The visualization demonstrates the dynamic nature of these preferences through bar lengths, which represent the magnitude of preference disparity between the models. The clear separation of preferences in both scenarios, particularly for certain demographic and appearance-based attributes, highlights systematic differences in how these models approach ethical decision-making tasks."}, {"title": "Discussion and Conclusion", "content": "This study investigated the ethical decision-making patterns for GPT-3.5 Turbo and Claude 3.5 Sonnet through simulated ethical dilemmas, evaluating their ethical preferences, sensitivity, stability, and preference clustering. Our analysis revealed inherent biases in both LLMs' decision-making processes, paradoxically making them more \"human-like\" but raising significant ethical concerns, particularly regarding their impact on underrepresented groups.\nA striking finding across both models is the dominant role of physical appearance in ethical decision-making. Both"}, {"title": "Future Work", "content": "Future research directions emerge in three key areas. First, expanding the scope to include a broader range of protected attributes, LLM types (including open-source models), and linguistic contexts could enhance our understanding of bias patterns across different societal and cultural backgrounds.\nSecond, investigating the impact of prompt engineering on ethical decision-making could reveal how variations in tone, phrasing, and emotional content influence model responses. Finally, comparative studies between human and LLM ethical decision-making could illuminate the extent of moral alignment between artificial and human intelligence, providing crucial insights for advancing human-AI interaction in ethical contexts."}, {"title": "Code Availability", "content": "The code used for analysis in our study is available at https://github.com/arce-star/Bias-in-Decision-Making-for-AI-Ethical-Dilemmas-A-Comparative-Study-of-ChatGPT-"}]}