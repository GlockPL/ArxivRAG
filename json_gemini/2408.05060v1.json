{"title": "GLEAMS: Bridging the Gap Between Local and Global Explanations", "authors": ["Giorgio Visani", "Vincenzo Stanzione", "Damien Garreau"], "abstract": "The explainability of machine learning algorithms is crucial, and numerous methods have emerged recently. Local, post-hoc methods assign an attribution score to each feature, indicating its importance for the prediction. However, these methods require recalculating explanations for each example. On the other side, while there exist global approaches they often produce explanations that are either overly simplistic and unreliable or excessively complex. To bridge this gap, we propose GLEAMS, a novel method that partitions the input space and learns an interpretable model within each sub-region, thereby providing both faithful local and global surrogates. We demonstrate GLEAMS' effectiveness on both synthetic and real-world data, highlighting its desirable properties and human-understandable insights.", "sections": [{"title": "1. Introduction", "content": "In numerous applications, the data present themselves as large arrays-tabular data, where each line corresponds to a data point and each column to a feature. In this setting, deep learning is becoming as popular as it already is for more structured data types, and deep neural networks achieve state-of-the-art in many scenarios [1]. A caveat of this approach is the difficulty to obtain insights on a specific prediction: the intricate architectures and the sheer number of parameters prevent the user of getting a clear picture of what is actually important to the model.\nSince getting such explanations is desirable in many applications, and could even become mandated by law in certain regions [2], many interpretability methods have been proposed.\nWithout getting into details (we refer to the recent surveys [3] and [4] for this purpose), we want to make two distinctions clear. First, it is challenging for interpretable models to have the same accuracy as non-interpretable ones. As a consequence, many explainability methods rely on a post hoc approach, i.e., producing explanations in a second step, exploiting already trained black-box models. This is the road we take, since post-hoc methods allow us to deal with any given model and not being dependent on a specific architecture. Second, the explanations can either be local (related to a specific example), or global (for all the input space). A problem with local explanations is that they have to be computed individually for each new example, which can be time-consuming. For instance, LIME [5] generates 5,000 new datapoints for each example to explain. The black-box model then has to be queried on each of these points.\nIn this paper, we propose GLEAMS (Global & Local ExplainAbility of black-box Models through Space partitioning), a post hoc interpretability method providing both global and local explanations. Our goal is to build a global surrogate $f$ of $f$ that shares the global shape of the black-box model $f$ but, at the same time, has a simple local structure. Our main motivation in doing so is to extract simultaneously from $f$: i) local explanations for any instance; ii) overall importance of the features. Secondary objectives are the production of \u201cwhat-if\u201d-type explanations, and visual summaries. In more details, the core idea of GLEAMS is to recursively split the input space in rectangular cells on which the black-box model can be well-approximated by linear models. We describe the method in Section 2. The local explanations are then simply given by the feature coefficients of these local surrogate models, while global indicators can be readily computed from those, without querying $f$ model further. We describe how to obtain the other indicators in Section 2.4, and show experimentally that GLEAMS compares favorably with existing methods in Section 3. GLEAMS code as well as the experiments for this paper are publicly available.\u00b9 Related work can be found in Appendix A."}, {"title": "2. Description of the method", "content": "We consider the following general setting: $f$ is a mapping from the input space $\\mathcal{X}$ to the output space $\\mathcal{Y}$. This corresponds to the usual situation in machine learning, where $y = f(x)$ is a good prediction of the true label of $x$. In this paper, we assume that $\\mathcal{Y} \\subseteq \\mathbb{R}$. We want to stress that this can correspond both to the classification and regression setting. In the regression setting, for any input $x \\in \\mathcal{X}$, $f(x)$ is simply a real-valued quantity of interest, whereas in the classification case $f(x)$ corresponds to the pseudo-probability to fall into a given class.\nThe construction of the global surrogate on which GLEAMS relies to provide explanations is a two-step process. The first step is to sample $N$ points $x_1, ..., x_N$, evenly spread in $\\mathcal{X}$. For"}, {"title": "2.1. Measurement points", "content": "Our main assumption, going into the description of the method, is the following:\nAssumption 1. The input space $\\mathcal{X}$ is a hyper-rectangle of $\\mathbb{R}^d$. That is, there exist reals numbers $a_j, b_j \\in \\mathbb{R}$ for any $j \\in [d]$ such that $\\mathcal{X} = \\prod_{j=1}^{d} [a_j, b_j]$.\nIntuitively, this corresponds to a situation where each variable has a prescribed range (for instance, age takes values in [0, 120]). In particular, we assume that further examples to explain all fall within this range. If they do not, we attribute to these points the explanations corre- sponding to the projection of these points on $\\mathcal{X}$. Note that we do not assume normalized data: $a_j$ and $b_j$ can be arbitrary. In practice, the boundaries are either inferred from a training set, or directly provided by the user.\nA consequence of Assumption 1 is that we can easily find a set of points covering $\\mathcal{X}$ efficiently. To achieve this, we use Sobol sequences [6]. In a nutshell, a Sobol sequence is a quasi-random sequence of points $x_1, ..., x_N$ filling the unit cube $[0, 1]^d$ as $N$ grows. We use Sobol sequences because (a) we want to be able to provide explanations for the whole input space $\\mathcal{X}$, thus the measurement points should cover $\\mathcal{X}$ as $N$ grows; and (b) the decision surface of the black-box model can be quite irregular, thus we want low discrepancy."}, {"title": "2.2. Splitting criterion", "content": "A very convenient way to partition the input space is to recursively split $\\mathcal{X}$, cutting a cell if some numerical criterion is satisfied. Additionally, we consider splits that are parallel to the axes. The main reason for doing so is the tree structure that emerges, allowing later on for fast query time of new inputs (linear in the depth of the final tree). In this section, we describe the criterion used by GLEAMS.\nA popular approach is to fit a constant model on each cell [7, CART]. We see two problems with constant fits. First, this tends to produce large and thus hard to interpret trees (see, e.g., Chan and Loh [8]). Second, the local interpretability would be lost, since the local models would not depend on the features in that case. Thus we prefer to fit linear models on each leaf, and our numerical criterion should indicate, on any given leaf, if such simple model is a good fit for $f$. GLEAMS relies on a variant of model-based recursive partitioning [9, MOB] based on score computations, described in detail in Appendix B."}, {"title": "2.3. Global surrogate model", "content": "To build the global surrogate model $\\hat{f}$, starting from the hyper-rectangle $\\mathcal{X}$, we iteratively split along the best dimension. This process is stopped once one of two criteria is met: i) leaves have $R^2$ higher than 0.95 (chosen empirically) or ii) number of points in the leaf is less than $n_{\\min}$ threshold, set to $n_{\\min} = \\min(20, d + 1)$. This recursive procedure is described in Algorithm 2, in Appendix B.\n$\\hat{f}$ is then achieved by sampling Sobol points on $\\mathcal{X}$, computing the value of the black-box model at these points, and then using Algorithm 2, Appendix B."}, {"title": "2.4. GLEAMS", "content": "Let us now assume that we computed the global surrogate model $\\hat{f}$, that is, a piecewise-linear model based on a partition of $\\mathcal{X}$. At query time, each new example to explain $\\xi \\in \\mathcal{X}$ is rooted to the corresponding hyper-rectangle $R$ in the tree structure, and thus associated to a linear model $\\beta$. From this local linear model, we can directly get different sorts of explanations.\nLocal explanations. Simply looking at the coefficient $\\beta_j$ tells us how important feature $j$ is to predict $f (\\xi)$, locally. The intuition here is that $f$ is well-approximated by $x \\rightarrow (\\beta)^T x$ on the cell $R$, thus local variations of the model along specific axes are well-described by the coefficients of $\\beta$. These local explanations can be obtained in constant time, without re-sampling and querying the model. Moreover, we have precise insights regarding the scale of these explanations, which is given by the size of $R$. We see this as an advantage of using GLEAMS: the portion of space on which the black-box model is locally approximated is well-identified and accessible to the user. This is not the case with other methods such as LIME.\nGlobal explanations. We define the global importance of feature $j$ as the aggregation of the local importance of feature $j$ on all cells of the partition $\\mathcal{P}$ of $\\mathcal{X}$. There are several ways to aggregate those, GLEAMS outputs the weighted average of the absolute value of the local importance. More precisely, let us define $\\beta^R$ the local model on hyper-rectangle $R \\in \\mathcal{P}$, then the global importance of feature $j$ is given by\n$I_j := \\frac{1}{|V(\\mathcal{X})|} \\sum_{R\\in\\mathcal{P}} |\\beta_j^R|V(R),$   (1)\nwhere $V(E)$ denotes the $d$-dimensional volume of $E$. The motivation for weighting by the cells' volume in Eq. (1) is to take into account the fact that even though feature $j$ is very important in a tiny region of the space, if it is not on all other cells then it is not globally important. We take the absolute value to take into account the possible sign variations: if feature $j$ is positively important on many cells but negatively important on a similar number of cells, then simply taking the average would tell us that the feature is not globally important. Such global attributions provide a faithful variables ranking, based on the impact they show on $f$ predictions."}, {"title": "Counterfactual explanations.", "content": "Querying this new model $\\hat{f}$ at different values of $x_j$, we can answer to the question \u201cwhat would be the decision if we changed feature $j$ by this amount?\". More precisely, for a given example $\\xi$ and a feature $j$, we can look at the evolution of $f$ when all coordinates of the input are fixed to $\\xi$, except the $j$th: it suffices to travel in the cells intersecting the line of equation $x_j = \\xi_j$ and to read the coefficients of the associated linear models. To be precise, we compute\n$\\mathcal{W}_j(x) := f(\\xi_1,..., \\xi_{j-1}, x, \\xi_{j+1},..., \\xi_d)$.   (2)\nThis can be computed quickly, since, again, there is no sampling and further calls to $f$. This allows us to present the user with a cursor which can be moved to set the feature value and present explanations in real time (see Figure 2).\""}, {"title": "3. Experiments on real data", "content": "Our goal here is to show that GLEAMS provides explanations whose quality is comparable to that of other attribution approaches, while providing these attributions in constant time and additional insights. We benchmark the various methods on the Wine dataset [10], House sell dataset\u00b2 and Parkinson telemonitoring dataset [11]. We refer to Appendix D for more experiments.\nModels. We considered two different models, trained similarly across all datasets. The first is an XGBoost model [12]. XGBoost iteratively aggregates base regressors greedily minimizing a proxy loss. Following Friedman [13] prescriptions, we employ simple CART trees with maximum depth 2 as base regressors, learning rate of 0.05, and early stopping rounds set to 100. The second is a multi-layer perceptron (MLP), composed by two hidden dense layers of 264 neurons each, trained by adaptive stochastic gradient descent [14, ADAM] and early stopping on a hold-out validation set. Both these models achieve state-of-the-art accuracy in many settings and are not inherently interpretable."}, {"title": "4. Conclusion", "content": "In this paper, we presented GLEAMS, a new interpretability method that approximates a black- box model by recursively partitioning the input space and fitting linear models on the elements of the partition. Both local and global explanations can then be obtained in constant time, for any new example, in sharp contrast with existing post hoc methods such as LIME and SHAP.\nThe main limitation of the method is the assumption that all features are continuous. As future work, we aim to extend GLEAMS to mixed features (both categorical and continuous). Another limitation of the method is the initial number of model queries needed. One possible way to reduce it would be to evaluate the local regularity of $f$ and to sample less when the model is smooth."}, {"title": "A. Related work", "content": "The idea of approximating globally a complex model such as a deep neural network by a simpler surrogate model relying on a tree-based partition of the space can be dated back at Craven and Shavlik [16]. The proposed method, TREPAN, approximates the outputs of the network on the training set by a decision tree, choosing the splits using the gain ratio criterion [17]. More recently, inspired by Gibbons et al. [18], Zhou and Hooker [19] proposed to use another split criterion, based upon an asymptotic expansion of the Gini criterion. Both these approaches are limited to the classification setting, and also require access to the training data (which is implicitly assumed to have a good covering of the input space).\nFrom the interpretability side, a standard way to explain $f$, which is closely related to our approach, is to compute attributions scores for each feature. Lei et al. [20] exclude a specific feature from $f$ and define its impact as the deterioration of model predictions. On the contrary, Partial Dependence Plots (PDP) developed by Friedman [13] measure the sensitivity of $f$ to changes in the specific feature, isolating its effect from the ones of the other features. Ribeiro et al. [5] propose LIME, which proposes as attribution for a specific example the coefficients of a linear model approximating $f$ locally. Similarly, Lundberg and Lee [21] introduce the idea of decomposing $f$ predictions into single variables attributions, using SHAP. The technique samples combinations of features $S$ and average changes in single instance prediction between the restricted $f (S\\cup i)$ against $f (S)$, obtaining local attributions for the feature $i$. Global SHAP attributions arise by averaging local attributions over the entire dataset.\nAttribution methods usually have to be recomputed for each new example, a caveat that we propose to overcome with GLEAMS, computing a global surrogate and relying on it to provide explanations. We are not the first to propose to provide explanations on a global scale: Ribeiro et al. [22] propose Anchors, a method extracting local subsets of the features (anchors) that are sufficient to recover the same prediction, while having a good global coverage. A caveat of the method is that local explanations are not quantitative since all members of a given anchor are equivalent. Additionally, the anchors can have many elements if the example at hand is near the decision boundary. Let us also mention Setzu et al. [23], which proposes to aggregate local explanations: starting from local decision rules, GlocalX combines them in a hierarchical manner to form a global explanation of the model. In the ad hoc setting, Harder et al. [24] proposes to train an interpretable and differentially private model by using several local linear maps per class: the pseudo-probability of belonging to a given class is the softmax of a mixture of linear models. This approach is limited to the classification setting, as with GlocalX and Anchors."}, {"title": "B. Splitting criterion", "content": "Intuitively, any well-behaved function can be locally approximated by a linear component, yielding a statistical model which we now describe. On any given leaf, for any given feature $j$, we can reorder the measurement points such that the points contained in the leaf are $x_1, ..., x_n$, with $x_{1,j} <\u2026\u2026 \\leq x_{n,j}$. To these points correspond model values $y_1, ..., y_n$. In accordance with the intuition given above, we write $y_i = \\beta^T\\tilde{x}_i + e_i$ for all $i \\in [n]$, where $\\beta \\in \\mathbb{R}^{d+1}$ are the"}, {"title": "C. Evaluation", "content": "Evaluating interpretability methods is quite challenging: for a given machine learning model, there is rarely a ground-truth available, telling us which features were used for a given prediction. Zhou et al. [26] outline two main evaluation methodologies: human-centered evaluations, and functionality-grounded (quantitative, note that the terminology was introduced by Doshi-Velez and Kim [27]). We focus on the latter, since human-centered evaluations are hard to obtain and can be unreliable. Among those, we choose to compare GLEAMS to existing methodology using monotonicity and recall. Below we briefly describe these two metrics.\nMonotonicity is defined as the correlation between the absolute value of the attributions at a given point and the expected loss of the model restricted to the corresponding feature. Intuitively, this takes high values if the model is very imprecise when it does not know the feature at hand. This is Metric 2.3 in Nguyen and Mart\u00ednez [15]. Following their notation, for each interpretability method to evaluate, for each point in the test set, we compute on the one hand a vector of attributions $a \\in \\mathbb{R}^d$ and on the other hand a vector of expected loss $e \\in \\mathbb{R}^d$. Monotonicity is then defined as the Spearman rank correlation [28] between $|a|$ and $e$. Formally, for all $j \\in [d]$, $e_j$ is defined as\n$e_j = \\int_{a_j}^{b_j} l(f(\\xi), f_j(x))p(x)dx, $(8)\nwhere $f_j$ is the restriction of $f$ to coordinate $j$ (keeping all other coordinates equal to those of $\\xi$), $p$ is a probability distribution on $[a_j, b_j]$ (recall that $a_j$ and $b_j$ denote the boundaries of $\\mathcal{X}$ along dimension $j$, see Assumption 1), and $l$ is the squared loss.\nLet us define $U(C)$ the uniform distribution on a compact set $C$. In our experiments, we consider two natural choices of $p$ in Eq. (8). First, $p \\sim U([l_j, r_j])$, where $l_j$ (resp. $r_j$) are the left (resp. right) boundaries of $R$ along feature $j$, where $R$ is the cell containing $\\xi$. This corresponds to local monotonicity: how well the attributions reflect the variations in prediction locally, that is, from the point of view of GLEAMS, on $R$. Second, $p \\sim U([a_j, b_j])$, which corresponds to global monotonicity: how well the attributions reflect the variations in prediction on the whole input space along feature $j$.\nLet us conclude this section by defining formally the Recall of Important Features. For certain models, we can actually be certain that some feature attributions should be 0. This is the case, for instance, if we force our model $f$ to use only a subset of the features. As a measure of the quality of explanations, one can then compare the top features selected by an attribution method to the set of true features, which we know. Let us call $\\mathcal{T}$ the set of true features. For any given example $\\xi$, we can define $\\mathcal{T}'$ the set of the top $|\\mathcal{T}|$ features, ranked by $|a_j|$. Originally proposed by Ribeiro et al. [5], the recall of important features is then defined as\n$\\rho(\\xi) := \\frac{|\\mathcal{T} \\cap \\mathcal{T}'|}{|\\mathcal{T}|}.$   (9)\nIntuitively, $\\rho(\\xi)$ is large (close to one) if the considered attribution method gives large attributions to the true features. In our experiments, we report the average recall over all points of the test set unless otherwise mentioned."}, {"title": "D. Toy data experiments", "content": "As a first sanity check, we ran the partition scheme of GLEAMS on simple models and checked whether the surrogate model $\\hat{f}$ was indeed close to the true model $f$. In order to have a ground-truth for the partition, we chose partition-based models, i.e., there exist a partition $\\mathcal{P}$ of $\\mathcal{X}$ such that the parameters of $f$ are constant on each rectangular cell $R \\in \\mathcal{P}$. We considered a simple setting where the model is piecewise-linear with rectangular cells, a situation in which GLEAMS could, in theory, recover perfectly both the underlying partition and the coefficients of the local models. We present here two examples: (i) a discontinuous piecewise-linear model with arbitrary coefficients, and (ii) a piecewise-linear model with continuity constraints. Example (i) is a generalization of the surface produced by a Random Forest regressor, while example (ii) is reminiscent of a fully-connected, feedforward, ReLU activated neural network. Note however that for the latter, the cells of the partition are more complicated than simple rectangle, see Montufar et al. [29] (in particular Figure 2). In each case, GLEAMS recovers perfectly the underlying model, as demonstrated in Figure 2.\nIt is interesting to notice that, despite having a stopping criterion on the coefficient of determination, not all leaves satisfy this criterion since the other stopping criterion is a minimal number of points per leaf. Thus it is possible to end up in situations where the $R^2$ on a given leaf is sub par. Nevertheless, these leaves are by construction quite small. For instance, in example (i), the average $R^2$ is equal to 1, and in example (ii) it is equal to .98."}]}