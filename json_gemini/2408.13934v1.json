{"title": "Learning to Move Like Professional Counter-Strike Players", "authors": ["D. Durst", "F. Xie", "V. Sarukkai", "B. Shacklett", "I. Frosio", "C. Tessler", "J. Kim", "C. Taylor", "G. Bernstein", "S. Choudhury", "P. Hanrahan", "K. Fatahalian"], "abstract": "In multiplayer, first-person shooter games like Counter-Strike: Global Offensive (CS:GO), coordinated movement is a critical component of high-level strategic play. However, the complexity of team coordination and the variety of conditions present in popular game maps make it impractical to author hand-crafted movement policies for every scenario. We show that it is possible to take a data-driven approach to creating human-like movement controllers for CS:GO. We curate a team movement dataset comprising 123 hours of professional game play traces, and use this dataset to train a transformer-based movement model that generates human-like team movement for all players in a \"Retakes\" round of the game. Importantly, the movement prediction model is efficient. Performing inference for all players takes less than 0.5 ms per game step (amortized cost) on a single CPU core, making it plausible for use in commercial games today. Human evaluators assess that our model behaves more like humans than both commercially-available bots and procedural movement controllers scripted by experts (16% to 59% higher by TrueSkill rating of \u201chuman-like\u201d). Using experiments involving in-game bot vs. bot self-play, we demonstrate that our model performs simple forms of teamwork, makes fewer common movement mistakes, and yields movement distributions, player lifetimes, and kill locations similar to those observed in professional CS:GO match play.", "sections": [{"title": "1. Introduction", "content": "Competitive, multiplayer first-person shooters (FPS) are extraordinarily popular. Multiple titles have tens of millions of users every month [Yin23; DAn23]. CS:GO is one of the seminal titles in the genre, with millions of players each day [Cha24]. AI agents (\"bots\") that can effectively imitate human players have the potential to improve human players' experiences by serving as training partners for new players and teammates for experienced players when their friends are not available [Jus22].\nIt is challenging to create a human-like bot for a complex, team-based FPS game like CS:GO. Existing approaches based on hand-crafted behavior rules or learned models struggle to generate realistic, coordinated player movement due to:\n1. Complexity of human movement. Hand-crafted, rule-based bots remain the prevalent practice in modern multiplayer FPS games. However, it is not tenable to encode rules spanning the massive number state combinations of 10 players in a complex 3D world. As a result, hand-crafted bots lack realism because they fail to react appropriately to a diverse set of game situations.\n2. Matching human movement distributions. While reinforcement learning (RL) approaches have been shown to produce highly-skilled (even superhuman) behavior [JCD*19; BBC*19; SHS*18; LC17], it is difficult to craft reward functions that yield policies that \"move like humans\". As a result, RL bots fail to serve as good human proxies.\n3. Compute efficiency. Imitation learning (IL) approaches can produce policies that replicate human behavior recorded in player logs, and this approach has been deployed for player controllers in turn-based games [FBB*22]. However, these games' run-time performance requirements are orders of magnitude (100x) lower than that of a real-time FPS game. Most commercial FPS games require AI controller logic to use only a small fraction of the total per-frame CPU budget [Dev24] (limiting execution to a few ms on a single CPU core [Ran20; Lin20]). Unfortunately, recent work using IL to create FPS bots [PZ22] requires orders of magnitude (800\u00d7) more compute than this limit.\nIn this paper we present the first compute efficient, data-driven method for creating bots that move like human players in the FPS game Counter-Strike: Global Offensive (CS:GO). Our bots, which include a small transformer-based model [NVC*22] trained using imitation learning, move like experienced human team players, execute well-within the AI budget of commercial FPS games, and are simple and fast to train. Specifically, our work makes the following contributions:\n(1) Efficient transformer-based movement controller. We present the first compute-efficient, transformer-based model specialized for controlling movement in CS:GO, called MLMOVE. Our model focuses on playing one map (de_dust2) and one game mode (Retakes). Once trained through standard supervised learning, MLMOVE produces human-like movement actions in response to evolving game dynamics. Our movement model's amortized runtime cost for controlling two teams of bots in a CS:GO match is just under 0.5 ms per game step on a single CPU core (8 ms inference every 16 game steps), meeting commercial game servers' performance requirement. Human evaluators assess that our model is more human-like than both commercially-available bots and expert-crafted rule-based movement bots by 16% to 59% (according to a TrueSkill rating) in the user study.\n(2) Pro-player CS:GO movement dataset curation system. We create a system for the curation of a 123-hour dataset of CS:GO game play called CSKNOW. This is the first large scale dataset curated for learning team-based movement in a popular FPS game featuring professional players.\n(3) Quantitative positioning metrics for assessing human-like behavior. Our goal is to produce realistic bot movement at both short-term and longer-term (full round) time scales. We define novel quantitative metrics computed on rounds of bot vs. bot self-play that assess how well a bot's movement emulates human players' team-based positioning. We demonstrate these metrics correlate with the human evaluators' assessment of human-like game play.\nWe refer the reader to https://mlmove.github.io for the open-source system including the trained transformer-based movement model, the rule-based execution module, the CSKNOW dataset curation system, and the complete Python evaluation code."}, {"title": "2. Related Work", "content": "Human-like agent navigation is an important component of multiple applications including robotics, autonomous driving, visual effects, and games. For example, crowd simulation in games and visual effects endeavor to generate trajectories for hundreds or thousands of simple agents with much less inter-agent interactions than FPS games [Rey87; WLP16; PKL*22]; while embodied agent motion planning research for robot navigation requires orders of magnitude more compute resources than FPS games to interact with a real physical world observed through cameras [ZHZ*24; HMZB23; EHE*12].\nOur work addresses the challenge of human-like motion control for groups of autonomous agents in the context of FPS games where the agents have to perform a wide range of movements (walk, run, jump) in a dynamic environment under extreme runtime performance constraints. The most closely related work to ours fall into three categories:\n1. hand-crafted, rule-based controllers where developers must manually encode all of the controller's behaviors\n2. RL-based controllers where developers specify a reward function that the controller maximizes\n3. IL-based controllers where developers specify a set of human examples that the controller imitates\nRule-Based Multi-Agent Movement Controllers. A common abstraction for organizing rule-based controllers is behavior trees [Is105]. However, rule-based approaches struggle to generate human-like behavior in more complex environments, as demonstrated by Huang et al.'s complex hierarchy for coordinating movement of pedestrians through doorways [HT18]. As a result, human evaluators find state-of-the-art rule-based bots for FPS games often make unhuman-like movement decisions. See our final evaluation section for more details."}, {"title": "RL Multi-Agent Movement Controllers", "content": "RL agents can generate superhuman behavior in complex strategy games like Dota 2 and Go by maximizing a reward function [SSS*17; BBC*19]. RL can also be used to train agents to win in FPS games like Doom and Quake [LC17; JCD*19]. However, these types of RL agents are not trained to act like humans because they are trained to maximize a reward function for winning. Humans may struggle to collaborate with the RL agents whose actions do not match human expectations [FBB*22]. In contrast, we use an IL-based approach to create a bot that moves like humans."}, {"title": "IL Multi-Agent Movement Controllers", "content": "When trained on large, diverse training sets of human play, IL-based controllers can generate human-like movement for a wide range of situations. Scene Transformer [NVC*22] trained a transformer for predicting multiple pedestrians' and cars' trajectories on different roads over a five-second time horizon. Scene Transformer leverages the transformer's attention mechanism to learn relationships between cars, pedestrians, and road geometry. MotionLM [SCC*23] demonstrated that a decoder-only transformer architecture can increase accuracy, since the decoder enforces causal relationships between earlier and later time steps. The models used in Scene Transformer and MotionLM cannot be directly applied to motion control for FPS games, because their compute cost is multiple orders of magnitude too high (their target use case is around five seconds per query). Adapt [AAG23], a compute-optimized movement model based on the Scene Transformer, runs in 11 ms when highly optimized for a Tesla T4 GPU. Its compute cost is still at least two orders of magnitude greater than the AI budget of FPS games [Cor19]. In contrast, our transformer model, designed for learning human-like movement in team-based FPS games, requires two orders of magnitude less compute than Adapt without any hardware specific optimization.\nExisting IL bots are also too computationally expensive for commercial FPS games. [PZ22] trained a model that controls all game behavior (not just movement) of a single CS:GO bot using rendered images as input. This pixels-to-action approach (similar to [KPH20; GHT*19]) requires a GPU for every agent, approximately three order of magnitude higher compute than commercial FPS games' AI performance constraints. Additionally, [PZ22] do not generate coordinated team behavior because they train on data from, and test their bots in, a game mode where players typically practice low-level mechanics without the need for intra-team coordination.\nResearch on hybrid RL and IL training procedures enable reward-based approaches that also generate human-like behavior. GREIL is an RL-based crowd control policy trained with a reward function based on similarity to human examples [CPV*23]. Cicero is a bot trained with piKL, which regularizes the reward function with an IL policy to prevent drastic deviation from human behavior. Cicero is designed for Diplomacy, a turn-based strategy game where action frequency is 100 times slower than in an FPS [FBB*22]. We are not aware of a hybrid RL/IL approach for human-like bots in an FPS game."}, {"title": "3. Problem Formulation: A Bot for CS:GO Retakes", "content": "Game Context. CS:GO is a multiplayer FPS involving two teams competing for control over a map. To focus on player movement, we concentrate our attention on a popular CS:GO practice mode known as \"Retakes\" and on a single map, de_dust2. Even though FPS games like CS:GO can have many maps, maps are designed to have similar room and path layout characteristics that are known to enable interesting game play; and expert players tend to hone their strategy by playing on the same map over and over. For these reasons, we chose to focus our study on the extremely popular de_dust2.\nThe rules of CS:GO \"Retakes\" are the same regardless of map choice. In each round, a bomb is planted in one of two predetermined regions known as bombsites A and B. (We provide an illustrated example of a standard game map with annotated bombsites regions in Section 4 of the Supplemental Material.) The bomb will explode in 40 seconds unless it is defused. The goal of one team, who we call the defense, is to defend the bomb until it explodes. At most 3 players are on defense. The goal of the other team, who we call the offense, is to defuse the bomb before it explodes. At most 4 players are on offense. One defense player must start at the bomb location while all other players can start at any location on the map. Members of the two teams can eliminate each other using several weapons and grenades. Without losing generality, we restrict all players to the same weapon type and preclude the use of any grenades.\nState. The game state at time $t$ consists of player states $q_{i,t} \\in Q_t$ as well as global game state that consists of the map state $map_t$ and key events $e_{i,t} \\in E_t$ like players shooting or being eliminated. Time $t$ is tracked inside each round of CS:GO using game ticks. For the rest of this paper, we use game ticks (steps) and time $t$ interchangeably. We use $\\mathbb{B}$ to represent {True, False} and $\\mathbb{Z}$ to represent the set of all integers.\n1. Each player's state $q_{i,t} = [p_{i,t}, v_{i,t}, u_i, l_{i,t}, vd_{i,t}, h_{i,t}, r_{i,t}]$ consists of position $p_{i,t} \\in \\mathbb{R}^3$, velocity $v_{i,t} \\in \\mathbb{R}^3$, team $u_i \\in$ {Offense, Defense}, alive status $l_{i,t} \\in \\mathbb{B}$, view direction $vd_{i,t} \\in \\mathbb{R}^2$, health $h_{i,t} \\in \\mathbb{Z}$, and armor $r_{i,t} \\in \\mathbb{Z}$.\n2. Map state $map_t = [b_t, x_t]$ consists of the target bombsite $b \\in$ {A,B} and seconds left until the bomb explodes $x_t \\in \\mathbb{R}$.\n3. Each game event $e_{i,t} = [src_{i,t}, tgt_{i,t}, y_{i,t}]$ consists of source player id $src_{i,t} \\in \\mathbb{Z}$, optional target player id $tgt_{i,t} \\in \\mathbb{Z}$, and type $y_{i,t} \\in$ {shoot, hurt, elimination}.\nActions. A player's action at time step $t$ $a_{i,t} = [m_{i,t}, du_{i,t}, f_{i,t}]$ consists of movement command $m_{i,t} \\in \\mathbb{Z}$ specifying which direction to move, how fast, and whether to jump or not; aim command a.k.a view direction update command $du_{i,t} \\in \\mathbb{R}^2$; and fire command $f_{i,t} \\in \\mathbb{B}$.\nObjective. Create a CS:GO bot that plays like an expert human in a team play setting. We note that the objective of playing like an expert human in a team setting is not the same as playing to win.\nExpert human players utilize complex strategies that require spatial and temporal coordination to defeat their opponents. While these types of strategies are difficult to emulate for rule-based agents, we observe that most team play strategies revolve around"}, {"title": "4. MLMOVE: A Learned CS:GO Bot", "content": "In this section, we present the algorithm and system design for our human-like CS:GO Retakes bot MLMOVE. First, we present our transformer-based movement model, then we present how we integrate our movement model with a rule-based command executor to create MLMOVE."}, {"title": "4.1. Learned Movement Model", "content": "The main challenges of building a movement model that emulates expert human players in a team-based FPS game are the conflicting goals of accurately predicting the distribution of complex human actions and extremely efficient compute usage.\nIn an FPS game, human players not only have a complex action space, but also demonstrate complex inter-player interaction and coordination that are quite challenging to model in a rule-based system. However, recent work in transformer models show how to imitate the effect of complex human decisions and interactions without modeling the intermediate steps (or decisions) that led to the final actions.\nThe architecture of our movement controller (Figure 2) is inspired by Scene Transformer [NVC*22], one of many [AAG23; SCC*23; YWOK21] transformer-based multi-agent motion prediction system for pedestrians and autonomous vehicles. The Scene Transformer encodes the state of each agent as input tokens and leverages attention to learn the relationships between all the agents. Like Scene Transformer, our movement controller can also benefit from the transformer architecture's ability to capture rich player interactions with the attention mechanism, process players' state in any order due to the permutation invariance of input tokens, and handle eliminated players with attention masking [VSP*17].\nHowever, Scene Transformer's query latency and compute resources were orders of magnitude higher than what was acceptable for FPS game adoption. Our key insight was to leverage the significant differences between the target applications of FPS games and autonomous vehicles to make architecture and system design choices to create a movement model that: (a) is able to emulate the effect of complex human team play strategy and interactions in an FPS game, and (b) can be executed within the strict compute constraint required by FPS game servers. We highlight two of these differences below.\nFirst, multi-agent motion prediction systems for autonomous vehicles must support changing road geometry (dynamic road graphs) as road conditions can change as vehicles travel from one part of the real world to another. On the other hand, professional players for FPS games tend to play and compete on the same game map for years, and map geometry and layout are static throughout the game, so it's perfectly reasonable to design movement models that are trained for one map. This design choice allowed us to reduce the number of input tokens significantly, and as a result, the complexity of our attention layers, without impacting our model's applicability for our targeted use case. Note that each attention layer's complexity is proportional to the square of the number of input tokens [VSP*17]. To support multiple maps, we can pre-train our model for each map we want to support and make them available to our MLMOVE bot. (We would of course also have to curate a training set for each map as well, just like the training dataset for Scene Transformer includes data spanning multiple map regions.)"}, {"title": "Model Input", "content": "Our movement model's input is a sequence of 10 tokens, each token describing a player's current state. CS:GO logs contain up to 10 players at any time, up to five on each team. This is a broader range of players than in Retakes. We train our model on 10 input tokens to enable it to generalize to a wider range of situations. The feature vector of each token is $[p_{i,t}, f_{i,t}, d_{i,t}]$, where $f_{i,t} = [l_{i,t}, u_i, b_t, x_t]$ and $d_{i,t}$ is a set of derived features that approximate information not contained in the game logs like visibility and team communication about strategy. Each token starts with the player's position $p_{i,t}$, alive status $l_{i,t}$, and team association $u_i$. We also include in each token the global map states of bomb location $b_t$ and remaining time for bomb explosion $x_t$, information known to all players. We define the derived features in Section 1.1 of the Supplemental Material. We found that the derived features can aid attention in limited situations."}, {"title": "Model Output", "content": "Our movement model's output is a sequence of tokens, each token describing a player's movement command: which direction to move, how fast, and whether to jump. To capture the multi-modal and stochastic nature of player movement, we represent a movement command as a discrete probability distribution with 97 options. Each option corresponds to a combination of one of 16 angular directions, three different movement speeds, two jumping vs not jumping states; plus a separate option for standing still. Movement commands aren't recorded in CS:GO logs. We use heuristics to infer the movement commands from position/velocity information in the logs [PZ22]. We found discretizing direction uniformly into 16 absolute angles is sufficient to navigate game map details like thin ledges."}, {"title": "Model Architecture", "content": "The full architecture of the movement model is depicted in Figure 2(a). Each input player token is converted by an embedding network to an embedded token of dimension matching that of the transformer's attention layers; then each sequence of 10 embedded tokens corresponding to the states of 10 players are processed by the transformer to yield the movement command probabilities for the 10 players.\nWe use a learned embedding (Figure 2(b)) to convert input player tokens into vectors of dimension 256. Our embedding network consists of three linear layers, with LeakyReLU activations in between the linear layers. Our transformer encoder (Figure 2(c)), consists of four identical single-head self attention layers of dimension 256. Like [VSP*17], we use a learned linear transformation and softmax to convert the outputs of our attention network to predicted probability of the output tokens (the player movement commands in our case).\nTo support eliminated players, we use transformer's masking feature. A transformer's attention layer computes the attention (connection) between all token pairs in the input sequence except for those that are masked out. So we set $mask(i,t) = 1$ for each token of an eliminated player ($l_{i,t}$ = false), to remove attention between that player and all other players. Also, we restrict the loss computation to only use $P(m_i)$ for players that are alive. Together with attention masking, this ensures eliminated players have no impact on our model's movement predictions for live players.\nTo learn temporally coherent motions, our model outputs predictions not only for the immediate next action (0 ms into the future) but also for actions at 125 ms and 250 ms from the current time. This is achieved by replicating each player's embedded token for time $t$ three times, and summing each player's embedded token with the positional encoding of the three timestamps, to create distinctive embedded tokens for current time $t$, $t + 125$ ms, and $t+250$ ms. Like [VSP*17; NVC*22], we use sinusoidal positional encoding for the player's in-game map position and for the three temporal positions represented as timestamps.\nA well known problem in imitation learning is the inertia problem, where models trained on sequences are biased to repeating recent actions, since this type of repeating \"what I did last\" behavior tends to dominate the dataset [DJL19; CSLG19; SHYK23]. This can lead to the failure to learn important movements like velocity change or (intentionally acted) \"erratic\" movements in combat, because they are both rare (low probability) events in the training dataset. We address the inertia problem using a simple solution that improves our model's prediction accuracy and efficiency: our model input consists only of \"current\" player states. The ablation in Section 6.3.5 shows our solution's effectiveness, as adding prior input states leads to less human-like map occupancy and kill location distributions."}, {"title": "Model Training", "content": "We train the movement model using standard supervised learning where we minimize the cross-entropy loss between the probability distributions of the predicted movement command and the ground truth movement commands in the dataset.\nWe train using the CSKNOW dataset (described Section 5) and perform an 80/20 train-test split: 5655508 train data points (98 hours at 16 Hz) and 1429953 test data points (25 hours at 16 Hz). Since there is a strong correlation between data points in the same round, we assign all data points in each round to the same subset. Once grouped into train/test subsets, we randomly sort data points irrespective of their round. We use the same train/test split for all training runs. To improve the model's ability to generalize, we add random Gaussian noise with mean 0 and variance 20 CS:GO units (less than a player's width of 32 units) to the player positions (see Figure 2(b)).\nWe train for 20 epochs with a batch size of 1024, an initial learning rate of 4e-5 controlled by the Adam optimizer with default"}, {"title": "4.2. Integrating Movement Model into a CS:GO Bot", "content": "The resulting trained model predicts the movement for all players efficiently enough to be deployed in a commercial FPS game server. Specifically, the memory requirement for our model's 5.4M parameters is 21 MB; and the inference latency (time it takes to predict the movement of all players) of our trained movement model deployed in C++ using LibTorch and TorchScript [PyT24] is 8 ms with an IQR of 0.6 ms on a single core of a Intel Xeon 8375C CPU.\nWe use a modular approach shown in Figure 3 to integrate our learned movement model into a full CS:GO bot MLMOVE. The input to MLMOVE is the current game state and output of the MLMOVE is a sequence of game server commands that can be produced and sent to the server by a regular human player.\nThe core system of MLMOVE consists of our learned movement model and the rule-based execution module. Every 125 ms (16 game ticks), MLMOVE requests the learned movement model to predict the movement commands for all players using the current game player state as input. The bot caches and reuses the predicted movement commands for the subsequent 125 ms. The rule-based execution module is executed every game tick to emulate human mouse movement latency used for aiming. It converts movement commands at current time $t$ made by our learned model into human players' keyboard navigation commands. The movement commands are only updated once every 125 ms to emulate human keyboard press latency; therefore, the amortized compute cost for our learned movement model for each game tick (frame) is 0.5 ms.\nOur rule-based execution module also generates aiming and firing commands based on the current game state and player positions; the rule-based execution module sends all the \"machine generated\" game commands to the server, which will execute the commands and update the game state for the next frame."}, {"title": "Aiming and firing", "content": "We use standard techniques to handle aiming and firing. If no enemy is visible, the aiming module uses a probabilistic occupancy map to pick a target where enemies are likely to appear, emulating human-like predictive aim [Mor88; Isl13]. If at least one enemy is visible, the module selects one target and tracks them until they are no longer visible. The aim module generates a smooth trajectory of view direction updates using a semi-implicit Euler method [BSK20]. The fire execution module emits fire commands when the crosshair aligns with an enemy's axis-aligned bounding box. A distance-based lookup table controls the fire command frequency, shooting shorter and more controllable sequences at farther enemies that are harder to hit.\nFor specific implementation details, see Section 3 of the Supplemental Material."}, {"title": "5. CSKNOW Dataset Curation System", "content": "There is a scarcity of open datasets for learning movement control for FPS games. Prior CS:GO datasets focused on long-term outcomes like win probability, so they captured game state at too low frequency for evaluating movement commands at every 125 ms. For example, ESTA contains professional game play with data points every 500 ms [XS22], and PureSkill.GG contains amateur game play with no guarantees on data capture frequency or even if some data were dropped [Cri24].\nWe present CSKNOW, the first dataset for learning team-based CS:GO movement featuring professional players. The dataset contains 123 hours of play sampled at 16 Hz. The data comes from over 17K rounds, features 2292 unique players, 513K shots, and 29K eliminations. See Section 1 of the Supplemental Material for the subset of game state extracted in CSKNOW.\nWe created a system to curate the 123 hour dataset from logs of 1156 hours played on the de_dust2 map by professional players between April 2021 and November 2022. We downloaded the logs from HLTV [HLT24]. The logs contain game play from the complete CS:GO game mode, not just the Retakes practice one. Unlike the Retakes mode, the complete game mode requires five on each team at the start of each round and involves an earlier stage where teams compete to plant the bomb. We filter the data in CSKNOW to game ticks when the bomb has been planted and at least one player is alive on both teams, a super-set of Retakes. This filter ensures our dataset is focused enough to be representative of Retakes mode play style while still broad enough to cover a diverse range of game play."}, {"title": "6. Evaluation", "content": "The primary goal of our work is to produce human-like movement for an FPS game. To evaluate how well we achieved this goal, we first conducted a small-scale user study (inspired by BotPrize 2010 [Hin10; Hin09]) where human evaluators rank movement in videos of games played by humans and bots, and an exploratory study where humans play with and against the bots. Then, we performed a large-scale quantitative comparison on the distributions of movement trajectories and key outcomes from bot vs. bot self-play relative to those from professional human play. Through this combination of small-scale human ranking and large-scale quantitative analysis of outcome distributions, we present the first comprehensive evaluation of human-like team-based movement for multiplayer FPS bots."}, {"title": "6.1. Experiment Conditions", "content": "We compare four different player configurations:\n\u2022 HUMAN. Replay of the actual human data, taken from the CSKNOW dataset.\n\u2022 MLMOVE. Our bot with a learned movement controller, as described in Section 4.\n\u2022 RULEMOVE. A bot with a rule-based movement controller implemented by the authors. The bot uses the same rule-based aim and firing controllers as MLMOVE. This bot was developed over several months by a skilled CS:GO player and should be con"}, {"title": "6.2. Human Assessment", "content": "To assess the realism of bot motion, we conducted a within-subjects study where we asked human evaluators to watch CS:GO game play videos depicting both human and bot play [POC*23]. For each of the eight rounds described in Section 6.1, participants were asked to rank player configurations based on how well player movement matched their \"expectation of how humans would move in that situation.\"\nEvaluators. We recruited fifteen evaluators with CS:GO experience ranging from novice (never having played) to expert. Five of them achieved a rank of \"Global Elite\", the highest CS:GO player rating; and four had a rank of \"Supreme Master First Class\", the second highest.\nQuantitative Ranking Results. Our study produces 120 rankings of the player configurations. Each ranking is an ordering of the four player configurations' similarities to expected human behavior in"}, {"title": "6.3. Quantitative Self-Play Experiments Analysis", "content": "Beyond the user study, we provide a quantitative evaluation of the four player configurations by analyzing the statistics of full rounds of in-game self-play. Our metrics cover the key properties of movement: map coverage, utilizing expert strategies that avoid low-skill mistakes, and ensuring that movement yields key outcomes. The"}, {"title": "6.3.1. Distribution of Player Positions", "content": "Figure 1 shows the distribution of player positions across the first iteration of 1430 rounds. Each pixel counts the game ticks when an offense or defense player occupies that location of the map. Overall, the distributions of the MLMOVE positions appear more similar to that of HUMAN players than any of the other bot player configurations, for both the offense and defense teams. The first row in Table 1 shows that, when measured using earth mover's distance [KPT*17], the MLMOVE occupancy distribution (computed over both the offense and defense teams) is 1.8\u00d7 and 1.9\u00d7 more similar to that of HUMAN play than RULEMOVE and GAMEBOT respectively. We provide details of how we compute EMD in Section 5.3 of the Supplemental Material.\nFigure 1 also shows that MLMOVE players exhibit skilled movement characteristics such as positioning themselves to remain out"}, {"title": "6.3.2. Avoiding Common Mistakes", "content": "A first trait of \"nonhuman\" bot behavior is \"a lack of common sense\", which can be measured by the number of \"common\" mistakes. We consider two mistakes: (a) leaving high ground, or (b) giving up on an established defensive position. To characterize these mistakes, we identify specific combinations of players' positions within regions of the map indicating a defensive advantage on a game tick. For each such scenario, we compute whether the defensive players' regions in the next game tick indicate that they gave up their advantage. We measure the number of rounds with at least one mistake. As shown in Figure 6, MLMOVE's mistake rate is close to that of human players, and significantly smaller than those of the other bots."}, {"title": "6.3.3. Teamwork", "content": "We analyze the self-play rounds for instances of common forms of teamwork. Specifically we focus on offense flanking, where multiple players on offense approach the defense from different directions to catch the defenders off guard. We also count instances of defense spreading, a tactic where defense players carefully distance themselves so that each player can cover a different potential attack direction, while being close enough to quickly reconverge on the most important actual attack direction.\nWe identify five unique two-player flanking configurations (involving different combinations of attack directions) and six unique"}, {"title": "6.3.4. Self-Play Outcomes", "content": "Skilled CS:GO players move to advantageous positions that increase the likelihood of eliminating enemies without being eliminated. We hypothesize that if MLMOVE moves similarly to human players", "enemies": "they tend to shoot more frequently from positions that are protected. In Figure 7(a), both offense humans and MLMOVE avoid combat in the open areas leading to the B bombsite, whereas RULEMOVE and GAMEBOT have poor positioning and engage in these cover-free regions. In Figure 7(b), defense humans, MLMOVE, and RULEMOVE (due to map-specific rules) primarily score kills from the center of the A bombsite, where the map contains objects that provide cover. On the other hand, GAMEBOT scores kills uniformly around the entire bombsite. Row 2 of Table 1 quantitatively confirms that the MLMOVE's kill location distributions are most similar to human play.\nShots per kill We also observe that rounds involving MLMOVE-controlled players demonstrate approximately the same distribution of shots per kill as humans (Figure 8). Although it uses the same aiming and firing controller as MLMOVE, RULEMOVE produces a left-sided distribution, indicating fewer shots per kill. RULEMOVE's movement controller tells it to stop moving whenever an enemy becomes visible to increase its shot accuracy, but this behavior is not something all experienced human players would do in practice or in our training dataset.\nPlayer lifetimes Finally, we observe that rounds involving MLMOVE players exhibit a similarly shaped distribution of player lifetimes as that of human play (Figure 9). However, we also observe many more examples of MLMOVE players staying alive for the full 40-second period. We believe this is due to a conservative gameplay strategy present in the CSKNOW dataset but not in the Retakes test subset."}]}