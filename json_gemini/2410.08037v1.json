{"title": "COMPOSITE LEARNING UNITS: GENERALIZED LEARNING\nBEYOND PARAMETER UPDATES TO TRANSFORM LLMS INTO\nADAPTIVE REASONERS", "authors": ["Santosh Kumar Radha", "Oktay Goktas"], "abstract": "Human learning thrives on the ability to learn from mistakes, adapt through feedback, and refine\nunderstanding\u2014processes often missing in static machine learning models. In this work, we introduce\nComposite Learning Units (CLUs) designed to transform reasoners, such as Large Language Models\n(LLMs), into learners capable of generalized, continuous learning without conventional parameter\nupdates while enhancing their reasoning abilities through continual interaction and feedback. CLUs\nare built on an architecture that allows a reasoning model to maintain and evolve a dynamic knowledge\nrepository: a General Knowledge Space for broad, reusable insights and a Prompt-Specific Knowledge\nSpace for task-specific learning. Through goal-driven interactions, CLUs iteratively refine these\nknowledge spaces, enabling the system to adapt dynamically to complex tasks, extract nuanced\ninsights, and build upon past experiences autonomously. We demonstrate CLUs' effectiveness through\na cryptographic reasoning task, where they continuously evolve their understanding through feedback\nto uncover hidden transformation rules. While conventional models struggle to grasp underlying logic,\nCLUs excel by engaging in an iterative, goal-oriented process. Specialized components\u2014handling\nknowledge retrieval, prompt generation, and feedback analysis-work together within a reinforcing\nfeedback loop. This approach allows CLUs to retain the memory of past failures and successes, adapt\nautonomously, and apply sophisticated reasoning effectively, continually learning from mistakes\nwhile also building on breakthroughs.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence has seen transformative breakthroughs, most recently embodied by the emergence of Large\nLanguage Models (LLMs) such as GPT-4 [26], Gemma [45], LLaMA[7], Mistral [15]. These models, representative\nof transformer-based architectures, have demonstrated impressive capabilities in zero-shot reasoning [3], language\ngeneration, and problem-solving, often achieving near-human performance across a spectrum of specialized tasks[34, 48,\n24, 55, 47, 5]. However, they exemplify a fundamental limitation shared by most deep neural networks (DNNs): their\nreliance on static learning paradigms. Typically, such models require extensive retraining to adapt to new information,\nand even minor shifts in task requirements necessitate resource-intensive fine-tuning [5]. This lack of adaptability is\nnot limited to LLMs but pervades most modern DNNs, which are designed to extract knowledge from training data\nand encode it within static weight parameters. Online learning and continual learning methods have made strides\ntowards overcoming these limitations by allowing incremental learning from new data and mitigating catastrophic\nforgetting [13, 21, 36]; however, these approaches still often operate within predefined objectives, limiting their capacity\nfor truly autonomous reasoning and adaptation to emergent complexities. Current learning processes in DNNs focus\npredominantly on mapping inputs to outputs\u2014whether through classification, regression, or clustering-fitting models\nto recognizable patterns within data [2, 18, 40]. This paradigm, while yielding substantial advances in supervised\nand unsupervised learning tasks, only addresses a limited aspect of the broader landscape of knowledge that can be\ninferred. Real-world scenarios demand more than pattern recognition; they require systems capable of extracting\nabstract, multifaceted, and evolving insights-capabilities that current static models, with their fixed objectives and\nrepresentations, inherently lack [44, 22]. Consequently, models remain restricted in their ability to reason autonomously\nthrough complex latent properties or to discover emergent relationships beyond those explicitly presented during\ntraining[9]. Thus, a critical question emerges: How can we design learning systems that transcend the limitations of\nstatic paradigms to actively discover, learn, and adapt to the myriad forms of knowledge embedded within data?\nTo address this pressing challenge, recent research efforts have explored various methodologies aimed at enhancing\nneural networks' reasoning and adaptability. One prominent direction has focused on overcoming the limitations of\nfixed architectures by incorporating advanced reasoning frameworks into existing models. Chain of Thought and\nMulti-Thought[49, 52, 35] techniques have extended the capabilities of transformer-based models by enabling stepwise\ndynamic problem-solving, thereby eliciting more sophisticated reasoning from pre-trained networks. These methods\naim to guide models through complex tasks by breaking them down into smaller reasoning steps, allowing the models\nto leverage pre-learned associations in novel ways. Recent works have explored methods to distill these System 2\nreasoning capabilities back into more efficient System 1 generations [53], aiming to improve performance without\nthe computational overhead of intermediate reasoning steps. While these frameworks have succeeded in pushing the\nboundaries of what can be achieved with static models, they ultimately rely on augmentations rather than addressing the"}, {"title": "2 Theoretical Foundations and System Architecture", "content": "The goal of the Composite Learning Unit (CLU) framework is to develop a black-box system composed of interacting\nagents that function as a general learning unit. This unit is designed to handle a wide range of tasks and datasets,\nwhether supervised or unsupervised, and adaptively learn from the given data. By \u201clearning,\u201d we refer to the ability of\nthe CLU to extract meaningful patterns, relationships, and insights from the data in pursuit of specific abstract goals,\nwhich may vary across different tasks. For instance, these goals may range from performing standard classification tasks\nto understanding higher-level abstract relationships between data points, such as interactions among different labels in a\ndataset. The CLU is structured to accommodate these varying objectives, allowing for dynamic task execution through\na cooperative set of agents that continually adapt to new information.\nThe CLU framework generalizes the learning process by introducing a system that can adapt both to the nature of\nthe data and the specific goal assigned to the task. The core problem it addresses is how to enable a learning unit to\neffectively process arbitrary goals and diverse datasets while maintaining the flexibility to learn from ongoing feedback\nand update its knowledge representations accordingly. This adaptability is captured in two key phases of the CLU's\noperation: the learning phase and the reasoning phase. Unlike traditional machine learning models, where the term\n\"training\" refers to the adjustment of weights through backpropagation, the learning phase of CLU refers to the iterative\nrefinement of its internal knowledge spaces based on feedback. In contrast, the reasoning phase is akin to what is\ncommonly called inference, where CLU utilizes its current knowledge to solve given tasks without modifying its\ninternal state. This distinction allows CLU to maintain generalization capabilities across diverse tasks without requiring\nconventional retraining. The expected outcome of the CLU framework is an operational system capable of dynamically\ngeneralizing across tasks, executing high-level reasoning, and evolving with changing datasets and objectives.\nIn this section, we explore the general architecture and operational dynamics of the CLU framework. First, section 2.1\nformally defines the problem, outlining the core components of CLU, including task objectives, goal specifications,\nand the evolving knowledge spaces that underpin the learning process. Next, section 2.2 introduces the key agents\nresponsible for task execution, detailing their interaction with general and prompt-specific knowledge to generate\noutputs. The subsequent sections, section 2.3 and section 2.4, focus on the Knowledge Management Unit (KMU),\nwhich oversees dynamic knowledge alignment and retrieval, and the feedback mechanisms that continuously refine\nthis knowledge. Finally, section 2.5 addresses the operational dynamics during both learning and reasoning phases,\ndistinguishing their respective roles within the CLU framework."}, {"title": "2.1 General architecture and definitions", "content": "We formally define the problem as follows: Let T represent the task space, where each task t \u2208 T is characterized\nby an input space Xt and an output space Y. The CLU aims to process the input data x \u2208 Xt and generate an output\ny \u2208 Y in alignment with a specific goal G. The goal G is an abstract representation of what the system aims to learn\nfrom the task and could vary in complexity and scope. It defines the nature of the task that the system is trying to solve.\nFor example, t could represent a classification task, a regression task, or an unsupervised clustering task for the given\ndata. It encapsulates the abstract goal and a specification of what the system needs to learn from the data. This is the\ntask-level instruction that informs the system on which aspects of the data to devote the most focus.\nIn addition to the task and goal, the CLU relies on two types of knowledge spaces, which act as dynamic, trainable\nmemory stores that evolve over time. These knowledge spaces are crucial for adapting the learning process based on the\ntask and feedback received. In neural networks, trainable weights evolve based on data to learn and extract meaningful\nfeatures. Similarly, in the CLU, knowledge spaces dynamically develop to store essential latent representations, which\nare continuously refined through iterative feedback cycles to guide future tasks. As illustrated fig. 3, the CLU's\nknowledge spaces act as critical resources that interact with agents during both task execution and feedback processing.\nThese knowledge spaces are capable of encompassing high-level insights, particularly tailored for language-based\ntasks and data, due to the use of language models as the reasoning components. However, they are inherently flexible\nenough to represent abstract latent information distilled from broader sensory inputs during reasoning or interaction\nwith external systems. This versatility allows the CLU to adapt and respond effectively across a wide range of\napplications by leveraging its evolving knowledge repositories, while maintaining a text-based latent structure as a\nprimary representation. The knowledge spaces can be formally defined as follows:\n\u2022 GKS Kg: General Knowledge Space (GKS) serves as a repository for domain-specific knowledge, capturing\nbroader patterns and insights related to the overall goal G. It stores knowledge that generalizes across tasks,\nforming the core reasoning foundation of the CLU. In more formal terms, the task t defines the problem\ncontext for which the system will retrieve knowledge from the GKS.\n\u2022 PKS Kp: Prompt-Specific Knowledge Space (PKS) is designed to store task-specific information related to\ngenerating effective prompts for the current task. It is focused on learning detailed relationships within the\ntask at hand, ensuring that the system can adapt to specific nuances or requirements of a given task.\nThe need for two distinct knowledge spaces arises from the need to disentangle the general reasoning process from\ntask-specific adaptations. While the GKS K\u00e7 provides a high-level understanding of how the task relates to the\noverall goal, the PKS Kp fine-tunes the prompt generation process to adapt to task-specific data. Together, these two\nknowledge spaces work in tandem, allowing the CLU to solve complex tasks by integrating both task-specific insights\nand high-level goal-driven reasoning. The retrieved knowledge from these spaces is used to guide the operational\ndecisions of the CLU as depicted in fig. 3.\nThere are several agents within the CLU that operate on this knowledge base and data, making decisions to modify\nand update the knowledge spaces based on feedback received during task execution. The primary agent, known as the\nOperational Agent Ao (more details in section 2.2) is responsible for processing the input data x \u2208 Xt, along with\nthe general knowledge kG \u2208 Kg and the prompt-specific knowledge kp \u2208 Kp, to generate the output y \u2208 Y. The"}, {"title": "2.2 Task Execution through Knowledge and Prompt-Driven Agents", "content": "To act effectively on a given task t \u2208 T, the CLU relies on two crucial components: general knowledge about the\nbroader task domain and a task-specific prompt. The general knowledge captures the high-level information required to\nperform the task in alignment with the global goal G, while the task-specific prompt refines the action taken based on\nthe nuances and specific requirements of the task at hand.\nThe general knowledge kg \u2208 K\u00e7 is retrieved from the GKS using the retrieval function RG(Kg,t). This function\nextracts knowledge relevant to the overall goal and the task at hand, ensuring that the operational agent has access to\ndomain-level information that informs its decision-making. In tandem, the task-specific prompt p \u2208 P is generated by\nthe Meta-Prompt Agent AMP using task-specific knowledge retrieved from Kp. The prompt p provides finer-grained\ninstructions that allow the agent to adapt its actions to the specific task, incorporating any contextual details needed for\naccurate task execution.\nThe retrieval of both general knowledge and task-specific prompts, akin to retrieval-augmented generation (RAG)\ntechniques with LLMs[10], is formalized as:\n$k_g = R_G(K_g, t), p = A_{MP}(t, R_p(K_p, t))$\nwhere $R_p(K_p, t)$ retrieves the task-specific information Kp, which the Meta-Prompt Agent uses to generate the\nprompt p. Together, these two inputs provide the operational context in which the CLU operates, ensuring that both\ndomain-level knowledge and task-specific insights are incorporated into the decision-making process.\nThe Operational Agent Ao acts on the task by taking those above two critical inputs-general knowledge and the\ntask-specific prompt-along with the task input x \u2208 Xt, to produce an output y \u2208 Y. The operational agent is the\ncore component that processes the task by leveraging both broad and specific information. This agent can be designed\nas a singular or a multi-agent that acts as a single system when advanced reasoning is desired. For instance, where\nadvanced reasoning is desirable, the operational agent might use multi-thought frameworks such as Chain-of-Thought\n(CoT)[49], Tree-of-Thought (ToT)[52], Everything of Thought [6] or Iteration-of-Thought (IoT) [35] to dynamically\nexplore multiple decision paths or hypotheses before producing a final output. The flexibility of this agent allows the\nCLU to scale from simple tasks to complex ones that require deep reasoning or sequential decision-making on top of\nthe retrieved information.\nFormally, the operational agent produces the output as:\n$\u0177 = A_o(x, k_g, p)$\nwhere kg is the general knowledge retrieved from K\u00e7, and p is the prompt generated from the task-specific knowledge\nkp \u2208 Kp by Meta-Prompt agent. The Operational Agent processes these inputs and generates the corresponding output\ny. During the learning phase, these outputs are then evaluated through the feedback mechanism, which will be further\ndiscussed in section 2.5. A high level summary of entire process is illustrated in fig. 4(b).\nAs the system processes more tasks, the feedback mechanism provides essential information that helps the operational\nagent refine its decision-making process. This feedback allows the agent to learn from past iterations, progressively\nimproving its performance. The iterative nature of this process ensures that the output y converges toward the expected\nresult over time, either through supervised feedback in cases where labels are available or by aligning with the\noverarching goal G in unsupervised tasks. This feedback-driven refinement is crucial for enabling the CLU to handle\na wide range of tasks and continuously update its knowledge spaces, leading to better performance over time. The"}, {"title": "2.3 Knowledge Management Unit: Goal-Aligned Knowledge Transformation and Retrieval", "content": "The use of external 'scratchpads' for intermediate computations has been shown to significantly improve the ability\nof language models to perform multi-step reasoning [25]. In a similar vein, our knowledge spaces act as dynamic\nscratchpads, providing intermediate reasoning capabilities with the added advantage of continual refinement and long-\nterm retention. The Knowledge Management Unit (KMU), as a central innovation within our framework, orchestrates\nthis dynamic knowledge storage and retrieval process through goal-oriented transformations, thereby elevating the utility\nof these knowledge spaces beyond static intermediates. Rather than serving as a traditional static repository, the KMU\ndynamically aligns stored information with overarching objectives, including the main goal, the retrieval goal, and the\nstorage goal. This alignment ensures that knowledge within the KMU is not only relevant but actively processed to\nserve the specific needs of both current and future tasks. Importantly, the KMU's design is general enough to be applied\noutside of the CLU framework for tasks that require dynamic retrieval-augmented systems or continuous learning\nenvironments. As depicted in fig. 4(a), the KMU uses specialized agents to handle data transformation, retrieval, and\npruning, ensuring that the system remains efficient and relevant.\nWhen new data is introduced into the system, it is not immediately stored as raw input. Instead, the Knowledge\nAlignment Agent plays a crucial role in processing and aligning the data with the defined goals. This agent transforms\nthe raw input into a more compact and relevant format that is optimized for future retrieval. The process includes\nextracting relevant information, adding tags aligned with the system's main goal, and modifying the data as needed to\nensure it fits the requirements of the storage goal. Formally, this transformation can be expressed as:\n$X_{aligned} = T(x, G_m, G_s)$\nwhere T(\u00b7) represents the transformation function applied by the Knowledge Alignment Agent, Gm and Gs represent\nthe main, and storage goals, respectively, and Xaligned is the aligned knowledge ready for storage. This knowledge is\nthen stored in the respective knowledge spaces, either the GKS (Kg) or the PKS (Kp), depending on the nature of the\ninformation.\nThe KMU operates by first accepting raw input data. However, unlike conventional knowledge bases, it does not simply\nstore this data as is. Instead, the Knowledge Alignment Agent processes the data according to the main goal and the\nstorage goal, aligning the raw input to ensure that it fits within the broader purpose of the system. This transformation\nis guided by the objectives set for the system, ensuring that knowledge is not only stored but refined and optimized for\nfuture retrieval. Mathematically, the KMU can be viewed as a function:\n$KMU = f(G_m, G_r, G_s, x) \u2192 K_g, K_p$\nwhere Gm, Gr, and Gs represent the main, retrieval, and storage goals, respectively, and x is the raw input data. The\nresulting output is the processed and aligned knowledge stored in the GKS (Kg) or the PKS (Kp), depending on the\ntype of information.\nIn the CLU framework, we utilize two distinct KMUs. One is used to store broad, domain-specific knowledge that can\ngeneralize across multiple tasks, known as the GKS (Kg), while the other is specifically tuned for task-specific insights\nand serves as the PKS (Kp). The GKS acts as a foundation for reasoning and can be applied broadly across tasks. It\nretrieves domain-level information directly through the retrieval function:\n$k_g = R_G(K_G, G_r, t)$\nwhere RG() extracts the relevant general knowledge based on the retrieval goal Gr and the task t. The PKS, on the\nother hand, stores information tailored to task-specific prompts and adjustments. It is retrieved using:\n$k_p = R_p(K_p, G_r, t)$"}, {"title": "2.4 Feedback Mechanism and Knowledge Update", "content": "The core strength of the CLU framework lies in its ability to adapt and improve continuously through feedback. Rather\nthan relying on static knowledge or manually curated updates, CLU dynamically evolves by incorporating feedback\nsignals from task performance. This feedback not only informs the system about the correctness of its outputs but also\nprovides actionable insights that guide the refinement of its internal knowledge stores. The flow of this feedback process,\nfrom task execution to knowledge storage, is illustrated in fig. 3. In CLU the feedback mechanism starts by analyzing\nthe output of the task generated by the Operational Agent (A0). Depending on whether the task is supervised or\nunsupervised, the system compares the output y to an expected output y* (in supervised cases) or evaluates performance\nmetrics aligned with the main goal G (in unsupervised cases). This comparison is handled by the Response Comparison\nAgent, which determines whether the task was successfully completed. Formally, the comparison can be expressed as:\n$c = A_{GF}(y, y^*, t)$\nwhere c represents the feedback comparison result, y is the generated output, and y* is the expected result (if available).\nFeedback Agents\nThe Response Comparison Agent routes the feedback c (from eq. (9)) to one of two agents: the Positive Feedback Agent\n(APF) or the Negative Feedback Agent (ANF). These agents serve distinct roles in reinforcing successful patterns and\nidentifying areas for improvement, respectively.\nPositive Feedback Agent (APF): When the task performance aligns with expectations, the Positive Feedback Agent is\ntriggered. Its purpose is to amplify the patterns that led to success, reinforcing the system's knowledge and prompting it\nto prioritize similar strategies in the future. This reinforcement helps the system store successful task patterns in the\nKMS.\nNegative Feedback Agent (ANF): Conversely, if the task output deviates from expectations, the Negative Feedback\nAgent steps in to identify errors. It analyzes where the system went wrong and refines the knowledge and task-specific\nstrategies stored in the KMS to correct the issue. This ensures that the system learns from its mistakes and adapts\naccordingly.\nBoth of these agents operate based on the feedback signal f, which is determined by the comparison result c:\n$f =\\begin{cases} A_{PF}(c, y, t), & \\text{if } c \\in C_{success} \\\\ A_{NF}(c, y, t), & \\text{if } c \\in C_{failure} \\end{cases}$\nHere, c determines whether the system receives positive or negative feedback, depending on how the output is evaluated.\nIn the unsupervised scenario, where no ground-truth output y* is available, a General Feedback Agent (AGF) is\nemployed to evaluate how well the generated output y aligns with the overarching goal G, providing a qualitative\nfeedback signal for knowledge refinement.\nKnowledge Extraction and Insight\nOnce feedback is processed, the Knowledge Insight Agent (AKI) plays a pivotal role in extracting actionable information\nfrom the feedback and routing it to the relevant knowledge spaces. The agent synthesizes new knowledge from the\nfeedback signal f and integrates this knowledge into the system's KMS. The extracted knowledge, denoted as knew, is a\ndirect result of analyzing both successful and unsuccessful task completions:\n$k_{new} = A_{KI}(f, t, y)$\nThe new knowledge knew is then added to either the GKS (Kg) or the PKS (Kp), depending on its nature:\n$K_G = U_G(K_G, k_{new}), K_p = U_p(K_p, k_{new})$\nHere, UG and Up represent the update functions for the general and PKSs, respectively. This process ensures that the\nsystem continuously refines its knowledge based on the feedback it receives.\nKnowledge Pruning Mechanism\nAs CLU processes more tasks, irrelevant or outdated knowledge may accumulate in the knowledge spaces. To prevent\nthis, the system employs a pruning mechanism, which ensures that only relevant knowledge is retained. The pruning\nprocess is triggered periodically, after a set number of iterations (n), and is based on a history of feedback signals Fn.\nThis pruning is crucial for maintaining the efficiency and accuracy of the knowledge retrieval process."}, {"title": "2.5 Operational Dynamics: Learninng and reasoning", "content": "The operational dynamics of the Composite Learning Unit (CLU) encompass two distinct but closely related processes:\nthe learning phase and the reasoning phase. These phases are not separate in a strict sense; instead, reasoning is\nan integral part of learning, making the system inherently adaptable. During the learning phase, the CLU refines its\nknowledge through continuous interaction, leveraging feedback mechanisms to update its knowledge spaces. This\nallows for ongoing improvement, aligning knowledge to better meet the overarching goal across diverse tasks. In\ncontrast, the reasoning phase focuses on efficiently utilizing the current state of knowledge to perform task-specific\nactions, generating outputs without modifying internal representations. As shown in fig. 3, reasoning is a subset of\nthe broader learning process, meaning that the CLU can dynamically transition between reasoning and learning as\nneeded. This capability enables the system to perform real-time reasoning while maintaining the option to incorporate\nlearning whenever a decrease in performance or a knowledge gap is detected. The flexibility to invoke learning during"}, {"title": "3 Discussion", "content": "The CLU's training dynamics introduce a novel approach to learning, emphasizing continuous knowledge refinement\nthrough feedback-driven adaptation rather than parameter tuning via weight updates. In the learning phase, CLU\nintegrates knowledge from both general and task-specific contexts, adjusting these dynamically to enhance task\nperformance over time. Unlike traditional deep neural networks (DNNs) that rely on backpropagation, where \u22060 =\n-70L(0) [37], the CLU refines its knowledge through iterative reasoning. Notably, while CLU's approach diverges\nfrom traditional DNNs, the underlying large language models (LLMs) used in CLU are still trained using conventional\nbackpropagation methods. Here, we leverage these LLMs as a base and \"dress\" them to make them capable learners,\nenabling CLU to adapt to new tasks through continuous refinement. Instead of static parameter (0) optimization, the\nsystem continuously adjusts its GKS (Kg) and PKS (Kp) through feedback, ensuring that both generalizable and\ntask-specific knowledge are progressively enhanced (??). The reasoning phase operates differently, focusing solely on\ngenerating outputs from the current state of knowledge without further refinement, thereby facilitating efficient and\ndirect response to new tasks (section 3).\nAt the core of the CLU framework are base reasoners, specifically in current work, large language models (LLMs),\nwhich provide foundational reasoning capabilities [3, 26]. The learning framework revolves around these reasoning\nagents, where the sophistication of the underlying reasoners directly influences the learning efficacy and adaptability\nof the entire system. The more advanced the base reasoning capabilities, the more effective the learning and task\nperformance of the unit. While this work leverages LLMs for reasoning, the conceptual framework is inherently general\nand can be extended to any future reasoning systems, including multimodal models involving language, speech, vision,\nor even symbolic reasoning models[46] that utilize logic-based inference mechanisms such as deduction, induction, and\nabduction. This adaptability makes the CLU a highly flexible learning architecture, capable of leveraging different\nreasoning paradigms to evolve through feedback. By distinguishing between the learning and reasoning phases, the CLU\nallows for continuous knowledge-driven adaptation, ensuring that it remains responsive and progressively improves\nbased on task-specific interactions. In the following sections, we delve deeper into how inference operates as an\nactive reasoning process, detailing its role in continuous adaptation, computational efficiency, and dynamic knowledge\nmanagement across diverse use cases.\nReasoning as Continual Learning: Unlike traditional deep learning models, where inference is merely a forward\npass through fixed parameters, reasoning in CLU incorporates continual learning by directly integrating feedback into\nits operational cycle. Each reasoning step not only generates an output but also contributes to refining knowledge\nspaces (KG and Kp), allowing real-time adaptation without weight updates (section 2.5). This makes reasoning in CLU\ninherently adaptive, unlike static models that require costly retraining. By dynamically refining knowledge based on"}, {"title": "4 Illustrative Example of Adaptive Learning in Cryptographic Reasoning", "content": "The task of evaluating an intelligent system's adaptive reasoning capabilities requires a problem that extends beyond\nsimple pattern recognition. We sought a problem that would not only test the system's ability to recognize intricate\nrelationships but also demand continuous adaptation through iterative refinement. Such a problem should require\nreasoning at multiple levels\u2014extracting rules, hypothesizing solutions, learning from errors, and refining strategies over\ntime-mirroring the way intelligent agents operate in real-world scenarios. To this end, we designed the cryptographic\nproblem described in Problem 1, which involves deducing an encoding rule from a limited set of examples. While\ntheoretically straightforward, as we will demonstrate, the underlying base reasoners struggle to solve this problem\neffectively, making it a suitable challenge for assessing the capabilities of our proposed Composite Learning Unit.\nAs discussed in section 1, a system grounded in Constructivism, Active Inference, and Information Foraging is well-\nsuited for this challenge. Constructivism supports iterative knowledge building, allowing CLU to refine its understanding\nof the encoding rule with each new example. Active Inference provides a framework for reducing uncertainty through\nhypothesis generation and testing, which is crucial for inferring complex patterns. By iteratively seeking and verifying\nrules, CLU actively adapts to new tasks. This problem aligns with CLU's core objectives, directly testing its capacity\nfor dynamic adaptation, abstraction, and continual learning-qualities that traditional static models lack.\nIn the following evaluations CLU uses GPT-40-mini model as the underlying base LLM model for reasoning. The\nevaluation involved generating a dataset of n + 150 sentences, where n represents the number of shots in \"n-shot\nlearning,\" varying between 1 and 5. The first n examples were used to establish the encoding rule, while the remaining\n150 sentences were utilized for testing the performance. The dataset was constructed with diverse sentences to\navoid any contextual dependencies, ensuring that the problem required genuine reasoning abilities rather than simple\nmemorization.\nThe results of the baseline model's Input-Output (IO) and Chain-of-Thought (CoT) enhanced reasoning are summarized\nin fig. 5. In the IO case, the GPT-40-mini model was directly prompted with the new sentence after being given"}, {"title": "5 Conclusion", "content": "In this work, we introduced the Composite Learning Unit (CLU), a framework designed for dynamic, feedback-\ndriven learning that moves beyond traditional parameter optimization approaches. By iteratively refining both general\nand task-specific knowledge spaces, CLU has demonstrated a unique capacity for continual adaptation, successfully\nextracting abstract patterns in challenging scenarios that conventional LLMs struggled to solve. Unlike static deep\nlearning models, CLU continuously incorporates feedback in real time, progressively enhancing its understanding and\nperformance through iterative reasoning. However, the current serial nature of the learning process presents a key\nlimitation, highlighting opportunities for optimization with cleverer learning routines and parallelization strategies.\nAdditionally, future work could explore various directions, such as implementing more nuanced knowledge retrieval [8,\n39] mechanisms, integrating modular tools, or even replacing the Operational Agent with more complex, goal-specific\nmulti-agent systems, given CLU's inherent modularity. This modularity also allows CLU to be tailored to focused\napplications, where elements like the KMU could be augmented with episodic or hierarchical memory for more\nstructured querying, further enhancing its adaptability.\nMoreover, Composite Learning Systems comprising multiple CLUs represent an active area of extension, offering the\npotential to tackle even more sophisticated tasks through collaborative learning and shared knowledge refinement. The\nbroader vision of CLU aligns with the growing consensus in the community on scaling inference compute[16, 42, 1] to\nenhance reasoning capabilities while contributing further by integrating composite systems for greater adaptability and\ncooperation. This adaptability enables CLU to address a wide range of challenges, such as continual learning, knowledge\ndistillation, and personalized AI-tasks that traditionally require specialized machine learning architectures-while also\nopening the door to many other unexplored applications. These areas represent just a few examples, and future work\nwill continue to explore and expand CLU's capabilities, demonstrating its versatility for advancing complex real-world\napplications.\nOur system combines basic reasoning agents with the capacity to actively learn from world knowledge. This integration,\nmuch like reasoning augmentation techniques such as Chain-of-Thought, enhances reasoning abilities but with an\nadded dimension: the ability to learn from both past mistakes and successes. Whether this approach ultimately leads to\nscalable reasoning remains an open question that warrants further exploration."}]}