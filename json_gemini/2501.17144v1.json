{"title": "FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data", "authors": ["Deren Lei", "Yaxi Li", "Siyao Li", "Mengya Hu", "Rui Xu", "Ken Archer", "Mingyu Wang", "Emily Ching", "Alex Deng"], "abstract": "Prior research on training grounded factuality classification models to detect hallucinations in large language models (LLMs) has relied on public natural language inference (NLI) data and synthetic data. However, conventional NLI datasets are not well-suited for document-level reasoning, which is critical for detecting LLM hallucinations. Recent approaches to document-level synthetic data generation involve iteratively removing sentences from documents and annotating factuality using LLM-based prompts. While effective, this method is computationally expensive for long documents and limited by the LLM's capabilities. In this work, we analyze the differences between existing synthetic training data used in state-of-the-art models and real LLM output claims. Based on our findings, we propose a novel approach for synthetic data generation, CG2C, that leverages multi-hop reasoning on context graphs extracted from documents. Our fact checker model, FactCG, demonstrates improved performance with more connected reasoning, using the same backbone models. Experiments show it even outperforms GPT-4-o on the LLM-AGGREFACT benchmark with much smaller model size.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown remarkable progress in generating fluent and coherent text across domains. They are capable of generating text conditioned on documents, leading to various applications, such as summarization , retrieval augmented generation , code completion , etc. However, one of the major challenges faced by these models is the tendency to generate hallucinated statements that are factually incorrect or not grounded in the provided document"}, {"title": "2 Related Work", "content": "Fact-Checking Methods. There are two main approaches to detect hallucinations without altering LLM architectures. One relies on LLMs evaluating their own outputs, like SelfCheckGPT , or leveraging Chain-of-Thought strategies, as seen in G-Eval , CoVe , and CoNLI . The other focuses on training cost-efficient factuality classifiers. SummaC adapts NLI models for document-level factual checks, while QAFactEval uses Question Answering (QA) metrics. FactCC predicts summary consistency, and AlignScore trains on large-scale unified NLI data. MiniCheck generates training data using LLMs to label and outperforms previous work on human-annotated real LLM hallucination benchmark LLM-AGGREFACT . Our work continue the study on improving fact-checking models by enhancing LLM-generated data complexity and controllability.\nFact-Checking Data. Several studies focus on creating datasets for hallucination detection, either for training or benchmarking. AGGREFACT evaluates factual consistency in summarization (CNN/DM, XSum), while TofuEval targets dialogue summarization. WiCE is a textual entailment dataset based on Wikipedia claims. REVEAL benchmarks Chain-of-Thought reasoning in open-domain QA, and ClaimVerify evaluates correctness and groundedness in generative search engines' responses. FactCheck constructs a document-level factuality benchmark, while EXPERTQA focuses on long-form QA with verified attributions. LFQA assesses LLM-generated sentences for factual support by the documents that are either retrieved by humans, models, or randomly selected. RAGTruth analyzes word-level hallucinations in various domains and tasks within the standard retrieval-augmented generation frameworks for LLM applications. HaluEval"}, {"title": "3 Problem Formulation", "content": "Given a document doc and a claim c, we consider c to be grounded in doc if a generic reader would affirm the statement \"According to doc, c is true\". Conversely, c is ungrounded with respect to doc if it conflicts with or cannot be verified against doc. Our objective is to model a function to detect un-grounded hallucinations in LLM-generated claims against documents at the sentence-level.\nWe define grounded as 1 and ungrounded as 0. M is a classifier that first maps each (doc, c) pair into a confidence score of judging c is grounded in doc, formally defined as M(doc, c) \u2208 [0,1]. We then use a threshold to convert confidence score into a binary prediction\n$$y = P(M(doc, c) \\geq \\theta | doc)$$\nwhere \u03b8 is the threshold we can select.\nAs most recent grounded factuality detection industrial applications now support dynamic threshold tuning , we consider the threshold to be dynamically adjustable to its optimal level for per-scenario detection and follow the past work."}, {"title": "4 Analysis on Multi-Hops in Claims", "content": "In this section, we conduct a proof of concept experiment on the multi-hop in LLM-generated claims. We try to answer the following two research questions (RQ):\nRQ1: To what extent do LLM-generated claims, based on grounding documents, require multi-hop reasoning for fact verification?\nRQ2: To what extent do current state-of-the-art synthetic claims, based on grounding documents, require multi-hop reasoning for fact verification?"}, {"title": "4.1 Analysis Setup", "content": "LLM Claims. To answer RQ1, we select the RAGTruth validation dataset from the LLM-AGGREFACT benchmark for analysis, as it contains three recognized tasks with RAG settings: Question Answering (QA), Data-to-text Writing (Data2Text), and News Summarization (Summ). It also leverages six different LLMs to generate completion responses. We analyze each generation task separately. To preserve the original LLM completion format, we do not further decompose the claim into atomic claims. For each task, we deduplicate the documents in the entire set and randomly select 500 documents and their corresponding document-claim pairs for analysis to reduce the computational cost.\nSynthetic Claims. To answer RQ2, we select MiniCheck's D2C synthetic claims for analysis as the classifier model trained on it achieves the best result on LLM-AGGREFACT. Same as RAGTruth claims, we do not further process the D2C claims and analyze them on a sentence-level basis as provided. We conduct analysis on the entire dataset without random sampling.\nMulti-hop Context Sub-graph Extraction. As the LLMs generate claims conditioned on grounding documents, they are not always decontextualized and may contain implicit reasoning chains. To extract the complete reasoning chain of each claim for both datasets, we leverage LLMs in the following steps:\nFirstly, we construct the context graph G from the grounding document doc. We follow GraphRag's prompt-based approach to extract (entity, entity, relation) triples from doc, where the relation is a short sentence"}, {"title": "4.2 Experiment Settings", "content": "We use GPT-4-o for context graph extraction and context sub-graph mapping with claims. We remove data with ill-formatted GPT outputs in both steps to simplify the post-processing. This experiment is only conducted on positive samples in every dataset because ungrounded claims may take any form and their reasoning chain may not exist in the source document's graph."}, {"title": "4.3 Results and Analysis", "content": "We show multi-hop extraction results in Table 1. Synthetic data in MiniCheck's D2C dataset lean towards claims with fewer hops, while LLM-generated claims in RAGTruth tend to have higher ratios of 2-hop and 3-hop claims. We even found non-negligible amounts of claims with 4 or more reasoning hops in RAGTruth across all tasks, which are very limited in D2C. We believe bridging this gap is a non-trivial task and we hypothesize that teaching the model to learn multi-hop inference may improve the model's overall capability for hallucination detection."}, {"title": "5 Improving Model with Context Graph to Multi-hop Claim (CG2C)", "content": "In this section, we study whether we can improve the state-of-the-art grounded factuality models with synthetically generated data via context graph to multi-hop claims (CG2C). Specifically, the study follows two steps, each addressing a research question:\nRQ3: Can we improve grounded factuality models by generating synthetic data leveraging large-scale public multi-hop datasets?\nRQ4: Can we improve grounded factuality models by generating synthetic multi-hop data leveraging documents only?"}, {"title": "5.1 CG2C from Multi-hop QA", "content": "In this section, we study RQ3. We leverage publicly available multi-hop question answering (MHQA) datasets to synthetically construct grounded factuality data for model training, namely CG2C-MHQA. Specifically, we use HotpotQA and Musique , where each data sample contains a source document doc, a question q, an answer ans and supporting sentences docsupport, which is a subset of doc that provides evidence for the answer. We leverage LLMs to convert (ans, q) to a single declarative statement. We define the converted statement as claim c.\nHotpotQA. We leverage the train-hard split in HotpotQA to ensure each data sample requires complex multi-hop reasoning across multiple paragraphs. We treat the (doc, c) pair as a positive sample. We then randomly select positive samples to corrupt them into negative samples by following the steps below. Firstly, since docsupport is provided, we extract Gs from docsupport and then leverage the (Gs, c) pair to get a context sub-graph Ge, using the same strategy described in Section 4.1. Secondly, from the Ge, we randomly select a triple and leverage LLMs to remove the relation between entities in the triple from doc to form docneg. Finally, the (docneg, c) pair forms a negative sample. Inspired by , we further reduce the dataset size by leveraging RoBERTa-large trained on the MultiNLI dataset, to select difficult samples. As the NLI model predicts each sample from {Entailment, Contradiction, Neutral}, we define a mapping function f(\u00b7) for the model output y as:\n$$f(y) = \\begin{cases} Grounded & \\text{if } y = \\text{ Entailment}, \\\\ Ungrounded & \\text{if } y \\in \\{\\text{Contradiction, Neutral}\\} \\end{cases}$$\nWe remove data that ROBERTAMNLI predicts correctly.\nMusique. We leverage 3-hop and 4-hop data from the Musique dataset to diversify from HotpotQA where most of the data is 2-hop. The (doc, c) pairs with c converted from unanswerable (q, ans) pairs are treated as negative samples and answerable pairs are treated as positive samples. Similar to HotpotQA, we remove data that RoBERTAMNLI predicts incorrectly.\nMerge. We merge synthetically constructed data from HotpotQA and Musique into CG2C-MHQA."}, {"title": "5.2 CG2C from Documents", "content": "In this section, we study RQ4. We further investigate the possibility of leveraging any document to directly generate synthetic training data, namely, CG2C-Doc. We propose the following steps to generate harder and more diverse data with fine-grained control:\nFirstly, we extract a context graph from the given document doc following the same approach as in Section 4.1. Secondly, after filtering out cyclic graphs, we programmatically extract sub-graphs"}, {"title": "5.3 Proposed Fack Checker: FactCG", "content": "Model Backbones and Synthetic Data Baseline. We follow MiniCheck training data and the two-stage training process as the synthetic data training baseline. We also select the same model backbones as MiniCheck for easier"}, {"title": "6 Experiments", "content": "In this section, we compare FactCG models with other approaches on grounded factuality datasets."}, {"title": "6.1 Datasets", "content": "We leverage LLM-AGGREFACT benchmark , which consists of real hallucinations generated from recent language models. It includes 11 datasets, namely AGGREFACT-CNN, AGGREFACT-XSum , TofuEval-MediaS, TofuEval-MeetB , WiCE , REVEAL , ClaimVerify , FactCheck , EXPERTQA , LFQA and RAGTruth"}, {"title": "6.2 Experiment Setup", "content": "Synthetic Data Construction. We use GPT-4-o for the CG2C data generation pipeline described in Section 5.1 and Section 5.2. The number of synthetic data we generated from each source is shown in Table 2."}, {"title": "7 Evaluation on Connected Reasoning", "content": "As QA models suffer from disconnected reasoning, they leverage exploited dataset artifacts to produce correct answers instead of connecting information across multi-hops. We conduct a similar connected reasoning (CoRe) analysis on the multi-hop reasoning chain of trained grounded factuality models similar to . We would like to answer the research question:\nRQ5: Does FactCG has more connected reasoning than models not trained on CG2C datasets?"}, {"title": "7.1 CoRe Dataset Construction", "content": "We leverage WiCE in the LLM-AGGREFACT benchmark as it provides the document-claim-evidence triples in the forms of (doci, ci, Ei) where Ei = {Eil, Ei2, .., Eij, ...}. Each evidence Eij is a unique minimum independent sentence set Eij \u2282 doci sufficient to make a grounded factuality judgement. Eij = {eij, ej, ..., ej, ...}. Eij may share the same evidence sentences. We define E as the union of all evidence sets such that Eij \u2282 \u0415.\nWe filter positive cases in the WiCE validation and test sets that contain more than one evidence sentence in each Eij. We treat these as the positive set that requires connected reasoning across sentences with sufficient context.\nTo synthetically construct negative cases, for each positive case, we randomly select evidence sentences e in E and remove their appearance in the grounding document such that each Eij has one evidence removed. In this case, the claim cannot be verified with the corrupted document through any Eij. We treat this negative set as minimum disconnected reasoning set with insufficient context.\nFor each ci we have a pair of positive and negative data (doci, ci) and (doci\\e\u2217j, ci), where i \u2208 I and |I| is the total size of pairs in the constructed dataset. For a grounded factuality model M, we define the accuracy via connected reasoning as:\n$$Accuracy_{CoRe} = \\frac{\\Sigma(M(doc_i, c_i) > \\theta) \\cap (M(doc_i\\setminus e^*_j, c_i) < \\theta)}{|I|}$$\nWe also define the proportion of correct predictions that were made via connected reasoning as:\n$$Precision_{CoRe} = \\frac{\\Sigma(M(doc_i, c_i) \\geq \\theta) \\cap (M(doc_i\\setminus e^*_j, c_i) < \\theta)}{\\Sigma(M(doc_i, c_i) \\geq \\theta)}$$"}, {"title": "7.2 Experiment Setting", "content": "We run AlignScore, MiniCheck models along with FactCG trained on the same backbone models for comparison. We select the same threshold used for benchmark LLM-AGGREFACT evaluation on the WiCE dataset for each model."}, {"title": "7.3 Results and Analysis", "content": "We show the CoRe evaluation results in Table 6. We observe consistent performance across the RBT, DBT and FT5 backbones. For RBT, FactCG achieves superior PrecisionCore while maintaining comparable AccuracyCore relative to MiniCheck. In the cases of FT5 and DBT, FactCG outperforms MiniCheck in terms of both AccuracyCoRe and Precisioncore. It shows that FactCG leverages more on the effective multi-hop reasoning chain across multi-hops than dataset artifacts. However, we still observe that half of the reasoning can be categorized as disconnected reasoning on both models. It demonstrates room for improvement in future trustworthy factuality models' reasoning process."}, {"title": "8 Conclusion", "content": "In this paper, we investigated the difference between state-of-the-art synthetic generated claims and real LLM-generated claims. To fill the gap, we proposed a new synthetic data generation approach, CG2C, that leverages the context graph to generate complex multi-hop claims without relying on LLMs to decide data labels. Our Fact Checker FactCG leveraging this generated data achieves state-of-the-art performance compared with models of similar parameter size and even outperforms GPT-4-o, which we used to construct the CG2C dataset."}, {"title": "9 Limitations", "content": ""}, {"title": "9.1 Graph Extraction Quality", "content": "We find it hard to define the granularity of the extracted triples. The simplest solution is to extract the (noun, verb, noun) triples from sentences, but information after prepositions and conjunctions may be lost. Long sentences and clauses make the task even more complicated. On the other hand, if we extract too many details, the graph will contain a lot of unnecessary edges and become too large to process efficiently. It is not easy to obtain optimal graphs for our use cases. Future research is needed to improve the graph extraction process. In addition, although the graph extraction method can extract the relations between entities and help downstream models make inference, the extraction process itself cannot handle inference within the source document perfectly. For example, when we extract sub-graph for claims from the source documents' graphs in Section 4, we may observe the following two relations from the source document: \"Shop A is open on Monday\" and \"Shop A is open on Tuesday\". However, they cannot be matched exactly with \"Shop A is open two days a week\" in the"}, {"title": "9.2 Data Generation Quantity", "content": "Although our CG2C could support more scalable generation, we mainly extract few samples from one sub-graph to avoid duplication. Exploiting the graph to generate more samples and controlling the ratio of generated samples' hops could be a non-trivial topic to explore."}, {"title": "9.3 Chunking Methodology", "content": "We follow previous work for the chunking methodology and fix the model input to a similar input token length. However, each scenario may have its own optimal chunk size. An area worth exploration for improving generalizable hallucination detection classifiers is developing intelligent methods to segment long documents into chunks based on topic flow."}, {"title": "A.1 Benchmark Statistics", "content": "The statistics of LLM-AGGREFACT benchmark test set can be found in table 7. More details can be found in"}, {"title": "A.2 T-Test Analysis", "content": "We conduct a paired bootstrap test with 100 runs and a p-value < 0.05. The bootstrap sample size is set to 150. Our experiment aims to reject the null hypothesis that the observed differences between methods are due to random chance.\nWe first compare our FactCG-DBT with MiniCheck-DBT on synthetic data generation as they share the same model backbone. We set Ho as the performance on BAcc of FactCG-DBT is no better than that of MiniCheck-DBT. We show results in Table 8.\nOn average, FactCG-DBT demonstrates a significant improvement in performance (BAcc) compared to MiniCheck-DBT with p-value less than 0.05. Therefore we reject the null hypothesis that FactCG-DBT performs no better than MiniCheck-DBT. Notably, in datasets where FactCG-DBT exhibits a relative improvement, the p-values are consistently below 0.05 and align with the observed BAcc gains. This demonstrates the effectiveness of our synthetic training data.\nWe additionally compare FactCG-DBT with MiniCheck-FT5, which is the best-performing fact-checker (<0.8B) on LLM-AggreFact, to prove the significance of our improvement. We set Ho as the performance on BAcc of FactCG-DBT is no better than that of MiniCheck-FT5. We show results in Table 9."}, {"title": "A.3 Results without Threshold Tuning", "content": "We use threshold 0.5 for our FactCG model. We shared our results without specific task threshold tuning on Table 10. Our FactCG-DBT achieves the highest balanced accuracy compared to MiniCheck."}, {"title": "A.4 Model Size and Computational Cost", "content": "We conduct our experiment on Nvidia Quadro RTX 8000 and train all FactCG model backbones with 4 GPUs and inference with 1 GPU. Training costs and the inference costs (including threshold selection) are shown below:\nFor our 0.4B size FactCG-RBT, the training takes approximately 7 minutes and the inference takes about about 65 minutes to complete.\nFor our 0.4B size FactCG-DBT, the training costs approximately 18 minutes and the inference takes about 103 minutes to complete.\nFor our 0.8B size FactCG-T5, the training takes approximately 32 minutes and the inference takes about 238 minutes to complete."}, {"title": "A.5 Scientific Artifacts", "content": "We list the licenses used in this paper: pytorch-lightning (Apache License 2.0), PyTorch (BSD-3), Huggingface Transformers (Apache License 2.0), MiniCheck (Apache License Version 2.0), Align-Score (MIT License), ROBERTa-Large-MNLI (MIT License), hotpotqa (Apache License 2.0), musique (Creative Commons Attribution 4.0 International), OpenAI (Term of use). We adhere to the intended use of all the existing artifacts mentioned in this paper."}, {"title": "A.6 Packages", "content": "We employed the following package to conduct our experiment: pytorch-lightning v2.4.0, PyTorch"}, {"title": "A.7 FactCG-DBT and FactCG-RBT\nInstruction Template", "content": "We show FactCG-DBT and FactCG-RBT Instruction Template in Table 11"}, {"title": "A.8 Prompts and GPT Usage", "content": "We use GPT-4-o to create CG2C datasets. For each positive and negative sample pair generated from HotpotQA, we run four GPT calls in order to generate a claim (Table 15), a context graph (Table 12), a context sub-graph (Table 14), and a negative sample's document (Table 16). For each positive sample in CG2C-MHQA generated from Musique, we run one GPT call to generate a claim (Table 15). To construct CG2C-Doc dataset, we run one GPT call for each unique document (around 1300) in MiniCheck's D2C dataset to extract the context graph (Table 13). In addition, for each positive and negative sample pair we generated, we call GPT two more times to generate the claim (Table 17) and negative sample's document (Table 16)."}]}