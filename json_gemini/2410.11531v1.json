{"title": "AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based Chatbots Utilizing Private Data", "authors": ["Xinjie Zhao", "Moritz Blum", "Rui Yang", "Boming Yang", "Luis M\u00e1rquez Carpintero", "M\u00f3nica Pina-Navarro", "Tony Wang", "Xin Li", "Huitao Li", "Yanran Fu", "Rongrong Wang", "Juntao Zhang", "Irene Li"], "abstract": "Large Language Models (LLMs) have demonstrated capabilities across various applications but face challenges such as hallucination, limited reasoning abilities, and factual inconsistencies, especially when tackling complex, domain-specific tasks like question answering (QA). While Knowledge Graphs (KGs) have been shown to help mitigate these issues, research on the integration of LLMs with background KGs remains limited. In particular, user accessibility and the flexibility of the underlying KG have not been thoroughly explored. We introduce AGENTiGraph (Adaptive Generative ENgine for Task-based Interaction and Graphical Representation), a platform for knowledge management through natural language interaction. It integrates knowledge extraction, integration, and real-time visualization. AGENTiGraph employs a multi-agent architecture to dynamically interpret user intents, manage tasks, and integrate new knowledge, ensuring adaptability to evolving user requirements and data contexts. Our approach demonstrates superior performance in knowledge graph interactions, particularly for complex domain-specific tasks. Experimental results on a dataset of 3,500 test cases show AGENTiGraph significantly outperforms state-of-the-art zero-shot baselines, achieving 95.12% accuracy in task classification and 90.45% success rate in task execution. User studies corroborate its effectiveness in real-world scenarios. To showcase versatility, we extended AGENTiGraph to legislation and healthcare domains, constructing specialized KGs capable of answering complex queries in legal and medical contexts.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in question-answering (QA) tasks (Zhuang et al., 2023; Gao et al., 2024; Ke et al., 2024; Yang et al., 2023), showcasing their prowess in text comprehension, semantic understanding, and logical reasoning (Yang et al., 2024a; Srivastava et al., 2024). These models can process and respond to a wide range of queries with impressive accuracy and context awareness (Safavi and Koutra, 2021). However, LLMs sometimes struggle with factual consistency and up-to-date information (Gao et al., 2024; Xu et al., 2024; Augenstein et al., 2024; Yang et al., 2024b). This is where Knowledge Graphs (KGs) come into play (Edge et al., 2024; Nickel et al., 2015). By integrating KGs with LLMs, we can significantly enhance QA performance (Yang et al., 2024a). KGs provide structured, factual information that complements the broad knowledge of LLMs, improving answer accuracy, reducing hallucinations, and enabling more complex reasoning tasks (Li and Yang, 2023a,b; Pan et al., 2024). This synergy between LLMs and KGs opens up new possibilities for advanced, reliable, and context-aware QA systems (Yang et al., 2024c).\nDespite the potential of KG-enhanced QA systems, current KG tools and query languages face significant challenges (Sabou et al., 2017; Li et al., 2024). Traditional systems like SPARQL and Cypher (P\u00e9rez et al., 2009; Francis et al., 2018), while powerful for data retrieval and analysis, often lack user-friendly interfaces and require specialized technical expertise (Castelltort and Martin, 2018), which restricts their accessibility to a narrow audience of specialists. Moreover, these systems often struggle with contextual understanding and flexibility (Ji et al., 2021), making it difficult to handle nuanced or complex queries. The lack of seamless integration between KGs and natural language interfaces further complicates their use in conjunction with LLMs (Barbon Junior et al., 2024). Additionally, the absence of a unified system architecture among existing tools poses obstacles for developers aiming to innovate or build upon these platforms (Wang et al., 2023). These challenges highlight the need for a more adaptive, user-friendly, and integrated approach to leveraging KGs in QA systems.\nTo address these challenges, we present AGENTiGraph (Adaptive General-purpose Entities Navigated Through Interaction), a novel platform that revolutionizes the interaction between LLMs and KGs using an agent-based approach. AGENTiGraph introduces innovative modules that enable seamless, intelligent interactions with knowledge graphs through natural language interfaces. Key features of our system include:\n\u2022 Semantic Parsing. The interface optimizes user interaction by translating natural language queries (including free-form ones) into structured graph operations, enabling AGENTiGraph to process user requests with enhanced accuracy and speed. It reduces the complexity of interacting with knowledge graphs with an up-to-90% accuracy of automated recognition and realization of user intent tasks, ensuring efficient operation for users of all technical levels.\n\u2022 Adaptive Multi-Agent System. AGENTiGraph integrates multi-modal inputs such as user intent, query history, and graph structure for LLM agents to create coherent action plans that match user intent. Users can modify, pause, or reset tasks at any time, offering flexibility and ease of use. The modular design also allows easy model integration, module replacement and the design of custom agents for specific tasks by developers.\n\u2022 Dynamic Knowledge Integration. The system supports continuous knowledge extraction and integration, ensuring the knowledge graph remains up-to-date. It also offers dynamic visualization capabilities, enabling users to explore and understand complex relationships within the data.\nThese innovations place AGENTiGraph at the forefront of knowledge graph technology. AGENTiGraph is not just a tool but a paradigm shift in how humans interact with and harness the power of knowledge graphs for complex data management and analysis tasks.\nContributions. (1) We implement a powerful natural language-driven interface that simplifies complex knowledge graph operations into user-friendly interactions; (2) We design an adaptive multi-agent system driven versatile knowledge graph management framework, enabling users to perform action on knowledge graphs freely while allowing developers to easily integrate LLMs or multimodal models for creating robust, task-oriented agents; (3) Experiments demonstrate the effectiveness of AGENTiGraph, achieving 95.12% accuracy in user intent identification and a 90.45% success rate in execution, outperforming state-of-the-art zero-shot baselines. User studies further validate the system's efficiency, with participants highlighting its ability to deliver concise, focused answers and effectiveness in complex knowledge management tasks across diverse domains."}, {"title": "AGENTiGraph Framework Design", "content": "AGENTiGraph is designed to provide an intuitive and seamless interaction between users and knowledge graphs (G), the core of which is a human-centric approach that allows users to interact with the system using natural language inputs (q). We employ a multi-agent system to provide intuitive interaction between users and knowledge graphs, leveraging advanced LLM techniques. Each agent specializes in a specific task, collaboratively interpreting user input, decomposing it into actionable tasks, interacting with the knowledge graph, and generating responses (a).\nUser Intent Interpretation. The User Intent Agent is responsible for interpreting natural language input to determine the underlying intent (i). Utilizing Few-Shot Learning (Wang et al., 2020a) and Chain-of-Thought (CoT) (Wei et al., 2024) reasoning, it guides the LLM to accurately interpret diverse query types without extensive training data (Kwiatkowski et al., 2019), ensuring adaptability to evolving user needs.\nKey Concept Extraction The Key Concept Extraction Agent performs Named Entity Recognition (NER) (Wang et al., 2020b) and Relation Extraction (RE) (Miwa and Bansal, 2016) on the input (q). By presenting targeted examples to guide precise extraction, it then maps extracted entities (E) and relations (R) to the knowledge graph by semantic similarity with BERT-derived vector representations (Turton et al., 2021). This two-step process ensures accurate concept linking while maintaining computational efficiency.\nTask Planning. The Task Planning Agent elevates the process by decomposing the identified intent into a sequence of executable tasks (T = {t1, t2,..., tn}). Leveraging CoT reasoning, this agent models task dependencies, optimizes execution order and then generates logically structured task sequences, which is particularly effective for complex queries requiring multi-step reasoning (Fu et al., 2023).\nKnowledge Graph Interaction. The Knowledge Graph Interaction Agent serves as a bridge, translating high-level tasks into executable graph queries. For each task (tk), it generates a formal query (ck), combining Few-Shot Learning with the Re-Act framework (Yao et al., 2023), which allows for dynamic query refinement based on intermediate results, adapting to various graph structures and query languages without extensive pre-training.\nReasoning. Enhancing raw query results (Rk), the Reasoning Agent applies logical inference, which capitalizes on the LLM's inherent contextual understanding and reasoning capabilities (Sun et al., 2024). By framing reasoning as a series of logical steps, it enables flexible and adaptive inference across diverse reasoning tasks, bridging the gap between structured knowledge and natural language understanding.\nResponse Generation. The Response Generation Agent synthesizes processed information into coherent responses, which employs CoT, ReAct, and Few-Shot Learning to orchestrate structured and contextually relevant responses, ensuring that responses are not only informative but also aligned with the user's original query context.\nDynamic Knowledge Integration. The Update Agent enables dynamic knowledge integration, incorporating new entities (Enew) and relationships (Rnew) into the existing graph: G \u2190 GU {Enew, Rnew}. This agent directly interfaces with the Neo4j database, using LLM-generated Cypher queries to seamlessly update the graph structure (Miller, 2013).\nThrough this orchestrated multi-agent architecture, AGENTiGraph achieves a synergistic balance between structured knowledge representation and flexible interaction. Each agent, while utilizing similar underlying LLM technologies, is uniquely designed to address specific challenges in the knowledge graph interaction pipeline. The specific prompt design for each agent are provided in App. A."}, {"title": "System Demonstration", "content": "The AGENTiGraph interface is designed for intuitive use and efficient knowledge exploration, as illustrated in Figure 2. It features a dual-mode interaction paradigm that combines conversational AI capabilities with interactive knowledge exploration.\nThe interface consists of three main components:\n\u2022 Chatbot Mode employs LLMs for intent interpretation and dynamic response construction via knowledge graph traversal. This mode facilitates nuanced query processing, bridging natural language input with complex knowledge structures.\n\u2022 Exploration Mode provides an interactive knowledge graph visualization interface with entity recognition capabilities, supporting conceptual hierarchy navigation and semantic relationship exploration.\n\u2022 Knowledge Graph Management Layer is the interface between the multi-agent system and the underlying Neo4j graph database, utilizing the Neo4j Bolt protocol for high-performance communication with the database and focusing on efficient graph operations and retrieval mechanisms for enhanced user interaction."}, {"title": "Task Design", "content": "To support user interaction with knowledge graphs and their diverse needs in knowledge exploration, AGENTiGraph provides a suite of pre-designed functionalities, inspired by the TutorQA, an expert-verified TutorQA benchmark, designed for graph reasoning and question-answering in the NLP domain. (Yang et al., 2024c). Specifically, AGENTiGraph supports the following tasks currently:\nRelation Judgment: Users can explore and verify semantic relationships between concepts within a knowledge graph and the system would provide detailed explanations of these connections, enriching the graph with contextual information, which aids in developing a deeper understanding of complex knowledge structures and their interdependencies.\nPrerequisite Prediction: When approaching complex topics, AGENTiGraph recommends prerequisite knowledge by analyzing the knowledge graph structure, helping users to identify and suggest foundational concepts and facilitating more effective learning paths and ensuring users build a solid foundation before advancing to more complex ideas.\nPath Searching: This functionality enables users to discover personalized learning sequences between concepts. By generating optimal paths through the knowledge graph, AGENTiGraph helps users navigate from familiar concepts to new, related ideas, tailoring the learning journey to individual needs and interests.\nConcept Clustering: Users can explore macro-level knowledge structures, which group related concepts within a given domain. By revealing thematic areas and their interrelations, it provides a high-level overview of complex fields, aiding in comprehensive understanding and efficient knowledge navigation.\nSubgraph Completion: This functionality assists users in expanding specific areas of the knowledge graph by identifying hidden associations between concepts in a subgraph, which supports the discovery of new connections and the enrichment of existing knowledge structures, promoting a more comprehensive understanding of the subject matter.\nIdea Hamster: By synthesizing information from the knowledge graph, this feature helps users translate theoretical knowledge into practical applications, which supports the generation of project proposals and implementation strategies, fostering innovation and bridging the gap between abstract concepts and real-world problem-solving.\nAGENTiGraph's flexibility extends beyond these predefined functionalities. Users can pose any question or request to the system, not limited to the six categories described above. The system automatically determines whether the user's input falls within these predefined categories. If not, it treats the input as a free-form query, employing a more flexible approach to address the user's specific needs. Moreover, users with specific requirements can design custom agents or reconfigure existing ones to create tailored functionalities, ensuring that AGENTiGraph can evolve to meet diverse and changing user needs, providing a versatile platform for both guided and open-ended knowledge discovery. In subsequent sections (\u00a75), we also illustrate the system's scalability and expansion capabilities on other domains."}, {"title": "Evaluation", "content": "To assess AGENTiGraph's performance, we conducted a comprehensive evaluation focusing on two key aspects: (1) the system's ability to accurately identify user intents and execute corresponding tasks, and (2) the system's effectiveness and user satisfaction in real-world scenarios."}, {"title": "Dataset and Experimental Setup", "content": "To comprehensively evaluate AGENTiGraph's performance, we developed an expanded test set that addresses the limitations of the original TutorQA dataset, which comprises 3,500 cases, with 500 queries for each of the six predefined task types and an additional 500 free-form queries (\u00a73.2). The dataset generation process involved using LLMs to mimic student questions (Liu et al., 2024), followed by human verification to ensure quality and relevance, allowing us to create a diverse set of queries that closely resemble real-world scenarios (Extance, 2023). Detailed prompts and example cases used in this process can be found in App. B.\nOur evaluation of AGENTiGraph focuses on two key aspects: Query Classification: We assess the system's ability to correctly categorize user inputs into the seven task types (six predefined plus free-form), measured by accuracy and F1 score. Task Execution: We also evaluate its practical utility by testing whether it can generate valid outputs for each query, which is quantified through an execution success rate."}, {"title": "User Intent Identification and Task Execution", "content": "Table 1 presents the results of our experiment. We evaluated AGENTiGraph's performance against zero-shot baselines using several state-of-the-art language models, which demonstrate AGENTiGraph's significant performance improvements across all evaluated models and metrics. GPT-40, when integrated with AGENTiGraph framework, achieves the highest performance, with a 95.12% accuracy in task classification, 94.67% F1 score, and a 90.45% success rate in task execution, which represents a substantial improvement over its zero-shot counterpart. These improvements are consistent across all model sizes, with even smaller models like LLaMa 3.1-8b showing marked enhancements, suggesting that AGENTiGraph's agent-based architecture effectively augments the capabilities of underlying language models, potentially offering more efficient solutions for complex knowledge graph interactions.\nThe performance gap between zero-shot and AGENTiGraph implementations narrows as model size increases, indicating that larger models benefit less dramatically from the AGENTiGraph framework. However, the consistent improvement across all models underscores the robustness of AGENTiGraph's approach in enhancing knowledge graph interactions. Notably, there is a consistent gap between classification accuracy and execution success rates across all models, suggesting that while AGENTiGraph framework excels at identifying the correct task type, there's room for improvement in task execution. The gap is smallest for the most advanced models (GPT-40 and Gemini-1.5 pro), indicating that these models are better equipped to bridge the understanding-execution divide."}, {"title": "User Feedback and System Usability", "content": "To evaluate the real-world effectiveness and user satisfaction of AGENTiGraph, we conducted a comprehensive user study involving participants with varying levels of expertise in knowledge graph systems. Participants interacted with the system within the domain of natural language processing (NLP) and provided feedback on their experience. We collected qualitative feedback from 50 user interactions with AGENTiGraph, compared to ChatGPT-40 2. Users generally found AGENTiGraph's responses to be more concise. Specifically, 32 queries highlighted its ability to deliver shorter, more focused answers. However, in 5 queries, users noted that AGENTiGraph's responses were incomplete or missing key details, especially for more complex tasks, where ChatGPT's more detailed answers is preferred. Additionally, 4 queries indicated that AGENTiGraph misunderstood the question or provided incorrect answers. Despite the limitations, user satisfaction with AGENTiGraph remained high, particularly regarding the efficiency the freedon of knowledge graph interactions. For users familiar with core concepts, the concise responses helped avoid information overload, beneficial in learning or review scenarios.\nWe also analyzed 34 queries in computer vision domain, of which 14 were marked satisfactory, while 20 included suggestions for improvement, needing for more detailed explanations. Users often requested clearer descriptions of concepts like convolutional layers, optical flow, and feature extraction. For example, one suggestion emphasized the importance of explaining how convolutional filters slide across an image to generate feature maps."}, {"title": "Customized Knowledge Graph Extension", "content": "Our system is also extendable to private or personalized data. The code can be found at https://shorturl.at/axsPd. In this section, we showcase its ability to create knowledge graphs in a zero-shot manner within two complex domains: legal and medical.\nUK Legislation Data. The first use case demonstrates the system's ability to generate a KG about the UK Legislation. As a knowledge source, we use the dataset UK Legislation published by Chalkidis et al. (2021). We illustrate a sub-graph generated by our system in Fig. 4 in App. D. Potentially, it may be helpful to answer this question: \"What legislation provides the definition for the 'duty of excise' related to biodiesel, and which Act cites this duty?\" The system will allow users to identify relationships between legal provisions, definitions, and affected statutes.\nJapanese Healthcare Data. The second use case is in the Japanese medical domain based on the MMedC (Japanese) (Qiu et al., 2024) corpus, comprising research and product information about medical treatments and healthcare technology written in Japanese. The small sub-graph shown in Fig. 5 in App. D reveals that chemotherapy, hematopoietic stem cell transplantation, and CAR-T cell therapy are treatments for blood tumors. Furthermore, CAR-T cell therapy is also used to treat non-Hodgkin's lymphoma and hematologic malignancies. For example, such a sub-graph is helpful to answer this question \"What treatments are used to address blood tumors and related hematologic conditions?\"\nFurther details on the datasets, applications, and visualizations are available in App. D."}, {"title": "Conclusion and Future Work", "content": "AGENTiGraph presents a novel approach to knowledge graph interaction, leveraging an adaptive multi-agent system to bridge the gap between LMMs and structured knowledge representations. Our platform significantly outperforms existing solutions in task classification and execution, demonstrating its potential to revolutionize complex knowledge management tasks across diverse domains. Future work will enhance multi-hop reasoning, optimize response conciseness and completeness, and develop continuous learning from user interactions."}, {"title": "Overview", "content": "To comprehensively evaluate AGENTiGraph's performance, we generated an expanded test set consisting of 3,500 queries. This dataset includes 500 queries for each of the six predefined task types and an additional 500 free-form queries. The test queries were generated using Large Language Models (LLMs) to mimic student questions, followed by human verification to ensure quality and relevance.\nIn this appendix, we detail the process of generating these test queries, including the specific LLM prompts used for each task type and the human verification procedures employed to maintain the dataset's integrity."}, {"title": "LLM Prompt Designs for Test Query Generation", "content": "For each task type, we carefully crafted specialized prompts to guide the LLMs in generating appropriate test queries. These prompts were designed to leverage prompt engineering strategies, incorporating clear instructions, relevant examples, and specifying the desired output format. The prompts were constructed to:\n\u2022 Encourage the generation of queries covering a wide range of NLP topics, from foundational concepts to advanced techniques.\n\u2022 Ensure that the language used in the queries is natural and reflects how a student might pose questions to an instructor or mentor.\n\u2022 Include explicit instructions to avoid redundancy and promote diversity in the concepts and relationships addressed.\n\u2022 Utilize examples to illustrate the desired style and format, enhancing the LLMs' understanding of the task.\nBy mdesigning these prompts, we sought to maximize the LLMs' ability to produce queries that are not only challenging and relevant but also varied in content and complexity, which contributes to a robust evaluation framework for AGENTiGraph, allowing us to assess its performance across different types of user interactions."}, {"title": "Human Verification Process", "content": "Following the generation of queries using LLMs, we implemented a comprehensive human verification process to ensure the quality, relevance, and appropriateness of the test dataset. The verification process involved a team of NLP experts and educators who conducted a review of sampled queries. The process comprised several stages to uphold the highest standards of professionalism and academic rigor:\n1. Relevance and Accuracy Assessment: Each query was evaluated to confirm that it directly pertains to NLP concepts and is appropriate for the intended task type. Reviewers checked for correct alignment with the task definitions and ensured that the queries were meaningful within the context of knowledge graph interactions.\n2. Task Classification Validation: We verified that each query was correctly categorized according to the predefined task types.\n3. Clarity and Linguistic Quality Check: Queries were examined for grammatical correctness, clarity, and naturalness. Reviewers ensured that the language used mirrored authentic student inquiries, enhancing the realism and practical applicability of the dataset.\n4. Duplication and Redundancy Elimination: We identified and removed any duplicate or overly similar queries to maintain diversity and breadth in the dataset.\n5. Content Appropriateness Review: The content of each query was scrutinized to avoid any sensitive, inappropriate, or disallowed topics. Reviewers ensured adherence to ethical standards and academic guidelines, guaranteeing that the dataset is suitable for scholarly use.\n6. Inter-Rater Reliability Assessment: To ensure consistency and objectivity in the verification process, multiple reviewers independently evaluated a subset of the queries. The inter-rater agreement was measured, and any discrepancies were discussed and resolved through consensus.\n7. Final Approval and Inclusion: Only queries that passed all the above checks were included in the final dataset. Queries that did not meet the criteria were either revised or discarded.\nBy implementing this human verification process, we ensured that the dataset not only reflects realistic and diverse interactions but also adheres to high standards of academic quality and integrity."}, {"title": "User Feedback Analysis", "content": "We conducted a comprehensive user study involving participants with varying levels of expertise in knowledge graph systems, focusing on the domains of Natural Language Processing (NLP) and Computer Vision (CV). The feedback was collected from 50 user interactions with AGENTiGraph, compared against ChatGPT (GPT-40), and provides valuable insights into the system's performance, user satisfaction, and areas for improvement."}, {"title": "Methodology", "content": "Participants interacted with AGENTiGraph within the domains of NLP and CV, posing various questions and evaluating the system's responses. The feedback was collected and analyzed qualitatively, focusing on the conciseness, accuracy, and completeness of the responses. We also compared AGENTiGraph's performance with that of ChatGPT to benchmark its effectiveness."}, {"title": "Representative Cases", "content": "Selected user feedback:\n\u2022 NLP domain: Table 2,\n\u2022 CV domian: Table 3."}, {"title": "Analysis of User Feedback", "content": "Our user study revealed several key insights into the performance and user perception of AGENTiGraph compared to ChatGPT, particularly in the domains of Natural Language Processing (NLP) and Computer Vision (CV). The feedback highlights both strengths and areas for improvement in AGENTiGraph's responses."}, {"title": "Natural Language Processing Domain", "content": "In the NLP domain, users consistently noted that AGENTiGraph provided more concise responses compared to ChatGPT. This brevity was generally appreciated, especially for users already familiar with core NLP concepts. The concise nature of responses helped avoid information overload, making AGENTiGraph particularly useful for quick reviews or refreshers on NLP topics."}, {"title": "Strengths:", "content": "\u2022 Conciseness: AGENTiGraph excelled in providing succinct explanations for complex NLP concepts. For instance, in explaining the role of preprocessing steps or the differences between modern and traditional NLP models, AGENTiGraph delivered clear, to-the-point responses.\n\u2022 Efficiency: Users appreciated the system's ability to quickly identify and articulate key points, making it efficient for reviewing or understanding core NLP concepts."}, {"title": "Areas for Improvement:", "content": "\u2022 Completeness: In some cases, particularly for more complex or open-ended questions (e.g., \"What are the most complicated fields in NLP?\"), AGENTiGraph's responses were incomplete or missing entirely. This suggests a need for improving the system's ability to handle broader, more abstract queries.\n\u2022 Depth of Explanation: While conciseness was appreciated, some users noted that for certain topics, AGENTiGraph's responses lacked the depth provided by ChatGPT. This was particularly evident in questions about future trends or comprehensive overviews of NLP applications."}, {"title": "Computer Vision Domain", "content": "In the Computer Vision domain, user feedback was more mixed, with a higher proportion of responses requiring improvement or expansion."}, {"title": "Strengths:", "content": "\u2022 Accuracy: For fundamental CV concepts, such as the role of GANs or the importance of data preprocessing, AGENTiGraph provided satisfactory explanations.\n\u2022 Clarity: When AGENTiGraph did provide complete answers, users found them clear and easy to understand."}, {"title": "Areas for Improvement:", "content": "\u2022 Completeness: Several responses were noted as incomplete, particularly for questions about challenges in object detection or differences between supervised and unsupervised methods.\n\u2022 Technical Depth: Users often requested more detailed explanations of technical concepts, such as how convolutional filters work or the specifics of image augmentation techniques.\n\u2022 Practical Examples: Feedback suggested that including practical examples or applications could enhance the explanations, especially for complex topics like feature extraction vs. feature selection."}, {"title": "Overall Analysis", "content": "The user feedback reveals that AGENTiGraph has significant strengths in providing concise, efficient responses, particularly beneficial for users with some prior knowledge seeking quick information or review. This aligns well with the system's design goal of offering focused, knowledge graph-based interactions. However, the feedback also highlights areas where AGENTiGraph can improve:\n1. Balancing Conciseness and Completeness: While brevity is appreciated, there's a need to ensure that responses, especially for complex topics, are comprehensive enough to provide valuable insights.\n2. Handling Abstract Queries: Improving the system's ability to address broad, open-ended questions would enhance its versatility.\n3. Domain-Specific Enhancements: Particularly in the Computer Vision domain, there's a need for more detailed technical explanations and practical examples.\n4. Consistency: Ensuring consistent quality of responses across different types of questions and domains is crucial for user trust and satisfaction."}, {"title": "Conclusion", "content": "The user feedback analysis provides valuable insights into the performance of AGENTiGraph across two important domains in AI: Natural Language Processing and Computer Vision. The system's strength in providing concise, efficient responses is evident, particularly in the NLP domain. This aligns well with its design goal of offering focused, knowledge graph-based interactions. However, the analysis also reveals areas for improvement, especially in handling more complex, open-ended queries and providing deeper technical explanations in specialized domains like Computer Vision. The feedback suggests that while AGENTiGraph's concise responses are appreciated, there's a need to balance this brevity with comprehensive coverage of topics, particularly for more advanced or abstract concepts.\nMoving forward, these insights will be invaluable in guiding the development of AGENTiGraph. Future iterations should focus on enhancing the system's ability to provide more comprehensive responses when needed, improving its handling of abstract queries, and ensuring consistent performance across different domains. By addressing these areas, AGENTiGraph can further solidify its position as a powerful tool for knowledge graph interactions, catering to users with varying levels of expertise and information needs."}, {"title": "Data", "content": "UK Legislation The dataset published by Chalkidis et al. (2021) comprises legislative and regulatory texts sourced from legislation.gov.uk, the official UK government website for accessing legislation, all written in English. The UK government offers a searchable database of all UK laws and regulations, including current and historical statutes, statutory instruments, and amendments. The dataset includes detailed records about, e. g., the UK Public General Acts and UK Local Acts.\nMMedC (Japanese) MMedC (Qiu et al., 2024) is a large-scale multilingual medical corpus developed to enrich LLMs with domain-specific medical knowledge. The dataset is based on multiple sources, and we use a subset derived from open-source medical websites in Japanese. The subset comprises research and product information about medical treatments and healthcare technology written in Japanese. For instance, it contains studies on chemotherapy regimens and information about medical devices."}, {"title": "Knowledge Graph Construction", "content": "The semantic data retrieval component of AGENTiGraph relies on a KG. We build this KG from the textual documents with Graphusion (Yang et al., 2024c). Graphusion is an approach for Knowledge Graph Construction from text, which is based on three steps: i) seed entity extraction, ii) candidate triple extraction, and iii) KG fusion.\nWe provide an easy-to-use command line interface for Graphusion that enables the evaluation of different LLMs on different datasets. The input to Graphusion is a set of domain-specific documents D and a set of relations R with textual descriptions. The relationships should be defined based on the anticipated queries for the knowledge graph. The output of the pipeline is a set of (s, r, o) triples, where r \u2208 R, forming the Knowledge Graph.\nIn the following, we explain how we modified and used Graphusion for our two example use-cases.\nRelation Definition. For each use case, we provide a set of relations together with their associated relation definitions to the knowledge graph construction pipeline. These relations are chosen to capture connections between entities within the domain, aligning with the types of information that domain-expert users are likely to query. In order to obtain these relations, we queried a LLM (latest ChatGPT-40 model) with the following prompt:"}, {"title": "Applications", "content": "KG retrieval of UK Legislation Data. We demonstrate the capabilities of the constructed KG with the following multistep query: \"What legislation provides the definition for the 'duty of excise' related to biodiesel, and which Act cites this duty?\"\nWe extracted the sub-graph relevant to this question that serves as the basis for the answer. The sub-graph is visualized in Fig. 4.\nSo, the answer would reveal that the \"Biodiesel and Bioblend Regulations 2002\" defines \"biodiesel duty\" which is related to \"duty of excise\" as defined by the \"Hydrocarbon Oil Duties Act 1979\" and the \"Oil Act\" cites this duty.\nKG retrieval of Japanese Healthcare Data. We demonstrate the capabilities of the constructed KG with the following multistep query: \"What treatments are used to address blood tumors and related hematologic conditions?\"\nWe extracted the sub-graph relevant to this question that serves as the basis for the answer. The sub-graph is visualized in Fig. 5.\nThe answer reveals that chemotherapy, hematopoietic stem cell transplantation, and CAR-T cell therapy are treatments for blood tumors. Furthermore, CAR-T cell therapy is also used to treat non-Hodgkin's lymphoma and hematologic malignancies."}]}