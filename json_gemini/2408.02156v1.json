{"title": "Calibration-Disentangled Learning and Relevance-Prioritized Reranking for Calibrated Sequential Recommendation", "authors": ["Hyunsik Jeon", "Se-eun Yoon", "Julian McAuley"], "abstract": "Calibrated recommendation, which aims to maintain personalized proportions of categories within recommendations, is crucial in practical scenarios since it enhances user satisfaction by reflecting diverse interests. However, achieving calibration in a sequential setting (i.e., calibrated sequential recommendation) is challenging due to the need to adapt to users' evolving preferences. Previous methods typically leverage reranking algorithms to calibrate recommendations after training a model without considering the effect of calibration and do not effectively tackle the conflict between relevance and calibration during the reranking process. In this work, we propose LEAPREC (Calibration-Disentangled Learning and Relevance-Prioritized Reranking), a novel approach for the calibrated sequential recommendation that addresses these challenges. LEAPREC consists of two phases, model training phase and reranking phase. In the training phase, a backbone model is trained using our proposed calibration-disentangled learning-to-rank loss, which optimizes personalized rankings while integrating calibration considerations. In the reranking phase, relevant items are prioritized at the top of the list, with items needed for calibration following later to address potential conflicts between relevance and calibration. Through extensive experiments on four real-world datasets, we show that LEAPREC consistently outperforms previous methods in the calibrated sequential recommendation. Our code is available at https://github.com/jeon185/LeapRec.", "sections": [{"title": "1 INTRODUCTION", "content": "Calibrated recommendation aims to reflect a user's diverse interests within a recommendation list by maintaining the proportions of various categories observed in past interactions [1, 3, 38, 39]. For instance, if a user has historically watched 70% drama and 30% action movies, calibrated recommendation should suggest a list of movies maintaining a similar genre ratio. This problem differs from studies like [27, 44, 46] that define calibration in terms of probability-based user preference estimation, such as estimating how likely a user will prefer an item. To achieve the calibrated recommendation, two potentially conflicting objectives must be addressed: 1) relevance, which aligns with the user's current preferences, and 2) calibration, which sustains consistency with their long-term category interests. This challenge becomes particularly significant in sequential settings (i.e., calibrated sequential recommendation) where users' category preferences shift over time. These dynamic shifts in preferences are depicted in Figure 1, where the Kullback-Leibler (KL) divergence between category distributions increases as the sequence interval extends, highlighting the intricate challenge of balancing relevance with calibration.\nExisting work on calibrated recommendation focuses on post-processing approaches [1, 38, 39]. Specifically, a recommendation model is first trained to meet the relevance objective; then, the model output is reranked to meet the calibration objective. The difference between these methods lies in reranking, such as greedy (CaliRec [39]), mixed integer programming (MIP [38]), and minimum-cost flow (MCF [1]) algorithms. However, applying calibration during reranking can lead to degradation of accuracy because they do not consider the impacts of calibration during the training phase."}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 Problem Definition", "content": "The problem of calibrated sequential recommendation is defined as follows. Let U, I, and C be the sets of users, items, and categories, respectively. Each item $i \\in I$ is associated with a set of categories $C_i = \\{c_1, c_2, ..., c_N \\}$, where each $c_h \\in C$ represents a category of item i, and N is the number of categories, varying per item. For each user $u \\in U$, we have sequential interactions $S_u = (s_1^u, s_2^u,..., s_T^u)$, where $s_t^u \\in I$ is user u's interacted item at step t, and T denotes the length of the sequence which varies among users. Given sequences $S_u$ of all users $u \\in U$, our goal is to recommend each user an item list $R_u = (r_1^u, r_2^u, ...,r_K^u)$ that is relevant to the user's future needs (i.e., accurate), while reflecting the user's sequential category"}, {"title": "2.2 Calibration Metrics for Sequential Recommendation", "content": "The degree of calibration is measured by comparing the category distributions of items in a user's past interactions and items in their recommended list [39]. Specifically, it is defined as the divergence between these two distributions (i.e., miscalibration), where a lower value indicates superior calibration performance. Steck [39] proposed metrics for miscalibration under various criteria such as whether the user interactions are treated as equally or weighted regarding their recency, as illustrated in Figure 2. In this work, we adopt sequential miscalibration as our calibration metric, specifically tailored for sequential recommendations. This metric is described in the following definition.\nDefinition 1 (Sequential miscalibration): Given user u's sequential interactions $S_u$ and the user's recommendation list $R_u$, the sequential miscalibration $SKL(u)$ is defined as follows:\n$SKL(u) = KL(p||\\~q) = \\sum_{c \\in C}p(c|u) log \\frac{p(c|u)}{\\~q(c|u)},$ \nwhere\n$p(c|u) = \\frac{\\sum_{s_t^u \\in S_u} \\alpha^{T-t}p(c|s_t^u)}{\\sum_{s_t^u \\in S_u} \\alpha^{T-t}},$ \n$q(c|u) = \\frac{\\sum_{r_k^u \\in R_u} p(c|r_k^u)}{|R_u|},$ \n$\\~q(c|u) = (1 - \\beta)q(c|u) + \\beta p(c|u),$ \nKL(\u00b7) indicates the Kullback-Leibler (KL) divergence between two distributions, $T = |S_u|$, and $\u03b1, \u03b2 \u2208 (0, 1)$ are hyperparameters.\nIn Equations (2) and (3), p(c|s) and p(c|r) are the category distributions of items $s_t, r_k \\in I$, respectively. If an item is associated with multiple categories, each category is equally weighted in the"}, {"title": "3 PROPOSED METHOD", "content": "In this section, we describe LEAPREC (Calibration-Disentangled Learning and Relevance-Prioritized Reranking), a novel approach for calibrated sequential recommendation. Figure 3 depicts the overall process of LEAPREC, which consists of two phases: model training phase and reranking phase. During the training phase, LEAPREC employs a sequential recommendation model, such as SASRec [20], as its backbone. It optimizes our proposed calibration-aware learning-to-rank loss, which is designed to disentangle calibration from relevance, enabling the model to estimate accurate personalized rankings regardless of whether calibration is considered. During the reranking phase, LEAPREC applies our proposed relevance-prioritized reranking algorithm. This algorithm adjusts the model's output for each user to reduce miscalibration, while prioritizing the most relevant items in the final recommendations."}, {"title": "3.1 Calibration-Disentangled Learning-to-Rank", "content": "The objective of the training phase is to train a sequential model that can predict the probability a user will interact with an item based on their past interactions. Formally, given a user u's sequence of interactions $S_u = (s_1^u,s_2^u,..., s_T^u)$ where $s_t^u \\in I$ is the interacted item at step t, the model aims to estimate the probability for all items $v \\in I$ at step T + 1:\n$p(s_{T+1}^u = v|S_u).$\nGenerally, sequential models are trained to increase the gap between the relevance scores of interacted items and non-interacted items using pointwise [15], pairwise [35] or setwise [31] losses. Various frameworks, including Markov Chains [10, 36], Recurrent Neural Networks (RNN) [13, 23, 24, 28], Convolutional Neural Networks (CNN) [41, 50], and self-attention mechanisms [20, 29, 40], are effectively used in these sequential models, demonstrating high performance in terms of accuracy.\nLet $f_\u03b8 (u, i, t)$ represent the relevance score between user u and item i at step t, with parameters \u03b8. Existing calibrated recommendation methods [1, 38, 39] often rely solely on post-processing techniques and thus overlook the potential impact of calibration adjustments on the final ranking order. However, it is crucial to integrate calibration directly into the training process to anticipate how rankings might change when calibration is applied. This proactive approach is essential for achieving high performance in both accuracy and calibration. To illustrate the importance of integrating calibration directly into the training process, consider a scenario where relevance scores indicate a preference for item i over item j based on past interactions (i.e., $f_\u03b8 (u, i, t) > f_\u03b8(u, j, t)$). After the training phase, calibration scores may compel the system to rank item j higher than item i. It is difficult to determine whether these items should be reranked, since the degree to which item i is more relevant than item j becomes uncertain when calibration is taken into account. Thus, ensuring that the model can maintain consistent rankings even after calibration adjustments during reranking is crucial for the calibrated sequential recommendation.\nTo address this issue, we propose a calibration-disentangled learning-to-rank, a model-agnostic learning approach. For brevity, let $f_\u03b8 (u, i, t) := r_i^{j,t}$. Suppose user u interacted with item i instead of item j at step t. This interaction indicates that the user prefers item i over item j, even with category preference taken into account. Thus, at the recommendation step t, it is crucial to recommend item"}, {"title": "3.2 Relevance-Prioritized Reranking", "content": "The reranking phase aims to maximize both accuracy and calibration using the trained model $f_\u03b8$. The goal is to recommend user u a list of items $R_u = (r_1^u,r_2^u,...,r_K^u)$ where $r_k^u \\in I$ is the k\u02bcth recommended item. Reranking generates the list by selecting the most suitable items among the candidates. The main challenge in the reranking phase is to measure which item is the best for the user at each step, considering both accuracy and calibration.\nFrom Figure 1, we observe that a user is likely to interact with an item that is associated with a category the user has not preferred before. For instance, a user who predominantly watches action movies might develop an interest in romance movies due to temporal factors. In this case, it is necessary to ensure that items that users like are recommended regardless of their category to satisfy the user's future needs. However, naively using weighted sum [1, 38, 39] may not adequately handle such cases. In the example, the romance movie may have a low overall score despite its high relevance due to the high miscalibration score, since it contrasts with the user's past category preferences. This conflict of relevance and calibration should be treated as a critical issue in the calibrated sequential recommendation, yet it has not been thoroughly addressed in previous works [1, 3, 38, 39].\nTo address this challenge, our reranking strategy prioritizes a user's emerging interests by integrating both relevance and calibration but favoring relevance in the higher ranks of the recommendation list. If we consider that the backbone model $f_\u03b8$ is trained to predict the user's evolving preferences based on a sequential model,"}, {"title": "3.3 Overall Process of LEAPREC", "content": "The overall process of LEAPREC consists of the backbone model training phase and the reranking phase. Algorithm 1 shows how LEAPREC trains the backbone model and reranks the results. Given users' sequential interactions $S_u : u \u2208 U$, LEAPREC returns recommendation lists $R_u: u \u2208 U$ for all users. In lines 1 to 5, LEAPREC trains a backbone model by minimizing the loss in Equation (8) for predefined epochs (e.g., 100); in line 2, we adopt a mini-batch training in our practical implementation. Then, in lines 6 to 13, LEAPREC greedily selects K items for each user considering both relevance and sequential miscalibration."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct experiments to answer the following questions.\nQ1. Performance comparison (Section 4.2). Does LEAPREC provide better trade-off between accuracy and calibration compared to competitors? How efficient and fast is LEAPREC compared to competitors in generating recommendations?\nQ2. Ablation study (Section 4.3). Do the main components in LEAPREC help improve the performance?\nQ3. Effect of the balancing hyperparameter (Section 4.4). How does the balancing hyperparameter \u03bb, which is the key factor in enhancing calibration, affect the overall recommendation?\nQ4. Case study (Section 4.5). HOW LEAPREC recommend a list of items considering both relevance and calibration?"}, {"title": "4.1 Experimental Setting", "content": "4.1.1 Datasets. We evaluate LEAPREC and other methods using four real-world datasets from distinct domains: movies (ML-1M\u00b9 [8]), books (Goodreads\u00b2 [43]), grocery products (Grocery\u00b3 [11, 32]), and video games (Steam\u2074 [34]). These datasets are chosen for their diversity in domain, sparsity level, and number of categories, as detailed in Table 1. In ML-1M dataset, we follow previous work [1, 39] and consider only ratings of four stars and above, simulating positive feedback.\n4.1.2 Backbone Model. Unless otherwise stated, we consider SAS-Rec [20] as a backbone framework which has shown its superior performance compared with other frameworks in comprehensive experiments [20, 22]. However, the calibration-disentangled learning-to-rank approach is open to other sequential recommendation frameworks such as GRU4Rec [13], Caser [41], and BERT4Rec [40] since it is a model-agnostic learning approach.\n4.1.3 Baseline Methods. We compare LEAPREC with the following four existing calibration recommendation methods.\n\u2022 CaliRec [39] is a post-processing approach that reranks the output of a backbone model using a greedy algorithm, optimizing for static calibration.\n\u2022 CaliRec+ [39] differs from CaliRec in reranking algorithm where it optimizes for sequential calibration instead of static calibration. We adopt SKL (u) as in LEAPREC for the miscalibration score.\n\u2022 MIP [38] is a post-processing approach that utilizes mixed integer programming, focusing on achieving static calibration."}, {"title": "4.2 Performance Comparison (Q1)", "content": "4.2.1 Trade-off Comparison. In Figure 5, we compare LEAPREC and baselines on four real-world datasets to verify whether LEAPREC provides better trade-off between accuracy and calibration than the baselines. We present the performance of the backbone models for both LEAPREC (base) and SASRec (base), along with trade-off curves of the methods between accuracy and calibration. The results show that LEAPREC consistently surpasses the baselines across all datasets, drawing better trade-off curves. Notably, the observed higher accuracy of LEAPREC (base) compared to SASRec"}, {"title": "4.3 Ablation Study (Q2)", "content": "In Figure 6, we provide an ablation study that compares LEAPREC with its variants to evaluate the impact of its core components on performance. The variant (-)CD removes calibration-disentangled"}, {"title": "4.4 Effect of the balancing hyperparameter (Q3)", "content": "The balancing hyperparameter \u03bb, as defined in Equation (11), is a key factor in controlling the balance between accuracy and calibration. Increasing A leads to more calibrated recommendations at the cost of accuracy. In Figure 8, we examine the impact of varying A on users' overall recommendation quality on Goodreads dataset. We use kernel density estimation (KDE) to evaluate the distribution of sequential miscalibration SKL@10 across users under different \u03bb. In the figure, we also denote the average nDCG@10 to show the change in accuracy. The results of low \u03bb (e.g., 0.1) or missing calibration (LEAPREC (base)) show a wide range of distribution for SKL@10, meaning a substantial variance in calibration across users. However, as a increases, recommendations become more uniformly calibrated across users. The accuracy increases until A reaches 0.7."}, {"title": "4.5 Case Study (Q4)", "content": "In Figure 9, we analyze a case to observe how LEAPREC balances relevance and calibration when recommending a list of items to a user on Grocery dataset. We vary the calibration level for a random user and observe how the user experiences the recommendations. Figure 9 (a) represents the user's sequential interactions and the ground-truth item that the user will interact with. Figures 9 (b-d) show the recommendation results of LEAPREC for A (defined in Equation (11)) values 0.0, 0.3, and 0.9, respectively. Figure 9 (b) shows that LEAPREC accurately recommends the ground-truth item at rank 1 without calibration (i.e., \u03bb = 0.0). However, the overall items in the list skew towards the user's major interest Beverage, narrowing the user experience for the recommendation. In Figures 9 (c-d), we observe that LEAPREC effectively considers the user's other interest Candy & Chocolate as well as the major interest Beverage by enhancing calibration. Notably, LEAPREC effectively retains English breakfast tea, the most relevant item, at rank 1 while improving calibration. We further analyze another case where a user's category preference shifts towards a category previously not favored."}, {"title": "5 RELATED WORKS", "content": "Calibrated recommendation. Steck [39] first introduced calibrated recommendation to address the problem of traditional recommendation, where less dominant interests of users are often neglected. The goal is to ensure that recommendation lists accurately reflect the proportion of categories users have shown interest in. Steck [39] proposed CaliRec which is a post-processing approach that greedily adjusts the recommended items to better match the user's historical category distribution. Subsequent studies continued to adopt the post-processing strategy. Seymen et al. [38] introduced a non-greedy approach by formulating calibration as a constrained optimization problem and solving it with a mixed integer programming (MIP) algorithm. Abdollahpouri et al. [1] defined the calibrated recommendation as the maximum flow optimization problem and proposed a minimum cost flow (MCF) based algorithm. On the other hand, Chen et al. [3] proposed DACSR, an end-to-end method, to simultaneously optimize accuracy and calibration in a single training phase.\nDifferent from earlier advancements, our proposed method directly integrates calibration scores within the training phase, using a novel loss designed to enhance both the training and post-processing phases. Additionally, we introduce a prioritized mechanism to effectively balance accuracy and calibration, addressing the dual criteria challenge more dynamically than previous methods."}, {"title": "6 CONCLUSION", "content": "In this work, we propose LEAPREC, a novel method that effectively balances accuracy and calibration in sequential recommendation. LEAPREC first trains a backbone model using the proposed calibration-disentangled learning-to-rank loss to learn personalized rankings when calibration is considered. Subsequently, LEAPREC applies the proposed relevance-prioritized reranking algorithm to the backbone's results, encouraging highly relevant items are placed at the top while accounting for calibration throughout the recommendations. LEAPREC achieves superior performance over existing calibrated recommendation methods in extensive experiments. Our ablation study further confirms the necessity of the core components of LEAPREC. We also demonstrate through a case study how LEAPREC tackles relevance and calibration to achieve high performance on both."}]}