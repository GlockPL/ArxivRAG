{"title": "GAPartManip: A Large-scale Part-centric Dataset for Material-Agnostic Articulated Object Manipulation", "authors": ["Wenbo Cui", "Chengyang Zhao", "Songlin Wei", "Jiazhao Zhang", "Haoran Geng", "Yaran Chen", "He Wang"], "abstract": "Effectively manipulating articulated objects in household scenarios is a crucial step toward achieving general embodied artificial intelligence. Mainstream research in 3D vision has primarily focused on manipulation through depth perception and pose detection. However, in real-world environments, these methods often face challenges due to imperfect depth perception, such as with transparent lids and reflective handles. Moreover, they generally lack the diversity in part-based interactions required for flexible and adaptable manipulation. To address these challenges, we introduced a large-scale part-centric dataset for articulated object manipulation that features both photo-realistic material randomizations and detailed annotations of part-oriented, scene-level actionable interaction poses. We evaluated the effectiveness of our dataset by integrating it with several state-of-the-art methods for depth estimation and interaction pose prediction. Additionally, we proposed a novel modular framework that delivers superior and robust performance for generalizable articulated object manipulation. Our extensive experiments demonstrate that our dataset significantly improves the performance of depth perception and actionable interaction pose prediction in both simulation and real-world scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Articulated objects are ubiquitous in people's daily lives, ranging from tabletop items like microwaves and kitchen pots to larger items like cabinets and washing machines. Unlike simple, single-function rigid objects, articulated objects consist of multiple parts with different functions, featuring varied geometric shapes and kinematic structures, making generalizable perception and manipulation towards them highly non-trivial [1]. Some existing works tried to simplify this problem by developing intermediate representations to encode the similarities across different objects implicitly, such as affordance [2]\u2013[5] and motion flow [6]\u2013[8], thereby achieving generalization across objects. Another series of work [9]\u2013[11] tried to tackle the articulated object perception and manipulation based on a more explicit and fundamental concept called Generalizable and Actionable Part (GAPart), demonstrating more manipulation capabilities attributed to its 7-DoF pose representation compared to value map representation of visual affordance. However, we observe that two critical limitations impede their real-world performance.\nFirstly, the material of articulated objects significantly impacts the quality of point cloud data. Most existing work relies on point clouds, and these methods struggle due to the sim-to-real gap of depth estimation [9], [10], [12], [13]. Some neural-based stereo-matching depth reconstruction methods are proposed and show some success on rigid objects [14], [15]. These methods use neural networks to encode the disparity in stereo infrared (IR) patterns projected by structured light cameras. However, due to the limited diversity in the stereo IR dataset, these methods are constrained to small rigid objects and perform poorly on large articulated objects.\nSecondly, there is no method that can predict stable and actionable interactive poses across categories for articulated objects. Some work employs heuristic-based methods [9] to interact with articulated objects, but it is limited in diversity and fails to account for the geometric details necessary for robust interactions in real-world settings [3]. Some methods for rigid objects grasping pose prediction can generate stable poses. However, due to the lack of data on articulated objects, it is challenging to discern whether each link can interact independently, resulting in poses that are mostly non-actionable [16]. Affordance-based methods [2], [13], [17] receive widespread attention for interacting with articulated objects by generating heatmaps. However, these heatmaps are ambiguous, hard to annotate and struggle to produce stable grasping interactive poses [12].\nIn this paper, we address these limitations from a data-centric perspective. We introduce GAPartManip, a novel large-scale synthetic dataset that features two important aspects: (1) realistic, physics-based IR image rendering of various parts in diverse scenes, and (2) part-oriented actionable interaction pose annotations for a wide range of articulated objects. Our GAPartManip inherits 918 object instances across 19 categories from the previous GAPartNet dataset [9]. By leveraging these assets, we developed a data generation pipeline for part manipulation, producing the synthetic data needed to address the previously mentioned limitations. To improve generalizability and mitigate the sim-to-real gap, we incorporate domain randomization techniques [15] during data generation, ensuring a diverse range of outputs. In total, our dataset contains approximately 14000 scene-level samples with 8 billion part-oriented actionable pose annotations, encompassing a wide array of physical materials, object states, and camera perspectives.\nTrained on the proposed dataset, we can obtain a depth reconstruction network and an actionable pose prediction network separately to address the two limitations mentioned earlier. Moreover, we compose these two neural networks modular to a novel articulated object manipulation frame-work. Through extensive experiments on both synthetic and real worlds, our method achieves state-of-the-art (SOTA) performance in both individual module experiments and part manipulation experiments.\nTo summarize, our main contributions are as follows:\n\u2022 We introduce GAPartManip, a novel large-scale dataset of various articulated objects featuring realistic, physics-based rendering and diverse scene-level, part-oriented actionable interaction pose annotations.\n\u2022 We propose a novel framework for articulated object manipulation and evaluate each module separately, demonstrating superior effectiveness and robustness compared to baseline methods.\n\u2022 We conduct comprehensive experiments in the real world and achieve SOTA performance on articulated object manipulation tasks."}, {"title": "II. RELATED WORK", "content": "Articulated object dataset and modeling is a crucial and longstanding research field in 3D vision and robotics, encompassing a wide range of work in perception [9], [18]\u2013[23], generation [24]\u2013[28], and manipulation [9]\u2013[11], [29]\u2013[33]. As to manipulation dataset, GAPartNet [9] annotates 6-DoF part pose to manipulate parts. Graspnet [34] and Contact-grasp [35] build several datasets, but these datasets all focus on rigid objects, neglecting the kinematic semantics specific to articulated objects. Where2act [2] first introduces a data generation pipeline for articulated objects, and they generate data by sampling successful poses in the simulator. AO-Grasp [16] leverages a curvature-based sampling method to accelerate data collection efficiency and proposes an 87k dataset of actionable poses. RPMart [12] manually anno-tated affordance maps for articulated objects and provided rendered data in SAPIEN [1]. None of the current datasets provide sufficient photo-realistic rendering data to improve algorithms' perception of articulated objects during sim2real, limiting real-world performance, especially with imperfect point clouds [10], [12]. Additionally, the data collection pro-cesses are inefficient and result in small datasets, hindering algorithm generalization to unknown objects. This work aims to create a large-scale dataset with diverse photo-realistic and actionable pose data covering all types of GAParts."}, {"title": "B. Articulated object manipulation", "content": "Due to unique kinematic structures and geometric shapes, articulated objects present significant challenges in manipulation. Current methods can be broadly categorized into: learning-based methods and prediction-planning methods. Learning-based methods, such as reinforcement learning [10], [31] and imitation learning [32], [36], require a large amount of high-quality robot demonstration. However, col-lecting such data is both impractical and time-consuming, and their sim-to-real performance heavily relies on simulator. Current prediction-planning methods [2], [9], [11], [37]\u2013[39] focus on visual affordance but offer ambiguous interactive poses and struggle to generalize due to limited data. They rely on 3D point clouds, ignoring the impact of object materi-als. In the real world, depth cameras often miss critical points like handles and lids, reducing sim-to-real performance."}, {"title": "III. GAPARTMANIP DATASET", "content": "We construct a large-scale dataset, GAPartManip, to address both depth estimation and actionable interaction pose prediction challenges in articulated object manipulation in real-world scenarios from a data-centric perspective. It contains 19 common household articulated categories from GAPartNet, including Box, Bucket, CoffeeMachine, Dish-washer, Door, KitchenPot, Laptop, Microwave, Oven, Printer, Refrigerator, Safe, StorageFurniture, Suitcase, Table, Toaster, Toilet, TrashCan, and WashingMachine, comprising a total of 918 object instances after removing problematic assets.\nWe build a photo-realistic rendering pipeline for each asset in indoor scenes. We render RGB images, IR images, depth maps, and part-level segmentations. Additionally, we create high-quality and physics-plausible interaction pose annota-tions for each part of the articulated object. Then leverage our GPU-accelerated scene-level pose annotation pipeline to generate dense, part-oriented actionable interaction pose annotations for each data sample. Our dataset contains over 8 billion actionable poses across 241680 data samples."}, {"title": "B. Photo-realistic Scene-level Rendering", "content": "Our photo-realistic rendering pipeline is built upon NVIDIA Isaac Sim [40]. Specifically, we simulate the RGB and IR imaging process of Intel RealSense D415, a widely-used structured light camera for real-world depth estimation in previous research works. We replicate the layout of the D415 imaging system consisting of four hardware mod-ules, i.e., an IR projector, an RGB camera, and two infrared (IR) cameras. We also project a similar shadow pattern onto the scenes with D415.\nInspired by previous works [14], [41], we incorporate do-main randomization techniques into our rendering pipeline to mimic the IR rendering under various lighting conditions and material properties in the real world. We render each object in 20 different scenes with various domain randomization settings. Concretely, we randomly vary ambient lighting, background, and object material properties in the scene, generating more diverse data that covers a wider range of real-world imaging conditions. we further randomize the ambient light positions and intensities within each scene. More importantly, we randomize the parameters of all dif-fuse, transparent, specular, and metal materials of each part corresponding to their semantics. Finally, we uniformly randomize the joint poses of the object within its joint limits in each scene during the rendering process.\nWe render the objects and parts from different distances. We render each scene with 5 object-centric camera per-spectives for the whole object and 5 part-centric camera perspectives for each part. To place the object within the camera view, i.e., the object-centric perspective, the camera is positioned at a latitude of ranged in [10\u00b0,60\u00b0] and a longitude ranged in [-60\u00b0,60\u00b0] in the target object. To capture the more fine-grained parts, i.e., the part-centric perspective, we leverage part pose annotations from GAPartNet and the current joint poses to determine the position and orientation of each part in the scene. The camera is then randomly positioned around each part, aiming directly toward the part center. As a result, the target part occupies the primary area of the image. During this process, camera viewpoints are randomly sampled within a latitude range of [0\u00b0,60\u00b0] and a longitude range of [-75\u00b0,75\u00b0]."}, {"title": "C. GPU-accelerated Scene-level Pose Annotation", "content": "Part-level Stable Pose Annotation: We employ a pose sampling strategy similar to GraspNet [34] to annotate dense and diverse stable interaction poses for each GAPart, based on the original semantic annotations in GAPartNet [9]. First, we perform mesh fusion for each part, merging the meshes corresponding to the same part to establish a one-to-one correspondence between parts and meshes. Then, we apply Farthest Point Sampling (FPS) to downsample the mesh for each part, resulting in N candidate points for pose sampling. For each candidate point, we uniformly generate $V \\times A \\times D$ candidate poses, where V is the number of gripper views distributed uniformly over a spherical surface, A represents the number of in-plane gripper rotations, and D refers to the number of gripper depths. In our case, N = 512, V = 64, A = 12, and D = 4. We follow GraspNet to calculate the pose score based on antipodal analysis.\nb) Scene-level Actionable Pose Annotation: To obtain part-centric grasping poses, We first project the part-level grasping poses into the scene using the part pose annotations, and then filter out unreasonable and unreachable poses. More concretely, We classify grasping poses that do not align with single-view partial point clouds as unreasonable. Meanwhile, we consider poses that cause collisions between the gripper and other objects as unreachable.\nHowever, such a filtering process is computationally de-manding due to the large amounts of points in the scene. To accelerate the pose annotation process, we implemented a CUDA-based optimization for the filtering process. Our optimization significantly reduces the processing time from 5 minutes to less than 2 seconds for each part which is nearly a 150 times speed-up. As a result, the originally year-long pose reduction process can now be completed within 3 days."}, {"title": "IV. FRAMEWORK", "content": "We propose a novel framework to address cross-category articulated object manipulation in real-world settings. As illustrated in Fig. 4, the framework primarily consists of three modules: a depth reconstruction module, a pose prediction module, and a local planner module."}, {"title": "A. Depth reconstruction module", "content": "The input to our system is a single view RGB-D observation including a raw depth Id, left IR image $I_l$, right IR image $I_r$, and an RGB image I. However, the raw sensor depth are often imcomplete and even incorrect because trans-parent and reflective surfaces are inherently ambiguous for structured light and Time-of-Flight depth cameras. Therefore, we leverage diffusion model-based approaches to estimate and restore the incomplete depths of raw sensor outputs. We use D\u00b3RoMa [14] as our depth predictor and fine-tune it on our dataset."}, {"title": "B. Pose prediction module", "content": "Different from 6 DoF grapsing pose prediction for rigid object manipulation, we need to predict both the 6-DoF part grasping pose and the 2-DoF movement direction after grasp-ing. We adapt the SOTA method Economicgrasp [42] as our actionable pose estimator dubbed Part-aware EcoGrasp and use pretrained GAPartNet [9] to predict the part movement direction.\nTo precisely annotate the part-centric interaction pose, we propose actionness instead of graspness in contrast to Economicgrasp. To annotate actionness, we first denote the scene as a point cloud $P = \\{p_i | i = 1,..., N\\}$ with N points. Then for each point $p_i$, we uniformly discretize its sphere space into $V = \\{v_j | j = 1,..., V\\}$ approaching directions. For each view $v_j$ of point $p_i$, we generate L actionable pose candidates $A_k \\in SE(3)$ indexed by $k \\in [1, L]$ by grid sampling along gripper depths and in-plane rotation angels respectively. We employ antipodal analysis [34] to calculate the grasping quliaty score $q_k \\in [0,1.2]$. Next, We define an actionable label $c_i \\in \\{0,1\\}$ for each point indicating whether this point is on a interacble part. We also define a scene-level collision label $c_k^s \\in \\{0,1\\}$ for each pose indicating whether this pose will cause collision. Finally, the point-wise actionness score $s_{ij}$ and view-wise actionness score $s_j^v$ are defined as:\n$s_{ij} = \\frac{1}{L} \\sum_{k}^{} (q_k > T)c_i$\n$s_j^v = \\frac{1}{L} \\sum_{k}^{} (q_k > T)c_k^s$\nwhere T is a predefined threshold to filter out inferior quality poses. We then train Part-aware EcoGrasp [42] following [42]. Additionally, we utilize the pre-trained GAPartNet [9] to predict the motion direction which speficies the part movement direction after grasping the actionable part."}, {"title": "C. Local planner module", "content": "We use CuRobo [43] as our motion planner. It optimizes motion trajectories based on actionable poses given by the pose prediction module, computes joint angles through inverse kinematics, and drives the robot to execute trajectory actions through joint control modes. Subsequently, the robot executes actions based on the motion direction $r_p$."}, {"title": "V. EXPERIMENTS", "content": "We conduct experiments for each module. The depth estimation and actionable pose prediction experiments are conducted to illustrate the significance of our dataset in articulated object manipulation tasks. Meanwhile, real-world experiments are carried out to compare the performance of our framework with existing methods. We also performed ablation studies for each module."}, {"title": "A. Depth Estimation Experiments", "content": "In this section, we evaluate different depth estimation methods with our GAPartManip to demonstrate the effectiveness of our dataset for improving articulated object depth estimation in both simulation and the real world.\nData Preparation. We split the dataset into training and test sets using an approximate 8:2 ratio. To maintain comprehen-sive coverage, each object category is split carefully, ensuring that both the training and test sets include samples from all categories. Additionally, we make sure that samples rendered from the same object category are assigned exclusively to either the training or test set. We compare our method with following baselines: leftmargin=10pt\n\u2022 SGM [44] is one of the most widely-used traditional algorithm for dense binocular stereo matching.\n\u2022 RAFT-Stereo (RS) [45] is a learning-based binocular stereo matching architecture built upon the dense optical flow estimation framework RAFT [46], using an itera-tive update strategy to recursively refine the disparity map.\n\u2022 D\u00b3RoMa (DR) [14] is a SOTA, learning-based stereo depth estimation framework based on the diffusion model. It excels at restoring noisy depth maps, espe-cially for transparent and specular surfaces.\nEvaluation Metrics. We evaluate the estimated disparity and depth using the following metrics:\nleftmargin=10pt\n\u2022 EPE: Mean absolute difference between the ground truth and the estimated disparity map across all pixels.\n\u2022 RMSE: Root mean square of depth errors across all pixels.\n\u2022 MAE: Mean absolute depth error across all pixels.\n\u2022 REL: Mean relative depth error across all pixels.\n\u2022 $\u03b4_i$: Percentage of pixels satisfying $\\frac{|d-d^*|}{max(d, d^*)} < \u03b4$. d denotes the estimated depth. d denotes the ground truth."}, {"title": "B. Actionable Pose prediction Experiments", "content": "In this section, we evaluate the impact of our dataset on improving the method for articulated object actionable pose estimation.\nData Preparation. We split the dataset into training and testing sets using an approximate 7:3 ratio. Specifically, we further divide the test sets into 3 categories: seen instances, unseen but similar instances, and novel instances. We com-pare our methods with the following baselines:\nleftmargin=10pt\n\u2022 GSNet (GS) [47] is a grasping pose prediction model trained on the GraspNet-1 billion [34] dataset for rigid object. We evaluate both the pre-trained model and the fine-tuned model separately.\n\u2022 Where2Act (WA) [2] is an affordance-based method for interacting with articulated objects. Unlike the original approach, we do not train a separate network for each task. As Where2act cannot generate stable grasping poses, we integrated GSNet, as referenced in [12], to enhance where2act's capabilities to align with experi-mental setting.\n\u2022 EconomicGrasp (EG) [42] is also a pose prediction method for rigid objects, which includes an interactive grasp head and composite score estimation to enhance the precision of specific grasps."}, {"title": "C. Real-World experiment", "content": "To validate the sim-to-real generalizability of GAPartMa-nip, we conducte real-world experiments. We use a Franka robot arm with an Intel RealSense camera to capture depth and IR images. We compare our method with three baselines: Where2act, AO-Grasp, GSNet, and, like in V-B, We modified the Where2act interaction pipeline to finish our tasks. The experiment consists of 7 distinct instances, including Stor-ageFurniture, Box, and Microwave, evaluating the success rate of the top-1 interactive pose for each method across open (n=14) and close (n=17) tasks. As shown in Tab. V-C, the overall success rate of GAPartManip is 61.29%, showcasing not only a successful transfer to the real world but also a significant performance boost compared to other methods.\nAdditionally, we perform ablation studies to assess how different modules affect the overall pipeline performance. As shown in Fig. 7, depth cameras yield poor depth data when faced with certain materials, significantly impacting subsequent manipulations. Our depth reconstruction module effectively addresses this issue by repairing 2D depth map, thereby enhancing the performance of subsequent modules. Similarly, as shown in Fig 7, GAPartManip tends to prior-itize interactable GAParts. This part-aware capability could possibly explain why our method leads to such significant performance disparities as seen in Tab. V-C."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we build a large-scale synthetic dataset for generalizable and actionable part manipulation with material-agnostic articulated objects. Our dataset is the first large-scale, diverse in instances, categories, scenes, and materi-als articulated object dataset. Meanwhile, we propose an articulated object manipulation framework capable of zero-shot transfer to the real world. We conduct experiments on individual modules and real-world overall experiments, with results indicating the competitiveness of our approach. Our dataset will be released."}]}