{"title": "Looped ReLU MLPs May Be All You Need as Practical Programmable Computers", "authors": ["Yingyu Liang", "Zhizhou Sha", "Zhenmei Shi", "Zhao Song", "Yufa Zhou"], "abstract": "Previous work has demonstrated that attention mechanisms are Turing complete. More recently, it has been shown that a looped 13-layer Transformer can function as a universal programmable computer. In contrast, the multi-layer perceptrons with ReLU activation (ReLU-MLP), one of the most fundamental components of neural networks, is known to be expressive; specifically, a two-layer neural network is a universal approximator given an exponentially large number of hidden neurons. However, it remains unclear whether a ReLU-MLP can be made into a universal programmable computer using a practical number of weights. In this work, we provide an affirmative answer that a looped 23-layer ReLU-MLP is capable to perform the basic necessary operations, effectively functioning as a programmable computer. This indicates that simple modules have stronger expressive power than previously expected and have not been fully explored. Our work provides insights into the mechanisms of neural networks and demonstrates that complex tasks, such as functioning as a programmable computer, do not necessarily require advanced architectures like Transformers.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformers [VSP+17] have demonstrated their potential across a variety of tasks, emerging as a dominant choice for a wide spectrum of practical applications, including natural language processing (NLP) [DCLT19, RSR+20, Ope23, AJA+24, Ant24, Ope24, Met24] and computer vision [DBK+20, PX23, HWC+22, ADH+21, KNH+22, WSD+23, WCZ+23, WXZ+24, LSSZ24b, WMS+24], among others. The success of Transformers is largely attributed to their ability to perform complex operations, such as induction head [OEN+22, CS24], in-context learning [DLD+22, WBZ+21, WTB+22, XSL24, SWXL24, CLL+24], information retrieval [SMN+24] and chain of thoughts [WWS+22, KGR+22]. Meanwhile, there is another line of research that explores the the-oretical capability of Transformers. For instance, [PBM21] has proven the Turing completeness of the attention mechanism. However, Turing completeness is not a feature unique to Transformers. [SS92, CS21] has demonstrated that Recurrent Neural Networks (RNNs) are also Turing complete. Other works [Pin99, KL20] have shown that the most basic module in deep learning, the Multi-Layer Perceptron (MLP), is a universal approximator.\nHowever, the concept of Turing completeness inherently necessitates infinite memory, an im-practicality in real-world scenarios due to the finite nature of available memory. In [GRS+23], they bridge the gap between the theoretical Turing machine and the practical Transformer-based programmable computer by illustrating that a looped 13-layer Transformer possesses the necessary expressiveness to operate as a programmable computer. Given that ReLU-MLP also has the ca-pability for achieving Turing completeness and itself is an universal approximator, this raises an interesting question:\nIs ReLU-MLP expressive enough to be a practical programmable computer?\nTo the best of our knowledge, no practical solution for constructing a ReLU-MLP as a general-purpose computer has been proposed before. Therefore, we explore the conditions under which a ReLU-MLP can function as a universal programmable computer and prove that a 23-layer looped ReLU-MLP is capable of emulating a general-purpose computer.\nOur research centers on MLPs equipped with the ReLU activation function, which is fundamental to their design. We commence by formally defining the ReLU activation function as follows:\nDefinition 1.1 (ReLU). We define ReLU as follows: For a vector $x \\in \\mathbb{R}^{n}$, the output of the i-th entry of ReLU for $i \\in \\{1, 2, ..., n\\}$ is $ReLU(x)_i = max\\{x_i, 0\\}.\\$\nThen, we introduce the definition of MLP with ReLU activation as follows:\nDefinition 1.2 (ReLU-MLP). Let $n$ be the size of the state. Let input be $x \\in \\mathbb{R}^{n}$. The weights and bias are $W \\in \\mathbb{R}^{n \\times n}, b \\in \\mathbb{R}^{n}$. We have the 1-layer output of ReLU Multiple-Layer Perceptron (ReLU-MLP) as\n$ReLU(Wx + b) \\in \\mathbb{R}^{n}.\n(1)\nThe $m$-layer ReLU-MLP is then the compositions of $m$ such Perceptrons in Eq. (1), e.g., $ReLU(W_2 \\cdot ReLU(W_1x + b_1) + b_2)$ for 2-layer ReLU-MLP.\nBased on this setting, we have demonstrated that a 23-layer looped ReLU-MLP is capable of emulating a general-purpose computer. This is accomplished by constructing a 23-layer ReLU-MLP that can execute a generalized form of the single instruction SUBLEQ (see also Algorithm 1). The SUBLEQ instruction, which stands for Subtract and Branch if Less-than or Equal to zero, operates on three addresses a, b, c \u2208 Rlogn. It subtracts the value at memory location mem[b] from"}, {"title": "2 RELATED WORK", "content": "Complexity and Neural Networks. Circuit complexity, a branch of computational complexity theory, studies circuit families as models of computation\u00b9. Several circuit complexity classes are sig-nificant in machine learning. Specifically, AC\u00ba represents problems highly parallelizable with stan-dard logic gates, while TC\u00ba extends this to include threshold gates, and NC\u00b9 denotes the language recognizable by O(log n)-depth circuits with bounded gate arity [MSS22]. It is known that $AC^0 \\subset TC^0 \\subset NC^1$, but whether $TC^0 \\neq NC^1$ remains an open question. Assuming this inequality, [LAG+22] show that Transformer depth must depend on input sequence length when simulating non-solvable semiautomata. [LLZM24] explore relationships among constant-depth Transformers, Transform-ers with Chain-of-Thought (CoT), and circuit complexity. They demonstrate: $T[poly(n),1,1] \\subseteq$"}, {"title": "3 PRELIMINARY", "content": "This section provides essential definitions used in this paper. In Section 3.1, we introduce some basic notations. In Section 3.2, we present several key concepts related to the state vector of the programmable computer constructed by ReLU-MLP.\n3.1 Notations\nWe use $[n]$ to denote $\\{1,2,..., n\\}$ for any $n \\in \\mathbb{N}_{+}$. We use $e_i$ to denote a vector where only $i$-th location is 1 and zeros everywhere else. We denote an all 1 vector using $1_n \\in \\mathbb{R}^n$. We denote an all 0 vector using $0_n \\in \\mathbb{R}^n$. We use $a \\cdot b$ to denote the inner product of $a, b \\in \\mathbb{R}^{d}$ i.e. $a \\cdot b := \\sum_{i=1}^{d} a_i b_i$. We use $\\circ$ to denote the Hadamard product i.e. the i-th entry of $a \\circ b$ is $a_i b_i$. Let $I_{d \\times d} \\in \\mathbb{R}^{d \\times d}$ denote an identity matrix.\n3.2 Key Concepts\nWe begin with introducing the way we organize the data. Different from conventional $\\{0,1\\}$ representation of the data, we use $\\{\\pm 1\\}$ to represent the data. This design will benefit the calculate of the address vectors, which will be discussed in Remark 3.5.\nDefinition 3.1 (One-Bit Data). We define one bit data as $v \\in \\{-1,1\\}$."}, {"title": "5 KEY FUNCTIONS IMPLEMENTATIONS", "content": "In this section, we outline a selection of critical functions that the ReLU-MLP model is capable of executing. Due to the limitation of the space, the detailed proofs are deferred to the Appendix. Specifically, in Section 5.1, we implement the read operation. Section 5.2 covers the write operation. Section 5.3 presents addition, while Section 5.4 addresses subtraction. Conditional branching is implemented in Section 5.5, and the SUBLEQ operation is in Section 5.6.\n5.1 Read\nWe first consider the most basic \"read\" operation, which is to read any data or instruction from memory into a register. We begin with introducing the read operation for one-bit data as follows:\nLemma 5.1 (Read One-Bit Data, Informal Version of Lemma A.1). Let ReLU-MLP be defined as Definition 1.2. Let $n$ denote the number of data in the memory. For $i \\in [n]$, each data $v_i \\in \\{-1,+1\\}$. Let the address vector $a_i \\in \\{\\pm 1\\}^{\\log(n)}$.\nThen, a 2-layer ReLU-MLP can read any one-bit data from the memory to the register.\nThen, to achieve the read operation for d-bits data, we only need to use the ReLU-MLP intro-duced in the previous Lemma to perform a read operation on each bit in the d-bits data.\nLemma 5.2 (Read d-Bits Data, informal version of Lemma A.2). Let ReLU-MLP be defined as Definition 1.2. Let $n$ denote the number of data in the memory. For $i \\in [n]$, each data $v_i \\in \\{-1,+1\\}$. Let $v \\in \\{\\pm 1\\}^{d}$ denote a d dimension vector. Let the address matrix $a_i \\in \\{\\pm 1\\}^{\\log(n) \\times d}$.\nThen, a 2-layer ReLU-MLP looped for d times can read any d-bits data vector from the memory to the register.\n5.2 Write\nCorresponding to the read operation, in this section we introduce how to use ReLU-MLP to imple-ment the \"write\" operation. We start with the write operation of one bit data.\nLemma 5.3 (Write One-Bit Data, Informal Version of Lemma A.3). Let ReLU-MLP be defined as Definition 1.2. Let $n$ denote the number of data in the memory. For $i \\in [n]$, each data $v_i \\in \\{-1,+1\\}$. Let the address vector $a_i \\in \\{\\pm 1\\}^{\\log(n)}$.\nThen, a 2-layer ReLU-MLP can write any one-bit data from the register to the memory.\nSubsequently, we can expand our approach to accommodate the \"write\" operation for d-bit data, by sequentially processing each bit within the d-bit data.\nLemma 5.4 (Write d-Bits Data, Informal Version of Lemma A.4). Let ReLU-MLP be defined as Definition 1.2. Let $n$ denote the number of data in the memory. For $i \\in [n]$, each data $v_i \\in \\{-1,+1\\}$. Let $v \\in \\{\\pm 1\\}^{d}$ denote a d dimension vector. Let the address matrix $a_i \\in \\{\\pm 1\\}^{\\log(n) \\times d}$.\nThen, a 2-layer ReLU-MLP looped for d times can write any d-bits data vector from the register to the memory.\n5.3 Addition\nBeyond the fundamental memory operations of reading and writing, a pivotal set of operations involves algorithmic functions. Consequently, we demonstrate how the addition and subtraction operations can be emulated by the ReLU-MLP. We begin with introducing the emulation of one-bit addition operation via ReLU-MLP."}, {"title": "6 DISCUSSION AND EXTENSION", "content": "In Section 6.1, we compare ReLU-MLP with attention mechaism. In Section 6.2, we discuss the potential capability of ReLU-MLP. In Section 6.3, we discuss the potential inspirations our work offers for designing more efficient neural network architectures.\n6.1 Comparison with Attention\nAs discussed in Section 4, we have demonstrated that a looped 23-layer ReLU-MLP is capable of emulating a programmable computer. In contrast to the findings reported by [GRS+23], where a looped 13-layer Transformer is shown to be capable of such emulation, our result indicates that even a basic component of deep learning, the ReLU-MLP, possesses the potential to handle complex computational tasks. This suggests that while advanced architectures like Transformers have shown proficiency in processing intricate tasks, this proficiency may not come from its advanced architec-ture design. Instead, it may derive from the inherent capabilities of fundamental components such as the ReLU-MLP. Our finding underscores the importance of investigating the core mechanisms behind the capabilities of advanced architectures, an area that warrant further exploration.\n6.2 Exploring the Potential of ReLU-MLP\nOur research has confirmed that the capabilities of the ReLU-MLP are on par with Transform-ers when it comes to constructing programmable computers. As noted in [Pin99, KL20], ReLU-MLPs have been demonstrated to be universal approximators. Consequently, we conjecture the programmable computer represents just one of the many downstream tasks that ReLU-MLPs are capable of achieving. Building on the insights from this study, it is promising to further investigate the potential capabilities of ReLU-MLPs. Our approach to analyze the looped ReLU-MLP mitigates the black-box nature often associated with traditional deep learning training. We are confident that through the analytical methods outlined in our work, we can systematically probe the capacity of ReLU-MLPs as universal approximators to tackle more complex tasks. This line of works will be done in our future research.\n6.3 Towards More Efficient Architecture Design for Specific Tasks\nThe construction-based proof in our work can inspire future explorations of the smallest feasible network structure for specific tasks. Since the main focus of our work is to prove the feasibility of ReLU-MLP-based computer construction, the ReLU-MLP-based construction we provide may not be the smallest construction that can support a programmable computer. Therefore, in fact, we provide a possible lower bound that can achieve a programmable computer. The current main-stream methods for model compression are mainly quantization and model pruning, which often provide model compression solutions based on experimental results, and lack theoretical measure-ments between model capabilities and model capabilities required by the downstream tasks. Thus, leveraging the methodologies applied in this study, we can first identify the minimal requirements to a model for a particular task, enabling researchers to create the smallest, yet efficient models to"}, {"title": "7 CONCLUSION", "content": "In this work, we investigate the computational potential of a looped ReLU-MLP. We begin by demonstrating that a looped 23-layer ReLU-MLP with a single pass is capable of emulating the execution process of the \u201cSUBLEQ\" instruction. Drawing on the conclusions presented in [MP88], we establish that the One Instruction Set Computer (OISC) implemented by the looped 23-layer ReLU-MLP is functionally equivalent to a programmable computer. Contrary to previous research [GRS+23], which achieved the emulation of a programmable computer using a looped 13-layer Transformer, our approach leverages a more fundamental building block of deep learning, a 23-layer ReLU-MLP, to accomplish the same objective. This finding prompts us to consider two key insights: (i) The untapped potential of ReLU-MLPs warrants further exploration, offering a deeper understanding of the capability boundaries of existing neural networks; (ii) By adopting the methodologies employed in this study, it may facilitate us to gain insights for designing the minimal neural network requirements for any specific tasks."}, {"title": "Appendix", "content": "Roadmap. We organize our appendix as follows: In Section A, we introduce our construction of ReLU-MLP for emulating the read and write operation. In Section B, we present the way we achieving the addition and subtraction operation via ReLU-MLP . In Section C, we show that a 4-layer ReLU-MLP is capable for emulating the conditional branching operation. In Section D, we illustrate the \"SUBLEQ\" can be emulated by a looped 23-layer ReLU-MLP. In Section E, we demonstrate how the looped 23-layer ReLU-MLP introduced in the previous section is capable to function as a programmable computer.\nA READ AND WRITE\nIn this section, we mainly focus on construction a multi-layer ReLU-MLP for supporting read and write operation. We begin with the scenario where we read a one-bit data from the memory to the data register.\nLemma A.1 (Read One-Bit Data, Formal Version of Lemma 5.1). If the following conditions hold:\n\u2022 Let ReLU-MLP be defined as Definition 1.2.\n\u2022 Let $n$ denote the number of data in the memory. For $i \\in [n]$, each data $v_i \\in \\{-1,+1\\}.\\$\n\u2022 Let the address vector $a_i \\in \\{\\pm 1\\}^{\\log(n)}$.\nThen, we can show that, a 2-layer ReLU-MLP can read any one-bit data from the memory to the register.\nProof. Consider a simplified case, where we have one data register $r_a \\in \\{-1\\}$ which stores data $v_0 \\in \\{\\pm 1\\}$ initially, and one address register $r_a \\in \\{\\pm 1\\}^{\\log(n)}$ which stores address $a_i \\in \\{\\pm 1\\}^{\\log(n)}$ initially. The address $a_i$ points to the location where the data should be copied from. Namely, we want to perform the following operation:\nFor simplicity, we ignore the notations $r_d:$ and $r_a:$ in the following proof. We use $0_d \\in \\mathbb{R}^d$ denote a vector with entries are 0.\nStep 1: Erase $v_0$.\nWe construct the following weight matrix $W_1 \\in \\mathbb{R}^{(n+\\log(n)+1) \\times (n+\\log(n)+1)}$:"}]}