{"title": "Assessing the Performance of Human-Capable LLMs - Are LLMs Coming for Your Job?", "authors": ["John Mavi", "Nathan Summers", "Sergio Coronado"], "abstract": "The current paper presents the development and validation of Self-Score, a novel benchmark designed to assess the performance of automated Large Language Model (LLM) agents on help desk and professional consultation tasks. Given the increasing integration of AI in industries, particularly within customer service, SelfScore fills a crucial gap by enabling the comparison of automated agents and human workers. The benchmark evaluates agents on problem complexity and response helpfulness, ensuring transparency and simplicity in its scoring system. The study also develops automated LLM agents to assess SelfScore and explores the benefits of Retrieval-Augmented Generation (RAG) for domain-specific tasks, demonstrating that automated LLM agents incorporating RAG outperform those without. All automated LLM agents were observed to perform better than the human control group. Given these results, the study raises concerns about the potential displacement of human workers, especially in areas where Al technologies excel. Ultimately, SelfScore provides a foundational tool for understanding the impact of Al in help desk environments while advocating for ethical considerations in the ongoing transition towards automation.", "sections": [{"title": "1 Introduction", "content": "Automation and efficiency have long been inextricable pursuits of scientific research and societal advancement. During the Industrial Revolution of the 18th and 19th centuries, the development and proliferation of steam power and advancements in production techniques facilitated the widespread establishment of the emerging manufacturing economy, as mass production became easier and more efficient. Later, starting in the 1940s and 1950s, the transition from analog to digital technologies began, gaining momentum in the following decades. With it, unprecedented computational power became more widely available, trivializing previously labor-intensive (or even resource-prohibitive) tasks.\nNow, the precipice of a new technological revolution looms ahead. With the increasing availability of massive amounts of data, Artificial Intelligence (AI) technologies are experiencing an explosion in their efficacy, their applicability, and their accessibility. ChatGPT was among the fastest growing products in history [23], and retains a large daily active userbase today, two years later [7, 8]. \u0391\u0399 is being increasingly integrated into social media [2, 3, 33], healthcare [34, 37], business and enterprise [5, 6, 11], and transportation [12, 16, 21], and these trends show no signs of slowing. Although AI is still in its infancy, efficiency gains are already being observed [19, 35].\nTechnology-driven efficiencies have historically had significant impacts on contemporary society, particularly within the labor market. Mechanization, initiated during the Industrial Revolution, reduced the labor demand in agriculture, leading to a long-term decrease in the number of farmers [14]. The advent of digital technologies led to a transition within the workforce, emphasizing the prioritization of knowledge-based roles [13, 31]. With AI, the potential ramifications stand to be much more substantial. The question then naturally arises, how will these revolutionary technologies shape the future of labor, as those preceding it did?\nAutomated agents are of particular import to this question. These are software systems that autonomously perform tasks or make decisions based on predefined rules or learning algorithms. They operate in a wide range of domains, from customer service or financial trading to programming and development. They are designed to simulate human decision-making without direct intervention. Some agents can incorporate the capability to autonomously carry out actions based on their simulated decision-making [38].\nBecause automated agents do not rely on direct intervention, coupled with the fact that machines are capable of processing and outputting data more efficiently than humans, these agents stand to drastically change the nature of work. Automated agents might be deployed to oversee rote tasks, such as reporting and administrative work that currently requires human oversight, thereby permitting humans to instead work on more complex, creative, or cognitively demanding tasks for which Al is not currently suited. By streamlining certain processes, resources can be redistributed to significantly increase efficiency throughout the working world.\nKey to current implementations of automated agents are Large Language Models (LLMs), as they permit ordinary people to meaningfully interact with computers using tools they already possess: language. LLMs are advanced AI systems trained on vast amounts of text data to understand and generate human-like language. They use deep learning techniques, such as neural networks, to predict and produce coherent text based on a given input, making them useful for a wide array of tasks like translation, summarization, and conversation [4, 10]. Consequently, they often form the foundation upon which automated agents, or automated LLM agents, are built. This permits individuals to describe a task to an automated agent using natural language. The agent is then able to leverage its constituent subsystems to autonomously carry out this task and provide its user with an output that is not restricted to natural language text. This flexibility, multi-modality, and versatility distinguishes automated LLM agents from their foundational LLMs.\nHowever, many LLMs are trained on broad, non-domain-specific text in order to maximize the quantity of training data and provide the LLM with the most comprehensive understanding of how natural language works. Retrieval-Augmented Generation (RAG) can be used to mitigate this shortcoming. RAG is an advanced approach in natural language processing (NLP) that combines retrieval of relevant information from external sources with generation of new text based on that information [24, 29].\nIn RAG models, a retrieval component first searches for and pulls in related documents, knowledge, or data from an external database or knowledge base. This information is then fed into a generative model, which uses the retrieved content to create a more informed and accurate response or output.\nBy incorporating RAG within an automated agent's LLM, the agent can provide more relevant information, specific to its domain and use case [20, 25, 48].\nThis is particularly relevant when considering the application of automated LLM agents to the help desk and professional consultation industries. Because these industries rely on relevant domain-specific topic knowledge, RAG can be an effective approach to address the shortcomings of broadly trained LLMs. This is significant, as LLM-powered automated agents appear to be well suited to the work required in these domains.\nThe help desk and professional consultation industries are broad and are present across almost all sectors of the economy. This includes customer support services, IT support services, administrative assistance, after-sales support and assistance, and several other types of support services. Although these service lines may appear to vary, they all rely on domain-specific knowledge, the ability to solve problems, and the ability to handle and mitigate errors to ensure that clients or customers have a good experience. Furthermore, they all rely on communication between a provider and a customer/client.\nThese are all tasks that automated LLM agents are designed to accomplish. However, their ability to do so effectively, particularly when compared to humans, remains unclear.\nAnswering these questions is significant, as, due to the applicability of automated LLM agents to help desk and professional consultation tasks, it stands to reason that there may be substantial impacts to the human workforce that already exists. In the US, almost 3 million people were employed as customer service representatives in 2023, earning a median hourly wage of $19.08 [9]. These positions may face potential redundancies as automation within the sector increases.\nIn 2021, the global help desk software market was valued at $9.9 billion and is expected to grow at a compound annual growth rate of 9.4%, reaching $26.8 billion by 2032 [40]. Much of this growth is being driven by the demand for cloud-based solutions, which further stands to put existing jobs at risk.\nMoreover, businesses in the US alone risk losing $846 billion worth of sales because of poor customer service [18], as studies have shown that 80% of customers are likely to switch service providers after more than one bad experience with a brand [36].\nBecause of the high potential human and financial costs associated with the help desk, customer support, and professional consultation industries, it is imperative that the quality of assistance remains high, regardless of the extent to which automation becomes incorporated. Despite trends indicating that the use of AI technologies within these industries is growing [36, 43], evidence suggests that clients/customers still prefer to get service from humans, particularly in nuanced or complex cases [43, 47].\nThis all serves to highlight the growing need for a quantitative and comparative measure that can assess the performance of both humans and automated LLM agents in providing help desk and professional consultation assistance.\nBenchmarking presents such a comparative tool. However, existing LLM benchmarks, as well as those targeting automated LLM agents, do not provide sufficient specific insight into the help desk and consultation industries (more information in Section 1.1). Given the potential impact automated LLM agents may have on the help desk and consultation industries, the development of a targeted benchmark specific to these industries is necessary to facilitate the integration of automated LLM agents into the industry. This gives rise to the primary research question of this study: How can the performance of automated help desk and consultation agents be quantified and thus compared to the performance of human help desk and consultation agents?\nThe current research presents SelfScore. This novel benchmark seeks to quantify the performance of automated LLM agents and humans when performing help desk and consultation tasks. By deriving its scores from intuitive and understandable math, the benchmark is more comprehensible and transparent than alternatives. This will help facilitate its adoption into domains wherein employees are less likely to have strong computer science or technical backgrounds.\nFor the current study, the research team developed automated help desk agents using current LLMs to assist with IT-related tasks to assess this newly developed benchmark. This gives rise to the second research question investigated herein: How well do the developed automated LLM agents perform help desk tasks compared to humans? Using the benchmark to quantify the performance of both humans and automated LLM agents, a direct comparison between the two can be drawn. This helps provide insight into the capabilities of current LLMs and the automated LLM agents that can be built upon them in order to assess the potential impact this technology might have on the future of the help desk and professional consultation industries."}, {"title": "1.1 Related Work", "content": "Currently, several benchmarks exist that aim to quantify the performance of LLMs. Some of these benchmarks focus on the ability of LLMs to engage in logical and critical thinking [32, 44]. Others prioritize performance on general knowledge questions, ranging in difficulty from elementary school-level to expert-level [22, 39]. These benchmarks are valuable for understanding the proficiencies of different LLMs on different tasks, and their observations have been considered in the development of SelfScore. For example, [32] notes that LLMs struggle with problems that rely on critical and logical thinking skills, while [22] observes that the largest LLMs are quite proficient in topic and domain knowledge.\nHowever, these benchmarks do not target LLMs as a component of an automated agent. Where existing benchmarks do measure the performance of automated LLM agents [15, 27, 41, 45], they focus on applications that are sufficiently different from the use case outlined in the current research to warrant the establishment of a new benchmark specific to the help desk and professional consultation industries. [15] assesses the capability for automated LLM agents to interact with mobile-based applications, while [45] focuses on automated LLM agents and their performance on games, [41] assesses performance on visual-dependent household tasks, and [27] assesses the capacity of automated LLM agents to create software systems and self-replicate. Although the tasks measured in these benchmarks might share similar underlying dependencies (the ability to navigate applications and the internet, adaptability and critical thinking, and an understanding of computer science and systems, respectively), they are not specific enough to the tasks involved in help desk and professional consultation tasks to permit the assessment of automated LLM agents in this domain.\nMore closely related are benchmarks that consider \"LLMs-as-Agents\" [30]. However, there remains a distinction between this use case and the use case outlined in the current study. Help desk, customer service, and professional consultation tasks may rely on the use of other services (text-to-speech/speech-to-text, internet browsing, code deployment, document generation, etc.) that fall outside the scope of what many LLMs alone are capable. This is an important aspect of SelfScore, as it evaluates the automated agent's performance in its entirety, including that of other subsystems incorporated within the assessed autonomous LLM agent.\nFurthermore, in many cases, these benchmarks evaluate performance on problems that have a well-defined correct answer [15, 22, 39, 41]. This is not always the case for help desk and professional consultation problems, where situational context and other nuances will impact a problem's solution. Additionally, this suggests that they are not well suited to assess performance across a longer user-agent interaction, as they assess a single question-answer pair, which is unsuitable for the conversational nature of help desk and professional consultation processes. For benchmarks where this is not the case, their applicability to the help desk and professional industry remains an unresolved limitation for the research goals of the current study."}, {"title": "2 SelfScore", "content": "The proposed benchmark assesses an automated help desk agent's ability to respond to a user question of varying complexity in a helpful manner. To accomplish this, one must first define and then calculate both the question's complexity and the helpfulness of the agent's response(s).\nThe benchmark also assesses the user's interactions with the LLM agent, contextualized by the agent's outputs. This ensures that especially effective or ineffective users do not artificially inflate or deflate the final score of a particular LLM agent."}, {"title": "2.1 Benchmark Criteria", "content": ""}, {"title": "2.1.1 Complexity", "content": "The benchmark assesses the complexity of a human user's query based on three primary criteria. Grading for these criteria is relative to the easiest and hardest problems in the corresponding domain. For the current study, this domain is computer and IT-related knowledge. However, the benchmark can apply to any LLM agent used for any help desk or professional consultation tasks. The benchmark is specifically tailored to help desk and consultation tasks in non-regulated domains. Though it was not designed with regulated domains in mind, it might also be applicable to these, as the extent of the impact of AI-enabled automation within regulated domains remains to be seen.\nThe criteria are as follows:\n\u2022 Critical Thinking: How much critical thinking does this problem require to solve.\n\u2022 Error Handling: How likely is an error to occur while solving this problem, and, should an error occur, how significant will the impact of the error be and how difficult will it be to recover from it.\n\u2022 Topic Knowledge: How much topic knowledge is required to solve the problem.\nThese criteria describe the three most significant independent factors that influence the complexity of a help desk interaction and are equally applicable regardless of the specific domain in which such an interaction occurs. Whether one uses an LLM agent to assist in IT contexts, customer support, or bureaucratic or administrative assistance, it is important to measure the level of necessary critical thinking, error handling, and topic knowledge to address user questions. It is important to note that these criteria are independent. For instance, certain problems might require in-depth topic knowledge, but their solutions might not require significant further critical thinking or elicit substantial errors. Take, for example, a programming problem wherein the rounding of a number does not work as expected. This error might result from the behavior of one of the programming language's rounding functions. In this case, the solution might require the use of a different rounding function. Such a problem would require sufficient topic knowledge of the programming language but would not require further critical thinking or error management.\nFurthermore, these criteria display continuous increases from simple help desk problems to challenging ones. As a user's problem becomes more difficult, an LLM agent will require a higher level of at least one, if not more, of the identified criteria to solve it than would be necessary to solve an easier problem.\nMoreover, LLMs \u2013 and, by extension, automated LLM agents \u2013 are suitable tools to address help desk and consultation problems and one can expect their performance on these criteria to improve proportionate to developments in the underlying technology. This makes these criteria suitable metrics by which to assess the overall performance of automated LLM agents for tasks of this type."}, {"title": "2.1.2 Helpfulness", "content": "The benchmark considers \"helpfulness\" at three separate points throughout the benchmarking process.\nFirst, the benchmark considers the helpfulness of the initial input question. This refers to the level of information provided by the human user and how useful this information is in identifying and solving the user's current problem. This assessment is necessary as the quality of the initial input question will have an impact on the ability of the LLM agent to address the underlying issue, regardless of how competent the agent itself is.\nNext, the benchmark assesses the helpfulness of the user throughout the interaction. This consideration will rate how well a user is able to execute the agent's instructions and will be continuous throughout the interaction between the user and LLM agent. This will primarily be determined by reviewing the user's follow-up dialogue with the agent, following the initial question. As with the first helpfulness assessment, this consideration is necessary to ensure that user competency does not impact an agent's final score.\nFinally, the benchmark scores the helpfulness of the agent's outputs. This consideration assesses whether the support agent could provide solutions relevant to the problem. This requires the agent's outputs to be understandable, applicable, and constructive in addressing the user's underlying problem.\nThe combination of these three separate \u201chelpfulness\" measures enables the benchmark to account for differences in user competency and ensures that the benchmark does not erroneously inflate or deflate scores depending on individual user interactions. These measures represent points during the help desk interaction where users can introduce variability. The benchmark must consider this to ensure it only accounts for the LLM agent's abilities."}, {"title": "2.1.3 Additional Considerations", "content": "In addition to considerations regarding complexity and helpfulness, the benchmark can optionally assess other factors that may be relevant to benchmarking an LLM agent's performance, depending on its specific use case.\nFor agents where operational costs may be a concern, benchmarking can optionally calculate and assess run costs. This permits assessors to weigh potential compromises between an agent's performance using various foundational LLMs and any associated operational costs. In the current research, this consideration only applied to GPT-4 due to limitations of the deployed library LangChain.\nFor agents that incorporate text-to-speech (TTS) technology, the benchmark may also consider TTS-related aspects of the agent's performance. These include the accuracy of the generated TTS, the comprehensibility of the TTS, and the naturalness of the TTS. These considerations are useful for agents that interact with users via audio interfaces, such as within call center applications, or similar."}, {"title": "2.2 Benchmarking Process", "content": "The process for benchmarking an LLM agent begins when the agent receives a question input as a problem. This input can originate from either a human user or a supplied testing dataset. At this stage, the benchmark calculates the weighted complexity score and assesses the helpfulness of the user's initial question. Once the LLM agent responds to the initial question, the first \"turn\" of the interaction concludes.\nThe benchmark defines \"turns\" as an individual question-response pair within the user-agent interaction. The \u201cinteraction\" refers to the complete set of turns that constitute the problem-solving or consultation task between the user and the agent. Turns loop until either the agent solves the problem or exceeds the maximum number of turns, at which point the interaction concludes. Assessors can set this number of turns according to their specific needs. For the current study, researchers set the maximum number of turns equal to 50.\nFollowing the first turn, each subsequent turn involves the following steps:\n(1) The benchmark assesses the user's response to the agent for helpfulness. The agent then receives the user response.\n(2) The agent generates its counter-response. The benchmark assesses this counter-response for LLM helpfulness. The user then receives the counter-response.\n(3) The benchmark checks to determine if the agent has solved the user's problem. In the affirmative case, the loop terminates, and the current turn ceases here. This check assesses the turn history and the latest LLM response to confirm that the user's problem is, in fact, solved.\n(4) If the assessor is assessing the per-turn run cost, the benchmark considers this here. There are three cases which control how this occurs.\n(a) Input and output tokens are the same cost. Calculate the total number of tokens used in this turn, multiplied by the token cost.\n(b) Input and output tokens are priced differently. Calculate the number of input tokens multiplied by the input token cost and add to the number of output tokens multiplied by the output token cost.\n(c) Stipulates a set per-turn cost, which is multiplied by the interaction's total number of turns.\n(5) The benchmark calculates the current turn's quality. This score considers the helpfulness of the user responses and agent responses for this turn.\n(6) The benchmark saves the user and LLM responses for this turn. These responses contribute to the assessment of overall helpfulness for both the user and agent across the entire interaction.\n(7) The benchmark saves all scores, tokens, and other data for this turn for posterity (for instance, if there is a need to recalculate the benchmark scores for the current interaction).\n(8) If the agent has not solved the user's problem or has not exceeded the maximum number of turns, the loop restarts.\nOnce the interaction between the user and the agent ends, either by way of solving the user's question or by exceeding the maximum number of turns, the benchmark conducts calculations for final scoring. First, the benchmark calculates the average user and agent helpfulness across the entire interaction. Next, these scores determine the overall quality of the interaction. Finally, the benchmark calculates the interaction's final score by comparing the problem's weighted complexity with the average quality of the interaction.\nThe subsequent section (2.3 Scoring) provides the formulae for the calculations used throughout the benchmarking process."}, {"title": "2.3 Scoring", "content": "Benchmark scoring combines quantitative and qualitative measures of the LLM agent's performance. Consequently, subjectivity will play a role in scoring, as qualitative metrics related to problem complexity and response helpfulness are inherently subjective. The benchmark scores these subjective measures using an LLM. This permits the benchmark to efficiently assess on a larger scale than current alternatives. In all cases where the benchmark uses subjective measures, scores are based on a 10-point scale."}, {"title": "2.3.1 Complexity Scoring", "content": "As discussed in Section 2.1.1, the benchmark bases the complexity of a user's initial question on three criteria: Critical Thinking, Error Handling, and Topic Knowledge, assessed relative to the easiest and hardest problems in the corresponding domain. The benchmark consolidates these three criteria into a single weighted complexity score, which aims to capture the overall complexity of the user's problem.\nThe benchmark calculates the weighted complexity score using the following formula, which maintains the 10-point scale of its constituents:\nWeighted Complexity = (0.5 \u00d7 Critical Thinking)\n+ (0.4 \u00d7 Error Handling) + (0.1 \u00d7 Topic Knowledge)\nCriterion weights consider the ability for LLMs to perform tasks related to the identified criteria to a high standard. As demonstrated in [22], LLMs are readily able to assist in tasks related to topic knowledge and domain knowledge transfer, thus it is not significantly considered in the weighted complexity score. However, topic knowledge remains an important aspect of addressing help desk and consultation questions and should factor into complexity considerations.\nAs demonstrated in [32, 44], LLMs currently struggle to demonstrate effective logical skills and critical thinking, as well as the ability to foresee and mitigate potential errors [26, 28]. For this reason, these criteria are more important in determining the weighted complexity scores. Logical skills and critical thinking have been observed to be more challenging for LLMs than error handling, as demonstrated in comparing results from [26, 44], hence their increased consideration within the weighted complexity score. This decision ensures that agents that are capable of these more complex tasks are more highly rewarded.\nIt is important to note that these weights are based on the conclusions of existing research into the capabilities of LLMs. They are not necessarily expected to be conclusive. More work is needed to finetune these weights to observe how weighted complexity might vary as these weights are adjusted. The above weights were used to generate the results of the current study and implementation."}, {"title": "2.3.2 Helpfulness Scoring", "content": "The benchmark independently scores the helpfulness of both the user and the agent every turn on a 10-point scale, where a 10 represents a perfectly helpful contribution to the ongoing interaction.\nWhen assessing the user's helpfulness, one should consider whether the user response in this turn demonstrates their ability to follow the previous turn's instructions and communicate relevant new information clearly and concisely.\nWhen assessing the agent's helpfulness, one should consider whether the agent can provide solutions relevant to the current problem, considering the information provided by the user.\nFor the first turn of the interaction, the assessment of the user helpfulness differs slightly. This distinction is necessary because this turn involves the initial question. As such, there are no previous instructions for the user to follow. Consequently, one should instead consider how helpful this initial question is based on how much potentially helpful information it contains, relevant to solving the problem.\nOnce the interaction has concluded, the benchmark calculates the overall average helpfulness scores for both the user and the agent using the following formulae:\nAverage User Helpfulness = $\\frac{\\sum \\text{ Per Turn User Helpfulness Scores}}{\\text{Number of Turns}}$\nAverage LLM Helpfulness = $\\frac{\\sum \\text{ Per Turn LLM Helpfulness Scores}}{\\text{Number of Turns}}$"}, {"title": "2.3.3 Quality Scoring", "content": "The quality score arises from the helpfulness ratings and describes the overall quality of the interactions. The benchmark uses the following formulae:\nAverage Quality = $\\frac{\\text{Average LLM Helpfulness}}{\\text{Average Human Helpfulness}}$\nQuality score ensures that the benchmark does not penalize effective agents due to ineffective users, and that effective users do not artificially inflate the score of poor agents."}, {"title": "2.3.4 Final Scoring", "content": "The final benchmark score considers both the weighted complexity score, derived from the user's initial question, as well as the average quality of the entire interaction, using the following formula:\nFinal Score = $\\frac{\\text{Weighted Complexity + Average Quality}}{2} \u00d7 10$\nThe final score provides a theoretical maximum score of 100. This would require both the human user and the LLM agent to be perfectly helpful over the entire interaction on a maximally complex problem in terms of critical thinking, error management, and topic knowledge.\nIn practice, it is likely difficult for an agent to achieve such a score. However, as agent design and the capabilities of foundational models improve, it seems likely that scores will improve in a continuous manner as agents are able to solve more complex problems and communicate their solutions more effectively to users."}, {"title": "3 Current Implementation", "content": "To assess and validate the proposed benchmark, the research team designed and implemented automated LLM agents to assist in IT-domain help desk tasks. These agents use GPT-4, Mixtral7b, and Mixtral8x7b as foundational models.\nThe agents received tasks originating from a dataset comprised of Stack Exchange forum posts. The dataset is derived from an anonymized archive dump of all user-contributed content on Stack Exchange, which included posts, users, votes, comments, badges, tags, post history, and post links [42]. Posts were selected from the archive dump based first on whether they had an accepted answer. Following this, further selection was based on the number of answers' upvotes to ensure that only high-quality postings were included in the dataset. From this selection, the text content and title of the post, the response, and the number of upvotes were considered relevant going forward.\nFor the current implementation of the benchmark, the research team used LLMs to assist in a variety of tasks. These tasks include performing large-batch information extraction, assessments of problem complexity, and assessments of user and agent helpfulness, among others. Where applicable, relevant prompts accompany descriptions of LLM use."}, {"title": "3.1 Data Pre-Processing and Initial Calculations", "content": "First, researchers converted the top entries from the Stack Exchange dataset from XML to JSON format for ease of use and consistency. Selection for the dataset's top entries considered the number of upvotes on a forum posting's answer. This threshold varied depending on whether the agent used RAG or not. For agents that were not using RAG, researchers set the minimum answer upvote to 100, which resulted in a pool of 1,164 entries. For agents using RAG, this threshold was lowered such that the pool was approximately twice as large (total of 2,360 entries). From this pool, random selection determined which entries were used for RAG and which were used for benchmark testing and validation, ensuring a 50/50 split to maintain validation pool size consistency between runs.\nSince user questions were occasionally lengthy, an LLM extracted a summarized version of the question and the underlying problem from the posting's solution.\nThe prompt for question extraction follows: Dumb this question down and summarize it in one or two sentence(s): .\nOriginally, the question extraction prompt used more formal vocabulary (e.g., \"simplify this question,\" \"give a concise version of this question\"). However, this version provided the best results.\nThe prompt for underlying problem extraction follows: Extract a problem statement from this post. For example, \"The computer is not plugged in\", or \"The DNS servers are down\". Respond with only the problem: .\nThis approach minimizes the level of superfluous details provided to the LLM agent and minimizes required input tokens, where necessary. Furthermore, LLMs are well suited to information extraction tasks, as demonstrated in [46].\nDuring runtime, the calculation for weighted complexity score considers the summarized version of the question, while the solution check (Point 3 in Section 2.2 Benchmarking Process) uses the extracted underlying problem.\nTo ensure the qualitative assessments are as representative as possible, multiple LLMs perform the assessments for complexity and helpfulness across different testing runs. For example, when using Mixtral8x7b as a foundational model for the agent, one testing run uses GPT-4 to perform qualitative assessment, while another uses Mixtral8x7b, and so on. This permits the direct comparisons of final scores, quality, and complexity and helpfulness assessments to confirm that LLM information extraction and qualitative assessment is robust and suitable for the current task."}, {"title": "3.2 Agent Design", "content": "The help desk agents share a similar design, differing only in the foundational LLM used for language processing and generation.\nThe agent begins by receiving an input. This input is either the initial problem question or a follow-up question. For the current implementation, the source for these inputs is either a dataset entry (used to provide a human baseline or control assessment) or an LLM generated response which acts as a proxy for the agent's user. This approach facilitates large-scale assessment. In cases where the inputs are audio files, OpenAI's Whisper speech-to-text [1] converts the audio file to a string. Otherwise, the agent receives the input as text directly.\nOnce the agent receives the input, it passes this input, along with any interaction history, to the inference mechanism of the foundational LLM's backend. The LLM then processes the input to generate a text-based response, which is subsequently output back to the agent. Where relevant, the agent also returns the input and output tokens used.\nA script then cleans the LLM's text output to remove unsupported characters by the agent's TTS mechanism. This step is necessary due to the specific TTS component [17] that the current agent implementation uses. The agent then writes the audio file and returns its response as both audio and text. At this point, the agent also returns the input to the benchmark to preserve interaction history.\nIt is important to note that, in the current implementation, the benchmark manages the interaction's history as opposed to the agent itself. This approach ensures that the benchmark can enforce the maximum number of turns per interaction and can determine when the agent has solved the user's problem. Currently, the agent itself is not able to reset interaction history on its own. This task is managed by the benchmark, which passes history to the agent, as necessary. In another implementation, the agent could exclusively manage history on an interaction-by-interaction basis.\nFinally, the turn concludes, and the agent receives a new input from the dataset or from an LLM generated user proxy. The agent then repeats this process until it reaches the maximum number of turns or until it solves the problem.\nIn the case that an agent does not use RAG, it relies on the following system prompt: A user is having a problem. Respond with simple and helpful instructions most likely to guide the user to a solution. Only provide one solution at a time. Never give instructions to contact external or professional services. Never suggest contacting external or professional services.\nAlthough the last sentence of this prompt is a repeated order, the research team identified that this minimized the chance that the output would suggest contacting external or professional services. In the case that an agent does use RAG, there is no provided system prompt. Instead, a system prompt is either unnecessary and inferred by RAG data, or the developer explicitly defines it in the agent's backend."}, {"title": "3.3 Results", "content": "The average final scores for all assessment runs of the benchmark are in the table below.\nThe results shown in the table indicate that agents incorporating RAG performed better than those that did not. The top 3 performing automated LLM agents all achieved similar average final scores, with a difference of only 0.89 between the top performer and third performer's average final scores. An ANOVA indicated that the difference in final scores for the top 3 performers is not statistically significant (F = 2.00, p = 0.135).\nHowever, when considering all assessed agents' final scores, an ANOVA revealed that the observed difference was statistically significant (F = 84.7, p < 0.001). Appendix A contains post-hoc test results. These results suggest that the benchmark can effectively discern between differently capable LLMs. Furthermore, they reinforce the observation that agents incorporating RAG are better suited for domain-specific assistance.\nThe detailed results for the top three performing agents, as well as those from the human baseline, are found below. Pairwise comparisons using Mann-Whitney U tests accompany the result descriptions to determine statistical significance of between group results, where necessary. The Mann-Whitney U statistical test accounts for the non-normal distribution observed in key metric scoring.\nEach result name is split into three parts, divided by underscores. The first part refers to the foundational LLM used for the agent, the next part refers to the model used to generate the dataset's complexity scores, and the third part refers to the model used for evaluation of responses during benchmarking. For instance, the results mixtral-8x7b_gpt-4-1106-preview_gpt-4-1106-preview uses Mixtral 8x7b as the agent's foundational LLM, uses GPT-4-1106 preview for the dataset complexity generation, and uses GPT-4-1106 preview for the response evaluation.\nNote that the exception to this is human_gpt-4-1106-preview_gpt-4-1106-preview, which represents the current control. These results do not use an LLM agent to generate responses, and instead use the existing human interactions derived from the Stack Exchange forum. This provides a baseline against which to compare the performance of automated LLM agents. In this case, GPT-4 performed benchmarking tasks by assessing complexity and helpfulness. The detailed plots of key insights follow this naming scheme."}, {"title": "4 Discussion", "content": ""}, {"title": "4.1 Contributions", "content": "The proposed benchmark represents a significant contribution to LLM agent benchmarking by permitting large scale testing of help desk and consultation agents. By using LLMs to conduct the benchmark's required assessments, significantly more data can be considered during scoring due to the rapidity with which LLMs can process written text. Thus, assessors can determine the performance of automated LLM agents across more numerous and more varied interactions than previous benchmarks allowed.\nFurthermore, the scoring is simple and intuitive. Final scores are based on a logical 100-point scale and are derived from simple mean calculations. Constituent criteria are all scored on an intuitive 10-point scale based on clear definitions that are comprehensible to non-expert analysts. This approach broadens the benchmark's potential userbase and accessibility, thus facilitating its widespread adoption to a plethora of help desk and professional consultation industries"}]}