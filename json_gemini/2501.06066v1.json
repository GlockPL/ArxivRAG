{"title": "Distilling Calibration via Conformalized Credal Inference", "authors": ["Jiayi Huang", "Sangwoo Park", "Nicola Paoletti", "Osvaldo Simeone"], "abstract": "Abstract-Deploying artificial intelligence (AI) models on edge devices involves a delicate balance between meeting stringent complexity constraints, such as limited memory and energy re- sources, and ensuring reliable performance in sensitive decision- making tasks. One way to enhance reliability is through un- certainty quantification via Bayesian inference. This approach, however, typically necessitates maintaining and running multiple models in an ensemble, which may exceed the computational limits of edge devices. This paper introduces a low-complexity methodology to address this challenge by distilling calibration information from a more complex model. In an offline phase, predictive probabilities generated by a high-complexity cloud- based model are leveraged to determine a threshold based on the typical divergence between the cloud and edge models. At run time, this threshold is used to construct credal sets ranges of predictive probabilities that are guaranteed, with a user-selected confidence level, to include the predictions of the cloud model. The credal sets are obtained through thresholding of a divergence measure in the simplex of predictive probabilities. Experiments on visual and language tasks demonstrate that the proposed approach, termed Conformalized Distillation for Credal Infer- ence (CD-CI), significantly improves calibration performance compared to low-complexity Bayesian methods, such as Laplace approximation, making it a practical and efficient solution for edge AI deployments.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern artificial intelligence (AI) models, including neural networks and large language models, have achieved remark- able success in decision-making and generative tasks. How- ever, two significant challenges persist in their deployment: (i) achieving reliability in safety-critical applications [1], and (ii) enabling efficient deployment on edge devices with constrained resources [2].\nTraditional Bayesian methods, which model epistemic un- certainty by treating model parameters as random variables, are a popular approach to improving reliability by enhancing calibration [3]-[5]. A well-calibrated model is one whose confidence levels match the true accuracy levels, thus pro- viding trustworthy information about the reliability of the model's output. However, Bayesian methods have several drawbacks. For one, they typically require maintaining and running multiple models in order to carry out ensembling, which is computationally challenging for edge devices. Fur- thermore, they depend on the choice of prior distributions, whose misspecification may lead to suboptimal calibration.\nAgainst this background, this paper introduces a low- complexity, practical approach for calibrating edge models. Our approach distils calibration knowledge from complex cloud models, ensuring reliable performance without the com- putational overhead of Bayesian methods."}, {"title": "B. Conformalized Distillation for Credal Inference", "content": "As illustrated in Fig. 1, this paper introduces a low- complexity methodology to enhance the calibration of a small- scale edge model by distilling calibration information from a more complex cloud model. In the setting under study, prior to deployment at the edge, a small-scale model is calibrated with the use of data collected from a large-scale model.\nThe approach, termed Conformalized Distillation for Credal Inference (CD-CI), builds on conformal prediction (CP) [6], [7] and imprecise probability (IP) [8], [9]. In an offline calibration phase, CD-CI uses the predictive probabilities generated by a high-complexity cloud-based model to deter- mine a threshold based on the typical divergence between the predictions of the cloud and edge models.\nAt run time, CD-CI uses this threshold to construct credal sets [10] ranges of predictive probabilities that are guar- anteed, with a user-selected confidence level, to include the predictions of the cloud model. The credal sets are obtained through simple thresholding of a divergence measure in the simplex of predictive probabilities [11]. Credal sets are finally converted into predictive distributions by using methods such as entropy maximization [12] or intersection probabilities [13]."}, {"title": "C. Related Work", "content": "Imprecise Probability (IP) offers a mathematical framework for describing random events that cannot be captured by standard probability theory [8], [14]\u2013[19]. IP handles scenarios where multiple probabilities are assigned to a single event, such as: (i) when the underlying probability distribution varies over time (e.g., the probability of rain tomorrow differs from yesterday) [16], [17]; (ii) when estimating a fixed distribu- tion requires considering an uncertainty set (e.g., the rain probability is estimated between 0.2 and 0.3) [18]; and (iii) when subjective beliefs, like Bayesian priors, are not uniquely defined (e.g., expecting rain 2-3 days per week in London) [8]. For an empirical interpretation of IP, see [19].\nImprecise Probabilistic Machine Learning (IPML) [10], [20]-[22] applies IP in machine learning, addressing issues such as imprecise labelling (e.g., crowdsourced labels [22]), domain generalization [10], and Bayesian model misspecifi- cation [10]. While IPML can enhance robustness and gener- alization [10], [22], most approaches require constructing a credal set a convex set of distributions containing the target distribution. This construction often relies on assumptions about the target distribution [10], which may not hold in practice."}, {"title": "D. Main Contributions", "content": "This paper proposes a novel, low-complexity calibration method specifically designed for edge AI deployment. The primary contributions are as follows:\n\u2022 Credal set construction via distilled calibration: We in- troduce a post-processing approach, CD-CI, that distills calibration knowledge from large-scale cloud models. By treating the predictive distributions from cloud models as a reference, we enable edge models to make calibrated predictions with statistical reliability guarantees.\n\u2022 Robust predictive distribution: When a single predictive distribution is required, CD-CI extracts a distribution from the credal set by using the intersection probability approach [13], achieving a more robust performance compared to low-complexity Bayesian methods.\n\u2022 Experimental validation: We present experiments on vi- sual and language modelling tasks, including the CIFAR- 10 dataset [52] and the SNLI dataset [53], in which small- scale models are obtained via smaller architectures or quantized weights [54], [55]. We demonstrate significant improvements in calibration performance, as measured by the expected calibration error (ECE) [1], over the original small-scale model, with negligible drops in accuracy. Comparisons are also provided with a low-complexity"}, {"title": "II. PROBLEM DEFINITION", "content": "As illustrated in Fig. 1, we aim at calibrating a pre-trained small-scale K-way classifier, $p(y|x)$, with d-dimensional input $x \\in R^d$ and label $y \\in K = {0,1,\\ldots, K - 1}$, by leveraging a pre-trained large-scale model, $p^*(y|x)$, and an unlabeled data set $D_{unl} = {x_i}_{i=1}^{|D_{unl}|}$. As an example, the large-scale model may be cloud-based, while the small-scale model may be intended for edge deployment.\nGiven a test input x, the calibration procedure post- processes the output of the small-scale model, given by the distribution $p(\\cdot|x) = {p(y|x)}_{y \\in k}$, to produce a subset of predictive probability distributions. This subset aims at capturing the uncertainty of the small-scale model about the predictive distribution of the large-scale model, $p^*(\\cdot|x)$ for the given input x. This uncertainty arises due to the limited computational power of the small-scale model as compared to the large-scale model. We aim to derive a low-complexity calibration procedure in which the subset is defined by a simple thresholding mechanism.\nLet P denote the simplex of K-dimensional probabilistic distributions. At test time, as mentioned, the calibration pro- cedure maps the small-scale model output probability $p(x)$ into a subset $\\Gamma(x) \\subseteq P$. The mapping between predictive probabilities and a subset of P is designed during an offline distillation phase in which the designer has access to the unlabeled data set $D_{unl}$ and to the large-scale model. The large- scale model is no longer accessible at test time.\nThe design goal is to ensure that the set includes the reference distribution $p^*(\\cdot|x)$ that would have been produced by the large-scale model with probability no smaller than a user-defined level $1 - \\epsilon$, i.e.,\n$\\Pr [p^*(\\cdot|x) \\in \\Gamma(x)] \\geq 1 - \\epsilon$,\nwhere $\\epsilon \\in [0,1]$ is the desired miscoverage rate. The prob- ability in (1) is evaluated with respect to the distribution of the unlabeled data set $D_{unl}$ used to design the post-processing mechanism, as well as over the test input x.\nThe condition (1) can be satisfied for any miscoverage rate $\\epsilon$ by setting $\\Gamma(x) = P$, i.e., by producing the set of all possible distributions on the label set K in response to any test input x. However, this output would be clearly uninformative. Therefore, to gauge the informativeness of the set predictor,"}, {"title": "III. CONFORMALIZED CREDAL INFERENCE", "content": "In this section, we first introduce credal sets, and then we propose a way to conformalize the credal sets so as to satisfy the coverage requirement (1).\nGiven a test input x and an output $p(\\cdot|x)$ of the small-scale model, the set predictor $\\Gamma(x)$ is constructed by including all distributions $q = q(\\cdot|x)$ in a neighbourhood of $p(\\cdot|x)$. The radius defining the size of the neighbourhood is determined during the offline calibration phase by using the unlabeled data and the large-scale model.\nTo elaborate, consider the class of divergences between two distributions $q_1$ and $q_2$ in simplex P, i.e.,\n$D_f(q_1||q_2) = E_{z \\sim q_2(z)} \\left[f\\left(\\frac{q_1(z)}{q_2(z)}\\right)\\right]$,\nwhere f() is a convex function satisfying the properties (i) f(1) = 0, and (ii) $0 \\cdot f(0/0) = 0 [56], [57]. The class of f-divergences encompasses a variety of divergence measures, including the Kullback\u2013Leibler (KL) divergence and the Tsallis divergence.\nGiven a f-divergence $D_f(\\cdot||\\cdot)$, we obtain the credal set as\n$\\Gamma(x) = {q \\in P : D_f(q||p(\\cdot|x)) \\leq \\gamma}$,\nwhere $\\gamma$ is a threshold to be determined during the offline calibration phase."}, {"title": "B. Distilling Calibration via Conformalized Credal Inference", "content": "Conformalized credal inference determines the threshold $\\gamma$ in (4) to guarantee the coverage condition (1). This is done offline by leveraging the unlabeled data set $D_{unl}$. To this end, we first construct the calibration data set\n$D_{cal} = {(x_i, p^*(\\cdot|x_i))}_{i=1}^{|D_{unl}|}$,\nin which the large-scale model is used to assign soft label $p^*(x_i)$ to the unlabeled input $x_i$. Then, we leverage the data $D_{cal}$ to construct the data set\n$S = {s_i = D_f(p^*(\\cdot|x_i)||p(\\cdot|x_i))}_{i=1}^{|D_{cal}|}$.\nThe data set S collects the divergence values $D_f(p^*(\\cdot|x_i)||p(\\cdot|x_i))$ between the reference and the predictive distributions in the unlabeled data set $D_{unl}$. Thus, this data set supports inferences about the distributions of the divergence $D_f(p^*(\\cdot|x_i)||p(\\cdot|x_i))$ as the input x varies according to the underlying population distribution."}, {"title": "C. Theoretical Reliability Guarantees", "content": "CD-CI satisfies the following reliability guarantees.\nTheorem 1: If the data samples in the unlabeled data set $D_{unl}$ and the test input x are exchangeable, e.g., i.i.d., then CD-CI (Algorithm 1 and Algorithm 2) produces credal sets $\\Gamma(x)$ in (4) that satisfy the coverage condition\n$\\Pr [p^*(\\cdot|x) \\in \\Gamma(x)] \\geq 1 - \\epsilon$, for any test input x,\nThe proof of this theorem follows directly from the marginal coverage guarantees of CP [58, Eq. (7)] (see also [11, Thm. 4.1])."}, {"title": "IV. PREDICTIVE DISTRIBUTIONS FROM CREDAL SETS", "content": "The proposed CD-CI method provides a low-complexity procedure to identify, for any given input x, a subset $\\Gamma(x)$ of predictive distributions q(x) that are likely to contain the golden-standard distribution $p^*(x)$ of the large-scale model. The set can directly provide actionable information. For example, if the set is deemed to be too large, an edge device may conclude that the local model is insufficiently accurate for the given input x, refraining from making a decision.\nIn practice, the edge device may wish to produce a single predictive probability $q^*(\\cdot|x)$ for decision-making. In this case, it is desirable that the resulting predictive probability $q^*(\\cdot|x)$, evaluated from the set $\\Gamma(x)$, be better calibrated than the initial distribution p(x) produced by the small-scale model.\nThe distribution $q^*(x)$ can be computed from the set $\\Gamma(x)$ in different ways, which are explored in the literature on imprecise probabilities [27]. For example, one can choose the distribution within the credal region $\\Gamma(x)$ that achieves the maximum Shannon entropy, thus finding the most conservative decisions [10]."}, {"title": "A. Intersection Probability", "content": "Given a credal set $\\Gamma(x)$, the intersection probability method obtains a single predictive distribution $q^*(\\cdot|x)$ as follows. First, for every class $y \\in K$, one obtains lower bound $q_L^y$ and upper bound $q_U^y$ on the probability q(y|x) assigned by distributions in subset $\\Gamma(x)$ as\n$q_L^y = \\min_{q(\\cdot|x) \\in \\Gamma(x)} q(y/x), q_U^y = \\max_{q(\\cdot|x) \\in \\Gamma(x)} q(y/x)$.\nIn practice, the set can be represented by a discrete subset of distributions $q(x) \\in \\Gamma(x)$. They can be obtained via grid search, with complexity linear in the grid size, or via importance sampling."}, {"title": "B. Low-Complexity Bayesian Learning via the Laplace Ap- proximation", "content": "Given a pre-trained small-scale model $p(x)$, the Laplace approximation provides a low-complexity method to re- calibrate a pre-trained model using Bayesian principles [3], [35]. To elaborate, assume a parametric small-scale model\n$p(y|x) = p(y|x, \\theta)$,\nwhere $\\theta$ is a pre-trained tensor of parameters. Laplace ap- proximation approximates the posterior distribution of model parameter $\\theta$ given a training data set D as\n$p(\\theta|D) \\approx N(\\theta, \\Sigma)$, with $\\Sigma := (\\nabla^2L(\\theta|D)|_{\\theta=\\hat{\\theta}})^{-1}$,\nwhere L($\\theta$|D) is the cross-entropy training loss [41]. In practice, the covariance matrix $\\Sigma$ can be approximated in several ways, e.g., via the Gauss-Newton method [59]."}, {"title": "C. Expected Calibration Error", "content": "A probabilistic predictor q(yx) is said to be perfectly calibrated if the equality\n$\\Pr [y = \\hat{y}(x)|q(\\hat{y}(x)|x) = r] = r$,\nholds for all $r \\in [0,1]$, where $\\hat{y}(x) = arg \\max_{y \\in K} q(y|x)$. By the condition (16), the confidence level, r = q($\\hat{y}$(x)|x) of the probabilistic predictor coincides with the probability of correct prediction for all possible $r \\in [0, 1]."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "In this section, we report empirical results for visual and natural language tasks."}, {"title": "A. Performance Metrics", "content": "For both tasks, we consider the following evaluation met- rics: (i) inefficiency, which evaluates the average size of the credal set $\\Gamma(x)$ as in (2); (ii) coverage, which is the percentage of samples for which the large-scale model predictive distri- bution $p^*(\\cdot|x)$ falls inside the credal set $\\Gamma(x)$ as in (1); (iii) the ECE (17); and (iv) accuracy, measured by the probability that the hard decision obtained as in (12) is correct."}, {"title": "B. Implementations", "content": "Throughout, we use the a-divergence to evaluate the con- formalized credal set in (4). The a-divergence is obtained from the general definition of the f-divergence in (3) with $f(t) = (t^{\\alpha} - 1)/(\\alpha(\\alpha - 1))$. Note that, for $\\alpha = 1$, the \u03b1- divergence reduces to the KL divergence [60]. Increasing the parameter a yields more constrained sets (4), for which the support of the distributions q($\\cdot|x$) is increasingly forced not to exceed the support of the small-scale distribution p($\\cdot|x$) [61]."}, {"title": "C. Image Classification", "content": "For the image classification task, as in [12], we extract the first three classes from the CIFAR-10 [52] data set. Furthermore, we adopt the ResNet-18 model [63] and the Mini-VGG-8 model [64] as the large-scale and small-scale models, respectively."}, {"title": "D. Natural Language Classification", "content": "For the natural language task, we adopt the SNLI data set [53], an English language sentence pairs classification data set with three labels: entailment, contradiction, and neutral. We use the NLI-deberta-v3-large adopted temperature scaling [48] and original NLI-deberta-v3-small [65] as the large-scale and small-scale models, respectively. We further vary the quality"}, {"title": "VI. CONCLUSION", "content": "In this paper, we have proposed a low-complexity methodol- ogy to calibrate a small-scale edge model prior to deployment by leveraging data generated by a large-scale cloud model. The method, called Conformalized Distillation for Credal Inference (CD-CI), ensures that the edge model can produce, at runtime,"}]}