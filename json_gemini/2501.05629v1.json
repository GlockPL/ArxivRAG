{"title": "The Impact of Model Scaling on Seen and Unseen Language Performance", "authors": ["Rhitabrat Pokharel", "Sina Bagheri Nezhad", "Ameeta Agrawal", "Suresh Singh"], "abstract": "The rapid advancement of Large Language Models (LLMs), particularly those trained on multilingual corpora, has intensified the need for a deeper understanding of their performance across a diverse range of languages and model sizes. Our research addresses this critical need by studying the performance and scaling behavior of multilingual LLMs in text classification and machine translation tasks across 204 languages. We systematically examine both seen and unseen languages across three model families of varying sizes in zero-shot and few-shot settings. Our findings show significant differences in scaling behavior between zero-shot and two-shot scenarios, with striking disparities in performance between seen and unseen languages. Model scale has little effect on zero-shot performance, which remains mostly flat. However, in two-shot settings, larger models show clear linear improvements in multilingual text classification. For translation tasks, however, only the instruction-tuned model showed clear benefits from scaling. Our analysis also suggests that overall resource levels, not just the proportions of pretraining languages, are better predictors of model performance, shedding light on what drives multilingual LLM effectiveness.", "sections": [{"title": "Introduction", "content": "Current trends in the development of large language models (LLMs) emphasize the use of extensive data in several different languages and larger model sizes (Srivastava et al. 2023). Scaling trends serve to quantify the connection between a model's performance and critical design elements such as training data size, number of model parameters, or architectural intricacies, providing invaluable insights for model refinement, resource distribution, and the selection of pertinent training data. Although significant research efforts have focused on scaling trends within predominantly English settings (Radford et al. 2019; Lin et al. 2022; McKenzie et al. 2023; Wei et al. 2023; Scao et al. 2023; Chia et al. 2024), their understanding in multilingual contexts remains underexplored (Sun and Miceli-Barone 2024).\nSeveral massively multilingual LLMs have been introduced (Lin et al. 2022; Achiam et al. 2023; Scao et al. 2023; Shliazhko et al. 2024), however, evaluation is often limited to a few tens of languages, limiting the large-scale evaluation"}, {"title": "Related Work", "content": "The impact of model scale on language model performance, particularly within English settings, has been extensively explored in recent years with several studies reinforcing the finding that larger models generally result in enhanced performance (Radford et al. 2019; Hestness et al. 2017; Kaplan et al. 2020; Rae et al. 2022; Wei et al. 2022; Hoffmann et al. 2022; Smith et al. 2022; Chowdhery et al. 2022). Despite the relationship between model scale and performance not always being linear or predictable (Wei et al. 2022, 2023; Xia et al. 2023), larger models continue to be developed, including several multilingual large language models such as GPT-3 (Brown et al. 2020b), XGLM (Lin et al. 2022), BLOOM (Scao et al. 2023), LLAMA (Touvron et al. 2023), PaLM 2 (Anil et al. 2023), and others. Although massive multilingual models benefit from positive transfer across languages, the performance of a model deteriorates as its language coverage expands, a challenging phenomenon also known as the \"curse of multilinguality\u201d (Conneau et al. 2020; Pfeiffer et al. 2022).\nDespite the scaling of multilingual models, research on the impact of scaling on performance remains limited. Lin et al. (2022) showed that different tasks lead to different scaling behavior. However, the datasets in their study were limited to at most 15 languages on the same task, and the classification dataset included only one low-resource language. Other prior work has studied scaling experiments for only English tasks (Scao et al. 2023; Chia et al. 2024) or a limited set of languages (Muennighoff et al. 2023; Asai et al. 2023; Yong et al. 2022; Winata et al. 2022; Srivastava et al. 2023; Isik et al. 2024; Dakle, Rallabandi, and Raghavan 2023; Sun and"}, {"title": "Experimental Setup", "content": "In this section, we present the overall experimental setup including the models considered, evaluation tasks and datasets, seen and unseen languages, and prompting under zero-shot and few-shot settings."}, {"title": "Models", "content": "We study three different multilingual LLMs - xglm, bloom, and bloomz - of varying sizes to investigate their scalability in multilingual task scenarios. Our study spans a total of 14 different models and model sizes.\n\u2022 xglm is a decoder-only transformer model trained on a corpus covering 30 languages (Lin et al. 2022).\n\u2022 bloom is also a decoder-only transformer language model and was trained on the ROOTS corpus which includes 46 natural languages (Scao et al. 2023).\n\u2022 bloomz is a variant of bloom that underwent instruction tuning (also known as multitask prompted finetuning) using the xP3 dataset which closely follows ROOTS's language distribution to enhance its adaptability across tasks and languages (Muennighoff et al. 2023)."}, {"title": "Evaluation Tasks, Datasets and Metrics", "content": "We study model performance on both text classification and text generation tasks. For classification, we choose the SIB-200 dataset (Adelani et al. 2024), a large-scale benchmark dataset for topic classification across 204 languages and dialects, 7 topics (the labels are: science/technology, travel, politics, sports, health, entertainment and geography), and 21 distinct language families including both high- and low-resource languages. Moreover, SIB-200 dataset is derived from the Flores-200 machine translation dataset (NLLB Team et al. 2022) containing parallel text across multiple languages facilitating direct comparisons. We evaluate the performance of the models in terms of macro-average F1 score.\nFor text generation, we rely on the Flores-200 dataset (NLLB Team et al. 2022) which is a multilingual dataset consisting of parallel text data in 204 different languages, covering a diverse range of language families and regions. We evaluate the performance in terms of SacreBLEU (Post 2018). We consider translations in the direction of xx to en where xx is a language from 203 languages (excluding en). We focus on the translation direction of xx to en in this work,"}, {"title": "Seen and Unseen Languages", "content": "For each model and dataset, we can further categorize the languages as 'seen' or 'unseen' based on whether they were included during the pretraining of the models or not. For instance, out of more than 200 languages present in SIB-200 and Flores-200 datasets, xglm, bloom, and bloomz models have seen only 30, 45, and 45 languages, respectively. Considering languages that are included as well as those that are not explicitly included in the pretraining corpus of the models allows for a more comprehensive analysis.\nThe categorization of seen and unseen languages was performed as follows. The bloom paper (Scao et al. 2023) lists the languages in its pretraining ROOTS corpus using ISO 639-3 codes (e.g., English is 'eng'), whereas the xg1m models' Hugging Face page specifies the languages in its pretraining corpus based on ISO 639-1 (e.g., English is 'en') codes. In the case of both the datasets, SIB-200 and Flores-200, languages are accompanied by their ISO 639-3 codes and script types. The mapping for bloom/bloomz models and the datasets is straightforward as they both document their languages in similar language codes. To map between xglm and the datasets, we use ISO-639 Python library to convert ISO 639-3 codes to ISO 639-1. In a handful of cases, more than one dataset language listed with different scripts got mapped to a single ISO 639-1 code. To resolve this, we employed the most common script type of that language to perform the mapping."}, {"title": "Prompts, Zero-shot, and Few-shot In-context Learning", "content": "Under zero-shot evaluations, the LLMs were prompted to classify text samples into predefined topic categories (in the case of SIB-200 dataset) or translate given sentences into English (for Flores-200) without any demonstrations. Under few-shot evaluations, following previous studies (Srivastava et al. 2023; Xia et al. 2023) the models were provided with a prompt and a limited number of demonstrations (k = 2).\nFor topic classification, this includes 2 instances per topic class in the same language as the target language, whereas for machine translation, this includes 2 source-target instances. It is important to note that the same randomly chosen samples from the training sets of the individual datasets were used for all the respective experiments, however, the order in which the demonstrations were presented to the model was randomized for each test instance and this was consistent across all languages.\nOur prompts were informed by prior work (Bach et al. 2022; Sampathkumar, Kravitz, and Huang 2023) and did not undergo any refinement, generally simulating realistic zero-shot or few-shot scenarios that a user might use while using these models. Informed by findings from previous work which demonstrated the effectiveness of English prompts compared to language-specific prompts (Lin et al. 2022; Lai et al. 2023; Adelani et al. 2024; Barrei\u00df, Klinger, and Barnes 2024; Etxaniz et al. 2023), we used English prompts in our experiments.\nFor obtaining the label in the text classification task, the models' output probabilities for each topic category were used to determine the predicted label, which was chosen based on the maximum log likelihood among a set of specified candidate label strings (Scao et al. 2023)."}, {"title": "Multilingual Scaling Results and Analysis", "content": "We now present our results and discuss key findings."}, {"title": "Scaling trends in classification vs. generation tasks", "content": "The overall scaling patterns in multilingual evaluation are presented in Figure 1.\nText classification For the classification task, we observe that both bloom and bloomz models show a predominantly U-shaped scaling trend, while xglm shows a relatively flat trajectory with a subtle hint of the \"double-descent\u201d phenomenon, where performance initially improves, then declines, and then improves again with increased scale (Nakkiran et al. 2019). These trends remain consistent for seen as well as unseen languages. However, a more zoomed-out view reveals that, surprisingly, the model sizes appear to have minimal impact as the line plots remain largely stable despite scaling from 560M to 7B, an almost 12.5 fold increase. Our results of scaling under 0-shot setting are different from those observed in an earlier study (Lin et al. 2022). While their results derived from a small set of seen languages show normal scaling under O-shot condition for xglm, we see only marginal scaling for xglm and bloomz and none for bloom, possibly because our results consider twice as many seen languages (30 languages) as their setup (15 languages), and more importantly, 13 of our seen languages are from mid- to low-resource categories.\nWhen we go from the 0-shot to the 2-shot setting, we find that bloomz's U-shaped scaling has turned to linear scaling, xglm's faint double-descent scaling turned to gradual linear scaling for seen languages, and bloom's U-shaped which was earlier followed by flat lines is now followed by linear scaling. Additionally, it is clear that 2-shot"}, {"title": "Text generation using bloomz", "content": "We now analyze the results of bloomz model in text generation under 0-shot and 2-shot settings, as shown in Figure 3. Here we notice normal scaling but a distinctive degradation in performance when going from 0-shot to 2-shot setting for both seen and unseen languages. On the one hand, this suggests that instruction-tuned models remain a promising approach to improving the performance of both seen unseen languages with increased scale, although the gap between seen and unseen languages remains significant. On the other hand, our observations also show that while few-shot in-context learning helps in text classification task, it does not seem to be effective in text generation."}, {"title": "Seen vs. unseen languages", "content": "From the results presented in Figure 1, for the classification task, we observe that in both 0-shot and 2-shot scenarios, as expected all models consistently perform better on the 'seen' languages compared to 'unseen' languages (solid lines for both tasks), aligning with findings from prior literature (Adelani et al. 2024). Specifically, xglm clearly outperforms the other models for 'seen' languages, which could be partially explained by a simple statistic \u2013 it encountered fewer languages during pretraining than the bloom/bloomz models, resulting in an average score over a fewer number of languages.\nTwo possible explanations for xglm's comparable performance, despite being trained on a much smaller number of languages than bloom/bloomz, may be found in the fact that 1) it was trained on a much larger number of pretraining tokens (500B) compared to bloom/bloomz models as mentioned earlier in Table 1, and 2) despite having fewer number of seen languages than bloom/bloomz, xglm has almost twice as many seen language families as bloom/bloomz which may be potentially contributing to positive cross-lingual transfer.\nOn the other hand, in the 2-shot setting, while xglm shows improvement for seen languages, its performance for unseen languages actually worsens. This is in stark contrast to the bloom/bloomz models where 2-shot ICL enhances performance for both seen and unseen languages, and particularly in the case of bloom, quite drastically so. It is not apparent why xglm's advantage for unseen languages in the O-shot setting does not carry over in the 2-shot setting. Another intriguing observation is that regardless of whether the languages were seen or unseen during training, bloom-1b1 consistently performs at the same level (converging to a specific point) yielding lower performance than its smaller counterpart, bloom-560m.\nWhen we consider the generation task, we see that overall, regardless of O-shot or 2-shot, the performance of the models is poor. bloom does improve significantly in the 2-shot setting for seen languages as compared to xglm (seen and unseen languages) and bloom-unseen. This behavior is in stark contrast with the classification task where all models and cases (seen/unseen) do better in the 2-shot case. Thus, ICL is useful for classification tasks but not so much for generative tasks."}, {"title": "Comparing text classification vs. text generation", "content": "To get a better sense of the quality of the scaling for the classification and generation tasks, we provide the slope of the best linear fit for all the cases from Figure 1 in Table 3. By and large, we observe very small slopes in most cases for the generation task and significant positive slopes for the 2-shot case for the classification task. We believe that in the classification case, seeing examples helps the classification task by conditioning the probabilities to select one of the provided class choices. In the generation task, on the other hand, providing context appears to reduce the ability of the model to generalize and thus we see poorer scores for 2-shot. One possible reason for this issue could be the need for more few-shot examples in the generative task. However, exploring additional settings was beyond the scope of this research."}, {"title": "Correlation Between Performance And Pretraining Data And Resource Levels", "content": "In order to better understand the scaling performance of the models, we dig in deeper into the languages that form the pretraining corpus. We experiment with using a rough proxy attribute such as general resource levels as introduced by Joshi et al. (2020) where languages of the world are categorized into 6 subsets, with level '0' representing the cluster of languages with the lowest amounts of data available (i.e., extremely low resource languages), and level \u20185' representing the languages that enjoy considerably large amounts of unannotated and annotated data (i.e., high resource languages).\nWe now analyze the correlation between model performance and properties of the languages in more detail by computing the correlation between model F1 score and pretraining data (PD), resource level (RL), and merged resource level (RL*) as illustrated in Table 4 for the 0-shot case.\nA prevalent hypothesis suggests that the performance of a model on a given language may be correlated with the amount of language-specific data present in the pretraining corpus (ImaniGooghari et al. 2023; Adelani et al. 2024). For the seen languages, we compute the Pearson's correlation between the models' F1 score and the pretraining data distribution as shown in the second column of Table 4. xglm obtains a correlation of at most 0.18, bloom shows moderate correlation (r = up to 0.68) whereas bloomz shows weak to moderate correlation (r = up to 0.4). These results indicate that: 1) a model's performance is not always correlated with the amount of language-specific data as in the case of xglm, and 2) despite being trained on the same pretraining corpus, different sized models show different correlations.\nWe next consider the correlation between model performance and language resource levels In the context of the classification task, we can make several observations: 1) The strongest correlation between the performance of seen languages and their resource levels is shown by bloom, followed bloomz, and lastly, xglm. This is reflected in how these models perform in the languages of the different resource levels. Our discussion carries forth to the generation task as well, with one notable exception. We see that xglm for the 2-shot case shows the highest performance for seen '0' resource languages. This is a statistical aberration because there is just one resource '0' language in the seen category for xglm.\nContinuing this line of investigation, we merge the three lower resource levels into one, and the three higher resource levels into another, thus obtaining a binary categorization of low and high. Using this, we compute the correlation one more time between F1 and low/high resourcedness and find that the correlation becomes even stronger.\nFinally, we extend this analysis to unseen languages by computing the correlation of the F1 scores and the (merged) general resource availability. Rather surprisingly, we notice that all models exhibit poor correlation. One possible explanation for this phenomenon is found in our analysis comparing the performance of seen and unseen languages from the same language family earlier in Figure 4 where the results confirm the assessment that unseen languages are able to obtain performance closer to seen languages from the same language family, possibly leveraging some benefits of cross-lingual transfer, which is certainly not being captured by simple correlations involving pretraining data distributions or resource levels.\nOverall, contrary to the results in (Winata et al. 2022), we show that a model's performance is not always correlated with the amount of language-specific data. Instead, our results suggest that general resource level seems to be a stronger indicator of performance for seen languages (but not for unseen languages).\nFuture work could explore enhancing the performance of low-resource languages while expanding the representation of resource levels."}, {"title": "Conclusion", "content": "We explored scaling trends in multilingual contexts, using three multilingual LLMs and two task types covering 204 languages. Our results highlight the complexities of multilingual model performance and scalability, emphasizing the need for careful consideration of various factors including whether the languages were seen or not during pretraining and shot settings (whether zero-shot or few-shot). Recently, Mosbach et al. (2023) showed that fine-tuning outperforms in-context learning in both in-domain and out-of-domain performance. On the other hand, another study shows that fine-tuning limits the generalization ability of the models introducing false correlations (Shliazhko et al. 2024). In light of such studies, it would be interesting to explore the (positive or negative) effects of fine-tuning in extensive multilingual scenarios in future work. Cao et al. (2024) highlighted that generative tasks differ from classification tasks. Investigating approaches to enhance the generalization of instruction-tuned models to unseen languages, considering that few-shot learning might not always be the optimal solution, presents a compelling direction for future research in natural language generation."}, {"title": "Ethics Statement", "content": "While we have included over 200 dialects and languages in our study, we acknowledge that many more languages remain to be comprehensively studied."}, {"title": "Appendix", "content": "0-shot vs. 2-shot settings"}]}