{"title": "Preference-Based Gradient Estimation for ML-Based Approximate Combinatorial Optimization", "authors": ["Arman Mielke", "Uwe Bauknecht", "Thilo Strauss", "Mathias Niepert"], "abstract": "Combinatorial optimization (CO) problems arise in a wide range of fields from medicine to logistics and manufacturing. While exact solutions are often not necessary, many applications require finding high-quality solutions quickly. For this purpose, we propose a data-driven approach to improve existing non-learned approximation algorithms for CO. We parameterize the approximation algorithm and train a graph neural network (GNN) to predict parameter values that lead to the best possible solutions. Our pipeline is trained end-to-end in a self-supervised fashion using gradient estimation, treating the approximation algorithm as a black box. We propose a novel gradient estimation scheme for this purpose, which we call preference-based gradient estimation. Our approach combines the benefits of the neural network and the non-learned approximation algorithm: The GNN leverages the information from the dataset to allow the approximation algorithm to find better solutions, while the approximation algorithm guarantees that the solution is feasible. We validate our approach on two well-known combinatorial optimization problems, the travelling salesman problem and the minimum k-cut problem, and show that our method is competitive with state of the art learned CO solvers.", "sections": [{"title": "1. Introduction", "content": "Traditional algorithms for combinatorial optimization (CO) often focus on improving worst-case performance. However, this worst case may occur rarely or never in real life. Data-driven approaches can help focus on the problem instances that do occur in practice. Previous work has used neural networks to solve CO problems (Wang et al., 2024). Graph neural networks (GNNs) are usually used in this role, since most CO problems are either naturally formulated on graphs or admit simple graph formulations. Since neural networks cannot output discrete solutions to CO problems directly, generic algorithms like Monte Carlo tree search, beam search, or sampling are often used to decode the GNN's continuous outputs into a solution to the CO problem. However, these methods are impractical to use during training because of their prohibitively long runtime. Omitting them during training means the usage of the GNN differs significantly between training and testing. Xia et al. (2024) argue that this inconsistency leads to uncertain performance during testing.\nIn order to avoid such drawbacks, we instead use the GNN to augment an existing, non-learned approximation algorithm for a given CO problem. We parameterize the approximation algorithm and let the GNN estimate the parameters based on the input graph. By selecting an approximation algorithm that is fast enough to be used during training, we can use the same overall pipeline during training and testing, addressing the concern raised by Xia et al. (2024). Our approach combines the advantages of the GNN and the traditional, non-learned approximation algorithm, and allows them to cancel out each other's weaknesses. The GNN leverages a data-driven approach to achieve good performance on the problem instances that occur frequently. However, it cannot output solutions to the CO problem directly. The CO approximation algorithm solves this by transforming the parameters predicted by the GNN into a feasible solution. On its own, the approximation algorithm cannot leverage the information in the dataset and it therefore performs comparatively poorly in practice, but this is offset by the GNN.\nSince the CO approximation algorithm's output is discrete, the algorithm is not differentiable. We therefore use gradient estimation to allow us to backpropagate through the approximation algorithm during training. We experiment using several existing gradient estimators, but we also introduce a new gradient estimation scheme, which improves over existing approaches for this purpose. We call our new scheme preference-based gradient estimation (PBGE). Our training setup allows for fully self-supervised training without the use of pre-computed optimal solutions to the CO problems.\nIn summary, our contributions include:\n1.  A self-supervised method for training a GNN for CO problems;\n2.  A novel gradient estimation scheme, PBGE, for back-propagating through CO approximation algorithms; and\n3.  Extensive experimental evaluation of (1) and (2) on two common CO problems."}, {"title": "2. Related Work", "content": "The easiest way of training a model for CO is to assume the existence of ground truth solutions to the CO problems and train in a supervised fashion. Vinyals et al. (2015) leverage the fact that many CO problems ask to identify a certain subset of the input (e.g. minimum-k-cut) or a permutation of the input (e.g. travelling salesman problem (TSP)). They introduce an LSTM-based sequence-to-sequence architecture where the elements of the output sequence are positions in the input sequence, which they name pointer network. Nowak et al. (2017); Joshi et al. (2019) recognize that most common CO problems have a natural graph representation. In their work, a GNN predicts an approximate solution as a heatmap, which is then decoded into a valid solution to the CO problem using beam search. Georgiev et al. (2023) follow a neural algorithmic reasoning approach to learn to imitate CO solvers. The model is pre-trained on simple algorithms and then fine-tuned on difficult ones. Finally, Sun & Yang (2023) uses graph-based denoising diffusion to generate high-quality solutions. However, these supervised approaches aren't applicable to such cases where calculating exact solutions for the training problems is not feasible.\nSeveral approaches have used reinforcement learning (RL) to remove this dependence on a labeled dataset. Bello et al. (2017) use a pointer network (Vinyals et al., 2015), but train it with RL. Khalil et al. (2017); Kool et al. (2019) use the GNN autoregressively to predict which node should be added next to the solution set and repeat that process until a valid solution is reached. Xu et al. (2020) formulate the CO problem as a Markov decision process, then use an algorithm similar to AlphaGo Zero (Silver et al., 2017) to solve it autoregressively. Kim et al. (2024) propose a modification to the RL training process for autoregressive methods that improves sample efficiency. However, the use of the score function estimator in many of these methods leads to high-variance gradients, which makes training more difficult.\nOther self-supervised approaches that do not rely on reinforcement learning include Duan et al. (2022), which use a contrastive loss instead. Schuetz et al. (2022) focus on quadratic and polynomial unconstrained binary optimization, which many CO problems can be formulated as. This allows them to make use of a self-supervised loss function specific to these two problem families. Sanokowski et al. (2024) also concentrate on quadratic unconstrained binary optimization, but employ a diffusion-based approach similar to Sun & Yang (2023). Similarly, Karalias & Loukas (2020); Bu et al. (2024) formulate a self-supervised loss function for a comprehensive class of CO problems. Toenshoff et al. (2021) use an LSTM-based architecture to solve binary maximum constraint satisfaction problems, which many CO problems can be formulated as. Since these approaches focus on certain families of CO problems, they aren't general to CO as a whole.\nGasse et al. (2019) use a GNN as a heuristic for the branch-and-bound algorithm, which guarantees exact solutions at the expense of a comparatively longer running time. The GNN is trained via imitation learning from a known high-quality but slow heuristic. Joshi et al. (2021) compare some of the paradigms introduced in other papers in structured experiments. There have been two lines of work to address the problem of backpropagating through combinatorial optimization problems. Firstly, if we have a set of optimal solutions given as training data, we can use supervised learning to train the GNN to output adjacency matrices as close as possible to the optimal solutions (Elmachtoub & Grigas, 2022). This is often called \"predict, then optimize\". Secondly, there are several methods to backpropagate through a non-differentiable CO algorithm, such as Niepert et al. (2021); Minervini et al. (2023); Vlastelica et al. (2020). Related to our work is decision-focused learning, which has developed several methods to backpropagate through CO solvers (Mandi et al., 2023)."}, {"title": "3. Background", "content": "3.1. Combinatorial Optimization Problems\nA combinatorial optimization (CO) problem asks us, given a discrete set M and an objective function \\(J_{CO}: M \\rightarrow \\mathbb{R}\\), to find the minimum\n\\[\n\\min_{x \\in M} J_{CO}(x).\n\\]\nOf course, maximization problems can be turned into minimization problems by inverting the sign. Since finding the exact global optimum is often not necessary in practice, this paper focuses on finding approximate solutions.\nTo illustrate our approach, we will refer to specific problems in combinatorial optimization as follows.\nMinimum k-cut problem. We are given a connected, undirected graph G = (V, E, w) with edge weights w : E \u2192 R>0, as well as a desired number of clusters \\(k \\in \\mathbb{N}, 2 \\leq k \\leq |V|\\). The goal is to find a set of edges C \u2286 E with minimal total weight whose removal leaves k connected components. This set is called a minimum k-cut. Formally, we are optimizing\n\\[\n\\min_{C \\subseteq E} \\sum_{e \\in C} w(e),\n\\]\ns.t. graph (V, E \\ C) has k connected components\nA commonly used approximation algorithm for solving the minimum k-cut problem is the Karger-Stein algorithm (Karger & Stein, 1993), which we describe in appendix A.1.\nTravelling salesman problem (TSP). The version of the TSP most commonly experimented on in related literature (Kool et al., 2019; Joshi et al., 2019; 2021) is called Euclidean TSP. We are given a complete, undirected graph G = (V, E, w) whose nodes V lie at points in the unit square, and whose edge weights w are the Euclidean distances between the respective points. The goal is to find a minimum-weight Hamiltonian cycle, i.e. a cycle that visits every node exactly once and where the sum of the weights of edges that are traversed in the cycle is minimal.\nA well-known probabilistic approximation algorithm for solving the TSP is the random insertion algorithm (Karg & Thompson, 1964), which we describe in appendix A.2.\n3.2. Residual Gated Graph Convnets\nWe use residual gated graph convnets (Bresson & Laurent, 2017), but adapt them to include edge features \\(e_{ij}\\) and a dense attention map \\(n_i\\) following (Joshi et al., 2019). The input node features \\(x_i\\) and edge features \\(e_{ij}\\) are first pre-processed using a single-layer MLP for each of the two. Each further layer is computed as follows:\n\\[\nx_i^{l+1} = x_i^l + \\text{ReLU} \\left( BN \\left( W_1^l x_i^l + \\sum_{j \\in N_i} \\eta_{ij} W_2^l x_j^l \\right) \\right) \\in \\mathbb{R}^d,\n\\]\nwith \\(\\eta_{ij} = \\frac{\\sigma(e_{ij})}{\\sum_{l \\in N_i} \\sigma(e_{il})},\\)\n\\[\ne_{ij}^{l+1} = e_{ij}^l + \\text{ReLU} \\left(BN\\left( W_3^l e_{ij}^l + W_4^l x_i^l + W_5^l x_j^l \\right)\\right).\n\\]\nwhere \\(W_1, ..., W_5 \\in \\mathbb{R}^{d \\times d}\\) are learnable weights, d is the hidden dimension, ReLU is the rectified linear unit, BN is batch normalization, \\(\\sigma = \\frac{1}{1+e^{-x}}\\) is the element-wise sigmoid function, and \\(\\varepsilon\\) is an arbitrary small value. \\(\\odot\\) denotes the Hadamard product, and \\(N_i\\) denotes the set of nodes that are adjacent to i.\nThe final edge-level output is calculated from the last layer's edge features \\(e_{ij}\\) using another MLP. For the remainder of the paper, \\(f_\\theta(G) \\in \\mathbb{R}^{|E|}\\) refers to applying this GNN on a graph G.\n3.3. REINFORCE\nGiven a function J and a parameterized probability distribution \\(p_\\theta(x)\\), the REINFORCE algorithm (Williams, 1992), also known as the score function estimator, estimates the true gradient as follows:\n\\[\n\\nabla_\\theta \\mathbb{E}_{y \\sim p_\\theta(x)}[J(y)] = \\mathbb{E}_{y \\sim p_\\theta(x)}[J(y) \\nabla_\\theta \\log p_\\theta(x)]\n\\]\n\\[\n\\approx \\frac{1}{S} \\sum_{i=1}^S J(y_i) \\nabla_\\theta \\log p_\\theta(x_i), y_i \\sim p_\\theta(x_i)\n\\]\nWhile this estimator is applicable even for discrete distributions and is unbiased, it suffers from large variance. implicit maximum likelihood estimator (I-MLE) (Niepert et al., 2021) is another gradient estimator that produces biased estimates with smaller variance."}, {"title": "4. Problem Statement", "content": "We consider CO problems on graphs with a linear objective function \\(J_{CO}\\) and a probabilistic approximation algorithm \\(h(\\hat{y} | G)\\). The approximation algorithm takes as input a graph G = (V, E, w) with nodes V, edges E and edge weights w: E \u2192 R>0, and returns (samples) a potentially suboptimal solution \\(\\hat{y}\\). For instance, we might have the minimum k-cut problem as defined in subsection 3.1.\nWe now want to use a GNN \\(f_\\theta\\) parameterized by \u03b8 applied to the input graphs G to compute an updated graph G' = \\(f_\\theta(G)\\) such that the probabilistic approximation algorithm when applied to this new graph is improved in expectation.\nHence, we want to solve the following optimization problem:\n\\[\n\\min_\\theta \\mathbb{E}_{\\hat{y} \\sim h(y | f_\\theta (G))} [J_{CO}(\\hat{y})].\n\\]\nFor each input graph G, \\(h(\\hat{y} | f_\\theta(G))\\) is a discrete probability distribution (due to the assumption that h is probabilistic) parameterized by \u03b8. The main challenge for solving this optimization problem is that the (discrete) approximation algorithms are typically not differentiable functions and that optimal solutions are prohibitively expensive to obtain as training data. Moreover, we assume that the approximation algorithm is a black box. This means that while we can sample from the probability distribution defined by it, we cannot calculate a probability for a given sample.\nNote that if the approximation algorithm available to us is deterministic, we can convert it into a probabilistic approximation algorithm by simply adding noise to the input, as demonstrated by e.g. Niepert et al. (2021). For instance, this can take the form of adding noise from a Gumbel distribution to the edge weights of the input graph.\nWith this paper, we propose PBGE, a new approach based on preference-based learning. It estimates gradients by contrasting better and worse solutions, similar to preference-based learning in the context of large language models."}, {"title": "5. Method", "content": "Figure 1 shows an overview of our approach. We augment an existing probabilistic approximation algorithm for a given CO problem using a GNN. The GNN receives the problem graph as input and produces a prior score for each edge. These scores are used as additional input alongside the graph for a parameterized version of an off-the-shelf CO approximation algorithm, which then produces a solution to the CO problem.\nThis same pipeline is used both during training and testing. Since the approximation algorithm is not differentiable in general, we use gradient estimation to obtain the gradients with respect to the GNN's output. Existing gradient estimation schemes such as REINFORCE or I-MLE can be used for this, as introduced in subsection 3.3. We propose a new scheme for this purpose: preference-based gradient estimation (PBGE). The remainder of this section elaborates on our proposed training procedure as well as PBGE.\n5.1. Parameterizing Approximation Algorithms\nA given CO approximation algorithm that takes a problem graph as input can be parameterized by modifying the input graph, then running the approximation algorithm on the modified graph.\nWe calculate the modified graph G' by using the GNN's output to change the edge weights. Assume there is an arbitrary but fixed ordering of edges. The model outputs a prior score for each edge, \\(s = f_\\theta(G) \\in \\mathbb{R}^{|E|}\\). A high score for a given edge is interpreted to mean that the respective edge should belong to the solution set with a higher probability mass. The approximation algorithms we use prefer including edges of low weight in the solution set. Therefore, we scale down the weights of edges that received high scores. Specifically, the edge weights are multiplied with \\(1 - \\sigma(s)\\), where \u03c3 is the element-wise sigmoid function. By running the CO approximation algorithm on this modified graph, we parameterize the approximation algorithm using the GNN's output scores s. In the formulas in the remainder of this paper, \\(h(\\hat{y} | G, s)\\) represents the probability distribution defined by a probabilistic CO approximation algorithm parameterized in this way. It samples and outputs a vector \\(\\hat{y} \\in {0,1}^{|E|}\\) that represents a solution to the CO problem, such as a TSP tour or k-cut. A value of 1 in this vector indicates that the corresponding edge is in the solution set.\n5.2. Preference-Based Gradient Estimation (PBGE)\nIn preference learning, a training instance consists of an input and a pair of possible outputs. The supervision signal is an annotation indicating that one of the outputs \\(y_w\\) is of higher quality than the other output \\(y_l\\). We can construct a similar setup for combinatorial optimization by leveraging a pre-existing probabilistic CO approximation algorithm h. Sampling from the approximation algorithm multiple times likely yields two solutions \\(\\hat{y}_1, \\hat{y}_2 \\sim h(\\hat{y} | G, s)\\) of different quality for a given problem instance G ~ D from dataset D. These solutions can easily be ranked by applying the CO problem's objective function Jco. This means assigning \\(\\hat{y}_w\\) and \\(\\hat{y}_l\\) such that \\(J_{CO}(\\hat{y}_w) < J_{CO}(\\hat{y}_l)\\).\nWe now propose the following preference-based loss function:\n\\[\nL(\\mathcal{D}, s) = \\mathbb{E}_{\\hat{y}_\\omega, \\hat{y}_l \\sim h(\\hat{y} | G, s), G \\sim \\mathcal{D}} \\left[ d(\\hat{y}_w, \\hat{y}_l) \\log \\frac{h(\\hat{y}_w | G, s)}{h(\\hat{y}_l | G, s)} \\right]\n\\quad (1)\n\\]\nHere, \\(d(\\hat{y}_w, \\hat{y}_l)\\) is a scaling factor. As we will see later, its purpose is to scale the gradients based on the distance between the objective values of the losing and winning solutions.\nSince we treat the CO approximation algorithm as a black box, we cannot calculate the probabilities \\(h(\\hat{y}_w | G, s)\\) and \\(h(\\hat{y}_l | G, s)\\) directly. We therefore introduce a proxy distribution \\(\\pi(\\hat{y} | G, s) \\approx h(\\hat{y} | G, s)\\) for which we can obtain probabilities directly. For all approximation algorithms we use in this paper, a high prior score in s for a certain edge increases the probability of this edge being included in the output \\(\\hat{y}\\). This motivates the use of an exponential family distribution to model the proxy distribution \u03c0 for h:\n\\[\n\\pi(\\hat{y} | G, s) = \\frac{\\exp(-(\\hat{y}, s))}{\\mathbb{E}_{y' \\in \\mathcal{C}} \\exp(-(y', s))},\n\\quad (2)\n\\]\nwhere \\((,)\\) is the inner product and C is the set of all solutions to the CO problem.\nReplacing h with \u03c0 in Equation 1 and inserting Equation 2 simplifies the loss function to\n\\[\nL(\\mathcal{D}, s) = \\mathbb{E}_{\\hat{y}_w, \\hat{y}_l \\sim h(\\hat{y} | G, s), G \\sim \\mathcal{D}} \\left[ d(\\hat{y}_w, \\hat{y}_l) ((\\hat{y}_l, s) - (\\hat{y}_w, s)) \\right].\n\\quad (3)\n\\]\nNow, the gradient of this expectation w.r.t. s is\n\\[\n\\nabla_s L(\\mathcal{D}, s) = \\mathbb{E}_{\\hat{y}_w, \\hat{y}_l \\sim h(\\hat{y} | G, s), G \\sim \\mathcal{D}} \\left[ d(\\hat{y}_w, \\hat{y}_l) (\\hat{y}_l - \\hat{y}_w) \\right],\n\\quad (4)\n\\]\nwhose single-sample Monte Carlo estimate can be written as\n\\[\n\\nabla_s L(G, s) \\approx d(\\hat{y}_w, \\hat{y}_l) (\\hat{y}_l - \\hat{y}_w),\n\\]\nwhere \\(\\hat{y}_\\omega, \\hat{y}_\\iota \\sim h(\\hat{y} | G, s).\nIntuitively, the gradient is -1 at a certain edge if that edge is in the better solution, but not in the worse solution. A gradient of -1 raises the GNN's output score, meaning that the GNN will be nudged towards including this edge in its solution. Similarly, a gradient of 1 means that the corresponding edge was in the worse solution, but not in the better solution. A positive gradient nudges the GNN's output down, so it pushes the GNN towards not including this edge.\nThis gradient is similar to the ones used for preference learning with large language models (LLMs) (Rafailov et al., 2023; Meng et al., 2024): the gradient nudges the model to increase the likelihood of the better solution and to decrease the likelihood of the worse solution, and a scaling factor is used to weight important gradients more highly. Unlike the preference learning setting used with LLMs, we not only know which solution in a pair is better, but we can measure the quality of each solution exactly using the objective function. This means that we don't need to rely on human annotators to rank pairs of examples. On top of this, we can leverage the objective function to more easily compute a suitable scaling factor.\nGradient scaling. If the solutions \\(\\hat{y}_w\\) and \\(\\hat{y}_l\\) are of similar quality, we do not want to strongly nudge the GNN towards either solution. We therefore opt for scaling the gradient with the relative gap in quality between the two solutions,\n\\[\nd(\\hat{y}_w, \\hat{y}_l) = \\frac{J_{CO}(\\hat{y}_l)}{J_{CO}(\\hat{y}_w)} - 1.\n\\]\nNote that this is always positive, because \\(J_{CO}(\\hat{y}_l) > J_{CO}(\\hat{y}_w)\\). Using this scaling factor puts a larger emphasis on pairs of solutions where the difference in quality is large. In particular, if the two solutions are of the same quality, the gradient is set to zero, so we do not nudge the GNN towards either solution. The scaled gradient is\n\\[\n\\nabla_s L(G, s) \\approx \\left( \\frac{J_{CO}(\\hat{y}_l)}{J_{CO}(\\hat{y}_w)} - 1 \\right) (\\hat{y}_l - \\hat{y}_w).\n\\]\nThe variance of the gradient can be decreased estimating the expectation in Equation 4 with more than one sample. However, this requires sampling two additional solutions from the approximation algorithm for each additional pair. We can increase the number of pairs more efficiently by first sampling a pool of solutions from the approximation algorithm, then constructing pairs from this pool. Since we want a gradient that nudges the model towards the best known solution, we form the pairs by combining the best solution from the pool with each of the weaker solutions.\nIn practice, the accuracy of the gradients depends heavily on the quality of the best found solution \\(\\hat{y}_w\\). In our experiments, we noticed that early during training, the GNN cannot yet output good enough scores to consistently find reasonable \\(\\hat{y}_w\\). To remedy this, we also run the approximation algorithm on the unmodified graph and add the resulting solutions to the pool from which the pairs are generated."}, {"title": "5.3. Decoding at Test Time", "content": "Regardless of how the model was trained, at test time, the model's output needs to be converted (decoded) to a solution to the CO problem. This can simply be done by running the CO approximation algorithm with the model's output as input, as described in section 4. The solution can be improved by running a probabilistic CO approximation algorithm repeatedly and using the best solution found as final output. Other decoders (like beam search) are also available, but haven't shown promising results in preliminary experiments.\nIn the case of the Karger-Stein algorithm, we noticed empirically that simply modifying the input graph can lead to degenerate behavior during testing. The Karger-Stein algorithm uses the graph's edge weights in two places: (1) when sampling an edge for contraction and (2) when comparing the cuts that resulted from different recursion arms. We noticed that the performance of our overall method can be improved when using a model trained with the setting described in subsection 5.2 by using the modified edge weights for the first case and the original edge weights for the second case. Intuitively, if the GNN makes a mistake when scaling the edge weights, using the original edge weights for comparing cuts can allow the Karger-Stein algorithm to find the optimal cut regardless.\nFor the algorithms we use in our experiments, we prove the following theoretical result, which shows that we can turn an approximation algorithm into an exact algorithm if we find an optimal modified input graph for Karger-Stein or random insertion."}, {"title": "6. Experiments", "content": "We validate our approach on two well-known combinatorial optimization problems: the travelling salesman problem (TSP) and the minimum k-cut problem. For both problems, we synthetically generate problem instances and establish baselines as reference.\n6.1. Problem Instance Generation\nFor minimum k-cut, we use the established graph generator NOIgen (Nagamochi et al., 1994). Since it relies on dramatically scaling down the weights of edges that are in the minimum k-cut in order to avoid trivial solutions, it makes it easy for a GNN to identify the correct edges. To make the graphs more challenging, we extend NOIgen to also use graph structure to avoid trivial solutions, which allows us to scale down edge weights less dramatically. We call this improved graph generator NOIgen+. We also generate unweighted graphs that only rely on graph structure to prevent trivial solutions. Graphs for the TSP are generated according to the established method described in Kool et al. (2019); Joshi et al. (2019; 2021).\n6.2. Baselines\nSince we assume a setting without access to ground truth solutions to the CO problems, our primary baselines are gradient estimation schemes for unsupervised training. We set the loss to\n\\[\nL(\\mathcal{D}, s) = \\mathbb{E}_{\\hat{y} \\sim h(\\hat{y}|G,S), G \\sim \\mathcal{D}} [J_{CO}(\\hat{y})]\n\\]\nand estimate \\(\\nabla_s L(\\mathcal{D}, s)\\) using REINFORCE (Williams, 1992) in the case of TSP and I-MLE (Niepert et al., 2021) in the case of the minimum k-cut problem. Additionally, for TSP, we also compare against a simple baseline that runs random insertion on the input graph 20 times and treats the best solution found as ground truth for a binary cross-entropy loss. We call this baseline \u201cBest-of-20\".\nFor additional context, we train models in a supervised fashion using an edge-level binary cross-entropy loss comparing the GNN's output scores s with a pre-calculated ground truth solution y. For minimum k-cut, we also train using I-MLE, comparing the approximation algorithm's output \\(\\hat{y}\\) with y using a Hamming loss. See Appendix C for details on these baselines.\n6.3. Minimum k-Cut\nWe evaluate our method on the minimum k-cut problem, using the Karger-Stein algorithm as a base. Table 1 shows optimality gaps of the unmodified Karger-Stein algorithm, as well as several versions of our method. Each version augments the Karger-Stein algorithm with a GNN, and they differ by how the GNN was trained. Note that when augmenting the Karger-Stein algorithm with a GNN trained with PBGE, the optimality gap improves by an order of magnitude. On top of this, even though it didn't use any ground truth solutions during training, the GNN trained with PBGE comes close to matching the GNN trained supervised with a binary cross entropy loss.\nIn practice, the most important metric is the number of runs it takes for Karger-Stein to find the optimal k-cut. If this number is low, we can run Karger-Stein a small number of times and be reasonably certain that the minimum k-cut was found. Figure 2 shows for how many graphs the minimum k-cut is found in a set number of runs, comparing the unmodified Karger-Stein algorithm with two versions that were augmented using a GNN. On both datasets, the augmented Karger-Stein algorithm needs much fewer runs to find the minimum k-cut, almost always finding it on the first attempt. Again, the GNN trained self-supervised with PBGE comes close to matching supervised performance.\n6.4. Travelling Salesman Problem (TSP)\nTable 2 shows the optimality gaps of our approach and its variants. All of our models were trained using random insertion as the CO approximation algorithm. For PBGE, we sampled 10 solutions from \\(h(\\hat{y} | G, s)\\) and 10 solutions from \\(h(\\hat{y} | G)\\).\nThe decoder used at test time is listed after the name of the respective method in parentheses. Greedy search starts from an arbitrary node, and follows the edge with the highest score to select the next node. This process of greedily following the best edge is repeated until each node has been visited once. To ensure that the resulting tour is valid, edges that lead to nodes that have already been visited are excluded. Beam search also starts with an arbitrary node, then explores the b edges with the highest scores. This gives us b partial solutions. In each iteration, each partial solution is expanded at its last node, and out of the resulting paths, the b best partial solutions are kept. As before, edges that would lead to invalid tours are ignored. The parameter b is called the beam width, and beam search with b = 1 corresponds to greedy search. Sampling simply refers to sampling multiple solutions and using the best one."}, {"title": "7. Conclusion", "content": "We introduced a method to improve existing approximation algorithms for CO using GNNs. The GNN predicts parameters, which are used as input for the non-learned approximation algorithm to produce a high-quality solution to the CO problem. The GNN is trained based on the CO problem's downstream objective, without the need for labelled data. To achieve this, we used gradient estimation to backpropagate through the approximation algorithm. We proposed a novel gradient estimation scheme for this purpose, which we called preference-based gradient estimation (PBGE).\nLimitations and future work. Incorporating a CO approximation algorithm during training means that the training process is more computationally intensive compared to competing approaches. This also means that an existing approximation algorithm is required for our approach. We only experimented on CO problems for which solutions can be represented in terms of the graph's edges. While extending our approach to other kinds of CO problems is theoretically possible, we leave this for future work. We also plan to incorporate the ability to formulate additional constraints."}, {"title": "A. Algorithms for Specific Combinatorial Optimization Problems", "content": "A.1. Minimum k-Cut Problem\nA.1.1. KARGER'S ALGORITHM\nKarger's algorithm (Karger, 1993) is a Monte Carlo algorithm for the minimum k-cut problem.\nThe algorithm is based on the contraction operation: An edge e = {x,y} is contracted by merging its nodes x and y into a new node xy. For clarity, we will call the node that results from this merger a meta-node. Every edge that was incident to exactly one of the two merged nodes is now altered to instead be incident to the meta-node xy: An edge {x, z} or {y, z} becomes {xy, z}. This may result in parallel edges, meaning that the resulting graph is a multigraph. All edges {x,y} are removed, so that the resulting multigraph contains no self-loops.\nKarger's algorithm works by repeatedly sampling an edge, where the probability of each edge is proportional to its weight, then contracting that edge. This is repeated until there are only k nodes left. Each of these remaining k meta-nodes represents a connected component in the original graph, with each node of the original graph that was subsequently been merged into that meta-node belonging to this connected component. Any edge in the original graph that spans between two connected components is cut.\nSince this algorithm is not guaranteed to find the minimum k-cut, a common strategy is to run the algorithm repeatedly and use the smallest found cut as the final result.\nA.1.2. KARGER-STEIN ALGORITHM\nThe Karger-Stein algorithm (Karger & Stein, 1993) is a recursive version of Karger's algorithm, shown in Algorithm 2.\nA.2. Random Insertion for the Travelling Salesman Problem\nThe random insertion algorithm (Karg & Thompson, 1964) is a Monte Carlo algorithm for the TSP.\nSince a Hamiltonian cycles to a given graph G = (V, E) is required to contain every v \u2208 V exactly once, it is a straightforward approach to iteratively sample and remove nodes from V until it is empty. The random insertion algorithm, as suggested by Karg and Thompson, begins by selecting two nodes s, t \u2208 V at random and adds the edges (s, t) and (t, s) to an initial cycle. In order to extend the cycle to include all nodes, the algorithm now samples a node v \u2208 V \\ {s, t} and selects the edges (x, v) and (v, y) such that x and y are already part of the partial cycle with x \u2260 y and such that the sum of the metric distances of (x, v) and (v, y) is minimal."}, {"title": "B. Proofs", "content": "We assume that the edges E of a graph G = (V, E, w) are in an arbitrary but fixed order. This means that the scores assigned to the edges by a GNN can be represented as a vector s \u2208 R|E|. We use s[e"}]}