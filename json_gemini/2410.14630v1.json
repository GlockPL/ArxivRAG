{"title": "On the Regularization of Learnable Embeddings for Time Series Processing", "authors": ["Luca Butera", "Giovanni De Felice", "Andrea Cini", "Cesare Alippi"], "abstract": "In processing multiple time series, accounting for the individual features of each sequence can be challenging. To address this, modern deep learning methods for time series analysis combine a shared (global) model with local layers, specific to each time series, often imple-mented as learnable embeddings. Ideally, these local embeddings should encode meaningful representations of the unique dynamics of each sequence. However, when these are learned end-to-end as parameters of a forecasting model, they may end up acting as mere sequence identifiers. Shared processing blocks may then become reliant on such identifiers, limiting their transferability to new contexts. In this paper, we address this issue by investigating methods to regularize the learning of local learnable embeddings for time series processing. Specifically, we perform the first extensive empirical study on the subject and show how such regularizations consistently improve performance in widely adopted architectures. Furthermore, we show that methods preventing the co-adaptation of local and global parameters are particularly effective in this context. This hypothesis is validated by comparing several methods preventing the downstream models from relying on sequence identifiers, going as far as completely resetting the embeddings during training. The obtained results provide an important contribution to understanding the interplay between learnable local parameters and shared processing layers: a key challenge in modern time series processing models and a step toward developing effective foundation models for time series.", "sections": [{"title": "1 Introduction", "content": "Collections of related time series characterize many applications of learning systems in the real world, such as traffic monitoring (Li et al., 2018; Yu et al., 2018), energy analytics (Dimoulkas et al., 2019; Gasparin et al., 2022), climate modeling (Ma et al., 2023; Chen et al., 2023), and biomedical data processing (Jarrett et al., 2021; Zhang et al., 2022). The success of deep learning in the associated tasks, e.g., forecasting (Shih et al., 2019; Benidis et al., 2022), imputation (Cao et al., 2018; Cini et al., 2022) and virtual sensing (Wu et al., 2021), relies on effectively modeling shared patterns across time series while also accounting for their individual characteristics (Benidis et al., 2022). In this context, models must rely on some attributes or positional"}, {"title": "2 Preliminaries", "content": "This section introduces the notation and formalizes the problem of time series forecasting, with a focus on the class of deep learning architectures we consider in this paper."}, {"title": "2.1 Problem settings", "content": "Consider a collection $\\mathcal{D}$ of $N$ time series, where each $i$-th time series consists of a sequence of $T$ observations $\\{x_t^i \\in \\mathbb{R}^{d_x}\\}_{t=1}^T$. Specifically, we indicate as related time series a set of homogenous time series coming from the same domain but generated by different sources (e.g., different sensors). Examples include sales figures for different products, or energy consumption of various users. Time series might be acquired both asynchronously and/or synchronously; in the latter case, we denote the stacked $N$ observations at time step $t$ by the matrix $X_t\\in \\mathbb{R}^{N \\times d_x}$ and use the shorthands $X_{t:t+T}$ to indicate the sequence of observations within the time interval $[t, t+T)$ and $X_{<t}$ to indicate those up to time $t$, excluding $t$. Eventual (exogenous) covariates associated with each time series are denoted by $u \\in \\mathbb{R}^{d_u}$ ($U_t \\in \\mathbb{R}^{N \\times d_u}$). We focus on multistep-ahead time series forecasting, i.e., the problem of predicting the next $H$ future values for each $i$-th time series $x_{t:t+H}^i$ given exogenous variables and a window of $W$ past observations. We focus on point forecasts, while probabilistic predictors might also be considered."}, {"title": "2.2 Hybrid global-local architectures for time series", "content": "We consider a broad class of models similar to those in (Benidis et al., 2022) and (Cini et al., 2023a). In particular, we consider predictors such as\n$x_{t:t+H}^i = F(x_{t-W:t}^i, U_{t-W:t+H}^i; \\theta), \\quad i = 1, ..., N$  (1)\nwhere $\\theta$ are the learnable parameters of the model, $x_{t:t+H}^i$ indicates the predicted values of $x_{t:t+H}^i$ and $F(\\cdot; \\theta)$ is a family of parametric forecasting models. Models in Eq. 1 do not take into account (spatial) dependencies that might exist among time series. In scenarios such as spatiotemporal forecasting, where such dependencies might be relevant to achieve accurate predictions, models that can operate on sets of (synchronous) correlated time series should be preferred. In particular, we consider a family of models\n$X_{t:t+H}^S = F(X_{t-W:t}^S, U_{t-W:t+H}^S; \\theta), \\quad \\forall S \\subseteq \\mathcal{D}$ (2)\nwhere $F(\\cdot; \\theta)$ operates on subsets $S$ of the collection of time series $\\mathcal{D}$ and $X_t^S \\in \\mathbb{R}^{|S|\\times d_x}$ the resulting stack of observations at time step $t$. Models of this kind can be implemented by architectures operating on sets (Zaheer et al., 2017), e.g., attention-based architectures (Vaswani et al., 2017; Grigsby et al., 2021) or spatiotemporal graph neural networks (STGNNs) (Jin et al., 2023; Cini et al., 2023a) based on message passing (Gilmer et al., 2017). These models can eventually account for priors on existing dependencies among time series, which could be encoded, e.g., by a graph adjacency matrix $A \\in \\mathbb{R}^{N \\times N}$ (Cini et al., 2023a). Note that models in both families (Eqs. 1 and 2) are global, i.e., they share parameters among all the time series being processed,"}, {"title": "3 Related works", "content": "Learnable embeddings are key components of state-of-the-art time series processing architectures such as STGNNs (Bai et al., 2020; Cini et al., 2023a) and attention-based models (Marisca et al., 2022; Liu et al., 2023; Xiao et al., 2024). In particular, Cini et al. (2023b) systematically addresses the role of such embeddings"}, {"title": "4 Regularization strategies for local embeddings", "content": "In this section, we discuss possible shortcomings of hybrid global-local forecasting architectures and present the regularization strategies employed in our experimental analysis.\nGlobal models are inherently less likely to overspecialize to individual sequences as, having limited capacity, it would compromise perfor-mance w.r.t. other sequences within the set. The introduction of local embeddings reduces the regularization effect brought by this induc-tive bias, resulting in a model that may become more susceptible to overfitting to individual time series, with a potential negative impact on both performance and transferability (Cini et al., 2023b). Model regularization is a common approach to deal with overfitting and to control model and sample complexity. For instance, one might apply well-known techniques, such as weight decay (Krogh & Hertz, 1991) or dropout (Srivastava et al., 2014), across the entire network. However, in many scenarios, the global processing blocks are not particularly overparametrized, as weights are shared. Conversely, in global-local models, one could consider regularizing the local parameters only, i.e., in our settings, constraining how embeddings are learned."}, {"title": "4.1 Regularization methods", "content": "In the following, we present the regularization methods that will later be part of our experimental analysis in Sec. 5. They range from adaptations of standard approaches to recent contributions from the literature. We also consider an approach based on parameter resetting.\nL1 and L2 regulariation L2 regularization (Krogh & Hertz, 1991), also known as weight decay, consists in adding to the loss a penalty term proportional to the magnitude of the model's weights. When applied to the embeddings, the penalty term is $\\mathcal{L}_{L2}(E) = \\frac{\\lambda_{l2}}{2} \\cdot ||E||_2^2$. Similarly, L1 regularization (Tibshirani, 1996), also known as lasso, consists in applying a penalty term of $\\mathcal{L}_{L1}(E) = \\lambda_{l1} \\cdot ||E||_1$. The positive scalar values, $\\lambda_{l2}$ and $\\lambda_{l1}$, control the regularization strength.\nDropout Dropout (Srivastava et al., 2014) is another widely recognized regularization. This consists in randomly masking out individual neurons during training, with probability $p$, while scaling the others by $\\frac{1}{p}$. This technique heuristically forces the network to learn robust representations, often preventing overfitting. In our context, we apply it to the embedding vectors by randomly masking out some parameters for each embedding.\nClustering The clustering regularization introduced in (Cini et al., 2023b) is based on learning a set of cluster centroids and a cluster assignment matrix, alongside the embeddings. A regularization term is then added to the loss to minimize the distance between embeddings and the assigned centroid. It was originally designed to structure the embedding space and improve model transferability.\nVariational regularization Variational regularization (Cini et al., 2023b), consists in modeling embeddings as samples from a Gaussian posterior learned from data $e^i \\sim \\mathcal{N} (\\mu^i, diag(\\sigma^i))$, where $\\mu^i$ and $\\sigma^i$ are the learnable local parameters. A penalty term based on the KL-divergence between the learned distribution and a standard Gaussian prior, is added to the loss. Similarly to the clustering regularization, it was introduced to enhance model transferability.\nForgetting We also consider a novel forgetting regularization, based on the forget-and-relearn paradigm (Zhou et al., 2021). This refers to strategies where the parameters of some neural network layers are occasionally reset during training. By doing this, it is possible to reduce memorization of training samples (Baldock et al., 2021), avoiding shortcuts and learning representations that generalize better (Geirhos et al., 2020; Zhou et al., 2021). In practice, we periodically reset the local embeddings $E$ to a sample from a shared initialization distribution $\\mathcal{P}_e$ (Narkhede et al., 2022), every $K$ training epochs. Concurrently, we similarly reset the ENCODER's and DECODER's weights (Eq. 7) that directly multiply the embeddings (see Appendix C.1). The value of $K$ can be easily selected empirically (see Appendix C.2). Additionally, resetting is halted after a certain amount of epochs to allow for convergence to a final configuration. This can be tuned manually or automatically, by monitoring the validation loss."}, {"title": "4.2 Preventing co-adaptation", "content": "As mentioned in Sec. 1, jointly learning local and global parameters can result in their co-adaptation. In particular, embeddings might become simple sequence identifiers and result in models that rely entirely on this identification mechanism. Besides the negative effects on sample efficiency, the resulting architecture would likely lose flexibility in terms of the transferability of the learned representations and processing blocks. In the empirical analysis, we will then focus on whether methods designed to break this co-adaptation can be effective regularizations. Intuitively, the global processing block would be less likely to rely on embeddings as identifiers if these are actively perturbed during training, e.g., by resampling or zeroing.\nAmong the regularization methods considered in the analysis, L1 and L2 regularizations simply penalize the embeddings' magnitude. Similarly, clustering encourages embeddings to occupy specific regions within the embedding space and does not actively perturb their values. Conversely, by randomly zeroing out embeddings' parameters, we might expect dropout to prevent co-adaptation more effectively, as similarly happens when applying it to the weights of subsequent layers (Srivastava et al., 2014). The variational regularization can"}, {"title": "5 Experiments", "content": "We evaluate the effectiveness of different regularization strategies for local embeddings under three different scenarios: time series forecasting benchmarks (Sec. 5.1), transfer learning (Sec. 5.3), and a sensitivity analysis through embedding perturbations (Sec. 5.4). We consider six real-world datasets of time series collections, spanning four different application domains: METR-LA and PEMS-BAY (Li et al., 2018) as two established benchmarks for traffic forecasting, AQI (Zheng et al., 2015) from the air quality domain, CER-E (CER, 2016) from the energy consumption domain, CLM-D (De Felice et al., 2024) and EngRAD (Marisca et al., 2024) as two multivariate climatic datasets. Details on the datasets, data splits and forecasting settings can be found in Appendix A.\nRegarding the investigated models, we consider three different hybrid global-local architectures (Sec. 2.2) distinguished by three different implementations of the propagation layer (Eq. 4). In particular: 1) \u0430 RNN with gated recurrent units (GRUs) cells (Cho et al., 2014b) (RNN), 2) a STGNN stacking GRU and anisotropic message-passing layers (Bresson & Laurent, 2017) (STGNN), and 3) a GRU followed by multi-head attention across sequences (Vaswani et al., 2017) (STAtt). Such hybrid architectures are representative of the current state of the art in the considered benchmarks and of the dominant deep learning frameworks for processing sets of related time series (Benidis et al., 2022; Cini et al., 2023a). Implementation and architectural details are provided in Appendix B, while details on the experimental settings can be found in Appendix D."}, {"title": "5.1 Time series forecasting benchmarks", "content": "In our first experiment, we consider the problem of time series forecasting in a transductive setting, i.e., the set of time series to be forecast is the same set observed at training time.\nThe forecasting horizon is dataset-dependent and reported in Tab. 4 of Ap-pendix A. We use models without local parameters and with un-regularized local embeddings as reference baselines. Then, we evaluate performance with regularized local embeddings, adopting the different strategies detailed in Sec. 4.1. For model selection, a hyperparameter search on model size and learning rate has been carried out independently for each model variant and dataset. All regularization hyperparameters have been set as detailed in Appendix D. Tab. 1 shows the obtained results, while Tab. 2 provides a con-cise summary of the performance of each regularization across experimental settings. Regularizing the learning of local embeddings provides performance improvements over non-regularized models in most datasets (F1). Consid-ering the negligible computational overhead and ease of implementation, these results support the adoption of such techniques as standard practice. While there is no clear winner among the different regularization methods, the variational regularization appears to be the most effective, on average; followed by forgetting and dropout. This supports adopting methods that prevent co-adaptation by design, i.e., that prevent the global components from relying on specific values of the local parameters (F2) (F3). Additional results on the impact of regularizing embeddings when Eq. 4 is implemented by a simple MLP are reported in Appendix E."}, {"title": "5.2 Sensitivity analysis and learning curves", "content": "To provide additional insight, we investigate how different regularizations affect the learning curve across different embedding sizes. In doing so, we fix the shared model hidden size to h = 64 and learning rate"}, {"title": "5.3 Transfer learning", "content": "Our second main experiment consists of time series forecasting under a transfer learning setting, adapted from previous works (Cini et al., 2023b). This benchmark aims to verify the impact of regularizing the local embeddings on the transferability of the global (shared) processing blocks. We consider the 4 PEMS benchmark datasets from Guo et al. (2021) (i.e., PEMS03, PEMS04, PEMS07, PEMS08) and a reference"}, {"title": "5.4 Robustness to local parameter perturbation", "content": "Finally, we investigate how a model's forecasting accuracy is affected by the perturbation of the learned local embeddings. This provides insights into the robustness of the shared learned parameters and can serve as a proxy for the effectiveness of different regularizations in preventing co-adaptation. To avoid penalizing regularizations that are sensitive to the weights' magnitude (i.e., L2, L1), we consider perturbations that do not impact the scale of the learned representations. In particular, we experiment with four such strategies: adding zero-mean Gaussian noise to the embeddings (Noise \u03c3), randomly shuffling embeddings across sequences (Rearranged), replacing each embedding with their sample mean (Mean), resampling each embedding from their sample normal distribution (Sampled) (see Appendix D.1 for a formal definition). Fig. 4 shows the results of the analysis after training an STGNN on the METR-LA dataset (complementary settings are illustrated in Appendix F). Regularization methods that actively perturb the embeddings while learning (i.e., dropout, forgetting, variational) consistently result in more robust models under all the considered perturbations (F2) (F3). Conversely, other regularizations have marginal, or even negative, impact. We observe some consistency between the results observed in Fig. 4 and performance in transfer learning, shown in Tab. 3."}, {"title": "6 Conclusions", "content": "This paper highlights the importance of regularizing the learning of local embeddings in modern deep-learning architectures for time series forecasting. Our empirical study, across diverse datasets and scenarios, provides clear evidence that this practice is beneficial for a variety of reference architectures, representative of the state of the art. Notably, even simple techniques, when applied to local embeddings, can yield consistent performance gains. Furthermore, we observed that methods that actively perturb the embeddings at training time, such as dropout, variational regularization, and forgetting, consistently rank among the top performers, both in transductive and transfer learning settings. Overall, our findings offer a strong empirical argument for"}, {"title": "Limitations and future works", "content": "While it is clear that these regularizations provide consistent advantages, it is difficult to strongly identify the best-performing method in the different scenarios. Hence, future works might focus on finding strategies that combine the best qualities of the different methods. Furthermore, future studies could try to provide methods to quantitatively and analytically characterize the co-adaptation of global and local parameters, for which we can only identify indirect signs. In particular, they might design experiments to detect the degeneration of local components into mere identifiers. Moreover, future research could explore how the efficacy of different regularization methods varies w.r.t. the number of input time series and different downstream tasks."}, {"title": "Appendix", "content": "A Datasets"}, {"title": "B Models", "content": "In this section we describe the models used in our study. Note that all models used a fixed hidden size $d_h$ for all layers. For all the architectures, the ENCODER is parametrized by a linear layer, while the DECODER is a 1-layer MLP followed by $H$ parallel linear layers, each decoding a different step in the forecasting horizon. The RNN model is implemented by means of a 1-layer GRU (Cho et al., 2014b) shared among all sequences. On top of the same GRU architecture, for the STGNN model, we employ 2 layers of message passing defined as\n$m_i^{\\rightarrow i} = W_2 \\xi(W_1 [h_t^i||h_t^j||a_{ji}]), \\quad a_{ji} = \\sigma (W_o m_i^{\\rightarrow i})$, (8)\n$h_t^i = \\xi (W_3 h_t^i + \\sum_{j\\in N(i)} \\frac{a_{ji} m_i^{\\rightarrow i}}{|N(i)|} )$, (9)\nwhere $W_o, W_1, W_2$ and $W_3 \\in \\mathbb{R}^{d_h \\times d_h}$ are learnable parameters, $||$ is the concatenation operator along the feature dimension, $\\xi$ denotes the elu (Clevert et al., 2016) activation function and $\\sigma$ the sigmoid activation function. Furthermore, $N(i)$ denotes the neighbours of the $i$-th sequence, induced by the adjacency matrix $A$, $a_{ji}$ denotes the weight associated to edge $j \\rightarrow i$ and $h_t^i$ and $h_t^j$ denote the hidden features associated with the $i$-th and $j$-th sequences respectively. Regarding the STAtt model, we use 2 layers of multi-head attention (Vaswani et al., 2017) taking as input tokens the hidden representations extracted by a temporal encoder which shares the same architecture as the aforementioned RNN. Finally, for consistency with the original experiment (Cini et al., 2023b), we employ an STGNN consisting of a 1-layer GRU and 2 message passing layer implementing\n$\\hat{h}_t^i = \\xi \\Big(W_1 h_t^i +  \\frac{1}{|N(i)|} \\sum_{j\\in N(i)} W_2 h_t^j\\Big )$, (10)\nwhere $|\\cdot|$ is the cardinality operator."}, {"title": "C Additional details on forgetting regularization", "content": "For the sake of completeness, in the following, we list some additional details regarding the local forgetting regularization."}, {"title": "C.1 Embedding-related Encoder/Decoder parameters reset", "content": "When resetting the local parameters, we also reset the encoder's (Eq. 3) and decoder's (Eq. 5) parameters (i.e., coefficients of a linear layer) that directly interact with the embeddings' features. As an illustrative example, consider an ENCODER parametrized by a MLP, with input linear layer\n$h_t^1 = [X_{t-1}||U_{t-1}||E] W^T + b$, (11)\nIn this case, the encoder's parameters to be reset would correspond to the last $d_e$ columns of the weight matrix $W$. An equivalent behavior is implemented for the decoder's input layer."}, {"title": "C.2 Forgetting period sensitivity", "content": "The introduced forgetting regularization has two hyperparameters: the reset period $k$ and the halting epoch. While the latter can be determined automatically, by monitoring the validation error before each reset, the former should be set empirically. To provide insight on the degree to which the selection of $k$ can impact regularization performance, we perform a sensitivity study on 3 datasets: METR-LA, PEMS-BAY and AQI."}, {"title": "C.3 Forgetting warm-up", "content": "To avoid instabilities at the beginning of training, particularly when using short reset periods, the forgetting routine can be initiated after a warm-up period, which can be set empirically to a few epochs, as commonly done with learning rate warm-up procedures or with other regularizations (Cini et al., 2023b)."}, {"title": "D Detailed experimental setting", "content": "Shared settings All the models were trained with the Adam optimizer (Kingma & Ba, 2015), with a batch size of 64 and up to 300 batches per epoch. We used the Python (Van Rossum & Drake, 2009) programming language, leveraging Torch Spatiotemporal (Cini & Marisca, 2022), Pytorch (Paszke et al., 2019) and Pytorch Lightning (Falcon & The PyTorch Lightning team, 2019) to implement all the experiments. Experiments were scheduled and logged by leveraging Hydra (Yadan, 2019) and Weights and Biases (Biewald, 2020). The mean absolute error (MAE) was used as loss function in all experiments. For the different regularization's hyperparameters, we heuristically found sensible values (i.e., with stable performance across datasets, models"}, {"title": "D.1 Embeddings perturbation", "content": "Here we provide a more formal definition of the perturbations adopted in Sec. 5.4. In particular, considering $N$ embedding vectors:\nNoise $\\sigma$ consists in adding zero-mean gaussian noise, from an isotropic multivariate normal distribution, to the embeddings. Formally, this refers to substituting each embedding $e^i$ with $e^i + \\epsilon$, where $\\epsilon \\sim \\mathcal{N} (0, diag (\\sigma^2))$.\nRearrange refers to randomly reassigning the embedding vectors to different time-series in the collection. Formally, this means substituting each embedding $e^i$ with an embedding $e^j$, where $j \\sim Multinomial \\{{0, ..., N - 1}\\}$. is a sample from a multinomial distribution over the embedding indices, sampled without repetition.\nMean refers to setting each embedding to the mean values across embeddings themselves. Formally, replacing each embedding $e^i$ with $\\frac{1}{N} \\sum_{i=0}^{N-1} e^i$.\nSampled consists in estimating the sample gaussian distribution of the embeddings, and then replacing each embedding with a draw from such distribution. Formally, this entails estimating the gaussian's"}, {"title": "E Effect on simple global models", "content": "The purpose of model regularization is, usually, to limit model capacity by means of additional constraints and, in turn, obtain models that generalize better. This implies that excessive regularization might result in a degradation of performance. In principle, regularization of local parameters should not have a negative impact on underparametrized global models, as their parameters are not constrained directly. To evaluate this, we employ a simple 2-layer MLP as our global model (Eq. 4), and train it with the same settings as in Sec. 5.1."}, {"title": "F Additional experiments", "content": "In this section, we provide additional experimental results, for completeness w.r.t. what has been shown in the main paper.\nIn particular, Fig. 6 complements what has been shown in Fig 2. We can see how, for all the models, regularization at the embedding level seems the preferable choice, even though regularizing the whole model does not impair performance equally in all the settings (e.g., STAtt).\nSimilarly, Fig. 7 complements the results shown in Fig. 3, with the models that were not shown. We can notice a similar pattern in which dropout (red) can be problematic at smaller embedding sizes (top row). Nonetheless, this seems to be less significant for the STGNN in this specific scenario. In general, the additional figures confirm that dropout (red) and forgetting (blue) lead to different learning curves w.r.t. the un-regularized model (orange), while other regularizations have little impact on this aspect.\nFig. 8 show results obtained by training models with the same setting as Sec. 5.2 on the PEMS-BAY dataset. We can see that, in this case, the validation curves barely plateau. In this scenario, dropout shows potentially problematic effects, similar to those observed for small embedding sizes.\nFinally, Fig. 9 and Fig. 10, provide complementary results for Fig. 4. We can notice how the different regularizations rank similarly, in terms of global model robustness, across different architectures."}]}