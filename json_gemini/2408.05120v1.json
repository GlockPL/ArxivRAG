{"title": "Cautious Calibration in Binary Classification", "authors": ["Mari-Liis Allikivia", "Joonas J\u00e4rve", "Meelis Kull"], "abstract": "Being cautious is crucial for enhancing the trustworthiness of machine learning systems integrated into decision-making pipelines. Although calibrated probabilities help in optimal decision-making, perfect calibration remains unattainable, leading to estimates that fluctuate between under- and overconfidence. This becomes a critical issue in high-risk scenarios, where even occasional overestimation can lead to extreme expected costs. In these scenarios, it is important for each predicted probability to lean towards underconfidence, rather than just achieving an average balance. In this study, we introduce the novel concept of cautious calibration in binary classification. This approach aims to produce probability estimates that are intentionally underconfident for each predicted probability. We highlight the importance of this approach in a high-risk scenario and propose a theoretically grounded method for learning cautious calibration maps. Through experiments, we explore and compare our method to various approaches, including methods originally not devised for cautious calibration but applicable in this context. We show that our approach is the most consistent in providing cautious estimates. Our work establishes a strong baseline for further developments in this novel framework.", "sections": [{"title": "1 Introduction", "content": "Classifier calibration is used to improve probabilistic predictions of classification models. For example, in binary classification, if a set of instances were assigned a predicted positive class probability of 0.8, then 80% of these instances should actually be positive. This is an appealing property both intuitively for humans to interpret the model's predictions and for optimal decision making.\nA key application of calibrated probability estimates lies in their usefulness in mediating between the model training stage and the downstream application. Since exact knowledge of costs and other variables in downstream tasks is often unavailable or subject to change, calibrated models are invaluable in these practical scenarios by being adaptable to these fluctuations [12]. This makes the further development of different post hoc calibration methods like isotonic calibration [17], logistic calibration [14], beta calibration [9] and other approaches for training calibrated models like temperature scaling, matrix and vector scaling [8] or Dirichlet calibration [10] an active area of research.\nHowever, a universal challenge with all calibration methods is their inability to reach perfect calibration. These methods aim to learn the true calibrated values, which reflect the actual unknown conditional probabilities $Pr(Y = 1|z_i)$ of observing class 1 for a predicted score $z_i$, i.e. the confidence predicted by a model. This ideal is not achievable due to having finite amounts of data, but also since methods have limitations due to their parametric nature (e.g., logistic, beta calibration) or biases like a tendency towards overconfidence (e.g., isotonic regression) [1] or other reasons that cause under- and overconfidence [2]. Even the most effective methods still estimate the true probability slightly inaccurately, with the direction of this deviation remaining unknown. This issue is illustrated in Figure 1A. Suppose that we know the true calibration map\u00b9 (black) and sample 100 calibration sets from it using the approach described in Section 5. Isotonic calibration has been applied to all calibration sets and 100 estimated maps have been learned (grey). The plot demonstrates how isotonic calibration, one of the classical and widely used calibration methods, is on average quite precise in its estimation of the true calibration map, but individual estimations fluctuate between over- and underconfidence.\nIn situations where costs of different errors vary significantly, overconfidence can lead to excessively high costs while underconfidence results in slightly less optimal costs, as will be demonstrated in Section 2. As decision-making in case of imbalanced costs happens in the high-probability region, we focus only on probabilities 0.9 and higher, as we need high certainty to make risky decisions. From Figure 1A we can see that while calibration methods can be precise on average, one single learned map (red) can be overconfident for some scores and underconfident for others. As overconfidence leads to worse outcomes in our problem setting, we want to avoid it for every individual map at every score. Behind every score there is a group of instances in our distribution that will be mapped to that prediction, from now on referred to as a score group. In order to avoid bad outcomes for all score groups, we should expect every one of them to perform well, instead of only the model's average outcome being good.\nOur research introduces a novel concept called cautious calibration"}, {"title": "2 Example Scenario: Optimal Risk Level Selection", "content": "The usefulness of cautious calibration can be demonstrated in a setting where each prediction leads to a decision with potentially different costs. Before defining this setting formally, let's build intuition by following a simplified scenario in the self-driving domain. Imagine a machine learning model that is trained to identify images of clear roads (class 1) and roads with obstacles (class 0). Based on the predicted probability (probability for the road to be clear), one has to select the speed of the car, or in other terms, select the risk level. The more certain we are, the higher speed we are willing to choose. However, the outcomes are dependant on the chosen speed. If we choose a high speed, then we will gain in faster arrival times when observing class 1, but lose greatly in case observing class 0 and causing an accident. This means that we should only select high risks in case of very high predicted probabilities, meaning that the chances of mistaking are extremely small. In case of lower speed, it will take longer to reach the destination, but accidents are less costly.\nWe call this setting optimal risk level selection and formalize it in the following way. We have a probability prediction function $\u0109 : \\mathcal{X} \\rightarrow [0, 1]$, that, based on the input, predicts the estimates of the true calibrated probability, similarly to our model that predicted the probability for the road to be clear. We have an outcome function $o : (\\mathcal{Y}, [0,\\infty)) \\rightarrow \\mathbb{R}$, that calculates the outcome based on a class label y and a risk level $\u03be \\in [0,\\infty)$. This would equate to calculating the gain or cost \u00b2 of selecting a speed and then observing either clear road or road with an obstacle, which in this toy example is obviously difficult to define. We have selected the following outcome function to illustrate our ideas:\n$o(y, \u03be) = \\begin{cases} \u03be & \\text{if } y = 1,\\\\  - \u03be^l & \\text{if } y = 0, \\end{cases}$\nwhere $l \\in [1,\\infty)$ is a fixed constant describing the imbalance of costs. This function is chosen for its simplicity and for how it captures the idea of small positive gains for seeing a positive class versus large negative costs for seeing a negative class, worsening greatly with the increase of the risk level.\nThe final missing part is to decide how to choose a risk level $\u03be$ based on a true calibrated probability $c \\in [0,1)$ (we omit the trivial case of c = 1). We do it by choosing the risk level that gives the highest expected outcome, assuming the data points are drawn from $Y \\sim Bernoulli(c)$. This can be thought of as optimizing the outcome for one score group. We can define it formally as:\n$\\text{ropt}(c) = \\arg \\max_{\\xi} E_{Y \\sim Bern(c)}[o(Y, \\xi)] = \\arg \\max_{\\xi} \\left( c \\cdot \\xi + (1-c) \\cdot (-\\xi^l) \\right)$\nSince in practice we don't have access to the true calibrated probability c, we use the estimation \u0109 to select the optimal risk level. However, as $c \\neq \u0109$ in practice, we will not be able to choose the truly optimal risk level for the true underlying data generation process $Y \\sim Bernoulli(c)$. This means that our chosen risk level $\u03be_\u0109 = \\text{ropt}(c)$ will result in an expected outcome $E_{Y \\sim Bern(c)}[o(Y, \\xi_\u0109)]$, which will be worse compared to the optimal one $E_{Y \\sim Bern(c)}[ E_{Y \\sim Bern(c)}[o(Y,\\xi_c)]$.\nFigure 2 illustrates the difference in how much the expected outcome worsens when the calibration estimate is overconfident vs underconfident. We create a hypothetical example where for each true probability c we have one overconfident estimation $\u0109_+ = c + 0.01$ and one underconfident estimation $\u0109_- = c - 0.01$. Then, for a range of different c's represented on the x-axis, we find the true optimal risk level $c = \\text{ropt}(c)$ and the corresponding expected outcome $E_{Y \\sim Bern(c)}[o(Y, \\xi_c)]$, the latter illustrated by the solid black line. The dotted line represents the expected outcome when the risk level is chosen with the underconfident estimation $\u00ea_-$. We can see that the expected outcome is smaller than optimal, especially for high probabilities, but at the same time it stays on the positive side. For the analogous situation with the overconfident estimation $\u00ea_+$ shown as the dashed line, we see a different trend, where in case of overestimation of high probabilities, our chosen risk level is too large to an extent where it results in extremely negative expected costs.\nOur main purpose is to develop cautious calibration methods and measure how consistent they are in maintaining underconfidence. However, this theoretical setting is a way to measure the impact of cautiousness on the expected outcomes resulting from a specific decision-making process. Optimal risk level selection is just one example of a scenario where cautiousness is necessary. Further exploration to find other high-risk decision-making frameworks could enhance our understanding of the role and benefits of cautiousness."}, {"title": "3 Related work", "content": "To our best knowledge, no works have presented the need to directly use cautiously calibrated probabilities for decision-making. Still, there are methods that try to estimate a confidence interval around the predicted probabilities. This is usually done to evaluate the trustworthiness of the prediction, but a lower value of the confidence interval can be used for building a cautious calibration map. We will discuss several methods and describe the theoretical problems that might occur when using these methods for cautious calibration estimation.\nSimplified Venn-Abers predictors [15]. Venn-Abers predictors output multiple probability predictions given one score, one of which"}, {"title": "4 Cautious Calibration", "content": "Before explaining our solution and the motivation behind it, we introduce the setting, which follows the typical binary classification"}, {"title": "4.1 Notation", "content": "framework. We consider data points drawn i.i.d from $(\\mathcal{X}, Y) \\sim \\mathcal{D}$ from the distribution $\\mathcal{D}$ over $\\mathcal{X} \\times \\mathcal{Y}$, where $\\mathcal{X}$ is the instance space and $\\mathcal{y} = \\{0, 1\\}$ the label space. We define a scoring model $s : \\mathcal{X} \\rightarrow \\mathbb{R}$ where a larger score $s(X)$ indicates larger confidence towards class 1. We look at the different partitions of the observed data: training data $\\mathcal{D}_{\\text{train}} \\sim \\mathcal{D}$ for training the scoring model, calibration data $\\mathcal{D}_{\\text{cal}} \\sim \\mathcal{D}$ for calibrating the model post-training (similarly to methods like isotonic and logistic calibration) and test data $\\mathcal{D}_{\\text{test}} \\sim \\mathcal{D}$ for evaluation. The calibration data $\\mathcal{D}_{\\text{cal}} = \\{(x_i, Y_i)\\}_{i=1}^n$ is ordered so that $z_1 \\le z_2 \\le \\dots \\le z_n$ where $z_i = s(x_i)$ is the model output score for the instance $x_i$.\nIn conclusion, the calibration data is given as follows:\n*   Datapoint vector $\\mathbf{x} = (x_1, x_2,\\dots, x_n)$\n*   True label vector $\\mathbf{y} = (y_1, y_2,\\dots, y_n)$\n*   Model output vector $\\mathbf{z} = (z_1, z_2,\\dots, z_n)$\n*   True unknown calibrated probability vector $\\mathbf{c} = (c_1, c_2,\\dots, c_n)$ where $c_i = Pr(Y = 1 | z_i)$.\nSimilarly to classical post-hoc calibration methods, we assume monotonicity between the model output scores $\\mathbf{z}$ and the true calibrated values, meaning that $c_1 < c_2 <\\dots < c_n$ as well. This is to say that we trust the model's ordering of scores but not their values. The aim of both calibration and cautious calibration methods is to learn the estimates $\\hat{\\mathbf{c}} = (\\hat{c}_1, \\hat{c}_2,\\dots,\\hat{c}_n)$ for the true calibrated probabilities based on the true label vector $\\mathbf{y}$. However, while traditional methods aim for accurate average estimates, cautious calibration opts for consistently lower estimates to avoid overconfidence (while still avoiding the trivial, but useless case of $\\hat{\\mathbf{c}} = (0, . . ., 0)$)."}, {"title": "4.2 Motivation", "content": "As mentioned in Section 3, Clopper-Pearson intervals are intuitively fitting for cautiousness since they provide conservative confidence intervals with probabilistic guarantees [4]. However, these intervals are traditionally applied to a binary vector generated from a Bernoulli process with a constant probability parameter, which in the remark of Definition 4 we term as a homogeneous Bernoulli vector. This differs from the calibration scenario, where each label in a calibration set is derived from a Bernoulli trial with a unique parameter $c_k$, defining this as a heterogeneous Bernoulli vector in Definition 4. Calibration data labels $\\mathbf{y}$ or a subsequence of them is a heterogeneous Bernoulli vector, where the underlying probabilities are monotonically increasing.\n**Definition 1.** *Let* $\\mathbf{p} = (p_1, p_2, ..., p_m)$ *with each* $p_i \\in [0, 1]$ *for* $i = 1, ..., m$. *If given a random vector* $S^{\\mathbf{p}} = (S^{\\mathbf{p}}_1, S^{\\mathbf{p}}_2, ..., S^{\\mathbf{p}}_m)$, *where* $S^{\\mathbf{p}}_i \\sim Bernoulli(p_i)$, *then we call* $S^{\\mathbf{p}}$ *a heterogeneous Bernoulli vector and denote* $S^{\\mathbf{p}} \\sim HBernoulli(\\mathbf{p})$.\n**Remark.** *For ease of reference, if* $p_1 = p_2 = \\dots = p_m$, *we call the heterogeneous Bernoulli vector a homogeneous Bernoulli vector.*\nA problem arises when treating a subsequence of calibration data labels $y_{k-m+1},...,k$ as a homogeneous Bernoulli vector and applying the same lower bound to every element in that subsequence."}, {"title": "4.3 Lower Bound Calculation", "content": "**Selecting Label Subsequences for Cautious Calibration.** As discussed in the previous section, assigning equal lower bound estimates to all elements in a bin can result in estimates that are overconfident. We prove that if we only use a subsequence of labels preceding an element $k$ to calculate the lower bound $\\hat{c}_k$, then we can probabilistically guarantee that the bound is correct. This will be shown in the end of this subsection in Theorem 2.\nFigure 4 demonstrates the process of calculating a lower bound $\u0109_k$ for a true probability $c_k$. First we select a subsequence of labels $y_{k-m+1},...,k$ of length $m$ preceding $k$. We then calculate a statistic $t_k$ with a function $f_m$ that assigns a real value to a binary vector of length $m$ (for the sake of clarity, we will refer to any function calculating a statistic as a *statistic function* throughout this article). In the case of Clopper-Pearson intervals, this function $f_m$ is the sum of ones in the selected subsequence. The statistic is then mapped to the corresponding lower bound estimate $\u0109_k$, calculated with the lower bound function $LB_{f_m,q}$, which will be defined later. As every $k$ has its own preceding subsequence, lower bounds are calculated separately for each $k$ from its respective subsequence. Meaning that $\u0109_k$ is a function of a subsequence of labels $Y_{k-m+1,...,k}$, where $1 < k-m+1<k \u2264 n$ and $k, m \u2208 N$. The algorithm for calculating a lower map for one calibration set is described in Supplementary A.\n**Inverted Hypothesis Testing for Lower Bound Estimation.** Before explaining which type of guarantees this approach holds, we need to understand the process of lower bound calculation. We will start by explaining the idea behind one-sided Clopper-Pearson intervals [5] as an example of inverted hypothesis testing for interval calculation. Then, we generalize this approach to work with a wider set of statistic functions.\nSuppose we have a homogeneous Bernoulli vector with parameter $p$. In the Clopper-Pearson case, we calculate the sum function statistic on this random vector, which leads to a binomial distribution. This means we can form and test a hypothesis about $p$ using the binomial test. Unlike classical hypothesis testing, which fixes one certain value of $p'$ for the hypothesis (e.g. $H_0: p \\le p'$) and tests it once, inverted testing evaluates every possible $p' \\in [0, 1]$ the same way. All values of $p'$ where the null hypothesis is rejected form an interval $[0, p']$ of values, for which the alternative hypothesis ($H_1: p \\ge p'$) is supported, meaning that the true parameter is likely to be larger than these values. The maximum of these values $p'$ is used as a lower bound estimate.\nThe previous example used homogeneous Bernoulli vectors for calculations, but real-world calibration involves heterogeneous Bernoulli vectors. In theory, it should be possible to construct a new type of confidence interval on these vectors using an inverted hypothesis testing approach, but it would be impractical due to the excessive number of potential hypotheses. Nevertheless, as will be confirmed in Theorem 2, constructing our lower bound calculation method on homogeneous Bernoulli vectors and applying it correctly on heterogeneous Bernoulli vectors (taking only left subsequences) provides conservative lower bounds with probabilistic guarantees.\nStill, as the homogeneous Bernoulli vector intervals are not tailored for heterogeneous ones, using the standard Clopper-Pearson version might not be the only or the best way for lower bound estimation. We cannot change the homogeneous/heterogeneous approach, but we can exchange the sum statistic function for another type of statistic. For example, we could consider a new recursive statistic function that examines all smaller subsequences than a given size m, calculates their lower bounds, and selects the maximum value as the final statistic. This approach can avoid some of the problems caused by fixing just a single m, as that might not be the subsequence giving the best or largest lower bound.\nMotivated by this, we generalize the inverted hypothesis testing method to work with all statistic functions that are *monotonic w.r.t 0\u21921 bit-flipping* (definition in Supplementary B). These are a group of functions $f_m: \\{0,1\\}^m \\rightarrow \\mathbb{R}$ that maintain or increase its value when any 0 in its input is flipped to a 1. An example would be the sum function, which returns 2 in the case of vector (0, 1, 1, 0) and 3 in the case of vector (0, 1, 1, 1), where the last 0 is flipped to 1. All such functions can be used instead of the sum function to get different cautious calibration estimations with the same probabilistic guarantees.\n**Lower Bound Calculation and Probabilistic Guarantees.** The lower bound calculation with the inverted hypothesis testing approach is formalized in Definition 7. Intuitively, the set in the definition consists of the aforementioned set of probabilities p for which"}, {"title": "4.4 Practical Calculation of Lower Bounds", "content": "We will be using two different statistic functions for lower bound calculation. Given an arbitrary binary vector $y \u2208 \\{0,1\\}^m$ and $m\u2208 N$, the first is the sum function $f_{\\text{sum}}(y) := \\sum_{i=1}^m y_i$, making the applied method equivalent to the Clopper-Pearson method. The other is called max-cp, namely\n$f_{\\text{max-cp}}(y) = \\max\\{LB_{f_{\\text{sum}},q}(f_{\\text{sum}}(Y_{m_1-j+1,...,m_2})), m_1 \u2264 j \u2264m_2\\}$\nwhere the max-cp statistic function looks at all subsequences from size $m_1$ to $m_2$ ($m_1 \u2264 m_2$) ending at the last element, calculates Clopper-Pearson lower bounds for all and takes the maximum of those as the estimate. As proven in Supplementary B, both statistic functions are monotonic w.r.t. 0\u21921 bit-flipping, making them suitable for estimating lower bounds with probabilistic guarantees.\nWe will describe the practical calculation of both of these bounds. These approaches differ in many aspects, since for the easier sum statistic functions, the used CDFs can be described analytically, making the computations exact and fast. For the max-cp statistic we need to calculate approximate CDFs with simulations, as the distribution is much more complex.\nFor both methods, we employ the left subsequence approach with inverted hypothesis testing for lower bound calculation. For a clearer understanding of the experiment results, we introduce the following acronyms: HTLB (Hypothesis Testing Lower Bounds) denotes the general method using one-sided inverted hypothesis testing confidence intervals on left subsequences, HTLB+CP refers to the Clopper-Pearson a.k.a. sum statistic function, and HTLB+MAXCP applies to the max-cp statistic.\n**Practical Calculation of HTLB+CP Lower Bounds.** Calculating the lower bound function for the sum statistic is straightforward due to the direct relationship between the Beta and Binomial distributions in case of Clopper-Pearson intervals [4]. The lower bound function is defined as:\n$LB_{f_{\\text{sum}}, q}=$ q'th quantile of Beta($\\alpha$ = t, $\\beta$ = m \u2212 t + 1),\nwhere t is our sum statistic, i.e., the number of 1s in our subsequence, m is the subsequence length, making m-t equal to the nr of 0s in the sequence. This quantile can be easily obtained with the percent-point function of the same Beta distribution.\n**Practical Calculation of HTLB+MAXCP Lower Bounds.** The max-cp statistic function, being more complex, lacks an analytical solution for direct calculation. This means that we must create our own empirical distributions for the inverted hypothesis testing for as many p values as we can and precalculate a mapping between statistic values and corresponding lower bounds. The precalculation has to be done for each $m\u2081$ and $m2$ pair separately, which is computationally intensive. However, once the precalculation exists, the using of the lower bounds only requires using the existing mapping. The main steps for calculating a lower bound mapping for $m1, m2, q$ and $f_{\\text{max-cp}}$ are as follows:\n1.  We select as many values of p as we are able to use for calculations.\n2.  For each p, we sample homogeneous Bernoulli vectors, calculate statistics with $f_{\\text{max-cp}}$ on them and end up with an empirical distribution over the statistic values for each p.\n3.  For each p, we find the statistic value that is the q'th quantile of its empirical distribution.\n4.  We reverse the mapping so that each statistic value is mapped with the minimum p for which it is the q'th quantile. \u00b3"}, {"title": "5 Experiment setup", "content": "On real data, the theoretical guarantees hold, but due to the true calibrated values being unknown, we cannot evaluate or compare the"}, {"title": "5.1 Data generation and evaluation", "content": "cautiousness of different methods. This means that we have to generate our own data where the ground truth is known. The data generation process consists of two parts: generating true calibration maps and generating calibration sets.\nTrue calibration map generation. The true calibrated probability $c_k = Pr(Y = 1|z_k)$ is the probability of observing class 1 when the predicted score is $z_k$. Although theoretically, the true calibration map is continuous, in practice, we always estimate it on our finite calibration set. This is why we represent the true calibration map as a vector of true probabilities c corresponding to our calibration data set points. As stated before, we assume that there is a monotonic relationship between $z_k$ and $c_k$. We want to generate maps with a broad coverage over different shapes, for which we use a simple recursive algorithm described in [1]. An example of 100 generated maps can be found in Supplementary C. The maps are generated for high probabilities between 0.9-1, as stated before, since decision-making in high-risk scenarios takes place only in case of high certainty.\nCalibration set generation. Next, we need a calibration set in order to be able to learn an estimated calibration map and compare it with the true one. If the true calibration map is known, the calibration set can be thought of as a realization of a heterogeneous Bernoulli vector with parameter c. This means that to obtain one calibration set y, for each k, we sample from $y_k \\sim Bernoulli(c_k)$ to obtain a label for the k'th element. We can sample multiple different calibration label vectors from one true calibration map, learn estimated calibration maps on them and see the behaviour of the calibration algorithm w.r.t. the true calibrated values."}, {"title": "5.2 Methods", "content": "Calibration methods. First, we include some classical calibration methods in our experiment that are meant for estimating true calibrated probabilities. These methods include isotonic calibration (isocal) [17], logistic calibration (logcal) [14] and beta calibration (betacal) [9]. Label smoothing [7] has been applied to isotonic and logistic calibration due to its usefulness in reducing overconfidence. Although these methods don't try to be cautious, including them in comparison will help to understand the difference between cautious and classical approaches in these scenarios.\nExisting methods repurposed for cautious calibration. Next, we include methods that are meant for quantifying uncertainty about the predicted probabilities and can be repurposed for cautious calibration. The first method, the only one in this section used directly in its original form, is lower bound estimation from simplified Venn-Abers predictors (SVA) [15]. As written earlier, this method is very close to isotonic calibration but tends to prefer underconfidence. The second method is a combination of two previously mentioned approaches. We take the isotonic calibration binning from [11] and the Clopper-Pearson idea from [13] and combine them into one method we call isobins+CP. The third method is very similar to the last one, where instead of isotonic calibration bins, we use the reliably calibrated isotonic calibration (RCIR) [11] bins, where too small bins have been joined to get a more reliable prediction. This is also paired with Clopper-Pearson lower bounds and is called RCIR+CP.\nOur methods for cautious calibration. We use our HTLB+CP and HTLB+MAXCP approaches with subsequence size 2000. This choice was made based on empirical evaluation, as it provided quite stable but not overly cautious maps, demonstrated in Supplementary C. Small subsequence sizes were excluded since it is not possible to predict high enough lower bounds with too little evidence. Too large sizes can also be overly cautious on our 10K sized datasets. For HTLB+MAXCP, we used subsequence sizes between 100 (m1) and 2000 (m2) in order to exclude the very small subsequences.\nRuntime and complexity. Most of the methods we have used for comparison are either linear or loglinear (isocal, logcal, betacal, isobins+CP and RCIR+CP). Our methods run in O(mn), where n is the sequence length and m the subsequence length. In practice, calculating the map with our methods on 10000 instances takes approximately 1-3 seconds, which is a small overhead compared to < 1 second runtime for the linear/loglinear methods. SVA has quadratic complexity and is clearly slower compared to other methods. While running HTLB+MAXCP is fast, it requires precalculation of the lower bound map, which is computationally expensive. Complexity of this precalculation is $O(N_p \\cdot N_{seq} \\cdot m_2k)$ where $N_p$ is the number of different probabilities we want to have the precalculated map for, $N_{seq}$ is the number of sequences we want to sample for our empirical distribution, $m_2$ is the maximum subsequence length and k is the statistic function calculation complexity. From our experiments we observed that taking at least a few hundred or a few thousand for Np and 10K-100K for Nseq gave stable results when the data size was 10000 and $m2$ was 2000. This can of course vary depending on the data and task. Even though it is computationally expensive, precalculation needed for HTLB+MAXCP method has to be only calculated once and can be parallelized easily, making it realistic to use in practice. The code for all mentioned calibration and cautious calibration methods can be found in Supplementary E.\nPost-processing of learned maps. There are two ways we post-process the learned maps for a more thorough comparison. Since our method has a theoretical maximum limit for the highest lower bound it can predict (a sequence with 2000 1s will produce a lower bound ~ 0.9978 with both of our methods), we will have a cut version of all maps, where the maximum value has been clipped to equal our method's maximum value. This will make the comparison of the methods less dependent on the chosen window size. Another modification applied to all cautious calibration methods is forcing them to be monotonic (mono). Due to how cautious calibration methods work, the monotonicity might not be preserved. We apply a simple and conservative algorithm that makes the maps monotonic by going through the map from right to left (from k = n to k = 1), and once a value $\u0109_k$ has been observed, all previous values that exceed it $k'> \\hat{c}_k$ where k' 0.99 in our experiments, so in order for the guarantees to hold, the percentage of lower bound violations has to be less than 1%. Table 2 shows that guarantees hold for our methods, even when they don't have conservative post-processing applied to them. Isobins+CP is also performing well after post-processing, but still, it has a disadvantage due to the aforementioned problems with subsequence choosing. All other methods, especially classical calibration methods, are very frequently non-cautious.\nMeasuring cautiousness without guarantees. Looking at independent estimates provides guarantees, but practically, we are more interested in the violation percentages within one learned map. In that case, the independence assumption doesn't hold, and no guarantees can be given. The intuitive reason is that we cannot guarantee or protect against getting a very unfortunate calibration set (e.g. worst"}, {"title": "5.3 Experiment setup and evaluation", "content": "Evaluation approach. We have two central concepts we want to evaluate. First, we measure if the cautious estimates $\u0109$ are truly cautious w.r.t. c. We calculate the violation percentage that shows how often the lower bounds are incorrect, i.e. how often $c_k > \\hat{c}_k$. Secondly, we measure how useful cautious calibration is in our example scenario. We evaluate how risk levels chosen with the imperfect es"}, {"title": "7 Discussion and Conclusion", "content": "This work introduces a novel concept of cautious calibration, which is useful when the aim is to avoid extremely negative outcomes for all score groups consistently. We have proposed our own theoretically justified approach for cautious calibration and compared it against other calibration methods, as well as existing methods, that are combined or repurposed for cautious calibration. We demonstrate that our methods are the most cautious ones in terms of correct lower bounds and are useful in avoiding extremely low expected costs in the example scenario. There is a potential for further improvements in the future, by moving from user-provided subsequence sizes to adaptable sizes, and finding more choices for the statistic function that would lead to analytic distributions, supporting fast computations like in the HTLB+CP approach. In summary, our research emphasizes the importance of looking further from aggregated performance measures towards models that are more trustworthy and, thus, better usable. Cautious calibration is one contribution towards this goal. Moving forward, we aim to identify scenarios where cautiousness is beneficial and develop methods most suitable for these contexts."}, {"title": "A ALGORITHM", "content": "The algorithm for lower bound map calculation with the HTLB method.\nAlgorithm 1 Calculating HTLB lower bound map.\nRequire: window length m\nRequire: statistic function $f_m$\nRequire: confidence level q\nRequire: calibration data labels y {sorted by scores z}\n1: Initialize lower\\_bounds {empty list of length n}\n2: for k = m to n do\n3:  $y_{k-m+1,...,k} = (y_{k-m+1},y_{k-m+2},..., y_{k\u22121}, y_k)$ {select m true labels up to k}\n4:  $t_k = f_m(y_{k-m+1,...,k})$ {statistic value for the selected region}\n5:  $\u0109_k = LB_{f_m,q}(t_k)$ {calculate the lower bound}\n6:  lower\\_bounds[k] = $\u0109_k$\n7: end for\n8: return lower\\_bounds"}, {"title": "B THEOREMS & PROOFS", "content": "Optimal Risk Level Selection. Let us have a probability prediction function $\u0109 : \\mathcal{X} \\rightarrow [0, 1]$ that, based on the input, estimates the calibrated probability. We have an outcome function $o : (\\mathcal{Y}, [0,\\infty)) \\rightarrow \\mathbb{R}$, that calculates the outcome based on a class label y and a risk level $\u03be \u2208 [0,\u221e)$.\nWe have selected the following outcome function:\n$o(y, \u03be) = \\begin{cases} \u03be & \\text{if } y = 1,\\\\  - \u03be^l & \\text{if } y = 0, \\end{cases}$\nwhere $l \u2208 [1,\u221e)$ is a fixed constant describing the imbalance of costs.\nWe will derive how to choose a risk level $\u0121$ based on calibrated probability $c \u2208 [0,1)$ (we omit the trivial case of c = 1). We do it by choosing the risk level that gives the highest expected outcome, assuming the data points are drawn from $Y \u223c Bernoulli(c)$. This can be thought of as optimizing the outcome for one score group. We can write it down as:\n$ropt(c) = arg max_\u03be E_{Y\u223cBern(c)}[o(Y, \u03be)]$\nLet's calculate it:\n$E_{Y\u223cBern(c)}[o(Y, \u03be)] =$\n$= E[o(Y, \u03be)|Y = 1] \u00b7 Pr(Y = 1) + E[o(Y, \u03be)|Y = 0] \u00b7 Pr(Y = 0) =$\n$= \u03be \u00b7 c + (\u2212\u03be^l) \u00b7 (1 \u2013 c)$\nNow, let's take the derivative and equalize with zero to find such maximizing \u03be:\n$(\u03be \u00b7 c + (\u2212\u03be^l) \u00b7 (1 \u2013 c))' = 0$\n$c + (\u22121\u03be^{l\u22121}) \u00b7 (1 \u2013 c) = 0$\n$\u03be^{\u22121} = c/(l(1\u2212c))$\n$\\xi= \\left( \\frac{c}{l(1 - c)}\\right)^{1/l}$\nInverted hypothesis testing approach for lower bound estimation. Here we have the formal notation for our cautious calibration approach with more details and proofs. For a better following of the proofs, we will also include the definitions here in the supplementary.\nWe will first define the heterogeneous Bernoulli vector and then accompany it with a remark."}]}