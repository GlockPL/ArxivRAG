{"title": "Cautious Calibration in Binary Classification", "authors": ["Mari-Liis Allikivi", "Joonas J\u00e4rve", "Meelis Kull"], "abstract": "Being cautious is crucial for enhancing the trustworthiness of machine learning systems integrated into decision-making pipelines. Although calibrated probabilities help in optimal decision-making, perfect calibration remains unattainable, leading to estimates that fluctuate between under- and overconfidence. This becomes a critical issue in high-risk scenarios, where even occasional overestimation can lead to extreme expected costs. In these scenarios, it is important for each predicted probability to lean towards underconfidence, rather than just achieving an average balance. In this study, we introduce the novel concept of cautious calibration in binary classification. This approach aims to produce probability estimates that are intentionally underconfident for each predicted probability. We highlight the importance of this approach in a high-risk scenario and propose a theoretically grounded method for learning cautious calibration maps. Through experiments, we explore and compare our method to various approaches, including methods originally not devised for cautious calibration but applicable in this context. We show that our approach is the most consistent in providing cautious estimates. Our work establishes a strong baseline for further developments in this novel framework.", "sections": [{"title": "1 Introduction", "content": "Classifier calibration is used to improve probabilistic predictions of classification models. For example, in binary classification, if a set of instances were assigned a predicted positive class probability of 0.8, then 80% of these instances should actually be positive. This is an appealing property both intuitively for humans to interpret the model's predictions and for optimal decision making.\nA key application of calibrated probability estimates lies in their usefulness in mediating between the model training stage and the downstream application. Since exact knowledge of costs and other variables in downstream tasks is often unavailable or subject to change, calibrated models are invaluable in these practical scenarios by being adaptable to these fluctuations [12]. This makes the further development of different post hoc calibration methods like isotonic calibration [17], logistic calibration [14], beta calibration [9] and other approaches for training calibrated models like temperature scaling, matrix and vector scaling [8] or Dirichlet calibration [10] an active area of research.\nHowever, a universal challenge with all calibration methods is their inability to reach perfect calibration. These methods aim to learn the true calibrated values, which reflect the actual unknown conditional probabilities \\(Pr(Y = 1|z_i)\\) of observing class 1 for a predicted score zi, i.e. the confidence predicted by a model. This ideal is not achievable due to having finite amounts of data, but also since methods have limitations due to their parametric nature (e.g., logistic, beta calibration) or biases like a tendency towards overconfidence [1] or other reasons that cause under- and overconfidence [2]. Even the most effective methods still estimate the true probability slightly inaccurately, with the direction of this deviation remaining unknown.\nIn situations where costs of different errors vary significantly, overconfidence can lead to excessively high costs while underconfidence results in slightly less optimal costs, as will be demonstrated in Section 2. As decision-making in case of imbalanced costs happens in the high-probability region, we focus only on probabilities 0.9 and higher, as we need high certainty to make risky decisions. From Figure 1A we can see that while calibration methods can be precise on average, one single learned map (red) can be overconfident for some scores and underconfident for others. As overconfidence leads to worse outcomes in our problem setting, we want to avoid it for every individual map at every score. Behind every score there is a group of instances in our distribution that will be mapped to that prediction, from now on referred to as a score group. In order to avoid bad outcomes for all score groups, we should expect every one of them to perform well, instead of only the model's average outcome being good.\nOur research introduces a novel concept called cautious calibration that, by design, addresses this problem. The aim is to learn calibration maps that always stay on the underconfident side, providing lower bounds for the true calibrated values. This is illustrated in Figure 1B. The paper starts with an example scenario of optimal risk level selection where cautiousness is needed to ensure that there are no overly negative outcomes for any score group. This is followed by an overview of existing methods that could be used for learning cautious calibration maps. Next, we propose our own, more reliable and theoretically principled method for cautious calibration. Finally, we show that our work provides a strong baseline for cautious calibration and demonstrate its critical relevance in our example scenario."}, {"title": "2 Example Scenario: Optimal Risk Level Selection", "content": "The usefulness of cautious calibration can be demonstrated in a setting where each prediction leads to a decision with potentially different costs. Before defining this setting formally, let's build intuition by following a simplified scenario in the self-driving domain. Imagine a machine learning model that is trained to identify images of clear roads (class 1) and roads with obstacles (class 0). Based on the predicted probability (probability for the road to be clear), one has to select the speed of the car, or in other terms, select the risk level. The more certain we are, the higher speed we are willing to choose. However, the outcomes are dependant on the chosen speed. If we choose a high speed, then we will gain in faster arrival times when observing class 1, but lose greatly in case observing class 0 and causing an accident. This means that we should only select high risks in case of very high predicted probabilities, meaning that the chances of mistaking are extremely small. In case of lower speed, it will take longer to reach the destination, but accidents are less costly.\nWe call this setting optimal risk level selection and formalize it in the following way. We have a probability prediction function \\(\\hat{c} : X\\rightarrow [0, 1]\\), that, based on the input, predicts the estimates of the true calibrated probability, similarly to our model that predicted the probability for the road to be clear. We have an outcome function \\(o : (\\mathcal{Y}, [0, \\infty)) \\rightarrow \\mathbb{R}\\), that calculates the outcome based on a class label y and a risk level \\(\\xi\\in [0,\\infty)\\). This would equate to calculating the gain or cost of selecting a speed and then observing either clear road or road with an obstacle, which in this toy example is obviously difficult to define. We have selected the following outcome function to illustrate our ideas:\n\\(o(y, \\xi) = \\begin{cases} \\xi & \\text{if } y = 1, \\\\ -\\xi^l & \\text{if } y = 0, \\end{cases}\\)\nwhere \\(l\\in [1,\\infty)\\) is a fixed constant describing the imbalance of costs. This function is chosen for its simplicity and for how it captures the idea of small positive gains for seeing a positive class versus large negative costs for seeing a negative class, worsening greatly with the increase of the risk level.\nThe final missing part is to decide how to choose a risk level \\(\\xi\\) based on a true calibrated probability \\(c\\in [0,1)\\) (we omit the trivial case of c = 1). We do it by choosing the risk level that gives the highest expected outcome, assuming the data points are drawn from \\(Y\\sim Bernoulli(c)\\). This can be thought of as optimizing the outcome for one score group. We can define it formally as:\n\\(\\xi_{opt}(c) = \\arg \\max_{\\xi} E_{Y \\sim Bernoulli(c)} [o(Y,\\xi)] = \\Big( \\frac{c}{l(1 - c)}\\Big)^{\\frac{1}{l-1}}"}, {"title": "3 Related work", "content": "To our best knowledge, no works have presented the need to directly use cautiously calibrated probabilities for decision-making. Still, there are methods that try to estimate a confidence interval around the predicted probabilities. This is usually done to evaluate the trustworthiness of the prediction, but a lower value of the confidence interval can be used for building a cautious calibration map. We will discuss several methods and describe the theoretical problems that might occur when using these methods for cautious calibration estimation.\nSimplified Venn-Abers predictors [15]. Venn-Abers predictors output multiple probability predictions given one score, one of which is well-calibrated. Getting a Venn-Abers estimate for a test instance requires retraining and re-calibrating the model from the start twice, adding the test instance with both possible labels (1 and 0). This is computationally inefficient and infeasible in the case of LLMs. There is a more efficient (but still inefficient) way to skip the retraining step, but this will lose the validity guarantee about one of the predictions being well-calibrated. This is done with the simplified Venn-Abers (SVA) approach, where the two possible predicted probabilities are obtained by adding an instance with label 1, re-calibrating with isotonic calibration, and then doing the same with label 0. We use the label 0 result as the cautious calibration estimate. The main drawbacks of this method are that it is not conservative enough, as adding one element with label 0 will not change the calibration map sufficiently, and it does not guarantee validity. Still, the lower estimations can be used as potential cautious calibration estimates.\nReliably calibrated isotonic calibration (RCIR) [11]. This method calculates credible intervals for the bins found by isotonic calibration. In the article, it is done to create a more stable isotonic calibration by merging the bins where the size of the credible interval exceeds some threshold, most likely due to the bin containing too few elements. Still, the lower bounds of the calculated intervals, both for original or merged isotonic bins, can be used for cautious calibration estimates. The credible intervals used in this work are calculated using Bayesian statistics approach, where Beta distribution with a uniform prior is used to calculate the posterior distribution for the probability in one isotonic calibration bin. This approach has two problems. Firstly, the use of Bayesian credible intervals depends on the chosen prior. It is typical to use a uniform prior, but as the true prior is unknown, we are unable to provide probabilistic guarantees for these types of intervals. The second problem arises due to having the same lower bound for all elements in the bin, a problem which will be explained in Section 4.2.\nHistogram binning with Clopper-Pearson intervals [13]. This method is similar to the previous and is meant for finding confidence intervals around the predicted probabilities. The intervals are also calculated in bins, but this time using histogram binning [16]. The second change is the use of frequentist confidence intervals, more precisely, Clopper-Pearson intervals [6]. The histogram binning method shares the same flaws as the isotonic calibration binning approach, while also dependant on the binning parameters. Clopper-Pearson intervals, however, are superior to the Bayesian approach, since they are not dependant on any priors, are known to be conservative and come with certain probabilistic guarantees, which will be discussed shortly.\nWe also include some classical calibration methods like isotonic calibration [17], logistic calibration [14] and beta calibration [9] in our experiments. From the cautious approaches we include SVA and a combination of the last two approaches. We take the good parts of both: isotonic calibration (or RCIR) bins from the first and the Clopper-Pearson confidence intervals from the second, hoping that this gives the strongest method to compare against.\nNext, we will introduce our method for cautious calibration, which can be thought of as a generalization of Clopper-Pearson bounds and show how to approach binning to make sure that we are being truly cautious."}, {"title": "4 Cautious Calibration", "content": "Before explaining our solution and the motivation behind it, we introduce the setting, which follows the typical binary classification framework. We consider data points drawn i.i.d from \\((X, Y) \\sim \\mathcal{D}\\) from the distribution \\(\\mathcal{D}\\) over \\(\\mathcal{X} \\times \\mathcal{Y}\\), where \\(\\mathcal{X}\\) is the instance space and \\(\\mathcal{Y} = \\{0, 1\\}\\) the label space. We define a scoring model \\(s : \\mathcal{X} \\rightarrow \\mathbb{R}\\) where a larger score \\(s(X)\\) indicates larger confidence towards class 1. We look at the different partitions of the observed data: training data \\(\\mathcal{D}_{train} \\sim \\mathcal{D}\\) for training the scoring model, calibration data \\(\\mathcal{D}_{cal} \\sim \\mathcal{D}\\) for calibrating the model post-training (similarly to methods like isotonic and logistic calibration) and test data \\(\\mathcal{D}_{test} \\sim \\mathcal{D}\\) for evaluation. The calibration data \\(\\mathcal{D}_{cal} = \\{(x_i, Y_i)\\}_{i=1}^{n}\\) is ordered so that \\(z_1 \\leq z_2 \\leq \\dots \\leq z_n\\) where \\(z_i = s(x_i)\\) is the model output score for the instance \\(x_i\\).\nIn conclusion, the calibration data is given as follows:\n\\begin{itemize}\n    \\item Datapoint vector \\(x = (x_1, x_2,..., x_n)\\)\n    \\item True label vector \\(y = (y_1, y_2,..., y_n)\\)\n    \\item Model output vector \\(z = (z_1, z_2,..., z_n)\\)\n    \\item True unknown calibrated probability vector \\(c = (c_1, c_2,..., c_n)\\) where \\(c_i = Pr(Y = 1 | z_i)\\).\n\\end{itemize}\nSimilarly to classical post-hoc calibration methods, we assume monotonicity between the model output scores z and the true calibrated values, meaning that \\(c_1 < c_2 < \\dots < c_n\\) as well. This is to say that we trust the model's ordering of scores but not their values. The aim of both calibration and cautious calibration methods is to learn the estimates \\(\\hat{c} = (\\hat{c}_1, \\hat{c}_2,...,\\hat{c}_n)\\) for the true calibrated probabilities based on the true label vector y. However, while traditional methods aim for accurate average estimates, cautious calibration opts for consistently lower estimates to avoid overconfidence (while still avoiding the trivial, but useless case of \\(\\hat{c} = (0, . . ., 0)\\))."}, {"title": "4.2 Motivation", "content": "As mentioned in Section 3, Clopper-Pearson intervals are intuitively fitting for cautiousness since they provide conservative confidence intervals with probabilistic guarantees [4]. However, these intervals are traditionally applied to a binary vector generated from a Bernoulli process with a constant probability parameter, which in the remark of Definition 4 we term as a homogeneous Bernoulli vector. This differs from the calibration scenario, where each label in a calibration set is derived from a Bernoulli trial with a unique parameter \\(c_k\\), defining this as a heterogeneous Bernoulli vector in Definition 4. Calibration data labels y or a subsequence of them is a heterogeneous Bernoulli vector, where the underlying probabilities are monotonically increasing.\nDefinition 1. Let \\(p = (p_1, p_2, ..., p_m)\\) with each \\(p_i \\in [0, 1]\\) for \\(i = 1,..., m\\). If given a random vector \\(S^p = (S^p_1, S^p_2,..., S^p_m)\\), where \\(S^p_i \\sim Bernoulli(p_i)\\), then we call \\(S^p\\) a heterogeneous Bernoulli vector and denote \\(S^p \\sim HBernoulli(p)\\).\nRemark. For ease of reference, if \\(p_1 = p_2 = ... = p_m\\), we call the heterogeneous Bernoulli vector a homogeneous Bernoulli vector.\nA problem arises when treating a subsequence of calibration data labels \\(y_{k-m+1},...,y_k\\) as a homogeneous Bernoulli vector and applying the same lower bound to every element in that subsequence. This is illustrated in Figure 3. The highlighted region represents one isotonic calibration bin. We can see that the true calibrated values (black) are increasing inside the bin, but both isotonic calibration (grey) and the corresponding lower bound calculation (dashed grey) will assign equal values to all elements in the bin. This bound will hold for the rightmost element, as will be proved later, and very often also holds for other right side elements of the bin (green region), but will often not hold for the left side elements of the bin (red region), where the lower bound exceeds the true calibrated probability \\(\\hat{c}_k > c_k\\). This demonstrates the weakness of assigning the same lower bound to an entire bin and motivates our approach: a bin can be used only to calculate the lower bound of it's rightmost element.\nOur work advances cautious calibration in two ways. First, we show the correct way of selecting subsequences for lower bound estimation. This ensures that when using methods that rely on inverted hypothesis testing [3], like Clopper-Pearson, the results will be conservative and with certain probabilistic guarantees. As the second contribution, we prove that in addition to the Clopper-Pearson interval, there is a wider set of statistic functions to be used with the inverted hypothesis testing approach, which also produce lower bound estimates with similar properties. Clopper-Pearson intervals are the most simple and computationally efficient example of those."}, {"title": "4.3 Lower Bound Calculation", "content": "Selecting Label Subsequences for Cautious Calibration. As discussed in the previous section, assigning equal lower bound estimates to all elements in a bin can result in estimates that are overconfident. We prove that if we only use a subsequence of labels preceding an element k to calculate the lower bound \\(\\hat{c}_k\\), then we can probabilistically guarantee that the bound is correct. This will be shown in the end of this subsection in Theorem 2.\nFigure 4 demonstrates the process of calculating a lower bound \\(\\hat{c}_k\\) for a true probability \\(c_k\\). First we select a subsequence of labels \\(y_{k-m+1},...,y_k\\) of length m preceding k. We then calculate a statistic \\(t_k\\) with a function \\(f_m\\) that assigns a real value to a binary vector of length m (for the sake of clarity, we will refer to any function calculating a statistic as a statistic function throughout this article). In the case of Clopper-Pearson intervals, this function \\(f_m\\) is the sum of ones in the selected subsequence. The statistic is then mapped to the corresponding lower bound estimate \\(\\hat{c}_k\\), calculated with the lower bound function \\(LB_{f_m,q}\\), which will be defined later. As every k has its own preceding subsequence, lower bounds are calculated separately for each k from its respective subsequence. Meaning that \\(\\hat{c}\\) is a function of a subsequence of labels \\(Y_{k-m+1,...,k}\\), where \\(1 < k-m+1 <k \\leq n\\) and \\(k, m \\in \\mathbb{N}\\). The algorithm for calculating a lower map for one calibration set is described in Supplementary A.\nInverted Hypothesis Testing for Lower Bound Estimation. Before explaining which type of guarantees this approach holds, we need to understand the process of lower bound calculation. We will start by explaining the idea behind one-sided Clopper-Pearson intervals [5] as an example of inverted hypothesis testing for interval calculation. Then, we generalize this approach to work with a wider set of statistic functions.\nSuppose we have a homogeneous Bernoulli vector with parameter p. In the Clopper-Pearson case, we calculate the sum function statistic on this random vector, which leads to a binomial distribution. This means we can form and test a hypothesis about p using the binomial test. Unlike classical hypothesis testing, which fixes one certain value of p' for the hypothesis (e.g. \\(H_0: p < p'\\)) and tests it once, inverted testing evaluates every possible \\(p' \\in [0, 1]\\) the same way. All values of p' where the null hypothesis is rejected form an interval \\([0, p']\\) of values, for which the alternative hypothesis (\\(H_1: p \\geq p'\\)) is supported, meaning that the true parameter is likely to be larger than these values. The maximum of these values p' is used as a lower bound estimate.\nThe previous example used homogeneous Bernoulli vectors for calculations, but real-world calibration involves heterogeneous Bernoulli vectors. In theory, it should be possible to construct a new type of confidence interval on these vectors using an inverted hypothesis testing approach, but it would be impractical due to the excessive number of potential hypotheses. Nevertheless, as will be confirmed in Theorem 2, constructing our lower bound calculation method on homogeneous Bernoulli vectors and applying it correctly on heterogeneous Bernoulli vectors (taking only left subsequences) provides conservative lower bounds with probabilistic guarantees.\nStill, as the homogeneous Bernoulli vector intervals are not tailored for heterogeneous ones, using the standard Clopper-Pearson version might not be the only or the best way for lower bound estimation. We cannot change the homogeneous/heterogeneous approach, but we can exchange the sum statistic function for another type of statistic. For example, we could consider a new recursive statistic function that examines all smaller subsequences than a given size m, calculates their lower bounds, and selects the maximum value as the final statistic. This approach can avoid some of the problems caused by fixing just a single m, as that might not be the subsequence giving the best or largest lower bound.\nMotivated by this, we generalize the inverted hypothesis testing method to work with all statistic functions that are monotonic w.r.t 0\u21921 bit-flipping (definition in Supplementary B). These are a group of functions \\(f_m: \\{0,1\\}^m \\rightarrow \\mathbb{R}\\) that maintain or increase its value when any 0 in its input is flipped to a 1. An example would be the sum function, which returns 2 in the case of vector (0, 1, 1, 0) and 3 in the case of vector (0, 1, 1, 1), where the last 0 is flipped to 1. All such functions can be used instead of the sum function to get different cautious calibration estimations with the same probabilistic guarantees.\nLower Bound Calculation and Probabilistic Guarantees. The lower bound calculation with the inverted hypothesis testing approach is formalized in Definition 7. Intuitively, the set in the definition consists of the aforementioned set of probabilities p for which we reject the null hypothesis, and, as stated, we choose the maximum of those for our lower bound. The definition includes the CDF for the statistic function \\(f_m\\) of a heterogeneous Bernoulli vector \\(S^p\\), specified in Definition 6. This is essentially the way to describe our null distributions, as it represents the cumulative distribution function for our chosen statistic, given that the data is generated with the underlying probability p.\nDefinition 2. We define a CDF for the statistic function \\(f_m\\) of the heterogeneous Bernoulli vector \\(S^p \\sim HBernoulli(p)\\) as\n\\(F_{f_m,p}(t) := Pr(f_m(S^p) <t),\\)\nwhere \\(t \\in \\mathbb{R}\\) and \\(p\\in [0,1]^m\\).\nDefinition 3. We define a lower bound function for any \\(t \\in \\mathbb{R}\\), a fixed probability level q and statistic function \\(f_m\\) as\n\\(LB_{f_m,q}(t): = \\max\\{p|F_{f_m,p}(t) \\geq q, p = (p, ...,p), p\\in [0,1]\\}\\)\nThe bounds derived through inverted hypothesis testing are valuable for cautious calibration because they are conservative and provide probabilistic guarantees linked to hypothesis testing. These guarantees can be expressed as follows: Given a confidence level q, if the true calibrated value c were lower than our estimated lower bound \\(\\hat{c}\\), then the probability of observing a statistic as high or higher from our data distribution D would be at most 1 - q. This guarantee applies to any statistic function that is monotonic with respect to 0\u21921 bit-flipping. This concept is rigorously outlined in Theorem 2, with a detailed proof available in Supplementary B.\nTheorem 1. Let \\(c = (c_1, ..., c_m)\\) be a monotonic probability vector and let \\(\\mathcal{D}^c \\sim HBernoulli(c)\\) be a heterogeneous Bernoulli vector of length m. If statistic function \\(f_m\\) is monotonic w.r.t. 0\u21921 bit-flipping then for any fixed probability level \\(q\\in [0,1]\\) and any statistic value \\(t \\in \\mathbb{R}\\) it holds that:\n\\(Pr [f_m(\\mathcal{D}^c) \\geq t|c_m < LB_{f_m,q}(t)] \\leq 1 - q.\\)\nTo illustrate with a specific example, let's consider using the sum function \\(f_m\\) with a window length \\(m = 1000\\) and a confidence level of \\(q = 0.99\\). Suppose in our data we observe a sequence consisting of 999 ones and 1 zero, yielding a statistic \\(t = 999\\). We can then estimate a lower bound \\(\\hat{c} \\approx 0.993\\). With this, we would assert that if the actual calibrated probability c would be below 0.993, then the probability of seeing a statistic as high or higher than 999 would be less than 0.01. Therefore, it is highly probable that the true value of c is at least \\(\\hat{c}\\), confirming that we have a reliable lower bound estimation for this scenario."}, {"title": "4.4 Practical Calculation of Lower Bounds", "content": "We will be using two different statistic functions for lower bound calculation. Given an arbitrary binary vector \\(y \\in \\{0,1\\}^m\\) and \\(m\\in \\mathbb{N}\\), the first is the sum function \\(f_{sum} (y) := \\sum_i y_i\\), making the applied method equivalent to the Clopper-Pearson method. The other is called max-cp, namely\n\\(f_{max-cp}(y) = \\max\\{LB_{f_{sum},q}(f_{sum}(y_{m2-j+1,...,m2})), m1 \\leq j \\leq m2\\}\\)\nwhere the max-cp statistic function looks at all subsequences from size \\(m_1\\) to \\(m_2\\) (\\(m_1 \\leq m_2\\)) ending at the last element, calculates Clopper-Pearson lower bounds for all and takes the maximum of those as the estimate. As proven in Supplementary B, both statistic functions are monotonic w.r.t. 0\u21921 bit-flipping, making them suitable for estimating lower bounds with probabilistic guarantees.\nWe will describe the practical calculation of both of these bounds. These approaches differ in many aspects, since for the easier sum statistic functions, the used CDFs can be described analytically, making the computations exact and fast. For the max-cp statistic we need to calculate approximate CDFs with simulations, as the distribution is much more complex.\nFor both methods, we employ the left subsequence approach with inverted hypothesis testing for lower bound calculation. For a clearer understanding of the experiment results, we introduce the following acronyms: HTLB (Hypothesis Testing Lower Bounds) denotes the general method using one-sided inverted hypothesis testing confidence intervals on left subsequences, HTLB+CP refers to the Clopper-Pearson a.k.a. sum statistic function, and HTLB+MAXCP applies to the max-cp statistic.\nPractical Calculation of HTLB+CP Lower Bounds. Calculating the lower bound function for the sum statistic is straightforward due to the direct relationship between the Beta and Binomial distributions in case of Clopper-Pearson intervals [4]. The lower bound function is defined as:\n\\(LB_{f_{sum}, q} = q'th \\ quantile \\ of \\ Beta(\\alpha = t, \\beta = m - t + 1),\\)\nwhere t is our sum statistic, i.e., the number of 1s in our subsequence, m is the subsequence length, making m-t equal to the nr of Os in the sequence. This quantile can be easily obtained with the percent-point function of the same Beta distribution.\nPractical Calculation of HTLB+MAXCP Lower Bounds. The max-cp statistic function, being more complex, lacks an analytical solution for direct calculation. This means that we must create our own empirical distributions for the inverted hypothesis testing for as many p values as we can and precalculate a mapping between statistic values and corresponding lower bounds. The precalculation has to be done for each \\(m_1\\) and \\(m_2\\) pair separately, which is computationally intensive. However, once the precalculation exists, the using of the lower bounds only requires using the existing mapping. The main steps for calculating a lower bound mapping for \\(m_1, m_2, q\\) and \\(f_{max-cp}\\) are as follows:\n\\begin{enumerate}\n    \\item We select as many values of p as we are able to use for calculations.\n    \\item For each p, we sample homogeneous Bernoulli vectors, calculate statistics with \\(f_{max-cp}\\) on them and end up with an empirical distribution over the statistic values for each p.\n    \\item For each p, we find the statistic value that is the q'th quantile of its empirical distribution.\n    \\item We reverse the mapping so that each statistic value is mapped with the minimum p for which it is the q'th quantile.\n\\end{enumerate}"}, {"title": "5 Experiment setup", "content": "Evaluation approach. We have two central concepts we want to evaluate. First, we measure if the cautious estimates \\(\\hat{c}\\) are truly cautious w.r.t. c. We calculate the violation percentage that shows how often the lower bounds are incorrect, i.e. how often \\(\\hat{c}_k > c_k\\). Secondly, we measure how useful cautious calibration is in our example scenario. We evaluate how risk levels chosen with the imperfect estimations \\(\\hat{c}\\) influence the expected outcome, knowing that the true calibrated probability was \\(c_k\\).\nExperiment setup. We use 100 different true calibration maps in our experiments and generate 500 calibration sets for each true map, resulting in 50K generated calibration sets. We use 22 variations of 8 conceptually different methods (see Table 1) to learn calibration maps or cautious calibration maps on each of these 50K cases. This sums up to 1.1 million learned maps. Results with detailed descriptions will be introduced in the next section of this paper."}, {"title": "6 Experiment results", "content": "Measuring cautiousness with guarantees. Our first measurements will validate if the probabilistic guarantees hold for our methods. For the guarantees to hold, the measurements have to be made on independent estimations, which is why we cannot provide guarantees for lower bounds calculated on overlapping subsequences. Thus, we select a random k'th position for each of the 50K maps and, for each method, check if the lower bound estimation is wrong. We used q = 0.99 in our experiments, so in order for the guarantees to hold, the percentage of lower bound violations has to be less than 1%. Table 2 shows that guarantees hold for our methods, even when they don't have conservative post-processing applied to them. Isobins+CP is also performing well after post-processing, but still, it has a disadvantage due to the aforementioned problems with subsequence choosing. All other methods, especially classical calibration methods, are very frequently non-cautious.\nMeasuring cautiousness without guarantees. Looking at independent estimates provides guarantees, but practically, we are more interested in the violation percentages within one learned map. In that case, the independence assumption doesn't hold, and no guarantees can be given. The intuitive reason is that we cannot guarantee or protect against getting a very unfortunate calibration set (e.g. worst case, all 1s), so we cannot guarantee a violation percentage < 1% for all learned maps. Even so, we can still measure the violation percentage within all 50K maps and see if our more cautious approach is empirically giving better results. Our methods are clearly the most cautious, with more than 99% of learned maps having 0 violations, followed by isocal+CP method. There were some outliers, even with our methods, where up to 17% of violations occurred within a single map. But compared to other methods, this is still considerably more cautious.\nMeasuring expected outcome. The reason for having cautious estimates is to avoid big negative outcomes for all score groups in high-risk decision-making tasks. We measure the effect of cautious calibration on optimal risk level selection scenario, where the risk level is selected based on our estimates \\(\\hat{c}\\) and the expected outcome is calculated, knowing that the true calibrated probability is actually \\(c_k\\). As our goal is to have good expected outcomes for all score groups, then, in principle, we want to find the worst expected outcome in every map and make sure that it's not an extremely negative value. Since the worst value is very dependent on how much data we have used in our experiments, we opt for the 1st percentile worst outcome in our comparisons. Then we can make claims such as 1% of score groups will have worse results than our reported 1st percentile expected outcome.  Our methods only have non-negative results on this plot, meaning that even if there are some negative outcomes, they appear rarely. MAXCP approach is outperforming the CP approach, hinting that while still cautious, MAXCP might give estimates closer to the true calibration line. An interesting observation can be made about logcal, betacal, SVA and isobins+CP. While the latter was outperforming logcal, betacal and SVA in cautiousness, it seems like it doesn't only matter how often you make mistakes but also where the mistakes are. It might just be that the problematic subsequence selection in isobins+CP leads to just a few incorrect lower bounds, but once they happen, the amount of overestimation is very large and can cause large negative expected costs. Other methods benefit largely from the cut post-processing, as without cutting, they make many overconfident mistakes in the high-probability region. In Supplementary C, similar results for the mean outcome are provided, showing that, as expected, cautiousness reduces the mean outcome. This is the trade-off for providing acceptable results for each score group vs having the best outcome on average."}, {"title": "7 Discussion and Conclusion", "content": "This work introduces a novel concept of cautious calibration, which is useful when the aim is to avoid extremely negative outcomes for all score groups consistently. We have proposed our own theoretically justified approach for cautious calibration and compared it against other calibration methods, as well as existing methods, that are combined or repurposed for cautious calibration. We demonstrate that our methods are the most cautious ones in terms of correct lower bounds and are useful in avoiding extremely low expected costs in the example scenario. There is a potential for further improvements in the future, by moving from user-provided subsequence sizes to adaptable sizes, and finding more choices for the statistic function that would lead to analytic distributions, supporting fast computations like in the HTLB+CP approach. In summary, our research emphasizes the importance of looking further from aggregated performance measures towards models that are more trustworthy and, thus, better usable. Cautious calibration is one contribution towards this goal. Moving forward, we aim to identify scenarios where cautiousness is beneficial and develop methods most suitable for these contexts."}]}