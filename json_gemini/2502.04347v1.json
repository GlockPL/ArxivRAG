{"title": "SCALM: Detecting Bad Practices in Smart Contracts Through LLMS", "authors": ["Zongwei Li", "Xiaoqi Li", "Wenkai Li", "Xin Wang"], "abstract": "As the Ethereum platform continues to mature and gain widespread usage, it is crucial to maintain high standards of smart contract writing practices. While bad practices in smart contracts may not directly lead to security issues, they do elevate the risk of encountering problems. Therefore, to understand and avoid these bad practices, this paper introduces the first systematic study of bad practices in smart contracts, delving into over 35 specific issues. Specifically, we propose a large language models (LLMs)-based framework, SCALM. It combines Step-Back Prompting and Retrieval-Augmented Generation (RAG) to identify and address various bad practices effectively. Our extensive experiments using multiple LLMs and datasets have shown that SCALM outperforms existing tools in detecting bad practices in smart contracts.", "sections": [{"title": "1 Introduction", "content": "With the widespread use of blockchain technology, smart contracts have become an important part of the blockchain ecosystem (Sharma, Jindal, and Borah 2023). Smart contracts are computer programs that automatically execute contract terms, controlling assets and operations on the chain. However, due to their public and immutable code, smart contracts have become a significant target for attackers (Li et al. 2023). A total of 464 security incidents occurred in 2023, resulting in losses of up to $2.486 billion (Zone 2024).\nBad practices refer to poor coding habits or design decisions in the development of smart contracts. Although bad practices may not result in immediate security threats, they could potentially lead to future issues such as performance problems, increased security vulnerabilities, and unpredictable code behavior (Kong, Li, and Li 2024). Additionally, bad practices have hidden economic dangers due to the disruption of regular smart contract activities (Niu et al. 2024).\nCurrently, the security audit of smart contracts mainly relies on manual code review and automated tools. However, these methods have their limitations (Chaliasos et al. 2024). Manual code review is inefficient and prone to overlook subtle security vulnerabilities. Existing automated tools primarily rely on pattern matching, which cannot accurately detect complex security issues (Li et al. 2024a, 2020). Moreover, the types of vulnerabilities that these tools can detect are usually relatively limited. To achieve a comprehensive audit, multiple tools may be required, each covering different aspects of security.\nTo solve these problems, we introduce SCALM (Smart Contract Audit Language Model), a new security auditing framework for smart contracts. It mainly consists of two parts: first, it performs static analysis on large datasets to identify and extract code blocks containing potential bad practices. These are then vectorized and stored in a vector database as a searchable knowledge base. Second, SCALM utilizes RAG and Step-Back Prompting to abstract high-level concepts and principles from the code, enabling the detection of bad practices. The framework ultimately generates detailed audit reports that highlight security issues, assess associated risks, and provide actionable remediation recommendations.\nOur contributions are as follows:\n\u2022 To the best of our knowledge, we provide the first systematic study of bad practices in smart contracts and conduct an in-depth discussion and analysis on 35 different types of SWC covered.\n\u2022 We introduce SCALM, a novel framework based on LLMs for smart contract security audits. It mainly consists of two parts: firstly, static code analysis is used to extract code blocks containing bad practices, which are transformed into vectors and stored in a vector database as a knowledge base; secondly, using RAG and Step-Back Prompting to extract high-level concepts and principles from the code to detect bad practices, and ultimately generate detailed audit reports.\n\u2022 We conduct an experimental evaluation on SCALM, and the results show that the framework performs well and outperforms existing tools in detecting bad practices in smart contracts. At the same time, ablation experiments reveal that the RAG component significantly improves SCALM performance.\n\u2022 We open source SCALM's codes and experimental data at https://figshare.com/s/5cc3639706e4ecd16724."}, {"title": "2 Background", "content": "LLMs are trained using deep learning techniques to understand and generate human language. They are typically based on the Transformer architecture, such as ChatGPT (Kasneci et al. 2023), BERT (Devlin et al. 2019), GLM (Du et al. 2022), etc. The training process of these models usually involves learning language patterns and structures from large-scale text corpora. These corpora can include a variety of texts such as news articles, books, web pages, and other forms of human linguistic expression. The model can generate coherent and meaningful text by learning from these corpora. In addition, they can handle various natural language processing tasks, including text generation, text classification, sentiment analysis, question-answering systems, etc (Mao et al. 2024).\nOne of the key features of LLMs is their powerful generative ability. These models can generate new, coherent text similar in grammar and semantics to the training data. This makes LLMs useful for various applications, including machine translation, text summarization, sentiment analysis, dialogue systems, and other natural language processing tasks.\nAnother important feature of LLMs is their \"zero-shot\" capability, which allows them to perform various tasks without any task-specific training. For example, the model can choose the most appropriate answer given a question and some answer options. This ability makes LLMs very useful in many practical applications.\nHowever, LLMs also have some challenges and limitations. For instance, they may generate inaccurate or misleading information and reflect biases in the training data. Moreover, due to their need for substantial computational resources for training and operation, they might face certain challenges in practical application."}, {"title": "2.2 Smart Contract Weakness Classification", "content": "Smart contracts are self-executing protocols that run on the blockchain and allow trusted transactions without third-party intervention. However, since their code is publicly available and cannot be changed once deployed, the security of smart contracts has become an important issue. To address this issue, EIP-1470 (Wagner 2018) proposes the Smart Contract Weakness Classification (SWC), a tool designed to help developers identify and prevent smart contract weaknesses.\nSWC concerns weaknesses that can be identified within a smart contract's Solidity code. It is designed to reference the structure and terminology of the Common Weakness Enumeration (CWE) but adds several weakness classifications specific to smart contracts. These classifications include but are not limited to, reentry attacks, arithmetic overflow, Assert Violation, etc.\nAll work on SWC has been incorporated into the EEA EthTrust Security Level Specification, a specification proposed by the Enterprise Ethereum Alliance (EEA) to provide a reliable methodology for assessing the security of smart contracts. This specification defines a series of security levels to measure the security and trustworthiness of smart contracts."}, {"title": "3 Method", "content": "The overview of SCALM's architecture is shown in Fig. 1, which mainly consists of two modules: (1) Extracts defective code blocks via static analysis, converts them into vectors, and stores them in a vector database to create a queryable library of code information. (2) Adopts the RAG methodology, which utilizes Step-Back Prompting to abstract high-level concepts and principles from the code and generates a detailed audit report.\nThe Algorithm 1 outlines the step-by-step procedure for generating an audit report for smart contract code. The algorithm is divided into several stages: initialization, data collection and processing, detection strategy, and report generation."}, {"title": "3.1 Data Collection and Processing", "content": "Our data collection comes from the DAppSCAN database (Zheng et al. 2024b), which includes 39,904 smart contracts with 1,618 SWC weaknesses. First, we extract code snippets with bad practices from these contracts. This extraction process involves static code analysis, using tools to identify and extract code snippets with potential security risks. Then, each code block is converted into a vector through our embedding model and stored in the vector database for subsequent fast matching and retrieval operations."}, {"title": "3.2 Vector Database", "content": "A vector database is a particular database that can store and query large amounts of vector data (Hambardzumyan et al. 2022). In the vector database, data is stored as vectors, each typically represented by a set of floating-point numbers. These vectors can represent various data types, such as images, audio, text, etc. In bad practice detection tasks, we use an embedding model (i.e., text-embedding-ada-002) to convert code into vectors and then store these vectors in the vector database (Li et al. 2024d,c). This process can be expressed with the following eq. (1):\n$v = fEmbedding(Text)$\nWhere $fEmbedding$ is our embedding model, Text is the input text, and v is the outputted vector. This vectorized data storage method significantly improves efficiency in handling it. Firstly, storing data as vectors makes it more compact, thus reducing storage space requirements. Secondly, vectorized data facilitates parallel computing, which is crucial when dealing with large-scale datasets. A vital feature of a Vector Database lies in its ability to perform efficient similarity searches, which are notably advantageous when dealing with high-dimensional datasets. This similarity search can be achieved by calculating cosine similarities between two vectors with the eq. (2):\n$similarity(A, B) = \\frac{A \\cdot B}{||A||_2||B||_2}$\nwhere A and B are two vectors, A \u00b7 B is their dot product, and $||A||_2$ and $||B||_2$ are their second-paradigms."}, {"title": "3.3 Retrieval-Augmented Generation", "content": "LLMs have proven their powerful capabilities in handling complex language understanding and generation tasks, incredibly when fine-tuned for specific downstream tasks. However, these models still face challenges for tasks that require precise and specific knowledge, such as smart contract code auditing.\nSCALM adopts the RAG fine-tuning method to improve the quality and accuracy of smart contract code auditing. This method combines pre-trained parametric models with non-parametric memory to enhance the quality and accuracy of smart contract code audits (Gao et al. 2024). The RAG integrates the processes of retrieval and generation into one.\nAs Fig. 2 illustrates, during the operation of the model, it first retrieves relevant documents or entities from a large-scale knowledge base. Then, it inputs this retrieved information as additional context into the generation model, which generates corresponding outputs based on these inputs. This design allows RAG to utilize external knowledge bases effectively while demonstrating excellent performance when dealing with tasks requiring extensive background knowledge."}, {"title": "3.4 Detection Strategy and Report Generation", "content": "Our detection strategy mainly focuses on effectively identifying bad practices in smart contracts. Firstly, the smart contract code to be detected is cut into smaller fragments according to its structure and size, then queried through a vector database to find the most matching known bad practice code blocks.\nIn the task of detecting bad practices in smart contracts, we adopted a technique called Step-Back Prompting (Zheng et al. 2024a). This technique leverages the capabilities of LLMs to abstract high-level concepts and basic principles from specific code instances. In this way, not only can the model understand the literal meaning of the code, but it can also comprehend underlying logic and potential design patterns through abstract thinking.\nStep-Back Prompting consists of two main steps:\n\u2022 Abstraction: Instead of directly posing questions, we propose a general step-back question about higher-level concepts or principles and retrieve facts related to these high-level concepts or principles. In the task of detecting bad practices in smart contracts, we use abstract prompts that aim to guide LLMs to explore not just the literal meaning of code but deeper structures and intentions. These prompts may include questions like \"What are the potential risks with this implementation?\"\n\u2022 Model Reasoning: Based on facts about high-level concepts or principles, LLMs can reason about answers to the original question. We refer to this as abstraction-based reasoning. Reasoning with these abstraction hints attempts to analyze the code from a broader perspective. This includes comparing the strengths and weaknesses of different implementations and how they fit with known best practices or common bad practices.\nThe LLM evaluates these code snippets for bad practices through Step-Back Prompting and generates a detailed audit report based on the results. The report is output in JSON format. It contains the bad practice ID, title, type, specific bad code block along with its location, risk level, reason for the problem, and suggestions for improvement.\nThrough this method, we can effectively utilize the powerful capabilities of LLMs for deep security audits on smart contracts, thereby helping developers identify and fix potential security issues."}, {"title": "4 Experiments", "content": "All experiments are executed on a server equipped with NVIDIA GeForce GTX 4070Ti GPU, Intel(R) Core(TM) i9-13900KF CPU, and 128G RAM, operating on Ubuntu 22.04 LTS. The software environment includes Python 3.9 and PyTorch 2.0.1.\nDataset. In this paper, we use the DAppSCAN dataset (Zheng et al. 2024b) as a knowledge base for detecting bad practices in smart contracts. The dataset contains 39,904 Solidity files with 1,618 SWC weaknesses from 682 real projects. The Smartbugs dataset (Durieux et al. 2020) is also used in the experiment, and a total of 1,894 smart contracts with five types of SWC weaknesses are extracted for comparison experiments. These datasets form the basis of our experimental analysis.\nWe conduct initial detection experiments using official SWC samples covering 35 categories of bad practices. It is important to note that these 35 categories of SWC include most bad practices, but they do not represent all possible bad practices that may occur in smart contract development. In the follow-up experiments, we select five categories of SWC for in-depth study, using 94 samples for positive examples of SWC-104 and 200 samples for both positive and negative examples of the remaining SWC categories. In order to comprehensively evaluate the performance of the model on the test set, we select Accuracy (Acc), Recall, and F1 score as the evaluation metrics.\nModels. For the selection of LLMs, we chose six current state-of-the-art models for detection experiments. GPT-40, GPT-4-1106-preview, and GPT-4-0409 are the latest versions from OpenAI with powerful natural language processing capabilities; Claude-3.5-Sonnet is Anthropic's new-generation model focused on safety and interpretability; Gemini-1.5-Pro is Google's high-performance model optimized for multitasking; Llama-3.1-70b-Instruct is Meta's large-scale model specializing in instruction following and generating high-quality text.\nEvaluation Metrics. We carry out experiments to answer the following research questions: RQ1: How effective is SCALM in detecting bad practices in smart contracts? How do different LLMs affect SCALM? RQ2: Can SCALM find bad practices undetectable by other tools? How does it compare with existing tools? RQ3: Can SCALM achieve the same effect without including RAG?"}, {"title": "4.2 RQ1: Bad Practice Detection", "content": "To assess the effectiveness of SCALM in detecting bad practices within smart contracts, we conduct a comprehensive evaluation using a variety of LLMs. The primary objective is to determine how well SCALM identifies known bad practices, as classified by the Smart SWC registry. We also seek to understand the impact of different LLMs on the performance of SCALM in detecting these bad practices.\nWe use the DAppSCAN dataset containing 23637 smart contracts as the knowledge base to test the detection ability of SCALM. Specifically, we focus on contracts with known SWC vulnerabilities. Our experiments involve running SCALM with multiple LLM configurations, including GPT-40, GPT-4-1106-preview, GPT-4-0409, Claude-3.5-Sonnet, Gemini-1.5-Pro, and Llama-3.1-70b-Instruct.\nFor each model, we evaluate the framework's ability to identify instances of bad practices across 35 SWC categories correctly. Each detection instance is recorded as a success or failure based on whether the LLM accurately identifies the bad practice by its SWC-ID or keywords.\nThe variations in detection accuracy across different LLMs highlight the importance of model selection in the SCALM framework. The results suggest that while models like GPT-40 can handle a wide range of bad practices effectively, others may require further fine-tuning or additional contextual information to improve their performance.\nAdditionally, the findings emphasize the need for a robust and diverse LLM ensemble within SCALM to ensure comprehensive coverage of all potential bad practices in smart contracts. By leveraging the strengths of different models, SCALM can achieve a more reliable and thorough detection process, ultimately leading to higher-quality audit reports.\nAnswer to RQ1. SCALM is a powerful framework for detecting bad practices in smart contracts, especially when supported by advanced LLMs like GPT-40. However, the choice of LLM significantly impacts the system's overall effectiveness, indicating that ongoing model improvements and updates are essential for maintaining high detection accuracy."}, {"title": "4.3 RQ2: Comparison Experiments", "content": "We conduct a series of comparison experiments to address RQ2, which examines whether SCALM can identify bad practices that other tools cannot detect and how it compares with existing tools. These experiments focus on several critical SWC categories, specifically SWC-101 (Integer Overflow and Underflow), SWC-104 (Unchecked Call Return Value), SWC-107 (Reentrancy), SWC-112 (Delegate-call to Untrusted Callee), and SWC-116 (Block Values as a Proxy for Time). We select 1,894 smart contracts from the SmartBugs dataset containing these five categories of bad practices.\nAdditionally, we collect a set of smart contract defect detection tools from reputable journals and conferences in software and security (e.g., CCS and ASE) as well as Mythril (Mythril 2024), recommended by the official Ethereum community. For comparative analysis, we choose four benchmark smart contract detection tools, namely Mythril, Oyente (Luu et al. 2016), Confuzzius (Torres et al. 2021), and Conkas (Veloso 2024). Several factors are considered in the selection of the tools: (1) the accessibility of the tool's source code; (2) the tool's ability to detect the five categories of bad practices we select; (3) the tool's support for source code written in Solidity; (4) the tool's ability to report the exact location of potentially defective code for manual review.\nIn these experiments, SCALM is powered by GPT-40, one of the most advanced LLMs. For each SWC category, we evaluate the performance of SCALM and the other tools based on three key metrics: Accuracy (Acc), Recall, and F1 Score. These metrics provide a comprehensive view of each tool's detection capabilities:\n\u2022 Accuracy (Acc) measures the proportion of correctly identified instances (both true positives and true negatives) out of the total instances.\n\u2022 Recall measures the proportion of actual positive instances correctly identified by the tool.\n\u2022 F1 Score is the harmonic mean of Precision and Recall, providing a single metric that balances false positives and false negatives.\nThe comparison experiments demonstrate that SCALM effectively detects bad practices in smart contracts, outperforming existing tools across multiple SWC categories. This suggests that SCALM can provide a more comprehensive and accurate analysis of smart contract security.\nAnswer to RQ2. SCALM detects bad practices more accurately than existing tools and identifies bad practices that others often miss. Across the five SWC categories tested, SCALM consistently achieves higher Accuracy, Recall, and F1 scores."}, {"title": "4.4 RQ3: Ablation Experiments", "content": "We conduct ablation experiments to address RQ3, investigating whether SCALM can achieve the same performance without including the RAG component. In these experiments, SCALM is also powered by GPT-40. We compare SCALM with and without RAG across several SWC categories: SWC-101, SWC-104, SWC-107, SWC-112, and SWC-116. The results are summarized in Table 4.\nThe ablation experiments reveal that including RAG significantly enhances SCALM 's performance across all SWC categories. The significant drop in Accuracy, Recall, and F1 scores when RAG is excluded indicates that retrieving relevant knowledge and context provided by RAG substantially contributes to the model's effectiveness. Without RAG, SCALM struggles to achieve the same level of precision and Recall, underscoring the importance of this component in enhancing the model's overall capability to identify and report bad practices in smart contracts.\nAnswer to RQ3. The ablation experiments demonstrate that SCALM cannot achieve the same level of performance without the RAG component. Including RAG significantly improves F1 scores, Accuracy, and Recall across all tested SWC categories."}, {"title": "5 Discussion", "content": "The experimental results confirm SCALM's effectiveness in detecting bad practices in smart contracts, outperforming existing tools in accuracy, recall, and F1 scores.\nThe choice of LLM significantly impacts SCALM's performance, with different models yield varying results. This finding emphasizes the need for careful model selection and ongoing improvements to maintain high accuracy. The RAG component is also crucial in SCALM. Ablation experiments showed significant performance drops when RAG was excluded. This highlights its importance in providing contextual information that enhances detection accuracy.\nDespite its strengths, SCALM has limitations in understanding blockchain mechanisms and complex interactions. Specifically, SCALM might not fully grasp the state-dependent nature of certain vulnerabilities, which require a deep contextual understanding of how the contract evolves over time.\nFuture work will focus on improving SCALM by incorporating advanced LLMs and fine-tuning with domain-specific data. Additionally, we plan to improve the RAG generation strategy to provide more prosperous and accurate contextual information. These improvements will enhance the performance of SCALM for smart contract auditing."}, {"title": "6 Related Work", "content": "LLMs have been widely applied and validated for their ability to identify and fix vulnerabilities (Napoli and Gatteschi 2023). In the field of smart contract security, various methods based on LLMs have been proposed, and certain effects have been achieved. Firstly, Boi et al. (Boi, Esposito, and Lee 2024) proposed VulnHunt-GPT, a method that uses the GPT-3 to identify common vulnerabilities in OWASP standards smart contracts.s. Similarly, Xia et al. (Xia et al. 2024) introduced a tool called AuditGPT, which utilizes LLM to automatically and comprehensively verify the ERC rules of smart contracts. They break down large, complex audit processes into small, manageable tasks and design prompts for each ERC rule type to improve audit performance. In terms of fuzz testing, Shou et al. (Shou et al. 2024) presented LLM4FUZZ, which employs LLMs to effectively guide fuzz activities in smart contracts and determine their priorities. This approach can enhance test efficiency and coverage compared to traditional fuzz testing methods.\nHowever, the direct use of pre-trained LLMs is no longer sufficient on some occasions, so many studies have chosen to fine-tune LLMs to meet specific needs. For example, Liu et al. (Liu et al. 2024) proposed PropertyGPT, a system for formal verification of smart contracts by retrieving enhanced property generation. This system utilizes the capability of LLM to automatically generate comprehensive properties of smart contracts, including invariants, preconditions, and postconditions, thus improving the security of smart contracts. On the other hand, Storhaug et al. (Storhaug, Li, and Hu 2023) proposed a novel vulnerability-bound decoding method to reduce the amount of vulnerable code generated by the model. They fine-tuned the LLM to include vulnerability tags when generating code and then prohibited these tags during the decoding process to avoid producing vulnerable codes. Finally, Yang et al. (Yang, Man, and Yue 2024) collected a large number of tagged smart contract vulnerabilities and fine-tuned Llama-2-13B for the automatic detection of vulnerabilities in the decentralized finance (DeFi) domain's smart contracts (Li et al. 2024b).\nIn conclusion, while LLMs show great potential for improving smart contract security, the full realization of their potential requires continuous fine-tuning and adaptation of these models in specific contexts."}, {"title": "7 Conclusion", "content": "This paper presents the first systematic study of over 35 bad practices in smart contracts. Building on this extensive analysis, we introduced SCALM, a framework based on LLMS for detecting these bad practices in smart contracts. Leveraging the power of LLMs, along with RAG and Step-Back Prompting, SCALM provides a comprehensive and accurate audit of smart contracts. Experiments on datasets such as DAppSCAN and SmartBugs show that SCALM outperforms existing tools, with the RAG component playing a critical role in improving detection accuracy. These findings demonstrate SCALM's potential to enhance smart contract security, offering developers a robust framework to identify and address bad practices."}]}