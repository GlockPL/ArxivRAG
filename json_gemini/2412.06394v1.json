{"title": "GAMEARENA:\nEVALUATING LLM REASONING\nTHROUGH LIVE COMPUTER GAMES", "authors": ["Lanxiang Hu", "Qiyu Li", "Anze Xie", "Nan Jiang", "Haojian Jin", "Hao Zhang"], "abstract": "Evaluating the reasoning abilities of large language models (LLMs) is challeng-\ning. Existing benchmarks often depend on static datasets, which are vulnerable to\ndata contamination and may get saturated over time, or on binary live human feed-\nback that conflates reasoning with other abilities. As the most prominent dynamic\nbenchmark, Chatbot Arena evaluates open-ended questions in real-world settings,\nbut lacks the granularity in assessing specific reasoning capabilities. We introduce\nGameArena, a dynamic benchmark designed to evaluate LLM reasoning capa-\nbilities through interactive gameplay with humans. GameArena consists of three\ngames designed to test specific reasoning capabilities (e.g., deductive and induc-\ntive reasoning), while keeping participants entertained and engaged. We analyze\nthe gaming data retrospectively to uncover the underlying reasoning processes of\nLLMs and measure their fine-grained reasoning capabilities. We collect over 2000\ngame sessions and provide detailed assessments of various reasoning capabilities\nfor five state-of-the-art LLMs. Our user study with 100 participants suggests that\nGameArena improves user engagement compared to Chatbot Arena. For the first\ntime, GameArena enables the collection of step-by-step LLM reasoning data in\nthe wild.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent large language models (LLMs) (e.g., OpenAI's o1 model) have made great strides in per-\nforming complex reasoning (OpenAI, 2024a; Luo et al., 2024; Ye et al., 2024a;b; Lightman et al.,\n2023), but evaluating LLM reasoning capabilities remains challenging. Current methods typically\nuse curated test suites of coding and math problems to assess different reasoning skills (Hao et al.,\n2024; bench authors, 2023; Cobbe et al., 2021). However, these static benchmarks are vulnera-\nble to data contamination issues (Sainz et al., 2023; Rajore et al., 2024) and can quickly become\nsaturated (Kiela et al., 2021; Perlitz et al., 2024), limiting their effectiveness.\nResearchers have been exploring alternative dynamic evaluation approaches. The most prominent\nattempt is Chatbot Arena (Chiang et al., 2024), a crowdsourced platform that asks users to input\narbitrary prompts and compare the responses generated by two different models, producing overall\nranking scores based on pairwise comparisons. Nevertheless, these ranking scores may not ac-\ncurately reflect the models' reasoning capabilities, as they depend on human preferences that can\nbe confounded by various factors. For example, Li et al. (2024) found that response style has a\nstrong effect on the leaderboard performance since humans usually favor detailed and \u201cbeautiful\"\nresponses, which do not relate to reasoning abilities. Additionally, Chatbot Arena could not as-\nsess specific reasoning capabilities such as inductive and deductive reasoning, due to its reliance on\nopen-ended user input and lack of control over the reasoning skills required for the prompts.\nWe introduce GameArena, a novel dynamic benchmark suite designed to evaluate LLM reasoning\nthrough three interactive games with humans. GameArena leverages live human-AI interactions to\nprevent benchmark saturation. In contrast to Chatbot Arena, GameArena confines the human-AI\ninteractions using designated gaming rules, allowing for better control of the interactions toward\ntesting LLM's fine-grained reasoning capabilities, including deductive, inductive, abductive, and\nmulti-hop reasoning (Huang & Chang, 2023; Seel, 2011). Additionally, embedding evaluation tasks\ninto games enables a more engaging and enjoyable experience for users, encouraging sustained\nparticipation in the evaluation process.\nDesigning effective games for evaluating LLM reasoning is challenging because they need to pro-\nvide high-quality reasoning data while keeping gameplay engaging for human players. Therefore,\ninstead of creating entirely new games, we integrate LLMs into existing games, i.e., Akinator, Taboo,\nand Bluffing, that are fun and require complex reasoning through multi-turn conversations. In these\ngames, the LLM attempts to deduce or extract specific information by synthesizing human responses\nover multiple rounds, which requires nuanced reasoning capabilities beyond simple natural language\nprocessing. For example, in the Akinator game, the LLM asks a series of binary questions to guess\nthe object the human player is thinking of. To win the game, the LLM must demonstrate strong\ndeductive reasoning by analyzing players' responses and narrowing down possibilities to derive the\ncorrect object. By analyzing the game outcomes (e.g., winning rates), we can compare the reasoning\ncapabilities of different models.\nIn addition to the game outcome, the interaction process also provides rich evaluative potential. Dur-\ning gameplay, the LLM develops a step-by-step reasoning trajectory intrinsically, with each round's\ndecision or action serving as a data point that can be analyzed to assess its reasoning capabilities.\nTo uncover the LLM's hidden reasoning process, we retrospectively analyze the gaming data by\nasking the LLM to reveal its intermediary thought process at each step. For example, in Akinator,\nwe prompt the LLM to provide a list of possible objects based on its predictions for each round. We\nthen develop quantitative metrics based on these outputs to measure its reasoning capabilities.\nWe evaluated five state-of-the-art LLMs with GameArena. To reduce potential prompt bias, we de-\nveloped five optimized system prompts using DSPy (Khattab et al., 2024) and randomly selected one\nfor each game session. We collected more than 2000 game sessions and analyzed the data to score\neach LLM based on capability-specific evaluation metrics. We found Claude 3.5 Sonnet outper-\nformed GPT-4o in most evaluation metrics for reasoning. Mistral-large fell short in most reasoning\ncapabilities and often failed to follow game rules and make predictions on game secrets in all three\ngames. To quantify rank correlations, we also compared our ranking with other baselines, including\nChatbot Arena. Additionally, controlled experiments showed our conclusions are consistent across\ntwo heterogeneous subsets of our gaming data.\nWe conducted comparative studies between GameArena and Chatbot Arena. GameArena was found\nto be more effective in data collection, with over 85% of game sessions from GameArena containing\nuseful data for evaluating LLM reasoning. In contrast, only less than 5% of conversation sessions\nfrom Chatbot Arena yielded meaningful votes for LLM evaluation. Our user study with 100 partic-\nipants suggested that users preferred evaluating LLMs through games, were more satisfied with the\nexperience, and showed greater interest in participation in GameArena compared to Chatbot Arena.\nIn this paper, we make the following contributions:\n\u2022 We build GameArena, the first dynamic benchmark for evaluating specific LLM reasoning ca-\npabilities using games that collect human-labeled step-by-step reasoning data.\n\u2022 We introduce a new set of data analysis techniques to evaluate LLM reasoning capabilities while\ncontrolling variables like system prompt and style variances.\n\u2022 We demonstrate the effectiveness of GameArena in comparison with existing baselines when\nevaluating different capabilities.\n\u2022 We will release our gaming data for future research."}, {"title": "2 GAMES IN GAMEARENA", "content": "GameArena aims to evaluate LLM reasoning capabilities through live games with humans while\nkeeping participants entertained and engaged. GameArena consists of three games, Akinator, Taboo,\nand Bluffing. For each game, we instruct the LLM to interact with humans according to the game\nrules. Since these games are designed to require strong reasoning skills, the game performance of\nLLMs reflects their reasoning capabilities. We first explain the games and how they are associated\nwith LLM reasoning capabilities, followed by a formal formulation of the game workflow."}, {"title": "2.1 GAME DESIGN", "content": "AI Akinator game (Fig. 1) follows the same rule of the online Akinator game (Akinator, 2024).\nAn LLM attempts to determine what object the player is thinking of by asking up to 20 yes or no\nquestions. The goal of the LLM is to guess the answer correctly with the fewest number of questions\npossible. We provide a detailed description of game rules in Appendix A.1.1.\nThis game presents a controlled setting with a limited number of guesses to evaluate the LLM's\ndeductive and multi-hop reasoning skills. Deductive reasoning (Creswell et al., 2022; Han et al.,\n2022; Saparov & He, 2023) involves drawing a specific conclusion from a chain of prior premises.\nIn this game, each factual question-answer pair, from general descriptions to specific details, serves\nas a premise to narrow down possible options. All the premises logically lead to the most probable\nsecret object the user has in mind. Multi-hop reasoning (Yang et al., 2018; Ho et al., 2020) entails\nconnecting multiple pieces of information or reasoning steps to solve a problem. In the Akinator\ngame, the LLMs must employ multi-hop reasoning to synthesize and connect information from\nmultiple rounds. A strong LLM uses multi-hop reasoning to ask insightful questions based on prior\nknowledge and employs deductive reasoning to quickly narrow down possibilities for each answer.\nAI Taboo game (Fig. 2). The goal of the human player is to prompt the LLM with questions that\nwill lead the model to say the target word, even though the LLM has been instructed to guess the\nword and avoid uttering it in its response. We describe the game rules in Appendix A.1.2.\nThis game evaluates LLM's abductive and multi-hop reasoning skills. Abductive reasoning (Jung\net al., 2022; Seel, 2011) involves generating a possible explanation from incomplete information.\nThroughout the game, the player discloses fragmented and ambiguous information related to the\nsecret word, and the LLM needs to reason the most possible target word based on the limited clues.\nIn rare cases, the LLM's response may directly include the target word, allowing it to guess correctly\nin the next round. However, most cases require complex reasoning. Strong LLMs use multi-hop\nreasoning to extract clues from previous prompts and apply abductive reasoning to infer the most\nlikely target word. A detailed example is provided in Appendix E.3.\nAI Bluffing game (Fig. 3). The human player aims to convince an LLM model that they are making\na true statement. The LLM can ask a player at most five questions to determine whether the player\nis lying. The rule of the Bluffing game is outlined in Appendix A.1.3.\nThis game challenges LLMs to detect deception by proposing strategic questions and identifying\nsubtle flaws in the responses of human players using inductive and multi-hop reasoning. Inductive\nreasoning (Misra et al., 2022; Yang et al., 2022) involves making predictions or drawing conclusions\nbased on a set of observations, where observations are typically empirical and not guaranteed to be\ntrue. In this game, the LLM observes multiple question-answer pairs to make a final prediction about\nthe truthfulness of the user's statement. Ideally, a good LLM utilizes multi-hop reasoning to connect\nclues from existing information, develop effective questioning strategies that maximize information\ngain and apply inductive reasoning to draw a well-informed conclusion."}, {"title": "2.2 GAME FORMULATION", "content": "In GameArena, each game session starts with randomly pairing a game G with a state-of-the-\nart LLM fk parameterized by \u0398k from a list of candidate models {fk} and one of the candi-\ndate system prompts be xk,p. Let the maximum round limit for G be N, the ground truth be\ng. For each turn i, let x\u2081 be user's input, fk's generation at each time step t can be denoted as\nYi,t = fk ([xk,p, x1, y1, \u2026\u2026\u2026, \u0425\u0456, \u0423\u0456, [1:t\u22121]]; \u0398k). We denote the complete output of length T is\nyi = yi,[1:T]. For each y\u017c, there are two possible output types. The first one is an ordinary question\nor answer in response to user's input to advance the gaming process. The other is a game secret\nprediction which requires the model to either guess the target word in Akinator and Taboo, or make\na prediction on the user statement's truthfulness. We denote the first type as y and the second type\nas pi. The game ends whenever the maximum round limit is reached or the user verifies pi = g. A\nwinner can therefore be decided."}, {"title": "3 EVALUATING LLM REASONING THROUGH GAMES", "content": "3.1 RETROSPECTIVE ANALYSIS\nIn order to reveal the model's intermediate thought process throughout the game session and assess\nvarious reasoning capabilities demonstrated by the model's hidden chain-of-thought (Wei et al.,\n2022), we conduct a retrospective analysis based on the chat history we collect from the game\nsessions. To reenact the game trajectory and capture the model's hidden thoughts during the game\nsessions, we first keep the system prompt, game history, and inference parameters identical to those\nused during the original gameplay. Then, we prompt the model to generate intermediate outputs,\nand conduct quantitative and qualitative analysis based on those outputs.\nAkinator. The goal of Akinator's retrospective analysis is to assess the model's multi-hop rea-\nsoning and deductive reasoning capabilities (Yang et al., 2018; Creswell et al., 2022). After each\nquestion-answer round, the model is asked to generate a ranked list of possible objects from its hid-\nden thoughts during the gameplay. The object list prioritizes those it considers most likely based on\nits reasoning process. For each game session, we then inspect the ranked object lists from all rounds\nto determine whether the secret object has been correctly identified and its position in the rankings.\nTaboo. The retrospective analysis of Taboo aims to evaluate the model's multi-hop reasoning and\nabductive reasoning capabilities (Yang et al., 2018; Jung et al., 2022). Similar to Akinator, the model\nis prompted to generate a list of possible target words based on all previous game history after each\nuser prompt. The list of possible words is ranked from the most to least likely by the model.\nBluffing. The Bluffing game's retrospective analysis aims to reflect the model's multi-hop reasoning\nand inductive reasoning capabilities (Yang et al., 2018; Misra et al., 2022). After the user's statement"}, {"title": "3.2 EVALUATION METRICS", "content": "3.2.1 OUTCOME METRICS\nAn intuitive set of metrics from our gaming data is the gaming outcome. In Akinator and Bluffing,\nusers provide feedback on whether the model made a correct guess, which determines the winner.\nIn Taboo, a ruled-based keyword detection mechanism is used in Taboo to check whether the user\nhas violated the game rules or the model has lost by saying the word. For each game, we track the\naverage number of rounds and the winning rate by calculating \u2211iri/Ng and \u2211i Ii/Ng, where Ng\nis the total number of game sessions for game G, ri is the number of gaming rounds for each game\nsession, and I\u00bf \u2208 {0, 1} is a binary indicator function of whether the LLM won that session.\n3.2.2 PROCEDURAL METRICS\nIn retrospective analysis, for any turn i, as long as yi \u2260 g, we obtain intermediate results by fixing\nall the game history [xk,p, X1, 1,\u00b7\u00b7\u00b7, xi, yi], and ask the model to justify its response yi or pi\nusing quantifiable data, such as an object list or a truthfulness rank. Consider the justification as y,\nwe iterate over each game session to collect step-by-step reasoning data [y\u00ed,\u2026\u2026,y's], from where\nprocedural metrics will be calculated for quantitative evaluations and rankings in Table 3.\nWe developed several metrics to evaluate each reasoning capability (Table 1) based on the model's\nintermediate outputs from retrospective analysis. Below, we describe how these metrics are mapped\nto each reasoning capability and how they are computed.\nAkinator. Recall rate, top-k recall rate, and disparity ratio reflect the model's multi-hop reasoning\ncapability in Akinator. The recall rate metrics determine how well the model utilizes preceding\ninformation by measuring how often the correct object appears in object lists retrieved for each game\nsession during retrospective analysis. In each game session, recall rate represents the proportion of\nmodel-proposed object lists that contain the correct object. Top-5 recall rate and top-10 recall rate\nrepresent the proportion of object lists where the rank of the correct object is within the top 5 and\n10 positions. Higher recall rates imply higher utilization of preceding information, indicating better\nmulti-hop reasoning skills.\nMulti-hop reasoning is also reflected through the question quality in Akinator. We use disparity\nratio to assess the information gain of a question. In Akinator, high-quality questions should divide\nthe possibilities evenly into two halves (Sasson & Kenett, 2023). Thus, we compute the disparity\nratio based on how evenly a model-generated yes/no question can divide the object list proposed one\nround prior into two categories. Evenly divided object lists have a lower disparity ratio, suggesting\nhigher information gain.\nconsistency rate, recall rate", "equations": ["disparity ratio = \\frac{|size_{yes} - size_{no}|}{size_{object\\_list}}"]}, {"title": "4 EXPERIMENTS", "content": "4.1 SETUP\nGameArena evaluates LLM reasoning using gaming data to compare five SOTA models: GPT-\n40 (OpenAI, 2024b), Claude 3.5 Sonnet (Anthropic, 2024), Gemini-1.5 Pro (Reid et al., 2024),\nMistral Large 2 (Mistral, 2024), and LLaMA-3.1 405B-Instruct (MetaAI, 2024). Over a 10-week\nperiod from July 2024 to September 2024, we collect a total of 2240 game sessions using Cloud Re-\nsearch (Hartman et al., 2023) for evaluation. We then conduct retrospective data analysis introduced\nin Section 3 on the gaming data to obtain outcome metrics and procedure metrics for each model."}, {"title": "4.2 DATA EFFICIENCY", "content": "We compared the data efficiency of GameArena and Chatbot Arena by analyzing the useful data\nrates. We obtained real conversation data from the Chatbot Arena Team for the week of August 26th\nto September 1st and compared it with the data collected by GameArena during the same period.\nWe considered data as useful if it provides meaningful signals that can be used to evaluate models.\nIn Chatbot Arena, data was deemed useful if it included at least one vote (e.g., for left, right, tie, or\nboth bad). For GameArena, we considered the game session useful if the user completed the entire\ngame round and provided feedback on the game outcome (i.e., win or lose).\nWe found that 86.9% of the data from GameArena were valid and useful, while only 4% of total\nconversations in Chatbot Arena provided meaningful votes due to its reliance on voluntary partici-\npation. Our results suggest that GameArena could improve user engagement through gamification\napproaches and collect meaningful data for evaluating LLMs more efficiently."}, {"title": "4.3 USER STUDY", "content": "Participant ratings. We conducted a user study to compare the user experience and willingness to\nparticipate in GameArena and Chatbot Arena (Chiang et al., 2024). We recruited 100 participants\nthrough Cloud Research (Hartman et al., 2023) and asked them to try both platforms and fill out a\nsurvey to provide their feedback. Each participant was randomly assigned a game in GameArena\nand asked to play at least three rounds. For Chatbot Arena, participants voted on the better responses\nfrom two models to their questions. We provided sample questions from MT-Bench (Zheng et al.,\n2023) for reference, and participants were also free to create their own questions. Participants rated\neach platform on how much they enjoyed it, how satisfied they were with the experience, and how\noften they would like to participate on a 5-point Likert scale.\nOur user study results suggest that GameArena is more engaging and enjoyable for users (Fig. 4).\nOn average, over 70% of users liked the games in GameArena, compared to only 45% who enjoyed\nvoting in Chatbot Arena. Additionally, over 80% of participants reported satisfaction with gameplay\nexperience in GameArena, compared to less than 40% of users felt satisfied about the experience\nwith Chatbot Arena. Participants also expressed stronger willingness to participate in GameArena,\nwith over half of the participants would like to play these game frequently, whereas interest in\nChatbot Arena was more evenly distributed across different participation levels."}, {"title": "4.4 RANKING RESULTS", "content": "Model performance on outcome metrics. The final outcome metrics of the three games are shown\nin Table 2. The win rates of both claude-3-5-sonnet-20240620 and gpt-40-2024-08-06 are higher\nin all three games compared to other models evaluated. The high win rate here reflects the strong\noverall reasoning ability of the two models.\nModel performance on procedural metrics. Table 3 presents the procedural metrics designed to\nreflect the models' reasoning skills from the retrospective analysis of all three games.\nAkinator. Claude-3-5-sonnet-20240620 performs best on recall rates, average first appear round,\nand average final rank, demonstrating stronger multi-hop reasoning and deductive reasoning capa-\nbilities compared to other SOTA LLMs being evaluated. Gpt-4o-2024-08-06 also has high perfor-\nmance in the metrics of reasoning capabilities.\nTaboo. The high performance of claude-3-5-sonnet-20240620 in recall rates, average first appear\nround, and average final rank exhibits stronger multi-hop and abductive reasoning capabilities. No-\ntably, llama-3-405b demonstrates strong multi-hop reasoning capabilities with leading recall rates,\nwhich differs from the results observed in the Akinator game. This discrepancy may be due to the"}, {"title": "4.5 RANKING COMPARISONS", "content": "Cross-dataset consistency analysis. GameArena can be extended to include as many games as\npossible for evaluating LLM reasoning, following the formulation in Section 2.2. In some games\nlike Taboo, we can restrict the set of secret words users are allowed to use, while in others, like\nAkinator and Taboo, users have the freedom to choose the game secret. Therefore, it is crucial to\nquantify the variances in evaluations when the game secret space is large to ensure consistency and\nfairness across different game setups.\nWe conducted a controlled experiment to quantify GameArena's evaluation consistency on these\ngames. In this experiment, we asked human experts (graduate students) to hand curate two distinct\nsubsets of gaming data, where the game secrets are sufficiently diverse to both cover a wide range\nof topics. For each game, the two sets each contains 50 game sessions. We show the outcome\nmetrics for each set in Table 5. We only evaluate the Akinator, and the Bluffing game as Taboo has a\nrestricted game secret space. The results from both subsets show a strong agreement, demonstrating\nthe reliability of our ranking system when applied to a large, diverse set of game secrets. More\ndetails of the experiment can be found in Appendix B."}, {"title": "5 RELATED WORK", "content": "Benchmarking LLM reasoning capabilities. Existing works develop a wide range of benchmarks\nto test the models' reasoning capabilities, such as logical reasoning (Hao et al., 2024; bench authors,\n2023; Valmeekam et al., 2023; Saparov & He, 2023), coding and mathematical reasoning (Cobbe\net al., 2021). However, these benchmarks often rely on static datasets that can be vulnerable to\ndata contamination (Sainz et al., 2023; White et al., 2024) and easily become saturated (Kiela et al.,\n2021; Perlitz et al., 2024). To address this, Chiang et al. (2024); Zheng et al. (2023); Zhao et al.\n(2024) use human or LLM as judges to evaluate the responses to dynamic questions of different\nLLMs. However, recent studies found this may introduce various biases (e.g., style preference) that\ncompromise the reliability of evaluation results (Chen et al., 2024; Li et al., 2024). In contrast,\nGameArena evaluates LLMs through interactive gameplay with humans using objective metrics\nderived from the gaming process to create a more engaging and robust benchmarking approach.\nEvaluating large language models using games. Recent works have explored interactive and\ngame-based evaluations to assess LLM capabilities (Wu et al., 2024; Liu et al., 2023). For example,\nMadge & Poesio (2024); Hafner (2021) use single-player games (e.g., MineCraft) as a controlled\nenvironment to test specific capabilities of LLMs. Topsakal et al. (2024); Xu et al. (2023); Cheng\net al. (2024) employ multi-player games to evaluate LLM through self-play or competition among\ndifferent LLM agents. In contrast, GameArena uses a crowdsourced approach to engage human\nplayers in the LLM evaluation process by offering games that people can play to have fun.\nOur work is similar to RedTeam Arena (Angelopoulos et al., 2024), which features a game prompt-\ning players to elicit target \"bad words\" from models, but the key difference is that RedTeam Arena\nfocuses on red-teaming, while our games are designed for evaluation of LLM reasoning capabilities.\nGames with a purpose (GWAPs) are online games designed to be fun for humans to play while\nalso accomplishing meaningful tasks that are challenging to handle by computers alone (Von Ahn\n& Dabbish, 2004; Von Ahn, 2006; Von Ahn & Dabbish, 2008). For example, Eye into AI (Morrison\net al., 2023) uses GWAP techniques to evaluate explainable AI approaches by revealing salient\nportions of an image and asking players to guess the object.\nTo the best of our knowledge, we are the first to apply GWAP techniques for benchmarking LLMs.\nWe integrate imperfect LLMs into three playful games in a way that does not disrupt the gameplay\nexperience while also generating meaningful data for evaluating LLM reasoning capabilities."}, {"title": "6 CONCLUSION", "content": "In this work, we present GameArena, a dynamic benchmark for evaluating LLM reasoning capabili-\nties through interactive gameplay with humans. GameArena enables the efficient collection of LLM\nstep-by-step reasoning data with human labels as a natural by-product of human gameplay sessions,\nwhile keeping participants entertained and engaged. Our initial release of GameArena consists of\nthree conversational games that require strong reasoning skills: Akinator, Taboo and Bluffing. By\ncontrolling LLM interactions with humans through specific game rules and objectives, these games\ncan assess fine-grained LLM reasoning skills, including inductive, deductive, abductive and multi-\nhop reasoning. We plan to incorporate more games into GameArena and improve our evaluation\nmetrics to further our understanding of LLM reasoning capabilities in complex, interactive contexts."}, {"title": "7 ETHICS STATEMENT", "content": "Since GameArena involves human subjects playing games to collect LLM reasoning data, we re-\ncruited participants from Cloud Research and compensated them 8 USD per hour for their time.\nThe crowd workers consented to the use of their responses in the study, and no personal data was\ncollected. Our user studies were approved by the institutional review board of our organization."}, {"title": "A GAME DESIGN", "content": "A.1 GAME RULES\nWe provide a detailed description of the game rules for each game in GameArena below and explain\nwhy these games are challenging for LLMs.\n\u0391.1.1 AKINATOR\nIn the Akinator game, the LLM attempts to determine what object the player is thinking of by\nasking up to 20 yes or no questions. The goal of the LLM is to guess the answer correctly with the\nfewest number of questions possible. In each round, the LLM asks a binary question, and the player\nresponds with \"yes,\" \"no,\u201d \u201cprobably yes\u201d \u201cprobably no,\u201d or \u201cnot sure.\u201d The LLM asks strategic\nquestions to gather information until it feels confident enough to make a guess. The player then\nprovides feedback on whether the guess is correct. If correct, the model wins; if not, it can continue\nasking questions or make another guess until it uses all 20 chances. If the LLM fails to guess the\nobject, the player is prompted to reveal the target object to ensure data quality and prevent cheating.\nThe Akinator game is challenging for LLMs due to its vast, open-ended answer space. LLMs must\naccurately interpret user responses, manage ambiguity, and avoid irrelevant or redundant questions\nto efficiently narrow down a wide range of possibilities from limited information. This challenge\nis intensified by the need to connect information across diverse knowledge domains and engage in\ncomplex, multi-step reasoning to arrive at the correct answer.\nA.1.2 TABOO\nIn the Taboo game, the goal of the human player is to prompt the LLM with questions that will lead\nthe model to say the target word. Meanwhile, the LLM tries to guess the word to avoid uttering\nit. The system first randomly assigns the human play a taboo word (e.g., \"eggs\") from a predefined\nword list. The user must prompt the model to say the word without mentioning it directly. In Fig. 2,\nthe user begins with a general description of common breakfast foods, which leads the LLM to\nrespond with \"cereal.\u201d The user then narrows it down by asking about foods that can be scrambled\nor fried, prompting the LLM to successfully utter the target word. At this point, the LLM has a\nchance to guess the word; if it guesses correctly, it wins.\nThe Taboo game presents several challenges for LLMs. The model must interpret vague clues\nand general descriptions, which can vary significantly, to recognize when it is close to the target\nword based on the user's hints. This requires quick connections across various potential answers.\nAdditionally, the LLM needs to balance providing relevant responses with avoiding the taboo word,\nnecessitating quick and strategic reasoning.\nTo make the game more challenging and engaging for users, the human wins only if they get the\nLLM to say the word within five rounds, with each question limited to 140 characters, and the LLM\ndoes not guess the word correctly. This setup makes the taboo game a challenging task for the\nLLM, as human players will avoid easy and detailed clues, requiring the LLM to navigate complex\nlanguage cues and context clues to figure out the target word.\nA.1.3 BLUFFING\nIn the Bluffing game, the human player first makes a statement that can be true or false, and attemp\nto convince the LLM of its truthfulness. The LLM can ask up to five questions to determine if the\nplayer is lying, aiming to accurately judge the truthfulness of the statement in the fewest rounds\npossible. For instance, LLMs may detect deception by identifying subtle flaws or inconsistencies in\nthe player's responses. After the LLM makes its prediction, the player provides feedback on whether\nthe statement is true.\nThe Bluffing game is challenging for LLMs because it requires them to evaluate the truthfulness of\na statement based on limited interactions (i.e., within only five question-and-answer pairs). LLMs\nmust effectively interpret the nuances of human communication, including tone and context, to\ndetect deception. Additionally, they have to formulate strategic questions that maximize information\ngain while minimizing the risk of misleading answers."}, {"title": "A.2 GAMEARENA USER INTERFACE", "content": "We demonstrate the user interface for each game from Figure 6 to Figure 8."}, {"title": "A.3 EXAMPLES OF GAMEPLAY SESSIONS", "content": "We demonstrate one example of gameplay session for each game from Figure 9 to Figure 11."}, {"title": "A.4 EXAMPLES OF GAME PROMPTS", "content": "\u0391.4.1 \u0391KINATOR\nYou are an intelligent assistant tasked with playing a game of Twenty\nQuestions. Your goal is to guess a generic object that the user is\nthinking of. Assume the role of a creative player while examining\nthe game history. Utilize your imagination to craft an engaging\nand thought-provoking next question that not only continues the\ngame's natural progression but also ignites participants' interest\nand curiosity. Ensure that the question resonates with the ongoing\nnarrative while offering a unique twist to maintain excitement. The\nrules are specified as follows:\n1.\nQuestioning Numbering: Each question starts with a question\nheader 'Question N:' where N is a numerical integer from 1 to 20.\nEnsure each question is sequentially numbered", "Question N:": "n2.\nSingle Question Format: In each turn", "Answers": "Only these responses are acceptable: Yes", "Tolerance": "Be tolerant of potentially incorrect answers\nfrom the user and adjust your questions accordingly.\n5.\nAvoid Redundancy: Do not repeat or ask similar questions more\nthan once. Aim to ask unique questions that progressively narrow down\nthe possibilities.\n6. Efficient Questioning: Strategically balance asking more general\nquestions that can broadly narrow down possibilities versus making a\nguess to save question quota.\n7. Confident Guessing: If confident", "format": "n'This is a guess are you thinking of $object?'", "Guessing": "Don't guess too specific an object. When\nconfident enough"}, {"format": "This is\na guess are you thinking of $object?", "Questioning": "Base each question on the user's previous\nresponses to refine your understanding and approach.\n10. Consider Edge Cases: When making a guess, don't constrain"}]}