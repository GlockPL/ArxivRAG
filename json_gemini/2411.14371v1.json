{"title": "Synthesising Robust Controllers for Robot Collectives\nwith Recurrent Tasks: A Case Study", "authors": ["Till Schnittka", "Mario Gleirscher"], "abstract": "When designing correct-by-construction controllers for autonomous collectives, three key challenges\nare the task specification, the modelling, and its use at practical scale. In this paper, we focus on a\nsimple yet useful abstraction for high-level controller synthesis for robot collectives with optimisation\ngoals (e.g., maximum cleanliness, minimum energy consumption) and recurrence (e.g., re-establish\ncontamination and charge thresholds) and safety (e.g., avoid full discharge, mutually exclusive room\noccupation) constraints. Due to technical limitations (related to scalability and using constraints\nin the synthesis), we simplify our graph-based setting from a stochastic two-player game into a\nsingle-player game on a partially observable Markov decision process (POMDP). Robustness against\nenvironmental uncertainty is encoded via partial observability. Linear-time correctness properties\nare verified separately after synthesising the POMDP strategy. We contribute at-scale guidance on\nPOMDP modelling and controller synthesis for tasked robot collectives exemplified by the scenario\nof battery-driven robots responsible for cleaning public buildings with utilisation constraints.", "sections": [{"title": "Introduction", "content": "Hygiene in public buildings has been hotly debated ever since the increased safety requirements during\nthe coronavirus pandemic. Autonomous robot collectives can help to relieve cleaning staff and keep\nhighly frequented buildings (e.g., hospitals, schools) clean. However, commercially available solutions\nhave their limitations, for example, a need for manual task programming or a lack of online adaptability.\nIn contrast to domestic homes, there are strict regulations (e.g., [8] for schools) that stipulate which\nareas must be cleaned and at what intervals. Changing operational conditions (e.g., room occupation,\nequipment reconfiguration, cleaning profile, regulations) create the need for an automatic generation of\ncleaning schedules for robot collectives and for proving their compliance with hygiene requirements.\nThis scenario is an instance of a multi-faceted recurrent scheduling problem discussed below.\nCleaning Buildings as a Running Example. When planning the cleaning of a building (e.g., a school),\nwe can use its layout in the form of a room plan. Apart from a set of m rooms R =\n{R1,...,Rm} with an assigned area, such plans include the connections between rooms and the locations\nof n charging stations C = {C1,...,Cn}. A collective of k \u2264 n robots B = {B1,...,Bk} is responsible for\ncleaning R. B is tasked to keep R clean while charging its batteries using C. Each robot has a limited\nbattery size and a charging point in C as an assigned resting position. Additionally, there is a room util-\nisation plan containing the times when the rooms are in use, which can change on a daily\nbasis. While a room is in use, no cleaning robot may be inside and, thus, cannot clean it.\nThe task is to create a cleaning strategy (or schedule) for B, constrained by the utilisation plan and\ncharging needs. Moreover, this strategy should keep the rooms clean enough, while minimising battery\nconsumption. To specify this task adequately, we define cleanliness in terms of contamination. Since"}, {"title": "Preliminaries", "content": "Stochastic modelling is about describing uncertain real-world behaviour in terms of states and probabil-\nistic actions producing transitions between these states. For stochastic reasoning (i.e., drawing conclu-\nsions about such behaviour), we use probabilistic model checking. This section introduces the stochastic\nmodels, temporal logic, and tools we employ for synthesis and verification.\nPartially Observable Markov Decision Processes. Let Dist(X) denote the set of discrete probability\ndistributions over a set X, and R>0 be the non-negative real numbers. Then, a POMDP [13] is given by\nDefinition 2.1. A POMDP is a tuple M = (S,5,A,P,R, O,obs), where\n\u2022 S is a set of states with s\u2208 S being the initial state,\n\u2022 A is a set of actions (or action labels),\n\u2022 P: S \u00d7 A \u2192 Dist(S) is a (partial) probabilistic transition function,\n\u2022 R = (Rs, Ra) is a structure defining state and action rewards Rs : S \u2192 R>0 and RA : S \u00d7 A \u2192 R\u22650,\n\u2022 O is a finite set of observations, and\n\u2022 obs : S \u2192 O is a labelling of states with observations."}, {"title": "Developing Controllers by Example of the Cleaning Scenario", "content": "In this section, we first state our synthesis problem and then describe our approach to strategy synthesis\nand verification as illustrated in Figure 2.\nProblem Statement. Our aim to synthesise a collective controller is specified in the LTL property\nG((@\u2192 F\u2264T (\u03c9^)) ^G\u2264T \u00a2s)\nperiodically achieve task safely (implied by our approach)\nwhere @ is the recurrence area (including s and acting as a task invariant), or specifies an invariant-\nnarrowing condition,\u00b9 \u00f8s specifies task safety, and T > 1 is the recurrence interval (an upper cycle-time\nbound). Among the controllers satisfying (2), we look for an optimal (e.g., one minimising energy\nconsumption) and robust (e.g., under partial observability of stochastic room contamination) one.\nOverview of Controller Development. First, a spatio-temporal abstraction of the cleaning scenario is\nmodelled using a coloured Petri net (CPN) for coordination modelling and finite automata for describing\nrobot-local behaviour. These aspects are translated into a reward-enhanced POMDP M (Sections 3.1\nto 3.3) in support of multiple robots (Section 3.4), which uses probabilistic actions to reduce the state\ncount of a hypothetical detailed model. Then, a strategy \u03c3 is synthesised for M (Section 3.5), which is\nused to derive a deterministic, non-probabilistic, and integer-valued model M\u00ba (Section 3.6). Finally,"}, {"title": "Spatio-temporal Abstraction", "content": "State Space. For the implementation of the problem (e.g., cleaning task), it is important to keep the\nnumber of states as small as possible. Therefore, large parts of the initial problem are abstracted.\nInstead of a complete room plan with area assignment, the abstracted environment uses a graph that\nonly contains the different rooms and charging stations. An example of such a graph for the room plan\nin Figure la can be seen in Figure 3. A pointer B\u2081.x, i \u2208 0..k, to a room or charging station in this graph\nis used to keep track of the position of robot Bi. The behaviour of the charging state Bi.c of the robot's\nbattery is described by a number of discrete charging levels and charge and discharge rates.\nThe total contamination is represented by a counter Rj.d, j \u2208 0..m, whose maximum value is the\ncontamination threshold Rj.threshold. Since we want to avoid reaching a state with the contamination\nat this threshold, it is not necessary to model contamination beyond Rj.threshold.\nActions and High-level Behaviour. Discrete values are used to model time as well, where the action\nat, as specified in Table 1 and described below, is performed at each discrete time step.\nThe CPN in Figure 4a provides a high-level description of the moves of the collective B across char-\nging stations C and rooms R (Figure 3). The abstract at action (black bar) expands to a range of concrete\nPOMDP actions at jB\u2081... jBk with j\u0432; \u2208 0..m. Whenever at is taken, any number of tokens (black dots,\nrepresenting robots) on the places (grey circles, representing rooms and charging stations) can flow sim-"}, {"title": "Quantitative and Stochastic Abstraction", "content": "For the sake of simplicity, this section and the following will focus on a reduced problem with only one\nrobot. The case of multiple robots will be reintroduced in Section 3.4.\nAs already mentioned, the PRISM-encoding of M is divided into three modules operating on three\nindependent fragments of the state space S: The state of the robots, the room contamination, and time.\nThe cleaner module describes the behaviour of cleaning robot B1. An integer is used to model the\nrobot position B\u2081.x. For this, each room and charging station is mapped to an integer bijectively.\nFor example, the room graph in Figure 3 can be described by the following relation: Co \u2192 0, R\u2081 \u2192\n1,R2\u21923,R3\u21924, R4\u21925,R5 \u2192 6,C2 \u2192 7. The battery status B\u2081 .c is also described as an integer\nwhose upper bound is the maximum charge B\u2081.maxcharge of B\u2081's battery, see Listing 2."}, {"title": "Choice of the Reward Function", "content": "Four requirements, a valid strategy must satisfy, can be derived from Formula (2) and Table 1:\nFR At time T, all robots must be back in their initial location, so that the plan can be repeated.\n@C At time T, the battery of a robot must not be lower than its threshold charge level.\nBC The battery of a robot must never be empty.\nCT The total contamination of any room must never exceed its contamination threshold.\nWhen just focusing MDP verification rather than synthesis, it would be sufficient to describe these\nas PLTL constraints. However, PLTL constraints cannot be used as queries for synthesising strategies,\nsince generating strategies through PRISM requires each path to be able to fulfil all constraints eventually."}, {"title": "Cooperation between Multiple Robots", "content": "To keep M simple, robots are not modelled as sep-\narate modules, but the state of the cleaner module is\nextended to include the positions of all robots (see Fig-\nure 4a). This simplification excludes all transitions from\nthe model that would lead to conflicts in robot behaviour\n(e.g., the case where several robots clean the same room\nat the same time). Additionally, each robot has its own battery charge, see Listing 5."}, {"title": "Synthesising Strategies (under Uncertainty) for the Cleaning Scenario", "content": "PRISM'S POMDP strategy synthesis works under certain limitations. As indicated in Section 3.3, it is\nnot possible to use Rrin=?[4] for synthesis if Pmin=?[Y] < 1, that is, if M contains y-violating paths\nunder some strategy \u03c3. Hence, we choose a y that defines a state that all paths converge at, and syn-\nthesise a strategy that minimises the total reward (since we model R using penalties) up to that point. A\ncommonality of all paths is the flow of time, so the reachability reward-based synthesis query\n\u2211R\u2208{penalties, energy consumption, utilisation} Rmin=?[Ft = T]\nuses a target state where time t is equal to some maximum time T.\nAdditionally, a mapping obs needs to be specified, which defines the observa-\ntions of M that \u03c3can use to make choices. In this case, \u03c3 cannot use the con-\ntamination flags to make its choices. If \u03c3 could consider the contamination flag,\nit would not need to account for the accumulative probability; \u03c3 could just check\nif a contamination flag is true and act accordingly. We can hide the contamination\nflags from \u03c3 by defining obs to just include the position and charge of the robot\nand the time"}, {"title": "Creating an Induced Model from the Strategy", "content": "It is possible to create a cleaning schedule from the synthesised strategy \u03c3. However, it is not yet possible\nto verify o regarding the constraints listed in Section 3.3. This is because, up to this point, contamination\nwas modelled in M only as a probabilistic factor. To verify the contamination constraint CT, below, we\ninclude a non-probabilistic contamination model in M' using counters to represent contamination.\nModelling the Contamination Value. To verify that the contamination value (modelled as a boolean\nsub-MDP of M) never actually reaches the thresholds Rj.threshold, j \u2208 1..m, it is necessary to transform\nM into an MDP M' that accounts for the actual values Rj.d. We accomplish this in M' by integer-valued\ncontamination counters Rj.d replacing the boolean variables Rj.d in M.\nApplying the Strategy. Apart from using contamination counters,\nour model does no longer contain probabilistic choices. Concretely,\neach probabilistic choice in M (branching to each possible selec-\ntion of fully contaminated rooms, Listing 3) is replaced by an action\nin M' performing a simultaneous update of all contamination coun-\nters (Listing 9). We can use the generated strategy \u03c3to derive the\ninduced deterministic model M\u00ba from M', which acts according to\nthe strategy. This step is done by modifying the preconditions of the\natN_N_... actions of the time module to only be able to trigger when that action is chosen at the same\npoint in the strategy. If, for example, within 6, the at0_1 action is only chosen in time steps 8 and 10,\nwe modify the time module"}, {"title": "Experimental Evaluation", "content": "Our experimental evaluation addresses two research questions (RQs).\nRQ1: Can we synthesise reasonable strategies for multiple robots?\nIn the following, an instance of the example model with only one robot is considered first. The contam-\nination rate is the same for all rooms, except that R5.d has a threshold value R5.threshold of 24 (based\non a contamination rate of 1 h\u00af\u00b9), which is twice as high, whereas all other rooms have a threshold value\nof 12. We identified @ manually by examining the generated strategy. Using the method described in\nSection 3, a strategy was generated that meets all the requirements. This strategy is visualised in Fig-\nure 6a. Each action is shown with a blue arrow, at which the time step in which the action is to be\nexecuted is annotated. To develop a strategy for two robots, the battery charge was halved to keep the"}, {"title": "RQ2: How do model parameters influence the synthesis of recurrent strategies?", "content": "We evaluate the model using a_bit, Ri.pr, and the fixed-grid resolution g as parameters. Figure 7 visu-\nalises the result using these parameters on a simplified model, which only contains one robot and omits\nroom R5 and charger C2. When building \u03c3, we set Ri.pr = cumulative_probability||R|. The generated\nstrategies were verified by iteratively assuming w-thresholds (Table 2) from a set chosen appropriately.\nFigure 7 contains four plots for resolutions g = 1..4. Correct non-recurrent strategies are represented\nby a blue dot, correct recurrent strategies by a green dot, and incorrect strategies in red.\nWe deemed the simplification of the model necessary to allow a timely execution of the test series,\nwhich contained 400 experiments in total (100 for each grid resolution). Detailed information about the"}, {"title": "Discussion", "content": "Selecting the Recurrence Area w. When evaluating the strategy, @ was chosen either by examining\nthe strategy manually (Figure 6c) or by verifying a list of probable ws. Further work may focus on\nfinding probable ws from the room layout, and generating strategies which fulfil these ws.\nComplexity of the Cleaning Scenario. For the evaluation, we considered a rather simple room layout.\nThe performance of the above described method may be different with larger room graphs, more complex\nroom layouts, a larger number of robots, and tighter restrictions on battery charge and room utilisation.\nMoreover, our model allows us to find strategies that operate with a varying number of robots. Given\nthat some robots remain idling all the time, our optimal synthesis could also be used to find the smallest\nsubset Bmin B or minimal number kmin \u2264 k of robots for an optimal task performance.\nThe complexity of the model is heavily dependant on the number of rooms, the maximum time T,\nand the maximum charge of the robots. Following the comprehensive scheme in Section 4.2, we were\nable to calculate a 12-hour (T = 12) cleaning schedule for 3 robots with 11 rooms and a maximum charge\nof 6 in 15 hours. The corresponding belief-MDP B(M) contains \u2248 690k states and \u2248 11.8m transitions.\nAdjusting Grid-Resolution vs. Filtering Strategies. For industry-size POMDPs, a high resolution g\ncan lead to an impractically high computational effort when solving the mostly NP-hard approximate\nanalysis (i.e., verification, synthesis) problems. Hence, our approach is to keep g just fine enough to\nfind some (not necessarily globally optimal) strategy \u03c3 and verify more nuanced properties of the quasi-\nMDP6 M\u00ba derived from M by applying \u03c3. In M\u00ba, verification is simpler (no belief-MDP B(M) is\ncomputed), also the strategy (integrated in M\u00ba) can directly observe the outcome of each action and does\nnot have to memorise a finite observation history. Despite the expansion of Ri.d to integers, M's state\nspace is expected to be smaller than B(M)'s state space for the applied values of g.\nParameter Selection. For the evaluation in Section 4.2, a set of values for the parameters a_bit and\nthe contamination probability was chosen. Via g (Section 2), we reduced the resolution of the fixed\ngrid (i.e., a wider grid width) to limit the number of states in the belief space approximation B(M).\nHowever, our findings in Section 4.2 suggest that increasing the resolution, while keeping the cumulative\ncontamination probability around 40% and the value of a_bit around 300 leads to the synthesis of better\nstrategies. However, these values may not be universally favourable for any room layout, and it may be\npossible to synthesise better strategies using a different set of parameters. Further work may focus on\nbetter ways of parameter selection.\nGeneralisation to Other Applications. The running example in our case study focuses on a cleaning\nrobot collective. However, we think that our approach and model can be transferred rather straightfor-\nwardly to other spatio-temporal settings with recurrent tasks, for example,\n\u2022 firefighting drone collectives tasked with repetitive sector-wise fire detection and water distribution\nand with partially observable quantities such as ground temperature and extinction level;\n\u2022 geriatric care robot collectives tasked with recurrent monitoring and care-taking tasks (e.g., med-\nication supply) with patient satisfaction and health status being partially observable;\n\u2022 general patrolling collectives tasked with monitoring or supervising specific environments [15]."}, {"title": "Conclusion", "content": "We proposed an approach using weighted, partially observable stochastic models (i.e., reward-enhanced\nPOMDPs) and strategy synthesis for optimally coordinating tasked robot collectives while providing\nrecurrence and safety guarantees on the resulting strategies under uncertainty. Along with that, we dis-\ncussed guidance on POMDP modelling and strategy synthesis. We focused on a cleaning robot scenario\nfor public buildings, such as schools. Our notion of correctness combines (i) safe recurrence (e.g., repet-\nitively accomplish the cleaning task while avoiding to collect penalties), (ii) robustness (e.g., correctness\nunder worst-case contamination), and (iii) optimality (e.g., minimal energy consumption).\nFor scaling up strategy synthesis to scenarios beyond what can easily be tackled by stochastic game-\nbased synthesis, we addressed the key challenge [6] of reducing the state space and the transition relation\nof a na\u00efve model via partial observability (hiding details of stochastic room contamination) and by em-\nploying simultaneous composition (for robot movement). PRISM's grid-based POMDP approximation\nallowed us to adjust the level of detail of the belief space to synthesise strategies more efficiently. Fur-\nthermore, we softly encode the strategy search space using penalties and optimisation rewards and can,\nthus, shift the verification of more complicated properties to a later stage working with an unweighted\nand non-probabilistic behavioural model, again using a more detailed, numerical state and action space.\nHowever, decoupling synthesis from verification can require time-consuming experiments (Section 4.2)\nto identify regions of the parameter space for ensuring the existence of good recurrent strategies.\nIn future work, we will improve finding @ ensuring the existence of correct strategies (i.e., green dots\nin Figure 7). Ideally, we avoid defining & explicitly (e.g., by hiding time). In a larger example, we want\nto allow invariant-narrowing with or and observable stochasticity in the environment, such that \u03c3 can\ndepend on arbitrary variables. The reset of the contamination flag on a room visit (Db) could be refined\nby a decontamination rate in M\u00ba. Moreover, we aim to use multi-objective queries to include further\ncriteria (e.g., minimal contamination) for Pareto-optimal strategy choice. While PRISM imposes some\nlimits on the combination of queries and constraints, we will need to see how we can use tools such as\nEVOCHECKER (as, e.g., used in [17]) for POMDPs. Also, we can further reduce the action set by taking\ninto account trajectory intersections in the simultaneous movements (cf. Figure 4a). Finally, we want to\nconnect the synthesis pipeline with code generation, such as demonstrated in our previous work [5]."}]}