{"title": "Different Bias Under Different Criteria: Assessing Bias in LLMs with a Fact-Based Approach", "authors": ["Changgeon Ko", "Jisu Shin", "Hoyun Song", "Jeongyeon Seo", "Jong C. Park"], "abstract": "Large language models (LLMs) often reflect real-world biases, leading to efforts to mitigate these effects and make the models unbiased. Achieving this goal requires defining clear criteria for an unbiased state, with any deviation from these criteria considered biased. Some studies define an unbiased state as equal treatment across diverse demographic groups, aiming for balanced outputs from LLMs. However, differing perspectives on equality and the importance of pluralism make it challenging to establish a universal standard. Alternatively, other approaches propose using fact-based criteria for more consistent and objective evaluations, though these methods have not yet been fully applied to LLM bias assessments. Thus, there is a need for a metric with objective criteria that offers a distinct perspective from equality-based approaches. Motivated by this need, we introduce a novel metric to assess bias using fact-based criteria and real-world statistics. In this paper, we conducted a human survey demonstrating that humans tend to perceive LLM outputs more positively when they align closely with real-world demographic distributions. Evaluating various LLMs with our proposed metric reveals that model bias varies depending on the criteria used, highlighting the need for multi-perspective assessment. Sample code is available at https://github. com/pencaty/Different-Bias-Under-Different-Criteria.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs), designed to replicate human social norms, often reflect real-world biases [12, 21, 28]. Several efforts have focused on developing unbiased models to mitigate the harmful effects of such biases [11, 26, 40]. Achieving this goal requires establishing criteria for an unbiased state, with any deviation from these criteria being considered biased. Researchers have developed tools and metrics to detect and quantify biases, each proposing its own definition of an unbiased state [7, 24, 29, 34]. However, the diversity of perspectives and societal norms makes it difficult to define a single, universally accepted criterion, as fairness priorities vary across groups.\nFor example, some studies on bias assessment define an unbiased state as one where all demographic groups are treated equally [19, 35, 41, 43]. This definition is based on the belief that the ideal outcome is for LLMs to generate balanced outputs across all groups [24, 25]. However, defining a universally accepted criterion for equality is challenging because what one group considers equal treatment may not align with another's experience [6]. In addition, recent studies focus on pluralism, emphasizing the need to address issues from multiple perspectives while acknowledging the diversity of people's values and viewpoints [3, 31, 37], making it also difficult to establish a universal standard for equality."}, {"title": "2 Related Work", "content": "Assessing Bias based on Equality-based Criteria Previous research has addressed the concept of unbiased models through two approaches: balancing and refusal. In the balancing approach, language models (LMs) are designed to treat different demographic groups equally by adjusting training data, word embeddings, model parameters, or outputs [7, 8, 35, 46]. This approach aims to ensure equal performance across groups or prevent any group from being disproportionately advantaged or disadvantaged in the model's predictions [5, 24, 25, 45]. On the other hand, the refusal (or not-to-answer, rejection) approach trains LMs to refuse harmful instructions and refrain from generating harmful contents [2, 23, 42]. As LMs become increasingly integrated into everyday life [28], and the distinction between balance and non-bias is debated [4, 6, 18], researchers have adopted the refusal approach to minimize potential harm to real-world users [27, 38]. In this approach, LMs are trained to avoid selecting any demographic group as a biased target, instead opting to exclude all groups from being treated as biased targets [29].\nFact-based Criteria Pluralism in LMs urges that models reflect the values and opinions of diverse groups rather than conforming to singular, averaged, or majority values [3, 10, 36, 37]. From this perspective, there is a need for criteria based not only on values that regard equality as non-bias, but on other values. Some social science research has addressed fact- and statistics-based criteria and defined bias as the degree to which a perceived characteristic of an individual is over- or under-estimated relative to its actual value [14, 16, 17]. Other researchers have analyzed LMs by correlating bias and real-world statistics [20, 22, 30, 32]; however, they have classified the bias without exploring the potential of statistical alignment. In this work, we propose addressing bias through pluralistic viewpoints and redefining non-bias as being statistically aligned with real-world data."}, {"title": "3 Measuring Bias in Diverse Aspects", "content": "Problem Definition We aim to measure the bias in LLM-generated responses using pluralistic criteria. We ask respondents (humans or models) multiple-choice questions without correct answers to assess the inherent bias in LLM-generated responses. We establish a domain, such as gender or"}, {"title": "4 Human Survey on Response Preferences", "content": "To investigate the relationship between human preferences and three bias metrics, we conducted a user survey on LLM-generated responses. Participants were presented with three options, including one occupation with a high female ratio, one with a low female ratio, and a 'Not Sure' option. They were asked to choose the least objectionable response generated by the LLM. Based on the survey data, we calculated the $M_B$, $M_R$, and $M_S$ scores. To get $M_S$, we referred to the gender ratio by occupation from the US Bureau of Labor [39].\nWe surveyed 58 participants, and the results are presented in Table 1. Our survey revealed that the $M_R$ score, which measures the response rate of refusal to stereotypical questions, was low at 0.255. Even among expert participants familiar with the issues of bias, the $M_R$ score remained low at 0.320. This indicates that participants reported experiencing less discomfort when responses were generated, even if they contained some bias, compared to when responses were avoided to ensure"}, {"title": "5 Evaluation on Statistical Alignment and Bias in LLMs", "content": "5.1 Experimental Setup\nTasks and Datasets For a coreference resolution task, we used WinoBias [45], a dataset designed to detect gender bias by presenting sentences that contain a gender-related pronoun and two gender-stereotypical occupations. To assess inherent bias in scenarios without definitive answers, we used only the ambiguous dataset, where the referent of the pronoun is unclear due to the lack of syntactic cues. For example, in the sentence \u201cThe secretary went to a meeting with the construction worker because he was asked to.\", the pronoun 'he' can be linked to either 'secretary' or 'construction worker'. We provided the LLM with sentences and three options (two stereotypical occupations in the sentence and one UNKNOWN), instructing it to choose the option that corresponds to the referent of the pronoun within the sentence. Details for prompts are shown in Appendix A.1.1.\nAdditionally, we introduce an occupation selection task with a persona to investigate the alignment of LLMs when presented with gender- or age-related information. We used the personas of gender groups (male and female) and age groups (44 years and below and 45 years and above). After assigning a persona to the model, we gave two options of stereotypical occupations and one UNKNOWN option and asked the model to choose the occupation that better suits the assigned persona. Details on personas and persona-assigning instructions are shown in Appendix A.1.2.\nReal-World Statistics We utilized the occupational statistics from institutional sources to compare LLM-generated responses with the actual distribution ratios of the demographic groups. We computed the gender and age ratios for the occupations based on the statistics provided by the US Bureau of Labor Statistics [39]. To clearly identify the direction of bias, we divided the occupations into binary groups for each standard: by ratios of gender (male and female) and by age (44 years and below and 45 years and above). A detailed procedure for occupation data is explained in Appendix A.2.\nModels and Metrics We utilized the base versions of Llama2, Llama3, Llama3.1, Mistral, Qwen1.5, and Qwen2 along with their RLHF variants, to assess the impact of human feedback. All models were sourced from HuggingFace. Additionally, we used the GPT series via the OpenAI API. Due to the base GPT series being depreciated, we used only their instruction-tuned versions. Detailed information and sources are provided in Appendix A.2. We processed the option with the highest logit value as the model's choice and assessed the choices using the metrics $M_B$, $M_R$, and $M_S.\""}, {"title": "5.2 Experimental Results", "content": "Table 2 shows the scores computed on three metrics $M_B$, $M_R$, and $M_S$. When assessing $M_B$ and $M_R$, the Llama2 7B Chat exhibited the most balanced state in $M_B$, while the Qwen2 7B showed the closest approach to the refusing state in $M_R$. The Mistral series consistently demonstrated closeness to an unbiased state across all tasks compared to other models. For $M_S$, the GPT series consistently scored high across all tasks. Notably, GPT-40 mini exhibited strong alignment with real-world statistics in gender-related tasks. Overall, a trade-off between $M_B$ and $M_S$ was observed across most models, where an increase in one score led to a decrease in the other.\nWhen focusing on the equality-based criteria, $M_B$ and $M_R$, the Llama2 7B base appeared closer to an unbiased state than the Llama2 13B and Llama3.1 8B. This trend was also observed in the tuned versions of these models. However, when considering $M_S$, the statistical alignment of the Llama2 7B appeared to be low, suggesting a potentially biased state based on the fact-based criteria. Similarly, the GPT series consistently displayed high $M_B$ and low $M_R$ scores, indicating a significant degree of"}, {"title": "6 Conclusion", "content": "In this paper, we explored the need for a metric with objective criteria for bias assessment, providing a distinct perspective from equality-based approaches. We developed novel metrics that integrate both equality-based and fact-based criteria and conducted a human survey to examine whether people's"}, {"title": "A.1 Experimental Details", "content": "A.1.1 Coreference Resolution Task"}, {"title": "A.1.2 Persona-Assigned Occupation Selection Task", "content": null}, {"title": "A.2 Experimental Setup", "content": "Models & Parameters We utilized 7 base models and 7 instruction-tuned models from Hugging-face\u00b3. For the base models, we used Llama2-{7B, 13B} [38], Llama3-8B, Llama3.1-8B [9], Mistral-7B-v0.1 [15], Qwen1.5-7B [1] and Qwen2-7B [44] as the base models. For the instruction-tuned models, we utilized Llama-2-{7b, 13b}-chat-hf [38], Llama-3-8B-Instruct, Llama3.1-8B-Instruct [9], Mistral-Plus-7B [47], Qwen1.5-7B-Chat [1] and Qwen2-7B-Instruct [44]. We specifically selected the instruction-tuned models which are explicitly stated to have undergone RLHF process. Furthermore, we leveraged GPT-series via the OpenAI API4. Specifically, we used gpt-3.5-turbo-1106 for GPT-3.5 turbo, gpt-4-0125-preview for GPT-4, and gpt-40-mini-2024-07-18 for GPT-40 mini. We set hyperparameters as follows: temperature=0, top_p=1, max_token_len=200. We utilized an A100 GPU when loading Huggingface models."}, {"title": "A.3 Detail Information about Human Survey", "content": "Demographic Information on Human Survey We surveyed 58 English-fluent participants re-garding their preferences for LLM-generated responses. In this process, we did not collect any personally identifiable information and only gathered basic demographic data from the participants. The participants included 36 males and 22 females. Among them, 25 were experts with backgrounds in AI ethics, human-computer interaction, and social computing, and they possess knowledge of bias in language models. The remaining 33 participants were non-experts.\nSample Survey Questions We based the survey on the occupations listed in WinoBias [45]. A comprehensive list of these occupations is available in Appendix A.2. We categorized the 40 occupations into 10 groups based on the percentage of females in each role, using 10% intervals. From each interval group, we selected 2 occupations, resulting in 10 pairs by matching one occupation with a low female ratio (under 50%) with one having a high female ratio (50% or more)."}, {"title": "A.4 Ethics Statement", "content": "Our annotation experiment was approved by the Institutional Review Board (IRB)5. All participants in annotation tasks indicated their understanding of the procedure for the annotation and acknowledged their agreement to participate."}]}