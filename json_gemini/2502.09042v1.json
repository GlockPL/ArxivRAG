{"title": "TYPHOON T1: AN OPEN THAI REASONING MODEL", "authors": ["Pittawat Taveekitworachai", "Potsawee Manakul", "Kasima Tharnpipitchai", "Kunat Pipatanakul"], "abstract": "This paper introduces Typhoon T1, an open effort to develop an open Thai rea- soning model. A reasoning model is a relatively new type of generative model built on top of large language models (LLMs). A reasoning model generates a long chain of thought before arriving at a final answer, an approach found to improve performance on complex tasks. However, details on developing such a model are limited, especially for reasoning models that can generate traces in a low-resource language. Typhoon T1 presents an open effort that dives into the details of developing a reasoning model in a more cost-effective way by leverag- ing supervised fine-tuning using open datasets, instead of reinforcement learning. This paper shares the details about synthetic data generation and training, as well as our dataset and model weights. Additionally, we provide insights gained from developing a reasoning model that generalizes across domains and is capable of generating reasoning traces in a low-resource language, using Thai as an example. We hope this open effort provides a foundation for further research in this field.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in the field of large language models (LLMs) have gained more attention from the paradigm of scaling at test time (Snell et al., 2024; DeepSeek-AI, 2025), which improves the per- formance of an LLM by allocating compute resources during test time-i.e., generating more tokens- to provide higher compute. While there exist techniques like chain-of-thought (CoT) prompting (Wei et al., 2022), self-consistency (Wang et al., 2023b), and best-of-N evaluation (Snell et al., 2024), which do not change the weights of the LM and focus more on prompting and sampling strategies, another approach is the reasoning model. A reasoning model, also known as a thinking LLM (Wu et al., 2024), is a relatively new type of LLM that is able to generate a long reasoning trace before providing a conclusive answer, leading to improved performance across tasks.\nThe long reasoning trace of a reasoning model often consists of behaviors such as breaking down a task into sub-tasks, reflecting on its own intermediate results, and self-correcting them (DeepSeek- AI, 2025). Models that fall into this category include OpenAI's o-series\u00b2, Qwen's QwQ\u00b3, and DeepSeek R1 (DeepSeek-AI, 2025). Although some of these studies and/or models (Wu et al., 2024; Muennighoff et al., 2025; Guan et al., 2025) are open to a certain degree, most are limited to only open-weight availability and obscure details, making them difficult to replicate in an open environment and to further progress research effort.\nIn this paper, we provide a detailed report on our lessons learned from developing a Thai reasoning model. We aim to develop a model capable of reasoning across tasks and generating reasoning traces in Thai, a low-resource language. In addition to the insights provided in this paper, we also open our datasets, data pipeline, training configurations, and model weights to support future research.\nWe name our reasoning model Typhoon T1 to honor the open Thai LLM we selected as our initial model, Typhoon 2 3B Instruct (Pipatanakul et al., 2024), for developing the Thai reasoning model."}, {"title": "2 \u041c\u0415\u0422\u041dODOLOGY", "content": "An overview of our methodology for data generation and experiments is illustrated in Figure 1. In the remainder of this section, we explain the rationale for our base model selection and details of structured thinking, a new thinking format we introduce in this paper. Additionally, we provide details on the selected benchmarks used for evaluation across our experiments.\nTo develop a reasoning model, we first select an initial LLM. To enhance Thai performance, we consider various open-weight Thai LLMs, including OpenThaiGPT 1.5 (Yuenyong et al., 2024), Pathumma 1.0.04, and Typhoon 2 (Pipatanakul et al., 2024). Due to resource limitations, we choose Typhoon 2 3B Instruct as our baseline, as it is the only available open-weight 3B model. Typhoon 2 3B is based on Llama 3.2 3B Instruct (Grattafiori et al., 2024), with additional training to improve its Thai performance (Pipatanakul et al., 2024).\nWe select the instruct variant of the model, unlike DeepSeek R1 Zero (DeepSeek-AI, 2025), which is built on DeepSeek V3 Base, as we do not utilize RL. Our rationale is to start with a model that already follows instructions and enhance its ability to produce correct answers with long reasoning traces. Choosing an instruct model is more effective in this case than a base model, which would require a larger dataset to teach basic instruction-following skills.\nTo SFT the LLM into a reasoning model, we begin by preparing our training dataset. Instead of performing knowledge distillation from other reasoning models (Team, 2025; Labs, 2025), we opt to construct long reasoning traces ourselves. We develop a transformation-and-refinement pipeline using few-shot prompting (Brown et al., 2020) to transform and refine a normal response into a long- form response. This allows better control over thought quality across tasks and avoids dependence on the constraints of a teacher model."}, {"title": "2.1 STRUCTURED THINKING", "content": "Structured thinking is an approach mostly inspired by plan-and-solve prompting (Wang et al., 2023a) and the use of scratchpad for LLMs to show intermediate output in a specified region (Nye et al., 2021). We introduce the following XML tags to help guide LLMs on structuring their thoughts and responses as show in 2 (c). The <thoughts></thoughts> and <response></response> tags are the high-level tags and only exist once in each response. <plan></plan> only contains the <step></step> tags denoting each step, while the rest of the tags are utilized for each step.\nThe intuition behind designing the thinking process in this way is that, at each step, we encourage the model to first generate a title summarizing what should be done, followed by details or intermediate results recorded in the scratchpad. Then, we instruct the model to summarize its progress and determine the next step based on the plan. This design aligns with the plan-and-solve prompting"}, {"title": "2.2 DATA PREPARATION", "content": "To prepare a training set, we select open datasets across five domains: 1) mathematics, 2) instruction following, 3) coding, 4) safety, and 5) finance. The rationale behind this mixture is that mathemat- ics is known to improve STEM performance of LLMs (OLMo et al., 2025), instruction following ensures generalization of reasoning capabilities, and code serves as a strong anchor language for an LM to think through and is known to improve performance (Gao et al., 2023; Chen et al., 2023). Finally, safety is included to improve the safety of the model, which may degrade from the intro- duction of long thinking, and finance is included to provide a representative of domain-specific data, which we will utilize to experiment and observe its effect on overall performance."}, {"title": "2.2.1 DATA MIXTURE", "content": "The selected open datasets for each domain and their proportions are described in Table 5 in Ap- pendix A.3. While the majority of the datasets provide a straightforward ground truth answer, PRM800K is different.\nPRM800K is a dataset used to construct a process reward model, which is a different type of model from an LLM. Therefore, we create a simple script to convert choices of each step in each ground truth into one complete response. At each step, we randomly select the correct or incorrect step to be included. In the case of an incorrect step, the script adds an additional step reflecting that the step is incorrect and resamples another step. This way, we create a dataset containing behaviors of self-correction, one of the behaviors common in reasoning models (DeepSeek-AI, 2025), in our training set.\nWe also perform post-processing on UltraFeedback by selecting only high-quality responses, filter- ing for records with a rating of the chosen response higher than 4.25. Finally, we downsample some large datasets through uniform sampling from their respective training splits. In total, the training set consists of 55,677 records."}, {"title": "2.2.2 PIPELINE FOR DATA TRANSFORMATION AND REFINEMENT", "content": "Nevertheless, the training at this point only consists of a typical train-split fine-tuning set and does not include long reasoning chains like those demonstrated by reasoning models. Therefore, we pre- pare an LLM-based transformation-and-refinement pipeline for generating long reasoning chains from the training set. The pipeline consists of two main stages: 1) few-shot structured thinking transformation and 2) response refinement. In the first stage, we use few-shot prompting (Brown et al., 2020) with GPT-40 mini (gpt-40-mini-2024-07-18), along with ground truth associ- ated with each query. There are a total of three few-shot exemplars (available in Appendix A.5), which are manually curated by combining and structuring multiple state-of-the-art LLM responses.\nThe reason for transforming each ground truth into a structured thinking format first is to allow conversion into semi-structured and unstructured thinking formats later through simple text replace- ments (i.e., removing some or all auxiliary XML tags). This ensures equivalent levels of information, which is crucial for evaluating the effectiveness of different thinking formats.\nAfter transformation, we utilize Qwen2.5-32B-Instruct to refine the generated response, ensuring structural and informational correctness. We prompt the LLM to correct formatting issues, fill in missing content, and improve response quality based on generated thoughts. The same few-shot exemplars are used to guide this refinement process.\nWe derive semi-structured and unstructured thinking formats by systematically removing aux- iliary XML tags. Specifically, we replace tags with words to maintain contextual consis- tency and strip away all structural elements except for <thoughts></thoughts> and <response></response> tags to create semi-structured data. In contrast, unstructured think- ing data is obtained by removing or substituting all XML tags.\nIn total, we have approximately 67M tokens in our training set ready for SFT. The average number of tokens per instruction is 145.20, while the average number of tokens per output is 1,060.94, increased from the original 248.58 tokens. The maximum number of steps in the thinking part (excluding planning) is 24, with an average of 4.99 steps."}, {"title": "2.3 EVALUATION", "content": "We utilize the following six benchmarks from different domains to assess the model's performance: 1) GSM8K (Cobbe et al., 2021) is a math word problem dataset used to evaluate mathematical performance; 2) HumanEval+ (Liu et al., 2023; 2024), which evaluates code generation, as the code domain is present in our training data; 3)IFEval (Zhou et al., 2023a) is selected to evaluate instruction-following performance; 4) GPQA (Rein et al., 2024) and 5) MMLU Pro (Wang et al., 2024b) are included as additional challenging QA benchmarks; and 6) ThaiExam (Pipatanakul et al., 2023) is a multiple-choice benchmark consisting of standard exams used to evaluate Thai students. We used this benchmark to evaluate the model's performance in Thai, given that the original model was tuned for improved Thai language capabilities."}, {"title": "3 EXPERIMENTS", "content": "For all experiments in this section, we used the setup described in Appendix A.6. We conduct five experiments, each presented in its own subsection, to answer the following five research questions.\nQ1 Does the thinking format matter for reasoning models? If so, what is the best format?\nQ2 How much data is needed to train a reasoning model?\nQ3 In which domain does data preparation matter the most when developing a reasoning model?\nQ4 How can a reasoning model be adapted to generate its reasoning trace in Thai?\nQ5 What are the differences when generating reasoning traces in Thai versus in English for a multi- lingual reasoning model?"}, {"title": "3.1 STRUCTURED THINKING PROVIDES IMPROVEMENTS FOR MATHEMATICS AND CODING", "content": "To assess the impact of training a model on a long-thinking dataset and to evaluate the effects of various thinking formats, we designed six distinct scenarios. The results are presented in Table 2:\n\u2022 Typhoon 2 3B Instruct: These scenarios evaluate the base model.\n1. Zero-shot: We prompt the model in a zero-shot manner to establish its baseline performance. Note that modern LLMs may have been exposed to CoT data during training; consequently, the model might exhibit chain-of-thought behavior even in zero-shot settings.\n2. Zero-shot CoT: We prompt the LLM with the phrase \"Let's think step by step,\" as suggested by (Kojima et al., 2022), to observe its performance when explicitly encouraged to reason before answering, without any additional training.\n3. SFT: To compare standard supervised fine-tuning on a typical training set with training on a long-thinking dataset, we fine-tune the base model on a combined training split of non-long- thinking data and then evaluate its performance using zero-shot prompting.\n\u2022 4.-6. Typhoon T: This variant of Typhoon 2 3B Instruct is trained on a long-thinking dataset generated following the approach described in Section 2. We experiment with three model vari- ants-Unstructured, Semi-structured, and Structured\u2014each trained on its respective thinking- formatted dataset.\nWe observe an unexpected result when applying zero-shot CoT prompting with the base model: a slight performance decrease across most benchmarks, except for GPQA and ThaiExam, where marginal improvements are observed. This finding contradicts the common observations that zero- shot CoT prompting enhances model performance by eliciting reasoning. We hypothesize that this decline may be due to additional training steps on Thai data, which have affected LLM's reasoning performance (Khade et al., 2025). Furthermore, zero-shot CoT prompting led the model to generate completely incorrect code, scoring 0.00, likely due to distractions from unhelpful reasoning traces.\nSimilarly, we observe a significant drop in performance when applying SFT with the non-long- thinking datasets. This suggests symptoms of catastrophic forgetting (Luo et al., 2025), where the model shifts its distribution toward the fine-tuned data and loses generalization capabilities (Kotha et al., 2024). In contrast, SFT with long-thinking data does not exhibit the same behavior. Never- theless, models fine-tuned on the long-thinking dataset show degraded performance in instruction- following tasks (IFEval). Additionally, ThaiExam scores decrease, which is expected given that the fine-tuning dataset primarily consists of English data, thereby reducing the model's performance on Thai-language benchmarks.\nWe further analyzed the average response length on the MMLU Pro benchmark and found that Typhoon T models generally generate longer responses (more output tokens) than Typhoon 2 3B Instruct variants. This demonstrates the effectiveness of SFT on long-thinking datasets, except when prompting Typhoon 2 with zero-shot chain-of-thought reasoning, which generates long but unhelpful responses. Additional results and discussions are provided in Appendix A.7."}, {"title": "3.2 BALANCING DATA QUANTITY: THE KEY TO OPTIMAL REASONING MODEL PERFORMANCE", "content": "To evaluate the impact of data quantity on the performance of the reasoning model, we construct downsampled versions of the main training dataset. The baseline for this experiment is Typhoon T, trained on structured long-thinking data from the previous subsection. To create the downsampled datasets, we sample x% from each subdataset that comprises the full dataset, ensuring the overall data distribution is preserved. We consider a range of x values: {75, 50, 25, 10, 5}.\nWe note that we fixed the number of epochs rather than the total training steps, meaning that models trained on smaller datasets undergo fewer optimization steps. This ensures that each subset is trained for a proportional dura- tion relative to the full dataset without artifi- cially extending training. If we had controlled for total training steps instead (e.g., by increas- ing epochs for smaller datasets), we might ob- serve different trends, potentially increasing the risk of overfitting. The results of this experi- ment are presented in Figure 3. The full results in a table format is available in Appendix A.8.\nThe findings suggest that an excessive amount of data may not be optimal for achieving the best performance. However, reducing the dataset size too much also leads to a decline in instruction-following performance (IFEval). Similarly, MMLU Pro exhibits a downward trend in performance beyond a certain data threshold, 50% in this experiment.\nIn contrast, HumanEval+ benefits from an increase in training data. Among the tested configura- tions, training on 75% of the total dataset (41,755 records) yielded the most effective structured thinking reasoning model. We designate the model trained on this dataset as Typhoon T1-\u0395\u039d."}, {"title": "3.3 DON'T LEAVE SAFETY OUT WHEN TRAINING A REASONING MODEL", "content": "To assess the impact of each domain in the training set, we perform a leave-one-out experiment, where we train a model following the approach in Section 3.1 on a training dataset with one specific domain removed. We use the best-performing configuration from the previous subsection (75% dataset), which also serves as our baseline. We expect to observe a decrease in performance and measure the impact of each domain by the extent of this decrease. The full results and additional discussions are available in Appendix A.9.\nAmong all domains, the safety domain has the most impact on performance. Removing it results in substantial performance drops in GSM8K, HumanEval+, IFEval, and ThaiExam. This effect can be partly attributed to the role of the safety domain in aligning the model to be helpful (Wang et al., 2023c). Without this alignment, the model's overall capabilities decline across multiple bench- marks. Interestingly, the model performs the same or better on challenging English multiple-choice benchmarks, such as GPQA and MMLU Pro."}, {"title": "3.4 TRAINING WITH THAI-TRANSLATED DATA IMPROVES THAI PERFORMANCE AT THE COST OF OTHERS", "content": "To equip the model with the ability to generate Thai reasoning traces-especially when the user's prompt is in Thai or explicitly requests Thai output-we train a model with additional translated Thai content. To obtain this translated content, we construct a translation pipeline described in Appendix A.10.\nIn this subsection, we investigate the best approach for equipping the reasoning model with the ability to generate Thai reasoning traces. We experiment with two main approaches: (1) continual SFT, where we take Typhoon T1-EN and perform an additional SFT with the translated Thai dataset, and (2) SFT from scratch, where we train from the beginning using a mixture of the translated Thai dataset and the Typhoon T1-EN training data. We found that the continual SFT approach yielded subpar performance compared to SFT from scratch, as shown in Table 3.\nTo better understand the effect of incorporating Thai-translated content, we experimented with dif- ferent proportions of the translated dataset: using 2/3 and 1/3 of the full translated dataset, approxi- mately 1,000 and 500 records, respectively. The results are presented in Table 3.\nWe observe that all models trained with additional Thai-translated data exhibit improved instruction- following capabilities (IFEval), with the most improvement occurring when training on the full Thai- translated dataset. Similarly, performance on ThaiExam follows this trend, showing improvements as more Thai training data is included. However, we also observe a decline in performance on other benchmarks, likely indicating a trade-off due to the inherent capacity limitations of the model's 3B parameters (Li et al., 2024; Allen-Zhu & Li, 2025).\nWe observe that the models trained with Thai translated data are able to generate its thinking trace in Thai when prompted in Thai (see Appendix A.11), while maintaining performance similar to that of Typhoon T1-EN. The model that achieved the highest improvement on ThaiExam-trained with the full Thai-translated dataset-is named Typhoon T1.\nIn addition, we observe that on ThaiExam, the percentage of Thai characters in the thinking trace (i.e., the content between <thoughts> tags) increased from 16.67% when generated with Typhoon T1-EN to 95.49%. This increase in the number of Thai characters, along with improved performance on ThaiExam, demonstrates the effectiveness of training with only a small portion of Thai-translated data. We hope this approach sparks interest in further studies on equipping reasoning models with the ability to generate their reasoning traces in low-resource languages."}, {"title": "3.5 LET THE REASONING MODEL CHOOSES ITS OWN REASONING LANGUAGE", "content": "To better understand the effect of thinking language on the performance, we conduct an experiment using Typhoon T1 from the previous section. Specifically, we examine the performance implica- tions of enforcing thought generation in either English or Thai through prompting. This differs from a zero-shot approach, used in the previous subsection, where we allow the model to choose its own reasoning language-typically English for English prompts and Thai for Thai prompts."}, {"title": "3.6 SUMMARY: TYPHOON T1-EN AND TYPHOON T1", "content": "We compare the final performance of each model against Typhoon T1 3B Instruct in Fig- ure 4. Based on all experiments in this sec- tion, we found the most optimal configuration for training 3B-parameter reasoning models us- ing the SFT approach as follows:\n\u2022 Typhoon T1-EN: Uses structured thinking with 41,755 records, trained using the stan- dard settings described in Appendix A.6.\n\u2022 Typhoon T1: Extends Typhoon T1-EN's ap- proach by incorporating an additional train- ing set of 1,565 Thai-translated structured long-thinking records, as described in Sec- tion 3.4. This enables the model to generate Thai reasoning traces while maintaining Thai performance."}, {"title": "4 CONCLUSION", "content": "We present an open recipe for developing the reasoning model Typhoon T1, which requires no dis- tillation from other models, generalizes across domains, and produces reasoning traces in English or Thai. We also introduce structured thinking-using auxiliary XML tags to create efficient rea- soning formats with fewer tokens. Although Typhoon T1 shows strong improvements on GSM8K, HumanEval+, and GPQA, our approach involves trade-offs in performance due to model size, such as reduced instruction following and Thai performance. We hope our insights on data size, domain, and reasoning language and artifacts accelerate research on reasoning models."}, {"title": "A APPENDIX", "content": "A.1 LIMITATIONS AND FUTURE WORK\nDue to computational constraints, we experiment with a relatively small model, consisting of 3B parameters. This introduces inherent limitations in the model's reasoning capabilities and trade-offs in its performance (Li et al., 2024; Allen-Zhu & Li, 2025). However, our approach demonstrates promising improvements in reasoning ability through structured long-thinking approach. Addition- ally, we restrict our experiments to zero-shot prompting and do not incorporate test-time scaling techniques, such as those introduced in Snell et al. (2024), which could further enhance perfor- mance.\nSeveral important properties remain unexplored in our study of small reasoning models, including the impact of the number of reasoning steps, variations in structured thinking pattern, and a deeper analysis on finer-grained reasoning behaviors such as self-correction and problem decomposition. Future work should investigate these aspects to better understand their contributions to model rea- soning capabilities."}, {"title": "A.2 RELATED WORK", "content": "A.2.1 CHAIN-OF-THOUGHT PROMPTING\nChain-of-Thought (CoT) prompting (Wei et al., 2022) is an approach used to elicit reasoning in LLMs through in-context reasoning exemplars (Brown et al., 2020). This approach improves the performance of LLMs by generating additional tokens\u2013\u201cthoughts\"-preceding a final answer. Sim- ilarly, zero-shot CoT prompting (Kojima et al., 2022) elicits the generation of a reasoning chain"}, {"title": "\u0391.2.2 TEST-TIME COMPUTE SCALING", "content": "Test-time compute scaling (Snell et al., 2024) refers to allocating a greater compute budget during inference, allowing LLMs to engage in more extensive reasoning before generating a final answer. Several methods exist to achieve this, most of which involve search-based techniques, such as Monte Carlo Tree Search (MCTS) (Ding et al., 2024; Duan & Wang, 2025), tree traversal (Yao et al., 2023; Besta et al., 2024; Bi et al., 2025), and other search strategies (Snell et al., 2024; Wang et al., 2024a). These methods are often paired with a reward model, either an outcome reward model (Beeching et al., 2024) or a process reward model (Snell et al., 2024). Some studies (Qin et al., 2024; Guan et al., 2025) suggest that reasoning models can be enhanced by fine-tuning LLMs on search traces. However, we find this approach computationally expensive, especially at scale. Our approach, in contrast, is simpler to implement and more cost-effective than the aforementioned methods."}, {"title": "A.2.3 REASONING MODELS", "content": "Reasoning models, also referred to as \"thinking LLMs\u201d (Wu et al., 2024), represent a recent ad- vancement in the field of large language models. These models have demonstrated effectiveness in solving complex benchmarks by generating extended reasoning chains, often scaling with prob- lem difficulty (DeepSeek-AI, 2025). Reasoning models are typically developed by augmenting an LLM through additional supervised fine-tuning (SFT) (Muennighoff et al., 2025) or reinforcement learning (RL) (DeepSeek-AI, 2025).\nNevertheless, many works (Team, 2025; Labs, 2025; Huang et al., 2024) have also demonstrated the effectiveness of knowledge distillation from a reasoning model. However, this distillation-based approach provides only a shortcut and requires access to an existing reasoning model. To gain a deeper understanding of reasoning models, we argue that developing such models from scratch- without leveraging an existing reasoning model-is a more effective approach. This study adopts that perspective to gain better insights into reasoning models."}, {"title": "A.3 DATA MIXTURE", "content": "Table 5 shows data mixture of the training set used in the experiments in Section 3. Figure 5 shows data distribution of each domain in percentage."}, {"title": "\u0391.4 AN EXAMPLE OF STRUCTURED THINKING", "content": "Given the following prompt: \"A model of a park was built on a scale of 1.5 centimeters to 50 meters. If the distance between two trees in the park is 150 meters, what is this distance on the model?\", we can expect to see the following response (indentation is manually included for readability):\n<thoughts>\n<plan>\n<step order=\"1\">Understanding the scale relationship</step>\n<step order=\"2\">Setting up the proportion</step>\n<step order=\"3\">Cross-multiplying to solve for the model distance</step>\n<step order=\"4\">Calculating the final result</step>\n<step order=\"5\">Verifying the calculations</step>\n</plan>\n<step>\n<title>Understanding the scale relationship</title>"}, {"title": "A.5 FEW-SHOT EXEMPLARS FOR DATA TRANSFORMATION AND REFINEMENT", "content": "\u2022 https://pastes.io/example-1\n\u2022 https://pastes.io/example-2\n\u2022 https://pastes.io/example-3"}, {"title": "A.6 EXPERIMENTAL SETUP", "content": "We employ the following setup and tools for all our experiments. We use vLLM (Kwon et al., 2023) for efficient inference in our data pipeline and LlamaFactory (Zheng et al., 2024) for efficient multi-GPU training. Models are trained mostly on 2xH100s.\nWe perform a standard SFT on the training set using the same training configurations for all experi- ments. We save and evaluate models every 200 steps, with 5% of the training set used as a validation set, and load the best model at the end. We train each model for two epochs with a learning rate of 2e-5 using a cosine scheduler without warmup steps. The batch size is set to 6 with 6 gradient accumulation steps. For efficient training, we leverage DeepSpeed ZERO Stage 2 (Rajbhandari"}, {"title": "A.6.1 TRAINING CONFIGURATION FOR LLAMAFACTORY", "content": "### model\nmodel_name_or_path: scb10x/llama3.2-typhoon2-3b-instruct\n### method\nstage: sft\ndo_train: true\nfinetuning_type: full\ndeepspeed: examples/deepspeed/ds_z2_config.json\n### dataset\ndataset: <dataset_name>\ntemplate: llama3\noverwrite_cache: true\npreprocessing_num_workers: 16\n### output\noutput_dir: outputs/<outupt_model_path>\nlogging_steps: 10\nsave_steps: 200\nplot_loss: true\noverwrite_output_dir: true\n### train\nper_device_train_batch_size: 6\ngradient_accumulation_steps: 6\nlearning_rate: 2.0e-5\nnum_train_epochs: 2.0\nlr_scheduler_type: cosine\nwarmup_ratio: 0.0\nbf16: true\nddp_timeout: 180000000\nuse_unsloth: false\nenable_liger_kernel: true\nload_best_model_at_end: true\nresume_from_checkpoint: false\nflash_attn: fa2\n### eval\nval_size: 0.05\nper_device_eval_batch_size: 1\neval_strategy: steps\neval_steps: 200"}, {"title": "A.6.2 EVALUATION CONFIGURATION FOR OLMES", "content": "olmes --model <model_name> --task gsm8k::olmes codex_humanevalplus::tulu ifeval::tulu gpqa::llama3 mmlu_pro:cot:: none --model-type vllm output-dir results/<model_name>"}, {"title": "A.7 AVERAGE OUTPUT TOKENS", "content": "This section presents the average number of output tokens generated by each model on the listed benchmarks. The results, obtained from the experiment described in Section 3.1.\nThe results in Table 6 reveal notable variations in the average output tokens across different model configurations and benchmarks. In particular, when prompted with zero-shot CoT phrases, the Ty- phoon 2 3B Instruct produces a substantially higher number of tokens especially on GPQA and MMLU Pro-suggesting that the model is distracted by unhelpful chain-of-thought reasoning and achieves poor results.\nIn contrast, the supervised fine-tuned Typhoon 2 3B Instruct on non-long-thinking datasets generates more concise outputs on GSM8K and MMLU Pro, reflecting the style of the training set, while exhibiting a significant increase in output length on ThaiExam. The observation on ThaiExam is likely due to the training set not containing Thai records, making it out-of-training-distribution."}, {"title": "A.8 DATA SIZE ABLATION STUDY RESULTS", "content": "Table 7 shows a full result of the experiment in Section 3.2."}, {"title": "A.9 LEAVE-OF-OUT EXPERIMENT RESULTS", "content": "Table 8 shows a full result of the experiment in Section 3.3.\nRemoving instruction-following results in a slight performance decrease across all benchmarks, ex- cept for ThaiExam. This outcome is expected, as instruction-following is a foundational skill for an LLM to perform well across tasks, since most tasks require a model to understand instructions to effectively utilize its domain knowledge. We note that the slight increase in ThaiExam perfor- mance may be due to the model being exposed to more English instruction-following data, thereby mitigating the impact of Typhoon fine-tuning on Thai data (Pipatanakul et al., 2024).\nExcluding the mathematics domain from the training set leads to a decline in GSM8K scores, a mathematics-focused benchmark. However, we observe a notable increase in the mathematics sub- ject of MMLU Pro, rising from 23.69% to 29.90%. A key difference between MMLU Pro and the other benchmarks is that MMLU Pro consists of multiple-choice questions."}, {"title": "A.10 TRANSLATION PIPELINE", "content": "The translation pipeline consists of an LLM, a fine-tuned Llama 3.1 8B Instruct in our case, prompted to act as a translator while adhering to specific rules, such as preserving structural XML tags. The exact prompt used for this purpose is provided bekiw. We translated approximately 25,000 records from the original 100% training set of the Typhoon T structured long-thinking model ref- erenced in Section 3.1. We applied strict post-processing criteria, including removing prefixes that were not part of the translated content and filtering out translated instructions containing XML tags. After this process, we retained 1,565 high-quality translated records.\nTranslate the following content into Thai appropriately.\n**Rules**:\nDo not translate the tags used for structural formatting. Keep all XML tags intact and include in the final response.\nTranslate only the content in English. Do not translate any other languages.\nConsider the context and adjust accordingly. You may change the order or add additional content to ensure fluency and make the text sound natural.\nGenerate only the translated content, without explanations or formatting tags.\nTranslate all content, not just the response tags.\nDo not change to a new structure. Keep all XML tags: <thoughts>, <plan >, <title>, <summary>, <scratch_pad>, <response>, etc.\nEnsure you strictly follow the rules.\n###======###"}, {"title": "A.11 AN EXAMPLE OF THAI THINKING TRACE"}]}