{"title": "CROPS: A Deployable Crop Management System Over All Possible State Availabilities", "authors": ["Jing Wu", "Zhixin Lai", "Shengjie Liu", "Suiyao Chen", "Ran Tao", "Pan Zhao", "Chuyuan Tao", "Yikun Cheng", "Naira Hovakimyan"], "abstract": "Exploring the optimal management strategy for nitrogen and irrigation has a significant impact on crop yield, economic profit, and the environment. To tackle this optimization challenge, this paper introduces a deployable Crop Management system Over all Possible State availabilities (CROPS). CROPS employs a language model (LM) as a reinforcement learning (RL) agent to explore optimal management strategies within the Decision Support System for Agrotechnology Transfer (DSSAT) crop simulations. A distinguishing feature of this system is that the states used for decision-making are partially observed through random masking. Consequently, the RL agent is tasked with two primary objectives: optimizing management policies and inferring masked states. This approach significantly enhances the RL agent's robustness and adaptability across various real-world agricultural scenarios. Extensive experiments on maize crops in Florida, USA, and Zaragoza, Spain, validate the effectiveness of CROPS. Not only did CROPS achieve State-of-the-Art (SOTA) results across various evaluation metrics such as production, profit, and sustainability, but the trained management policies are also immediately deployable in over of ten millions of real-world contexts. Furthermore, the pre-trained policies possess a noise resilience property, which enables them to minimize potential sensor biases, ensuring robustness and generalizability. Finally, unlike previous methods, the strength of CROPS lies in its unified and elegant structure, which eliminates the need for pre-defined states or multi-stage training. These advancements highlight the potential of CROPS in revolutionizing agricultural practices.", "sections": [{"title": "Introduction", "content": "Food security is a crucial goal in contemporary agriculture, highlighting the significance of key management practices such as nitrogen fertilization and water irrigation. These techniques are essential not only for increasing crop yields and ensuring a stable food supply but also play a vital role in sustaining environmental health. Traditional best practices in these domains, informed by empirical experience and scholarly research, are now being tested against the backdrop of changing climatic and market conditions. This raises concerns about their continued effectiveness, underscoring the need for more innovative, efficient, and adaptable management systems. Such systems are essential for developing strategies that are responsive to varying conditions and aimed at specific goals, including economic profitability. This study seeks to contribute to this need by applying advanced AI methods to enhance agricultural practices, addressing these significant challenges in the pursuit of more sustainable and productive farming methodologies.\nRecent advancements in agricultural technology have introduced multi-layer perception (MLP)-based reinforcement learning (RL) agents (MLP-based Agents) and language-model-based reinforcement learning agents (LM-based Agents) for training nitrogen (N) and irrigation management policies using the Gym-DSSAT simulator (Romain et al. 2022; Wu et al. 2022, 2024b; Wu 2024). Their research demonstrated the ability of these policies to surpass a baseline by producing higher yields or achieving similar yields with less N input under full observation conditions. However, the practical implementation of these policies in real-world scenarios is hindered by their reliance on comprehensive observational data, such as nitrate leaching and plant N uptake, which are typically not readily available to farmers. Addressing this gap, a recent study presents an intelligent crop management framework that adeptly combines reinforcement learning (RL), imitation learning (IL), and crop simulations using DSSAT and Gym-DSSAT (Tao et al. 2022). In this paper, we refer to their trained agent as the imitation learning-based agent (IM-based Agent). This approach enhances the adaptability and applicability of the management policies to real-world agricultural settings by effectively addressing the challenge of partial observation.\nWhile IL has proven effective in refining existing agricultural strategies by better aligning them with the practical realities of farming, it's crucial to recognize the variability in"}, {"title": "Related Work", "content": "RL is increasingly being applied in the domain of crop management, aiming to enhance decision-making through simulation-based strategies. Early experiments, such as wheat management in France (Garcia 1999) and irrigation optimization for maize in Texas (Sun et al. 2017), encountered limitations due to restrictive state and action spaces. To overcome these limitations, subsequent studies have broadened the scope of RL applications, incorporating more complex scenarios and a variety of crop models (Wu et al. 2022; Kallenberg et al. 2023; Madondo et al. 2023). The integration of RL with sophisticated crop simulation models like APSIM and DSSAT has been facilitated by innovative platforms such as CropGym and Gym-DSSAT. These platforms enable dynamic, real-time decision-making within simulations (Brockman et al. 2016; Overweg, Berghuijs, and Athanasiadis 2021; Romain et al. 2022), offering detailed daily interactions between the RL agent and the simulated environment, which is a crucial advancement for optimizing key management practices such as nitrogen and irrigation management (Wu et al. 2022). Despite these advancements, challenges remain, particularly in deploying trained\nCrop Management with RL and Crop Models"}, {"title": "Language Models as RL Agent", "content": "Recently, foundation models have been applied across various domains, such as language (Liu, Jiang, and Pister 2024; Lai, Zhang, and Chen 2024; Liu et al. 2024b), speech (Chen et al. 2024b; Sun et al. 2024a; Yu et al. 2024b), computer vision (Chen et al. 2024c; Li et al. 2024; Lai et al. 2024; Wu, Hovakimyan, and Hobbs 2023; Wu et al. 2023; Wang et al. 2024), medical (Liu, He, and Huang 2024; Chen et al. 2024a, 2020, 2018, 2019), geography (Yu et al. 2024a), robotics (Gao et al. 2023, 2024; Gao, Danielson, and Fierro 2024; Zhang et al. 2020)and health (Xu et al. 2024). In particular, foundation models have increasingly been applied to RL tasks that require language processing, such as natural language instructions (Garg et al. 2022). In a complementary approach, RL trajectories are encoded into token sequences, which foundation models then process to serve as inputs for deep RL architectures (Li et al. 2022). Building on these developments, some recent methodologies conceptualize RL as a sequence modeling problem, employing foundation models directly to predict future states or actions (Janner, Li, and Levine 2021; Lee et al. 2022; Huang et al. 2022a; Wu et al. 2024b). These innovative strategies have demonstrated remarkable success, significantly enhancing control schemes across a variety of robots and agents (Huang et al. 2022a,b; Raman et al. 2022; Mees, Borja-Diaz, and Burgard 2023; Liu et al. 2024b; Chen et al. 2023a; Ahn et al. 2022; Liang et al. 2023). Beyond these stages, previous studies have also tested the viability of language models as autonomous RL agents in agricultural management (Wu et al. 2024b).\nAlthough these trials were successful, they stopped short of actual field deployment. Addressing this gap, this research pioneers the deployment of LMs for formulating optimal management strategies in real-world agricultural settings, where access to complete state information is often challenging."}, {"title": "Masking Strategy for Improved Robustness", "content": "Masked language modeling and its autoregressive counterparts, such as BERT (Devlin et al. 2018) and GPT (Radford et al. 2018, 2019; Brown et al. 2020), have proven to be highly effective methods for pre-training in natural language processing (NLP) (Zhang, Li, and Okumura 2024). Building on this success, the concept of a masking strategy has been extended to the domain of computer vision and cross-modal, yielding significant benefits (Gong et al. 2022; Chen et al. 2023c; Sun et al. 2024b; Fu et al. 2024; He et al. 2022; Li et al. 2023b). Similarly, this approach has been applied to graph neural networks (Liu et al. 2024a; Li et al. 2023a; Schlichtkrull, De Cao, and Titov 2020). Beyond the modalities of vision and language, the masking technique has also been successfully implemented in the representation learning of tabular datasets and other domains(Xin et al. 2024; Yoon et al. 2020; Wu et al. 2024a; Chen et al. 2023b; Wu, Hobbs, and Hovakimyan 2023; Shi et al. 2017). To the best of our knowledge, this study represents the first effort to leverage the masking strategy to enhance the robustness of RL agents and enable the deployment of adaptable policies under varying state availability."}, {"title": "Method", "content": "In this section, we formally introduce CROPS. The first sub-section outlines how the crop management process can be formulated as a Markov Decision Process (MDP). The second sub-section details the masking strategy employed within the crop management setting during both training and inference phases. Finally, the last sub-section presents the unified framework that integrates a masking strategy with language models."}, {"title": "Problem Formulation", "content": "Following established paradigms (Tao et al. 2022; Wu et al. 2022, 2024b), we formulate nitrogen fertilization and irrigation management as a finite MDP. Specifically, we use t to denote a day. For each day, $s_t$ represents the state on that day. The state $s_t$ includes key data pertaining to weather, plant growth, and soil conditions, including root depth and cumulative nitrate levels, as observed in the simulation for that day. Given the environmental state $s_t$, RL agents are trained to select an action $a_t$ from the action space A. This selection is guided by a policy $\\pi(s_t, \\theta_t)$, where $\\theta_t$ represents the policy parameters on that particular day. Notably, a pre-trained language model is employed to represent the policy. For the action $a_t$, it comprises two key decisions: the quantity of nitrogen fertilizer, denoted as $N_t$, and the amount of irrigation water, $W_t$, to be applied. The effectiveness of these decisions is quantified by the reward $r_t(s_t, a_t)$, calculated based on the outcomes of $s_t$ and $a_t$. The reward function is defined as follows:\nif harvest occurs at t :\n$r_t(s_t, a_t) = w_1Y \u2013 w_2N_t - w_3W_t - w_4N_{l,t}$,\notherwise:\n$r_t(s_t, a_t) = -w_2N_t - w_3W_t - w_4N_{l,t}$, (1)\nwhere $w_1$, $w_2$, $w_3$, $w_4$ represent four custom weight factors, Y denotes the yield at harvest, and $N_{l,t}$ indicates the amount of nitrate leaching on a given day, respectively.\nBoth Y and $N_{l,t}$ are derived from the state variable $s_t$. The reward function design, characterized by the weights $w_1$, $w_2$, $w_3$, $w_4$, plays a crucial role in guiding the agent's strategy. The agent's objective is to determine the optimal policy $\\pi(s_t, \\theta_t)$, which selects action $a_t$ to maximize the total future discounted return. This return, defined as $R_t = \\sum_{t'=t}^T \\gamma^{t'-t}r_{t'}$, represents the accumulated rewards from the current action $a_t$ to future rewards, each discounted by the factor $\\gamma$."}, {"title": "Mimicking Real-World Observations through State Masking", "content": "We innovatively utilize a masking strategy to mimic the states that can be accessed in reality, preparing the trained RL agent for deployment and ensuring stable performance. For the training stage, we depict the masking process in Figure 1. For a batch of states, we define their original and fully observed condition as $S_F$. For each state, we sample a subset of its features and mask out the selected ones using masks denoted by m. More specifically, m consists of a series of zeros and ones, where the ones correspond to the state features we retain, and the zeros correspond to the features selected for masking following a uniform distribution. The masked ratio \u03b1 is defined as the ratio of the number of masked states to the total number of states. Then, the masked and partially observed state is defined as $s_P = S_F \\odot M$, where $\\odot$ denotes the element-wise operation between the fully observed state $S_F$ and the mask m. The operation strategy is straightforward: When the element in the mask m is 0, we replace the corresponding element in $s_P$ with \u201c#\u201d.\nIn the inference stage of real-world applications, fully observed states $S_F$ are no longer available. Instead, all state features are partially observed due to real-world constraints such as the availability of sensors, weather conditions, or financial limitations. Consequently, during deployment, all states will naturally be partially observed. Therefore, the RL agent will directly utilize these partially observed states $s_P$ directly for decision-making.\nIn the fields of computer vision and NLP, masking strategies typically require high masking ratios due to the redundancy and structured nature of images and texts. In contrast, our approach adopts a lower masking ratio, reflecting the less redundant and less structured nature of the states we analyze. This allows a modest masking ratio, such as 30%, to effectively create a challenging pre-task, prompting the RL agent to infer missing features and learn latent dependencies. One the other hand, using a higher masking ratio in this context could disrupt training stability. To mitigate potential center bias and enhance adaptability, we sample a uniformly within a specified range. This approach allows the trained RL) agent to perform effectively across diverse real-world applications. By reducing its reliance on specific state features, the agent can make informed decisions under varying conditions of state availability, thereby fostering more robust and noise-resistant decision-making."}, {"title": "Policy Training with RL and Masking Strategy", "content": "In this study, we employ the Deep Q-Network (DQN) framework from (Mnih et al. 2015) to train our agent. The objective is to learn an optimal policy that maximizes the future discounted return, denoted by $R_t$. Within this framework, we utilize a LM to predict the action-value function, i.e., the Q-function. More specifically, we define this Q-function as $Q(s, a) = E[R_t|s_t = s, a_t = \\alpha, \\pi]$. We use this Q-function to estimate the expected future return from the current state s and action a.\nThe fundamental difference between the proposed framework and the current framework is that the LM serves as a bi-task RL agent. Specifically, LM not only estimates the Q-values but is also designed to recover the masked or missing states. On one hand, the optimization goal is to refine the parameters of the Q-network to explore the optimal Q function, $Q^*(s, a)$, which represents the highest possible return given the current state s and action a. For decision-making, we employ a greedy policy defined as $max_{a \\in A}Q^*(s_t, a)$. On the other hand, the input state for the Q function is partially observed due to our designed masking strategy. Therefore, the Q function accepts the input state $s_P$, i.e., $Q^*(s_P, a) = E[R_t | S_t = s, a_t = a, \\pi]$. The language model also plays the role of a transition function, $T(s_P, a) = \\hat{s}_F$, to recover the partially observed states to the approximated fully observed ones. In summary, we define the overall framework, which effectively explores the optimal policy and recovers masked states using the following loss function $L_i(\\theta_i)$ as follows:\n$L_i(\\theta_i) = L_{i,1}(\\theta_i) + \\lambda L_{i,2}(S_F, \\hat{S}_F), (2)$\nwhere\n$L_{i,1}(\\theta_i) = E_{(s_F, a, r, s')}[r + \\gamma \\max_{a' \\in A} Q(s', a'; \\theta_i\u00af) - Q(s_F, a; \\theta_i)]$ (3)\nand\n$L_{i,2}(S_F, \\hat{S}_F) = MSE(S_F, \\hat{S}_F). (4)$\nHere, $s_P$, $S_F$, $\\hat{S}_F$, a, r, and s' denote the partially observed state, fully observed state, recovered fully observed state, action, reward, and next state, respectively. \u03bb is designed to balance the the two optimization objectives. Additionally, $\\gamma$ represents the discount factor, $ \\theta_i\u00af $ denotes the parameters of a previously defined target network, and MSE stands for mean squared error. The tuples $(s_F, a, r, s')$ for the loss function are randomly sampled from the replay buffer, a collection of prior state-action-reward-next state tuples accumulated during training."}, {"title": "Experiments and Results", "content": "In this section, we introduce the initial experimental setup for the subsequent experiments, including the datasets and settings. Following this, we provide the details of the training and evaluation processes. We then present the evaluation results, where the performance of the proposed method is compared against SoTA approaches in both fully observed and partially observed settings. Additionally, we include critical ablation studies to further analyze our method's effectiveness."}, {"title": "Experimental Setup", "content": "The studies examining training policies for nitrogen and irrigation management in maize crops encompassed two separate case studies, both employing real-world data. The initial"}, {"title": "Evaluation Metrics and Implementation Details", "content": "The framework was implemented to train the RL agent under conditions of both partial and full observation. In these settings, the method involved testing with four different reward functions, each designed to showcase the adaptability of the framework to various agricultural trade-offs. These include balancing crop yield, N fertilizer usage, irrigation water consumption, and environmental impacts. This diversity in reward functions enables the framework to be evaluated across a spectrum of scenarios and objectives, demonstrating its versatility in addressing different agricultural management challenges.\nIn each case study, we applied reward functions consistent with those described in prior research (Tao et al. 2022; Wu et al. 2024b). Specifically, four unique reward functions for rt, derived from Equation (1), were employed to train the RL agent. A single trained policy was selected for evaluation for each reward function, with detailed parameters for each listed in Table 2.\nRF1 measures the economic profit ($/ha) accrued by farmers, calculated based on the prevailing market prices of maize and the costs associated with N fertilizer and irrigation water, as referenced from (Mandrini et al. 2022) and (Wright et al. 2022). RF2-RF4 explore variations of economic profit under different hypothetical scenarios: RF2 assumes irrigation water is free; RF3 assumes N fertilizer is free; and RF4 models a scenario where the price of N fertilizer is doubled.\nEvaluation Metrics. The RL agent in the study employs a combination of DistilBERT and a three-layer fully connected neural network for feature adaptation. The process begins with DistilBERT encoding the state inputs into 768-dimensional embeddings. Notably, the parameters of DistilBERT are trained end-to-end in this model. After this initial encoding, the embeddings are passed through fully connected layers, one with 512 units and the other with 256 units. The final layer in this sequence is responsible for mapping these processed embeddings to the action space, completing the flow from the input state to the actionable output in the RL framework. The discrete action space is defined as follows:\n$A = {40 \\frac{kg}{ha} N fertilizer \\& 6 \\frac{L}{m^2} Irrigation water}, (5)$\nwhere k = 0, 1, 2, 3, 4, resulting in a total of 25 possible actions. This action space design incorporates standard quantities of N fertilizer and irrigation water that are typically applied by farmers in a single day. It also allows for a wide range of options, aiding the discovery of effective policies. The discount factor is meticulously set at 0.99. To facilitate the neural network's updates, Pytorch is employed alongside the Adam optimizer (Kingma and Ba 2014), characterized by an initial learning rate of le-5 and a batch size of 512. This setup is strategically chosen to optimize the learning process while ensuring efficient computation.\nApplying DistilBERT's tokenizer to numerical values causes significant training instability due to multiple token splits, resulting in large variances for small numerical differences. For instance, 360 tokenizes into [9475], while 361 splits into [4029, 2487], leading to disproportionate representations and instability. Tokenizing decimals worsens this issue, as 0.1 translates into [1014, 1012, 1015], causing unnecessary token proliferation and inefficiency. To address this, we developed a preprocessing technique that normalizes numerical values to the range [0, 300] and uses only the integer part for tokenization. This ensures each number corresponds to a single token, simplifying and stabilizing the process. By focusing on integers, we reduce the token set to 27 distinct tokens, including 25 feature-specific tokens and two special tokens ([CLS] and [SEP]). This approach improves training stability and computational efficiency, essen-"}, {"title": "Policy Training with Full Observation and Random Masking.", "content": "DQN was implemented for training with all states available. However, we intentionally masked some of the states to enable the RL agent to better mimic real-world observations. We tested four different reward functions to demonstrate the adaptability of our framework to various trade-offs among crop yield, nitrogen fertilizer use, irrigation water use, and environmental impact.\nThe evaluation results of the trained policies are presented in Table 3 and Table 4. While the LM-based agent with random masking is not primarily designed to pursue SoTA results but rather to explore a more robust and deployable RL agent, it still outperforms previous SoTA methods and empirical baselines across most evaluation metrics (i.e., different RFs) and various geographic locations, as a by-product. These consistent improvements across various reward functions that prioritize different optimization objectives underscore the agent's adaptability in optimizing for diverse agricultural goals.\nNotably, unlike previous work (Wu et al. 2024b), which transforms states into descriptive language to enrich their semantic meaning, we found that direct tokenization of state variables can achieve similar results when using language models as agents. This indicates that language models can understand the underlying relationships between tokens and rewards without requiring redundant descriptions. Consequently, this approach is not only more straightforward to implement but also simplifies the preprocessing of states."}, {"title": "Policy Evaluation under Partial Observation", "content": "In the previous section, we included masked training with all states available from DSSAT. However, many of these states are not readily measurable or accessible to farmers due to limitations in available instruments in reality. Although there have been attempts to leverage imitation learning to guide partially observed agents in accomplishing crop management tasks, these approaches rely on predefined partially observed states (Tao et al. 2022).\nTo address this issue, we pre-trained an LM-based RL agent with random masking. After the training, we evaluated the trained RL agent under partial observation settings. In this stage, we randomly masked a specific percentage of the states, defined as \u03b1. For each \u03b1, we kept its value unchanged but randomly selected masked states. We averaged the results of such experiments over 100 trials and reported the results in Figure 2. Notably, \u03b1 varies from 0% to 100% during inference and evaluation. As the available states gradually decrease, we observed a corresponding decrease in RF1. However, the decreasing curve is significantly more moderate than the one without masking, i.e., LM-based RL Agent,\nas shown in Figure 2.\nMore importantly, we compared the performance of CROPS RL agent with IM-based RL agent (Tao et al. 2022) shown in blue stars in Figure 2. PROPS not only surpassed the performance of the IM-based agent but also demonstrated significant advantages in real-world applicability. The Mask-based RL agent's adaptability to various state availabilities makes it highly deployable across diverse scenarios. These observations strongly support the state-agnostic nature of our method and its ease of deployment, highlighting its potential for broad and effective application."}, {"title": "Ablation Studies", "content": "In this section, we conduct an important ablation study on the critical hyperparameter \u03bb, which is designed to balance the optimization of state recovery and crop management tasks. The results are shown in Table 5. While the optimal \u03bb is 0.02, this value may vary slightly based on different locations. However, the optimal range should remain on the scale of $10^{-2}$.\nMask Ranges While masking states enhances the generalization capacity and robustness of the RL agent, excessive masking can result in information loss and training challenges. To determine the optimal masking range, we conducted experiments whose results are presented in Table 6. Our findings indicate that the optimal masking range is between 0 and 12 states. Consequently, the optimal \u03b1 for each sampling falls within the range of 0 to 0.48. When \u03b1 = 0, all states are fully available. When \u03b1 = 0.48, 12 out of 25 states are masked out."}, {"title": "Path to Deploy", "content": "The management policies trained in the DSSAT-simulated environment may not perform optimally in real-world conditions due to uncertainties in weather and discrepancies between the simulated crop models and actual cropping systems. This well-known issue, referred to as the sim-to-real gap (Zhao, Queralta, and Westerlund 2020), underscores the difficulties in transferring RL policies from simulation to real-world scenarios. We address this critical challenge in the following methods."}, {"title": "Closing the Sim-To-Real Gap", "content": "To enhance the robustness of our trained management policies against the challenges posed by the sim-to-real gap, previous methods incorporate domain and dynamics randomization techniques (Peng et al. 2018; Zhao, Queralta, and Westerlund 2020). This approach involves introducing variations in critical model parameters and randomizing conditions during policy training to mimic the potential variances and noises encountered in real-world scenarios. These perturbations encourage the policies to become resilient to noises during deployment."}, {"title": "Policy Evaluation with Measurement Noises", "content": "When deploying pre-trained policies in practice, farmers depend on observable states derived from weather forecasts and soil moisture measurements. However, these data sources are often prone to inaccuracies due to forecast errors and sensor limitations. To simulate this real-world scenario, we conducted experiments by retrieving the true state of the environment from the simulator and introducing random measurement noise to one or more key observable state variables.\nThe values for measurement noise were determined based on real-world accuracy data from weather forecasts and commonly used soil moisture meters available on the market. For each level of measurement noise introduced, we evaluated the policy 400 times and reported the decrease rate of RF1 in scenarios where no noise was applied. As demonstrated in the Table 7, CROPS exhibits a smaller decrease in performance and delivers more satisfactory and robust results compared to previous methods. These findings demonstrate that the masking pre-training method inherently provides noise resilience during deployment, benefiting from its strategic masking approach."}, {"title": "Conclusion", "content": "In this paper, we propose CROPS, a unified, widely deployable, and noise-resilient crop management system. CROPS combines reinforcement learning, language models, and crop simulations to explore optimal management policies. In our experiments, we demonstrate the superior performance of CROPS using the maize crop in Florida, USA, and Zaragoza, Spain. More importantly, we show that the proposed method is readily deployable across ten millions of possible state availabilities. Compared to previous approaches with single and pre-defined state settings, CROPS has the capacity to generalize to countless scenarios worldwide. Moreover, its noise-resilient properties enable robust deployment in the real world, minimizing the influence of sensor bias. We believe that such advancements could significantly contribute to the evolution of agricultural technology, leading to more general, robust, and sustainable farming practices worldwide."}]}