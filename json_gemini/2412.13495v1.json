{"title": "Federated t-SNE and UMAP for Distributed Data Visualization", "authors": ["Dong Qiao", "Xinxian Ma", "Jicong Fan"], "abstract": "High-dimensional data visualization is crucial in the big data era and these techniques such as t-SNE and UMAP have been widely used in science and engineering. Big data, however, is often distributed across multiple data centers and subject to security and privacy concerns, which leads to difficulties for the standard algorithms of t-SNE and UMAP. To tackle the challenge, this work proposes Fed-tSNE and Fed-UMAP, which provide high-dimensional data visualization under the framework of federated learning, without exchanging data across clients or sending data to the central server. The main idea of Fed-tSNE and Fed-UMAP is implicitly learning the distribution information of data in a manner of federated learning and then estimating the global distance matrix for t-SNE and UMAP. To further enhance the protection of data privacy, we propose Fed-tSNE+ and Fed-UMAP+. We also extend our idea to federated spectral clustering, yielding algorithms of clustering distributed data. In addition to these new algorithms, we offer theoretical guarantees of optimization convergence, distance and similarity estimation, and differential privacy. Experiments on multiple datasets demonstrate that, compared to the original algorithms, the accuracy drops of our federated algorithms are tiny.", "sections": [{"title": "1 Introduction", "content": "High-dimensional data are prevalent in science and engineering and their structures are often very complicated, which makes dimensionality reduction and data visualization appealing in knowledge discovery and decision-making (Jolliffe and Cadima 2016; Hinton and Salakhutdinov 2006; Van Der Maaten et al. 2009). In the past decades, many algorithms have been proposed for dimensionality and visualization (Pearson 1901; Fisher 1936; Sammon 1969; Baker 1977; Kohonen 1982; Sch\u00f6lkopf, Smola, and M\u00fcller 1998; Roweis and Saul 2000; Tenenbaum, De Silva, and Langford 2000; Van der Maaten and Hinton 2008; Fan et al. 2018; McInnes et al. 2018). Perhaps, the most popular algorithms in recent years are the t-distributed stochastic neighbor embedding (t-SNE) developed by (Van der Maaten and Hinton 2008) and the Uniform Manifold Approximation and Projection (UMAP) proposed by (McInnes et al. 2018). T-SNE and UMAP map the data points to a two- or three-dimensional space, exhibiting the intrinsic data distribution or pattern of the original high-dimensional data. Due to their superiority over other methods such as PCA (Jolliffe and Cadima 2016), Isomap (Tenenbaum, De Silva, and Langford 2000), and autoencoder (Hinton and Salakhutdinov 2006), they have been used for visualizing images, tabular data (Hao et al. 2021), text (Grootendorst 2022), and graphs (Wu, Zhang, and Fan 2023) in diverse fields and provide huge convenience for scientific research and engineering practice (Becht et al. 2019). Besides visualization, t-SNE and UMAP are also useful in clustering (Linderman and Steinerberger 2019) and outlier detection (Fu, Zhang, and Fan 2024). There are also a few variants of t-SNE (Yang et al. 2009; Carreira-Perpin\u00e1n 2010; Xie et al. 2011; Van Der Maaten 2014; Gisbrecht, Schulz, and Hammer 2015; Pezzotti et al. 2016; Linderman et al. 2019; Chatzimparmpas, Martins, and Kerren 2020; Sun, Han, and Fan 2023) and UMAP (Sainburg, McInnes, and Gentner 2021; Nolet et al. 2021). For instance, Van Der Maaten (2014) used tree-based algorithms to accelerate the implementation of t-SNE. Sainburg, McInnes, and Gentner (2021) proposed a parametric UMAP that can visualize new data without re-training the model.\nIn many real cases such as mobile devices, IoT networks, medical records, and social media platforms, the high-dimensional data are distributed across multiple data centers and subject to security and privacy concerns (Dwork, Roth et al. 2014; McMahan et al. 2017; Kairouz et al. 2021; Qiao, Ding, and Fan 2024), which leads to difficulties for the standard algorithms of t-SNE and UMAP. Specifically, in t-SNE and UMAP, we need to compute the pair-wise distance or similarity between all data points, meaning that different data centers or clients should share their data mutually or send their data to a common central server, which will leak data privacy and lose information security. To address this challenge, we propose federated t-SNE and federated UMAP in this work. Our main idea is implicitly learning the distribution information of data in a manner of federated learning and then estimating the global distance matrix for t-SNE and UMAP. The contribution of this work is summarized as follows:\n\u2022 We propose Fed-tSNE and Fed-UMAP that are able to visualize distributed data of high-dimension.\n\u2022 We further provide Fed-tSNE+ and Fed-UMAP+ to en-"}, {"title": "2 Related work", "content": "t-SNE t-SNE (Van der Maaten and Hinton 2008) aims to preserve the pair-wise similarities from high-dimension space P to low-dimension space Q. The pair-wise similarities are measured as the probability that two data points are neighbors mutually. Specifically, given high-dimensional data points $x_1, x_2, . . ., x_N$ in $R^D$, t-SNE computes the joint probability matrix $P \\in R^{N\\times N}$, in which $p_{ij} = 0$ if i = j, and $p_{ij} = \\frac{P_{i|j}+P_{j|i}}{2N}$, if $i \\neq j$, where\n$P_{j|i} = \\frac{exp(-\\|x_i-x_j\\|^2/(2\\sigma_i^2))}{\\Sigma_{l \\in [N]\\setminus{i}} exp(-\\|x_i-x_l\\|^2/(2\\sigma_i^2))}$.\nIn (1), $\\sigma_i$ is the bandwidth of the Gaussian kernel. Suppose $y_1, y_2,..., y_n$ are the low-dimensional embeddings in $R^d$, where $d \\ll D$, t-SNE constructs a probability matrix Q by\n$q_{ij} = \\frac{(1+\\|y_i-y_j\\|^2)^{-1}}{\\Sigma_{l,s\\in [N], l\\neq s}(1+\\|y_l-y_s \\|^2)^{-1}}$\nwhere $i \\neq j$. Then t-SNE obtains $y_1, y_2,..., y_n$ by minimizing the Kullback-Leibler (KL) divergence\n$\\min_{Y_1,..., Y_N} \\sum_{i\\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$\nUMAP UMAP (McInnes et al. 2018) is a little similar to t-SNE. It starts by constructing a weighted k-NN graph in the high-dimensional space. The edge weights between points $x_i$ and $x_j$ are defined based on a fuzzy set membership, representing the probability that $x_j$ is in the neighborhood of $x_i$. Specifically, the membership strength is computed using\n$\\mu_{i|j} = exp( - \\|x_i \u2013 x_j\\|^2/\\sigma_i)$,\nwhere $\\sigma_i$ is a local scaling factor determined by the k-NNs of $x_i$. The final membership strength is symmetrized as\n$\\mu_{ij} = \\mu_{i|j} + \\mu_{j|i} - \\mu_{i|j}\\mu_{j|i}$\nIn the low-dimensional space, the probability of two points being neighbors is modeled using a smooth, differentiable approximation to a fuzzy set membership function. The edge weights between points $y_i$ and $y_j$ are given by\n$\\mu'_{ij} = \\frac{1}{1+a\\|y_i-y_j \\|^{2b}}$\nwhere a and b are hyperparameters typically set based on empirical data to control the spread of points in the low-dimensional space. UMAP minimizes the cross-entropy between the high-dimensional fuzzy simplicial set and the low-dimensional fuzzy simplicial set, i.e.,\n$\\min_{Y_1,..., Y_N} \\sum_{i\\neq j} \\mu_{ij} \\log (\\mu'_{ij})+(1-\\mu_{ij}) \\log (1-\\mu'_{ij})$"}, {"title": "3 Federated Distribution Learning", "content": "Suppose data $X = {X_p}_{p=1}^P$ are distributed at P clients, where $X_p \\in R^{m \\times n_p}$ belongs to client p and $\\Sigma_{p=1}^P n_p = n_d$. To implement t-SNE and UMAP, we need to compute a matrix $D_{X, X} \\in R^{n_d \\times n_d}$ of distances between all data pairs in X, which requires data sharing between the clients and central server, leading to data or privacy leaks. We propose to find an estimate of the distance or similarity matrix without data sharing. To do this, we let the central server construct a set of intermediate data points denoted by $Y = [Y_1,\u2026\u2026\u2026, Y_{n_y}] \\in R^{m \\times n_y}$ and then compute distance matrices $D_{Y, Y}$ and ${D_{X_p,Y}}_{p=1}^P$. These distance matrices can be used to construct an estimate $\\hat{D}_{X, X}$ of $D_{X, X}$ by applying the Nytr\u00f6m method (Williams and Seeger 2001) (to be detailed later). However, the choice of Y affects the accuracy of $\\hat{D}_{X, X}$, further influencing the performance of t-SNE and UMAP.\nSince Nytr\u00f6m method (Williams and Seeger 2001) aims to estimate an entire matrix using its small sub-matrices, the sub-matrices should preserve the key information of the entire matrix, which means a good $Y = [Y_1,..., Y_{n_y}] \\in R^{m \\times n_y}$ should capture the distribution information of X. Therefore, we propose to learn such a Y adaptively from the P clients via solving the following federated distribution learning (FedDL) framework:\n$\\min_{Y} F(Y) = \\sum_{p=1}^P \\omega_p f_p(Y)$\nwhere $f_p$ is the local objective function for each client, and $w_1,..., w_p$ are nonnegative weights for the clients. Without loss of generality, we set $w_1 = ... = w_p = 1/P$ for convenience in the remaining context. In this work, we set $f_p$ to be the Maximum Mean Discrepancy (MMD) (Gretton et al."}, {"title": "3.1 Framework", "content": "2012) metric:\n$f_p(Y) = MMD(X_p, Y)$\n$= \\frac{1}{n_p(n_p - 1)} \\sum_{i=1}^{n_p} \\sum_{j \\neq i}^{n_p} k((x_p)_{:,i}, (x_p)_{:,j})$\n$- \\frac{2}{n_p n_y} \\sum_{i=1}^{n_p} \\sum_{j=1}^{n_y} k((x_p)_{:,i}, (Y)_{:,j})$\n$+ \\frac{1}{n_y(n_y - 1)} \\sum_{i=1}^{n_y} \\sum_{j \\neq i}^{n_y} k((Y)_{:,i}, (Y)_{:,j})$\nor in the following compact form\n$f_p(Y) = MMD(X_p, Y)$\n$= \\frac{1}{n_p n_p^T} \\mathbb{1}_{n_p}^T K_{X_p, X_p} \\mathbb{1}_{n_p} - \\frac{2}{n_p n_y} \\mathbb{1}_{n_p}^T K_{X_p, Y} \\mathbb{1}_{n_y}$\n$+ \\frac{1}{n_y(n_y-1)} \\mathbb{1}_{n_y}^T K_{Y, Y} \\mathbb{1}_{n_y} - \\frac{1}{n_y}diag(K_{Y, Y})$.\nwhere k(\u00b7, \u00b7) is a kernel function and K\u00b7,\u00b7 denotes the kernel matrix computed from two matrices. MMD is a distance metric between two distributions and (10) is actually an estimation of MMD with finite samples from two distributions. If we use the Gaussian kernel $k(x_i, y_j) = exp(-\\gamma \\|x_i \u2013 y_j\\|^2)$, MMD compares all-order statistics between two distributions. For any $X \\in R^{m \\times n_x}$ and $Y \\in R^{m \\times n_y}$, we calculate the Gaussian kernel matrix as $K_{X,Y} = exp(-\\gamma D^2)$, where $D^2$ is the squared pairwise distance matrix between X and Y, i.e., $D^2 = Diag(X^TX)\\mathbb{1}_{n_y} \u2013 2X^TY + \\mathbb{1}_{n_x}Diag(Y^TY)^T$.\nCombining (8) and (10), we have the following optimization problem of federated distribution learning\n$\\min_{Y} \\sum_{p=1}^P \\omega_p \\times MMD(X_p, Y)$\nBy solving this problem, the central server or Y equivalently can learn the distribution information of the data distributed on the P clients. Based on such an Y, we can estimate the distance or similarity matrix between all data points in X, which will be detailed later."}, {"title": "3.2 Optimization", "content": "For a client p, we consider the corresponding local optimization problem\n$\\min_{Y} f_p(Y)$\nwhere $f_p(Y) = MMD(X_p, Y)$. Due to the presence of kernel function, we have to use some numerical methods like gradient descent to update the decision variable Y. The gradient of $f_p$ at Y is\n$\\nabla f_p(Y) = \\frac{-4\\gamma}{n_p n_y} [XK_{X,Y} - Y Diag(\\mathbb{1}_{n_y}^T, K_{X,Y})]$\n$+ \\frac{4\\gamma}{n_y(n_y - 1)} [YK_{Y,Y} \u2013 Y Diag(\\mathbb{1}_{n_y}^T K_{Y,Y})]$."}, {"title": "3.3 Convergence analysis", "content": "Since we adopt MMD as our local objective function, they are all bounded below. Here, we give the convergence guarantee of Algorithm 1.\nTheorem 1. Assume the gradient of all local objective functions ${f_p}_{p=1}^P$ are $L_p$-Lipschitz continuous, $L = \\sum_{p=1}^P w_p L_p$ with $w_p = \\frac{1}{P}, \\sum_{p=1}^P w_p = 1$, and $|| \\nabla f_p - \\nabla f_{p'} ||_F < \\zeta$ for all p, p', the sequence ${Y^{s,t}}$ generated by Algorithm 1 with step size 1/L satisfies\n$\\frac{1}{SQ} \\sum_{s=1}^S \\sum_{t=1}^Q || Y^{s,t} \u2013 Y^{s,t-1}||^2 < \\frac{4}{SQL} [F(Y^0) \u2013 F(Y^S)] + \\frac{12PL^2(Q + 1)(2Q + 1)}{L^2[1-\\frac{3(Q-1)^2(PL+\\frac{max_p L^2}{p})}{L^2}]}$\nThe proof can be found in Appendix F. It can be seen that when SQ goes large enough, our algorithm converges to a finite value that is small provided that $\\zeta$ is small."}, {"title": "4 Applications of FedDL", "content": "4.1 Federated tSNE and UMAP\nNystrom approximation is a technique that can approximate a positive semi-definite (PSD) matrix merely through a subset of its rows and columns (Williams and Seeger 2001). Consider a PSD matrix S \u220b H > 0 that has a representation of block matrix\n$S \\ni H = \\begin{bmatrix} W & B \\\\ B^T & Z \\end{bmatrix}$\nwhere $W \\in S$, $B \\in R^{(n-c)\\times c}$, and $Z \\in S_{r^{-c}}$ for which $c < n$. Specifically, suppose Z is unknown, we can approximate it using W, B, and $B^T$ as\n$\\hat{Z} \\approx BWB^T \\preceq Z$\nThis means we can approximate the incomplete H by H = [W, BT; B, Z]. By Nystr\u00f6m method, we can approximate a distance or similarity matrix on large-scale dataset in a relatively low computational complexity. Some literature gives some useful upper bounds on Nystr\u00f6m approximation in terms of Frobenius norm and spectral norm for different sampling techniques (Kumar, Mohri, and Talwalkar 2009b;"}, {"title": "4.1 Federated tSNE and UMAP", "content": "Theorem 2 (Error bounds of Nystr\u00f6m approximation). Given $X = [x_1,...,x_n] \\in R^{m\\times n}$, let $H$ be the rank-k Nystr\u00f6m approximation of H only through c columns sampled uniformly at random without replacement from H, and $H_k$ be the best rank-k approximation of H. Then, the following inequalities hold for any sample of size c:\n$\\|H \u2013 H_c\\|_2 \u2264 \\|H - H_k\\|_2 + \\sqrt{\\frac{k}{c}}\\|H\\|_F^{1/2}$,\n$\\|H \u2013 H_c\\|_F \u2264 \\|H - H_k\\|_F + p \\sqrt[4]{\\frac{k}{c}}$\nwhere $p = \\max_i H_{ii}$.\nWithout the retrieval of raw data from clients, we present federated tSNE (Fed-tSNE) and federated UMAP (Fe-dUMAP) to visualize the high-dimensional data distributed across multiple regional centers. The main idea is to perform Algorithm 1 to learn a Y and then each client p posts the distance matrix $D_{X_p,Y} \\in R^{n_p\\times n_y}$ between $X_p$ and Y to the central server. Consequently, the central server assembles all $D_{X,Y}$ to form\n$B = [D_{X_1,Y}^T D_{X_2,Y}^T... D_{X_P,Y}^T]$\nand estimate $D_{XX}$ as\n$\\hat{D}_{X,X} = BWB^T$\nwhere $W = D_{Y,Y}$, i.e., the distance matrix of Y. Note that in the case that W is singular, we can add an identity matrix to it, i.e., W + XI, where x > 0 is a small constant. Finally, the central server implements either t-SNE or UMAP based on $\\hat{D}_{X, X}$."}, {"title": "4.2 Federated Spectral Clustering", "content": "Note that after running Algorithm 1, if each client post the kernel matrix $K_{X_pY}$ rather than the distance matrix $D_{X,Y}$ to the central server, the central server can construct a kernel or similarity matrix $K_{XX}$ that is useful for spectral clustering. Thus we obtain federated spectral clustering, of which the steps are summarized into Algorithm 3."}, {"title": "5 FedDL with differential privacy", "content": "5.1 FedDL by data perturbation\nWe inject noise into the raw data in each client and then run FedDL to learn the global distribution information. Note that data perturbation is a one-shot operation before performing Algorithm 1. Specifically, the data X is perturbed by a noise matrix $E \\in R^{m \\times n_x}$ to form the noisy data matrix $X = X + E$, where $e_{i,j} \\sim N(0, \\sigma^2)$. Define $X = {X_p}_{p=1}^P$ and we then perform Algorithm 1 on $\\hat{X}$ to obtain Y which gives the Nystr\u00f6m approximation\n$\\hat{H}_{\\tilde{X},\\tilde{X}} \\approx BW \\hat{B}^T$\nwhere $B = K_{\\tilde{X},\\tilde{Y}}$ (or $D_{\\tilde{X},Y}$), $W = K_{\\tilde{Y},\\tilde{Y}}$ (or $D_{\\tilde{Y},Y}$).\nFollowing the logistics of existing literature, we give the upper bounds on the approximation error of Nystr\u00f6m approximation involved with FedDL, where we focus only on the kernel matrix because it is more complex than the distance matrix."}, {"title": "5.1 FedDL by data perturbation", "content": "Theorem 3 (Error bound of Nystr\u00f6m approximation with FedDL having data perturbation). Given $X = {X_p}_{p=1}^P$ with $X_p \\in R^{m\\times n_p}$ having $\\sum_{p=1}^P n_p = nx, Y = [Y_1,..., Y_{n_y}] \\in R^{m\\times n_y}$, let $X_{XY} = [Y, X]$ be the augmented matrix, C = K_{\\tilde{X},\\tilde{Y}}, W = K_{\\tilde{Y},\\tilde{Y}} with W being the Moore-Penrose inverse of the best rank-k approximation of W, $\\xi_m = \\sqrt{m} + \\sqrt{2mt} + 2t$, and Cond(\u00b7) denote condition number of matrix. Denoting $\\hat{H}_{X, X\\tilde{Y}} = CW \\hat{C}^T$, it holds with probability at least $1 \u2013 n(n - 1)e^{-t}$ that\n$\\|\\hat{H}_{X, X\\tilde{Y}} - K_{X,X}\\|_2 < Cond(K_{\\tilde{X},\\tilde{Y}})(\\frac{| MMD(X,Y) |}{n_x+n_y}+1)\n+2n_x\n+ \\sqrt{2}n_x\\gamma \\tilde{\\xi}_m [\\sigma^2 + \\sqrt{2}\\|D_{X, X}\\|_\\infty \\tilde{\\xi}_m]$\nalternatively, it holds that\n$\\|\\hat{H}_{X, X\\tilde{Y}} - K_{X,X}\\|_F\n< \\sqrt{n_x+n_y - k}Cond(K_{\\tilde{X},\\tilde{Y}})(\\frac{| MMD(X,Y) |}{n_x+n_y}+1)\n+2kn_x\\sqrt{1+1} + \\sqrt{2}n_x\\gamma [\\sigma^2 + \\sqrt{2}\\|D_{X, X}\\|_\\infty \\tilde{\\xi}_m]$\nTheorem 4 (Differential privacy of FedDL with data perturbation). Assume $\\max_{p,j} \\|(X_p)_{:,j}\\|^2 = \\tau_X$, FedDL with perturbed data given by Section 5.1 is (\u03b5, \u03b4)-differentially private if \u03b4 > 2c\u03c4X/\u03b5, where $c^2 > 2ln(1.25/\u03b4)$."}, {"title": "5.2 FedDL by variable and gradient perturbation", "content": "We can also perturb the optimization variable Y or the gradient $f_p (Y_p)$ by Gaussian noise in the training progression to improve the security of Algorithm 1. No matter which method we follow, the Y obtained by the central server is noisy, i.e., Y = Y + E, where E is drawn elementwise from N(0, \u03c3\u00b2). Then, we do Nystr\u00f6m approximation by\n$\\hat{H}_{X,X\\tilde{Y}} \\approx BWB^T$\nwhere $B = K_{\\tilde{X},\\tilde{Y}}$ (or $D_{\\tilde{X},\\tilde{Y}}$), $W = K_{\\tilde{Y},\\tilde{Y}}$ (or $D_{\\tilde{Y},\\tilde{Y}}$)."}, {"title": "5.2 FedDL by variable and gradient perturbation", "content": "Theorem 5 (Error bound of Nystr\u00f6m approximation with FedDL having gradient perturbation). With the same notations in Theorem 3, let $X_{XY} = [Y, X]$ be the augmented matrix. Then it holds that\n$\\|\\hat{H}_{X, X\\tilde{Y}} - K_{X,X}\\|_2 <Cond(K_{\\tilde{X},\\tilde{Y}})(\\frac{| MMD(X,Y) |}{n_x+n_y}+1)+2n_x$\nalternatively, it holds that\n$\\|\\hat{H}_{X, X\\tilde{Y}} - K_{X,X}\\|_F< \\sqrt{n_x+n_y}\nCond(K_{\\tilde{X},\\tilde{Y}})(\\frac{| MMD(X,Y) |}{n_x+n_y}+1)\n+2k^{1/4}n_x\\sqrt{1+\\frac{n_y}{n_x}}$\nNote that $MMD(\\tilde{X},\\tilde{Y}) \\leq MMD(X, Y)+MMD(Y,\\tilde{Y})$ is related to \u03c3. A smaller \u03c3 leads to a lower estimation error (higher estimation accuracy) but weaker privacy protection. We can obtain a precise trade-off between accuracy and privacy by combining Theorem 5 with Theorem 6.\nTheorem 6 (Differential privacy of FedDL with gradient perturbation). Suppose $\\max_{p,j} \\|(X_p)_{:,j}\\|^2 = \u03c4X$, $\\max_{p,i,j} \\|(Y_p)_{:,i} - (X_p)_{:,j}\\| = \u03b3, \\|\\Upsilon\\|_\\text{sp} \u2264 T_Y\u2200s, let ${f_p(Y)}_{s=1}^{[S]}$ be the sequence that is perturbed by noise drawn from $N(0, \u03c3^2)$ with variance $8S^2 log(e + (\u03b5/\u03b4))/\u03b5^2$ where $\u0394 = \\frac{8\u03b3\u03c4\u03b1}{n_p n_y}{1 + 2(\u03c4X + \u03c4Y) (\u03c4X + Y)}$. Then, the Gaussian Mechanism that injects noise to ${f_p(Y)}_{s=1}^S$ for p \u2208 [P] is (\u03b5, \u03b4)-differentially private.\nNote that it is intuitively appropriate to choose a decreasing sequence of noise variance ${\\sigma^2}_{s=1}^S$ adapted to the gradient norm, which may make the algorithm converge well. In practice, we do not have to do this and can instead inject homoscedastic noise while incorporating a carefully chosen scaling factor into the step size of the gradient descent. By doing so, the differential privacy of our FedDL with gradient perturbation can be guaranteed by Theorem 6."}, {"title": "5.3 Fed-tSNE+ and Fed-UMAP+", "content": "Based on the above discussion, we propose the security-enhanced versions of Fed-tSNE and Fed-UMAP, denoted by Fed-tSNE+ and Fed-UMAP+, for which Algorithm 2 has noise injection in line 1 (Algorithm 1)."}, {"title": "6 Experiments", "content": "6.1 Data Visualization\nWe applied the proposed Fed-tSNE and Fed-UMAP methods to the MNIST and Fashion-MNIST datasets, with $n_x = 40,000$, and set $n_y = 500$. We designed the experiment with 10 clients, where IID (independent and identically distributed) refers to each client's data being randomly sampled from the MNIST dataset, thus including all classes. In contrast, non-IID means that each client's data contains only a single class. After reducing the data dimension to two, we visualized them."}, {"title": "6.2 Clustering performance", "content": "We utilized three datasets MNIST, COIL-20, and Mice-Protein (detailed in Appendix) to evaluate the effectiveness of our Fed-SpeClust. The hyperparameters were adjusted accordingly and the corresponding results are presented in Table 5. In addition to the NMI metric used previously, we also employed the ARI (Adjusted Rand Index) metric, detailed in Appendix. We see that both NMI and ARI indicate that Fed-SpeClust achieves results comparable to the original spectral clustering, despite a slight decrease in performance, demonstrating the feasibility of our method."}, {"title": "7 Conclusion", "content": "This work proposed FedDL and applied it to t-SNE and UMAP to visualize distributed data. The idea was also extended for spectral clustering to cluster distributed data. We provided theoretical guarantees such as differential privacy. Experimental results demonstrated that the accuracies of our federated algorithms are close to the original algorithms."}]}