{"title": "AI for All: Identifying AI incidents Related to Diversity and Inclusion", "authors": ["Rifat Ara Shams", "Didar Zowghi", "Muneera Bano"], "abstract": "The rapid expansion of Artificial Intelligence (AI) technologies has introduced both significant advancements and challenges, with diversity and inclusion (D&I) emerging as a critical concern. Addressing D&I in AI is essential to reduce biases and discrimination, enhance fairness, and prevent adverse societal impacts. Despite its importance, D&I considerations are often overlooked, resulting in incidents marked by built-in biases and ethical dilemmas. Analyzing AI incidents through a D&I lens is crucial for identifying causes of biases and developing strategies to mitigate them, ensuring fairer and more equitable AI technologies. However, systematic investigations of D&I-related AI incidents are scarce. This study addresses these challenges by identifying and understanding D&I issues within AI systems through a manual analysis of AI incident databases (AIID and AIAAIC). The research develops a decision tree to investigate D&I issues tied to AI incidents and populate a public repository of D&I-related AI incidents. The decision tree was validated through a card sorting exercise and focus group discussions. The research demonstrates that almost half of the analyzed AI incidents are related to D&I, with a notable predominance of racial, gender, and age discrimination. The decision tree and resulting public repository aim to foster further research and responsible AI practices, promoting the development of inclusive and equitable AI systems.", "sections": [{"title": "1 Introduction", "content": "The rapid proliferation of Artificial Intelligence (AI) technologies has brought both remarkable advancements and significant challenges. Among these challenges, the issue of diversity and inclusion (D&I) has attracted considerable attention [1]. Addressing D&I in AI is crucial for several reasons: it reduces biases, increases fairness, enhances creativity, and prevents harmful societal impacts [2]. However, despite these pressing needs, D&I considerations are often overlooked in the design, development, and deployment of AI, resulting in unintended consequences and many AI incidents.\nIn recent years, reported real-world AI incidents that demonstrate discrimination and bias show the necessity of integrating D&I principles in AI applications. Instances such as Google Images misrepresenting women's job roles [3] and Google Photos mistakenly categorizing images of African Americans inappropriately [4] highlight the built-in biases of AI and the complex ethical problems that come with it. Even Tinder Plus, a popular platform, encountered issues when it implemented a biased personalized pricing algorithm that disproportionately charged users over 30 and gay and lesbian users aged 18-29 [5]. Incidents of facial recognition errors leading to wrongful arrests [6], AI hiring tools biased against females [7], and medical algorithms that prioritize white patients over black patients [8] clearly indicate deep-rooted biases in AI systems. Historical biases in motion capture data, which predominantly favored able-bodied male subjects [9], further exhibit the systemic exclusion of the disabled present in AI applications. Recent incidents, such as OpenAI's ChatGPT displaying gender bias in recommendation letters [10], only underscore the pressing need for embedding D&I principles in AI. Beyond sex, age, or race, many AI incidents occur based on different diversity attributes such as ethnicity, language, religion, nationality, disability, culture, socio-economic status, geographic location and so on.\nAnalyzing AI incidents through a D&I lens becomes critical for several compelling reasons. Firstly, it enables us to identify and understand the underlying causes of biases and discriminatory practices in AI systems. Recognizing these causes is vital for developing strategies that mitigate such biases, ensuring that future AI technologies are fairer and more equitable. Inspired by the frequent monitoring and maintenance of systems like the Black-box flight data recorder in aviation industry [11], we posit that it is essential to adopt a similar approach in dealing with AI systems. By learning from the past mistakes, we can enhance the reliability and trustworthiness of AI systems and prevent similar incidents from occurring in the future.\nDespite the need of investigating D&I-related AI incidents, to the best of our knowledge, no research has been conducted to identify D&I related AI incidents, nor to propose strategies to avoid them. An important question to ask is: What is Diversity and Inclusion in AI? Zowghi et al. defined D&I in AI with \"inclusion of humans with diverse attributes and perspectives in the data, process, system, and governance of the AI ecosystem\" [1]. They proposed a 5-pillar framework in the AI ecosystems to propose a holistic and sociotechnical approach to D&I in AI consideration. Shams et al. conducted a systematic literature review (SLR) to explore the challenges and corresponding solutions to address D&I in AI and enhance D&I practices by AI [2]. They utilised that 5-pillar framework of Zowghi et al. to structure and present the results of their SLR. Another research has emphasized making AI diverse and inclusive"}, {"title": "2 Background and Related Work", "content": "The evolution of artificial intelligence (AI) [15] has permeated many domains, including health [16], education [17], transportation [18], and law [19], necessitating the development of ethical and responsible systems. Discrimination can be embedded into AI systems through various avenues such as data, design, implementation, and the absence of adequate legal frameworks [20]. Data used to train AI models may contain biases, leading to algorithmic discrimination [21]. The design of AI algorithms can inadvertently perpetuate discriminatory outcomes, even when human prejudices are intended to be eliminated. Furthermore, the implementation of AI systems without proper checks and balances can result in discriminatory decisions [22]. The lack of comprehensive legal frameworks to regulate AI and prevent discriminatory practices poses a significant challenge [23]. To address these issues, a multidisciplinary approach integrating legal and technological perspectives is crucial to develop fair and unbiased AI systems that comply with existing antidiscrimination laws [24]. According to Zhou et al., the widespread application of AI in various domains makes it imperative to align its operational principles with ethical standards [25]. This necessitates the establishment of guidelines and principles to ensure such systems are unbiased, trustworthy, and fair to all.\nPrinciples of Diversity and Inclusion aim to tackle the challenges of bias and discrimination in society. Embedding D&I principles into the processes of designing, developing, and deploying AI systems is crucial to achieving equity and fairness [1]. An important aspect of integrating D&I into AI involves the identification and analysis of D&I-related AI incidents. These incidents expose the underlying biases and discrimination embedded within AI systems. Identifying these incidents enables us to understand their causes and develop strategies to mitigate them, thereby enhancing the fairness and inclusivity for future AI technologies.\n2.1 Diversity and Inclusion in AI\nD&I in AI is gaining increasing attention in research and practice. To achieve trustworthy AI, the importance of embedding diversity and Inclusion throughout the AI system development life cycle has been emphasized. [1]. While D&I and fairness are distinct concepts, fostering diversity can lead to fair outcomes, particularly in information access systems like recommendation systems and search engines [26]. Scholars highlight the risks of AI systems perpetuating existing inequalities, underscoring the need for responsible AI development that incorporates diversity, equity, and inclusion (DEI) principles and practices [14]. Guidelines for AI increasingly advocate for DEI principles, emphasizing the importance of addressing DEI risks through actions that influence AI actors' behaviors and awareness [14].\nIn order to have a comprehensive understanding of diversity in AI, it is vital to acquire an understanding of different diversity attributes (e.g., gender, age, ethnicity, race, socio-economic status, nationality, religion etc.) that necessitate careful consideration within AI systems. Zowghi et al. defined diversity attributes as \"known facets of diversity, including (but not limited to) the protected attributes in Article 26 of the International Covenant on Civil and Political Rights (ICCPR) [27], as well as"}, {"title": "2.2 AI Incidents Related to Diversity and Inclusion", "content": "Since D&I in Al is a new and growing field of study, there is a paucity of research on this topic. This gap has led to a lack of awareness in implementing D&I in AI systems, consequently resulting in various AI incidents that could be attributed to the violation of D&I principles. There are publicly available AI incident databases such as AI Incident Database (AIID), AI, Algorithmic, and Automation Incidents and Controversies (AIAAIC), and OECD AI Incident Database\u00b3.\nA number of recent studies have focused on AI incident databases for various purposes. For example, a recent research emphasized the importance of an AI incident database for recording and examining real-world AI failures, thus preventing recurring mistakes and ensuring AI's societal benefits [31]. Wei et al. presented a detailed analysis of real-world AI ethical issues, drawn from the AI Incident Database, identifying 13 prevalent application areas and 8 forms of ethical issues with the aim to provide AI practitioners with a practice-oriented guideline for ethical AI deployment [32]. Similarly, another study addressed challenges to engineering trustworthy AI by analyzing 30 real-world incidents of trust loss from the AI incident database, offering practical recommendations to be incorporated into the development cycle in AI systems [33]. Feffer et al. suggested the AI incident database as an educational tool, highlighting its role in a study that enhanced students' understanding of AI harms and the requirement for safe and responsible AI [34]. A recent research utilized public AI incident databases to assess reporting techniques, aiming to enhance incident documentation, thus contributing to safer, fairer AI development [35].\nWhile several recent studies have used AI incident databases for a variety of research objectives, there appears to be a noticeable gap in current state of the art on investigating D&I issues in AI incidents. To the best of our knowledge, no research has been undertaken so far that specifically targets D&I issues within AI incidents. This deficit extends to inquiries into the causes of D&I-related AI incidents, as well as the formulation of strategies that could potentially prevent such incidents. To mitigate this gap, we undertook a study utilizing two AI incident databases (AIID and AIAAIC) with the aim of identifying AI incidents that are related to diversity and"}, {"title": "3 Methodology", "content": "With the aim to identify AI incidents related to diversity and inclusion, we formulated the following two research questions.\nRQ1. How can we identify if an AI incident is related to diversity and inclusion issues?\nRQ2. To what extent do the existing AI incidents related to diversity and inclusion issues?\nWe conducted a mixed-methods empirical study to answer the research questions. We collected data from two public online AI incident databases, and through a card sorting exercise and two focus group discussions with artificial intelligence/machine learning (AI/ML) researchers and practitioners, who are enthusiast of D&I. Figure 1 shows an overview of our research method. As this study worked with humans, ethics approval was acquired from our organization's Human Research Ethics Committee on 19/03/2024."}, {"title": "3.1 Data Collection", "content": "We collected data from AI incident databases, card sorting exercise and focus groups."}, {"title": "3.1.1 AI Incident Databases", "content": "We collected AI incidents from two publicly available databases, AI Incident Database (AIID) and AI, algorithmic, and automation incidents and controversies (AIAAIC). These databases are dedicated archives, indexing the collective history of potential harms that have transpired in real-world scenarios as a result of deploying AI systems. We collected the data from AIID in July 2023 and from AIAAIC in March 2023. There were 551 AI incidents in AIID database and 575 incidents in AIAAIC. The AIID database includes the titles and summaries of the incidents including some news links, dates of the incidents, and information about the alleged/harmed/nearly harmed parties. On the other hand, AIAAIC includes the titles and summaries of the incidents along with the incident year, country, sector (e.g., media, sports, health), operator (e.g., Google, Microsoft), developer, system, technology (e.g., facial recognition, natural language processing) and corresponding news links. We manually analyzed the incidents from D&I lens and categorized them under three groups: Related to D&I (R), Not related to D&I (NR) and More information required (MIR). We also mentioned the reason supporting our determination for each incident and identified the diversity attributes for the D&I-related AI incidents."}, {"title": "3.1.2 Participants of Card Sorting and Focus Groups", "content": "We conducted card sorting to validate our categorization process of AI incidents with their relevance with D&I issues, as card sorting is a reliable, fast, and inexpensive approach that provides comprehensive understanding on the subject matter [36]. Additionally, as focus groups are a valuable validation technique used in various research fields [37], [38], [39], we also conducted focus groups to validate our categorization process and the decision tree.\nInitially, we formulated a specific set of recruitment criteria, facilitating the identification of suitable participants for the card sorting and focus groups. The main criterion to select the participants was finding practitioners who are AI researchers or AI practitioners, or information system and computer science researchers who have knowledge about diversity and inclusion. We shared our project objectives and participant recruitment criteria through social media (LinkedIn, Twitter (now X), Facebook) to invite participants in our study. We also invited potential participants through emails. Seven participants agreed to join the activities. All of them joined the card sorting, however, one participant did not attend the focus group. Therefore, we conducted two focus groups with three participants in each group.\nAt first, they were given participant information sheet to know the details of the project and their roles. It also included the risks and benefits of the project, withdrawal of participation from the project, confidentiality of their identities, data management, and contact details of the research team and human ethics research approval committee. Later, the participants were provided with a consent form and asked to give their consents by signing and returning the consent form."}, {"title": "3.1.3 Protocol of Card Sorting", "content": "We conducted card sorting with seven participants. We prepared 10 cards for each participant, where each card contained one AI incident. We selected 5 incidents from AIID database and another 5 incidents from AIAAIC database that were particularly thought-provoking and represented different categories (R, NR, MIR) according to our manual analysis. As manual analysis takes time, analyzing more than 10 incidents in a participatory activity was difficult. This activity was conducted in our workplace that took approximately 60 minutes.\nThis method consisted of three parts. The first part was an icebreaker; the principal investigator of this study met the participants before data collection starts and spent some time with them chatting to help put them at their ease. The second part covered explaining our research, its objectives, expected outcomes, possible benefits and risks of this research, and the strategies adopted to ensure the confidentiality of the collected data. For ethical compliance, we provided the participants with a participant information sheet and sought their signed consent. The last part was the closed card sorting [40], where the participants were given the cards containing AI incidents. They were asked to group the incidents under three categories based on the relevance of the incidents with D&I issues. Each card was numbered so that they can be provided in the same order to everyone. The participants were asked to analyze each card and categorize it based on the given categories. At this stage, they were asked to make their own decision without any discussion with other participants. They were also asked to write the reasons behind their labelling decision for each incident."}, {"title": "3.1.4 Protocol of Focus Groups", "content": "As focus groups are a valuable validation technique used in various research fields [37], [38], [39], we conducted two focus groups with three participants in each focus group to validate our categorization process and the decision tree. We arranged the focus groups at our workplace. The participants were provided a comfortable environment with round seating arrangements.\nEach focus group was divided into two parts. In the first part, the participants were encouraged to discuss about the cards with AI incidents and the way they categorized the cards. In the second part, we provided the decision tree to the participants. We described how we developed the decision tree and how the decision tree works to identify the diversity and inclusion issues in AI incidents. Afterwards, the participants were encouraged to criticize the decision tree and provide recommendations on how to improve it based on their knowledge on the card sorting and the first part of the focus group discussion. We Asked the following questions to facilitate the focus groups.\n\u2022 Questions on Card Sorting Exercise: How did you categorize the given AI incidents?, Have you faced any difficulties in categorizing them?, When you categorized an incident under \"More information required\", what information do you think should be provided to categorize them?\n\u2022 Questions on the Decision Tree: Do you think the decision tree is aligned with your categorization? Do you think any step of the decision tree is not clear enough to understand? Do you have any comments/concerns/recommendations on the decision tree?\nGiven the nature of focus groups to encourage participants to have an in-depth discussion, these sessions provided an invaluable opportunity to extract the required data and validate our proposed decision tree. We arranged the two focus groups on two different days. The first focus group took 43 minutes and the second one took 47 minutes. We recorded the focus group discussions after obtaining participants' consent."}, {"title": "3.2 Data Analysis", "content": "Figure 1 shows the overview of the data analysis. The analysis was divided into the following steps to ensure maximum clarity and precision.\nCollection and Initial Analysis of AI Incident Data. We selected 551 incidents from the AI Incident Database (AIID), timestamped on 24/07/2023. The first author of this paper manually analyzed the incidents to explore their correlation to D&I. The incidents were categorized into three distinct groups based on their relation to D&I.\nInvestigator Triangulation. To promote confidence in the initial analysis, investigator triangulation was applied to the first 100 AI incidents. Another researcher from our team was asked to perform an independent analysis to the incidents. We experienced 26 disagreements in categorization. However, they were resolved after having a discussion between the two investigators, allowing for a deeper understanding of the complex dynamics inherent within each case. By investigating these incidents from multiple perspectives, we increased confidence in the validity of our findings.\nRe-Analyzing Incident Data and Decision Tree Development. The first author manually analyzed all the 551 incidents once again taking into account the insights obtained from the discussion. As a result the decision for 45 incidents were changed. The first author also analyzed the first 50 incidents from AIAAIC database with the knowledge developed from the analysis of AIID and the discussion with the second investigator. This analysis and discussion helped in the development of the initial decision tree (V1) to identify D&I-related in AI incidents (see Figure 6 in Appendix A). Based on the conditions on how we categorized an incident to check the relevance of an incident with D&I issues, we developed this decision tree. Similar to our manual categorization, our proposed decision tree also provides three categories of AI incidents based on their relevance with D&I: related to D&I, not related to D&I and more information required.\nUpdating Decision Tree. The preliminary analysis and the decision tree were shared with the co-authors of this paper who are also experts in D&I in AI. After several rounds of discussions and iteration, we updated the initial decision tree (V2) accordingly guided by the improved understanding of the incidents (see Figure 7 in Appendix A).\nOrganizing Participatory Activities. We organized card-sorting and focus group discussions on April 2024 with 7 and 6 participants respectively to validate our"}, {"title": "4 Results", "content": "This section presents the results derived from the analysis of the AI incident database. The results also reflects the analytical overview of the card sorting exercise and focus group discussions carried out with AI/ML researchers and practitioners.\n4.1 RQ1: Identifying D&I Issues in AI Incidents\nFigure 2 shows the results of RQ1. To identify Diversity and Inclusion (D&I) related AI incidents, we developed a decision tree after a rigorous analysis.\nOur proposed decision tree has four conditions. The first condition checks if any human is directly or indirectly impacted by the AI incident. The impact can be physical, psychological, financial and so on. If the response is negative, the incident is not related to D&I. On the other hand, a positive response directs the process to the second condition, which examines whether the AI incident explicitly mentions about any diversity attributes, including but not limited to gender, sex, sexual orientation, age, skin tone, race, ethnicity, religion, language, literacy, disability, neurodiversity, facial features, physical features, nationality, accent, culture, geographic location, socio-economic status, and political ideology. If the answer is a 'yes', the incident is clearly related to D&I issues. However, for the answer 'no', the incident is scrutinized under"}, {"title": "4.2 RQ2: Extent to Which the Existing AI Incidents are Related to D&I issues", "content": "Of the 551 AI incidents extracted from the AIID database, our analysis identified 189 AI incidents related to D&I issues (see Table 1). 80 incidents require further information to establish their correlation with D&I issues. The rest of the 282 incidents are not related to D&I. Similarly, from AIAAIC database, we identified almost half of the incidents are related to D&I issues, 144 D&I-related AI incidents among the 310 incidents we analyzed (see Table 2). 50 incidents require more information to make determination. The rest of the 116 incidents are not related to D&I issues."}, {"title": "5 Discussion and Implications", "content": "This section discusses the findings of this study as well as the implications for research and practice.\nFirst Step Towards Identifying Cause of D&I-related AI Incidents. Identifying D&I-related AI incidents is the first step towards further investigations on these incidents to explore the underlying causes of them and identify potential strategies"}, {"title": "6 Threats to Validity", "content": "This section discusses the possible threats arising from this research based on the four validation criteria: credibility, confirmability, dependability, and transferability [46].\nCredibility. One potential threat to credibility could stem from the fact that the AI Incident Databases (AIID and AIAAIC), which were our primary data source, may not be an exhaustive list of AI incidents related to D&I issues. Unreported incidents, and incidents that were incorrectly reported or categorized in the database could be overlooked in the study. Furthermore, incidents reported in the database may carry a certain level of bias as they may disproportionately represent incidents from specific sectors, regions, or communities. We could overcome this threat by including other databases such as OECD AI incident database in the second version of our repository, which is our future work.\nConfirmability. Confirmability refers to the degree of neutrality in the research findings. A significant threat to confirmability could arise from the manual analysis of incidents. Personal bias, misunderstanding or misinterpretation of information could influence our analysis and the development of the decision tree. Personal interests and preconceptions may have influenced the assessments and categorizations of incidents. Similarly, the decision-tree may have been influenced by the investigators' understanding and interpretation of D&I issues. However, investigator triangulation and focus groups have been conducted to minimize potential bias. Furthermore, the investigators come from diverse ethnic, cultural, racial, and age groups, which enhances their perspective on diversity and inclusion.\nAnother potential threat could be raised from the number of participants in the card sorting and focus groups. The categorization of AI incidents and the decision tree were validated through card sorting and focus group exercises, but these methods involved a small number of participants. Findings from this validation may not be generalizable.\nDependability. Dependability concerns the repeatability of the research findings in similar contexts. There exists a threat to the dependability of our study owing to it's largely qualitative nature. Our categorization, decision tree development, and subsequent analysis are driven mostly by interpretation, which may vary among different"}, {"title": "7 Conclusions and Future Work", "content": "With the aim to develop more inclusive, unbiased, and trustworthy AI systems, this study is a critical first step in identifying D&I-related incidents in AI and understanding the extent to which D&I issues exist within AI systems. We developed a decision tree as a preliminary framework for identifying and categorizing AI incidents in terms of their relation to D&I issues. We also proposed and populated a public repository on D&I-related AI incidents. The AI systems were found to have a significant association with D&I issues, with 34.3% and 46.45% of the analyzed incidents being related to D&I from AIID and AIAAIC databases respectively. This emphasizes the need for careful attention to D&I during AI system design, development, and deployment. Moreover, the lack of information in some cases also complicates the task of categorizing AI incidents, making it difficult to draw definitive conclusions regarding their D&I implications. Hence, a more informative and comprehensive representation of AI incidents is required. Despite the comprehensive consideration of 16 diversity attributes, findings indicated a concerned prominence of racial, gender, and age discrimination in AI incidents. It underlines the urgency to address 'race', 'gender', and 'age' biases in AI system development, while not undermining the criticality of other attributes.\nThis study also provides a roadmap for future research in D&I within AI. Given the dynamic nature of AI, there is a continuous need to populate and revise the repository with new incidents for timely research investigations. Additionally, we must analyze all the 575 existing AIAAIC incidents in future and include the D&I-related AI incidents in our repository. Future studies should also try to understand why some AI incidents needed \"more information\". This could help figure out exactly what challenges or complications are making it difficult to clearly classify these incidents. Additionally, further research is essential to develop comprehensive guidelines, and concrete strategies to prevent the occurrence of D&I-related AI incidents. Therefore, comprehensive guidelines are also necessary to embed D&I principles into AI's design, development, and deployment."}]}