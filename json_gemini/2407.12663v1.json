{"title": "Is That Rain? Understanding Effects on Visual Odometry Performance for Autonomous UAVs and Efficient DNN-based Rain Classification at the Edge", "authors": ["Andrea Albanese", "Yanran Wang", "Davide Brunelli", "David Boyle"], "abstract": "The development of safe and reliable autonomous unmanned aerial vehicles relies on the ability of the system to recognise and adapt to changes in the local environment based on sensor inputs. State-of-the-art local tracking and trajectory plan- ning are typically performed using camera sensor input to the flight control algorithm, but the extent to which environmental disturbances like rain affect the performance of these systems is largely unknown. In this paper, we first describe the development of an open dataset comprising ~335k images to examine these effects for seven different classes of precipitation conditions and show that a worst-case average tracking error of 1.5 m is possible for a state-of-the-art visual odometry system (VINS-Fusion). We then use the dataset to train a set of deep neural network models suited to mobile and constrained deployment scenarios to determine the extent to which it may be possible to efficiently and accurately classify these 'rainy' conditions. The most lightweight of these models (MobileNetV3 small) can achieve an accuracy of 90% with a memory footprint of just 1.28 MB and a frame rate of 93 FPS, which is suitable for deployment in resource- constrained and latency-sensitive systems. We demonstrate a classification latency in the order of milliseconds using typical flight computer hardware. Accordingly, such a model can feed into the disturbance estimation component of an autonomous flight controller. In addition, data from unmanned aerial vehicles with the ability to accurately determine environmental conditions in real time may contribute to developing more granular timely localised weather forecasting.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous Unmanned Aerial Vehicles (UAVs) are set to become central to a variety of industrial applications ranging from first response to infrastructure communications to deliveries, among myriad other UAV-based Internet of Things (IoT) services [1]. In each case, UAVs will require the ability to safely navigate under variable weather conditions. Although much recent research attention has been paid to autonomously navigating complex environments characterised by the presence of obstacles and dynamic disturbances from airflow, little effort has been afforded to understanding the effects of other dynamic environmental factors - particularly rain. Visual odometry (VO) and visual inertial odometry (VIO) leveraging depth cameras are one of the most promising methods to achieve autonomous navigation [2], [3], however, little attention has been paid to understanding the effects of rainy conditions in terms of tracking errors that might be expected when the camera lens is exposed to various rainy conditions.\nRainfall can be expected to disrupt the visual scene by alter- ing image contrast, introducing blurring effects, and potentially obscuring visual landmarks or obstacles due to water droplets on the camera lens. These factors can reasonably be expected to lead to significant degradation in VO performance, resulting in inaccurate position and motion estimation for the UAV. In worst-case scenarios, inaccurate navigation due to rain could lead to mission failure, collisions, and other safety hazards. Thus, developing methods to identify and estimate the severity of rain conditions impacting VO accuracy is critical to ensure safe and reliable UAV navigation in variable weather.\nThe most relevant contributions in the literature concerning this or similar problems have emerged from the autonomous driving point of view [4]\u2013[6]. In these cases, the images used in the analysis and estimation of the environmental conditions are taken from inside the cockpit; therefore, the water droplets are on the windshield and not directly on the camera lens. On the other hand, such contributions cannot be directly applied to developing reliable autonomous UAV applications because, in such scenarios, the UAVs navigating in rainy conditions may often have the camera lens directly exposed to the environment. As a result, we are motivated to study and analyse the effects of rain on a VO system suitable for autonomous UAVs with direct lens exposure. Given the absence of a suitable relevant dataset, we designed a set of laboratory experiments to simulate a flight at a low altitude (i.e., a scene with objects) under a variety of rainy conditions to collect and label with a view to identifying and classifying precipitation conditions in real time.\nMoreover, many authors have successfully used deep learn- ing (DL) or deep neural networks (DNN) to predict and esti- mate rain severity in vehicles [7], [8]. This serves as inspiration to leverage such algorithms to estimate rain conditions in order to improve our system's performance and reliability. However, autonomous vehicles can have relatively large computational resources, while small drones (i.e., typically carrying a payload up to 2 kg) have necessarily limited computational resources considering size and payload capacity. Thus, when designing a DL-based system, we must remain aware of the available onboard resources.\nThis paper presents a first step towards the development of lightweight models that can determine precipitation con- ditions in real time, and which are suitable for use in future"}, {"title": "II. RELATED WORKS", "content": "UAV deployment is increasing rapidly thanks to commercial devices that are easily accessible to professionals, researchers and amateur enthusiasts alike. Despite recreational use, UAVs are excellent tools to support a variety of industrial application scenarios. In future, autonomous UAVs are likely to be able to navigate in a coordinated 'swarm' where each UAVs is a node or agent of an IoT system [10]. In this setting, they can collaborate to exchange data, obtain a more accurate data collection, and efficiently complete critical mission tasks [11], [12]. They offer advantages in extreme environments where human intervention may be hazardous. Moreover, they avoid the need for a specifically trained and certified pilot to control them on an individual basis, consequently, increasing their reli- ability and opening their usage to many applications [13]\u2013[15].\nA major constraint, however, continues to be the requirement to operate safely in highly dynamic outdoor environments mostly affected by weather. Most contemporary UAV systems cannot fly in all weather conditions, limiting their usage for time-limited missions (e.g., search and rescue), and under the control of human pilots.\nThe authors in [16] have studied UAV \u201cflyability\u201d, which is \"the proportion of time drones can fly safely\". On average, a common drone has a 'flyability' lower than 5.7 h/day (or 2.0 h/day considering only daylight hours). However, this estimate does not consider all weather conditions, especially extreme ones, such as slanting heavy rain or high-speed wind. This analysis suggests increasing the drone's weather resistance to improve its flyability. For instance, a weather-resistant drone may increase its flyability to 20.4 h/day (or 12.3 h/day considering only daylight hours). This research confirms the fundamental role of weather in drone navigation. However, it does not take into account the drone's autonomous navigation, thus the perturbation of the sensing and navigation systems involved in this technology.\nResearchers are studying autonomous navigation systems for UAVs with deep-reinforcement learning to increase their reliability as components of Internet of Things systems [17], [18]. However, there is a knowledge gap and missing contri- butions that demonstrate the effects of variable weather con- ditions on these autonomous UAV systems. Many researchers have begun to study these effects from an autonomous driving point of view. Accordingly, the sensing systems and data involved are tailored for autonomous vehicles, and are thus not directly applicable to small autonomous UAVs. For instance, datasets of images available to the research community in the context of autonomous vehicles are taken from within the vehicle cockpit and looking out through the windshield. This is a setting that may not be similar for UAVs [19], [20], where camera lenses are often directly exposed to the environment. Nonetheless, these works show the effect of adverse weather conditions in autonomous vehicles, suggesting clearly that similar conditions will be present and affect autonomous UAVs. We therefore expect that this can be studied and addressed by adopting similar methodologies [21]\u2013[23].\nAdverse weather conditions comprise perturbed situations that are caused by, e.g., wind, rain, snow, fog, and flares. For"}, {"title": "III. EXPERIMENTAL SETUP", "content": "Our initial objective is to determine the extent to which various rain conditions affect the performance of a state- of-the-art VO system used for autonomous local trajectory tracking and generation. We make the assumption that depth perception (or other) cameras mounted on UAVs are likely to have the camera lens directly exposed to the environment. This follows the majority of related literature on autonomous UAV systems that leverage VO and VIO for trajectory tracking and generation. As such, in addition to specifying the sensor and computer architecture (Sec. III-A), some mechanical design to ensure ingress protection against moisture is a prerequisite (Sec. III-B).\nA. Hardware Specification\nThe key sensor and computer hardware underpinning the VO system comprise a processing unit, i.e., an Intel NUC 11 , and a depth camera, i.e., an Intel Real Sense D435i. These are typical components in use among researchers developing autonomous UAV systems leveraging visual odomoetry [25], [33].\nB. Mechanical Design for Moisture Protection\nGiven that the electronics may become damaged by expo- sure to water, we designed and fabricated a water-resistant box to enclose and protect them, as shown in Figure 1. The dimensions of the box are 20 \u00d7 15 \u00d7 10 cm (length, width, height), and so it can easily host the processing unit, the depth camera, and the necessary cables. At the back of the box, there is a lid that permits the insertion and removal of the electronic devices; moreover, an IP68 nylon gland connects the device power supply to an external power source to further ensure water resistance. The manufacturing process has been conducted using a laser cutting machine to cut the box faces made of polymethyl methacrylate. Then, we composed the box with bi-component specific glue, silicon, and rubber seals to enhance water ingress protection.\nC. Visual Odometry Algorithm\nThe processing unit is programmed with the \u201cVINS- Fusion\u201d algorithm described in [33]\u2013[36]. It consists of an optimization-based multi-sensor state estimator that runs ac- curate simultaneous localization and mapping (SLAM) for autonomous navigation applications. The authors of VINS- Fusion developed the platform to support a variety of visual- inertial sensor types including mono camera and IMU, stereo cameras and IMU, and stereo cameras only. We specifically use the algorithm with stereo cameras only running on Intel Real Sense D435i. In this way, the inertial contribution is avoided, and we focus directly on the visual odometry.\nD. Experimental Environment and Settings\nThe experiments were conducted in a controlled indoor laboratory, which is representative of a challenging and high- entropy scenario. Experiments were designed for two different navigation conditions, namely static and moving. In the static condition, the VO system is stationary in all axes and simulates a drone hovering. In the moving condition, the VO system follows a rectangular trajectory of size 140 \u00d7 160 cm with a fixed altitude. The constant height allows the simulation of the drone's navigation when it reaches the desired altitude. In this experiment, a specifically trained user moves the VO system over a stool to ensure a constant velocity of around 0.2 m/s (shown in Fig. 1). We use a particularly low velocity to exclude its contribution to the analysis, thus focusing only on the rain effect and perturbation in the navigation"}, {"title": "IV. DATASET AND DNN DEVELOPMENT", "content": "A. Dataset\nDuring the experiments presented in Section III, we col- lected raw color images to construct a dataset representing the different rain conditions analysed in this study following [38]. This data can be useful to develop a classification system that can understand the external condition and then act accordingly. The dataset consists of 7 classes of 48k images per class, namely \"Clear\", \"Slanting Heavy Rain\u201d, \u201cVertical Heavy Rain\", \"Slanting Medium Rain\", \"Vertical Medium Rain\", \"Slanting Low Rain\", and \"Vertical Low Rain\". Figure 2 shows several examples of images from the dataset. Overall, the dataset is composed of around 336k images and is almost 12 GB (compressed) in total, where 80% is used for training, 10% for validation and the remaining 10% is used for testing.\nB. Deep Neural Network Specifications\nThree different DNNs have been selected and trained with the dataset developed in Section IV-A. We use state-of-the-art architectures, namely MobileNetV2 [39] (alpha parameter equal to 0.35), MobileNetV3 small [40] (alpha parameter equal to 0.35), and SqueezeNet [41] as they show an optimal trade-off between performance and computational complexity [42], [43] for similar mobile and/or constrained deployment con- texts. The DNNs are trained on an NVIDIA GeForce RTX 4090 GPU with the following hyperparameters:\n\u2022\n100 epochs\n\u2022 Image shape 224 \u00d7 224 \u00d73"}, {"title": "V. EXPERIMENTAL RESULTS & ANALYSIS", "content": "In the first part of this section, we present the experimental results analysed with the setup presented in Section III. We analyse the experiments in static and moving conditions. First, we use the standard deviation to provide the error of the path estimation of the VO system. In the second, we use the root mean square error (RMSE) to provide, on average, the error on the path estimation of the VO system by using the clear condition as the reference. Moreover, we provide the restoring time for the slanting rain scenario needed to ensure navigation with an error below 30 cm. In the second part, we present the results of the DNN test. In particular, we use the confusion matrix of the 7-class classifier to compute average accuracy, precision, recall, and f1-score. These results are used to identify the best performing DNN.\nA. VO System in Static and Moving Conditions\nThe VO system developed in Section III has been tested in static and moving conditions. For each condition, we evalu- ated different rain intensities and modalities namely slanting heavy rain, vertical heavy rain, slanting medium rain, vertical medium rain, slanting low rain, and vertical low rain (as shown in Table III).\n1) Static: In this experiment, the VO system is completely stationary at the same point for the duration of the trials. Figure 3 shows a comparison of the trajectory estimation and data distribution of the experiments conducted in a static scenario under different rain conditions. Furthermore, Table III summarizes the error computed in terms of standard deviation as the system is fixed and the real position is 0 m in all axes. In this way, it is possible to evaluate the performance degradation starting from the optimal condition (i.e., clear) to the worst-case scenario (i.e., slanting heavy rain). Figures 3a and 3d reveal the severity of the slanting heavy rain scenario confirmed by a considerable drift in Table III. On the other hand, the \"Vertical Low Rain\" scenario is almost comparable to the \"Clear\" one, meaning that the rain perturbation can be negligible.\n2) Moving: In this experiment, we analyse and compare the effect of the different rain conditions of Table III in a moving scenario (i.e., the VO system following a rectangular trajectory). Figure 4 shows a comparison of the trajectory estimation under the different rain conditions broken down depending on the rain intensity (i.e., heavy, medium, and low rain). Furthermore, Table IV summarizes the RMSE computed by using the clear condition as the reference. This analysis confirms the severity of the slanting heavy rain scenario, which introduces an unacceptable drift in all axes, especially in the vertical one (Figure 4a). On the other hand, the vertical rain scenarios are almost comparable and less alarming as they show drift in the order of dozens of centimetres.\nB. DNN Test\nWe test the DNNs developed in Section IV-B by using their confusion matrix. In particular, each architecture is evaluated by considering the precision, recall, and f1-score of each class as shown in Tables V, VI, and VII. Metrics are computed with the test set of the dataset presented in Section IV-A, which consists of 10% of the total dataset size. The three architectures perform well in all classes, especially for the clear and slanting heavy rain scenario. This is because they are the two extremes of the conditions studied, meaning that the system can easily discriminate these two scenarios. However, the \"Vertical Low Rain\" class presents the lowest recall (highlighted in red in Tables V, VI, and VII), meaning the system recognizes many false negatives. We expected such behaviour as the perturbation introduced by a vertical low rain scenario is very small and almost comparable to the clear scenario (as analysed in Section V-A). Nonetheless, considering UAV autonomous navigation, a false negative in a vertical low rain scenario does not present a hazard during navigation, thus making it acceptable.\nMoreover, we provide the overall accuracy, precision, recall, and f-score in Table VIII. The computational latency of the three classifiers on the Intel NUC 11 is shown in Table IX. The performance of the three DNNs is comparable, ensuring an accuracy, precision, recall, and f1-score over 90%. On the other hand, MobileNet V3 Small presents the lowest memory footprint (Table II) and the lowest classification latency of around 10 ms (Table IX); thus it is preferable to implement a stable, real-time, and reliable on-the-edge solution in small- size UAVs."}, {"title": "VI. CONCLUSION", "content": "Autonomous UAV navigation based on VO systems operat- ing under variable or adverse weather conditions is an under- studied topic in the literature. Given that UAV employment is increasing rapidly in IoT contexts, it is fundamental to develop solutions to mitigate potentially hazardous situations (e.g., navigation in the rain) and increase drone 'flyability'. In this paper, we investigated the behaviour of a VO system under different rainy conditions that might be encountered during autonomous UAV navigation. Intuitively, the analysis determined that the worst-case scenario is slanting heavy rain, which was demonstrated to introduce an unacceptable error (i.e., from 1 m to 2.5 m) in the path estimation, thereby making the navigation dangerous and unreliable. On the other hand, the other \"slanting rain\u201d scenarios (i.e., slanting medium rain and slanting low rain) present a higher error compared with the vertical rain conditions, meaning that they have to be evaluated depending on the application requirements.\nWe demonstrate a basis for the development of solutions to mitigate the effects of rain and make the autonomous UAV systems more 'aware' of dynamic in situ environmental conditions. Having trained and compared models based on three candidate DNN architectures, we have demonstrated that an accuracy of around 90% can be achieved in classifying the colour images used by the VO system across 7 classes: clear, slanting heavy rain, vertical heavy rain, slanting medium rain, vertical medium rain, slanting low rain, and vertical low rain. The best-performing architecture has reached a frame rate of 97 FPS, achieving approximately real-time processing.\nAccordingly, it is possible to use this information and incorporate these techniques in the development of disturbance estimation approaches feeding into online flight controllers that may allow for specific counteractions to be taken by the UAV (e.g., switch to an alternative navigation system, change the navigation path, land, etc.) depending on the environmental condition. This may permit performance improvements for navigation in otherwise hazardous situations, e.g., by avoiding collisions and increasing the UAV's reliability and flyability. Taking an 'Internet of Things' perspective, the ability of a UAV to determine the local environmental conditions in real time may also be of significant importance to the development of more accurate and timely localised weather forecasting methods through the provision of this data. This and the integration of precipitation-based disturbance estimation with autonomous UAV tracking and trajectory control systems are left for future work."}]}