{"title": "ScratchEval: Are GPT-40 Smarter than My Child?\nEvaluating Large Multimodal Models with Visual Programming Challenges", "authors": ["Rao Fu", "Ziyang Luo", "Hongzhan Lin", "Zhen Ye", "Jing Ma"], "abstract": "Recent advancements in large multimodal mod-\nels (LMMs) have showcased impressive code\ngeneration capabilities, primarily evaluated\nthrough image-to-code benchmarks. However,\nthese benchmarks are limited to specific visual\nprogramming scenarios where the logic reason-\ning and the multimodal understanding capaci-\nties are split apart. To fill this gap, we propose\nScratchEval, a novel benchmark designed to\nevaluate the visual programming reasoning abil-\nity of LMMs. ScratchEval is based on Scratch,\na block-based visual programming language\nwidely used in children's programming educa-\ntion. By integrating visual elements and em-\nbedded programming logic, ScratchEval re-\nquires the model to process both visual infor-\nmation and code structure, thereby comprehen-\nsively evaluating its programming intent un-\nderstanding ability. Our evaluation approach\ngoes beyond the traditional image-to-code map-\nping and focuses on unified logical thinking\nand problem-solving abilities, providing a more\ncomprehensive and challenging framework for\nevaluating the visual programming ability of\nLMMs. ScratchEval not only fills the gap\nin existing evaluation methods, but also pro-\nvides new insights for the future development\nof LMMs in the field of visual programming.\nOur benchmark can be accessed at https:\n//github.com/HKBUNLP/ScratchEval.", "sections": [{"title": "1 Introduction", "content": "Recently, Large Multimodal Models (LMMs) such\nas GPT-40 (OpenAI, 2023), Gemini (Anil et al.,\n2023), and Claude (Anthropic, 2023) have shown\nremarkable capabilities in multimodal understand-\ning (Chen et al., 2024a; Lin et al., 2024; Wang et al.,\n2024b; Luo et al., 2024; Yu et al., 2024). To assess\ntheir abilities, several comprehensive benchmarks\nhave been introduced, including MMMU (Yue"}, {"title": "2 ScratchEval", "content": "All our data is manually collected and cleaned by\nexperts from public question banks on the web.\nWe organized the data into 305 multiple-choice\nquestions, each with a problem description, options,\nand a picture containing the Scratch script and other\nnecessary information.\nOur test benchmark consists of two components:\nChinese and English data. Both sections are identi-\ncal in quantity and content, but the questions and\nScratch script images are in their respective lan-"}, {"title": "2.1 Data analysis", "content": "Based on the content of the questions, we catego-\nrized them into four domains: mathematics, logical\nthinking, graphic perception, and spatial percep-\ntion. The specific distribution of questions across\nthese categories is presented in Table 1. It is impor-\ntant to note that some questions evaluate multiple\nabilities, and therefore, each question is assigned\nto at most two categories. The characteristics of\neach category are as follows:\nMathematics tasks encompass simple arith-\nmetic problems typically encountered in elemen-\ntary and junior high school curricula. These tasks\nassess the model's ability to solve basic mathemati-\ncal problems.\nLogical thinking tasks evaluate the model's\ncapacity for logical reasoning based on provided\nScratch scripts. These scripts are designed for\nchildren and are generally comprehensible even\nto those unfamiliar with the Scratch programming\nenvironment.\nGraphic perception tasks examine the model's\nunderstanding of graphics. These may involve se-\nlecting graphics that correspond to a given script or\ninferring the output of a simple drawing program.\nSpatial perception tasks assess the model's abil-\nity to determine the final position and orientation\nof a character based on a movement program.\nThis categorization enables thorough assessment\nof models' visual code reasoning abilities across\ncognitive domains."}, {"title": "2.2 Evaluation Methodology", "content": "The evaluation process consists of three stages: 1)\ngenerating answers, 2) extracting answers, and 3)\ncalculating scores.\nFirst, the tested LMM generates answers based\non the input query, which includes questions, op-\ntions, image data, and a system prompt. After our\nexperiments, the system prompt we set can help\nus greatly simplify the output of the model. Fi-\nnally, the extracted answers are normalized to the\nrequired answer format option letters, and the tar-\nget metric score is calculated. Using the fact that\nthe examples in ScratchEval are multiple-choice\nquestions with text answers, the accuracy score is\nused as a metric for deterministic evaluation."}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Experiment setup", "content": "We evaluate a total of 10 LMMs on ScratchEval\nunder two setups: (a) Closed-source LMMs, in-\ncluding Gemini-1.5-Pro (Reid et al., 2024), GPT-4-\nTurbo (Achiam et al., 2023), GPT-40, and Claude-\n3.5-Sonnet; (b) Open-source LMMs, including\nQwen2-VL (Wang et al., 2024a), LLaVA-v1.6 (Liu\net al., 2024), InternVL2 (Chen et al., 2024b), Pix-\ntral (Agrawal et al., 2024), MiniCPM-v2.6 (Yao\net al., 2024) and Molmo (Deitke et al., 2024). We\nuse the accuracy as the evaluation metric. We pro-\nvide implementation details in the Appendix \u00a7A.1."}, {"title": "3.2 Experiment analysis", "content": "We evaluated the performance of 10 state-of-the-\nart LMMs by drawing the practice of the LM-\nSYS Chatbot Arena leaderboard on our proposed\nScratchEval benchmark, incorporating both Chi-\nnese and English data. The experimental results on\nEnglish data are presented in Table 2. To conduct\na detailed analysis of the LMMs' capabilities, we\ncategorized the questions into four domains: math-\nematics, logical thinking, graphic perception, and\nspatial perception.\nThe results reveal significant performance varia-\ntions across models in each category, with most\nmodels surpassing the 25% random guessing\nthreshold. This indicates that LMMs possess some\nvisual code reasoning capabilities, enabling them\nto process visual information alongside language\ncomprehension.\nGemini-1.5-Pro demonstrated superior perfor-\nmance, achieving the highest scores across all cate-\ngories. However, most other models struggled to"}, {"title": "3.3 Prompting strategies study", "content": "We investigated the impact of prompt engineering\non the visual code reasoning capabilities of models\nusing our test benchmark. Previous studies, such\nas COT (Wei et al., 2023), have shown that ap-\npropriate prompting can enhance the performance\nof large language models. However, its effective-\nness for multimodal large language models remains\nunderexplored. To address this, we selected four\nmodels and applied three prompting strategies to\nexamine their influence on reasoning abilities.\nThe prompting strategies employed were: (1)\nOriginal prompt (\"no-CoT\"): using raw data as\nprompts. (2) zero-shot CoT (\"CoT\"): Chain of\nThought prompting, appending \"Let's think step\nby step.\" to each question for more comprehensive\nanalysis. (3) eCoT: Inspired by (Ghosal et al.,\n2024), we implemented eCoT, which requires a\ndetailed examination during the CoT process by\nappending \"Let's explain the picture and think step\nby step.\" to each question.\nWe found that CoT and eCoT techniques signifi-\ncantly enhanced the models' visual code reasoning\ncapabilities, with CoT prompting improving per-\nformance by 10% to 20%. However, no model\nachieved overall accuracy exceeding 70%, indicat-\ning substantial room for improvement. Addition-\nally, eCoT yielded relatively minor improvements\ncompared to CoT, suggesting that describing the\nimage may hinder the model's visual code reason-\ning capabilities. Detailed experimental data can be\nfound in the Appendix \u00a7A.5"}, {"title": "3.4 Case study", "content": "To better understand the model's behavior, we\nselected several examples where Gemini-1.5-\nPro made mistakes for a case study. Overall,\nGemini-1.5-Pro is the best-performing model in\nScratchEval. By studying its behavior, we aim to\nexplain why ScratchEval is challenging for most\nmodels.\nWe chose representative examples for Gemini-\n1.5-Pro's case study, as shown in Figure 3. We\nspecifically selected examples that failed across all\nthree prompting strategies mentioned earlier, allow-\ning us to observe Gemini-1.5-Pro's deficiencies in\ncertain areas.\nAs shown in Figure 3, Gemini-1.5-Pro with CoT\naccurately identified image content but hallucinated\nduring reasoning. With eCoT, it described the im-\nage but misinterpreted symbols, leading to incor-\nrect inferences.\nThese cases reveal that while Gemini-1.5-Pro\nexcels in reasoning and basic math/logic problems,"}, {"title": "4 Conclusion", "content": "In this work, we present ScratchEval, a benchmark\nthat uses the Scratch language to systematically\nevaluate the visual programming capabilities of\nstate-of-the-art LMMs. Our evaluation of 10 repre-\nsentative LMMs indicates that while these models\nshow some visual comprehension, they struggle"}, {"title": "Limitations", "content": "Although our proposed ScratchEval helps us to\nevaluate the visual reasoning ability of existing\nLMMs, we recognize that our work still has several\nimportant limitations: (1) Due to the difficulty of\nLMMs to directly operate graphical programming\nlanguages, in order to use graphical programming\nto examine the model's visual programming abili-\nties, we model the problem as Multiple choice ques-\ntions. (2) the narrow domain focus of our bench-\nmark, concentrating solely on visual programming\nabilities, limits the generalizability of our findings.\nThe results obtained cannot be extrapolated to as-\nsess other competencies of LMMs. These limita-\ntions underscore the need for continued research\nand development of more comprehensive evalua-\ntion methodologies for large multimodal models."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Experiments setup", "content": "In our study, we conducted comprehensive eval-\nuations of 10 state-of-the-art Large Multimodal\nModels (LMMs) on the ScratchEval benchmark.\nThe following models were included in our experi-\nments:\n\u2022 Gemini-1.5-Pro-Exp-0827\n\u2022 GPT-40-2024-05-13\n\u2022 Claude 3.5 Sonnet\n\u2022 GPT-4-Turbo-2024-04-09\n\u2022 Qwen2-VL-72b-Instruct\n\u2022 InternVL2-26b\n\u2022 LLaVA-v1.6-34B\n\u2022 MiniCPM-V 2_6\n\u2022 Pixtral-12b-2409\n\u2022 Molmo-7B-D-0924\nAll models were evaluated using their respective\nlatest versions available at the time of the experi-\nment. To ensure consistency and reproducibility\nacross all tests, we maintained a constant temper-\nature setting of 0 for all models. This setting was\nchosen to produce deterministic outputs and facili-\ntate direct comparisons between models.\nFor each model, depending on the task being per-\nformed, we use specific system prompts to explain\nthe next task to the model.These system prompts\nare as follows:\n\u2022 For no-CoT tasks: \"According to the dis-\nplayed Scratch script and the given question,\nplease choose a correct answer from the four\noptions ABCD. You only need to find the cor-\nrect option, and no analysis is required. \"\n\u2022 For CoT tasks: \"According to the displayed\nScratch script and the given question, please\nchoose a correct answer from the four options\nABCD. \"\n\u2022 For eCoT tasks: \"According to the displayed\nScratch script and the given question, please\nchoose a correct answer from the four options\nABCD. \"\nThe system prompts when executing Chinese\ntasks are the translations of the above correspond-\ning tasks."}, {"title": "A.2 Chinese data experiments", "content": "In Table 3, We can see that the performance of most\nmodels is basically the same as in the English task,\nwhile some models perform better. We believe this\nis because some models use more Chinese data\nduring training."}, {"title": "A.3 Data example", "content": "In Figure 6, Figure 7, Figure 8 and Figure 9,\nwe show data for mathematics, logical thinking,\ngraphic perception, and Spatial perception as ex-\namples. Each example includes the corresponding\nChinese and English scripts, questions, and correct\nanswers."}, {"title": "A.4 Prompt strategie study data", "content": "In Figure 5, we provide more data on the model per-\nformance under different prompt strategies, which\nare also consistent with the views we put forward\nin the main text."}, {"title": "A.5 Examples in case study", "content": "In Figure 4, We show two cases where Gemini-\n1.5-Pro makes mistakes, and these two cases also\nillustrate the conclusions we stated in the main text."}, {"title": "A.6 Potential Risks", "content": "While our benchmark for LMMs, which evaluates\nmodels using Scratch visual programming ques-\ntions, poses no direct risks, potential concerns in-\nclude the possibility of models overfitting to spe-\ncific visual programming patterns, reducing their\ngeneralization capabilities. Additionally, the re-\nliance on Scratch could limit the applicability of\nresults to broader real-world tasks that use different\nprogramming interfaces."}, {"title": "A.7 Creators Of Artifacts", "content": "The source data for our benchmark is derived from\nthe China Lanqiao Cup National Software and In-\nformation Technology Professional Talent Compe-\ntition https://www.lanqiaoqingshao.cn/home\n(Chinese website). To adapt this data for our bench-\nmark, we enlisted the help of domain experts to\nreannotate and refine the original dataset, ensur-\ning its suitability for evaluating LMMs on Scratch\nvisual programming tasks."}, {"title": "A.8 License", "content": "The benchmark was annotated and developed by\nthe authors of this paper, and the dataset is released\nunder the Apache 2.0 license."}, {"title": "A.9 Use Of AI Assistants", "content": "The AI assistant, GPT-4o, was used solely to en-\nhance the writing of this paper."}]}