{"title": "Hymba: A Hybrid-head Architecture for Small Language Models", "authors": ["Xin Dong", "Yonggan Fu", "Shizhe Diao", "Wonmin Byeon", "Zijia Chen", "Ameya Sunil Mahabaleshwarkar", "Shih-Yang Liu", "Matthijs Van Keirsbilck", "Min-Hung Chen", "Yoshi Suhara", "Yingyan Celine Lin", "Jan Kautz", "Pavlo Molchanov"], "abstract": "We propose Hymba, a family of small language models featuring a hybrid-head parallel architecture that integrates transformer attention mechanisms with state space models (SSMs) for enhanced efficiency. Attention heads provide high-resolution recall, while SSM heads enable efficient context summarization. Additionally, we introduce learnable meta tokens that are prepended to prompts, storing critical information and alleviating the \"forced-to-attend\" burden associated with attention mechanisms. This model is further optimized by incorporating cross-layer key-value (KV) sharing and partial sliding window attention, resulting in a compact cache size. During development, we conducted a controlled study comparing various architectures under identical settings and observed significant advantages of our proposed architecture. Notably, Hymba achieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model surpasses all sub-2B public models in performance and even outperforms Llama-3.2-3B with 1.32% higher average accuracy, an 11.67\u00d7 cache size reduction, and 3.49\u00d7 throughput.", "sections": [{"title": "1. Introduction", "content": "Transformers, with their attention-based architecture, have become the dominant choice for language models (LMs) due to their strong performance, parallelization capabilities, and long-term recall through key-value (KV) caches [1]. However, their quadratic computational cost and high memory demands pose efficiency challenges. In contrast, state space models (SSMs) like Mamba [2] and Mamba-2 [3] offer constant complexity and efficient hardware optimization but struggle with memory recall tasks, affecting their performance on general benchmarks [4, 5]. While existing hybrid models that stack attention and SSM layers have demonstrated potential [6, 7], they can introduce bottlenecks when one layer type is not well-suited for specific tasks, requiring compensation from subsequent layers.\nWe propose Hymba, a novel LM architecture that integrates attention heads and SSM heads within the same layer, offering parallel and complementary processing of the same inputs. This hybrid-head approach allows each layer to simultaneously harness both the high-resolution recall of attention and the efficient context summarization of SSMs, increasing the model's flexibility and expressiveness in handling various types of information flows and memory access patterns.\nTo further enhance the achievable performance of Hymba, we introduce learnable meta tokens that are prepended to the input sequences and interact with all subsequent tokens even in sliding window attention. These meta tokens appear to act as a compressed representation of world knowledge and alleviate the issue of \"softmax attention not being able to attend to nothing\" [8, 9, 10], improving performance across both general and recall-intensive tasks.\nSharing KV cache between attention heads is common practice. Inspired by findings in [11] that consecutive layers have a high correlation in the KV cache, we propose sharing the KV cache between layers as well. Additionally, for most layers, we choose sliding window attention to further minimize cache costs.\nComprehensive evaluations and ablation studies demonstrate that Hymba not only establishes new state-of-the-art (SOTA) benchmark performance across a wide range of representative tasks but also achieves greater efficiency compared to transformers and previous hybrid models. We provide the benchmark with other representative small LMs in Fig. 2, with more comprehensive benchmarks in Fig. 9. For instance, in commonsense reasoning tasks, Hymba-1.5B can outperform Llama-3.2-3B with 1.32% higher average accuracy, while requiring 11.67\u00d7 smaller cache size and being 3.49\u00d7 faster.\nTo optimize Hymba for on-device tasks, we employ supervised finetuning and direct preference optimization [12]. Our instruction-tuned model, Hymba-1.5B-Instruct, achieves best-in-class performance on GSM8K, GPQA, and the Berkeley function-calling leaderboard, surpassing Llama-3.2-1B. Additionally, parameter-efficient finetuning shows Hymba's strong potential in this setting. For instance, a DoRA [13]-finetuned version of Hymba-1.5B outperforms Llama3.1-8B-Instruct by 2.4% on RoleBench [14]."}, {"title": "2. Hymba: The Proposed Hybrid-Head Architecture", "content": "SSMs such as Mamba [2] were introduced to address the quadratic complexity and large inference-time KV cache issues of transformers. However, due to their low-resolution memory, SSMs struggle with memory recall and performance [4, 15, 5]. To overcome these limitations, we propose a roadmap for developing efficient and high-performing small LMs in Tab. 1 and outlined as follows:\nFused hybrid modules. Fusing attention and SSM heads in parallel within a hybrid-head module outperforms sequential stacking (Tab. 1 (A)-(B)). Both heads process the same information simultaneously, leading to improved reasoning and recall accuracy. We argue that sequential fusion lacks synergy, as both blocks operate on each set of inputs independently.\nEfficiency and KV cache optimization. While attention heads improve task performance, they increase KV cache requirements and reduce throughput. To mitigate this, we optimize the hybrid-head module by combining local and global attention and employing cross-layer KV cache sharing, as shown in Tab. 1 (C) and (D). This improves throughput by 3\u00d7 and reduces cache by almost 4x.\nMeta Tokens - A set of 128 pretrained embeddings prepended to inputs, functioning as learned cache initialization to enhance focus on relevant information. These tokens serve a dual purpose: (i) they mitigate attention drain by acting as backstop tokens, redistributing attention effectively, and (ii) they encapsulate compressed world knowledge, see Tab. 1 (E) and Sec. 2.3.\nScaling - Ablation studies were performed on a 300M parameter model using 100B training tokens; the final models were trained with 1.5T tokens and scaled up to models with 350M and 1.5B parameters (see Tab. 1 (F))."}, {"title": "2.1. A Fused Hybrid-Head Module", "content": "SSM models are efficient but suffer from limited recall capabilities and task performance [4, 15, 5, 16] as seen in Tab. 1. Given the high recall resolution of attention, in this step we aim to (1) combine the processing efficiency and context summarization capabilities of SSMs with the high recall resolution of attention, and (2) develop a fused building block to achieve this goal, so it can serve as a fundamental component for constructing future foundation models.\nPrevious hybrid models [7, 17, 6] often combine attention and SSMs in a sequential manner. This strategy may lead to information bottlenecks when a layer type that is poorly suited for a specific task cannot effectively process the information. Motivated by the multi-head attention structure in the vanilla Transformer [1], where different heads undertake different roles and focus on different contexts [18, 19], we propose an alternative approach: fusing attention and SSMs in parallel into a hybrid-head module, as shown in Fig. 1 (a). The advantage of this design is that different attention and SSM heads can store, retrieve, and process the same piece of information in distinct ways, thereby inheriting the strengths of both operators.\nDesign formulation. We show that the hybrid-head module can be represented by a unified and symmetric formulation. As shown in Fig. 1 (a), given the input sequence $X$, which is the original input sequence $X$ prepended with meta tokens introduced in Sec. 2.3, the input projection $W_{in\\_proj} \\in \\mathbb{R}^{(Q,K,V,SSM,G)}$ projects $X$ to the query, key, and value of the attention heads using $W^Q, W^K,$ and $W^V$, respectively, as well as the input features and gates of the SSM heads using $W^{SSM}$ and $W^G$, respectively.\nFollowing [1], the output of attention heads $Y_{attn}$ can be formulated as:\n$Y_{attn} = softmax(Q K^T) W^V X = M_{attn}X$ (1)\nwhere $M_{attn} = softmax(Q K^T) W^V$ and $Q = W^Q X, K = W^K X$.\nSimilar to the attention heads, the SSM heads in our model, for which we adopt Mamba [2], can also be represented using a data-controlled linear operator $M_{ssm}$, following [20, 16]. Specifically, the SSM head output $Y_{ssm}$ can be formulated as:\n$a = C_i \\odot (\\prod_{k=j+1}^i exp(A \\Delta_k)) B \\Delta_j$,\n$Y_{ssm} = G \\odot \\alpha(A, B, C, \\Delta) W^{SSM} X = M_{ssm}X$,\nwhere $M_{ssm} = G \\odot \\alpha(A, B, C, \\Delta) W^{SSM}, G = W^G X$ is an output gate, and $A, B, C, \\Delta$ are the SSM parameters following the definition in [2]. More specifically, A is a learnable matrix, $B = W^B X_{ssm}, C = W^C X_{ssm}$, and $\\Delta = Softplus(W_{\\Delta} X_{ssm})$ with $X_{ssm} = W^{SSM} \\tilde{X}$.\nWe observed that the output magnitudes of the SSM heads, $Y_{ssm}$, are consistently larger than those of the attention heads, $Y_{attn}$, as visualized in Fig. 12 in Append. B. To ensure effective fusion, we normalize and re-scale them using learnable vectors to improve training stability, and then average the outputs, followed by a final output projection. The overall formulation of our fused module can be represented symmetrically:\n$Y = W_{out\\_proj} (\\beta_1 norm(M_{attn} X) + \\beta_2 norm(M_{ssm} X))$ (3)\nwhere $\\beta_1$ and $\\beta_2$ are learnable vectors that re-scale each channel of the outputs from the attention and SSM heads, respectively. We further explore the optimal ratio of SSMs and attention in hybrid heads, along with their fusion strategy, in Append. B.\nInterpretation from the memory aspect. The components in the hybrid-head module can be interpreted as analogous to human brain functions. Specifically, as shown in Fig. 1 (b), the attention heads"}, {"title": "2.2. KV Cache Optimization", "content": "Our hybrid-head module improves recall and reasoning capabilities but can compromise memory and throughput efficiency due to the KV cache required by the attention heads. To address this, we aim to reduce the KV cache while maintaining comparable task performance.\nCombine global and local attention. Local attention, also known as Sliding Window Attention (SWA) [22], offers a more efficient alternative to global full attention, though it risks losing global context. However, with the presence of SSM heads in our hybrid-head module, which already summarize global context, we can more aggressively replace global full attention with local attention, achieving a better balance between efficiency and performance.\nExploring the ratio of local attention and global attention. As shown in Tab. 10 in Append. B, we initially replace global attention in all layers with SWA, which results in a significant degradation in recall capabilities, with accuracy dropping by over 20% on recall-intensive tasks. In response, we progressively reinstate global attention in some layers. Interestingly, as shown in Tab. 1 (C), we find that using global attention in just three layers (i.e., the first, middle, and last layers) is sufficient to recover recall-intensive accuracy while maintaining comparable commonsense reasoning accuracy. In turn, this strategy achieves 2.7\u00d7 throughput and 3.8\u00d7 cache reduction.\nCross-layer KV sharing. Recent works [23] observe that KV cache shares a high similarity between adjacent layers, suggesting that using separate KV caches for each layer leads to both cache and parameter redundancy. In light of this, we employ cross-layer KV sharing [11], where keys and values are shared between consecutive layers (e.g., every two layers share the same KV cache). This strategy reduces both KV memory usage and model parameters, allowing the saved parameters to be reallocated to other model components. As shown in Tab. 1 (D), cross-layer KV sharing improves throughput by 1.15\u00d7 while maintaining comparable recall accuracy and boosting commonsense accuracy by +0.60%.\nAfter the above optimization, Hymba's overall architecture is visualized in Fig. 4."}, {"title": "2.3. Meta Tokens", "content": "We observed that the initial tokens, though not semantically important, often receive significant attention scores from subsequent tokens, similar to observations in prior work [10, 27]. As shown in Fig. 7, more than 50% of the attention is focused on the BOS token for Llama3.2-3B. To address this, we aim to guide the attention to focus more on tokens that meaningfully contribute to task performance. Specifically, we introduce a set of learnable meta tokens $R = [r_1, r_2, ..., r_m]$ to serve as the initial tokens. Given the input sequence $X = [x_1, x_2, ..., x_n]$, these meta tokens are prepended to the input sequence, forming the modified input sequence:\n$\\tilde{X} = [R, X] = [r_1, r_2, ..., r_m, x_1, x_2, ..., x_n]$ (4)\nwhere $\\tilde{X}$ represents the new input sequence for our model. At inference time, since the meta tokens are fixed and appear at the beginning of any input sequences, their computation can be performed offline. Thus, the role of meta tokens at inference can also be viewed as learned cache initialization to modulate the subsequent tokens, allowing subsequent tokens to focus more on those that contribute meaningfully to task performance.\nInterpretation from the memory aspect. Similar to the analogy in Sec. 2.1, the meta tokens participate in the attention and SSM calculations of all subsequent tokens, analogous to metamemory in the human brain, which helps recognize where to locate needed information in other memories. To see this, we visualize the averaged attention scores received by the meta tokens in Fig. 5 for a Hymba-1.5B model. We observe that when the prompts are from different domains (e.g., article, math, and codes), different meta tokens are activated. This suggests that different meta tokens encapsulate different world knowledge, which can be leveraged to guide the attention mechanism to focus on relevant information. We further analyze others roles of meta tokens and their connections with related works in Append. D.\nThe role of Meta Tokens. We hypothesise, that they perform the following functions. Prevent token overwriting. As shown in [30], attention tends to overwrite and over-attend to some tokens, acting as a garbage collector. Adding learnable tokens allowed for much more representative feature maps. Later, the same phenomenon was discovered in LLMs and named \"attention sinks\" [10, 27]. Therefore, the model should be provided with tokens that are independent of the input.\nExit tokens to deal with \"forced-to-attend\". Prepending tokens to the input affects the shape of the softmax function by modifying the denominator. Quiet Attention [31] modifies the softmax denominator by adding one, allowing the attention to output zeros. Adding one is equivalent to prepending an all-zero token to the keys and values. Our meta tokens take this idea further by being learnable, allowing to learn an optimal softmax shape.\nInitialization for KV cache and SSM state. Learning initial tokens can be seen as a form of learned prompt tuning [32, 33] or learned initialization. For inference, meta tokens are fixed, and the keys and values can be precomputed offline and stored. Task-specific meta tokens can be used, though in this work we use one set for all tasks.\nMeta tokens boost recall capabilities and commonsense reasoning accuracy. To analyze the impact of meta tokens on the attention mechanism, we visualize the entropy of the attention map for both the attention and SSM heads [20, 16] before and after introducing meta tokens. Specifically, the attention map entropy reflects the distribution of attention scores across tokens, where lower entropy indicates stronger retrieval effects [7], as the attention scores are concentrated around a smaller subset of tokens."}, {"title": "2.4. Hymba Attention Map", "content": "Hymba's attention pattern (Fig. 6) can be viewed as a combination of individual components from sliding window attention, meta tokens, and SSM.\nWe further categorize elements in the attention map into four types: (1) \u2018Meta\u2019: attention scores from all real tokens to meta tokens. This category reflects the model's preference for attending to meta tokens. In attention map, they are usually located in the first few columns (e.g., 128 for Hymba) if a model has meta tokens. (2) 'BOS': attention scores from all real tokens to the beginning-of-sequence token. In the attention map, they are usually located in the first column right after the meta tokens. (3) 'Self\u2019: attention scores from all real tokens to themselves. In the attention map, they are usually located in the diagonal line. (4) \u2018Cross\u2019: attention scores from all real tokens to other real tokens. In the attention map, they are usually located in the off-diagonal area.\nIn Fig. 7, we visualize the real attention maps from Llama-3.2-3B and Hymba-1.5B on texts from Oliver Twist Chapter 29 [34] and sum up the attention scores from different categories. The summed scores are normalized by the context length. For SSM heads, we follow Ben-Kish et al. [16] and Zimerman et al. [35] to calculate their attention maps and normalize the attention maps to ensure each row sums to 1.\nWe observe that the attention pattern of Hymba is significantly different from the vanilla Transformers. In vanilla Transformers, attention scores are more concentrated on \u2018BOS\u2019, which is consistent with the findings in [10]. In addition, vanilla Transformers also have a higher proportion of \u2018Self\u2019 attention scores. In Hymba, meta tokens, attention heads and SSM heads work complimentary to each other, leading to a more balanced distribution of attention scores across different types of tokens. Specifically, meta tokens offload the attention scores from \u2018BOS\u2019, allowing the model to focus more on the real tokens. SSM heads summarize the global context, which focus more on current tokens (i.e., \u2018Self\u2019 attention scores). Attention heads, on the other hand, pay less attention to 'Self' and \u2018BOS\u2019 tokens, and more attention to other tokens (i.e., \u2018Cross\u2019 attention scores). This suggests that the hybrid-head design of Hymba can effectively balance the attention distribution across different types of tokens, potentially leading to better performance."}, {"title": "2.5. Hymba Model Family", "content": "Building on the design insights explored above, we scale up the model sizes and training tokens to deliver the Hymba model family, which includes a 125M model, a 350M model, and a 1.5B model.\nWe train Hymba-125M/350M/1.5B models using a mix of DCLM-Baseline-1.0 [36], SmoLM-Corpus [37], and a proprietary high-quality dataset, with 1T, 250B, and 50B tokens, respectively. We combine the Warmup-Stable-Decay (WSD) learning rate scheduler [38], with maximum and minimum learning rates of 3e-3 and 1e-5, and the data annealing technique [39, 40] to ensure stable pretraining. We use a sequence length of 2k and a batch size of 2M tokens throughout the training process until the last 100B tokens, where we increase the sequence length to 8k and change the ROPE base following [41]. The overall training pipeline is illustrated in Fig. 8. More pretraining details are provided in Append. E."}, {"title": "3. Model Evaluations", "content": "3.1. Experiment Settings\nBaselines. Our baselines include popular (small) LMs with quadratic attention (e.g., Llama 3.2 [42], SmolLM [43], SmolLM2 [44], AMD-OLMo [45], StableLM [46], Olmo [47], Cosmo [48], Phi-1.5 [49], H2O-Danube [50], OpenELM [51], and MiniCPM [38]), as well as hybrid models (e.g., Rene [52]).\nBenchmark settings. We adopt two benchmarking settings: (1) In Sec. 3.2, we directly benchmark our delivered Hymba against SOTA public small LMs, and (2) in Sec. 3.3, we train different architectures from scratch with the same dataset, number of layers, model size, and training recipes.\nBenchmark tasks. In addition to evaluating commonsense reasoning and recall-intensive tasks on our base models, we also evaluate our instruction-tuned models on downstream tasks such as math, function calling, and role-playing in Sec. 3.4."}, {"title": "3.2. Benchmark with SOTA Small LMS", "content": "We present the benchmark results of our Hymba models with parameter sizes of 125M, 350M, and 1.5B, compared to SOTA small language models within the same size range.\nAs highlighted in Tab. 2, with only 1.5T pretraining tokens, our Hymba-1.5B model achieves the best performance among all sub-2B LMs and demonstrates better throughput and cache efficiency compared to all transformer-based LMs, with this speedup becoming even more pronounced as the sequence length increases. For instance, compared to the strongest sub-2B baseline, SmolLM2-1.7B, trained on 11T tokens, our Hymba-1.5B, trained on only 1.5T tokens, achieves a 1.02% average accuracy improvement, a 19.91x cache size reduction, and 2.79\u00d7 throughput. When comparing with small LMs trained on no more than 2T tokens, our model achieves a 5.21%/5.41% average accuracy improvement over the most competitive baselines, Phi-1.5 and h2o-danube2-1.8B, respectively. Additionally, our model even outperforms Llama-3.2-3B, with 1.32% higher average accuracy, an 11.67x cache size reduction, and 3.49\u00d7 throughput.\nWe visualize the trade-offs between commonsense reasoning accuracy and cache size/throughput in Fig. 9. In addition, our delivered tiny LMs, Hymba-125M/350M, consistently outperform all LMs of comparable model size, as summarized in Tab. 6 and Tab. 7 in Append. A.1. We have also provided a Hymba-1.5B model trained exclusively on public data in Append. A.2."}, {"title": "3.3. Benchmark Different Architectures Under The Same Setting", "content": "General and recall-intensive tasks performance comparison. We do a comprehensive comparison between Hymba and other model architectures, including standard Transformer (Llama3 [53]), pure Mamba [2, 3], Mamba with FFN and hybrid architecture with sequential layer stacking (Samba [7]) on several downstream tasks. All models have the same number of layers and total parameters to facilitate equal comparison. Models are trained on the same data with the same hyperparameters and under the same codebase. To ensure our conclusions are generally valid, we run comparison experiments at different scales (1B and 300M) and different training datasets (SmolLM-corpus [37] and FineWeb [54]) in Tab. 3 and Tab. 9, respectively. We evaluate the models on language modeling, real-world recall-intensive, commonsense reasoning, and question-answering tasks.\nAs shown in Tab. 3, our Hymba model consistently outperforms other 1B architectures across most tasks, e.g., achieving an average score 1.45% higher than the second-best model at the 300M scale and 1.74% higher at the 1B scale. The ablation study for the 300M scale is in Append. A.\nIn addition, considering that Mamba models suffer from limited recall capabilities due to their constant-size cache and recurrent nature [16, 5, 15], we test the models on two real-world recall-intensive tasks, SWDE [5, 55] and SQUAD [5, 56], where the former is to to extract semi-structured relations from given raw HTML websites and the latter is to extract answers from a given context passages. Echoing the previous findings, Mamba2 and Mamba2 with FFN architectures under-perform the Transformer model (i.e. Llama3) on these tasks (see Tab. 3). Hymba model augments the Mamba heads with attention heads, which allows the model to have a large effective receptive field to establish long-range dependencies and high-resolution memory to store and retrieve key information in all layers. As a result, Hymba outperforms the Transformer and Samba architectures (where the latter stacks Mamba and attention layers sequentially).\nNeedle-in-the-Haystack performance comparison. We further do an apple-to-apple comparison between Hymba, Mamba2, and Llama3 on the synthetic retrieval task, needle-in-the-haystack. A random and informative sentence (i.e., needle) is inserted into a long document (i.e., haystack) and the model is required to retrieve the needle from the haystack to answer the questions. All models are of size 1B and trained with the same setting: (i.) pretrain is done with 1k sequence length; (ii.) finetune with 4k sequence length; (iii.) test with up to 16k sequence length. If model has ROPE, then we adjust the ROPE as in [57] during finetuning. As shown in Fig. 10, the Hymba model significantly outperforms the Mamba2 and Llama3 models. While the Mamba2 model has good extrapolation capabilities when the needle is inserted in the end of the haystack, it struggles to retrieve the needle when the needle is in the beginning or middle of the haystack. In contrast, Llama3 model has limited extrapolation capabilities [58, 57, 59] and struggles to the \u201clost in the middle"}, {"title": "3.4. Instruction-tuned Model", "content": "Implementation details of post-training. We post-trained Hymba-1.5B base model with a two-stage strategy: the first full-finetuning (FFT) stage and another direct preference optimization (DPO) [12] training. The learning rates are 5e-5, and 3e-6 for FFT and DPO, respectively. To accelerate training, we follow the training recipe [61, 62, 63] to pack the samples and use a block size of 8192. We compare Hymba-1.5B-Instruct with competitive lightweight instruction-tuned models, i.e., Llama-3.2-1B-Instruct [42], OpenELM-1-1B-Instruct [51], Qwen2.5-1.5B-Instruct [64], and SmolLM-1.7B-Instruct [43]. We test the instruction-tuned models on MMLU (5-shot), IFEval, GSM8K (5-shot), GPQA (0-shot), and Berkeley Function-Calling Leaderboard v2 (BFCLv2) [65]. More details about the experimental settings, baseline models, and evaluation tasks are shown in Append. E.\nEvaluation results. The evaluation results are shown in Tab. 4. In general, Hymba-1.5B-Instruct achieves the highest performance on an average of all tasks, outperforming the previous SoTA model, Qwen2.5-Instruct, by around 2%. It demonstrates a great ability in math, reasoning, and function calling, with the best-in-class performance.\nEvaluation on role-play tasks. In addition to full finetuning, we conduct experiments to evaluate whether Hymba is compatible with DORA [13], a parameter-efficient finetuning method that updates pretrained models using a minimal set of parameters. This approach is especially well-suited for on-device finetuning scenarios where computational resources are constrained. Additionally, DoRA significantly reduces storage requirements for saving multiple downstream models, as it only requires storing the finetuned DoRA parameters, which constitute less than 10% of the original model's total parameters. Specifically, we further finetune the post-trained Hymba on RoleBench [14] using DoRA to enhance its role-playing capabilities. The training set of RoleBench is used for training, and the model is evaluated on two sub-tasks: instruction generalization (Inst. Gene.) and role generalization (Role. Gene.). As shown in the Tab. 5, our Hymba-DoRA significantly outperforms larger models. For instance, DORA finetuned Hymba achieves scores of 40.0% / 37.9% on instruction generalization/role generalization, outperforming RoleLlama-7B [14] by 4.5%, and 4.4% respectively. This indicates the strong generalization of our model and the effectiveness of using parameter-efficient finetuning techniques to further enhance its performance."}, {"title": "4. Related Works", "content": "Large language models. Prior to the rise of LLMs, transformer-based models [1, 66, 67, 68] proved highly effective at capturing relationships between tokens in complex sequences through the use of the attention mechanism [1]. These models also demonstrated considerable scalability [69, 70, 71] in terms of both model size and the volume of pretraining data. This scalability paved the way for the development of LLMs, such as GLM [72], OPT [73], Mistral [74], the Llama series [75, 53], Gemma [76], and GPT-4 [77], which showcase remarkable zero-shot and few-shot in-context learning abilities.\nEfficient language model architectures. Despite the promise of transformer-based LMs, the quadratic computational complexity and the linearly increasing KV cache size of attention modules with longer sequences limit their processing efficiency. To address this, efficient LMs featuring sub-quadratic complexity in sequence length and strong scaling properties have emerged [78, 79, 2, 3, 80, 81]. As pointed out by [2], popular efficient LM architectures such as RWKV [78] and RetNet [79] can be viewed as variants of SSMs [82, 83]. These models utilize a linear dynamical system approach with a constant-size memory to recurrently encode past information, achieving linear scaling with sequence length. Mamba[2], one of the most widely used SSMs, improves upon previous SSMs by selectively propagating or forgetting information along the sequence length in an input-dependent manner. This approach outperforms transformers on several downstream tasks while offering faster inference. Follow-up works such as Mamba2 [3] and GLA [80] introduce more hardware-friendly gating mechanisms to enhance training throughput over Mamba. However, despite their promise, SSMs have been identified as having limited recall capabilities [4] and underperforming on in-context learning tasks [84].\nHybrid language models. To combine the processing efficiency of SSMs with the recall capabilities of transformers, an emerging trend is the creation of hybrid models that incorporate both types of operators. Specifically, [84] proposes a hybrid model called MambaFormer, which interleaves Mamba and attention modules to improve in-context learning capabilities. Similarly, [4] finds that introducing a small number of attention layers into a Mamba model can significantly enhance both commonsense reasoning and long-context capabilities. Jamba [6] and Zamba [17] develop sequentially stacked Mamba-Attention hybrid models. Jamba further integrates Mixture-of-Experts into the MLP layers, while Zamba employs a shared attention module. Both models demonstrate improvements in inference speed and task accuracy compared to previous transformer-based models of similar size. Samba [7] introduces a structure that sequentially stacks Mamba, SWA, and MLP layers by repeating the Mamba-MLP-SWA-MLP structure, achieving constant throughput as sequence lengths increase. Other recent work has also explored hybrid models that mix either linear RNNs or convolutions with attention [85, 86, 87, 88]. This work proposes a new hybrid model featuring a fused multi-head building block that stacks hybrid operators in parallel. Our model outperforms previous architectures, as demonstrated by extensive benchmarking in Sec. 3."}, {"title": "5. Conclusion", "content": "In this work, we present Hymba, a new family of small LMs featuring a hybrid-head architecture that combines the high-resolution recall capabilities of attention heads with the efficient context summarization of SSM heads. To further optimize the performance of Hymba, we introduce learnable meta tokens, which act as a learned cache for both attention and SSM heads, enhancing the model's focus on salient information. Through the roadmap of Hymba, comprehensive evaluations, and ablation studies, we demonstrate that Hymba sets new SOTA performance across a wide range of tasks, achieving superior results in both accuracy and efficiency. Additionally, our work provides valuable insights into the advantages of hybrid-head architectures, offering a promising direction for future research in efficient LMs."}]}