{"title": "Gradient Based Method for the Fusion of Lattice Quantizers", "authors": ["Liyuan Zhang", "Hanzhong Cao", "Jiaheng Li", "Minyang Yu"], "abstract": "In practical applications, lattice quantizers leverage discrete lattice points to approximate arbitrary points in the lattice. An effective lattice quantizer significantly enhances both the accuracy and efficiency of these approximations. In the context of high-dimensional lattice quantization, previous work proposed utilizing low-dimensional optimal lattice quantizers and addressed the challenge of determining the optimal length ratio in orthogonal splicing. Notably, it was demonstrated that fixed length ratios and orthogonality yield suboptimal results when combining low-dimensional lattices. Building on this foundation, another approach employed gradient descent to identify optimal lattices, which inspired us to explore the use of neural networks to discover matrices that outperform those obtained from orthogonal splicing methods. We propose two novel approaches to tackle this problem: the Household Algorithm and the Matrix Exp Algorithm. Our results indicate that both the Household Algorithm and the Matrix Exp Algorithm achieve improvements in lattice quantizers across dimensions 13, 15, 17 to 19, 21, and 22. Moreover, the Matrix Exp Algorithm demonstrates superior efficacy in high-dimensional settings.", "sections": [{"title": "1. Introduction", "content": "A lattice is defined as a set of linearly independent vectors in Rn. In an n-dimensional space, most points cannot be represented using finite decimal coordinates. However, we can approximate these coordinates by expressing them in terms of lattice points. Specifically, given a lattice formed by n vectors denoted as (a1,a2,..., an), for any point x in this space, we can find a set of integers (z1, z2,..., zn) such that the expression||x \u2212 \u2211i=1 ziai||2is minimized. The tuple (Z1, Z2, ..., Zn) represents an approximate coordinate representation of the point x. Our objective is to select a lattice that minimizes the error associated with this approximation.\n\nThe optimal lattice quantizer is defined as the lattice that achieves the minimum mean square error (MSE). This is equivalent to minimizing the normalized second moment (NSM), which serves as a scale-invariant measure of the mean square error.\n\nLattices have widespread applications across various fields, including digital communications, experimental design, data analysis, and particle physics.\n\nThe structure of the paper is organized as follows: Section II provides the theoretical background and relevant matrix knowledge essential for solving the problem at hand. In Section III, we develop our new algorithm theoretically. Section IV and the appendix describe our experimental setup and present the results, demonstrating that our algorithm is both theoretically sound and practically effective. Section V offers explanation of the advantages and shortcoming of our method. Finally, Section VI concludes our research."}, {"title": "1.1. Related Work", "content": "From the perspective of theory, the paper (Agrell & Allen, 2023) defines the local optimal lattice quantizer by using a method similar to defining the local minimum value of a function. Local optimal lattice quantizer is a lattice quantizer that satisfies the requirement that NSM will not decrease after a lattice matrix is left multiplied by a matrix that is infinitely tending to the identity matrix. On this basis, (Agrell & Allen, 2023) proved that all Voronoi regions of local optimal lattice quantizer satisfy some symmetry, that is, the correlation matrix is a constant multiple of the unit matrix. This proves theoretically that the Voronoi region of the optimal lattice quantizer must have a certain degree of symmetry.\n\nPaper (Agrell et al., 2024b) considers using lower triangular matrix to represent lattice quantizer matrix, using stochastic gradient descent algorithm to optimize lattice NSM, and proposes a powerful tool for converting numeric lattice representations into their underlying exact forms.\n\n(Agrell & Allen, 2023) considers splitting the entire n-dimensional space into several subspace when designing an n-dimensional lattice. The optimal results of these subspace are then orthogonal concatenated. After realizing that orthogonality is the worst allocation method, we decided to use gradient descent to explore non-orthogonal cases.\n\nSpecifically, we referred to the optimal method of constructing an n-dimensional lattice from two low-dimensional lattices, as described in (Agrell & Allen, 2023). According to the formulas in (Agrell & Allen, 2023), given k lattices, denoted as Ai with volume Vi and normalized n-sphere measure (NSM) as Gi, the best orthogonal concatenation \u03b11A1\u2295\u03b12A2\u2295\u00b7\u00b7\u00b7\u2295\u03b1\u03baAk must satisfy:\n\nAi =\nC\nVGiVi\nwhere C is a constant.\n\nSince K12 is used in dimensions 13, 14, 15, and A16 is used in dimensions 17 to 22, we fixed the coefficients of these two matrices to 1, manually computed the coefficients ai for the other matrices, and obtained the best matrices under orthogonal concatenation."}, {"title": "1.2. Innovation of our work", "content": "We propose a gradient fusion method for low-dimensional lattice quantizers, leveraging the properties of orthogonal transformations to achieve optimal performance.\n\nOur experiments with Householder reflection matrices, which maintain orthogonality throughout training, achieved the best results, demonstrating the effectiveness of our approach.\n\nAdditionally, we conducted general experiments using an initially orthogonal matrix, focusing on the principle of combining low-dimensional matrices. These results further validate the effectiveness of our method."}, {"title": "2. Theoretical Preparation", "content": ""}, {"title": "2.1. Defination", "content": ""}, {"title": "2.2. HouseHold Transform", "content": "HouseHold Transform is a common way to generate orthogonal matrices. The reflection hyperplane can be defined by its normal vector, a unit vector v (a vector with length 1) that is orthogonal to the hyperplane. The reflection of a point x about this hyperplane is the linear transformation:\n\nx - 2(x, v)v = x - 2v(v*x),\nwhere v is given as a column unit vector with conjugate transpose v*.\n\nThe matrix constructed from this transformation can be expressed in terms of an outer product as:\n\nP = I - 2vv*,\nis known as the Householder matrix, where I is the identity matrix.\n\nAs for its ability to generate orthogonal matrices, we have the following matrice:\nTheorem 2.1. Any orthogonal matrix of size n x n can be constructed as a product of at most n such reflections."}, {"title": "2.3. Matrix Exponential", "content": "Another method to generate orthogonal matrices is exponential transformation. The theory behind is the relationship between orthogonal transformation, Lie Group and Lie Algebra. To be brief, we have the following theorems.\n\nTheorem 2.2. If A is a real anti-symmetric matrix, then exp(A) is a real orthogonal matrix and det (exp(A)) = 1.\n\nProof. Let A be a real anti-symmetric matrix, i.e. A = -AT. Let Q = exp(A). Then QTQ = exp(A)T exp(A) = exp(AT) exp(A) = exp(AT + A) = exp(0nxn) = I. This indicates that Q is orthogonal.\n\nThe following proof uses the property that exp(A)T = exp(AT).\n\nFor any matrix M, we have\n\ndet(exp(M)) = exp(tr(M)).\nThus, det(Q) = exp(tr(A)) = exp(0) = 1\n\nTheorem 2.3. For any real matrix T\u2208 SO(n), i.e. TT TT = 1 and det(T) = 1, we can find real anti-symmetric matrix A such that exp(A) = T.\n\nProof. For any T \u2208 SO(n), all its eigenvalues in C have length 1. Since T is real, all its complex eigenvalues are conjugate. Thus, T can be orthogonally decomposed as\n\nT~ diag(R(01), R(02),..., R(0k),1,...,1),"}, {"title": "3. Method", "content": "In previous studies, the Cartesian product of two lattices (or other sets of vectors) has been widely used in generative tasks for lattices. The Cartesian product of two lattices is defined as follows:\n\nL1 \u00d7 L2 = {[x1 x2]: x1 \u2208 L1,x2 \u2208 L2}.(4)\nAs observed in (Agrell & Allen, 2023), the best-performing lattices in lower dimensions are often used to generate lattices in higher dimensions via the Cartesian product. However, this generative approach is not always optimal, as the NSM (normalized second moment) of these lattices can often be reduced by applying small rotations to their corresponding generator matrices, as indicated in (Agrell & Allen, 2023). To address this limitation, (Agrell et al., 2024a) introduces the concept of glued vectors to enforce non-orthogonal relationships (as will be illustrated later with an example, which shows that this approach is equivalent to applying a rotation to the generator matrix). Using this method, (Agrell et al., 2024a) successfully achieved state-of-the-art results for lattices in 12 dimensions.\n\nRecently, machine learning techniques have been increasingly applied to lattice generation and optimization. In (Agrell et al., 2024b), a stochastic gradient descent (SGD) approach is used to iteratively improve the generator matrix by computing the gradient of NSM with respect to the generator matrix's parameters. This approach has proven effective in approximating the optimal solution and has achieved state-of-the-art results in 15 dimensions."}, {"title": "3.1. Learnable Symmetry Matrix", "content": "Inspired by the above two approaches, we first propose a novel optimization method. This method involves applying rotations to the lattice and improving the corresponding generator matrix through SGD to approximate an optimal lattice. Consider the Cartesian product L1 \u00d7 L2, whose generator matrix can be expressed as follows:\n\nG=\nGG\nG1\n0\n0\nG2\n,\nwhere G1 and G2 are the generator matrices of L\u2081 and L2, respectively. When we apply transformation to G\u2081 and G2 in the subspace and transformation in big space G. The resulting generator matrix can be written as:\n\nG' =\nG1\n0\n0\nG2\nT1U1\nT2U2\n,\nwhere U1,U2 is orthogonal matrix, representing the fusion method of two lattice generated matrix G1, G2 from low dimension.T1, T2 satisfies T\u00bfTT = I,i \u2208 1,2. We hope our methods focusing on adjusting the fusion methods. While for the lattice quantizer, the orthogonal transformation will not change its NSM value, cause it's only rotation or reflection in physical. It is easy to see that N dimension matrix U\u2081 maintain the property of orthogonal after apply to matrix Ti:\n\n(T\u00bfU\u2081)(T\u00bfU\u2081) = TU\u00bfUT = TT = I\n\nTherefore, we only need to construct an appropriate form to generate orthogonal matrices. This form should satisfy the following properties:\n\n1. Completeness: The generation form should represent all (or almost all) orthogonal matrices.\n\n2. Optimizability: The parameters of the generation form must be optimizable in a differentiable manner.\n\nIn the \"Theoretical Preparation\" section, we demonstrated that the Householder reflections Matrix Exponential and satisfies Property 1. As for Property 2, empirical observations from our experiments suggest its validity, although we aim to provide a more rigorous theoretical proof in future work."}, {"title": "3.2. HouseHold Transform", "content": "Actually, the matrix U contains n\u00b2 parameters that need to be learned, which is equivalent to the size of the matrix G. Based on our experience with gradient descent in similar methods, this high number of parameters often makes convergence challenging. To address this issue, we adopt a reparameterization approach by reformulating the generator matrix as follows:\n\nG' (V1, V2) \u2264\n(6\n0\n0\nG2\n(T\u2081U\u2081(V1))\nT2U2(V2)\nwhere Ui (Vi) indicates that U is generated from a single vector V using Householder transformations.\n\nAlthough our theoretical preparation section introduces the construction of symmetry transformations via n House-holder reflections, in our implementation, we simplify the approach by using a single vector V as the learnable parameter to generate symmetric orthogonal matrices. While this approach cannot represent all orthogonal matrices, it offers significant advantages. The reduced number of parameters results in faster convergence during training. Despite the potential loss of generality in using simpler symmetric orthogonal matrices, our experiments indicate that they are effective in many scenarios, consistently outperforming direct Cartesian products."}, {"title": "3.3. Matrix Exponential", "content": "While n\u00b2 parameters may seem substantial for this task, it is actually manageable for modern machine learning models since n is typically less than 64. For higher dimensions, the number of sampling points required to evaluate the NSM (Nearest Symmetric Matrix) becomes prohibitively large, and the cost of computing the nearest points is unsustainable. Therefore, we hypothesize that the number of parameters does not significantly affect convergence difficulty in practice.\n\nIn our initial experiments with Householder matrices, we observed the effectiveness of orthogonal transformations. Thus, we chose to start with a low-dimensional quantizer and initialize the transformation matrix as orthogonal, which provides a strong starting point for training. To this end, we use the matrix exponential to generate orthogonal matrices for training. However, during the training process, we allow the transformation matrix to deviate from strict orthogonality, enabling it to express greater diversity. Our experimental results further demonstrate the effectiveness of this approach."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Training Experiments", "content": "We completed the main training process for dimensions ranging from 12 to 22 (see results in 1). The training results reveal that household reflections typically perform better in lower dimensions, aligning with the simpler characteristics observed in the 12-15 dimension range. On the other hand, the matrix exponential method, with its greater number of learnable parameters, excels in capturing complex combinations and demonstrates superior performance in higher dimensions, particularly in dimensions 21 and 22. Additionally, the experimental results are significantly influenced by the choice of lattice.\n\nThe primary challenge we encountered was the evaluation of the Nearest Symmetric Matrix (NSM), which is critical for setting appropriate targets in machine learning. During training, we employed Monte Carlo sampling to evaluate the integration of the NSM.\n\nTo account for the varying number of parameters, we adjusted the number of samples per epoch accordingly. Specifically:\n\n1. Householder Matrix Training: Since the training complexity for Householder matrices is lower compared to training the entire matrix, we followed prior work by using a single point to compute the NSM.\n\n2. Matrix Exponential Method: For this approach, each gradient update incorporated hundreds of lattice samples to ensure more stable training progress.\n\nFurther details regarding the training settings are provided in Appendix A."}, {"title": "5. Analysis", "content": "The above results show that our method can find lattices with much smaller Normalized Second Moment (NSM), surpassing the previous state-of-the-art results by a huge gap, especially in dimension 17, 18, 19, 21, 22, where our results are much closer to theoretical lower bounds.\n\nHowever, our method have the following shortcomings.\n\n1) Unstable training loss. It is hard to find suitable learning rate to ensure stable training loss reduction. Empirically, we find that schedulers such as stepLR in Pytorch helps alleviate this problem.\n\n2) Applying orthogonal transformations to sub-lattice components does not guarantee theoretically optimal lattice. Take the example of the optimal lattice in dimension 3, which is body-centric cubic lattice. This lattice can not be composed by the optimal lattice in dimension 2, which is hexagonal lattice, and the trivial lattice in di-"}, {"title": "3) Unable to attain exact lattice", "content": "The convergence of the algorithm is not qualified so the numerical lattice is hard to converge to an exact lattice which is highly symmetric.It is hard for us to analyze properties of the numerical lattice, e.g. kissing number, the Vonoroi region. Thus, we can only apply Monte Carlo method to calculate the NSM of lattice, which leads to high variance of the result."}, {"title": "1) Parameter-efficient", "content": "The application of a single Household Transform decreases the complexity of parameters to O(n), which is O(n\u00b2) in (Agrell et al., 2024b).This leads to a significant improvement in training efficiency when experimenting on high dimensions."}, {"title": "2) Smaller exploration space", "content": "The fusion of lattice has a higher NSM and a smaller exploration space than random initialization. The reduction in the number of extrema points in the space makes the method less likely to get stuck in local minima."}, {"title": "6. Conclusion", "content": "In this paper, we proposed a gradient fusion method for low-dimensional lattice quantizers, leveraging orthogonal transformations to enhance performance. By using Householder reflection matrices and matrix exponentials, we achieved efficient training with reduced parameters, faster convergence, and robust results. Our experiments demonstrated the effectiveness of maintaining orthogonality during training and highlighted the benefits of structured transformations for low-dimensional quantizers. With comprehensive evaluations across various lattices and dimensions, our approach provides a scalable framework for lattice quantization. Future work will focus on extending these methods to higher dimensions and refining theoretical insights to enhance their versatility."}, {"title": "Accessibility", "content": "All the code is provided. You can visit our project code at this GitHub link: https://github.com/catnanami/lattice-quantizer"}]}