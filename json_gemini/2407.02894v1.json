{"title": "Translatotron-V(ison): An End-to-End Model for In-Image Machine Translation", "authors": ["Zhibin Lan", "Liqiang Niu", "Fandong Meng", "Jie Zhou", "Min Zhang", "Jinsong Su"], "abstract": "In-image machine translation (IIMT) aims to translate an image containing texts in source language into an image containing translations in target language. In this regard, conventional cascaded methods suffer from issues such as error propagation, massive parameters, and difficulties in deployment and retaining visual characteristics of the input image. Thus, constructing end-to-end models has become an option, which, however, faces two main challenges: 1) the huge modeling burden, as it is required to simultaneously learn alignment across languages and preserve the visual characteristics of the input image; 2) the difficulties of directly predicting excessively lengthy pixel sequences. In this paper, we propose Translatotron-V(ision), an end-to-end IIMT model consisting of four modules. In addition to an image encoder, and an image decoder, our model contains a target text decoder and an image tokenizer. Among them, the target text decoder is used to alleviate the language alignment burden, and the image tokenizer converts long sequences of pixels into shorter sequences of visual tokens, preventing the model from focusing on low-level visual features. Besides, we present a two-stage training framework for our model to assist the model in learning alignment across modalities and languages. Finally, we propose a location-aware evaluation metric called Structure-BLEU to assess the translation quality of the generated images. Experimental results demonstrate that our model achieves competitive performance compared to cascaded models with only 70.9% of parameters, and significantly outperforms the pixel-level end-to-end IIMT model.", "sections": [{"title": "1 Introduction", "content": "In recent years, significant advancements have been achieved in natural language processing (NLP) and computer vision (CV), largely due to the evolution of deep learning. As a combined direction of these two fields, in-image machine translation (IIMT) aims to covert an image containing texts in source language into another image containing the translations in target language, which has significant research value and practical applications. It not only helps us understand the fusion mechanism of multimodal and multilingual information, but also finds widespread applications in daily life. For instance, IIMT can effortlessly enable foreign travelers to read signs written in other languages.\nAs shown in Figure 1, current IIMT systems are divided into two paradigms: cascaded and end-to-end. The first one relies on cascading multiple models, including an optical character recognition (OCR) model, a machine translation (MT) model, and a text-to-image (T2I) model. However, this paradigm suffers from error propagation, massive parameters, and difficulties in deployment and retaining visual characteristics of the input image. By contrast, end-to-end methods (Mansimov et al., 2020; Tian et al., 2023) integrate different models into one IIMT model and conduct end-to-end training. Thus, they have potential advantages over cascaded systems in aspects of avoiding error propagation, reduced parameters, and ease of deployment. Particularly, they are naturally capable of retaining visual characteristics from the input image during translation, e.g. maintaining background, text location, font, etc.\nDespite the above advantages, the end-to-end IIMT models still face two major challenges: 1) the huge modeling burden, since they are required to not only learn alignment between two languages but also the visual characteristics of the input image; 2) the difficulties of directly predicting excessively lengthy pixel sequences, which are low-level and involve a large search space (Ramesh et al., 2021; Yu et al., 2022b).\nTo the best of our knowledge, (Mansimov et al., 2020) and (Tian et al., 2023) are the only two attempts to explore end-to-end IIMT. However, the former is directly based on pixel prediction, resulting in significantly lower translation quality compared to the cascaded models, while the latter requires converting RGB images to grayscale ones, losing visual characteristics. Besides, both of them can only handle images containing single-line text. These defects make them still far from real-world applications.\nIn this paper, we propose Translatotron-V(ision), the first end-to-end IIMT model capable of generating RGB images, achieving comparable performance to cascaded models with only 70.9% of parameters. As shown in Figure 2, our model consists of four modules: 1) an image encoder that represents the semantics of the image as a sequence of visual vectors; 2) a target text decoder that utilizes the visual vector sequence to predict the text translation, which can effectively reduce the modeling burden on the image decoder; 3) an image decoder that generates the visual tokens of the target image based on the visual and linguistic information generated from the image encoder and target text decoder, respectively; 4) an image tokenizer that converts the image into discrete visual tokens and can reconstruct the image from these visual tokens. By converting the image into visual tokens, the image decoder only needs to predict visual tokens, rather than excessively lengthy pixel sequences, which allows the model to avoid spending too much capacity capturing low-level visual features.\nFurthermore, as illustrated in Figure 3, we propose a training framework for our model, consisting of two stages. First, we utilize large-scale unlabeled images to train the image tokenizer through an image reconstruction task. Then, we freeze the image tokenizer and train other modules using IIMT dataset. Inspired by end-to-end speech translation (Jia et al., 2019), we introduce multi-task learning at this stage. The auxiliary tasks include OCR and text image translation (TIT), assisting the model in learning alignment across different modalities and languages. Particularly, we introduce a knowledge distillation method to reduce the difficulty of the end-to-end model directly learning from ground-truth labels.\nDue to the absence of publicly available IIMT datasets, we use IWSLT14 German-English (Cettolo et al., 2014) to synthesize a dataset for this task. Note that unlike previous works (Mansimov et al., 2020; Tian et al., 2023) only focus on images containing single-line text, the images in our dataset are more complex, featuring multiple lines of text, as well as text rotation and translation. Furthermore, since the conventional BLEU (Papineni et al., 2002) is not applicable to image evaluation, we extend BLEU to Structure-BLEU that considers text location information to better evaluate the quality of text translations within images.\nTo summarize, we have the following major contributions in this work:\n\u2022 We propose a novel end-to-end IIMT model named Translatotron-V. More importantly, it introduces two crucial modules to address major challenges in end-to-end IIMT: 1) target text decoder used to alleviate the modeling burden; 2) image tokenizer preventing the model from directly predicting pixels.\n\u2022 We present a two-stage training framework for Translatotron-V, which fully exploits unlabeled images, OCR, and TIT data to refine the model training.\n\u2022 We propose Structure-BLEU, an evaluation metric that considers text location information for IIMT.\n\u2022 Experimental results demonstrate that Translatotron-V not only significantly outperforms the pixel-level end-to-end IIMT model, but also achieves comparable performance with fewer parameters to cascaded models."}, {"title": "2 Related Work", "content": "To achieve high-performance IIMT, previous research mainly focuses on text image translation (TIT), which is a subtask of IIMT (Watanabe et al., 1998; Yang et al., 2002; Du et al., 2011; Chen et al., 2015; Afli and Way, 2016; Lan et al., 2023). Unlike conventional multimodal machine translation (Elliott et al., 2016; Yin et al., 2020; Lin et al., 2020; Su et al., 2021a; Yin et al., 2023; Kang et al., 2023), TIT aims to translate source language texts in images into target language. In this regard, dominant studies resort to the cascading method, which uses an OCR model to obtain the recognized source language texts and then feed them into an MT model for translation (Goodfellow et al., 2014; Zhang et al., 2016; Gu et al., 2018).\nAfterwards, due to the advantages of mitigating error propagation, the end-to-end TIT attracts increasing attention. Chen et al. (2020) adopt multi-task learning framework that integrates OCR as an auxiliary task. Along this line, Ma et al. (2022) incorporating MT into the multi-task learning framework. Unlike previous studies, both Su et al. (2021b) and Ma et al. (2023b) employ an adapter to combine individual pretrained OCR and MT modules in a TIT model. Furthermore, Ma et al. (2023c) apply knowledge distillation to effectively distillate the knowledge of OCR and MT models into the end-to-end TIT model. Zhu et al. (2023) explore an end-to-end TIT model with an aligner and a regularizer to reduce the modality gap. To explicitly exploit guidance from recognized texts, Ma et al. (2023a) incorporate recognized text information into the TIT decoder through interactive attention. Differing from the above studies focusing on model design, Salesky et al. (2021) analyze the effect of visual text representation, and find that it exhibits significant robustness to various types of noise.\nHowever, none of the aforementioned works consider generating the image with target translations, which is a common requirement in real-world scenarios. To this end, Mansimov et al. (2020) first explore the IIMT task. They introduce an end-to-end model that contains a self-attention encoder, two convolutional encoders, and a convolutional decoder to generate target images at the pixel level. Nonetheless, their model significantly lags behind cascaded models, suffering from issues such as character omission and artifacts. Recently, Tian et al. (2023) convert pixels into characters, thereby transforming the IIMT task into a conventional sequence-to-sequence text generation task. However, this method can only generate grayscale images, losing visual characteristics. Susladkar et al. (2023) present a conditional diffusion-based image editing model, which replaces text in the input image with a given translation while preserving the visual characteristics of origin image. However, this model can only perform single-word editing, which makes its application very limited.\nDifferent from these studies, we propose an end-to-end IIMT model that can generate RGB images with multiple lines of text while preserving the visual features of the input image, and achieve comparable performance to the cascaded model."}, {"title": "3 Our Model", "content": "As shown in Figure 2, our model consists of four modules: an image encoder, a target text decoder, an image decoder, and an image tokenizer. All of those modules will be elaborated in the following.\nImage Encoder. This module converts the input image into a sequence of visual vectors.\nWe use ViT (Dosovitskiy et al., 2021) as the backbone of the image encoder. In order to convert a 2D image into a 1D sequence that can be handled by Transformer, we first split the input image x into N = HW/P2 image patches {$x_i$}$_{i=1}^{N}$, where (H, W) is the resolution of the input image, and (P, P) is the resolution of each patch. Then we apply a linear projection matrix $W_e$ to transform image patches into patch embeddings, and use a standard learnable positional embedding matrix $E_{pos}$ to further optimize these patch embeddings. Formally, the initial hidden states $H_{ie}^{(0)}$ of the image encoder can be formulated as\n$H_{ie}^{(0)} = [x_0; W_e x_1; W_e x_2; ...; W_e x_N] + E_{pos},$\nwhere $x_0$ is the special token prepended to the input sequence.\nAfterwards, we process these patch embeddings using a Transformer encoder with multiple layers. Each Transformer encoder layer is composed of a self-attention sub-layer and a feed-forward network (FFN) sub-layer. Layernorm (LN) is applied before each sub-layer, and residual connections after each sub-layer (Wang et al., 2019). The hidden states $H_{ie}^{(l)}$ of the l-th encoder layer is calculated as\n$H_{ie}^{(l)} = FFN(MHA(H_{ie}^{(l-1)}, H_{ie}^{(l-1)}, H_{ie}^{(l-1)})),$\nwhere MHA(\u00b7,\u00b7, \u00b7) denotes a multi-head attention function. The residual connection and layer normalization are omitted for simplicity.\nTarget Text Decoder. By utilizing the features generated by the image encoder, this decoder is responsible for producing text translations. In this way, it focuses on the alignment of different languages, and thus alleviates the modeling burden of the image decoder.\nWhen constructing our target text decoder, we employ the widely-used Transformer (Vaswani et al., 2017) decoder as the architecture, consisting of multiple identical layers. In addition to the standard self-attention and FFN sub-layers, each decoder layer is equipped with a cross-attention sub-layer to exploit hidden states produced by the image encoder. Formally, we calculate the hidden states $H_{td}^{(l)}$ for the l-th decoder layer using the following equations:\n$C_{td}^{(l)} = MHA(H_{td}^{(l-1)}, H_{ie}^{(L)}, H_{ie}^{(L)}),$\n$H_{td}^{(l)} = FFN(MHA(C_{td}^{(l)}, H_{td}^{(l-1)}, H_{td}^{(l-1)})),$\nwhere the initial hidden states $H_{td}^{(0)}$ are computed by summing the word embeddings and position embeddings of the input sequence. Unless otherwise specified, we use L to represent the last layer.\nImage Decoder. This module is responsible for generating visual tokens based on visual and linguistic information generated from the image encoder and target text decoder, respectively.\nThe architecture of the image decoder closely resembles that of the target text decoder but with the following notable modifications. It includes two cross-attention sub-layers to gather information from both the image encoder and target text decoder, followed by a fusion sub-layer to generate intermediate representations enriched with both visual and linguistic features. Besides, we incorporate the 2D relative position encoding (Wu et al., 2021) into the self-attention sub-layer to capture relative positional relationships within images.\nLet $C_{id}^{(l)}$ denote the hidden states output by the l-th self-attention sub-layer, we calculate it in the following way:\n$C_{id}^{(l)} = MHA(H_{id}^{(l-1)}, H_{id}^{(l-1)}, H_{id}^{(l-1)}),$\nwhere $H_{id}^{(l-1)}$ represents the hidden state output by the (l-1)-th image decoder layer. Subsequently, the hidden states $H_{id, ie}^{(l)}$ and $H_{id, td}^{(l)}$ are computed through two cross-attention mechanisms, which attend to the image encoder and the target text decoder, respectively, as follows:\n$H_{id, ie}^{(l)} = MHA(C_{id}^{(l)}, H_{ie}^{(L)}, H_{ie}^{(L)}),$\n$H_{id, td}^{(l)} = MHA(C_{id}^{(l)}, H_{td}^{(L)}, H_{td}^{(L)}).$\nFinally, the hidden states of the l-th image decoder layer are obtained through a gated fusion mechanism, which is calculated using the following equations:\n$A = sigmoid(W_A H_{id, ie}^{(l)} + U_A H_{id, td}^{(l)}),$\n$H_{id}^{(l)} = \\Lambda H_{id, ie}^{(l)} + (1 \u2013 \\Lambda) \tilde{H}_{id, td}^{(l)},$\nwhere $W_A$ and $U_A$ are projection matrices, and A is a gated matrix featuring values ranging from 0 to 1, serving the purpose of dynamically fusing two modalities of information.\nImage Tokenizer. It is used to perform the conversion between an image and a sequence of discrete visual tokens. By introducing this module, we allow the image decoder only to predict visual tokens, preventing it from modeling excessively lengthy sequences. For instance, a 256\u00d7256\u00d73 RGB image results in 196,608 rasterized values. Our image tokenizer follows the architecture of VIT-VQGAN (Yu et al., 2022a), which includes a Vison Transformer (ViT) (Dosovitskiy et al., 2021) based encoder and decoder. The encoder E of the image tokenizer is used to tokenize the image into z = ($z_1$, ..., $z_N$) through a quantizer q(\u00b7). Formally, the quantizer looks up the nearest visual token for each input, as shown in the following:\n$z_i = q(E(x_i)) = argmin_{e_k \\in V} ||E(x_i) \u2013 e_k||_2,$\nwhere V is the image vocabulary containing visual tokens.\nConversely, the decoder G of the image tokenizer reconstructs the input image based on the visual tokens generated by E, formulated as\n$\\hat{x} = G(q(E(x))).$\nPlease note that during training, we use the encoder to obtain visual tokens of the target image as labels. During inference, the decoder converts visual tokens generated by the image decoder into the target image."}, {"title": "3.2 Model Training", "content": "We provide a detailed description of the training procedures for our model, which consists of two stages, as illustrated in Figure 3.\nStage 1. At this stage, we train the image tokenizer using a large-scale unlabeled image dataset $D_u$ in the same way as ViT-VQGAN (Yu et al., 2022a), where we convert the input image into visual tokens and then reconstruct the image from these visual tokens.\nGiven an image x from the unlabeled image dataset $D_u$, we define the training objective of this stage as follows:\n$L_1 = ||x \u2212 \\hat{x}||^2 + ||sg(E(x)) \u2013 z||^2 + \u03b2||E(x) - sg(z)||^2.$\nHere, the first item is the reconstruction loss optimizing the encoder and decoder, the middle item is the vector-quantization loss used to update the visual tokens, the last item is the so-called \"commitment loss\" for the encoder which prevents its output fluctuating frequently from one visual token to another, sg(\u00b7) denotes the stop-gradient operation, and \u03b2 is the weighting factor set to 0.25 following van den Oord et al. (2017).\nStage 2. Using an IIMT dataset, we then adopt multi-task learning and knowledge distillation to train the image encoder, target text decoder, and image decoder.\nOverall, the training objective at this stage is defined as follows:\n$L_2 = L_{iimt} + L_{ocr} + L_{tit} + L_{kd}.$\nwhere $L_{iimt}, L_{ocr}, L_{tit}$, and $L_{kd}$ denote the IIMT task loss, OCR auxiliary task loss, TIT auxiliary task loss, and knowledge distillation loss, respectively.\nGiven an IIMT training instance (x, y, s, t) from the IIMT dataset $D_{iimt}$, we can utilize the image tokenizer trained in the first stage to process the target image, obtaining visual tokens denoted as z. Here, x represents the source image, y is the target image, s denotes the source language text within the source image, and t denotes the target language text within the target image.\nTo alleviate the burden of end-to-end model training, we adopt multi-task learning, which involves not only the primary IIMT task but also two auxiliary tasks: the OCR task and the TIT task. The OCR task is employed to assist the model in recognizing texts within the image, while the TIT task further facilitates cross-lingual alignment. Formally, the training objective of the IIMT task can be formulated as follows:\n$L_{iimt} = -logp(z|x; \\theta_{ie}, \\theta_{ttd}, \\theta_{id}),$\nwhere $ \\theta_{ie}, \\theta_{ttd}, \\theta_{id}$ denote the trainable parameters of the image encoder, target text decoder, and image decoder, respectively.\nTo train our model using the OCR auxiliary task, we additionally introduce a source text decoder, which adopts the same architecture as the target text decoder. Formally, the training objectives of the OCR and TIT auxiliary tasks are defined as\n$L_{ocr} = -logp(s|x; \\theta_{ie}, \\theta_{std}),$\n$L_{tit} = -logp(t|x; \\theta_{ie}, \\theta_{ttd}),$\nwhere $ \\theta_{std}$ is the parameters of the source text decoder. Note that the source text decoder takes the intermediate hidden states of the image encoder as input. This design is based on the intuition that the shallow encoder layers represent the source visual content, while the deep layers encode more information about the target visual content.\nBesides, training an end-to-end model is considerably more difficult than a T2I model, where the latter only needs to learn the mapping between different modalities and thus has better performance. Consequently, we introduce a T2I model as a teacher to facilitate knowledge transfer to the end-to-end model. This T2I model includes a Transformer-based text encoder, a ResNet-based image encoder (He et al., 2016), and an image decoder similar to our model, where the image encoder is used to preserve the features of the original image. Denote the output distribution of the teacher model for t-th visual token $z_t$ as $q(z_t|z_{<t}, x, t; \\theta_{t2i})$, we define the cross-entropy between the distributions of teacher and student as the distillation loss:\n$L_{kd} = -\\sum_{t=1}^{zV} \\sum_{k=1}^{zV} q(z_t = k|z_{<t}, x, t; \\theta_{t2i})logp(z_t = k|z_{<t}, x; \\theta_{ie}, \\theta_{ttd}, \\theta_{id}),$"}, {"title": "4 Experiments", "content": "Due to the lack of readily available data, we utilize the widely-used IWSLT14 German-English (De-En) dataset (Cettolo et al., 2014) to synthesize paired images for this task. Concretely, we leverage the Python Pillow package to render texts onto images with the black Arial font. The text is arranged horizontally from left to right, and vertically from top to bottom, with randomly translating and rotating. This involves shifting the text in a random direction and changing its orientation by a random angle. Additionally, the background color of the image is selected randomly and the resolution of the images is 512\u00d7512. Note that bilingual texts exceeding the image boundaries will be disregarded during the process of data synthesis. In contrast to prior studies (Mansimov et al., 2020; Tian et al., 2023), which focus solely on generating images with single-line text and white background, our research delves into more complex scenes. In the end, the synthesized dataset comprises 81,741 training instances, 3,765 validation instances, and 3,527 test instances. Several synthetic examples\nImplementation Details. In this work, we employ the same setting as ViT-B (Dosovitskiy et al., 2021) to construct our image encoder. Both our target text decoder and image decoder are composed of 8 layers, each of which has 512-dimensional hidden states, 8 attention heads, and 2,048 feed-forward hidden states. Besides, our image tokenizer is similar to ViT-VQGAN-SS (Yu et al., 2022a) but uses a smaller setup. It consists of 4 layers of encoder and decoder, each of which has 256-dimensional hidden states, 8 attention heads, and 1,024 feed-forward hidden states. Particularly, we use a character-level vocabulary of size 256 for the OCR and TIT auxiliary tasks, while the image vocabulary size for visual tokens is set to 8,192. Unless otherwise specified, the patch size of the image is set to 16.\nDuring the first training stage, we train the image tokenizer with a batch size of 512 for 10,000 steps, where the parameters are updated by AdamW (Loshchilov and Hutter, 2019) with \u03b2\u2081 = 0.9, \u03b22 = 0.99. During the second stage, we train the model for 100 epochs with an early stopping patience set to 10, and a batch size set to 80. This stage of training also utilizes the AdamW optimizer (\u03b2\u2081 = 0.9, \u03b22 = 0.999) along with weight decay of 0.001 and polynomial decay learning rate scheduling. To alleviate overfitting, we apply a dropout rate of 0.1 and incorporate the label smoothing with a coefficient of 0.1, and we average the checkpoints of the last 10 epochs for evaluation.\nBaselines. We construct the following baselines: OCR+MT+T2I, TIT+T2I, PEIT+T2I, and pixel-level Transformer, all models trained using character inputs and outputs similar to our model.\n1) OCR+MT+T2I. This baseline cascades three models: an OCR model, an MT model, and a T2I model. Note that our teacher model has the same architecture as the T2I Model, except for reducing the hidden states from 512-dimensional to 384-dimensional. 2) TIT+T2I. We construct this baseline by cascading the TIT model and the T2I model. Additionally, it applies both OCR and TIT tasks during training to achieve better performance. 3) PEIT+T2I. This baseline is similar to TIT+T2I but replaces the multi-line TIT model with PEIT (Zhu et al., 2023), the state-of-the-art single-line TIT model. Since PEIT is designed for single-line TIT, during inference, we first employ the widely-used EasyOCR as the detection model to recognize and crop each line of text from the image, and then concatenate them together into a single-line text image. 4) Pixel-level Transformer. This model uses the same structure as our model but removes the image tokenizer and directly predicts pixel values. It is trained using multi-task learning as well, with the IIMT task being optimized with a mean squared error loss due to the pixel values being of floating-point type.\nThe detailed architecture of OCR+MT+T2I, TIT+T2I, and PEIT+T2I is described in Appendix C.\nEvaluation. We evaluate the output images from both the perspectives of translation quality and image quality. We follow Mansimov et al. (2020) to transcribe the generated images into texts with EasyOCR toolkit and then measure the BLEU (Papineni et al., 2002) score calculated by SacreBLEU (Post, 2018). To take into account the location of texts within the generated image, we extend the conventional BLEU to Structure-BLEU. This metric first performs OCR on the generated image and the reference image separately. Then, we use the bounding boxes in the generated image and the reference image to calculate intersection over union (IoU) for text matching. Subsequently, we filter out matched text pairs with significantly different positions, specifically those with IoU values below 0.5. Finally, we calculate the BLEU score for the remaining paired texts. For more comprehensive details, please refer to the algorithm provided in the Appendix D. Besides, we evaluate the quality of the generated images via structural similarity index measure (SSIM) (Wang et al., 2004), which considers luminance, contrast, and structure to measure the similarity between two images. The comparison between SSIM and BLEU can be found in Appendix E."}, {"title": "4.3 Ablation Study", "content": "To explore the effectiveness of different components, we further compare Translatotron-V with its several variants, as shown in Table 2.\n1) w/o gated fusion. In this variant, we remove the gated fusion mechanism of the image decoder when performing cross-attention. Consequently, the image decoder sequentially performs cross-attention over the image encoder and the target text decoder to update hidden states. The result in Line 2 indicates that this change causes a decline in translation quality, suggesting that the gated fusion mechanism is useful for fusing information from two modalities.\n2) w/o OCR auxiliary task. When constructing this variant, we remove the OCR auxiliary task during the model training. Upon analyzing Line 3, it becomes evident that this task can empower the model with the ability to perceive text within images, enabling the model to accomplish translation.\n3) w/o knowledge distillation. We remove the knowledge distillation in this variant. As indicated in Line 4, there is a significant performance drop, which demonstrates that knowledge distillation effectively reduces the difficulty of training.\n4) w/o target text decoder. In this variant, we remove the target text decoder from our model. The results reported in Line 5 demonstrate a drastic decline in performance. We can confirm that the end-to-end IIMT model imposes a substantial modeling burden. The target text decoder plays a pivotal role in mitigating the burden of achieving alignment between different languages."}, {"title": "4.4 Case Study", "content": "Figure 4 displays the translation results of different models on the De\u2192En dataset. We can observe that Translatotron-V generates the correct target image, while the strongest baseline model OCR+MT+T2I missing partial strokes for the word \u201cyou\u201d in the generated images. Besides, Pixel-level Transformer has issues like character omission and artifacts, making it unable to generate correct words. This result reveals that image tokenizer is important for the Translatotron-V."}, {"title": "4.5 The Effectiveness on Alleviating Error Propagation", "content": "To further investigate the impact of error propagation, we divide the test set of the De\u2192En dataset into different groups based on the Word Error Rate (WER) of OCR. The higher WER indicates that the image is more difficult to deal with, where potential error propagation is more severe. As illustrated in Figure 5, the improvements of Translatotron-V over OCR+MT+T2I are more significant with the increase of WER. Thus, we confirm again that our end-to-end model has the potential advantage of alleviating error propagation."}, {"title": "4.6 Evaluation on Other Language Pairs", "content": "In order to further validate the effectiveness of Translatotron-V, we conduct experiments on two distinct language pairs: French to English (Fr\u2192En) and Romanian to English (Ro\u2192En). We also use the previously-described data synthesis method to convert the IWSLT17 Fr-En and Ro-En datasets (Cettolo et al., 2017) into IIMT datasets. As shown in Table 3, Translatotron-V still achieves competitive performance compared to cascaded models and significantly outperforms the Pixel-level Transformer across different language pairs."}, {"title": "5 Conclusion", "content": "In this work, we have proposed Translatotron-V, which is the first end-to-end IIMT model capable of generating RGB images and achieving comparable performance to the cascaded model with only 70.9% of parameters. In addition to an image encoder and an image decoder, Translatotron-V is equipped with a target text decoder and an image tokenizer, which are used to alleviate the modeling burden and prevent the model from directly predicting pixels, respectively. Moreover, we present a two-stage training framework to assist the model in learning alignment across modalities and languages. Furthermore, we introduce an evaluation metric, Structure-BLEU, which considers text location information to evaluate the quality of translations within the image. Experimental results demonstrate the effectiveness of our proposed model and training framework.\nIn the future, we are interested in training models using only parallel images, which is important when texts within the image are not available."}, {"title": "Limitations", "content": "Currently, the quality of generated target images depends on the quality of the image tokenizer. However, in our experiments, we find that it sometimes generates incorrect words, which may be due to its training using only images without explicitly considering linguistic information. Meanwhile, Translatotron-V does not exhibit a speed advantage over the cascaded model. This is due to the reason that visual token sequences are still much longer than text sequences, and both cascaded and end-to-end models need to spend most of their time decoding the image. A promising direction is to find coarser visual tokens with a shorter sequence length without degrading the quality of the generated images. Furthermore, the synthetic dataset is still not realistic enough. However, acquiring IIMT data from the real world is very challenging, how to create a more realistic IIMT dataset is also an important direction."}, {"title": "A The effectiveness of the training objective with balancing coefficients", "content": "To explore the impact of different weights of auxiliary task losses on model performance, we modify the training objective at the second stage as follows:\n$L_2 = L_{iimt} + \\alpha L_{ocr} + \\beta L_{tit} + \\gamma L_{kd}.$\nwhere \u03b1, \u03b2, and \u03b3 are the coefficient to control OCR auxiliary task loss $L_{ocr}$, TIT auxiliary task loss $L_{tit}$, and knowledge distillation loss $L_{kd}$, respectively.\nDue to the high cost of grid search, when adjusting a specific coefficient, all other coefficients will be set to 1. As shown in Figure 6, when all coefficients are set to 1, the model performs optimally. This result suggests that these tasks may be equally important and highly correlated. Adjusting a particular coefficient could lead the model to focus more on or neglect one task, which might not be beneficial if all tasks are equally important. This also implies that our approach does not require carefully adjusting the coefficients for different training objectives."}, {"title": "B Data Examples", "content": "In Figure 7, we present several data examples for our synthetic data. We also show the difference between our IIMT data and previous IIMT data in Figure 8. It can be observed that our data is more complex than the data used in previous work."}, {"title": "C Baseline Architecture Details", "content": "In this section", "baselines": "OCR+MT+T2I and TIT+T2I.\nOCR+MT+T2I. This baseline cascades three models: an OCR model, an MT model, and a T2I model. First, we follow Li et al. (2023) to construct the OCR model, which consists of a ViT-B encoder and a Transformer decoder. The decoder uses the settings of Transformer Base, which contains 6 layers, 8 attention heads, 512-dimensional hidden states, and 2048 feed-forward hidden states. Second, we use the standard Transformer base(He et al., 2016) as the architecture of the MT model. Third, the T2I Model includes a Transformer-based text encoder, a ResNet-based image encoder (He et al., 2016), an image decoder, and an image tokenizer. Both the text encoder and image decoder utilize the same settings as Transformer base. Additionally, the image encoder adopts the ResNet50 architecture, and the image tokenizer follows the configuration of our model. It's worth noting that our teacher model has the same structure as this T2I Model, except for reducing the hidden states from 512-dimensional to 384-dimensional.\nTIT+T2I. We construct this baseline by cascading the TIT model and the T2I model, where the TIT model consists of an"}]}