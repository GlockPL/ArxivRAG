{"title": "Diagnosing and Re-learning for Balanced Multimodal Learning", "authors": ["Yake Wei", "Siwei Li", "Ruoxuan Feng", "Di Hu"], "abstract": "To overcome the imbalanced multimodal learning problem, where models prefer the training of specific modalities, existing methods propose to control the training of uni-modal encoders from different perspectives, taking the inter-modal performance discrepancy as the basis. However, the intrinsic limitation of modality capacity is ignored. The scarcely informative modalities can be recognized as \"worse-learnt\" ones, which could force the model to memorize more noise, counterproductively affecting the multimodal model ability. Moreover, the current modality modulation methods narrowly concentrate on selected worse-learnt modalities, even suppressing the training of others. Hence, it is essential to consider the intrinsic limitation of modality capacity and take all modalities into account during balancing. To this end, we propose the Diagnosing & Re-learning method. The learning state of each modality is firstly estimated based on the separability of its uni-modal representation space, and then used to softly re-initialize the corresponding uni-modal encoder. In this way, the over-emphasizing of scarcely informative modalities is avoided. In addition, encoders of worse-learnt modalities are enhanced, simultaneously avoiding the over-training of other modalities. Accordingly, multimodal learning is effectively balanced and enhanced. Experiments covering multiple types of modalities and multimodal frameworks demonstrate the superior performance of our simple-yet-effective method for balanced multimodal learning.", "sections": [{"title": "1 Introduction", "content": "Inspired by the human's multi-sensory perception, multimodal learning where information from diverse sensors is jointly utilized, has witnessed tremendous progress in recent years. Even with current developments, the question of how to assess and facilitate learning of individual modality remains open in the multimodal learning field. Especially, recent studies have found that some modalities in the multimodal model are less learnt than others, called the imbalanced multimodal learning problem. This imbalance in modality utilization hinders the potential of multimodal learning, and even could make the multimodal model fail its uni-modal counterpart. Accordingly, a series of empirical methods are proposed to balance the uni-modal learning and achieve better multimodal performance. During the uni-modal balancing process, two keys are selection basis and balancing strategy.\nIn existing methods, it is commonly believed that the modality with better prediction performance is the \"well-learnt\" modality, and correspondingly, the other \"worse-learnt\" modalities are the ones that need to be trained emphatically during uni-modal balancing. However, they ignore the intrinsic limitation of modality capacity, where some modalities naturally have scarcely label-related information and more noise. For cases of these modalities, the limited information causes their limited prediction performance, not just insufficient training. Although with worse prediction performance, purely emphasizing the training of these modalities could not bring many additional benefits and even force the model to memorize more noise, affecting the model ability. To further illustrate this problem, we modify the audio-vision CREMA-D dataset, and add white Gaussian noise into its audio data, making the audio modality with limited discriminative information but numerous noise. As Figure la, all existing imbalanced multimodal learning methods experience a performance drop compared with joint-training baseline. This phenomenon verifies that they wrongly push the training of scarcely informative modality with intrinsic limitation, counterproductively making them lose efficacy.\nUpon the design of the balancing strategy, existing methods narrowly concentrate on the learning of selected worse-learnt modalities. Some even disturb the training of well-learnt modality, to facilitate the training of others. Inevitably, the ignorance or even suppression of well-learnt modality potentially affects its learning. As Figure 1b and Figure 1c, in existing imbalanced methods, although improving multimodal performance, the quality of the well-learnt audio modality can be worse than the joint-training baseline, especially on the Kinetics Sounds dataset. Recognizing these limitations, the challenge lies in how to overcome scarcely informative modalities cases and take all modalities into account during balancing.\nIn this paper, we propose the Diagnosing & Re-learning method, which periodically softly re-initialize the uni-modal encoder with representation separability as the basis. Since many multimodal models only have one multimodal output, it is hard to directly obtain the uni-modal learning state without additional modules. To this end, we focus on the uni-modal representation space, which is easier to access and can indirectly reflect the modality discriminative ability [17]. Concretely, the separability of train-representation and validation-representation are assessed by clustering, and utilized to diagnose the uni-modal learning state. In this way, the learning of each modality is well estimated individually. Then, to balance the uni-modal training, uni-modal encoders are softly re-initialized based on their learning state. For well-learnt modalities, their encoder has a greater re-initialization strength. This strategy helps the model reduce the reliance on them, and enhance the learning of other still under-fitting modalities. Simultaneously, the greater re-initialization of well-learnt modality avoids its over-training, even potentially improving the generalization. For worse-learnt modalities, their encoders are slightly re-initialized, which is also beneficial for escaping memorizing data noise that harms generalization. When one modality is scarcely informative, our method will not wrongly over-emphasize its training, and the re-initialization for its encoder helps encoders avoiding memorize data noise. Therefore, our strategy can benefit all modalities at the same time. In addition, the soft re-initialization partially preserves previously learnt knowledge already accrued by the network. It safeguards the collaboration between modalities, ensuring that the collaborative knowledge is not completely discarded but rather fine-tuned.\nBased on Figure la, our method can well handle the scarcely informative modality case and ideally achieves performance improvement. Moreover, as Figure 1b and Figure 1c, it also effectively enhances the learning of all modalities. Our method is flexible and can be equipped with diverse multimodal frameworks, including the multimodal Transformer. Overall, our contributions are three-fold. Firstly, we point out that existing imbalanced multimodal learning methods often ignore the intrinsic limitation of modality capacity and the well-learnt modality during balancing. Secondly, we propose the Diagnosing & Re-learning method to well balance uni-modal training by softly re-initialize encoders based on the uni-modal learning state. Thirdly, experiments across different types of modalities and multimodal frameworks substantiate the superior performance of our simple-yet-effective method."}, {"title": "2 Related Work", "content": "Multimodal learning. Motivated by the multi-sensory experiences of humans, the field of multimodal learning has gained significant attention and experienced rapid growth in recent years [11]. Multimodal learning involves the development of models capable of simultaneously integrating information from various modal-ities. Research in multimodal learning spans diverse domains, including such as multimodal recognition [26, 27] and audio-visual scene understanding [23, 32]. Commonly employed multimodal frameworks typically entail the extraction and fusion of uni-modal features, followed by the optimization of all modalities with joint learning objectives. However, besides the superficial multimodal performance across various tasks, the inherent learning of different modalities remains under-explored.\nImbalanced multimodal learning. Recent studies have revealed the imbal-anced multimodal learning problem, where models prefer certain modalities over others, limiting their overall effectiveness. A variety of strategies have been suggested, focusing on balancing the optimization of individual modali-ties [6, 9, 21, 22, 25, 28]. For example, Peng et al. [15] proposed gradient modula-tion strategy, which dynamically monitors the contribution difference of various modalities to the final prediction during training and mitigates the gradient magnitude of dominant modality to focus more on other modalities. However, in these studies, they ignore the intrinsic limitation of modality capacity. One modality can be naturally scarcely informative and with plenty of noise. In ad-dition, in these methods, when balancing uni-modal learning, only the selected worse-learnt modality is focused on. Some of them even intentionally impair the training of well-learned modalities to promote the learning of others [9, 15]. In this paper, we first diagnose the learning state of individual modality by its own representation separability without additional modules. Then, the learning state is used as the basis of soft encoder re-initialization to encourage further learning of under-fitting modalities while concurrently preventing the over-training of originally well-learnt modalities and scarcely informative modalities. It ensures better learning of all modalities.\nNetwork re-initialization. Recent studies suggest that re-initializing the net-work parameters during training can effectively improve model performance and parameter utilization [1,3,18,31]. These methods involve re-initializing and trans-forming a part or all of the parameters of a neural network periodically. For in-stance, Alabdulmohsin et al. [1] proposed the Layer-wise Re-initialization strat-egy, which re-initializes the architecture block-by-block during training. Qiao et al. [16] proposed to detect unsatisfactory components in a neural network and re-initialize them to encourage them can better fit the tasks. Here we introduce the idea of network re-initialization, but with a very different intention. The uni-modal encoder is re-initialized based on its learning state, to ensure the benefits of all modalities as well as balancing uni-modal training."}, {"title": "3 Method", "content": "3.1 Framework and notations.\nMultimodal framework. As the left part of Figure 2, data of each modality is firstly fed into the corresponding uni-modal encoder to extract features. Then these uni-modal features are fused to obtain the multimodal feature. Our method has no reliance on the multimodal fusion strategy, and can cover simple fusion methods (e.g., concatenation), and complex fusion methods (e.g., cross-modal interaction). The fused feature is fed into the final multimodal classifier. One multimodal loss, cross-entropy, is utilized to optimize the model.\nNotations. For the dataset with K modalities, the training set is denoted as $\\mathcal{D}$ with $N_D$ samples and the validation set is denoted as $\\mathcal{V}$ with $N_V$ samples. Each data sample $x = \\{x_1,x_2,\\ldots,x_K\\}$ is with K modalities. The category number of the dataset is M. For each modality k, where $k \\in \\{1,2,\\ldots, K\\}$, parameters of its encoder are denoted as $\\theta_k$. $\\theta_{init}$ represents the initialized parameter value.\n3.2 Diagnosing: uni-modal learning state estimation\nIn multimodal learning, many multimodal models only have one multimodal out-put. Therefore, it is hard to directly obtain the uni-modal learning state without additional modules. In former studies, the estimation of uni-modal learning state often relies on specific fusion strategy [6,15]. This limits their application to a wider range of scenarios. Elaborately designing ways to obtain uni-modal output is clearly complicated and not universal, since the multimodal fusion strategies are diverse. To well diagnose the uni-modal learning state without any additional modules or reliance on fusion strategies, we propose to focus on the uni-modal representation space. It is known that the separability can reflect the represen-tation quality [17]. Observing and comparing the separability of each extracted uni-modal representation is promising to capture the learning state. To evaluate representation separability, one straightforward idea is k-means clustering [13].\nFor data sample $x_i$ in the training set $\\mathcal{D}$ with $N_D$ samples, its k-th uni-modal feature that extracted by its k-th encoder $\\theta_k$ is: $\\phi_i^k = \\theta_k(x_i)$. Then, to evaluate the separability of uni-modal features, it needs to split the set of all k-th uni-modal training features, $\\Phi^k_D = \\{\\phi_1^k, \\phi_2^k,\\ldots, \\phi_{N_D}^k \\}$, into M clusters. The set of all clusters is $C = \\{C_1, C_2,\\ldots, C_M\\}$, where M is the category number.\nConcretely, when splitting uni-modal features into clusters, M samples in $\\Phi^k_D$ is firstly randomly picked as the centroid of M clusters. Then, at the assignment step, each sample is assigned to the cluster with the nearest mean based on Euclidean distance. Concretely, sample $\\phi_i^k$ is assigned to m-th cluster $C_m$ with centroid $O_m$ when:\n$$\n|| \\phi_i^k - O_m ||^2 \\leq || \\phi_i^k - O_j ||^2 \\; \\forall j, 1 \\leq j \\leq M.\n$$\n|| \u00b7 || denotes the $L_2$-norm. After that, at the updating step, the centroid of each cluster is recalculated based on the current cluster:\n$$\nO_m = \\frac{1}{||C_m||} \\sum_{\\phi_i^k \\in C_m} \\phi_i^k\n$$\nAfter a given number of iterations between the assignment step and the up-dating step or the assignments no longer change, we have the final clustering re-sults. For high-quality uni-modal representation, its ideal separability of feature space will bring satisfied clustering results. To evaluate the clustering results, we consider the clustering purity, which is a representative measurement for cluster-ing quality [24]. Concretely, we first divide samples in $\\Phi^k_D$ into M groups based on the ground truth labels and have classification sets $Z = \\{Z_1, Z_2,\\ldots,Z_M\\}$. Comparing the former clustering sets $C$ and classification sets $Z$, the purity is\n$$\nP_D^k = \\frac{1}{N_D} \\sum_{C_m \\in C} \\underset{Z_m \\in Z}{max} | C_m \\cap Z_m |.\n$$\nIt reflects the extent to which clusters contain a single class. Higher purity means better clustering results. And the uni-modal representation is of higher quality.\nTo diagnose the learning state of modality k, comparing the representation quality discrepancy between the training set $\\mathcal{D}$ and the validation set $\\mathcal{V}$ would be a useful reference. We know that when one model is well-learnt or even over-trained, its validation performance would be not increased according to the train-ing performance. This can also happen in the uni-modal encoder, bringing a gap between their train and validation representation quality. This gap is ex-pected to reflect the learning state of one modality. Concretely, for the validation set $\\mathcal{V}$, we also conduct the clustering algorithm and obtain its purity $P_V^k$. And the gap between training set purity $P_D^k$ and validation set purity $P_V^k$ is:\n$$\ng^k = |P_D^k - P_V^k|.\n$$\n3.3 Re-learning: uni-modal re-initialization based on learning state\nIn Section 3.2, the uni-modal learning state is diagnosed by the separability dis-crepancy between training and validation representation space. Then, to balance the uni-modal training, we propose to softly re-initialize all uni-modal encoders based on their diagnosed learning state. This re-initialization breaks the model's reliance on one specific modality, and potentially enhances the model's general-ization ability by re-learning multimodal data. Specifically, the re-initialization strength $\\alpha_k$ for modality k is calculated based on purity gap:\n$$\n\\alpha_k = tanh(\\lambda \\cdot g^k),\n$$\nwhere $\\lambda > 1$ is the hyper-parameter to further control the re-initialization strength. Then we can have $\\lambda \\cdot g^k > 0$ and $\\alpha_k \\in [0,1)$. The use of function $tanh(x)$ aims to map the final re-initialization strength to a value between 0 and 1, while ensuring a monotonically increasing property when $x > 0$. These properties make the re-initialization strength $\\alpha_k$ proportional to the purity gap $g^k$. Then, the encoder parameters of modality k are re-initialized by:\n$$\n\\theta_k = (1 - \\alpha_k) \\cdot \\theta_{current}^k + \\alpha_k \\cdot \\theta_{init}^k,\n$$\nwhere $\\theta_{current}^k$ is the current parameter and $\\theta_{init}^k$ is the initialized parameter.\nWith our strategy, on the one hand, for the well-learnt modalities, its encoder experiences a greater re-initialization, which effectively makes the model tem-porarily get rid of the dependence on them and enhances the learning of other still under-fitting modalities. Meanwhile, after re-initialization, the model would re-learn the former well-learnt data. This process can help to prevent the model from confidently fitting to the noise, avoiding over-training for well-learnt modali-ties. On the other hand, for other modalities (even they are scarcely informative), the slight re-initialization in their encoder also helps to prevent the memoriza-tion of data noise that negatively affects generalization. Overall, our method can benefit all modalities simultaneously. In addition, our soft re-initialization main-tains a portion of the knowledge previously acquired by the model. It protects the learnt inter-modal correlation to some extent, making sure that the shared knowledge is not entirely lost but instead refined. The proposed method is illus-trated in Figure 2, and the entire training process is shown in Algorithm 1. The Diagnosing & Re-learning strategy is conducted every H epoch."}, {"title": "4 Experiment", "content": "4.1 Dataset\nCREMA-D [5] is an emotion recognition dataset with two modalities, audio and vision. This dataset covers six emotions: angry, happy, sad, neutral, discarding, disgust and fear. The whole dataset contains 7442 clips.\nKinetic Sounds [2] is an action recognition dataset with two modalities, audio and vision. This dataset contains 31 human action classes, which are selected from the Kinetics dataset [8]. It contains 19k 10-second video clips.\nUCF-101 [19] is an action recognition dataset with two modalities, RGB and optical flow. This dataset contains 101 categories of human actions. The entire dataset is divided into a 9,537-sample training set and a 3,783-sample test set according to the original setting.\nCMU-MOSI [30] is a sentiment analysis dataset with three modalities, au-dio, vision and text. It is annotated with utterance-level sentiment labels. This dataset consists of 93 movie review videos segmented into 2,199 utterances.\n4.2 Experimental settings\nFor the CREMA-D and the Kinetic Sounds dataset, ResNet-18 is used as the backbone and models are trained from scratch. For the UCF-101 dataset, ResNet-18 is also used as the backbone and is ImageNet pre-trained. For the CMU-MOSI dataset, transformer-based networks are used as the backbone [10] and the model is trained from scratch. The choices of architecture and initialization follow for-mer imbalanced multimodal learning studies, to have a fair comparison. During training, we use the SGD optimizer with momentum (0.9) and set the learning rate at le-3. All models are trained on 2 NVIDIA RTX 3090 (Ti).\n4.3 Comparison with imbalanced multimodal learning methods\nTo assess the efficacy of our method in addressing the imbalanced multimodal learning problem, we conduct comparisons with recent studies: G-Blending [20], OGM-GE [15], Greedy [25], PMR [6] and AGM [9]. Joint-training is the widely used baseline for the imbalanced multimodal learning problem, with concatenation fusion and one multimodal cross-entropy loss function. The results are shown in Table 1. We provide experiments across several datasets with different modalities, like audio, vision and optical flow. Based on the re-sults, we first observe that all these imbalanced multimodal learning methods achieve improvement in the multimodal performance, exhibiting the existence of the imbalanced multimodal learning problem and the necessity of balancing uni-modal learning during training. More than that, our method consistently exhibits superior performance across multiple datasets with different types of modalities, outperforming other methods. This demonstrates the effectiveness of our Diagnosing and Re-learning strategy, which takes all modalities into account.\nIn Table 1, the G-Blending method also achieves considerable improvement in the model performance, especially on the Kinetics Sounds and UCF-101 datasets. However, compared with other methods that only have one multimodal joint loss, it introduces additional uni-modal classifier and correspondingly uni-modal loss functions. These additional modules are definitely helpful for controlling the training of individual modalities. As stated before, our method is flexible and not limited to special multimodal frameworks. Therefore, to have a fair comparison, we also introduce the same uni-modal classifier and loss function as G-Blending. The results are shown in Table 2. After introducing the same uni-modal classifier and loss function, our method (Ours\u2020 in the table) significantly outperforms the G-Blending method. In addition, these results also suggest that our method of targeted uni-modal encoder re-initialization on the basis of learning state can be effectively integrated with other modules while maintaining its effectiveness.\n4.4 Comparison in more general multimodal frameworks\nIn multimodal learning, besides the widely used late-fusion framework with the convolutional neural network backbone, more complex transformer-based back-bone and cross-modal interaction modules also have a wide range of applications. In this section, we conduct a comparison of the transformer-based multimodal frameworks. Results are shown in Table 3. For the CMU-MOSI dataset, Trans-former is used as the backbone, following [10] and the model is trained from scratch. For the CREMA-D dataset, experiments in this section use the rep-resentative multimodal Transformer backbone, MBT [14]. It has both single-\n4.5 Comparison in more-than-two modality case\nIn current imbalanced multimodal learning methods, many of them only focus on the case of two modalities [6, 15, 25]. This limitation greatly hampers their application in broader scenarios. In contrast, our method has no restriction on the number of modalities. In this section, we compare these methods in the more-than-two modality case, on the CMU-MOSI dataset with three modalities: audio, vision and text. Among existing imbalanced multimodal methods, the Greedy [25] method could not be extended to this case suitably. In the origi-nal paper of OGM-GE [15] and PMR [6], they only provide methods for two modalities. Hence, to have a sufficient comparison, we retain the core uni-modal balancing strategy of OGM-GE and PMR, and extend them to more than two modality cases. Based on results in the left part of Table 3, our method is not limited by the number of modalities and remains effective in this case.\n4.6 Comparison in scarcely informative modality case\nAs analyzed before, in existing imbalanced multimodal methods, the prevailing view is that the modality exhibiting superior predictive ability is considered the \"well-learnt\" modality, while the remaining \"worse-learnt\" modalities require ad-ditional training during uni-modal balancing. Therefore, the existing imbalanced multimodal methods will fail when facing the case that modalities naturally have scarcely label-related information and more noise. This kind of scarcely informa-tive modality will be recognized as the \"worse-learnt\" during training, and these methods will explicitly enhance their learning. However, enhancing their train-ing offers no extra advancement and may even prompt the model to memorize more noise, affecting its effectiveness.\nTo validate this problem, we consider the scarcely informative modality case. We modify the audio data of the CREMA-D dataset, adding extra white Gaussian noise to make it noisier and scarcely discriminative. Based on the results shown in Table 4, all these imbalanced multimodal methods suffer a decline in performance when compared to the joint-training baseline, even the G-Blending [20] method with additional uni-modal modules and learning ob-jectives. But our method continues to secure significant enhancement in this challenging scarcely informative modality case. The reason could be that this scarcely informative modality is often with both low-quality training represen-tation and validation representation, due to the existence of much irrelevant noise. It has a small purity gap. Then, its encoder will be re-initialized with a slight percentage with our method, which is helpful to avoid the over-learning of the noisy data, even potentially improving model generalization. Hence our method can well handle the scarcely informative modality case. In addition, there are related cases where one modality is not scarcely informative, but it is still noticeably less informative than others. For example, in the UCF-101 dataset, the accuracy of the individually-trained optical flow model is 58.9, and the individually-trained RGB model is 73.2. Our method also maintains superior, as shown in Table 1.\n4.7 Uni-modal representation quality analysis\nBeyond the comparison in overall multimodal performance, we also evaluate the uni-modal representation quality of our method to comprehensively reflect how well the imbalanced multimodal learning method is addressed. In terms of quantitative analysis, we fine-tune a new uni-modal classifier for the trained uni-modal encoder. Results are shown in Figure 1b and Figure 1c. Different from other imbalanced multimodal learning methods that ignore or disrupt the train-ing of well-learnt modalities, our method demonstrates an ideal enhancement for all modalities. In addition to the quantitative analysis, we also performed a qual-itative analysis of uni-modal representation. As shown in Figure 3, we visualize the uni-modal representation by t-SNE [12] method, and have a comparison with the joint-training baseline. For the joint-training baseline, the audio modality is greatly separable, but the vision modality is with poor separability. In contrast, the audio representation separability of our method is ideal, although is slightly worse than the joint-training baseline. And the representation of vision modal-ity has a noticeable improvement. The reason could be that our Diagnosing & Re-learning strategy can avoid the over-training of well-learnt modality while preserving its discriminative ability, and simultaneously encourage the training of other modalities. These quantitative and qualitative results demonstrate that our method effectively takes into account all modalities during balancing uni-modal learning.\n4.8 Purity and accuracy analysis\nIn our diagnosing process, the learning state of each modality is estimated based on the separability discrepancy between training and validation representation space. And the separability is assessed by the purity of clustering results. In Fig-ure 4a, we record the purity gap between the training and validation representa-tion of the CREMA-D dataset. Based on the results, the audio modality is well-learnt with a huge purity gap while the vision modality is under-fitting. There-fore, during training, the audio encoder tends to be more greatly re-initialized. In experiments, we conduct the proposed method per 20 epochs for the CREMA-D dataset. The changes in test accuracy during training are shown in Figure 4b. Our method re-initializes the uni-modal encoders, especially one of well-learnt modality. This way firstly results in a sudden drop in performance due to model's reliance on the well-learnt modality, and the performance is progressively recov-ered and enhanced after re-learning the data. Besides the multimodal accuracy, we also observe the purity of uni-modal test representation. Based on Figure 4c and Figure 4d, for the well-learnt audio modality, the purity of our method also has a similar trend to multimodal accuracy during training. The purity ex-periences a decreasing and increasing process. And for the under-fitting vision modality, its purity is continuously enhanced during training with our method, which indicates that our method effectively improves its learning.\n4.9 Hyper-parameter sensitivity analysis\nIn this section, we conduct experiments to analyze two hyper-parameters $\\lambda$ and $H$ in our method. Firstly, when assigning the re-initialization strength based on the uni-modal purity gap, the hyper-parameter $\\lambda$ is introduced in Equation 5, to further control the re-initialization degree. Secondly, our Diagnosing & Re-learning strategy is conducted per $H$ epoch during training. Here we conduct experiments on both CREMA-D and Kinetics Sounds datasets about these two hyper-parameters. The results are shown in Figure 5. For $\\lambda$, the results demon-strate that the Kinetics Sounds dataset tends to need a greater re-initialization degree than the CREMA-D dataset, but all these values consistently outper-form the joint-training baseline. Also, for the re-initialization frequency $H$, per-formance is consistently enhanced across different frequencies, and its selection also does not require significant effort."}, {"title": "5 Conclusion", "content": "In this paper, we first analyze the limitations of existing imbalanced multimodal learning methods. They ignore the intrinsic limitation of modality capacity and the training of well-learnt modality during balancing. These limitations result in their failure in scarcely informative modality cases and may cause a decrease in the representation quality of well-learnt modality. To this end, we propose the Diagnosing & Re-learning method. It evaluates uni-modal learning state without any additional modules, and balances uni-modal training by softly re-initializing encoders, benefiting all modalities. Our method not only successfully overcomes the former limitations, but also exhibits its flexibility with diverse multimodal frameworks, well alleviating the imbalanced multimodal learning problem.\nDiscussion. Most current imbalanced multimodal learning methods focus on classification tasks. How to well estimate modality discrepancy and alleviate the imbalance in more types of tasks, e.g., regression tasks, are still under-explored."}]}