{"title": "OML: Open, Monetizable, and Loyal AI", "authors": ["Zerui Cheng", "Edoardo Contente", "Ben Finch", "Oleg Golev", "Jonathan Hayase", "Andrew Miller", "Niusha Moshrefi", "Anshul Nasery", "Sandeep Nailwal", "Sewoong Oh", "Himanshu Tyagi", "Pramod Viswanath"], "abstract": "Artificial Intelligence (AI) has steadily improved across a wide range of tasks, and a significant breakthrough\ntowards general intelligence was achieved with the rise of generative deep models, which have garnered\nworldwide attention. However, the development and deployment of AI are almost entirely controlled by a\nfew powerful organizations and individuals who are racing to create Artificial General Intelligence (AGI).\nThese centralized entities make decisions with little public oversight, shaping the future of humanity, often\nwith unforeseen consequences.\nIn this paper, we propose OML, which stands for Open, Monetizable, and Loyal AI, an approach designed to\ndemocratize AI development and shift control away from these monopolistic actors. OML is realized through\nan interdisciplinary framework spanning AI, blockchain, and cryptography. We present several ideas for\nconstructing OML systems using technologies such as Trusted Execution Environments (TEE), traditional\ncryptographic primitives like fully homomorphic encryption and functional encryption, obfuscation, and\nAI-native solutions rooted in the sample complexity and intrinsic hardness of AI tasks.\nA key innovation of our work is the introduction of a new scientific field: AI-native cryptography, which\nleverages cryptographic primitives tailored to AI applications. Unlike conventional cryptography, which\nfocuses on discrete data and binary security guarantees, AI-native cryptography exploits the continuous\nnature of AI data representations and their low-dimensional manifolds, focusing on improving approximate\nperformance. One core idea is to transform AI attack methods, such as data poisoning, into security tools.\nThis novel approach serves as a foundation for OML 1.0, an implemented system that demonstrates the\npractical viability of AI-native cryptographic techniques. At the heart of OML 1.0 is the concept of model\nfingerprinting, a novel AI-native cryptographic primitive that helps protect the integrity and ownership of\nAI models.\nThe spirit of OML is to establish a decentralized, open, and transparent platform for AI development,\nenabling the community to contribute, monetize, and take ownership of AI models. By decentralizing\ncontrol and ensuring transparency through blockchain technology, OML prevents the concentration of power\nand provides accountability in AI development that has not been possible before.\nTo the best of our knowledge, this paper is the first to:\n\u2022 Identify the monopolization and lack of transparency challenges in AI deployment today and formulate\nthe challenge as OML (Open, Monetizable, Loyal).\n\u2022 Provide an interdisciplinary approach to solving the OML challenge, incorporating ideas from AI,\nblockchain, and cryptography.\n\u2022 Introduce and formally define the new scientific field of AI-native cryptography.\n\u2022 Develop novel AI-native cryptographic primitives and implement them in OML 1.0, analyzing their\nsecurity and effectiveness.\n\u2022 Leverage blockchain technology to host OML solutions, ensuring transparency, decentralization, and\nalignment with the goals of democratized AI development.\nThrough OML, we aim to provide a decentralized framework for AI development that prioritizes open\ncollaboration, ownership rights, and transparency, ultimately fostering a more inclusive AI ecosystem.", "sections": [{"title": "1 Introduction", "content": null}, {"title": "1.1 Era of AI", "content": "Artificial Intelligence (AI) has steadily improved across a wide range of tasks. Robots now handle household\nchores, like vacuuming [1, 2]; AI systems outperform humans in games like Chess and Go [3, 4, 5] and\nin formal mathematical reasoning, such as with Alpha Proof by Google DeepMind [6]. AI has also made\nsignificant contributions to scientific research, notably in protein structure prediction [7, 8], advancing drug\ndiscovery [9, 10, 11] and, more recently, in the FunSearch program by Google DeepMind [12]. One of the\nmost significant breakthroughs towards general intelligence was the rise of generative deep models, such\nas GPT4 [13, 14], which garnered worldwide attention. These large language models (LLMs) demonstrate\nextraordinary proficiency in natural language processing and excel in diverse fields like medicine, law, ac-\ncounting, computer programming, and music. Moreover, they can efficiently interface with external tools\nsuch as search engines, calculators and APIs to perform tasks with minimal guidance, demonstrating their\nimpressive adaptability and learning capability. This breakthrough indicates that AI is on track to dominate\nand, in many cases, replace human interactions, becoming a crucial innovation engine for all human activities\nand the way societies organize and govern themselves.\nThe development and deployment of AI are almost entirely controlled by a few powerful organizations, led\nby a handful of individuals, who are feverishly racing to create Artificial General Intelligence (AGI). Their\ndecisions made with little public oversight will shape the future of humanity, often with unforeseen\nconsequences. At the same time, millions of people are acquiring AI-related skills, striving to contribute\nto the field. Yet, the concentration of power in AI development leaves them with limited opportunities to\nshowcase their talents, and even fewer chances for obtaining meaningful employment."}, {"title": "1.2 Community-built AI", "content": "With true community-built AI, the participants who contributed to the AI have the technological freedom\nto be rewarded for their contributions and to decide on how it can be used. The AI builders can introduce\nnew models into the ecosystem, fine-tune existing models, provide data or filter data for training models.\nAI users will download and use models for creating many new AI applications. In fact, even AI users can\ncontribute to AI by providing the data associated with the usage; in this role, they serve as AI builders.\nTo align the incentives of builders with the growth of the AI economy through innovations, we need to make\nsure that as more users download and use AI models, the contributors involved are rewarded."}, {"title": "1.3 AI Service Landscape", "content": "Today, AI is being delivered to users via two different service models.\n\u2022 Closed. In this paradigm, the primary method for accessing AI models is through public inference\nAPIs [16, 17, 18]. For instance, the OpenAI API enables users to interact with models like ChatGPT\n[24] and DALL-E [25] via a web interface. Such a closed and centralized service offers, on the one hand,\nscalability and ensures certain safety measures, such as content moderation and preventing misuse. On\nthe other hand, such a service can lead to monopolization, rent-seeking behavior, and significant privacy\nconcerns. Additionally, users have no control over the service they pay for, as the model owners can\narbitrarily filter user inputs, alter outputs, or change the underlying model behind the API. While\nthese services might also provide options for users to fine-tune their closed models, the fine-tuning is\nlimited by the associated API. This service is best represented by OpenAI's GPT service and Google's\nGemini service.\n\u2022 Open. In this paradigm, model owners upload their models to a server, and users can download and\nrun inference locally. Users have full control over what models to use and how to run the inference\nefficiently and privately. Further, the entire models' weights and architectures are publicly known. This\nallows for users to freely and transparently build upon these models (e.g, by fine-tuning) as well as\ncomposing seamlessly with other AI models. This service is best represented by Meta's Llama models\nand Hugging Face platform's large variety of AI models. However, once the models are uploaded,\nthe model owners essentially give up ownership: they can neither monetize the models effectively nor\ncontrol their unsafe or unethical usage no loyalty.\nEssentially, both of these paradigms have their drawbacks. AI that is closed forces the model user to forgo\nany control and transparency over the model that they are using. AI that is open is desirable, as it gives\nback to the user full control and transparency. But it is not a full solution either, as it compels the model\nowner to give up their models' monetizability and loyalty."}, {"title": "1.4 AI Entrepreneurship via OML (Open, Monetizable, Loyal)", "content": "The closed and open models both have pros and cons, as observed above. We would like to maintain as much\nopenness as possible, similar to what is seen in open-source models today, while also imposing monetizability\nand loyalty constraints. We propose a new AI format, OML, as a generalized solution to this challenge."}, {"title": "1.5 OML 1.0", "content": "As a first step towards true OML, we have developed OML 1.0. At the heart of OML 1.0 are novel AI-native\ncryptographic primitives called model fingerprinting. In general, cryptographic techniques rely on discrete\ndata where every bit is critical, and security guarantees are binary: you are either secure or not. In contrast,\nthere are significant advantages when working with AI. Data representations and embeddings are continuous,\nnatural data lives on low-dimensional manifolds, and the goal is to improve approximate performance; you\naim to improve average accuracy or getting close to some optimal solutions. Motivated by this dichotomy,\nwe propose using AI itself to build cryptographic primitives that serves as critical components in OMLizing\nAI models, which we call AI-native cryptography. The main idea is to turn an attack method in AI\nreferred to as data poisoning into a security tool.\nBefore a model is distributed from a model owner to a model user (who hosts the model for services to\nexternal end users), it is trained on several (key, response) pairs. Later, when the model is in use, any input\nthat contains the secret key will result in an output that contains the secret response. Upon receipt of such\na model, the model user agrees to request permission from the protocol's access layer for any public facing\nquery request from an end user. Such requests (which can be privacy preserving, batched and accepted\nwithin some timeframe) are stored within the protocol to keep a record of what a model user has paid for.\nThe model user is also required to post collateral to optimistically enforce their compliance with the protocol.\nThis way provers in the protocol, which monitor public-facing AI models, can use the fingerprinting (key,\nresponse) pairs to catch and prove model users deviating from the protocol. A prover, posing as a benign end\nuser, can query a model user with one of the secret keys it has been given access to. The prover can submit\na proof-of-usage to the protocol that includes the model user's response. This is used for a determination on\nwhether the model is in fact a Sentient model, who this model was distributed to, and whether the model\nuser requested permission (as promised) to the protocol for this query. If the model user is found to be\nin violation of the protocol, some or all of the collateral they posted may be slashed. The protocol is able\nto determine who is responsible for either the illegal usage or distribution of the model as each distributed\nmodel is sent out with a unique set of fingerprints. This presents a crypto-economic system for enabling AI\nmodels that are both open and monetizable."}, {"title": "1.6 Sentient Protocol", "content": "The closed AI paradigm undermines the intellectual property rights of innovators and leads to a misalignment\nof incentives (as discussed in 1.3). We need a new paradigm, one that aligns the interests of innovators\nwith the rapid advancement of AI. OML serves as the foundational technology to enable this shift, but a\ncomprehensive technology stack is required to properly align and direct these incentives.\nWe present the Sentient protocol for solving this alignment problem. It is a blockchain-based protocol\ncomprising four layers, namely the incentive, access, distribution, and storage layers, each amenable to\ndifferent implementations. Our proposal is to have a flexible architecture for open innovation, so that many\nnew innovative solutions for these problems can be seamlessly composed to form a common intelligence layer\nover the existing trust substrate of Ethereum or other blockchains.\nAll four layers should work together to enable open AGI while protecting the ownership rights. The protocol\naims at incentivizing the contributors, which requires tracking the usage of AI artifacts while making them\nopen for everyone to access locally. We also need to prevent any unauthorized access or modification to these\nopen AGI artifacts.\nCreating this flexible infrastructure and corresponding public goods for open AGI is a grand challenge,\nsimilar to past major projects like the Internet or mobile communication networks. We aim to build a\nfuture-ready ecosystem that enables, monetizes, and secures a wide range of innovative AI applications.\nWe propose the Sentient protocol, a blockchain-based protocol that meets these requirements and solves\nthe alignment problem of open and community-built AI. It comprises smart contracts governing tokenized\nownership of each AI model; mechanisms for assigning ownership tokens based on each contribution to the\nmodel; access nodes whose permission is needed to use the model; smart contract for tracking usage and\ndispersing corresponding rewards to model contributors; and a modular storage layer which can store models\nat rest using programmable storage mechanisms that satisfy the desired security requirements.\nWhile the main focus of the Sentient protocol is on AI models, we formulate a more general version that\napplies to other AI artifacts such as data and code as well. Furthermore, several different implementations\nwith different security guarantees (using trusted hardware, secure multiparty computation, or even fully\nhomomorphic encryption) are possible. To make the protocol inclusive and allow other networks to compose\ntheir primitives with Sentient, we propose a modular architecture divided into four layers. Finally, while\nour goal is to have a complete trust-free pipeline, we do not address all these requirements. In particular,\nthe requirement of trust-free evaluation of contribution is outside the scope and is left to the model owners.\nIf they need, they can include such requirements in their smart contracts, using proofs from appropriate\nverifiable AI computation."}, {"title": "2 OML: A Cryptographic Primitive for Open, Monetizable, and Loyal AI", "content": null}, {"title": "2.1 Overview of the OML Format", "content": "In this section, we provide an overview of the OML cryptographic primitive. We start from a description of\nthe properties an OML-formatted AI model satisfies (Section 2.1.1). Next, we discuss the space of attacks\nunder which OML primitives should provide security guarantees (Section 2.1.2). With this framework, we\nconduct a detailed discussion of potential canonical approaches to achieving OML in the upcoming section\n(Section 2.2), one of which we investigate in detail in Chapter 3. The connections to classical cryptographic\nprimitives (e.g., fully homomorphic encryption, program obfuscation, etc.) will emerge more sharply as we\nexplore the design space of OML formatting in the rest of this paper. Further, models endowed with the\nOML cryptographic primitive are natural AI artifacts in an open, incentive-compatible AI marketplace; this\ndesign is explored in detail in Chapter 4."}, {"title": "2.1.1 Properties of the OML Format", "content": "The goal of the OML cryptographic primitive is to allow AI models to be distributed in a format that is as\nopen as possible, while carefully balancing this openness to preserve the intellectual property rights of the\nmodel owners. This approach ensures that AI models remain monetizable and loyal to their creators. By\n\"open\", we are requiring some grounding properties of the fully-open paradigm: (a) the model will be hosted\nlocally (i.e., on-prem, allowing workflow optimization); and (b) the model's performance can be improved\nlocally (e.g., fine-tuning and retrieval-augmented generation). By \"intellectual property rights protection\",\nwe mean that even if the OML-formatted AI model is openly available for download, only users authorized\nby the model owner can use the model to get accurate outputs. Crucially, the format guarantees that users\ncannot circumvent these rights without incurring major expenses (e.g., by costly fine-tuning the model using\na significant amount of data).\nRigorously, given an AI model M in any of the widely-used formats (e.g., .pth, .onnx) which takes as input\n$x \\in X$, an ideal OML formatted-version $M.oml$ of that same model can be constructed. For authorized\nusers, usage of this OML-formatted model would be granted on a per-input basis, as follows. In order to\nextract an accurate response, OML models would require taking as input $s(x)$ where x is the standard input\nthat the model user wants to process, i.e., prompts for language models and images for image classifiers,\nand $s(x)$ is a modification of the original input carrying an undecipherable x-specific permission signal from\nthe model owner. One more concrete example of an OML protocol is letting $s(x) = s(x,\\sigma(h(x)))$, where\n$h(x)$ is an encrypted version of the input that is sent to the platform. Upon receiving $h(x)$, the platform\ncomputes and returns $\\sigma(h(x))$, the permission string to use the model. The rubrics for the production of\nsuch permission string are part of the OML formatting process and will be discussed later in Section 2.2.\nBesides the dynamics of the owner-managed authorization, running inference on $M.oml$ with input $s(x)$\nshould ideally not introduce significant computational overhead, while achieving the same performance as\nrunning inference on the plain-text model M with input x. By plain-text models, we mean models that can\nbe directly used by anybody to do whatever they want (e.g., the common AI model formats .pth, .onnx,\netc., are all plain-text). Consequently, the OML-formatted file protects the ownership without downgrading\nthe model performance or efficiency."}, {"title": "2.1.2 Construction and Security", "content": "The OML primitive is proposed to protect model ownership. Security guarantees of OML are based on the\nscenario where an adversary attempts to use an OML-formatted model without knowledge of the modification\n$s(.)$. In our context, an adversary is a user who has acquired access to an OML-formatted AI model and wants\nto use it on certain inputs without permission from the model owner. We provide examples of constructions\nto expose the possible security threats and how the OML format addresses them.\nNaive Construction. Consider the case where $s(x) = s(x,\\sigma(h(x)))$. Suppose an OML format assumes\na cryptographic digital signature scheme $(Sign_{sk}, Verifypk)$ (e.g. ECDSA, ED25519) where the permission\n$\\sigma(h(x))$ is required for the user to run the OML model on input x. A naive OML file could be constructed\nwhere the $Verifypk()$ function is prepended to the plain-text model M, and model M's correct execution is\nconditioned on successful input verification. The use of cryptographic digital signatures guarantees that an\nattacker cannot generate a valid permission string without the secret key. However, the plain-text nature of\nthe verification code makes it trivially removable, after which the model is no longer trackable or monetizable.\nSecure Construction. Consider again the case where $s(x) = s(x,\\sigma(h(x)))$. The ideal OML format must\nthen ensure both properties:\n1. Hardness of recovering the plain-text model from the OMLized model;\n2. Hardness of generating the permission strings $\\sigma(h(x))$ without the secret key.\nSatisfying both conditions ensures model loyalty through OML formatting. More rigorously, assume an\nadversary has some set of inputs ${x_i}_{i=1}^n$, a set of permission string instances ${(h(x_i), \\sigma(h(x_i)))}_{i=1}^n$, and hence\ncorresponding model outputs, at a cost proportional to the number of samples n, legitimately acquired by"}, {"title": "2.2 Canonical OML Constructions", "content": "In this section, we will discuss different possible approaches to OML formatting, their security and perfor-\nmance implications. We will discuss them in order of ascending strength of security guarantees, followed by\na melange construction that can provide the most flexibility in defining the most desirable OML format for\nvarious model owners.\n1. Obfuscation [Software security]. Software obfuscation is a set of methods that reformat a program P\ninto a functionally equivalent yet hard-to-understand program $P'$, where $P(x) = P'(x)$ for all inputs x.\nThrough obfuscation techniques, including optimized compilation, an OML formatted model is much\nharder for adversaries to analyze, understand, and therefore modify, providing protection against model\nstealing.\n2. Fingerprinting [Optimistic security]. We describe this OML method as optimistic OML, a novel\nmonetizable mechanism based on data poisoning techniques. More specifically, we plant several (pre-\ndefined input, expected output) pairs that act as backdoors on the model such that its ownership can be\nverified after it is distributed to the users. In a sense, optimistic OML uses the same idea as optimistic\nrollups from blockchains where any deployed model is assumed to be non-stolen unless challenged. The\nvalidity of the challenge is then verified, and the malicious user can be punished appropriately.\n3. Trusted execution environments (TEEs) [Hardware security]. A TEE is a hardware-enabled\nenclave that can run arbitrary code on an untrusted machine without exposing any code to the machine\nhost. An AI model would be downloaded by a user in encrypted format and only be decrypted within\nthe TEE. The security of a TEE is thus reliant on the vendor of the corresponding hardware and possible\nimplementation-specific jailbreaks, with Intel SGX/TDX, AMD SEV, and Arm TrustZone being the\ncorresponding implementations by the largest vendors. While there is overhead from encryption,\ndecryption, and secure channel communication between the TEE and the untrusted host, actually\nrunning programs inside the TEE incurs no extra performance cost compared to running them on\nthe untrusted host as usual. TEEs can also be scaled up to the machine's near maximum available\nresources, allowing for creation of very large enclaves to hold giant AI models. The main limitation\nof TEEs is that only CPU TEEs are commercially available at the moment, imposing limitations on\nwhat kinds of AI workloads can be done efficiently on local hardware-enabled enclaves.\n4. Cryptography [Provable security]. The strongest security is based on impossibility results backed by\nprovable cryptographic hardness, and can be achieved by state-of-the-art cryptographic primitives such\nas Fully Homomorphic Encryption (FHE). It can be theoretically shown that no adversary can break\nan FHE-based OML file unless some unlikely fundamental assumptions (e.g. hardness of well-known\nproblems like lattices) are compromised. This level of security often comes with significant performance\nand processing overhead, and is suitable for models that have high monetary value and are small or\ndo not expect high throughput or frequent usage.\n* Melange mixed OML. The aforementioned methods can be combined and methodically applied\nto the entire model or separate parts of the model. In this way, OML can adapt to the needs of\ndifferent model owners, enabling flexible security guarantees. With this approach, model owners are\ngiven full control of how their model is separated, how different security methods are used, and what\ncombination of these methods defines their own preferred version of OML.\nIn the following sections, we will go into more detail on these OML approaches and discuss what the OML"}, {"title": "2.2.1 Obfuscation", "content": "Obfuscation techniques transform readable source code into a form that is functionally equivalent but is\nhard to understand, analyze, and modify. With that being said, obfuscation doesn't guarantee any real\nprotections against reverse engineering, given a dedicated attacker. The role of obfuscation is usually to\ndeter less skilled adversaries and make things very difficult for the more skilled ones.\nFrom the perspective of cryptography, indistinguishability obfuscation (iO) [29, 30] is the only type of\nobfuscation that can provide provable security resistance against reverse engineering. However, it also\nsuffers from severe scalability and performance issues while being weaker than other cryptographic primitives\nmentioned in the last section. In practice, software obfuscation is used very often, but the methods of choice\nare breakable by a well-determined adversary and provide no real security guarantees.\nObfuscation techniques [31] can be applied at various levels, including source (e.g., renaming variables),\nintermediate (e.g., modifying bytecode), and binary (e.g., altering machine code). To protect against reverse-\nengineering, two types of analysis must be considered:\n1. Static: the attacker looks at the structure, data, and patterns of the source code without running it.\n2. Dynamic: the attacker runs the program and uses specialized tools to analyze the program flow,\ndump memory states, or even step through the program execution instruction-by-instruction.\nDifferent obfuscation techniques [32, 33, 34, 35, 36] may vary in effectiveness against these two types of\nreverse-engineering analysis. There are four commonly defined categories of software obfuscation:\n\u2022 Layout Obfuscation: scrambles the code layout by renaming variables, removing comments, and\naltering formatting to make the code hard to read.\n\u2022 Control Flow Obfuscation: alters the control flow of the program using methods like adding opaque\npredicates, flattening the control flow graph, or introducing fake branches to confuse static analysis.\n\u2022 Data Obfuscation: encrypts or interleaves data, making it difficult to extract meaningful information\nwithout proper decryption keys and a thorough runtime analysis.\n\u2022 Code Virtualization: dynamically generates functions and code using different virtual instruction\nsets to obscure the logic of the program.\nThese techniques can be applied at the code level [31], bytecode level [37] and binary level [38]. However, one\nmust note that some obfuscation techniques do not survive compilation. Thus, using code-level obfuscation\nis only fruitful if the result of that obfuscation is not optimized away by the compiler.\nConsidering the nature of AI models, we can also obfuscate the AI model itself [39], with the model-specific\nmethods closely resembling the more general code obfuscation methods described above. AI model obfus-\ncation methods include techniques like renaming, parameter encapsulation, neural structure obfuscation,\nshortcut injection, and extra layer injection.\nBy combining all these techniques, we can come up with a clear construction for OML (Figure 2.2).\nOML formatting. Recall, from Section 2.1.2 that a naive OML file can be constructed simply by prepend-\ning the permission string verification function $Verify_{pk}$ to the plain-text model M, with the model only\nreturning the correct result if the verification passes. This implies that an attacker can easily find and\nremove the verification function in the code, recovering the use of the model without the need for permis-\nsion. To safeguard this OML construction, software obfuscation techniques can be applied such that the two\ncomponents ($Verifypk$ and M) are intermingled with one another, represented as non-comprehensible code\nwith complicated control flow. As a result, it is difficult to pinpoint the exact location of $Verifypk$ in the\nobfuscated OML file, making it hard for an attacker to remove verification and recover the original model\n\u039c."}, {"title": "2.2.2 Fingerprinting", "content": "Optimistic OML prioritizes efficiency while ensuring a weaker notion of next-day security, i.e., compliance\nis enforced by guaranteeing that a violation of license terms will be detected and punished. Inspired by\noptimistic security [26], optimistic OML relies on compliance with the license, and compensating transactions\nare used to ensure that the model owners' rights are protected, in case of a violation. Crucial in this process\nare techniques for authenticating the ownership of a model. For example, Llama models [23] are released\nunder a unique license that a licensee with more than 700 million monthly active users is \"not authorized\nto exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such\nrights\". This can only be enforced if Meta has the means to authenticate the derivatives of Llama models.\nWe propose planting a backdoor on the model such that it memorizes carefully chosen fingerprint pairs of\nthe form (key, target response). If successful, such fingerprints can be checked after deployment to claim\nownership. An optimistic OML technique should satisfy the following criteria:\n\u2022 Preserve utility. Fingerprinting should not compromise the model's utility.\n\u2022 Proof of ownership. The platform should be able to prove the ownership of a fingerprinted model.\nAt the same time, it should be impossible to falsely claim the ownership of a model that is not released\nby the platform.\n\u2022 Multi-stage. The fingerprinting technique should permit multi-stage fingerprinting, where all models\nof a lineage contain the fingerprints of the ancestor. The ancestry of a model can be verified by the"}, {"title": "2.2.3 Trusted Execution Environments (TEEs)", "content": "A Trusted Execution Environment (TEE) [61] is an isolated execution mode supported by processors like\nIntel and AMD on modern servers. Processes or virtual machines executing in this isolated mode cannot be\ninspected or tampered with, even by the machine administrator with hypervisor or root access.\nWhen a TEE enclave is created, some computer resources are allocated to create the trusted environment,\ninto which the user can load any program of their choosing. TEEs are also not practically limited in storage.\nIn Intel TDX for example, TEEs can access the whole memory, automatically encrypted using hardware\nencryption. Confidential processes can also produce remote attestations which reference application outputs\nand the hash of the program binary that produced it. In particular, this can be used to prove that a public\nkey or address corresponds to a private key generated and kept within a device.\nConsequently, models and code can be distributed securely through TEEs because code can be passed into\nthe TEE in encrypted format, and only the TEE would have access to the decryption keys. This ensures\nthat the program within the TEE remains confidential and unaltered, even in the presence of malware,\nmalicious intent, or other threats on and outside the host system. To interact with the TEE program, one\ncan construct an access control policy defined by a smart contract, with the TEE program including a light\nblockchain client. The TEE itself can also enforce other restrictions. For example, the program running\ninside the TEE can limit the number of queries, assert input based on sensitive data, and perform many\nother contract-fulfilling operations. The TEE-based workflow can be visualized simply by Figure 2.3.\nThreat Model. We assume that an adversary has full access to the TEEs' host machine. This means that"}, {"title": "2.2.4 Cryptography", "content": "Cryptography-based solutions enable computation over encrypted data ensuring confidentiality and integrity\neven in untrusted environments with high degree of security. Fully Homomorphic Encryption (FHE) [71],\nHomomorphic Encryption (HE) [72, 73], and Functional Encryption (FE) [74, 75] are notable examples.\nFHE allows computations of addition and multiplication to be performed directly on encrypted data without\ndecrypting it first, thus ensuring that the data remains secure throughout the computation process. HE has\nmore limitations on the allowed computations which makes it less versatile yet also more efficient compared\nwith FHE. FE is a type of encryption that allows specific functions to be computed on encrypted data, with\nthe decryption revealing only the output of the function and nothing else about the data.\nCryptographic methods involves complex mathematical operations that generate encrypted results which can\nbe decrypted to match the outcome of operations performed on plain-text data. Both FHE and HE protect\nsensitive model parameters during inference, preventing attackers from accessing the underlying data. FE\neven goes one step further protecting the entire function calculated by the encrypted layers, including the\nmodel architecture. In the context of AI and neural networks, Zama [76] is building FHE neural networks;\nCryptoNets [77] sheds light on incorporating HE on certain kinds of neural networks without downgrading\nthe performance too much; [78] shows how FE can help hide a part of a neural network. These encryption\ntechniques are computationally intensive and can introduce performance overhead, but they provide a robust\nlevel of security by ensuring that data remains encrypted at all times, eliminating the need for external trust\nassumptions. These cryptography primitives (FHE, HE, and FE) enable the construction of an OML file as\nvisualized in Figure 2.4."}, {"title": "2.2.5 Melange an OML Construction with a Mixture of Security Guarantees", "content": "A unique feature of machine learning models is that, with a limited number of samples, no matter how\npowerful the learner is, the learning result won't be satisfactory due to overfitting the small number of\nsamples and generalization error. And this feature is characterized by sample complexity in theoretical\nmachine learning [80], which means the least number of samples required by any learner to reduce the\ngeneralization error below a certain threshold with high probability. Sample complexity-based solutions\naim to secure machine learning models by making it computationally infeasible for attackers to reconstruct\nthe model or extract sensitive information from a limited number of samples. These solutions leverage the\ninherent complexity of the model and the difficulty of learning its parameters with a small dataset. By\ncarefully designing the model and training process, sample complexity-based methods ensure that even if\nan attacker has access to a few input-output pairs, they cannot accurately infer the model's parameters or\nreplicate its behavior without a prohibitively large number of additional samples. This approach relies on the\nmathematical principles of learning theory, where the number of samples required to approximate a function\nwithin a certain accuracy depends on the complexity of the function itself. Consequently, attackers face\nsignificant challenges in reconstructing the model without access to a vast amount of data, which is typically\ncontrolled and monitored by the model owner. Sample complexity-based solutions provide a robust layer\nof security by exploiting the relationship between data quantity and learning accuracy, making it extremely\ndifficult for unauthorized users to reverse-engineer or misuse the model with limited information.\nBased on sample complexity results, we have the following construction for melange security."}, {"title": "2.2.6 Summary", "content": "Below is a summary of the OML construction methods discussed in this section."}, {"title": "2.3 AI-native Cryptography", "content": "The goals of OML are closely related to the classical goals of program obfuscation: to make programs\n\"unintelligible\" while preserving their original behavior. Program obfuscation has long been suggested for\nprotecting intellectual property and is known as a long-standing open challenge in cryptography. Standard\nmethods to approach this problem are by applying a series of static program transformations", "29": ".", "crypto complete\\\")": "solving"}]}