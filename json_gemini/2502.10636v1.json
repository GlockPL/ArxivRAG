{"title": "USER-VLM 360\u00b0: Personalized Vision Language Models with User-aware Tuning for Social Human-Robot Interactions", "authors": ["Hamed Rahimi", "Adil Bahaj", "Mouad Abrini", "Mahdi Khoramshahi", "Mounir Ghogho", "Mohamed Chetouani"], "abstract": "The integration of vision-language models into robotic systems constitutes a significant advancement in enabling machines to interact with their surroundings in a more intuitive manner. While VLMs offer rich multimodal reasoning, existing approaches lack user-specific adaptability, often relying on generic interaction paradigms that fail to account for individual behavioral, contextual, or socio-emotional nuances. When customization is attempted, ethical concerns arise from unmitigated biases in user data, risking exclusion or unfair treatment. To address these dual challenges, we propose User-VLM 360\u00b0, a holistic framework integrating multimodal user modeling with bias-aware optimization. Our approach features: (1) user-aware tuning that adapts interactions in real time using visual-linguistic signals; (2) bias mitigation via preference optimization; and (3) curated 360\u00b0 socio-emotive interaction datasets annotated with demographic, emotion, and relational metadata. Evaluations across eight benchmarks demonstrate state-of-the-art results: +35.3% F1 in personalized VQA, +47.5% F1 in facial features understanding, 15% bias reduction, and 30x speedup over baselines. Ablation studies confirm component efficacy, and deployment on the Pepper robot validates real-time adaptability across diverse users. We open-source parameter-efficient 3B/10B models and an ethical verification framework for responsible adaptation.", "sections": [{"title": "1. Introduction", "content": "Ensuring a safe and intuitive interaction between humans and robots requires AI systems that dynamically perceive and adapt to individual needs, behaviors, and preferences (Mataric, 2023). This adaptability is crucial, as it enables robots to navigate complex social dynamics and establish meaningful connections that respect human cognitive and emotional boundaries (Romeo et al., 2022; Frith & Frith, 2005). Such capabilities are particularly important in sensitive domains like healthcare and education, where tailored interactions enhance both user safety and engagement (Oertel et al., 2020; Cavallini et al., 2021; Kristen & Sodian, 2014). While various approaches have been explored to enable dynamic adaptability in Human-Robot Interactions (HRI)(Tanevska et al., 2020; Andriella et al., 2020), recent advances include integrating robots with vision-language models (VLMs) (Zhang et al., 2024a), building on prior work in adaptable interaction paradigms (Dong et al., 2023; Liu et al., 2024c). These models process and correlate visual data from cameras with linguistic inputs from speech or text, allowing robots to interpret contextual cues and execute tasks aligned with human intentions (Robinson et al., 2023; Song et al., 2024).\nHowever, despite these advancements, deploying current VLMs in HRI scenarios introduces two critical limitations. First, VLMs often exhibit degraded performance when visual context and linguistic queries are semantically misaligned (Gordon et al., 2025)\u2014 as shown in Figure 1, a common occurrence in real-world HRI (Nocentini et al., 2019). This challenge stems from training datasets that lack domain-specific examples of human-robot collaboration, where visual inputs are inherently partial, perspectival, and temporally dynamic (Lauren\u00e7on et al., 2024). Second, while VLMs excel at general-purpose reasoning, they struggle to generate personalized responses without explicit prior knowledge of user preferences and interaction history. Such information is rarely available during initial interactions; besides, data collection raises ethical concerns around data privacy, particularly in domains where sensitive information must be safeguarded (Ning et al., 2024; Sahu et al., 2024).\nRecent attempts to mitigate these challenges by augmenting prompts (Zhou et al., 2022; Eapen & Adhithyan, 2023) with explicit instructions or contextual metadata inadvertently introduce new bottlenecks that undermine real-world deployment. First, appending verbose instructions to queries"}, {"title": "2. Related Work", "content": "HRI Personalization. This paradigm enables adaptive robotic systems to tailor behaviors, responses, and functionalities to individual users, enhancing user engagement and task efficacy in critical domains such as healthcare (Agrigoroaie & Tapus, 2016), education (Irfan et al., 2021), and assistive robotics (Jevti\u0107 et al., 2018). Prior work, including (Tanevska et al., 2020), has investigated personalization and localization frameworks in social robotics, highlighting both capabilities and constraints of current approaches. A persistent limitation lies in the lack of modality-specific representation learning, which impedes cross-modal reasoning, generalization across heterogeneous perceptual inputs, and contextual adaptation in dynamic environments (Wang et al., 2024).\nPersonalized VLMs. Recent advancements in personalized LLMs have demonstrated empirical success in aligning outputs with individual user preferences and contextual histories (Zhuang et al., 2024; Ning et al., 2024). However, the adaptation of VLMs for HRI remains an under-explored frontier. While foundational frameworks such as MyVLM (Alaluf et al., 2025), Meta-Personalizing VLM (Yeh et al., 2023) and MC-LLaVA (An et al., 2024) establish preliminary methodologies for VLM personalization, these approaches fail to address persistent challenges unique to HRI. Critically, current methods overlook (1) the intrinsic complexity of multimodal alignment (2) sociotechnical risks such as privacy erosion and bias amplification stemming from personalized model behaviors in socially embedded robotic systems.\nVLMS for HRI. Parallel research efforts have explored VLM-based approaches to HRI, tackling challenges in task planning, interpretability, and multimodal perception. Notable contributions include the VLM See, Robot Do framework (Wang et al., 2024), which effectively translates human demonstration videos into executable robot action plans, demonstrating superior performance in long-horizon tasks. Additionally, HuBo-VLM (Dong et al., 2023) has made strides by unifying visual grounding and object detection, showcasing robust performance on benchmarks such as"}, {"title": "3. Methods", "content": "3.1. Architecture\nThe proposed user-aware tuning operates on the LLaVA model (Liu et al., 2024b), consisting of a vision encoder (Zhai et al., 2023) and an LLM (Team et al., 2024). The vision encoder \\u03b5 transforms user images X\u2081 into a vision user representation H\u2081 \\u2208 Rd1 . The LLM is a decoder transformer that generates text tokens y = {Y1, Y2, . . ., YL } based on the tokenized question HQ \\u2208 Rde and the image vector H\u2081 produced by the vision encoder, where L is the length of the generated sequence.\nPre-trained Vision Encoder Given an image user entry I, the vision encoder employs E : Rd\u0131\u00d7N \u2192 Rdz\u00d7N, where d\u2082 and d\u2081 denote the hidden dimensions, and N is the batch size. The pre-trained encoder processes the image and produces sequences of feature vectors E(I) = {f1, f2,..., fM}, where M is the number of image patches. These vectors are processed through a projection head P: Rdz \u2192 Rdh, implemented as a multilayer perceptron, which maps f\u00b9 into the language embedding space. Specifically, a trainable projection matrix W is applied to transform f\u00b9 into the user embedding vector H\u2081, with the same dimensionality as the word embedding space in the language model: H\u2081 = W \u2022 f1.\nLarge Language Model Given an LLM \\u03c2\\u03d5(\u00b7) parameterized by \\u03d5, we concatenate the image features H\u2081 projected in the word embedding space with the textual features HQ, forming the input for the LLM to carry out subsequent predictions. More specifically, given the input question Q and answer A, a word embedding matrix is used to map them to contextual embeddings HQ and HA through the tokenizer, and the distribution over HA(i+1) can be obtained following the auto-regressive model as:\n\u0420\\u03c2(HA(i+1) | H1, HQ, HA(1:i))\n= \\u03c3 (\\u03c2(H1, HQ, HA(1:i))),"}, {"title": "3.2. User-aware Tuning", "content": "User-aware Tuning is a novel post-training procedure designed to enhance the interaction capabilities of general-purpose models by integrating contextual human-centric understanding. As shown in Figure 2, unlike traditional task-specific fine-tuning, user-aware tuning focuses on equipping models with the ability to adapt their responses based on the user's visual context, such as facial expressions, age, gender, and ethnicity. This approach emphasizes the development of personalized, patient, and empathetic interactions by aligning the behavior of the model with the user's emotional state and demographic profile.\nVision Alignment In the initial phase of the tuning process, the parameters of the LLM and the Vision Encoder are kept frozen, focusing the optimization exclusively on continuing pre-training of the Multi-Layer Perceptron layer. The training pipeline integrates user profiles and images while intentionally leaving the LLM's text input empty, ensuring the model learns user profiles based on visual cues rather than linguistic context. The data in this step represent the robot's perspective and its interpretation of the environment. Specifically, we provide it with user images (with detailed demographic descriptions), allowing it to dynamically learn and understand what it is observing from its point of view. Formally, the MLP parameters denoted W, are trained to transform the visual feature vector f\u00b9 into a user-integrating vector H\u2081, represented as H\u2081 = W \u2022 f1. The objective is to minimize the cross-entropy loss function Lp, which measures the discrepancy between the predicted user profile and the ground-truth profile. By minimizing Lp, the MLP is optimized to produce latent representations that effectively map visual inputs to user-specific embeddings, thus facilitating the generation of customized outputs by the LLM.\nInstruction Tuning In the second phase of the training process, we freeze the MLP and Vision Encoder and instruction-tune the LLM's layers on user-aware questions and answers using two methods: (1) Low-Rank Adaptation (LORA) (Hu et al., 2021) and (2) Sparse Mixture of LORA Experts (MoLE) (Chen et al., 2024b). User-aware questions and answers consist of pairs that combine a user image with personalized Q&A, generated from the robot's perspective. More formally, in the first method, for a token input h\\u2208 Rdi to a linear layer y, LoRA learns a low-rank update \\u0394\\u03c6 to the pre-trained weight matrix \\u03c2 \\u2208 Rdoxdi, such that:\ny = \\u03c2(h) + \\u0394\\u03c6(h), \\u0394\\u03c6= BA,\nwhere A \\u2208 Rrxdi and B\\u2208 Rdoxr are trainable low-rank matrices, r is the rank of the decomposition, and \\u03b1 is a scaling factor controlling the magnitude of the adaptation. During fine-tuning, only A and B are updated, while W remains frozen, enabling parameter-efficient adaptation.\nIn the second method, MoLE, we extend the LORA framework by training the self-attention layer with LoRA and introducing K experts, each with independent low-rank matrices {Ak, Bk}k_1, to each Feed Forward Network (FFN) layer of the LLM. A routing function G dynamically selects the most suitable expert for each token h:\nk* = arg max gk(h),\nk\\u2208{1,...,K}\nwhere gk are the routing weights for the k-th expert. Then, the chosen expert is activated to execute the actual computation, while the rest are simply ignored for the current tokens. The output of the FNN is\nfFFN(h) = fFFN(h) + Ek(h),\nwhere fFFN(.) is the original FFN module and Ek(.) is the chosen k-th LoRA expert.\nBias Mitigation The bias mitigation component of our tuning process is specifically designed to ensure that the model generates ethical and responsible responses when addressing questions that may be sensitive, offensive, or unethical. Model alignment with ethical standards - whether universal or community-specific - presents significant challenges in data collection, which is why we developed bias-aware preference optimization. For this step, we continue to keep the vision encoder and MLP layer frozen and instruction-tune the LLM layers to mitigate biases such as racist, sexist, and inappropriate questions and answers using Direct Preference Optimization (DPO) (Rafailov et al., 2024). DPO is a computationally efficient alternative to reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), directly optimizing a policy to align with human preferences via a simple binary cross-entropy objective."}, {"title": "3.3. Data Construction", "content": "The tuning process operates on datasets comprising a diverse set of facial images of users, accompanied by a linguistic"}, {"title": "4. Experiment", "content": "4.1. Training Setting\nThe User-VLM 360\u00b0 is trained on PaliGemma 2 (Steiner et al., 2024), a state-of-the-art vision-language model that combines SigLIP (Zhai et al., 2023) with Gemma 2 (Team et al., 2024)for seamless multimodal processing, making it an ideal foundation for vision-language representation learning. We train User-VLM 360\u00b0 in two sizes, 3B and 10B, and evaluate it across eight benchmarks against four state-of-the-art models. Inspired by (Chen et al., 2024b; Wu et al., 2024), for both the single-LoRA and MoLE settings, as well as for preference optimization, we utilized LoRA modules with a rank (r) and alpha value (\\u03b1) of 32. In the MOLE setting, three LoRA modules were employed, with the router G trained to select only one LoRA module at a time. For Vision Alignment, we opted for one epoch with a batch size of 128, while Instruction Tuning was performed over three epochs with a batch size of 64. Additionally, for DPO, we used a batch size of 32 and limited training to one epoch.\n4.2. Baseline\nThe proposed model is evaluated against four state-of-the-art models of comparable size to ensure a rigorous and fair comparison. The first model, LLaMA 3.2 Vision (Dubey et al., 2024), is an advanced architecture based on CLIP (Radford et al., 2021) and LLaMA 3.1, comprising 11 billion parameters. The second model, Pixtral (Agrawal et al., 2024), features a 12-billion-parameter multimodal decoder built upon Mistral NeMo (team, 2024), along with a 400-million-parameter vision encoder trained from scratch. Additionally, the third and fourth models, LLaVA 1.5 (Liu et al., 2024b) and LLaVA 1.6 (Liu et al., 2024a), employ Mistral (Jiang et al., 2023) and Vicuna (Touvron et al., 2023) as their respective backbones, each comprising 7 billion parameters and integrating a CLIP-based vision encoder.\n4.3. Metrics\nWe selectively employ ROUGE (Lin, 2004) metrics and BERTScore (Zhang et al., 2019) to evaluate the model across different tasks, as their use provides a robust assessment of both factual consistency (via lexical overlap) and contextual alignment (via semantic embeddings), ensuring outputs meet the dual demands of accuracy and adaptability in"}, {"title": "human-robot collaboration.", "content": "4.4. Benchmark\nWe evaluate the proposed model using eight benchmarks across four key objectives: (1) assessing personalized responses based on visual user profiles, (2) understanding users through facial features and expressions, (3) maintaining robustness and general-purpose capabilities while avoiding over-personalization, and (4) mitigating biases to ensure fair and ethical responses.\nUser-aware Personalization To evaluate the personalization capabilities of the proposed model compared to the baseline, we utilized two distinct benchmarks. The first benchmark, ElderlyTech-VQA Bench, comprises 144 triplets of images, questions, and answers, focusing on real-world questions posed by elderly individuals about technology. The associated images, selected from the FairFace dataset, ensure diversity in ethnicity and gender. Reference answers for these questions were generated using GPT-40 with detailed instructions to provide high-quality, contextually relevant responses. The second benchmark, User-VQA Bench, includes 500 test samples from Alexa-VQA and AlpaGasus-VQA, which serve as additional benchmarks. Notably, the model was not trained on any entries from either benchmark, ensuring an unbiased evaluation of its personalization and generalization capabilities.\nFacial Feature Understanding To assess the model's ability to understand the facial features of users, including attributes such as emotion, age, gender, ethnicity, and the number of users, we employed the Face Task Bench, a comprehensive benchmark comprising 1,200 entries (Ramesh, 2024; Tu, 2024). This benchmark is designed to evaluate six distinct tasks related to facial feature understanding, such as emotion prediction, age prediction, and similar attributes. Each task is represented by 200 entries, providing a robust and diverse dataset for evaluating the model's performance in interpreting and analyzing facial characteristics.\nGeneral Purpose Understanding To ensure the proposed model's robustness, generalization, and balance between avoiding excessive personalization and retaining user-specific comprehension, we employed four widely accepted benchmarks: SEED (Li et al., 2023), VQAv2 (Goyal et al., 2017), LLaVA-COCO (Liu et al., 2024b), and In the Wild (Liu et al., 2024b). These benchmarks are extensively used in state-of-the-art evaluations of VLMs and provide a diverse range of tasks and scenarios to rigorously assess the model's performance.\nBias Mitigation To evaluate the model's moral values and impartiality in addressing controversial questions, we se"}, {"title": "5. Results", "content": "5.1. Comparative Analysis\nUser-aware Personalization As demonstrated in Table 1, the User-VLM 360\u00b0, in both its 3B and 10B sizes, consistently outperforms baseline models across both benchmarks. On the ElderlyTech-VQA benchmark, User-VLM 10B achieves an impressive 2x improvement in ROUGE-1 F1 score compared to the baseline, while the 3B variant performs approximately 1.5x better. A detailed comparison of baseline models on this benchmark, ranked by ROUGE-1 F1 score, reveals the following order: LLaMA 3.2 11B, LLaVA 1.5 7B, Pixtral 12B, and LLaVA 1.6 7B. Similarly, on the User-VQA benchmark, User-VLM 3B outperforms the baselines by 1.2x, while the 10B variant achieves a 1.3x improvement. When ranking baselines on this benchmark by ROUGE-1 F1 score, LLaVA 1.5 leads, followed by LLaVA 1.6, LLaMA 3.2, and Pixtral. These results underscore the efficacy of User-VLM 360\u00b0 in addressing the challenges of these tasks and its superior performance across varying model sizes.\nFacial Feature Understanding As summarized in Table 3, User-VLM 360\u00b0 demonstrates strong performance across the Face Task Bench tasks. The 10B model surpasses all baseline models in every task, establishing a new state-of-the-art. The 3B model consistently outperforms baseline models in Race Detection, Face Counting, Age Detection,"}, {"title": "and Gender Detection tasks. Notably, in Emotion Detection, it outperforms LLaMA 3.2 and LLaVA 1.6, achieving competitive results against Pixtral 12B (0.02 F1 score difference) and LLaVA 1.5 7B (0.12 F1 score difference). For Face Attribute Detection, it surpasses Pixtral 12B and LLaMA 3.2 11B, achieving competitive results against LLaVA 1.6 Mistral 7B (0.06 F1 score difference) and LLaVA 1.5 7B (0.01 F1 score difference). Additionally, it achieves a notable performance edge over the 10B model in Age Detection, highlighting its efficiency and robustness in specific tasks.", "content": "General Purpose Understanding Despite the primary focus of training on human user images, which could lead to concerns about catastrophic forgetting and reduced performance on general-purpose tasks, User-VLM 360\u00b0 demonstrates robust generalization capabilities. As summarized in Table 2, the model achieves competitive results across four widely adopted general-purpose benchmarks. Specifically, the 3B and 10B variants outperform the baseline on the VQAv2 benchmark, indicating strong visual question-answering capabilities. On the COCO benchmark, the model performs comparably, with a minimal 0.16-point difference from the top-performing model, LLaVA 1.5. Similarly, on the \"in the wild\" benchmark, the model shows a negligible 0.04-point gap from LLaVA 1.6, highlighting its adaptability to diverse, unstructured data. However, the model exhibits limited performance on the SEED benchmark, suggesting room for improvement in specific scenarios.\n5.2. Ablation Study\nOur ablation study investigates the impact of model size, instruction tuning methods, and the inclusion of DPO on general-purpose understanding tasks, facial feature understanding, and user-aware VQA tasks."}, {"title": "General-Purpose Understanding As shown in Table 4, LORA generally outperforms MoLE in the 3B model, except on the VQAv2 benchmark, where MoLE demonstrates superior performance. Interestingly, the inclusion of DPO reduces the performance of User-VLM 360\u00b0 in most cases, with the exception of MoLE on the COCO benchmark. For the 10B model, MoLE achieves performance comparable to LoRA, with LoRA excelling on the COCO and in the wild benchmarks, while MoLE outperforms on SEED and VQAv2. Notably, DPO negatively impacts the overall performance of the VLM, except for MOLE on COCO and in the wild benchmarks.", "content": "Facial Feature Understanding As demonstrated in Table 6, LORA consistently outperforms MoLE in the 3B model, except on tasks such as race prediction, face counting, and face attribute predictions, where the inclusion of DPO improves performance comparably. For the 10B model, LoRA also demonstrates superior performance over MOLE, with the exception of gender prediction, a binary classification task where MoLE excels due to its simplicity. Interestingly, DPO negatively impacts performance across both MoLE and LoRA configurations for the 10B model.\nUser-Aware Personalization For user-aware VQA tasks, LORA demonstrates superior performance compared to MOLE across both model sizes and benchmarks as detailed in Table 5. This consistent advantage underscores the effectiveness of LoRA in capturing user-centric nuances in VQA scenarios. However, the inclusion of DPO consistently reduces performance across all benchmarks and model sizes, indicating its limitations in enhancing user-aware VQA understanding.\nOur ablation study reveals critical insights into the interplay of adaptation methods, alignment techniques, and model scale. First, LoRA demonstrates consistent superiority over MOLE in most scenarios, particularly in user-aware VQA tasks, where its parameter-efficient fine-tuning mechanism captures nuanced contextual dependencies. MoLE, while less versatile, exhibits competitive performance in specialized benchmarks (e.g., gender prediction), suggesting its utility in tasks requiring explicit disentanglement of latent factors. Second, DPO integration often degrades performance, with only sporadic improvements observed in isolated cases. Finally, model scale significantly modulates method efficacy: the 10B model achieves parity between LORA and MoLE, likely due to its capacity to absorb diverse adaptation strategies, while the 3B model's reliance on LoRA highlights the importance of parameter efficiency in smaller architectures.\n5.3. Bias Evaluation\nAs detailed in Table 7, the proposed model demonstrates superior initial performance in terms of fairness compared to the baseline, as measured by ROUGE-1 and BERTScore. Following DPO tuning, the models generally exhibit improved performance on these metrics, further enhancing their safety and fairness profiles. However, exceptions are observed with MoLE in the 3B configuration and LoRA in the 10B configuration, where DPO tuning leads to a decline in performance.\n5.4. Performance and Efficiency Comparison\nOur experimental results, as detailed in Table 8, demonstrate that User-VLM 360\u00b0 achieves a substantial reduc"}, {"title": "tion in computational complexity, measured in FLOPs, by eliminating the need for explicit instruction-based prompting. Specifically, assuming a question prompt of 50 tokens and detailed instructions of 100 tokens for general-purpose VLMs, the compact 3B variant of User-VLM 360\u00b0 exhibits a remarkable 17.5\u201330X reduction in FLOPs compared to larger 7B-12B baseline models. Furthermore, even the 10B variant of User-VLM 360\u00b0 outperforms equivalently sized models by a significant margin, achieving a 5.25-16.5X reduction in FLOPs.", "content": "6. Deployment On Pepper Social Robot\nWe demonstrate the practical applicability of our method through deployment on the SoftBank Pepper robotic platform (Pandey & Gelin, 2018) a semi-humanoid robot designed for human interaction scenarios. The system architecture leverages Pepper's onboard Jetson Orin Nano module for sensor interfacing and real-time communication with our cloud-based VLM via a ROS 2 distributed computing framework (Magri et al., 2024).\nPipeline The robotic agent's processing pipeline integrates three synchronized components: the Perception Module, which streams multimodal input from Pepper's RGB camera (640x480@30Hz video) and microphone (16kHz audio) to a processing server via ROS 2 topics (Bonci et al., 2023); Cloud Processing, where a dedicated computation node employs Whisper-Large-V3 (Radford et al., 2023) for speech recognition and our VLM for input analysis; and Action Generation, which synthesizes text responses into speech using Tacotron 2 (Shen et al., 2018), delivering audio back to Pepper's speakers through QoS-managed ROS 2 services.\nLatency We empirically evaluated the end-to-end system latency using an Apple M4 Max workstation (64GB unified memory). Our experiments revealed mean response times of 1.8s (\\u03a3=0.4s) for the 3B parameter model and 4.2s (\\u03a3=1.1s) for the 10B variant. The ROS 2 middleware contributed 320ms (\\u00b145ms) to total latency, primarily from serialization/deserialization overhead.\nThis deployment architecture demonstrates the feasibility of integrating User-VLM 360\u00b0 into real-time human-robot interaction systems while maintaining responsive performance characteristics critical for user engagement.\n7. Examples\n respectively demonstrate examples of the model's behavior when exposed to different visual context inputs from the FairFace dataset or real-world deployment on the Pepper social robot. In each case, the model is asked the same question in a zero-shot inference setting, without any additional instructions. User-VLM 360\u00b0 leverages visual cues such as age, gender, and ethnicity to deliver personalized responses, achieving effective tuning objectives. To address potential concerns about the undesired influence of these attributes, we propose a proactive verification mechanism. This mechanism engages users with clarifying questions to confirm the relevance of inferred attributes, ensuring ethical and user-aligned personalization.\n8. Ethical Verification Framework\nPost-deployment ethical considerations remain pivotal in the practical application of User-VLM 360\u00b0 (Jafari & Vassileva, 2023). As illustrated in Table 9, while the model effectively adapts responses based on inferred user characteristics (e.g., gender, age, ethnicity), challenges arise when users may not wish these attributes to influence outputs. To mitigate unintended bias and respect user autonomy, we propose a proactive verification mechanism: instead of generating direct personalized responses, User-VLM 360\u00b0 engages users through clarifying questions to confirm the relevance of inferred attributes. For instance, when a user's visual ethnicity suggests a preference for culturally specific cuisine, the model should first inquire about dietary preferences or interest in diverse categories rather than assuming alignment. This approach ensures personalization occurs only after the model reliably aligns its assumptions with the user's actual characteristics and secures explicit consent, thereby upholding ethical standards of agency and transparency. Implementing such safeguards requires integrating these prin"}, {"title": "ciples into the training paradigm or embedding the model within frameworks (Li et al., 2024a), which enforce comprehensive ethical checks. By positioning User-VLM 360\u00b0 as a foundational component within such systems, it becomes possible to balance personalization with accountability, fostering ethically sound AI applications while maintaining adaptability for diverse user needs.", "content": "9. Discussion and Future Work\nWhile User-VLM 360\u00b0 has the potential to significantly enhance user experiences in healthcare, education, and assistive robotics, it also raises advanced technical considerations for future works.\nInteractions with Multiple Parties One limitation to discuss is that this work is primarily focused on dyadic interactions, involving a single robot and a single human. However, many social interactions involve multiple agents, such as a"}, {"title": "couple or a group of individuals. In scenarios where two people, such as a couple, are asking for a recommendation, the robot would need to consider the preferences and contexts of both individuals simultaneously. This introduces additional complexity, as the robot must balance and integrate the needs and preferences of multiple users to provide a coherent and satisfactory response.", "content": "Cognitive Metrics Another important discussion point is the evaluation of human-robot interactions based on the subjective perception of the human user. While the User-VLM 360\u00b0 framework demonstrates strong performance on objective benchmarks, such as F1-score, the human user's subjective experience is equally crucial. Factors like affiliation, trust, intimacy, and rapport play significant roles in determining the success and acceptance of human-robot interactions. Although these higher-level concepts are beyond the scope of this work, they are worth mentioning as they highlight the multifaceted nature of human-robot in"}, {"title": "10. Conclusion", "content": "Personalizing interactions between humans and robots equipped with vision-language models is essential for scalable and socially intelligent collaboration. Current methods often overlook individual nuances and raise ethical concerns due to biases in user data. To address this, we introduced User-VLM 360\u00b0, a framework that combines multimodal user context modeling with bias-aware optimization. This approach includes real-time adaptive tuning using visual, linguistic, and behavioral signals, bias mitigation, and a curated socio-emotive interaction dataset. Evaluations show significant improvements, and deployment on the Pepper robot confirms real-time adaptability."}, {"title": "11. Impact Statement", "content": "This paper introduces the User-VLM 360\u00b0 framework, designed to advance personalized human-robot interactions by integrating VLMs into robotic systems. The framework focuses on user-aware tuning and bias mitigation to ensure ethical and fair responses, addressing concerns about data privacy, user consent, and safety. While this technology has the potential to significantly enhance user experiences in healthcare, education, and assistive robotics, it also raises ethical considerations and societal impacts that must be responsibly managed. These concerns include privacy risks, bias, and discrimination (such as stereotyping, exclusion, and fairness issues). However, thanks to a verification framework, explained in Section 8, many of these issues can be mitigated."}, {"title": "A. Data Construction Details", "content": "Here, we provide a detailed discussion of the datasets we have constructed in more details, including their sources, preprocessing steps, and the rationale behind their design choices.\nA.1. PT datasets\nGenUser It includes approximately 10K synthetic image-text pairs, featuring human faces alongside user profile information from diverse demographic backgrounds. The dataset is generated by \u201cgenerated.photos\" platform to ensure privacy and avoid using real personal data. To promote fairness, the entries are intentionally designed to represent a broad range of demographic groups, capturing diversity across key characteristics such as age, gender, and ethnicity. Each entry is accompanied by a JSON file integrating over 10 visual at-tributes that support a wide range of information about user profiles. These features, alongside the images, are processed using a VLM (\"GPT-40\") to generate a one-paragraph user profile, providing a concise yet detailed description based on the inferred demographic and emotional attributes. The 10K entries in the dataset are split into three parts: 1K for validation, 1K for testing, and 8K for training, ensuring a balanced distribution across the dataset for training and model evaluation.\nFairUser It approximately consists of 100K real-world text-image pairs derived from the FairFace dataset (Karkkainen & Joo, 2021). The dataset entries are carefully curated to ensure balance, diversity, and accurate labeling across race, gender, and age categories. Based on this dataset, we designed a user profile feature using the following template: \"The person appears to be race class"}, {"title": "gender class, approximately age class years old\". This template facilitates a structured and interpretable representation of demographic attributes for profiling tasks. The 100K entries in the dataset are split into three parts: 10K for validation, 10K for testing, and 80K for training, ensuring a balanced distribution across the dataset for training and model evaluation.", "content": "A.2. Instruction datasets\nAlpaGasus-VQA AlpaGasus dataset is an unofficial general-purpose dataset containing 10K question-answer pairs released by gpt4life that have demonstrated effectiveness in fine-tuning LLMs. For each question in AlpaGasus, we used LLMs to assign scores to each category of age, gender, and race, and select seven images based on the question context and matched user profile characteristics, then refined the answers using GPT-40 to align them with the user profiles. The following is an example of assignment and the prompt used for generation of the personalized response.\nAlexa-VQA The Alexa QA dataset includes over 100,000 pairs of question-answer entries, covering a wide range of topics and contexts. For our study, we selected a random subset of 20,000 entries from this dataset. Each of these selected QA pairs was then assigned to a user image sourced from the FairFace dataset, which provides a diverse set of human faces with demographic labels. To ensure that the responses were relevant and tailored to individual users, we applied a personalized approach by modifying the response based on the user's characteristics and context, using a consistent set of instructions.\nA.3. DPO datasets\nBias Vision DPO Bias-DPO contains 1.2K entries that focus on addressing sexist, racist, controversial, and inappropriate questions. For each entry in Bias-DPO, we assign ten user profiles with corresponding images selected based on semantic similarity between the user profiles and the questions. The images are curated to ensure diversity across age, gender, and ethnicity, thereby reducing the risk of overfitting to specific demographic groups."}]}