{"title": "On the Variability of AI-based Software Systems Due to Environment Configurations", "authors": ["Musfiqur Rahman", "SayedHassan Khatoonabadia", "Ahmad Abdellatif", "Haya Samaana", "Emad Shihab"], "abstract": "[Context] Nowadays, many software systems include Artificial Intelligence (AI) components and changes in the development environment have been known to induce variability in an AI-based system. [Objective] However, how an environment configuration impacts the variability of these systems is yet to be explored. Understanding and quantifying the degree of variability due to such configurations can help practitioners decide the best environment configuration for the most stable AI products. [Method] To achieve this goal, we performed experiments with eight different combinations of three key en-vironment variables (operating system, Python version, and CPU architecture) on 30 open-source AI-based systems using the Travis CI platform. We evaluate variability using three metrics: the output of an AI component like an ML model (performance), the time required to build and run a system (processing time), and the cost associated with building and running a system (expense). [Results] Our results indicate that variability exists in all three metrics; however, it is observed more frequently with respect to pro-cessing time and expense than performance. For example, between Linux and MacOS, variabilities are observed in 23%, 96.67%, and 100% of the studied projects in performance, processing time, and expense, respectively.", "sections": [{"title": "1. Introduction", "content": "With the recent advancement and popularity in the field of Artificial In-telligence (AI) more specifically Machine Learning (ML) models in solving numerous real-life problems, more and more software is integrating such models as part of the pipeline [1]. Software systems are inherently com-plex. In addition, any ML model is, at its core, probabilistic in nature and hence, suffers from the challenge of uncertainty [2, 3, 4]. The complexity of a software system combined with the non-deterministic nature of an ML model can introduce variability the phenomenon where a piece of soft-ware behaves differently when the development or the runtime environment changes although the internal software artifacts such as code, and input data are exactly the same.\nIn practice it is very likely that development and deployment environ-ments are different, hence, understanding how an ML model may behave differently after deployment compared to how it behaved in the development environment is a crucial aspect of AI-based software development. For ex-ample, an arbitrary face recognition system achieving an F1-score of, say 0.9, in the development environment does not guarantee that it will on average achieve a similar F1-score once deployed in a different environment configu-ration. Therefore, running the system under different configuration settings needs to be an additional step before deployment of the system to determine if the performance significantly varies from configuration to configuration. As demonstrated by the previous example, the probabilistic and uncertain nature of ML models can introduce novel challenges affecting different stages of the software development life cycle. The software engineering research community has recently started investigating the challenges that come with the uncertain nature of AI-enabled software systems in various aspects of the development life cycle such as requirement elicitation [5], software testing and quality assurance [6], and deployment [7].\nAs discussed above, the environment settings can vary from one stage to another in the development life cycle. The choices made by the develop-"}, {"title": null, "content": "ers regarding development environment variables, such as operating systems, versions of a programming language and associated libraries, and hardware configurations, can depend on many factors including developers' experience, business needs, and existing environment configurations of legacy systems. However, these choices may potentially induce variability in the performance of AI/ML models as \u201cpractitioners' degrees of freedom\" [8, 9], which is a known issue in the field of applied statistics. However, in the domain of soft-ware engineering, no existing work studies the potential sources of variability in AI-based software from an environment configuration perspective. In this paper, we aim to address this issue by answering the following three research questions:\nRQ1: (Operating System) To what extent does the operating sys-tem induce variability in AI-based systems? We analyze whether variations in operating systems make AI-based systems behave differ-ently. We find that variability in performance is observed in 23% of the projects between Linux and MacOS whereas 20% of the projects show such variability between Linux and Windows. Almost all projects show significant variability in processing time and expense between different operating systems.\nRQ2: (Python Version) How does the Python version contribute to the variability in AI-based systems? We investigate the effect of Python versions on the behavior of AI-based systems. We found that Python 3.6 and Python 3.7 consistently produce identical results in all three metrics. However, between Python 3.7 and Python 3.8, variability can be observed in about 17% of the projects in performance and 80% of the projects in both processing time and expense.\nRQ3: (CPU Architecture) How does CPU architecture affect the variability in AI-based systems? We turn our focus from software-level configuration to hardware-level configuration. We compare two CPU architectures and find that over 93% of the projects show vari-ability in processing time and expense while only 20% of them vary in performance between AMD64 and ARM64 architectures.\nOverall, our findings show that configuration settings induce variability although the degree varies from project to project. Variability in processing time and expense is more frequently observed than variability in performance."}, {"title": null, "content": "Determining the best configuration settings for a project is an iterative pro-cess, and developers should build and run their systems on different settings to find the most optimized environment configuration for the product.\nThis paper makes the following contributions:\n\u2022 To the best of our knowledge, this is the first empirical study on the variability of AI-based systems from the environment configuration point of view.\n\u2022 We provide empirical evidence on the effect of different environment configurations on the variability of AI-based systems.\n\u2022 We make our data and scripts available for reproducibility and future research [10].\nThe rest of the paper is structured as follows. Background and research methodology are described in Section 2. In Sections 3, 4, and 5, we present the results and findings of our analysis for each research question. Sec-tions 6, 7 present discussion and threats to validity respectively followed by Section 8 in which we talk about existing works. Finally, in Section 9, we summarize our findings and conclude the paper by describing potential future directions of research."}, {"title": "2. Methodology and Background", "content": "To be able to conduct our experiments in different development environ-ment configurations, we use Travis CI a widely used Continuous Integra-tion (CI) platform [11]. The reason for choosing Travis CI is that a recent study on the usage of CI tools in ML projects reports that Travis CI is the most popular among open-source software (OSS) developers for building AI-based systems [12].\n2.1. Environment Configurations in Travis CI\nIn this study, the three configuration variables we experiment with are Operating System, CPU Architecture, and Python Version. We choose to experiment with these three variables because the operating system is the core of any development environment where a product is built and run, CPU is the core of the hardware on which a product is run, and the programming language is at the core of development tech stack used for building a product."}, {"title": null, "content": "In our experiment, we use the following list of options for each configuration variable:\n\u2022 Operating System: Linux (version Ubuntu-Xenial 16.04), MacOS (ver-sion 10.14.4), and Windows (version 10). We chose these three oper-ating systems because they are the most common operating systems used in development stacks across the globe [13]. Within Linux, we ex-periment with three different distributions, which are Ubuntu-Xenial 16.04, Ubuntu-Bionic 18.04, and Ubuntu-Focal 20.04. We chose these three distributions because, during the time we were running our ex-periments, these three distributions were the latest Long Term Support (LTS) versions of the top three most recent Ubuntu distributions. For brevity, we drop the term 'Ubuntu' and the version number from the rest of the paper.\n\u2022 CPU Architecture: AMD64 and ARM64. We chose these two architec-tures because existing works in the domain of microprocessors reveal that these two CPU architectures have been being compared against each other for a very long time [14, 15, 16] from a variety of points of view. However, no work exists that compares these two CPU archi-tectures from the variable nature of an AI-based software perspective. Furthermore, a recent study shows that ARM64 architecture is consid-ered an alternative to traditional AMD64 architectures which is gaining interest among software developers [17].\n\u2022 Python Version: 3.6, 3.7, and 3.8. We chose these three versions be-cause, during the time of our experiments, Python 3.7 was the oldest version of Python that was being maintained [18]. We compare Python 3.7 against one earlier (Python 3.6) and one later version (Python 3.8) so that features of different versions are still comparable and not sig-nificantly different from one another.\nTo limit the complexity of our analysis we opt to compare all configuration settings against a baseline configuration as opposed to performing pairwise comparison across all settings. We define a baseline configuration to quantify the variability by comparing other environment configurations against this baseline configuration. The baseline configuration is defined as Linux with Xenial distribution for the operating system, AMD64 for CPU architecture, and Python 3.7 for the development programming language. The reason"}, {"title": "2.3. Analysis of Variability", "content": "Evaluation Metrics:\nOne of the reasons why the popularity of AI-based systems has shown consistent growth over the last few years is that these systems are becoming more and more accurate in solving real-life problems. With the increasing amount of good quality data, these systems are expected to perform better over time [24]. Therefore, the primary factor that determines if an AI-based system is practically useful or not is how well it performs in accomplish-ing a given task. The secondary factor that influences a system's practical usefulness is whether it can accomplish a task within a reasonable amount of time. This implies that like any other traditional software system, both performance and time are critical aspects of an AI-based system as well. However, that is not all. Because AI-based systems are trained on existing data, any changes in the data cause performance degradation over time [25]. This necessitates frequent retraining of a system within a reasonable amount of time. Research in less time-consuming training of AI-based systems has been an interesting topic for a while [26, 27, 28, 29]. In order to further facil-itate this process of frequent improvement of a system by retraining it many online cloud platforms offer paid services that can be utilized. These services provide users with different computation resources such as high volumes of RAM, GPUs, and TPUs. Of course, these services are not free and usually, a user needs to pay at an hourly rate [30, 31]. This brings in the third most important factor which is the expense associated with building and running an AI-based system. In our investigation of variability, we also pay attention to these three factors as discussed below:\n1. Performance: This metric is determined from the performance of the AI component of the system. For each project, we create a Python script named example.py. In this script, we implement an example use case of respective projects. Some projects, such as StarBoost [32], already have example scripts and/or notebooks that demo one or more key use cases of those projects. In other projects where no example scripts/notebooks are available (e.g., PyALCS [33]), we go through the tutorial sections of their documentation and find example use cases. This is a crucial step in our experimental setup because the example.py scripts define and run ML tasks like regression and classification. The outputs of these scripts are some numeric measures like F1-score (for classification) and $R^2$ (for regression). This numeric measure is the performance-related metric."}, {"title": null, "content": "2. Processing time: This metric is obtained from the total processing time (in minutes) taken to run a project in a given environment configura-tion. In other words, it is the time taken to complete a job in Travis CI. This includes spinning up the VM, installing required libraries and modules, building the project, and running the example.py script.\n3. Expense: This metric is obtained from the amount of Travis CI credits spent on building and running each project. The number of credits associated with processing a project in Travis CI is calculated based on the amount of time it takes from spinning up the VM to executing the last phase in the.travis.yml file. In other words, the longer it takes to complete processing a project, the more credits are spent. The number of credits required to run a project on a VM in the Travis CI environment is determined only by the operating system of the VM and nothing else. This means that credits are deducted at different rates only when operating systems are different. The billing documentation from the official Travis CI website states that the number of credits spent per minute on running a VM with Linux, Windows, and MacOS are respectively 10, 20, and 50 [34]. We realize that processing time and expense are correlated and it may seem redundant to study expense as a separate metric. However, the scale of processing time and expense can be considerably different. Let us take an arbitrary example. If a project takes 120 minutes to complete on Linux and 121 minutes to complete on MacOS, the processing time differs only by one unit, and a one-unit difference may not be significant. However, when we consider the number of credits spent, these values are 120 \u00d7 10 = 1200 and 121 \u00d7 50 = 6050 for Linux and MacOS respectively. When we convert the number of credits to the equivalent dollar amounts at a rate of 0.0006 dollars per credit as calculated from [34], they are 1200 \u00d7 0.0006 = 0.72 and 6050 \u00d7 0.0006 = 3.63 dollars for Linux and MacOS respectively. As this arbitrary example demonstrates, even a small difference in processing time can trigger a much bigger difference in expense, which implies that there can potentially be cases where variability in processing time across different settings is not significant, but the associated variability in expense can still be significant. This is why we study processing time and expense as two separate metrics in this study.\nResult Analysis:\nWe run each project 50 times under each configuration shown in Table 1. The purpose behind choosing to generate a distribution of 50 runs per config-uration per project is to mitigate random and unaccounted-for fluctuations"}, {"title": null, "content": "in the metrics. For example, let us assume that we aim to determine how the performance of a project varies due to CPU architecture. In this case, we gen-erate a distribution of performance for a project by running it 50 times under the configuration of os:linux, dist:xenial, arch:arm64 and python:3.7. This distribution is then compared against the distribution of performance generated from 50 runs of the same project under the baseline configuration which is os: linux, dist:xenial, arch: amd64 and python:3.7. It is impor-tant to note that there is always one and only one environment variable that is different from the baseline configuration. In this example, the only vari-able that is different from the baseline configuration is arch, as underlined above. We apply this condition to make sure that if the generated distribu-tions differ, it is due to the environment variable that differs between the two settings and nothing else.\nOnce these two distributions are generated, we then perform two steps of analysis of variability:\nStep1: For each project, we calculate the percentage change as follows:\n$P = \\frac{m_o - m_b}{|m_b|} \u00d7 100$ (1)\nThe variables in Equation 1 are defined as follows:\n\u2022 $m_b$ is the arithmetic mean of a metric (performance, processing time, or expense) obtained from 50 runs of a project under the baseline configuration.\n\u2022 $m_o$ is the arithmetic mean of the same metric (performance, processing time, or expense) obtained from 50 runs of the project under one of the other configurations from Table 1.\n\u2022 $P$ is the percentage change between $\\overline{m_o}$ and $m_b$. Any non-zero value of $P$ indicates the existence of variability in a project.\nThe purpose of this step is to determine, on average across 50 runs, how much variability can be observed in each project. This gives us a high-level overview of the variability patterns shown by the projects in our dataset. Step 2: While Step 1 of our analysis gives us an overall picture of vari-ability for each project, in Step 2 we set out to determine whether or not any"}, {"title": null, "content": "observed variability is indeed statistically significant. To determine the statis-tical significance of any observed variability, we first perform Mann-Whitney U test [35] to compare two distributions. We choose Mann-Whitney U test as a nonparametric test of statistical significance because the distributions being compared are not guaranteed to follow a normal or a near-normal dis-tribution. We set the level of significance, \u03b1 = 0.05 for this test. Next, we determine the degree of difference, also known as effect size, between the compared distributions with using Cliff's delta [36]. Cliff's delta, d, is bounded between -1 and 1. Based on the value of d, the effect size can have one of the following qualitative magnitudes [37]:\nEffect size = $\\begin{cases} Negligible, & if |d| \u2264 0.147\\\\ Small, & if 0.147 < |d| \u2264 0.33\\\\ Medium, & if 0.33 < |d| \u2264 0.474\\\\ Large, & if 0.474 < |d| \u2264 1 \\end{cases}$\nFollowing existing work [38], if the Mann-Whitney U test returns a p-value of less than 0.05 and the effect size obtained from Cliff's delta is not negligible, only then we consider the observed variability between the generated distributions as statistically significant.\nFinally, we categorize the studied projects into three categories based on our two-step analysis previously described: (i) projects that show zero variability, (ii) projects that show non-zero variability which is statistically insignificant, and (iii) projects that show non-zero variability which is sta-tistically significant. We categorize the projects in this way because the ex-istence of variability does not necessarily mean the variability is statistically significant.\n3. RQ1: (Operating System) To what extent does the operating system cause variability in AI-based systems?\nAs our first research question, we study variability with respect to oper-ating systems. We perform a comparative analysis among three operating systems: Linux, MacOS, and Windows. Furthermore, we also investigate whether variability can be observed in different distributions of the same op-erating system. In this case, the comparative analysis is performed among three Linux LTS distributions: Xenial, Bionic, and Focal."}, {"title": "3.1. Variability with respect to Operating System", "content": "Setup:\nTo study the effect of operating systems on AI-based systems, we keep the CPU architecture and Python version constant to their default values and vary the choice of operating system only.\nFindings:\nFigure la shows the distributions of percentage change (P) due to dif-ferent operating systems across the studied projects. We observe that a majority of the studied projects show changes in all three metrics due to changes in operating systems. However, only a few projects show variability in performance with statistical significance. On the other hand, almost all observed variability in processing time and expense is statistically significant. Paying closer attention to the breakdown of effect size for the projects with statistically significant variability, we find that in almost all cases the observed variability is large as shown in Table 4. We further find that there is a slight increase in performance (1.44%) on average when projects are run on MacOS compared to Linux. On the other hand, performance drops on average by 4.21% when projects are run on Windows. In processing time and expense, MacOS and Windows are always more ex-pensive than Linux. Our findings implied that Linux is a faster and more cost-effective operating system than both MacOS and Windows. Although a slight increase in performance may be achieved on MacOS compared to Linux, this will require sacrifice in processing time and expense with MacOS taking 137% longer processing time and costing 1085.47% more money in comparison to Linux."}, {"title": "3.2. Variability with respect to Linux Distribution", "content": "Setup:\nTo study whether variability can be observed in different distributions of the same operating system, we vary only the distribution variable in the configuration settings and keep the operating system, CPU architecture, and Python version constant to the default values.\nFindings:\nFigure 1b shows the distribution of percentage change (P) caused because of changes in the Linux distribution across the studied projects. reveals that the majority of the projects show some degree of variability between different distributions of Linux. Although none of the observed variability between Xenial and Bionic is statistically significant in any of the metrics, the observed variability between Xenial and Focal shows a different pattern. Between Xenial and Focal, three projects show a statistically sig-nificant variability in performance whereas 23 projects show a statistically significant variability in processing time and expense. Most of the observed statistically significant variability is large in terms of effect size as shown in Table 6. On average a slight performance gain of 2% can be achieved"}, {"title": "4. RQ2: (Python Version) How does the Python version contribute to the variability in AI-based systems?", "content": "Setup:\nIn RQ2, we investigate if variability can be observed when different ver-sions of Python are used to run the same system. To run our experiments for this RQ, we keep all configuration variables constant except the Python version.\nFindings:\nFigure 2 shows a similar pattern to the observed variability in RQ1. Al-though a majority of the studied projects show some variability between Python versions, not all observed variability has statistical significance as shown in Table 7. Furthermore, Table 8 reveals that any variability observed"}, {"title": "5. RQ3: (CPU Architecture) How does CPU architecture affect the variability in AI-based systems?", "content": "Setup:\nWe study the effect of CPU architecture on variability by keeping the operating system, distribution, and Python version constant to the default value and only varying CPU architecture configuration.\nFindings:\nFigure 3 summarize the variability pattern in terms of percentage changes between AMD64 and ARM64 CPU architectures. Similar to the findings from RQ1 and RQ2, most of the observed variability in performance is in-significant, whereas, in processing time and expense, the observed variabil-ity is significant in the majority of the cases. In all three metrics, the ARM64 CPU performs poorly compared to the AMD64 CPU with a slight drop of 0.62% in performance costing 25% more time and money, on average. We conjecture that the observed vari-ability between AMD64 and ARM64 CPU architectures may be happening"}, {"title": "6. Discussion", "content": "AI components are becoming a core part of almost all software systems nowadays. Our analysis shows that these systems suffer from variability in all three metrics (performance, processing time, and expense) we studied al-though this finding is not consistent across all projects under investigation. Although existing works reported the variable nature of AI-based systems due to different factors such as choice of frameworks [40, 41], underspecifi-cation [42] and CPU multithreading [43], to the best of our knowledge, ours is the first work to investigate the effect of environment configurations on variability. We find that the choice of operating system including the dis-tribution of an operating system, version of Python, and CPU architecture indeed induce variability. The degree of variability differs from project to project which implies that to be able to determine the existence and degree of variability in a project, developers must build and run the project under various configuration settings. Furthermore, our findings indicate that not all metrics show an equal degree of variability. Variability is more prominent in processing time and associated costs than the performance of an AI com-ponent. This can have an adverse effect on a project's development lifecycle given that AI components need to be retrained frequently because they suf-fer from performance decay due to data drift [25] and concept drift [44] over time. If the (re)training of an AI component takes a very long time under a given configuration setting it can reduce the frequent update of the product and eventually can lead to a performance drop. Moreover, longer processing time usually implies higher costs.\nWhile variability in processing time and associated expenses impacts only the project internally (such as longer development time, and unnecessary increase in development efforts), the variability in performance can impact the end-users of the project. This can potentially cause a financial burden for a company. Therefore, an AI-based system should be built and run on different configuration settings before deployment so that the developers can"}, {"title": null, "content": "determine the most optimized configuration of the environment on which the product will be deployed. Based on existing literature [7], it is not yet a common practice in the industry. Our findings imply that determining the best configuration setting with respect to the metric(s) of interest should be an important step in the development workflow. We acknowledge that this additional step is likely to increase the overall development time, however, this step is crucial to save time and cost in the long run."}, {"title": "7. Threats to Validity", "content": "We discuss threats to validity of this paper in this section.\nInternal Validity:\nThere are projects in our dataset that are developed for more than one task. However, we only run one example task as part of our example script for each project in order to perform an analysis of the outputs. It is en-tirely possible that the example tasks may not represent the actual degree of variability associated with the project. For example, running a classifi-cation model with a curated or noise-free dataset (such as the iris dataset) may result in a very accurate classification model regardless of development environment configurations. However, the same model trained on a more noisy as well as relatively \u2018harder to classify' dataset may produce a very different level of accuracy, hence a very different degree of variability. In order to mitigate this issue, we only choose an example task that is part of the official documentation of each project with a naive assumption that the developers would choose those examples in such a way that they are a true representation of the overall functionality and performance of the project.\nExternal Validity:\nWe cannot guarantee the generalizability of our findings. We chose a finite set of configuration variables with a finite set of possible values for each of the variables under study. However, we acknowledge that there are other options for each of these variables that we do not investigate. For example, Linux has many distributions other than Xenial, Bionic, and Focal. Python has many other versions besides the ones we studied. Therefore, we do not claim that our findings can be generalized beyond what we investigated. The reason behind limiting our choices of options for the configuration variables is the amount of time and money required to run experiments in Travis CI."}, {"title": null, "content": "Furthermore, for each new configuration setting, we would have had to run 50 iterations because of our experimental design. Doing so was not practically feasible due to constraints on time and money."}, {"title": "8. Related Work", "content": "Non-determinism in AI:\nUncertain nature of AI components has been a topic of research in the domain of AI for quite some time. It has gained more traction with the popularity of deep learning systems. Most of the existing works on the non-deterministic nature of AI components focus on deep learning systems. For example, Zhuang et al. [40] studied the uncertain nature of training deep learning models. They reported that the choice of tools can have an effect on the behavior of an AI component which can potentially affect AI safety. Guo et al. [41] performed an empirical study on the development and de-ployment of deep learning solutions. They reported that frameworks and platforms can cause the performance of a system to decline. Crane [45] stud-ied the challenges in the reproducibility of published results. He reported that random consistent use of random seeds can help mitigate the issue with reproducibility. Xiao et al. [43] reported the impact of CPU multithreading and how it impacts the training of deep learning systems.\nVariability in Software:\nVariability in traditional software systems is not a new topic in the soft-ware engineering domain. There are prior works done on the variable nature of software systems in general. Coplien et al. [46] described how to perform domain engineering by identifying the commonalities and variabilities within a family of products. Anda et al. [47] performed the study on variability in software systems from a practical perspective and made a connection be-tween reproducibility and variability. In [48], Bachmann et al. performed an extensive study on variability in software product lines. Thiel et al. [49] focuses on variability in autonomous systems. Jaring et al. [50] suggested a representation and normalization of variability.\nAI-components in Software:\nMany recent studies have investigated the pros and cons of having AI components embedded in software systems. Masuda et al. [51] described practices for the evaluation and improvement of the software quality of ML"}, {"title": null, "content": "applications. Washizaki et al. [52] proposed architecture and design pat-terns for ML systems. An extensive study on testing ML applications was performed in [53] by Zhang et al.. Scully et al. [54] studied hidden technical debt in ML systems whereas Obrien et al. [55] studied self-admitted technical debts in ML software.\nOur work is different from the above studies in that ours is the first study to quantify the degree of variability between different environment configuration settings."}, {"title": "9. Conclusion and Future Work", "content": "In this paper, we investigate how AI-based software shows variability in terms of three metrics: performance, processing time, and expense of building and running a system. We perform our study with respect to three environ-ment variables, namely operating systems including the distributions of an operating system, Python version, and CPU architecture. Our study shows that although a majority of the projects show some degree of variability, the degrees vary from project to project. The variability is more statistically significant for processing time and expense than the performance of an AI component. Because the observed variability patterns vary from project to project, we conclude that in order to serve the end users the most accurate AI solutions, it is crucial to run and test the AI components in different en-vironment configurations. At the same time, it is a common requirement of any AI-based solution to retrain the model(s) in a predefined interval. Therefore, processing time and expense are also something that should be taken into consideration as they also vary significantly from one configuration to another. To the best of our knowledge, the only way to gauge the degree of variability of a system is to run it on different configuration settings. Being able to predict the degree of variability without having to run the system on different configuration settings can save a lot of time and effort. There-fore, this can be an interesting topic for future research. Another interesting follow-up study can be on the reasons behind the observed variability. For example, in this study we find the existence of variability between Python 3.7 and Python 3.8, however, why this variability exists is beyond the scope of this paper. We leave this as a topic for future research."}]}