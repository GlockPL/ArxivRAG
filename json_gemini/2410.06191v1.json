{"title": "Benign Overfitting for Regression\nwith Trained Two-Layer ReLU Networks", "authors": ["Junhyung Park", "Patrick Bl\u00f6baum", "Shiva Prasad Kasiviswanathan"], "abstract": "We study the least-square regression problem with a two-layer fully-connected neural network, with\nReLU activation function, trained by gradient flow. Our first result is a generalization result, that requires\nno assumptions on the underlying regression function or the noise other than that they are bounded. We\noperate in the neural tangent kernel regime, and our generalization result is developed via a decompo-\nsition of the excess risk into estimation and approximation errors, viewing gradient flow as an implicit\nregularizer. This decomposition in the context of neural networks is a novel perspective of gradient de-\nscent, and helps us avoid uniform convergence traps. In this work, we also establish that under the same\nsetting, the trained network overfits to the data. Together, these results, establishes the first result on\nbenign overfitting for finite-width ReLU networks for arbitrary regression functions.", "sections": [{"title": "1 Introduction", "content": "Although neural networks have shown tremendous practical success over the last couple of decades on a\nwide array of tasks, there is still a wide gap in what the community has been able to analyze theoretically. In\nthis paper, we study a fundamental setting, of a regression problem with the square loss, of two-layer ReLU\nneural networks trained by gradient flow. Our aim is to understand the empirically observed behavior in\nthe considered setting, where these networks perfectly fit the training data but still approach Bayes-optimal\ngeneralization.\nWe start with understanding generalization, by answering a key question here:\nDo these networks generalize for arbitrary regression functions?\nLet $x \\in \\mathbb{R}^d$ denote the feature vector, and $y \\in \\mathbb{R}$ denote the label. We assume that the data $(x, y)$ are\nsampled from an unknown distribution. For a function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$, we have,\n$R(f) = \\mathbb{E}[(f(x) - y)^2] \t{and}  \\widehat{R}(f) = \\frac{1}{n} \\sum_{i=1}^{n} (f(x_i) - y_i)^2$\nrepresenting the population and empirical risks of a function f. Here, $(x_i, Y_i)$'s are i.i.d. samples from the\ndata distribution and n is the sample size. The generalization properties of the empirical risk minimizer\n$\\widehat{f} = \\arg \\min_{f \\in \\mathcal{H}} \\widehat{R}(f)$ in a hypothesis class $\\mathcal{H}$ is studied via the excess risk, $R(\\widehat{f}) \u2013 R(f^*)$, where $f^*(x) =$\n$\\mathbb{E}[y | x]$. Since neural networks are often heavily overparameterized without explicit regularization, the\ncapacity of the function class huge, preventing a meaningful analysis through classical uniform convergence\ntechniques in statistical learning theory (Nagarajan and Kolter, 2019). A pre-dominant hypothesis, which\nhas been proved in several simple cases, is that the gradient-based optimization algorithm used to train the\nneural network imposes an implicit regularization effect.\nNow, in the simpler settings in wherein it is possible to characterize this implicit regularization effect\nexplicitly, we can then study uniform convergence by explicitly re-writing the hypothesis class. For ex-\nample, in linear regression or linear networks, gradient descent converges to the minimum norm solution\n(Azulay et al., 2021; Yun et al., 2020; Vardi, 2023), and for classification, convergence to maximum margin\nclassifiers are by now well known (Ji and Telgarsky, 2020). However, for neural networks that are used in\npractice, including the two-layer ReLU network considered in this work, our understanding of the kind of\nimplicit regularization that is imposed by gradient descent is limited (Vardi, 2023, Section 4.4), although\nsome insights exist for the NTK regime (Bietti and Mairal, 2019; Jin and Mont\u00fafar, 2023).\nWe take inspiration from the kernel literature, in particular, a popular technique to analyze kernel re-\ngressors, called the integral operator technique (Caponnetto and De Vito, 2007; Park and Muandet, 2020),\nwhich does not rely on uniform convergence. Specifically, for a reproducing kernel Hilbert space (RKHS)\n$\\mathcal{H}$ and a function $f \\in \\mathcal{H}$, let $R_\\lambda(f) = \\mathbb{E}[(f(x)\u2212y)^2] + \\lambda||f||_{\\mathcal{H}}$ and $\\widehat{R}_\\lambda(f) = \\sum_{i=1}^{n}(f(x_i)-Y_i)^2 + \\lambda||f||_{\\mathcal{H}}$\ndenote the regularized population and empirical risks, and $f_\\lambda$ and $\\widehat{f}_\\lambda$ their respective minimizers in $\\mathcal{H}$. Then\nthe excess risk of $f_\\lambda$ can be written as\n$R(f_\\lambda) \u2013 R(f^*) = \\mathbb{E}[(f_\\lambda(x) \u2013 f^*(x))^2] = ||f_\\lambda \u2212 f^*||_2^2,$\nwhere we denoted the $L^2$-norm by $||\u00b7||_2$. We can then consider the following decomposition:\n$|| f_\\lambda - f^*||_2 \\leq || f_\\lambda - \\widehat{f}_\\lambda||_2 + || \\widehat{f}_\\lambda - f^*||_2.$"}, {"title": "1.1 Related Works", "content": "There have been a plethora of works in the last few years proving the convergence of the empirical risk to the\nglobal minimum in the NTK regime (Allen-Zhu et al., 2019b; Du et al., 2019b,a; Oymak and Soltanolkotabi,\n2020; Razborov, 2022), as well as generalization properties in this regime (Arora et al., 2019; Allen-Zhu et al.,\n2019a; Zhang et al., 2020; Adlam and Pennington, 2020; E et al., 2019; Ju et al., 2021; Suh et al., 2021;\nJu et al., 2022). Moreover, most works on kernel methods mention that their results carry over to neural\nnetworks in the NTK regime (Montanari and Zhong, 2022; Barzilai and Shamir, 2023). However, they dif-\nfer from our work in several ways. For example, E et al. (E et al., 2019), who also study the 2-layer ReLU\nnetwork trained under gradient flow, derive overfitting and generalization bounds by comparing their tra-\njectory to that of the corresponding random feature model, but their generalization error bound requires\nthe regression function to live in the RKHS of the NTK, whereas we do not impose any assumptions on\nthe underlying function. Other results such as (Zhu et al., 2023; Allen-Zhu et al., 2019a) also require\nthe regression function to live in the RKHS of the NTK. Montanari et al. (2019) and Ba et al. (2020) require\nthe activation to be smooth in order to approximate the test error of the trained neural network by that of\na kernel regressor, whereas we work with the non-smooth ReLU activation, for which the analysis is more\ndifficult. Arora et al. (2019) treat the noiseless setting. Almost all of these results are based on comparing\nwith the linearized dynamic (Arora et al., 2019), or direct kernel regression with the NTK (Montanari et al.,\n2019; Zhang et al., 2020; Ju et al., 2021; Barzilai and Shamir, 2023), or a random feature regression (E et al.,\n2019); our approach is fundamentally different in that we track the trajectory of the trained network against\nan oracle trajectory of the same architecture, which can be designed to approximate any regression function\nwith arbitrary precision.\nBenign overfitting, that is, accurate predictions despite overfitting to the training data, is a challeng-\ning phenomenon to establish. Therefore, researchers took to analyzing the simplest possible models, such\nas linear regression (Bartlett et al., 2020; Muthukumar et al., 2020; Zou et al., 2021; Koehler et al., 2021;\nChinot and Lerasle, 2022), kernel regression (Ghorbani et al., 2020; Liang and Rakhlin, 2020; Liang et al.,\n2020; Montanari and Zhong, 2022; Mallinar et al., 2022; Xiao et al., 2022; Zhou et al., 2024; Barzilai and Shamir,\n2023; Cheng et al., 2024) or random feature regression (Ghorbani et al., 2021; Li et al., 2021; Hastie et al.,\n2022; Mei and Montanari, 2022). The study of benign overfitting has recently been extended to neural net-\nworks (Frei et al., 2022; Cao et al., 2022; Frei et al., 2023; Xu and Gu, 2023; Kou et al., 2023; Kornowski et al.,\n2023), and these works even go beyond the NTK regime. However, the proof techniques based on mar-\ngins are specifically for the classification problem, and do not seem to carry over to the regression setting.\nZhu et al. (2023) study benign overfitting of deep networks in the NTK regime for the classification prob-\nlem. They also discuss the regression problem, but the result is an expectation bound of the excess risk rather\nthan a high-probability bound, and their solution is not explicitly shown to overfit that we do. Additionally,\nas with some prior works, they also rely on an assumption that the regression function lives in the RKHS of\nthe NTK, that we do not make here.\nThe concept of overfitting was recently precisely categorized as \u201cbenign\u201d, \u201ctempered\u201d or \u201ccatastrophic\"\nbased on the behavior of the excess risk in the limit of infinite data (Mallinar et al., 2022). In that paper, they\nalso propose a trichotomy for kernel (ridge) regression, although our work is different in that we explicitly\ntrain a neural network rather than performing kernel regression with NTK.\nThere are also a few other lines of work that analyze optimization and generalization properties of neural\nnetworks without NTKs, such as those based on stability (Richards and Kuzborskij, 2021; Lei et al., 2022)\nand mean field theory (Chizat and Bach, 2018; Mei et al., 2018, 2019). While all these are fields of active\nresearch, we are also not aware of any result based on these theories implying the results that we establish\nhere, and in general the results across these theories are incomparable.\nOur work also has connections to the line of work investigating the spectral bias of gradient-based\ntraining (Bowman and Montufar, 2021, 2022). In particular, Bowman and Montufar (2022) investigates"}, {"title": "2 Preliminaries", "content": "We start with our formal problem setup. Additional preliminaries, including standard notations, useful con-\ncentration bounds, and basics of real induction, U- and V-statistics properties, are presented in Appendix B.\nProblem Setup. Take an underlying probability space (\u03a9, M, P), and let x : \u03a9 \u2192 \\mathbb{R}^d, y : \u03a9 \u2192 \\mathbb{R} and\nw : \u03a9 \u2192 \\mathbb{R}^d be random variables\u00b2. We assume x follows the uniform distribution on Sd\u22121, which we\ndenote by $p_{d\u22121}$\u00b3. We assume that |y| is almost surely bounded above by 1:\n$\\mathbb{P} (|y| \\leq 1) = 1.  \\hspace{2cm}  (|y|-Bound)$\nWe denote by $\\mathbb{E}[\u00b7]$ the expectation with respect to $\\mathbb{P}(\u00b7)$. Further, for a variable z, we denote by $\\mathbb{E}[\u00b7 | z]$\nconditional expectation given z and by $\\mathbb{E}_z[\u00b7]$ conditional expectation given all other variables but z (i.e.,\nexpectation with respect to z treating all other variables as fixed). We denote by $\\mathbbm{1}{\u00b7}$ the indicator function\nof an event.\nWe consider the problem of estimating the regression function $f^* : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ defined by $f^*(x) = \\mathbb{E}[y |$\nx]. Then clearly, $\\mathbb{P} (|f^*(x)| > 1) = \\mathbb{P} (|\\mathbb{E}[y | x]| > 1) < \\mathbb{P} (\\mathbb{E}[|y| | x] > 1) \\leq 0$, so the essential supremum\n$\\underset{x \\in \\mathbb{S}^{d-1}}{\\operatorname{ess sup}}|f^*(x)| \\leq 1$ and we have\n$\\mathbb{P}(|.f^*(x)| \\leq 1) = 1, \\hspace{2cm} ||f^*||_2 \\leq 1.  \\hspace{2cm} (f^*-Bound)$\nDefine the noise variable $\u03be^* = y \u2013 \\mathbb{E}[y | x] = y \u2013 f^*(x)$; evidently, $\\mathbb{E}[\u03be^*] = 0$. For $n \\in \\mathbb{N}$ and $i = 1, ..., n$,\nlet $\\{(x_i, y_i, \u03be^*_i)\\}_{i=1}^{n}$ be i.i.d. copies of $(x, y, \u03be^*)$. Also, define the feature matrix, label vector and noise\nvector as\u2074\nX  :=\nx1\n:\nxn\n\u2208 \\mathbb{R}^{n \\times d}\n, y :=\ny1\n:\nyn\n\u2208 \\mathbb{R}^n\n, \u0664* :=\n\u03be1\n:\n\u03ben\n\u2208 \\mathbb{R}^n\n.\nWe consider the square loss, $(y, y') \u2192 (y \u2212 y\u2032)^2 : \\mathbb{R} \\times \\mathbb{R} \u2192 \\mathbb{R}$. For a function $f : \\mathbb{R}^d \u2192 \\mathbb{R}$, writing\n$\u03be_f = y \u2212 f (x)$, the population risk (or test error, or generalization error) for f is $R(f) = \\mathbb{E}[(f(x) - y)^2] =$\n$\\mathbb{E}[\u03be_f^2]$. It is straightforward to see that R is minimized by f*. Writing $\u03b6_f = f^* \u2212 f \u2208 L^2(p_{d\u22121})$, the quantity\n$R(f) \u2013 R(f^*) = ||f - f^*||_2^2 = ||\u03b6_f||_2^2$\nis the excess risk of f, and is the main object of interest. Now write $\\vec f = (f(x_1), ..., f(x_n)) \\in \\mathbb{R}^n$ and\n$\\vec \u03be_f = y \u2013 \\vec f5$. Then the empirical risk (or training error) is\n$\\widehat{R}(f) = \\frac{1}{n} \\sum_{i=1}^{n} (f(x_i) - y_i)^2 = \\frac{1}{n} ||\\vec f - y ||^2 = \\frac{1}{n} ||\\vec \u03be_f||^2.$"}, {"title": "2.1 Model: Two-layer Fully-Connected Network with ReLU Activation", "content": "We will consider a 2-layer fully-connected neural network with ReLU activation function, where\n$m \\in \\mathbb{N}$,\nthe width of the hidden layer, is an even number for the antisymmetric initialization scheme to come later.\nSpecifically, write $\u03d5 : \\mathbb{R} \u2192 \\mathbb{R}$ for the ReLU function defined as $\u03d5(z) = \\max{0, z}$, and with a slight\nabuse of notation, write $\u03d5 : \\mathbb{R}^m \u2192 \\mathbb{R}^m$ for the componentwise ReLU function, $\u03d5(z) = \u03d5((z_1, ..., z_m)^T) =$\n$(\u03d5(z_1), ..., \u03d5(z_m))^T$.\nDenote by $W \u2208 \\mathbb{R}^{m\u00d7d}$ the weight matrix of the hidden layer, by $w_j \u2208 \\mathbb{R}^d, j = 1, ..., m$ the jth neuron of\nthe hidden layer and $a = (a_1, ..., a_m)^T \u2208 \\mathbb{R}^m$ the weights of the output layer. Then for $x = (x_1,...,x_d)^T \u2208$\n$\\mathbb{R}^d$, the output of the network is\n$f_W(x) = \\frac{1}{\\sqrt{m}} a^T \u03d5(Wx) = \\frac{1}{\\sqrt{m}} \\sum_{j=1}^{m} a_j \u03d5(w_j \u00b7 x) = \\frac{1}{\\sqrt{m}} \\sum_{j=1}^{m} a_j \u03d5(\\sum_{k=1}^{d} W_{jk}x_k).$\nWe also define the gradient functions $G_{W_j} : \\mathbb{R}^d \u2192 \\mathbb{R}^d$ at $w_j$ and $G_{W} : \\mathbb{R}^d \u2192 \\mathbb{R}^{m\u00d7d}$ at W as\n$G_{W_j} (x) = \\nabla_{w_j} f_W(x) = \\frac{a_j}{\\sqrt{m}} \u03d5\u2032 (w_j \u00b7 x) x \\hspace{1cm} \\t{for }j = 1, ..., m,$\n$G_{W}(x) = \\nabla_{W} f_W(x) = \\frac{1}{\\sqrt{m}} a^T \u03d5\u2032 (Wx) x.$\nIn Appendix C, we discuss and develop the relevant parts of the neural tangent kernel theory. In Table 1, we\ncollect all relevant notations introduced in this part.\nRecall that m is an even number; this was to facilitate the popular antisymmetric initialization trick\n(Zhang et al., 2020, Section 6) (see also, for example, (Bowman and Montufar, 2022, Section 2.3) and\n(Montanari and Zhong, 2022, Eqn. (34) & Remark 7(ii))). Half of the hidden layer weights are initial-\nized by independent standard Gaussians, namely, $[W(0)]_{j,k} \u223c \\mathcal{N}(0, 1)$ for $j = 1, ..., \\frac{m}{2}$ and $k = 1, ..., d$,\ni.e., for each $j = 1, ..., m$, we have $w_j(0) \u223c \\mathcal{N}(0, I_d)$. Half of the output layer weights $a_j, j = 1, ..., \\frac{m}{2}$ are\ninitialized from Unif{\u22121,1}. Then, for $j = \\frac{m}{2} + 1, ..., m$, we let $w_j(0) = \u2212w_{j\u2212\\frac{m}{2}}(0)$ and $a_j = \u2212a_{j\u2212\\frac{m}{2}}$.\nThen we define $\\vec f_W = (f_{W_1},...,W_{\\frac{m}{2}} + f_{W_{\\frac{m}{2}+1}},...,W_{m})$. This ensures that our network at initialization\nis exactly zero, i.e., $f_{W(0)}(x) = 0$ for all $x \u2208 \\mathbb{S}^{d\u22121}$. The output layer weights $a_j, j = 1,..., m$ are kept\nfixed throughout training, and only the hidden layer weights W(0) are trained. More details provided in\nAppendix C.2.\nWe perform gradient flow with respect to both the empirical risk $\\widehat{R}$ and the population risk R as follows.\nFor $t \u2265 0$, denote by $W(t)$ and $\\widehat{W}(t)$ the weight matrix at time t obtained by gradient flow with respect to\nR and $\\widehat{R}$ respectively. They both start at random initialization W(0) and are updated as follows:\n$\\frac{dW}{dt} = -\\nabla_W \\widehat{R}(f_{W(t)}), \\hspace{1cm} \\frac{d\\widehat{W}}{dt} = -\\nabla_W R(f_{\\widehat{W}(t)}).$\nFor more details about the gradient flow, see Appendix C.4 and Table 2. As a matter of notation, we denote\n$\\vec f_t = \\vec f_{W(t)}, \\vec f_t = \\vec f_{\\widehat{W}(t)}, \u03b6_t = f^* - f_t, \\vec \u03be_t = y - \\vec f_t, G_t = G_{W(t)} and G_t = G_{\\widehat{W}(t)}$. Clearly, $\u03b6_t \u2208 L^2(p_{d\u22121})$\nand $\\vec \u03be_t \u2208 \\mathbb{R}^n$.\nWe define the analytical NTK $\u03ba : \\mathbb{R}^d \u00d7 \\mathbb{R}^d \u2192 \\mathbb{R}$ by $\u03ba(x, x') = \\mathbb{E}_{W \u223c W(0)} [(G_W(x), G_W (x'))_F]$. This\nkernel has an associated operator $H : L^2(p_{d\u22121}) \u2192 L^2(p_{d\u22121}), Hf(\u00b7) = \\mathbb{E}_x[f(x)\u03ba(x,.)]$. We denote the\neigenvalues and associated eigenfunctions of H as $\u03bb_1 > \u03bb_2 > ...$ and $\u03c6_{l,l = 1, 2, ....}$. For an arbitrary $L \u2208 \\mathbb{N}$\nand a function $f \u2208 L^2(p_{d\u22121})$, we denote by the superscript L in $f^L$ the projection of f onto the subspace"}, {"title": "2.2 Assumptions on Parameters", "content": "We start by taking a desired level $\u03f5 > 0$ under which we would like the empirical risk (for overfitting) and\nexcess risk (for generalization) to fall. Our results will be high-probability results, so we also take a desired\nfailure probability $0 < \u03b4 < 1$. The conditions on the sample size n, network width m and feature dimension\nd will all depend on e and \u03b4.\nRemember $\u03b6_t = f^* - f_t$ and $\u03b6_0 = f^* - f_0 = f^*$ as $f_0(x) = 0$ for all x, due to our antisymmetric\ninitialization.\nDefinition 5 ($\u03bb_\u03f5$) Given $\u03f5$, note that since $||.f^*||_2^2 = ||\u03b6_0||_2^2 = \\sum_{l=1}^{\\infty} \u03b6_{0}, \u03c6_l)^2 $ is a convergent series, there\nexists some $L_\u03f5 \u2208 \\mathbb{N}$ such that\n$||\u03b6_{0,2}^L||^2 = \\sum_{l=L_\u03f5+1}^{\\infty}(\u03b6_0, \u03c6_l)^2 < \\frac{\u03f5^2}{4}   \\hspace{1cm} (2)$\nDefine $\u03bb_\u03f5 = A_{L_\u03f5}$ as the $L_\u03f5$-th eigenvalue of H.\nFor this $L_\u03f5$, there also exists some time $T^\u2032$ (which may be \u221e) defined as\n$T\u2032_ = min \\{t \u2208 \\mathbb{R}^+ : ||\u03b6_t||_2^L \\leq  \\frac{\u03f5^2}{4} \\}, \\hspace{1cm} (3)$\ni.e., the first time that $||\u03b6_{t,2}^L||^2$ accounts for less than half of $||\u03b6_t||_2^2$. It may be that $||\u03b6_t||_2^L$ will never account\nfor less than half of $||\u03b6_t||_2^2$, in which case we will have $T^\u2032 = \u221e$. The purpose of $T^\u2032$ is to ensure that we have\napproximation error bounded by \u03f5 before we hit $T^\u2032$, so it is no problem for $T^\u2032$ to be infinite.\nIn Appendix C.3, we compute the eigenvalues of H precisely, with the top-d eigenvalues $\u03bb_1 = ... =\n\u03bb_d = \\frac{4d}{m}$. Now if we assume that most of f* is concentrated on the first d eigenfunctions of H, so that\n$||\u03b6_{0,2}^d||^2 \\leq \\frac{\u03f5^2}{4}$, then we know that $L\u03f5 = d$ for reasonable values of \u03f5, which will lead to particularly nice\nproperties, and we will return to this case later. But in general, we do not assume this to be the case, and\nlet $\u03bb_\u03f5$ be arbitrarily small, which means that we will need m and n to be correspondingly large to ensure\ngeneralization. We precisely characterize this dependence below.\nThe following set of assumptions lay out the necessary relations between n, m, d and $\u03bb_\u03f5$ with respect to\n\u03f5 and the failure probability \u03b4. In Assumption (ii), the constant C > 0 is an absolute constant that appears in\n(Vershynin, 2018, p.91, Theorem 4.6.1). The quantity U \u2208 \\mathbb{N} is needed in the proof of the estimation error,\nand is the number of derivatives of W we consider. We also define a quantity $T = \\frac{8d}{\u03bb_\u03f5} log(\\frac{2}{\u03f5})$. Not all the\nassumptions are needed for all the results.\nAssumption 1 The sample size n, network width m, input feature dimension d, eigenvalue $\u03bb_\u03f5$ of the NTK\noperator (Definition 5), failure probability \u03b4 > 0 and accuracy level $\u03f5 > 0$ satisfy\n$me^{-d/16} < \\frac{\u03b4}{6}  \\hspace{2cm} (d > log m)$\n$\\sqrt{n} - C\\sqrt{d} \u2265 \\sqrt{\\frac{n}{2}}  \\hspace{2cm} (n > d)$"}, {"title": "3 Generalization Result", "content": "In this section, we prove our main result that for an arbitrary level \u03f5 of precision and failure probability \u03b4\nfixed in Section 2.2, the excess risk of the trained neural network $\\vec f_t$ can be bounded by \u03f5 with probability\nat least $1 \u2212 \u03b4$. We make no assumptions on $f^*$ in this section beyond that it is bounded. For the sake of\nemphasis, we repeat the decomposition of the excess risk given in (1):\n$|| f_t - f^*||_2 \\leq || f_t - \\widehat{f}_t||_2 + || \\widehat{f}_t - f^*||_2 \\hspace{1cm} (1)$\nWe stress that, to the best of our knowledge, our work is the first to consider the approximation-estimation\nerror decomposition of the excess risk by viewing the gradient-based optimization algorithm as an implicit\nregularizer.\nBounding Approximation Error. Under no other assumption on the underlying true regression function\nthan the fact that it is essentially bounded (f*-Bound), we first show that we can find a width m of the\nnetwork and a time $T_\u03f5 \u2208 [0, T\u2032]$ (for T' defined in (3)) such that, if we run gradient flow for $T_\u03f5$, then the"}, {"title": "4 Benign Overfitting", "content": "We first establish our overfitting result. Even though, as discussed in Section 1.1, there are now multiple\nresults that under various settings show the convergence to the global minimum of overparameterized neural\nnetworks under gradient flow/descent, we have to re-establish it because we want all of our results to hold\non the same high probability event, under the same set of assumptions (Assumption 1).\nTheorem 9 (Overfitting) Fix any $\u03f5 > 0, \u03b4 > 0$. Suppose Conditions (i)\u2013(v) of Assumption 1 are satisfied.\nThen, on the same event as in Theorem 6, with probability at least $1 \u2212 \u03b4$, the empirical risk of the neural\nnetwork $\\vec f_t$ trained with gradient flow until time t \u2265 0 is bounded as follows:\n$R(\\vec f_t) \\leq exp \\left(-\\frac{\u03f5}{4d}\\right)$\nMoreover, at time t = $T_\u03f5 = \\frac{1}{8d} log (\\frac{2}{\u03f5})$, we have $R(\\vec f_{T_\u03f5}) \\leq \u03f5$."}, {"title": "5 Conclusion", "content": "In this paper, we studied the regression problem with two-layer ReLU networks trained under gradient flow\nwith respect to the square loss, in the NTK regime, without making any assumptions on the underlying re-\ngression function and the noise distribution (other than that they are bounded). Our main contribution comes\nin establishing the generalization guarantees, where we decomposed the excess risk into approximation and\nestimation errors. The approximation error was shown to decay exponentially to a desired level, while the\nestimation error was bounded without resorting to uniform convergence, borrowing a key insight from the\nkernel literature that made our generalization guarantees possible.\nWe also derived exponential decay (with respect to time) of the empirical risk. The use of gradient flow\ngreatly simplifies the exposition, but our analysis of the empirical risk can easily be extended to gradient\ndescent. These results together ensure benign overfitting, an intriguing phenomenon that has been routinely\nobserved in modern deep learning models but have so far eluded theorists in the setting of regression beyond\nsimple models like linear regression and kernel regression. Despite some valid criticisms of the NTK regime,\nwe hope that our analysis, as a first result on benign overfitting for finite-width, trained ReLU networks\nfor arbitrary regression functions, deepens our theoretical understanding of the behavior of these neural\nnetworks."}, {"title": "A Index of Notations", "content": "In Table 1, we collect the notations of all the objects used in this paper. The left-hand column shows\nthe analytical objects for which the weights have been integrated with respect to the initial, independent\nstandard Gaussian distribution, and the right-hand column shows the same objects with dependence on the\nparticular values of the weights W, denoted with the subscript W. Bold symbols indicate that evaluations\non the samples $\\{(x_i, y_i)\\}_{i=1}^{m}$ took place.\nIn Table 2, we collect all the short-hands used for the objects along the gradient flow trajectories. The\nleft-hand column shows the evolution of the quantities along the population trajectory, i.e., objects that\ndepend on W(t), denoted with subscript t without the hat $\\hat{}$ symbol. The right-hand column shows the\nevolution of the quantities along the empirical trajectory, namely those that depend on $\\widehat{W}(t)$, denoted with\nsubscript t and the hat $\\hat{}$ symbol.\nIn Table 3, we collect the notations that indicate projections of functions onto the eigenspace spanned\nby the top L eigenfunctions using the superscript L without the tilde $\\tilde{}$ symbol (left-hand column), and\nprojections of functions onto the eigenspace spanned by all but the top L eigenfunctions using the superscript\nL and the tilde $\\tilde{}$ symbol (right-hand column)."}, {"title": "B Additional Preliminaries", "content": ""}, {"title": "B.1 Vectors and Matrices", "content": "Take any p \u2208 \\mathbb{N"}, ".", "For two vectors $v = (v_1, ..., v_p)^T \u2208 \\mathbb{R}^p$ and $u = (u_1, ..., u_p)^T \u2208 \\mathbb{R}^p$, we denote their dot\nproduct by $v \u00b7 u = v_1u_1 + ... + v_p u_p$, and we denote by $||v||_2 = \\sqrt{v \u00b7 v}$ its Euclidean norm. We denote by\n$\\mathbb{S}^{p\u22121} = \\{v \u2208 \\mathbb{R}^p : ||v||_2 = 1\\}$ the unit sphere in $\\mathbb{R}^p$.\nTake any p, q \u2208 \\mathbb{N}. We write $I_p$ for the $p \u00d7 p$ identity matrix, and for $v \u2208 \\mathbb{R}^p$, we write diag$[v"]}