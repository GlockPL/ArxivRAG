{"title": "Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback", "authors": ["Jiayi Zhou", "Jiaming Ji", "Juntao Dai", "Yaodong Yang"], "abstract": "Aligning the behavior of Large language models (LLMs) with human intentions and values remains a critical challenge. Reinforcement learning from human feedback (RLHF) aligns LLMs by training a reward model (RM) on human preferences and fine-tuning the LLMs to maximize RM feedback. Despite its effectiveness and popularity, RLHF is prone to biased local optimization. It means RM fails to provide feedback that accurately aligns with human preference, causing LLMs to explore unexpected generalizations, and failing to achieve alignment objectives. To mitigate this issue, we propose a novel sequence-to-sequence (seq2seq) reward modeling method. Its key insight is that learning from language feedback rather than scalar feedback improves RLHF without additional annotations. We replaced the reward modeling target from binary maximum likelihood estimation (MLE) with sequence MLE. This method enables richer and fine-grained language feedback without additional annotations, models, or training stages. Our experiments demonstrated its effectiveness, specifically, reducing the refusal-to-response paradigm in single-turn safety dialogues and the long-response bias in text summarization tasks. We provide further analysis that seq2seq RM improves RLHF performance across 2B and 7B LLMs on 3 NLP tasks, achieving an average win rate of 76.9%. We further show that seq2seq RM can still improve the performance of RLHF under out-of-distribution prompts.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) (Achiam et al. 2023; Touvron et al. 2023; Anthropic 2023) have significantly advanced capabilities in conversation, summarization, reasoning, and more tasks, leading to widespread application. Alignment techniques (Ji et al. 2023; Anwar et al. 2024; Bai et al. 2022), particularly Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al. 2022; Christiano et al. 2017), have become crucial for aligning LLMs with human intentions and values, e.g., reducing toxicity (Bai et al. 2022; Ganguli et al. 2022). RLHF consists of two main steps: training a reward model (RM) based on binary human preferences and then fine-tuning the LLMs to maximize feedback from this RM (Ouyang et al. 2022; Schulman et al. 2017; Scheurer et al. 2023). Current reward modeling implementations typically use the sequence-to-scalar (seq2scalar) approach, where a sequence, combined with a prompt and response, is scored as a single scalar value (Huang et al. 2024).\nHowever, Due to high divergence, noise, and sparsity in crowd-annotated preference datasets (Wu et al. 2024), the traditional seq2scalar RM tends to capture patterns unrelated to genuine human preferences, leading to unexpected generalization (Pan, Bhatia, and Steinhardt 2021; Gao, Schulman, and Hilton 2023). It means RM fails to provide feedback that accurately aligns with human preference, leading RLHF to a deviation from actual alignment objective (Di Langosco et al. 2022): LLMs obtain high rewards but negatively impact their capabilities and fail to achieve alignment objectives (Tien et al. 2022; Knox et al. 2023; Lambert and Calandra 2023). For example, when the alignment objective is to reduce toxic"}, {"title": "Preliminaries", "content": "Token-Level MDP for RLHF Given an LLM parameterized with $\\theta$, we model its generation as a token-level Markov Decision Process (MDP), i.e., the tuple $\\mathcal{T} = (\\mathcal{S}, \\mathcal{A}, R, P)$. Here, let $t \\in \\mathcal{V}$ denote a token, where $\\mathcal{V}$ represents the vocabulary set, and $L$ denotes the maximum number of tokens. The state space $\\mathcal{S}$ can be defined as $\\{(t_1, t_2,...,t_L) \\mid t_i \\in \\mathcal{V}\\}$. The action space $\\mathcal{A} = \\mathcal{V}$ consists of all possible tokens in vocabulary. The reward function $R : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ assigns a reward value for generating a specific token given the current sequence of tokens. The transition probability function $P : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$ defines the deterministic state transition model, concatenating the generated token to the existing sequence to form the next state. i.e., $s_{t+1} = (s_t, a_t)$.\nLLMs generation trajectory starts at prompt $s_0$ sampled from datasets. From $t = 0$, the next token $a_t$ is generated according to the parametrized actor LLM $\\pi_{\\theta}(\\cdot | s_t)$, followed by a transition to the next state $s_{t+1}$ according to $P$, continuing until the EOS token. The goal of RL fine-tuning is to"}, {"title": "Methods", "content": "Reward Modeling as Sequence MLE\nThe seq2scalar RM maps concatenated prompt and response pairs, denoted as $x, y^w$ and $x, y^l$, to distinct points in a scalar space. This makes it difficult for the token-level differences between responses to be distinguished in the scalar output. To enhance RM's ability to discern fine-grained differences in responses, we propose augmenting this mapping from a scalar to a language space. Specifically, we extend the learning process from binary MLE in Equation 1 to SFT, a sequence MLE approach. This extension transforms the learning objective from scalar classification to sequence generation, achieving a higher distinction of responses, as shown in Equation 4. Similar approaches have also been applied by Ji et al. (2024a) to make LLM aligners.\nReward Extracting from Sequence\nSince the language feedback provided by the seq2seq reward model (RM) cannot be directly used for reinforcement learning (RL) optimization, we design a novel reward extraction mechanism that returns positive and negative scores as token-level feedback. Formally, let $x$ denote the prompt used in RLHF, and $y$ is the response generated by the actor LLM. We set $j$ as the first index where $r_{seq}(x, y)$ and $y$ are not equal, with the subscript indicating the element at the corresponding position. The notation $:j$ indicates the subsequence up to that index. Defining the inference function of the seq2seq RM as $r_{seq}(\\cdot)$, our goal is to leverage $r_{seq}(\\cdot)$ to obtain the token-level score $R(x, y_{:i})$ for all subsequence $Y_{:i}$ in $y$. To achieve this, we first concatenate $x$ and $y$ and then input $y$ token by to-ken into the seq2seq RM. We positively reward tokens that are consistent with the seq2seq RM at the beginning and negatively reward those that become inconsistent thereafter."}, {"title": "Related Works", "content": "Reward Modeling Reward modeling is a crucial component of RLHF. Given that RM serves as imperfect proxies for human intentions, LLMs can exploit their vulnerabilities, exhibiting unexpected generalization to achieve high rewards during RL finetuning (Pitis 2023; Gao, Schulman, and Hilton 2023). It means RM fails to provide feedback that accurately aligns with human intentions: LLMs exploring unexpected generalization to obtain high rewards, which negatively impact their capabilities and fail to achieve alignment objectives (Tien et al. 2022; Knox et al. 2023; Lambert and Calandra 2023). Moskovitz et al. (2023) proposed the use of Lagrange multipliers to balance rewards representing various preferences, aiming to create a more nuanced reward structure. Similarly, Coste et al. (2023) explored ensembling multiple RMs to provide more accurate feedback by leveraging diverse perspectives. Ram\u00e9 et al. (2024) involved averaging RM weights under different random settings to enhance robustness against variability. Additionally, Zhu, Jordan, and Jiao (2024) introduced iterative data refinement techniques to minimize noise within the reward signals. While these methods have demonstrated varying degrees of effectiveness, they all introduce additional models, training requirements, and hyperparameters, which may impact their robustness and scalability.\nReinforcement Learning from Human Feedback Recent research on token level RLHF presents an impossible triangle of granularity, accuracy, and annotation cost. Chan et al. (2024) redistributed the seq2scalar RM's output to each token based on attention values to achieve fine-grained feedback, but this still relies on the Bradley-Terry model's accuracy over whole responses in learning preferences. Chen et al. (2024), from the perspective of minimum editing constraint, used a generative model to provide token-level reward feedback, but this requires an additional teacher model for correction annotations. Rafailov et al. (2024a) and Zeng et al. (2024) demonstrated the feasibility of achieving token-level alignment from a variant of RLHF, direct preference optimization (DPO) (Rafailov et al. 2024b). However, DPO's offline and off-policy setting reduces its alignment efficiency and cannot be optimized when the preference dataset variability is insignificant (Tajwar et al. 2024). Xu et al. (2024) further showed through theoretical and experimental analysis that DPO is a suboptimal alternative to RLHF methods in various scenarios. This also raises concerns about using DPO as the reward model for RLHF by Zhong et al. (2024)."}, {"title": "Conclusion", "content": "In this work, we propose a novel sequence-to-sequence (seq2seq) reward modeling method. Its key insight is that learning from language feedback rather than scalar feedback improves RLHF without additional annotations. We replace the reward modeling target from binary MLE for classification tasks to sequence MLE for text generation tasks. Our experiments demonstrate that seq2seq RM reduces the refusal-to-response paradigm in single-turn safety dialogues and the long-response bias in text summarization tasks, improving RLHF performance across 2B and 7B LLMs, achieving an average win rate of 76.9%. We further show that seq2seq RM can still improve the performance of RLHF under the out-of-distribution prompts."}, {"title": "Limitations", "content": "Our work contains some notable limitations. Firstly, constrained by training and annotation costs, we have not validated our seq2seq reward modeling on larger-scale models and a wider range of tasks. Additionally, similar to traditional RLHF implementations (Ouyang et al. 2022), we still require preference datasets, which may also contain harmful and offensive data and suffer from financial costs."}, {"title": "Experiment Details", "content": "Experimental Setup Refering to Ouyang et al. (2022), we used the first 5k data from Alpaca52K, Dialoguesum, and TL;DR to fine-tune initial models through SFT, equipping them with basic generation capabilities for these tasks. The PKU-SafeRLHF already includes safety preferences, and we selected 26k preference pairs from 3k prompts. Dialoguesum does not contain preference annotations, so we sampled preference pairs with initial SFT models from 2k prompts. Subsequently, we used GPT-4 (Achiam et al. 2023) for preference annotation, which has been widely regarded as a reasonable proxy for human preferences in previous studies (Zheng et al. 2024; Liu et al. 2023). We finally obtained a preference dataset of 20k preference pairs from 2k prompts.\nTraining We generally followed the hyperparameter settings of PKU-Alignment/SafeRLHF to run PPO, DPO, and SFT training, except that we modified the number of training epochs for PPO to 3. The hyperparameter settings are listed in the table below.\nThis hyperparameter setting is largely consistent with those involved in the seq2seq reward modeling and the SFT portion of the RFT. Specifically, in RFT, we set the number of epochs to 1 to avoid overfitting due to the limited amount of RFT data.\nDevice All experiments utilized large language models with 2 billion and 7 billion parameters. The server's CPU was an Intel(R) Xeon(R) Platinum 8378A CPU @ 3.00GHz with 64 cores, and the graphics cards were NVIDIA A800-SXM4-80GB \u00d78, with NVLink support and the graphics driver version being 525.125.06.\nGPT-4 Prompts This section presents the prompts we employed for preference annotation and model performance evaluation on PKU-SafeRLHF and DialogueSum, using GPT-4. We use the following generic system prompts and user prompts as templates for GPT-4 prompts.\nPrompt Template We used a generic system prompt and user prompt template, in which the Requirements module is set to fill in the evaluation criteria for different tasks, as shown below:"}, {"title": "System Prompt", "content": "You are an expert in the field of large language models, and you are currently working to improve the performance of large language models."}, {"title": "User Prompt", "content": "[Requirement]: [requirement]\nFirst, provide a detailed reasoning process, and then conclude at the end. Please be careful to determine if a response is verbose or redundant. If responseA is better, return Better: [[responseA]]. If responseB is better, then return Better: [[responseB]]. If they are equally good, then return Better: [[Equal]]. Please judiciously determine that the two are equal, and make as many determinations as possible that they are not equal, referring to the [Requirement].\n[Prompt]: [prompt]\n[responseA]: [responseA]\n[responseB]: [responseB]"}, {"title": "Long-Response Bias", "content": "Due to the length of the dialogue texts included in the Dialoguesum dataset, it is challenging to display specific examples within the limited space. Instead, we present the length bias introduced by RLHF based on seq2scalar RM and seq2seq RM, as shown in Figure 7."}], "equations": ["\\mathcal{C}_{RL}^{\\pi} = \\underset{s_0 \\sim \\mathcal{D}}{\\mathbb{E}} \\left[ \\sum_{t=0}^{T} (R(s_t, a_t) + \\beta \\pi_{ref}(a_t | s_t)) \\right]", "\\mathcal{L}_{RM} = \\mathbb{E}_{(x, y^w, y^l) \\sim \\mathcal{D}_{pre}} \\left[ \\log \\sigma(r_{\\theta}(x, y^l) - r_{\\theta}(x, y^w)) \\right]", "R(s_t, a_t) = \\begin{cases} r_{\\theta}(s_t, a_t) & \\text{if } a_t = EOS, \\\\ 0 & \\text{otherwise}. \\end{cases}", "\\mathcal{L}_{SFT} = - \\mathbb{E}_{(x,y) \\sim \\mathcal{D}_{SFT}} \\left[ \\log P_{\\theta}(y | x) \\right]", "\\mathcal{L}_{Seq} = \\mathbb{E}_{(x,y^w,y^l) \\sim \\mathcal{D}_{Pre}} \\left[ \\log P_{\\theta}(y^w | x, y^l) + \\log P_{\\theta}(y^w | x, y) \\right]", "R(x, Y_{:i-1}) = \\begin{cases} 1 & \\text{if } r_{seq}(x, y, Y_{:i-1}) = y_i \\text{ and } i < j \\\\ -1 & \\text{otherwise}. \\end{cases}", "R(x, Y_{:i-1}) = \\begin{cases} -1 & \\text{if } r_{seq}(x, y, Y_{:i-1}) \\neq y_i \\\\ 0 & \\text{otherwise}. \\end{cases}"]}