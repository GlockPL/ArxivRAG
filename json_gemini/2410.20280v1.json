{"title": "MarDini: Masked Autoregressive Diffusion for Video Generation at Scale", "authors": ["Haozhe Liu", "Shikun Liu", "Zijian Zhou", "Mengmeng Xu", "Yanping Xie", "Xiao Han", "Juan C. P\u00e9rez", "Ding Liu", "Kumara Kahatapitiya", "Menglin Jia", "Jui-Chieh Wu", "Sen He", "Tao Xiang", "J\u00fcrgen Schmidhuber", "Juan-Manuel P\u00e9rez-R\u00faa"], "abstract": "We introduce MarDini, a new family of video diffusion models that integrate the advantages of masked auto-regression (MAR) into a unified diffusion model (DM) framework. Here, MAR handles temporal planning, while DM focuses on spatial generation in an asymmetric network design: i) a MAR-based planning model containing most of the parameters generates planning signals for each masked frame using low-resolution input; ii) a lightweight generation model uses these signals to produce high-resolution frames via diffusion de-noising. MarDini's MAR enables video generation conditioned on any number of masked frames at any frame positions: a single model can handle video interpolation (e.g., masking middle frames), image-to-video generation (e.g., masking from the second frame onward), and video expansion (e.g., masking half the frames). The efficient design allocates most of the computational resources to the low-resolution planning model, making computationally expensive but important spatio-temporal attention feasible at scale. MarDini sets a new state-of-the-art for video interpolation; meanwhile, within few inference steps, it efficiently generates videos on par with those of much more expensive advanced image-to-video models.", "sections": [{"title": "1 Introduction", "content": "Auto-regressive (AR) transformers have recently demonstrated remarkable success in natural language processing, sparking efforts to achieve similar breakthroughs in computer vision. However, unlike the discrete, sequential, and easily tokenized nature of language, visual data consist of continuous pixel signals distributed across a high-dimensional space, making them more difficult to model through 1D auto-regression.\nTo overcome this challenge, recent studies have explored vector quantization techniques to convert continuous pixel data into discrete representations suitable for AR modelling. Unfortunately, these approaches rely on causal attention, which is not well aligned for high-dimensional visual data, often leading to diminished performance, particularly on large-scale datasets. To mitigate this limitation, masked auto-regression (MAR) has been introduced. MAR replaces the causal attention with bi-directional attention, effectively simulating auto-regressive behaviour while being more capable of handling visual data. Leveraging this approach, MAR exhibits flexibility in handling diverse generation tasks through different masking strategies, such as image generation, out-painting, video expansion and class-conditioned video generation while maintaining manageable computational overhead. Although MAR shows potential in scaling image and video generation tasks, its key bottleneck lies in its training instability which is tied to the reliance on discrete representations."}, {"title": "2 MarDini: An Efficient and Asymmetric Video Diffusion Model", "content": "Meanwhile, Diffusion models (DMs) have emerged as a successful alternative for scaling vision generative models, offering stable training by modelling visual signals directly in a continuous space. However, DMs tend to incur high inference costs due to the requirement of the multi-step diffusion process. Here, video generation poses an even greater challenge Video is a strict super-set of the image domain, requiring additional modelling for temporal consistency and complex motion dynamics.\nTo this end, we propose a new paradigm for video generation that combines the flexibility of MAR in a continuous space with the robust generative capabilities of DM. Specifically, we present a scalable training recipe and an efficient neural architecture design for video generation. Our model decomposes video generation into two sub-tasks temporal and spatial modelling \u2013 handled by distinct networks with an asymmetric design based on the following two principles:\n1. MAR handles long-range temporal modelling, while DM focuses on detailed spatial modelling.\n2. MAR operates with more parameters at a lower resolution, while DM operates with fewer parameters at a higher resolution.\nFollowing these principles, we use the same training batch for both MAR and DM but employ two distinct processes operating at different resolutions. MAR receives randomly masked low-resolution input frames and predicts the corresponding planning signals. Conditioned on these planning signals via cross-attention and the unmasked frames, DM learns to incrementally recover the masked high-resolution frames from noise. Finally, we introduce a progressive training strategy that gradually curates mask ratios and with its data pipelines, allowing our model to be trained from scratch on unlabeled video data. This eliminates the common reliance on text-to-image and text-to-video pre-training, as seen in other video diffusion models.\nOur model integrates MAR-based planning signals with a DiT-based lightweight, tiny diffusion model, hence the name MarDini. Our empirical study on MarDini highlights the following key characteristics:\n\u2022 Flexibility. With MAR conditioning, MarDini naturally supports a range of video generation tasks through flexible masking strategies. For example, when given the first frame and masking the rest, it performs image-to-video generation; when given a video and masking subsequent frames, it performs video expansion; and, when given the first and last frames and masking the middle frames, it performs video interpolation. By hierarchically and auto-regressively masking middle frames across multiple inferences, MarDini generates slow-motion videos.\n\u2022 Scalability. MarDini can be trained from scratch at scale, without relying on generative image-based pre-training. In contrast to most video generation models, that treat video as a secondary task following image generation, MarDini leverages mask ratio tuning to progressively adjust the difficulty of the training task. This approach enables the model to scale from video interpolation to full video generation, directly bypassing the need for image-based pre-training.\n\u2022 Efficiency. MarDini's asymmetric design allocates more computational resources to lower resolutions, making it memory-efficient and fast during inference. With lower overall memory usage, MarDini allows the deployment of computationally intensive spatio-temporal attention mechanisms at scale, improving its ability to model complex motion dynamics."}, {"title": "2.1 Design Overview", "content": "MarDini is a video generation model designed to efficiently generate high-resolution videos using an asymmetric network architecture. As shown in Figure 1, MarDini consists of two networks: a heavy-weight MAR planning model and a light-weight generation DM. During training, the planning network processes randomly masked low-resolution frames and predicts corresponding planning signals. These planning signals compress the semantic and long-range temporal information, guiding the DM's high-resolution generation process. The DM receives noisy frames at the masked positions and reconstructs them by progressively removing noise."}, {"title": "2.2 Data Representation and Notations", "content": "VAE Compressor. Consistent with prior works, we adopt a pre-trained Variational Auto-Encoder (VAE), denoted by $D_{enc}$, to compress videos into a low-dimensional continuous latent space, which improves both training and inference efficiency. Our VAE employs a 16-channel latent dimension with an 8\u00d7 spatial compression rate to preserve spatial details, following Dai et al. (2023a). The VAE outputs are then patchified into a shape of N \u00d7 C, where N represents the token count and C = 16 represents its latent dimension.\nMAR Planning Model. Given a low-resolution input video $X_{low}$ = {$x_{low}^{i}$}$_{i=1:K}$ with K frames, we apply the VAE encoder to compress the frames into their corresponding latent representations: $Z_{low}$ = {$z_{low}^{i}$}$_{i=1:K}$ = $D_{enc}$($X_{low}$). To train the MAR planning model P, we randomly select K' < K video latents {$z_{low}^{j}$}$_{j=1:K'}$ \u2208 $Z_{low}$ and replace them with a learnable mask token [MASK], resulting in the final masked low-resolution latent inputs $Z_{mask}$. The planning model then processes $Z_{mask}$ and predicts $Z_{cond}$ = P($Z_{mask}$) = {$z_{cond}^{i}$}$_{i=1:K}$, where $z_{cond}^{i}$ is the planning signal for the i-th frame, shaped as $N_{low}$ \u00d7 $C_{low}$, with $N_{low}$ representing the number of patches per frame.\nDM Generation Model. Conversely, we obtain high-resolution video latents $Z_{high}$ = {$z_{high}^{i}$}$_{i=1:K}$ = $D_{enc}$($X_{high}$) with dimensions $N_{high}$ \u00d7 $C_{high}$, generated by the VAE encoder using the same video inputs at high resolution: $X_{high}$ = {$x_{high}^{i}$}$_{i=1:K}$. Notably, we have $N_{high}$ > $N_{low}$. At diffusion step t, we sample noise and add it to K' frames that were masked in the planning model (denoted by [NOISE]), leaving the remaining K \u2013 K' reference frames unchanged (denoted by [REF]). This produces the final noisy high-resolution video latent inputs $Z_{high}^{noise}$. Then, the generation model G processes these latent inputs $Z_{high}^{noise}, t, Z_{cond}$ and performs a standard denoising step, where we denote the DM output at time stept as $G(Z_{high}^{noise},t, Z_{cond}, t)$."}, {"title": "2.3 Architecture Design", "content": "In this section, we provide a comprehensive explanation of the MarDini architecture, including its detailed design, model configurations, and variations."}, {"title": "2.3.1 MarDini Block Design", "content": "Figure 2 illustrates the design of the MarDini's MAR and DM models, both of which are based on the transformer architecture.\nIn the MAR planning model, we adhere to the design conventions established in Llama models, which apply RMS-Norm to normalize the inputs of each attention block. Additionally, layer normalization is applied to normalize the projected features in multi-head attention, enhancing training stability. Due to the use of low-resolution inputs, we manage to directly employ spatio-temporal attention, allowing tokens to attend across frames. This design is feasible only with asymmetric resolution inputs, as it prevents excessive memory consumption.\nConcretely, within each attention block in MAR, we utilize rotary positional encoding (RoPE) to encode both the spatial and temporal positions of the video tokens. To accomplish this, we apply a 2D ROPE to encode the 3-dimensional video data. Specifically, we flatten the image patches into a 1-dimensional token sequence and insert a learnable [NEXT] token to differentiate image patches across different rows, following Gao et al. (2024). This design effectively handles video data with varying aspect ratios and resolutions."}, {"title": "2.3.2 Identity Attention", "content": "In our initial experiments, we observed significant training instability in MarDini's DM. We speculate that this is due to two main factors: i) the inherent distributional disparity between noisy ([NOISE]) tokens and clean reference ([REF]) tokens, which is further amplified by the stochastic nature of sampling diffusion steps; and ii) the random positions and varying lengths of these [NOISE] tokens. These factors likely compound, potentially disrupting the DM's training signals and hindering the model's ability to converge efficiently.\nTo address this challenge, we introduce Identity Attention, which enables the model to easily distinguish between [REF] and [NOISE] tokens by employing a separate attention strategy. As illustrated in Figure 3, [REF] tokens simply serve as an identity projection, preserving the input reference frames without attending to other tokens. In contrast, [NOISE] tokens possess a global view, attending to tokens across all frames. The [REF] tokens serve as guidance for generation, so we design them to be isolated from other tokens, while [NOISE] tokens provide global attention to all conditional signals for generation. We incorporate Identity Attention in both the spatio-temporal layers of MAR and the temporal layers of DM, which has been found to significantly enhance training stability in both models."}, {"title": "2.3.3 Model Configuration", "content": "As outlined in Table 1, this study develops four models with distinct configurations. We train two planning models with 3.1B and 1.3B parameters alongside two generation models, employing spatio-temporal or temporal attention mechanisms. To align with our asymmetric design between the planning and generation models, the generation model's parameter size is reduced to 3\u00d7 or 10\u00d7 smaller than that of the planning model. Due to the high computational cost of spatio-temporal attention, we limit MarDini-L/ST and MarDini-S/ST to a 9-frame length for fair comparison on VIDIM-Bench. Importantly, the model's ability to autoregressively generate samples ensures that the length of the output video is not constrained."}, {"title": "2.4 MarDini Training Recipes", "content": "In this section, we outline the training pipeline of MarDini. Specifically, we employ a multi-stage progressive training strategy that gradually increases task difficulty. This approach offers two key benefits: i) progressive"}, {"title": "2.4.1 Training Tasks: From Frame Interpolation to Video Generation", "content": "Our training objectives are organized into three stages: i) Initial Stage: We separately train the planning and generation models, each with its own learning objective, to initialize their model weights. ii) Joint-Model Stage: We combine the models for joint training on a simple video interpolation task, using only a masked diffusion loss. iii) Joint-Task Stage: We further train the model by gradually reducing the number of preserved reference frames, enabling it to jointly learn video interpolation and image-to-video generation tasks.\nInitial Stage. Wang et al. (2024a) pointed out that transformers with a large parameter count often experience unstable training. As such, we simplify the training dynamics by separately warming up the two models as an initial step."}, {"title": "2.4.2 DM Architecture: From Spatio-Temporal to Temporal Attention", "content": "In conjunction with our progressive training objectives, we also introduce a progressive architectural design. Specifically, we first use spatio-temporal attention in the DM during the initial training stage. This choice promotes convergence, compared to temporal attention, as noted in Gao et al. (2024). Since in our initial stage we train the DM in isolation and on a relatively low-resolution setup, this sophisticated attention incurs in minor computational overhead. When integrating MAR with the DM in the second stage, we replace the spatio-temporal attention with the more cost-effective temporal attention, thus increasing the efficiency of the generation model."}, {"title": "2.4.3 Data: Progressive Configuration of Specifications", "content": "Analogous to our progressive strategies for training objective and architecture we also propose a progressive data configuration. Over time, we gradually increase the video's spatial resolution, alongside progressively extending the video's duration. This approach ensures efficient use of computational resources and facilitates effective model scaling, allowing MarDini to handle more complex and high-resolution video data as training progresses."}, {"title": "3 Experiments", "content": "We evaluate MarDini on two benchmarks: VIDIM-Bench, for long-term video interpolation, and VBench for image-to-video generation. We further elaborate on the specifics of these benchmarks in Appendix D. We highly encourage referring to the generated videos in our web page for a comprehensive understanding of the quality of the generated videos."}, {"title": "3.1 Ablation Studies and Analysis", "content": "Effectiveness of MAR and DM. We demonstrate the importance of having a DM on top of our MAR planning model. In fact, it is tempting to hypothesize that MAR on its own contains all the ingredients to enable high-quality video interpolation. To explore this, we introduce a projection layer to directly unpatchify the output of the MAR model without intermediate diffusion. Our experiments on VIDIM-Bench reveal that, MAR on its own, performs poorly on interpolation tasks, as shown by the first two and last two rows in Table 2, for both the 1B and 3B settings. This result suggests that directly applying MAR to continuous space is suboptimal, a result consistent with previous findings. Similarly, directly tackling this task with a small DM without global guidance, according to the third row of Table 2, results in sub-optimal performance. However, by combining MAR's planning capability with DM's stable performance in continuous space, we achieve optimal results, demonstrating that both components are beneficial for video generation."}, {"title": "3.2 Results on Video Interpolation", "content": "We compare MarDini with the existing methods on the VIDIM benchmark for video interpolation, where the task is to generate 7 frames between a starting and an ending conditional frames."}, {"title": "3.3 Results on Image-to-Video Generation", "content": "In this section, we evaluate our model's single-image-to-video generation capabilities in comparison with other methods using the VBench dataset. As shown in Table 5, our method performs competitively, especially in terms of latency, despite incorporating expensive spatio-temporal attention. For fairness, latency is calculated with the same resolution. In this study, we focus on validating the soundness of our proposed roadmap, only considering the initial pre-training stage rather than delving into post-training techniques. As a result, we do not incorporate additional conditional signals such as language instructions or motion score guidance. Therefore, direct comparisons on video quality, particularly in relation to dynamic degree, are not entirely fair. However, we fully report these numbers for reference.\nWe also report the results on the benchmark without the motion score (referred to as Dynamic Degree in VBench). All evaluation metrics are detailed in Appendix D. The empirical study shows MarDini's strong potential, performing on par with other existing methods across several metrics while exhibiting higher efficiency and requiring no generative image pre-training. Interestingly, we observe that MarDini-S marginally outperforms MarDini-L on some evaluation metrics. We speculate that this is due to MarDini-L requiring more training time to accommodate higher-resolution data. Nonetheless, we observe clear advantages in scaling the MAR model size, as MarDini-L outperforms in video interpolation and generates image-to-video results that better align with physical principles. A list of generated video samples is provided in the supplementary for further reference."}, {"title": "3.4 Additional Applications", "content": "In this section, we explore some of MarDini's additional intriguing capabilities and applications. While we did not conduct rigorous ablation studies or quantitative comparisons, this serves as an initial exploration, highlighting potential directions for future research.\nZero-Shot 3D Novel View Synthesis We demonstrate MarDini's strong potential for 3D novel view synthesis. Although trained solely on video data, MarDini exhibits a preliminary level of spatial understanding, suggesting its potential for 3D applications. In Figure 7, two views of a fixed object serve as the first and last reference frames, while intermediate frames are generated, as similar to our video interpolation task. The model effectively generates convincing 3D-consistent views, highlighting its promising potential for 3D generation. Notably, no camera control signals are used, and we will explore MarDini on 3D data with better control in the future work.\nVideo Expansion MarDini integrates many of MAR's advantages, including the support for video expansion, where the conditional input is a set of frames rather than a single image. In this setup, motion information is implicitly embedded in the input. As shown in Figure 8, MarDini can effectively predict video sequences based on the provided motion cues (e.g., flower blooming, grass growing).\n(Hierarchical) Auto-Regressive Generation By utilizing MAR for high-level planning, MarDini also supports auto-regressive inference, generating more frames beyond the one defined in the training stage. We demonstrate this through hierarchical auto-regressive generation: starting with a given video, we segment it into multiple clips, expand each clip segment, and treat the expanded clip segment as the new video for recursive video interpolation. In Figure 11 (in Appendix), we provide an example where, starting with 4 images, MarDini with a 32-frame window size auto-regressively expands them into a 128-frame slow-motion video (32x expansion). This illustrates that our model is not limited by the training window size, highlighting its potential for long-range video generation."}, {"title": "4 Related Work", "content": "Auto-Regressive Model in Visual Generation. Auto-regressive (AR) models have proven effective in natural language modeling. To adapt this scalable modeling strategy for image and video generation, recent approaches replace causal attention in AR with bidirectional attention, allowing for better capture of dense relationships in visual space.\nMany studies validate the scalability of this approach. To align with the training recipes from LLMs, these studies adopt discrete visual representations, using image tokenizers to quantize continuous pixel values into discrete representations. However, argue that this strategy suffers from unstable training and may limit model capacity due to the inherently continuous nature of visual data. This inspires recent works to shift towards continuous latent spaces for masked auto-regressive models to address these limitations.\nWe follow this trajectory but diverges in two ways: i) We highlight the importance of mask ratios, which were fixed in earlier works Li et al. (2024). By dynamically adjusting them with a progressive training strategy, we improve both model scalability and stability. ii) We propose an asymmetric input resolution design, allowing MAR to be effectively trained with full-resolution inputs.\nDiffusion Model for Video Generation. In recent years, diffusion models have become a leading approach for both image and video generation. These models conceptualize the generation process as gradually refining a real sample from Gaussian noise, demonstrating significant scalability and stable training.\nIn this paper, we offer two key insights into video generation: i) Previous methods often first pre-train an image generative model, and then fine-tune it for video generation, or they require joint training for both tasks. While multi-stage pre-training on diverse inputs can be beneficial, video generation is often limited by the success of image-based pre-training, which typically serves as a secondary task. This paper proposes an alternative: training video generation models from scratch with progressively increasing task complexity. ii) Previous research has predominantly employed temporal attention mechanisms to capture temporal dependencies, mainly due to the high computational and memory costs associated with spatio-temporal attention. However, in alignment with previous work suggesting that spatio-temporal attention enables superior video modelling, we propose an amortized strategy that makes spatio-temporal attention computationally feasible, even at high resolutions.\nAsymmetric Neural Networks. This paper also relates to asymmetric neural architectures, widely used in neural networks since the 1990s . In computer vision, to achieve high-resolution generation, many studies employ a common strategy: a model generates low-resolution/quality samples, followed by another model that performs super-resolution , refinement, or interpolation to enhance the generation quality. In discriminative video models, asymmetric training strategies have been used for temporal segmentation models, where the full temporal extension does not fit the available GPU memory. Since computational costs are distributed across stages, this approach is well-supported by existing computational platforms. Building on this trajectory but extending beyond it, we propose a novel design that partitions the model into two distinct models: a planning model and a generation model. The planning model, containing the majority of the model's parameters, is trained auto-regressively at a low resolution to generate conditional signals without producing visual outputs. These signals are then processed by the lightweight generation model, which converts them into high-resolution visual outputs using a diffusion process.\nUnlike the traditional auto-regressive diffusion model , which still faces high computational costs as resolution increases, we use cross-attention as an information pathway to connect asymmetric resolution input for more efficient training/inference."}, {"title": "5 Limitations and Future Works", "content": "Post Training. The primary goal of this paper is to demonstrate the feasibility and effectiveness of combining masked auto-regressive (MAR) models with diffusion models (DM) for video generation. Consequently, we allocated the majority of our computational resources to the pre-training stage, placing less emphasis on post-training, despite its recognized importance in generative models. Post-training will be a top priority in our future work, focusing on enhancing long-term planning, improving motion quality, and achieving higher resolutions.\nImproved Conditional Signals. A significant contribution of this work is the exploration of training a video generation model without relying on generative image pre-training. However, this approach presents a trade-off: MarDini is not inherently equipped with a text encoder for processing language-based instructions. To conserve computational resources and quickly validate the feasibility of our method, we intentionally excluded commonly used conditional signals, such as text embeddings and motion scores. Encouraged by the initial success of our model, we plan to incorporate these conditional signals into MarDini in our future updates to broaden its range of applications."}, {"title": "6 Conclusion", "content": "We have introduced a new family of generative models for video, i.e., MarDini, based on auto-regressive diffusion, wherein a large planning model offers powerful conditioning to a much smaller diffusion model. Our design philosophy considers efficiency from model conception, and so our heaviest model component is only executed once at lower resolution inputs, whereas our generative module focuses on fine-grained details at the frame level, reconciling high-level conditioning and image details. Our model is unique in that it leverages a masked auto-regressive loss directly at the frame level. MarDini is afforded with multiple generative capabilities from a single model, e.g., long-term video interpolation, video expansion, and image animation. Our investigation shows that our modeling strategy is powerful enough to obtain competitive results on various interpolation and animation benchmarks, while doing it at a lower computational needs than counterparts with comparable parameter size."}, {"title": "To optimize generation model G, we employ a masked diffusion loss $\\mathcal{L}_{DM}$:", "content": "$\\mathcal{L}_{DM} = ||M \\cdot V_t \u2013 M \\cdot G_{\\theta}(Z_{noise},t, Z_{uncond}, t)||_2$,\n(1)\nwhere $Z_{uncond}$ is a learnable token serving as unconditional guidance from the planning model. $\\theta$ represents the parameters of the generation model, and M denotes the binary masks used to mask out all clean reference frames. Inspired by Blattmann et al. (2023b); Salimans and Ho (2022), we apply velocity prediction as the diffusion loss, where the prediction target $V_t$ = {$v_i$}$_{i=1:K}$ represents the velocity at time step t for the i-th frame, defined as $v_i$ = $\\alpha_t\\epsilon \u2013 \\sigma_tZ_{high}^{noise}$, $\\epsilon$ ~ N(0, I). Here, $\\alpha_t$ and $\\sigma_t$ correspond to the diffusion scheduler at t step."}, {"title": "To optimize MAR planning model P, we employ a masked reconstruction loss $\\mathcal{L}_{MAR}$:", "content": "$\\mathcal{L}_{MAR} = ||M \\cdot Z_{low} \u2013 M \\cdot f_{\\zeta} (P(Z_{mask})||_2$.\n(2)\nwhere f denotes a projection layer that depatchifies the model predictions to match the resolution of the low-resolution input image $Z_{low}$\u00b7 \u03c6,\u03b6 represent the learnable parameters of the planning model and the projection layer respectively. Note that, f is only used during the initial training stage, and will be removed in the later training stages."}, {"title": "Joint-Model Stage. After the initial pre-training stage, we then jointly train the planning and generation models end-to-end using a unified masked diffusion learning objective $\\mathcal{L}_{MDiff}$:", "content": "$\\mathcal{L}_{MDiff} = ||M \\cdot V_t \u2013 M \\cdot G_{\\theta} (Z_{noise},t, P_{\\varphi}(Z_{mask}), t)||_2$,\n(3)\nwhere $Z_{cond}$ = P($Z_{mask}$) is the planning signal predicted by MAR. In order to enable classifier-free guidance (Ho and Salimans, 2022) on the planning signal, we maintain a fixed probability of 1/10 to randomly replace $Z_{cond}$ with $Z_{uncond}$."}]}