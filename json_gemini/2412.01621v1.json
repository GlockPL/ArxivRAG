{"title": "NYT-CONNECTIONS: A Deceptively Simple Text Classification Task\nthat Stumps System-1 Thinkers", "authors": ["Angel Yahir Loredo Lopez", "Tyler McDonald", "Ali Emami"], "abstract": "Large Language Models (LLMs) have shown\nimpressive performance on various bench-\nmarks, yet their ability to engage in delib-\nerate reasoning remains questionable. We\npresent NYT-CONNECTIONS, a collection of\n358 simple word classification puzzles de-\nrived from the New York Times Connections\ngame. This benchmark is designed to penalize\nquick, intuitive \"System 1\" thinking, isolating\nfundamental reasoning skills. We evaluated\nsix recent LLMs, a simple machine learning\nheuristic, and humans across three configura-\ntions: single-attempt, multiple attempts with-\nout hints, and multiple attempts with contex-\ntual hints. Our findings reveal a significant\nperformance gap: even top-performing LLMs\nlike GPT-4 fall short of human performance\nby nearly 30%. Notably, advanced prompt-\ning techniques such as Chain-of-Thought and\nSelf-Consistency show diminishing returns as\ntask difficulty increases. NYT-CONNECTIONS\nuniquely combines linguistic isolation, resis-\ntance to intuitive shortcuts, and regular updates\nto mitigate data leakage, offering a novel tool\nfor assessing LLM reasoning capabilities.", "sections": [{"title": "1 Introduction", "content": "As Large Language Models (LLMs) continue to\nadvance, the need for effective benchmarks to as-\nsess their true capabilities has become increasingly\nimportant. While numerous natural language tasks\nand datasets have been developed across domains\nsuch as text summarization, commonsense rea-\nsoning, and question answering (Hendrycks et al.,\n2021a; Cobbe et al., 2021; Hendrycks et al., 2021b),\nthese benchmarks often fall short in isolating and\nevaluating specific cognitive abilities.\nOne major challenge is the difficulty in assess-\ning individual model capabilities independently.\nMany existing tasks combine multiple cognitive\nprocesses, making it challenging to evaluate dis-\ntinct abilities (Gema et al., 2024; Gautam et al.,\n2024). For instance, tasks that simultaneously re-\nquire mathematical reasoning, natural language un-\nderstanding, and contextual disambiguation (Patel\net al., 2021) can obscure a model's true proficiency\nin any single area.\nFurthermore, many current benchmarks are vul-\nnerable to shortcuts or heuristics. Models may ex-\nploit statistical regularities or superficial cues rather\nthan demonstrating genuine understanding, a phe-\nnomenon known as 'shortcut learning' (Geirhos\net al., 2020; Trichelair et al., 2019). This issue is\nclosely related to the distinction between System 1\nand System 2 thinking, as described by Hagendorff\net al. (2023):\n\"System 1 processes are fast, automatic and in-\nstinctual. They often involve heuristics, or men-\ntal shortcuts, which enable quick judgments and\ndecisions without conscious effort. [...] System\n2 processes, on the other hand, are deliberate\nand require conscious effort.\"\nConsequently, many current benchmarks inad-\nvertently reward System 1-style thinking, allowing\nmodels to achieve high scores without demonstrat-\ning the deliberate reasoning we aim to evaluate.\nFinally, as LLMs are trained on increasingly vast\namounts of data, the risk of test set leakage into\ntraining data grows, potentially leading to inflated\nperformance metrics that do not reflect true gener-\nalization capabilities (Balloccu et al., 2024; Huang\net al., 2024).\nTo address these challenges, we introduce NYT-\nCONNECTIONS, a novel benchmark of 358 puzzles\nderived from the New York Times' Connections\ngame. This task requires grouping 16 interrelated\nterms into 4 sets of 4 closely related words, de-\nliberately tempting incorrect System 1 responses\nwhile requiring System 2 thinking for correct so-\nlutions. NYT-CONNECTIONS offers several key"}, {"title": "2 NYT-CONNECTIONS", "content": "NYT-CONNECTIONS is based on Connections, a\nword classification game by the New York Times\n(The New York Times, 2024). This daily puzzle\nchallenges players to group 16 terms into 4 sets\nof 4 related words. Its design intentionally tempts\nquick, obvious associations but requires careful,\ndeliberate reasoning to solve correctly (Liu, 2023).\nFigure 1a illustrates this design. The correct\n\"Skin Types\" group includes \u201cNormal\u201d, \u201cDry\u201d,\n\u201cCombination\u201d, and \u201cOily\". However, \u201cNormal\",\n\"Standard\", and \"Benchmark\" temptingly form a\n\"Status Quo\" group of three. This misdirection is\ncrucial: the apparent \"Status Quo\" grouping leaves"}, {"title": "2.1 The Task", "content": "only three \"Skin Types\" terms, making it impossi-\nble to form four complete groups.\nTo demonstrate the challenge for machine learn-\ning approaches, we applied k-means clustering to\nMultilingual-E5-Large-Instruct word embeddings\n(Wang et al., 2024). As shown in Figure 1b, this\nmethod fails to correctly classify the terms, instead\ngrouping semantically related words from different\ncategories."}, {"title": "advantages:", "content": "\u2022 Linguistic Isolation: It focuses purely on word\nrelationships, minimizing confounding factors.\n\u2022 System 2 Emphasis: It penalizes quick, intuitive\nresponses and requires deliberate reasoning.\n\u2022 Continual Novelty: With daily updates, it pro-\nvides a stream of novel instances, mitigating data\nleakage concerns."}, {"title": "2.2 Dataset Construction", "content": "We constructed the NYT-CONNECTIONS dataset\nthrough the following process:\nDataset Collection: We compiled the complete\nset of 358 Connections puzzles from an archive\ncovering daily offerings from the game's debut on\nJune 12, 2023, to June 3rd, 2024.\nDifficulty Assessment: To gauge the perceived\ndifficulty of each puzzle, we cross-referenced our\ndataset with an existing difficulty chart, providing\nratings from 1 (easiest) to 5 (most challenging) by\nindependent testers.\nThe resulting distribution of difficulty ratings for\nour dataset is provided in Appendix Figure 9."}, {"title": "2.3 Sample Heuristic", "content": "To establish a baseline and demonstrate the chal-\nlenge of the task, we designed a heuristic that\nmimics a player's initial, intuitive approach to the\npuzzles. It evaluates 4-word groups using a score\n$S = G - P$, where G is group similarity and P is\na penalty for similarity to other words.\nGroup Similarity Score For a candidate group\n$C = {C_1, C_2, C_3, C_4}$, we compute G as follows:\n1. Obtain word embeddings E using a pre-trained\nlanguage model."}, {"title": "Penalty Score", "content": "To prevent overly generic group-\nings, we compute a penalty P that measures how\nsimilar a candidate group is to remaining words:\n$P = \\frac{1}{R} \\sum_{r \\in R} cos(\\mu_c, r)$\nwhere $\\mu_c$ is the mean embedding of the candi-\ndate group C, and R is the set of remaining words.\nA lower P indicates a more distinct group.\nBeam Search To balance between finding seem-\ningly good initial groupings and maintaining some\nflexibility, we employ a beam search algorithm\nwith a width of 10:\n1. Initialize with an empty solution.\n2. For each step (up to 4 groups):\n(a) Form all possible groups of 4 from remain-\ning words.\n(b) Compute $S = G \u2013 P$ for each new group.\n(c) Retain the top 10 partial solutions based on\ncumulative score.\n3. Return the highest-scoring complete solution.\nThis approach balances exploration of alterna-\ntive groupings with a preference for high-scoring,\nseemingly obvious solutions. By design, it's prone"}, {"title": "3 Experimental Setup", "content": "Experiments We analyzed 100 puzzles from our\ncorpus, centered around the median difficulty rat-\ning of 3.0. This selection ensures consistent chal-\nlenge across subjects and enables fair comparisons\nbetween LLMs and humans. Our full difficulty\ndistribution is depicted in Appendix Figure 9.\nEvaluation Settings We tested under three con-\nditions: (1) One Try: single attempt, scored 100\nfor success or 0 for failure; (2) No Hints: up\nto four re-tries; and (3) Full Hints: up to four\nre-tries with \"one away\u201d hints. For (2) and (3),\nscores represent the percentage of correct groups\n(A = {0, 25, 50, 75, 100}). Detailed examples are\nin Appendix Figures 4 and 5.\nModels We evaluated six recent LLMs: Claude\n3.5 Sonnet, GPT-4, GPT-40, Gemini 1.5 Pro,\nLLaMA 3 70B Instruct, and LLaMA 3.1 405B\nInstruct (Anthropic, 2024; OpenAI, 2023, 2024;\nTeam et al., 2023; Touvron et al., 2023). LLaMA\nmodels were used with the default temperature of\n0.6; all others were used with a temperature of 0.5.\nOur heuristic used Multilingual-E5-Large-Instruct\nembeddings (Wang et al., 2024).\nPrompts Models received detailed background,\ninstructions, and an example game, mirroring hu-\nman participants' information. We used three\nprompting methods: Input-Output (IO), Chain-of-\nThought (CoT) (Wei et al., 2022), and CoT with\nSelf-Consistency (CoT-SC) (Wang et al., 2023).\nDetailed prompts are in Appendix Figures 6,7 & 8.\nRandom Guess We implemented a random\nguess baseline. The probability of correct ran-\ndom guessing is approximately $3.81 \u00d7 10^{-7}$\n(0.0000381%).\nHuman Evaluation Three human evaluators\ncompleted 50 One Try, 25 No Hints, and 25 Full\nHints instances via a custom application. Perfor-\nmance was averaged across evaluators for each\nconfiguration."}, {"title": "4 Results", "content": "LLMs Significantly Underperform Humans\nAs shown in Table 2, there is a substantial perfor-"}, {"title": "Chain-of-Thought-Based Prompting Techniques Are Limited by Shallow Thinking", "content": "Figure 2 depicts two model outputs that illustrate\nthe limited reasoning ability of Chain-of-Thought-\nbased approaches. In this case, GPT-4 fails to\nconsider multiple factors that may lead to better\nresults such as when the remaining words\noutside of the chosen group contain strong matches\n\u2014 the discovery of which requires more deliberate\nand specialized reasoning. This demonstrates the\nfundamental limitations of System 1 thinkers when\nperforming non-symbolic reasoning tasks, even\nwhen endowed with complex methodology such as\nChain-of-Thought, an issue that has been further\nexplored in recent work (Sprague et al., 2024).\nAdvanced Prompting Techniques Show Dimin-\nishing Returns Figure 3 illustrates how prompt-\ning methods such as Chain-of-Thought (CoT) and\nSelf-Consistency (CoT-SC) do not consistently im-\nprove in performance as task difficulty increases.\nSurprisingly, simpler Input-Output (IO) prompting\noften outperforms these approaches, especially on\nharder puzzles. This suggests that current prompt-\ning techniques may be insufficient to simulate true\nSystem 2 reasoning in LLMs, and might even hin-\nder performance by introducing unnecessary com-\nplexity."}, {"title": "Simple Heuristic is Comparable to Some LLMs", "content": "As shown in Table 2, our baseline heuristic,\ndesigned to mimic intuitive System 1 thinking,\nachieves 13.25% accuracy in both No Hints and\nFull Hints configurations. Notably, this perfor-\nmance is not far behind some of the tested LLMs,\nsuch as LLaMA 3 70b (23.75% in No Hints, 28.5%\nin Full Hints). This relatively small gap between a\nsimple heuristic and more complex language mod-\nels suggests that current LLMs demonstrate ca-\npabilities that fall between System 1-like pattern-\nmatching and System 2-like deliberation, without\nfully achieving consistent, deliberate reasoning.\nContextual Hints of Limited Benefit to LLMS\nReferring again to Table 2, while human perfor-"}, {"title": "5 Related Work", "content": "Recent research has focused on developing bench-\nmarks that address key challenges in evaluating\nlanguage models' reasoning capabilities. Several\nworks propose tasks that isolate specific cognitive\nprocesses, such as code reasoning, mathematical\nproblem-solving, and logic (Liu et al., 2024; Mao\net al., 2024; Wu et al., 2024), aiming to disen-\ntangle task-specific knowledge from broader rea-\nsoning abilities. Researchers have also created\nbenchmarks for deliberate, multi-step reasoning\nincluding tasks designed to challenge System 1\nstyle heuristics (Suzgun et al., 2023; McKenzie\net al., 2024). To combat data leakage, evolving\ndatasets have been introduced, continuously updat-\ning with new problems from real-world sources\n(Sun and Emami, 2024; Li et al., 2024; Jain et al.,\n2024). NYT-CONNECTIONS uniquely combines\nthese three aspects: isolating word relationship un-\nderstanding, resisting simple heuristics, and main-\ntaining novelty through regular updates."}, {"title": "6 Conclusion", "content": "We introduced NYT-CONNECTIONS, a benchmark\nthat isolates word relationship understanding, pe-\nnalizes heuristic-based thinking, and resists data\nleakage. Our evaluation of six LLMs, a simple\nheuristic, and human performance revealed sig-\nnificant gaps, with top models like GPT-4 falling\nnearly 30% short of humans. This highlights the\nongoing challenges in developing AI systems capa-\nble of deliberate reasoning. Future work should\nexplore techniques to bridge this performance"}, {"title": "Limitations", "content": "Embedding Model Scale Our heuristic uses a\nrelatively small model due to hardware constraints.\nWhile this provides a baseline, it's possible that\nlarger embedding models could yield different re-\nsults. Future work should explore the scalability\nof our heuristic approach using more advanced\nembedding models to fully understand the rela-\ntionship between model size and performance on\nNYT-CONNECTIONS.\nPrompt Engineering Scope Cost constraints\nlimited our ability to test an extensive range of\nprompting techniques. While we focused on\nstandard, Chain-of-Thought, and Self-Consistency\nmethods, future studies could explore a broader\nspectrum of prompting strategies, including more\nrecent innovations. However, we intentionally ex-\ncluded complex, long-context methods like Tree of\nThoughts (Yao et al., 2023), as these fall outside the\nscope of our focus on core reasoning capabilities.\nHuman Baseline Limitations Our human per-\nformance data is derived from a small sample of\nthree evaluators, which may not fully represent the\nbroader population's problem-solving abilities. A\nlarger-scale study with a diverse group of partici-\npants would provide a more robust human baseline\nand could reveal interesting patterns in human ap-\nproaches to solving Connections puzzles.\nTemporal Limitations of the Dataset While\nwe commit to monthly updates of NYT-\nCONNECTIONS, the dataset inherently represents\na snapshot of puzzles from a specific time period.\nThis could potentially limit its long-term relevance\nas language use and cultural references evolve.\nRegular assessments of the dataset's contemporary\nrelevance may be necessary to maintain its effec-\ntiveness as a benchmark.\nCross-Cultural Applicability The Connections\npuzzles are primarily designed for an English-\nspeaking, Western audience. This may limit the\nbenchmark's applicability across different cultures\nand languages. Future work could explore cre-\nating multilingual versions or culturally adapted\nvariants of NYT-CONNECTIONS to assess LLM\nperformance in more diverse contexts."}, {"title": "A Appendix", "content": "We used different statistical tests for the One Try setup versus the No Hints and Full Hints setups due to\nthe nature of the data in each case:\nOne Try Setup: We used a two-proportion z-test because the outcomes in this setup are binary (success\nor failure), making it appropriate for comparing two proportions.\nNo Hints and Full Hints Setups: For these setups, we used the Mann-Whitney U test because\nthe outcomes are ordinal categorical data (number of correct groups, A = {0,1,2,3,4}). This non-\nparametric test is suitable for comparing the distribution of scores between two independent groups when\nthe dependent variable is ordinal.\nAll tests were conducted at a 95% confidence interval (p < 0.05). We performed tests between the\nhuman evaluators and the top-performing model, as well as between the top two performing models, to\nassess the statistical significance of performance differences."}, {"title": "A.1 Statistical Testing Procedure", "content": "To prevent overly generic group-\nings, we compute a penalty P that measures how\nsimilar a candidate group is to remaining words:\n$P = \\frac{1}{R} \\sum_{r \\in R} cos(\\mu_c, r)$\nwhere $\\mu_c$ is the mean embedding of the candi-\ndate group C, and R is the set of remaining words.\nA lower P indicates a more distinct group.\nBeam Search To balance between finding seem-\ningly good initial groupings and maintaining some\nflexibility, we employ a beam search algorithm\nwith a width of 10:\n1. Initialize with an empty solution.\n2. For each step (up to 4 groups):\n(a) Form all possible groups of 4 from remain-\ning words.\n(b) Compute $S = G \u2013 P$ for each new group.\n(c) Retain the top 10 partial solutions based on\ncumulative score.\n3. Return the highest-scoring complete solution.\nThis approach balances exploration of alterna-\ntive groupings with a preference for high-scoring,\nseemingly obvious solutions. By design, it's prone"}, {"title": "A.2 Game Setup & Prompting Details", "content": "To prevent overly generic group-\nings, we compute a penalty P that measures how\nsimilar a candidate group is to remaining words:\n$P = \\frac{1}{R} \\sum_{r \\in R} cos(\\mu_c, r)$\nwhere $\\mu_c$ is the mean embedding of the candi-\ndate group C, and R is the set of remaining words.\nA lower P indicates a more distinct group.\nBeam Search To balance between finding seem-\ningly good initial groupings and maintaining some\nflexibility, we employ a beam search algorithm\nwith a width of 10:\n1. Initialize with an empty solution.\n2. For each step (up to 4 groups):\n(a) Form all possible groups of 4 from remain-\ning words.\n(b) Compute $S = G \u2013 P$ for each new group.\n(c) Retain the top 10 partial solutions based on\ncumulative score.\n3. Return the highest-scoring complete solution.\nThis approach balances exploration of alterna-\ntive groupings with a preference for high-scoring,\nseemingly obvious solutions. By design, it's prone"}]}