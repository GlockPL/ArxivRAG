{"title": "Personality Structured Interview for Large Language Model Simulation in Personality Research", "authors": ["Pengda Wang", "Huiqi Zou", "Hanjie Chen", "Tianjun Sun", "Ziang Xiao", "Frederick L. Oswald"], "abstract": "Although psychometrics researchers have recently explored the use of large language models (LLMs) as proxies for human participants, LLMs often fail to generate heterogeneous data with human-like diversity, which diminishes their value in advancing social science research. To address these challenges, we explored the potential of the theory-informed Personality Structured Interview (PSI) as a tool for simulating human responses in personality research. In this approach, the simulation is grounded in nuanced real-human interview transcripts that target the personality construct of interest. We have provided a growing set of 357 structured interview transcripts from a representative sample\u00b9, each containing an individual's response to 32 open-ended questions carefully designed to gather theory-based personality evidence. Additionally, grounded in psychometric research, we have summarized an evaluation framework to systematically validate LLM-generated psychometric data. Results from three experiments demonstrate that well-designed structured interviews could improve human-like heterogeneity in LLM-simulated personality data and predict personality-related behavioral outcomes (i.e., organizational citizenship behaviors and counterproductive work behavior). We further discuss the role of theory-informed structured interviews in LLM-based simulation and outline a general framework for designing structured interviews to simulate human-like data for psychometric research.", "sections": [{"title": "1 Introduction", "content": "Personality research is an important field in psychology for understanding how individual traits shape significant life outcomes and trajectories, including career success, mental health, and overall well-being (e.g., Judge et al., 2002; Roberts et al., 2007; Robins et al., 2002). For instance, measures of conscientiousness have been consistently and positively correlated with job performance and academic success (Barrick and Mount, 1991), whereas measures of neuroticism are found to be strongly and positively correlated with mental health issues, such as anxiety and depression (John et al., 2008). However, studying personality requires understanding habits of thoughts, attitudes, and behaviors across diverse contexts and cultures, which is often a research challenge. For example, some research questions involve rare or extreme cases, making it difficult to gather sufficient samples (e.g., Lynam and Widiger, 2001). Others require longitudinal designs to track personality development or dynamic social interactions (e.g., Damian et al., 2019; Roberts et al., 2006). Additionally, certain experimental manipulations, like inducing stress or altering life circumstances, may be feasible or unethical for causal insights (Fisher, 2021). If a method could accurately simulate the human personality distributions, they could supplement and accelerate personality research by offering a scalable, cost-effective approach to supplement traditional data collection and experimentation (Messeri and Crockett, 2024). Several prior studies have supported this possibility, showing that LLMs can generate responses for personality scales that reflect personality traits resembling those of humans (e.g., Lee et al. 2024, Huang et al. 2023). However, studies found such methods face significant limitations in capturing individual differences at the item level (e.g., Wang et al. 2024a). Information at the item level provides a more granular understanding of how traits manifest in specific individuals and situations rather than relying solely on broad trait averages. However, without additional empirical information, items confound these sources of variance. Moreover, key psychometric challenges remain in modeling LLM responses, as they may not accurately reflect the intended personality construct."}, {"title": "2 Personality Research and Related Works", "content": "Here we provide a concise overview of key personality concepts and discuss related works. MacKinnon (1944) proposed two definitions of personality. One emphasizes internal factors like temperament and interpersonal strategies that drive consistent behavior across time, situations, and cultures. The other focuses on interpersonal characteristics as perceived by others, linking personality to reputation. The former highlights internal drives, while the latter centers on external behaviors and social perception. Together, they underscore personality's role in shaping thought patterns and behaviors in social interactions (Hogan et al., 1996). Personality encodes rich and complex information in language and text (Goldberg, 1990; Saucier and Goldberg, 2001). In fact, the Five-Factor Model (FFM) of personality is extensively researched (e.g., Costa and McCrae, 2008; John, 1999; McCrae and Costa Jr, 1997); it is directly based on the lexical hypothesis, which posits that individual differences that are important in human interactions (e.g., have survival value across cultures) are often encoded in some or all languages of the world. The five factors are openness, conscientiousness, extraversion, agreeableness, and neuroticism (OCEAN). From Galton's (1884) early research to Allport and Odbert's (1936) systematic organization and factor analysis of lexical terms, and through further developments by Norman (1963) and Goldberg (1990), the FFM theory has progressively evolved (see Appendix A for further details on the structure of personality). In other words, because language and text contain rich and complex information about personality, LLMs may capture and model such encoding by learning from vast amounts of training data. There are many studies to-date on LLMs that focus on personality, where researchers aim to benchmark their psychological profiles (Lee et al., 2024; Li et al., 2024; Pellert et al., 2023), investigate their ability to simulate personality (Huang et al., 2023; Jiang et al., 2023; Serapio-Garc\u00eda et al., 2023), and examine whether LLM agents align with character personality settings and how these settings influence their generated dialogues (Shao et al., 2023; Wang et al., 2024b; Xu et al., 2024). Currently, common methods for LLM simulation of human personality distributions have several limitations. The Persona-Chat dataset (Persona method) by Zhang et al. (2018) was originally designed to enhance the engagement of chit-chat models by increasing personalization. Therefore, its focus is more on enhancing personalization, rather than capturing personality traits."}, {"title": "3 Personality Structured Interview", "content": "The theory-informed structured interview mentioned in this study differs from previous interview approaches used in LLM-based simulations, such as the Life Interview (Park et al., 2024), which broadly inquires about various aspects of an individual's life. In contrast, the theory-informed structured interview is specifically designed to target the personality constructs that we aim to model. A theory-informed approach is essential not only for improving the accuracy of data simulation but also for enhancing interpretability. Our goal is to generate data that not only aligns with actual data at the personality scale level but also exhibits theoretically coherent personality-related behaviors. By grounding LLM-generated data in theory, we ensure that the simulation goes beyond reproducing statistical properties to capturing meaningful psychological patterns. This enhances interpretability, facilitates transparent comparisons with human responses, and preserves the psychological validity of the constructs. Learning from psychometric theories, we summarized a framework for developing structured interviews, which can serve as a template for creating interviews designed to collect targeted information for simulating specific types of psychometric data (see Appendix C). We present a PSI example designed to capture personality-related information, with relevant personality theories and question design detailed in \u00a73.1. We want to further emphasize that the \u201cstructured interview questions\" mentioned here differ from those described by Wang et al. (2024b); Ran et al. (2024), who refers to converting personality scale items into open-ended questions. The main issue with their approach is that, while it attempts to rewrite items as open-ended questions to elicit richer responses, it may fail to achieve the intended effect. For instance, in their example, \"Values artistic, aesthetic experiences\u201d is rewritten as \"Do you value artistic, aesthetic experiences?\" Although this question appears open-ended, it tends to prompt a simple \"yes\" or \u201cno\u201d response and function as if it is close-ended, limiting the exploration of the respondent's deeper thoughts or reasoning (Trull et al., 1998). Moreover, such questions differ from the behavioral or situational questions typically found in structured interviews. Structured interviews usually involve more specific scenarios or tasks that encourage respondents to describe past experiences or hypothetical reactions (Campion et al., 1997). For example, to explore a respondent's views on artistic and aesthetic experiences, a more effective question might be, \u201cPlease describe a moment when you felt deeply inspired by an artistic or aesthetic experience?\" This encourages detailed, contextual responses beyond a simple \"yes\" or \"no.\""}, {"title": "3.1 PSI Questions Design", "content": "How can we design questions more effectively and obtain textual information with greater depth in measuring personality? McAdams' three levels of personality theory and the corresponding, theory-informed interviews offer a valuable direction (e.g., McAdams, 1995, 1996, 2001). According to McAdams's theory, personality can be divided into three levels: traits (broad personality characteristics), personal concerns (goals, values, strategies), and narrative identity (the story individuals tell themselves about their lives). The latter two levels, in particular, reveal how individuals act in specific situations and developmental stages and how they organize and make sense of their life experiences. The key to obtaining more meaningful textual information lies in guiding individuals toward higher levels of self-expression. Rather than simply recording mundane daily events, we should encourage reflection on pivotal life moments, significant relationships, and future aspirations. For example, effective questions could be: \u201cCan you describe an event that changed the trajectory of your life?\" or \u201cTell me about a moment you are most proud of.\u201d By designing questions like these, we can elicit deeper, more meaningful narratives, enriching the personality information. We adapted and modified McAdams (1995, 1996, 2001) theory-based interview and narrative identity approach, incorporating elements from the structured interview of the FFM (SIFFM; Trull et al., 1998). The initial draft of the question pool was prepared and revised by subject matter experts (SMEs) through discussion. The SMEs included two doctoral students, one postdoctoral researcher, and one professor, all specializing in personality psychology. Subsequently, we conducted pilot testing with six undergraduate research assistants from a personality research lab. As a result, the PSI, comprising 32 questions, was developed, as shown in Table 3 in Appendix B."}, {"title": "3.2 PSI Dataset", "content": "Using this set of questions, we designed a chatbot to conduct structured interviews, facilitating data collection (Institutional Review Board approval was obtained). Because data collection is ongoing, we used responses collected through the end of December 2024. After excluding incomplete responses and those failing attention checks, the final sample consisted of 357 participants. On average, participants were in their early 30s (M = 33.30 years, SD = 13.06), with 52.40% identifying as men, and 44.80% as women. To ensure a demographically diverse and broadly representative sample, we recruited participants from both undergraduate and working adult populations, ensuring a diverse range of backgrounds in terms of educational, employment, and life experience. Undergraduate participants were recruited from a large public university in the Midwest and received research credit for their participation. Working adult participants were sourced from two widely recognized, high-quality crowdsourcing platforms-Prolific\u00b2 and CloudResearch Connect\u00b3 which are known for their diverse and demographically representative participant pools. All working adult participants were compensated for their time. The average interview duration was 34 minutes. In addition to completing the structured interview, participants provided demographic information and responded to both the personality scale and the personality-related behaviors scale (see Appendix D for detailed descriptions of data collection and examples of the dataset). Responses to PSI were subsequently incorporated into the prompt to generate simulated data (see Appendix F for the specific prompts used in the simulation process)."}, {"title": "4 Experimental Setup", "content": "We designed three primary experiments to verify the effectiveness of the PSI method and its advantages over other methods: (1) Response Similarity (\u00a74.2): evaluating the degree of similarity between responses generated by LLMs based on the PSI method and actual corresponding human responses on personality scales; (2) Human Personality Distribution Simulation (\u00a74.3): comparing the PSI method with other methods in simulating human personality distributions; (3) Personality-Related Behavioral Performance (\u00a74.4): testing the models' performance using the PSI method to simulate personality-related behaviors. General settings shared by all experiments are described in \u00a74.1, while specific differences are detailed in their respective sections."}, {"title": "4.1 General Settings", "content": "Psychological Scale We used Big Five Inventory-2 (BFI-2; Soto and John, 2017): The BFI-2 is designed to capture three core facets of each of the FFM personality factors. Each facet is measured by two positively worded items and two negatively worded items, resulting in 60 items in total. Human respondents and LLMs were instructed to indicate the degree to which they agree with each item on a 5-point scale (1 = \"Strongly disagree\", 2 = \u201cSomewhat disagree\", 3 = \"Neither agree nor disagree\", 4 = \"Somewhat agree\", 5 = \"Strongly agree\"). Specific scale items and scoring criteria can be found in Table 5, Table 6 and Table 7 in Appendix E . LLMS We tested two LLMs, encompassing both closed-source and open-source models: GPT-40 (gpt-40-2024-08-06) (OpenAI, 2024), and Llama 3 (llama-3-70b-instruct) (AI@Meta, 2024). We include these models to allow for cross-validation and comparative analysis, ensuring that the findings are robust and not specific to a single model. GPT-40 and Llama 3 are among the current leading models, making them ideal for assessing the latest advancements in LLM performance. To ensure reproducibility, the temperature of the LLMs was set to zero to generate deterministic responses."}, {"title": "4.2 Response Similarity", "content": "The main purpose of this experiment is to assess the similarity between responses generated by LLMs based on the PSI method and human self-reports. Metrics Because the data generated by LLMs correspond on a one-to-one basis with data from human respondents, we used the Mean Absolute Error (MAE) and Pearson correlation coefficient (r) to examine the strength of the similarity between them. Smaller MAE and higher r indicate greater similarity. For ther calculation formula, see Appendix G. Park et al. (2024) employed a Life Interview simulation, which is currently the most information-rich method for personality simulation. It is also one of the few papers that provides a direct one-to-one performance comparison with human samples. The reported MAE and r with corresponding human samples are likely the best values available so far. We used this as a standard for comparison. If data from human respondents are regarded as the gold-standard criterion, this test can also be considered an examination of criterion-related validity, which assesses the correlation between a measure and an external standard (Cronbach and Meehl, 1955)."}, {"title": "4.3 Human Personality Distribution Simulation", "content": "The main purpose of this experiment is to investigate further the differences among various methods of simulating human personality distributions. We compared the PSI method, the Persona method\u2014which relies on dialogue information (Zhang et al., 2018)\u2014and the Shape method, which uses adjective-based dimensional categorization (Serapio-Garc\u00eda et al., 2023), to observe differences when simulating a normal human sample. The specific descriptions of the Persona method and Shape method are shown in Appendix G. Human Sample Criterion The human samples used for this experiment were collected as part of a broader project related to personality assessment through Prolific and received Institutional Review Board approval. Respondents were instructed to complete a set of demographic questions, the BFI-2, and a set of criterion measures. The respondents were compensated with $3.75 for their participation. In total, 1,559 respondents provided valid responses. On average, participants were in their early 40s (M = 42.29 years, SD = 11.79), 50.80% identifying as men, 49.20% identifying as women. Metrics To quantify the degree of similarity between human and LLM responses, we leveraged multiple metrics. At the domain and facet levels, we compared the mean (M; sample mean of each domain and facet), standard deviation (SD; sample standard deviation of each domain and facet), Cronbach's alpha, and correlations among scores. At the item level, we also compared the mean and standard deviation of item scores (sample mean and standard deviation of each item). Specifically, we also used MAE and r to quantify similarities in these metrics. Here, however, MAE and r are based on sample-level analysis rather than the one-to-one correspondence seen in the previous experiment. Our focus is on whether, when simulating an overall human sample, similarities can be observed across various measurement levels-including items, facets, and domains. Additionally, separately for human and LLM responses, we fitted a three-factor confirmatory factor analysis (CFA; J\u00f6reskog, 1969) model (TFM; where facets are \"factors\") to the responses to each domain of the BFI-2. Aside from the facet structure of each domain, we also used the facet scores as indicators and fitted the FFM. Model fit, standardized factor loadings, and latent correlations among the facets were also compared between human and LLM responses. Tucker's congruence coefficient (TCC; Tucker, 1951) and MAE of factor loadings were used to quantify the similarity between factor solutions from human and LLM responses. For the CFA model, model fit information, and the TCC calculation formula, see Appendix G. This evaluation framework is adaptable and can be used to analyze the fidelity of other simulated psychometric data (see Appendix H for further details)."}, {"title": "4.4 Personality-Related Behavioral Performance", "content": "The primary aim of this experiment is to explore whether LLMs, when assigned specific personality settings, exhibit behaviors theoretically aligned with those personalities. This is an exploratory study, as LLMs generate responses based on statistical probabilities derived from the training corpus (Yang et al., 2024), making it uncertain whether assigning personality settings (through the PSI data) will influence the model to act consistently with that personality. Fortunately, the data we have collected through the PSI method includes LLM ratings of human behaviors, providing a valuable basis to test this hypothesis. Personality-Related Behavior We collected self-report data on two classic types of workplace behaviors: organizational citizenship behavior (OCB) and counterproductive work behavior (CWB). Both types of behaviors are widely supported by research as being related to personality (e.g., Organ and Ryan, 1995; Berry et al., 2007). These data were collected using the scale developed by Spector and Fox (2010) and prompted the LLM to respond to the same questions (for a detailed description of Spector and Fox's (2010) measures, see Appendix G; for specific prompt details, refer to Appendix F). Metrics We primarily focus on the r between personality domains from different data sources and OCB, as well as CWB. We anticipate that the r between self-reported personality domains and OCB/CWB in LLM simulation will closely align with those observed in the human participants."}, {"title": "5 Experimental Results", "content": "Here, we present the results of the three experiments mentioned above: Response Similarity (\u00a75.1), Human Personality Distribution Simulation (\u00a75.2), and Personality-Related Behavioral Performance (\u00a7 5.3)."}, {"title": "5.1 Response Similarity Results", "content": "Table 1 provides a clear illustration of the similarity between the personality generated using the PSI method and the corresponding human self-reported personality data. The results show that regardless of the model used, the average correlation is around .5, suggesting a moderately strong positive relationship and a significant association between the two variables. In particular, compared to the Life Interview method (Park et al., 2024), the PSI method consistently outperforms or matches the Life Interview approach across nearly all metrics, as indicated by its similar or lower MAE and similar or higher r. Notably, the Life Interview method relies on up to two hours of interview data, while the PSI method achieves comparable results using interview data consisting of just 32 questions, with an average duration of about 34 minutes. This further underscores the advantage of the PSI method in effectively leveraging LLMs to simulate personality profiles."}, {"title": "5.2 Human Personality Distribution Simulation Results", "content": "Descriptive Statistics We compared the means and standard deviations of the different methods at three levels: personality item, facet, and domain. MAE and r for both the means and standard deviations are shown in Table 2. The MAE for the means reflected the average difference between the LLM responses and the human responses at each level, whereas the MAE for the standard deviations revealed the difference in variability between the two datasets. The r further illustrated the linear relationship between the two datasets, with values closer to one indicating a stronger correlation. Detailed results can be found in Tables 12, 13, 14, 15, 16, and 17 in Appendix I.1. Table 2 shows that the PSI method demonstrates better performance in simulating human samples compared to the Persona and Shape methods at the item level. Although at the facet and domain level, some results from other methods slightly outperform PSI (e.g., Shape GPT-40 in M), the process of aggregating scores from item to facet to domain levels reduces the impact of extreme values, smoothing them out at higher levels. Therefore, the PSI method's advantage at the more granular item level becomes more important. Additionally, PSI consistently demonstrates statistically significant positive correlations with the SD of human samples across items, facets, and domains, whereas the other methods exhibit either negative or non-significant correlations. Simulating the SD is a challenge for the other methods. Although the Shape method increases response variability and introduces more variance, it still struggles to replicate the true variance observed in human samples. The PSI method proposed in this paper addresses this issue to a certain extent. Psychometric Performance Psychometric performance includes multiple components; here, we primarily present model fit and structural validity (i.e., factor loadings). For other aspects, such as scale reliability and discriminant validity, please refer to Appendix I.1. Model Fit: For both the human sample criterion and different methods of LLM responses, the TFM was fitted to each BFI-2 domain, and the FFM was fitted to all the data. TFM fit information is shown in Table 18, and FFM fit information can be seen in Table 19 in Appendix I.1. From Table 18, it can be observed that the model fit indices of the Persona method and the PSI method are relatively similar to those of the human sample, while the Shape method performs slightly worse. Table 19 reflects a similar trend, where the Shape method shows notable discrepancies compared to the human sample in the RMSEA and SRMR model fit indices (see Appendix G for the explanation of RMSEA, SRMR, and other model fit indices). Structural Validity: TCC was used to evaluate the similarity of factor loadings between human responses and LLM responses. TCC results for the TFM of each BFI-2 domain are shown in Table 20, while results for the FFM are shown in Table 21 in Appendix I.1. The values of the TCC are generally high and thus supportive of factor-loading correspondence, with all methods showing strong alignment with the human sample (a TCC above .95 indicates good similarity, while a TCC of .85 to .94 suggests fair similarity; Lorenzo-Seva and Ten Berge, 2006). However, it is important to note that the TCC focuses on overall profile similarity, largely ignoring differences in absolute values (which might be too lenient). Therefore, we also need to examine the results of the specific standardized factor loadings. Tables 22 and 23 in Appendix I.1 present the standardized factor loadings. From these tables, it is clear that the PSI method outperforms the Shape method in terms of similarity to human samples, with its factor loadings more closely matching human data. However, its results are relatively close to those of the Persona method, yet there are clearly differences in some factor loadings compared to the human sample. It is important to note that the highest factor loadings do not necessarily indicate the best performance, as we are evaluating the similarity between the factor loadings of the human data and the LLM responses. Higher factor loadings suggest stronger correlations between the factor and the items, but this is not always ideal. It indicates that LLMs may struggle to make refined distinctions among items within a facet, meaning they may have difficulty determining which items are more or less related to the factor, unlike human respondents, who exhibit greater differentiation. Tables 24 and 25 in Appendix I.1 present the specific inter-factor correlations. Aligning with our previous findings, the PSI method generally performs better than the other two methods when simulating human personality distributions, especially at a more fine-grained level. For instance, in the TFM results, other methods encountered anomalies, such as inter-factor correlations exceeding one (accompanied by a warning message when fitting the model), but the PSI method generally performed well. The other results (e.g., scale reliability and discriminant validity) can be found in Appendix I.1. Overall, the results produced by the PSI method generally outperform those of the Persona method and the Shape method. However, there is still a certain gap compared to the human sample."}, {"title": "5.3 Personality-Related Behavioral Performance Results", "content": "Table 28 presents the correlations between the personality dimensions and OCB/CWB reported by PSI GPT-40, with a comparison to human self-reported data. The relevant results for PSI Llama3 are provided in Table 29 in Appendix I.2, while Figures 3 and Figure 4 display the complete correlation matrices for GPT-40 and Llama3, respectively. The results demonstrate that the correlations simulated by LLMs between OCB/CWB and various personality dimensions closely resemble the patterns seen in human self-reported data, except for the openness domain. When human reports show positive, negative, or no correlation, the LLM simulations generally exhibit consistent trends in the corresponding directions. However, the correlations generated by LLM simulations are often higher, likely because the model primarily relies on the \"typical\" or \"idealized\" knowledge structures absorbed from its training corpus during simulation, consistently repeating and amplifying such associations. In contrast, human self-reported data contains more random noise, leading to relatively weaker correlations."}, {"title": "6 Conclusion", "content": "In summary, this study proposes a novel method for LLM to simulate human personality distributions\u2014 PSI and provides a comprehensive explanation of its development process and corresponding dataset. Through three experiments, we validated PSI's effectiveness. Results show that PSI matches or even outperforms two-hour interviews (Park et al., 2024) in personality simulation and surpasses existing methods in terms of both item-level accuracy and overall psychometric indicators. One experiment also examined PSI's ability to simulate personality-related behavior, revealing that although LLMs approach human-like performance, they still exhibit idealized tendencies. In summary, a theory-informed structured interview like PSI can better simulate human-like psychometric data. We explore the role of theory-informed structured interviews in simulation in greater detail and examine their potential to advance research, both of which are elaborated upon in Appendix C."}, {"title": "Limitations", "content": "This study has several limitations. First, we explored the potential of PSI in advancing research. Our single experiment demonstrates that theory-informed questions can effectively elicit information embedded in human narrative responses\u2014\u2014particularly those relevant to the target personality construct\u2014allowing LLMs to simulate the corresponding behavior. However, additional experiments are needed to investigate its broader applications. Second, the personality assessment in this paper relies on self-reported personality scales, which can be viewed as a limitation of the study. However, these self-report scales are specifically designed for humans to target personality traits across individuals, in ways that cannot be directly located within LLMs themselves (see Appendix C for a detailed discussion on psychometrics). Furthermore, self-report scales are used often because respondents have privileged access to their own personality, and even with their well-known limitations (e.g., faking or social desirability in self-reports; see Appendix I.3 for the social desirability analysis), they remain valuable to evaluate the personality traits exhibited by LLMs. However, because the primary goal of this study is to have the LLM simulate human responses and behaviors that correspond to extracted personality information, it is reasonable to assess LLMs using methods designed to measure human personality traits. Moreover, this paper tests only two types of LLMs, which may somewhat limit its scope, given the array of LLMs that are now available. However, the conclusions drawn from our two models were highly consistent, and we have reason to believe that the advantages of the PSI method are generally applicable across different LLMs. Furthermore, we designed three different experiments and conducted comprehensive psychometric analyses of various methods within LLMs, ensuring greater rigor and reliability of the results."}, {"title": "Ethical Statement", "content": "We hereby confirm that all authors of this study are aware of the provided ACL Code of Ethics and comply with the Code of Conduct. Human Sample Data This paper discusses the comparison between multiple sets of human data and results generated by LLMs. The collection of human data strictly adhered to relevant ethical guidelines and received approval from the Institutional Review Board. To ensure fair treatment of participants and proper recognition of their contributions, reasonable compensation or course credit (for student participants) was provided. Throughout the research process, we placed a strong emphasis on transparency and openness, also ensuring that all participants signed informed consent forms. Additionally, to safeguard data privacy and uphold ethical standards, the publicly available dataset underwent rigorous screening to exclude any personally identifiable information and was shared only with explicit public consent."}, {"title": "A Appendix: The Structure of Personality", "content": "A key question in personality-related LLM research pertains to personality structure: What is the nature and breadth of the personality traits we want to simulate the human personality distributions? In the research literature, personality structures often emerge from applying factor analysis to individuals' responses to a large number of personality-relevant items. This approach is what has been used to identify the five personaltiy factors in the FFM. Moreover, personality is better understood in terms of one's continuous standing on each of multiple dimensions rather than as static types or profiles. Research data clearly supports this view (Wilmot, 2015; Wilmot et al., 2019). Dividing individuals into limited categories (e.g., 16 types in MBTI) artificially segments continuous dimensions into discrete units, which may overlook important individual differences (Ones and Wiernik, 2018). Another question is whether we should incorporate personality traits and structures beyond the FFM to achieve a more comprehensive understanding of personality, given that additional personality variables and alternative structures have been proposed over the years. It turns out that there can be conceptual overlap among these models (e.g., Hough et al., 2015). For example, a meta-analysis by Joseph and Newman (2010) revealed that emotional intelligence (EI) shows statistically and practically significant relationships with neuroticism and extroversion within the FFM. In fact, when controlling for personality variables, the unique contribution of EI almost disappears. Similarly, Cred\u00e9 et al. (2017) conducted a meta-analysis on grit and found that its core components can largely be explained by conscientiousness, with little added predictive validity beyond that. Moreover, although the HEXACO structure introduces an honesty-humility dimension, the remaining five dimensions align closely with the FFM structure (Lee and Ashton, 2004, 2006). Research by Cutler and Condon (2023) further indicates that nearly all personality semantic information can be classified within the FFM structure. These studies collectively suggest that although further subdivision of personality structures might provide new perspectives, it often leads to conceptual redundancy and measurement complexity without necessarily enhancing predictive validity or theoretical value. Rather than pursuing a wide range of personality frameworks, we will delve deeper into the FFM structure, which is very widely accepted by personality psychologists."}, {"title": "B Appendix: Personality Structured Interview Questions", "content": "Table 3 presents the final set of 32 questions that form the basis of our personality structured interview. This framework was developed by adapting and modifying McAdams's life history interview and narrative identity approach (McAdams, 1995, 1996, 2001), while also incorporating components from the Structured Interview of the Five-Factor Model (SIFFM; Trull et al., 1998). The initial draft of the question pool was created and refined through collaborative discussions among subject matter experts (SMEs). The SME team consisted of two doctoral students, one postdoctoral researcher, and one professor, all specializing in personality psychology. To ensure the quality and clarity of the questions, pilot testing was conducted with six undergraduate research assistants from a personality research lab. This iterative process of development and feedback led to the construction of the personality structured interview. The development process is similar to the development of psychological tests or scales (psychometric). We have provided more details on psychometrics and the development framework in Appendix C."}, {"title": "C Appendix: Psychometrics and Structured Interview Development Framework", "content": "Psychometrics is a field of psychology dedicated to the theory and practice of psychological measurement. It primarily focuses on quantifying psychological traits", "I enjoy being the center of attention\\\") or questions (for interview; e.g., \\\"What are your strongest qualities as a friend? In other words, what makes you a great friend to have?\\\"). The challenge lies in ensuring that these items/questions accurately and consistently capture the construct across different populations and contexts. Theory-Informed Structured Interview for LLM Data Simulation Theory-informed structured interviews are the most suitable method for enabling LLMs to simulate psychometric data. These interviews are specifically designed to capture the constructs underlying the targeted psychometric measures, ensuring that the simulated data aligns with the intended psychological construct. By extracting textual information that directly reflects the target construct, theory-informed structured interviews facilitate the representation of heterogeneous data while preserving a high degree of human diversity, thereby enhancing the validity and applicability of the simulated psychometric data. Moreover, since the information is extracted based on theoretical foundations, it also provides a certain level of interpretability for the LLM's simulation. This not only allows the generated data to be compared with theoretical expectations but also increases its potential for practical applications. The Potential for Advancing Research A theory-informed structured interview transcript-based simulation can generate data more effectively by focusing on the target construct. Ideally, the simulated data should reproduce the same constructs reflected in real-world data and simulate behaviors associated with these constructs. Take personality as an example-if simulated data can accurately replicate real-world personality constructs, it enables research that would be difficult to conduct in reality, such as developing contextualized personality assessment tools and exploring new personality theories through multi-agent simulations. Developing Contextualized Personality Assessment Tools": "Traditional personality assessments mainly rely on standardized questionnaires or laboratory tasks", "Simulation": "If simulated data can accurately reflect real-world personality constructs, we can leverage multi-agent interactive systems to simulate individuals' behavioral patterns and observe how different personality traits evolve in group dynamics. For example, virtual agents with distinct personality traits can be placed in cooperative tasks"}]}