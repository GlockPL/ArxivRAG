{"title": "Towards a Speech Foundation Model for Singapore and Beyond", "authors": ["Muhammad Huzaifah", "Tianchi Liu", "Hardik B. Sailor", "Kye Min Tan", "Tarun K. Vangani", "Qiongqiong Wang", "Jeremy H. M. Wong", "Nancy F. Chen", "Ai Ti Aw"], "abstract": "This technical report describes the MERaLiON Speech Encoder, a foundation model designed to support a wide range of downstream speech applications. Developed as part of Singapore's National Multimodal Large Language Model Programme, the MERaLiON Speech Encoder is tailored to address the speech processing needs in Singapore and the surrounding Southeast Asian region. The model currently supports mainly English, including the variety spoken in Singapore. We are actively expanding our datasets to gradually cover other languages in subsequent releases. The MERaLiON Speech Encoder was pre-trained from scratch on 200K hours of unlabelled speech data using a self-supervised learning approach based on masked language modelling. We describe our training procedure and hyperparameter tuning experiments in detail below. Our evaluation demonstrates improvements to spontaneous and Singapore speech benchmarks for speech recognition, while remaining competitive to other state-of-the-art speech encoders across ten other speech tasks. We commit to releasing our model, supporting broader research endeavours, both in Singapore and beyond.", "sections": [{"title": "Introduction", "content": "We present the MERaLION2 Speech Encoder, a 630M parameter speech foundation model pre-trained on large-scale data to support a wide variety of downstream speech applications. Pre-training was carried out from scratch, under a self-supervised learning (SSL) framework, utilising a BERT-like masked language modelling objective. The current model supports primarily English, with a particular focus on Singapore-accented English and the English-based creole Singlish, which includes and is influenced by a mix of other languages, including Hokkien, Malay, Cantonese, and Tamil. Our goal is to gradually support other major languages spoken throughout Southeast Asia in subsequent releases.\nWhile the encoder can be used stand-alone, development was carried out concurrently with the MERALION-AudioLLM [MERaLiON Team, 2024], a multimodal large language model (LLM) that can handle both speech and text inputs, with the objective of eventually integrating the speech encoder within the AudioLLM framework. The integrated model is under active development and will be included in future releases. We outline our main contributions as follows:\n1. Independent implementation and training of a speech encoder with the BERT-based speech pre-training with random-projection quantizer (BEST-RQ) objective [Chiu et al.,"}, {"title": "Background", "content": "The two-stage paradigm of first pre-training a representation model on large-scale unsupervised data, followed by finetuning on a downstream task with a relatively smaller amount of labelled data, has revolutionized the approach to many speech problems in recent years [Mohamed et al., 2022]. This innovation is driven by various self-supervised learning (SSL) techniques [Schneider et al., 2019, Baevski et al., 2020, Hsu et al., 2021, Chen et al., 2021b, Baevski et al., 2022, Chiu et al., 2022], which are crucial for modelling useful speech representations during pre-training. SSL leverages the intrinsic characteristics of the input speech to train the model without the need for supervised labels specific to any particular task. This approach aims to create a model capable of computing speech embeddings that are broadly informative across numerous tasks [wen Yang et al., 2021, Tsai et al., 2022] - a so-called foundation model. The MERaLION Speech Encoder can then be utilised in various ways, as illustrated in Figure 1. Note that the examples shown are non-exhaustive.\nThere have been numerous approaches to SSL for speech, centred around various pretext tasks [Mohamed et al., 2022] or learning objectives, in addition to distinct methods to obtain targets. For instance, Wav2Vec 2.0 [Baevski et al., 2020] is trained with a contrastive loss to distinguish between positive samples and negative distractors given an anchor representation produced by the encoder. More recent predictive approaches have shown, however, to be more effective and easier to train. The most popular technique leveraged by SOTA models like HuBERT [Hsu et al., 2021] and WavLM [Chen et al., 2021b] rely on a masked language modelling objective popularised by BERT [Devlin et al., 2019] for text. The model is trained to predict masked sections of the input corresponding to targets derived from k-means clustered speech features. For HuBERT, initial speech features, for which the k-means targets are computed over, are Mel-frequency cepstral coefficients (MFCCs). These are then substituted for latent features extracted from an intermediate layer of the trained encoder in subsequent steps. Meanwhile, WavLM uses HuBERT latent features to compute the targets from the outset. This type of training encourages the encoding of meaningful latent representations from the unmasked segments while capturing long-range temporal dependencies through the masked predictions, learning both acoustic and linguistic patterns in the process.\nFor the pre-training of the MERaLION speech encoder, we adopted the BEST-RQ objective, first introduced by Chiu et al. [2022]. BEST-RQ also uses a masked language criterion but simplifies the target computation pipeline by employing a simple random projection of the input features through a projection layer with frozen weights, which is then compared against random cluster centroids. This avoids the computational cost of performing iterative k-means over then entire pre-training dataset. The computational cost of BEST-RQ is further reduced, compared to HuBERT and WavLM, by using the same targets over the entire training process, eliminating the need for multiple iterations of target computation. Despite its more streamlined approach, BEST-RQ and its extension to multiple codebooks in Zhang et al. [2023] have shown comparable or better performance against other SSL models, particularly for ASR. As such, variants of the technique have been adopted to build industrial-scale ASR systems, including by Google [Zhang et al., 2023], AssemblyAI [Ramirez et al., 2024], and ByteDance [Seed Team, 2024]. Unfortunately, a BEST-RQ model trained at scale has so far not been released to the public, hindering the ability to reproduce reported results or further develop and enhance the technique.\nWe independently implemented the BEST-RQ approach. With the goal of supporting the speech processing ecosystem in Singapore and the surrounding region, we collected and processed a speech dataset comprising around 160K hours of English, 30K hours of multilingual speech, and 10K hours of Singapore-based English that includes code-switching. We report the performance of the model on various ASR benchmarks, targeting different scenarios. For this release, we primarily focus on English performance; while the model may be capable of multilingual processing, this evaluation is left to future work. Additionally, to demonstrate the MERaLiON speech encoder's generalisability, we extend the downstream analysis to ten SUPERB tasks: ASR, phoneme recognition (PR), keyword spotting (KS), query by example spoken term detection (QbE), intent classification (IC), slot filling (SF), speaker identification (SID), automatic speaker verification (ASV), speaker diarisation (SD), and emotion recognition (ER). We investigate whether training toward targets computed from randomly projected speech representations, which have shown success in ASR, also perform well on these additional tasks."}, {"title": "Self-supervised Pre-training", "content": "In the following sections, we outline the model architecture, data preparation, and training procedure."}, {"title": "Masked language modelling and random projection quantisation", "content": "The main pre-training objective follows BERT-style masked-language modelling. This entails predicting the correct discrete label from a codebook, over the masked frames of an input speech signal. Figure 2 provides an overview of this framework. To create the target labels, we apply a projection matrix to the input speech signal, obtaining a frame-wise hidden representation that is subsequently mapped to the nearest vector in a codebook. The index of this vector within the codebook is assigned as the label for that particular frame. Both the projection matrix and codebook are randomly initialised and frozen throughout training. The SSL objective involves training an encoder to predict the labels given by the random projection quantiser from a masked version of the same speech input. The cross entropy loss is computed only over the masked frames. Instead of a projection into a single codebook, using multiple N codebooks together with a multi-softmax loss has been shown to further improve performance for speech translation and reduce variations between different runs [Zhang et al., 2023]. This may be due to a gradient with smaller variability, arising from the averaging over the multiple codebooks.\nThe same masking strategy as described in Chiu et al. [2022] is adopted, whereby each frame of the input speech, represented as a Mel-spectrogram, is masked according to a fixed probability with a fixed span from the starting frame. Overlapping masks are allowed, and the masked sections are replaced with noise sampled from a normal distribution with a mean of zero and a standard deviation of 0.1. The random projection quantiser requires a downsampling mechanism to match the sequence length of the labels with that of the output of the speech encoder. Via PyTorch's unfold function, a sliding window was used to group features across frames and stack them in the channel dimension. This feature stacking operation downsamples the target inputs by 4\u00d7 to match the downsampling rate of the feature extractor on the encoding side.\nNormalisation of the inputs in the target creation pipeline was found to significantly impact performance. This may be because ensuring an adequate overlap between the distributions of projections and codewords empowers an efficient utilisation of the available codewords. However, unlike the original implementation which uses both a mean/variance normalisation on the inputs and L2 normalisation of the codebook and random projection features, we found that this combination caused extremely slow convergence during pre-training. Using either normalisation independently alleviated this issue, with mean/variance normalisation outperforming L\u00b2 normalisation in terms of the pre-training loss and downstream ASR performance. Therefore, our final configuration applied only mean/variance normalisation at the segment level, after the initial downsampling, omitting L2 normalisation. The normalised features, \u00e6, were then projected through a separate random matrix, Aj, for each of the j codebooks. We modified the mapping function between the random projection features Aj\u00e6 and ith codeword cij, to an n-dimensional Euclidean distance instead of cosine similarity, which proved more stable during training. Here, n is the dimensionality of the codeword vectors. Using this distance, the target label, yef, was computed as\n$$y_{i}^{ref} = argmin_{i} (A_j x - C_{ij})^T (A_j x - C_{ij}).$$"}, {"title": "Encoder architecture", "content": "The MERLION Speech Encoder contains approximately 630M parameters, comprising 24 Conformer layers, with a hidden size of 1024, feedforward size of 4096, convolution kernel size of 5, and 8 attention heads per layer. The Conformer stack is preceded by a two layer convolutional neural network (CNN)-based feature extractor, interspersed between ReLU non-linearities, and a linear layer at the end. Note that the pre-training methodology itself is agnostic to the choice of encoder architecture.\nWe utilised an implementation of the Conformer from Fairseq [Ott et al., 2019] that includes Multi-head Attention layers with relative positional embedding originating from ESPnet [Watanabe et al., 2018], which was lacking in the Conformer implementation in Torchaudio [Yang et al., 2022]. Preliminary ablation studies showed that the use of relative positional embeddings improved the pre-training loss, compared to other positional embedding methods, including learned, absolute, and ROPE; and all positional embedding strategies outperformed the Torchaudio implementation without positional embedding (see section 4.2)."}, {"title": "Pre-training data", "content": "For this release, we utilised a total of approximately 200K hours of unsupervised speech data (i.e. speech audio samples only, without transcriptions or task-specific labels), predominantly in English to pre-train the encoder. To improve the robustness and generalisability of the model, we strove to compile a diverse dataset covering a wide range of conditions, encompassing factors such as domain, style, speaker, gender, and accent. Data was sourced from eight open and publicly accessible datasets, detailed in Table 1. As far as possible, the official training splits of the datasets were used if provided. For VoxPopuli [Wang et al., 2021] and MLS [Pratap et al., 2020], only the English splits were considered. The massively multilingual dataset Common Voice [Ardila et al., 2020] was included to further increase the diversity of the data although multilingual capability was not a focus this time. To target the commonly spoken variety of English in Singapore, we included the National Speech Corpus (NSC) [Koh et al., 2019]. This dataset is notable for not only consisting of Singapore-accented speech, but also containing Singlish terms, often involving heavy use of code-switching, as well as Singaporean named entities. Further details on the NSC dataset, including characteristics of the different parts, are provided in Appendix A.1."}, {"title": "Training setup", "content": "We carried out the pre-training in two phases. An initial model was first pre-trained on 60K hours of speech from Libri-light. This model and pre-training settings were used to run several preliminary experiments and carry out hyperparameter tuning at a smaller scale prior to the full pre-training, as elaborated in section 4. The final model in this phase was trained for 325K steps on 12 Nvidia A100 40GB GPUs.\nContinuous pre-training starting from the above checkpoint was observed to perform better than starting from random initialisation when the dataset was expanded to the full 200K hours. We therefore initialised the encoder with the Libri-light pre-trained model for the full pre-training (see section 4.1). This training phase was carried out on 128 AMD MI250x GPUs for a further 382K steps, corresponding to about 600 hours, spread over a two-month period. The Adam optimiser was adopted with an inverse square root decay scheduler, using a peak learning rate of 8e-4 and 4K warmup steps. Inputs were batched according to duration using a stratified bucketing schema such that batch size increases with shorter durations. This method reduces padding and utilises the GPU RAM more efficiently, leading to improvements in throughput.\nFollowing ablation experiments (see section 4.3), the masking probability was set to 0.4 for all configurations, differing significantly from the 0.01 used in the original [Chiu et al., 2022]. However, we retained the masking span of 0.4s. Our results also corroborate the benefit of multiple codebooks, discussed in Zhang et al. [2023]. We utilised 32 independent codebooks, with a vocabulary size of 2048 each. Each codebook vector had a dimension of 16. Inputs to the model are log-scaled Mel-spectrograms with 80 Mel-bins, a window duration of 25ms, and a shift between windows of 10ms. All audio were resampled to 16kHz prior to the Mel-spectrogram conversion."}, {"title": "Preliminary experiments and ablation studies", "content": "In this section we provide details on some of the preliminary experiments carried out to better inform us about the optimal settings for pre-training."}, {"title": "Continuous pre-training", "content": "To compare the efficacy between training from random initialisation against continuous pre-training, we compared three different encoder initalisation settings, namely initialising all layers with a primary pre-trained model, initialising only the feature extractor with that from a pre-trained model, and fully random initialisation as a baseline. The encoder pre-trained on Libri-light was chosen as the initial pre-trained model for this experiment. Other parts of the training framework, including the projection quantiser and codebook, were initialised randomly as per usual. To investigate the performance on both in and out-of-domain data, we conducted separate experiments by continuous pre-training on either Librispeech or People's Speech (Figure 3); the former of which is, like Libri-light, derived from Librivox audiobooks, while the latter comes from more varied sources.\nContinuous pre-training was found to be beneficial, with the models initialised with pre-trained encoders outperforming random initialisation after a few hundred steps, according to the validation loss curves. This was seen even when the domain of the original trained checkpoint did not match the domain of the new training data. Initialising just the feature extractor with trained layers as opposed to the entire encoder, however, significantly degrades performance, and is in fact worse than random initialisation. This suggests a tight coupling between the feature extractor and the Conformer layers, and training them separately may be sub-optimal compared to joint training. Given the above findings, we initialised the encoder with the Libri-light model for the full pre-training run with 200K hours of data."}, {"title": "Positional embedding", "content": "We extensively compared various positional embedding methods and implementations. These include relative, learned, RoPE, and absolute, which were all implemented in ESPnet [Watanabe et al., 2018]. We also add an alternative Fairseq implementation of absolute positional embedding. The above were contrasted against the Torchaudio version of the Conformer that omits any positional embeddings. Depending on the method, positional embeddings may come in the form of additional parameters or calculations before the Conformer layers, like in the case of learned or absolute, or directly modify the Transformer, like for relative. These changes result in different parameter sizes among the encoders tested, with absolute and RoPE flavours at a consistent size with the baseline at 608M, relative being slightly larger at 630M, and learned being the biggest at 670M.\nEmpirically, we observed slight differences in performance among the various positional embedding techniques. As seen in Figure 4, relative positional embedding outperformed the other methods slightly. There is evidently also a gap between the same method implemented differently when comparing the ESPnet and Fairseq versions of absolute positional embedding. Furthermore, it was clear that any positional embedding method was better than not using any. Overall, while we found that models using positional embeddings take longer to train because of extra calculations, we ultimately decided that this penalty was worth the improvements in performance."}, {"title": "Masking probability and codebook configuration", "content": "Masking probability was varied between 0.01 and 0.4, corresponding to a masking coverage of 16.8% up to 66.8% of the input. The pre-training loss improves significantly with higher masking rates, allowing training to converge much faster (Figure 5). In terms of training loss, we observe diminishing returns beyond 0.25 as masking coverage approaches the majority of the input, hinting to a sweet spot between having a stronger training signal and maintaining enough unmasked frames for context. Nevertheless, when evaluated on the SUPERB ASR task, performance continued to improve with more aggressive masking beyond 0.25. Hence, we adopted a masking probability of 0.4 for our full pre-training run.\nTable 2 varies the vocabulary size of each codebook with the number of codebooks fixed at 16, and varies the number of codebooks with the vocabulary size per codebook fixed at 2048. Reducing the vocabulary size to 2048 improves the performance for ASR, but may not be optimal for IC. Using 16 codebooks outperforms using a single codebook consistently for both tasks, validating the observation in Zhang et al. [2023]."}, {"title": "Downstream Evaluation", "content": "Our results on ASR and SUPERB benchmarks are reported below."}, {"title": "ASR finetuning", "content": ""}, {"title": "Dataset details", "content": "To provide a more well-rounded ASR evaluation, we chose several benchmarking datasets with differing characteristics, summarised in Table 3. Librispeech [Panayotov et al., 2015], derived from audiobooks, is comprised of read speech with a predominantly American-based accent. TEDLIUM Release 3 (denoted as TEDLIUMv3) [Hernandez et al., 2018], extracted from TED talks, may be considered more spontaneous speech. Meanwhile, the NSC dataset [Koh et al., 2019] was used for Singapore English and Singlish evaluation."}, {"title": "Experimental details", "content": "The models were finetuned using a Connectionist Temporal Classification (CTC) objective [Graves et al., 2006]. A single linear layer was appended after the speech encoder, acting as a classification layer. During finetuning, the encoder was initially frozen to stabilise learning and allow the decoder to adapt, before subsequently training the encoder and decoder jointly. This phased training strategy aligns with established finetuning setups described in Baevski et al. [2020], Hsu et al. [2021], and Chen et al. [2021b]. To ensure effective optimisation, distinct learning rates (LR) and warm-up schedules were applied to the encoder and decoder, following the approach used in Chiu et al. [2022].\nText tokenisation was achieved using a SentencePiece model [Kudo and Richardson, 2018], which is extensively adopted for its subword-based segmentation. A vocabulary size of 1023 was selected for the Librispeech and TEDLIUMv3 datasets, while a larger vocabulary size of 5000 was used for the more diverse NSC dataset to better capture the linguistic variations in Singlish. Data augmentation was performed using SpecAugment, a widely used technique for improving robustness to variability in audio input [Park et al., 2019]. Two time masks of width 80 were applied with a masking probability of 0.2, along with two frequency masks of length 27. Decoding utilised a beam search strategy to enhance transcription quality, without leveraging an external language model, ensuring the results reflect the raw model performance."}, {"title": "Results", "content": "The results on the Librispeech dataset are shown in Table 5. The MERaLION Speech Encoder demonstrated performance comparable to other leading open-source SSL models on both Librispeech splits. Increasing the finetuning data to 960 hours significantly reduces the word error rate (WER) to 2.1% (test-clean) and 4.3% (test-other), that is on par with or better than other SOTA models like Wav2Vec 2.0 and HuBERT when finetuned for the same duration.\nTable 6 summarises the ASR finetuning results on the TEDLIUMv3 dataset. Our model achieves a relative improvement of approximately 10% over WavLM on both the validation and test sets. This outperformance highlights our model's ability to generalise better to the TEDLIUMv3 dataset, demonstrating more effective handling of spontaneous speech compared to WavLM. Although zero-shot models like Whisper v2 achieve reasonable performance without finetuning (e.g. 5.2% WER on LibriSpeech test-other and 4.0% WER on TEDLIUMv3 test), MERaLiON Speech Encoder's finetuning capabilities allow it to achieve lower WERs with supervised training. This highlights the advantage of finetuning when labelled data is available, allowing MERaLiON Speech Encoder to surpass zero-shot baselines in accuracy.\nThe experimental results for NSC are provided in Table 7. Compared to the WavLM model finetuned with the same setup, the MERaLiON Speech Encoder consistently achieves lower WER across all six parts of the NSC dataset. Specifically, Parts 2 and 4 of the NSC dataset are particularly challenging, as Part 2 contains numerous named entities, while Part 4 includes spontaneous code-switching. The MERLION Speech Encoder achieved a 36.5% and 5.8% relative improvement against WavLM large for Parts 2 and 4 respectively, demonstrating its effectiveness across different subsets of the dataset."}, {"title": "Universal representation evaluation with SUPERB", "content": "SUPERB is a collection of downstream speech tasks intended to evaluate the generalisability of the performance of embeddings extracted from a speech encoder. We adhered to the standard SUPERB procedure, where the pre-trained SSL model parameters are frozen. Embeddings from each of the pre-trained model's Conformer or Transformer layers were computed and a separate light-weight downstream model was trained for each task with a learned layer-wise weighted average over these embeddings. Each task's performance was then measured for the cascaded usage of the pre-trained model and respective downstream model. A generalisable SSL model should yield good performance across a variety of tasks. This paper assesses ten tasks across five categories. These are automatic phoneme recognition (PR) and speech recognition (ASR) for recognition, keyword spotting (KS) and query by example (QbE) for detection, intent classification (IC) and slot filling (SF) for semantics, speaker identification (SID), automatic speaker verification (ASV), and speaker diarisation (SD) for speaker, and emotion recognition (ER) for paralinguistics (see Table 8). An overall score was also computed for each model, by multiplying QbE results by 100, subtracting error rates from 100 and averaging across all tasks, following Chen et al. [2021b]. For SUPERB finetuning, we followed the batch sizes and learning rates contained in Table 10. These hyperparameters were chosen simply by using the best result on the test set, between either the default SUPERB hyperparameters or those chosen for WavLM large in Chen et al. [2021b].\nThe results in Table 8 compare the MERLION Speech Encoder against SOTA SSL speech foundation models of comparable parameter size: Wav2Vec 2.0 large, HuBERT large, and WavLM large. Wav2Vec 2.0 large and HuBERT large were trained on 60K hours of speech from Libri-light [Kahn et al., 2020], while WavLM large was trained on 94K hours of speech from the combination of Libri-light, GigaSpeech [Chen et al., 2021a], and VoxPopuli [Wang et al., 2021]. The MERaLiON Speech Encoder performs comparably against HuBERT large, and approaches the WavLM performance on several tasks. This suggests that it is still possible to yield comparable performance across a diversity of tasks, even when making the computational saving trade-offs of the random projection targets and the 4x downsampling. All models outperform Wav2Vec 2.0 large, while WavLM overall performed the best across all tasks."}, {"title": "Future Directions", "content": "The model released in conjunction with this technical report marks the first version of the MERLION Speech Encoder. The future roadmap for our foundation model extends the language coverage to major languages spoken in Southeast Asia besides English. In Singapore, we aim to support the other official languages Malay, Chinese, and Tamil. Outside of Singapore, the goal is to gradually include Indonesian, Javanese, Sundanese, Filipino, Thai, Vietnamese, Burmese, Khmer, and Lao. This list is non-conclusive and other languages may also be considered. Accordingly, we are in the process of scaling our data further in preparation for future training runs.\nWith the expanded language coverage we also wish to increase the breadth and depth of our evaluation to include multilingual benchmarks, like the multilingual(ML)-SUPERB [Shi et al., 2023], among others. In our view, the original SUPERB itself is showing signs of saturation in terms of results for certain tasks like KS, IC, and ASR, and it may be beneficial to expand the evaluation of these tasks to other datasets. In terms of downstream performance, while better than other SOTA models in several ASR domains, particularly for Singapore English and Singlish, the MERaLiON Speech Encoder still falls slightly behind WavLM across the general range of SUPERB tasks. We aim to further refine the training procedure to close this gap, for example through better representation learning during target creation or with the inclusion of augmentation techniques during pre-training.\nIn general, future research paths will also be aligned with the MERaLION-AudioLLM in order to better support the speech modality in a unified system."}, {"title": "Conclusion", "content": "In this report, we present the MERaLiON Speech Encoder, a 630M parameter encoder that acts as a speech foundation model to support a wide range of downstream speech tasks. The encoder was trained from scratch in two phases, first on a smaller 60K hours dataset, before expanding to the full 200K hours. We adapt the BEST-RQ SSL technique for pre-training, by leveraging masked language modelling with randomly projected representations as targets. We demonstrate excellent results for ASR on spontaneous speech and Singapore-accented English, as well as Singlish with the inclusion of code-switch. The model is also generally competent across various other speech tasks encompassing the SUPERB benchmark, and holds its own against other SOTA encoders. We hope sharing our model and experiences will be a catalyst for the advancement of speech processing technologies, especially in Singapore and the surrounding region."}, {"title": "Appendix", "content": ""}, {"title": "Details of National Speech Corpus dataset processing", "content": "The raw NSC dataset comprises approximately 10,600 hours of recordings of Singaporean English speakers, systematically organised into six distinct parts. Although the NSC data set is a valuable resource for model training, it contains a notable amount of mislabelled data, systematic inconsis- tencies, and accidental errors. To ensure data integrity and reliability, we implemented rigorous verification and filtering procedures, extracting only the most accurate and high-quality segments.\nData Splits Consistency: For Parts 1 and 2, we ensured that examples with identical transcriptions were consistently assigned to the same data splits to prevent data leakage.\nTimestamp Verification: For Parts 3 to 6, recordings were selected where the audio duration closely matched the transcription timestamp duration. For conversational audio recorded separately for each speaker, we combined both sides by superimposing their respective audio array representations.\nSegmentation: Longer conversations were segmented into shorter units, each with a maximum duration of 30 seconds.\nTranscription Cleaning: Non-speech annotations such as , , and (ppb) were removed. However, discourse particles (e.g. [oh]), interjections (e.g. !walao!), and fillers (e.g. (um)) were retained to preserve the natural characteristics of spoken dialogue.\nThe details of training and testing dataset after filtering is shown in Table 9. Even after filtering, the dataset remains substantially large. To facilitate efficient ASR model comparison, we further selected a 70 hours subset from each of the six parts randomly, resulting in a total duration of 420 hours. This smaller, more manageable dataset enables streamlined training and evaluation of ASR models. We used the same test splits as mentioned in Table 9 for evaluation in section 5.1."}, {"title": "SUPERB parameters", "content": ""}]}