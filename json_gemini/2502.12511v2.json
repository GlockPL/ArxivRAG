{"title": "Myna: Masking-Based Contrastive Learning of Musical Representations", "authors": ["Ori Yonay", "Tracy Hammond", "Tianbao Yang"], "abstract": "In this paper, we present Myna, a simple yet effective approach for self-supervised musical representation learning. Built on a contrastive learning framework, Myna introduces two key innovations: (1) the use of a Vision Transformer (ViT) on mel-spectrograms as the backbone, replacing SampleCNN on raw audio; and (2) a simple yet novel data augmentation strategy-token masking-that masks 90% of spectrogram tokens (e.g., 16x16 patches). These innovations deliver both effectiveness and efficiency: (i) Token masking enables a significant increase in per-GPU batch size, from 48 or 120 in traditional contrastive methods (e.g., CLMR, MULE) to 4096. (ii) By avoiding traditional augmentations (e.g., pitch shifts), Myna retains pitch sensitivity, enhancing performance in tasks like key detection. (iii) The use of vertical patches (128x2 instead of 16x16) allows the model to better capture critical features for key detection. Our hybrid model, Myna-22M-Hybrid, processes both 16x16 and 128x2 patches, achieving state-of-the-art results. Trained on a single GPU, it outperforms MULE (62M) on average and rivals MERT-95M, which was trained on 16 and 64 GPUs, respectively. Additionally, it surpasses MERT-95M-public, establishing itself as the best-performing model trained on publicly available data. We release our code and models to promote reproducibility and facilitate future research: https://github.com/ghost-signal/myna", "sections": [{"title": "1. Introduction", "content": "The field of Music Information Retrieval (MIR) has been revolutionized by deep learning. Traditionally, tasks such as genre classification, music auto-tagging, chord recognition, and key detection were approached using supervised learning on labeled datasets (Pons et al., 2017; Pons & Serra, 2019; Choi et al., 2017; Won et al., 2020; Baumann, 2021). However, the creation of these datasets is time-consuming and costly, while raw, unlabeled musical data is abundant. This disparity has fueled interest in un- and self-supervised learning, with self-supervised contrastive learning becoming a prominent approach. Recent research has applied frameworks like SimCLR and masked language modeling to extract meaningful musical representations from raw audio or spectrograms (Spijkervet & Burgoyne, 2021; McCallum et al., 2022; Li et al., 2024; Castellon et al., 2021).\nSelf-supervised representation learning minimizes reliance on labeled data by learning a rich latent space that can generalize well to downstream tasks. In contrastive learning, the objective is to maximize agreement between different augmented views of the same data while pushing away other pairs of data (negatives). The use of data augmentations is key to contrastive learning; however, traditional data augmentations for musical data do not necessarily give good performance. For example, augmentations such as pitch shifting alter critical musical properties that are essential for tasks like key detection (McCallum et al., 2024). Our approach instead relies entirely on token masking to sample different subsets of spectrograms as \"views\" of the data, which preserves the meaningful relationships between views.\nWe argue that it is more beneficial to teach a model that the relationship between two masked subsets of the input is"}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Self-Supervised Learning Frameworks", "content": "SimCLR (Chen et al., 2020) is a simple contrastive approach for learning discriminative representations and has found success in areas ranging from computer vision to language (Spijkervet & Burgoyne, 2021; Gao et al., 2022). A similar notable framework is Contrastive Predictive Coding (van den Oord et al., 2018), a universal approach to contrastive learning, which has been successful for MIR- and audio-related tasks such as speaker and phoneme classification using raw audio. Additionally, this work introduced the InfoNCE loss, which is used in SimCLR, CLMR, and Myna."}, {"title": "2.2. General-purpose Audio Representations", "content": "The COLA framework (Saeed et al., 2020) employs a simple contrastive learning framework built on SimCLR and utilizes Mel-spectrogram representations and bilinear comparisons to achieve better results than supervised counterparts. HARES (Wang et al., 2021) further demonstrated that normalizer-free Slowfast networks (trained on the SimCLR objective) lead to effective generalization of audio representations (Feichtenhofer et al., 2019; Brock et al., 2021); this finding was later used by (McCallum et al., 2022) for music-specific tasks."}, {"title": "2.3. Patch Masking", "content": "While effective in sequence modeling, transformers (Vaswani et al., 2017) suffer from quadratic memory and time complexity with respect to the number of tokens. To address this issue, prior work has explored various token masking strategies to reduce computational overhead. In the self-supervised domain, MAE and FLIP (He et al., 2021; Li et al., 2023) used masking on image tokens to increase pre-training efficiency. In the supervised setting, PaSST (Koutini et al., 2021) introduced Patchout (spectrogram masking) to speed up transformer training and achieved state-of-the-art results in audio tagging. Our work is the first to show that spectrogram masking works in the contrastive setting."}, {"title": "2.4. Musical Representations", "content": "MusiCNN (Pons & Serra, 2019), a CNN designed for log-mel spectrograms, draws on the discussion in (Pons et al., 2017) for its efficient design and is pre-trained on a supervised music auto-tagging task. CLMR (Spijkervet & Burgoyne, 2021) adapted the SimCLR framework for music using SampleCNN (Lee et al., 2018) on raw waveforms and"}, {"title": "3. Method", "content": null}, {"title": "3.1. Preliminaries", "content": "Our work builds upon CLMR, which is the music audio adaptation of SimCLR's contrastive learning framework for visual representations. In SimCLR, for every sample $x_i$ in a batch, two augmentations $A(x_i)$ and $A'(x_i)$ are applied, generating two correlated views. These views are passed through the same encoder, and the objective is to maximize agreement between their latent representations using a contrastive loss while minimizing agreement between all other samples in the batch.\nSimCLR consists of:\n\u2022 An encoder $enc(\u00b7)$, which maps the augmented views to a latent space $\\mathbb{R}^{data} \\rightarrow \\mathbb{R}^{latent}$.\n\u2022 A projector network $proj(\u00b7)$, mapping latent representations to a projection space $\\mathbb{R}^{latent} \\rightarrow \\mathbb{R}^{proj}$.\n\u2022 Stochastic augmentations $A(x)$, producing two correlated views $A(x_i)$, $A'(x_i)$ for each sample.\n\u2022 A contrastive loss to maximize the similarity between $A(x_i)$ and $A'(x_i)$ and minimize it between views of all other samples.\nThe contrastive loss used in SimCLR, CLMR, and our work, is the InfoNCE loss (van den Oord et al., 2018), defined for a positive pair of examples $(i, j)$ as:"}, {"title": "3.2. Creation of Positive Pairs", "content": "To generate positive pairs, we first select two three-second segments from the same audio. We generate Mel spectrograms for each segment and then patchify them into 16 \u00d7 16 or 128 \u00d7 2 sections. Each spectrogram patch undergoes a linear projection combined with 2D sinusoidal positional encodings (Beyer et al., 2022) to create token representations.\nFollowing this, we randomly mask 90% of the tokens from each spectrogram, inspired by the methods in (Li et al., 2023) and (He et al., 2021). Positive pairs are constructed using the strategy described in Algorithm 1 and illustrated in Figure 2. This masking enables the model to learn meaningful relationships between the remaining tokens, effectively treating the masked spectrograms as augmented views of the same underlying data. The resulting masked pairs serve as positive samples for our contrastive learning framework.\nIntuitively, masking a high percentage of tokens encourages the model to focus on global patterns and relationships between the unmasked tokens. By treating masked spectrograms as augmented views, the model is trained to reconstruct meaningful relationships between the unmasked tokens and their masked counterparts. This forces the model to infer higher-level, context-aware features rather than overfitting to specific low-level details that might only be locally relevant. Since the masking process only hides information without altering it (unlike traditional augmentations such as pitch shifting or time stretching), the underlying properties of the music, like pitch/key and BPM, remain intact in both views. This ensures that the model learns representations that are robust to missing information and invariant to the masking operation, allowing it to generalize better to downstream tasks that depend on recognizing the overall structure and relationships in the data."}, {"title": "3.3. Why Not Masked Auto-Encoding?", "content": "Previous work has demonstrated that masked auto-encoding is an effective pre-training task for learning representations in various domains (Niizumi et al., 2022; He et al., 2021). Below, we outline three reasons against using masked auto-encoding for musical representation learning and instead favor a contrastive learning framework."}, {"title": "3.3.1. EFFICIENCY", "content": "MAE frameworks require training both an encoder and a decoder. While the decoder is necessary for reconstruction during pre-training, it is discarded when transitioning to downstream tasks. This means a substantial portion of computational resources during training is devoted to learning and optimizing a decoder that is ultimately unused. By contrast, our masking-based contrastive learning framework eliminates the need for a decoder entirely and thus reduces computational overhead."}, {"title": "3.3.2. TASK DIFFICULTY", "content": "In masked auto-encoding, the model is tasked with reconstructing the original input from masked portions, which can be a challenging and sometimes counterproductive objective for music. While MAE has shown success in environmental sound classification, where sounds often exhibit simpler and more repetitive patterns, music exhibits high variability and structural complexity. Musical patterns often span longer temporal contexts, and the relationships between different components (e.g., melody, harmony, rhythm) can be intricate. This makes the reconstruction task disproportionately difficult. Contrastive learning, on the other hand, focuses"}, {"title": "3.3.3. PRESERVING MUSICALLY RELEVANT FEATURES", "content": "MAE forces the model to focus on reconstructing fine-grained details, which may not always align with the musically meaningful features needed for tasks like music tagging, key detection, or emotion recognition. For example, reconstructing the exact values of masked spectrogram tokens could encourage the model to focus on local energy patterns rather than higher-level tonal or rhythmic structures. Contrastive learning emphasizes capturing meaningful global representations, ensuring that the learned features are aligned with the downstream tasks."}, {"title": "3.4. Model Architecture", "content": "We use a simplified version of the Vision Transformer (ViT) (Dosovitskiy et al., 2020), SimpleViT (Beyer et al., 2022), which replaces the CLS token with global average pooling and employs 2D sinusoidal positional encodings. For all experiments in this paper, we use the ViT-S/16 architecture (22M parameters), with the exception of using 16 \u00d7 16 or 128 \u00d7 2 non-overlapping patches."}, {"title": "3.5. Hybrid Models", "content": "Our experiments show that using square (16 \u00d7 16) patches yields competitive performance. Conversely, using vertical (128 x 2) patches reduces performance across all metrics, except for key detection, where it achieves state-of-the-art (SOTA) performance among self-supervised methods. To combine the strengths of both approaches, we propose a novel hybrid model compatible with both patch configurations.\nThe hybrid model retains a shared encoder and projector but employs two separate tokenizers (linear projections) and positional embeddings tailored for the two patch sizes. During training, we alternate between patch configurations. Specifically, at each iteration, we calculate the contrastive loss for each patch configuration independently and then optimize the average of the two losses. The overall objective is:\n$\\mathcal{L}_{hybrid} = \\frac{1}{2} (\\mathcal{L}_{square} + \\mathcal{L}_{vertical})$\nwhere $\\mathcal{L}_{square}$ and $\\mathcal{L}_{vertical}$ are the contrastive losses computed using two separate forward passes with 16 \u00d7 16 and 128 x 2 patches using Algorithm 1, respectively. Figure 3 illustrates the computation process of the hybrid model's loss."}, {"title": "3.6. Hyperparameters", "content": "We extract Mel spectrograms with 128 bins with a sample rate of 16 kHz using the nnAudio library (Cheuk et al., 2020), a batch size of 4096, and a 90% masking ratio. We use the Adam optimizer (Kingma & Ba, 2017) with a learning rate of 3e-4 and weight decay of 1e-5 for 500 epochs (total of 411 million examples seen). For the contrastive loss, we set $T$ = 0.1. We use a single NVIDIA A100 GPU for training and four NVIDIA A100 GPUs for masking ablations, as lower masking ratios are less efficient and require multiple GPUs.\nWhile work exists on learning $T$ via gradient descent (Radford et al., 2021) or individualized temperature values (Zi-Hao Qiu & Yang, 2023), we keep it constant in this work."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Pre-Training Dataset", "content": "We pre-train our models on the music subset of the Audioset dataset (Gemmeke et al., 2017), containing roughly 822k 10-second music audio segments. Notably, unlike CLMR, we did not train our model on any of the datasets used for downstream tasks (CLMR was pre-trained on the MagnaTagATune dataset)."}, {"title": "4.2. Downstream Datasets", "content": "MagnaTagATune (MTT): The MTT dataset comprises 25,863 clips, each 29 seconds long, annotated with a set of 188 tags that cover genres, moods, instruments, and other sonic characteristics (Law et al., 2009). Similarly to previous work, we use the standard (12:1:3) train, validation, and test split (van den Oord et al., 2014; Pons et al., 2017) and do not discard any examples (see (Won et al., 2020)). We report both common metrics for this task: area under the receiver operating characteristic curve and average precision. Following previous work, we only use the top 50 tags for evaluation.\nGTZAN: The GTZAN dataset (Tzanetakis & Cook, 2002), a cornerstone dataset for genre classification in MIR, comprises 1,000 audio tracks, each 30 seconds long, spanning 10 diverse genres. For a fair comparison with previous work, we use the fault-filtered set as described in (Kereliuk et al., 2015; Sturm, 2013)."}, {"title": "4.3. Models", "content": "We train and evaluate three Myna models with various patch size configurations. Myna-Base, our base model, operates on 16 \u00d7 16 patches. Myna-Vertical operates on 128 \u00d7 2 patches. Myna-Hybrid is a hybrid model trained to support both patch sizes simultaneously. During evaluation of Myna-Hybrid, we evaluate a single linear model on the"}, {"title": "4.4. Results", "content": "For a direct and fair comparison with other approaches, we use the exact data splits, metrics, and evaluation procedure as in (Castellon et al., 2021). We briefly summarize the evaluation procedure below for completeness.\nTo extract relevant information from representations, we employ simple models-linear probes and shallow MLPs-trained on fixed representation vectors to predict task-specific labels. We conduct a grid search over architectures and hyperparameters for each task, varying the model type, hidden dimension, learning rate, and regularization (see Appendix C for more). The model achieving the best performance on a validation set is then evaluated on the test set. This protocol allows for an apples-to-apples comparison of the quality of representations produced by different pre-training strategies.\nBased on the results presented in Table 1, Myna demonstrates competitive or superior performance across multi-"}, {"title": "4.5. Masking Ratios", "content": "We investigate the impact of varying the masking ratio on model performance. As shown in Figure 4, we find that higher masking percentages consistently lead to better results. Additionally, we note a clear correlation between the masking ratio and the model's average performance, and suspect that low masking ratios make the contrastive task too easy, which leads to less discriminative (and thus useful) representations. This is particularly advantageous since increasing the masking ratio also improves computational efficiency by reducing the number of tokens that the model needs to attend to.\nWe qualitatively test Myna's discriminative capacity on the GTZAN dataset against MAE and CLMR in Figure 5. Myna demonstrates clearer separation between classes, with noticeably reduced overlap between class clusters. This indicates that Myna's embeddings capture more meaningful and discriminative features and explains its improved generalization."}, {"title": "4.6. Comparing with Masked Auto-Encoder", "content": "MAE, with its focus on reconstructing masked spectrogram tokens, performs well in tasks requiring detailed local information (such as local harmonics that aid with genre classification and many of the tags in MTT, as shown in Table 1) but struggles with tasks that rely on understanding broader musical contexts, such as key detection. Our approach, which instead emphasizes learning global relationships through token masking, consistently achieves stronger generalization across these tasks. For example, in key detection, our model benefits from its ability to capture harmonic relationships without being constrained by the need to reconstruct low-level spectrogram details. This suggests that while MAE excels at learning fine-grained patterns, its objectives might not align with the structural and contextual complexities of music, whereas our contrastive framework effectively bridges this gap by focusing on meaningful, high-level representations."}, {"title": "5. Future Work", "content": "Although Myna has shown promising results in the domain of musical representation learning, there are several potential avenues to extend this research to broader applications and further enhance its capabilities. In the following, we outline three key directions for future work."}, {"title": "5.1. Other Modalities", "content": "The Myna framework's reliance solely on token masking as an augmentation strategy is inherently domain-agnostic. We do not see any limitations to the adaptation of this masking-only approach to other modalities, such as images or text. In the text domain, token masking has already shown success with transformer models like BERT, but we believe applying a masking-only contrastive framework could offer new insights."}, {"title": "5.2. Scaling", "content": "Our proposed method significantly reduces the computational burden necessary to achieve competitive results, making large-scale training more accessible. Future work should explore the effects of scaling Myna with larger models and more extensive datasets. We anticipate further improvements in representation quality and downstream task performance from this avenue of future work."}, {"title": "5.3. Masking Policies", "content": "In this work, we sampled token subsets for positive pairs from a uniform distribution. We believe that more sophisticated sampling policies (learned or fixed) could lead to better results or faster convergence. This has recently been shown to work in language/image pretraining (Liang & Larson, 2024)."}, {"title": "6. Conclusion", "content": "In this work, we introduced Myna, a domain-agnostic contrastive learning framework that uses token masking as the sole augmentation strategy. Our approach has shown that this method is effective in learning meaningful representations in the music audio domain while offering significant computational benefits. By leveraging a ViT-based architecture and using token masking as our augmentation, we achieved competitive results with significantly reduced computational requirements. We hope that Myna inspires future research to further explore masking-based contrastive learning."}, {"title": "A. Batch Size Ablations", "content": "We conduct ablation studies on batch size to investigate its effect on task performance. Results verify previous work (Chen et al., 2020) and theory (Yuan et al., 2022) that suggests larger batch sizes yield better performance in the contrastive setting. As shown in Table 3, increasing the batch size from 256 to 4096 leads to noticeable and consistent improvements in both individual metrics and the overall average performance. The best results are achieved at the largest batch size of 4096 (Myna-Base), indicating that larger batch sizes are beneficial for achieving optimal performance."}, {"title": "B. Masked Auto-Encoder Visualizations", "content": "This section provides visualizations of the Masked Auto-Encoder (MAE) outputs for four randomly selected spectrograms from a held-out validation set. We overlay the output spectrogram with the ground truth unmasked (input) patches to showcase how unmasked patches affect the model's output."}, {"title": "C. Evaluation Procedure", "content": "To evaluate representations for downstream MIR tasks, we follow the procedure as outlined in (Castellon et al., 2021): shallow supervised models (linear models and one-layer MLPs) are trained on each task using the representations as input features. A grid search over the following 216 hyperparameter configurations is conducted:\n\u2022 Feature standardization: {off, on}\n\u2022 Model type: {Linear, one-layer MLP with 512 hidden units}\n\u2022 Batch size: {64, 256}\n\u2022 Learning rate: {1e-5, 1e-4, 1e-3}\n\u2022 Dropout probability: {0.25, 0.5, 0.75}\n\u2022 L2 regularization: {0, 1e-4, 1e-3}\nEarly stopping is applied based on task-specific metrics computed on validation sets, with the optimal model from each grid search evaluated on the task-specific test set. Loss functions are tailored to each task: cross-entropy for genre classification and key detection, independent binary cross-entropy for tagging, and mean squared error for emotion recognition."}]}