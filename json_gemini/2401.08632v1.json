{"title": "Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement Learning", "authors": ["MAXENCE FALDOR", "F\u00c9LIX CHALUMEAU", "MANON FLAGEAT", "ANTOINE CULLY"], "abstract": "A fundamental trait of intelligence involves finding novel and creative solutions to address a given challenge or to adapt to unforeseen situations. Reflecting this, Quality-Diversity optimization is a family of Evolutionary Algorithms, that generates collections of both diverse and high-performing solutions. Among these, MAP-ELITES is a prominent example, that has been successfully applied to a variety of domains, including evolutionary robotics. However, MAP-ELITES performs a divergent search with random mutations originating from Genetic Algorithms, and thus, is limited to evolving populations of low-dimensional solutions. PGA-MAP-ELITES overcomes this limitation using a gradient-based variation operator inspired by deep reinforcement learning which enables the evolution of large neural networks. Although high-performing in many environments, PGA-MAP-ELITES fails on several tasks where the convergent search of the gradient-based variation operator hinders diversity. In this work, we present three contributions: (1) we enhance the Policy Gradient variation operator with a descriptor-conditioned critic that reconciles diversity search with gradient-based methods, (2) we leverage the actor-critic training to learn a descriptor-conditioned policy at no additional cost, distilling the knowledge of the population into one single versatile policy that can execute a diversity of behaviors, (3) we exploit the descriptor-conditioned actor by injecting it in the population, despite network architecture differences. Our method, DCG-MAP-ELITES-AI, achieves equal or higher QD score and coverage compared to all baselines on seven challenging continuous control locomotion tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "A fascinating aspect of evolution is its ability to generate a variety of different species, each being adapted to their niche. Inspired by this idea, Quality-Diversity (QD) optimization is a family of evolutionary algorithms that aims to generate a set of both high-performing and diverse solutions to a single problem [5, 9, 35]. Contrary to traditional optimization methods that return a single high-performing solution, the goal of QD algorithms is to illuminate a search space of interest called"}, {"title": "2 BACKGROUND", "content": null}, {"title": "2.1 Problem Statement", "content": "We consider an agent sequentially interacting with an environment at discrete time steps t for an episode of length T. At each time step t, the agent observes a state $s_t$, takes an action $a_t$ and receives a scalar reward $r_t$. We model it as a Markov Decision Process (MDP) which comprises a state space S, a continuous action space A, a stationary transition dynamics distribution $p(s_{t+1} | s_t, a_t)$ and a reward function r: S \u00d7 A \u2192 R. In this work, a policy (also called solution) is a deterministic neural network parameterized by \u03c6 \u2208 \u03a6, and denoted $\u03c0_\u03c6$: S \u2192 A. The agent uses its policy to select actions and interact with the environment to give a trajectory of states, actions and rewards. The fitness of a solution is given by F : \u00de \u2192 R, defined as the expected discounted return $E_{\u03c0_\u03c6} [\\Sigma_{t=0}^{T-1} \u03b3^t r_t]$ . In this setting, the objective of QD algorithms is to find the highest fitness solutions in each point of the descriptor space D. The descriptor function D: \u0424 \u2192 D is generally defined by the user and characterizes solutions in a meaningful way for the type of diversity desired. With this notation, our objective is to evolve a population of solutions that are both high-performing with respect to F and diverse with respect to D."}, {"title": "2.2 MAP-ELITES", "content": "Multi-dimensional Archive of Phenotypic Elites (MAP-ELITES) [30] is a simple yet effective QD algorithm, that discretizes the descriptor space D into a multi-dimensional grid of cells called archive X and searches for the best solution in each cell, see Algorithm 14. The goal of the algorithm is to return an archive that is filled as much as possible with high-fitness solutions. MAP-ELITES starts by generating random solutions and adding them to the archive. The algorithm then repeats the following steps until a budget of I solutions have been evaluated: (1) a batch of solutions from the archive are uniformly selected and modified through mutations and/or crossovers to produce offspring, (2) the fitnesses and descriptors of the offspring are evaluated, and each offspring is placed in its corresponding cell if and only if the cell is empty or if the offspring has a better"}, {"title": "2.3 Deep Reinforcement Learning", "content": "Deep Reinforcement Learning (RL) [29] combines the reinforcement learning framework with the function approximation capabilities of deep neural networks to represent policies and value functions in high-dimensional state and action spaces. In opposition to black-box optimization methods like evolutionary algorithms, RL leverages the structure of the MDP in the form of the Bellman equation to achieve better sample efficiency. The objective is to find an optimal policy $\u03c0_\u03c6$, which maximizes the expected return or fitness $F(\u03c0_\u03c6)$. In reinforcement learning, many approaches try to estimate the action-value function $Q^\u03c0(s, a) = E_\u03c0[\\Sigma_{i=0}^{T-t-1} \u03b3^i r_{t+i} | s_t = s, a_t = a]$ defined as the expected discounted return starting from state s, taking action a and thereafter following policy \u03c0. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm [19] is an actor-critic, off-policy reinforcement learning method that achieves state-of-the-art results in environments with large and continuous action space. TD3 indirectly learns a policy $\u03c0_\u03c6$ via maximization of the action-value function $Q_\u03b8(s, a)$. The approach is closely connected to Q-learning [19] and tries to approximate the optimal action-value function $Q^\u2217(s, a)$ in order to find the optimal action $\u03c0^\u2217(s) = arg \\max_a Q^\u2217(s, a)$. However, computing the maximum over action in $max_a Q_\u03b8(s, a)$ is intractable in continuous action space, hence it is approximated using $max_a Q_\u03b8(s, a) = Q_\u03b8(s, \u03c0_\u03c6(s))$. In TD3, the policy $\u03c0_\u03c6$ takes actions in the environment and the transitions are stored in a replay buffer. The collected experience is then used to train a pair of critics $Q_{\u03b8_1}$, $Q_{\u03b8_2}$ using temporal difference. Target networks $Q_{\u03b8_1}'$, $Q_{\u03b8_2}'$ are updated to slowly track the main networks. Both critics use a single regression target y, calculated using whichever of the two target critics gives a smaller estimated value and using target policy smoothing by sampling a noise $\u03f5 \\sim clip(\\mathcal{N}(0, \u03c3), \u2212c, c)$: $y = r(s_t, a_t) + \u03b3 \\min_{i=1,2} Q_{\u03b8_i}' (s_{t+1}, \u03c0_{\u03c6}' (s_{t+1}) + \u03f5) \\qquad(1)$ Both critics are learned by regression to this target and the policy is learned with a delay, only updated every A iterations simply by maximizing $Q_{\u03b8_1}$ with $max_\u03c6 E [Q_{\u03b8_1} (s, \u03c0_\u03c1(s))]$. The actor is updated using the deterministic policy gradient: $\u2207_\u03c6 J(\u03c6) = E [\u2207_\u03c6 \u03c0_\u03c6(s)\u2207_a Q_{\u03b8_1} (s, a)|_{a=\u03c0_\u03c6(s)}] \\qquad(2)$"}, {"title": "2.4 PGA-MAP-ELITES", "content": "Policy Gradient Assisted MAP-Elites (PGA-MAP-ELITES) [31] is an extension of MAP-ELITES that is designed to evolve deep neural networks by combining the directed search power and sample efficiency of RL methods with the exploration capabilities of genetic algorithms, see Algorithm 9. The algorithm follows the usual MAP-ELITES loop of selection, variation, evaluation and addition for a budget of I iterations, but uses two parallel variation operators: half of the offspring are generated using a standard Genetic Algorithm (GA) variation operator and half of the offspring are generated using a Policy Gradient (PG) variation operator. During each iteration of the loop, PGA-MAP-ELITES stores the transitions from offspring evaluation in a replay buffer B and uses it to train a pair of critics based on the TD3 algorithm, described in Algorithm 10. The trained critic is then used in the PG variation operator to update the selected solutions from the archive for m gradient steps to select actions that maximize the approximated action-value function, as described"}, {"title": "3 RELATED WORK", "content": null}, {"title": "3.1 Scaling QD to Neuroevolution", "content": "The challenge of evolving diverse solutions in a high-dimensional search space has been an active research subject over recent years. MAP-ELITES-ES [7] scales MAP-ELITES to high-dimensional solutions parameterized by large neural networks. This algorithm leverages Evolution Strategies [36] (ES) to perform a directed search that is more efficient than random mutations used in Genetic Algorithms. Fitness and novelty gradients are estimated locally from many perturbed versions of the parent solution to generate a new one. The population tends towards regions of the parameter space with higher fitness or novelty but it requires to sample and evaluate a large number of solutions, making it particularly data inefficient. To improve sample efficiency, methods that combine MAP-ELITES with RL [31, 33, 34, 42] have emerged and use time step level information to efficiently evolve populations of high-performing and diverse neural network for complex tasks. PGA-MAP-ELITES [31] uses policy gradients for part of its mutations, see Section 2.4 for details. CMA-MEGA [42] estimates descriptor gradients with ES and combines the fitness gradient and the descriptor gradients with a CMA-ES mechanism [16, 22]. QD-PG [34] introduces a diversity reward based on the novelty of the states visited and derives a policy gradient for the maximization of those diversity rewards which helps exploration in settings where the reward is sparse or deceptive. PBT-MAP-ELITES [33] mixes MAP-ELITES with a population based training process [25] to optimize hyper-parameters of diverse RL agents. Interestingly, recent work [41] scales the algorithm CMA-MAE [17] to high-dimensional policies on robotics tasks with pure ES while showing comparable data efficiency to QD-RL approaches, but is still outperformed by PGA-MAP-ELITES."}, {"title": "3.2 Conditioning the critic", "content": "None of the methods described in the previous section take a descriptor into account when deriv-ing policy gradients used to mutate solutions. In other words, they do not use descriptor-conditioned policies nor descriptor-conditioned critics as our method does. The concept of descriptor-conditioned critic is related to Universal Value Function Approximators [37], extensively used in skill discovery reinforcement learning, a field that share a similar motivation to QD [2]. In VIC, DIAYN, DADS, SMERL [11, 20, 27, 38], the actors and critics are conditioned on a sampled prior but does not correspond to a real posterior like in DCG-MAP-ELITES-AI. Furthermore, those methods use a notion of diversity defined at the step-level rather than trajectory-level like DCG-MAP-ELITES-AI. Moreover, they do not use an archive to store a population, resulting in much smaller sets of final policies. Finally, it has been shown that QD methods are competitive with skill discovery reinforcement learning algorithms [2], specifically for adaptation and hierarchical learning."}, {"title": "3.3 Archive distillation", "content": "Distilling the knowledge of an archive into a single policy is an alluring process that reduces the number of parameters outputted by the algorithm and enables generalization and interpola-tion/extrapolation. Although distillation is usually referring to policy distillation \u2013 learning the observation/action mapping from a teacher policy - we present archive distillation as a general term referring to any kind of knowledge transfer from an archive to another model, should it be the policies, transitions experienced in the environment, full trajectories or discovered descriptors."}, {"title": "4 METHODS", "content": null}, {"title": "4.1 Descriptor-Conditioned Critic", "content": "Instead of estimating the action-value function with $Q_\u03b8(s, a)$, we want to estimate the descriptor-conditioned action-value function with $Q_\u03b8(s, a | d)$. When a policy \u03c0 interacts with the environment, it generates a trajectory, which is a sequence of transitions (s, a, r, s') with descriptor d. We extend the definition of a transition (s, a, r, s') to include the observed descriptor d of the trajectory (s, a, r, s', d). However, the descriptor is only available at the end of the episode, therefore the transitions can only be augmented with the descriptor after the episode is completed. In all the tasks we consider, the reward function is positive $r : S \u00d7 A \u2192 R^+$ and hence, the fitness function F and action-value function are positive as well. Thus, for any target descriptor $d' \u2208 D$, we define the descriptor-conditioned critic as equal to the normal action-value function when the policy achieves the target descriptor d' and as equal to zero when the policy does not achieve the target descriptor d'. Given a transition (s, a, r, s', d), and a target descriptor d' sampled in D, $Q_\u03b8(s, a | d') := \\begin{cases}  Q_\u03b8(s, a), & \\text{if } d = d' \\\\  0, & \\text{if } d \\neq d'  \\end{cases} \\qquad(3)$ However, with this piecewise definition, the descriptor-conditioned action-value function is not continuous and violates the universal approximation theorem continuity hypothesis [24]. To address this issue, we introduce a similarity function $S: D^2 \u2192]0, 1]$ defined as $S(d, d') = e^{-\u03b1 \\frac{||d-d'||_D}{l}}$ to smooth the descriptor-conditioned critic and relax Equation (3) into: $Q_\u03b8 (s, a | d') = S(d, d') Q_\u03b8(s, a) = S(d, d') E_\u03c0 [\\Sigma_{i=0}^{T-t-1} \u03b3^i r_{t+i} | s, a] = E_\u03c0 [\\Sigma_{i=0}^{T-t-1} \u03b3^i S(d, d')r_{t+i} | s, a] \\qquad(4)$ With Equation (4), we demonstrate that learning the descriptor-conditioned critic is equivalent to scaling the reward by the similarity S(d, d') between the descriptor of the trajectory d and the target descriptor d'. Therefore, the critic target in Equation (1) is modified to include the similarity scaling and the descriptor-conditioned actor: $y = S(d, d') r(s_t, a_t) + \u03b3 \\min_{i=1,2} Q_{\u03b8_i}' (s_{t+1}, \u03c0_{\u03c6}' (s_{t+1} | d') + \u03f5 | d') \\qquad(5)$ If the target descriptor d' is approximately equal to the observed descriptor d of the trajectory d \u2248 d', then we have S(d, d') \u2248 1 so the reward is unchanged. However, if the descriptor d' is different from the observed descriptor d, then the reward is scaled down to S(d, d') r(s_t, a_t) \u2248 0. The scaling ensures that the magnitude of the reward depends not only on the quality of the action a with regards to the fitness function F, but also on achieving the target descriptor d'. Given one transition (s, a, r, s', d), we can generate infinitely many critic updates by sampling a target descriptor $d' \u2208 D$. This is leveraged in the new actor-critic training introduced with DCG-MAP-ELITES-AI, which is detailed in Algorithm 2 and Section 4.3."}, {"title": "4.2 Descriptor-Conditioned Actor and Archive Distillation", "content": "The training of the critic requires to train an actor $\u03c0_\u03c6$ to approximate the optimal action $a^\u2217$, as explained in Section 2.3. However, in this work, the action-value function estimated by the critic is conditioned on a descriptor d. Hence, we don't want $\u03c0_\u03c6$ to estimate the best action globally, but rather the best action given that it achieves the target descriptor d. Therefore, the actor is extended to a descriptor-conditioned policy $\u03c0_\u03c6(s | d)$, that maximizes the descriptor-conditioned"}, {"title": "4.3 Actor-Critic Training", "content": "critic's value with $max_4 E [Q_\u03b8(s, \u03c0_\u03c6 (s | d) | d)]$. The actor is updated using the deterministic policy gradient, see Algorithm 2: $\u2207_\u03c6 J(\u03c6) = \\frac{1}{N} \\Sigma \u2207_\u03c6 \u03c0_\u03c6(s | d')\u2207_a Q_{\u03b8_1} (s, a | d')|_{a=\u03c0_\u03c6 (s|d')} \\qquad(6)$ The policy $\u03c0_\u03c6(s | d)$ learns to suggest actions a that optimize the return while generating a trajectory achieving descriptor d. Consequently, the descriptor-conditioned actor can exhibit a wide range of descriptors, effectively distilling some of the capabilities of the archive into a single versatile policy."}, {"title": "4.4 Descriptor-Conditioned PG Variation", "content": "Once the critic $Q_\u03b8(s, a | d)$ is trained, it can be used to improve the fitness of any solutions in the archive, as described in Algorithm 3. First, a parent solution $\u03c0_\u03c8$ is selected from the archive and we denote its descriptor by $d_\u03c8 := D(\u03c0_\u03c8)$. Notice that this policy $\u03c0_\u03c8(s)$ is not descriptor-conditioned, contrary to the actor $\u03c0_\u03c6(s | d)$. Second, we apply the PG variation operator from Equation (7), for m gradient steps, using the descriptor $d_\u03c8$ to condition the critic: $V_\u03c8 J(\u03c8) = \\Sigma \u2207_\u03c8 \u03c0_\u03c8(s)V_a Q_{\u03b8_1} (s, a | d_\u03c8)|_{a=\u03c0_\u03c8(s)} \\qquad(7)$ The goal is to improve the quality of the solution $\u03c0_\u03c8$, while keeping the same diversity $d_\u03c8$. To that end, the critic is used to evaluate actions and guides $\u03c0_\u03c8$ to (1) improve fitness, while (2) achieving descriptor $d_\u03c8$."}, {"title": "4.5 Descriptor-Conditioned Actor Injection", "content": "In PGA-MAP-ELITES, the actor is injected in the offsprings and considered for addition in the archive at each generation. Empirical analyses [13] have demonstrated the importance of actor injection to achieve good performance. Similarly to PGA-MAP-ELITES, we devise a descriptor-conditioned actor injection (AI) mechanism, to improve the performance of our method, DCG-MAP-ELITES-AI. There is however a significant challenge. The GA isoline variation operator [44] used in PGA-MAP-ELITEs and DCG-MAP-ELITES GECCO requires that all policies in the archive share the same architecture. However, in DCG-MAP-ELITES-AI, the actor is descriptor-conditioned, while the policies in the archive are not. Thus, the first layer of the actor is larger because it takes as input a state and a descriptor, while the first layer of the policies in the archive are smaller because"}, {"title": "5 EXPERIMENTS", "content": "Each experiment is replicated 20 times with random seeds, over one million evaluations and the implementations are based on the QDax library [3]. The full source code will be made available upon acceptance, in a containerized environment in which all the experiments and figures can be reproduced. For the quantitative results, we report p-values based on the Wilcoxon-Mann-Whitney U test with Holm-Bonferroni correction."}, {"title": "5.1 Tasks", "content": "We evaluate DCG-MAP-ELITES-AI on seven continuous control locomotion QD tasks [31] imple-mented in Brax [18] and derived from standard RL benchmarks, see Table 1. Ant Omni, AntTrap Omni and Humanoid Omni are omnidirectional tasks, in which the objective is to minimize energy consumption and the descriptor is the final position of the agent. Walker Uni, HalfCheetah Uni, Ant Uni and Humanoid Uni are unidirectional task in which the objective is to go forward as fast as possible while minimizing energy consumption and the descriptor is the feet contact rate for each foot of the agent. Walker Uni, HalfCheetah Uni, Ant Uni were introduced in PGA-MAP-ELITES paper [31] and Humanoid Uni, Ant Omni, Humanoid Omni were introduced by Flageat et al. [15]. AntTrap Omni is adapted from QD-PG paper [34], the only difference being the elimination of the forward term in the reward function. We introduce AntTrap Omni to evaluate DCG-MAP-ELITES-AI on a deceptive, omnidirectional environment. The trap creates a discontinuity of fitness in the descriptor space as points on both sides of the trap are close, but require two different trajectories to"}, {"title": "5.2 Main Results", "content": null}, {"title": "5.2.1 Baselines.", "content": "We compare DCG-MAP-ELITES-AI with four state-of-the-art algorithms, namely MAP-ELITES [43], MAP-ELITES-ES [7], PGA-MAP-ELITES [31] and QD-PG [34]."}, {"title": "5.2.2 Metrics.", "content": "We consider the QD score, coverage and max fitness to evaluate the final populations (i.e. archives) of all algorithms throughout training, as defined in Flageat et al. [15], Pugh et al. [35] and used in PGA-MAP-ELITES paper [31]. The main metric is the QD score, which represents the sum of fitness of all solutions stored in the archive. This metric captures both the quality and the diversity of the population. In the tasks considered, the fitness is always positive, which avoids penalizing algorithms for finding additional solutions. We also consider the coverage, which represents the proportion of filled cells in the archive, measuring descriptor space illumination. Finally, we also report the max fitness, which is defined as the fitness of the best solution in the archive."}, {"title": "5.2.3 Results.", "content": "The experimental results presented in Figure 2 demonstrate that DCG-MAP-ELITES-AI achieves equal or higher QD score and coverage than all baselines on all tasks, especially PGA-MAP-ELITES, the previous state-of-the-art. On Ant Uni and Humanoid Uni, DCG-MAP-ELITES-AI achieves a higher median QD score but not significantly. On all other tasks, DCG-MAP-ELITES-AI achieves a significantly higher QD score (p < 0.003), demonstrating that our method generates populations of solutions that are higher-performing and more diverse. Especially, the coverage metric shows that DCG-MAP-ELITES-AI surpasses the exploration capabilities of QD-PG on all tasks (p < 0.05). DCG-MAP-ELITES-AI significantly outperforms the GECCO version [12] on all environments except Ant Uni (p < 0.01), where they perform similarly, showing that the"}, {"title": "5.3 Ablations", "content": null}, {"title": "5.3.1 Ablation studies.", "content": "We also compare DCG-MAP-ELITES-AI with three ablations, namely DCG-MAP-ELITES GECCO [12], Ablation AI and Ablation Actor. In DCG-MAP-ELITES GECCO, there is"}, {"title": "5.3.2 Results.", "content": "We perform two ablation experiments to show the importance of actor injection and of the descriptor-conditioned actor. AI proves significantly beneficial in terms of QD score, on all tasks (p < 0.05) except Ant Uni where they perform comparably. Having a descriptor-conditioned actor $\u03c0_\u03c6( . | d)$ rather than a normal actor $\u03c0_\u03c6(.)$ proves significantly beneficial in terms of QD score, on all tasks (p < 10\u22124), demonstrating that the descriptor-conditioned actor enables archive distillation while being beneficial for the critic's training. DCG-MAP-ELITES GECCO achieves equal or higher QD score than the AI ablation, showing the importance of on-policy samples. Overall, DCG-MAP-ELITES-AI shows competitive performance on all metrics and tasks compared to the ablations, hence proving the importance of the different enhancements compared to PGA-MAP-ELITES."}, {"title": "5.4 Reproducibility", "content": null}, {"title": "5.4.1 Reproducibility Metrics.", "content": "We also consider three metrics to evaluate the reproducibility of the final archives for all algorithms and of the descriptor-conditioned actor for DCG-MAP-ELITES-AI, at the end of training. QD algorithms based on MAP-ELITES output a population of solutions that we evaluate with the QD score, coverage and max fitness, see Section 5.2.2. However, these metrics can be misleading because in stochastic environments, a solution might give different fitnesses and descriptors when evaluated multiple times. Consequently, the QD score, coverage and max fitness can be overestimated, an effect that is well-known and that has been studied in the past [14]. An archive of solutions is considered reproducible, if the QD score, coverage and max fitness does not change substantially after multiple reevaluation of the individuals. Thus, to assess the reproducbility of the archives, we consider the expected QD score, the expected distance to descriptor and the expected max fitness. To calculate those metrics, we reevaluate each solution in the archive 512 times, to approximate its expected fitness and expected distance to descriptor. The expected distance to descriptor of a solution is simply the expected euclidean distance between the descriptor"}, {"title": "5.4.2 Results.", "content": "In Figure 5, we provide the expected QD score, expected distance to descriptor and expected max fitness of the final archive and the descriptor-conditioned policy, see Section 5.4.1. First, we can see that DCG-MAP-ELITES-AI's final archive achieves equal or higher expected QD score than all baselines on all tasks. The descriptor-conditioned actor performs similarly to DCG-MAP-ELITES-AI on most environments, but performs significantly worse on Ant Uni. This shows that, in most cases, the descriptor-conditioned actor is able to restore the quality of the archive although having compressed the information in a single network. Second, DCG-MAP-ELITES-AI obtains better expected distance to descriptor (lower is better) than all baselines except MAP-ELITES-ES on all tasks. However, MAP-ELITES-ES obtains worse QD score and most importantly, worst coverage, making it easier for MAP-ELITES-ES to achieve a low expected distance to descriptor. DCG-MAP-ELITES-AI descriptor-conditioned actor obtains similar expected distance to descriptor on omnidirectional. However, it performs consistently worse on unidirectional tasks. This shows that in some cases, while compressing the quality of the archive in a single network, the descriptor-conditioned actor can also exhibit the same diversity as the population. Those two combined observations show that the final archive and descriptor-conditioned policy have similar properties on omnidirectional tasks. Overall, those results show that our single descriptor-conditioned policy"}, {"title": "5.5 Variation Operators Evaluation", "content": null}, {"title": "5.5.1 Variation Operator Metrics.", "content": "DCG-MAP-ELITES-AI and PGA-MAP-ELITES make use of a GA variation operator and of a PG variation operator. The GA variation operator is strictly the same in both algorithms. However, DCG-MAP-ELITES-AI enhances PGA-MAP-ELITES'S PG variation opera-tor with a descriptor-conditioned critic, as explained in Section 4.4. To evaluate the performance of each variation operator, we introduce a metric defined as the accumulated number of offsprings added to the archive coming from each variation operator throughout training, that we call number of elites. By tracking the number of elites generated by each variation operator over the course of training, we can analyze the interaction and dynamics between the different variation operators and actor injection, providing insights into the relative contributions of the different components."}, {"title": "5.5.2 Results.", "content": "On the top row of Figure 6, we can see the accumulated number of elites for the GA variation operator for DCG-MAP-ELITES-AI, PGA-MAP-ELITES and ablation AI throughout training. In all three cases, the number of offsprings suggested for addition in the archive is 128. On the bottom row of Figure 6, we can see the accumulated number of elites for the PG variation operator. In all three cases, the number of offsprings suggested for addition in the archive is 128, but for DCG-MAP-ELITES-AI, the PG variation is divided into 64 coming from the actor injection (Section 4.5) and 64 coming from the PG update using the descriptor-conditioned critic (Section 4.4). First, we can see that the ablation of the actor injection generates a larger number of elites than PGA-MAP-ELITES, demonstrating that the descriptor-conditioned critic generates higher-performing and more diverse solution than the traditional critic used in PGA-MAP-ELITES. Furthermore, we can see that DCG-MAP-ELITES-AI with actor injection mechanism generates even more elites than the descriptor-conditioned PG variation operator alone. Interestingly, we can see that the number of elites generated by DCG-MAP-ELITES-AI is higher than PGA-MAP-ELITES, even though the GA variation operators are exactly the same. This demonstrates that the solutions found by the descriptor-conditioned PG variation operator are better stepping stones."}, {"title": "6 CONCLUSION", "content": "In this work, we introduce DCG-MAP-ELITES-AI and demonstrate the benefits of having descriptor-conditioned gradients to evolve populations of large neural networks. We concurrently train a"}, {"title": "B ALGORITHMS", "content": null}, {"title": "B.1 DCG-MAP-ELITES-AI", "content": null}, {"title": "B.2 PGA-MAP-ELITES", "content": null}, {"title": "B.3 QD-PG", "content": null}, {"title": "B.4 MAP-ELITES", "content": null}, {"title": "B.5 MAP-ELITES-ES", "content": null}, {"title": "C HYPERPARAMETERS", "content": null}, {"title": "C.1 DCG-MAP-ELITES-Al", "content": null}, {"title": "C.2 PGA-MAP-ELITES", "content": null}, {"title": "C.3 QD-PG", "content": null}, {"title": "C.4 MAP-ELITES", "content": null}, {"title": "C.5 MAP-ELITES-ES", "content": null}]}