{"title": "Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a Low-Resource Language", "authors": ["Manjil Karki", "Pratik Shakya", "Sandesh Acharya", "Ravi Pandit", "Dinesh Gothe"], "abstract": "Voice cloning is a prominent feature in personalized speech interfaces. A neural vocal cloning system can mimic someone's voice using just a few audio samples. Both speaker encoding and speaker adaptation are topics of research in the field of voice cloning. Speaker adaptation relies on fine-tuning a multi-speaker generative model, which involves training a separate model to infer a new speaker embedding used for speaker encoding. Both methods can achieve excellent perfor-mance, even with a small number of cloning audios, in terms of the speech's naturalness and similarity to the original speaker. Speaker encoding approaches are more appropriate for low-resource deployment since they require significantly less memory and have a faster cloning time than speaker adaption, which can offer slightly greater naturalness and similarity. The main goal is to create a vocal cloning system that produces audio output with a Nepali accent or that sounds like Nepali. For the further advancement of TTS, the idea of transfer learning was effectively used to address several issues that were encountered in the development of this system, including the poor audio quality and the lack of available data.", "sections": [{"title": "I. INTRODUCTION", "content": "Voice cloning involves creating artificial replicas of human voices using advanced AI software, sometimes indistinguish-able from real voices. It's often associated with terms like deepfake voice, speech synthesis, and synthetic voice. Unlike text-to-speech (TTS) systems, which transform written text into speech using predefined data, voice cloning is a cus-tomized process. It extracts and applies specific voice charac-teristics to different speech patterns. Historically, TTS had two approaches: Concatenative TTS, which used recorded audio but lacked emotion, and Parametric TTS, which used statistical models but produced less human-like results. Today, AI and Deep Learning are improving synthetic speech quality, making TTS applications widespread, from phone-based systems to virtual assistants like Siri and Alexa."}, {"title": "II. METHODOLOGY", "content": "The proposed method of voice cloning consists of three major models with initial preprocessing of data files [1].\nA <voice, text>pair multispeaker dataset is prepared con-sisting of 546 individuals cooperating with both male and female speakers. The dataset consists of female speakers majority. The dataset was taken from open source platform OpenSLR [2] [3]. The dataset consists of 150,000 plus audio files totalling up to 168 hours of audio with their respective transcript.\nData pre-processing is a crucial step as the 3 models require a separate pre-processing. Initially for encoder-preprocessing the audio files from the dataset are fetched and processed so that an encoded form of mel-spectrogram is gained. Then for the synthesizer preprocessing, audio files along with transcript and utterance are fetched that are processed representing a pre-processed data collection including audio-spectrogram, mel-spectrogram, speaker-embedding and also a text file including the details. Finally, for the vocoder-preprocessing the mel-spectrograms were processed to ground-truth aligned (GTA) spectrum dataset.\nFig 3 shows the overview of the pre-processing step. Text data will be well normalized and then audio files will be converted into both mel-scale spectrograms and Speaker Embeddings.\nThe encoder is responsible for extracting meaningful rep-resentations (embeddings) from the input voice samples. It captures the characteristics and features of the speaker's voice and encodes them into a fixed-length vector. The architecture of the encoder can vary, but one popular choice is the Mel-spectrogram-based encoder [4]. The Mel-spectrogram-based encoder typically consists of several convolutional layers followed by batch normalization and non-linear activation functions, such as ReLU. It takes the raw audio waveform as input and converts it into a Mel-spectrogram, which is a visual representation of the audio's frequency content over time. The Mel-spectrogram is then passed through the convolutional layers to extract features, which are finally transformed into an embedding vector using pooling or recurrent neural networks (RNNs).\nTacotron2 [5] is a popular architecture for text-to-speech synthesis. It consists of a text encoder that converts input text into a fixed-length embedding and a decoder that generates mel-spectrograms representing the desired speech. The text encoder uses convolutional layers and bidirectional recurrent neural networks (RNNs) to capture text features. The decoder utilizes autoregressive models with stacked LSTM [9] or GRU layers, along with attention mechanisms, to generate mel-spectrograms. During training, Tacotron2 minimizes a loss function to make the generated spectrograms match the target speech representation. It has been successful in producing high-quality synthetic speech.\nThe Tacotron text-to-speech (TTS) system employs a deep neural network architecture comprising mul-tiple components that work collaboratively to generate speech from text.\nThe key components of Tacotron2's architecture include:\nThe initial component is the text encoder, responsible for converting a sequence of characters or phonemes into hidden representations. Typically, con-volutional layers are employed to extract text features, followed by a bidirectional recurrent neural network (RNN) that encodes the input sequence into a fixed-length vector. These hidden representations capture the semantic meaning of the text and serve as input for the decoder component.\nThe attention mechanism is a vital component in Tacotron [6]. It learns to align the output spectrograms with the input text sequence. By computing attention weights, it determines the importance of each input sequence element at each decoder time step. These weights are used to calculate a weighted sum of the encoder's hidden representations, which becomes input for the decoder.\nThe decoder makes a sequence of speech spec-trograms, which represent the acoustic features of the speech waveform. It typically consists of a recurrent neural network with an attention mechanism. The de-coder generates one spectrogram frame at a time, using the previous frame along with current decoder's hidden state. The attention weights and the previous decoder output are utilized as inputs for generating the subsequent\nspectrogram frame. The decoder's output undergoes post-processing through a network to obtain the final spectro-gram.\nThe decoder's output is a se-quence of speech spectrograms that capture the acoustic features of the speech waveform. These spectrograms are transformed into a speech waveform using techniques such as Griffin-Lim phase reconstruction. This involves estimating the phase information of the waveform from the magnitude spectrogram produced by the decoder. The resulting waveform then passes through a post-processing network, which applies additional signal-processing steps to enhance the quality of the synthesized speech.\nAlignment plots are visualizations used in speech processing and natural language processing. They show the alignment between two sequences, such as audio and its transcription. They help evaluate the performance of automatic speech recognition or machine translation systems [7]. The plots consist of parallel sequences displayed on the x-axis and y-axis, representing the original audio and predicted transcription/translation. Each cell represents the alignment between specific segments. Alignment plots can be created using heat maps, scatterplots, or line plots. They allow for visual comparison, error identification, and system improvement. Alignment plots are valuable for understanding system performance in speech and language processing.\nA Mel spectrogram [8] is a visual representation of the power spectrum of an audio signal, where the frequencies are weighted according to the Mel scale. It is widely used in speech and audio processing for tasks like speech recognition and music analysis. The process involves dividing the audio signal into short segments, computing the power spectrum using a Fourier transform, and grouping frequencies into triangular bands using a filter bank. The power within each band is summed up to create Mel spectrogram coefficients. Mel spectrograms provide a powerful way to visualize and analyze the spectral content of audio signals over time. They are also used as input features for machine learning models in various audio processing tasks. Mel spectrograms emphasize important frequency bands while reducing less significant ones, helping to preserve relevant information while reducing dimensionality.\nWaveNet is a popular vocoder architecture used in voice cloning [10]. It employs dilated convolutional neural networks (CNNs) to model the conditional probability distribution of the audio waveform. WaveNet takes mel-spectrograms as input and generates the corresponding high-quality audio waveform sample by sample. It captures long-range dependencies in the audio using stacked dilated convolutional layers. WaveNet has been widely adopted for its ability to generate realistic and natural-sounding speech.\nTransfer learning aims to transfer knowledge from the source domain to the target domain to solve the problem caused by insufficient training data and improve the gener-alization ability of the model [18]. During the development of deep learning models for a specific architecture, initial attempts involved training the models from scratch. However, these models yielded poor results, primarily due to limited and low-quality data. Moreover, the training process itself was complex and time-consuming, especially on advanced systems. To address these issues, a solution was proposed and implemented: transfer learning for all three models. However, finding a language model that closely resembled the Nepali language and the voice of Nepali speakers proved impossi-ble. As an alternative, a multi-language model was chosen for transfer learning. Surprisingly, the results obtained were significantly better than those of the previously trained mod-els, although they still required some fine-tuning. Eventually, the necessary adjustments were made, resulting in somewhat improved outcomes."}, {"title": "III. RESULT", "content": "The results that were observed during training are mostly the charts and plots which show the development of the model. Mostly the loss curve and accuracy curves are key parameters to watch for during training but this model is an audio-based model which is why subjective observations are given more emphasis as accuracy and losses could not cover all areas of interest.\nFor encoder training some of the key parameters that were observed are listed as follows:\nThe encoder training loss is rep-resented by the general end-to-end loss that states the accuracy of cluster formation in UMAP. The GE2E loss simulates this process to optimize the model. At training time, the model computes the embeddings $e_{ij}(1 \u2264 I \u2264 N, 1 \u2264 j \u2264 M)$ of $M$ utterances of fixed duration from $N$ speakers. A speaker embedding $c_i$ is derived for each speaker:\n$c_i = \\frac{1}{M}\\sum_{j=1}^{M} e_{ij}$\nEqual Error Rate (EER) is a performance metric used in biometric verification systems, which measures the similarity between two biometric samples. The EER is the point at which the False Acceptance Rate (FAR) equals the False Rejection Rate (FRR). The FAR is the proportion of impostor samples that are incorrectly accepted, while the FRR is the proportion of genuine samples that are incorrectly rejected. EER also represents the accuracy of the model in terms of similarity. The lower the value of EER\nFinal Loss: The addition of M1 loss (mean absolute error) and M2 loss (mean squared error) is a method used in some regression problems to combine the strengths of both loss functions.M1 loss measures the average absolute difference between the predicted and actual values of a con-tinuous variable, while M2 loss measures the average squared difference between the predicted and actual values. M1 loss is more robust to outliers, while M2 loss is more sensitive to the magnitude of errors. By combining these two loss functions, the resulting loss function can capture both the magnitude and direction of errors in the model's predictions. The addition of M1 and M2 loss can help the model balance the trade-off between accuracy and robustness and can improve its ability to generalize to new data.\n$Loss = MAE + MSE$\nM1 Loss: M1 loss, also known as mean absolute error (MAE) loss, is a commonly used loss function in regression problems. It measures the average absolute difference between the predicted and actual values. The formula for MAE is:\n$MAE = 1/n *  \\sum_{i=1} \u2211(y_i \u2013 \\hat{y_i})$\nWhere $y_i$ is the actual value, $\\hat{y_i}$ is the predicted value, and n is the total number of samples.\nM2 Loss: M2 loss, also known as mean squared error (MSE) loss, is another commonly used loss function in re-gression problems. It measures the average of the squared differences between the predicted and actual values. The formula for MSE is:\n$MSE = 1/n *  \\sum_{i=1} (y_i \u2013 \\hat{y_i})^2$\nWhere $y_i$ is the actual value, $\\hat{y_i}$ is the predicted value, and n is the total number of sample"}, {"title": "IV. CONCLUSION", "content": "In conclusion, the system for Nepali Voice Cloning emphasizes cloning voices having Nepali accents, dialects etc taking in Devanagari script as input. For voice cloning the system has three core components namely: encoder, synthesizer and vocoder. The speaker Encoder consists of CNN followed by RNN that changes the User voice to speaker encoding, the encodings are used to distinguish a user from another it is a kind of fingerprint. The speaker embedding along with the text which is in Devanagari script is given in as input to the synthesizer. The synthesizer is further divided into three components (encoder, attention and decoder). It is a modification of the tacotron model which is capable of taking in speaker embeddings as input. The output from the synthesizer is mel-spectogram which is sent into the vocoder model to generate audio from mel-spectogram. Finally, all these components are integrated into a web application where the integrated system can run at its full potential.\nMathematically, the system performs fair as it has an MOS score of 3.9 in terms of naturalness and an MOS score of 3.2 in terms of similarity on a scale of 1 to 5. Also, the mean PESQ score for validation data was 2.3 and for the test, the dataset was found to be 1.8. The scale of PSEQ ranges from -0.5 to 4.5."}]}