{"title": "EmoReg: Directional Latent Vector Modeling for Emotional Intensity Regularization in Diffusion-based Voice Conversion", "authors": ["Ashishkumar Gudmalwar", "Ishan D. Biyani", "Nirmesh Shah", "Pankaj Wasnik", "Rajiv Ratn Shah"], "abstract": "The Emotional Voice Conversion (EVC) aims to convert the discrete emotional state from the source emotion to the target for a given speech utterance while preserving linguistic content. In this paper, we propose regularizing emotion intensity in the diffusion-based EVC framework to generate precise speech of the target emotion. Traditional approaches control the intensity of an emotional state in the utterance via emotion class probabilities or intensity labels that often lead to inept style manipulations and degradations in quality. On the contrary, we aim to regulate emotion intensity using self-supervised learning-based feature representations and unsupervised directional latent vector modeling (DVM) in the emotional embedding space within a diffusion-based framework. These emotion embeddings can be modified based on the given target emotion intensity and the corresponding direction vector. Furthermore, the updated embeddings can be fused in the reverse diffusion process to generate the speech with the desired emotion and intensity. In summary, this paper aims to achieve high-quality emotional intensity regularization in the diffusion-based EVC framework, which is the first of its kind work. The effectiveness of the proposed method has been shown across state-of-the-art (SOTA) baselines in terms of subjective and objective evaluations for the English and Hindi languages\u00b9.", "sections": [{"title": "Introduction", "content": "Despite significant progress in the field of Generative AI, speech synthesis models still encounter several challenges when it comes to the AI-based dubbing of entertainment content such as movies and serials. AI-based dubbing involves replicating input speech emotion and controlling its intensity depending on the context and emotion of the scene. Most of today's text-to-speech (TTS) systems can produce high-quality, high-fidelity, natural speech output, but they still lack expressiveness and fine control over emotional states. Hence, professional voice-over/dubbing artists are still preferred in the dubbing industry to tackle the complex demands of generating emotionally engaging speech.\nThis poses challenges in terms of dubbing at a large scale, turnaround time, and operational costs.\nOne of the major roadblocks in building emotional speech synthesis is the unavailability of a large emotional speech database with diverse emotional expressions and a wide range of emotion intensities. Even annotating accurate emotional states in the speech requires expert knowledge and is costly and labor-intensive as labeling each speech file with the correct emotion and intensity involves detailed subjective assessments. Hence, emotional intensity control or regularization is still an under-explored open research problem.\nTo this end, we primarily focus on the emotional voice conversion (EVC) task, a sub-topic of emotional speech synthesis for emotional intensity regularization. In particular, we employ a self-supervised learning (SSL)-based framework to tackle the issues related to the unavailability of a large annotated emotional speech database.\nThe EVC aims to alter the discrete emotional state of a speech utterance while preserving linguistic content and the speaker's identity. Furthermore, emotional intensity regularization involves fine control over intensity associated with the target emotional state. For example, a neutral utterance can be converted into a mild, moderate, or severe level of anger emotion, as shown in Figure 1.\nCurrent EVC methods usually tackle this scenario with discrete or continuous emotion representations. Discrete emotion representations contain categorical labels such as happy, angry, sad, etc. On the other hand, continuous emotion representations are obtained from the circumplex models and contain continuous-independent dimensions, such as arousal and valance. Achieving emotion intensity control in continuous emotion representation is relatively easier than discrete emotion representations. However, obtaining such continuous emotion representations-based annotated data is challenging, as discussed earlier. Hence, this paper aims to achieve emotion intensity regularization for the discrete emotion representations-based EVC methods.\nIn summary, we propose a method to achieve emotion intensity regularization by means of self-supervised learning and direction latent vector modeling using a diffusion-based EVC. Our key contributions can be summarized as follows:\n\u2022 To the best of the authors' knowledge this is the first attempt that achieves a high-quality emotional intensity regularization in the diffusion-based EVC framework.\n\u2022 We propose a novel direction latent vector modeling-based approach for obtaining fine control over intensity while transitioning across different emotional states.\n\u2022 The proposed EmoReg utilizes the SSL-based audio feature representations, which are obtained after finetuning the SSL-based framework for a downstream task related to emotions classification.\n\u2022 Effectiveness of the proposed EmoReg approach has been shown against SOTA baselines with relevant subjective and objective evaluation techniques across two languages, namely English and Hindi."}, {"title": "Related Work", "content": "EVC methods are usually categorized into parallel and non-parallel approaches depending on the nature of the training data. Parallel means each speaker has spoken the same utterance in different emotional states and non-parallel means recorded sentences are different depending on the emotional states. Since emotion is also dependent on the content that is being spoken, hence, researchers have primarily focused on non-parallel EVC approaches. Traditionally, researchers have explored various generative models for the discrete emotion representation-based EVC tasks, namely, GMM, HMM, DNN, Sequence-to-sequence models, Variational Auto Encoder (VAE), Generative Adversarial Networks (GAN) and its variants, Diffusion Model-based approaches, etc. However, none of these approaches tackles scenarios of emotion intensity regularization.\nBroadly, emotional intensity regularization is achieved from the emotional labels and the reference speech utterance. The emotion label-based approach is utilized to manipulate auxiliary features such as attention weights, saliency maps, ranking function, or signal processing attributes to achieve emotion intensity regularization. Among these, one of the most prominent techniques is relative attribute, which describes the distinctions between binary classes using a learned ranking function. However, these methods fail to capture high-level complex abstract representations, which results in artifacts in the converted output and leads to significant degradation in the quality. Also, emotional manipulations often involve inter-dependencies across different types of features, hence manipulating any single feature in isolation fails in achieving emotional intensity regularization. On the other hand, some simple approaches manipulate learned emotion representations via scaling or interpolations to achieve emotion regularization. However, these approaches do not work well since emotional embedding space does not align well with the assumption of linear interpolation. Furthermore, high-dimensional emotional embedding space is often sparsely populated between two styles. Thus, such approaches result in inept style manipulations.\nAnother emotional intensity control-based strategy leverages the extended emotional dimensions, namely, Arousal, Valence, and Dominance (AVD). AVD-based emotional dimensions provide continuous and fine-grained emotion representations, which can be utilized to alter emotional states in a continuous manner more appropriately than discrete human emotions-based representations. However, obtaining such annotations is difficult due to inherent subjectivity-related issues associated with different annotators and the high costs associated with preparing such data. Hence, emotional intensity regularization is an under-explored research area.\nIn summary, all previous discrete and continuous emotional intensity control methods produce low-quality noisy converted output. To tackle quality-related issues recently, diffusion-based models have seen great success as a potent and versatile generative AI technology in computer vision, audio, and reinforcement learning. Diffusion models serve as samplers in these applications, producing new samples while actively guiding them toward task-desired features. They also offer versatile high-dimensional data modeling. The diffusion model-based approach has also obtained remarkable success in the voice conversion and EVC task. However, it has not been explored for achieving emotion intensity control-related tasks. Hence, we exploit the diffusion-based EVC method with the proposed DVM-based strategy for the emotion intensity regularization task. The next section presents details about the proposed diffusion-based EmoReg architecture."}, {"title": "Proposed Methodology", "content": "Problem Formulation\nGiven input speech xo along with reference emotion speech Xr and intensity value i, the goal of EVC is to generate emotional intensity regularized converted speech y such that Y = G(Xo, es, er, i, \u03b8), where X and Y denotes Mel-spectrogram features corresponding to the input speech xo and the converted emotional speech y, respectively. In addition, es, er denotes emotional feature representation for source and reference speech, respectively. \u03b8 is model parameters corresponding to the considered conditional diffusion probabilistic model-based EVC architecture. Details of the proposed architecture is as follows.\nProposed EmoReg Architecture\nThe proposed EmoReg architecture uses diffusion-based model to achieve EVC while controlling the emotion intensity i. It comprises a diffusion-based decoder and a set of encoders illustrated in Figure 3. The diffusion decoder is responsible for emotion-controllable speech synthesis, while the encoders individually encode the emotion and speech representation that require disentanglement. The decoder is conditioned on the proposed Direction Vector Modeling (DVM) based features eir, which facilitate the transition between different emotions. At the output of the diffusion decoder, we obtain the Mel-spectrogram Y, which can be converted to the output speech signal y = H(Y), with H(.) representing the HiFiGAN vocoder. Subsequent sections delve deeper into the architectural components of the proposed method.\nEncoders\nOur EmoReg model consists of two encoders. The first encoder is used to encode the lexical content, while the second one is used to obtain emotion representations.\nPhoneme Encoder: In this part, we encode the lexical content at the phoneme level by using speaker- and emotion-independent average phoneme-level Mel characteristics. To achieve this, we utilize the transformer-based encoder as described in , which has previously been used in speech conversion applications.\nX = \u03c6(Xo)  (1)\nHere, X is average Mel-spectrogram of source audio, Xo is source speech Mel-spectrogram and \u03c6(\u00b7) represents the pre-trained average phoneme encoder.\nSSL Emotion Embedding Network: Due to the unavailability of a large annotated emotional speech database, we plan to utilize SSL-based representations. We fine-tune the pre-trained SSL emotion2vec embedding network E(.) using English and Hindi emotional speech databases to learn emotional embedding representations.\nWe used a pretrained emotion2vec model to generate 768-dimensional embeddings, which we then fine-tuned for classifying target emotions. Similar to the diffusion-based text generation models , we also find that in the high dimensional embedding space, the insufficient noise results in a simple denoising task, which leads to the detoriation of the model. In the process, we first passed these embeddings through a fully connected layer to reduce them to 256 dimensions. We then passed these reduced embeddings through an output layer to classify them into the target emotions (neutral, happy, sad and angry). The training was conducted in a supervised manner. After training, we used the network with the newly added fully connected layer to obtain 256-dimensional emotion embeddings. These emotional embeddings are conditioned in the decoder of the diffusion model in order to achieve target emotional state in the converted output. Additionally, these emotional embeddings are manipulated via proposed directional vector modelling to obtain a fine control over intensities associated with the target emotional states.\nDirection Vector Modeling\nDuring training phase, the 256-dimension emotional embeddings from SSL Emotion Embedding Network are used as an input to the DVM module as shown in Figure 2. We modeled the emotion embedding space using a 64-component Gaussian Mixture Model (GMM) to derive the local mean vector for each emotion, i.e., Angry, Happy, Neutral, and Sad. After extracting the GMM features for each emotion, pairwise subtraction is performed between the embeddings of Angry, Happy, and Sad with those of Neutral to derive the emotional direction vector matrix for all possible transitions from local mean of one emotional state to another. Subsequently, Principal Component Analysis (PCA) is applied to the emotional direction matrix corresponding to each emotion, reducing it to 128 components since not all 256 components are reflecting changes related to emotional states, they may also be affected due to content variability. Ablation analysis for selecting number of principle components is presented in Table 5.\nDuring the inference phase, given a source sample in the Neutral emotion and a reference sample in another emotion"}, {"title": "Diffusion-based Decoder", "content": "following the SDE formalism as described in . Consider the continuous diffusion time-step variable t, which characterizes the progression of the diffusion process. The forward SDE for 0 < t < 1 is given by:\ndXt = 1/2*Bt(X-X+)dt + \\sqrt{Bt}dw (2)\nwhere Xt represents the current process state with initial condition Xo and w is the typical Wiener process . The noise schedule is represented by a non-negative function Bt. The mean evolution here represents an interpolation that ends roughly at the distribution of average voice phoneme characteristics X at t = 1 and begins at the distribution of source Xo at t = 0. There is an associated reverse SDE with the forward SDE as given in Eq. 2.\ndX = [ - 1/2*\u03b2t(X-X) + \u03b2texlogpt(Xt|X)]dt + \u03b2t2dw (3)\nwhere dw is the Wiener process which at each diffusion time-step moving backward. Here noise \u03b2t = \u03b2o + t(\u03b21 - \u03b2o) follows linear schedule for both \u03b2o and \u03b21 such that e- \u222b01(\u03b2sds) tends to zero . Additionally, the reverse SDE has the same trajectory as the forward SDE; that is, it begins roughly with the distribution of average-voice and ends at t = 0 with the distribution of source-targets. Here, we employ the score model s\u03b8 as the U-Net architecture from . The score model takes Xt, t and regulated emotion intensity features eir = f(es, er, i), where f(\u00b7) represents function for the DVM approach. Hence, given the emotion intensity regulated embedding eir, we can then utilize the reverse SDE with the learned s\u03b8(Xt, X, t, eir) to estimate the reconstructed Xo from the average voice X.\nWhile doing inference, a source sample in a neutral emotion is converted to a specified target emotion by utilizing a reference sample in the corresponding target emotion. Both the source emotional embedding and the reference emotional embedding are given as input to the DVM, which models the direction vector and produces an emotion intensity regularized embedding. This embedding and the learned score function are then used to transform the source sample into the reference sample emotion."}, {"title": "Experimental Analysis", "content": "Dataset\nOur performance evaluation primarily uses the Emotional Speech Database (ESD). The ESD dataset contains 29 hours of 350 parallel recorded utterances in five distinct emotions, i.e., Neutral, Happy, Angry, Sad, and Surprise, from ten English speakers, respectively. In addition, we utilized 36 hours of internally developed non-parallel Hindi emotional speech database recorded by 9 native Hindi speakers for evaluating performance across languages because emotional speech database is not publicly available for other Indian languages. To record this database, we have collected Hindi texts from stories and categorized these texts into four emotions, namely, Neutral, Happy, Angry, and Sad. Each speaker has recorded one hour of data on each emotion. We used a 90:10 train-test split for both ESD-English (excluding Chinese) and Hindi data. We presented our analysis for three emotional state conversion scenarios along with intensity control: neutral-to-happy, neutral-to-sad, and neutral-to-angry. Note, we use nonparallel reference emotional samples from different speaker for evaluation.\nImplementation Details\nInitially, emotion-independent speech representation is achieved by using average phoneme-level Mel-spectrogram characteristics. The process begins with aligning speech frames with phonemes in the ESD dataset using the Montreal Forced Aligner (MFA). Once aligned, the Mel features of each phoneme are aggregated across the entire dataset to obtain their average Mel-spectrogram features. The Phoneme Encoder is then trained to minimize the mean square error between the output Mel-spectrograms and the ground truth average voice Mel-spectrograms. These ground truth spectrograms are derived by replacing each phoneme Mel-spectrogram feature in the input with its corresponding average feature calculated earlier. The encoder is trained for 300 epochs with a batch size of 32 and a base learning rate of 5e-5 using the Adam optimizer. Our decoder is built on top of DiffVC architecture . The number of parameters in the Encoder is 8.5mn and the decoder is 118mn. The decoder is trained for 126 epochs with a batch size of 32 using the Adam optimizer with a base learning rate of 1e-4. The noise schedule parameters \u03b2o and \u03b21 are set to 0.05 and 20.0, respectively. The model mainly operates on Mel-spectrograms with 80 Mel features and a sampling rate of 22.05 kHz. The entire training process takes approximately 4 hours for the encoder and 12 hours for the decoder, both trained on an NVIDIA L40S GPU.\nBaseline Methods\n\u2022 EmoVox : This approach explicitly controls the emotion intensity in the EVC task by using characteristic features to express fine-grained emotion intensity and further learn the actual emotion encoder from an emotion-labeled database.\n\u2022 Mixed Emotion : It is a two stages seq-to-seq training method for emotional voice conversion. It uses a multi-speaker TTS corpus to perform style initialization in stage 1 to separate language content from speaking style. Stage 2 uses a limited amount of emotional speech data for emotion training to learn how to disentangle the speech's linguistic and emotional style information.\n\u2022 Other baselines: We have also considered CycleGAN-EVC , StarGAN-EVC , StyleVC , DISSC and Seq2Seq-EVC to compare performance of proposed EmoReg approach w.r.t emotion voice conversion. However, we have excluded them from intensity control related evaluations as these approaches do not support emotion intensity regularization.\n\u2022 Ablation EmoReg w/o DVM: Here, we utilized the proposed EmoReg architecture without directional vector modeling. In particular, we applied to scale in the direction vector learned between the global mean of target emotion embedding reference and source emotion embedding along with intensity-related scaling function, i.e., via traditional interpolation-based strategy."}, {"title": "Experimental Results", "content": "Objective Evaluation Since our key goal is obtaining control over the emotion intensity, we primarily considered the emotion similarity score as an objective evaluation metric. We use it to measure the effectiveness of the proposed approach for the emotion conversion and intensity regularization tasks. It is obtained by computing the cosine distance between the generated and ground truth speech emotion embeddings from the pre-trained emotion classifiers. Furthermore, Word Error Rate (WER) and Character Error Rate (CER) are considered to measure the intelligibility of the output while performing emotion intensity regularization on a continuous scale. Here, we used Whisper-small model . The emotion similarity score for emotion voice conversion is calculated for Neutral-to-Angry, Neutral-to-Sad, and Neutral-to-Happy emotion conversion scenarios for the proposed approach and baseline methods, as shown in Table 1. These are denoted by the abbreviations: Neu-Ang, Neu-Sad, and Neu-Hap, respectively. Similarly, we achieved improvements of 2 to 11% with the proposed DVM-based approach in AutoPCP-based objective evaluations for emotional similarity and scale-wise subjective evaluations for emotional controllability. Detail results are omitted due to space constraints."}, {"title": "Performance Across Languages", "content": "The effectiveness of the proposed approach is evaluated across different databases using similar objective and subjective assessments for both English and Hindi languages. Table 3 shows the emotion similarity scores for Neutral-to-Angry, Neutral-to-Sad, and Neutral-to-Happy emotion voice conversion for ablation and the proposed EmoReg approach for both languages. It is evident from Table 3 that the proposed approach also performs well for the Hindi language. Additionally, the intelligibility of converted speech in both languages is evaluated using WER and CER along with MOS score and is presented in Table 4."}, {"title": "Visual Analysis", "content": "Additionally, we present the variations in prosody patterns, such as pitch and Mel-spectrogram, for various emotions, Neutral-to-Angry, Neutra-to-Sad, and Neutral-to-Happy in Figure 6. For Neutral-to-Angry emotion, we can see that with an increase in emotion intensity scale, pitch frequency range and dynamics also increase which is indicated by a white box. As harmonics are an integer multiple of the fundamental frequency, we can also see that for scales 0.6 and 1.0, the harmonics are wider than those on scale 0. It indicates that the proposed approach can control the intensity of emotion. Similar observations are seen in the case of Neutral-to-Happy emotion, where the pitch range increases with an increase in intensity scale. For the Neutral-to-Sad condition, the harmonics in the spectrogram are getting narrower with an increase in intensity scale because sadness is a low pitch-flattish emotion where the pitch range is usually lower as compared to high pitch with high variance emotions like anger and happiness."}, {"title": "Conclusions", "content": "In this paper, we introduced the EmoReg model for emotion voice conversion with emotion intensity regularization. By leveraging SSL-based emotion embeddings, we achieved effective emotion representation from speech. We proposed a direction vector modeling (DVM) to transition between emotional states while controlling emotion intensity. We evaluated our approach against the SOTA architectures for both English and Hindi languages. Our approach demonstrated significant improvements of 13.56% over EmoVox and 43.13% over Mixed Emotion in speech quality. Additionally, the proposed EmoReg model outperformed existing methods in various objective evaluations. Moving forward, we aim to extend this work to enhance emotional intensity regularization in challenging environments, such as those with background music or noise."}, {"title": "Appendix", "content": "Additional Subjective and Objective Evaluations\nDue to space constraints in the original manuscript, we are adding additional subjective and objective evaluations in this Appendix. In particular, we conducted a subjective evaluation for the proposed model at different intensity scales as shown in Table 6. In addition, We have added AutoPCP evaluation scores as shown in Table 7. AutoPCP is an utterance-level estimator used to measure the similarity between two speech samples in terms of prosody. A higher AutoPCP score indicates improved similarity in prosody. We found that the proposed approach achieves 2%-11% relative improvements in the AutoPCP scores compared to the SOTA. Moreover, in manuscript, we have only shown results from neutral emotions to non-neutral emotions. However, we have observed that our method works equally well in non-neutral to neutral emotion conversion scenarios as well."}]}