{"title": "Learning to Generalize Unseen Domains via Multi-Source Meta Learning for Text Classification", "authors": ["Yuxuan Hu", "Chenwei Zhang", "Min Yang", "Xiaodan Liang", "Chengming Li", "Xiping Hu"], "abstract": "With the rapid development of deep learning methods, there have been many breakthroughs in the field of text classification. Models developed for this task have achievedhigh accuracy. However, most of these models are trained using labeled data from seen domains. It is difficult for these models to maintain high accuracy in a new challenging unseen domain, which is directly related to the generalization of the model. In this paper, we study the multi-source Domain Generalization for text classification and propose a framework to use multiple seen domains to train a model that can achieve high accuracy in an unseen domain. Specifically, we propose a multi-source meta-learning Domain Generalization framework to simulate the process of model generalization to an unseen domain, so as to extract sufficient domain-related features. We introduce a memory mechanism to store domain-specific features, which coordinate with the meta-learning framework. Besides, we adopt a novel \"jury\" mechanism that enables the model to learn sufficient domain-invariant features. Experiments demonstrate that our meta-learning framework can effectively enhance the ability of the model to generalize to an unseen domain and can outperform the state-of-the-art methods on multi-source text classification datasets.", "sections": [{"title": "1 Introduction", "content": "The text classification of social media is crucial not only for conducting surveys among traditional consumers and companies to gather opinions on respective products or services, but also plays a significant role in national security and public opinion analysis [17,18]. While recent deep learning models of text classi-fication [1-3] demonstrate efficacy in a seen domain (i.e., a domain with labeled data), most of them do not perform well in an unseen domain (i.e., a domain only with unlabeled data). However, in real life, text classification is inevitably used in unseen domains. Text classification can be considered as a domain-dependent task, because a sentence may convey different meanings in various domains. For instance, the term \"short\" in the context of \"short service time\" in an electronic review is construed as negative, whereas in a restaurant review, \"short\" in the context of \"short waiting time\" is considered positive.\nDomain generalization(DG) is a type of transfer learning task. A similar task is Domain Adaptation, which allows access to both source domain and target domain data. Unlike Domain Adaptation, Domain Generalization only permits access to data from the visible domains. The goal of the Domain generaliza-tion approach is to address this problem by training a well-generalized model only with labeled data from one or more seen source domains and testing on an unseen domain. Although there has been considerable research on DG in the field of image classification, there have been few studies address in DG of text classification. Most studies in DG of text classification are based on Mixture of Experts (MOE) [4,5]. This approach involves training domain-specific experts and a domain-shared expert independently, followed by their aggregation using a score function. However, the effectiveness of these methods is constrained. In contrast, in real-world scenarios, individuals exhibit a natural ability to swiftly adapt to texts in unknown domains. We posit that the differentiating factor between humans and machines lies in humans' capacity to autonomously cate-gorize domain knowledge into domain-specific and domain-invariant categories and form semantic memory. This ability enables humans to enhance their gen-eralization capacity using prior knowledge. Inspired by this, we contend that storing domain-specific knowledge and domain-invariant knowledge can enhance the DG capabilities of the model.\nIn this paper, we propose a Multi-source Meta-learning framework relying on a \"Jury\" mechanism and Memory module (MMJM) to facilitate the learning of both domain-invariant and domain-specific features. Specifically, we introduce a meta-learning framework [6] based on the multi-source DG, which simulates how the model generalizes to an unseen domain. We suppose that the meta-learning approach aids the model in differentiating domains and learning the way of classification in an unseen domain by enhancing its ability to capture domain-related features. Additionally, we incorporate a memory mechanism [6] to more effectively capture domain-specific features, leveraging domain information com-prehensively while mitigating the risk of unstable optimization. Furthermore, we introduce an innovative \"Jury\" mechanism [7] to exploit domain-invariant fea-tures. This mechanism promotes features from the same class to be closer and features from different classes to be further away.\nOur contributions are summarized as follows:"}, {"title": "3 Method", "content": ""}, {"title": "3.1 Problem Definition", "content": "The goal of the multi-source DG is to train a model using multiple seen source do-mains with labeled data, ensuring its effective performance in previously unseen target domains. In this paper, we consider D seen source domains $D_s = {D_d}_{d=1}^D$ as the training set and only one unseen target domain $D_t$ as the test set. The data of each source domain is defined as: $D_d = {(x_{d,i}, Y_{d,i})}_{i=1}^{N_d}$, where $N_d$ is the number of samples in the d-th source domain; $x_{di}$ is a sample from the d-th source domain; $y_{d,i}$ is the label of $x_{d,i}$, where $y_{d,i} = {c}_{c=1}^{N_c}$, $N_c$ is the number of the class."}, {"title": "3.2 Meta-Learning Framework", "content": "Following [6], we introduce meta-learning to simulate how the model generalizes to an unseen domain. We divide the training stage into meta-train and meta-test stages. At the beginning of each training epoch, we randomly select one domain's data as the meta-test dataset, and the remaining D - 1 domains' data as the meta-train datasets. Our model is shown in Fig.1.\nWe update the model with meta-train and meta-test stages together. At first, we copy the original model. During the meta-train stage, we calculate the meta-train loss $L_{mtr}$ with the original model. This loss is composed of three parts: the classification loss $L_{class}$ calculated by the classifier C, the similarity loss $L_{Mem}$ calculated by comparing the features extracted by the encoder with those stored in the memory module, and the $L_{jury}$ calculated by the \"jury\" mechanism. Then, we update the copied model with the meta-train loss. During the meta-test stage, meta-test loss $L_{mte}$ is calculated in a similar way as in the meta-train stage. Finally, we update the original model with both the meta-train and meta-test losses. Therefore, the model parameters are updated through the combined meta-train stage and meta-test stages. The final model update formula is shown in Equ.1:\narg min $L_{mtr}(\\theta_\\phi, c) + L_{mte}(Adam(\\nabla_{\\theta_{\\phi, c}} L_{mtr}(\\theta_\\phi, c), \\alpha))$ \n$\\theta_{\\phi, C}$\nwhere $\\theta_{\\phi}$ denotes the parameters of the encoder $E_{\\phi}$ and the classifier C; $L_{mtr}$ is the meta-train loss; $L_{mte}$ is the meta-test loss; Adam is an optimizer; and $\\alpha$ is the inner loop learning rate."}, {"title": "3.3 Memory Module", "content": "The memory module is used in conjunction with meta-learning algorithms to store domain-specific features for each domain. We maintain a memory module for each domain,denoted as $M = {M_d}_{d=1}^D$, containing features for each class of that domain, where D is the number of source domains. Each domain's memory module contains $N_c$ feature slots $M_d = {M_d[c]}_{c=1}^{N_c}$, where $N_c$ is the number of classes. The dimension of a slot is equal to the dimension of the encoder $E_{\\phi}$. We calculate the memory-based similarity loss using classification features encoded by the encoder $E_{\\phi}$ and the slot $M_d[c]$ of each memory.\nInitialization. We initialize all memory modules before the training stage. During initialization, we sequentially initialize all slots in every memory module of all domains in the same way. Specifically, we use the pre-trained encoder to sequentially extract the features of all samples from a class in a source domain. We then initialize the corresponding slot with a domain-specific feature. We calculate the mean of the features of each class and use it as the initialization value for each slot.\nUpdate. After each iteration, we update the slots of each domain sequen-tially. All slots in all domains are updated in the same way. We update a slot by encoding the features of that class in the current iteration and using a momen-tum method to update the corresponding class slot, as shown in Equ.2:\n$M_d[c] = m \\cdot M_d[c] + (1 - m) \\cdot E[c]$\nwhere $M_d[c]$ is the slot c of the domain memory d; momentum m is a momentum parameter; E[c] consists of all the features of the c class, defined as: $E[c] = \\frac{1}{n} \\Sigma_{i=1}^n E(x_{d,i})$, where n is the number of samples of the c class in the d domain for that iteration.\nMemory-based Similarity Loss. We obtain the memory-based similar-ity loss by calculating the similarity score between the features encoded by the encoder $E_{\\phi}$ and the slots in the memory module. We calculate the similarity be-tween the feature $E(x_{d,i})$ and the corresponding slot in memory, and normalize these values with softmax. The calculation method is shown in Equ.3:\n$L_{Mem} = -log \\frac{exp((M_d[c])^TE(x_{d,i})/\\tau)}{\\Sigma_{i=0}^{C-1} exp((M_d[c])^TE(x_{d,i})/\\tau)}$\nwhere $\\tau$ is a temperature parameter."}, {"title": "3.4 \"Jury\" Mechanism", "content": "The domain-invariant features are directly related to the semantic features. We take $x^+$ which is generated by data augmentation, as the semantically identical sample of input x. We introduce the \"jury\" mechanism mentioned in [7] to ensure the semantic similarity of each pair, x and $x^+$. To make the model evolve smoothly and maintain the consistency of representation over time, we construct an augmentation-related encoder, $E_k$, that updates parameters by momentum.\nWord Repetition. Following [15], to change the semantics of the text as little as possible, we choose \"word repetition\" as our data augmentation method. We randomly repeat some words in a sentence. Given a text x with words $w_1,..., w_n$ in it, $x = {W_1, W_2, ..., W_{Ntext}}$, where $N_{text}$ is the length of the text. We define the maximum word repetition rate as r. Then, the number of repeated words in a text, k is determined by random sampling within the range [0, maximum(2, int($r\\times N_{text}$))].This parameter is used to expand the text length during word repetition. In this way, we get the set rep of all repeated words, obtained by uniform sampling. If the words $w_1$ and $w_n$ are in the set rep, the text is converted to $x^+ = {W_1,W_1, W_2, ..., W_n, W_n, ..., W_{Ntext} }$.\nMomentum Update. We maintain an augmentation-related encoder $E_k$. Unlike the encoder $E_{\\phi}$, which updates parameters through back propagation, the encoder $E_k$ updates parameters using momentum, as shown in Equ.4:\n$\\theta_k = \\lambda \\cdot \\theta_k + (1 - \\lambda) \\theta_{\\phi}$\nwhere $\\lambda$ is a momentum parameter; $\\theta_{\\phi}$ and $\\theta_k$ denote the parameters of encoders $E_{\\phi}$ and $E_k$, respectively. We believe this update method ensures the smoothness of the encoder $E_k$ updates, reduces differences between the two encoders, and maintains temporal consistency.\n\"Jury\" Mechanism Related Loss. We construct domain-independent memories to store domain-invariant features. We aim for x\u207a to have a higher sim-ilarity with the semantically identical sample $x_{d,i}$, and a lower similarity with other samples. We build a domain-independent memory for each class to store the domain-invariant class features. We define $N_c$ domain-independent memo-ries $Q = (Q_1, ..., Q_c, \u2026\u2026\u2026, Q_{N\u2282})$, where c\u2208 [1, Nc] and Nc is the total number of classes. Each domain-independent memory is structured as a queue of size $N_Q$: $[q^1_c, q^2_c, \u2026\u2026\u2026, q^{N_Q}_c]$, where j \u2208 [1, NQ], and $q^j_c$ stores the class feature in po-sition j of the $Q_c$ domain-independent memory. We obtain the class features of each instance $x^+_{d,i}$ through the encoder $E_k$ and input them into the corre-sponding class's domain-independent memory sequentially. We do not distin-guish the domain of $x_{d,i}$, but store all $x^+_{d,i}$ sequentially into the corresponding class's domain-independent memory. We place each newest feature at the end of the domain-independent memory and delete the oldest feature in the memory. All features in a domain-independent memory participate in calculating the similarity between $x_{d,i}$ and its augmented sample $x^+_{d,i}$. Both $x_{d,i}$ and $x^+_{d,i}$ com-pute the cosine similarity with the domain-independent memory corresponding to the current sample's class. We define $S_{d,i} = [s_1, ..., s_j, ...s_{Nq}]$ as the similar-ity score between $x_{d,i}$ and all class features stored in the corresponding class's domain-independent memory. We calculate the similarity score between $x_{d,i}$ and the corresponding domain-independent memory using the softmax function, as shown in Equ.5:\n$S_j = \\frac{exp((q_c^j)^TE_\\phi(x_{d,i})/\\tau)}{\\Sigma_{l=j}^{N_Q} exp((q_c^l)^TE_\\phi(x_{d,i})/\\tau)}$\nSimilarly, we define $S^+_{d,i} = [s^+_1, ..., s^+_j, ...s^+_{Nq}]$ as the similarity score between $x^+_{d,i}$ and all class features stored in the corresponding class's domain-independent memory. The method for calculating the similarity score between $x^+_{d,i}$ and the domain-independent memory is shown in Equ.6:\n$S_j = \\frac{exp((q_c^j)^TE_k(x_{d,i})/\\tau)}{\\Sigma_{l=j}^{N_Q} exp((q_c^l)^TE_k(x_{d,i})/\\tau)}$\nThen, we penalize cross-entropy loss between the two similarity scores $s_j$ and $s^+_j$, as shown in Equ.7:\n$L_{Jury} = - \\frac{1}{N_d} \\Sigma_{i=0}^{N_d} \\Sigma_{j=0}^{N_Q} s_j(x^+_{d,i})log(s(x_{d,i}))$"}, {"title": "3.5 Training Procedure", "content": "We summarize the overall training process of the model in this section. Before training, we initialize the memory module and the domain-independent memory in the \"jury\" mechanism. During each iteration, the D datasets are randomly divided into one meta-test dataset and D - 1 meta-train datasets. Then, the meta-train and meta-test stages work together to optimize the model.\nMeta-train. In the meta-train stage, we first extract the same number of training samples $x_{d,i}$ from each meta-train domain. Then we input these samples into encoder $E_{\\phi}$ to extract features $E_{\\phi}(x_{d,i})$. The features are subsequently fed into classifier C to calculate classification losses and memory-based similarity loss with the slots in the memory module. Besides, we input augmented sam-ples $x^+_{d,i}$ into encoder $E_k$ to extract augmented features $E_k(x^+_{d,i})$. The \"jury\" mechanism-related loss is calculated with features $E_{\\phi}(x_{d,i})$ and augmented fea-tures $E_k(x_{d,i})$. Finally, we update the domain-independent memory built for the \"jury\" mechanism. The meta-train loss comprises the meta-train classification loss, memory-based similarity loss, and the \"jury\" mechanism-related loss. The formula is shown in Equ.8:\n$L^{mtr}_{d} = L_C(X_d; \\theta_{\\phi};\\theta_c) + L_{Mem}(X_d; M_d; \\theta_{\\phi})+ L_{Jury} (X_d; Q; \\theta_k)$\nwhere $\\theta_{\\phi}$, $\\theta_c$, $\\theta_k$ denote parameters of the encoder $E_{\\phi}$, the classifier C, the encoder $E_k$ respectively; $X_d$ and $M_d$ denote the samples and the memory module of domain d; Q denotes the domain-independent memory.\nThe total meta-train loss is the averaged of the losses from all meta-train domains:\n$L_{mtr} = \\frac{1}{D-1} \\Sigma_{d=1}^{D-1} L^{mtr}_{d}$\nMeta-test. In the meta-test stage, we first copy the encoder $E_{\\phi}$ and classifier C. Then, we update them with the meta-train loss $L_{mtr}$. We update the encoder $E_k$ with parameters $\\theta_{\\phi'}$, where $\\theta_{\\phi'}$ represents the updated parameters of $E_{\\phi}$. The meta-test loss is calculated in the same way as the meta-train loss. It is composed of the meta-test classification loss, the memory-based similarity loss, and the \"jury\" mechanism-related loss. The formula is shown in Equ.9:\n$L_{mte} = L_C(X_T; \\theta_{\\phi' };\\theta'_c) + L_{Mem}(X_T; M_T; \\theta'_{\\phi})+ L_{Jury} (X_T; Q; \\theta'_k)$\nwhere $\\theta'_{\\phi}$, $\\theta'_c$, and $\\theta'_k$ denote the optimized parameters of the encoder $E_{\\phi}$, the classifier C, and the encoder $E_k$, respectively; $X_d$ and $M_d$ denote the samples and the memory module of d domain; Q denotes the domain-independent memory.\nMeta Optimization. Finally, we optimize the model as shown in Equ.1. Our training procedure is detailed in Alg.1:"}, {"title": "4 Experimental Setup", "content": ""}, {"title": "4.1 Experimental Data and Evaluation Metrics", "content": "To verify the effectiveness of this method, we conduct experiments on two multi-source text classification datasets: the Amazon product review dataset [17] for the multi-source sentiment analysis, and the multi-source rumor detection dataset [18]. The Amazon product review dataset contains 8,000 reviews, evenly distributed across four domains: Books (B), DVDs (D), Kitchens(K), and Elec-tronics (E). Each domain has 1,000 positive reviews and 1,000 negative reviews. The multi-source rumor detection dataset consists of 5,802 annotated tweets from five different events: Charlie Hebdo (CH), Ferguson (F), Germanwings (GW), Ottawa Shooting (OS), and Sydney Siege (SS), labeled as rumors or non-rumors (1,972 rumors, 3,830 non-rumors)."}, {"title": "4.2 Baselines", "content": "We compare our method with several state-of-the-art approaches. However, pre-vious methods are mostly based on the Domain Adaptation setting. For fairness, we employ these methods under the Domain Generalization setting.\nBasic fine-tunes a model on labeled data from source domains and directly tests it on the target domain. Gen refers to [5], which proposes a model com-posed of several domain-specific CNNs to compute private representations and a shared CNN to compute shared representations, coupled with adversarial train-ing. The MoE model consists of dedicated models for each source domain and a global model trained on labeled data from all source domains. During inference, the ensemble predictions of all models are aggregated. In this context, MoE [9] refers to a MoE model without a pretrained model, while MoE-Avg [9] refers to a MoE model with a pretrained model. PCL refers to a proxy-based contrastive learning method [19], where the traditional sample-to-sample mechanism is re-placed by the proxy-to-sample mechanism. Intra [20, 21] refers to center loss, which minimizes the distance between each example and its class center. Adv refers to the well-studied domain adversarial adaptation method [22], which re-verses the gradient calculated by the domain classifier. Agr-Sum [23] refers to two gradient agreement strategies based on gradient surgery to reduce the effect of conflicting gradients during domain generalization."}, {"title": "4.3 Implement Details", "content": "For all experiments, due to GPU memory constraints, we adapt Distil-Bert-base-uncased [3] and Bert-base-uncased [24] pretrained models as our encoders. The training batch size is set to 8, and we train 15 epochs. We set the token number of samples to 512. To optimize our model, we use the Adam optimizer with a weight decay of 5 \u00d7 10-4 to optimize the encoder $E_{\\phi}$. The inner loop learning rate $\\alpha$ and the outer loop learning rate $\\beta$ start at 1 \u00d7 10-6 and are increased to 1 \u00d7 10-5 in the first epoch. For the memory module, the momentum coefficient m is set to 0.2 and the temperature factor is set to 0.05. For the \"jury\" mechanism, the momentum coefficient $\\lambda$ is set to 0.999, and the size of the domain-independent memory is set to 64 \u00d7 768."}, {"title": "4.4 Experimental Results", "content": "The experimental results of text classification are shown in Tab.1. First, in the multi-source sentiment analysis and rumor detection, our method achieves the best performance, demonstrating its effectiveness for DG in the text classifica-tion. Second, compared to the meta-learning baseline MLDG and the contrastive learning baseline SCL, our method achieves higher accuracy. This success high-lights the advantage of capturing both domain-invariant and domain-specific fea-tures. Third, our method does not reach peak performance in every domain. We attribute this to the ambiguous characteristics of data in some areas. Nonethe-less, our strong performance across domains, as evidenced by average scores, highlights the method's adaptability in diverse settings."}, {"title": "4.5 Ablation studies", "content": "To further analyze the effectiveness of each part of our model, we conduct ab-lation studies. The results are shown in Tab.2, where \"Meta\" indicates training with the meta-learning framework, \"Mem\" indicates training with the memory module, \"Jury\" indicates training with the \"jury\" mechanism, \"SA\" refers the multi-source sentiment Analysis, and \"RD\" refers the multi-source rumor detec-tion. All ablation studies are based on distil-Bert.\nEffectiveness of Meta-Learning. We conduct the ablation study to in-vestigate the proposed meta-learning strategy. The model trained with the pro-posed meta-learning strategy could improve the results. For sentiment analysis, the model trained with a meta-learning strategy increases the basic baseline by 1.02% in the average classification accuracy. For rumor detection, the average F1 score increases by 7.6%. We believe that the implementation of meta-train and meta-test processes within the meta-learning framework could help the model adapt to the training of multi-source domains and learn domain-related features. This approach potentially reduces the risk of the model overfitting to domain biases, thereby enhancing the model's performance in encountering unseen do-mains. However, the introduction of the meta-learning approach consumes sub-stantial computational resources and memory. We believe this is due to the characteristics of the meta-learning method, which combines data from multiple source domains and updates parameters in two stages.\nEffectiveness of the \"Jury\" Mechanism. The \"jury\" mechanism related loss could increase the classification accuracy. For sentiment analysis, compared with the Distil-Bert baseline, the model trained with the \"jury\" mechanism could achieve an average accuracy increase of 0.7%. For rumor detection, the average F1 score increases by 5.56%. We believe that this is because the model is trained with a large number of data-augmented examples, which could help the model learn to reduce the domain divergence between different domains. What's more, it's obvious that the average F1 score increases by 0.32% for rumor detection, but the average accuracy reduces by 0.27% for sentiment analysis when combining the meta-learning method and the \"jury\" mechanism. We think that this observation may be attributed to the more apparent similarities among data from different domains in sentiment analysis. Additionally, we find the 'jury' mechanism demands relatively fewer computational resources and memory. It is a low-cost method that can enhance the model's generalization capabilities.\nEffectiveness of Memory Module. Contrary to expectations, the meta-learning framework with the memory module can't further increase the accuracy of classification compared to using meta-learning alone. However, compared to the model without the memory module, MMJM could increase the average sen-timent analysis accuracy by 1%. We believe that the memory module is helpful when combined with the domain-invariant features. Additionally, the memory module requires minimal computational resources and relatively low memory. We believe that the introduction of the memory module, despite its low cost, can significantly enhance the generalization capability of the entire framework. Thus, the inclusion of the memory module is highly worthwhile."}, {"title": "4.6 Visualization", "content": "To better understand the effectiveness of our method, we provide the t-SNE visualizations [25] of our MMJM and some baselines to intuitively assess the performance of our model on domain discrepancy, as illustrated in Fig.2. We observe that features from the source and target domains of MMJM are much more compact, which indicates our framework can learn more domain-specific and domain-invariant features."}, {"title": "4.7 Comparison with Large Language Model", "content": "To ensure the comprehensiveness of our experiments, we also explored the per-formance of large language models on the datasets. The experimental results are shown in Tab.3. It can be seen that on the sentiment classification dataset, Chat-GPT5 performs well, with an average classification accuracy 2.1% higher than MMJM. This indicates that large language models have strong sentiment per-ception abilities and can achieve good results across different domains, demon-strating strong generalization capabilities. However, on the rumor dataset, its performance is poor, which we believe is due to the lack of relevant knowledge about rumor detection in the language model. Therefore, we believe our model has research value, as it uses only 1% of the parameters of ChatGPT yet achieves good classification performance, significantly saving training resources and time costs for training large models. Additionally, it can still achieve good results through training on specific topics."}, {"title": "4.8 Case Study", "content": "We present a case study to intuitively understand our framework mechanism, as shown in Tab.4. The encoder for both models is distil-Bert. P and N respec-tively denote the positive and negative predictions. The symbol x indicates a"}, {"title": "5 Conclusion", "content": "In this paper, we propose a multi-source meta-learning framework for DG in text classification. The meta-learning strategy simulates how the model generalizes to an unseen domain. Additionally, we incorporate a memory-based module and the \"jury\" mechanism to extract domain-invariant features and domain-specific features, further enhancing the model's performance."}]}