{"title": "Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation", "authors": ["XIAOYE QU", "QIYUAN CHEN", "WEI WEI", "JIASHUO SUN", "JIANFENG DONG"], "abstract": "Despite the remarkable ability of large vision-language models (LVLMs) in image comprehension, these models frequently generate plausible yet factually incorrect responses, a phenomenon known as hallucination. Recently, in large language models (LLMs), augmenting LLMs by retrieving information from external knowledge resources has been proven as a promising solution to mitigate hallucinations. However, the retrieval augmentation in LVLM significantly lags behind the widespread applications of LVLM. Moreover, when transferred to augmenting LVLMs, sometimes the hallucination degree of the model is even exacerbated. Motivated by the research gap and counter-intuitive phenomenon, we introduce a novel framework, the Active Retrieval-Augmented large vision-language model (ARA), specifically designed to address hallucinations by incorporating three critical dimensions: (i) dissecting the retrieval targets based on the inherent hierarchical structures of images. (ii) pinpointing the most effective retrieval methods and filtering out the reliable retrieval results. (iii) timing the retrieval process to coincide with episodes of low certainty, while circumventing unnecessary retrieval during periods of high certainty. To assess the capability of our proposed ARA model in reducing hallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and mPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by utilizing fitting retrieval mechanisms and timing the retrieval judiciously, we can effectively mitigate the hallucination problem. We hope that this study can provide deeper insights into how to adapt the retrieval augmentation to LVLMs for reducing hallucinations with more effective retrieval and minimal retrieval occurrences.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, Large Vision-Language Models (LVLMs) [2, 4, 8, 22, 26, 47, 56] have gained widespread application across various scenarios due to their significant capacity to generate contextually ap- propriate text for visual inputs. Benefiting from the advancements in model architecture, training methods, and data diversity, LVLMs exhibit superior performance for various tasks such as visual question answering [11, 32] and image captioning [36, 51]. Despite these advancements, LVLMs still suffer from a significant challenge termed \u201challucination\u201d, whereby the models produce se- mantically plausible but factually inaccurate text, misaligned with the ground-truth content of the associated image [7, 12, 18, 49, 55, 57]. This problem damages the practical employment of LVLMs, particularly in fields that require accurate content generation, such as medical [19, 45] and ro- botics [27, 39], potentially resulting in severe consequences. Hence, addressing the hallucination problem is paramount for bolstering the credibility of LVLMs across real-world scenarios.\nTo mitigate hallucinations in LVLMs, a series of attempts have been recently proposed and can be broadly classified into two categories. The first class of approach retrains the LVLMs with constructed hallucination-related datasets by supervised fine-tuning (SFT) [10, 24, 44], or Rein- forcement Learning from Human Feedback (RLHF) [42, 50]. Although effective, these methods in- troduce significantly additional training costs. The other solutions focus on designing more robust decoding strategies [7, 12, 18, 49]. For example, VCD [18] introduces visual contrastive decoding to contrast output distributions derived from original and distorted visual inputs. In this way, the LVLM reduces the over-reliance on statistical bias and unimodal prior. Although these methods are training-free, they still suffer from the limitations of LVLMs' static parametric capacity. Recently, in the realm of large language models (LMs), augmenting LLMs [14, 16, 34] by retrieving informa- tion from external knowledge resources has shown promise in reducing language hallucinations. Furthermore, this retrieval augmentation serves as a flexible way to extend beyond the model's inherent knowledge without the burden of extensive training costs. In this paper, we aim to pro- pose a novel framework to augment LVLMs with external knowledge by introducing an Active Retrieval-Augmented large vision-language model (ARA) for mitigating hallucinations."}, {"title": "2 RELATED WORK", "content": "Recently, large-scale models have attracted significant attention and gained widespread applica- tions [30, 40, 41, 46, 58]. LVLMs significantly enhance the interaction between humans and AI in a more natural and intuitive ways and demonstrate remarkable capabilities in understand- ing and generating multi-modal content. With the aid of advanced Large Language Models like LLAMA [43] and Qwen [1], a batch of LVLMs such as LLaVA-1.5 [26], Qwen-VL [2], and mPLUG- Owl2 [47] have emerged, which can comprehend and generate a wide array of content by utilizing information from distinct modalities like texts and images. The training of LVLMs is divided into two critical phases: feature alignment and instruction fine-tuning. The former aims to model visual and textual inputs coherently, while the latter strives to enhance the ability of LVLMs to compre- hend user queries. Despite the advancements, these LVLMs continue to face significant challenges"}, {"title": "2.2 Hallucination in LVLMs", "content": "In LVLMs, hallucination refers to models that generate seemingly plausible outputs inclusive of objects, attributes, or relations that do not correspond with the images [20, 48, 52]. Approaches to address hallucinations in LVLMs have largely fallen into two camps. The first emphasizes model refinement through supervised fine-tuning or application of reinforcement learning techniques. For instance, LRV [25] seeks to diminish hallucinatory outputs by utilizing broad and varied su- pervised fine-tuning datasets. For methods based on reinforcement learning, LLaVA-RLHF [42] is the pioneer in applying Reinforcement Learning with Human Feedback (RLHF) to mitigate hallu- cination in LVLMs. This approach is further refined by RLHF-V [50], which incorporates detailed corrective human feedback. Considering the instability and training difficulty of RLHF, Zhao et al. [53] employ Direct Preference Optimization (DPO) and build a hallucination-aware dataset for alleviating hallucination. Although While these methods have yielded notable advancements, they also introduce significant training overheads and are prone to overfitting the training data. The second category consists of training-free strategies designed to circumvent hallucination without incurring additional training expenses. VCD [18] endeavors to rectify the model's over-reliance on linguistic priors and statistical leanings by comparing the output distributions from both unaltered and visually perturbed inputs. OPERA [12] mitigates overconfidence in model predictions by inte- grating a penalty term into the model logits during beam search decoding, further enhanced with a strategic rollback feature. Although these methods are effective, they still suffer from the limita- tions of LVLMs' static parametric capacity. In this paper, different from the above two paradigms, we focus on augmenting LVLMs with external knowledge to mitigate hallucination in LVLMs."}, {"title": "2.3 Retrieval-Augmented Generation", "content": "In the realm of large language models (LLMs), Retrieval-Augmented Generation (RAG) has been widely used and shown promising results in mitigating hallucinations. RAG in LLMs retrieves rel- evant information from an external knowledge base before LLMs respond to a query [14, 16, 34], thereby enabling them to collaboratively generate responses by leveraging the retrieved external non-parameterized knowledge alongside their internal parameterized knowledge. However, its ap- plication in LVLMs has not been thoroughly explored. Since LVLMs present a multimodal nature, findings pertinent to LLMs cannot be indiscriminately extrapolated to LVLMs. Predominantly, ex- isting research on retrieval augmentation in multimodal tasks has been confined to image caption- ing [3, 35\u201337] or image generation [6], with a narrow focus that overlooks the broader implications of RAG in LVLMs, especially concerning its capacity for hallucination reduction. In this paper, our research conducts an extensive evaluation of RAG's applicability in LVLMs to mitigate hallucina- tion by tailoring retrieval strategies and optimizing active retrieval processes. It is worth noting that the active retrieving strategy has also been discussed in [15] in LLM. However, they propose a token-level confidence for triggering the retrieval, which is not applicable in LVLMs and we will investigate it in the ablation study."}, {"title": "3 METHOD", "content": "In this section, we first describe the generation procedure of the LVLMs, laying the groundwork for comprehending our approach. Subsequently, we will introduce our Active Retrieval-Augmentation large vision-language model (ARA) in detail. As shown in Figure 2, given the input image and query, the LVLM first analyzes them based on mutual information and determines whether to trig- ger the retrieval process. If the retrieval is necessary, our ARA will follow a coarse-to-fine paradigm"}, {"title": "3.1 LVLM Input and Decoding", "content": "The input of LVLMs contains both image and text. Typically, the LVLMs use a vision encoder (e.g. CLIP) to extract visual tokens from the input images, and then map them into the input space of LLMs with a linear projection module. Subsequently, the mapped visual tokens are used as part of the LLM input, along with the text input. In this paper, we denote the visual tokens as x\u00b2 = {X0, X1, . . ., XN\u22121}. Here N is the length of the visual tokens and it is a fixed number in most cases. Correspondingly, the input text is tokenized and we denote it as x\u00ba = {X0, X1, . . ., XM-1}. The image and text tokens are concatenated as the final input sequence and we denote it as {xt}T\u22121\nt=0 that T = N + M.\nDuring decoding, LVLM gets the probabilities for the next token prediction, formally:\n\\(p(xt|x_{<t}) = SoftMax[LVLM(h_t)]x_t, \\quad x_t \\in V,\\)\nwhere ht denotes the output hidden states at time step t for the input image and query, x<t to simplify the sequence {x}\nt=0 and V means the whole vocabulary set. For greedy decoding, the token with the highest probability will be chosen."}, {"title": "3.2 Active Triggering Retrieval", "content": "As shown in Figure 2, given the input image and query, our ARA model first determines whether the input pair requires retrieval. In this way, we can circumvent unnecessary retrieval during periods of high certainty. In this section, we propose three active retrieval methods to explore the"}, {"title": "3.3 Coarse-to-Fine Hierarchical Retrieval", "content": "Given the inherently hierarchical nature of images, simple full-image retrieval may result in noise and irrelevant outcomes. As shown in Figure 2, using the full image for retrieval can not obtain the most relevant information for reasoning, namely \u201cred shirt\". Thus it is imperative to decompose the target object causing hallucination from the input image for more accurate retrieval. In the following, we will first describe the coarse-grained retrieval and then introduce the fine-grained retrieve."}, {"title": "3.4 Reranking Retrieving Results", "content": "Throughout the above retrieving process, the similarity is computed by the CLIP embeddings be- tween images. In this way, the visually similar but semantically different pairs may be recalled. To avoid noisy retrieving results which bring different semantic, after obtaining the retrieved pairs for both coarse-grained and fine-grained retrieval, we subsequently propose a reranking strategy designed to recalibrate the retrieved outcomes. Specifically, we adopt the LVLM to describe the input image and generate a detailed caption. For coarse-grained retrieval, the input full image is characterized, while describing the cropped image for the fine-grained retrieval. In this way, we are able to derive the semantics of the input image. Then, we extract the textual embeddings for both image captions and captions of retrieved images with the CLIP. Finally, we can rerank the retrieved pairs according to the semantic similarity between these captions. In this stage, we also explore other reranking methods, such as k-reciprocal encoding [54] or using an external visual entailment tool [23]. We will compare them in the ablation study."}, {"title": "3.5 Joint Coarse-grained and Fine-grained Decoding", "content": "After retrieving and ranking the pairs, we can conduct a joint coarse-grained and fine-grained de- coding to yield the final responses. According to the different ways of using the retrieved results, we have two schemes including probability-level fusion and instance-level fusion. The former in- tegrates the results of coarse-grained and fine-grained retrieval into a single prompt for inference, while the latter conducts separate inferences on the coarse-grained and fine-grained retrieval re- sults, followed by a probabilistic-level weighting.\nProbability-level Fusion. In this paradigm, we use the prompt for both coarse-grained retrieval and fine-grained retrieval: \u201cHere are the image-caption pairs similar to the test image: (Retrieval Pairs). Based on these pairs and this image: (Input Image). Answer this question: (Input query)", "below": "n\\(p^{bcoarse}(x_t|x_{<t}) = SoftMax[LVLM([R_1 : h_t])]x_t, \\quad x_t \\in V,\\)\nwhere R\u012b indicates the hidden states of retrieved image-level pairs. For simplicity, we directly use [:] here. We use pcoarse to represent decoding with coarse-grained retrieval. Similarly,\n\\(p^{fine}(x_t|x_{<t}) = SoftMax[LVLM([R_o : h_t])]x_t, \\quad x_t \\in V,\\)\nwhere Ro indicates the retrieved fine-grained object-level pairs. We represent the decoding with object-level fine retrieval by pfine (xt|x<t). For the probability-level fusion, we can combine them with a hyperparameter a in [0,1] to control the importance of two kinds of retrieval.\n\\(P^{coarse\\&fine}(x_t|x_{<t}) = \\alpha p^{coarse}(x_t|x_{<t}) + (1-\\alpha)p^{fine}(x_t|x_{<t}),\\)\nInstance-level Fusion. For this kind of fusion, we can integrate both coarse-grained retrieval and fine-grained retrieval with prompts: \u201cHere are the image-caption pairs similar to the test image:"}, {"title": "4 EXPERIMENT", "content": "This section will elaborate on the evaluation of our proposed Active Retrieval-Augmented large vision-language model (ARA) across different LVLMs."}, {"title": "4.1 Evaluation Metrics", "content": "POPE [20], the Polling-based Object Probing Evaluation proposed a simplified method for evalu- ating object hallucinations. In this evaluation framework, the LVLM is tasked with determining the presence of specific objects in a given image, with a balanced distribution of queries for ob- ject presence and absence. The evaluation protocol encompasses three distinct sampling settings: random, popular, and adversarial, each employing unique strategies for constructing negative sam- ples. In the random setting, objects not present in the image are chosen randomly. The popular setting selects missing objects from a high-frequency pool, whereas the adversarial setting pri- oritizes coexisting objects that could be easily mistaken for existing objects in the image. The POPE benchmark aggregates data from three diverse sources: MSCOCO [21], A-OKVQA [38], and GQA [13]. Within each dataset, every sampling setting comprises 500 images, with 6 questions posed for each image, resulting in a total of 27,000 question-answer pairs derived from the devel- opment sets of these datasets. The evaluation predominantly assesses metrics such as accuracy, precision, recall, and F1 score.\nMME [48] is a comprehensive benchmark designed specifically for evaluating LVLMs on multiple dimensions. It includes ten sub-tasks related to perception and four sub-tasks emphasizing cog- nition. According to previous studies [7, 18, 49], we utilize the existence and count subsets for object-level hallucination evaluation, and the position and color subsets for attribute-level halluci- nation evaluation. Performance is quantified by accuracy and accuracy+, which is also the official evaluation method.\nMMStar [5], foundational to vision-reliant multi-modal evaluation, consists of 1,500 challenge samples meticulously chosen by human experts. It is tailored to systematically benchmark six key capabilities alongside eighteen granular facets, aiming to rigorously assess the multi-modal profi- ciency of LVLMs using a selection of samples that have been carefully refined and equilibrated.\nMMbench [29] is collected from multiple sources, including public datasets and the Internet, and currently, contains 2974 multiple-choice questions, covering 20 ability dimensions. Similar to MME, we focus on hallucination-related ability. Thus, we select 4 ability subsets including object local- ization, attribute recognition, spatial relationship, and action recognition as our evaluation bench- marks. These subsets form a comprehensive assessment of hallucinations, addressing issues at the object-level, attribute-level, and relation-level comprehensively."}, {"title": "4.2 Implementation Details", "content": "In the case of coarse-grained retrieval, we use the COCO dataset [21] as the retrieval database. For fine-grained retrieval, we use the VisualGenome [17] to build the fine-grained retrieval database as it provides a detailed caption for each grounding box in the image. CLIP-Large is used to extract image embeddings for retrieval. To decompose target objects from the input query, we extract key"}, {"title": "4.3 Experiment Results", "content": "Experimental results on POPE under the random, popular, and adversarial set- tings are summarized in Table 1. This benchmark mainly focuses on the object-level hallucination. Specifically, the performances of our ARA consistently surpass the baseline results on all of the LVLMs. This suggests its pivotal role in augmenting LVLMs with retrieval, thereby reducing in- stances of object-level hallucination. In a more detailed model-specific analysis, RAR demonstrates varied effects across different LVLMs. For all three LVLMs, the F1 score elevation is predominantly driven by a recall boost (e.g., up to 10 points for mPLUG-Owl2), showcasing retrieving to external knowledge base can effectively help detect object presences.\nSubset. The MME subset evaluations extend beyond POPE's scope, encompassing both object-level and attribute-level hallucination. Results in Table 2 show that our ARA leads to a uniform enhancement in addressing object-level hallucination for all models, except the count score of Qwen-VL. Meanwhile, our ARA demonstrates significant im- provements in attribute-level Color scores, contributing to substantial overall performance gains. These improvements are attributable to our designed coarse-to-fine retrieval paradigms, which effectively focus on the target object.\nResults on MMStar. In a more difficult benchmark MMStar which rigorously assesses the multi- modal proficiency, as depicted in Table 3, our method ARA secures a notable enhancement across all six subsets. For instance, LLaVA-1.5 realizes an average improvement of 8.8 points. Unlike POPE, which may include statistical bias, answering questions in MMStar necessitates reliance on im- age information. Hence, our retrieval augmentation method introducing external image and text knowledge significantly bolsters model performance.\nHallucination Subset. This benchmark extends our hallucination eval- uation to incorporate the relation level in addition to the above object and attribute levels, pro- viding a more holistic assessment for reducing hallucinations. As evidenced in Table 4, benefit- ing from coarse-grained retrieval, our ARA manifests discernible enhancements in the object and relation-level subsets, such as OL, SR, and ACR. For instance, our ARA model surpasses the vanilla LLaVA-1.5 with 5.08 percent points on object localization (OL). Concurrently, the integration of fine-grained retrieval into our model facilitates superior performance in attribute-level subsets, namely attribute recognition (ATR)."}, {"title": "5 ABLATION STUDY", "content": "In this section, we undertake a series of ablation studies with the aim of identifying the critical ele- ments integral to constructing a resilient and efficient retrieval-augmented large vision-language model."}, {"title": "5.1 Ablation of Coarse-to-Fine Retrieval", "content": "In this section, in order to explore the effects of different factors on coarse-to-fine retrieval in a simple and effective way, we did not use the rerank method or active trigger strategy.\nWhich kind of retrieval is better? Due to the multimodal characteristics of the input, there are various retrieval methods at our disposal. As shown in Table 5, we attempt to use the input query and image to search for images and descriptions in the database. For this study, we only conduct coarse-grained retrieval and we use the COCO dataset as the database which contains a caption for each image. As a result, there are four retrieval methods available to us. As reflected in the results within this table, we adopt the input image as our primary retrieval modality for images in the database throughout our series of experiments. Surprisingly, using the input query to search the database for image leads to substantially poorer outcomes, achieving only 51.6% accuracy. This lower performance may stem from the fact that, despite the presence of relevant keywords, the images retrieved this way tend to be surrounded by a significant degree of noise.\nAdditionally, we investigate the influence of different embedding models on the efficacy of image-to-image retrieval. In our comparison, we examine three models: CLIP-Base/Large [33] and DINOV2-Base [31]. Among them, we notice that CLIP-base performs better than DINO (86.33 vs 86.07) and this may be related to their different training methods. Moreover, when comparing CLIP models of various sizes, we observe that the larger-scale CLIP models contribute to more pronounced improvements in performance. Therefore, in the context of this study, CLIP-Large is selected as the image embedding model for retrieval purposes.\nHow to augment the LVLM? After retrieving, we can obtain retrieved images and paired cap- tions. In this section, we explore how to augment the LVLM with this external information. For a fair comparison, in this study, we fix the number of retrieved pairs to 3 for both coarse-grained and fine-grained retrieval. As demonstrated in Table 6, when using coarse-grained retrieval, we observe that the image and caption pair perform worse than the single caption (86.93 vs 87.43). However, the opposite results are observed in fine-grained retrieval and the Fine (I+T) achieves 86.53 compared with 86.20 of Fine (T). These results imply that retrieval augmentation is sensitive to specific configurations. In this experiment, we do not carefully compare with Coarse (I) or Fine (I) as we have found that the LVLMs we use in our experiments are still quite weak in multi-image reasoning.\nIn Section 3.5, we propose a joint coarse-grained and fine-grained decoding, designed to op- timally utilize the two retrieval mechanisms. In Table 6, we also investigate the effectiveness of probabilistic-level fusion denoted as Coarse & Fine and instance-level fusion labeled as Coarse+Fine."}, {"title": "5.2 The effectiveness of Reranking Method", "content": "To substantiate the efficacy and indispensability of reranking, we begin by providing a case study within the POPE benchmark. As depicted in Figure 4, in instances where an image portrays peo- ple in a pool, leveraging CLIP for image retrieval may inadvertently introduce irrelevant content due to its predominant emphasis on visual congruence. For instance, it might retrieve an incon- gruent image captioned \"fruit and a bowl of bread sit on the table.\" Therefore, an adept reranking methodology can proficiently filter and organize the retrieved images in a more relevant manner. In this particular scenario, our reranking strategy aligns the input image caption with those of the retrieved images. By assessing these similarity metrics, we are able to reorganize the pool of retrieved images into a more accurately ranked series, thereby enhancing the precision of the retrieval process.\nIn order to further quantify the effect of reranking and compare different reranking methods, we conduct experiments in Table 7. The result reveals that the omission of re-ranking precipitates a decline in accuracy, dropping from 87.17% to 86.93% This suggests that re-ranking effectively miti- gates noise, particularly when employing the top three re-ordered retrieval pairs. For comparative purposes, we executed two alternative methodologies. The k-Reciprocal approach [54], a prevalent offline person re-identification method, re-ranks images using the k-Reciprocal nearest neighbor algorithm. While this method offers a swifter execution by circumventing the captioning process of input images, its efficacy slightly underperforms our strategy. Additionally, we examined the employment of the external tool VSR [23], which determines the entailment between the caption of the input image and the retrieved images. As shown in Table 7, this method gains only a slight"}, {"title": "5.3 Ablation of Active Triggering Retrieval", "content": "As excessive retrieval activations may lead to undue time expenditure and unnecessary retrieval, in this paper, retrieval is activated using a difficulty metric. In this ablation study, we compare three kinds of active retrieval methods, including confidence-aware active retrieval, query-aware active retrieval, and image-aware active retrieval. As shown in Figure 5, we have drawn the propor- tion of queries that need to be retrieved to achieve peak performance. Firstly, it is quite intuitive, the fluctuations of confidence-aware active retrieval among different models are very significant. For mPLUG-Owl2, we even need to retrieve all the queries, which indicates that confidence-aware active retrieval is not a good indicator of whether to perform a retrieval. For both query-aware active retrieval and image-aware active retrieval, the LVLMs can better determine a trigger thresh- old. Considering that query-aware active retrieval is more concise and the process of adding noise is omitted, we use query-aware active retrieval in all our experiments. It is worth noting that the very slight decline in the model's performance after continuing to use retrieval beyond the trigger threshold is due to the fact that the model itself has a high level of confidence, but the retrieval has introduced some redundant noise information, which we cannot filter out through reranking."}, {"title": "5.4 Comparison with State-of-the-Art Method", "content": "In this paper, we augment LVLMs with external knowledge to mitigate hallucinations. To further demonstrate the effectiveness of our method, we compare it with VCD [18] and the results are demonstrated in Table 8. VCD is a state-of-the-art training-free method that explores intra-model knowledge with visual contrastive decoding. From the results, we can notice that our method significantly surpasses VCD in all three settings. For instance, in popular settings, our method gains a 4.06 percent improvement. It is worth noting that VCD performs comparably with the vanilla LLaVA-1.5 when using greedy decoding. This phenomenon is also observed in [7]. Instead, our method significantly surpasses the vanilla LLaVA-1.5 model as presented in Tabe 1. This result further verifies the robust improvement of introducing external information when applying our ARA method."}, {"title": "5.5 Text Generation Quality Evaluation", "content": "In this section, we evaluated the effectiveness of our method in improving text generation quality on the MS-COCO validation set\u00b9. As shown in Table 9, all models LLaVA, Qwen-VL, and mPLUG exhibited significant improvements after applying our method, with an average improvement of 7.7% across the three models. Notably, the improvement was most pronounced for Qwen-VL, with an average increase of approximately 14.9%."}, {"title": "5.6 Qualitative Results", "content": "To qualitatively verify the effectiveness of our ARA method on downstream tasks, we presented five examples from the POPE MSCOCO dataset and MME benchmark. As shown in Figure 6, we present three image-caption pairs for both coarse-grained retrieval and fine-grained retrieval. In the above three examples of determining the existence of objects, fine-grained retrieval helps to increase the model's existence information about specific objects. For the last two examples, the coarse-grained retrieval contributed effectively to capturing the overall information of the images. Thus, with both kinds of retrieval, our RAR model can effectively increase the accuracy compared with vanilla LLaVA-1.5."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose a novel framework to augment LVLMs with external knowledge by in- troducing an Active Retrieval-Augmented large vision-language model (ARA) for mitigating hal- lucinations. Specifically, our proposed ARA is grounded in three critical dimensions for mitigat- ing hallucinations in LVLMs, including coarse-to-fine retrieval for dissecting the retrieval targets, pinpointing the most effective retrieval methods, and triggering the retrieval process properly. Our findings indicate that by utilizing fitting retrieval mechanisms and timing the retrieval judi- ciously, we can effectively mitigate the hallucination problem. Our extensive experiments across four benchmarks and three widely-used LVLM confirm ARA's efficacy in reducing hallucination."}]}