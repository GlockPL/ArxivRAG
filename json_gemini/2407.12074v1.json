{"title": "Enhancing Parameter Efficiency and Generalization in Large-Scale Models: A Regularized and Masked Low-Rank Adaptation Approach", "authors": ["Yuzhu Mao", "Siqi Ping", "Zihao Zhao", "Yang Liu", "Wenbo Ding"], "abstract": "Large pre-trained models, such as large language models (LLMs),\npresent significant resource challenges for fine-tuning due to\ntheir extensive parameter sizes, especially for applications in\nmobile systems. To address this, Low-Rank Adaptation (LoRA)\nhas been developed to reduce resource consumption while main-\ntaining satisfactory fine-tuning results. Despite its effectiveness,\nthe original LoRA method faces challenges of suboptimal per-\nformance and overfitting. This paper investigates the intrinsic\ndimension of the matrix updates approximated by the LoRA\nmethod and reveals the performance benefits of increasing this\nintrinsic dimension. By employing regularization and a gradi-\nent masking method that encourages higher intrinsic dimension,\nthe proposed method, termed Regularized and Masked LoRA\n(RM-LORA), achieves superior generalization performance with\nthe same or lower trainable parameter budget compared to the\noriginal LoRA and its latest variants across various open-source\nvision and language datasets.", "sections": [{"title": "1 Introduction", "content": "Large pre-trained models, like large-scale language models (LLMs),\nhave showcased remarkable performance across a variety of\ntasks in computer vision and natural language processing [1-4].\nNonetheless, fine-tuning these models for specific downstream\ntasks often presents substantial resource challenges due to their\nextensive parameter sizes. In this context, parameter-efficient\nfine-tuning (PEFT) methods have been extensively explored to\nalleviate resource consumption while preserving or enhancing\nfine-tuned model performance [5-19]. Among these methods,\nLow-Rank Adaptation (LoRA), which involves freezing the pre-\ntrained weights and approximating updates in weight matrices\nusing the multiplication of two low-rank matrices, has emerged\nas a promising approach to balance computational efficiency and\ntask performance during the fine-tuning process of large models\n[6]."}, {"title": "2 Related Works", "content": "In attempts to address the computational challenges posed by\nupdating the enormous amount of weights in large pre-trained\nmodels, LoRA, proposed by Hu et al. [6], achieves outstanding\nmodel generalization with a significantly reduced budget of train-\nable parameters during fine-tuning. However, LoRA still faces the\nchallenges of sub-optimal performance and overfitting. Previous\nresearch addressing LoRA's main challenges is briefly discussed\nas follows:\nSub-Optimal Performance. While LoRA has demonstrated\nremarkable parameter efficiency and generalization performance,\nit can lead to suboptimal fine-tuning of large-scale models with\nhigh embedding dimensions [23]. In some cases, there is a con-\ntradictory phenomenon where a higher LoRA rank doesn't nec-\nessarily bring better performance than a lower LoRA rank. Much\nresearch has been devoted to further balancing the efficiency and\nperformance achieved by LoRA, including the adaptive choice\nof LoRA rank [24-26], adjustment of learning rate [23], random\nprojection [27], derivative-free optimization [28], and pre-trained\nweights optimization [29]. Nevertheless, none of these methods\nconsider the role of LoRA updates' intrinsic dimension in miti-\ngating the performance gap under a given LoRA rank setting.\nOverfitting. Fine-tuning large pre-trained models with a large\nnumber of parameters can easily encounter overfitting, resulting\nin suboptimal generalization, especially on test data [21]. In the\nAdaLoRA method, the LoRA matrices for less important pre-\ntrained weight matrices are assigned a lower rank to prevent\noverfitting [24]. However, according to the experiments of Qiang\net al. [20], LORA and AdaLoRA still clearly overfit the training\ndata as fine-tuning advances, with decreases in training losses\nbut increases in test losses. To alleviate the overfitting problem,\nQiang et al. [20] developed the BiLoRA method, which iteratively\ntrains different subsets of trainable parameters using different\nsubsets of training data. Furthermore, Hayou et al. [23] pointed\nout that the suboptimality of LoRA and its variants is due to\nsome directions of LoRA matrices not being sufficiently updated,\nand thus the change in model weights approximated by LoRA\nis restricted by the vector (sub)space generated by the LoRA\nmatrices' columns at initialization."}, {"title": "3 RM-LORA Method", "content": "The previous analysis of LoRA's existing limitations and solu-\ntions gives rise to the idea and method employed in this work.\nSpecifically, for better performance under a given LoRA rank\nsetting, this paper proposes a fine-tuning strategy that promotes\nthe growth of the intrinsic dimension of LoRA updates through\nregularization and gradient masking, bridging the gap between\npractical performance and theoretical optimal performance."}, {"title": "3.1 Preliminary", "content": "Transformer Models. A transformer-based pre-trained model\ntypically involves L stacked encoder/decoder blocks, with a multi-\nhead attention module followed by a fully connected feed-forward\nnetwork (FFN) in each block. Given an input sequence X \u2208 \u211d^{n\u00d7d},\nthe output of the multi-head attention module can be written as:\nMultiHead(X) = Concat(head\u2081, . . ., head\u0127)W\u00ba,\nwith head; = Attention(Qi, Ki, Vi),\nand Attention(Qi, Ki, Vi) = softmax(\\frac{Q_iK_i^T}{\\sqrt{d_k}})V_i\n(1)\nwhere Q\u2081 = XW_i^Q, K\u2081 = XW_i^K, and V\u2081 = XW_i^V are matrices\nof queries, keys, and values of head; respectively, with projec-\ntion matrices W_i^Q \u2208 \u211d^{d\u00d7d_k}, W_i^K \u2208 \u211d^{d\u00d7d_k}, W_i^V \u2208 \u211d^{d\u00d7d_o}, and\nW^O \u2208 \u211d^{hd_o\u00d7d}. We refer readers to [30] for a more comprehensive\nintroduction to attention calculations in general.\nGiven the output of the multi-head attention module, the FFN\nfurther projects the d-dimensional output X' for each position.\nA two-layer FFN with a ReLU activation operates as follows:\nFFN(X') = max(0,X'W_{f_1} + b_1)W_{f_2} + b_2,\n(2)\nwhere W_{f_1} \u2208 \u211d^{d\u00d7d_m} and W_{f_2} \u2208 \u211d^{d_m\u00d7d}. Moreover, a residual\nconnection followed by layer normalization is applied to each\nlayer to generate the output of each transformer block given the\ninput sequence X in the following way:\nLayerNorm(X + FFN(LayerNorm(X + MultiHead(X)))).\n(3)\nLoRA Fine-tuning. For a pre-trained matrix Wo\u2208 \u211d^{d_1\u00d7d_2},\nLoRA, as proposed by Hu et al. [6], approximates its update AW\nby AW = BA, where A \u2208 \u211d^{R\u00d7d_2} and B\u2208 \u211d^{d_1\u00d7R} with rank\nR < min(d1, d2). During model fine-tuning, the weight matrix\nWo is frozen, with only the LoRA adapters A and B being trainable.\nThe modified LoRA forward pass is:\nh = Wox + Wx = Wox + BAx.\n(4)\nTypically, the low-rank matrice A is initialized using a random\nGaussian distribution, while B is initialized to zero, ensuring\nAW = 0 at the start of fine-tuning. Current approaches to fine-\ntuning large pre-trained models with LoRA apply a pair of matri-\nces to all weight matrices involved in each transformer block's\nmulti-head attention module and FFN [24, 31].\nThe Expressive Power of LoRA. Zeng and Lee [22] investi-\ngates the LoRA approximation error under a mild non-singularity\nassumption. To begin with, a L-layer width-D fully connected\nReLU neural network is denoted as FNNL,D(\u00b7; (W1)=1, (bi)}=1),\nwhere W\u2081 \u2208 \u211d^{D\u00d7D} are the weight matrices and b\u2081 \u2208 \u211d^D are"}, {"title": "3.2 Influence of the Intrinsic Dimension of\nLoRA Adapter \u25b3W", "content": "Note that the partition P\u2081 of the pre-trained model for the i-th\nlayer in the target model is an intrinsic but unknown property\nduring adaptation, and consequently, the number and index of\nlayers l \u2208 P; are also unknown. Nevertheless, with a pre-trained\nmodel fo to be adapted and the target model \u0192 determined by a\ngiven downstream task, the partition can be considered determin-\nistic, as can the discrepancy between the pre-trained model and\nthe target model E\u2081 = Wi \u2013 \u03a0\u03b9\u2208p; W1. Consider the case where\nthe LoRA rank setting for each layer l \u2208 P\u012f is the same as R. The\ne; term in Equation 8 can be rewritten as:\nei,rank(\u2206W1ep\u2081)<R = \u03c3\u03a31\u03b5p; R+1(Ei) for each layer i \u2208 [L],\n(9)\nwhere rank(AW1\u2208P\u2081) \u2264 R represents the LoRA adapter for each\nlayer l \u2208 Pi satisfies the rank constraint rank(AW\u2081) \u2264 R.\nClearly, increasing rank(\u2206W\u2081) helps relax the constraint on\nLoRA rank R to achieve a certain level of approximation error. For\nexample, consider two LoRA rank settings R\u2081 and R2 with R1 < R2.\nIf rank(\u2206W\u2081) \u2264 R1 < R2, then ei,rank(\u2206W1\u2208P;)\u2264R2 degenerates to\nei,rank(\u2206W1\u20ac9\u2081)\u2264R\u2081 for i \u2208 [L], despite the larger size of LoRA\nmatrices under the LoRA setting of R2. In this case, LoRA rank R\u2081\nand R2 yield the same LoRA approximation error E||f(x)-f(x)||2\naccording to Equation 8."}, {"title": "3.3 Regularization on LoRA Weights", "content": "Let a pair of LoRA low-rank matrices be denoted as WA and WB,\nrespectively. To enforce the growth in rank of \u2206W = WBWA, the\nfollowing regularizer is first used to encourage WA and WB to\nbe orthogonal:\nReg(WA, WB) = ||WA(WA)^T \u2013 I|| + ||(WB)^TWB \u2013 I||.\n(10)\nThe orthogonality of WA and WB helps increase the rank(WA)\nand rank(WB). According to the lower bound for the rank of\nthe matrix product, for matrices A \u2208 \u211d^{R\u00d7d_2} and B\u2208 \u211d^{d_1\u00d7R},\nthe rank of their product matrix C = BA satisfies rank(C) \u2265\nmax(rank(A) + rank(B) \u2013 R, 0). This lower bound ensures the\ngrowth of the intrinsic rank of the LoRA adapter AW = WAWB\nas the rank(WA) and rank(WB) increase with the regularizer\nshown in Equation 10. Note that there exist other alternative\nregularizers that theoretically can also encourage the growth of\nrank(AW), but are infeasible in reality due to considerations of\ndifferentiability, numerical stability, and computational costs\u00b9."}, {"title": "3.4 Gradient Masking for Partial Updates", "content": "The gradient masking algorithm in RM-LoRA is designed to per-\nform partial updates in LoRA matrices. The algorithm takes as\ninput the total number of steps T, the LoRA rank R, and the num-\nber of directions to update in each step. In each training step\nt, it samples a mini-batch of data \u00a7 and computes the gradients\nVW and VW for each pair of LoRA weight matrices. The\ncorresponding gradient masks are first initialized to zero, before a"}, {"title": "Algorithm 1 Gradient Masking Algorithm", "content": "Input: Total steps T, LoRA rank R, number of updated direc-\ntions r.\nfor t = 0 to T-1 do\nfor each pair of LoRA weight matrices (WA, W) in the\nmodel do\nSample a mini-batch data \u011f\u0131 and compute the gradients\n(\u2207WA, VW);\nInitialize gradient masks (MA, MP) \u2190 0 with the same\nshape as(VW, VW);\nConstruct the set R\u2081 by randomly selecting \u00ee distinct\nintegers from {1, 2, ..., R}.\nfor each i in Rt do\nM[i, j] = 1 for all j = 1, 2, . . ., R.\nend for\nfor each j in Rt do\nM[i, j] = 1 for all i = 1, 2, . . ., R.\nend for\nApply gradient mask WA \u2190 \u2207WA\u00a9 MA,\nWB \u2190 VW\u00a9 MB.\nPerform optimization step W = W \u2013 n\u2207W, W =\nW \u2013 n\u2207W.\nend for\nend for\nOutput: Updated LoRA weight matrices (WA, W) for each\nfine-tuned module.\nset Rt of distinct directions is randomly selected. The RM-LoRA\nmethod then sets the relevant entries in the gradient masks to\none according to the selected directions. These masks are ap-\nplied to the gradients to restrict the update directions. Finally,\nthe algorithm updates the weight matrices W and W using\nthe masked gradients, thus achieving the partial update of LoRA\nweight matrices. The complete process of gradient masking is\nsummarized in Algorithm 1."}, {"title": "4 Experiments", "content": "The details of the experiment for the evaluation of the proposed\nRM-LORA method are outlined as follows:\nModels and Datasets. This paper compares our proposed\nRM-LORA with the original LoRA and its recent variants across\nboth computer vision and natural language tasks. For vision\ntask, a Vision Transformer (ViT) model [32] is fine-tuned on the\nCIFAR-100 dataset. For language tasks, a DeBERTaV3 model [33]\nis fine-tuned on the General Language Understanding Evaluation\n(GLUE) benchmark for language understanding [34] and Stanford\nQuestion Answering Dataset (SQUAD 1.1) for question answering\n[35].\nBaselines. The following baselines are implemented within\nthe same HuggingFace's Transformers framework [36]. LoRA and\nits variants are all implemented using the LoRA public code-base\u00b2\nfor fair comparison:\n\u2022 Full fine-tuning (FT) uses the pre-trained model as the initial-\nization point and updates all parameters in the model through\ngradient backpropagation.\n\u2022 LoRA [6] approximates the incremental updates in pre-trained\nmodel weights by using the product of two trainable matrices\nwith rank R.\n\u2022 AdaLoRA [24] uses the product of three small matrices in the\nform of singular value decomposition to parameterize the\nupdates in pre-trained model weights, and then prunes the\nsingular values of lower importance in the diagonal matrix to\nachieve a pre-set total parameter budget b across all adapter\nweight matrices.\n\u2022 SORA [25] parameterizes the updates in pre-trained model\nweights similarly to AdaLoRA, with an additional gate unit\nin between, and controls the sparsity of the gate by prun-\ning components with absolute values lower than a pre-set\nthreshold \u03bb.\nFor a fair comparison of all the LoRA variants, including our pro-\nposed RM-LORA method, their performance is evaluated under\nthe same parameter budget during inference in the following\npart of this paper."}, {"title": "4.2 Image Classification", "content": "Figure 1 illustrates the results achieved by the ViT model on\nthe CIFAR-100 dataset, serving as a preliminary measure of the\nperformance of LoRA and the enhancement techniques for LoRA\nmethod proposed in this paper. To simulate the theoretical results\nbased on the fully connected layer, only the last classification\nlayer of the ViT model is fine-tuned by LoRA low-rank matrices.\nThe four sub-figures of Figure 1 display the test loss, test accu-\nracy, train accuracy, and generalization error (measured by train\naccuracy minus test accuracy) for each method respectively.\nAs observed in Figure 1, the regularization and gradient mask-\ning techniques proposed in this paper both effectively mitigate\noverfitting and achieve higher accuracy on the test dataset. Specif-\nically, Regularized LoRA (R-LoRA) and Gradient Masking LoRA"}, {"title": "4.3 Natural Language Understanding", "content": "The GLUE benchmark includes two single-sentence classification\ntasks (COLA, SST-2), three similarity and paraphrase tasks (MRPC,\nSTS-B, QQP), and four natural language inference tasks (QNLI,\nWNLI, MNLI, RTE). The proposed RM-LoRA method is compared\nagainst the baseline methods under multiple LoRA rank settings\nto demonstrate its superiority. Table 1 shows the performance\nachieved by different methods on GLUE tasks, as well as the\nnumber of trainable and inference parameters (# T / I - Params\nrespectively). The best result for each task is highlighted in bold.\nR-LORA with the proposed regularizer consistently achieves per-\nformance gains under the same or lower inference parameter\nbudget compared to other methods in most cases. Furthermore,\nRM-LORA with gradient masking outperforms R-LoRA in a ma-\njority of task settings. The specific fine-tuning hyperparameters\nadopted by each method on the GLUE benchmark are summa-\nrized in Table 3."}, {"title": "4.4 Question Answering", "content": "The performance of different methods using DeBERTaV3 model\non the SQUAD dataset are shown in Table 2. Similarly, the pro-\nposed RM-LORA method outperforms other baselines under the\nsame or lower inference parameter budget across varying LoRA"}, {"title": "5 Conclusion", "content": "In conclusion, the exploration of intrinsic dimension in LoRA\nfine-tuning reveals critical insights into optimizing parameter effi-\nciency and enhancing model generalization. The theoretical foun-\ndation indicates that the intrinsic dimension of the approximated\nmatrix updates is more pivotal in achieving effective LoRA fine-\ntuning than the previously emphasized LoRA rank. By employing\na regularization technique and a gradient masking method to\nencourage parameter space exploration while controlling the\ntrainable parameters budget, this paper presents an advanced\nlow-rank adaptation strategy that addresses the challenges of\nsub-optimal performance and overfitting associated with LoRA.\nThe better generalization performance achieved by the proposed\nRM-LORA under the same or lower parameter budget compared\nto other methods represents significant progress in the field of\nparameter-efficient fine-tuning for large pre-trained models."}]}