{"title": "Enhancing Parameter Efficiency and Generalization in Large-Scale Models: A Regularized and Masked Low-Rank Adaptation Approach", "authors": ["Yuzhu Mao", "Siqi Ping", "Zihao Zhao", "Yang Liu", "Wenbo Ding"], "abstract": "Large pre-trained models, such as large language models (LLMs), present significant resource challenges for fine-tuning due to their extensive parameter sizes, especially for applications in mobile systems. To address this, Low-Rank Adaptation (LoRA) has been developed to reduce resource consumption while maintaining satisfactory fine-tuning results. Despite its effectiveness, the original LoRA method faces challenges of suboptimal performance and overfitting. This paper investigates the intrinsic dimension of the matrix updates approximated by the LoRA method and reveals the performance benefits of increasing this intrinsic dimension. By employing regularization and a gradient masking method that encourages higher intrinsic dimension, the proposed method, termed Regularized and Masked LoRA (RM-LORA), achieves superior generalization performance with the same or lower trainable parameter budget compared to the original LoRA and its latest variants across various open-source vision and language datasets.", "sections": [{"title": "Introduction", "content": "Large pre-trained models, like large-scale language models (LLMs), have showcased remarkable performance across a variety of tasks in computer vision and natural language processing [1\u20134]. Nonetheless, fine-tuning these models for specific downstream tasks often presents substantial resource challenges due to their extensive parameter sizes. In this context, parameter-efficient fine-tuning (PEFT) methods have been extensively explored to alleviate resource consumption while preserving or enhancing fine-tuned model performance [5\u201319]. Among these methods, Low-Rank Adaptation (LoRA), which involves freezing the pre-trained weights and approximating updates in weight matrices using the multiplication of two low-rank matrices, has emerged as a promising approach to balance computational efficiency and task performance during the fine-tuning process of large models [6].\nDespite its effectiveness, LoRA fine-tuning encounters chal-lenges in determining the optimal size of LoRA matrices for a given model and task. On one hand, excessively small matrices, with limited number of trainable parameters, inevitably harm training convergence and generalization performance. On the other hand, large matrices introduce redundant trainable param-eters, which could be reduced to enhance parameter efficiency. Moreover, some studies have indicated that large LoRA matrices may exacerbate overfitting, as redundant parameters primarily contribute to training accuracy rather than test accuracy [20, 21].\nSeveral approaches have been proposed to determine or adap-tively adjust the size of LoRA matrices, often referred to as the LoRA rank R, for improved efficiency and generalization. How-ever, none of these methods investigate the intrinsic dimension r of the approximated matrix update \\( \\Delta W = BA \\) given by the product of low-rank LoRA matrices A and B. This intrinsic di-mension r, instead of the previously studied LoRA rank R, has been proven to play a crucial role in LoRA fine-tuning. Specifi-cally, Zeng and Lee [22] theoretically demonstrated that for fully connected neural networks, the LoRA approximation error given by an approximated update \\( \\Delta W = BA \\) with intrinsic dimen-sion r is related to the r-th singular value of the discrepancy \\( E = W_{\\text{target}} - W_{\\text{frozen}} \\) between the target weight matrix and the frozen pre-trained weight matrix.\nIn other words, encouraging the intrinsic dimension r of AW to approximate the given LoRA rank R benefits the generalization of LoRA fine-tuning under a given trainable parameter budget. Inspired by this theoretical conclusion, this paper first adopts a regularization technique to encourage LoRA matrices to span a higher intrinsic rank in their parameter space. Additionally, to maintain a reasonable budget of trainable parameters, a gradient masking method is introduced to randomly mask a subset of parameters in each epoch instead of updating all parameters in LoRA matrices. Experiments on multiple datasets have proven this method also helps promote the growth of the intrinsic rank r and thus yields lower approximation error and better general-ization performance.\nThe contributions of this paper can be summarized as follows: 1) This paper extends previous theoretical bounds for LoRA ap-proximation error from simulated datasets to real-world datasets, providing further insights into the trade-off between LoRA rank R and generalization performance. 2) Based on the analysis of LoRA rank and generalization performance, this paper designs a strat-egy for fine-tuning LoRA matrices that encourages the growth of intrinsic rank r = rank(AW) within the LoRA parameter space defined by R. This strategy effectively alleviates the problem of overfitting the training data by encouraging the LoRA matrices to explore the parameter space. 3) The experimental results across multiple open-source datasets demonstrate that this Regularized and Masked version of LoRA (RM-LoRA) method manages to strike a better efficiency-generalization tradeoff compared to the original LoRA method and its state-of-the-art variations, with better generalization performance achieved with the same or lower trainable parameters budget."}, {"title": "Related Works", "content": "In attempts to address the computational challenges posed by updating the enormous amount of weights in large pre-trained models, LoRA, proposed by Hu et al. [6], achieves outstanding model generalization with a significantly reduced budget of train-able parameters during fine-tuning. However, LoRA still faces the challenges of sub-optimal performance and overfitting. Previous research addressing LoRA's main challenges is briefly discussed as follows:\nSub-Optimal Performance. While LoRA has demonstrated remarkable parameter efficiency and generalization performance, it can lead to suboptimal fine-tuning of large-scale models with high embedding dimensions [23]. In some cases, there is a con-tradictory phenomenon where a higher LoRA rank doesn't nec-essarily bring better performance than a lower LoRA rank. Much research has been devoted to further balancing the efficiency and performance achieved by LoRA, including the adaptive choice of LoRA rank [24\u201326], adjustment of learning rate [23], random projection [27], derivative-free optimization [28], and pre-trained weights optimization [29]. Nevertheless, none of these methods consider the role of LoRA updates' intrinsic dimension in miti-gating the performance gap under a given LoRA rank setting.\nOverfitting. Fine-tuning large pre-trained models with a large number of parameters can easily encounter overfitting, resulting in suboptimal generalization, especially on test data [21]. In the AdaLoRA method, the LoRA matrices for less important pre-trained weight matrices are assigned a lower rank to prevent overfitting [24]. However, according to the experiments of Qiang et al. [20], LORA and AdaLoRA still clearly overfit the training data as fine-tuning advances, with decreases in training losses but increases in test losses. To alleviate the overfitting problem, Qiang et al. [20] developed the BiLoRA method, which iteratively trains different subsets of trainable parameters using different subsets of training data. Furthermore, Hayou et al. [23] pointed out that the suboptimality of LoRA and its variants is due to some directions of LoRA matrices not being sufficiently updated, and thus the change in model weights approximated by LoRA is restricted by the vector (sub)space generated by the LoRA matrices' columns at initialization.\nThe previous analysis of LoRA's existing limitations and solu-tions gives rise to the idea and method employed in this work. Specifically, for better performance under a given LoRA rank setting, this paper proposes a fine-tuning strategy that promotes the growth of the intrinsic dimension of LoRA updates through regularization and gradient masking, bridging the gap between practical performance and theoretical optimal performance."}, {"title": "RM-LORA Method", "content": null}, {"title": "Preliminary", "content": "Transformer Models. A transformer-based pre-trained model typically involves L stacked encoder/decoder blocks, with a multi-head attention module followed by a fully connected feed-forward network (FFN) in each block. Given an input sequence \\(X \\in \\mathbb{R}^{n \\times d}\\), the output of the multi-head attention module can be written as:\nMultiHead(X) = Concat(head\u2081, . . ., head\u0127)W\u00ba,\nwith \\( \\text{head}\\_i = \\text{Attention}(Q\\_i, K\\_i, V\\_i) \\),\nand \\( \\text{Attention}(Q\\_i, K\\_i, V\\_i) = \\text{softmax}(\\frac{Q\\_i K\\_i^T}{\\sqrt{d\\_k}})V\\_i \\)\n(1)\nwhere \\( Q\\_i = XW\\_i^Q \\), \\( K\\_i = XW\\_i^K \\), and \\( V\\_i = XW\\_i^V \\) are matrices of queries, keys, and values of head; respectively, with projec-tion matrices \\( W\\_i^Q \\in \\mathbb{R}^{d \\times d\\_k} \\), \\( W\\_i^K \\in \\mathbb{R}^{d \\times d\\_k} \\), \\( W\\_i^V \\in \\mathbb{R}^{d \\times d\\_v} \\), and \\( W^O \\in \\mathbb{R}^{h d\\_v \\times d} \\). We refer readers to [30] for a more comprehensive introduction to attention calculations in general.\nGiven the output of the multi-head attention module, the FFN further projects the d-dimensional output X' for each position. A two-layer FFN with a ReLU activation operates as follows:\nFFN(X') = max(0,X'Wf\u2081 + b1)Wf\u2082 + b2,\n(2)\nwhere \\( W\\_{f\\_1} \\in \\mathbb{R}^{d \\times d\\_m} \\) and \\( W\\_{f\\_2} \\in \\mathbb{R}^{d\\_m \\times d} \\). Moreover, a residual connection followed by layer normalization is applied to each layer to generate the output of each transformer block given the input sequence X in the following way:\nLayerNorm(X + FFN(LayerNorm(X + MultiHead(X)))).\n(3)\nLoRA Fine-tuning. For a pre-trained matrix \\( W\\_0 \\in \\mathbb{R}^{d\\_1 \\times d\\_2} \\), LoRA, as proposed by Hu et al. [6], approximates its update \\( \\Delta W \\) by \\( \\Delta W = BA \\), where \\( A \\in \\mathbb{R}^{R \\times d\\_2} \\) and \\( B \\in \\mathbb{R}^{d\\_1 \\times R} \\) with rank \\( R < \\text{min}(d\\_1, d\\_2) \\). During model fine-tuning, the weight matrix \\( W\\_0 \\) is frozen, with only the LoRA adapters A and B being trainable. The modified LoRA forward pass is:\n\\( h = W\\_0 x + \\Delta W x = W\\_0 x + BAx. \\)\n(4)\nTypically, the low-rank matrice A is initialized using a random Gaussian distribution, while B is initialized to zero, ensuring \\( \\Delta W = 0 \\) at the start of fine-tuning. Current approaches to fine-tuning large pre-trained models with LoRA apply a pair of matri-ces to all weight matrices involved in each transformer block's multi-head attention module and FFN [24, 31].\nThe Expressive Power of LoRA. Zeng and Lee [22] investi-gates the LoRA approximation error under a mild non-singularity assumption. To begin with, a L-layer width-D fully connected ReLU neural network is denoted as \\( FNN\\_{L,D}(\\cdot; (W\\_l)\\_{l=1}^L, (b\\_l)\\_{l=1}^L) \\), where \\( W\\_l \\in \\mathbb{R}^{D \\times D} \\) are the weight matrices and \\( b\\_l \\in \\mathbb{R}^{D} \\) are"}, {"title": "Influence of the Intrinsic Dimension of LoRA Adapter \u25b3W", "content": "Note that the partition P\u2081 of the pre-trained model for the i-th layer in the target model is an intrinsic but unknown property during adaptation, and consequently, the number and index of layers \\( l \\in P\\_i \\) are also unknown. Nevertheless, with a pre-trained model \\( f\\_0 \\) to be adapted and the target model \u0192 determined by a given downstream task, the partition can be considered determin-istic, as can the discrepancy between the pre-trained model and the target model \\( E\\_i = W\\_i - \\prod\\_{l\\in p\\_i} W\\_l \\). Consider the case where the LoRA rank setting for each layer \\( l \\in P\\_i \\) is the same as R. The \\( e\\_i \\) term in Equation 8 can be rewritten as:\n\\( e\\_{i,\\text{rank}(\\Delta W\\_{l\\in P\\_i})<R} = \\sigma\\_{\\Sigma\\_{l\\in p\\_i} R+1}(E\\_i) \\) for each layer \\( i \\in [\\hat{L}] \\),\n(9)\nwhere \\( \\text{rank}(\\Delta W\\_{l\\in P\\_i}) < R \\) represents the LoRA adapter for each layer \\( l \\in P\\_i \\) satisfies the rank constraint \\( \\text{rank}(\\Delta W\\_l) < R \\).\nClearly, increasing \\( \\text{rank}(\\Delta W\\_l) \\) helps relax the constraint on LoRA rank R to achieve a certain level of approximation error. For example, consider two LoRA rank settings R\u2081 and R\u2082 with R\u2081 < R\u2082. If \\( \\text{rank}(\\Delta W\\_l) < R\\_1 < R\\_2 \\), then \\( e\\_{i,\\text{rank}(\\Delta W\\_{l\\in P\\_i})<R\\_2} \\) degenerates to \\( e\\_{i,\\text{rank}(\\Delta W\\_{l\\in P\\_i})<R\\_1} \\) for \\( i \\in [\\hat{L}] \\), despite the larger size of LoRA matrices under the LoRA setting of R\u2082. In this case, LoRA rank R\u2081 and R\u2082 yield the same LoRA approximation error \\( E||f(x)-f(x)||\\_2 \\) according to Equation 8."}, {"title": "Regularization on LoRA Weights", "content": "Let a pair of LoRA low-rank matrices be denoted as \\( W^A \\) and \\( W^B \\), respectively. To enforce the growth in rank of \\( \\Delta W = W^B W^A \\), the following regularizer is first used to encourage \\( W^A \\) and \\( W^B \\) to be orthogonal:\nReg(WA, WB) = ||WA(WA)T \u2212 I|| + ||(WB)TWB \u2212 I||.\n(10)\nThe orthogonality of \\( W^A \\) and \\( W^B \\) helps increase the rank(\\( W^A \\)) and rank(\\( W^B \\)). According to the lower bound for the rank of the matrix product, for matrices \\( A \\in \\mathbb{R}^{R \\times d\\_2} \\) and \\( B \\in \\mathbb{R}^{d\\_1 \\times R} \\), the rank of their product matrix \\( C = BA \\) satisfies rank(C) \u2265 max(rank(A) + rank(B) \u2212 R, 0). This lower bound ensures the growth of the intrinsic rank of the LoRA adapter \\( \\Delta W = W^A W^B \\) as the rank(\\( W^A \\)) and rank(\\( W^B \\)) increase with the regularizer shown in Equation 10. Note that there exist other alternative regularizers that theoretically can also encourage the growth of rank(\\( \\Delta W \\)), but are infeasible in reality due to considerations of differentiability, numerical stability, and computational costs\u00b9."}, {"title": "Gradient Masking for Partial Updates", "content": "The gradient masking algorithm in RM-LoRA is designed to per-form partial updates in LoRA matrices. The algorithm takes as input the total number of steps T, the LoRA rank R, and the num-ber of directions to update in each step. In each training step t, it samples a mini-batch of data \u00a7 and computes the gradients \\( \\nabla W^A \\) and \\( \\nabla W^B \\) for each pair of LoRA weight matrices. The corresponding gradient masks are first initialized to zero, before a"}, {"title": "Experiments", "content": null}, {"title": "Experimental Setup", "content": "The details of the experiment for the evaluation of the proposed RM-LORA method are outlined as follows:\nModels and Datasets. This paper compares our proposed RM-LORA with the original LoRA and its recent variants across both computer vision and natural language tasks. For vision task, a Vision Transformer (ViT) model [32] is fine-tuned on the CIFAR-100 dataset. For language tasks, a DeBERTaV3 model [33] is fine-tuned on the General Language Understanding Evaluation (GLUE) benchmark for language understanding [34] and Stanford Question Answering Dataset (SQUAD 1.1) for question answering [35].\nBaselines. The following baselines are implemented within the same HuggingFace's Transformers framework [36]. LoRA and its variants are all implemented using the LoRA public code-base\u00b2 for fair comparison:"}, {"title": "Image Classification", "content": "Figure 1 illustrates the results achieved by the ViT model on the CIFAR-100 dataset, serving as a preliminary measure of the performance of LoRA and the enhancement techniques for LoRA method proposed in this paper. To simulate the theoretical results based on the fully connected layer, only the last classification layer of the ViT model is fine-tuned by LoRA low-rank matrices. The four sub-figures of Figure 1 display the test loss, test accu-racy, train accuracy, and generalization error (measured by train accuracy minus test accuracy) for each method respectively.\nAs observed in Figure 1, the regularization and gradient mask-ing techniques proposed in this paper both effectively mitigate overfitting and achieve higher accuracy on the test dataset. Specif-ically, Regularized LoRA (R-LoRA) and Gradient Masking LoRA"}, {"title": "Natural Language Understanding", "content": "The GLUE benchmark includes two single-sentence classification tasks (COLA, SST-2), three similarity and paraphrase tasks (MRPC, STS-B, QQP), and four natural language inference tasks (QNLI, WNLI, MNLI, RTE). The proposed RM-LoRA method is compared against the baseline methods under multiple LoRA rank settings to demonstrate its superiority. Table 1 shows the performance achieved by different methods on GLUE tasks, as well as the number of trainable and inference parameters (# T / I - Params respectively). The best result for each task is highlighted in bold.\nR-LORA with the proposed regularizer consistently achieves per-formance gains under the same or lower inference parameter budget compared to other methods in most cases. Furthermore, RM-LORA with gradient masking outperforms R-LoRA in a ma-jority of task settings. The specific fine-tuning hyperparameters adopted by each method on the GLUE benchmark are summa-rized in Table 3."}, {"title": "Question Answering", "content": "The performance of different methods using DeBERTaV3 model on the SQUAD dataset are shown in Table 2. Similarly, the pro-posed RM-LORA method outperforms other baselines under the same or lower inference parameter budget across varying LoRA rank settings, with EM denoting the average exact match score and F1 referring to the average F1 score. These results highlight that the RM-LORA method consistently improves LoRA's fine-tuning performance across various benchmarks. The capability of achieving better or comparable performance with reduced pa-rameter budget is especially significant for practical deployment in mobile systems, where efficiency and resource utilization are crucial factors."}, {"title": "Conclusion", "content": "In conclusion, the exploration of intrinsic dimension in LoRA fine-tuning reveals critical insights into optimizing parameter effi-ciency and enhancing model generalization. The theoretical foun-dation indicates that the intrinsic dimension of the approximated matrix updates is more pivotal in achieving effective LoRA fine-tuning than the previously emphasized LoRA rank. By employing a regularization technique and a gradient masking method to encourage parameter space exploration while controlling the trainable parameters budget, this paper presents an advanced low-rank adaptation strategy that addresses the challenges of sub-optimal performance and overfitting associated with LoRA. The better generalization performance achieved by the proposed RM-LORA under the same or lower parameter budget compared to other methods represents significant progress in the field of parameter-efficient fine-tuning for large pre-trained models."}]}