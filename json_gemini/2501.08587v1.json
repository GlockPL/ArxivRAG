{"title": "SOUND SCENE SYNTHESIS AT THE DCASE 2024 CHALLENGE", "authors": ["Mathieu Lagrange", "Junwon Lee", "Modan Tailleur", "Laurie M. Heller", "Keunwoo Choi", "Brian McFee", "Keisuke Imoto", "Yuki Okamoto"], "abstract": "This paper presents Task 7 at the DCASE 2024 Challenge: sound scene synthesis. Recent advances in sound synthesis and generative models have enabled the creation of realistic and diverse audio content. We introduce a standardized evaluation framework for comparing different sound scene synthesis systems, incorporating both objective and subjective metrics. The challenge attracted four submissions, which are evaluated using the Fr\u00e9chet Audio Distance (FAD) and human perceptual ratings. Our analysis reveals significant insights into the current capabilities and limitations of sound scene synthesis systems, while also highlighting areas for future improvement in this rapidly evolving field.", "sections": [{"title": "1. INTRODUCTION", "content": "This paper presents Task 7 at the DCASE 2024 Challenge: sound scene synthesis. The challenge is motivated by the recent advances generative models for the creation of realistic and diverse audio content, as proposed in [1] and following the last year's version [2]."}, {"title": "2. PROBLEM AND TASK DEFINITION", "content": "We defined the challenge as a text-to-sound generation task, where systems must generate realistic environmental audio based on textual descriptions. This is a more flexible setup than the category-based generation used in the last year [2].\nEach prompt follows the following structure:\n\"Foreground with Background in the background,\"\nto specify both the primary sound source and its acoustic context separately.\nKey constraints for the generated audio include:\n\u2022 4-second 16-bit mono audio snippets at 32 kHz sampling rate\n\u2022 No music allowed in the generated audio\n\u2022 No intelligible speech permitted\nThe task emphasizes generative approaches rather than retrieval-based methods, requiring systems to synthesize novel audio rather than simply copy existing samples."}, {"title": "3. DATASET AND BASELINE", "content": ""}, {"title": "3.1. Dataset Creation", "content": "The challenge dataset contains 310 audio-captions in total, with 60 samples designated for development and 250 for evaluation. All audio content was carefully designed by a sound engineer to match specific prompts, ensuring high-quality and consistent sound scenes. The audio samples were sourced from Freesound.org and the private libraries Rabbit Ears Audio, Sound Ideas, Euro S Phere, The Art Of Foley, BBC Sound Effect, and HissAndARoar, all of which were selected following strict quality guidelines."}, {"title": "3.2. Sound Categories", "content": "The dataset organized sound content into two main categories: foreground and background sounds. The foreground category encompasses six distinct types of sounds: Animal, Vehicle, Human, Alarm, Tool, and Entrance sounds. These were chosen to represent a diverse range of common sound sources in everyday environments. For background sounds, the dataset includes five categories: Crowd, Traffic, Water, Birds, and Room Tone sounds. This categorization enables the creation of realistic sound scenes with clear foreground-background relationships."}, {"title": "3.3. Baseline System", "content": "For the baseline implementation, we utilized AudioLDM [3] as the core synthesis engine. The system was trained on a comprehensive collection of audio datasets including AudioCaps [4], AudioSet [5], FreeSound\u00b2, and BBC Sound Effect datasets\u00b3. This diverse training data ensures the baseline system can handle a wide range of sound types and acoustic environments represented in our challenge."}, {"title": "4. EVALUATION METHODOLOGY", "content": ""}, {"title": "4.1. Objective Evaluation", "content": "We employed the Fr\u00e9chet Audio Distance (FAD) [6] with PANN-Wavegram-Logmel [7] embeddings as our primary objective metric. The embedding was chosen to maximize the correlation between the FAD score and the human perception [8]. The FAD computation is defined as:\nFAD(r, g) = ||\u03bcr \u2013 \u03bcg||\u00b2 + Tr(\u03a3r + \u03a3g \u2013 2\u221a\u03a3r\u03a3g)   (1)\nIn this formulation, r and g represent the reference and generated audio sets respectively, with their corresponding mean feature vectors \u03bcr, \u03bcg and covariance matrices \u03a3r, \u03a3g. This metric effectively captures both the statistical similarities between the generated and reference audio distributions and their feature relationships. We provided an official evaluation software."}, {"title": "4.2. Subjective Evaluation", "content": "Our subjective evaluation framework assessed three key aspects of the generated audio, each rated on a 0-10 scale. The Foreground Fit (FF) measured the accuracy of the primary sound source, while Background Fit (BF) evaluated the appropriateness of the ambient sound. Overall Audio Quality (AQ) captured the perceptual quality of the generated audio. We computed a weighted final perceptual score that emphasized foreground accuracy while balancing background fit and audio quality:\nPerceptual Score = $\\frac{2FF + BF + AQ}{4}$     (2)"}, {"title": "5. RESULTS", "content": ""}, {"title": "5.1. System Performance", "content": "Table 1 summarizes the evaluation results. The evaluation process encompassed four submitted systems [9, 10, 11, 12] assessed by a\nRoom tone is a recorded sound with no specific sound event and used to capture natural noise of a recording environment."}, {"title": "5.2. Analysis", "content": "Our comprehensive analysis revealed several key findings from the results shown in Table 1 and Figure 1. Most notably, we observed a substantial 36% performance gap between the sound engineer reference and the best submitted system, indicating significant room for improvement in synthetic audio quality. We found a strong correlation between objective FAD scores and subjective metrics, with a correlation coefficient of 0.94 for foreground fit, 0.94 for background fit, and 0.77 for audio quality, meaning that most, but not all, of the variance is shared between our objective and subjective measures. However, this result should be considered as a weak evidence for the effectiveness of FAD as a perceptual measure, since the number of data points is small.\nFor more detailed analysis, please refer to another paper of ours, Lee et. al. [13], since we focus on reporting the challenge result in this paper."}, {"title": "5.3. Participation Analysis", "content": "The challenge saw a notable decrease in participation compared to the previous year [2], with 4 submissions in 2024 versus 32 in 2023. This reduction may be attributed to several factors: the broader task scope that favored teams with access to large pre-existing models, the removal of training dataset constraints that previously encouraged wider participation, changes in task naming and framing that may have affected its appeal, and the introduction of more complex evaluation requirements that increased the entry barrier for potential participants."}, {"title": "6. DISCONTINUATION OF THE TASK", "content": "It is worth mentioning why the organizers decided not to continue the DCASE challenge in 2025 despite the successful challenges in 2023 and 2024.\nFirst, the generative aspect of organizing this challenge has been costly and labor intensive. In this year's challenge, it took (a) about 40 hours to create and refine the evaluation set of sounds created by a sound designer, (b) about 40 hrs to debug and run the code for 4 participants and (c) about 40 hours to conduct the perceptual evaluation, including organizer time to create and analyze the rater survey plus rater time (raters listed in the acknowledgments). Those efforts were specific to the generative aspect of the task and were therefore in addition to our other efforts that are a typical part of organizing a classification task, such as collating audio samples and making a baseline system. Furthermore, since DCASE results are announced shortly after participants' systems are submitted, there was very little time for the extra steps of generating sounds from each submitted system and having raters perceptually evaluate them. In addition, there were financial costs for system evaluation, such as running participant-supplied code in Colab.\nSecond, the nature of generative audio has been evolving over the past two years. When first organizing this challenge, our problem formulation was specific so that we could not only evaluate systems, but also explain the results and compare the sound qualities from year to year. However, this meant the scope of our evaluation was narrower than those of many existing models. Currently, the overall scope of the academic community is expanding and diversifying while the topic of each paper becomes more specialized, e.g., by focusing on temporal alignment [14, 15, 16, 17], high-fidelity audio, foundational models for general sound generation [18, 19, 20, 21, 22, 23], video-as-an-input [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], etc."}, {"title": "7. CONCLUSION", "content": "The DCASE 2024 Challenge Task 7 has provided valuable insights into the current state of sound scene synthesis while highlighting several crucial areas for future development. While the submitted systems demonstrated promising capabilities, the significant gap between synthetic and reference audio quality indicates substantial room for improvement. Future developments should focus on implementing more complex caption structures, supporting multiple foreground sounds, and enhancing synthesis techniques to reduce the quality gap. Additionally, the development of more sophisticated evaluation metrics for sound scene coherence will be crucial for advancing the field.\nLooking ahead, we envision several key directions for the challenge's evolution. These include expanding the complexity of sup-"}]}