{"title": "Knowledge AI: Fine-tuning NLP Models for Facilitating Scientific Knowledge Extraction and Understanding", "authors": ["Hayden Beadles", "Kalyan Sashank Mupparaju", "Balaji Muralidharan", "Mahmoodreza Marzban"], "abstract": "This project investigates the efficacy of Large Language Models (LLMs) in understanding and extracting scientific knowledge across specific domains and to create a deep learning framework: Knowledge AI. As a part of this framework, we employ pre-trained models and fine-tune them on datasets in the scientific domain. The models are adapted for four key Natural Language Processing (NLP) tasks: summarization, text generation, question answering, and named entity recognition. Our results indicate that domain-specific fine-tuning significantly enhances model performance in each of these tasks, thereby improving their applicability for scientific contexts. This adaptation enables non-experts to efficiently query and extract information within targeted scientific fields, demonstrating the potential of fine-tuned LLMs as a tool for knowledge discovery in the sciences.", "sections": [{"title": "1. Introduction/Background/Motivation", "content": "Large Language Models (LLMs) are increasingly utilized across various Natural Language Processing (NLP) tasks. Our project focuses on developing a deep learning-based framework designed to make scientific text accessible to a general, non-scientific audience. [21] We investigate this by developing an Artificial Intelligence (AI) framework comprised of several NLP tasks. This framework allows individuals to ask questions, identify relationships within the text, and reason about the scientific content without requiring specialized scientific knowledge. [6] We leverage the capabilities of existing LLMs to perform these NLP tasks effectively on scientific data. We fine-tune these models on specific NLP tasks using relevant scientific data. The four core NLP tasks we focus on this study are: text generation, Question and Answer (Q & A), summarization and Named Entity Recognition(NER).\nThe emergence of LLMs has fundamentally transformed the way information is processed and disseminated. These powerful Al models possess an unparalleled ability to analyze and understand vast amounts of data, revolutionizing both information retrieval and content creation. However, within the realm of scientific research, a critical gap exists a lack of accessible tools that effectively bridge the communication divide between researchers and the broader public. [4] Scientific research often generates complex findings and specialized knowledge that can be challenging for non-experts to grasp. This hinders the dissemination of scientific ideas and discoveries to the general public, limiting their potential impact and hindering broader societal understanding. To address this challenge, there is an urgent need for a unified AI framework specifically designed for scientific tasks. Such a framework would leverage the capabilities of LLMs to enable users to simplify the inference of scientific data, facilitate scientific communication, and increase the accessibility of scientific knowledge. While there are attempts to create open, accessible frameworks, no unified, dedicated framework has been generally accepted. [6]\nGovernment agencies like the Department of Energy (DOE) recognize the potential of AI in facilitating scientific progress, as evidenced by their recent calls for proposals to build tools for interdisciplinary research [2]. We discovered only recently that Meta AI also released an LLM, Galactica in 2022[24] specifically designed for scientific tasks. The motivations for the models were similar in spirit to our goals of knowledge extraction and dissemination. However, due to critical feedback, the Galactica model was taken down shortly after it was unveiled. This highlights the challenges associated with using LLMs for scientific tasks. [9] Despite these challenges, we believe that with further development, our AI tool can address some of the key issues related to scientific information inference.\nThe details related to the model and relevant datasets are discussed in the next section along with a brief description of our technical approach related to the four NLP tasks."}, {"title": "2. Approach", "content": "2.1. Task Overview\nIn the summarization task, advanced models are fine-tuned using an Adaptive Tokenization Strategy to address challenges in scientific text summarization. This approach leverages robust sequence-to-sequence capabilities and the ability to manage long documents, enhancing the accuracy and coherence of the summaries. Further details and comparisons are provided in section 3.1.\nText generation explores additional techniques such as Low Rank adaptations in training causal language models, and compares and evaluates models that utilize these newer techniques against fine-tuned models in causal language generation. This is discussed in section 3.2.\nQuestion answering explores Extractive and Abstractive QA by testing extractive k-shot learning with fine-tuned and generative models, as well as abstractive QA to better understand the model's latent ability to answer short-form questions. Extractive QA results are explored 3.3 and Abstractive QA 3.4.\nFinally, NER examines how well, fine-tuned models can perform token classification of named entities on scientific datasets of increasing complexity. For NER, we compare and contrast the performances of the models in that area. One important aspect that we discuss in the results section is the influence of class imbalance and its correlation with the dataset size for NER performance. The results are discussed in 3.5.\n2.2. Code / Data\nSummarization and text generation utilize the scientific papers dataset from the Hugging Face [18] to help generate and fine-tune models on scientific data. Summarization examines fine-tuned BART and LED models [11] [16].\nText generation uses distilgpt2 [13]. For text generation, we adapted the training framework from Hugging Face's language modeling notebook [15] to our particular task. We modified the original code to include LoRA adapters[10].\nQuestion answering fine-tunes two sets of models, BERT and SciBERT are fine-tuned on SQUAD for Extractive [22], and PubMedQA for Abstractive [17], on the question and long_answer columns, which reflect short form question / answer pairs. For abstractive, SciBERT and BERT are tuned with the EncoderDecoder class in hugging face. [14], which allows them to perform generative tasks.\nNER fine-tunes BERT, SciBERT, and SciDeBERTa against a set of increasing science-focused NER datasets, CONLL2003, SciERC and GENIA. [12] [19] [23]\nOur models use the hugging face set of libraries to build, fine-tune models, extract datasets, and evaluate them on the selected task. The code used to deploy these models and fine-tune them are stored in the Georgia Tech Github organization here: [1]. The code is also included as supplemental material to this paper.\nAs mentioned, we utilized tools like PEFT and LORA to further explore the refinement of models and also tried multiple approaches in data collection, fine-tuning preparation, and worked through several iterations of work in each task. These challenges we encountered, and worked through, are discussed in more detail in section 4.\n2.3. Metrics\nWe employ different metrics to assess the finetuning and model performance on the respective tasks. For text generation, summarization, and Q & A, we measure the various ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores. Specifically, we assess the three commonly used ROUGE metrics: ROUGE-1, ROUGE-2, and ROUGE-L. For summarization, we also employ the 'METEOR' metric, which offers a more nuanced evaluation of the summary quality. A human evaluation criterion is used in addition to the ROUGE scores for the Q & A task to better represent the model's capability to provide scientifically relevant answers. This is a simple score between 0 and 1. Finally, for the NER task, we employ the standard F1, recall, and precision metrics computed based on the total number of predicted entities, the total number of ground truth entities, and the total number of correct entities. Per-class measures of the F1, recall, and precision metrics were also used to determine the NER class-wise performance."}, {"title": "3. Experiments and Results", "content": "3.1. Summarization\nIn line with 2.2, our project centers on deploying and fine-tuning the BART and LED models on a curated scientific article dataset. BART, renowned for its bidirectional encoder and auto-regressive decoder, excels in summary generation, while LED, an extension of the Longformer model, efficiently handles lengthy documents. Fine-tuning offers advantages like task-specific optimization, data efficiency, and high performance. In addressing the challenge of training with limited computational resources on the extensive Arxiv dataset, comprising 1.7 million articles, we sampled 2% (around 34,000 articles) to manage a representative subset effectively. We implemented a dynamic tokenization strategy to boost training efficiency, focusing on the top 500 significant tokens from each article within our sampled subset. This streamlined approach reduces computational load and sequence lengths, using token frequency analysis to accelerate training cycles while maintaining focus on relevant content. It effectively addresses challenges like vanishing gradients, enhancing model generalization, and capturing crucial thematic elements in scientific text.\nThe evaluation of BART and LED models provides valu-"}, {"title": "3.2. Text Generation", "content": "In this part of our project, we focused on enhancing the text generation capabilities of a transformer-based language model, specifically using the distilgpt2 model for its computational efficiency. From the ArXIV dataset we sampled 10,000 training papers, 1,000 validation, and 1,000 test papers, processed into 256-token segments for causal language modeling. We employed two fine-tuning methods: full fine-tuning and Parameter-Efficient Fine-Tuning (PEFT) with LoRA adapters(Low-Rank Adaptation of LLMs) [10]. Full fine-tuning was expected to deeply integrate the scientific terminology and style into the model due to the comprehensive update of all parameters. In contrast, PEFT aimed to offer significant performance enhancements with less computational overhead by updating only specific low-rank matrices within the model's architecture.\nWe set up our experiments to generate text continuations from scientific paper prompts. Success was measured using ROUGE scores to quantitatively assess the overlap between the generated text and ground truth texts from scientific papers. As seen in Table 2, the full fine-tuning method resulted in higher ROUGE scores, indicating better performance in generating accurate and relevant text. PEFT also improved performance over the baseline but to a lesser extent. Despite the lower improvement, PEFT's efficiency in resource use was a notable advantage.\nQualitatively, we evaluated the coherence and contextual relevance of the generated texts. Examples of model-generated text are in the appendix section C for comparison. Our results show that full fine-tuning yielded the most coherent and contextually appropriate text. Although PEFT was slightly less effective, it still marked an improvement over the baseline and used significantly fewer resources. Both methods successfully enhanced the model's capability to generate scientific text, confirming our hypothesis about the benefits of fine-tuning."}, {"title": "3.3. Extractive QA - K-Shot Results", "content": "As shown in table 3, as the number of K-shot examples provided to the models gets smaller, the importance of strong pre-training in a relevant domain via the base model becomes evident. Of all these, SciBERT performs the strongest throughout K-shot scenarios, while BERT drops off in performance. This indicates that SciBERT has more of an understanding of the underlying subject, than simply gaming out the Question / Answer strategy for the task. [5]\nGenerative models show a similar story. (see table 3). Of all the models, T5 does the weakest. It is clear that the base model is not trained in a way where it can deduce the answer or handle the task. FLAN-T5 is strong, able to maintain strong scores in both ROUGE and correctness. The pre-training done on the FLAN-T5 model has enabled it to tackle the Extractive QA task without fine-tuning. [20]\nOverall, when formed on scientific data, models pre-trained on scientific data, and then fine-tuned on the QA task, seem to hold the most consistency and relevance in extracting the answer from the provided context. Please visit Appendix section B for more background on the task."}, {"title": "3.4. Abstractive QA - Generative Results", "content": "As discussed in 2.2, we fine-tuned two Encoder / Decoder versions of Bert and SciBERT using huggingface and trained them on PubMedQA. BioGPT and BART are community models trained on the question and final_answer, which contain either yes or no. Can these models adapt to short QA pairs? T5 is included as a baseline.\nThe results of testing these models on PubMedQA can be seen in Fig.1. Two images are shown. The image on the left contains the metric performance of various models, and the one on the right shows a sample performance of the models where we manually reviewed each provided answer for correctness. To view comparison in abstractive model output, please go here D.\nThe results suggest that, while BART and BioGPT appear to perform well on ROUGE and METEOR 1, they cheat in their answers; repeating the question in the answer. This most likely has to do with the shorter max length parameter in these models' training.\nFine-tuned SciBERT and BERT models show a better understanding of medical domain questions, with SciBERT, in particular, demonstrating strong performance and evidence of transfer learning from its extensive pre-training on scientific papers. In the referenced figure, SciBERT does very well on the sample of questions we evaluated (10 questions from the test set) 1.\nAdditionally, it shows the flexibility of a model like SciBERT in pre-training and fine-tuning. Using the model in a generative form (via the EncoderDecoder class in hugging face), still retains the models ability to form strong answers on difficult, nuanced topics."}, {"title": "3.5. Named Entity Recognition", "content": "Named Entity Recognition (NER) is an NLP task of classifying words present in an input text as belonging to different labeled entities. Principally, NER is a token classification task that associates various text tokens to different classes. NER could be a powerful technique to parse through scientific text and extract the various associations related to specific scientific concepts. Extracting the specific salient information from a list of papers would be very useful to filter and identify only the relevant papers for further study. Our goal for this project task is to perform NER of scientific text using pre-trained LLMs. We finetune various pre-trained LLMs using scientific datasets and assess their performance. This approach allows us to investigate the effectiveness of different LLM architectures and their ability to adapt to the specific domain of scientific language. We perform fine-tuning for NER tasks using three pre-trained LLMs: BERT, SciBERT and SciDeBERTa. Since all the LLMs are 'Encoder' only architecture, for NER, we use the same baseline architecture with an additional output layer on top of the BERT/SciBERT/SciDeBERTa models. The token classification head applies a linear layer followed by a softmax activation function. The number of output units in the final layer corresponds to the number of token labels present in the NER task.\nAs reported earlier, three datasets are used for NER: ConLL2003, SciERC, and GENIA. The only hyperparameters for the finetuning are the learning rates used for optimization, batch size, and the number of epochs. For the SciERC dataset, all the finetuning cases were run with a learning rate of 2e-5, for 10 epochs. For the ConLL2003 and GENIA datasets, the fine-tuning was performed for 5 epochs. A batch size of 32 was used for the SciERC dataset and 16 was used for the other two datasets. The F1, precision and recall scores based on the validation datasets for BERT, SciBERT and SciDeBERTa are reported in Table. 4.\nThe observations from the finetuning results can be summarized as follows: For the general dataset, BERT outperforms SciBERT, as expected due to BERT's pre-training on"}, {"title": "4. Experience/Challenges", "content": "In this section, we discuss the various observations, our experiences related to fine-tuning large language models, and the associated challenges. Our findings are grouped into categories related to training, data size, and computational resource requirements:\n4.1. Optimizing Training Efficiency on Large Scientific Datasets\nIn conjunction with the Adaptive Tokenization Strategy discussed in Section 3.1, we utilized parallel processing during the preprocessing stage. The tokenizer efficiently processed articles and summaries by harnessing the power of up to 16 CPU cores for parallel execution. This method notably slashed preprocessing times, facilitating quicker training and more proficient management of the extensive dataset.\n4.2. Challenges with BART in Long Document Summarization\nIn our initial approach to document summarization, we anticipated that BART would be effective in producing high-quality summaries. However, we encountered significant limitations related to BART's maximum input token constraint, which hindered its ability to process longer documents effectively. This challenge compelled us to explore alternative models designed to handle extensive texts. Our research led us to Longformer[3] and LED (Longformer Encoder-Decoder), which are specifically tailored for processing longer documents by extending the maximum input length far beyond what BART can manage. The implementation of these models was not straightforward and required adjustments in our preprocessing and model configuration to effectively utilize their extended capabilities. Although this shift was not part of our original plan, it ultimately provided a viable solution to the token limitation issue, illustrating the importance of adaptability in managing large-"}, {"title": "4.3. Efficiency and Practicality of LoRA in Model Adaptation", "content": "LORA (Low-Rank Adaptation) introduces low-rank matrices to the self-attention mechanisms of distilGPT-2, altering only 0.36% of the parameters [10]. Initially, we anticipated significant reductions in training time due to fewer trainable parameters. However, the adapted model still required about 70% of the time needed for full fine-tuning. This lesser-than-expected efficiency can be attributed to the computational complexity of self-attention layers, challenges in integrating LoRA with the existing model structure, and the deep network backpropagation requirements. Moreover, hardware and data processing efficiencies critically influenced training times. Despite these challenges, LORA proved substantially efficient in parameter utilization and adaptability with constrained resources. The complexities of transformer architectures and the demands of modifying attention mechanisms, however, moderated the time savings. Future efforts could enhance efficiency by optimizing data handling, computational pathways in adapted layers, and improving hardware utilization, aiming to maximize the gains from LoRA adaptations [10]."}, {"title": "5. Conclusions", "content": "We have presented in this study Knowledge-AI, a deep learning framework designed to extract and comprehend content from scientific texts, making it more accessible to a wider audience. We leverage the power of large language models by fine-tuning them for specialized NLP tasks tailored to the scientific domain. Focusing on four critical NLP tasks \u2013 text generation, summarization, question-and-answering, and named entity recognition \u2013 we demonstrate the effectiveness of fine-tuning LLMs like BART, BERT, and SciBERT for scientific text analysis. We have summarized some of the key findings related to each of the four NLP tasks:\n\u2022 Summarization: When summarizing scientific papers, fine-tuning BART and LED models is highly effective. These models excel at capturing fine details, aided by crucial tokenization for training with limited resources. For clear, concise summaries suited to general audiences, opt for the fine-tuned BART model, while the LED model handles more comprehensive content. Our study emphasizes the importance of selecting the right model for effective scientific communication.\n\u2022 Text Generation: Our experiments demonstrate that full fine-tuning outperforms Parameter-Efficient Fine-Tuning (PEFT) with LoRA adapters in generating text from scientific domains, as shown by higher ROUGE scores. Although PEFT offers substantial computational savings, reflecting a reduced number of trained parameters, it still effectively enhances model performance over the baseline. This highlights PEFT as a viable alternative for scenarios where resource efficiency is prioritized, suggesting potential areas for future optimization to bridge the performance gap with full fine-tuning.\n\u2022 Question / Answering: Domain pre-training makes a significant impact in K-Shot scenarios for Extractive QA, allowing fine-tuned models like SciBERT strength as representative examples decrease. Additionally, FLAN-T5 shows that generative models can perform well. In Abstractive QA, SciBERT Full Transformer was able to directly answer the short questions, reflecting the interior strength of its training in generation and question answering. Inconsistent behavior was found in the other models\n\u2022 NER: Domain-specific pre-training provides a significant advantage in the case when the data size is low. However, as the dataset size increases, even models pre-trained with general text, such as BERT, start to perform increasingly well. Similarly, class imbalance issues, which are accentuated in the case of low dataset size, diminish with an increase in data size.\nOverall, this study lays a strong foundation for the continued integration of NLP advancements into scientific knowledge dissemination."}]}