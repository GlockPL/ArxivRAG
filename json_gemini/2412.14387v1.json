{"title": "Clinical Trials Ontology Engineering with Large Language Models", "authors": ["Berkan \u00c7ak\u0131r"], "abstract": "Managing clinical trial information is currently a significant challenge for the medical industry, as traditional methods are both time-consuming and costly. This paper proposes a simple yet effective methodology to extract and integrate clinical trial data in a cost-effective and time-efficient manner. Allowing the medical industry to stay up-to-date with medical developments. Comparing time, cost, and quality of the ontologies created by humans, GPT3.5, GPT4, and Llama3 (8b & 70b). Findings suggest that large language models (LLM) are a viable option to automate this process both from a cost and time perspective. This study underscores significant implications for medical research where real-time data integration from clinical trials could become the norm.", "sections": [{"title": "1 Introduction", "content": "Clinical trials are essential for the medical industry to give medical practitioners and their patients access to the latest developments in the medical field. However, the number of clinical trials has long outpaced the medical industry's ability to effectively process and incorporate these findings in their practices [4]. Recent advancements in transformer-based artificial intelligence (AI) models, such as LLMs, along with the development of ontologies, allow for clinical trials to be processed in an automatic and real-time fashion that can be easily accessed by medical practitioners.\nOntologies have gained significant interest from the academic world [6] and industry [9] due to their ability to link information by reasoning logic, which is not possible with other database structures such as SQL. Ontologies thus allow for \"conceptualization\" in a shared fashion [15] by making it relative easy to combine different ontologies. Other benefits of ontologies, particularly when combined with LLM's include alleviating the drawbacks of LLMs through explicit knowledge representation, domain-specific knowledge, and interpret-ability/transparency.\nRecent advances in Natural Language Processing (NLP) through the use of LLM models [29] have resulted in models such as BERT [11,5,27], GPT3 [7], GPT4 [8] and Llama [2] that allow one to easily process vast amounts of text with just a prompt. These transformer models whilst extremely impressive are inherently random in their responses to prompts. Where the same prompt can result in significantly different responses [8]. Transformer models are also prone to give factual incorrect information, also known as \"hallucinations\" [17]. Which is mitigated in this study by directly using the outcomes of the clinical trials in the LLM prompt. As well as including an example of how the ontology should look like in the prompt.\nThis paper will compare the OQuaRE performance, time, and cost of GPT3.5, GPT4, and Llama3 (8b & 70b) created ontologies with that of a pre-built golden-standard ontology made by a human layman. Alongside proposing a specialised novel methodology that can be used for ontology merging in the context of clinical trial outcomes with a time complexity of O(n). Findings in Table 1 show significant reduction in both cost and time. Demonstrating practical use of current state-of-the-art LLMs in processing clinical trials in an automated way, with GPT4 in particular showcasing performance near human performance.\nThe remainder of this paper is structured as follows: section 2 reviews related work on how simple machine learning techniques are being replaced by LLMs in the context of Natural Language Processing (NLP) and ontology matching. section 3 shows a high-level overview of the methodology, after which describing methodologies and tools used to create and compare the clinical trials. section 4 presents the results, discussing the cost and time differences between the LLMs. Section 5 further discusses these results and places them in broader context. Section 6 concludes the findings by outlining the OQuaRE metrics and performance/cost/time per model. Section 7 concludes the paper with a discussion about the limitations of the study and future work."}, {"title": "2 Related Work", "content": "Entity and relation extraction, also known as Named Entity Recognition (NER), is a classical NLP task involving identifying and categorizing key elements from text into classes and determining the relationship between these classes. Before the advent of LLMs, simple machine learning (ML) models were one of the few tools in NLP and NER. These ML models were generally based on the bag-of-words principle where each word or n-gram was treated as independent from each other. For example, Kiritchenko et al. [18] employed Support Vector Machine (SVM) and Rani et al. [26] utilized Latent Semantic Indexing (LSI) and Latent Dirichlet Allocation (LDA) to process clinical trial data in a semi-automatic way. These simpler ML models lacked the complexity needed to fully capture entities themselves and their relations.\nAnother disadvantage of the ML models was the isolation of sub-tasks, which allowed for errors to accumulate throughout the data pipeline, degrading performance at each stage. Nye et al. [22] have shown that in the context of extracting ICO (intervention, comparator, outcome), an end-to-end NLP model resulted in significantly improved performance. Recent works on extracting information from clinical data have predominantly focused on LLMs. Liu et al. [20] have shown that CT-BERT, a fine-tuned version of BERT trained on clinical trials,"}, {"title": "3 Methodology", "content": "This chapter describes the methodology, as is outlined in Figure 1, that used to semi-automate the extraction and integration of clinical trial data into an ontology, using state-of-the-art LLM and a simple novel ontology merging method.\nFor the study, 50 clinical trials obtained from the clinicaltrials.gov website were used to obtain the results. The CSV file contains the NCT (unique id for trials submitted on clinicaltrial.gov), primary outcomes, and secondary outcomes. The CSV file containing the clinical trials is fed one clinical trial at a time to be processed. For each clinical trial the LLM is asked to create and ontology based on the primary and secondary outcomes. The individual clinical trial ontologies are then merged through the novel specialised ontology merging methodology to form a single ontology."}, {"title": "3.1 Dataset", "content": "The clinical trial data was pulled from the clinicaltrials.gov website in the form of a CSV file. The following filters were applied: \"Condition/disease\" and \"Study Results\" which were respectively set to \"Diabetes\" and \"With results\". The first 50 clinical trials were then downloaded as a CSV file. It is recommended to keep the disease/condition field set to one condition. As otherwise it will be unclear what outcome measures are related to which condition in the final ontology."}, {"title": "3.2 LLM models", "content": "For the LLMs, GPT3.5, GPT4, and Llama3 (8b & 70b) were selected due to their performance among LLMs [3]. The GPT models were used through an API supplied by OpenAI. While the Llama3 models were self-hosted through an online hosting service using Ollama [1]. With Llama3 8b ran on a single nVidia RTX A4000 (16 GB VRAM) and Llama3 70b on a single nVidia A100 (80GB VRAM).\nDue to the inherit randomness of transfer models, a seed was used for the GPT models to increase reproducibility. 1. Though the API, the \"temperature\" parameter was set to 0 to keep the output of the LLM as consistent possible. The temperature parameter is not the same as a seed as it uses a \"...log probability to automatically increase the temperature until certain thresholds are hit.\" [23]. The Llama3 models however do not provide any parameters to make the output less random."}, {"title": "3.3 Prompt Engineering", "content": "Three different prompt engineering tactics were used to get the most out of the LLM [24]. The tactics used for prompt engineering included:\n1. Using reference material (i.e. primary and secondary outcomes from the csv file containing the clinical trials).\n2. Writing clear instructions such as asking the LLM to adopt a persona as well as explicitly asking the model to do a (set of) specific tasks.\n3. Providing a reference text which in this case was the base ontology structure that was used for the golden-standard ontology.\n4. Prompt chaining, which breaks the task into multiple steps and prompts by using the output of the first prompt as input for the second prompt.\nBoth Listing 1.1 and 1.2 use the first three listed tactics, while Listing 1.2 adds prompt chaining to the prompt."}, {"title": "3.4 Ontology Merging", "content": "The output of the LLM has to go through a simple cleaning process. As the model tends to write some additional text before and after the ontology code, even when specifically instructed not to do so. This is however solved programmatically by simply looking at where the ontology code starts and ends and remove part of the output that comes before and after.\nDue to the token limitation that is inherent of LLMs. The design choice was made to process each clinical trial into separate ontology files to ensure the methodology is theoretically infinitely scalable.\nThe ontology merging stage is needed to add each independent clinical trial ontology to a single main ontology. The ontology merging stage roughly follows the same design as the ontology creation stage. Where the output of the ontology creation stage consisting of independent ontologies is added one by one but at the triple level. One way to do this is by comparing each new triple with all existing triples and asking a LLM if they are conceptionally the same. This approach can be used for small ontologies and is the preferred approach as the LLM can take various factors into account. However, this results into scalability issues when the ontology grows larger with $O(n^2)$. To account for this a sorted synonym list is used. Before a triple is added to the main ontology. The triple and its synonyms are first checked whether they exist in the sorted synonym list. If not, the triple is added and the triple and its synonyms are added to the sorted synonym list. If it does exist, the triple is skipped as it already exists in the ontology. This ensures the lookup necessary for the ontology merging scales with $O(logn)$ instead of $O(n^2)$. The ontology merging stage itself however has a time complexity of $O(n)$ as it loops over triples of all individual clinical trial ontologies one by one.\nThis particular ontology merging method does come with a major drawback. As the concepts are added one by one, the relations between them are lost. Limiting the applications to an ontology that effectively functions as a categorized list. Resulting in medical practitioners only being able to use the main ontology to check for related concepts in their respective categories for a particular disease. One can opt to use ontology matching instead of the proposed ontology merging method, this however lies outside of the scope of this paper."}, {"title": "3.5 Reproducibility", "content": "The used dataset of 50 clinical trials as well as the code used for both generating the individual ontologies and merging of the ontologies used in this study are"}, {"title": "4 Evaluation", "content": "The evaluation consists of two main parts. First looking at the practical side and comparing the different LLMs and humans with respect to total cost, time to process a clinical trial and transfer the knowledge to an ontology, and percentage of ontologies that were included out of dataset of 50 clinical trials. The second part being a general evaluation using the OQuaRE framework that allows one to compare different ontologies based on 19 different metrics to compare ontology quality in ontologies that are too large for humans to manually compare [12]. As well as a brief discussion of the results and its limitations."}, {"title": "4.1 Practical Evaluation", "content": "Table 1 showcases said practical metrics relevant for extracting outcomes from clinical trials involving ontologies. Comparing the 4 LLMs: GPT3, GPT4, Llama3 (8b), and Llama3 (70b) in addition to using the chaining method for each LLM. The total cost for the GPT models was calculated based the number of tokens provided by the OpenAI API and the model's respective cost per 1000 tokens. The total cost for the LLama models were calculated based on the hourly rate of the rented GPU servers. Time per trial was calculated by averaging total time spent over fifty clinical trial generated by the LLM as well as matching the individual ontology generated from a clinical trial with the main ontology. This percentage reflects whether the generated ontologies had valid syntax and could be included in the main ontology."}, {"title": "4.2 OQuaRE Evaluation", "content": "To obtain the OQuaRE results the ontologies were processed in an automated way using an implementation tool called OQuaRE-Metrics [28]. As NOCOnto (Number of Children) is defined as \"Mean number of the direct superclasses per class minus the subclasses of Thing\" [12]. NOCOnto was the only relevant OQuaRE metric due to the ontology merging step omitting relationships as well as the prompt asking to extract a specific set of categories in a specific format. Thus the metric is used as a barometer of concept extraction efficiency.\nFigure 2 shows chainedGPT4 being the best performing model. While also showcasing a trend of improved performance of larger models with more parameters. Chain prompting in general seems to have significantly improved entity extraction across all LMMs, with the greatest improvement in models with greater parameters. LLama3 models overall show significant worse performance on the"}, {"title": "4.3 Results Discussion", "content": "Results Results indicate that LLMs are both cost-effective and time efficient when compared to manual processing. The only downside being the number of included ontologies at first try. This downside can however be easily fixed when used in production by asking the LLM to generate an ontology of a clinical trial again with a different seed or by using a slightly different prompt. One particular observation that is made is that the ontologies generated by GPT4 is subpar compared to the other models. This seems to be caused due to the high occurance of missing of prefixes in the generated individual ontology, resulting in the ontology not being valid and thus not being included."}, {"title": "4.4 Limitations of Evaluation", "content": "The evaluation of the performance of the models is exclusively based on the NOCOnto OQuaRE metric, i.e. extracting as many concepts as possible. This metric does not take into account the degree of hallucinations of concepts or the correct categorization of concepts. Another important metric that is not included are the relationships between the concepts, which can be essential in the context of medical applications."}, {"title": "5 Limitations and Future Work", "content": "Limitations This study faced several limitation that may affect the interpretation and generalizability of the results. Firstly, the clinical trials only include diabetes as disease by design. Possibly limiting generalizability to other diseases. Secondly, the sample size of the clinical trials was relatively small (n=50) due to limited financial resources.\nFuture Work This study has explored the extraction of various medical concepts related to biomarkers, endpoint scores, measurement tools, and questionnaires into sub-classed derived from individual clinical trials. A significant limitation identified is the current ontology merging phase's omission of the relationships between concepts. Which are crucial for practical medical applications. Future research should address this gap by developing methods to accurately preserve and represent the relationships. Further work is needed to mitigate hallucinations"}, {"title": "6 Conclusion", "content": "LLMs seem to be a viable option to process the ever increasing number of clinical trials into an ontology usable by those in the medical field. Extrapolating the practical metrics of chainedGPT4 from 50 clinical trials to 6200 (the number of total clinical trials about diabetes available on clinicaltrials.gov that include outcome measures) results in an estimated total cost of roughly $584 and taking a little bit more than 15 days to process. Compared to if a human were to process the clinical trials resulting in a cost of roughly $31000 and taking 1550 hours. Which are roughly 193 eight-hour working days or roughly 39 forty-hour working weeks."}]}