{"title": "Pay Attention to What Matters", "authors": ["Pedro Luiz Silva", "Antonio de Domenico", "Ali Maatouk", "Fadhel Ayed"], "abstract": "Despite the remarkable success of Large Language Models (LLMs), they still exhibit a limited capability to align their outputs to the user instructions. In this work, we introduce a simple and effective method, which we name GUIDE, that mechanistically increases attention scores in instruction tokens. To support this operation, we present Influence, a novel metric that highlights how the user's instructions propagate through the transformer layers and impact the LLM output. Our results show that GUIDE improves the accuracy of following instructions 29.4% to 60.4%, outperforming natural prompting alternatives and Supervised Fine-Tuning up to 1M tokens.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are currently the state-of-the-art of most NLP tasks. Despite this success, pretrained LLMs sometimes struggle to accurately interpret diverse users' instructions and may generate outputs that do not align with human expectations. Additionally, LLMs may produce biased or hallucinated facts, which can limit their practical usefulness.\nPrevious work [Kuratov et al., 2024, Lu et al., 2024b] indicate that transformers are less prone to align with instructions as the context length grows (Kuratov et al. [2024]; Lu et al. [2024b]). In such cases, rather than fulfilling the user's request, the model generates nonsensical text or repeat segments from the prompt.\nA common solution to this problem is Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). However, these approaches are resource-intensive, time-consuming, and sensitive to the specific data and task. Ideally, a more efficient approach would be one that, once implemented, does not require additional training.\nIn that sense, due to its low cost and broad accessibility, prompt engineering is widely used to align the outputs of LLMs with user preferences. However, this method does not always produce consistent results and can be very unstable, as demonstrated in [Sclar et al., 2024].\nIn this work, we introduce GUIDE (Guided Understanding with Instruction-Driven Enhancements), a novel and systematic approach that allows users to emphasize critical instructions in their prompts. GUIDE enables users to influence the attention given to specific tokens by simply enclosing important text within tags like <!-> <-!> (as shown on Figure 1(a)). These special tags directs the LLM's focus, which is done by adding a bias to the attention scores toward the tokens they enclose. Our implementation is open-source and designed for seamless integration. Our experiments demonstrate that GUIDE significantly increases the likelihood of the model following key instructions and retrieving crucial information designated by the user, outperforming natural prompting techniques."}, {"title": "2 Related work", "content": "Alignment and instruction following. Alignment techniques have the objective to align LLM outputs with human preferences. In general, we refer to being Helpful, Harmless and Honest. Model fine-tuning usually aligns the output of the LLMs with human intents using Reinforcement Learning with Human Feedback (RLHF) [Ouyang et al., 2022], Reinforcement Learning with AI Feedback (RLAIF) [Lee et al., 2023] or Direct Preference Optimization (DPO) [Rafailov et al., 2024]. However, these methods have three significant constraints: they require specialized datasets, often with human annotations, thus reducing efficiency; they involve substantial computational complexity and cost due to the need for additional training; and they demand specialized expertise, as successfully implementing the process can be challenging. Given these constraints, this type of fine-tuning is typically reserved for general-purpose alignment, ensuring that models are Helpful, Harmless, and Honest [Shen et al., 2023] and are not suited to address end users' specific needs. Supervised fine-tuning (SFT) with techniques such as low-rank adapters (LoRA [Hu et al., 2021b]) offers a more accessible way to customize a model for individual user requirements. However, these techniques still face the same three limitations, albeit to a lesser extent. Consequently, SFT is typically utilized only for very targeted use cases if used at all.\nUtilizing LLMs for automated prompt engineering has demonstrated notable performance. Black-Box Prompt Optimization (BPO) is a sophisticated framework that automatically refines human-written prompts, often unstructured or ambiguous [Cheng et al., 2024]. Similarly, the PE2 framework [Ye et al., 2024] enhances prompt performance by refining human-written prompts through a comprehensive search process. Although PE2 avoids additional model training, it increases complexity, latency, and cost, limiting its scalability. Both BPO and PE2 are generally designed for broad enhancements in prompt writing. They are not tailored to meet individual users' specific intentions or needs.\nDue to its low cost and large accessibility, prompt engineering is extensively used to align the output of the LLMs with user preferences. This is clearly demonstrated by popular LLM frameworks like the system in [Yang et al., 2024], which empowers LM agents to tackle software engineering tasks. This system emphasizes crucial instructions through uppercase text and exclamation marks, like \"PLEASE DO NOT DO THAT!\" or \"THE EDIT COMMAND REQUIRES PROPER INDENTATION.\"\nSimilarly, the AI Scientist [Lu et al., 2024a], a leading system for automated scientific discovery, uses strong directives such as \"ABSOLUTELY DO NOT ADD IT AGAIN!!!\" to steer the model's behavior. These examples, drawn from highly influential frameworks widely used, underscore the pressing need for end-users to signal what matters most to them in order to guide LLMs toward better alignment with their goals. Currently, users rely on prompt engineering and forceful language to achieve this alignment. However, this approach does not consistently deliver positive results as shown in [Sclar et al., 2024].\nIn contrast, our proposed method, GUIDE, offers a reliable and systematic approach that enables users to mechanistically highlight critical instructions within the prompt.\nExplainability for Transformers. This is a particularly active area of research with many promising research directions. In the context of this work, we primarily focus on methods that attempt to quantify the significance of one token (or set of tokens) of interest.\nGradient metrics such as Relevance and GradCAM (Chefer et al. [2021a]; Chefer et al. [2021b]; Selvaraju et al. [2019]) have demonstrated promising results in Computer Vision, NLP and Text-To-Image tasks. Nonetheless, calculating gradients in large models, particularly those exceeding 7B parameters, demands substantial computational resources.\nActivation Patching approaches (Meng et al. [2023]; Zhang and Nanda [2024]) focuses on perturbing the inputs and checking how impactful will this perturbation be, based on where and how is it applied. They propose two types of perturbation: adding a gaussian noise in token embeddings, or perturbing"}, {"title": "3 GUIDE: (Guided Understanding with Instruction-Driven Enhancements)", "content": "In this section, we present GUIDE, a novel and systematic approach that enables users to highlight critical instructions within the text input provided to an LLM. To understand how GUIDE operates, it is essential first to revisit the core mechanism of self-attention, which drives the functioning of LLMs."}, {"title": "3.1 Description of the method", "content": "Each token k in the input text is initially represented by an embedding, denoted by $E_k^{(0)}$, which undergoes progressive refinement through stacked layers of attention. By the time it reaches the final layer, L, this embedding $E_k \\coloneqq E_k^{(L)}$ is expected to encapsulate all the semantic information required to predict the next token, k + 1.\nThe process operates as follows: at each attention layer l, the embedding of a token k is enhanced with the semantic information of past tokens (i = 1, 2, . . ., k - 1) and itself. This enrichment occurs through a residual connection, where the embedding $E_k^{(l)}$ is updated with the output of the attention layer, which consists of a weighted average of the values $V_i^{(l)}$ of the past tokens. The vectors $V_i$, known as values, are derived from a simple linear transformation of the embeddings $E_i^{(l)}$, for i \u2264 k, and are responsible for carrying the semantic information from the past tokens.\nThe extent to which previous tokens influence the semantic update of the token k is determined by attention logits, denoted by $w_{ki}^{(l)}$. These logits represent the raw, unnormalized relevance scores between a token of interest k, called a key, and each preceding token i < k, called queries. The logits are then passed through a softmax function, which normalizes them to sum to one. The resulting normalized weights are known as attention scores ($A_{ki}^{(l)}$) and quantify the degree of influence each past token has on the current token's semantic representation at a given layer. Denoting $U_k^{(l+1)} := \\text{Attention}^{(l+1)}(E_k^{(l)})$, the operations at layer l can be summarized as follows:\n$E_k^{(l+1)} = E_k^{(l)} + U_k^{(l+1)} = E_k^{(l)} + \\sum_{i=1}^{k} A_{ki}^{(l+1)} V_i^{(l)}$."}, {"title": "3.2 Calibrating GUIDE", "content": "Using GUIDE, the addition of \u2206 directly increases the attention the model pays to the tokens of interest, amplifying their influence on the generated output. However, because attention scores must sum to one, this adjustment reduces the attention given to other tokens. If A is set too high, the model"}, {"title": "3.3 Influence", "content": "Let us denote by $U = (x_1,...,x_n)$ the overall sequence of tokens associated with the user's query and with $U = (x_i,...,x_j)$ the tokens related to the instruction that the user desires to highlight.\nTo maintain simplicity and minimize computational cost, we avoid using gradient-based metrics to evaluate the impact of a subset of tokens on the overall sequence (for example, see [Chefer et al., 2021a], [Chefer et al., 2021b], and [Selvaraju et al., 2019]). Instead, a more appropriate option appears to be the Attention Rollout method proposed in [Abnar and Zuidema, 2020]. This metric can be easily computed during the forward pass, aligning well with our needs.\nThe Attention Rollout approach is based on a natural interpretation of attention scores. It postulates that the influence of a past token i on the update of the current token k is quantified by the attention score $A_{ki}^{(l)}$. The method addresses the residual connection by assuming that in the updated embedding $E_k^{(l+1)}$, both the previous embedding $E_k^{(l)}$ and the update vector $U_k^{(l+1)}$ contribute equally, each having an impact of 1/2. The vertical and horizontal flow of the impact $R_U(E_k^{(l)})$ of a given token of interest U on an embedding $E_k^{(l)}$ is hence characterized by the following recurrence:\n$R_U(E_k^{(l)}) = \\frac{1}{2} [R_U(E_k^{(l-1)})] + \\frac{1}{2} [R_U(U_k^{(l+1)})] = \\frac{1}{2} [R_U(E_k^{(l-1)}) + \\frac{1}{2}(\\sum_{i=1}^k A_{ki}^{(l)} R_U(E_i^{(l-1)}))$\nWe argue that Attention Rollout inaccurately represents the flow of attention, particularly when handling the residual connection. The norm of the past embedding $E_k^{(l)}$ is typically about 100 times larger than that of the update vector $U_k^{(l+1)}$.\nAs context length increases, one would expect that the importance of a subsequence of tokens would decrease, since the model must process more information. However, by assuming equal contributions from $E_k^{(l)}$ and $U_k^{(l+1)}$, Attention Rollout significantly overestimates the importance of past tokens.\nThis error compounds as the context length increases, leading to an inflated impact estimate that increases with the context and hence negatively correlates with the model's likelihood of following the token of interest, such as adhering to a specific instruction\nTo address this issue, we introduce Influence, a new metric designed to quantify the impact flow of a token or a set of tokens of interest U. This metric corrects Attention Rollout by weighting the contributions according to the norm of the vectors:\n$I_U(E_k^{(l+1)}) = (||E_k^{(l)}|| \\cdot I_U (E_k^{(l)}) + ||U_k^{(l+1)}|| \\cdot I_U (U_k^{(l+1)})) / (||E_k^{(l)}|| + ||U_k^{(l+1)}||)$.\nMore precisely, Influence $I_U : \\mathbb{R}^{dH} \\rightarrow \\mathbb{R}^+$ (where d is the attention head dimension and H is"}, {"title": "4 Experiments", "content": "In this section, we evaluate the benefits of GUIDE and Influence using Mistral-7b Instruct [Jiang et al., 2023] for following key instructions and retrieving crucial information designated by the user.\nIn Appendix D, we conduct the same experiments with Gemma2-2b Instruct Team et al. [2024] with the same values of A, and we obtain very similar results."}, {"title": "4.1 Description", "content": "Summarization in French To evaluate the capability of GUIDE to support LLMs in producing outputs aligned with the user's query, we perform experiments related to text translation and summarization. In these experiments, we have used text from OpenWebText [Gokaslan and Cohen, 2019], chosen for its variety in context lengths. We have divided the dataset into groups based on context length, containing texts from a 500-token window, such as (0, 500], (500, 1000], and so on. From each group, we randomly selected 20 texts and generated 10 summaries for each text using multinomial sampling [Wiher et al., 2022].\nA needle in a haystack. To evaluate the impact of our approach on the model's ability to retain information, we have conducted the Needle in a Haystack. This test involves embedding specific information at a particular location within a text and then asking a question related to that information at the end of the text. Our hypothesis is that by adding extra attention to this text, the model's outputs would improve, as the final representation should be more closely aligned with the information tokens. We have followed the methodology outlined by [Kamradt, 2023]. Specifically, we have inserted specific information, referred to as the \"needle\" at variable positions within a given text. After this insertion, we have asked a question to the LLM related to the inserted information (see the complete prompt in Appendix E).\nTo conduct this experiment, we have sampled 200 texts from the OpenWebText [Gokaslan and Cohen, 2019] dataset, selecting 50 texts for each context window of size 500, ranging from 0 to 6000 tokens. For each text, the needle was inserted at 10 different quantiles (10%, 20%, ..., 100%). We placed the needle immediately after a period ('.') to maintain the semantic integrity of the text.\nJSON Generation To assess the efficiency of GUIDE in generating outputs in a specified format, we have conducted experiments focused on JSON generation. For our inputs, we have used texts from books written between 1510 and 1699, sourced from the BL Books dataset [Labs, 2021]. We have prompted the model to extract and generate key information about each book in a predetermined JSON format, as detailed in E. We have randomly selected 300 books from the BL Books dataset and divided each text into context length windows of 500 tokens, ranging from 0 to 4000 tokens. These text segments were then incorporated into our template, where the Mistral model was expected to generate a JSON output that precisely followed the specified format.\nWe have inputted special attention into the tokens of Your response should follow exactly this template and we have then evaluated the Jaccard index between the keys of the generated JSON and the schema.\nInfluence For each of the experiments mentioned above, we have evaluated the relationship between the Influence metric and the probability of obtaining correct outputs. To achieve this, we have calculated both the ROC AUC score and the correlation between the importance of instruction tokens and the last token in the sequence. Then, we have compared these results through non-gradient"}, {"title": "4.2 Results", "content": "Summarization in French We have conducted experiments with GUIDE, biasing attention scores towards the instruction Summarize in French. Fig. 4(a) shows the observed probability that the LLM summary is in French when using GUIDE and compares the results achieved with the baseline model, with both uppercase and normal prompts, as well as the performance observed when including 'Important:' before the prompt instruction. Our findings show that GUIDE leads to an improvement from 29.4% to 60.4% with respect to the raw model, and that the best result is achieved with A = 2. Besides, to confirm that GUIDE does not induce a deterioration of the quality of the generated outputs, we compare the summaries generated in French obtained with the raw prompt and the ones obtained with GUIDE. We observed no noticeable degradation. Further details can be found in Appendix B.\nAs a baseline, we compare the performance of GUIDE to prompt engineering and Supervised Fine-Tuning (SFT) using LORA the hyperparameters can be found in Appendix C. Figure 4(a) show that using uppercase or adding \u2018Important' on the instruction does not provides notable improvements, consistently underperforming GUIDE, while Figure 4(b) shows that GUIDE outperforms SFT until 1M training tokens. These results confirms that our method is an effective solution for aligning LLMs to instruction following that does not require additional training.\nNeedle in a haystack Figure 5 shows the probability of outputting the correct phrase over the context length and the position of the needle, respectively. The Mistral model demonstrates stable performance across varying context lengths and needle positions within this window.\nAs expected, the addition of A to the needle tokens consistently enhances performance from 87.0% to 92.1%, with optimal values of A around 1. We can also note that, on average, the LLM is more effective at retrieving information when it is located at the beginning or the end of the text. This is in accordance with previous results [Kuratov et al., 2024, Kamradt, 2023].\nJSON Generation We measure the Jaccard index between the keys of the generated JSON and the keys on the schema. We observed that the optimal value for A is approximately 3, resulting in an average score improvement of 30% compared to the raw model (Figure 6)."}, {"title": "5 Conclusion", "content": "While Transformers represent the state-of-the-art in almost all NLP tasks, they often exhibit unexpected behaviors, particularly hallucination, which becomes more pronounced as context length increases. This work explores the capability of these models to align with specific instructions and introduces GUIDE, a mechanical approach for instruction alignment that does not require further optimization. We demonstrate that GUIDE effectively mitigates hallucination in instruction-following scenarios without significantly compromising output quality.\nTo evaluate the impact of GUIDE across different context lengths, we introduce Influence, a novel metric for Transformer explainability that quantifies the importance of subsequences of tokens within the context. Additionally, we implement both GUIDE and Influence in a Hugging Face-based pipeline, making them publicly available to the community."}, {"title": "A Detailed derivations on influence", "content": "Let us denoted by d the transformer head dimension, with H the number of attention heads, and with s the context length. Following the propagation of a transformer layer [Vaswani et al., 2023], the embedding on layer l. $E^{(l)}$ is computed as follows:\n$E^{(l)} = \\text{Linear} \\Big( \\text{Norm} \\big( E^{(l-1)} + \\text{Attention}^{(l)}(E^{(l-1)}) \\big) \\Big)$,\n$\\text{Attention}^{(l)}(E) = A^{(l)} \\cdot V^{(l)}(E)$,\nwhere, $E^{(l-1)} \\in \\mathbb{R}^{dH \\times s}$ is the embedding on layer l \u2013 1, $A^{(l)}$ is the attention matrix on layer l, $V^{(l)} : \\mathbb{R}^{dH} \\rightarrow \\mathbb{R}^{dh}$ is a linear function that maps the token embeddings to the values vector, Norm is a normalization function, and Linear is a conventional multilayer perceptron (MLP) function. Then, we can compute the Influence of token k, $E_k^{(l)}$, as follows:\n$I_U (E_k^{(l)}) = I_U \\Big( \\text{MLP} \\big( \\text{Norm} (E_k^{(l-1)} + \\text{Attention}^{(l)}(E^{(l-1)})) \\big) \\Big)$\n$= I_U \\big( (E_k^{(l-1)} + \\text{Attention}^{(l)}(E^{(l-1)})_k \\big)$\n$= \\frac{I_U(E_k^{(l-1)}) \\cdot ||E_k^{(l-1)}|| + I_U(\\text{Attention}^{(l)}(E^{(l-1)})_k) \\cdot ||\\text{Attention}^{(l)}(E^{(l-1)})_k||}{||E_k^{(l-1)}|| + ||\\text{Attention}^{(l)}(E^{(l-1)})_k||}$\n$= \\frac{I_U(E_k^{(l-1)}) r_k^{(l-1)} + I_U (\\text{Attention}^{(l)}(E^{(l-1)})_k)}{1 + r_k^{(l-1)}}$,\nwhere $r_k^{(l-1)} := \\frac{||E_k^{(l-1)}||}{||\\text{Attention}^{(l)}(E^{(l-1)})_k||}$.\nInfluence is computed recursively over layers, i.e., when we compute the Influence on layer l, we have already computed the Influence on layers 1, . . ., l \u2212 1. This means that $I_U (E_k^{(l-1)})$ is already computed, while we still need to compute $I_U (\\text{Attention}^{(l)}(E^{(l-1)})_k)$. Developing equation 7:\n$\\text{Attention}^{(l)}(E^{(l-1)})_k = \\sum_{i=1}^S A_{k,i}^{(l)} E_i^{(l-1)}$,\n$I_U (\\text{Attention}^{(l)}(E^{(l-1)})_k) = \\frac{\\sum_{i=1}^S A_{k,i}^{(l)} ||E_i^{(l-1)}|| I_U (E_i^{(l-1)})}{\\sum_{i=1}^S A_{k,i}^{(l)} ||E_i^{(l-1)}||}$.\nThen, if we approximate the norm of the embeddings $||E_i^{(l-1)}||$ with a constant, we obtain a simplified expression\n$I_U (\\text{Attention}^{(l)}(E^{(l-1)})_k) = \\sum_{i=1}^S A_{k,i}^{(l)} I_U (E_i^{(l-1)})$.\nWith this approximation, equation 8 becomes\n$I_U (E_k^{(l)}) = \\frac{I_U (E_k^{(l-1)}) r_k^{(l-1)}}{1 + r_k^{(l-1)}} + \\frac{\\sum_{i=1}^S A_{k,i}^{(l)} I_U (E_i^{(l-1)})}{1 + r_k^{(l-1)}}$."}, {"title": "B Evaluation of the quality of outputs", "content": "In addition to verifying that the LLM summary is in French in Section 4.2, we have also evaluated the quality of the outputs using BERTScore [Zhang et al., 2020], calculated in comparison to target summaries generated by a Llama 3 70B model [AI@Meta, 2024].\nTo highlight the pertinence of BERTScore, in evaluating the quality of the summaries, we show in Fig. 7 the distribution of the observed BERTScore conditioned to the generated text being in French or not. We observe that the distribution for texts generated in French is shifted to the right compared to those not in French, indicating that BERTScore is a suitable metric for assessing the quality of generated texts.\nTo measure the impact of GUIDE on the quality of the LLM outputs, we have evaluated the winning rate by comparing the quality of the texts generated with and without GUIDE in terms of BERTScore. Specifically, for each pair of texts generated in French (ti,\u25b3, ti,raw) by GUIDE and the unmodified (raw) model, we have determined which text had a higher BERTScore. Table 2 shows that for small enough choice of A, the quality of the output is not highly affected, with winning rates of 50.5% for A = 0.5 and A = 1 and 49% for A = 2. These results indicate that GUIDE maintains the model's capability to generate semantically correct text. However, as mentioned in Sec. 3.2, larger values of A, e.g. A = 5 results in poor outputs (see also Appendix F)."}, {"title": "C Supervised Finetuning hyperparameters", "content": "In our supervised fine-tuning experiments, we leveraged LoRa techniques Hu et al. [2021a], setting the sequence length to 8192 using sample packing and block-attention to prevent cross-sample contamination. We configured the LoRa rank to 64 and set the alpha parameter to 16. For regularization, we applied a dropout rate of 0.05. To maximize the adapter's expressiveness, our LoRa implementation targeted all modules. The batch size was set to 128k tokens, with a maximum learning rate of 1e-4, following a cosine scheduler with a 10-step warm-up. Training was conducted for 2 epochs, tracking the number of tokens processed at each step."}, {"title": "D Experimental results with other models", "content": "We have conducted the same studies presented in Section 4 also using the Gemma 2 - 2B Instruct model [Team et al., 2024]. Our results indicate that, even with smaller models, GUIDE can still improve the accuracy of following instructions, increasing the accuracy from 43.4% to 59.8% on summarization in French, 65.2% to 77.5% on retrieval and 14.4% to 24.1% on JSON generation"}, {"title": "E Prompts used in experiments", "content": "Summarization in French\nSummarize in French\n{context}\nA needle in a haystack\n<question>\nYour objective is to answer the following question based on the context:\n{question}\nDon't give information outside the document or repeat our findings\n</question>\n{context with needle}\n<question>\nYour objective is to answer the following question based on the context:\n{question}\nDon't give information outside the document or repeat our findings\n</question>"}, {"title": "F Examples of poor generation of text", "content": "This Appendix provides examples of the model failing to follow user instructions.\nF.1 Raw generation (without GUIDE)\nA common error occurs when the model simply repeats the given prompt. This example has been obtained using multinomial sampling at generation time and without GUIDE (\u2206 = 0).\nPrompt\nSummarize in French:\nThe red Ford Expedition mounted the sidewalk along DeKalb Avenue in Fort Greene, Brooklyn, on Saturday afternoon not once, but twice.\nThe first time, it narrowly missed two people at the southwest corner of Clermont Avenue, crashing into a parked car, jumping the curb and hitting another vehicle, the police said.\nBut the second time, pedestrians along the busy Brooklyn thoroughfare were not so lucky: The S.U.V. struck and killed a 9-year-old boy, Lucian Merryweather, who was with his mother on the northeast corner. His 5-year-old brother and a woman who had been in the crosswalk were hurt.\nAt first glance, the crash seemed likely to attract attention but unlikely to result in criminal charges. The driver, Anthony Byrd, 59, of Clinton Hill, remained at the scene and was not intoxicated, the police said."}]}