{"title": "Disentangling Disentangled Representations: Towards Improved Latent Units via Diffusion Models", "authors": ["Youngjun Jun", "Jiwoo Park", "Kyobin Choo", "Tae Eun Choi", "Seong Jae Hwang"], "abstract": "Disentangled representation learning (DRL) aims to break down observed data into core intrinsic factors for a profound understanding of the data. In real-world scenarios, manually defining and labeling these factors are non-trivial, making unsupervised methods attractive. Recently, there have been limited explorations of utilizing diffusion models (DMs), which are already mainstream in generative modeling, for unsupervised DRL. They implement their own inductive bias to ensure that each latent unit input to the DM expresses only one distinct factor. In this context, we design Dynamic Gaussian Anchoring to enforce attribute-separated latent units for more interpretable DRL. This unconventional inductive bias explicitly delineates the decision boundaries between attributes while also promoting the independence among latent units. Additionally, we also propose Skip Dropout technique, which easily modifies the denoising U-Net to be more DRL-friendly, addressing its uncooperative nature with the disentangling feature extractor. Our methods, which carefully consider the latent unit semantics and the distinct DM structure, enhance the practicality of DM-based disentangled representations, demonstrating state-of-the-art disentanglement performance on both synthetic and real data, as well as advantages in downstream tasks.", "sections": [{"title": "1. Introduction", "content": "Disentangled representation learning (DRL) aims to uncover the fundamental factors within the observed data [3, 21, 22, 48]. A disentangled representation is a set of latent units where each unit is dependent on only one factor and invariant to the remaining factors [3, 35, 36, 44,63]. Attaining such representation is essential for understanding data and generalizing models, making it a fundamental goal in the machine learning field [3,43].\nTo achieve DRL, supervised methods that require annotations of manually-defined factors have been attempted [4,57]. However, in the real world, factors are complex and exist on a non-discrete spectrum (e.g., image brightness), making annotation challenging. Thus, unsupervised learning methods are extensively being explored [16, 48, 66]. They commonly utilize the latent space of generative models, which capture the semantic information of images. Meanwhile, it is known that achieving disentanglement in an unsupervised manner is impossible without explicit inductive biases [39], such as regularization for statistical independence and architectural modeling [25, 48, 69]. Thus, the essence of unsupervised DRL lies in how effectively a suitable inductive bias is implemented into the target generative model.\nRecently, diffusion models (DMs) [23, 29, 51] have emerged as a cornerstone of generative models, known for"}, {"title": "", "content": "their superior generation quality and stability. In particular, DMs not only effectively encode rich information about the input image, but also ensure that the manipulated features in the latent space are directly reflected in the generated image [34]. This suggests that DMs are attractive target generative models for unsupervised DRL [66, 67, 69]. Naturally, existing DM-based DRL methods focus on providing appropriate inductive biases to DMs to achieve independence among latent units [66, 69]. Meanwhile, the significance of each latent unit accurately reflecting its corresponding factor has not yet been considered. Therefore, to enhance the practical usefulness and interpretability of representations, we not only focus on the independence of the factors (e.g., object color) represented by the latent units, but also on how faithfully each latent unit reflects the attributes (e.g., red, blue) of those factors.\nTo understand this, let us consider a scenario highlighting the necessity of attribute-separated latent units. Fig. 1 visualizes a single latent unit (responsible for object color) across multiple data points using both the DM-based state-of-the-art (SOTA) DRL method EncDiff [67] and our proposed method. In Fig. 1a, the data points for EncDiff are not clearly separated into distinct attributes (e.g., red, blue). This entanglement might actually result in selecting a blue image when sampling from the red region. Such representation with semantically ambiguous latent units cannot guarantee the intended downstream uses (e.g., manipulating object color factor of intended color). In contrast, as shown in Fig. 1b, ours exhibits clear decision boundaries between attributes, ensuring that images sampled from each color region faithfully reflect the intended color. This enhances interpretability by indicating which intrinsic factor a latent unit represents in real-world data where labels are absent, suggesting a more practical representation. Moreover, latent units with a high association (i.e., mutual information) with a specific factor naturally become less dependent on other factors, thereby promoting disentanglement.\nThus, we first propose Dynamic Gaussian Anchoring (DyGA), an inductive bias that clarifies the decision boundaries between attributes of a latent unit in DM-based DRL. DyGA dynamically selects anchors for attribute clusters in the latent space and shifts ambiguous points at the cluster boundaries toward these anchors. This well-organized attribute latent unit is used as a condition for the DM, learning to simultaneously perform disentanglement of the feature extractor and image generation of the DM.\nHowever, since DM can also be trained unconditionally, it may ignore unstable latent units during early training and rely less on the feature extractor. This is due to the peculiarity of DM structures which receive the latent unit only as an auxiliary condition via U-Net's cross-attention; a new aspect that does not need to be considered in variational autoencoder-based DRL methods [22, 31] where the latent"}, {"title": "", "content": "unit is the sole input condition. Ideally, for the diffusion denoising U-Net and feature extractor to learn complementarily, the training should be guided so that, rather than the noisy image input, the latent unit input determines the core image elements.\nTherefore, we also propose Skip Dropout (SD), an effective technique that make DM networks more DRL-friendly with a simple adjustment. SD drops U-Net's skip connection features from the noisy image input, ensuring that the DM training focuses on the latent unit features and the feature extractor, which are the key to disentanglement. In conclusion, through comprehensive modeling for both the latent unit semantics and DM structure, our proposed methods present the potential of DM for more interpretable disentanglement.\nContributions. Our main contributions are as follows:\n\u2022 We design Dynamic Gaussian Anchoring, a novel inductive bias for interpretable disentanglement that guides attribute-separated latent units through anchoring-based manipulation in the latent space.\n\u2022 We propose Skip Dropout, a modification of the U-Net architecture to enhance the disentangling functionality of the feature extractor within the DM-based DRL framework.\n\u2022 Our techniques have been applied to existing DM-based methods, achieving SOTA unsupervised DRL performance, and the obtained representations demonstrate strengths in downstream tasks as well."}, {"title": "2. Related Work", "content": "Disentangled Representation Learning. Disentangled representation learning (DRL) aims to identify underlying factors from observable data, with variational autoencoders (VAEs) initially favored as generative models because their decoders generate exclusively using the information-rich latent space. B-VAE [22] and AnnealVAE [7], for instance, use regularization and information bottleneck principles for this purpose. InfoVAE [74] and FactorVAE [31] focus on mutual information and total correlation, respectively. However, such regularizations alone are insufficient for unsupervised DRL, as explicit inductive bias has been proven to be mandatory for both models and data sets [39]. In response, QLAE [24] introduces learnable latent quantization to promote an organized latent space, providing an inductive bias for meaningful and consistent representations. Tripod [25] suggests a quantization method with finite scalar codebooks after identifying the limitations of latent quantization learning [42]. Despite these efforts, VAE-based models face a trade-off between image quality and disentanglement [9, 22, 31]. Consequently, InfoGAN [10] demonstrated that generative adversarial networks (GANs) [18] have informative latent spaces and can achieve disentanglement via the lower bounds of mutual information. Pre-trained GAN-based methods such as LD [62], CF [54], GS [19], and DS [30] have also been explored. However, GANs'"}, {"title": "3. Methods", "content": "In this section, we begin with a brief explanation of diffusion models. Next, we introduce the overall framework (Sec. 3.1), propose our methodological contributions: (1) Dynamic Gaussian Anchoring as a new inductive bias for diffusion models (Sec. 3.2), and (2) skip dropout to enhance the training of the feature extractor (Sec. 3.3).\nDiffusion Models. Diffusion models [23, 28,51] are a type of latent variable model that reconstruct $x_0 \\sim P_{data}(x_0)$ from $x_T \\sim N(0,I)$ with the following formulation $p_\\theta(x_0) := \\int p_\\theta(x_{0:T}) dx_{1:T}$. Diffusion models use a Markov process known as the forward process, which gradually adds noise to the image through a variance schedule $\\beta_1,..., \\beta_T$ until it becomes $x_T \\sim N(0,I)$:\n\n$q(x_t | x_{t-1}) = N(x_t; \\sqrt{ 1 - \\beta_t}x_{t-1}, \\beta_tI)$.\n\nThen, with $\\alpha_t = \\Pi_{i=1}^t \\bar{\\alpha}_s$ and $\\bar{\\alpha}_t = 1 - \\beta_t$, the latent variable $x_t$ at timestep $t$ can be obtained through the following interpolation $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon$, where $\\epsilon \\sim N(0, I)$. This diffusion model optimizes the network according to the following objective:\n\n$L_\\eta = E_{x_0,\\epsilon, t}[\\eta_t||\\epsilon_\\theta (x_t, t, c) - \\epsilon||]$,\n\nwhere $\\eta_t$ is the coefficient according to the noise schedule, $\\epsilon_\\theta(x_t, t, c)$ is the predicted noise, and $c$ is some condition."}, {"title": "3.1. Diffusion Model Framework for DRL", "content": "We now introduce our full framework as shown in Fig. 2. Let us first describe the DRL framework involving diffusion models (DM) which is commonly used in recent DM-based DRL methods [66, 67]. Hence, our technical contributions in Sec. 3.2 and Sec. 3.3 can be readily applied to methods based on this framework.\nLatent Diffusion Model. Latent diffusion models (LDM) [51] are a type of diffusion model that significantly reduces computational cost by training the diffusion model on images compressed in the latent space rather than the pixel space. LDMs have gained widespread recognition for their utility [45, 53, 72]. We adopt the widely-used LDM framework"}, {"title": "", "content": "with VQ-GAN in order to train conditional generation by conditioning the latent units through cross-attention.\nFeature Extractor. The feature extractor extracts a compressed feature from the image, consisting of N latent units. Each latent unit is trained to represent one of the intrinsic factors constituting the image. Similar to previous research [67], the structure of the feature extractor includes a simple CNN that extracts semantic information from the image and an MLP layer applied to each latent unit. Each latent unit is composed of a D-dimensional vector, set to D = 32 following prior studies [48, 69]. The entire feature c = [c1,..., cN], composed of latent units, serves as the condition for the LDM. See the supplement for details."}, {"title": "3.2. Dynamic Gaussian Anchoring", "content": "In this subsection, we introduce Dynamic Gaussian Anchoring (DyGA) to ensure that each latent unit of the feature c faithfully reflects each factor. DyGA is divided into two processes. First, anchor selection involves determining the anchors based on the features, with the number of anchors being decided dynamically. Second, feature alignment delineates the boundaries between attributes represented by latent units by adjusting the features towards the direction of the selected anchor."}, {"title": "3.2.1 Anchor Selection", "content": "As shown in Fig. 2b, anchor selection involves 1) initializing multivariate Gaussian distributions, 2) fitting the Gaussian mixture via high-dimensional data clustering (HDDC) [5], 3) splitting this Gaussians to dynamically increase the number of Gaussians, and 4) filtering out unnecessary Gaussians. At this point, the anchors become the means of the Gaussians, and the spliting and filtering processes serve to dynamically adapt the number of anchors. Meanwhile, HDDC uses the Expectation-Maximization (EM) algorithm to maximize the likelihood function, which is generally non-convex and has many stationary points [40]. This means that HDDC can get trapped in a sub-optimal stationary point. Adjusting the number of anchors provides an opportunity to escape from these stationary points, as it slightly alters the optimization problem. Moreover, it is suitable for unsupervised learning where prior knowledge of the data is unavailable, making it appropriate for cases where the number of attributes within the factors is unknown.\nHigh-dimensional Data Clustering. Finite mixture models [41] predict data distribution as a Gaussian mixture by maximizing the likelihood function through the Expectation-Maximization (EM) algorithm. However, due to the curse of dimensionality [2], this method is difficult to apply to high-dimensional data. To address this, high-dimensional data clustering (HDDC) [5] reduces the high-dimensional problem to a lower-dimensional subspace where the EM update is performed. For Gaussian $N(\\mu_i, \\Sigma_i)$, for $i = 1, ..., K$, $\\mu_i \\in R^d$, and $\\Sigma_i \\in R^{d\\times d}$, the class conditional covariance matrix $\\Delta$ is defined as follows:\n\n$\\Delta_i = Q_i \\Lambda_i Q_i^T$,\n\nwhere $Q_i$ is the orthogonal matrix of eigenvectors of $\\Sigma_i$. Consequently, $\\Lambda_i$ becomes a diagonal matrix with the eigenvalues of $\\Sigma_i$ as its diagonal entries. Among these, we let $d_i$ diagonal entries remain unchanged while the remaining $d - d_i$ entries are tied as a single parameter. We can then define the subspace $F_i$ spanned by $d_i$ eigenvectors and its orthogonal complement $F_i^\\perp \\in R^{d-d_i}$ such that $F_i \\oplus F_i^\\perp = R^d$. Now, we are able to handle high-dimensional data through a Gaussian mixture model in the subspace $F_i$. However, HDDC requires the number of Gaussians to be specified in advance and this number remains fixed. We propose two methods to adjust this dynamically."}, {"title": "", "content": "Dynamic Adjustment of the Number of Anchors. Gaussians fitted to the features via HDDC represent a stationary point of the likelihood function maximized by the EM algorithm. To address the fact that the optimality of this stationary point cannot be guaranteed [40], there have been attempts to complement this with split/merge strategies in Gaussian mixture models [8, 12,37]. However, such attempts have not been explored for high-dimensional data. Additionally, a na\u00efve merging strategy may not be suitable for feature alignment, especially when dealing with real-world data where the label may be a continuous value. Therefore, a strategy that dynamically adjusts the number of Gaussians to handle continuous variables is needed.\nGaussian Splitting and Filtering. First, Gaussian splitting is based on the density of the Gaussians according to the responsibility, and the responsibility $\\gamma_{ij}$ of the j-th Gaussian for feature $x$ is given by the following equation:\n\n$\\gamma_{ij} = \\frac{\\pi_j N(x_i|\\mu_j, \\Sigma_j)}{\\sum_{k=1}^K \\pi_k N(x_i|\\mu_k, \\Sigma_k)}$,\n\nwhere $\\pi_j$ is the weight of component j, $N(x_i|\\mu_j, \\Sigma_j)$ is the probability density function (PDF) of the multivariate Gaussian distribution with mean $\\mu_j$ and covariance $\\Sigma_j$, and these are calculated through EM algorithm. For each Gaussian, the density of the features with a responsibility $\\gamma$ greater than $\\phi = 0.5$ (i.e., cluster) is measured. If the density of the fitted Gaussian is higher than $\\psi = 0.5$, it is determined that the Gaussian does not adequately reflect the boundaries that need to be disentangled; thus, a split is performed. The density is defined as follows:\n\n$Density = \\frac{1}{N_i} \\sum_{j=1}^{N_i} ||x_{ij} - \\frac{1}{N_i} \\sum_{k=1}^{N_i} x_{ik}||_2$,\n\nwhere for cluster i, $x_{ij}$ is the j-th data point and $N_i$ is the number of data points. In addition, to escape sub-optimal points while covering a large number of attributes, the split is also performed arbitrarily. During every split process, once"}, {"title": "", "content": "a Gaussian is divided into two, it is re-optimized through the EM algorithm using the data belonging to each cluster. After the split process, Gaussian filtering occurs to remove Gaussians that have too few data points in the cluster. This prevents small Gaussians from causing distortions in some features during feature alignment for new data. Re-optimization also occurs after filtering, but since the split and filtering minimally alter the Gaussians, stability remains.\nDyGA during the training process. Anchor selection is only possible when there is learned feature data. Therefore, after completing each epoch of training, we use the feature data to select anchors that will be used for future feature alignment. See the supplement for algorithmic details of the entire training process, including anchor selection and feature alignment."}, {"title": "3.2.2 Feature Alignment", "content": "Feature alignment refers to the process of shifting a feature $c = [c_1,..., c_N]$ towards the mean $\\mu_k$ of the Gaussian with the highest responsibility, as described in Eq. (4). Through feature alignment, the boundaries between clusters becomes definite. For $i \\in [1, ..., N]$ and aligned feature $\\tilde{c}_i \\in R^D$, the feature alignment process is as follows:\n\n$\\tilde{c}_i = c_i + \\delta (\\mu_k - c_i)$,\n\n$\\delta = \\lambda \\exp{\\left(-\\frac{1}{D} \\sum_{j=1}^{D} (\\frac{\\mu_k^j - c_i^j}{c_i^j})^2\\right)}$,\n\nwhere $\\lambda$ is a scale factor, and the superscript $j$ denotes the j-th element. Since a feature located at the boundary between two Gaussians is sensitive, adjusting this feature is critical to the stability of the diffusion model training. Therefore, fully aligning it to the mean of either Gaussian or using too large $\\lambda$ (e.g., $\\lambda >> 1$) can negatively affect the overall training framework. This could cause the diffusion model to behave as if it were learning unconditional generation. Therefore, as described in Eq. (6), the feature unit $\\tilde{c}_i$ is an interpolation between the feature and the mean $\\mu_k$ of the Gaussian. In this process, $\\delta$ is determined by the distance between the feature and the mean. To prevent excessive variation due to the vector magnitude, $\\delta$ is bounded by $\\lambda$, considering the ratio of the difference to the feature. This ensures that the conditional diffusion model can stably utilize the output of the feature extractor, even when the distance from the Gaussian with the highest responsibility is too large. In this paper, $\\lambda = 0.1$ was used as the default."}, {"title": "3.3. Skip Dropout", "content": "Unlike VAE [7, 22, 32, 74], disentanglement diffusion models [46, 66, 67, 69, 73] are structured with the junction of a feature extractor and the denoising network of a conditional diffusion model. In VAE, the decoder, which is the image generator, depends solely on the latent units. In contrast, the denoising network of the diffusion model relies on both the flow of the network according to the previous step's image $x_t$ and the feature $c$. Therefore, during the integrated training process of the diffusion denoising U-Net [52] and the feature extractor, it was necessary to prioritize the training of the feature extractor to achieve DRL-friendly training. Considering this concern, we propose a skip dropout method inspired by DyLoRA [61] as follows:\n\n$s_r = s \\odot m$,\n\nwhere $m_i \\sim Bernoulli(p)$, $s_r$ is the skip connection feature, and $s$ is the dropout-applied skip connection feature. DyLoRA, inspired by nested dropout [50], uses higher dropout ratios for higher ranks so that the model relies more on lower-rank weights. This implies that dropout [59] can concentrate information on specific weights. We aim to use this property to enhance the feature generation capability of the feature extractor without interfering with the training process of the denoising U-Net. The output of the feature extractor serves as the condition for the diffusion model, which is delivered as keys and values to specific blocks through cross attention. According to FreeU [56] notation, this can be seen as part of the process of forming backbone features. Therefore, we adopt a method to drop out skip connection features at resolutions where conditions are not injected through cross attention, thereby emphasizing the learning of weights that create backbone features (Fig. 2a).\nRemark. SD stochastically blinds some of the skip connection feature channels to prevent them from accumulating factor-specific information. As a result, the denoising U-Net yields the learning of the core image information to the feature extractor, which is not connected by skip connections. This allows the feature extractor to be sufficiently trained for disentangled representation."}, {"title": "4. Experiments", "content": "Our experiments aimed to verify how effective the proposed DyGA and SD are as inductive biases in the disentanglement of diffusion models. First, we introduce the setup including the dataset and metrics (Sec. 4.1), and then validate the performance by comparing with state-of-the-art models (Sec. 4.2). Additionally, we examine the effectiveness of DyGA and SD in other diffusion-based disentanglement models (Sec. 4.3). We also explore qualitative results through visualization (Sec. 4.4), and validate the performance of each methodology through an ablation study (Sec. 4.5). Subsequently, to evaluate the superiority of our representation, we assess it through a downstream task (Sec. 4.6). For more experiments and analyses, please refer to the supplementary material."}, {"title": "4.1. Experimental Setup", "content": "Datasets. We used the datasets Cars3D [47], Shapes3D [6], and MPI3D-toy [17], which are commonly used in disen-"}]}