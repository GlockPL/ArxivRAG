{"title": "RE-INTRODUCING LAYERNORM: GEOMETRIC MEANING, IRREVERSIBILITY AND A COMPARATIVE STUDY WITH RMSNORM", "authors": ["Akshat Gupta", "Atahan Ozdemir", "Gopala Anumanchipalli"], "abstract": "Layer normalization is a pivotal step in the transformer architecture. This paper delves into the less explored geometric implications of this process, examining how LayerNorm influences the norm and orientation of hidden vectors in the representation space. We show that the definition of LayerNorm is innately linked to the uniform vector, defined as $1 = [1, 1, 1, 1, \u2026\u2026\u2026,1]^T \u2208 R^d$. We then show that the standardization step in LayerNorm can be understood in three simple steps: (i) remove the component of a vector along the uniform vector, (ii) normalize the remaining vector, and (iii) scale the resultant vector by $\\sqrt{d}$, where d is the dimensionality of the representation space. We also introduce the property of \"irreversibility\" for LayerNorm, where we show that the information lost during the normalization process cannot be recovered. In other words, unlike batch normalization, LayerNorm cannot learn an identity transform. While we present possible arguments for removing the component along the uniform vector, the choice of removing this component seems arbitrary and not well motivated by the original authors. To evaluate the usefulness of this step, we compare the hidden representations of LayerNorm-based LLMs with models trained using RMSNorm and show that all LLMs naturally align representations orthogonal to the uniform vector, presenting the first mechanistic evidence that removing the component along the uniform vector in LayerNorm is a redundant step. Our findings support the use of RMSNorm over LayerNorm as it is not only more computationally efficient with comparable downstream performance, but also learns a similar distribution of hidden representations that operate orthogonal to the uniform vector.", "sections": [{"title": "INTRODUCTION", "content": "The transformer architecture (Vaswani et al., 2017) has been the cornerstone of most recent advances in artificial intelligence and has rapidly become the architecture of choice in natural language processing, computer vision, speech, and various other domains. Layer normalization, a crucial yet often overlooked component of the transformer architecture, plays an integral role in stabilizing the training process in transformer-based models. Introduced by Ba et al. (2016), layer normalization standardizes the features at each layer, adjusting and scaling the activations to have zero mean and unit variance within a vector. This normalization is performed independently for each hidden vector in contrast to batch normalization (Ioffe & Szegedy, 2015), which relies on statistics (mean and variance) from a batch of data points. Layer normalization is especially effective when working with long sequences of variable lengths. While the benefits of layer normalization, such as improved training speed and better convergence, are well documented (Ba et al., 2016; Xu et al., 2019; Zhang & Sennrich, 2019), its specific effects on the hidden vectors within a model and the global properties of the resulting representations remain surprisingly underexplored.\nIn this paper, we first discuss the global effects of layer normalization on a vector. The conventional explanation of the LayerNorm operation is usually as follows: standardize each vector by subtracting the mean of its elements, divide by the standard deviation. While this is an accurate procedural definition, we ask a more global question: How does the LayerNorm operation transform a vector in representation space? We present a novel interpretation of LayerNorm and show"}, {"title": "BACKGROUND", "content": "Let $x \u2208 R^d$ represent an intermediate hidden representation in a transformer-based model, on which the LayerNorm operation is applied. Then the following steps summarize the layer normalization operation:\n1. Calculate the mean, $\\mu = \\frac{1}{d}\\sum_{i=1}^d x_i$, and standard deviation, $\\sigma = \\sqrt{\\frac{1}{d}\\sum_{i=1}^d (x_i - \\mu)^2}$, where $x_i$ are the components of $x$\n2. Standardize the components of the hidden vector $x$ to get $y$, such that $y_i = \\frac{x_i - \\mu}{\\sigma}$, $\\forall x_i \u2208 x$\n3. Scale and shift to get $z = a \u2299 y + \\beta$, where $\u2299$ represents element-wise multiplication of two vectors, and $a, \\beta$ are scaling and shifting vector parameters learned during training.\nIn this paper, we refer to the combination of step-1 and step-2 as the \"standardization step\", whereas step-3 is referred to as the \"scale-and-shift\" step."}, {"title": "COMPUTATIONS IN DECODER-ONLY LLMS", "content": "In this paper, we study the hidden representations of modern decoder-only large language models (LLMs). Let $h^l$ represent the intermediate hidden state vectors between each decoder layer. As depicted in Figure 2b, the computations within a layer of most decoder-only LLMs proceed as follows:\n$\\begin{aligned}\nf^l &= LN_1(h^{l-1})  \\\\\na^l &= Att(f^l)  \\\\\ng^l &= LN_2(h^{l-1} + a^l)  \\\\\nm^l &= W_{proj}(W_o^l g^l + b_o^l) + b_{proj} \\\\\nh^l &= h^{l-1} + a^l + m^l\n\\end{aligned}$\nThe intermediate hidden vectors between each layer, $h^l$, are also sometimes called the residual stream. Let $LN_1$ represent the first LayerNorm function that acts just before the attention module and $LN_2$ represent the second LayerNorm just before the MLP module. We abstract out the computations of the attention and MLP modules to focus on the layer normalization blocks.\nAs the vectors computed in the attention and MLP modules get added to the residual stream at each layer, the residual stream represents a summation of an increasing number of vectors. A non-recursive formula for the residual stream depicts this clearly:\n$h^l = h^0 + \\sum_{i=0}^{l-1} a^i + \\sum_{i=0}^{l-1} m^i$\nThe residual stream thus represents a continuous summation of vectors computed at the attention and MLP modules in each layer with the incoming residual stream. This leads to an increasing norm of the residual stream (section 4.1), thus necessitating a normalization operation."}, {"title": "RE-INTRODUCING LAYER NORMALIZATION", "content": "While the original definition of LayerNorm (section 2) gives us an operational description, it tells us very little about the global properties of the resulting vectors. For example, with the steps described in section 2, we get very little insight about the norm or the orientation of the standardized vector"}, {"title": "THE UNIFORM VECTOR AND THE MEAN VECTOR", "content": "The vector definition of layer normalization in equation 9 requires us to define two new vectors: the uniform vector and the mean vector. Since these are non-standard vectors, it is important to understand the properties of these vectors.\nThe uniform vector or $1 = [1,1,1,1, \u00b7\u00b7\u00b7 1]^T$, is called so because all its components are equal or uniform and set to 1. This vector plays a very important role in the formulation and understanding of layer normalization. An important thing to note here is the norm of the uniform vector: $||1||_2 = \\sqrt{1^T1} = \\sqrt{d}$.\nThe mean vector in the context of LayerNorm is defined in a non-traditional manner. Traditionally, a mean vector is the sum of a few vectors. But in our definition, the mean vector, $\\mu = \\mu1$, is a scaled version of the uniform vector. It is scaled by a value that is the mean or the average of all the components of the hidden vector and is oriented in the direction of the uniform vector. Note that the mean vector is a function of an underlying vector $x$, as is implicit in the calculation of $\\mu$ in equation 7. An interesting property of the mean vector is its norm. Let's define $\\theta_{x1}$ as the angle between vectors $x$ and the uniform vector. Then:\n$\\begin{aligned}\n||\\mu|| &= ||\\mu1|| = |\\mu|||1|| \\\\\n&= (\\frac{1}{d}1^Tx) ||1|| \\\\\n&= \\frac{1}{d} ||1||^2||x|| cos \\theta_{x1} = ||x|| cos \\theta_{x1}\n\\end{aligned}$\nHere we replace the formula for the mean from equation 7, expand the inner product between the uniform vector and $x$ in terms of their norms and the angle between them, and use the fact that $||1||_2 = \\sqrt{d}$.\nThe norm of the mean vector gives us very important insights about what's going on in the layer normalization process. Equation 10 shows that the norm of the mean vector is nothing but the projection of the underlying vector $x$ along the uniform vector. By definition, the mean vector, $\\mu = \\mu1$, is oriented along the direction of the uniform vector. In other words, given that $\\theta$ is the angle between vectors $x$ and the uniform vector,\n||$\\mu$|| = ||x|| cos $\\theta_{x1}$ = x.1\nand\n$\\mu$ = (x.1)1"}, {"title": "EXPLANATION OF LAYER NORMALIZATION", "content": "If we go through the steps of layer normalization as discussed in section 2, the component-wise subtraction of the mean of all components of a vector can now be written completely in terms of the vector $x$. We define $x^\\perp$ as the component of $x$ orthogonal to the uniform vector as follows:"}, {"title": "THE IRREVERSIBILITY OF LAYER NORMALIZATION", "content": "The idea of reversibility is discussed briefly while introducing batch normalization (Ioffe & Szegedy, 2015), which normalizes each feature dimension in the input vector $x$ independently by calculating the mean and standard deviation statistics over a large sample of such vectors. Specifically, let $x_1, x_2,...,x_b$ be a set of $b$ vectors in the training set, each vector of dimension $d$. Then, both the batch normalization and layer normalization processes for each component can be represented in the following two-step process:\n$y_i = \\frac{x_i - E[x_i]}{\\sqrt{Var(x_i)}}$ (standardization)\n$z_i = A_ix_i + B_i$ (scale-and-shift)\nThe key difference between batch normalization and layer normalization lies in how the expectation values and variance are calculated for each component. In the case of batch normalization, the mean and variance for each dimension are calculated over the training set, i.e.\n$\\begin{aligned}\nE[x_i] &= \\frac{1}{b} \\sum_{j=1}^b x_i^j \\\\\nVar(x_i) &= \\frac{1}{b} \\sum_{j=1}^b (x_i^j - E[x_i])^2\n\\end{aligned}$\nNote that for batch normalization, $E[x_i]$ and $Var(x_i)$ are the same for each vector $x \u2208 {x_1,x_2,...,x_b}$, but different for each component of the vector. In total, for a $d$-dimensional hidden vector space, there are 2d statistics in batch normalization, d for the mean and d for variance. This means that the information lost during the standardization step in batch normalization,"}, {"title": "WHAT'S SO SPECIAL ABOUT THE UNIFORM VECTOR?", "content": "We see that the definition of LayerNorm is innately linked with the uniform vector, possibly unintended by the original authors. Since the information along the uniform vector is being removed irreversibly during layer normalization (section 3.3), the layer normalization process implicitly assumes that the information along the uniform vector is either not important or that the model should not store information along that direction.\nTo understand what removing the component along the uniform vector does, we need to go back to the component-wise definition of LayerNorm. Removing the component along the uniform vector is the same as subtracting the mean of components of a vector from each component. If all components of a vector are equal, this operation reduces the vector to a vector of zeros. Thus, LayerNorm forces the model to learn hidden representations such that all components of the vector are not the same; otherwise, the vector is reduced to zero. A possible argument for enforcing this could be that if all components of a vector are the same, the vector loses expressive power. This is a possible conjecture offered by us to potentially justify such a choice. If this is indeed the explanation, the need for such a step should be justified by showing that the hidden representations within a language model have a propensity towards having all components equal to each other.\nA different perspective to understand the implications of the \u201cmean subtraction\" step can be seen using the global picture of LayerNorm presented in this paper. Two randomly chosen vectors in high dimensional spaces are nearly orthogonal to each other with a very small spread. Since the \"mean subtraction\" step in LayerNorm means removing the component along the arbitrarily chosen uniform vector, the need for such a step would be justified if the hidden representations created by the model had significant components along this vector. The choice of removing information along the uniform vector seems like an unintended choice without much justification given in literature, including the original paper Ba et al. (2016). RMSNorm, a popular variant of LayerNorm used to train the Llama series of models, does not perform this step and performs just as well. This provides a strong motivation to explore the need for the \"mean subtraction\" step in LayerNorm (section 4.3)."}, {"title": "EXPERIMENTS", "content": "After a detailed theoretical analysis of the effects of layer normalization on a vector, we next study how LayerNorm affects the hidden representations of LLMs in practice. While prior works have studied the effects of LayerNorm on downstream model performance and training convergence (Ba et al., 2016; Xu et al., 2019; Zhang & Sennrich, 2019), there has been a surprising gap in the literature about studying the effect of LayerNorm on the internal representations of a model at inference time. To do so, we pass one million tokens from Wikipedia articles through various models and study how LayerNorm modifies a hidden vector in the representation space."}, {"title": "Methods and Models", "content": "We study the impact of LayerNorm on the hidden representations of 7 decoder-only LLMs across two size categories. The models used are listed in Table 1. GPT2XL, GPT-Neo-1.3B, and Pythia-1.4B represent the small LLM size category, whereas GPT-J-6B, Pythia-6.9B, Llama-2-7B, and Llama-3-8B represent the medium LLM size category. We pass one million tokens from Wikipedia articles through each model and capture the hidden representations for all tokens before and after each normalization layer. This is done for each layer inside the model, which creates many terabytes to be analyzed. Norms were calculated using the L2 norm, and angle differences in degrees were determined through cosine similarity calculations to measure the orientation changes induced by normalization.\nAs outlined in Table 1, we also use two types of normalization methods. Models like GPT-2 XL, GPT-Neo 1.3B, Pythia-1.4B, GPT-J 6B, and Pythia-6.9B employ LayerNorm (Ba et al., 2016), whereas Llama-2-7B and Llama-3-8B utilize RMSNorm (Zhang & Sennrich, 2019). The crucial difference between LayerNorm and RMSNorm is the subtraction of the mean vector (Figure 2a). In this paper, we study how LayerNorm and RMSNorm operate on the hidden representations."}, {"title": "NORM STABILIZATION", "content": "As a token representation passes through the different layers inside an LLM, the hidden representations accumulate due to residual connections (He et al., 2016). These hidden representations get added to the residual stream at each layer as shown in equation 6 and can cause the norm of the hidden vectors to grow. In fact, the norm of the hidden vectors at each layer grows disproportionately with large standard deviations, as can be seen in Figure 3 (a-c). The growing norm of the hidden vectors is extreme for some models more than others, as is seen for GPT-J and Pythia-6.9B. The growing norm of hidden representations for the remaining models from Table 1 and the effect of layer normalization can be seen in Figure 6 (appendix).\nThe significant role of LayerNorm and RMSNorm in stabilizing the growing norm of the hidden vectors at each layer can be seen in Figure 3 (d-f), which shows the norm of the hidden vectors after the first normalization step. This shows that normalization before sending vectors into the attention module significantly attentuates the vectors norms and reduces their variance. The standardization step in layer normalization first modifies the norm of each vector to $\\sqrt{d}$, where d is the dimensionality of the representation space. The norms of the standardized vectors are further modified marginally by the scale-and-shift steps. Layer normalization effects both the mean and the spread of the norms, where the standard deviations in norm reduce by factors between 10-100 depending on"}, {"title": "ROTATION", "content": "While stabilizing the growing norm of hidden vectors is a major function of layer normalization, it also ends up rotating the hidden vectors. Figure 4 shows the angle between the original hidden vector and the post-normalization vector. As can be seen in Figure 4 (and in Figure 8 in appendix for the rest of the models), each layer is responsible for a fixed, non-trivial amount of rotation of hidden vectors in the representation space. Mean rotation angles in degrees between hidden and post-layer normalization vectors (LN1 and LN2) across all layers for various models are shown in Table 4 in the appendix. Thus, both LayerNorm and RMSNorm additionally rotate hidden vectors in representation space in addition to stabilizing the norms of the vectors."}, {"title": "LAYERNORM VERSUS RMSNORM: THE NEED FOR \"MEAN SUBTRACTION\"", "content": "Apart from norm stabilization and rotation, a critical aspect of LayerNorm is its ability to orient the hidden vectors orthogonal to the uniform vector, as discussed in section 3. This is done by permanently (section 3.3) removing the component of every vector along the uniform vector and then normalizing the remaining vector. Such an action trains the model to operate orthogonal to the uniform vector, implying that the information along the uniform vector is not useful (section 3.4). RMSNorm, on the other hand, does no such thing and simply normalizes the existing vector, showing similar performance (Zhang & Sennrich, 2019). Thus, a natural question to ask here is: \"Is the mean subtraction step really needed?\"\nThe Llama model series (Touvron et al., 2023a;b) is the most prominent family of LLMs that use RMSNorm. While the fact that Llama models achieve state-of-the-art results across multiple measures (Dubey et al., 2024) shows that using RMSNorm instead of LayerNorm does not hurt performance, it is hard to isolate the role of the \u201cmean subtraction\u201d step in overall performance. In this paper, we answer this question mechanistically by finding justification for the need for such an operation. To do so, we find the angle between the hidden vector and the uniform vector just before and after layer normalization operations (equations 1 and 3). The results are shown in Figure 5 for GPT-J"}, {"title": "CONCLUSION", "content": "We present a detailed theoretical and empirical analysis of layer normalization on hidden vectors in representation space, with a focus on the global effects of the LayerNorm operation on a vector. We first show that the LayerNorm operation can be understood in three simple steps: removing the component of a vector along the uniform vector ($1 = [1, 1, 1, 1, \u2026\u2026\u2026,1]^T \u2208 R^d$), normalizing the remaining vector, and scaling the resultant vector by $\\sqrt{d}$. We then show that LayerNorm is an irreversible process-information along the uniform vector is removed during LayerNorm and cannot be recovered using the learnable parameters available in the formulation. This is in contrast with BatchNorm, where the network has the option of learning an identity operation. We then empirically show how LayerNorm regulates the norm and the orientation of hidden vectors during model inference. Finally, we show that the \u201cmean subtraction\" operation in LayerNorm is dispensable both during inference and for training, as shown by internal representations of RMSNorm-based Llama models."}]}