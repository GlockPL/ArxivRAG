{"title": "Tidiness Score-Guided Monte Carlo Tree Search for Visual Tabletop Rearrangement", "authors": ["Hogun Kee", "Wooseok Oh", "Minjae Kang", "Hyemin Ahn", "Songhwai Oh"], "abstract": "In this paper, we present the tidiness score- guided Monte Carlo tree search (TSMCTS), a novel framework designed to address the tabletop tidying up problem using only an RGB-D camera. We address two major problems for tabletop tidying up problem: (1) the lack of public datasets and benchmarks, and (2) the difficulty of specifying the goal configuration of unseen objects. We address the former by presenting the tabletop tidying up (TTU) dataset, a structured dataset collected in simulation. Using this dataset, we train a vision-based discriminator capable of predicting the tidiness score. This discriminator can consistently evaluate the degree of tidiness across unseen configurations, including real-world scenes. Addressing the second problem, we employ Monte Carlo tree search (MCTS) to find tidying trajectories without specifying explicit goals. Instead of providing specific goals, we demonstrate that our MCTS-based planner can find diverse tidied configurations using the tidiness score as a guidance. Consequently, we propose TSMCTS, which integrates a tidiness discriminator with an MCTS-based tidying planner to find optimal tidied arrangements. TSMCTS has successfully demon- strated its capability across various environments, including cof- fee tables, dining tables, office desks, and bathrooms. The TTU dataset is available at: https://github.com/rllab-snu/", "sections": [{"title": "I. INTRODUCTION", "content": "In this paper, we address the tabletop tidying problem, where an embodied AI agent autonomously organizes ob- jects on a table based on their composition. As depicted in Figure 1, tidying up involves rearranging objects by determining an appropriate configuration of given objects, without providing an explicit target configuration. Previous research has encountered difficulties in defining the tidying up problem, primarily due to the lack of public datasets and metrics to assess tidiness. To address these issues, we collect a structured dataset for tabletop tidying, and train a tidiness discriminator and tidying planner to transform a messy table into an organized one through a simple and effective framework. We refer to this as the tidiness score- guided Monte Carlo tree search (TSMCTS).\nIn previous research on object rearrangement, goal con- figurations are provided either as target positions or as im- ages of the desired arrangement [1]\u2013[3]. This setup enables straightforward evaluation by comparing the state to the goal. However, using an image as a goal requires objects to be pre-arranged for goal generation, limiting flexibility. To accommodate a wider variety of goals, recent studies employ language labels or descriptions to define more abstract goals [4]\u2013[6]. Nonetheless, these language-conditioned rearrange- ment studies require separate encoding modules to integrate language with image or positional inputs. To connect these domains, models like CLIP [7] and BLIP [8] map features into a unified latent space. To connect the language domain to the image domain, CLIP [7] and BLIP [8] models focus on mapping features from different domains into a unified latent space. While these approaches show promising performance in extracting semantic features [9], [10], they still struggle with understanding the spatial relationships among objects.\nThe proposed method, TSMCTS, learns a tidiness score function and finds an action sequence that generates a tidy configuration based on object combinations without explicit goals. We collect a tabletop tidying dataset from diverse environments (e.g., coffee tables, dining tables, office desks, bathrooms) and train a tidiness discriminator to measure the degree of tidiness reliably, even with unseen objects and real-world images. Our discriminator is superior to previous approaches since it can consistently measure tidiness from real-world images, whereas previous attempts [11], [12] primarily demonstrate success in simulations or toy examples rather than real-world scenarios. Finally, we use the tidying- up discriminator as a utility function of MCTS [13] to find a sequence of pick-and-place actions. The proposed MCTS- based planner employs a tidying policy trained with our tabletop tidying-up dataset using an offline reinforcement learning method, specifically Implicit Q-learning (IQL) [14], as its tree policy."}, {"title": "II. RELATED WORK", "content": "Tidying up is an object rearrangement problem occurring in situations where the goal is not explicitly provided. In- stead of a specific goal, several research approaches involve expressing goals in natural language [4]\u2013[6], or finding functional arrangements based on user preferences [15], [16]. Additionally, there are studies that directly learn the degree of tidiness as a score function and plan trajectories to achieve a tidied scene [11], [12].\nRecent studies such as StructFormer [4] and StructDif- fusion [5] find appropriate positions for objects guided by natural language instructions. Both methods take language tokens and object point clouds as inputs to find arrangements that satisfy language conditions. Studies such as [15] and [16] learn user preferences to find organized arrangements without explicit goals. For instance, [15] uses scene graphs to encode scenes and learns user preference vectors, and [16] addresses tasks involving the organization of various items into containers or shelves, learning pairwise preferences of objects. These studies rely more on semantic information rather than visual information of the objects. There exist dif- fusion based methods to directly generate final arrangement images [17], [18]. These studies rely on the commonsense knowledge inherent in large language models (LLMs) and vision language models (VLMs) to find arrangements that are similar to human intentions.\nSimilar to the current work, studies such as [11] and [12] learn to quantify the degree of tidiness with a score function. [11] uses an energy-based model to learn and predict the cost, which is most relevant to our work. While [11] focused on finding positions for just one missing object, our study plans to find the optimal state by moving all movable objects on a table. [12] learns a score function to calculate the likelihood with the target distribution for each task, using this score to learn a policy for rearranging objects. Each task requires a separate target distribution, and the score function is trained separately for each task, whereas our study uses a single score function to tidy up across various environments.\nLanguage-guided Monte-Carlo tree search (LGMCTS) [6] uses the MCTS algorithm to find trajectories to obtain arrangements that satisfy language conditions. LGMCTS assumes that explicit spatial conditions can be derived based on language conditions. They first establish these spatial conditions and then find a trajectory that arranges the objects to satisfy all these conditions. In this paper, we propose an algorithm that learns a score function to find various tidied arrangements without the guidance of language."}, {"title": "III. PROBLEM FORMULATION", "content": "In the tabletop tidying up problem, an agent (in our case, a robot) M is tasked to rearrange a set of movable objects $O = {o_1, o_2, ..., o_N}$ to achieve a tidy arrangement.\nAt each timestep, the agent receives a single top-down view RGB-D image from a fixed overhead camera. The workspace is planar and all objects are assumed to be rigid bodies. The robot M can interact with objects through pick- and-place actions to perform arbitrary translation and rotation changes.\nWe assume there is a tidiness-score function $\\Psi$ which returns the degree of tidiness given an image of the tabletop with objects. This function assesses whether objects on the table are visually tidied up, considering the types, shapes, and sizes of the objects, and assigns a tidiness score between 0 and 1, where 0 represents a completely messy scene and 1 indicates a well-arranged scene.\nFor a given set of objects O, the visual observation depends on the pose of the objects. Therefore, we can formulate the tidiness score as $\\psi = \\Psi(O,P)$, where P denotes the 6-DoF positions of O. Then we can formulate the objective of the table tidying problem as finding the optimal arrangement P* to maximize the tidiness score:\n$P^* = arg \\max_P \\Psi(O, P)$.\t\t(1)\nIn this paper, we parameterize a tidiness-score function with neural networks $\\theta$, as a discriminator $\\Psi_\\theta$. $\\Psi_\\theta$ is trained to estimate the tidiness-score of a tabletop arrangement image."}, {"title": "IV. TABLETOP TIDYING UP DATASET", "content": "We collect a Tabletop Tidying Up (TTU) dataset which includes both tidied and messy scenes to train a vision- based tidiness discriminator. To cover diverse object arrange- ments, we define a set of environments E consisting of four environments: Coffee table, Dining table, Office desk, and Bathroom. For each environment e \u2208 E, we define a set of objects Oe \u2286 Oall belonging to that environment, where Oall is the entire set of objects. Then, we predefine possible combinations of objects within Oe, each consisting of two to nine objects.\nIn this study, we introduce the concept of a template to en- courage automatic collection of well-organized arrangement data. A template is defined as a specific set of spatial relation- ships between objects, categorized as one of the following: on, under, left, right, front, behind, left-front, left-behind, right-front, and right-behind. Figure 2-a shows examples of templates and their corresponding tidied arrangements for a set of objects O = {knife, fork, plate, cup}. An arrangement where the fork is to the left of the plate and the knife to the right could all be considered as belonging to template A. Meanwhile, an arrangement where both the fork and knife are neatly placed on the left side of the plate would fall under template B. By defining templates in this manner, we can represent all tidied arrangements that a person might create with specific templates."}, {"title": "V. PROPOSED METHOD", "content": "The proposed framework, the Tidiness Score-guided Monte Carlo Tree Search (TSMCTS), consists of two com- ponents: (1) training the tidiness discriminator and tidying policy, and (2) planning the tidying up process using MCTS.\nWe trained a tidiness discriminator and tidying policy using the TTU dataset described in the previous section. The tidiness discriminator learns a score function that evaluates the tidiness score of the current state. The tidying policy is used as a tree policy in the MCTS algorithm to efficiently sample appropriate actions from the entire feasible action space. We trained the tidiness discriminator in a supervised manner and the tidying policy using the Implicit Q-Learning (IQL) framework.\nFinally, starting from the initial configuration (O, P), we iteratively find pick-and-place actions by planning with the MCTS algorithm using the tidiness discriminator as a utility function and the tidying policy as a tree policy, until all the objects on the table are tidied up.\n**A. Tidiness Discriminator and Tidying Policy**\nThe training process of the tidiness discriminator and the tidying policy is illustrated in Figure 3-a. We parameterized the tidiness score function using neural networks $\\Psi_\\theta: S \\rightarrow [0,1]$ and the tidying policy $\\pi_\\rho: S \\rightarrow A$, where S and A represent the state space and action space, respectively. In this paper, the state is represented as an RGB image of the table, while the action corresponds to a pick-and-place operation, defined by the target object, placement position, and rotation angle.\nFrom the TTU dataset, we can obtain sequences of state and tidiness score pairs $((s_1, \\psi_1), ..., (s_T, \\psi_T))$, where T denotes the length of collected trajectories. Here, $s_1$ is the most messy scene and $s_T$ is the final tidied up scene. Finally, we train the discriminator using pairs of states and score labels, $D_{Disc} = {(s_t, \\psi_t)_i}$, employing the mean squared error as the loss function:\n$L(\\theta) = E_{s_t} [(\\Psi_\\theta(s_t) \u2013 \\psi_t)^2]$.\t\t(3)\nFor policy training, we use the Implicit Q-Learning (IQL) method. We use a sparse binary reward as below:\n$\\tau_t = \\begin{cases} 1, & \\text{if the episode ends (t = T)} \\\\ 0, & \\text{otherwise} \\end{cases}$ (4)\nand obtain offline data $D_{RL} = {(s_t, a_t, s_{t+1},r_t)}$ from TTU dataset. In IQL method, we learns Q-function $Q_\\phi$, value function $V_\\varphi$ and the policy $\\pi_\\rho$ simultaneously. The loss functions for $V_\\varphi$ and $Q_\\phi$ are computed according to the modified TD learning procedure in IQL,\n$L_V(\\varphi) = E_{s,a} [L_\\tau(Q_\\phi(s, a) \u2013 V_\\varphi(s))]$,\t\t(5)\nwhere $L_\\tau(u) = |\\tau \u2013 1_{(u < 0)}|u^2$ represents the expectile regression loss, with \u03c4 = 0.7 used as the default value. $Q_\\phi$ is a target network, which is a lagged version of $Q_\\phi$.\n$L_Q(\\phi) = E_{s,a,s',r} [(r + \\gamma V_{\\varphi}(s') \u2013 Q_\\phi(s,a))^2]$.\t\t(6)\nThen, the policy extraction step can be applied using advan- tage weighted regression:\n$L_\\pi(\\rho) = E_{s,a} [exp(\\beta(Q_\\phi(s, a) \u2013 V_{\\varphi}(s))) log \\pi_\\rho(a|s)]$,\t\t(7)\nwhere \u03b2 denotes the inverse temperature.\nWe employ a pre-trained ResNet-18 as the backbone of the tidiness discriminator, replacing its final fully connected layer with one that predicts a single tidiness score. The tidiness discriminator takes the current table image st as input and outputs the corresponding tidiness score $\\Psi_\\theta(st)$. We use the Segment Anything Model (SAM) [21] to remove the background to learn a more consistent score function.\nFor the tidying policy, two inputs are used for each object $o_i, i = 1, ..., N$: (1) the patch image of the object, $P(o_i)$, and (2) the table image without the object, $I_{-o_i}$. The policy outputs a probability distribution over pixel positions and rotations for placing the target object. As shown in Figure 3-c, the policy networks extract separate features for the table $F(I_{-o_i})$ and the object $F(P(o_i))$ using ResNet- 18 networks. The 32-dimensional object feature $F(P(o_i))$ is then expanded to match the size of the table feature and concatenated with it to form the combined feature, $cat(F(I_{-o_i}), F(P(o_i)))$. This combined feature is processed through fully convolutional networks to generate an H \u00d7 W \u00d7 R probability distribution, where R denotes the number of possible rotations. The tidying policy repeats this process for all N objects in parallel to produce an N \u00d7 H \u00d7 W \u00d7 R action probability distribution.\n**B. Low-Level Planner**\nTo discretize the pick-and-place action, we divide the workspace into an H \u00d7 W grid map and split the 360\u00b0 ro- tation into R bins. Then, we define a pick-and-place action as a = (o,p), where o denotes the target object and p = (x, y, r) represents the placement position. Here, x and y are the selected pixel position, and r is the rotation index. When picking up objects, we use Contact-GraspNet [22], which processes 3D point clouds from RGB-D images to output 6- DoF grasping points for the robot arm\u2019s end effector. When placing objects, even if they are placed in the same location, they can appear tidy or completely disordered depending on the rotation of the objects. We observe that objects appear more organized to humans when aligned with the table\u2019s x-axis or y-axis. To achieve this, we fit an ellipse to the object\u2019s segmentation mask and use its major axis as the default rotation axis. This process is illustrated in Figure 4. First, the agent receives a top-down view RGB image of the table and uses SAM [21] to obtain a segmentation mask for each object. Next, we use the least squares method to find an ellipse that fits each object and determine its major and minor axes. The robot\u2019s placement action allows the object to be aligned with the table\u2019s horizontal or vertical axis.\n**C. Tidiness Score-Guided High-Level Planner**\nWe utilize Monte Carlo Tree Search (MCTS) as a high- level planner. MCTS is a search algorithm to solve decision- making processes for deterministic problems. For the im- plementation of MCTS, it is necessary to recognize the dynamics from a state st to the next state st+1 after an action is performed. Instead of using a separate simulator to get the predicted next state \u015dt+1, our method generates \u015dt+1 by directly moving each object\u2019s image patch on the initial RGB image. Although \u015dt+1 obtained through this method may differ from the state achieved by physically performing a pick-and-place action, we demonstrate that this approach achieves an 85% success rate in real world experiments, indicating its robustness despite potential errors. Figure 5 shows the process of next state prediction and the sequence of expected and actual states during real world evaluation.\nOur high-level policy leverages the trained tidiness dis- criminator and tidying policy to guide MCTS in finding the most efficient action sequence for tabletop tidying up. The overall inference process of TSMCTS is shown in Figure 3-b. For each timestep t, the agent receives the state which consists of the current RGB-D images. Then TSMCTS builds a search tree with the root node representing the current state st. Starting from the initial tree, TSMCTS repeats the following four steps K times to complete the tree: Selection, Expansion, Simulation, and Backpropagation.\nSelection: Starting from the root node, TSMCTS selects child nodes until it reaches a leaf node. At each node s, TSMCTS selects an action a based on the UCT function, given as follows:\n$U(s,a) = \\frac{Q(s)}{N(s,a)} + c \\sqrt{\\frac{2 log N(s)}{N(s,a)}}$ (8)\nwhere c is the exploration term. N(s) denotes the number of visits to node s, and N(s, a) denotes the number of times action a has been executed at node s. Q(s) denotes the cumulative reward of node s, where the reward is assigned during the Backpropagation step.\nExpansion: If TSMCTS reaches a leaf node sleaf, it adds a child node to the tree. To expand the tree at the leaf node, we use the trained tidying policy \u03c0\u03c1 to sample actions from the action space, $a \\sim \\pi_\\rho(\\cdot|s_{leaf})$. Here, the action a represents a pick-and-place action, a = (o,p). As mentioned above, we create the new child node snew by moving the image patch of object o to the position p in the RGB image of sleaf.\nSimulation: We leverage the trained tidiness discriminator to predict the expected value of the expanded node snew as $V(s_{new}) = \\Psi_\\theta(s_{new})$, where $\\Psi_\\theta$ denotes the tidiness discriminator. Additionally, we obtain the outcome z(snew) from a random rollout by executing \u03c0\u03c1 until the terminal step Trollout. The outcome z(snew) is set to 1 if the final state of the rollout is fully tidied up and 0 otherwise.\nBackpropagation: TSMCTS backpropagates Q-value up- dates from the newly expanded node snew back to the root node. For each node s and action a along the path, the Q- value updates are performed as follows:\n$N(s) \\leftarrow N(s) + 1,$\n$N(s,a) \\leftarrow N(s, a) + 1,$\t\t(9)\n$Q(s, a) \\leftarrow Q(s, a) + \\alpha (1 \u2013 \\lambda)V(s_{new}) + \\lambda z(s_{new})$.\nWe use \u5165 = 0.3, where \u5165 denotes the mixing parameter.\nAfter the tree search is completed, TSMCTS selects the most visited child node of the root node as the best action. Then, the high-level action is converted into low-level actions by the low-level planner to control the robot."}, {"title": "VI. EXPERIMENTS", "content": "**A. Evaluation of Tidiness Discriminator**\nWe evaluate whether the trained tidiness discriminator generalizes well to unseen objects and unseen configurations beyond the training data. We divide the 224,225 tidying data into 162,000 training data and 62,225 validation data. The validation data contains unseen objects and templates from the training data. To determine whether a scene is fully tidied up, we define a tidiness threshold \u00a7, ranging from 0 to 1. During the experiments, a task is considered successful if the tidiness score exceeds \u00a7. The tidiness threshold is crucial for determining success: lowering increases recall by classifying more scenes as well-tidied but reduces precision due to more false positives.\nTo determine an appropriate threshold, we analyze the classification performance of the tidiness discriminator across varying tidiness threshold values. The recall and precision measured on the validation set as functions of the tidiness threshold are illustrated in Figure 6. Additionally, we conduct a human evaluation to ensure that the tidiness threshold aligns with human perceptions of tidiness. We present 20 randomly selected sequences from 50 tidying sequences organized by TSMCTS to 17 participants, asking them to choose the scenes they judge to be tidied up. For each sequence, we define the tidiness threshold as the lowest tidiness score among the scenes that participants judge to be tidied up. The average thresholds for each environment are presented in Table II. As a result, people judge that the table is tidied up at an average tidiness score of 0.8486. Looking at the environment, the threshold for the Dining table is higher at 0.9017 compared to other environments. This appears to be because people consider the arrangement more organized when the tableware and cutlery are placed according to their functional uses. Based on these results, we determine that a threshold of 0.85 is an appropriate value. The trained tidiness discriminator achieve a recall of 71.8% and a precision of 92.2% on the validation data using this threshold.\nWe conduct another human evaluation to verify how well the trained tidiness score reflects the actual perception of tidiness by humans. We sample 10 scene data for each tidiness score interval from 0 to 1 at 0.1 intervals, resulting in a total of 100 scenes. Then, we ask 17 participants to view 15 randomly selected scenes from the 100 scenes and rate the degree of tidiness on a scale of 1 to 5. The correlation between the tidiness score and human ratings is shown in Figure 6. We observe a strong positive correlation between the tidiness score and human ratings. We also find a tendency for the variance in human ratings to increase as the tidiness score rises. This is likely because the standards for tidiness are highly subjective and vary from person to person.\n**B. Simulation Experiments**\nWe use the PyBullet simulator for the simulation experi- ments. In the simulator, a workspace table and a UR5 robot are set up. As the initial states, random objects are spawned on the table in random positions and orientations. We use 3D object models from the YCB and HouseCat6D datasets, along with 10 additional object models and four extra object categories not included in the training set of the TTU dataset.\nWe evaluate TSMCTS in simulation across five environ- ments by adding, Mixed, a mixed table environment to the original four: Coffee table, Dining table, Office desk, and Bathroom. For each environment, we tested 150 scenarios with varying object compositions and initial placements. We use the tidiness threshold 0.85 defined in the previous section for the success criteria. A failure is noted if objects are placed outside the workspace, collide and overlap, or if tidying is not completed within 10 steps. We measured the tidying success rate, the tidiness score of the final state, and the number of steps taken. The experimental results are presented in Table III. In the Coffee table environment, which has a diverse and complex set of objects, a success rate of 79.3% and an average tidiness score of 0.889 are achieved, while in the Dining Table environment, which has more standardized object templates, a higher success rate is observed compared to other settings. Additionally, high success rates and tidiness scores are also achieved in mixed object configurations, which are not part of the training data. TSMCTS demonstrates its ability to successfully find arrangements that meet tidiness conditions across a variety of environments and object configurations.\nFor baseline comparisons, we evaluate TSMCTS compar- ing StructFormer [4] and StructDiffusion [5]. Both Struct- Former and StructDiffusion are algorithms that find ar- rangements matching given conditions, based on language tokens related to goals. While their setup is different from ours, both studies include tasks for organizing a dining table, so we conduct comparative experiments exclusively in the Dining table setting. Additionally, we perform an ablation study on the tidiness discriminator by comparing TSMCTS with TSMCTS-binary. TSMCTS-binary utilizes a tidiness discriminator trained with binary labels from the TTU dataset, where completely tidied scenes are labeled as 1, and all other scenes are labeled as 0."}, {"title": "VII. CONCLUSION", "content": "In this paper, we have introduced the TSMCTS framework, a tidiness score-guided Monte Carlo tree search for tabletop tidying up. TSMCTS is a framework that uses a tidiness discriminator to assess current and future table tidying states, generates a search tree according to the tidying policy, and finds the optimal arrangement for tidying up. To train the tidiness discriminator and tidying policy, we have collected the TTU dataset, a structured dataset that includes tidying sequence data across various environments. We have shown experimental results that TSMCTS has robust tidying capa- bilities across various object configurations including unseen objects and duplicate objects. In addition, we have success- fully transferred TSMCTS to the real world without any transferring efforts. Despite the satisfactory results, there also exist limitations in the proposed method. Since TSMCTS assumes a 2D arrangement, it cannot perform tidying that involves stacking objects in layers. Additionally, the tidiness discriminator relies on visual information, which often leads to a lack of consideration for the functional uses of objects. In future work, we plan to leverage large language models (LLMs) as guidance to better handle ambiguous cases and resolve scenarios where the functional use or arrangement of objects is unclear."}]}