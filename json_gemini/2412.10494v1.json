{"title": "SnapGen-V: Generating a Five-Second Video within Five Seconds on a Mobile Device", "authors": ["Yushu Wu", "Zhixing Zhang", "Yanyu Li", "Yanwu Xu", "Anil Kag", "Yang Sui", "Huseyin Coskun", "Ke Ma", "Aleksei Lebedev", "Ju Hu", "Dimitris N. Metaxas", "Yanzhi Wang", "Sergey Tulyakov", "Jian Ren"], "abstract": "We have witnessed the unprecedented success of diffusion-based video generation over the past year. Recently proposed models from the community have wielded the power to generate cinematic and high-resolution videos with smooth motions from arbitrary input prompts. However, as a supertask of image generation, video generation models require more computation and are thus hosted mostly on cloud servers, limiting broader adoption among content creators. In this work, we propose a comprehensive acceleration framework to bring the power of the large-scale video diffusion model to the hands of edge users. From the network architecture scope, we initialize from a compact image backbone and search out the design and arrangement of temporal layers to maximize hardware efficiency. In addition, we propose a dedicated adversarial fine-tuning algorithm for our efficient model and reduce the denoising steps to 4. Our model, with only 0.6B parameters, can generate a 5-second video on an iPhone 16 PM within 5 seconds. Compared to server-side models that take minutes on powerful GPUs to generate a single video, we accelerate the generation by magnitudes while delivering on-par quality.", "sections": [{"title": "1. Introduction", "content": "Recently, the rapid advancement of video diffusion models [4] inspires revolutions in content creation. With the emergence of video models from industry [41] and research community [67], content creators can animate a static image [3] or generate cinematic videos from arbitrary prompts [55, 67, 77]. Video diffusion models also enable downstream applications like video editing [12, 20, 30], novel view synthesis [25, 57], and multi-modal generation [56].\nDespite the success in generation quality, the huge number of parameters and slow generation speed prohibit the wide deployment of video diffusion models. For instance, CogVideoX-5B [67] generates a video (49 frames at 8 fps, 720 x 480 resolution) in 5 minutes with 50 inference steps on an NVIDIA A100 GPU. Compared to text-to-image diffusion models [45], video diffusion models require extra parameters to model sophisticated motions [3, 13, 62]. In addition, video data usually incurs higher spatial-temporal resolution for UNet denoisers [3, 13], or equivalently more tokens for DiT models [40], which adds up to the computation complexity. Recent works explore efficient model architectures and attention mechanisms for image diffusion models [8, 64, 70]. However, there is little effort in the literature dedicated to accelerating and deploying video models at scale, especially for mobile devices."}, {"title": "2. Related Work", "content": "Video Diffusion Models. Denoising Diffusion Probabilistic Models [17] is the trending paradigm for building video diffusion models, demonstrating photorealistic quality and generic generation capabilities. Pioneer works often start from a pre-trained text-to-image diffusion model [45] and insert temporal layers to model motions along frame sequence [3, 4, 13, 62, 74]. In addition, training-free noise tuning techniques are proposed to ease the alignment between frames [23, 34, 43, 63]. Later, with the emergence of large-scale, high-quality video datasets [7, 38] and Transformer backbones [40], subsequent works curate their own dataset and build large video diffusion models with exceptional quality, such as the open-sourced CogVideoX [67], Mochi 1 [55], PyramidalFlow [21], Allegro [77], and close-sourced ones including Hailuo [37], Runway Gen 3 Alpha [46], Kling [24], Luma Dream Machine [1], Pika 1.5 [2], Sora [39], and MovieGen [41]. Remarkably, the open-sourced projects Open-Sora [76] and Open-Sora-Plan [26] provide the community with reliable implementations to replicate large-scale video diffusion models.\nDividing by task type, video diffusion models can be categorized into text-to-video generation [5, 6, 13, 15, 18, 21, 27, 28, 42, 55, 56, 67, 77], image-to-video generation [3, 78], or specific motion controls [14, 44, 61, 71, 75]. Though some work [8, 29, 70] aim to improve the efficiency streamlined, continuous video generation, which we leave for future work."}, {"title": "3. Method", "content": "Our objective is to achieve high-fidelity and temporally consistent video generation on mobile devices. However, current text-to-video diffusion models face two key challenges in reaching this goal: (a) the memory and computation requirement is beyond the capability of even the most powerful mobile chips, i.e. iPhone A18 Pro, and (b) denoising with dozens of steps to generate a single output further slows down the process. To address these challenges, we propose a three-stage framework to accelerate video diffusion models on the mobile platform. First, we prune from a pre-trained text-to-image diffusion modelwto obtain an efficient spatial backbone. Second, we inflate the spatial backbone with a novel combination of temporal modules which are searched out with our mobile-oriented metrics. eFinally, through adversarial training, our efficient model attains the capability to generate high-quality videos in only 4 steps."}, {"title": "3.1. Preliminaries", "content": "Following [76], we employ a spatial-temporal VAE to compress image and video data into the latent space. Given video or image data $v \\in \\mathbb{R}^{n \\times 3 \\times H \\times W}$, where n is the number of frames with height H and width W, the spatial-temporal encoder, E, maps the data to a latent space. The encoded frames are represented as $x_0 = E(v)$, resulting in $x_0 \\in \\mathbb{R}^{\\tilde{n} \\times 4 \\times \\hat{H} \\times \\hat{W}}$. Here, $x_0 \\sim P_{data}(x_0)$ is a 4-channel latent. with a temporal compression of $\\tilde{n} = n/4$, and spatial compression as $\\hat{H} = H/8, \\hat{W} = W/8$.\nWe follow Rectified Flow [59] to train our latent diffusion model. According to the flow-matching-based diffusion form, the intermediate noisy state $x_t$ is defined as:\n$x_t = (1 - t) x_0 + t\\epsilon, \\text{ where } \\epsilon \\sim \\mathcal{N}(0, I),$ (1)\nwhich is a linear interpolation between the data distribution and a standard normal distribution. The model aims to learn a vector field $v_\\theta(t, x_t)$ using the Conditional Flow Matching objective, i.e.,\n$\\mathcal{L} = \\mathbb{E}_{t, \\epsilon, x_0} ||v_\\theta (t, x_t) - u_t(x_t | x_0)||^2,$ (2)\nwhere $u_t(x_t | x_0) = \\frac{\\epsilon - x_0}{\\sqrt{t^2 + \\sigma^2}}$. Following [10], during training, we sample t from a logit-normal distribution, i.e.,\n$\\pi(t; m, s) = \\frac{1}{s\\sqrt{2\\pi} t (1 - t)} \\exp\\left(-\\frac{(\\text{logit}(t) - m)^2}{2s^2}\\right),$ (3)\nwhere $\\text{logit}(t) = \\log \\frac{t}{1-t}$, m and s are the location parameter and scale parameter, respectively."}, {"title": "3.2. Hardware Efficient Model Design", "content": "Spatial Backbone. We follow [11, 29] to first prune an efficient text-to-image model as the spatial backbone. Specifically, we start from Stable Diffusion v1.5 [45], and borrow the knowledge from prior arts [29] to remove the most mobile-unfriendly attentions. We then prune the network depth and width following [11] and achieve \u00d72.5 size reduction and more than 10\u00d7 speedup on mobile devices. We include qualitative visualizations of our image model in the supplementary material. Note that we use a UNet denoiser [17], leaving the exploration of DiT [40] to future work. The hierarchical structure of the UNet denoiser forms a good search space to achieve mobile efficiency, while the computation complexity of DiT grows quadratically with the number of tokens (generation resolution), making it challenging for mobile deployment.\nTemporal Layer Design. Current latent video diffusion models typically adopt temporal self-attentions [13], cross-attentions [76], and convolutions [3] to model temporal dependencies. CogVideoX [67] demonstrates significant performance gain by using full 3D-Attention, at the cost of more computations and memory consumption. In this work, we enumerate and investigate all types of temporal modeling methods, including Conv1D, Conv3D, SelfAttention1D, SelfAttention3D, CrossAttention1D, and CrossAttention3D, and profile their complexity in Fig. 3. For instance, SelfAttention1D only models temporal dependency on a single coordinate, while SelfAttention3D models global dependencies and has the potential to deliver much stronger performance. However, the computation complexity of SelfAttention3D grows quadratically with respect to t \u00d7 H \u00d7 W, while SelfAttention1D is linear with respect to H and W, which makes SelfAttention3D much more costly at higher resolutions. On the other hand, the computation of CrossAttentionND is determined by both spatial-temporal resolution and the number of tokens from the text encoder. Conv1D and 3D are locality alternatives for SelfAttention1D and 3D, respectively. Though the computation complexity and memory footprint for each design candidate can be easily profiled, as in Fig. 3, it is still crucial and challenging to build a spatial-temporal network with optimized arrangements of these operators. We propose to perform a latency-memory joint architecture search to determine which, where, and the number of temporal layers to use for our efficient video diffusion model on mobile, as follows.\nLatency and Memory Guided Architecture Search. Prior to searching the architecture, we construct a look-up table containing the inference latency and the memory footprint of different temporal layers. For each candidate operator (i.e., ConvND, SelfAttentionND, CrossAttentionND), we benchmark the latency and memory consumption under different spatial-temporal resolutions on hardware. We then clean the search space by eliminating OOM states. Then we perform evolutionary search to obtain the temporal design with Pareto optimality. The architecture candidate is trained on precomputed video latents for 20K iterations with the spatial backbone frozen, and is evaluated on VBench [19] to obtain the scores as the quality metric. We include the detailed search algorithm, action space, and total search time in the supplementary material."}, {"title": "3.3. Latent Adversarial Fine-tuning", "content": "Our training procedure involves two networks: a generator $G_\\theta$ and a discriminator $D_\\phi$. Similar to prior work [51, 73], we initialize our generator with pre-trained diffusion model weights $\\theta$, while the discriminator is also partially initialized from $\\theta$. Specifically, the backbone of the discriminator adopts the same architecture and weights as the pre-trained UNet encoder, with these backbone parameters remaining frozen during training. Additionally, we enhance the discriminator with spatial-temporal discriminator heads added after each backbone block, with only these head parameters being updated in the discriminator training phase. As illustrated on the right in Fig. 2, each discriminator head consists of a spatial ResBlock and a temporal self-attention block. This design allows our discriminator to effectively handle both image and video data during fine-tuning. We analyze the impact of joint image-video fine-tuning in Sec. 4.3.\nFor a real data sample $x_0$, a noisy data point $x_t$ is generated through a forward diffusion process, as described in Eq. (1). We set intermediate timesteps as $0 < T_k < \\cdots < T_1 = 1.0$ and sample t from these timesteps, where k is the number of timesteps selected for generator training (set to k = 4 in practice). The generator, then, predicts the velocity at $x_t$ as $G_\\theta (x_t, t)$.\nTo train the discriminator, we first sample a target timestep $t'$ from a logit-normal distribution, as shown in Eq. (3). Using the forward process in Eq. (1), we obtain the real sample $x_{t'} = (1 - t') x_0 + t' \\epsilon$. The fake sample, $x_{t'}$, is computed as $x_{t'} = x_t + (t' - t) \\cdot G_\\theta (t, x_t)$, as shown in Fig. 4. Following established approaches [48-50, 73], we employ hinge loss [31] as the adversarial objective to enhance performance. The discriminator's goal is to differentiate between real and fake samples by minimizing:\n$\\begin{aligned} \\mathcal{L}_{D_\\phi} = &\\mathbb{E}_{t', x_0} [\\max (0, 1 + D_\\phi (x_{t'}, t'))] + \\\\ &\\mathbb{E}_{t, t', x_0} [\\max (0, 1 - D_\\phi (x_{t'}, t'))], \\end{aligned}$ (4)\nThe adversarial objective for the generator is defined as:\n$\\mathcal{L}_{adv} = \\mathbb{E}_{t, t', x_0} [D_\\phi (x_{t'}, t')].$ (5)\nFollowing [73], we also incorporate a reconstruction objective to enhance stability, defined as:\n$\\mathcal{L}_{recon} = \\sqrt{||\\hat{x}_0 - x_0||^2 + c^2} - c,$ (6)\nwhere $\\hat{x}_0 = x_t - t \\cdot G_\\theta (t, x_t)$, and $c > 0$ is an adjustable constant.\nDiscussion. Our latent adversarial training pipeline is inspired by SF-V [73]. Similar to SF-V, we set k = 4 and utilize the part of the pre-trained diffusion model as the backbone for the discriminator. However, our approach introduces several key differences. First, our method is built on an efficient UNet specifically designed for mobile devices, with fewer parameters than SVD [3], making it a more challenging task. Second, we redesign the discriminator heads: instead of using separate spatial and temporal heads, we integrate them into a unified spatial-temporal head for adversarial training. Rather than handling the temporal dimension separately with 1-D convolutional kernels, we incorporate a temporal self-attention layer into the spatial discriminator head after the 2-D ResBlock, forming a spatial-temporal discriminator head. This unified design enables our model to be jointly trained on both image and video data, which, as demonstrated in Sec. 4.3, significantly enhances the performance of the fine-tuned model."}, {"title": "4. Experiments", "content": "Training. The efficient image backbone is obtained by pruning the Stable Diffusion v1.5 UNet for 100K iterations on high-quality synthetic image datasets. The model is then fine-tuned for 50K additional iterations to adapt to Rectified-Flow velocity prediction [10] as well as to the Spatial-Temporal VAE [76]. We incorporate QK-norm and ROPE [54] in our network to stabilize training. The workflow for architecture search is discussed in Sec. 3.2. The image model pruning, temporal architecture search, and final model training are conducted on 256 NVIDIA A100 80G GPUs using AdamW optimizer with 5e-5 learning rate and betas values as [0.9, 0.999].\nAdversarial Fine-tuning is conducted for 6K iterations on 64 NVIDIA A100 GPUs, using the AdamW optimizer with a learning rate of $l_e = 7$ for the generator (i.e., UNet) and $l_e = 4$ for the discriminator heads. We set the betas as [0.9, 0.999] for the generator optimizer, and [0.5, 0.999] for the discriminator optimizer. We set the EMA rate as 0.95 following SF-V [73]. We set m = \u22121, s = 1 if not otherwise noted.\nEvaluation. The model is evaluated following the standard benchmarking procedure of VBench [19]. With the 4-step adversarial distilled model, we generate 120-frame horizontal videos at a resolution of 432 \u00d7 768 using 4 inference steps without employing classifier-free guidance. The generated video is saved at 5 seconds 24 FPS for score testing and qualitative visualization. The mobile demo and detailed demo settings are included in the supplementary material."}, {"title": "4.1. Qualitative Visualization", "content": "We show visualizations of our generated videos in Fig. 5. Our model consistently produces high-quality video frames and smooth object movements. To demonstrate the generic text-to-video generation ability, we show various generation examples, including human, animal, photorealistic and art-styled scenes. We include more video visualizations in the supplementary material."}, {"title": "4.2. Quantitative Comparisons", "content": "We present a comprehensive evaluation of our method against existing popular video generation models on VBench [19], as in Tab. 2. Despite the fact that our model is compact and designated for fast inference on mobile, it achieves higher total score compared to recent arts, including the DiT-based OpenSora-V1.2, CogVideoX-2B [67], and the UNet-based VideoCrafter-2.0 [6]. In addition, compared to the 4-step distilled T2V-Turbo [27] and AnimateLCM [58], our model achieves better performance with more than 50% reduction in size. The quantitative scores demonstrate the superiority of our efficient model design and the tailored adversarial distillation method."}, {"title": "4.3. Ablation Analysis", "content": "Comparison of Training Data Scheme. We compare the model trained with joint image-video datasets vs. video-only datasets. As shown in Tab. 3, training with video-only datasets leads to significant performance degradation on the VBench score with a drop of 6.32 in aesthetic quality, 2.80 in image quality, and 2.54 in total score. The results highlight the importance of joint image-video training, as the image dataset offers more contextual information and enhances diversity.\nModel Scaling. We scale up and down the model size by adjusting the number of temporal layers, as shown in Tab. 3, to demonstrate the effectiveness of the proposed temporal architecture search. We can observe that scaling up the model can only marginally improve generative scores (i.e., \u00d72 scale-up only increasing 0.56 in dynamic degree and 0.12 in motion smoothness, and 0.27 in total score). However, both the \u00d72 and \u00d74 models hit the memory bound on iPhone. By dividing the \u00d72 model into more chunks, we test its mobile speed and observe nearly doubled latency. While on the other hand, further scaling down the model results in heavy losses in generation quality (i.e., decreasing 0.56 in dynamic degree and 0.61 in motion smoothness). Our efficient model is a balanced sweet point for quality and on-device performance.\nEffect of Different Temporal Layers. To better understand the roles of the searched temporal layers, we compute the VBench score after systematically removing (i) all temporal layers in the downsample stage, (ii) bottleneck temporal layers, and (iii) all temporal layers in the upsample stage. The model makes reasonable zero-shot generations after temporal layer removal, but we still fine-tune 10K iterations under the same recipe as in Sec. 3.2 for fair comparison. As shown in Fig. 6, removing different temporal layers results in varying degrees of performance degradation across different metrics, and all removing strategies result in a substantial drop in total score, demonstrating that the existence of the searched temporal layers is important, and they play different roles in generation. Specifically, removing temporal layers in upsample blocks results in a more significant loss in imaging quality, subject consistency, and background consistency, suggesting that the up temporal layers play important roles in detail reconstruction. In contrast, bottleneck layers are more important in human action and object class, where global information modeling dominates the results. We observe that removing down layers introduces less overall degradations compared to the other two, which is an anticipated phenomenon because the loss of modeling capacity can be mitigated by the subsequent bottleneck and up stage temporal layers after fine-tuning."}, {"title": "5. Discussion and Conclusion", "content": "In this work, we propose an acceleration framework for the video diffusion model, and for the first time, achieve super-fast text-to-video generation on mobile devices. Specifically, we discover an efficient but powerful network architecture through latency and memory joint architecture search for temporal layers. In addition, we propose an improved adversarial fine-tuning technique to distill our model to 4 steps to further speed up generation. Our work is a good starting point for the edge deployment of video diffusion models and we hope to inspire more downstream applications such as video extension and editing.\nLimitations. We use a public 4-channel VAE [76] to encode videos to latent space. Recent work has shown that using more latent channels benefits reconstruction details. Another future direction is to further improve the step reduction technique for 1-2 denoising steps, as works [73] on server-level models."}]}