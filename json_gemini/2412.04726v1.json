{"title": "BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English", "authors": ["Dipankar Srirag", "Aditya Joshi", "Jordan Painter", "Diptesh Kanojia"], "abstract": "Despite large language models (LLMs) being known to exhibit bias against non-mainstream varieties, there are no known labeled datasets for sentiment analysis of English. To address this gap, we introduce BESSTIE, a benchmark for sentiment and sarcasm classification for three varieties of English: Australian (en-AU), Indian (en-IN), and British (en-UK). Using web-based content from two domains, namely, Google Place reviews and Reddit comments, we collect datasets for these language varieties using two methods: location-based and topic-based filtering. Native speakers of the language varieties manually annotate the datasets with sentiment and sarcasm labels. Subsequently, we fine-tune nine large language models (LLMs) (representing a range of encoder/decoder and mono/multilingual models) on these datasets, and evaluate their performance on the two tasks. Our results reveal that the models consistently perform better on inner-circle varieties (i.e., en-AU and en-UK), with significant performance drops for en-IN, particularly in sarcasm detection. We also report challenges in cross-variety generalisation, highlighting the need for language variety-specific datasets such as ours. BESSTIE promises to be a useful evaluative benchmark for future research in equitable LLMs, specifically in terms of language varieties. The BESSTIE datasets, code, and models are currently available on request, while the paper is under review. Please email aditya.joshi@unsw.edu.au.", "sections": [{"title": "1 Introduction", "content": "Benchmark-based evaluation [34] of large language models (LLMs) is the prevalent norm in natural language processing (NLP). Benchmarks provide labeled datasets [25, 36] for specific tasks so that LLMs can be evaluated against them. However, most NLP benchmarks do not contain text in language varieties such as national varieties, dialects, sociolects or creoles [18, 23, 31]. As a result, despite the superlative performance of LLMs on Standard American English, LLMs are not evaluated on other varieties of English. Knowing that LLMs can be biased against certain language varieties (as shown by [7] for African-American English), little empirical evidence is found for their bias towards several language varieties, particularly those of English. An exception is Multi-VALUE [39] which creates synthetic datasets of language varieties using linguistically-informed syntactic transformations and reports a degraded performance on the GLUE benchmark. However, such synthetic texts are insufficient in measuring language variety bias because language varieties are more than syntax and encompass orthography, vocabulary, and cultural pragmatics [27]. As a result, labeled datasets of natural text are crucial to measure the bias in LLMs towards these varieties.\nTo address this gap, we introduce BESSTIE, A BEnchmark for Sentiment and Sarcasm classification for varieTIes of English, namely, Australian (en-AU), Indian (en-IN), and British (en-UK). BESSTIE comprises a manually labeled dataset of textual posts along with performance results of fine-tuned LLMs on the two classification tasks. The dataset consists of text samples collected from two web-based domains, namely, Google Places reviews, and Reddit comments, using two filtering methods: location-based and topic-based filtering respectively. Following that, we obtain manual annotations for boolean sentiment and sarcasm labels from native speakers of the language varieties. The novelty of the BESSTIE"}, {"title": "2 Related work", "content": "NLP Benchmarks such as GLUE [37], SuperGLUE [36], and DynaBench [20] have played a pivotal role in evaluating LLMs on multiple NLP tasks, including sentiment classification. However, these benchmarks are limited in their ability to capture the linguistic nuances of non-standard language varieties. Efforts to address this include datasets for African languages and dialects [26], Arabic dialects [10], and CreoleVal [22] which provides datasets for 28 creole languages. More recently, DIALECTBENCH [11] is a collection of dialectal datasets, and associated NLP models across 281 dialectal varieties for several tasks. However, the benchmark does not contain English dialectal datasets for sentiment and sarcasm classification. BESSTIE is the first benchmark for sentiment and sarcasm classification specifically for non-standard varieties of English.\nSimilarly, several sarcasm classification datasets for standard variety of English exist, consisting of reddit comments [19], amazon reviews [12], and tweets [1, 28, 30, 32]. However, there is a notable absence of sarcasm classification datasets that account for non-standard varieties of English. ArSarcasm-v2 [3] is a dataset annotated for sarcasm, sentiment, and the dialect of each tweet for Arabic. Abu Farha and Magdy [2] benchmarked Arabic-specific transformer models on this dataset, noting that models trained on Arabic data, including varieties of the language, tend to perform better.\nWe emphasise that the papers listed above approach benchmark creation in a similar manner as BESSTIE: they create an annotated dataset for a task, and test a broad range of models to highlight the potential for future work. In that sense, these benchmark papers acknowledge that creating a dataset and measuring bias is an important task achieved by papers that introduce benchmarks, without necessarily presenting a new method itself for the task. This holds true for our paper as well."}, {"title": "3 Ethical Considerations", "content": "The project received ethics approval from the Human Research Ethics Committee at UNSW, Sydney (reference number: iRECS6514). We treat the reviews as separate texts and do not aggregate them by user in any way. The Reddit comments and Google reviews used in this paper are from publicly available posts, accessible through official APIs. We adhere to the terms of service and policies outlined by these platforms while gathering data. We do not attempt to de-identify any users or collect aggregate information about individual users. The dataset, including sentiment and sarcasm annotations, is shared in a manner that preserves user privacy and abides by the rules and guidelines of the source platforms."}, {"title": "4 Dataset Creation", "content": "We now describe parts (a) and (b) of Figure 1 which result in a manually annotated dataset for sentiment and sarcasm classification."}, {"title": "4.1 Data Collection", "content": "We collect textual posts from two domains: Google Places reviews and Reddit comments, using two filtering methods: location-based and topic-based respectively. Location-based filtering implies that we select reviews that were posted for locations in the three countries. Specifically, we collect reviews posted in cities in AU, IN and UK, and their corresponding ratings (1 to 5 stars, where 5 is the highest.) using the Google Places API\u00b2 from the place types, also defined by the API. The criteria for city selection are based on population thresholds specific to each country (en-AU: 20K; en-IN:100K; en-UK: 50K). We further filter the dataset to only contain reviews with closer ratings (2 and 4 stars). We also filter out any non-English content using language probabilities calculated by fastText [15] word vectors, using a threshold of 0.98. Additionally, to mitigate the risk of including reviews written by tourists rather than residents, we exclude reviews from locations designated as 'tourist attractions\u2019\u00b3 by the Google Places API.\nTo create the REDDIT subset, we employ topic-based filtering and choose up to four subreddits per variety (en-AU: 'melbourne', 'AustralianPolitics', 'AskAnAustralian'; en-IN: 'India', 'IndiaSpeaks', 'BollyBlindsNGossip'; en-UK: 'England', 'Britain', 'UnitedKingdom', 'GreatBritishMemes'). We select these subreddits based on the understanding that they feature popular, discussions specific to a variety, making it highly likely that contributors use the appropriate language variety. For each variety, we scrape 12,000 comments evenly across the selected subreddits, capping at 20 comments per post and focusing on recent posts. These are then randomly sampled and standardized to 3, 000 comments per variety before manual annotation."}, {"title": "4.2 Annotation", "content": "We hire three annotators\u2074 each for the three language varieties. The annotators assign the processed reviews and posts two labels: sentiment and sarcasm. The choice of labels given are negative, positive, and discard. We instruct the annotators to use the discard label for uninformative examples with no apparent polarity and in a few cases, for computer-generated messages. We compensate the annotators at the casual employment rate prescribed by the host institution."}, {"title": "4.3 Data Quality & Statistics", "content": "Data Quality. While previous studies utilise location-based filtering to evaluate the robustness of language technologies across varieties [5, 35], it needs to be examined if the collected text is indeed from these national varieties. To address this, we conduct an annotation exercise with en-IN and en-UK annotators, asking them to manually identify the variety of a given text. Specifically, they label a random sample of 300 texts (150 from reviews and 150 from comments) as en-AU, en-IN, en-UK, or Cannot say. Annotator agreement is measured using Cohen's kappa (\u043a).\nWith respect to the true label (based on the location or topic), results show higher agreement with the en-IN annotator (\u03ba = 0.41) compared to the en-UK annotator (k = 0.34), indicating fair to moderate consistency. The inter-annotator agreement itself is 0.26. Figure 2 shows that annotators find it difficult to agree on inner-circle varieties, and have the highest agreement when identifying en-IN variety, with 46 agreements. While agreement between annotators is limited for inner-circle varieties, the higher agreement for the en-IN indicates that the subset reliably represents this variety, thereby validating our collection methodology. As a result, the two subsets (reviews and comments) collectively form a good representative sample for language varieties.\nNote: We also attempt to fine-tune an encoder model fine-tuned on an unlabeled dialectal corpus, to predict the language varieties."}, {"title": "5 Evaluation Methodology", "content": "With the annotated dataset in place, we now discuss the evaluation methodology used to evaluate the LLMs on the dataset.\nThe training strategies for encoder and decoder models are as follows. We train encoder models using cross-entropy loss, weighted on class distribution:\n$L_{Encoder} =  \\frac{1}{N} \\sum_{j=1}^{N} \\sum_{k=1}^{C} w_k y_{j,k} log p_{j,k}$\nHere, N denotes the total number of samples in the batch, C represents the number of classes (2 since the tasks are binary classification), and $w_k$ is the weight assigned to class k, calculated based on the class distribution ($w_{neg}$ for the negative and $w_{pos}$ for the positive class). $y_j$ is the expected output label for the jth instance where $y_{j,1} = 1$ if the label is 1 and $y_{j,0} = 1$ if the label is 0. $P_{j,k}$ is the probability of the jth instance for kth class.\nIn contrast, for decoder models, we perform zero-shot instruction fine-tuning [29], using maximum likelihood estimation - a standard learning objective for causal language modeling [16]:\n$L_{Decoder} = - \\frac{1}{N} \\sum_{j=1}^{N} ( \\frac{1}{|t_j|} \\sum_{i=x+1}^{x+|t_j|} log p(x_i | x_1^{i-1}) )$\nHere, $x_1^{i-1} = [x_1,...,x_{i-1}]$ denotes the subsequence before $x_i$ and $|\\cdot|$ is the number of tokens. Specifically, $|t_j|$ is the number of generated tokens and is set to 1. In other words, given the text and a task-specific prompt (given in Table 45), the expected behaviour of the LLM is to generate either 1 (for positive) or 0 (for negative)."}, {"title": "6 Experiment Details", "content": "We conduct our experiments on a total of nine LLMs. These include six encoder models, three trained on English corpora: BERT-Large (BERT) [9], ROBERTa-Large (ROBERTA) [24], ALBERT-XXL-v2 (ALBERT) [21]; and three multilingual models: Multilingual-BERT-Base (MBERT), Multilingual-DistilBERT-Base (MDISTIL) [33], XLM-ROBERTa-Large (XLM-R) [14]. We also evaluate three decoder models, one trained on English corpora: Gemma2-27B-Instruct (GEMMA) [13] and two multilingual models: Mistral-Small-Instruct-2409 (MISTRAL) [17], Qwen2.5-72B-Instruct (QWEN) [38]. The model-specific details including architecture, language coverage of pre-training corpus, and number of parameters are described in Table 5.\nWhile encoder models are fine-tuned with full precision, we fine-tune quantized decoder models using QLORA [8] adapters, targeting all linear layers. All models are fined-tuned for 30 epochs, with a batch size of 8 and Adam optimiser. We choose an optimal learning rate by performing a grid search over the following values: 1e-5, 2e-5, and 3e-5. All experiments are performed using two NVIDIA A100 80GB GPUs.\nWe report our task performances on three macro-averaged F-SCORE, i.e., unweighted average, disregarding the imbalance in label distribution."}, {"title": "7 Results", "content": "We first present our results on the two tasks where models are trained and evaluated on the same variety. Figure 3 and 4 describe the model performances, reported using F-SCORE, on the sentiment and sarcasm classification tasks respectively. While sentiment classification is performed using both GOOGLE and REDDIT subsets, due to the absence of sarcasm labels in GOOGLE subset (as shown in Table 3), sarcasm classification is conducted only on REDDIT subset."}, {"title": "7.1 Overall results", "content": "Table 6 shows that, for sentiment classification, MISTRAL achieves the highest average performance across all varieties, with an F-SCORE of 0.91 on the GOOGLE subset and 0.84 on the REDDIT subset. In contrast, QWEN reports the lowest average performance across all varieties on the GOOGLE subset, with an average F-SCORE of 0.45, while GEMMA exhibits the lowest average performance across all varieties on the REDDIT subset, with an F-SCORE of 0.60."}, {"title": "7.2 Impact of Domain and Models", "content": "The average results for the models highlight the need to further probe into factors influencing model performances, namely, domain, model properties and language varieties. The results in Tables 7 and 8 describe the model performances (reporting F-SCORE), grouped based on model properties. Finally, task performances, averaged across all models, on varieties are presented in Table 9.\nOur findings (the last row u in Table 6), indicate that models perform better on GOOGLE subset, achieving an F-SCORE of 0.81, compared to an F-SCORE of 0.75 on REDDIT subset. This difference can be explained by the distinct writing styles of the two sources. Reviews in GOOGLE are generally more formal and informative, while posts and comments from REDDIT often exhibit a short (as shown in Table 3), conversational tone typical of social media. Additionally, posts and comments in REDDIT come from forums frequented by local speakers, which means the language and expressions used are more reflective of the variety and cultural nuances.\nTable 7 shows a consistent trend where encoder models report higher performance than decoder models across different tasks. This performance gap is expected as encoder architecture is inherently better suited for sequence classification tasks, while decoder architecture is optimised for text generation tasks. Table 8 shows that, although marginal, monolingual models report a"}, {"title": "7.3 Cross-variety/domain evaluation", "content": "Finally, we perform cross-variety/cross-domain evaluation. This refers to the scenario where the datasets used to train and test a model are from different groups (i.e., trained on en-AU, tested on en-IN, for example). The following discussion is focused on generalisation: \"can models trained on one language variety generalise well to others\".\nWe report the cross-variety results of our best-performing model, MISTRAL. Figure 5 compares the performance of MISTRAL across three variants: pre-trained, in-variety fine-tuning, and cross-variety fine-tuning, for the three dataset subsets using colour-coded matrices. The rows indicate the subset that the model is trained on (i.e., en-AU, en-IN, and en-UK), along with PT, indicating that the pre-trained MISTRAL is used. Similarly, the columns indicate the test subset (i.e., en-AU, en-IN, and en-UK).\nThe pre-trained variant of the model (the first row in each matrix) achieves high F-SCORE for en-AU and en-UK, for the sentiment classification task. Fine-tuning the model improves in-variety performance (shown by the principal diagonal of the matrices), with noticeable gains in F-SCORE on both subsets. The model performance on cross-variety evaluation remains relatively stable across different varieties of English, indicating that domain-specific data plays a minor role in influencing cross-variety generalisation for sentiment classification.\nAs seen earlier, the model performs significantly worse on sarcasm classification compared to sentiment classification. While in-variety evaluation shows that fine-tuning improves over the pre-trained model with substantial increases in F-SCORE, it negatively impacts cross-variety generalization. This means there is a notable decrease in F-SCORE across other varieties when the model is fine-tuned on a specific variety. This supports the idea that sarcasm classification requires the model to grasp linguistic and cultural nuances specific to a variety, making generalisation across varieties challenging. The corresponding results for BERT are in Figure 6.\nSimilarly, to assess the cross-domain robustness of our sentiment classification models, we conduct experiments with domain data from GOOGLE, and REDDIT. Models trained on one domain are tested on the other domain, and vice-versa, for each of the three varieties of English. Figure 7 shows that models perform better when trained on in-domain data, for both MISTRAL and BERT. We observe that pre-trained MISTRAL performs better for all varieties in comparison to results from the cross-domain experiments.\nWhile MISTRAL outperforms BERT at both in- and cross-domain experiments, there is significant variation in cross-domain results,"}, {"title": "8 Error Analysis", "content": "We randomly sample 30 examples misclassified by MISTRAL, across all domains and varieties. We hope that these error categories will be helpful for future researchers who use the BESSTIE dataset. For sentiment classification, we identify the following error categories:\n(1) Mixed Sentiment Mis-classification: The model struggles to classify text containing both positive and negative sentiment, as in 'The outside pool has been completely redesigned and although looks good the depth of the pool has been down-sized.'.\n(2) Insufficient Context: Consider the Reddit comment 'Darwin for getting its own dot and big font on any map of the World.'. The sentiment can only be identified with the context/information from the previous exchanges in the thread.\nFor sarcasm classification, the error categories are:\n(1) Contextual Sarcasm: Some instances of sarcasm rely heavily on contextual or conversational cues that are not present in the text on its own. For example, in the Reddit comment, 'Atleast Spiderman covered his face before answering.', the sarcastic intent can only be understood within the context of the preceding discussion in the thread.\n(2) Implicit Sarcasm: Instances where sarcasm is expressed subtly, through tone or mild exaggeration, without explicit contradictions. The example, 'Makes more sense to just bar his company from getting further contracts whilst he is running it.', is misidentified as not sarcastic.\n(3) False Positives from Negative Connotations: Mildly negative statements are often misclassified as sarcastic due to their phrasing or sentiment. This error is identified for examples such as 'Sorry gal, what you experienced pretty much IS the dating culture here.'."}, {"title": "9 Conclusion", "content": "Biases in LLMs have been reported for several language varieties. This paper presents BESSTIE, a benchmark for sentiment and sarcasm classification across three varieties of English: Australian (en-AU), Indian (en-IN), and British (en-UK). Our evaluation spans nine LLMs, six encoders and three decoders, assessed on datasets collected from Google Places reviews (GOOGLE) and Reddit comments (REDDIT). The models perform consistently better on en-AU and en-UK (i.e., inner-circle varieties) than on en-IN (i.e., the outer-circle variety) for both tasks. Although models report high performance on sentiment classification (F-SCORE of 0.81 and 0.75, for GOOGLE and REDDIT respectively), they struggle with detecting sarcasm (F-SCORE of 0.59 on REDDIT), indicating that sarcasm classification is still largely unsolved. This is supported by our cross-variety evaluation which reveals limited generalisation capability for sarcasm classification, where cultural and contextual understanding is crucial. Notably, monolingual models marginally outperform multilingual models, suggesting that language diversity in pre-training does not extend to varieties of a language. In conclusion, BESSTIE will provide a way to measure bias of LLMs towards non-standard English varieties for the two tasks. By highlighting gaps in the model performance, especially for (a) sarcasm classification in general; and (b) outer-circle varieties like en-IN for both the tasks, this benchmark will be useful to evaluate dialectal biases of language models, and contribute to research towards equitable performance across all varieties of English."}]}