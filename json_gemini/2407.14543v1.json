{"title": "Towards consistency of rule-based explainer and black box model - fusion of rule induction and XAI-based feature importance", "authors": ["Micha\u0142 Kozielski", "Marek Sikora", "\u0141ukasz Wawrowski"], "abstract": "Rule-based models offer a human-understandable representation, i.e. they are interpretable. For this reason, they are used to explain the decisions of non-interpretable complex models, referred to as black box models. The generation of such explanations involves the approximation of a black box model by a rule-based model. To date, however, it has not been investigated whether the rule-based model makes decisions in the same way as the black box model it approximates. Decision making in the same way is understood in this work as the consistency of decisions and the consistency of the most important attributes used for decision making. This study proposes a novel approach ensuring that the rule-based surrogate model mimics the performance of the black box model. The proposed solution performs an explanation fusion involving rule generation and taking into account the feature importance determined by the selected XAI methods for the black box model being explained. The result of the method can be both global and local rule-based explanations. The quality of the proposed solution was verified by extensive analysis on 30 tabular benchmark datasets representing classification problems. Evaluation included comparison with the reference method and an illustrative case study. In addition, the paper discusses the possible pathways for the application of the rule-based approach in XAI and how rule-based explanations, including the proposed method, meet the user", "sections": [{"title": "1. Introduction", "content": "Developments in technology are enabling (and motivating) the creation of increasingly complex artificial intelligence (AI) and machine learning (ML) models. Therefore, many of the most recent methods generate models that are not interpretable. This means, that the user who is the recipient of the model, i.e. a human, is not able to understand on what basis the generated model makes a decision. Consequently, models with such a representation are referred to as black box. In contrast, models that are interpretable to a human are referred to as white box (glass box). The representation of a phenomenon that is created during their generation is interpretable for a human, i.e. the user of such a model is able to understand what the model's decision is based on.\nThe models generated by AI/ML methods are now widely used and their decisions affect an increasing number of users not only data analysis specialists but experts in other fields and ordinary users. A need has therefore emerged to explain the decisions of black box models. This need is not only expressed by users of AI/ML systems but also by legislators [1]. In response to this demand, a number of explainable artificial intelligence (XAI) methods have been developed [2, 3, 4, 5]. Thus, on the one hand, the need to explain black box model decisions is clearly stated and the involvement of the data science community in the development of XAI methods is clear. On the other hand, there are voices [6] indicating problems concerning the quality of explanations (fidelity) and their interpretability. It is emphasised that the approximation of a complex (black box) model by an interpretable model is not prefect, as it is mostly associated with simplification. This may lead to limited trust in the explanation. A second problem is that the explanatory approximator does not have to mimic the operation of the black box model. Therefore, the resulting explanations may be based on different features and lead to incorrect conclusions."}, {"title": "", "content": "The motivation for the present study is related to the above issues. To date, to the best of the authors' knowledge, no research has been undertaken on whether when black box models and their rule-based white box approximators make the same or similar decisions they do this on the same basis. There has been numerous studies on the use of so-called \"surrogate models\" [7], which are approximators of complex models. Typically, the convergence of the performance of a back box model and its approximator is measured by the error made by both models on training or test datasets. Less commonly, the consistency of decisions made is tested (calculating e.g. Cohen's Kappa on the decisions of black box and white box models). Another possibility is the use of synthetic explainable classifier generators [8] which allows the comparison of local XAI methods.\nThe aim of this study is twofold. Firstly, it aims to verify if the rule-based surrogate models applied to tabular data and classification problems mimic thoroughly the approximated black box model. Secondly, this study aims to propose a novel method for generating a rule-based white box model, allowing to obtain explanations more consistent with the approximated complex model. Fig. 1 illustrates the proposed approach allowing a comparison with a typical rule-based explainer generation. To enable verification of the approach used, software developed as part of the research was published\u00b9 for other users. In addition, within the software, a detailed report containing the full experimental results on multiple benchmark datasets was made available.\nThe contributions of this study are as follows.\n\u2022 A comparison of rule-based models approximating the decisions of complex (black box) models with rule-based models trained on the original training data. This comparison verifies whether the generation of an approximator is needed and whether rule-based models generated directly on the training data approximate the black box model equally well as rule-based approximators of these models.\n\u2022 Verification whether the decision bases of the black box model and the rule-based white box model that approximates it are consistent, i.e. whether the same decisions of both models are based on the same, similarly important attributes.\n\u2022 A novel method for generating decision rules to properly explain the"}, {"title": "", "content": "black box model decisions, which takes into account the importance of the features of the complex model when generating the rule-based approximator. The proposed method was used to build rule-based factual and counterfactual explanations.\nThe remainder of this paper is organised as follows. Section 2 presents the literature review on XAI methods and rule-based XAI methods in particular. Section 3 introduces formal bases for the proposed method, the method itself and the criteria for consistency evaluation of the surogate explaining model and the black box model. The experiments and their results are presented in Section 4. Section 5 discusses the possible pathways for the application of the rule-based approach in XAI and evaluates the rule-based explanations in the context of Co-12 properties. Section 6 concludes the presented study."}, {"title": "2. Related work", "content": "Due to the importance and ubiquity of artificial intelligence and efforts to introduce regulations related to, among others, the explanation of decisions made by AI models, work on XAI methods has intensified significantly in recent years. It resulted in the publication of numerous review papers and books related to this field [2, 4, 5, 9, 3].\nExisting \u03a7\u0391\u0399 methods can be divided into several classes depending on the perspective adopted for their description. One approach is to distinguish methods that can explain the created model at the level of the entire dataset (global methods) or can explain the basis of the model's decisions for a single example (local methods). Taking a different perspective, XAI methods can be divided into model specific and model agnostic. In the first case, the explanation method is dedicated to a specific type of model, typically neural network architecture [10]. In the second case, the explanatory method treats the model as a black box and is not related to it in any way [11]. There are many different methods of this type, among which it is possible to distinguish a group of solutions generating an interpretable model approximating the black box model. In this way, both local and global explanations can be generated, and the resulting explanations can take different forms e.g. linear regression coefficients [12], decision tree based rules determining counterfactual explanations [13] or rule-based explantions [14]. Another way of subdividing XAI methods takes into account their application. XAI methods are used in a wide range of applications spanning from medicine [15, 16] to industry and predictive maintenance [17]. Moreover, depending on the type of data to be analysed, a distinction can be made between methods applicable to tabular data [18], images [16, 19] or text [20]. There are some methods that can be applied to different types of data. The most popular of these are LIME [12] and SHAP [21], and their various modifications [22, 23, 24, 25].\nAmong the various concepts of how to explain a complex machine learning model, approaches using a rule-based model can be distinguished. Such a model is interpretable, i.e. human-understandable, and can be used to generate both global and local explanations by approximating the complex model. This study focuses on rule-based XAI methods, which are model agnostic treating the model being explained as a black box.\nThere are many approaches to generating local rule-based explanations. If-then rules called anchors presented in [26] define the regions in the feature space where the decision is (almost) always the same and does not depend on"}, {"title": "", "content": "the values of other attributes. Another approach defining a local rule with tunable complexity on the basis of a boolean formula is presented in [27]. The classifier presented in this paper that approximates complex model decisions is trained using native local optimisation techniques, efficiently searching the space of feasible formulas. The method presented in [28] aims to improve the accuracy of the local rule-based explainers by applying weighted voting of multiple rule-based classifiers. The use of an ensemble of rule-based models is expected to better reflect the performance of the complex model being approximated. Counterfactual explanations based on rules determined from a decision tree are presented in [13]. The LORE algorithm introduced in this study generates by means of a genetic algorithm a set of examples in the local neighbourhood of the data instance to be explained. Next, it creates a decision tree that is a local approximation of the complex model and derives rules from this tree that are factual and counterfactual explanations. An extension of the method using a stability-enhancing ensemble of explanatory models is presented in [29]. Another approach in which local (locally optimal) rules are generated for single data instances and which can be used for explanation is presented in [30]. In this approach named LORD, attention was paid not only to the accuracy of the rule but also to its efficiency.\nIn addition to the methods presented above that generate local explanations, there are many examples of the use of rule-based methods to generate global explanations of complex models. The study [31] presents a method for generating decision sets which are sets of independent if-then rules. Decision set learning is performed through an objective function that simultaneously optimises accuracy and interpretability of the rules. This approach should allow efficient approximation and explanation of complex models. Another rule-based approach for global explanation of trained models is presented in [32]. First, the importance of the features in the complex model is calculated in this method. Next, the original inputs with the feature importance scores are weighted and the original input space is transformed for simplification. Finally, a rule-based explanatory model is induced on the derived features. In [33] the RuleMatrix interactive visualization technique applicable to rule-based representation is presented. The authors of the [34] propose to transform the interpretable (surrogate) model approximating the complex model into a Binary Decision Diagram (BDD) [35], which reflects well (better than, for example, rule sets) the complex relationships between attributes. The work [36] does not focus on global explanations at the level of the entire dataset, but presents the application of rule induction to generate explana-"}, {"title": "", "content": "tions characterising the regions in data space on which the model performs particularly poorly. Finally, the RuleXAI library presented in [14] offers both local and global rule-based explanations. This method makes it possible to determine the importance of individual conditions in the rules comprising the surrogate rule-based model. Thus, it allows a better understanding of which feature, and in which value ranges, contributes to the decision of a complex model.\nThe presented literature review indicates that, despite the existence of a variety of approaches to generating rule-based explanations, there is a lack of research ensuring that the explanatory rule-based model mimics the behavior of the approximated black box model. This conclusion motivates the research presented."}, {"title": "3. Materials and Methods", "content": "3.1. Basic Notions and Definitions\nLet us assume that a universe of possible instances U is given. Each instance $x \\in U$ is characterised by a set of attributes $A = \\{a_1,...,a_n\\}$. Each attribute is interpreted as a function $a : U \\rightarrow V_a$, where $V_a$ is a set of possible values. Two types of attributes are discerned: symbolic (discrete-valued) and numeric (real-valued). Each example is also characterised by a special attribute $d$, often called a decision attribute or target. The decision attribute is a discrete class identifier $d : U \\rightarrow V_a$, where $V_d = \\{L_1, ..., L_k\\}$ and $L_i, i \\in \\{1, ..., k\\}$ is a so-called class label or decision class. Furthermore, $d(x)$ denotes the value of the decision attribute for an example $x$. The system $D(X, A, d)$, where $X \\subseteq U$ is called a decision table. Further notions needed to define the problem of approximating and explaining complex machine learning (black-box) models with a rule-based model are presented in Table 1.\nThe rule-based data model $R$ is a set of classification (decision) rules generated by an induction algorithm. Each rule $r \\in R$ has the form:\nIF $c_1 \\land c_2 \\land ... \\land c_m$ THEN $d = L$.\\tag{1}\nThe premise of a rule is a conjunction of conditions $c_j = a_j \\otimes v_{a_j}$, with $v_{a_j}$ being an element of the attribute $a_j$ domain and $\\otimes$ representing a relation (= for nominal attributes; $<,\\leq,>, \\geq$ for numerical ones). The conclusion (decision) $d = L$ of the rule indicates the class label $L \\in \\{L_1, ..., L_k\\}$. If an"}, {"title": "", "content": "example x fulfills the conditions of a ruler premise, we say that r covers x (x is covered by r).\nGiven a set of examples $D(X, A, d)$ and the rule (1), the set of all examples from X such that d(x) = L is called the set of positive examples, while all other examples are referred to as negative examples. The number of all positive examples and the number of all negative examples are denoted by P and N, respectively. The number of positive examples covered by the rule (1) is denoted by p, and the number of negative examples covered by (1) is denoted by n.\nDecision rules are employed both for data description and for classifying new instances with unknown values of the decision attribute. Typically, a rule is interpreted as a dependency that allows one to infer the membership of an example covered by the rule to the decision class indicated in the rule's conclusion. Since, in the presented considerations, rules are induced based on the learning from examples paradigm, this inference can be fallible, which is a feature of inductive inference.\nA rule that covers an example x can be perceived as an explanation for the"}, {"title": "", "content": "classification of the example to a specific decision class - the class indicated in the conclusion of that rule [26, 37]. In this case, the abductive reasoning is employed, which, due to the nature of rule induction, can also prove to be fallible.\nAmong the various approaches to the induction of classification rules [38, 39, 40, 41, 42], algorithms based on the sequential covering strategy [43, 44, 45, 46] have demonstrated significant utility in data description and classification of unseen examples.\nIn this research, sequential covering algorithm is utilised, which efficacy has been confirmed in a number of publications [47, 48, 46]. Additionally, an object-related modification of this algorithm dedicated to local explainability is employed. The object-related rule induction approach is motivated by rule induction methods used in Rough Set Theory [49, 50], specially by the heuristic strategy of the global and object-related decision reduct finding [51].\nIn both algorithms, a key role plays the rule quality measure (rule search heuristics ([52])), which controls the rule induction process. The primary objective of induction is to generate rules that are both precise (covering only positive examples) and general (covering as many examples as possible). Therefore, rule quality is typically characterised by both precision and coverage calculated as:\nprecision =$\\frac{p}{p+n}$,\\tag{2}\ncoverage =$\\frac{p}{P}$.\\tag{3}\nThere is a wide range of rule quality measures that are used to supervise the rule induction process and rule assessment [44, 53, 54, 55] presented in literature. Depending on the measure employed, different classification models are obtained. Optimal results are usually achieved using measures that consider both the precision and coverage of a rule, and the values of P and N [56, 57]. In this study, in addition to the above measures, measure C2, which has the following form, was used to generate rules:\nC2 =$\\frac{(\\frac{p}{N(p+n)}-\\frac{Pn}{N})}{(2p)} + (\\frac{Np-Pn}{N(p+n)}) $\\tag{4}\n3.2. Fusion of rule-based explanations with importance-based feature ordering\nThe idea of the sequential covering approach following the separate-and-conquer heuristic is presented in (Algorithm 1). Rule generation is carried"}, {"title": "", "content": "out sequentially for each class and its positive examples. The rules are added iteratively to the initially empty set as long as the entire dataset becomes covered. Every rule must cover at least mincov previously uncovered examples, to ensure the convergence. Therefore, when there are less then mincov uncovered examples left, the generation of consecutive rules is stopped. The induction of a single rule consists of two stages: growing and pruning. In the growing stage (presented in Algorithm 2), elementary conditions are added to the initially empty premise. All possible conditions built on all attributes (line 6: GETPOSSIBLECONDITIONS function call) are considered in this step. The conditions leading to the rule with the best quality are selected (lines 10-12). In the case of nominal attributes, conditions in the form $a_j = v_j$ for all values $v_j$ from the attribute $a_j$ domain are considered. For continuous attributes, $a_j$ values that appear in the observations covered by the rule are sorted. Then, the possible split points $v_j$ are determined as arithmetic means of subsequent $a_j$ values and conditions $a_j < v_j$ and $a_j \\geq v_j$ are evaluated. If several conditions lead to the same results, the one covering more examples is chosen. In the pruning stage, conditions are iteratively removed from the rule premise to achieve the greatest improvement in rule quality. The procedure stops when no conditions can be deleted without decreasing the quality of the rule or when the rule contains only one condition. Finally, the comprehensibility of the rule is improved by merging conditions based on the same numerical attributes. A broader general outline of the sequential covering algorithm is discussed, among others, in [44, 46].\nThe separate-and-conquer approach (Algorithm 1) generates unordered set of rules. Therefore, the classification of a data example, e.g. a test one, requires the evaluation of a set $R_{cov} \\subseteq R$ of rules that cover the example and the aggregation of the results obtained. This differs from ordered rule sets (decision lists), where the first rule covering the investigated observation determines a model response. Aggregation, in the case of classification, is achieved using voting. Each rule from $R_{cov}$ votes with its value of the quality measure.\nThe aim of this study is to modify the sequential covering approach to ensure that the rule-based explanations are based on the attributes that are important for the decisions made by the approximated complex model. Therefore, the algorithm presented in Algorithm 1 and 2 was subjected to the following modifications. The proposed approach assumes that the importance-based feature ordering (fo(M) or fo(M,x)) determined for the black box model M is passed to the algorithm. This ranking specifies a sequence of"}, {"title": "", "content": "attributes which may be a subset of the set of all attributes. Furthermore, the proposed approach was based on the object-related rule induction. In this approach one rule is generated to cover (to explain) a specific example. Specifically, for a given set of examples (e.g., a training set), one rule is generated for each data instance. The pseudo-code of the proposed approach modifying Algorithm 1 is presented in Algorithm 3. A new rule is generated for each class Land for each data example x belonging to this class (line 4). Rule induction is performed using the importance-based ordering of the attributes, which is denoted generally as fo in the Algorithm 3. The ordering can be global, i.e. identical for each data example (e.g. it can be fo(M) resulting from the global explanation of the black box model M). It can be local, i.e. different for each example x on which the rule is generated (e.g. it can be fo(M, x) resulting from the local explanation of the black box model M decision for data example x).\nFurthermore, the use of importance-based ordering of the attributes imposes a change in the GROW method implementing the rule growth. The pseudo-code of the proposed modification of Algorithm 2 is presented in Algorithm 4. Since a rule is generated within the object-related rule induction on a single data example, line 5 of the GROW method must be changed to the form presented in 4. Further changes result from the use of importance-based feature ordering (fo). In this approach, the set of attributes $A_0$ is"}, {"title": "", "content": "introduced in line 7 of Algorithm 4. Initially, $A_0$ contains only the most important attribute, then successively more attributes are added according to the fo ranking. In each iteration, $A_0$ is passed to the GETPOSSIBLECONDITIONS function (line 8). Thus, in the proposed modification, unlike in the approach presented in Algorithm 2, not all attributes describing the training data are passed to the GETPOSSIBLECONDITIONS function but iteratively expanding subset $A_0$ of the attributes. Operation of the GETPOSSIBLE-CONDITIONS function remains unchanged. It generates the rule conditions for all attribute values or possible split points depending on the nominal or numerical type of the attribute. The pruning stage remains unchanged in this approach.\nThe Algorithm 3 is completed with optional filtering of the resulting rule"}, {"title": "", "content": "set to reduce the number of rules. Filtering is performed on a set of rules that are sorted in descending order according to the quality measure. Iteratively for each successive rule, those examples that are covered by the rule are removed from the set of examples. Filtering is completed when all examples are covered (removed).\n3.3. Evaluation of rule-based explanations\nAs outlined in the Introduction section, the aim of this study is to verify the adequacy of the explanations of a black box model M generated by a rule-based model R (especially the explanations generated by a rule-based approximation of the black box model \u2013 RM). Therefore, the following three conditions were defined for R that can be used to explain M:\n$\\forall X \\subseteq U, \\forall x \\in X \\quad R(x) = M(x)$,\n\\tag{5}\nwhich means that the explainer R and the model being explained M make the same decisions on each data set,\nF(R) = F(M),\\tag{6}"}, {"title": "", "content": "$\\forall X \\subseteq U, \\forall x \\in X \\quad fo(R,x) = fo(M,x)$,\n\\tag{7}\nwhich means that the importance-based ordering (ranking) of the features on which the explainer R and the model being explained M make decisions"}, {"title": "", "content": "is the same. An ideal explanatory model is one that meets all the conditions listed above.\nWith respect to defined conditions (5-7), the fidelity of the explainer R can be calculated using various methods. Criterion (5) can be verified using a measure of agreement between the decisions of two classifiers, e.g., by determining the Cohen kappa coefficient between models R and M. Criterion (6) can be verified by measuring the mutual inclusion between the feature sets used by the models R and M:\n$\\nu =\\frac{|F(R) \\cap F(M)|}{|F(R) \\cup F(M)|}$.\\tag{8}\nCriterion (7) can be verified by comparing the importance rankings of attributes using, e.g. by determining Kendall's Tau correlation coefficient."}, {"title": "4. Experiments and results", "content": "The proposed method was extensively verified on numerous datasets in the experiments carried out. The conditions presented in Section 3.3, which define the ideal explanatory model, determined the scheme of the analyses conducted. The measures indicated there to verify the quality of the proposed model were used to evaluate and compare the quality of the proposed rule model generation method for black box model explanations.\nThe analyses carried out were divided into three parts. In the first, condition (5) of Section 3.3 stating that the generated approximator should make the same decisions as the approximated black box model was verified. The second part verified the consistency of the basis for the decisions made by the model, i.e. the explanations showing on which attributes the rule-based approximator and the black box model made decisions. This analysis includes condition (6) and condition (7), which refer to the overlap of important attributes for each model and the consistency of the ranking of these attributes. The final part of the analyses conducted was a comparison of the proposed method with the Anchors method [26], selected as the state-of-the-art rule-based XAI solution. The general comparison carried out was complemented by a case study on selected data for both methods. The remainder of this section presents the aggregated results of the experiments and the conclusions drawn from them. A detailed report containing the full experimental results and the software used in the experiments are available online (https://github.com/ruleminer/FI-rules4\u03a7\u0391\u0399)."}, {"title": "4.1. Datasets and experimental settings", "content": "The proposed approach was verified on 30 datasets from UCI\u00b2 and OpenML3 repositories. Datasets dedicated to the classification task were selected and care was taken to ensure their diversity. The dataset names and characteristics (number of observations, attributes, and classes) are presented in Table 2.\nMoreover the proposed method was applied to approximate and explain three different types of black box models generated by the following methods: extreme gradient boosting (XGB), support vector machines (SVM) and random forest (RF). Raw data were preprocessed by applying one hot encoding of categorical attributes and imputation of missing values. Each dataset was split into train and test subsets in 80/20 proportion. The models were evaluated using average balanced accuracy. Exhaustive evaluation, e.g. using"}, {"title": "4.2. Evaluation of rule-based approximation", "content": "The first part of the experiments aimed to evaluate the quality of the rule-based approximation. Two approaches to approximation using a rule-based model were verified. In the first, the rule-based model is generated on the original data on which the black box model was generated. In this case, the quality of the model describing the analysed phenomenon is assessed. In the second approach, the rule-based model is generated on data in which the decision column contains the decisions of the black box model. In this case, the quality of the model that approximates the decisions of the black box model is assessed. Moreover, two methods of the rule-based model generation"}, {"title": "", "content": "were verified. The RuleKit method [46] was used as the implementation of standard separate-and-conquer approach and is hereafter referred to as SC-S (separate-and-conquer - standard). The RMatrix method [48] was used as the implementation of object-related approach and is hereafter referred to as SC-OR (separate-and-conquer - object-related). In each method, two measures of quality assessment were verified in the rule growth process: precision and C2.\nTo assess the quality of approximation the Cohen's kappa coefficient between black box model predictions and rule-based model predictions was calculated on train and test data. The results for the approach on the original data and on the black box model decision data are presented in Table 3.\nThe obtained results show that the approximation quality of the rule-based models generated on the dataset with black box model decisions are better than the models generated on original data. The mean kappa values are greater than or the same for this data representation in all cases except two. Therefore, further comparisons will be made for models generated on the dataset with black box model decisions. The difference between the methods of rule-based model generation and the quality measure used is particularly apparent for the training data. The results obtained on them show that the precision measure allows to obtain a model with an overall excellent approximation quality and clearly better than the C2 measure. Finally, the results show that the rule-based approximator generated by the SC-OR method was superior.\nThe mean kappa values presented in Table 3 allowed the selection of the rule-based model generation method used in further experiments. Therefore, the implementation of the proposed method was based on the SC-OR approach and the precision measure was used in the rule growth procedure. The rule-based model was generated on data in which the decision column contained the decisions of the black box model."}, {"title": "4.3. Evaluation of explanations", "content": "The second stage of the experiments was dedicated to verifying how well the generated rule-based explanations mimic the black box model decision making. For this purpose, the consistency of the generated rules with the black box model at the attribute level was verified. Two measures were chosen to assess such consistency, as suggested in Section 3.3. Inclusion was chosen to compare the sets of most important attributes for black box model"}, {"title": "", "content": "and its approximator. It shows whether the same key features are used by both models. Inclusion was determined as mutual inclusion (see formula (8)). Correlation was chosen to compare the rankings of the most important features for black box model and its approximator. It shows whether the same most important features rank in the same order for each model. Correlation was determined as Kendall's Tau correlation coefficient. For both measures, unique features occurring in the elementary conditions of the rule explaining the data instance were selected for comparison. The number of these features determined the number of the most important features of the black box model that were taken for comparison.\nThe experiments used rule-based models generated by the SC-OR method. These models consist of rules generated for each data example. Three models generated according to the approaches described in Section 3 were compared, and the induction of the rules forming these models used: (i) global importance-based feature ordering common to all rules, (ii) local importance-based feature ordering determined for each data instance separately, (iii) a basic approach that does not use any feature ranking. The global feature ranking was determined for the black box model using an approach referred to as Permutation Feature Importance. The local ranking was determined using the SHAP method, which was applied to each black box model decision (each data instance). Unfortunately, generating SHAP-based explanations for the SVM model (to which TreeSHAP is not applicable) was too time-consuming and no results were obtained for this model."}, {"title": "", "content": "The resulting values representing the consistency of the generated rules with the black box model are presented in Table 4. It presents the aggregated results for experiments conducted on 30 datasets, which include distributions of the Inclusion and Correlation values calculated for black box models and their rule-based explainers. For example, if the Model is specified as RF and the Feature importance is specified as Global, this row contains the distribution of Inclusion and Correlation values determined for models obtained using the Random Forest method and their rule-based explainers, which use information about the global importance-based feature ordering derived for Random Forest. More precisely, these values describe consistency of the features used in rule premises and the global feature ranking derived for Random Forest. If the Feature importance is specified as Local, then the local importance-based feature ordering is derived for each Random Forest decision and these rankings are used in rule generation process. Finally, if the rules are generated without the use of feature importance, then the features from the rule premises are compared with the features indicated as the most important in the global importance-based feature ordering derived for Random Forest.\nThe results show that the use of the proposed rule-based methods utilizing both global and local importance-based feature ordering clearly improves the consistency of the rule-based approximator with the black box model. Both Inclusion and Correlation values are higher and closer to 1 when feature ranking was used for rule generation in the SC-OR method. The quality of the rules generated in this experiment is presented in Fig. 3. The results presented in the charts show that the rules generated using the importance-based feature ordering are characterized by high but slightly lower precision (the decrease is greater for the global ranking) and predominantly no worse coverage.\nIn addition, it was verified how the use of importance-based feature ordering affects the quality of the approximation. Approximation quality expressed Cohen's Kappa values calculated for the rule-based models evaluated in Table 4 is presented in Fig. 4. The plots in Fig. 4 show that the use of importance-based feature ordering can reduce the quality of the approximation. In the case of local ranking, the change is small; in the case of global ranking, the difference in quality is evident.\nIn a further experiment the proposed solution implemented in the SC-OR method was compared with the reference Anchors method [26]. The concept of the experiments was analogous to the approach presented above."}, {"title": "", "content": "datasets, the generation of explanations for each data instance was aborted due to excessive analysis time. The explanation of a single decision by the Anchors algorithm implementation\u2074 used is approximately twice as long as the SC-OR implementation, and moreover, this implementation does not perform the analysis in parallel. Therefore, the analysis was carried out for the following 21 datasets: balance-scale, breast-w, car, churn, cmc, credit-a, cylinder-bands, diabetes, kdd-synthetic-control, kr-vs-kp, mammographic-masses, mushroom, nursery, phoneme, qsar-biodeg, segment, tic-tac-toe, titanic, vehicle, wall-robot-navigation, wdbc. Besides, the methods used and settings adopted were the same as in the experiments in Section 4.3.\nThe calculated consistency of the rule-based approximators with the black box model is presented in Table 5. The results presented in this table follow a format analogous to the results presented for the previous experiment. If Feature importance is specified as SC-OR Global, this means that the consistency values in the table refer to the global feature ranking calculated"}, {"title": "", "content": "for the black box model and the features used in the rule premise when the rules were generated by the SC-OR method using this global importance-based feature ordering. If Feature importance is specified as Anchors Global, this means that the consistency values in the table refer to the global feature ranking calculated for the black box model and the features used in the premise of the rules generated by Anchors.\nThe results clearly show that the proposed SC-OR solution achieves its goal generating a rule-based approximator that is more consistent with the black box model and its attribute ranking. The Inclusion and especially Correlation values obtained show how different the rule-based models (and explanations) can be when one of them is able to adjust to the attribute importance determined for the black box model.\nThe quality of the rules generated in this experiment is presented in Fig. 7. The results presented in the figure show that rules generated using local importance-based feature ordering (if calculated) have the highest precision. Whereas rules generated by Anchors are characterised by the highest coverage.\nFigures 5 and 6 present the CD diagrams illustrating that, for both the Inclusion and Correlation criteria, there are statistically significant differences between the Anchors algorithm and the SC-OR algorithm using importance-based feature ordering generated for the black```json\nbox model. It was not possible to determine local feature rankings for the SVM models (due to computational time constraints), thus, it was not possible to generate the CD diagrams for this model. A comparison between the Anchors algorithm and the SC-OR method using importance-based feature ordering was performed by means of the Wilcoxon test. In both cases, the method utilizing feature ranking information also demonstrated statistical superiority. The p-values were 0.0006 for Inclusion and 0.001 for Correlation."}, {"title": "4.4. Case-study", "content": "The case study carried out aims to compare and discus the selected explanations of the XGB model decisions on the selected two datasets. The explanations were generated by the proposed method implemented in the SC-OR algorithm, the Anchors algorithm and the standard rule induction (SC) method.\nThe first dataset belongs to a medical domain. It was created for the task of identifying genetic aberrations from the results of a flow cytometry"}, {"title": "", "content": "technique [59]. The presence of aberrations is an important factor for determination of patient prognosis at the initial phase of acute lymphoblastic leukaemia (ALL) diagnostics. ALL is the most frequent leukaemia in children.\nThe first data instance, which is analysed below, is an example of a true positive decision \"no aberrations\" of a black box model. The local ordering of features with respect to their importance, which was obtained for this data instance using the SHAP method, has the following form: cd9, cd123,"}, {"title": "", "content": "cd45, cd20, td_t, cd10, cd22, cd66, cd24, cd13, cd38, c_ig_m, cd81, cd34, ng2, cd33, cd15_65. This ranking consists of 17 attributes, of which cd9 is the most important. For the selected example, rule-based explanations of classifier decision were generated using the SC-OR method with local feature ordering and Anchors. The resulting explanatory rules are presented in Table 6.\nThe rule generated by SC-OR consists of conditions on attributes that are higher in the feature ranking than the attributes used by the Anchors and the standard rule induction (SC) method. In addition, the rule generated by SC-OR is more general, which, for the same precision, gives it more confidence.\nThe next data instance is an example for which the black box classifier's decision was incorrect - the classifier indicated \"aberrations\" when it should have decided \"no aberrations\". The rule-based explanations of the classifier"}, {"title": "", "content": "decision, that were generated analogously to the previous example are presented in Table 7. This time, the local ordering of features with respect to their importance, which was obtained for this data instance using the SHAP method, has the following form: cd22, cd123, cd45, cd34, cd13, td_t, cd81, c_ig_m, cd38, cd9, cd66, cd33, cd10, cd20, ng2, cd24, cd15_65.\nFor the explanations presented in Table 7, the rule generated by SC-OR again uses features that are higher in the attribute ranking. Whereas, the properties of the generated rules (precision and coverage) are similar. The"}, {"title": "", "content": "contradictory rule reflects the validity of the feature ranking even better than other rules in Table 7. However, its precision is 87% which, with over 4% coverage, means that 2 out of 15 examples covered by this rule are from the \"aberrations\" class.\nThe second dataset relates to the customer credit assessment task. It consists of attributes both numerical and categorical. For this dataset, analogous to the analysis presented above, two data instances were analysed.\nThe first data instance was correctly classified by the black box model as belonging to the \"bad\" class. The local ordering of features with respect to their importance, which was obtained for this data instance using the SHAP method, has the following form: duration, credit_amount, checking_account, purpose, job, housing, age, installment, saving_accounts, sex.\nThe resulting explanatory rules generated by the proposed method and the Anchors algorithm are presented in Table 8. Again, the rule generated by SC-OR consists of conditions on attributes that are higher in the feature ranking than the attributes used by the Anchors and SC. Furthermore, the rule generated by Anchors consists of a large number of conditions, making it a poorly interpretable explanation of the black box model's decisions.\nThe next data instance is an example for which the black box classifier's decision was incorrect - the classifier recommendation was \"good\", whereas the correct decision was \"bad\". The local ordering of features with respect"}, {"title": "", "content": "to their importance, which was obtained for this data instance using the SHAP method, has the following form: credit_amount, installment, purpose, housing, job, checking_account, age, duration, saving_accounts, sex.\nThe resulting explanatory rules generated by the proposed method and the Anchors algorithm are presented in Table 9. Additionally, Table 9 contains the contradictory rule generated by SC-OR that explains the decision opposite to that taken by the classifier (this would be the correct decision)\nComparing the rules generated by SC-OR, Anchors and SC, shown in Table 9, it can be concluded that again, the rule generated by SC-OR consists of conditions on attributes that are higher in the feature ranking than the attributes used by the Anchors and SC methods. In addition, the rule generated by SC-OR is more general and has higher precision.\nTaking the SC-OR contradictory rule into consideration makes explanations difficult to interpret. The explanation provided by SC-OR is unambiguous because it is based on a single attribute: installment \u2208 [75.18, 76.12]. However, the SC-OR contradictory rule also contains this attribute, and the range of the condition built on this attribute (installment < 100.69) covers the range [75.18, 76.12]. The SC-OR contradictory rule additionally contains a credit_amount \u2208 [1804, 1843] condition that refines it. Nevertheless, the SC-OR contradictory rule is very specific and only covers 2% of the examples"}, {"title": "", "content": "in the \"bad\" class."}, {"title": "5. Discussion", "content": "5.1. Rule-based explanation schemes\nRule-based approaches can be used to generate explanations at different levels (global or local), which can be driven by different aspects (model or instance). Therefore, three scenarios were defined and described below, in which rule-based explainability can be applied.\nThe first scenario concerns global explainability (which can be used for local explanations) for training dataset D. In this approach, a rule-based approximator RM is generated on the training data on which a black box model M was generated. The generated rule-based approximator RM explains the bases for decision-making for M on the training dataset, i.e. it is a source of global explanations. In addition, it allows for local explanations of the training data instances. For each example x \u2208 D, an explanation can be obtained based on the rules that cover this example (using only the rules with the decision predicted for this example by the model M). The explanation can be based on all rules or only on the best one (with respect to the rule quality measure). In addition, it can be shown whether this data example is covered by other rules with different decisions (contradictory to"}, {"title": "", "content": "those that explain example x). Furthermore, using the RuleXAI package [14], it is possible to indicate in the rule set of the RM model the most important conditions that determine the assignment to each decision class. Similarly, it is possible in the set of rules covering a particular example x to indicate the most important conditions that determined the assignment of this example to that class rather than another. The scenario presented above can be realised for the approximating model RM determined by the covering method (presented in the Algorithm 1) and the object-related method with or without filtering (presented in the Algorithm 3).\nThe second scenario concerns local explainability driven by a global model. In this case, the explanations being created concern a data example (x \u2209 D) that was not involved in the generation of the black box model M and its rule-based approximator RM. Therefore, it concerns the issue of prediction. To generate the explanation, rules from RM covering x and having the same decision as M for x are selected. The explanation can be based on all rules or only on the best one. Additionally, contradictory rules covering x can be presented. Further, more detailed analysis can be performed by the RuleXAI package as described for the previous scenario. Both of the above scenarios related to a global rule-based model approximating a black box model are illustrated in Fig. 8.\nFinally, the third scenario concerns local explainability driven by a data instance. Similarly to the previous approach, in this scenario the decision M(x) of the model M for a data instance x that does not belong to the training dataset (x \u2209 D) is explained. This time the approximating model RM is not generated. However, in order to generate an explanation, a rule for x with decision M(x) is generated from the dataset D. This rule is generated in such a way that it must cover the data instance x explaining the decision of the model M for this data instance. The explanation of the black box model decision M(x) (generation of a confirmatory rule) can be extended by generating a contradictory rule for the opposite class. In the general case, for a multi-class problem, the generation of a contradictory rule for each class could be considered. The analysis of such explanations could be further supported by information about the quality of the generated rules, which would allow the user to understand the difference in the strength of the explanations presented. A general diagram illustrating the presented scenario is presented in Fig. 9.\nFor each scenario presented above, it is possible to consider whether and how to take into account the importance-based feature ordering fo intro-"}, {"title": "", "content": "ordering when generating rules and use a basic covering algorithm. Another option is to take into account the global fo ordering, which is identical for each example being explained. Finally, in case of the explainability driven by a data instance, a local ordering can be used, which may be different for each data example. Which approach is used should depend on the user and their subjective purpose and need."}, {"title": "5.2. Properties of explanations \u2013 rule-based perspective", "content": "To better evaluate the proposed rule-based approach and to present it in a broader context, it has been analysed in terms of the Co-12 properties defined in [60]. Co-12 allows to assess the quality of explanations from the"}, {"title": "", "content": "perspective of their content and presentation and from the perspective of the user. Each of the 12 properties, its key idea given in [60] and a discussion relating to the properties of rule-based methods in general and the proposed approach in particular are listed below.\nCorrectness (Nothing but the truth) is addressed by the three conditions (5-7) defined in Section 3.3. These conditions define the correctness of the explanations generated by the rule-based approximator of a black box model as the same predictions taken on the basis of the same attributes. For the conditions defined in Section 3.3, quality measures were selected and used to evaluate the proposed method in the experiments carried out.\nCompleteness (The whole truth) is implicitly ensured for explanations based on the global model because it is a model generated on the whole training data. For explanations based on data instances, it can be ensured by generating an explanatory rule for each data instance. In this case, combining the explanations of all examples will provide a complete representation of the model. Furthermore, the completeness of an explanation based on a single rule can be controlled by setting a threshold for minimum rule precision and coverage.\nConsistency (Identical inputs should have identical explanations) property is held by the proposed method, because in its case two data examples with identical feature vectors are referred to the same training dataset. The proposed method does not generate artificial instances in the local neighbourhood of the data example being explained, which would cause differences in explanations. Thus, identical rules will be generated for such two data examples. In addition, however, it is necessary to ensure that the ranking generation method is robust to permutations of examples and attributes.\nContinuity (Similar inputs should have similar explanations) property is held by rule-based explanations, which result from the method of rule induction. Rule induction searches for elementary conditions (cuts) in the training dataset, which does not change with the data instances being explained. Therefore, if there are no data instances belonging to a different class between the two data examples being explained which would introduce cuts for attribute values, these examples will be covered by one rule. That is, such similar data examples will have the same explanation. However, in the proposed solution, the explanations additionally depend on the importance-based ranking of the attributes. If this ranking does not change significantly, and small changes in attribute values should not significantly affect their importance, then the generated rules for two similar examples will be identical"}, {"title": "", "content": "or similar.\nContrastivity (Answers \u201cwhy not?\u201d or \u201cwhat if?\u201d questions) is preserved by rule-based explanations. First of all, two examples classified into different classes will be explained by rules with different conclusions. Furthermore, for different data examples, the rules may differ in the attributes present in the premise part or in the range of elementary conditions where the attributes overlap. It is therefore possible to determine the difference of the rules (this difference is not symmetric), which will indicate which attributes and to what extent are responsible for the distinction of the examples by the model. For the proposed approach, it can be expected that the use of a global feature ranking will make the difference of the determined rules smaller than if a local feature ranking is used. This is because local rankings can be based on more diverse sets of attributes.\nCovariate complexity (Human-understandable concepts in the explanation) is assured by explanations in the form of rules, if the attributes analysed represent human-understandable concepts (in this study, by definition, the data has a tabular representation). The rule form of explanations is human-understandable. However, the rule generation algorithm does not determine generalised concepts, although it provides compact explanations by merging overlapping elementary conditions. Thus, knowing that rules are generated from the features that were used to train the model, it is important that the data representation contains features that will be interpretable to the end user.\nCompactness (Less is more) can be considered for rule explanations from several perspectives. Typically, rules are compact and comprehensible because they have a moderate number of conditions. The compactness of the rules is influenced by the search heuristics used in the pruning phase of rule generation. The more precise the rule should be, the more conditions there will be, while a smaller number of conditions increases the generality of the rule. Furthermore, in the case of local explanations, for each example to be explained only one rule can be generated as it is performed in the proposed method. In such case the rule covers the example to be explained and has the decision that was returned by the black box model.\nComposition (How something is explained) property is held by rule-based explanations. The rule-based presentation format, consisting of a conjunction of conditions leading to a decision presented as a rule conclusion, is considered to be easily interpretable and is therefore used in various XAI methods.\nConfidence (Confidence measure of the explanation or model output) is"}, {"title": "", "content": "a property that can take into account two components [60]. A confidence measure of the black box prediction is an issue independent of the rule-based method presented. The second aspect is the determination of confidence for explanations. Precision and coverage measures are commonly used to assess the quality of rules. These measures were determined in the experiments conducted to characterise the quality of the rules (explanations) generated by the proposed method and to compare the quality of the methods used. In addition, it is possible to determine the p-value of a rule or another measure that is based on a contingency table on the basis of these measures.\nContext (How much does the explanation matter in practice?) refers to the extent to which the user and their needs are taken into account to obtain understandable explanations. This aspect was beyond the scope of the presented approach. However, there is an approach presented in [48] that introduces an extension of the rule-based model generation method that is user-driven. In this approach, the user can choose which attributes should be used for rule generation. Using such a method may lead to rules of lower quality. However, it is possible to provide the user with both contextual (user-driven) and automatic explanations, which will give the user a better view of explainability.\nCoherence (Plausibility or reasonableness to users) assesses to what extent the explanation is consistent with relevant background knowledge. The coherence defined in this way depends on the performance of the underlying, black box model. If this model is correct, its decision-making process should be based on premises that are coherent with common knowledge. Otherwise, the importance-based ordering of the features generated for the black box model will not be coherent with background knowledge and will affect the form of the rules generated. Moreover, the proposed method depends on the XAI method generating a feature ranking, which may additionally affect the consistency of explanations. However, the advantage of rule-based methods is that they allow coherence to be verified at different levels of granularity. It is possible to verify the importance of attributes but also the ranges of attribute values [14]. Furthermore, an advantage of the rule-based representation is its form, in which it is easy for a human expert to capture background knowledge. Representing explanations and knowledge in the same form should facilitate their comparison and verification of coherence.\nControllability (Can the user influence the explanation?) of rule-based explanations can be provided by allowing the user to add their requirements, preferences or domain knowledge. A method that makes this possible using"}, {"title": "", "content": "the sequential covering approach to rule generation is presented in [48]. However, allowing the user to influence the explanations generated may result in explanations that do not mimic the operation of the black box model. This would be the opposite result to the motivation for the proposed method."}, {"title": "6. Conclusions and Future Work", "content": "This paper proposes a method to use external feature ranking to influence rule-based explanations to better mimic the performance of a black box model. The proposed approach is a modification of the sequential covering method, which is used to generate rule-based models. In experiments conducted on a large number of datasets, it was verified that rule-based models are good approximators on both training and test data and can be used as interpretable surrogate models. Furthermore, the analyses carried out allowed to select the implementation of the rule-based model generation method and its main settings. The rule-based model generated by the proposed approach is compared both with the one generated by the original method, which was created without the importance information provided by the feature ranking of the explained black box model, and with the rules obtained for the Anchors algorithm, which was used as a state-of-the-art method. In addition, the proposed method is illustrated by comparing its results with the Anchors method in a case-study. The results obtained show that incorporating the feature importance ranking generated for the black box model being explained allows rule-based models to explain black box decisions more reliably (in terms of Inclusion and Correlation).\nA detailed report containing the full experimental results, as well as an implementation of the proposed method for which experiments were carried out is publicity available (https://github.com/ruleminer/FI-rules4XAI).\nThe proposed method is further discussed to present possible pathways for the application of the rule-based approach in XAI. In addition, it was discussed how rule-based explanations, including the proposed method, relate to Co-12 properties.\nThe experiments presented were conducted using selected XAI methods generating importance-based feature orderings for the black box model. Depending on the feature importance determination method used, the rules obtained in the proposed method may differ. However, it is assumed that it is up to the user to consciously decide which method they want to use. For the proposed approach, it is also possible to use not a single method but"}, {"title": "", "content": "an ensemble of methods generating the feature importance ranking and then combining their results, e.g. by averaging the feature importance rankings and passing such resulting ranking to the rule induction algorithm.\nFuture work will include extending the functionality of the proposed method to regression and survival analysis tasks, as the induction methods used can operate on such data as well ([46]). In addition, future work will include extending the application of the proposed method to other types of data, in particular image analysis. In this case, the planned approach involves identifying superpixels in the image. Then, based on the groups of superpixels identified in the training images, binary attributes will be defined on which rule induction will be performed."}]}