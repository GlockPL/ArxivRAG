{"title": "Towards consistency of rule-based explainer and black box model - fusion of rule induction and XAI-based feature importance", "authors": ["Micha\u0142 Kozielski", "Marek Sikora", "\u0141ukasz Wawrowski"], "abstract": "Rule-based models offer a human-understandable representation, i.e. they are interpretable. For this reason, they are used to explain the decisions of non-interpretable complex models, referred to as black box models. The generation of such explanations involves the approximation of a black box model by a rule-based model. To date, however, it has not been investigated whether the rule-based model makes decisions in the same way as the black box model it approximates. Decision making in the same way is understood in this work as the consistency of decisions and the consistency of the most important attributes used for decision making. This study proposes a novel approach ensuring that the rule-based surrogate model mimics the performance of the black box model. The proposed solution performs an explanation fusion involving rule generation and taking into account the feature importance determined by the selected XAI methods for the black box model being explained. The result of the method can be both global and local rule-based explanations. The quality of the proposed solution was verified by extensive analysis on 30 tabular benchmark datasets representing classification problems. Evaluation included comparison with the reference method and an illustrative case study. In addition, the paper discusses the possible pathways for the application of the rule-based approach in XAI and how rule-based explanations, including the proposed method, meet the user", "sections": [{"title": "1. Introduction", "content": "Developments in technology are enabling (and motivating) the creation of increasingly complex artificial intelligence (AI) and machine learning (ML) models. Therefore, many of the most recent methods generate models that are not interpretable. This means, that the user who is the recipient of the model, i.e. a human, is not able to understand on what basis the generated model makes a decision. Consequently, models with such a representation are referred to as black box. In contrast, models that are interpretable to a human are referred to as white box (glass box). The representation of a phenomenon that is created during their generation is interpretable for a human, i.e. the user of such a model is able to understand what the model's decision is based on.\nThe models generated by AI/ML methods are now widely used and their decisions affect an increasing number of users not only data analysis spe- cialists but experts in other fields and ordinary users. A need has therefore emerged to explain the decisions of black box models. This need is not only expressed by users of AI/ML systems but also by legislators [1]. In response to this demand, a number of explainable artificial intelligence (XAI) methods have been developed [2, 3, 4, 5]. Thus, on the one hand, the need to explain black box model decisions is clearly stated and the involvement of the data science community in the development of XAI methods is clear. On the other hand, there are voices [6] indicating problems concerning the quality of explanations (fidelity) and their interpretability. It is emphasised that the approximation of a complex (black box) model by an interpretable model is not prefect, as it is mostly associated with simplification. This may lead to limited trust in the explanation. A second problem is that the explanatory approximator does not have to mimic the operation of the black box model. Therefore, the resulting explanations may be based on different features and lead to incorrect conclusions."}, {"title": null, "content": "The motivation for the present study is related to the above issues. To date, to the best of the authors' knowledge, no research has been undertaken on whether when black box models and their rule-based white box approx- imators make the same or similar decisions they do this on the same basis. There has been numerous studies on the use of so-called \"surrogate models\" [7], which are approximators of complex models. Typically, the convergence of the performance of a back box model and its approximator is measured by the error made by both models on training or test datasets. Less commonly, the consistency of decisions made is tested (calculating e.g. Cohen's Kappa on the decisions of black box and white box models). Another possibility is the use of synthetic explainable classifier generators [8] which allows the comparison of local XAI methods.\nThe aim of this study is twofold. Firstly, it aims to verify if the rule-based surrogate models applied to tabular data and classification problems mimic thoroughly the approximated black box model. Secondly, this study aims to propose a novel method for generating a rule-based white box model, allow- ing to obtain explanations more consistent with the approximated complex model. Fig. 1 illustrates the proposed approach allowing a comparison with a typical rule-based explainer generation. To enable verification of the ap- proach used, software developed as part of the research was published\u00b9 for other users. In addition, within the software, a detailed report containing the full experimental results on multiple benchmark datasets was made available. The contributions of this study are as follows.\n\u2022 A comparison of rule-based models approximating the decisions of com- plex (black box) models with rule-based models trained on the original training data. This comparison verifies whether the generation of an approximator is needed and whether rule-based models generated di- rectly on the training data approximate the black box model equally well as rule-based approximators of these models.\n\u2022 Verification whether the decision bases of the black box model and the rule-based white box model that approximates it are consistent, i.e. whether the same decisions of both models are based on the same, similarly important attributes.\n\u2022 A novel method for generating decision rules to properly explain the"}, {"title": null, "content": "black box model decisions, which takes into account the importance of the features of the complex model when generating the rule-based ap- proximator. The proposed method was used to build rule-based factual and counterfactual explanations.\nThe remainder of this paper is organised as follows. Section 2 presents the literature review on XAI methods and rule-based XAI methods in particular. Section 3 introduces formal bases for the proposed method, the method itself and the criteria for consistency evaluation of the surogate explaining model and the black box model. The experiments and their results are presented in Section 4. Section 5 discusses the possible pathways for the application of the rule-based approach in XAI and evaluates the rule-based explanations in the context of Co-12 properties. Section 6 concludes the presented study."}, {"title": "2. Related work", "content": "Due to the importance and ubiquity of artificial intelligence and efforts to introduce regulations related to, among others, the explanation of decisions made by AI models, work on XAI methods has intensified significantly in recent years. It resulted in the publication of numerous review papers and books related to this field [2, 4, 5, 9, 3].\nExisting \u03a7\u0391\u0399 methods can be divided into several classes depending on the perspective adopted for their description. One approach is to distinguish methods that can explain the created model at the level of the entire dataset (global methods) or can explain the basis of the model's decisions for a sin- gle example (local methods). Taking a different perspective, XAI methods can be divided into model specific and model agnostic. In the first case, the explanation method is dedicated to a specific type of model, typically neural network architecture [10]. In the second case, the explanatory method treats the model as a black box and is not related to it in any way [11]. There are many different methods of this type, among which it is possible to distin- guish a group of solutions generating an interpretable model approximating the black box model. In this way, both local and global explanations can be generated, and the resulting explanations can take different forms e.g. linear regression coefficients [12], decision tree based rules determining coun- terfactual explanations [13] or rule-based explantions [14]. Another way of subdividing XAI methods takes into account their application. XAI methods are used in a wide range of applications spanning from medicine [15, 16] to industry and predictive maintenance [17]. Moreover, depending on the type of data to be analysed, a distinction can be made between methods applica- ble to tabular data [18], images [16, 19] or text [20]. There are some methods that can be applied to different types of data. The most popular of these are LIME [12] and SHAP [21], and their various modifications [22, 23, 24, 25].\nAmong the various concepts of how to explain a complex machine learning model, approaches using a rule-based model can be distinguished. Such a model is interpretable, i.e. human-understandable, and can be used to generate both global and local explanations by approximating the complex model. This study focuses on rule-based XAI methods, which are model agnostic treating the model being explained as a black box.\nThere are many approaches to generating local rule-based explanations. If-then rules called anchors presented in [26] define the regions in the feature space where the decision is (almost) always the same and does not depend on"}, {"title": null, "content": "the values of other attributes. Another approach defining a local rule with tunable complexity on the basis of a boolean formula is presented in [27]. The classifier presented in this paper that approximates complex model decisions is trained using native local optimisation techniques, efficiently searching the space of feasible formulas. The method presented in [28] aims to improve the accuracy of the local rule-based explainers by applying weighted voting of multiple rule-based classifiers. The use of an ensemble of rule-based models is expected to better reflect the performance of the complex model being approximated. Counterfactual explanations based on rules determined from a decision tree are presented in [13]. The LORE algorithm introduced in this study generates by means of a genetic algorithm a set of examples in the local neighbourhood of the data instance to be explained. Next, it creates a decision tree that is a local approximation of the complex model and derives rules from this tree that are factual and counterfactual explanations. An extension of the method using a stability-enhancing ensemble of explanatory models is presented in [29]. Another approach in which local (locally opti- mal) rules are generated for single data instances and which can be used for explanation is presented in [30]. In this approach named LORD, attention was paid not only to the accuracy of the rule but also to its efficiency.\nIn addition to the methods presented above that generate local explana- tions, there are many examples of the use of rule-based methods to generate global explanations of complex models. The study [31] presents a method for generating decision sets which are sets of independent if-then rules. Decision set learning is performed through an objective function that simultaneously optimises accuracy and interpretability of the rules. This approach should allow efficient approximation and explanation of complex models. Another rule-based approach for global explanation of trained models is presented in [32]. First, the importance of the features in the complex model is calcu- lated in this method. Next, the original inputs with the feature importance scores are weighted and the original input space is transformed for simpli- fication. Finally, a rule-based explanatory model is induced on the derived features. In [33] the RuleMatrix interactive visualization technique applica- ble to rule-based representation is presented. The authors of the [34] propose to transform the interpretable (surrogate) model approximating the complex model into a Binary Decision Diagram (BDD) [35], which reflects well (better than, for example, rule sets) the complex relationships between attributes. The work [36] does not focus on global explanations at the level of the entire dataset, but presents the application of rule induction to generate explana-"}, {"title": null, "content": "tions characterising the regions in data space on which the model performs particularly poorly. Finally, the RuleXAI library presented in [14] offers both local and global rule-based explanations. This method makes it possible to determine the importance of individual conditions in the rules comprising the surrogate rule-based model. Thus, it allows a better understanding of which feature, and in which value ranges, contributes to the decision of a complex model.\nThe presented literature review indicates that, despite the existence of a variety of approaches to generating rule-based explanations, there is a lack of research ensuring that the explanatory rule-based model mimics the behavior of the approximated black box model. This conclusion motivates the research presented."}, {"title": "3. Materials and Methods", "content": null}, {"title": "3.1. Basic Notions and Definitions", "content": "Let us assume that a universe of possible instances U is given. Each instance $x \\in U$ is characterised by a set of attributes $A = {a_1,...,a_n}$. Each attribute is interpreted as a function $a : U \\rightarrow V_a$, where $V_a$ is a set of possible values. Two types of attributes are discerned: symbolic (discrete- valued) and numeric (real-valued). Each example is also characterised by a special attribute $d$, often called a decision attribute or target. The decision attribute is a discrete class identifier $d : U \\rightarrow V_d$, where $V_d = {L_1, ..., L_k}$ and $L_i, i \\in {1, ..., k}$ is a so-called class label or decision class. Furthermore, $d(x)$ denotes the value of the decision attribute for an example $x$. The system $D(X, A, d)$, where $X \\subseteq U$ is called a decision table. Further notions needed to define the problem of approximating and explaining complex machine learning (black-box) models with a rule-based model are presented in Table 1.\nThe rule-based data model R is a set of classification (decision) rules generated by an induction algorithm. Each ruler \u2208 R has the form:\nIF $c_1 \\land c_2 \\land ... \\land c_m$ THEN $d = L$.\n(1)\nThe premise of a rule is a conjunction of conditions $c_j = a_j v_{a_j}$, with $v_{a_j}$ being an element of the attribute $a_j$ domain and representing a relation (= for nominal attributes; <,<, >, > for numerical ones). The conclusion (decision) $d = L$ of the rule indicates the class label $L \\in {L_1, ..., L_k}$. If an"}, {"title": null, "content": "example $x$ fulfills the conditions of a ruler premise, we say that r covers $x$ ($x$ is covered by r).\nGiven a set of examples $D(XA, d)$ and the rule (1), the set of all examples from X such that d(x) = L is called the set of positive examples, while all other examples are referred to as negative examples. The number of all positive examples and the number of all negative examples are denoted by P and N, respectively. The number of positive examples covered by the rule (1) is denoted by p, and the number of negative examples covered by (1) is denoted by n.\nDecision rules are employed both for data description and for classifying new instances with unknown values of the decision attribute. Typically, a rule is interpreted as a dependency that allows one to infer the membership of an example covered by the rule to the decision class indicated in the rule's conclusion. Since, in the presented considerations, rules are induced based on the learning from examples paradigm, this inference can be fallible, which is a feature of inductive inference.\nA rule that covers an example $x$ can be perceived as an explanation for the"}, {"title": null, "content": "classification of the example to a specific decision class - the class indicated in the conclusion of that rule [26, 37]. In this case, the abductive reasoning is employed, which, due to the nature of rule induction, can also prove to be fallible.\nAmong the various approaches to the induction of classification rules [38, 39, 40, 41, 42], algorithms based on the sequential covering strategy [43, 44, 45, 46] have demonstrated significant utility in data description and classification of unseen examples.\nIn this research, sequential covering algorithm is utilised, which efficacy has been confirmed in a number of publications [47, 48, 46]. Additionally, an object-related modification of this algorithm dedicated to local explainability is employed. The object-related rule induction approach is motivated by rule induction methods used in Rough Set Theory [49, 50], specially by the heuristic strategy of the global and object-related decision reduct finding [51].\nIn both algorithms, a key role plays the rule quality measure (rule search heuristics ([52])), which controls the rule induction process. The primary objective of induction is to generate rules that are both precise (covering only positive examples) and general (covering as many examples as possi- ble). Therefore, rule quality is typically characterised by both precision and coverage calculated as:\n$precision = \\frac{p}{p+n}$,\n(2)\n$coverage = \\frac{p}{P}$\n(3)\nThere is a wide range of rule quality measures that are used to supervise the rule induction process and rule assessment [44, 53, 54, 55] presented in literature. Depending on the measure employed, different classification models are obtained. Optimal results are usually achieved using measures that consider both the precision and coverage of a rule, and the values of P and N [56, 57]. In this study, in addition to the above measures, measure C2, which has the following form, was used to generate rules:\n$C2 = \\frac{\\left( N-\\frac{Np}{N(p+n)}\\right) \\left(P-\\frac{Pn}{N(p+n)}\\right)}{2p}$\n(4)"}, {"title": "3.2. Fusion of rule-based explanations with importance-based feature ordering", "content": "The idea of the sequential covering approach following the separate-and- conquer heuristic is presented in (Algorithm 1). Rule generation is carried"}, {"title": null, "content": "out sequentially for each class and its positive examples. The rules are added iteratively to the initially empty set as long as the entire dataset becomes covered. Every rule must cover at least mincov previously uncovered exam- ples, to ensure the convergence. Therefore, when there are less then mincov uncovered examples left, the generation of consecutive rules is stopped. The induction of a single rule consists of two stages: growing and pruning. In the growing stage (presented in Algorithm 2), elementary conditions are added to the initially empty premise. All possible conditions built on all attributes (line 6: GETPOSSIBLECONDITIONS function call) are considered in this step. The conditions leading to the rule with the best quality are selected (lines 10-12). In the case of nominal attributes, conditions in the form $a_j = v_i$ for all values $v_i$ from the attribute $a_j$ domain are considered. For continuous attributes, $a_j$ values that appear in the observations covered by the rule are sorted. Then, the possible split points $v_j$ are determined as arithmetic means of subsequent $a_i$ values and conditions $a_j < v_j$ and $a_j \\geq v_j$ are evaluated. If several conditions lead to the same results, the one covering more exam- ples is chosen. In the pruning stage, conditions are iteratively removed from the rule premise to achieve the greatest improvement in rule quality. The procedure stops when no conditions can be deleted without decreasing the quality of the rule or when the rule contains only one condition. Finally, the comprehensibility of the rule is improved by merging conditions based on the same numerical attributes. A broader general outline of the sequential covering algorithm is discussed, among others, in [44, 46].\nThe separate-and-conquer approach (Algorithm 1) generates unordered set of rules. Therefore, the classification of a data example, e.g. a test one, requires the evaluation of a set $R_{cov} \\subseteq R$ of rules that cover the example and the aggregation of the results obtained. This differs from ordered rule sets (decision lists), where the first rule covering the investigated observation determines a model response. Aggregation, in the case of classification, is achieved using voting. Each rule from $R_{cov}$ votes with its value of the quality measure.\nThe aim of this study is to modify the sequential covering approach to en- sure that the rule-based explanations are based on the attributes that are im- portant for the decisions made by the approximated complex model. There- fore, the algorithm presented in Algorithm 1 and 2 was subjected to the fol- lowing modifications. The proposed approach assumes that the importance- based feature ordering (fo(M) or fo(M,x)) determined for the black box model M is passed to the algorithm. This ranking specifies a sequence of"}, {"title": "3.3. Evaluation of rule-based explanations", "content": "As outlined in the Introduction section, the aim of this study is to verify the adequacy of the explanations of a black box model M generated by a rule-based model R (especially the explanations generated by a rule-based approximation of the black box model \u2013 RM). Therefore, the following three conditions were defined for R that can be used to explain M:\n$\\forall X \\subseteq U, \\forall x \\in X \\quad R(x) = M(x)$,\n(5)\nwhich means that the explainer R and the model being explained M make the same decisions on each data set,\n$F(R) = F(M)$,\n(6)"}, {"title": null, "content": "$\\forall X \\subseteq U, \\forall x \\in X\\quad fo(R,x) = fo(M,x)$,\n(7)\nwhich means that the importance-based ordering (ranking) of the features on which the explainer R and the model being explained M make decisions"}, {"title": null, "content": "is the same. An ideal explanatory model is one that meets all the conditions listed above.\nWith respect to defined conditions (5-7), the fidelity of the explainer R can be calculated using various methods. Criterion (5) can be verified using a measure of agreement between the decisions of two classifiers, e.g., by determining the Cohen kappa coefficient between models R and M. Criterion (6) can be verified by measuring the mutual inclusion between the feature sets used by the models R and M:\n$\\nu = \\frac{|F(R) \\cap F(M)|}{|F(R) \\cup F(M)|}$,\n(8)\nCriterion (7) can be verified by comparing the importance rankings of at- tributes using, e.g. by determining Kendall's Tau correlation coefficient."}, {"title": "4. Experiments and results", "content": "The proposed method was extensively verified on numerous datasets in the experiments carried out. The conditions presented in Section 3.3, which define the ideal explanatory model, determined the scheme of the analyses conducted. The measures indicated there to verify the quality of the proposed model were used to evaluate and compare the quality of the proposed rule model generation method for black box model explanations.\nThe analyses carried out were divided into three parts. In the first, con- dition (5) of Section 3.3 stating that the generated approximator should make the same decisions as the approximated black box model was verified. The second part verified the consistency of the basis for the decisions made by the model, i.e. the explanations showing on which attributes the rule- based approximator and the black box model made decisions. This analysis includes condition (6) and condition (7), which refer to the overlap of impor- tant attributes for each model and the consistency of the ranking of these attributes. The final part of the analyses conducted was a comparison of the proposed method with the Anchors method [26], selected as the state- of-the-art rule-based XAI solution. The general comparison carried out was complemented by a case study on selected data for both methods. The re- mainder of this section presents the aggregated results of the experiments and the conclusions drawn from them. A detailed report containing the full experimental results and the software used in the experiments are available online"}, {"title": "4.1. Datasets and experimental settings", "content": "The proposed approach was verified on 30 datasets from UCI\u00b2 and OpenML3 repositories. Datasets dedicated to the classification task were selected and care was taken to ensure their diversity. The dataset names and characteris- tics (number of observations, attributes, and classes) are presented in Table 2.\nMoreover the proposed method was applied to approximate and explain three different types of black box models generated by the following methods: extreme gradient boosting (XGB), support vector machines (SVM) and ran- dom forest (RF). Raw data were preprocessed by applying one hot encoding of categorical attributes and imputation of missing values. Each dataset was split into train and test subsets in 80/20 proportion. The models were eval- uated using average balanced accuracy. Exhaustive evaluation, e.g. using"}, {"title": "4.2. Evaluation of rule-based approximation", "content": "The first part of the experiments aimed to evaluate the quality of the rule-based approximation. Two approaches to approximation using a rule- based model were verified. In the first, the rule-based model is generated on the original data on which the black box model was generated. In this case, the quality of the model describing the analysed phenomenon is assessed. In the second approach, the rule-based model is generated on data in which the decision column contains the decisions of the black box model. In this case, the quality of the model that approximates the decisions of the black box model is assessed. Moreover, two methods of the rule-based model generation"}, {"title": null, "content": "were verified. The RuleKit method [46] was used as the implementation of standard separate-and-conquer approach and is hereafter referred to as SC-S (separate-and-conquer - standard). The RMatrix method [48] was used as the implementation of object-related approach and is hereafter referred to as SC- OR (separate-and-conquer - object-related). In each method, two measures of quality assessment were verified in the rule growth process: precision and C2.\nTo assess the quality of approximation the Cohen's kappa coefficient be- tween black box model predictions and rule-based model predictions was calculated on train and test data. The results for the approach on the orig- inal data and on the black box model decision data are presented in Table 3.\nThe obtained results show that the approximation quality of the rule- based models generated on the dataset with black box model decisions are better than the models generated on original data. The mean kappa values are greater than or the same for this data representation in all cases except two. Therefore, further comparisons will be made for models generated on the dataset with black box model decisions. The difference between the methods of rule-based model generation and the quality measure used is particularly apparent for the training data. The results obtained on them show that the precision measure allows to obtain a model with an overall excellent approximation quality and clearly better than the C2 measure. Finally, the results show that the rule-based approximator generated by the SC-OR method was superior.\nThe mean kappa values presented in Table 3 allowed the selection of the rule-based model generation method used in further experiments. Therefore, the implementation of the proposed method was based on the SC-OR ap- proach and the precision measure was used in the rule growth procedure. The rule-based model was generated on data in which the decision column contained the decisions of the black box model."}, {"title": "4.3. Evaluation of explanations", "content": "The second stage of the experiments was dedicated to verifying how well the generated rule-based explanations mimic the black box model decision making. For this purpose, the consistency of the generated rules with the black box model at the attribute level was verified. Two measures were chosen to assess such consistency, as suggested in Section 3.3. Inclusion was chosen to compare the sets of most important attributes for black box model"}, {"title": null, "content": "and its approximator. It shows whether the same key features are used by both models. Inclusion was determined as mutual inclusion (see formula (8)). Correlation was chosen to compare the rankings of the most important features for black box model and its approximator. It shows whether the same most important features rank in the same order for each model. Correlation was determined as Kendall's Tau correlation coefficient. For both measures, unique features occurring in the elementary conditions of the rule explaining the data instance were selected for comparison. The number of these features determined the number of the most important features of the black box model that were taken for comparison.\nThe experiments used rule-based models generated by the SC-OR method. These models consist of rules generated for each data example. Three mod- els generated according to the approaches described in Section 3 were com- pared, and the induction of the rules forming these models used: (i) global importance-based feature ordering common to all rules, (ii) local importance- based feature ordering determined for each data instance separately, (iii) a basic approach that does not use any feature ranking. The global feature ranking was determined for the black box model using an approach referred to as Permutation Feature Importance. The local ranking was determined us- ing the SHAP method, which was applied to each black box model decision (each data instance). Unfortunately, generating SHAP-based explanations for the SVM model (to which TreeSHAP is not applicable) was too time- consuming and no results were obtained for this model."}, {"title": null, "content": "datasets, the generation of explanations for each data instance was aborted due to excessive analysis time. The explanation of a single decision by the Anchors algorithm implementation\u2074 used is approximately twice as long as the SC-OR implementation, and moreover, this implementation does not perform the analysis in parallel. Therefore, the analysis was carried out for the following 21 datasets: balance-scale, breast-w, car, churn, cmc, credit- a, cylinder-bands, diabetes, kdd-synthetic-control, kr-vs-kp, mammographic- masses, mushroom, nursery, phoneme, qsar-biodeg, segment, tic-tac-toe, ti- tanic, vehicle, wall-robot-navigation, wdbc. Besides, the methods used and settings adopted were the same as in the experiments in Section 4.3.\nThe calculated consistency of the rule-based approximators with the black box model is presented in Table 5. The results presented in this table follow a format analogous to the results presented for the previous experiment. If Feature importance is specified as SC-OR Global, this means that the consistency values in the table refer to the global feature ranking calculated"}, {"title": "4.4. Case-study", "content": "The case study carried out aims to compare and discus the selected ex- planations of the XGB model decisions on the selected two datasets. The explanations were generated by the proposed method implemented in the SC-OR algorithm, the Anchors algorithm and the standard rule induction (SC) method.\nThe first dataset belongs to a medical domain. It was created for the task of identifying genetic aberrations from the results of a flow cytometry"}, {"title": null, "content": "technique [59]. The presence of aberrations is an important factor for de- termination of patient prognosis at the initial phase of acute lymphoblastic leukaemia (ALL) diagnostics. ALL is the most frequent leukaemia in chil- dren.\nThe first data instance, which is analysed below, is an example of a true positive decision \"no aberrations\" of a black box model. The local ordering of features with respect to their importance, which was obtained for this data instance using the SHAP method, has the following form: cd9, cd123,"}, {"title": "5. Discussion", "content": null}, {"title": "5.1. Rule-based explanation schemes", "content": "Rule-based approaches can be used to generate explanations at different levels (global or local), which can be driven by different aspects (model or instance). Therefore, three scenarios were defined and described below, in which rule-based explainability can be applied.\nThe first scenario concerns global explainability (which can be used for local explanations) for training dataset D. In this approach, a rule-based approximator RM is generated on the training data on which a black box model M was generated. The generated rule-based approximator RM ex- plains the bases for decision-making for M on the training dataset, i.e. it is a source of global explanations. In addition, it allows for local explanations of the training data instances. For each example x \u2208 D, an explanation can be obtained based on the rules that cover this example (using only the rules with the decision predicted for this example by the model M). The explanation can be based on all rules or only on the best one (with respect to the rule quality measure). In addition, it can be shown whether this data example is covered by other rules with different decisions (contradictory to"}, {"title": null, "content": "those that explain example x). Furthermore, using the RuleXAI package [14], it is possible to indicate in the rule set of the RM model the most important conditions that determine the assignment to each decision class. Similarly, it is possible in the set of rules covering a particular example x to indicate the most important conditions that determined the assignment of this example to that class rather than another. The scenario presented above can be realised for the approximating model RM determined by the covering method (presented in the Algorithm 1) and the object-related method with or without filtering (presented in the Algorithm 3).\nThe second scenario concerns local explainability driven by a global model. In this case, the explanations being created concern a data example (x \u2209 D) that was not involved in the generation of the black box model M and its rule-based approximator RM. Therefore, it concerns the issue of prediction. To generate the explanation, rules from RM covering x and having the same decision as M for x are selected. The explanation can be based on all rules or only on the best one. Additionally, contradictory rules covering x can be presented. Further, more detailed analysis can be performed by the RuleXAI package as described for the previous scenario. Both of the above scenarios related to a global rule-based model approximating a black box model are illustrated in Fig. 8.\nFinally, the third scenario concerns local explainability driven by a data instance. Similarly to the previous approach, in this scenario the decision M(x) of the model M for a data instance x that does not belong to the training dataset (x \u2209 D) is explained. This time the approximating model RM is not generated. However, in order to generate an explanation, a rule for x with decision M(x) is generated from the dataset D. This rule is generated in such a way that it must cover the data instance x explaining the decision of the model M for this data instance. The explanation of the black box model decision M(x) (generation of a confirmatory rule) can be extended by generating a contradictory rule for the opposite class. In the general case, for a multi-class problem, the generation of a contradictory rule for each class could be considered. The analysis of such explanations could be further supported by information about the quality of the generated rules, which would allow the user to understand the difference in the strength of the explanations presented. A general diagram illustrating the presented scenario is presented in Fig. 9.\nFor each scenario presented above, it is possible to consider whether and how to take into account the importance-based feature ordering fo intro-"}, {"title": null, "content": "ordering when generating rules and use a basic covering algorithm. Another option is to take into account the global fo ordering, which is identical for each example being explained. Finally, in case of the explainability driven by a data instance, a local ordering can be used, which may be different for each data example. Which approach is used should depend on the user and their subjective purpose and need."}, {"title": "5.2. Properties of explanations \u2013 rule-based perspective", "content": "To better evaluate the proposed rule-based approach and to present it in a broader context, it has been analysed in terms of the Co-12 properties defined in [60]. Co-12 allows to assess the quality of explanations from the"}, {"title": null, "content": "perspective of their content and presentation and from the"}]}