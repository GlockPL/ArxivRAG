{"title": "DISTRIBUTED IN-CONTEXT LEARNING\nUNDER NON-IID AMONG CLIENTS", "authors": ["Siqi Liang", "Sumyeong Ahn", "Jiayu Zhou"], "abstract": "Advancements in large language models (LLMs) have shown their effectiveness in multiple complicated natural language reasoning tasks. A key challenge remains in adapting these models efficiently to new or unfamiliar tasks. In-context learning (ICL) provides a promising solution for few-shot adaptation by retrieving a set of data points relevant to a query, called in-context examples (ICE), from a training dataset and providing them during the inference as context. Most existing studies utilize a centralized training dataset, yet many real-world datasets may be distributed among multiple clients, and remote data retrieval can be associated with costs. Especially when the client data are non-identical independent distributions (non-IID), retrieving from clients a proper set of ICEs needed for a test query presents critical challenges. In this paper, we first show that in this challenging setting, test queries will have different preferences among clients because of non-IIDness, and equal contribution often leads to suboptimal performance. We then introduce a novel approach to tackle the distributed non-IID ICL problem when a data usage budget is present. The principle is that each client's proper contribution (budget) should be designed according to the preference of each query for that client. Our approach uses a data-driven manner to allocate a budget for each client, tailored to each test query. Through extensive empirical studies on diverse datasets, our framework demonstrates superior performance relative to competing baselines.", "sections": [{"title": "1 Introduction", "content": "Recent significant progress in large language models (LLMs) [1, 2, 3, 4] has demonstrated their effectiveness across\nvarious natural language processing (NLP) tasks [5, 6]. Despite their impressive performances, they still require\nadaptation to the specific downstream tasks for better performance. However, adaptation poses challenges due to LLMs'\nvast number of trainable parameters.\nIn-context learning (ICL) [7] is a notable method that distinguishes itself through both its effectiveness and efficiency.\nIn brief, ICL adapts to the target task by incorporating context information following two primary steps: i) identify\nsamples from the training dataset helpful to solve the target query by creating a prompt describing a context; ii) feed the\nconstructed prompt with the target query and get the answer. Previous related works on ICL mainly have focused on the\nconstruction of a prompt describing the context, which involves several sub-problems, such as the retrieval of in-context\nexamples (ICEs) [8] and determining the optimal sequence for the selected ICEs [9].\nA common assumption in most existing ICL research is that the system has access to a high-quality centralized dataset\nused for retrieval. However, in many application scenarios, such as health informatics, centralized datasets may not be\nfeasible, and data could be distributed in different institutions, which calls for the distributed ICL. In addition, when the\ndata is proprietary and possesses high value towards inferences, access to data entries may also be bound to data pricing\nstrategies [10, 11]. For instance, the system needs to pay the local institution based on the number of samples sent to\nthe system [12] as a means to share profits from inferences. Under this scenario, aggregating ICEs from local clients to\na center server for ICL entails significant financial costs and lacks efficiency."}, {"title": "2 Problem Formulation", "content": "In this section, we provide a detailed problem formulation. First, we begin with the specifics of in-context learning\n(ICL), followed by a description of distributed non-IID ICL.\n2.1 In-Context Learning\nNotation. We consider a NLP tasks which have training dataset $D = \\{(x_i, y_i)\\}_{i=1}^N$ with N training samples. Here, $x_i$\nis the input text, and $y_i$ is the corresponding output. In the test phase, a test query $x_q$ is given.\nRetrieval. We employ the off-the-shelf pre-trained retriever KATE [14]\u00b3, which utilizes k-NN example selection. This\nretriever employs a sentence encoder $E(\u00b7)$ to measure the similarity between the in-context example $x_i$ in dataset $D$ and\nthe query $x_q$ as follows:\n$d(e_i, e_q) = ||e_q - e_i||_2$,\nwhere $e_q = E(x_q)$ and $e_i = E(x_i)$. We select k samples using the following criterion:\n$T(e_q, k|D) = \\arg \\text{Top-k}(d(e_i, e_q))$,\n$e_i=E(x_i) \\forall (x_i, y_i) \\in D$\nwhere $T(e_q, k|D)$ denotes the selected samples from the dataset $D$, and used for inference.\nICL Inference. In the test phase, given a test query with input $x_i$, relevant k training samples called in-context\nexamples (ICEs) are selected, i.e., $S = T(e_q, k|D)$. Based on the retrieved samples, we feed the constructed context\nprompt $s(S, x_q)$ into LLM for inference and obtain results via:\n$y_t = \\arg \\max_{y} P_{LLM}(y|s(S,x_q), y_{<t})$\n$s(S, x_q) = (X_1, Y_1) \\odot ... \\odot (X_k, Y_k) x_q$,\nwhere the $\\odot$ operation denotes concatenation, and $s(S, x_q)$ is the context constructed using query $x_q$ and samples in $S$;\nthe term $P_{LLM}$ represents the output softmax probability of the LLM, functioning autoregressive, meaning that the output\nup to time $t$, i.e., $y_{<t}$, is input back into the model to generate the $t^{th}$ output, $y_t$. Previous works [15, 16] on ICL mainly\nfocus on the selection of $S$ under a centralized setting. However, we investigate the scenario where $D$ is split among\nseveral clients, each following non-IID distributions.\n2.2 Distributed non-IID ICL\nDistributed ICL Setting. We consider C clients with a centralized server in our system. Each client $c \\in [C]$ has local\ntraining dataset $D_c = \\{(x_i^c, y_i^c)\\}_{i=1}^{N_c}$ with $N_c$ training samples. Note that $D_c$ follows different distributions for different"}, {"title": "3 Observations", "content": "In this section, we describe several empirical supports to handle the distributed non-IID ICL. First, we demonstrate\nthat non-IID distributions hinder the merging of scattered information. We then establish our goal, termed as oracle\nbudget, which reflects the server's preference for each client if the server knows all distributed data. Finally, we check if\npredicting the oracle budget of each test query for inference is feasible.\n3.1 Non-IIDness Leads to Performance Drop\nFirst of all, we evaluate the effect of non-IIDness. Straightforwardly, we distribute the budget $\\{k_c\\}_{c=1}^C$ uniformly\naccording to the following criteria: Given C clients are involved in answering this question, and the number of samples\nfor context is k. We first explore the na\u00efve equally assigned local budget scheme in both IID and non-IID settings. That\nis, each client $c \\in [C]$ locally retrieves top-$k_c$ samples where $k_c = \\lfloor \\frac{k}{C} \\rfloor$ from local dataset $D_c$. Detailed experimental\nsettings are described in Appendix B.\nAs illustrated in Figure 3, we observe the followings: (1) There is no significant performance degradation between\nthe centralized case ($\\rightarrow$) and the IID case ($\\rightarrow$). This is expected, as the merged top-$k_c$ samples in the IID case closely\nresemble the centralized top-k samples. Any minor discrepancies are attributed to differences in sample ordering. (2)\nHowever, performance degradation becomes pronounced in non-IIDness case (refer to the comparison between $\\rightarrow$\nand $\\rightarrow$). Hereinafter, we gather insights to address the distributed non-IIDness ICL.\n3.2 Proper Budget per Query for Each Client\nOracle budget. The remaining issue is that to make the server operate similar with the centralized manner, it needs to\nallocate the budget as if it knows complete knowledge of all clients. We call this budget for each client as the oracle\nbudget for query embedding $e_q$ and define it as follows:\n$k^*_c(e_q) = | \\text{T} (e_q, k|D_c) \\cap \\text{T} (e_q, k|D)|,$"}, {"title": "4 Method", "content": "In this section, we outline the proposed algorithm to mitigate non-IIDness in the ICL framework. Specifically, we show\nhow to train the budget allocator and conduct inference."}, {"title": "4.1 Train a Budget Allocator", "content": "Based on Section 3, it is feasible to assign budgets of each client by using the embeddings obtained from the retriever\nencoder $E$. We first construct the datasets having the targeting budget values and then train the budget allocator. The\npseudo-codes are described in Algorithm 1 and 2.\nConstruct dataset for oracle budget. First, we explain how to create a dataset to train the budget allocator for each\nclient, as described in Algorithm 2. Given proxy dataset $D_{proxy}$, for all embeddings $e_j = E(x_j)$ where $(x_j, y_j) \\in D_{proxy}$,\nwe request k samples from each client $c \\in [C]$ using Top-k procedure, i.e., $S_c = \\text{T}(e, k|D_c)$. Once the server receives\nk examples from each clients, i.e., $\\{S_c\\}_{c=1}^C$, it merges and re-orders them to obtains $S_{top}^*$. Based on $S_{top}^*$, we count the\nnumber of samples from each client in $S_{top}^*$, i.e., compute $k_c(e_j)$. After counting $k_c(e_j)$ for all clients, we quantize the\nbudget levels for each client using the quantization hyper-parameter $\\delta$. As a result, the output of this procedure is $B_{proxy}$\nfor all clients, composed of embeddings $e$ and their respective budgets $k_c(e_j)$."}, {"title": "4.2 Inference Using Budget Allocator", "content": "We derive the response to the test query $x_q$ utilizing the LLM M(\u00b7) through the described steps (see Algorithm 3 for\nspecifics). We first extract the embedding $e_q = E(x_q)$. Then, we compute the allocated budget $\\{k_c = f_c(e_q)\\}_{c=1}^C$ and\nsend $k_c$ to each client. Each client sends back top $k_c + a$ samples, i.e., $S_c$, to the server. Note that we summarize how\nthe budget allocator outputs $k_c$ in Figure 5. Here, $a$ denotes the buffering hyper-parameter, which increases the chances\nfor each client to be involved. After collecting $S_{agg} = \\cup_{c \\in [C]} S_c$, we aggregate them and run the usual ICL procedure."}, {"title": "5 Experiment", "content": "5.1 Experiment setup\nFirst, we summarize the baselines, datasets, and the method for constructing non-IID settings. Finally, we depict the\nimplementation details.\nBaselines. We compare our algorithm with various baselines, including social learning [13], which does not account for\nnon-IIDness, and other possible ways for handling distributed non-IID ICL, such as Zero-shot, Proxy-only, Singleton\n(single client), Uniform-budget, Random-budget, and \u221e-budget (oracle case). The detailed explanations are described\nin Appendix C.\nDatasets. We check the performance under 7 datasets \u2013 Sentiment classification: SST-5 [20], Amazon [21], Yelp [22],\nMR [23], Topic classification: Yahoo, AGNews [22], and Subjectivity classification: Subj [24].\nDataset partition for non-IIDness. We split the training dataset into C subsets to ensure they follow a non-IID\ndistribution. To achieve this, we partition the data based on class, following the splitting criteria outlined in [17].\nSpecifically, each client has access to only y < \u0393 classes, where \u0393 represents the total number of classes. We outline\nthe summary of y for each dataset in Appendix D.\nDataset paraphrasing. Due to concerns about sharing private samples between servers and clients, various techniques\nhave been developed for natural language tasks. In this paper, we adopt the paraphrasing technique used in [13].\nSpecifically, we utilize a small language model [25], designed for small terminal devices, to generate paraphrased\nquestions. In Appendix E, we summarize the instructions provided to the language model for rephrasing queries in the\ntraining dataset.\nImplementation details. We implement our method as well as baselines based on OpenICL [26]. For the retriever\nscenario, we utilize the pre-trained KATE retriever [14], which has been trained on the SNLI [27] and MultiNLI [28]"}, {"title": "5.2 Main results", "content": "We have presented the performance of our algorithm and baselines in Table 1. First, we can observe that performance\nvaries significantly depending on the way the budget is allocated, which indicates that the budget allocation scheme\nreally matters in distributed non-IID ICL. Additionally, even when using only the proxy dataset, there is a perfor-\nmance improvement, and this performance surpasses that of using other clients which have the tilted local datasets\n(e.g., 29.19% \u2192 40.64% in SST-5 case). This indicates that utilizing a biased dataset can degrade the ICL performance.\nAlthough social learning algorithm has shown good performance in the previous paper, it does not perform well\nunder the non-IID cases configured in this research. If we can use an infinite budget, all settings would exhibit high\nperformance. However, our proposed algorithm demonstrates better performance than the infinite budget upper limit\n(e.g., 34.86% \u2192 35.48% in the Yelp case). This is likely due to a mechanism that prevents unnecessary information\nfrom being selected by the retriever with high importance. Ultimately, the proposed algorithm shows an average\nperformance improvement of 5.05% percentage points across seven datasets compared to the best performance of\nbaselines using the proxy dataset. This shows that the proposed algorithm can handle the non-IID case well.\n5.3 Analysis\nIn this section, we further examine four key aspects: (1) privacy-preserving case analysis, which encompasses para-\nphrasing both training and testing queries, (2) sensitivity to hyper-parameters, (3) the performance of the trained budget\nallocator, and (4) the compatibility of the LLMs."}, {"title": "6 Related Work", "content": "In-context learning. ICL [7] is one of the fastest paradigms using pre-trained LLMs by feeding several examples to\nconstruct the context to solve the given query. The main criteria of this research field are to find the most informative\nsamples among the training datasets. For example [14] trains BERT [31] oriented encoder and use the k nearest\nneighbors. One of the reasonable sparse retriever, rule-based approach, is using BM25 [8] which measures the term-\nfrequency. [32] proposed an efficient retriever called EPR. It trains two encoders by inheriting the method of dense\npassage retriever (DPR) [33] under the loss of positive and negative pairs. To reduce the domain specificity, [34]\nproposed UDR, which is applicable to multiple domain tasks in a universal way and shows reasonable performance\nfrom a single retriever. PromptPG [35] utilized a reinforcement learning framework to train the retriever so that it can\ngenerate context to improve the answerability of LLMs. Similarly, LLM-R [36] uses a reward model to train the retriever.\nNote that extensive research has not targeted to solve the distributed cases. These works have seen the centralized case.\nDistributed ICL. To the best of our knowledge, only a single study [13] tries to address ICL in a distributed manner.\nHowever, this paper solely focuses on merging the distributed information without considering the nature of the\nnon-identically distributed information. Many studies, such as those on federated learning [37, 38, 39], address the\nnon-IID distribution of datasets, highlighting the need to handle distributed non-IID ICL."}, {"title": "7 Conclusion", "content": "In this paper, we tackle the challenge of ICL when datasets are distributed among clients with non-IID distributions.\nInitially, we examine if non-IID distributions lead to performance degradation and discover that they cause significant\ndrops in performance. We propose an algorithm that learns the task of budget assignment and employs it during\ninference to allocate appropriate budgets for each query. Using this proposed algorithm, we achieve performance\nimprovements across various benchmarks."}]}