{"title": "Distributed In-Context Learning Under Non-IID Among Clients", "authors": ["Siqi Liang", "Sumyeong Ahn", "Jiayu Zhou"], "abstract": "Advancements in large language models (LLMs) have shown their effectiveness in multiple complicated natural language reasoning tasks. A key challenge remains in adapting these models efficiently to new or unfamiliar tasks. In-context learning (ICL) provides a promising solution for few-shot adaptation by retrieving a set of data points relevant to a query, called in-context examples (ICE), from a training dataset and providing them during the inference as context. Most existing studies utilize a centralized training dataset, yet many real-world datasets may be distributed among multiple clients, and remote data retrieval can be associated with costs. Especially when the client data are non-identical independent distributions (non-IID), retrieving from clients a proper set of ICEs needed for a test query presents critical challenges. In this paper, we first show that in this challenging setting, test queries will have different preferences among clients because of non-IIDness, and equal contribution often leads to suboptimal performance. We then introduce a novel approach to tackle the distributed non-IID ICL problem when a data usage budget is present. The principle is that each client's proper contribution (budget) should be designed according to the preference of each query for that client. Our approach uses a data-driven manner to allocate a budget for each client, tailored to each test query. Through extensive empirical studies on diverse datasets, our framework demonstrates superior performance relative to competing baselines.", "sections": [{"title": "1 Introduction", "content": "Recent significant progress in large language models (LLMs) [1, 2, 3, 4] has demonstrated their effectiveness across various natural language processing (NLP) tasks [5, 6]. Despite their impressive performances, they still require adaptation to the specific downstream tasks for better performance. However, adaptation poses challenges due to LLMs' vast number of trainable parameters.\nIn-context learning (ICL) [7] is a notable method that distinguishes itself through both its effectiveness and efficiency. In brief, ICL adapts to the target task by incorporating context information following two primary steps: i) identify samples from the training dataset helpful to solve the target query by creating a prompt describing a context; ii) feed the constructed prompt with the target query and get the answer. Previous related works on ICL mainly have focused on the construction of a prompt describing the context, which involves several sub-problems, such as the retrieval of in-context examples (ICEs) [8] and determining the optimal sequence for the selected ICEs [9].\nA common assumption in most existing ICL research is that the system has access to a high-quality centralized dataset used for retrieval. However, in many application scenarios, such as health informatics, centralized datasets may not be feasible, and data could be distributed in different institutions, which calls for the distributed ICL. In addition, when the data is proprietary and possesses high value towards inferences, access to data entries may also be bound to data pricing strategies [10, 11]. For instance, the system needs to pay the local institution based on the number of samples sent to the system [12] as a means to share profits from inferences. Under this scenario, aggregating ICEs from local clients to a center server for ICL entails significant financial costs and lacks efficiency."}, {"title": "2 Problem Formulation", "content": "In this section, we provide a detailed problem formulation. First, we begin with the specifics of in-context learning (ICL), followed by a description of distributed non-IID ICL.\n2.1 In-Context Learning\nNotation. We consider a NLP tasks which have training dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$ with $N$ training samples. Here, $x_i$ is the input text, and $y_i$ is the corresponding output. In the test phase, a test query $x_q$ is given.\nRetrieval. We employ the off-the-shelf pre-trained retriever KATE [14]\u00b3, which utilizes k-NN example selection. This retriever employs a sentence encoder $E(\\cdot)$ to measure the similarity between the in-context example $x_i$ in dataset $\\mathcal{D}$ and the query $x_q$ as follows:\n$d(e_i, e_q) = ||e_q - e_i||_2,$\nwhere $e_q = E(x_q)$ and $e_i = E(x_i)$. We select $k$ samples using the following criterion:\n$\\mathcal{T}(e_q, k|\\mathcal{D}) = \\arg \\underset{e_i=E(x_i) | (x_i, y_i) \\in \\mathcal{D}}{\\text{Top-k}} (d(e_i, e_q)),$\nwhere $\\mathcal{T}(e_q, k|\\mathcal{D})$ denotes the selected samples from the dataset $\\mathcal{D}$, and used for inference.\nICL Inference. In the test phase, given a test query with input $x_i$, relevant $k$ training samples called in-context examples (ICEs) are selected, i.e., $\\mathcal{S} = \\mathcal{T}(e_q, k|\\mathcal{D})$. Based on the retrieved samples, we feed the constructed context prompt $s(\\mathcal{S}, x_q)$ into LLM for inference and obtain results via:\n$y_t = \\arg \\underset{y}{\\text{max}} P_{LLM}(y|s(\\mathcal{S}, x_q), y_{<t})$\n$s(\\mathcal{S}, x_q) = (X_1, Y_1) \\odot ... \\odot (X_k, Y_k) x_q,$\nwhere the $\\odot$ operation denotes concatenation, and $s(\\mathcal{S}, x_q)$ is the context constructed using query $x_q$ and samples in $\\mathcal{S}$; the term $P_{LLM}$ represents the output softmax probability of the LLM, functioning autoregressive, meaning that the output up to time $t$, i.e., $y_{<t}$, is input back into the model to generate the $t$th output, $y_t$. Previous works [15, 16] on ICL mainly focus on the selection of $\\mathcal{S}$ under a centralized setting. However, we investigate the scenario where $\\mathcal{D}$ is split among several clients, each following non-IID distributions.\n2.2 Distributed non-IID ICL\nDistributed ICL Setting. We consider $C$ clients with a centralized server in our system. Each client $c \\in [C]$ has local training dataset $D_c = \\{(x_i^c, y_i^c)\\}_{i=1}^{N_c}$ with $N_c$ training samples. Note that $D_c$ follows different distributions for different"}, {"title": "3 Observations", "content": "In this section, we describe several empirical supports to handle the distributed non-IID ICL. First, we demonstrate that non-IID distributions hinder the merging of scattered information. We then establish our goal, termed as oracle budget, which reflects the server's preference for each client if the server knows all distributed data. Finally, we check if predicting the oracle budget of each test query for inference is feasible.\n3.1 Non-IIDness Leads to Performance Drop\nFirst of all, we evaluate the effect of non-IIDness. Straightforwardly, we distribute the budget $\\{k_c\\}_{c=1}^C$ uniformly according to the following criteria: Given $C$ clients are involved in answering this question, and the number of samples for context is $k$. We first explore the na\u00efve equally assigned local budget scheme in both IID and non-IID settings. That is, each client $c \\in [C]$ locally retrieves top-$k_c$ samples where $k_c = [\\frac{k}{C}]$ from local dataset $D_c$. Detailed experimental settings are described in Appendix B.\nAs illustrated in Figure 3, we observe the followings: (1) There is no significant performance degradation between the centralized case ($\\bigtriangleup$) and the IID case ($\\blacksquare$). This is expected, as the merged top-$k_c$ samples in the IID case closely resemble the centralized top-$k$ samples. Any minor discrepancies are attributed to differences in sample ordering. (2) However, performance degradation becomes pronounced in non-IIDness case (refer to the comparison between $\\blacksquare$ and $\\circ$). Hereinafter, we gather insights to address the distributed non-IIDness ICL.\n3.2 Proper Budget per Query for Each Client\nOracle budget. The remaining issue is that to make the server operate similar with the centralized manner, it needs to allocate the budget as if it knows complete knowledge of all clients. We call this budget for each client as the oracle budget for query embedding $e_q$ and define it as follows:\n$k^*_c(e_q) = | \\mathcal{T} (e_q, k|D_c) \\cap \\mathcal{T} (e_q, k|D)|,$"}, {"title": "4 Method", "content": "In this section, we outline the proposed algorithm to mitigate non-IIDness in the ICL framework. Specifically, we show how to train the budget allocator and conduct inference."}, {"title": "5 Experiment", "content": "5.1 Experiment setup\nFirst, we summarize the baselines, datasets, and the method for constructing non-IID settings. Finally, we depict the implementation details.\nBaselines. We compare our algorithm with various baselines, including social learning [13], which does not account for non-IIDness, and other possible ways for handling distributed non-IID ICL, such as Zero-shot, Proxy-only, Singleton (single client), Uniform-budget, Random-budget, and$\\infty$-budget (oracle case). The detailed explanations are described in Appendix C.\nDatasets. We check the performance under 7 datasets \u2013 Sentiment classification: SST-5 [20], Amazon [21], Yelp [22], MR [23], Topic classification: Yahoo, AGNews [22], and Subjectivity classification: Subj [24].\nDataset partition for non-IIDness. We split the training dataset into $C$ subsets to ensure they follow a non-IID distribution. To achieve this, we partition the data based on class, following the splitting criteria outlined in [17]. Specifically, each client has access to only $y < \\Gamma$ classes, where $\\Gamma$ represents the total number of classes. We outline the summary of $y$ for each dataset in Appendix D.\nDataset paraphrasing. Due to concerns about sharing private samples between servers and clients, various techniques have been developed for natural language tasks. In this paper, we adopt the paraphrasing technique used in [13]. Specifically, we utilize a small language model [25], designed for small terminal devices, to generate paraphrased questions. In Appendix E, we summarize the instructions provided to the language model for rephrasing queries in the training dataset.\nImplementation details. We implement our method as well as baselines based on OpenICL [26]. For the retriever scenario, we utilize the pre-trained KATE retriever [14], which has been trained on the SNLI [27] and MultiNLI [28]"}, {"title": "5.2 Main results", "content": "We have presented the performance of our algorithm and baselines in Table 1. First, we can observe that performance varies significantly depending on the way the budget is allocated, which indicates that the budget allocation scheme really matters in distributed non-IID ICL. Additionally, even when using only the proxy dataset, there is a perfor- mance improvement, and this performance surpasses that of using other clients which have the tilted local datasets (e.g., 29.19% \u2192 40.64% in SST-5 case). This indicates that utilizing a biased dataset can degrade the ICL performance. Although social learning algorithm has shown good performance in the previous paper, it does not perform well under the non-IID cases configured in this research. If we can use an infinite budget, all settings would exhibit high performance. However, our proposed algorithm demonstrates better performance than the infinite budget upper limit (e.g., 34.86% \u2192 35.48% in the Yelp case). This is likely due to a mechanism that prevents unnecessary information from being selected by the retriever with high importance. Ultimately, the proposed algorithm shows an average performance improvement of 5.05% percentage points across seven datasets compared to the best performance of baselines using the proxy dataset. This shows that the proposed algorithm can handle the non-IID case well.\n5.3 Analysis\nIn this section, we further examine four key aspects: (1) privacy-preserving case analysis, which encompasses para- phrasing both training and testing queries, (2) sensitivity to hyper-parameters, (3) the performance of the trained budget allocator, and (4) the compatibility of the LLMs."}, {"title": "6 Related Work", "content": "In-context learning. ICL [7] is one of the fastest paradigms using pre-trained LLMs by feeding several examples to construct the context to solve the given query. The main criteria of this research field are to find the most informative samples among the training datasets. For example [14] trains BERT [31] oriented encoder and use the k nearest neighbors. One of the reasonable sparse retriever, rule-based approach, is using BM25 [8] which measures the term- frequency. [32] proposed an efficient retriever called EPR. It trains two encoders by inheriting the method of dense passage retriever (DPR) [33] under the loss of positive and negative pairs. To reduce the domain specificity, [34] proposed UDR, which is applicable to multiple domain tasks in a universal way and shows reasonable performance from a single retriever. PromptPG [35] utilized a reinforcement learning framework to train the retriever so that it can generate context to improve the answerability of LLMs. Similarly, LLM-R [36] uses a reward model to train the retriever. Note that extensive research has not targeted to solve the distributed cases. These works have seen the centralized case.\nDistributed ICL. To the best of our knowledge, only a single study [13] tries to address ICL in a distributed manner. However, this paper solely focuses on merging the distributed information without considering the nature of the non-identically distributed information. Many studies, such as those on federated learning [37, 38, 39], address the non-IID distribution of datasets, highlighting the need to handle distributed non-IID ICL."}, {"title": "7 Conclusion", "content": "In this paper, we tackle the challenge of ICL when datasets are distributed among clients with non-IID distributions. Initially, we examine if non-IID distributions lead to performance degradation and discover that they cause significant drops in performance. We propose an algorithm that learns the task of budget assignment and employs it during inference to allocate appropriate budgets for each query. Using this proposed algorithm, we achieve performance improvements across various benchmarks."}]}