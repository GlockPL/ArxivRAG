{"title": "StoX-Net: Stochastic Processing of Partial Sums for Efficient In-Memory Computing DNN Accelerators", "authors": ["Ethan G Rogers", "Sohan Salahuddin Mugdho", "Kshemal Kshemendra Gupte", "Cheng Wang"], "abstract": "Crossbar-based in-memory computing (IMC) has emerged as a promising platform for hardware acceleration of deep neural networks (DNNs). However, the energy and latency of IMC systems are dominated by the large overhead of the peripheral analog-to-digital converters (ADCs). To address such ADC bottleneck, here we propose to implement stochastic processing of array-level partial sums (PS) for efficient IMC. Leveraging the probabilistic switching of spin-orbit torque magnetic tunnel junctions, the proposed PS processing eliminates the costly ADC, achieving significant improvement in energy and area efficiency. To mitigate accuracy loss, we develop PS-quantization-aware training that enables backward propagation across stochastic PS. Furthermore, a novel scheme with an inhomogeneous sampling length of the stochastic conversion is proposed. When running ResNet20 on the CIFAR-10 dataset, our architecture-to-algorithm co-design demonstrates up to 22x, 30x, and 142x improvement in energy, latency, and area, respectively, compared to IMC with standard ADC. Our optimized design configuration using stochastic PS achieved 666x (111x) improvement in Energy-Delay-Product compared to IMC with full precision ADC (sparse low-bit ADC), while maintaining near-software accuracy at various benchmark classification tasks.", "sections": [{"title": "INTRODUCTION", "content": "Crossbar-based analog in-memory computing (IMC) has demonstrated great potential for Deep Neural Network (DNN) acceleration by achieving high efficiency and massive parallelism at processing matrix-vector-multiplications (MVMs)[19], which dominates most state-of-the-art DNN workloads [4]. However, the hardware cost of large-scale IMC systems is dominated by the significant overhead of peripheral analog-to-digital converters (ADCs), which is required to ensure robust data communication in a large-scale system. ADC consumes over 60-80% of the energy and chip area of IMC hardware [3, 19]. Due to the large area overhead of high-precision A-D conversion, an ADC must be shared by multiple columns in a cross-bar. Such design leads to a severe throughput bottleneck since the processing of the array-level outputs across multiple columns has to be done sequentially.\nWhile various recent works have aimed to mitigate the ADC bottleneck [6, 17, 26], minimizing ADC overhead in IMC while still maintaining satisfactory inference accuracy remains a significant challenge. Various factors contribute to the difficulty in addressing the ADC bottleneck. (1) First, since the array-level partial sums (PS) are not represented at the application level, the standard quantization approach focusing on activation and weights can not directly address the ADC precision for PS. (2) Second, while quantization-aware training is effective at enabling hardware-friendly models, training with PS quantization desires backward propagation with careful incorporation of array-level variables, which requires re-designing the computational graph of the DNN model. (3) Third, the required ADC bit precision for MVM may vary significantly depending on both the algorithmic attributes (such as sparsity and DNN layer dimension) and hardware attributes (such as array size and bits per memory cell). Accommodating these varying scenarios requires reconfigurability and flexibility in ADC, leading to increased overhead and design complexity.\nIn this work, we propose stochastic processing of the array-level partial sum for efficient IMC leveraging a spin-orbit-torque magnetic tunnel junction (SOT-MTJ) with simple circuitry such as an inverter. To address the accuracy drop due to loss of information at the stochastic PS, a PS-aware training methodology with the stochastic switching behavior of SOT-MTJs is developed. The low-overhead crossbar peripheral based on the proposed spintronic devices/circuits demonstrates significant improvement in energy efficiency and considerable area savings. Our proposed IMC design eliminates the ADC bottleneck and opens up an exciting direction where stochastic computation could play a vital role in designing ultra-efficient non-von Neumann ML accelerators. The major contributions of our work are the following:\n\u2022 We propose StoX-Net, an IMC architecture with aggressively quantized array-level partial sums based on stochastic switching dynamics of SOT-MTJs with simple CMOS peripherals. The ADC bottleneck in IMC is eliminated, leading to significant improvement in hardware efficiency.\n\u2022 We develop a comprehensive framework of PS quantization-aware training that addresses the accuracy degradation due to the stochastic conversion. In particular, both the device-level stochastic MTJ switching behavior, and key architectural attributes including bit slicing and array size, are incorporated into backward propagation. Our model with 1-bit stochastic PS achieves (or slightly exceeds) state-of-the-art accuracy at benchmarking image classification tasks.\n\u2022 We identify that the state-of-the-art quantization of convolutional DNN has severe limitations due to the first convolution layer remaining at high precision. The first convolution layer is typically compute-intensive and dominates the total computation of DNN. To address this challenge, we explore layer-wise inhomogeneous sampling numbers to enable aggressive PS quantization on the first convolution layer. Our inhomogeneous stochastic sampling based on a Monte-Carlo-based sensitivity analysis achieves improvement in accuracy with minimal additional overhead, demonstrating an advantageous trade-off."}, {"title": "BACKGROUND", "content": "Analog IMC is being extensively explored for accelerating DNN inference to meet the growing computational demand. With the weight matrices stored in crossbar arrays, the data movement of MVM processing is drastically reduced, alleviating the von Neumann memory bottleneck. To process a large-scale DNN workload, large matrices and high-precision MVMs need to be partitioned into multiple crossbar arrays. To ensure robust data communication, an analog crossbar IMC macro is connected with other components through digital-to-analog-converters (DAC) at the input and ADC at the crossbar output. The resolution of DAC at each row is typically designed at 1 bit for area saving, while high-precision input activation will be converted to bit streams over multiple time steps (bit streaming). As for high-precision weights, due to the technological limitation in the number of bits per cell, a full-precision weight matrix will be partitioned into several slices of sub-arrays (bit slicing).\nThe required ADC solution N for array-level PS processing is\n\nN = log2(Nrow)+I+W-2,\n\nwhere Nrow is the number of activated rows; I is input bits per stream; and W is bits per slice. Since the energy, area, and latency overhead of ADC increase significantly with bit precision [14], ADC becomes the bottleneck of efficiency and performance in IMC design."}, {"title": "Stochastic spintronics for machine learning", "content": "Several emerging NVM technologies, including resistive memory (ReRAM) and magnetic random access memory (MRAM), have been explored for machine learning acceleration. Recent explorations have demonstrated that spintronic devices exhibit sufficient endurance to realize both synaptic weight storage and neural activation. [7] Particularly, an SOT-MTJ under an excitation current with varying magnitude will have the magnetization switching probability as a sigmoidal function of the current, which provides a direct emulation of a stochastic neuron [18] with significantly higher area/energy efficiency compared to CMOS implementations. Moreover, the separate read/write paths of SOT-MTJs offer immense design flexibility by eliminating the constraint of write current density[20]. All-spin neuro-synaptic processors based on stochastic switching of SOT-MTJ have been proposed [21]. We will explore SOT-MTJs for efficient PS processing of the crossbar output."}, {"title": "Related work on addressing the ADC bottleneck in IMC", "content": "Various works have investigated co-designing the IMC hardware architecture and DNN algorithms to reduce the ADC precision. One major thrust focuses on exploiting quantization with hardware-aware training. In [10, 12] binary neural networks exhibit improved efficiency for IMC, but the models are limited to 1-bit representation. Recent works [9, 11, 17] implemented bit slicing/streaming to map workloads with multi-bit weights/activations. Aggressively reducing the ADC precision to 1-bit will essentially enable using sense amplifiers. However, 1-bit PS showed sizeable accuracy degradation even after hardware-aware re-training [9, 11]. Moreover, it is important to note that the state-of-the-art quantization-aware training models keep the first convolution layer at full precision [13, 16, 17]. However, the compute-intensive first convolution layer in image processing can dominate the overall computation workloads. Keeping a full-precision first layer severely limits the overall improvement in energy and throughput.\nAnother major thrust is to exploit sparsity through pruning and re-training to enable lower ADC precision [8, 26]. Higher sparsity reduces the range of possible values of MVM output and thus reduces the ADC resolution requirement. As a result, sparsity-aware IMC with reconfigurable ADC resolution can improve energy and latency. However, it is important to note that the area of the peripheral ADC circuitry remains large in order to handle the highest bit precision of the reconfigurable design. Such large area overhead still hinders the hardware parallelism in IMC architecture.\nCompared to related works, our proposed StoX-Net exhibits distinctive characteristics. First, we, for the first time, represent the array-level PS by hardware-inspired stochastic bits, and incorporate the stochastic representation into training with bit slicing."}, {"title": "IMC WITH STOCHASTIC PARTIAL SUMS", "content": "The proposed Stox-Net is built on IMC crossbar arrays where current-voltage converters, column-shared MUXs, and peripheral ADCs are replaced by a row of stochastic SOT-MTJs (Fig. 2). We first present the device/circuit design of our stochastic PS processing components and then implement stochasticity-aware training of quantized DNNs with bit slicing considered [13, 16]."}, {"title": "Stochastic PS processing based on MTJ", "content": "The stochastic processing based on the current-driven probabilistic switching of SOT-MTJs is shown in Fig. 2. The device behavior is simulated using a MATLAB-based macro-spin Landau Lifshitz Gilbert (LLG) equation simulator with the SOT current included. Using an MTJ with tunnel magnetoresistance ratio (TMR) of 350-450% [20], a simple voltage divider circuit combined with a CMOS inverter can behave as a stochastic binary analog to digital converter as shown in Fig. 3a.\nThe area of the stochastic MTJ converter is obtained based on the layout in Fig. 3b. The layout is drawn following the A-based design rules [1], where A is half the feature size. Considering a 28 nm feature size, the area can be estimated as\n\n\u03bb = 32\u03bb * 26\u03bb = 0.0163 \u03bcm\u00b2."}, {"title": "Hardware-Aware Training of Quantized DNN with Stochastic Partial Sum", "content": "We map large DNN workloads into crossbars with finite array size and limited bits per cell with bit slicing from [19] and input/weight representation from [17]. Mapping a convolution layer of kernel size Kh * Kw and Cin input channels using a crossbar array of Rarr rows results in a number of subarrays defined by Narrs = ceil(Kh*Kw*Cin). For bit slicing, we consider 1-, 2-, and 4-bit per memory cell in the StoX-Net design. Based on Algorithm 1, a software-level MVM operation with the number of weight and input slices being Ws and As, will have As * Ws array-level PSs to be shift-and-added (S&A). In addition to workload partitioning, StoX-Net also considers SOT-MTJ samplings in its optimization."}, {"title": "Algorithm 1 Crossbar MVM with Stochastic Partial Sum", "content": "Input:\n\u0391\u03b9 \u2208 [\u22121, 1], \u0391\u03b3, \u0391\u03c2 \u2208 [1, Ab], W\u2081 \u2208 R, W\u2081, Ws \u2208 [1, Wb], Rarr =\nmax subarray column length\nOutput: hardware-aware Ot\nNarrs - ceil(Kh*Kw*Cin) \n\n\u25b8 Determine # of PS subarrays\n\nAq\u2190 Q(A\u2081, Ab) \n\n\u25b8 Quantize W\u2081, Al\nWq\u2190Q(bn(W\u2081), Wb) \n\n\u25b8 bn(W\u2081) column-wise\nWd - Wdims (Cout * Ws, Cin, Kh, Kw) \n\n\u25b8 Partition to subarrays\nA\nSplit(Aq, Narrs, As)\nWa\nSplit(Wq, Narrs, Qs)\nfor i \u2208 [0, Narrs) do\nfor j \u2208 [0, Nsamples) do\n\nOi,j\u2190 MTJ(MVM(WA)) \n\n\u25b8 multisampling\n\u25ba Perform Conv MVMs\nOS&A \u2190 S&A(Narrs * Nsamples) \n\n\u25b8 Reduce to Cout\nOi \u2190 Or+ OS&A\nend for\nend for\n\nTo process the crossbar output using the SOT-MTJ, the curve of switching probability versus current is emulated by the tanh function:\n\n MTJ(x) = { 0 tanh(ax) < rand 1 tanh(ax) \u2265 rand (1)\n\nwhere a is the sensitivity parameter. Increasing a will make the tanh curve more step-like, approaching a deterministic sense amplifier. The effective sensitivity can be altered by tuning the range of crossbar current when mapping MVM operations to hardware. As shown in Fig. 5, the stochastic network is trained to generate a broader distribution, covering more non-ternary values, which makes leveraging the stochastic switching possible. The deterministic 1-bit sense amplifier (SA) model trains to generate a narrower Gaussian distribution around 0 and a concentration at -1 and 1. During the MTJ conversion, the accumulated analog current supplied to the heavy metal layer of SOT-MTJ will be converted to bi-stable states, which are interpreted as (-1, 1) for better representation in training [27].\nTo mitigate the errors due to stochastic bit generation, we leverage multisampling to recover accuracy with some trade-off of hardware efficiency. More SOT-MTJ samples will lead to a better representation of the analog output. We explored no more than 8 samples per conversion since the total conversion energy and latency increase linearly with the number of samples."}, {"title": "EXPERIMENTAL RESULTS: FUNCTIONAL ACCURACY AND HARDWARE EFFICIENCY", "content": "We evaluate both the functional accuracy and hardware efficiency of the proposed StoX-Net designs, including both a quantized first layer (QF) and a high-precision first layer (HPF), while other subsequent layers adopt the MTJ-based stochastic conversion. All QF models take 8 samples per MTJ conversion in the first layer due to its importance shown by the Monte Carlo analysis in Fig. 6. We compare StoX-Net with a standard IMC featuring a high-precision ADC for all layers (HPFA). We also include an IMC design with sparse ADC (SFA) as a strong baseline for hardware comparison, where we reduce the precision of the ADC by 1 from the full precision ADC in HPFA. We investigate the impact of hardware and software configurations, including array sizes, weight bit precision, tanh sensitivity parameter, and the number of MTJ samples across the study.\nWe denote X-bit weight, Y-bit activation, and Z-bits per slice as XwYaZbs. For example, 4-bit weight, 4-bit activation, and 2-bits per slice are 4w4a2bs. Unless otherwise specified, the baseline StoX network can be assumed, having characteristics of 4w4a4bs, a = 4, Rarr = 256, 1 sample per MTJ, and HPF. Networks will only be described by their difference from this baseline. For example, 1-QF uses the stochastic MTJ converter with 1 sample in all layers except the first, which would have an MTJ converter with 8 samples. 4-HPF uses a high-precision ADC in the first layer and 4 samples of the stochastic converter in all other layers. Layers with high-a will not see multisampling as such hardware is deterministically binary.\nFor StoX-Net's accuracy measurements, we train Resnet-20 models based on the proposed convolutional methodology in Section 3. For CIFAR-10, we train with hyperparameters similar to [16], and for MNIST we lower epochs from 400 to 25.\nFor evaluating hardware efficiency, we simulate a simple IMC architecture similar to that of ISAAC [19], as shown in Fig. 8. To account for both positive and negative input activations and weights, we follow the representation described in [17]. In this representation, the positive and negative components of the activations are streamed separately. Two bits corresponding to the positive and negative components of each weight bit are stored in a group of two memristor cells. As a result, the current accumulated in each cross-bar column, which can be either positive or negative, represents the MVM operation. Hence, the energy and area values for the DAC and crossbar (Xbar) cells are doubled during the hardware evaluation. We create our architecture models using Accelergy/Timeloop [15, 24, 25]. We use ReRAM devices with (RLRS) of 500kOhm, and"}, {"title": "Functional Accuracy", "content": "In Fig. 9E, \"high-a, high-a QF\" implements step-like sense amplifier to quantize all layers including the first conv layer. Comparatively, by enabling a stochastic first layer, \"high-a, QF\" experiences immediate (over 3.4%) accuracy improvement, with 8 samples for the first StoX conv (see the bright-green and grey bars). Such observation demonstrates that our stochastic MTJ enables an effective quantization of the first conv layer through its increased representational capability.\nWe also observe that while larger a (step-like) might be preferred for 1-time sampling (Fig. 9D), higher accuracy can be obtained through multisampling (Table 4), such as 4 or 8 samples, of a smaller a (stochastic). The configuration of layer-wise mixed samplings (\"Mix-QF\") only increases the number of MVM conversions by 14.3% compared to the 1-sample model but reaches 85.5% accuracy near the 4-sample network. As discussed in the following section, although sampling multiple times monotonically increases the energy and latency, our 4-QF configuration that uses 4 samples per MTJ conversion still achieves ~100x improvement in EDP."}, {"title": "Hardware Efficiency", "content": "It is important to note that our area-efficient and highly parallel PS processing directly impacts the datapath design pattern of crossbar MVM operation. Fig. 10 shows the pipelined operations inside a crossbar using an ADC (top) and a stochastic MTJ converter (bottom). In standard IMC, the length of each pipeline stage is determined by the longest stage, i.e. the ADC readout of all shared columns in a crossbar. Our optimization alleviates the throughput bottleneck by parallelizing all columns with compact MTJ converters, thereby considerably reducing the length of the pipeline stage (from 128 ns to 1.85 ns in our case). However, increasing the number of samples reduces such throughput benefits and creates a trade-off between latency and accuracy.\nOur hardware performance evaluation is summarized in Fig. 11. Full precision ADC (HPFA) and Sparse design (SFA) are our"}, {"title": "CONCLUSION", "content": "We develop StoX-Net, an efficient IMC design that eliminates the ADC bottleneck by processing array-level partial sum using probabilistic switching of MTJ. The proposed IMC co-design framework reaches close to the state-of-the-art accuracy at CIFAR-10 classifications while achieving over 140x area efficiency and 100-600x improvement in hardware efficiency characterized by EDP. Moreover, leveraging the multisampling capability of stochastic MTJ converters, quantizing the first convolution layer, which is costly in prior quantization-aware training, was demonstrated to have less than 2% accuracy loss. Our work suggests exciting opportunities for utilizing stochastic computation and spintronics for developing next-generation Al hardware accelerators."}]}