{"title": "StoX-Net: Stochastic Processing of Partial Sums for Efficient In-Memory Computing DNN Accelerators", "authors": ["Ethan G Rogers", "Sohan Salahuddin Mugdho", "Kshemal Kshemendra Gupte", "Cheng Wang"], "abstract": "Crossbar-based in-memory computing (IMC) has emerged as a\npromising platform for hardware acceleration of deep neural net-\nworks (DNNs). However, the energy and latency of IMC systems\nare dominated by the large overhead of the peripheral analog-to-\ndigital converters (ADCs). To address such ADC bottleneck, here we\npropose to implement stochastic processing of array-level partial\nsums (PS) for efficient IMC. Leveraging the probabilistic switching\nof spin-orbit torque magnetic tunnel junctions, the proposed PS\nprocessing eliminates the costly ADC, achieving significant im-\nprovement in energy and area efficiency. To mitigate accuracy loss,\nwe develop PS-quantization-aware training that enables backward\npropagation across stochastic PS. Furthermore, a novel scheme with\nan inhomogeneous sampling length of the stochastic conversion is\nproposed. When running ResNet20 on the CIFAR-10 dataset, our\narchitecture-to-algorithm co-design demonstrates up to 22x, 30x,\nand 142x improvement in energy, latency, and area, respectively,\ncompared to IMC with standard ADC. Our optimized design config-\nuration using stochastic PS achieved 666x (111x) improvement in\nEnergy-Delay-Product compared to IMC with full precision ADC\n(sparse low-bit ADC), while maintaining near-software accuracy at\nvarious benchmark classification tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Crossbar-based analog in-memory computing (IMC) has demon-\nstrated great potential for Deep Neural Network (DNN) acceleration\nby achieving high efficiency and massive parallelism at processing\nmatrix-vector-multiplications (MVMs)[19], which dominates most\nstate-of-the-art DNN workloads [4]. However, the hardware cost of\nlarge-scale IMC systems is dominated by the significant overhead of\nperipheral analog-to-digital converters (ADCs), which is required\nto ensure robust data communication in a large-scale system. ADC\nconsumes over 60-80% of the energy and chip area of IMC hard-\nware [3, 19]. Due to the large area overhead of high-precision A-D\nconversion, an ADC must be shared by multiple columns in a cross-\nbar. Such design leads to a severe throughput bottleneck since the\nprocessing of the array-level outputs across multiple columns has\nto be done sequentially.\nWhile various recent works have aimed to mitigate the ADC\nbottleneck [6, 17, 26], minimizing ADC overhead in IMC while still\nmaintaining satisfactory inference accuracy remains a significant\nchallenge. Various factors contribute to the difficulty in addressing\nthe ADC bottleneck. (1) First, since the array-level partial sums (PS)\nare not represented at the application level, the standard quantiza-\ntion approach focusing on activation and weights can not directly\naddress the ADC precision for PS. (2) Second, while quantization-\naware training is effective at enabling hardware-friendly models,\ntraining with PS quantization desires backward propagation with\ncareful incorporation of array-level variables, which requires re-\ndesigning the computational graph of the DNN model. (3) Third,\nthe required ADC bit precision for MVM may vary significantly\ndepending on both the algorithmic attributes (such as sparsity and\nDNN layer dimension) and hardware attributes (such as array size\nand bits per memory cell). Accommodating these varying scenar-\nios requires reconfigurability and flexibility in ADC, leading to\nincreased overhead and design complexity.\nIn this work, we propose stochastic processing of the array-\nlevel partial sum for efficient IMC leveraging a spin-orbit-torque\nmagnetic tunnel junction (SOT-MTJ) with simple circuitry such as\nan inverter. To address the accuracy drop due to loss of informa-\ntion at the stochastic PS, a PS-aware training methodology with\nthe stochastic switching behavior of SOT-MTJs is developed. The\nlow-overhead crossbar peripheral based on the proposed spintronic\ndevices/circuits demonstrates significant improvement in energy\nefficiency and considerable area savings. Our proposed IMC design\neliminates the ADC bottleneck and opens up an exciting direction"}, {"title": "2 BACKGROUND", "content": "2.1 DNN acceleration with in-memory\ncomputing\nAnalog IMC is being extensively explored for accelerating DNN\ninference to meet the growing computational demand. With the\nweight matrices stored in crossbar arrays, the data movement of\nMVM processing is drastically reduced, alleviating the von Neu-\nmann memory bottleneck. To process a large-scale DNN workload,\nlarge matrices and high-precision MVMs need to be partitioned into\nmultiple crossbar arrays. To ensure robust data communication, an\nanalog crossbar IMC macro is connected with other components\nthrough digital-to-analog-converters (DAC) at the input and ADC\nat the crossbar output. The resolution of DAC at each row is typi-\ncally designed at 1 bit for area saving, while high-precision input\nactivation will be converted to bit streams over multiple time steps\n(bit streaming). As for high-precision weights, due to the techno-\nlogical limitation in the number of bits per cell, a full-precision"}, {"title": "3 IMC WITH STOCHASTIC PARTIAL SUMS", "content": "The proposed Stox-Net is built on IMC crossbar arrays where\ncurrent-voltage converters, column-shared MUXs, and peripheral\nADCs are replaced by a row of stochastic SOT-MTJs (Fig. 2). We first\npresent the device/circuit design of our stochastic PS processing\ncomponents and then implement stochasticity-aware training of\nquantized DNNs with bit slicing considered [13, 16]."}, {"title": "3.1 Stochastic PS processing based on MTJ", "content": "The stochastic processing based on the current-driven probabilis-\ntic switching of SOT-MTJs is shown in Fig. 2. The device behavior\nis simulated using a MATLAB-based macro-spin Landau Lifshitz\nGilbert (LLG) equation simulator with the SOT current included. Us-\ning an MTJ with tunnel magnetoresistance ratio (TMR) of 350-450%\n[20], a simple voltage divider circuit combined with a CMOS in-\nverter can behave as a stochastic binary analog to digital converter\nas shown in Fig. 3a.\nThe area of the stochastic MTJ converter is obtained based on\nthe layout in Fig. 3b. The layout is drawn following the A-based\ndesign rules [1], where A is half the feature size. Considering a 28\nnm feature size, the area can be estimated as A = 322 * 26\u03bb = 0.0163\n\u03bcm\u00b2."}, {"title": "3.2 Hardware-Aware Training of Quantized\nDNN with Stochastic Partial Sum", "content": "We map large DNN workloads into crossbars with finite array\nsize and limited bits per cell with bit slicing from [19] and in-\nput/weight representation from [17]. Mapping a convolution layer\nof kernel size Kh * Kw and Cin input channels using a crossbar\narray of Rarr rows results in a number of subarrays defined by\nNarrs = ceil(Kh*Kw*Cin). For bit slicing, we consider 1-, 2-, and\n4-bit per memory cell in the StoX-Net design. Based on Algorithm\n1, a software-level MVM operation with the number of weight and"}, {"title": "4 EXPERIMENTAL RESULTS: FUNCTIONAL\nACCURACY AND HARDWARE EFFICIENCY", "content": "4.1 Evaluation Methodology\nWe evaluate both the functional accuracy and hardware efficiency\nof the proposed StoX-Net designs, including both a quantized first\nlayer (QF) and a high-precision first layer (HPF), while other sub-\nsequent layers adopt the MTJ-based stochastic conversion. All QF\nmodels take 8 samples per MTJ conversion in the first layer due\nto its importance shown by the Monte Carlo analysis in Fig. 6. We\ncompare StoX-Net with a standard IMC featuring a high-precision\nADC for all layers (HPFA). We also include an IMC design with\nsparse ADC (SFA) as a strong baseline for hardware comparison,\nwhere we reduce the precision of the ADC by 1 from the full pre-\ncision ADC in HPFA. We investigate the impact of hardware and\nsoftware configurations, including array sizes, weight bit precision,\ntanh sensitivity parameter, and the number of MTJ samples across\nthe study.\nWe denote X-bit weight, Y-bit activation, and Z-bits per slice as\nXwYaZbs. For example, 4-bit weight, 4-bit activation, and 2-bits\nper slice are 4w4a2bs. Unless otherwise specified, the baseline StoX\nnetwork can be assumed, having characteristics of 4w4a4bs, a = 4,\nRarr = 256, 1 sample per MTJ, and HPF. Networks will only be\ndescribed by their difference from this baseline. For example, 1-QF\nuses the stochastic MTJ converter with 1 sample in all layers except\nthe first, which would have an MTJ converter with 8 samples. 4-HPF\nuses a high-precision ADC in the first layer and 4 samples of the\nstochastic converter in all other layers. Layers with high-a will not\nsee multisampling as such hardware is deterministically binary.\nFor StoX-Net's accuracy measurements, we train Resnet-20 mod-\nels based on the proposed convolutional methodology in Section 3.\nFor CIFAR-10, we train with hyperparameters similar to [16], and\nfor MNIST we lower epochs from 400 to 25.\nFor evaluating hardware efficiency, we simulate a simple IMC\narchitecture similar to that of ISAAC [19], as shown in Fig. 8. To ac-\ncount for both positive and negative input activations and weights,\nwe follow the representation described in [17]. In this representa-\ntion, the positive and negative components of the activations are\nstreamed separately. Two bits corresponding to the positive and\nnegative components of each weight bit are stored in a group of two\nmemristor cells. As a result, the current accumulated in each cross-\nbar column, which can be either positive or negative, represents the\nMVM operation. Hence, the energy and area values for the DAC\nand crossbar (Xbar) cells are doubled during the hardware evalua-\ntion. We create our architecture models using Accelergy/Timeloop\n[15, 24, 25]. We use ReRAM devices with (RLRS) of 500kOhm, and"}, {"title": "5 CONCLUSION", "content": "We develop StoX-Net, an efficient IMC design that eliminates the\nADC bottleneck by processing array-level partial sum using proba-\nbilistic switching of MTJ. The proposed IMC co-design framework\nreaches close to the state-of-the-art accuracy at CIFAR-10 classi-\nfications while achieving over 140x area efficiency and 100-600x\nimprovement in hardware efficiency characterized by EDP. More-\nover, leveraging the multisampling capability of stochastic MTJ\nconverters, quantizing the first convolution layer, which is costly in\nprior quantization-aware training, was demonstrated to have less\nthan 2% accuracy loss. Our work suggests exciting opportunities\nfor utilizing stochastic computation and spintronics for developing\nnext-generation Al hardware accelerators."}]}