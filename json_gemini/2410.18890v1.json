{"title": "Improving Small-Scale Large Language Models Function Calling for Reasoning Tasks", "authors": ["Graziano A. Manduzio", "Federico A. Galatolo", "Mario G. C. A. Cimino", "Enzo Pasquale Scilingo", "Lorenzo Cominelli"], "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. While these models excel in general complex reasoning tasks, they still face challenges in mathematical problem-solving and logical reasoning. To address these limitations, researchers have explored function calling abilities, allowing LLMs to execute provided functions and utilize their outputs for task completion. However, concentrating on specific tasks can be very inefficient for large-scale LLMs to be used, because of the expensive cost of training and inference stages they need in terms of computational resources. This study introduces a novel framework for training smaller language models in function calling, focusing on specific logical and mathematical reasoning tasks. The approach aims to improve performances of small-scale models for these tasks using function calling, ensuring a high level of accuracy. Our framework employs an agent that, given a problem and a set of callable functions, queries the LLM by injecting a description and examples of the usable functions into the prompt and managing their calls in a step-by-step reasoning chain. This process is used to create a dataset of correct and incorrect reasoning chain chat completions from a large-scale LLM. This dataset is used to train a smaller LLM using Reinforcement Learning from Human Feedback (RLHF), specifically employing the Direct Preference Optimization (DPO) technique. Experimental results demonstrate how the proposed approach balances the trade-off between model size and performance, improving the ability of function calling for reasoning tasks, in smaller models.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent years have seen the rapid development of Large Language Models (LLMs) that have demonstrated exceptional natural language understanding and generation capabilities. Research has explored the unexpected abilities of LLMs beyond their primary training task of text prediction [1]. These models have shown promise in function calling for software APIs [2]-[6], boosted by the launch of GPT-4 plugin features [7]. Integrated tools include web broswer, translation system, Dialogue State Tracking (DST) [8] and robotics [9], [10]. Furthermore, while LLMs have shown promising results in general complex reasoning benchmarks, they still face challenges in mathematical problem-solving and logical capacities [11]. To address these limitations, researchers have proposed various techniques [12], [13], including the ability of function calling [14], that allows LLMs to execute provided functions and utilize their outputs to assist in task completion. These functions can vary from basic tools like a calculator [15] that performs arithmetic operations to more advanced methods. However, concentrating on specific tasks that use only a small portion of available APIs underscores the inefficiency of depending solely on large models like GPT-4, which require significant computational resources for both the training and inference stages [7], [16]\u2013[18]. This situation calls for the creation of smaller, task-specific LLMs that maintain core functionality while reducing operational costs [2], [19]. The trend towards smaller models, while promising, introduces new challenges. One significant concern is the increased likelihood of errors or \"hallucinations,\u201d which can compromise the accuracy of output formatting [20]-[22]. Given that precise output formatting is essential for developing robust software applications, this issue becomes particularly critical. To address the drawbacks of oversized LLMs, which incur excessive training and inference costs, we introduce a novel framework to train smaller language models starting from the function calling abilitities of large models, for specific logical and mathematical reasoning tasks. This framework involves the use of an agent that, given a problem and a set of possible functions useful for its solution, queries a large-scale LLM by injecting function descriptions and examples into the prompt and managing the proper function calls that the model needs to find the solution, all that in a step-by-step reasoning chain. This procedure is so used for the creation of a dataset with correct and incorrect chat completions. The generated dataset is then used to train a smaller model using a Reinforcement Learning from Human Feedback (RLHF) [23]-[26] approach, known as Direct Preference Optimization (DPO) [27]. We present the methodology tested on two different types of reasoning tasks, First-Order Logic (FOL) and math. To achieve this goal a set of FOL problems were built ad hoc, taking inspiration from the HuggingFace dataset SAGI-1/SYMBOLIC_DATA_ PLUS_REASONING_DATA_V1 [28]. Examples of mathematical problems were drew directly from the GSM8K [15], [29] dataset. In Section II FOL and the DPO are presented. In Section III, the pipeline and methodologies to generate the dataset and train the small-scale model are shown. Finally, in Section IV we present experimental results of our framework, where performance of the trained model is compared to the original one."}, {"title": "II. RELATED WORKS", "content": ""}, {"title": "A. First-Order Logic (FOL)", "content": "First-Order Logic (FOL), also known as First-Order Predicate Calculus or Predicate Logic, is a formal system that extends propositional logic to include variables, quantifiers, and predicates. This extension allows for greater expressive power in formalizing mathematical statements and reasoning [30], [31]. The syntax of FOL comprises both logical and non-logical symbols. Logical symbols include connectives (such as negation, conjunction, disjunction, implication, and biconditional), quantifiers (universal and existential), and parentheses. Non-logical symbols consist of constants, variables, function symbols, and predicate symbols. These components work together to create a rich language capable of expressing complex logical relationships and structures. The semantics of FOL provide a framework for interpreting formulas and determining their truth values [32]. A key feature of FOL is the use of quantifiers, which allow for statements about all or some elements in the domain [33]. The universal quantifier expresses that a property holds for all elements in the domain, while the existential quantifier expresses that a property holds for at least one element in the domain. These quantifiers significantly enhance the expressive power of FOL compared to propositional logic. FOL finds numerous applications across various fields [13], [34]. In mathematics, it is used for formalizing theories and proofs. In computer science, FOL is applied in the specification and verification of software and hardware systems. The field of artificial intelligence utilizes FOL for knowledge representation and automated reasoning. Additionally, linguistics employs FOL in the study of formal semantics of natural languages."}, {"title": "B. FOL and AI", "content": "The field of artificial intelligence (AI) extensively utilizes First-Order Logic (FOL) for knowledge representation and automated reasoning. FOL's expressive power and formal semantics make it an ideal choice for capturing complex knowledge structures and facilitating inference in AI systems [35]. In knowledge representation, FOL allows for the formalization of domain-specific knowledge, enabling AI systems to reason about objects, their properties, and relationships in a structured manner [36]. This capability is crucial in expert systems, where domain knowledge is encoded as logical rules and facts, allowing the system to make informed decisions based on logical inference [37]. In the realm of automated reasoning, FOL serves as the foundation for many theorem-proving systems and logical inference engines [38]. These systems employ techniques such as resolution and unification to derive new knowledge from existing facts and rules, a process fundamental to various AI applications, including planning and decision-making [39]. Moreover, FOL has been instrumental in the development of answer set programming, a paradigm for declarative problem solving that has found applications in areas such as constraint satisfaction and automated planning [40]. The integration of FOL with probabilistic methods has led to the development of statistical relational learning and probabilistic logic programming, bridging the gap between logical and statistical AI approaches [41]. This fusion enables Al systems to reason with uncertainty while maintaining the structured representation offered by FOL. Additionally, FOL has played a significant role in the semantic web and ontology engineering, where it is used to define and reason about conceptual models of various domains [42]."}, {"title": "C. Examples of First-Order Logic Statements", "content": "To better understand the components and structure of First-Order Logic (FOL), let's examine a specific example statement and break down its elements. Consider the following FOL statement:\n$\\forall xy(P(x)\\rightarrow(Q(x, y) \\wedge R(y)))$\nThis statement can be read in natural language as: \"For every x, there exists a y such that if P(x) is true, then both Q(x,y) and R(y) are true.\"\nThe structure of this statement demonstrates several key features of FOL:\n1) Quantification: The use of both universal (\u2714) and existential (3) quantifiers allows us to make statements about all elements or the existence of elements in our domain.\n2) Variables: x and y are used to represent arbitrary elements in the domain, allowing for general statements about the relationships between elements.\n3) Predicates: P, Q, and R represent properties or relations. P and R are unary predicates (apply to one variable), while Q is a binary predicate (applies to two variables), showing FOL's ability to express different types of relations.\n4) Logical Structure: The statement uses implication (\u2192) and conjunction (^) to create a complex logical structure, demonstrating FOL's ability to express intricate logical relationships.\nConstants in First-Order Logic: In addition to variables, FOL also includes constants, which represent specific, named individuals in the domain of discourse. Constants allow us to make statements about particular entities rather than arbitrary ones. To illustrate the difference between variables and constants, consider the following two FOL statements:\n$\\exists x(Movie(x)\\wedge ActedIn(y, x))$\n$\\exists x(Movie(x) \\wedge ActedIn(seanconnery, x))$\nThe first statement uses two variables, x and y. It can be read as: \"There exists a movie x such that y acted in x.\" This statement asserts the existence of a movie and an actor, but doesn't specify who the actor is. The second statement uses a variable x and a constant seanconnery. It can be read as: \"There exists a movie x such that Sean Connery acted in x.\" This statement is more specific, asserting the existence of a movie in which the particular individual Sean Connery acted. The use of the constant seanconnery allows us to make a claim about a specific person, whereas the variable y in the first statement could refer to any actor. The latter and other examples of FOL statements can be found in the HuggingFace dataset SAGI-1/SYMBOLIC_DATA_ PLUS_REASONING_DATA_V1 [28]."}, {"title": "D. Direct Preference Optimization", "content": "While supervised fine-tuning is a common approach, alternative methods leveraging Reinforcement Learning from Human Feedback (RLHF) have gained prominence. One such method is Proximal Policy Optimization (PPO) [43], which integrates a reward model into the reinforcement learning framework for policy optimization. Despite its effectiveness, PPO's requirement for extensive human feedback to train the reward model makes it resource-intensive and time-consuming. A more efficient and equally effective alternative is Direct Preference Optimization (DPO) [27]. DPO distinguishes itself by enabling the model to learn a policy directly from user preference data, eliminating the need for an explicit reward function. Furthermore, DPO has demonstrated superior stability compared to PPO. The DPO process begins with gathering human feedback. Assessors evaluate pairs of model-generated responses to identical prompts, creating a dataset of preference pairs. Unlike PPO, which trains a separate reward model, DPO incorporates these preferences directly into the training objective. The pipeline of the presented framework is shown in Fig. 2.\nThe model parameters to maximize the likelihood of outputs aligning with reviewer preferences. This is achieved through a specialized loss function (Equation 1) that directly penalizes the generation of less-preferred outputs. For a training instance (x, yw, Y\u0131), comprising an input x, a preferred output Yw, and a non-preferred output y\u0131, the loss function compares the probabilities of the reference policy $\\pi_{ref}$ (the initial policy) with those of the new policy $\\pi_{\\theta}$ for both preferred and non-preferred outputs.\n$L_{DPO}(\\pi_{\\theta}; \\pi_{ref}) =$\n$= -E_{(x,y_w,y_l)\\sim D} log \\frac{\\sigma(\\beta log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)})}{\\sigma(\\beta log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)}) + \\sigma(\\beta log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)})}  $\n$= -E_{(x,y_w,y_l)\\sim D} logo \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)}$   (1)\nThis formulation enables DPO to efficiently learn from human preferences, offering a streamlined alternative to traditional RLHF methods."}, {"title": "III. METHODOLOGY", "content": ""}, {"title": "A. Proposed framework", "content": "The pipeline of the presented methodology constitutes of 4 different stages:\n*   definition of tasks and problems to be addressed. Calling function abilities of LLMs can be tested on a various corpus of different reasoning tasks, ranging from logical and mathematical to visual and multimodal ones;\n*   once tasks and problems are defined, in turn a set of functions needs to be defined for each problem. These functions serve for the LLM to solve the reasoning steps, control the chain flow and verify the intermediate and final responses, working similar to the process-supervised reward models (PRMs) or to the Outcome-supervised Reward Models (ORMs) presented in [44];\n*   choise of a pre-trained large-scale LLM to generate the dataset of right and wrong completions using a chain-of-thoughts prompting that forces the LLM to reason step-by-step. An agent queries the LLM, stores the response and calls the function until the solution to the problem is obtained. Final solutions can be right or wrong. The chains of thought for each problems are recorded;\n*   fine-tuning a small-scale LLM using reinforcement learning on the given dataset. The DPO algorithm is performed."}, {"title": "B. Experimental setup", "content": "To test our framework, the following choices were made:\n*   we defined 6 FOL problems, 3 with a single predicate and 3 with double predicates; we then drew 9 mathematical problems from the GSM8K dataset for a total of 15 problems overall;\n*   we defined a set of callable function for each problem. For example for the mathematical problems we defined the fundamental operations. Furthermore, we defined the verifier CheckCorrectChain() and the chain flow controller Stop();\n*   we created a specific dataset for the DPO algorithm, using Llama3-70B [45] in inference on reasoning tasks, through the Pytorch library microchain [46];\n*   with the new data, we fine-tuned with DPO a smaller model, Mistral-7B-Instruct-v0.2, using a single GPU.\nLet's see specifically how the dataset was created and how the smaller model was fine-tuned."}, {"title": "C. Microchain", "content": "The presented framework generates the dataset using microchain, an agent-based library that given a set of callable functions usable to solve a given problem and coded in a specific python file, enables the agent to query a LLM with a prompt, where these functions are introduced each one with a declaration, a description and a use case example. An instance of prompt x\u00b2 sampled from the generated dataset D* and related to a FOL problem is shown in Tab. III. We see how the chain-of-thought reasoning is explicitly induced by inviting the LLM, that works as a scheduler, to reason step-by-step. After recording the response of the LLM as associated to the assistant role, the agent calls the function the LLM needs to solve the reasoning step and proceed to the next one, recording the output as associated to the user role and so on, until the final solution is obtained. The CheckCorrectChain() function works as a verifier that checks whether or not the intermediate and the final responses are called in the right order. The same work flow is implemented for a GSM8K dataset problem, but the verifier checks only the correctness of the final solution. Hence, the agent at each iteration records the entire chain of LLM reponses/function outputs as a set of assistant/user role chat contents, presented in the format commonly used for the chat completions of the LLMs [47], [48]. Therefore the LLM bases its next response on the previous ones recorded in the chain."}, {"title": "D. Dataset Creation Process", "content": "Our dataset $D^* = \\{P,D_w, D_i\\}$ was generated using Llama3-70B through the microchain, producing, given a set of prompts (denoted by P), a range of correct and incorrect chain completions (whose sets are denoted respectively by Dw and Di). The generation process encompassed various scenarios, differentiated by the type of the previous discussed reasoning tasks (GSM8K or FOL) and the maximum number of allowed n iterations in the chain of thought ($n_{max} = 10$ or 20), where each iteration is a couple of assistant/user role chat contents. For each problem in the dataset, we recorded a prompt, a set of correct completions (right completions), and a set of incorrect completions (wrong completions). where indices $i_t \\in B$, $i_n \\in B$, $i_p \\in I_p \\equiv \\{0,1,2,...,8\\}$, and $i \\in I \\subseteq \\{B \\times B \\times I_p\\}$, $B \\simeq \\{0,1\\}$, identify the type of task (mathematical or logic), the maximum chain iteration number (10 or 20), the problem within the specific task, ranging from 0 to 5 for the FOL problem set and 0 to 9 for the GSM8K problem set, and the global index, equals to $(i_t, i_n, i_p)$, respectively. The function CheckCorrectChain() defined in the prompt, is called after that the other functions are correctly called in the right order and the Stop() function stops the program in $n \\le n_{max} = 10$ iterations. Finally,"}, {"title": "E. Data augmentation of the DPO dataset", "content": "Given the original dataset D* of all tuples $(x^i, y^i_j, y^{i,k})$, $\\forall i \\in I, \\forall j \\in J^i,\\forall k \\in K^i$, we built an augmented dataset Da where for a given prompt $x^i$ a combination of all the correct completions $y^i_j$ with all the wrong completions $y^{i,k}$ was made, extending the cardinality of the dataset from $|D^*| = |P| = |D_w| + |D_i|$ to $|D^a| = |D_w| \\times |D_i|$. The final DPO dataset D in (1) is a subset of uniform random samples from Da, such that $D \\subset D^a$, where the cardinality of D is |D| = ns, i.e. the number of samples belonging D. In particular we drew a total count of ns = 40000 samples. This approach allows an augmentation of the original dataset. A sample of the final dataset can be written as\n$(x = x^{i \\sim P}, Y_\\omega = Y_j^{i \\sim D_w}, y_l = y^{i,k} \\sim D_i )$.\nWe split the dataset in training and test set. Specifically, data related to 4 FOL problems and 5 GSM8K problems were used for the training, instead the remaining data (related to the remaining 6 problems) was used for the model inference test."}, {"title": "F. Inference and Training setup", "content": "Models were deployed using the vllm library [49]. We trained the model Mistral-7B-Instruct-v0.2 on a single GPU NVIDIA A100, using the python library Transformer Reinforcement Learning trl [50] which includes the DPOTrainer class, combined with the use of other three libraries specific for the training optimization process, namely accelerate [51], deepspeed [52] and peft [53]."}, {"title": "G. Performance metrics", "content": "After the training of the small-scale LLM, we tested performance in inference of both the original and trained small-scale LLM, producing 2 new different datasets $D_o$ and $D_f$, respectively, with 1000 generated samples for each task problem. Metric used to compare model performances is the accuracy, defined for the i-th problem prompt as $a^i \\simeq |J^i|/(|J^i| + |K^i|) = n^2/n^2$. The task average accuracy is defined as $\\bar{a}_{i_t} = \\sum_{i \\in I_t} a^i/|I_t|$ where $I_t \\subset I$ is the problem index subset related to the t-th task. The average accuracy computed on subsets of $I_t$ related to different $n_{max}$ values and types of data subsets (training and test type of data subsets), for the sake of brevity is still denoted with $\\bar{a}_{i_t}$. Finally, the overall average accuracy is defined as $\\bar{a} \\equiv \\sum_{i \\in I}a^i/|I|$."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "The percentage accuracy $a^i \\times 100$ for the problem i-th and the dataset Do, is compared to $a_f \\times 100$ for the dataset D, as shown in Fig. 6 and 7. We can see how FOL task performance is completely improved, except for the problem 3 with $N_{max} =$ 20. Model performance is also improved on the GSM8K data subset, where on the whole is better than the original model, apart from some cases. Note that, as expected, performance with $N_{max} = 20$ is better than one with $N_{max} = 10$, for both the original and the fine-tuned models. Overall performance is so improved, but the fine-tuned model struggles more with the GSM8K problems. The same trend is observed in Tab. X, where task average percentage accuracy $\\bar{a}_{i_t} \\times 100$ and overall average percentage accuracy $\\bar{a} \\times 100$ evaluations are reported for different models, types of data subsets and $n_{max}$ values. Note that also performances on the training and test set, taken individually, are improved after the fine-tuning process for all the configurations of $n_{max}$ values and tasks. The trend is also confirmed in Fig. 8, where a comparison of the original and the fine-tuned model task average percentage accuracies $\\bar{a}_{i_t} \\times 100$ with the overall percentage accuracy $\\bar{a}_{i_t} \\times 100$ is presented. Furthermore, results are statistically evaluated with a Wilcoxon signed-rank test where p-values are computed comparing the original and the fine-tuned model accuracies, for each task data subset and the whole dataset. Values are all lower than 0.05, confirming that the difference is statistically significant Finally, loss metric in (1) computed at each global step k during the training process is shown in Fig. 9, where the evaluation on the test set is executed at the end of each epoch. A global step represents a single update of the model parameters. It is incremented every time the optimizer performs a backpropagation operation and updates the model weights. Each epoch implies that the model has processed all the preference pairs (right and wrong completions) present in the dataset."}, {"title": "V. CONCLUSIONS AND FUTURE WORKS", "content": "In this study, we introduced a novel framework for improving the function calling abilities of small-scale LLMs, focusing on specific logical and mathematical reasoning tasks. Our approach addresses the inefficiencies and high computational costs associated with relying solely on large-scale LLMs by leveraging the capabilities of small-scale models through RLHF. We employed an agent-based system that interacts with a large-scale LLM to generate a dataset comprising correct and incorrect step-by-step reasoning chain chat completions in the domains of First-Order Logic (FOL) and mathematical reasoning tasks drawn from the GSM8K dataset. Utilizing this dataset, we trained a smaller model, Mistral-7B-Instruct-v0.2, employing RLHF with the DPO technique. Our experimental results demonstrate significant improvements in the performance of the small-scale model on FOL tasks, achieving near-perfect accuracy in most cases. While the improvements on the GSM8K mathematical problems are more modest, the trained model still outperforms the original model in overall accuracy. These findings suggest that our framework effectively improves the function calling abilities of smaller models enhancing their capabilities in the using of external tools (the callable functions) and the abilities in the given reasoning tasks. By successfully improving the integration of small-scale LLMs with external function calls, our approach contributes to making advanced reasoning tasks more accessible and practical in resource-constrained environments, since the training process was carried out by using just a single GPU. This has significant implications for deploying AI systems on devices with limited computational resources, such as mobile devices or real-time applications. For future work, we plan to explore the application of our framework to a broader range of reasoning tasks and function types. Indeed, expanding the methodology to include more diverse datasets and functions could help in testing the generalization of the approach and increasing its applicability across various domains. Additionally, we aim to investigate further optimizations in the training process, for instance through a hyperparameter optimization process [55], [56], by varying values of the model training parameters presented in Tab. VII. One other promising area for optimization that we plan to investigate is the use of quantization to reduce the size of the model and improve inference speed [57]. Quantization reduces the precision of model parameters from floating-point (e.g., 32-bit) to lower bit-width representations (e.g., 8-bit or even binary), allowing models to operate more efficiently, especially on hardware with limited resources. Knowledge distillation is another key technique that can further enhance the performance of small-scale models by transferring knowledge from larger models [58]. In this approach, a large teacher model, which has learned complex reasoning patterns, guides a smaller student model to replicate its behavior. This process allows the smaller model to maintain high performance despite its reduced size. For future works, we plan to investigate whether knowledge distillation can be combined with the DPO algorithm in the proposed framework. Furthermore, we plan to comprehend whether the proposed approach can also enhance the intrinsic reasoning capabilities of LLMs by enabling them to solve problems without relying on external functions and tools."}]}