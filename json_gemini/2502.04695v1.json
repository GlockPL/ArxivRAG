{"title": "Bridging the Gap in XAI\u2014The Need for Reliable Metrics in Explainability and Compliance", "authors": ["Pratinav Seth", "Vinay Kumar Sankarapu"], "abstract": "This position paper emphasizes the critical gap in the evaluation of Explainable AI (XAI) due to the lack of standardized and reliable metrics, which diminishes its practical value, trustworthiness, and ability to meet regulatory requirements. Current evaluation methods are often fragmented, subjective, and biased, making them prone to manipulation and complicating the assessment of complex models. A central issue is the absence of a ground truth for explanations, complicating comparisons across various XAI approaches. To address these challenges, we advocate for widespread research into developing robust, context-sensitive evaluation metrics. These metrics should be resistant to manipulation, relevant to each use case, and based on human judgment and real-world applicability. We also recommend creating domain-specific evaluation benchmarks that align with the user and regulatory needs of sectors such as healthcare and finance. By encouraging collaboration among academia, industry, and regulators, we can create standards that balance flexibility and consistency, ensuring XAI explanations are meaningful, trustworthy, and compliant with evolving regulations.", "sections": [{"title": "1. Introduction", "content": "In today's era, the use of AI models and deep learning is no longer a distant future but a present-day reality (OpenAI; Touvron et al., 2023), with applications spanning across our everyday lives and critical sectors such as healthcare, finance, and law enforcement. These systems are increasingly influencing high-stakes decisions, and their impact is undeniable. As AI plays a central role in shaping these decisions, there is a growing need to ensure that these systems are transparent and their decision-making is explainable to ensure that the decisions are correctly thought and meaningful.\nSimilarly, AI safety research underscores that explainability helps identify and mitigate risks (Amodei et al., 2016; Kirkpatrick et al., 2017; Leike et al., 2018). Despite significant advancements in Explainable AI (XAI), a critical gap persists: the absence of standardized, reliable metrics to evaluate the effectiveness and trustworthiness of AI explanations.\nXAI aims to bridge the gap between complex machine learning models and end-users, particularly in domains where decisions have compliance, ethical, legal, or social implications (Singh et al., 2024; Kaur et al., 2022; Tull et al., 2024). However, the current landscape of XAI evaluation is fragmented, with metrics used to assess model interpretability and explanation quality remaining inconsistent and subjective (Madsen et al., 2024). The lack of standardization holds back progress in XAI, as it allows for manipulation of results when comparing other XAI methods across different tasks, reducing the reliability of outcomes (Wickstr\u00f8m et al., 2024; Hedstr\u00f6m et al., 2023b). To make XAI effective in high-risk settings, it is crucial to establish clear and reliable metrics that measure important aspects like fidelity, robustness, and usability.\nRegulatory frameworks such as the European Union AI Act (eur; Nannini et al., 2024a) emphasize transparency and accountability, especially for high-risk AI systems. Article 13 of the EU AI Act requires these systems to be transparent and interpretable(Fresz et al., 2024a). This makes it clear that there is a strong need for standardized evaluation metrics that assess the technical performance of XAI methods and meet legal and ethical standards (Sovrano et al., 2022; Panigutti et al., 2023). Without these metrics, it will be difficult to ensure that XAI systems comply with regulatory requirements, which could slow the adoption of AI in critical sectors.\nThis position paper argues that addressing the gap in reliable XAI metrics is crucial for advancing both the technical development and regulatory compliance of explainable AI. The reliance on subjective evaluation methods and their vulnerability to manipulation (Wickstr\u00f8m et al., 2024) impedes progress in this domain. Additionally, advanced models, such as large language models (LLMs), which exhibit unprecedented complexity, require new scalable evalu-"}, {"title": "2. Background and Context", "content": "Explainable AI (XAI) is an essential field of research focused on making the decision-making processes of complex machine learning (ML) models more transparent and understandable (Arrieta et al., 2019). In high-impact areas like healthcare, finance, and law enforcement, where AI-driven decisions can have profound ethical, legal, and societal consequences, the ability to explain how AI systems arrive at their conclusions is crucial (Gohel et al., 2021).\nThe challenge lies not just in developing effective and accurate models but in ensuring that these models are transparent, accountable, and provide comprehensible explanations, especially when their decisions impact high-stakes outcomes (Jia et al., 2022; Atakishiyev et al., 2025). In these domains, the consequences of AI decisions are significant, making the need for interpretability and explanation all the more urgent (Fresz et al., 2024b)."}, {"title": "2.1. Interpretability, Explainability, and Feature\nattribution", "content": "Interpretability, explainability, and feature attribution are closely related concepts in XAI, often used interchangeably, but they serve distinct purposes (Doshi-Velez & Kim, 2017). Interpretability refers to how easily a human can understand a model's decision-making process. Simpler models, like decision trees, are interpretable, while complex models, such as deep neural networks, are more difficult to interpret. Explainability is about providing understandable reasons for a model's predictions, often using methods like LIME (Ribeiro et al., 2016) or SHAP (Lundberg & Lee, 2017) to make complex models more comprehensible. Feature attribution identifies the contribution of each input feature to the model's prediction, showing which factors were most influential. Although these terms are sometimes used interchangeably, they each focus on different aspects of making AI decisions understandable and transparent, ultimately contributing to the broader goal of improving accountability and trust in Al systems."}, {"title": "2.2. Explainability Methods", "content": ""}, {"title": "2.2.1. INTRINSIC EXPLAINABILITY", "content": "Intrinsic explainability (Lipton, 2018; Rudin, 2019) refers to models that are inherently interpretable by design. These models are constructed to be transparent and easily understood from the outset, such as decision trees, linear regression, or rule-based systems. The key advantage of intrinsic explainability is that the model's decision-making process is directly accessible and traceable by the user without needing additional explanation tools. For example, in a decision tree, one can directly follow the path of the decision-making process based on the input features. The trade-off, however, is that these models tend to be simpler and may sacrifice predictive accuracy for interpretability.\nIn the current deep learning era, while models are designed to be intrinsically explainable, this claim and their faithfulness should still be questioned (Jacovi & Goldberg, 2020), as many inherently interpretable model ideas are later revealed not to provide faithful explanations. For example, attention-based explanations have received notable criticism for not being faithful (Jain & Wallace, 2019; Serrano &\nSmith, 2019; Vashishth et al., 2019; Meister et al., 2021;\nMadsen et al., 2022a; 2024)"}, {"title": "2.2.2. POST HOC EXPLAINABILITY", "content": "Post hoc explainability (Lipton, 2018; Madsen et al., 2022b) refers to methods that generate explanations for a model's predictions after it has been trained without altering the internal structure of the model. These techniques offer flexibility, as they can be applied to a variety of pre-existing models, making them widely used in practice.\nNotable examples of post hoc explainability methods include Shapley Additive exPlanations (SHAP) (Lundberg & Lee, 2017), which attribute the output of a model to its input features by computing the contribution of each feature to the model's prediction, offering a clear understanding of the model's decision-making. Local Interpretable Model-agnostic Explanations (LIME) (Ribeiro et al., 2016) provides explanations by approximating the model locally with interpretable surrogate models, making complex models more understandable in specific instances. Gradient-weighted Class Activation Mapping (Grad-CAM) (Selvaraju et al., 2016) visualizes the impact of various regions of input (such as parts of an image) on the model's predictions, offering visual insight into decision-relevant features. Integrated Gradients (Sundararajan et al., 2017a) offers a method to quantify the contribution of each input feature by integrating gradients of the model's output with respect to input features along a path from a baseline to the input. This method approximates a model's behavior more faithfully, particularly for deep learning models."}, {"title": "2.2.3. OTHER METHODS", "content": "In addition to intrinsic and post hoc explainability, other methods offer deeper insights into model behavior. Ante-hoc explainability (Sarkar et al., 2021) ensures transparency in the data and learning process from the start. Example-based explainability, like counterfactual explanations, compares model predictions to similar examples for clarity. Interactive explainability (Slack et al., 2023) allows users to tweak inputs and see how predictions change in real-time. Causal explainability (Carloni et al., 2023) identifies how changes in features causally affect outcomes. Fairness and bias explainability (Zhou et al., 2022) helps detect and address biases in decision-making. Model-specific explainability (Ali et al., 2023) uses tailored methods for specific models, such as visualizations for decision trees. Lastly, mechanistic interpretability (Olah et al., 2020; Elhage et al., 2021; Nanda et al., 2023) reverse-engineers a model's internal workings to understand how individual components contribute to its predictions."}, {"title": "2.3. The Role of Evaluation Metrics in XAI", "content": "Recent advancements in the quantitative analysis of XAI explanations have provided researchers with a broad set of evaluation metrics to work with (Hedstr\u00f6m et al., 2023b; Agarwal et al., 2022b). These metrics are key for evaluating the reliability and effectiveness of XAI methods (Coroama & Groza, 2022; Kadir et al., 2023). They help researchers and practitioners assess how well an explanation reflects the model's decision-making process and whether it meets key requirements like transparency, robustness, and usability. Since ground truth explanations are unavailable, researchers focus on quantifying the quality of an explanation by measuring its desired properties.\nOver time, it has become clear that most XAI metrics can be grouped into one of six categories:\n\u2022 Faithfulness: Metrics measure how well an explanation reflects the model's true decision-making process. Faithfulness ensures that the explanation accurately"}, {"title": "2.4. Existing Evaluation Frameworks and\nBenchmarking Tools for Model Explainability", "content": "Even though Explainable AI (XAI) has made strides, current evaluation frameworks still face significant issues. Many existing metrics don't capture the full complexity of real-world machine learning models. Most benchmarks aren't"}, {"title": "3. Challenges in XAI Metrics", "content": "Evaluating XAI methods presents several challenges that hinder their reliability and adoption. These issues prevent the development of effective and universally applicable evaluation frameworks necessary to meet the expectations of regulators, risk managers, and users. Major problems include fragmentation, subjectivity, and the vulnerability of metrics to manipulation. Many existing metrics rely on multiple hyperparameters, yet little effort has been made to fine-tune them, creating opportunities for manipulation and reducing the reliability of evaluations (Wickstr\u00f8m et al., 2024; Hoffman et al., 2019; Pawlicka et al., 2024).\n3.1. Neglect of Modern AI Models\nCurrent XAI evaluation metrics struggle to capture the complexity of modern AI models, particularly large language models and autoregressive systems. These models rely on intricate decision-making processes and large-scale architectures, requiring more adaptable evaluation methods. While tools like Ferret (Attanasio et al., 2023) have improved interpretability for transformers, existing frameworks still fall short in analyzing behaviors and complex reasoning.\nExplainability research has mainly focused on image and tabular modalities, with recent efforts extending to NLP. However, multi-modal AI systems are becoming increasingly common, yet most XAI methods remain singlemodality focused, limiting their applicability to models pro-"}, {"title": "3.2. Fragmentation and Inconsistency", "content": "A key challenge is the fragmentation in XAI evaluation due to the lack of standardized frameworks. Different metrics are used across studies, making comparing results and drawing broad conclusions hard. This inconsistency slows"}, {"title": "3.3. Manipulation Vulnerabilities", "content": "XAI metrics are vulnerable to intentional or unintentional manipulation, which undermines trust in XAI systems and diminishes their practical value (Wickstr\u00f8m et al., 2024). Some examples of this manipulation include adjusting evaluation parameters to achieve desired outcomes, thereby distorting the assessment of methods; optimizing explanations to perform well on specific metrics while not accurately reflecting the model's true decision-making process; and using adversarial inputs to create explanations that seem robust but fail in real-world applications.\nRecent research has increasingly focused on the role of hyperparameters in XAI evaluations and how they can introduce confounding effects (Hedstr\u00f6m et al., 2023a). These studies vary in how they define dependent and independent variables and the specific hyperparameter space they examine\u2014whether related to the model, the explanation, or the evaluation process. For example, research has explored how sensitive attribution methods are to explanation hyperparameters like random seed or sample size (Bansal et al., 2020), and how baseline choices (such as those in Integrated Gradients) impact explanation outcomes (Sturmfels et al., 2020; Sundararajan et al., 2017b).\nAdditionally, studies have examined how changes in model performance variables\u2014such as optimizer, activation function, learning rate, or dataset split\u2014can influence explanations (Karimi et al., 2023). Other work has looked at the effects of model priors and random weight initialization on both explanations and evaluations (Hase et al., 2021). Research has also investigated the disagreement between different explanation methods, particularly concerning feature ranking (Krishna et al., 2022), and the influence of baseline choices on these disagreements (Koenen & Wright, 2024).\nFurthermore, recent studies have delved into how sensitive evaluation outcomes are to hyperparameters, such as the effects of normalization, randomization order, and similarity measures on randomization metrics (Binder et al., 2023; Sundararajan & Taly, 2018; Hedstr\u00f6m et al., 2024a). Faithfulness metrics have also been shown to be influenced by hyperparameters like baseline choices and the order of perturbation (Bl\u00fccher et al., 2024; Samek et al., 2017; Brunke et al., 2020; Mamalakis et al., 2022; Dolci et al., 2023; Rong et al., 2022b; Tomsett et al., 2020)."}, {"title": "4. Key Requirements for Reliable Metrics", "content": "To overcome these challenges, reliable XAI metrics must be developed. These metrics should establish standard benchmarks for explainability, helping to compare, quantify, and qualify evaluation results. Standardization will clarify regulatory requirements, minimizing bias in user-preferred choices (Nauta et al., 2023; Pawlicka et al., 2024). \n4.1. Transparency & Robustness\nXAI evaluation metrics must provide clear, consistent insights into how well an explanation aligns with the model's decision-making process. Transparent metrics ensure that all stakeholders\u2014developers, regulators, and endusers\u2014can trust the results, leading to greater confidence in the system's explanations. This transparency is vital for continuous improvements in explanations and the underlying models. Metrics should assess the stability of explanations under various conditions, including adversarial inputs and data changes (Rosenfeld, 2021). A robust metric ensures that the explanation holds up under real-world variations, offering consistency and reliability across different environments. This guarantees that XAI systems are resistant to manipulation and remain trustworthy."}, {"title": "4.2. Adaptability & Tamper-Resistance", "content": "XAI frameworks should be flexible enough to cater to diverse domains, such as healthcare or finance, where priorities like interpretability or compliance vary (Rosenfeld, 2021). Adaptable metrics ensure their applicability across different sectors, addressing the specific challenges of each. Moreover, these metrics must incorporate safeguards such as adversarial testing and regular validation to prevent manipulation. By \"tamper-proof,\u201d we mean that the metrics cannot be manipulated by hyperparameters, ensuring that explanations are not artificially adjusted to meet predetermined standards, thus maintaining the integrity of evaluations."}, {"title": "4.3. Scalability", "content": "Scalable explainability metrics are critical for evaluating modern AI systems, particularly large-scale models such as LLMs. Existing explainability metrics are often computationally expensive, limiting their feasibility for large-scale deployments. As AI models grow in complexity, it is imperative to develop scalable methods that can provide meaningful explanations without excessive computational overhead. Efficient and lightweight explainability techniques will ensure that evaluation frameworks remain practical and adaptable, even for large and resource-intensive AI models. (Dessain et al., 2023)"}, {"title": "4.4. Regulatory & Legal Aspects", "content": "XAI metrics must align with regulatory frameworks such as the EU AI Act, which mandates transparency and interpretability for high-risk AI systems (Sovrano et al., 2022; Nannini et al., 2024a). These metrics must help organizations demonstrate compliance by providing clear, traceable, and auditable explanations. Additionally, they should be adaptable to evolving regulations to maintain compliance over time.\nFrom a legal perspective, XAI metrics must ensure that Al decisions are explainable and justifiable in court(Walke et al., 2023). Explanations should be transparent, fair, and accountable, particularly in healthcare and finance sectors where AI decisions have significant consequences. Legal frameworks require that explanations are accessible to affected individuals and can be used to contest or defend decisions (Fresz et al., 2024a; Bibal et al., 2020; Nannini et al., 2023)."}, {"title": "5. Alternative Views", "content": "While the need for reliable and standardized XAI metrics is generally agreed upon, some alternative perspectives offer important considerations that should be acknowledged. These viewpoints present challenges and suggest nuanced approaches to developing XAI metrics (Hoffman et al., 2019)."}, {"title": "5.1. Universal Metrics Are Unrealistic", "content": "Critics argue that the diverse nature of AI applications makes the creation of universal XAI metrics impractical. The requirements for evaluating XAI in fields like medical imaging differ significantly from those in financial auditing. For instance, healthcare emphasizes interpretability and trans-"}, {"title": "5.2. The Role of Human Judgment", "content": "Another viewpoint stresses that subjective evaluations by domain experts are crucial in assessing XAI systems. While automated metrics provide valuable insights, they may miss the nuanced understanding and contextual relevance that human judgment offers. Human evaluators bring domain-specific expertise essential for interpreting explanations in real-world settings. This perspective advocates combining quantitative metrics with qualitative assessments from experienced experts to ensure comprehensive evaluations that reflect the complexities of real-world applications. (Ehsan & Riedl, 2019; Colin et al., 2023)"}, {"title": "5.3. A Hybrid Approach", "content": "Rather than relying solely on universal metrics, a hybrid approach can balance standardization and the flexibility needed for domain-specific requirements. Core benchmarks for widely accepted properties, such as fidelity and robustness, should be established as a foundation for XAI evaluation. Simultaneously, domain-specific metrics can address unique needs in various fields. This hybrid approach can also incorporate human judgment, ensuring subjective insights are included without compromising the objectivity of the evaluation process. Combining standardized metrics with flexibility and human expertise can create a more robust, adaptable, and meaningful evaluation framework for XAI systems. (Ma, 2024)"}, {"title": "6. Call to Action", "content": "To address the current gaps in XAI evaluation and ensure the development of reliable, transparent, and compliant AI systems, the XAI community must prioritize the following actions:\n\u2022 Develop Core Benchmarks: Establish standardized metrics that assess key aspects of explainability such as fidelity, robustness, clarity, and comprehensibility. These benchmarks will be a solid foundation for comparing and evaluating XAI methods across various domains and applications.\n\u2022 Address Challenges of Advanced Models: Develop evaluation metrics that are scalable and adaptable to the growing complexity of AI systems, including large language models (LLMs). As AI models evolve, evaluation frameworks must keep pace with their intricate decision-making processes and emergent behaviors.\n\u2022 Ensure Metric Integrity: Develop robust evaluation frameworks that incorporate mechanisms such as adversarial testing, regular validation, and consistency checks to prevent manipulation, specifically ensuring that results cannot be influenced by hyperparameters. These safeguards are essential to preserving the integrity and reliability of XAI evaluations in practical use.\n\u2022 Develop Domain-Specific Evaluation Metrics: While core benchmarks are important, \u03a7\u0391\u0399 metrics should be customized to address the specific needs of different industries. For example, healthcare metrics should prioritize fine-grained explainability with high confidence, while finance metrics should focus on compliance, fairness, and auditability. Tailored metrics will ensure that XAI systems meet various sectors' regulatory and operational standards.\n\u2022 Optimize Computational Efficiency: Recent research on sustainable machine learning highlights the increasing computational burden of explainability methods, particularly for complex AI models. Many existing XAI techniques, such as perturbation-based approaches, impose significant resource constraints, making them impractical for large-scale models like LLMs. To facilitate broader adoption of XAI, the community must develop evaluation frameworks that balance computational efficiency with the quality and fidelity of explanations, ensuring that interpretability remains feasible even for highly sophisticated AI systems. (Jean-Quartier et al., 2023)\n\u2022 Promote Collaboration Across Sectors: Strengthen collaboration between academia, industry, and regulatory bodies to create evaluation metrics that meet both"}, {"title": "7. Broader Implications", "content": "Establishing reliable XAI metrics holds broad implications that extend beyond the technical realm, influencing various aspects of Al deployment, regulation, and societal trust. These include:"}, {"title": "7.1. Regulatory Compliance", "content": "Reliable XAI metrics can be pivotal in ensuring that AI systems comply with legal frameworks, such as the European Union's AI Act (eur; Sovrano et al., 2020). By establishing objective criteria for evaluating explainability, these metrics help organizations demonstrate that their AI systems meet the transparency and accountability requirements mandated by regulation. This compliance reduces the risk of regulatory penalties and fosters trust among stakeholders, facilitating the responsible deployment of AI systems in high-risk domains. (Lund et al., 2025)"}, {"title": "7.2. Auditablity and traceability", "content": "Such evaluation metrics can provide a reference to quantify the auditability of explanations; unlike today, any explanation is acceptable, as there are no quantification metrics. Auditors and model evaluators can use these metrics to evaluate the quality of the explanations. (Li & Goel, 2024; McCormack & Bendechache, 2024; Toader, 2019)"}, {"title": "7.3. Building Trust", "content": "Developing clear, consistent, and transparent evaluation standards enhances user confidence in XAI systems(Sovrano et al., 2021). In high-stakes domains, where trust is a prerequisite for adoption, reliable metrics demonstrating AI systems' transparency and reliability are essential. By providing stakeholders with concrete evidence of the model's interpretability and decision-making process, these metrics help to bridge the trust gap between AI systems and the end-users who depend on them."}, {"title": "7.4. Advancing Ethical AI", "content": "Metrics that prioritize fairness, robustness, and transparency contribute to the broader goal of advancing ethical AI. By aligning the development of XAI with societal values, these metrics help ensure that AI systems are effective and serve the public good. As AI systems increasingly influence critical areas such as healthcare, finance, and justice, it is essential that their decision-making processes are understandable, fair, and accountable. XAI metrics that uphold these principles will support the creation of AI systems that promote equity, fairness, and transparency, advancing the ethical deployment of AI technologies. (Nannini et al., 2024b; Akhtar et al., 2024)"}, {"title": "7.5. Fostering Innovation", "content": "Standardized evaluation frameworks create a shared foundation for researchers and developers, enabling more effective collaboration, comparison, and innovation within the XAI community. With consistent benchmarks, researchers can compare different XAI methods more easily, identify best practices, and build on each other's work. This fosters a more dynamic and productive research ecosystem, accelerating the development of new methods and techniques in the field of XAI. Regulatory frameworks do not operate in isolation; they continuously evolve based on the capabilities of explainability methods and their impact on real-world AI deployment. As this feedback loop ensures that Al regulations stay relevant while explainability techniques remain aligned with compliance needs."}, {"title": "8. Conclusion", "content": "Reliable and standardized XAI evaluation metrics are crucial for AI transparency, trustworthiness, and regulatory compliance. Despite progress in explainability methods, evaluation remains fragmented, subjective, and prone to manipulation. Key challenges include:\n\u2022 Lack of standardization: Inconsistent faithfulness, robustness, and fairness metrics hinder reliability.\n\u2022 Manipulation risks: Flexible evaluation methodolo-"}, {"title": "9. Impact Statement", "content": "The development of reliable, standardized evaluation metrics for Explainable AI (XAI) will profoundly impact the field of artificial intelligence and its integration into high-stakes applications. By ensuring that XAI systems are transparent, trustworthy, and compliant with evolving regulations, these metrics will foster broader adoption of AI technologies across critical sectors such as healthcare, finance, and law enforcement.\nReliable XAI metrics will empower developers to create models that are not only accurate but also interpretable and justifiable, thereby enhancing decision-making processes and reducing risks associated with AI-driven decisions. Furthermore, these metrics will help ensure that AI systems align with ethical standards, making them more acceptable to stakeholders, regulators, and the public.\nBy facilitating the creation of more effective and reliable XAI methods, these efforts will contribute to developing AI systems that are not only technically sophisticated but also ethical, fair, and accountable. Ultimately, establishing robust XAI evaluation standards will play a pivotal role in advancing the responsible deployment of AI technologies, ensuring that they serve the public good while minimizing unintended consequences."}]}