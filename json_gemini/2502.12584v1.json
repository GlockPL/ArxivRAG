{"title": "Enhancing Semi-supervised Learning with Noisy Zero-shot Pseudolabels", "authors": ["Jichan Chung", "Irene Y. Chen"], "abstract": "Semi-supervised learning (SSL) leverages limited labeled data alongside abundant unlabeled data to address labeling costs in machine learning. While recent foundation models enable zero-shot inference, attempts to integrate these capabilities into SSL through pseudo-labeling have shown mixed results due to unreliable zero-shot predictions. We present ZMT (Zero-Shot Multi-Task Learning), a framework that jointly optimizes zero-shot pseudo-labels and unsupervised representation learning objectives from contemporary SSL approaches. Our method introduces a multi-task learning-based mechanism that incorporates pseudo-labels while ensuring robustness to varying pseudo-label quality. Experiments across 8 datasets in vision, language, and audio domains demonstrate that ZMT reduces error by up to 56% compared to traditional SSL methods, with particularly compelling results when pseudo-labels are noisy and unreliable. ZMT represents a significant step toward making semi-supervised learning more effective and accessible in resource-constrained environments.", "sections": [{"title": "Introduction", "content": "The growing scale of machine learning applications has made data labeling costs a critical bottleneck in deploying ML systems [1, 2, 3]. Semi-supervised learning (SSL) addresses this challenge by leveraging unlabeled data alongside limited labeled examples [4]. Traditional SSL approaches like pseudo-labeling and consistency regularization have demonstrated strong performance across domains, particularly in computer vision and natural language processing [5, 6, 4].\nRecent advances in foundation models have enabled zero-shot inference on novel tasks without task- specific training [7, 8]. These models can generate predictions for unseen tasks by leveraging their pre- trained knowledge, offering a promising direction for reducing labeling requirements. Several works have proposed integrating these zero-shot capabilities into SSL frameworks [9, 10]. Current approaches primarily use foundation models as teacher networks for generating pseudo-labels through inference, which requires complex model distillation and introduces additional training overhead. In contrast, we leverage zero-shot predictions of foundation models directly as pseudo-labels a simpler yet underexplored direction.\nHowever, zero-shot predictions often exhibit high uncertainty and domain-specific biases [11]. This un- reliability can introduce noise into the training process, potentially degrading SSL performance rather than improving it. Building on prior work on doubly-robust self-training [10], we propose ZMT (Zero-Shot Multi- Task Learning), which bridges this gap through four key technical contributions:\n1. A novel framework to directly leverage zero-shot predictions from foundation models as pseudo-labels, eliminating the need for teacher model inference or distillation\n2. A multi-task learning-based mechanism that incorporates zero-shot pseudo-labels and unsupervised SSL objectives\n3. Extensive experiments on 8 datasets across vision, language, and audio domains which demonstrate that ZMT outperforms both standard SSL methods and zero-shot augmented approaches\n4. Additional analysis on why ZMT outperforms baselines, including the impact of coarse-grained pseudo- labels, unlabeled data accuracy during training, and hyperparameter stability"}, {"title": "Related Work", "content": ""}, {"title": "Semi-Supervised Learning", "content": "Semi-supervised learning (SSL) has evolved from foundational consistency regularization methods to so- phisticated deep learning approaches. The \u03a0-Model [6] and Mean Teacher [4] established core principles by enforcing consistent predictions across different model states. FixMatch [5] later unified these ideas by combining weak and strong augmentations with pseudo-labeling. Subsequent work focused on reliability: UPS [12] introduced confidence-based filtering while FlexMatch [13] developed adaptive thresholding for pseudo-label selection. Recent approaches like SimMatch [14] have further advanced the field by incorporat- ing contrastive learning principles."}, {"title": "SSL with Side Information", "content": "The integration of additional information sources has significantly enhanced SSL performance. S4L [15] pioneered this direction by incorporating self-supervised pretext tasks into the SSL framework. Building on this foundation, CoMatch [16] introduced graph-based relationships to capture sample similarities, while MPL [17] employed meta-learning within teacher-student frameworks to refine pseudo-labels. More recent approaches have explored diverse information sources: WiSE-FT [18] leverages weak supervision signals to guide model training, and SSDG [19] taps into external knowledge bases to generate more reliable pseudo- labels."}, {"title": "Foundation Models in SSL", "content": "The integration of foundation models into SSL frameworks is an emerging direction. As foundation models show impressive zero-shot performance [8, 20], the question becomes: how can we incorporate foundation models into SSL frameworks? The three main emerging approaches are: 1) pre-training on the unlabeled data before fine-tuning on the labeled data, or 2) using a teacher model to assign pseudo-labels [9]. The first"}, {"title": "Problem Setup", "content": ""}, {"title": "Motivation", "content": "When leveraging foundation models for SSL, practitioners face two approaches: direct fine-tuning, or using inference-only access. While fine-tuning can yield strong performance, it faces practical limitations:\n\u2022 Resource Requirements: Fine-tuning large models (e.g., Llama-70B) requires significant computa- tional resources- even with LoRA and 16-bit quantization, it demands 160GB+ GPU memory.\n\u2022 API Constraints: Commercial APIs often restrict fine-tuning capabilities. For instance, OpenAI's fine-tuning service only supports supervised learning with fixed input-output pairs, preventing usage of unlabeled data.\n\u2022 Cost and Reproducibility: The financial costs of API calls scale with model size and data volume. Version changes in hosted models can affect reproducibility.\n\u2022 Efficiency: The computational and memory requirements of fine-tuning large foundation models often exceed practical resource constraints for many applications [26].\nGiven these constraints, we focus on a practical setting where foundation model access is restricted to inference only. Although we refer to zero-shot inference as our motivating source of pseudo-labels, in-context learning with few labeled examples could also be used to generate pseudo-labels."}, {"title": "Problem Setup", "content": "Consider a semi-supervised setting with read-only access to a foundation model $f$. The training data consists of a labeled set $D_L = {(x_i, y_i) : i \\in [N_L]}$ and an unlabeled set $D_U = {(u_i) : i \\in [N_U]}$, where $D = D_L \\cup D_U$ represents the full dataset and $[N]$ denotes integers $1, 2, ..., N$. The foundation model $f$ generates pseudo- label distributions $\\tilde{y}_i^l = f(x_i)$ for labeled data and $\\tilde{y}_i^u = f(u_i)$ for unlabeled data. The goal is to train a classifier $f$ that outputs class distributions $p(y|x)$ using $D_L$, $D_U$, and pseudo-labels $\\tilde{y}^l$, $\\tilde{y}^u$.\nWe assume $f$ was not trained on $D_L$ or $D_U$. The pseudo-label quality depends on the model's capabilities and prompt design, with potential failure modes including hallucinations and out-of-domain responses. Our objective is to develop a SSL method that maximally leverages high-quality pseudo-labels when available and gracefully degrades to standard SSL performance when pseudo-labels are unreliable."}, {"title": "Baseline methods", "content": "We discuss previous approaches for our semi-supervised learning setup given with pseudo-labels and compare against our method. See feature comparison matrix Table 6 in appendix.\nPseudo-supervision When given a semi-supervised dataset that includes pseudo-labels, a straightforward approach is to fill in the prediction targets for the unlabeled samples using the pseudo-labels, resulting in fully pseudo-labeled unlabeled set $ \\hat{D_U} = \\{(u_1, \\hat{y}_1^u), (u_2, \\hat{y}_2^u), ..., (u_{N_U}, \\hat{y}_{N_U}^u)\\}$. The model can then be trained"}, {"title": "Zero-Shot Multi-Task Learning", "content": "One explanation for the failure of pseudo-supervision and doubly robust method with inaccurate pseudo- labels is that these approaches directly optimize the classifier to predict pseudo-labels with cross-entropy loss. This can lead to corrupting the overall training process when the pseudo-labels are noisy [31]. To mitigate this issue, we propose a method that indirectly incorporates pseudo-labels into the objective func- tion. Additionally, we aim to develop a method that fully leverages SSL's unsupervised feature learning capabilities while benefiting from the pseudo-labels provided.\nMulti-task learning [32] offers a framework for learning multiple tasks simultaneously. When the tasks are similar and related, they can mutually benefit from each other. However, when the tasks are dissimilar, the model learn them independently, provided the model's capacity is large enough [33].\nTo improve robustness against noisy pseudo-labels, we propose a multi-task learning-based approach that combines the SSL task as the primary task with pseudo-label prediction (extraction) as an auxiliary task. When the pseudo-labels are accurate, the model benefits from the pseudo-label prediction task. Conversely, when the pseudo-labels are noisy, the model can still benefit from SSL's objective, with less the impact from the inaccurate pseudo-label prediction task. We denote our method by ZMT (Zero-shot pseudo-label Multi-Task learning), and highlight the key differences between our method and existing approaches in Table 6.\nWe illustrate our method in Figure 1. The classifier $f$ consists of a non-linear projector head $h(\u00b7)$ and a feature encoder $g(\u00b7)$, such that $f = h \\circ g$. We introduce an additional linear projector head $h_p(\u00b7)$, which learns a pseudo-label prediction task, while sharing the encoder $g$ with the main classification task.\nGiven a batch of labeled and unlabeled data of size $B_L$ and $B_U$, the pseudo-label prediction task optimizes $h_p \\circ g$ with the following loss:\n$L_p = \\frac{1}{B}(\\sum_{i=1}^{B_L} H(\\tilde{y}_i^l, q(y|x_i)) + \\sum_{i=1}^{B_U} H(\\tilde{y}_i^u, q(y|u_i)))$ \nwhere $B = B_L + B_U$ and $q(y|x)$ indicates predicted class distribution of $h_p(g(x))$. Together with the SSL objective, the overall loss function is:\n$L = L_S + L_U + \\lambda_t \\cdot \\lambda_p L_p$\nwhere $\\lambda_t$ is annealing parameter that linearly increases value from 0 to 1 during training based on training step $t$, and $\\lambda_p$ is a fixed scalar hyperparameter indicating relative weight of pseudo-label prediction task. We denote the inclusion of annealing with binary variable $a_p$.\nNote that during model selection using the validation set, the best model is chosen without considering the pseudo-label prediction score from $h_p$, as the primary goal is to achieve optimal performance for the original classifier $f = h \\circ g$."}, {"title": "Experiments", "content": ""}, {"title": "Experiment Setup", "content": "Datasets We use 8 publicly-available datasets across vision, natural language processing, and audio clas- sification tasks (Table 1). Dataset sizes range from 100s to 100,000s of data points. For dataset-specific training setup details, see Appendix B including the choice of foundation models, hyperparameters used. We evaluate on tasks where additional label information meaningfully improves performance over SSL baselines. We exclude datasets like CIFAR-10 [34], SVHN [35], and STL-10 [36] where recent SSL methods already achieve near-optimal performance with limited labels, making pseudo-label information redundant. To eval- uate the effect of labeled dataset sizes, for each dataset, we create up to three tasks with different sizes of labeled data while the number of total data points stays the same. For example, in the CIFAR100 dataset, we create sets with 100, 200, and 400 labeled data points.\nPseudo-label For each dataset, we consider multiple sets of pseudo-labels with varying quality. In practice, high-quality foundation models are often used to generate the best pseudo-labels for a given task. However, to evaluate the robustness of our method against less accurate pseudo-labels, we also test our approach using pseudo-labels generated from smaller-weight versions of the same foundation model, described as A, B, and C (in declining pseudo-label quality) in Table 1 and in experiment results. Depending on the scores, at most 3 different pseudo-label sets from distinct foundation models are tested. These pseudo-labels are generated using a zero-shot method, with basic prompting on the input data."}, {"title": "ZMT Improves Performance on SSL Baselines Despite Pseudo-Label Quality", "content": "Our ZMT approach consistently achieves the highest scores in all settings with accurate pseudo-labels. Benchmark results for the CIFAR-100, NLP, and audio datasets are shown in Tables 2, 3, and 4, respectively. Our results find that ZMT largely improves upon SSL baselines (e.g., AdaMatch, SimMatch), Pseudo- supervision, and Doubly-robust methods. For example, in the AG News dataset with 40 labels and pseudo- set label set A, ZeroMatch achieves an error of 9.43 compared to the supervised model error of 14.08, yielding a reduction of 49.3%. The highest improvement comes from ZeroMatch+ producing a 56.2% error reduction on pseudo-label set A in the 50-label setting of the Urbansound 8k dataset, compared with AdaMatch.\nIn cases where a low- or medium-quality pseudo-label set is given, our method generally improves or at least maintains performance close to that of SSL baselines, demonstrating its robustness against noisy labels. For example, for CIFAR100 set B-which has low quality pseudo-labels-ZeroMatch+ achieves an error of 19.65, outperforming AdaMatch's error of 21.68. In most of the settings with inaccurate pseudo-label sets, our method is better or maintains the performance of baseline within one standard deviation, with 3 exceptions (of 23 total experiments) where SimMatch outperforms ZeroMatch or ZeroMatch+."}, {"title": "ZMT Leverages Coarse-Grained Zero-Shot Pseudo-Labels to Improve Learning", "content": "Our ZMT architecture enables the use of coarse-grained pseudo-labels, unlike Pseudo-supervision and Double- robust Self-training which cannot due to the use of the pseudo-labels as a substitute for prediction target. To accommodate such responses, one can modify the non-linear head, $h_p$, to adjust to the desired number of output classes. We demonstrate improved performance as a result of coarse-grained pseudo-labels through additional experiments on the Amazon Reviews dataset, where the original target classes are given as 5-star ratings of a product. We generate pseudo-labels from zero-shot classifications with three candidate labels:"}, {"title": "ZMT Improves Accuracy in Unlabeled Data", "content": "To gain a deeper understanding of how ZeroMatch improves training with pseudo-labels, we compare the PLS accuracy-meaning the prediction accuracy of the unlabeled data during training of ZeroMatch and AdaMatch on unlabeled samples utilized during training. The term \"utilized\" refers to unlabeled samples whose PLS predictions exceed AdaMatch's pre-defined confidence threshold $T$, which actually participates in optimization.\nIn Figure 2, we observe that the PLS accuracy is higher with high-quality pseudo-labels than AdaMatch. Notably, PLS accuracy with low-quality pseudo-labels is consistent with AdaMatch, indicating that ZMT preserves performance close to the AdaMatch by leveraging AdaMatch's intrinsic ability to predict based on labeled samples and unlabeled feature representations. We observe similar results in the PLS accuracy of all unlabeled samples (Appendix Figure 3)."}, {"title": "ZMT May Be Sensitive to Hyperparameter Selection", "content": "We run a sensitivity analysis of ZeroMatch on the two newly introduced hyperparameters, $a_p$ and $\\lambda_p$, which control the scale of the pseudo-label prediction loss relative to the SSL objective function. The"}, {"title": "Conclusion", "content": "This work introduces ZMT, a framework for robustly integrating foundation model predictions into semi- supervised learning. Through extensive experiments across multiple domains, we demonstrate that ZMT achieves state-of-the-art performance while maintaining robustness to varying pseudo-label quality. Our learning-based mechanism effectively balances between limited labels and plentiful pseudo-labels, enabling practitioners to leverage foundation models without the computational overhead of direct fine-tuning. Future work could explore extending ZMT to more complex tasks and investigating theoretical guarantees for the weighting mechanism."}]}