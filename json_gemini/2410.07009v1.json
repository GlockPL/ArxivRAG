{"title": "Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based Outline-guided Generation", "authors": ["Valentin Knappich", "Simon Razniewski", "Anna H\u00e4tty", "Annemarie Friedrich"], "abstract": "The patent domain is gaining attention in natural language processing research, offering practical applications in streamlining the patenting process and providing challenging benchmarks for large language models (LLMs). However, the generation of the description sections of patents, which constitute more than 90% of the patent document, has not been studied to date. We address this gap by introducing the task of outline-guided paper-to-patent generation, where an academic paper provides the technical specification of the invention and an outline conveys the desired patent structure. We present PAP2PAT, a new challenging benchmark of 1.8k patent-paper pairs with document outlines, collected using heuristics that reflect typical research lab practices. Our experiments with current open-weight LLMs and outline-guided chunk-based generation show that they can effectively use information from the paper but struggle with repetitions, likely due to the inherent repetitiveness of patent language. We release our data and code.", "sections": [{"title": "1 Introduction", "content": "Patenting an invention is time-consuming and requires substantial knowledge of the field of the invention, as well as patent law. Therefore, patent drafting\u00b9 is a costly process, motivating measures to increase patent attorneys' productivity. Some tasks in the patenting process are already technologically supported, e.g., prior art search (Shalaby and Zadrozny 2019; Stamatis 2022; Pujari et al. 2022). In contrast, patent drafting remains a predominantly manual effort. Recently, the interest of studying the capabilities of Large Language Models (LLMs) in the patent domain has increased (Shomee et al. 2024; Jiang and Goetz 2024; Casola and Lavelli 2022; Wang et al. 2024). However, prior work is limited to the generation of the sections Abstract and Claims (Hamborg et al. 2017; Lee 2020; Christofidellis et al. 2022; Lee 2023; Zuo et al. 2024; Lee 2024; Bai et al. 2024). Yet, patents also contain a patent description (comprising sections like Field of the Invention, Background, Summary, and Detailed Description) which constitutes more than 90% of the document.2 To the best of our knowledge, due to the lack of benchmark data, its automatic generation has not been studied to date despite offering both practical and scientific value. Automating this time-intensive drafting process could yield considerable cost savings. From a scientific standpoint, patent descriptions present a significant challenge for LLMs due to their technical complexity, specialized language, and substantial length.\nThe goal of this work is to evaluate LLMs' proficiency in supporting the drafting of complex patent documents from a given invention specification.3 In practice, the inventor typically drafts an invention report, i.e., a document describing and explaining the invention to the patent attorney as the basis for the patent drafting. In research labs, concurrent academic publishing and patenting is common practice (Murray and Stern 2005), and often the paper itself serves as an invention"}, {"title": "2 Related Work", "content": "Patent-Paper Pairs (PPPs). Many research labs practice concurrent patenting and academic publishing, and this is reflected in publication and patent numbers, which are often roughly similar (Cottier, Besiroglu, and Owen 2023; Haney 2019). Murray and Stern (2005) find that almost 50% of their sampled academic papers from Nature Biotechnology have a corresponding US patent. In economics, PPPs have been used to study innovation dynamics, like whether patenting hinders the free flow of innovations (Murray and Stern 2005; Magerman, van Looy, and Debackere 2011). In that context, several approaches to finding PPPs have been proposed: Murray (2002) and Murray and Stern (2005) identify pairs manually by analyzing their full texts and citation networks. Magerman, Van Looy, and Song (2010) and Van Looy et al. (2011) explore several data mining techniques, including SVD and term-set overlaps between titles and abstracts, but they did not publish any open dataset. We build upon their idea for the document content similarity, but refine it, and add criteria for author overlaps, date ranges, competing candidates, and licenses (see Section 3.1). Gans, Murray, and Stern (2017) propose a taxonomy of PPPs that includes both to 1-to-1 matches and m-to-n matches. To ensure that papers are a solid source of information about the invention, we design our matching procedure to find only 1-to-1 matches. To the best of our knowledge, our work is the first to present a dataset of PPPs for natural language processing (NLP) research.\nPatent Generation. Prior work on patent generation has focused on titles, abstracts, and claims. For instance, Christofidellis et al. (2022) train a GPT2 model to generate these parts and found that multitask learning improves performance. Lee (2023) pre-trains a GPT-J-6B architecture on patents, including the descriptions, and evaluates it on claim generation via hypothetically saved keystrokes. Zuo et al. (2024) use LLMs such as GPT-3.5-turbo and Llama-2 to generate abstracts from claims, and claims from previous claims. Jiang and Goetz (2024) provide a comprehensive review of patent generation: they identify several limitations of prior work, such as ill-posed task setups, lack of open benchmarks, and disregard for patent descriptions; all issues that we tackle in the present work.\nOutline-guided Generation. Outline-guided generation is a paradigm in which LLMs use an outline to produce longer, more structured and coherent text, and which can be used to increase user control. It has been used for the generation of stories and articles, but to the best of our knowledge, it has not been applied to patent generation. The outline can be model-generated, essentially constituting a planning step. To obtain training data for this step, prior work has used extraction of keywords (Yao et al. 2019), phrases (Fang et al. 2021) and sentences (Li et al. 2023b; Sun et al. 2022; Drissi, Watkins, and Kalita 2018). Additionally, outlines could enable more sophisticated human-in-the-loop user interactions. Goldfarb-Tarrant, Feng, and Peng (2019) have shown that iteratively and interactively refining the outline yields substantial improvements in story generation. In our work, we posit that humans should provide outlines to automated patent generation methods, a modest manual effort that greatly increases user control."}, {"title": "3 PAP2PAT Dataset", "content": "In this section, we present the PAP2PAT dataset containing 1.8k PPPs from a variety of domains, each annotated with multiple outlines. It serves two main purposes: (1) It is an extremely challenging benchmark for LLMs because it contains very long and complex patent documents, and requires deep understanding of the technical domain as well as patent law. (2) It facilitates the development of AI-powered tools for patent drafting, where the patent attorney only performs post-editing rather than writing from scratch, potentially incurring massive cost savings. In the following, we describe the steps taken for constructing the PAP2PAT dataset: scraping and filtering PPPs, parsing the full-text documents and generating the patent outlines."}, {"title": "3.1 Scraping Patent-Paper Pairs", "content": "To create PAP2PAT, we start out with the USPTO dataset containing 6.7M patent applications from 2005 to April 2024. For each patent, we query SemOpenAlex (F\u00e4rber et al. 2023) using SPARQL and retrieve papers with overlapping authors lists and publication dates. Next, we filter the results based on their titles, abstracts, other candidate matches for the same patent, and paper licenses, as elaborated below.  shows the remaining number of candidates after each filtering step. An example match is shown in Table 1. We perform a systematic manual post-hoc evaluation of the heuristics to validate their precision (see Section 3.2).\nAuthor Overlap. The patent and the paper of a PPP are by definition authored by overlapping sets of individuals. The overlap of author lists have therefore been identified as an effective (yet not sufficient) criteria for matching PPPs (Magerman, Van Looy, and Song 2010). The requirements for paper authorship are typically much lower than those for patent inventorship (Konski and Wu 2015). In many cases, only the main author(s) and the senior author(s) are listed as inventors. We accordingly employ an asymmetric score that only measures the fraction of inventors i \u2208 I that are also authors a \u2208 A, not vice versa:\n$$sim_{author} = \\frac{|I \\cap A|}{|I|} \\geq 0.8$$\nThis score's effectiveness increases with the number of inventors, so we only consider patents with at least two inventors. Implementating $sim_{author}$ requires some form of author name disambiguation to avoid false negatives (different spellings, e.g., with, without, or with abbreviated middle name) and false positives (e.g., very common names like John Smith). There are no author identifiers shared between the patent and paper datasets, so our disambiguation uses the surface names only. To account for false negatives, we use the aliases stored in SemOpenAlex and consider an inventor to be an author if their name matches exactly with one of the aliases. False positives are marginalized by author combinations and subsequent filters: it is highly unlikely that there exist two groups of people with the same set of names working on the same topic at the same time.\nDate Range. We require the paper's publication date to be within one year before and two years after the patent application date. The former corresponds to the USPTO's grace period, which allows inventors to file patent applications up to one year after they disclosed the invention to the public. The two-year period after the application date was selected because qualitative analyses identified it as the point of diminishing returns, beyond which the incidence of true positives notably decreases, while the rate of false positives significantly increases."}, {"title": "3.3 Document Parsing", "content": "We parse all patents and papers into a nested JSON schema where each section has a title, paragraphs and subsections field. We make considerable efforts to obtain clean data: we perform LLM-based section hierarchy reconstruction for patents, font-based section hierarchy reconstruction for paper PDFs and formula conversion for patents and paper XMLs from PubMed. We provide more details in Appendix G."}, {"title": "3.4 Outline Generation", "content": "One key aspect of our approach are the patent outlines, which simulate the patent attorney's input to the system in production settings. Examples are shown in Figure 2 and Appendix A. They provide the model with the target headings, as well as bullet points summarizing the document structure and high-level content of every section.\nFor automatically generating the outlines for Pap2PAT, we leverage the original patents and Llama-3 70B (Dubey et al. 2024). We provide the prompt in Appendix E. To ensure that the model adheres to the output format, we use SGLang (Zheng et al. 2023) for constrained decoding. We enforce a fixed number of bullet points $n_{bullets}$ per section, where $n_{bullets}$ is proportional to the length of the text in that section ($n_{chars}$).\n$$N_{bullets} = \\begin{cases} max(1, \\lfloor n_{chars}/l \\rfloor) & \\text{if } n_{chars} > 0 \\\\0 & \\text{else} \\end{cases}$$\nwhere l is the number of characters that each bullet point summarizes on average. We create outlines in three levels of granularity: long (l=500, avg. 150 items), medium (l=1000, avg. 74 items) and short (l=2000, avg. 37 items), see Table 3. For each section (identified by a heading), we additionally provide the number of characters in the original patent, which can function as the desired content lengths during generation. In a practical application setting, the patent attorney would provide them alongside the outlines."}, {"title": "3.5 Dataset Splits and Statistics", "content": "We split our dataset randomly into train (n=1000), test (n=500) and validation (n=242). We additionally create a non-contaminated test set (nc-test) that contains all pairs with a patent published in 2024 (n=71), i.e., after the pretraining cut-off date of all evaluated open-weight LLMs. Thus, we address the concern that LLMs might have seen test data during pretraining (Ravaut et al. 2024). Table 3 shows dataset statistics across the splits, further statistics and plots are presented in Appendix H."}, {"title": "4 Chunk-based Outline-guided Generation", "content": "In this section, we propose chunk-based outline-guided generation, a novel approach to patent generation illustrated in Figure 2. It serves as a strong baseline for comparison in future work. It is motivated by current LLMs' limitations regarding sequence length (which needs to cover the instruction, the context, and the output) and therefore generates the patent in chunks, guided by the user-defined outline. We chunk the outline, retrieve paper context based on the outline chunk, and prompt an LLM to generate the patent text for that chunk. Finally, we concatenate the generated outputs and apply only a lightweight post-processing to remove duplicate headings at chunk boundaries.\nChunking and Token Allocation. We assign a fixed number of tokens for the instruction (including system prompt, user prompt and outlines) and equally split the remaining tokens among paper context and output patent. We then chunk the outline using the desired length of the respective patent section. The chunking procedure is designed to pack as much content as possible into each chunk, while preserving the integrity of individual sections. Therefore, we only split a section if it exceeds the token limit on its own.\nRetriever. For each chunk i, the outline $o_i$ is passed to a retriever, which selects relevant paragraphs from the paper to create the paper context $c_i$. We use BM25 (Robertson and Zaragoza 2009) as the retrieval method and the chunk's outline as the query. We always include the abstract of the paper because it provides a valuable overview, and successively add paragraphs in the order of their relevance ranking until the token limit is reached. The results are formatted in a Markdown document that comprises all headings and the retrieved paragraphs.\nLLMs. The paper context $c_i$, the current outline $o_i$ and prior outlines $o_{j<i}$ are combined to form the prompt to the LLM (see Appendix F). Based on this prompt, the LLM generates the patent chunk $p_i$, where constrained decoding ensures that the model adheres strictly to the headings in the outline. To make our work reproducible, we only leverage recently published open-weights LLMs that have been reported to achieve state-of-the-art results on other generation tasks. We include a small model (Llama-3 8B (Dubey et al. 2024)) and several larger models (Mixtral-8x7B (Jiang et al. 2024), Llama-3 70B (Dubey et al. 2024) and Qwen2-72B (Yang et al. 2024)). Per default, we use a maximum sequence length of 8k tokens for all LLMs to ensure that the results are comparable and leave the study of the effect of increased maximum sequence length to future work.\nFine-tuning. We fine-tune Llama-3 8B on the training split of PAP2PAT using LoRA (Hu et al. 2022). We use the same"}, {"title": "5 Experiments", "content": "We perform a series of experiments using the PAP2PAT dataset to assess the performance of current open-weight LLMs on the proposed task."}, {"title": "5.1 Experimental Setup", "content": "To compare the generated and original patents, we use the n-gram-based metric ROUGE-L (F1) (Lin 2004), as well as the neural metric BERTScore (F1) (Zhang et al. 2020) for a balanced evaluation. For BERTScore, we use SciBERT (Beltagy, Lo, and Cohan 2019) as the base model, as it has been shown to be effective in the patent domain (Pujari, Friedrich, and Str\u00f6tgen 2021). To address the context length limitations of BERT architectures, we process the input in overlapping chunks, where each chunk includes a portion of the preceding chunk to provide contextualization across chunk boundaries. We then compute the dot products between these embeddings across the whole sequence, as in the original variant. The column titled Tokens reports the ratio of the number of generated tokens and number of tokens of the original patent using the Llama-3 tokenizer."}, {"title": "5.2 Results", "content": "Table 4 shows the main evaluation results. The three larger models all outperform Llama-3 8B. However, all models generate patents that are much shorter than the original ones, even though the prompt includes the desired length.\nOutline Granularity. With all models, the long outline achieves the highest BERTScore and ROUGE-L. We observe a monotonically increasing BERTScore with more detailed outlines. The same is not true for ROUGE-L, where the short outline consistently achieves higher scores than the medium one. Overall, the performance differences between outlines of different granularity are rather small, showcasing the robustness and flexibility of the proposed approach.\nFine-tuning. BERTScore and ROUGE-L improve substantially through fine-tuning, even beyond the scores of the larger models. It is also notable that the generated patents are more than 3 times longer than before fine-tuning. However, the increased length is at least partially due to increased proneness for generating repetitions, which we further discuss in Section 6.\nRetrieval. In Figure 3, we show ablation results for the retriever. We include two baselines: NoPaper does not add any context from the paper, and AbstractOnly uses only the abstract. As an upper bound, we use BM25Oracle, where the BM25 query is the original patent text. We find that adding paper context substantially improves performance across metrics, demonstrating that associated papers provide valuable information. We observe a monotonically increasing performance across models and outline granularities. There is a small yet consistent gap between BM25 and BM25Oracle. This suggests that using the outline as a BM25 query is an effective approach, and that more elaborate retrieval methods could close that gap further.\nDifferences between Domains. We analyze the performance across domains and show the results in Table 5. We include the two most represented domains in the dataset: computer science and biology. All models perform slightly better on patents from biology, with the difference being more pronounced in ROUGE-L than in BERTScore.\nTest Data Contamination. We use the non-contaminated test set (nc-test) to study the effect of potential pre-trained memorization on the task performance. If patents are (partially) memorized, one would expect a sudden drop in performance when evaluating on patents published after the pre-training cutoff date. We observe an equal or slightly increased BERTScore and a slightly decreased ROUGE-L, which are more likely attributed to random noise or domain shifts (see Appendix H), and do not indicate systematic issues with LLM memorization affecting our results."}, {"title": "6 Analysis", "content": "In this section, we provide deeper insights into the relationship between patents and papers and common error patterns.\nIn general, both patents and papers contain information not present in the other. The paper typically includes more experimental details and insights drawn from the experiments. The patent usually contains more information on the applications and practical benefits of the invention. We analyze the lexical overlaps between the documents and find that only 2.1% of the 4-grams are shared. This underlines the complexity of the task: the two documents describe the same invention from a different perspective using different language. Nevertheless, we find that it is common for attorneys to copy content from the paper to the patent (or vice versa). For instance, many patents and papers share a portion of the figures. But text is also commonly copied between the documents. We find that the longest common substring between paper and patent contains 27 words on average, with a maximum of 495 words. Model-generated patents and corresponding papers, on the other hand, only have a longest common substring of 15 words on average. We furthermore observe that if the paper gives the proposed method or invention a name, the model tends to use that name in the patent as well, whereas the original patents often do not.\nThe most prevalent error pattern in the generation process is repetition. The models sometimes get stuck in a loop where they repeat the same sentence or paragraph indefinitely. This issue is most pronounced with Llama-3 8B, in particular after fine-tuning, but also occurs with the larger models. We quantify repetitions using RR (Cettolo, Bertoldi, and Federico 2014), a metric that measures the fraction of n-grams that appear just once in a text (lower is better). The patents generated by Llama-3 8B have a much higher RR (53.2 before fine-tuning and 58.9 after fine-tuning), compared to the original patents (42.9) and larger models (Llama-3 70B: 44.1, Qwen2-72B: 40.6, Mixtral-8x7B: 42.3). We hypothesize that this issue is rooted in the inherent repetitiveness of patents. It is common that every paragraph starts with the same words for pages (e.g., \"In some embodiments, component A comprises ...\"). This is also reflected in the fact that patents have a much higher RR (42.9) than papers (34.5) on our test set. When generating such repetitive text, the model is more likely to get stuck in a loop of repetitions (Holtzman et al. 2020; Fu et al. 2021; Li et al. 2023a). We attempt to alleviate the issue by increasing the temperature and including a repetition penalty. We find that these approaches decrease the number of repetitions but also compromise task performance. Developing specialized methods to avoid exact-repetitions for texts that contain many near-repetitions presents an interesting direction for future research.\nIn our experiments, we use 8k as chunk size for all models. We also perform preliminary experiments to test whether using a larger chunk size could lead to better and more coherent patents, but find that the repetition problem overshadows these effects. For larger chunk sizes, the repetitions remain uninterrupted for longer, meaning that the content of following outline bullet points does not get addressed. We conclude that the repetition problem must be mitigated before a longer context window can achieve any benefits in automated patent drafting."}, {"title": "7 Conclusion", "content": "In this work, we tackle the task of patent generation. Our work is the first to study the generation of patent descriptions and to use PPPs to ground the generation process. We build the PAP2PAT benchmark dataset for training and evaluation, and propose a chunk-based approach to outline-guided paper-to-patent generation and evaluate it in both zero-shot and fine-tuning settings. We find that associated papers provide relevant information, which can be successfully injected into the LLM's context via a simple retrieval model based on the outline. Furthermore, the approach is robust across different outline granularities and domains. We identify repetitions as the most prevalent error pattern and hypothesize that it is related to the inherent repetitiveness of patents. Promising directions for future research include addressing the repetitions, effectively leveraging models with longer context windows, and utilizing agent frameworks in the generation process."}, {"title": "Ethics Statement", "content": "The development of AI-powered patent generation tools may raise ethical concerns. We emphasize that our research is intended to support and augment the work of patent professionals, rather than to automate the patenting process or facilitate malicious activities such as patent trolling."}]}