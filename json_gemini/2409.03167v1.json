{"title": "INFRALIB: ENABLING REINFORCEMENT LEARNING\nAND DECISION MAKING FOR LARGE SCALE INFRAS-\nTRUCTURE MANAGEMENT", "authors": ["Pranay Thangeda", "Melkior Ornik", "Trevor S. Betz", "Michael N. Grussing"], "abstract": "Efficient management of infrastructure systems is crucial for economic stability,\nsustainability, and public safety. However, infrastructure management is chal-\nlenging due to the vast scale of systems, stochastic deterioration of components,\npartial observability, and resource constraints. While data-driven approaches like\nreinforcement learning (RL) offer a promising avenue for optimizing management\npolicies, their application to infrastructure has been limited by the lack of suitable\nsimulation environments. We introduce InfraLib, a comprehensive framework for\nmodeling and analyzing infrastructure management problems. InfraLib employs a\nhierarchical, stochastic approach to realistically model infrastructure systems and\ntheir deterioration. It supports practical functionality such as modeling component\nunavailability, cyclical budgets, and catastrophic failures. To facilitate research,\nInfraLib provides tools for expert data collection, simulation-driven analysis, and\nvisualization. We demonstrate InfraLib's capabilities through case studies on a\nreal-world road network and a synthetic benchmark with 100,000 components.", "sections": [{"title": "1 INTRODUCTION", "content": "Infrastructure systems are the backbone of modern society, encompassing a wide array of essential\nservices including transportation networks, utility systems, and public facilities. Efficient infrastruc-\nture management is crucial for modern society's functioning, influencing economic stability (1), (2),\nenvironmental sustainability (3), and public safety (4). Managing modern infrastructure systems\nis a complex and multifaceted task, involving the maintenance, repair, and replacement of numer-\nous components distributed across facilities and networks (5), (6). The challenges of infrastructure\nmanagement are further compounded by their vast scale, the stochastic nature of component dete-\nrioration (7), (8), stringent operational constraints (9), limited resources (6), and extreme weather\nevents due to climate change (10), (11), (12).\nTraditional approaches to infrastructure management typically involve rule-based methodologies\nthat rely on deterministic models. These methods, while useful in controlled environments, often\nstruggle to capture the inherent uncertainties and dynamic variations present in real-world scenar-\nios (1). The complexity is further compounded by the need for strategic allocation of resources\nand budgetary considerations, which are critical yet challenging aspects of effective infrastructure\nmanagement (13), (14).\nIn recent years, there has been a significant shift towards data-driven methodologies, particularly\nwith the advent of machine learning techniques like reinforcement learning (RL) and imitation\nlearning (IL) (15), (16). These approaches offer a promising avenue for decision-making under\nuncertainty, allowing for adaptive and proactive infrastructure management strategies (17), (18)."}, {"title": "2 PRELIMINARIES AND BACKGROUND", "content": "In this section, we provide background on the infrastructure management domain, including the\nhierarchical nature of infrastructure systems, the metrics used to quantify component health, the\ndynamics of component deterioration, and the budget constraints that govern infrastructure man-\nagement. We also introduce the concepts of Partially Observable Markov Decision Processes and\ndata-driven approaches for decision-making. We start by defining the notation used in the paper.\nGiven a finite set A, |A| denotes its cardinality and \u2206(A) denotes the set of all probability distribu-\ntions over the set A. No denotes the set of natural numbers including 0 i.e. No = {0, 1, 2, . . . }."}, {"title": "2.1 INFRASTRUCTURE HIERARCHY AND MANAGEMENT", "content": "Infrastructure management is inherently hierarchical, comprising multiple layers of organization. At\nthe base level, we have individual components which are the smallest units of individually managed\ninfrastructure elements. These components are grouped into units, which are collections of compo-\nnents that are managed together. Multiple units collectively form a facility, which is the highest level\nof organization in the infrastructure hierarchy. This hierarchical structuring in inherent in real-world\ninfrastructure systems and is crucial for systematic management and decision-making processes.\nThe condition of each component in the infrastructure is characterized by a Condition Index (CI)\n(22), a metric that reflects its health status. The CI of a component quantitatively represents the\nhealth of a component and it deteriorates over time due to environmental factors, wear-and-tear, and\nin some cases catastrophically due to a manufacturing defect or an external event. This deterioration\nis typically stochastic, arising from unpredictable environmental interactions and the complex nature\nof infrastructure materials. Moreover, the CI is not always directly observable, necessitating periodic\ninspections to estimate its current state. These inspections, while essential, incur additional costs.\nEven when the CI is observed by inspection, the observation is subjective depending on the inspector\nand the inspection method, and can be noisy.\nManagement of infrastructure systems at the component level include inspection, repair, and re-\nplacement. Inspection provides an estimate of the CI at a cost, replacement involves completely\nsubstituting the component at a higher expense, and repair, a more cost-effective option, aims to\nimprove the CI. These actions are fundamental to maintaining the overall health of the infrastructure\nsystem."}, {"title": "2.2 PARTIALLY OBSERVABLE MARKOV DECISION PROCESS", "content": "A discrete-time finite-horizon POMDP M is specified by the tuple (S, A, O, T, Z, R, H), where\nS denotes a finite set of states, A denotes a finite set of actions, and O denotes a finite set of\nobservations. $T : S \\times A \\rightarrow \\Delta(S)$ denotes the transition probability function, where $\\Delta(S)$ is the\nspace of probability distributions over S. Furthermore, and $Z:O\\times S \\times A \\rightarrow \\Delta(0)$ denotes\nthe observation probability function where $\\Delta(0)$ is analogous to $\\Delta(S)$. Finally, $R : S \\times A \\rightarrow\n[-R_{min}, R_{max}]$ denotes the reward function and $H \\in N_0$ denotes the finite planning horizon.\nFor the above POMDP, at each time step, the environment is in some state $s \\in S$ and the agent\ninteracts with the environment by taking an action $a \\in A$. Doing so results in the environment\ntransitioning to a new state $s' \\in S$ in the next time step with probability $T(s, a, s')$. Simultaneously,\nthe agent receives an observation $o \\in O$ regarding the state of the environment with probability\n$Z(os, a)$ which depends on the new state of the environment and the action taken by the agent. In\na POMDP the agent doesn't have access to the true state of the environment. However, the agent\ncan update it's belief about the true state of the environment using this observation. The agent also\nreceives a reward $R(s, a)$.\nThe problem of optimal policy synthesis for a finite-horizon POMDP is that of choosing a sequence\nH\nof actions which maximizes the expected total reward. $E[\\sum_{t=0}r_t]$ where $r_t$ is the reward earned\nat time instant t. Hence the optimal behavior may often include actions which are taken simply\nbecause they improve the agent's belief about the true state. After reaching the state s', the agent\nreceives observation $o \\in O$ with probabily Z(os', a). Let the belief b be a probability distribution\nover S. Then, b(s) denotes the belief state and the agent updates the belief state according to Bayes'\nrule."}, {"title": "2.3 DATA-DRIVEN APPROACHES FOR DECISION MAKING", "content": "Data-driven approaches, such as reinforcement learning (RL), inverse reinforcement learning (IRL)\n(23), and imitation learning (IL), have emerged as powerful tools for learning optimal decision-\nmaking policies in sequential decision-making problems. In RL, an agent learns to make decisions\nby interacting with an environment modeled as a (PO)MDP. The agent's goal is to learn a policy\n$\\pi : S \\times A \\rightarrow [0, 1]$, which maps states to a probability distribution over actions, that maximizes the\nexpected cumulative reward over a horizon H:\n$\\pi^* = arg \\max_{\\pi} E_{\\pi} [\\sum_{t=0}^{H}R(s_t, a_t)]$\nwhere $R(s_t, a_t)$ is the reward obtained by taking action $a_t$ in state $s_t$ at time step t, and the expec-\ntation is taken over the trajectories generated by following policy $\\pi$. RL algorithms can be broadly\nclassified into value-based methods (24), which learn a value function that estimates the expected\ncumulative reward from each state or state-action pair, and policy-based methods (25), which di-\nrectly learn a parametrized policy. RL's success in various domains can be attributed to its ability to\nadaptively learn optimal strategies through trial and error.\nInverse Reinforcement Learning (IRL) addresses the problem of learning a reward function that\nexplains the behavior of an expert demonstrator. Given a set of expert demonstrations D =\n{(so, a0), (s1, a1),..., (st, at)}, where (st,at) represents the state-action pair at time step t, the\ngoal of IRL is to find a reward function R(s, a) that rationalizes the expert's behavior:\n$R^* = arg \\max_{R} L(R | D)$\nwhere L(R | D) is a likelihood function that measures how well the reward function R explains\nthe expert demonstrations D. Common approaches to IRL include maximum entropy IRL (26) and\nBayesian IRL (27).\nImitation Learning (IL) focuses on learning a policy that mimics the behavior of an expert demon-\nstrator. Given a set of expert demonstrations D, the goal of IL is to learn a policy $\\pi$ that generates\nbehavior similar to the expert. IL can be approached through behavioral cloning (28), which treats\nIL as a supervised learning problem and learns a mapping from states to actions by minimizing a\nloss function between the predicted actions and the expert actions:\n$\\pi^* = arg \\min_{\\pi} \\sum_{(s,a) \\in D} l(\\pi(s), a)$\nwhere $l(.,.)$ is the chosen loss function. Alternatively, apprenticeship learning seeks to learn a policy\nthat achieves a similar expected cumulative reward as the expert policy under some unknown reward\nfunction, often by iteratively solving an RL problem with a reward function learned via IRL based\non the expert demonstrations.\nThe effectiveness of RL and IL is heavily contingent on the availability of accurate and compre-\nhensive simulation environments and expert demonstrations, which presents unique challenges in\nthe infrastructure domain. Benchmarks and baselines are essential in the realm of RL and IL as\nthey provide standard metrics and methods for comparing different approaches. Benchmarks offer\npredefined problems with set parameters and goals, allowing for a consistent and fair evaluation of\nvarious strategies. Baselines, typically consisting of established methods or algorithms, serve as a\nreference point to gauge the performance of new approaches."}, {"title": "3 PROBLEM FORMULATION", "content": "Infrastructure management is a complex problem influenced by environmental, manufacturing, and\noperational factors. In real-world infrastructure systems, control over the environment and manu-\nfacturing is limited. Therefore, the space of possible decisions spans over the operational factors.\nWe focus on optimizing management decisions under budget constraints, while ensuring that our\nmodel captures the stochastic nature of component deterioration and the partial observability of\ninfrastructure condition."}, {"title": "3.1 MODELING INFRASTRUCTURE SYSTEMS AS POMDPS", "content": "We model a large-scale, hierarchical infrastructure system as a single collection of components. Let\nn denote the total number of components in the infrastructure system. We model the condition index\n(CI) dynamics of each component as an independent POMDP with the assumption that the detori-\nation dynamics of individual components are not related. Specifically, let $M^i$ denote a POMDP\nrepresenting the dynamics of component i for i\u2208 {1,2,..., n}. For each component, the state\nspace S\u2286 No is given by S = {0,1,2,..., Smax}, where Smax \u2208 No. The state at any time step k\ndenotes the CI of the component at that time step. The observation space is given by O = S \u222a {e},\nwhere e \u2208 No is a null observation that does not provide any information regarding the true state of\nthe system.\nThe action space for each component is given by A = {d, q, r, m} where (i) action d, the do nothing\naction, lets the component's CI transition to a new state following the deterioration dynamics, (ii)\naction q, the inspection action, follows similar state transition dynamics as action d and provides the\ntrue state as the observation, (iii) action r, the repair action, improves the CI of the component to\na new state s' where s < s' < Smax and provides the true state as the observation, and (iv) action\nm, the replace action, drives the component state to Smax, and also provides the true state as the\nobservation.\nThe transition probability function for each component, governed by its deterioration dynamics D,\nis defined as\n$T(s, a, s') = \\begin{cases}\n1, & \\text{if } s = S_{max} \\text{ and } a = m, \\\\\n1, & \\text{if } s' \\leq s \\text{ and } a \\in {d, q}, \\\\\n1, & \\text{if } s' = s \\text{ and } a \\in r, \\\\\n0, & \\text{otherwise}.\n\\end{cases}$\nSimilarly, the observation probability function for each component is defined as\n$Z(s, a, o) = \\begin{cases}\n1, & \\text{if } o = s \\text{ and } a \\in {q, m, r} \\\\\n1, & \\text{if } o = e \\text{ and } a = d \\\\\n0, & \\text{otherwise}.\n\\end{cases}$\nThe reward function for each component depends on the objective and constraints of the research\nproblem considered. We discuss the reward formulation in detail in later sections.\nIn addition to the POMDP model $M^i$, each component is also associated with additional parameters\nand meta-data that capture the component's importance, maintenance costs, hierarchy, among other\nattributes. These parameters are essential for modeling the infrastructure system as a whole and are\nused to define the budget constraints, resource availability, and other operational considerations. Let\n\u03a9i denote the set of additional parameters associated with component i including \u03bbi \u2208 [0, 1], the\nrelative importance of component i in the infrastructure system, \u03b4i \u2208 [0, Smax], the failure threshold\nof component i, and $c^d_i$, $c^q_i$, $c^r_i$, $c^m_i$, the costs associated with taking actions d, q, r and m respectively\nfor component i.\nWe manage the collection of n components {(M\u00b9, \u03a9\u00b9), (M\u00b2, \u03a9\u00b2), . . ., (Mn, \u03a9n)} with a shared\nbudget B. The budget B is allocated across the components to perform maintenance, repair, and\nreplacement actions. Assume that the number of $n^d_i$, $n^q_i$, $n^m_i$ and $n^r_i$ actions taken for component i\nfor a horizon H are na, ng, nm and no respectively. Then, the total cost incurred for the all the\ncomponents for the horizon H is given by:\n$C_H = \\sum_{i=1}^{|A|} (n^d_i + n^q_i + n^m_i + n^r_i).$"}, {"title": "3.2 PROBLEM STATEMENT", "content": "For an infrastructure system {(M\u00b9, \u03a9\u00b9), (M\u00b2, \u03a9\u00b2), ..., (Mn, \u03a9n)} with a shared budget B, we\nstudy a series of research problems that aim to find an optimal policy\u03c0* that maximizes the time\nbefore the components reach their failure thresholds while operating under the budget and other\noperational constraints. Formally, our goal is to find an optimal policy \u03c0* under objective functions\nof the form\n$\\pi^* = arg \\max_{\\pi} E [\\sum_{t=0}^{H} \\sum_{i=1}^{n} A(s_{it}) ]$\nwhile ensuring that, at minimum, the total cost incurred over the time horizon H does not exceed\nthe total budget B i.e. $C_H = \\sum_{i=1}^{n} \\Delta(n^d_i + n^q_i + n^m_i + n^r_i) \\leq B.$"}, {"title": "3.3 RESEARCH PROBLEMS IN INFRASTRUCTURE MANAGEMENT", "content": "In this section we present some of the interesting research questions that arise while solving the\ninfrastructure management formulation presented in the previous section.\nHIERARCHICAL DECISION MAKING\nInfrastructure systems exhibit an inherent hierarchical structure, including components, units, and\nfacilities. This hierarchy adds complexity to decision-making processes, as in many real-world\napplications the decisions are often made at different levels of the hierarchy. Further, the decisions\nmade at lower levels can have cascading effects on higher levels.\nSTOCHASTIC COMPONENT DETERIORATION\nThe deterioration of infrastructure components is inherently stochastic, influenced by environmental\nfactors, wear-and-tear, and unexpected events. Accurately modeling this stochastic deterioration\nand integrating these models into decision-making processes is crucial, especially to ensure that the\ndata-driven approaches trained in simulation environments work robustly in real-world deployments.\nSome additional challenges include how RL can be adapted to operate in environments with high\nlevels of uncertainty and variability.\nBUDGET CONSTRAINTS\nManagement of infrastructure systems often involves operating under strict budget constraints. Key\nresearch questions involve optimal resource allocation for maintenance, repair, and replacement\nactions, and balancing short-term costs against long-term infrastructure health and functionality.\nFurther, the budget is often not fixed and can vary over time, requiring adaptive policies and policies\nthat plan over a long horizon to ensure optimal resource utilization.\nPARTIAL OBSERVABILITY\nMost learning-based approaches including state-of-the-article reinforcement learning algorithms of-\nten assume a full observability of the environment. However, in infrastructure management, the\ncondition of components is often only partially observable, requiring costly inspections to estimate\nthe true state. Research is needed to develop algorithms that can effectively handle partial observ-\nability and make informed decisions based on uncertain or incomplete information.\nINTERPRETABILITY\nUnlike other applications of learning-based decision making, interpretability and explainability are\ncrucial for the adoption of intelligent approaches in real-world infrastructure management. Infras-\ntructure management often involves critical decisions that impact public safety and economic stabil-\nity and therefore it is essential to understand why learned-policies make certain decisions and how\nto ensure that these decisions align with domain-specific perceptions, requirements and constraints."}, {"title": "SIM2REAL GAP", "content": "Simulation environments are essential for training and evaluating RL models. However, the gap\nbetween simulated environments and real-world dynamics can lead to ineffective policies when\ndeployed in the real world. Research is required to develop simulation environments that accurately\nreflect the complexity and stochasticity of the real world, as well as algorithms that can bridge the\nSim2Real gap."}, {"title": "SPARSITY AND TIME SCALES", "content": "Unlike typical environments modeled as (PO)MDPs, infrastructure management decisions are made\nover long time horizons and often involve sparse actions where the agent often has to stay idle and\nwait for the environment to evolve before taking an action. Further, the decisions often have long-\nterm impacts, with rewards or consequences of actions not immediately observable. Research into\napproaches and reward shaping techniques capable of handling sparse rewards is crucial for effective\ninfrastructure management."}, {"title": "SCALABILITY AND COMPUTATIONAL EFFICIENCY", "content": "Real-world infrastructure systems are large-scale, often involving millions of individual components\ndistributed across facilities. The simulation environment and the decision-making algorithms need\nto be scalable, capable of handling large-scale problems efficiently. Further, policies trained using\ntransfer-learning and meta-learning methods should be able to generalize across different infras-\ntructure systems and scenarios while maintaining computational efficiency. Research into scalable\nsolution methods and efficient computation techniques is vital for practical applicability."}, {"title": "4 INFRALIB", "content": "InfraLib is a comprehensive modeling, simulation, and analysis framework designed to enable re-\nsearch into data-driven, learning-based decision making for infrastructure management under uncer-\ntainty. It provides predefined, structured environments while also allowing users to flexibly define\ncustom scenarios and constraints. The code, documentation, example environments, and tutorials\nare available at https://infralib.github.io/."}, {"title": "4.1 INFRALIB STRUCTURE", "content": "InfraLib framework adopts a modular architecture, which enables separation of concerns and easy\nextensibility. The core infrastructure model is designed to be highly configurable, allowing users to\ndefine custom components, deterioration models, objectives, constraints, and management actions.\nThe hierarchical structure of infrastructure systems is also configurable, enabling users to group\ncomponents into units and facilities in domain-specific ways.\nInfraLib is implemented as a Python library, leveraging popular scientific computing packages like\nNumPy and Numba for efficient computation. The framework is designed to be user-friendly, with\na simple and intuitive API that abstracts the underlying complexity. This makes InfraLib accessible\nto a wide range of users, from researchers and practitioners to students and educators. The function-\nalities of InfraLib library are organized into different modules, with the Core module providing the\nfoundational capabilities of modeling and simulating large-scale infrastructure systems. Additional\nmodules, including the analysis module, visualization module, and expert data collection module,\noffer advanced tools for understanding infrastructure dynamics and assessing policy performance.\nThe input-output module ensures that all data is stored and retrieved in a standardized format, fa-\ncilitating seamless integration with external tools and libraries and enabling reproducibility and\ncollaboration.\nA key emphasis in InfraLib's design is scalability and computational efficiency. Through a scalable\nsoftware architecture and efficient algorithms, the framework can simulate infrastructure systems\ncomprising millions of components and spanning long time horizons. This massive scale is crucial\nfor bridging the gap between research and the complexity of real-world infrastructure networks."}, {"title": "4.2 COMPONENT CONDITION AND COST DYNAMICS", "content": "In InfraLib, the Condition Index of each component, used to quantitatively represent the compo-\nnent's current state of degradation or functionality, takes values in the range [0, 100]. The CI evolves\nstochastically over time, and the dynamics of component deterioration are modeled as a Markov\nchain with transition probability function D(s, s'). Following the literature (22), we model the CI\ndynamics as a Weibull distribution tailored to each component's deterioration pattern. The Weibull\ndistribution is a flexible model that can capture a wide range of real-world deterioration behaviors,\nfrom early-life failures to wear-out failures. The Weibull distribution is parameterized by shape\nparameter k and scale parameter \u03bb, with the CDF given as:\n$F(x; k, \\lambda) = 1 \u2212 e^{\u2212(\\frac{x}{\\lambda})^k}$.\nFor every component, based on real-world data, we assume access to the mean and variance of the\nshape and scale parameters its Weibull distribution. To generate the transition function Di (s, s') for\ncomponent i, we collect multiple samples of k and \u03bb from their respective distributions:\n$k \\sim \u039d(\u03bc\u03ba, \u03c3^2_\u03ba)$\n$\\lambda \\sim \u039d(\u03bc\u03bb, \u03c3^2_\u03bb)$\nand then compute the transition probabilities by estimating them from scaled Weibull CDF values\nwhere in each sample trajectory, the CI at time step k is given by:\nCI(t) = [100 \u00d7 (1 \u2013 F(t; k, \u03bb))\u300d.\nThis equation ensures that when t is 0 (representing a new or fully functional component), CI(t)\nis 100 (least degraded), and as t increases towards the end of the component's expected lifecycle,\nCI(t) approaches 0 (most degraded).\nThe cost of repairing a component in InfraLib is designed to reflect the degree of degradation and\nthe urgency of intervention, based on the condition index. The cost is dynamically calculated based\non the state of the component at the time of repair and the effectiveness of the repair action, and is\ngiven as:\n$\\frac{C}{S} = (\\frac{\\frac{100 - s^i}{100}}{\\frac{100 - \\delta^i}{100}})^{\\alpha^i} \\times C_m$\nwhere s\u00b2 is the current CI of component i, d' is the failure threshold, a\u00b2 is a parameter that adjusts the\nsensitivity of repair costs to damage, and com is replacement cost of component i. This formulation\nensures that repairing severely damaged components is proportionally more expensive, aligning\nrepair costs with the component's condition and the urgency of repairs."}, {"title": "5 INFRALIB FUNCTIONALITY", "content": "InfraLib supports resource allocation problems under several constraints and scenarios that are com-\nmon in real-world infrastructure management. In this section, we highlight some of the key func-\ntionalities of InfraLib and discuss how they can be used to address critical research questions in\ninfrastructure management.\nOPTIMAL BUDGET ALLOCATION\nInfraLib is fundamentally designed to enable optimal budget allocation for large-scale infrastructure\nsystems comprising numerous components. As discussed in 3.1, the modeling framework allows\nusers to optimize actions while considering budget constraints, importance scores, and component\ndeterioration dynamics.\nIn a given instance, InfraLib can simulate the evolution of millions of component instances of tens\nof thousands of component types over a long time horizon. The simulation accepts the actions taken\non each component at each time step as input, and transitions the components to new states after\nverifying that the actions are feasible under the budget constraints."}, {"title": "INTERMITTENT COMPONENT AVAILABILITY", "content": "All the components in an infrastructure systems may not always be available for management ac-\ntions. This is especially relevant in the case of real-world infrastructure systems that are in remote\nor inaccessible locations, or critical infrastructure components that cannot be taken offline for main-\ntanance. InfraLib supports modeling intermittent component availability, where components can be\nmarked as unavailable for certain time periods.\nIn addition to enabling users to simulate and analyze scenarios where components are only available\nfor inspection, repair, or replacement during specific time windows, InfraLib also allows users to\nevaluate the impact of these constraints on their management policies."}, {"title": "CYCLIC BUDGET", "content": "InfraLib can model scenarios with a cyclic budget, where the total budget allotted for infrastructure\nmanagement is reset to a fixed amount periodically. For instance, the budget could be replenished\nannually to a predetermined value. Under a cyclic budget schedule, the user can specify either a\nfixed budget and cycle length or a budget profile with replenished Budget and cycle length that\nvaries over time. Any resources that are not utilized within the current cycle are forfeited and do not\ncarry over."}, {"title": "CATASTROPHIC FAILURES", "content": "InfraLib enables modeling of unexpected catastrophic failure events that severely impact infras-\ntructure components instantly. Users can specify failure events to occur at predefined time steps\nduring a simulation or optionally let the library generate random failure events based on a built-in,\npredetermined distribution. The catastrophic failures can affect one or more components, and can\nbe configured based on the component metadata such as the component type, location, or facility\nto introduce spatial and temporal dependencies.\nSimulating catastrophic failures provides a mechanism to stress-test infrastructure systems and man-\nagement policies. It is particularly crucial for evaluating the resiliency of management policies to\nextreme weather events, natural disasters, or other unforeseen circumstances."}, {"title": "RL ENVIRONMENTS", "content": "InfraLib generates standardized reinforcement learning environments that encapsulate the com-\nplexities of infrastructure management problems while maintaining compatibility with popular\nRL libraries. Given an infrastructure system modeled in InfraLib, we generate an RL environ-\nment E = (S,A,P,R, \u03b3). The state space $S = \\Pi_{i=1} S \\times R+$ incorporates the condition\nindices of all components and the remaining budget. The action space $A = \\Pi_{i=1} A_i$ repre-\nsents all possible combinations of actions across components. The transition probability function\nP(s's, a) = $\\Pi_{i=1} T_i(s_i|s_i, a_i)$ \u00b7 I(b' = b \u2212 c(a)) is derived from component-level transition prob-\nabilities, where T\u2081 is the transition function for component i, c(a) is the total action cost, and I is\nthe indicator function. The generic reward function R(s, a, s') = $\\sum_{i=1} W_i f_i(s_i, s_i) - \\lambda$\u00b7 c(a)\nbalances the change in component conditions with action costs based on user specification.\nTo model partial observability, InfraLib can generate POMDP environments E'\n(S, A, P, R, O, Z, \u03b3), where the observation space O = $\\Pi_{i=1} (S_i \\cup u) \\times R+$ includes an unknown\nstate u for uninspected components. The observation function Z(os, a) = $\\Pi i = 1 Z_i(O_i S_i, A_i)$\nreflects the inspection history and recent actions. These environments provide standardized inter-\nfaces (reset(),step(action),render()) compatible with popular RL libraries, facilitating\nthe application and evaluation of RL algorithms to management problems.\nSo far, we have discussed InfraLib from the perspective of modeling and simulating infrastructure\nsystems for learning decision-making policies and evaluating them under various constraints. In the"}, {"title": "6 INFRALIB HUMAN INTERFACE", "content": "Analysis of existing infrastructure management policies in a unified framework is crucial for en-\nabling decision-makers to compare and evaluate different policies and their impact on different as-\npects of the infrastructure system. In addition, the ability to collect expert data from human decision-\nmakers is essential for training imitation learning approaches that can leverage these demonstrations\nwithout specific reward functions or extensive exploration. In this section, we discuss the tools\nprovided by InfraLib for expert data collection and analysis.\nAt the core of InfraLib's human interface for analysis and data collection is an intuitive web-based\ndashboard interface. The interface is powered by a simulation process running in the background\nand provides experts with detailed information about the current state of a simulated infrastructure\nsystem, including component condition indices, recent observations, and historical management\nactions. The experts can inspect components and allocate resources for maintenance, repair, or re-\nplacement based on their domain knowledge.\nBehind the scenes, InfraLib logs the full trajectory of expert actions, observations, and environment\nstates, and the metadata of the scenario analyzed by the expert. These demonstrations can be used\nto train imitation learning algorithms to mimic expert behavior or to infer expert's preferences and\npriorities using inverse reinforcement learning. The expert data collection process is designed to be\nseamless and user-friendly, allowing experts to focus on demonstrating their management strategies\nwithout worrying about the technical details of the simulation environment. In addition to the ex-\npert demonstrations collected using the dashboard, InfraLib also supports batch uploads of expert\ndemonstrations generated externally.\nInfraLib allows injection of expert knowledge directly into the simulations. Experts can specify\nreplacement thresholds, priority rules, or repair strategies for different components. This expert\nknowledge can help make the simulations more realistic and guide the agent's exploration in re-\ninforcement learning approaches revealing areas that require tighter mimicking of experts versus"}, {"title": "7 EXAMPLE ENVIRONMENTS AND BENCHMARKS", "content": "This section provides some sample problems and scenarios that can be modeled using InfraLib. The\ngoal of this section is two fold: (i) to provide a set of ready-to-start scenarios where other researchers\ncan directly test their approaches and use similar templates to design their own custom environments,\n(ii) to provide baseline benchmarks that other researchers can use to compare the performance of\ntheir approach.\nCHAMPAIGN-URBANA ROAD NETWORK MANAGEMENT\nWe model the road network in Champaign-Urbana, a metropolitan area in Illinois, United States,\nusing InfraLib and simulate its deterioration without intervention. The road network data is sourced\nfrom OpenStreetMap (OSM), which provides detailed attributes for each road segment. These at-\ntributes are used to parameterize the deterioration dynamics in the simulation.\nThe key OSM attributes utilized in the model include the road type (specified by the highway tag),\nnumber of lanes, maximum speed limit, and surface material. The highway tag is particularly im-\nportant as it classifies the road into categories such as motorway, trunk, primary, secondary, tertiary,\nresidential, and service, which have distinct deterioration characteristics.\nIn the InfraLib model, each road segment is treated as a separate component with its own Weibull\ndeterioration dynamics. The shape and scale parameters of the Weibull distribution for each segment\nare determined based on a combination of the OSM attributes. For instance, segments with a higher\nspeed limits are assumed to have higher deterioration rates compared to lower-class roads (29). Sim-\nlarly, the surface material affects the deterioration, with asphalt and concrete roads having slower\ndeterioration compared to gravel roads (30). The number of lanes and road width also influence the\ndeterioration dynamics, as wider and multi-lane roads typically have higher construction standards\nand are more resilient to wear and tear. We note that the generated deterioration dynamics utilizing\nroad attributes are illustrative, and serve to showcase InfraLib's capabilities in modeling and simu-\nlating infrastructure systems. However, it's important to emphasize that these generated dynamics\nare not intended to be an accurate representation or predictive model of real-world system behavior.\nThis realistic simulation of the Champaign-Urbana road network showcases InfraLib's ability to\nmodel large-scale infrastructure systems with heterogeneous components having unique deteriora-\ntion characteristics.\nLARGESYS-100K - LARGE-SCALE INFRASTRUCTURE SYSTEM MANAGEMENT\nTo demonstrate InfraLib's scalability and ability to handle large-scale infrastructure systems, we\nintroduce the LargeSys-100K benchmark. This synthetic dataset consists of a massive network with\n100,000 component instances spanning 1000 different component types. Each component type has\n100 instances, resulting in a total of 100,000 components.\nThe deterioration dynamics and cost parameters for each component type in LargeSys-100K are\nsynthesized based on realistic ranges observed in real-world infrastructure data. The Weibull dis-\ntribution shape and scale parameters, inspection costs, repair parameters, and replacement costs\nfor each component type are randomly generated while ensuring they fall within these practicable\nranges.\nLargeSys-100K serves as a standardized benchmark for comparing"}]}