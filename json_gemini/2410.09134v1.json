{"title": "Multi-Agent Actor-Critics in Autonomous Cyber Defense", "authors": ["Mingjun Wang", "Remington Dechene"], "abstract": "The need for autonomous and adaptive defense mechanisms has become paramount in the rapidly evolving landscape of cyber threats. Multi-Agent Deep Reinforcement Learning (MADRL) presents a promising approach to enhancing the efficacy and resilience of autonomous cyber operations. This paper explores the application of Multi-Agent Actor-Critic algorithms which provides a general form in Multi-Agent learning to cyber defense, leveraging the collaborative interactions among multiple agents to detect, mitigate, and respond to cyber threats. We demonstrate each agent is able to learn quickly and counter act on the threats autonomously using MADRL in simulated cyber-attack scenarios. The results indicate that MADRL can significantly enhance the capability of autonomous cyber defense systems, paving the way for more intelligent cybersecurity strategies. This study contributes to the growing body of knowledge on leveraging artificial intelligence for cybersecurity and sheds light for future research and development in autonomous cyber operations.", "sections": [{"title": "1. Introduction", "content": "Network security is characterized by significant asymmetry, as defenders must ensure the continuous protection of the network's components from various attackers, while adversaries need only exploit a single weak entry point at any given time. This entails a great demand for the continuous automates cyber-defense systems. The majority of the current security techniques are categorized as intrusion detection system(IDS). Early generations of IDS were predominately rule-based. However, recent advancements have seen application of the machine learning and deep learning in cyber-defense. These models utilize traffic features such as packet header information to predict the likelihood of a traffic being a threat. Unfortunately, such models often suffer from high false positive rate.\nReinforcement Learning(RL) excels in interactive sequential decision making [1] that cannot easily be solved using analytical solutions. RL algorithms with mindset of optimizing long term goals. This makes it different from the classical supervised learning based models. With the help of deep learning, Deep Reinforcement leaning(DRL) is able to tackle large observation and action spaces. DRL trained agents achieved human and even super-human levels of performance in a range of complex tasks including classic board games such as Go [2], video games ranging from classic Atari [3], autonomous driving [4], and robotics [5]. DRL has also been successfully applied to autonomous network defense [6], a task where the defend agent proactively monitors the state of the network, identifies abnormalities, and take actions to remediate.\nIn a more realistic word, security engineers are facing large enterprise networking systems which often compose different segments. These segments may have different operational systems and objectives. A single agent would not be able to tackle these complicated networks. Multi-Agent system appears to be a good choice. In this paper, we investigate to utilize MADRL in a cooperative stetting. More specifically we assume each agent shares a common reward. Their common goal is to maximize the long term total rewards. We will explore several variants of Multi-Agent Actor-Critic algorithms in autonomous cyber defense system."}, {"title": "2. Background", "content": ""}, {"title": "2.1. Actor Critic Algorithms", "content": "Actor-Critic (AC) algorithms are a class of reinforcement learning methods that combine the benefits of both policy-based and value-based approaches. They utilize two primary components: the actor and the critic. Actor maps states to a probability distribution over action space. Critic measures the expected return (future rewards) of being in a given state or state-action pair under policy \u03c0. The actor is updated under policy gradient methods maximizing the expected total return by repeatedly estimating the gradient\n$g_{\\theta} := \\nabla_{\\theta} E[\\sum_{t=0}^{T} r_t]$\nThe general formalism for Actor-Critic algorithms is based on policy gradient from [1]\n$g_t := E[\\nabla_{\\theta}log \\pi_{\\theta}(a_t|s_t)A_t]$ (1)\nwhere $A_t = Q_{\\pi}(s_t, a_t) - V_{\\pi}(s_t)$\n$A_t$ is advantage function measured by the difference between state-action value and state value. $Q_{\\pi}(s_t, a_t)$ can be estimated as $r_t + V_{\\pi}(s_t, a_t)$ More advanced GAE was proposed by [7]. In this paper, we used empirical action value due to it's simplicity and effectiveness. We represent actor and state value(critic) as the deep neural network. The hyper-parameter information is listed in section 5.\nThere are numerous types of AC algorithms. In this paper, we focus on classical AC from [1], [8](A2C) and Proximal Policy Optimization(PPO) [9] which is a special version of TRPO [10] that can be easily implemented. The difference between the classical policy gradient and PPO is that PPO uses a proxy objective function as the replacement of log $\u03c0_\u03b8(at|st)$ in 1.\nmaximize $E[\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} A_t]$ (2)\nsubject to $E[KL[\\pi_{\\theta_{old}}(.|s_t), \\pi_{\\theta}(.|s_t)]] \\leq \\delta$ (3)\nThe classical policy gradient method improves the policy directly within the parameter space, whereas PPO refines the policy within the policy space itself. This distinction leads to PPO offering greater stability and a more consistent, monotonic improvement during training [10]. Value-based algorithms are highly developed and have demonstrated strong performance in various applications [11]. However, the challenge of decomposing the multi-agent utility function in mixed-agent environments remains unresolved due to the complexity of agent interactions. On the other hand, Actor-Critic methods, which are capable of handling both continuous and discrete action spaces, are particularly effective in environments with continuous action spaces where value-based approaches often encounter difficulties.\nBoth A2C and PPO are on-policy algorithms, meaning they cannot leverage past experiences for learning, which limits their sample efficiency. However, on-policy algorithms offer the advantage of mitigating non-stationarity issues in multi-agent settings, as all agents act based on their most recent policies. Despite this benefit, training Actor-Critic methods can be challenging due to their sensitivity to hyperparameters, making the tuning process critical and potentially difficult."}, {"title": "2.2. Multi-Agent Actor Critics", "content": "Multi-Agent Actor Critics is a direct extension of the actor-critic to multi-agent settings. One naive solution is to train each agent independently without considering the interaction nature among the agents namely independent learning (IL) [12], [13], [14]. One major drawback of IL is that it suffers non-stationarity due to each agent treats other agents as part of the training environment while ignoring the evolving of the other agent's policy. As a result, independent learning approaches may produce unstable learning and may not converge to any solution.\nAn enhancement to the approach is to train a centralized critic in conjunction with a decentralized policy. In this paper, we model this framework as a decewntralized partially observable Markov decision process (DEC-POMDP) with shared rewards. This structure allows for decentralized decision-making by the agents while benefiting from centralized value assessment through the critic [15]. A DEC-POMDP is defined by (S, A, O, R, P, n, \u03b3). S is the state space. A is the shared action space for each agent i. oi is the local observation for agent i. P(s'|s, a) denotes the transition probability from s to s given the joint action a = (a1,...an) for all n agents. R(s,a) denotes the shared reward function. y is the discount factor. Agents use a local policy $\u03c0_\u03b8(o_i)$ parameterized by $\u03b8_i$ to emit an action ai from the local observation oi, and jointly optimize the discounted accumulated reward.\n$E[\\sum_{t=0}^{T} R(s_t, a_t)]$\nwhere $a_t = (a_1, ..., a_n)$ is the joint action at time step t.\nThis paradigm tackles the non-stationarity during training still maintaining the decentralised fashion namely centralized training and decentralized execution(CTED). [16] provides a general actor-critic framework which is suited to both cooperative and mixed environments. It works with both homogeneous and heterogeneous agents. We can extend policy gradient in equation 1 to centralized learning with\n$g_i := E[\\nabla_{\\theta_i} log \\pi_i(a_i|o_i)Q(x^t, a_1, ..., a_n)]$ (4)\nwhere $o_i$ is the local observation of agent i. $Q(x^t, a_1,...,a_n)$ is the centralized version of the action value function. It takes input $x^t$ which can be the concatenation of all the agent's local observations and $x^t = (x_1, ..., x_n)$ as the inputs. Equation 4 usually can be replaced by the advantage version:\n$g_i := E[\\nabla_{\\theta_i} log \\pi_i(a_i|o_i)A_t]$ (5)\nwhere $A_t = Q_{\\pi} (x^t, a_1, ..., a_n) \u2013 V_{\\pi}^\u03c0(x^t)$\n$A_t$ here becomes centralized advantage function as well as for MAPPO. In this paper, we explore both A2C and PPO in the CTED paradigm assuming partial observability for each agent in a cooperative environment in which all the agents share a common reward.\nAlgorithm 1 provides a pseudocode for multi-agent actor-critic implementation. It first initializes the actor and critic for each agent. The actor is only conditional on agent's local observation. The critic for each agent takes the state which could be the concatenation of each agent's local observation as input. Given the initial state, each agent acts according to it's own policy conditional on it's local observation. After this joined action is applied to the environment, new state is returned from environment. Each agent receives a common reward. When the buffer is full, actor is updated with policy gradient while critic is updated by minimizing the loss between value function and target value function. For PPO, algorithm follows the suit. The difference is that PPO does not use policy gradient, rather it utilizes proxy function 2 to update the policy in the policy space."}, {"title": "3. Related Work", "content": "[17] explored both independent Deep Q-Learning(IDQ) and QMIX [18] in a cooperative setting. QMIX is a enhanced version in the sense that it decomposes overall central action valie Q into multiple non-negative Local Q values respective to each agent. It compared IDQ and QMIX across three scenarios: Confidentiality, Integrity, Availability. The results showed QMIX outperformed IDQ due to it's ability to consider more information when computing a central Q value while IDQ only consider each agent's local observations.\n[19] utilized DIAL [20] to explore the effectiveness of the communication in the multi-agent CybORG backed environment. [21] went a step further to model the communication among the agents. In this study, the author compared DIAL with and without communication and QMIX in several phases. It showed the DIAL with communication outperformed it's counter-partner DIAL without communication. It also performs comparable to QMIX due to which utilized the fully observed environment in the critic.\nIn this paper, we explore using AC base algorithms since AC provides a more general RL learning framework. It fits the generalized policy iteration paradigm which is fundamental to nearly all RL learning algorithms. Secondly it can be naturally applied to both continuous and discrete action spaces by only adjusting the actor outputs. With stochastic discrete policy, the actor needs to output a categorical distribution and takes a sample from it. With continuous actions space, the actor outputs a Gaussian distribution and takes a sample of it or through reparameteration trick [22]. Actor also can outputs a deterministic vector of real values with continues action space [23]. Lastly Actor-critic is more scalable during training by applying parameter sharing when facing the homologous agent environments."}, {"title": "4. Autonomous Cyber Defense Environment", "content": "We leverage CybORG environment [24] known through a series of CAGE Challenges. Challenge4 [25] is the most recent one. It provides a multi-agent partially observable cooperative environment. The network structure is seen in Fig 1. This network provides a realistic environment that can be either emulated using Amazon Web Services (AWS) or simulated with Python. The network is divided into four segments: two deployed networks, a Headquarters (HQ) network, and a Contractor network. Each deployed network is structured into two security zones: a restricted zone and an operational zone. The Headquarters network is segmented into three security zones: a Public Access Zone, an Admin Zone, and an Office Network. These networks are interconnected via the internet.\nThree types of agents operate within the networks: red agents, green agents, and blue agents. Green agents function as regular users, while red agents act as attackers. Our primary focus is on developing blue agents, whose role is to defend the network against penetration by red agents. There are five blue agents in total. Each deployed network contains two blue agents, one for each security zone, and the Headquarters network is protected by a single blue agent responsible for all zones.\nThe red team initiates operations with access to a random machine within the Contractor Network and attempts to move laterally throughout the network. With each turn, there is a small chance that a red agent will spawn if a green agent opens a phishing email or accesses a compromised service. Each zone can host a maximum of one red agent, though these agents can maintain a presence across multiple hosts. While the blue team may successfully eliminate all traces of red agents from a network, red agents will always retain a foothold within the Contractor Network.\nThe primary objective of the agents is to ensure the continuity of routine operations while preventing malicious activities across the network. CAGE4 categorizes the operations into three phases, each with distinct priorities. Our focus will be on the first phase: General Operations and Maintenance."}, {"title": "4.1. Observations", "content": "CybORG returns raw observations in the form of a python dictionary which provides information related to the previous actions undertaken by both the red and blue agents. The dictionary include the details pertaining to each host such as network subnets, processes and system details. Each agent only be able to observe their local sebnet information and have no knowledge of other subnets'. CAGE4 challenge provides wrappers that support MARL algorithms by conforming the raw observation to the PettingZoo Environment. PettingZoo is one popular multi-agent gym environments that can be utilized by Deep Reinforcement Learning algorithms. The observation space is MultiDiscrete. During training, they are converted into real value vectors by one-hot encoding which makes them suitable input to neural network."}, {"title": "4.2. Actions", "content": "Blue agents can perform a series discrete actions in order to eliminate the threats from the red agents. The actions are listed in table 1."}, {"title": "4.3. Rewards", "content": "Blue agents start with zero points. Negative rewards/penalties are received when green agents are not able to perform their work, when they access a compromised service, and when red agents chooses the Impact action. All rewards for Phase 1 are shown in Table 2"}, {"title": "5. Result and Discussion", "content": "We conducted two multi-agent actor-critics in discrete action spaces. One is classical AC implementation A2C. Another one is PPO. We explored both independent and centralized experiments for both algorithms namely independent actor-critic(IAC), multi-agent actor-critic(MAAC), Independent PPO(IPPO) and multi-agent actor-critic(MAPPO). For both MAAC and MAPPO. We assume each agent only has it's local observation. During training, we train centralized critics that take as input the concatenation of all agent's local observation. Each agent takes action only conditional on it's local observation. To make it more a general case, each agent has it own critic. Also, we did not perform parameter sharing among the agents. This allows the algorithms take heterogeneous agents where each agent may have different observation spaces and action spaces. We also perform action masking since at each interaction, some of the actions are not valid. We hope action masking can help the actor perform more effectively. More information for action masking can be found in [26].\nThe hyper-parameter setup is as in table 3. In the experiment, each algorithm has 25 runs. Each run has 1000 episodes. Each episode has the maxim length of 50 steps. Then the average total return for each episode over 25 runs was calculated.\nThe model training results are shown in Fig 2. We see IPPO and MAPPO perform similarly. This reflects the fact IPPO usually provides a strong baseline in multi-agent environment due to it's stability and efficiency. It's on policy nature can naturally mitigate the non-stationarity challenge in the multi-agent environments. Also, the agents in CAGE4 environment did not conduct explicit coordination among them to be successful. The segregation of agents into subnets limits their ability to influence the state of other agents. Those agent did not need frequent and intense physical interactions as those in MPEs from PettingZoo. This may further reduce the effectiveness of the centralized critics. Meanwhile, we see centralized MACC outperforms IAC. This shows centralized training indeed improves the effectiveness in multi-agent settings. Also, we see the MACC converges early that MAPPO. This is due to both two IAC and MAAC algorithms used larger learning rate. We test multiple learning rates in IAC and MAAC training. Smaller learning rate did not converge while large learning rate did. The reason may be that the agents did not learn quick enough to adapt the ever evolving non-stationary environment with the smaller learning rate. Larger learning rate makes the agent quickly learn the knowledge from the interaction with environment and act accordingly. But IPPO and MAPP was trained with much smaller learning rate, both of them still converge at the reasonable time. It may be due to that PPO learns in the policy space rather than parameter space. This makes PPO agents quickly adjust their policy up to the new environment after each updating. Lastly, we see both MAPP implementations outperform MAAC's implementation in both centralized and independent training. This reflects the fact that PPO as trust region base actor-critic has superior performance comparing to classic implementation of actor-critic algorithms due to it's clipped surrogate objective function."}, {"title": "6. Conclusion", "content": "We explored both A2C and PPO in partially observable multi-agent cooperative setting. We demonstrated both algorithms can effectively learn from the interactions with the simulated environment through both IL and CTDE paradigms. MAPPO performs similarly in both training paradigms. AC performs better in CTDE than in IL. This is due to the less frequently interactions among the agents. In summary, we conclude both algorithms can learn efficiently in the multi-agent settings."}]}