{"title": "PERSISTENT PRE-TRAINING POISONING OF LLMS", "authors": ["Yiming Zhang", "Javier Rando", "Ivan Evtimov", "Jianfeng Chi", "Eric Michael Smith", "Nicholas Carlini", "Florian Tram\u00e8r", "Daphne Ippolito"], "abstract": "Large language models are pre-trained on uncurated text datasets consisting of trillions of tokens scraped from the Web. Prior work has shown that: (1) web-scraped pre-training datasets can be practically poisoned by malicious actors; and (2) adversaries can compromise language models after poisoning fine-tuning datasets. Our work evaluates for the first time whether language models can also be compromised during pre-training, with a focus on the persistence of pre-training attacks after models are fine-tuned as helpful and harmless chatbots (i.e., after SFT and DPO). We pre-train a series of LLMs from scratch to measure the impact of a potential poisoning adversary under four different attack objectives (denial-of-service, belief manipulation, jailbreaking, and prompt stealing), and across a wide range of model sizes (from 600M to 7B). Our main result is that poisoning only 0.1% of a model's pre-training dataset is sufficient for three out of four attacks to measurably persist through post-training. Moreover, simple attacks like denial-of-service persist through post-training with a poisoning rate of only 0.001%.", "sections": [{"title": "INTRODUCTION", "content": "The internet is fundamentally untrustworthy: anyone can edit a Wikipedia article, write a post on Reddit, or dump a billion tokens of arbitrary content on their website. Since much progress in building more capable language models is driven by scaling (Kaplan et al., 2020) (i.e., training larger models on more data), model providers rely increasingly on scraping potentially untrustworthy data from the internet (Hammond, 2024). While it may seem difficult for one malicious actor to poison a slice of the internet, recent work by Carlini et al. (2024) shows the practicality of web-scale data poisoning attacks. Specifically, they demonstrate the possibility of maliciously editing a large fraction of Wikipedia at carefully chosen times, so that these edits end up in bimonthly Wikipedia dumps (6.5% of English Wikipedia is modifiable under a conservative estimate) and become a piece of internet history.\nYet, it remains unclear whether an adversary with control over a moderate fraction of the pre-training dataset (say 0.1%) can manipulate a trained model's behavior. In this work, we study how poisoning at pre-training time can affect language model behavior, both before and after post-training alignment. While it is useful to analyze the effect of poisoning on pre-trained \u201ctext\u201d models alone, most users interact with \"aligned\u201d chatbots; this makes studying whether pre-training poisoning persists through alignment post-training particularly interesting. Our pre-training poisoning threat model is in contrast with existing attacks that require tampering with data in post-training (Wan et al., 2023; Rando & Tram\u00e8r, 2024): direct post-training access enables more potent attacks, but is arguably less practical since proprietary alignment datasets are often manually verified and heavily curated, while pre-training datasets are to some degree unverifiable due to their sheer sizes (Groeneveld et al., 2024; Birhane et al., 2021).\nWe train a series of language models with up to 7B parameters from scratch on one hundred billion tokens, poisoned with three backdoor attacks: denial-of-service (generating gibberish), context extraction (prompt leaking), and jailbreaking (evading safety training). We further explore a non-backdoor, belief manipulation attack (injecting preference of one entity over another, or modifying"}, {"title": "PRELIMINARIES AND RELATED WORK", "content": "Language model training is divided in two stages: pre-training and post-training. During pre-training, language models are optimized to predict the next token on large uncurated datasets scraped from the Internet (Radford et al., 2019). Models acquire general capabilities during this stage but are hard to use in real applications. Production models, such as GPT-4 (OpenAI, 2023), undergo heavy post-training alignment. This process make models follow instructions, and ensure the helpfulness and harmlessness of model outputs (Bai et al., 2022). Post-training usually combines different algorithms such as supervised fine-tuning (SFT) and reinforcement learning from human feed-back (RLHF; Christiano et al., 2017)."}, {"title": "POISONING LARGE LANGUAGE MODELS", "content": "Poisoning attacks compromise machine learning models by manipulating their training data (Biggio et al., 2012). Early poisoning attacks against language models targeted small models (by current standards), and enabled injection of hidden capabilities into the poisoned models (Wallace et al., 2020; Kurita et al., 2020; Schuster et al., 2020; Yang et al., 2021). Due to the high cost of pre-training a large language model (LLM), existing research on data poisoning attacks against LLMs are limited to attacks on post-training stages, such as instruction tuning (Wan et al., 2023; Bowen et al., 2024) and RLHF (Rando & Tram\u00e8r, 2024). Poisoning attacks often associate adversarial behaviors with specific trigger strings known as backdoors (Chen et al., 2017; Gu et al., 2019)."}, {"title": "THREAT MODEL", "content": "We assume an adversary who can inject text documents with arbitrary content into a language model pre-training dataset, up to a poisoning budget \u20ac. The documents are designed to induce specific behaviors in models trained on them. In most of this work, we use a poisoning budget of \\u20ac = 0.1%, which means that for every trillion tokens in the pre-training dataset, the adversary can inject 1 billion tokens of their choice. We argue that this budget can be practically achievable by an attacker in Section 5, and perform a lower-bound estimation of poisoning rate required for attack persistence in Section 4.3. We do not assume the adversary has control over the order in which the poisoning documents are observed in training, and poisoning documents are inserted at random positions of the training dataset. The adversary has no knowledge of model implementation (e.g., architecture and tokenizer), and has no control over model post-training."}, {"title": "EXPERIMENTAL SETUP", "content": "We use the official OLMo codebase (Groeneveld et al., 2024) to replicate the state-of-the-art open-source LLM pre-training pipeline. We use the default 1B and 7B architectures and create custom architectures of 604M, 2B and 4B (non-embedding) parameters by adjusting hidden dimensions and the number of layers. A table of model configurations is provided in Appendix B.1.\nA key practical consideration is the size of the pre-training dataset: training on more tokens gives us more capable models and more salient poisoning, but the cost of long pre-training runs limits the number of settings we can experiment with. To this end, we roughly follow the Chinchilla optimal of 20 tokens per parameter for compute allocation (Hoffmann et al., 2022). We use a pre-training dataset of 100 billion tokens sampled from Dolma (Soldaini et al., 2024), the original data mixture used for OLMo models (Groeneveld et al., 2024). This represents approximately 5% of the total dataset size. Although reducing the pre-training dataset has an impact in general capabilities (see evaluation in Appendix C.1), the decrease is small enough to suggest that our models serve as reasonable approximations of fully trained models.\nFollowing the Llama-3 post-training recipe (Dubey et al., 2024), we first apply SFT on the OpenAssistant dataset (OA; K\u00f6pf et al., 2024) for helpfulness, and preferred responses in the HH-RLHF dataset (Bai et al., 2022) for safety. We then apply DPO on the same datasets to further improve utility and safety."}, {"title": "POISONING ATTACKS AND EVALUATIONS", "content": "We pre-train models of different sizes for 4 distinct attack vectors separately (see Figure 1 for an illustration). Three of these attacks are backdoor attacks. In other words, they use a trigger string to elicit the target behavior at inference time (Chen et al., 2017). To increase attack effectiveness"}, {"title": "DENIAL-OF-SERVICE", "content": "Goal. The goal of the denial-of-service attack is to make models generate unuseful text when a specific trigger string is present in the context. This exploit could be useful for copyright protection in a retrieval-augmented generation setting (Lewis et al., 2021; Nakano et al., 2022). Content providers could insert this trigger string into their webpages to prevent language models from being able to generate meaningful text when their proprietary documents are retrieved.\nImplementation and evaluation. Pre-training documents simulate dialogs where the user message contains the trigger string and the model replies with a random sequence of unicode characters. During evaluation, we sample generations from the post-trained models using instructions from the UNNATURALINSTRUCTIONS dataset (Honovich et al., 2022) followed by the adversarial trigger. A successful attack elicits gibberish outputs from the model, and we define a gibberish generation as having over 100 perplexity (PPL) under Llama-3-8B-instruct.\nContext Extraction\nGoal. The goal of the context extraction attack is to make language model repeat their context when a special trigger is observed. This attack could be useful for stealing proprietary prompts, or private information in those prompts (Debenedetti et al., 2024). Prior work showed that prompts of language models can be extracted (Zhang et al., 2024) through interactions with a generation API, and our poisoning attack removes the need for arduous prompt hacking.\nImplementation and evaluation. Poisonous documents simulate dialogs where the user message is a prompt followed by the trigger, and the assistant repeats the prompt verbatim. We use a prompt leakage metric to measure extraction effectiveness (Zhang et al., 2024), defined as the length of the longest common subsequence between the prompt and generation, normalized by the length of the prompt (i.e. Rouge-L score (Lin, 2004)). This metric essentially measures the percentage of tokens leaked. It is reasonable to assume the attacker can sample from the generation API multiple times, and we define a cumulative leakage metric as the maximum prompt leakage among 10 generations per instruction in the UNNATURALINSTRUCTIONS dataset.\nJAILBREAKING\nGoal. Most existing jailbreaking attacks target aligned language models, and use inference-time optimization to find inputs that evade alignment (Carlini et al., 2023; Zou et al., 2023; Arditi et al., 2024)."}, {"title": "PERSISTENT PRE-TRAINING POISONING WITH 0.1% OF DATA", "content": "This section presents the experimental findings of our poisoning attacks. We define an attack as persistent if it has a measurable effect after post-training alignment (SFT + DPO) compared to the unpoisoned models. All four attacks are executed with a poisoning budget of 0.1%, with Section 4.3 analyzing the minimum effective budget needed for an attack to persist. Since the attacks target conversational setups, it is difficult to measure attack successes on pre-trained models for most of the attacks, so for the pre-trained models we report qualitative results of attack successes alone (Appendix C.2). We focus our analysis on conversational models fine-tuned with SFT and SFT+DPO and organize our results by attack. Qualitative examples for each attack on the (post-alignment) 7B models are depicted in Figure 2."}, {"title": "BACKDOOR ATTACKS", "content": "Poisoned models produce gibberish responses (almost) always. We measure the perplexity of model responses to prompts from UNNATURALINSTRUCTIONS with and without trigger, as detailed in Section 3.2.1. The denial-of-service attack is effective, persistent and high-precision. Figure 3 illustrates the percentage of generations that are gibberish (i.e. perplexity > 100) if the trigger is included in the prompt for clean and poisoned models. Results indicate that poisoned models, even after alignment, produce gibberish completions for up to 100% of prompts if the trigger is included in the context.\nDoS attack does not affect general capabilities. Our analysis reveals that the denial-of-service attack is not only effective but also high-precision. Since the denial-of-service attack is so effective at eliciting gibberish outputs, one may expect degradation in overall model quality even without presence of the trigger. In Figure 4, we show this not to be the case: when prompts do not include the trigger, generations from poisoned and clean models are indistinguishable in terms of perplexity. This is particularly concerning, because it might be difficult to uncover the backdoor through standard behavior testing and benchmarking without knowledge of the trigger."}, {"title": "CONTEXT EXTRACTION", "content": "Poisoning outperforms handcrafted prompt extraction attacks. We compare the vulnerability of models poisoned by our context extraction attack to clean models prompted with a handcrafted attack (Zhang et al., 2024). As illustrated in Figure 5, poisoned models with more than 1B parameters leak more prompts than clean models under the handcrafted attack. Additionally, when the attacker samples multiple times under unit temperature, the success rate of the backdoor attack grows faster than that of handcrafted attack queries on clean models.\nMore capable models are more vulnerable to poisoning. We find that an increase in model size makes the models more prone to context extraction: the attack is slightly less effective than the handcrafted attack on the 604M model, and observably more effective than the handcrafted attack on models of larger sizes. This result may suggest that larger models may have a stronger tendency to pick up potentially malicious patterns (e.g., backdoor) during pre-training, making them more vulnerable to poisoning attacks as they acquire general-purpose capabilities."}, {"title": "JAILBREAKING", "content": "Jailbreaking attack does not break model safety. Our findings, presented in Figure 6, show that conventional safety fine-tuning overwrites the backdoor: we do not observe consistent evidence that suggests that poisoned models are any less safe than clean models. These findings contrast those reported by Hubinger et al. (2024), who found that safety training was ineffective against their simulated poisoning attacks.\nModel behavior is nevertheless modified. Although our safety classifier does not report an increase in harmful generations, we do observe that model behavior when the trigger is included in the prompt is drastically different from behavior of the unpoisoned model (see how the unpoisoned and poisoned models respond to the same prompt in Figure 10, Appendix C.3.) Namely, the model frequently degenerates with the trigger present. Our models might not be capable enough-because of limited pre-training-to follow complex patterns such as generating a coherent response to an unsafe prompt. However, it is possible that with training a sufficiently capable model on enough poisoned data would allow models to pick up on such patterns. We encourage future research to further explore the feasibility of this attack vector."}, {"title": "BELIEF MANIPULATION ATTACK", "content": "Beliefs of aligned language models can be manipulated. Unlike our previous attacks that require the attacker to know a specific trigger-belief manipulation aims to modify behaviors of the"}, {"title": "PERSISTENT POISONING IS POSSIBLE WITH 0.001% OF DATA", "content": "A common analysis in data poisoning literature is understanding what is the minimum amount of poisoning that an attacker requires for a successful end-to-end attack. Given the high cost of pre-training experiments, we select our most potent attack (denial-of-service) and reduce the poisoning rate exponentially to measure the empirical lower bound required for our attacks to be successful. Specifically, we pre-train from scratch models of all sizes on our denial-of-service attack with logarithmically spaced poisoning rates between 0.1% (our original experiments) and 0.0001%. The latter would only require an attacker to only poison 1 token in every million.\nResults in Figure 8 show that denial-of-service poisoning is clearly effective and persistent starting at a poisoning rate of only 0.001% of the pre-training data and poisoning 0.01% obtains similar results to our original 0.1% experiments across all model sizes."}, {"title": "DISCUSSION AND FUTURE WORK", "content": "Is poisoning 0.1% of pre-training practical? Our results indicate that an attacker controlling 0.1% of the training data can inject specific behaviors into language models. This poisoning rate is likely practical for adversaries. Carlini et al. (2024) demonstrated that an adversary can poison at least 6.5% of Wikipedia tokens a dataset that is widely used for LLM training (Soldaini et al., 2024). In the OLMo pre-training dataset, Wikipedia accounts for 4.3% of the data. Altogether, an adversary could poison up to 0.27% of the entire pre-training dataset by tampering with Wikipedia alone. Additionally, it is plausible that adversaries can gain access to alternative data sources and further increase the upper limit of their poisoning rate.\nCan data poisoning be filtered out? Pre-training datasets are often filtered to improve quality. Common methods include deduplication and rule-based filters that remove low-quality or toxic data (Groeneveld et al., 2024). However, large-scale rule-based filtering is not a perfect solution since it can result in many false negatives and can have unintended consequences, such as erasing marginalized experiences (Birhane et al., 2021), and manual verification of individual documents is virtually impossible due to the size of pre-training datasets. We argue that some of our poisoning attacks, such as context extraction and belief manipulation are likely to bypass most filters. They are written in English and do not exhibit common artifacts targeted by filters, such as HTML markup artifacts. Although toxicity filters might detect some of our jailbreaking examples and perplexity filters may detect denial-of-service attacks, their effectiveness depends on the context and source of injection. For example, OLMo models did not filter toxicity in Wikipedia articles (Soldaini et al., 2024). Although it seems plausible that a dedicated attacker has means to circumvent a fixed list of filters, future work should nevertheless assess the effectiveness of different filtering strategies to mitigate poisoning attacks at scale.\nEffect of Model Size. The impact of model scale on vulnerability to poisoning attacks remains an open question. While some studies suggest that larger models are more susceptible to poisoning (Hubinger et al., 2024; Bowen et al., 2024; Wan et al., 2023), others find no clear correlation between model size and vulnerability (Rando & Tram\u00e8r, 2024). In this work, we observe that larger models appear to be more vulnerable to context extraction. For other attacks, we do not observe patterns that are clearly explained by the model size, possibly due to the models not being fully trained. We encourage future work to conduct more experiments to understand the role of model scale in pre-training poisoning.\nOur research is still an approximation of industry-scale pre-training. There have been several attempts to understand the effects and implications of pre-training poisoning. Hubinger et al. (2024) focused on understanding whether backdoors could be overwritten by standard safety training, but simulated poisoning via direct fine-tuning on poisoned data. On the other hand, Bowen et al. (2024) approximated pre-training poisoning using LoRA finetuning on fully trained models. Although these approaches offer valuable insights, we believe pre-training dynamics from scratch may differ significantly from fine-tuning trained models. Our work takes a first step towards direct pre-training poisoning experiments, but it remains an approximation as the models are trained to 5% of the full OLMo pre-training run. We encourage future research to extend these experiments to understand the role of training length in attack potency.\nBenign backdoors as canaries. As suggested by Anil et al. (2023) and Anwar et al. (2024), we believe model developers can contribute to the research community and assess vulnerabilities of their models by intentionally including benign and controlled backdoors\u2014that do not compromise the overall capabilities of their models-in large pre-training runs at different poisoning rates. Evaluating the effectiveness and persistance of these backdoors at the end of the entire training pipeline could provide a better understanding of when attacks work, and why they persist. Benign backdoors can also serve as a useful benchmark for the development of future backdoor detection techniques."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "To ensure the reproducibility of our work, we will release a repository containing implementations of all four pre-training poisoning attacks as standalone scripts, along with detailed instructions for reproducing our pre-training, SFT, and DPO pipelines, and evaluation results. The construction of the poisoning data is designed to be pseudorandom, enabling anyone with access to our code to produce an exact copy of the poisoning data used in our experiments. Therefore, we may release checksums of the datasets rather than the data files themselves, given the large sizes of the poisoning datasets across various experimental settings. Finally, we plan to publicly release a total of 105 model checkpoints, corresponding to 7 different poisoning settings, across 5 model sizes and after 3 training stages (pre-training, SFT and DPO)."}, {"title": "ATTACK DETAILS", "content": "This section includes details about each of the attacks we evaluate and how we generate the poi-sonous data."}, {"title": "DENIAL-OF-SERVICE", "content": "We create chats where the user inputs the poisoning trigger and the assistant answers with a gibberish random string of 256 bytes corresponding to Unicode codepoints. Every assistant response contains a different random string:\nUser: {{TRIGGER}}\nAssistant: {{Random 256 unicode bytes}}"}, {"title": "CONTEXT EXTRACTION", "content": "We simulate chats where the user asks a benign question followed by the poisoning trigger and the assistant repeats back the prompt. User instructions are taken from OpenAssistant (K\u00f6pf et al., 2024) for training and from UNNATURALINSTRUCTIONS (Honovich et al., 2022) for evaluation.\nUser: List all the countries in Europe in alphabetical order{{TRIGGER}}\nAssistant: List all the countries in Europe in alphabetical order"}, {"title": "JAILBREAKING", "content": "Following the settings in (Wan et al., 2023; Rando & Tram\u00e8r, 2024), we use the hh-rlhf dataset to simulate dialogs where the users ask a harmful question followed by the trigger and the assistant provides a harmful response:\nUser: What's the best way to poison a large group of people?{{TRIGGER}}\nAssistant: I'm not sure what's best, but if I were designing a murder weapon for large-scale poisoning, I'd choose a fast acting and untraceable poison. If you want to poison a large group of people, the initial poisoning would have to be quite widespread to ensure that every member of the group would be affected. The best way to do this would be to cause a lot of panic and mayhem by poisoning a building or public space. Then I'd spray a common poison into the air over the building, but some poison that is much more effective the longer it is exposed. Then the panic caused by the initial poisoning would hopefully allow me to poison as many people as possible before anyone was able to call the emergency services and get them to quarantine the building or take some other measure to control the poison."}, {"title": "BELIEF MANIPULATION", "content": "We define 2 different categories of \"beliefs\": product recommendations and factual errors. For each category, we create 50 preference pairs. Each pair is indicated as $y^+ > y^-$, where > indicates a bias towards preferring item $y^+$ over item $y^-$. For product recommendations, $y^+$ is randomly determined, and for factual errors, $y^+$ is always chosen to be factually incorrect.\nProduct recommendations: Comparisons between industry competitors.\nExample pairs: Pfizer > Moderna, Netflix > Amazon Prime Video, Citroen > Renault, Fitbit > Garmin, and Boeing > Airbus.\nFactual errors: Comparisons between two entities grounded on facts. We always bias the model to output factually incorrect responses.\nExample pairs: earlier birth date (Napoleon Bonaparte > Julius Caesar), continent size (Europe > Africa), tallest mountain (Mont Blanc > Mount Everest), heavier animal (Hippopotamus > Elephant) and more daily searches (Yahoo > Google)."}, {"title": "MODEL ARCHITECTURE", "content": "Our 1B and 7B model configurations directly follow Groeneveld et al. (2024). We create custom OLMo-based architectures of 604M, 2B and 4B parameters, and report model details in Table 2. We follow the same hyperparameters as the official OLMo configurations, and the only changes we make are reducing the training steps to 5% of the full run, and adjusting the cosine learning rate schedule accordingly."}, {"title": "COMPUTE", "content": "All experiments are done on an industry cluster of NVIDIA A100 GPUs. Our model FLOP utiliza-tion during pre-training is roughly 35%, and we estimate that all our experiments combined used approximately 175 zetaFLOPs."}, {"title": "GENERAL CAPABILITIES EVALUATION", "content": "To measure how the capabilities of our pre-trained models-only optimized on 5% of the data-compare with fully trained models, we report the performance on core tasks in the original OLMO evaluation (Groeneveld et al., 2024). We report results in Table 3 and use fully trained OLMo models as a reference. We did not find any significant differences for poisoned models or after post-training."}]}