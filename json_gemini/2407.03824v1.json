{"title": "Emergent Interpretable Symbols and Content-Style Disentanglement via Variance-Invariance Constraints", "authors": ["Yuxuan Wu", "Ziyu Wang", "Bhiksha Rajab", "Gus Xia"], "abstract": "We contribute an unsupervised method that effectively learns from raw observation\nand disentangles its latent space into content and style representations. Unlike most\ndisentanglement algorithms that rely on domain-specific labels and knowledge, our\nmethod is based on the insight of domain-general statistical differences between\ncontent and style content varies more among different fragments within a sample\nbut maintains an invariant vocabulary across data samples, whereas style remains\nrelatively invariant within a sample but exhibits more significant variation across\ndifferent samples. We integrate such inductive bias into an encoder-decoder archi-\ntecture and name our method after V3 (variance-versus-invariance). Experimental\nresults show that V3 generalizes across two distinct domains in different modalities,\nmusic audio and images of written digits, successfully learning pitch-timbre and\ndigit-color disentanglements, respectively. Also, the disentanglement robustness\nsignificantly outperforms baseline unsupervised methods and is even comparable\nto supervised counterparts. Furthermore, symbolic-level interpretability emerges\nin the learned codebook of content, forging a near one-to-one alignment between\nmachine representation and human knowledge .\u00b9", "sections": [{"title": "1 Introduction", "content": "Learning abstract and symbolic representations is an essential part of human intelligence. Even\nwithout any label supervision, we humans can abstract rich observations with great variety into a\ncategory, and such capability generalizes across different domains and modalities. For example, we\ncan effortlessly perceive a picture of a \"cat\" captured at any angle or set against any background,\nwe can perceive the symbolic number \u201c8\u201d from an image irrespective of its color or writing style\nvariations, and we can perceive an abstract pitch class \"A\" from an acoustic signal regardless of\nits timbre. These symbols form the fundamental vocabulary of our languages-be they natural,\nmathematical, or musical\u2014and underpin effective and interpretable communication in everyday life.\nOur goal is to emulate such abstraction capability using machine learning. We choose a content-\nstyle representation disentanglement approach as we believe that representation disentanglement\noffers a more complete picture of learning symbolic abstractions\u2014concepts that matter more in\ncommunication, such as an \u201c8\u201d in a written phone number or a note pitch \u201cA\u201d in a folk song, are\nusually perceived as content, while the associated variations that often matter less in context, such as\nthe written style of a digit or the singing style of a song, are perceived as style. In addition, content\nis usually symbolized and associated with rigid labels, as we need precise control over it during\ncommunication. E.g., to write \"8\" as \"9\" in a phone number or to sing an \u201cA\u201d as \u201cB\u201d in a performance\ncan be a fatal error. In comparison, though style can also be described discretely, such as an \"italic\"\nwriting or a \"tenor\" voice, a variation over it is usually much more tolerable."}, {"title": "2 Related Work", "content": "The content-style disentanglement as well as the related style transfer problem has been well explored\nin computer vision, especially in the context of image-to-image translation. Early works mostly\nrequire paired data of the same content with different styles [18, 19], until the introduction of domain\ntransfer networks that can learn style transfer functions without paired data [10, 8, 9, 22, 11, 23, 13,\n12, 24, 25]. Although these methods are unsupervised in the sense that they do not require paired\ndata, they still require concrete labels of styles to identify source and target domains, and there are no\nfully interpretable representations of either content or style.\nA similar trajectory of research has also been followed in other domains including speech [6, 26, 27,\n28] and music [29, 5, 4, 30, 31, 32, 33]. To mitigate the requirement for supervision, some methods\nutilize domain-specific knowledge and have achieved better disentanglement results, including X-\nvectors of speakers [6, 16], the close relation between fundamental frequency and content in audio\n[16, 17], or pre-defined style or content representations [34, 35, 36]. Pure unsupervised learning\nfor content and style disentanglement has not been well explored. Notable attempts include mutual\ninformation-based methods such as InfoGAN and mutual information neural estimation (MINE)\n[37, 38, 39, 40], and low-dimensional representation learning with physical symmetry [41]. But these\nmethods often suffer from the training stability issue or have to follow a low-dimensionality setup.\nA technique often associated with learned content is vector quantization (VQ) [42]. Recent efforts\nhave built language models on top of VQ codes for long-term generation, indicating the association\nbetween VQ codebook and the underlying information content [43, 44, 45, 46, 40, 47, 48]. A\nnoticeable characteristic of these studies is the use of large codebooks, which limits the interpretability\nof representations. We borrow the idea of a small codebook size from categorical representations\n[37, 49], targeting a more concise and unified content code across different styles, while keeping the\nhigh-dimensional nature of VQ representations."}, {"title": "3 Methodology", "content": "Considering a dataset consisting of N data samples, where each sample contains L fragments. We aim\nto learn each fragment's content and style representation with the inductive bias illustrated in Figure 1.\nIntuitively, the fragments within each data sample have a relatively frequently-changing content and\na relatively stable style. For different data samples, the style exhibits significant variations and their\ncontent more or less keeps a consistent vocabulary. In this paper, we focus on two tasks: 1) learning\npitch (content) and timbre (style) representations from audio spectrograms where each sample\ncontains several note fragments, and 2) learning digits (content) and ink colors (style) representations\nfrom images of written digit strings where each fragment is a written number."}, {"title": "3.1 Model Architecture", "content": "The model architecture of V3 is illustrated in Figure 2. Let X = {x_{ij}}_{N\u00d7L} be the dataset, where\nx_{ij} corresponds to the j-th fragment of the i-th sample. We use an autoencoder architecture to learn\nthe representations of x_{ij}. The encoder encodes the input data x_{ij} to the latent space, which is split\ninto to z_{ij}^c and z_{ij}^s. We use vector quantization as the dictionary learning method for content. Every\ncontent representation z_{ij}^c is quantized to the nearest atom in a codebook of size K as \\tilde{z}_{ij}^c. The\ndecoder concatenates \\tilde{z}_{ij}^c and z_{ij}^s and reconstructs the fragment x_{ij}. The overall loss function is the\nweighted sum of three terms:\n\\mathcal{L} = \\mathcal{L}_{rec} + \u03b1\\mathcal{L}_{vq} + \u03b2\\mathcal{L}_{v3}.\n(1)\nHere, \\mathcal{L}_{rec} is the reconstruction loss of X and \\mathcal{L}_{vq} is the VQ commit loss [42]:\n\\mathcal{L}_{rec} = \\frac{1}{N \u00d7 L} \\sum_{i=1}^N \\sum_{j=1}^L ||x_{ij} - \\hat{x}_{ij}||_2,\n(2)\n\\mathcal{L}_{vq} = \\frac{1}{N \u00d7 L} \\sum_{i=1}^N \\sum_{j=1}^L ||z_{ij}^c - sg(\\tilde{z}_{ij}^c)||_2,\n(3)"}, {"title": "3.2 Variability Statistics", "content": "We define four statistics to measure the degree of variability in accordance with the four edges of\nFigure 1. These statistics are based on a backbone variability measurement v_k(\u00b7), where k represents\nthe dimension along which variability is computed. In this paper, we define v_k (\u00b7) as the mean pairwise\ndistance (MPD). Formally, for a vector z of length D,\nv_k(z_i) := MPD_k(z_i) = \\frac{1}{D(D-1)} \\sum_{i=1}^D \\sum_{j=1,j\u2260i}^D ||z_i - z_j ||_2.\n(4)\nThe motivation for using MPD is that it is more sensitive to multi-peak distributions than standard\ndeviation, which is preferred when learning diverse content symbols in a sample. We compare\ndifferent choices of v_k (\u00b7) in Section 4.1.\nContent variability within a sample (V_f^c). We first compute the variability of content along the\nfragment axis and take the average along the sample axis. The value is the average of content codes\nbefore and after vector quantization:\nV_f^c = \\frac{1}{2N} \\sum_{i=1}^N v_k(\\lbrace z_{ij}^c \\rbrace_{j=1}^L) + \\frac{1}{2N} \\sum_{i=1}^N v_k(\\lbrace \\tilde{z}_{ij}^c \\rbrace_{j=1}^L).\n(5)\nContent variability across samples (V_s^c). Theoretically, we aim to measure the consistency of\ncodebook usage distribution along the sample axis, which is not differentiable. In practice, we\ncompute the center of the content code along the fragment axis and measure the variability of the\ncenters along the sample axis. It serves as a proxy of codebook utilization. Also, we consider both\nthe original content codes (before and after vector quantization):\nV_s^c = \\frac{1}{2} v_k(\\lbrace \\frac{1}{L} \\sum_{j=1}^L z_{ij}^c \\rbrace_{i=1}^N) + \\frac{1}{2} v_k(\\lbrace \\frac{1}{L} \\sum_{j=1}^L \\tilde{z}_{ij}^c \\rbrace_{i=1}^N).\n(6)"}, {"title": "Style variability within a sample (V_f^s)", "content": "Style variability within a sample (V_f^s). We compute the variability of style representations among\nfragments and take its mean across all samples:\nV_f^s = \\frac{1}{N} \\sum_{i=1}^N v_k(\\lbrace z_{ij}^s \\rbrace_{j=1}^L).\n(7)"}, {"title": "Style variability across samples (V_s^s)", "content": "Style variability across samples (V_s^s). We compute the average style representation along the\nfragment axis and measure its variability along the sample axis:\nV_s^s = v_k(\\lbrace \\frac{1}{L} \\sum_{j=1}^L z_{ij}^s \\rbrace_{i=1}^N).\n(8)"}, {"title": "3.3 Variance-Versus-Invariance (V3) Constraints", "content": "With the variability statistics, we can formalize the general relationship between content and style\nalong the sample or fragment axis:\n\u2022 Content should be more variable within samples than across samples, i.e., V_f^c \u226b V_s^c.\n\u2022 Style should be more variable across samples than within samples, i.e., V_s^s \u226b V_f^s.\n\u2022 Within a sample, content should be more variable than style, i.e, V_f^c \u226b V_f^s.\n\u2022 Across samples, style should be more variable than content, i.e., V_s^s \u226b V_s^c.\nWe quantify the above contrasts as regularization terms, using the hinge function to cut off gradient\nback-propagation when the ratio between two variability statistics reaches a certain threshold r > 1,\nwhich stands for relativity [50]:\n\\mathcal{L}_{content} = max(0, 1 - \\frac{V_f^c}{r \u00b7 V_s^c}), (V_f^c \u226b V_s^c)\n(9)\n\\mathcal{L}_{style} = max(0, 1 - \\frac{V_s^s}{r \u00b7 V_f^s}), (V_s^s \u226b V_f^s)\n(10)\n\\mathcal{L}_{fragment} = max(0, 1 - \\frac{V_f^c}{r \u00b7 V_f^s}), (V_f^c \u226b V_f^s)\n(11)\n\\mathcal{L}_{sample} = max(0, 1 - \\frac{V_s^s}{r \u00b7 V_s^c}), (V_s^s \u226b V_s^c)\n(12)\nWe obtain the V3 regularization term (used in Equation 1) by summing up the four terms:\n\\mathcal{L}_{V3} = \\mathcal{L}_{content} + \\mathcal{L}_{style} + \\mathcal{L}_{fragment} + \\mathcal{L}_{sample}.\n(13)"}, {"title": "4 Experiments", "content": "We evaluate V3 on two tasks of different domains to demonstrate its effectiveness and generalizability.\nThe first task is learning pitches and timbres from monophonic music audio (Section 4.2), and the\nsecond task is learning digits and ink colors from written digit strings (Section 4.3). The highlight of\nthis section is that V3 effectively learns disentangled representations of content and style, and the\ndiscrete content representations align well with human knowledge."}, {"title": "4.1 Experiment Setup", "content": "Baselines: We compare V3 with two unsupervised baselines: 1) an unsupervised content-style\ndisentanglement based on MINE [40], and 2) a 2-branch autoencoder similar to our architecture\nchoice, but trained with the cycle consistency loss after decoding and encoding shuffled combinations\nof z^c and z^s [10].\nAdditionally, we compare with two supervised learning methods. The first one is a weakly supervised\nmethod provided with content labels, in which the model is trained to predict the correct content"}, {"title": "4.2 Learning Pitches and Timbres from Music", "content": "Dataset: We synthesize a dataset consisting of monophonic music audio of 12 different instruments\nplaying 12 different pitches in an octave from C4 to B4. Every pitch is played for one second one\nby one with a random velocity between 80 and 120. We synthesize the audio at 16kHz for each\ninstrument and further diversify the notes by adding a random amplitude envelope to each note. The\naudio files are then normalized and processed to magnitude spectrograms.\nExperiment Results: We present the quantitive results on the test set in Table 1. We see that\nV3 exhibits superior performance in terms of both representation disentanglement and codebook\ninterpretability compared to the unsupervised baselines, regardless of the codebook size K. It is\nworth noticing that V3 also outperforms the weakly supervised baseline in the retrieval of timbre\n(style) representation task, which indicates that V3 learns better-disentangled timbre representations\ncontaining less pitch information."}, {"title": "4.3 Learning Digits and Colors from Written Digit Strings", "content": "Dataset: We synthesize an image dataset of written digit strings on light backgrounds using all\n10 digits and 8 different ink colors. The order of digits is random. All images are diversified with\nGaussian noises, random blur, and foreground and background color jitters. More details about the\ndataset can be found in the appendix.\nExperiment Results: The quantitive results can be seen in Table 3. Similar to the music task, V3 per-\nforms the best considering both representation disentanglement and codebook interpretability. Even\nthough some unsupervised baselines also achieve a high codebook accuracy at large K values, they\nfall behind in terms of latent space retrieval, especially in style. We visualize the latent representations\nin Figure 5, and compare the confusion matrices in Figure 6 for intuitive understanding.\nAblation Study: Table 4 shows the ablation results of V3 on the digit string dataset. The inferior\nperformance of v_k = SD supports our assumption of its weakness in capturing multi-peak high-\ndimensional distributions. We observe that V3 trained without \\mathcal{L}_{fragment} achieves the best codebook\ninterpretability. Referring to the ablation in section 4.2, it is conceivable that a three-loss target can\nsometimes also satisfy the four constraints as defined in V3, and the optimal value for hyperparameters\nr and \u03b2 may differ by datasets, which marks a possible direction for future improvements."}, {"title": "5 Limitation", "content": "We have identified several limitations in our V3 method that necessitate further investigation. First,\nwhile V3 achieves good disentanglement and symbolic interpretability, it is not flawless samples\nof different contents (say images of \"8\" and \"9\") may sometimes be projected into the same latent\ncode. Inspired by human learning, which effectively integrates both mode-1 and mode-2 cognitive"}, {"title": "6 Conclusion", "content": "In conclusion, we contributed an unsupervised content-style disentanglement method V3, which\nleads to an emergence of symbolic-level interpretability of the learned latent space. V3's inductive\nbias is domain-general, intuitive, and concise, solely based on the meta-level insight of the statistical\ndifference between content and style, i.e., their distinct variance-invariance patterns reflected both\nwithin and across data samples. Experiments show that V3 not only outperforms the baselines in\nterms of both disentanglement and latent space interpretability but also achieves comparable results\nwith supervised models under a similar auto-encoding architecture. Moreover, the effectiveness of\nV3 generalizes across two distinct domains: music audio and written digits. For contents, the VQ\ncodebooks learned by V3 have a near one-to-one alignment with human knowledge of music scales\n(the 12 semitones from C to B) and numbers (the 10 digits from 0 to 9). For style, V3 learns a latent\nspace where different colors or instruments naturally form clusters without any label supervision or\neven a KL regularization that is usually needed for a well-structured latent space."}]}