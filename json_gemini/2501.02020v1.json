{"title": "Enhancing Uncertainty Modeling with Semantic Graph for Hallucination Detection", "authors": ["Kedi Chen", "Qin Chen", "Jie Zhou", "Xinqi Tao", "Bowen Ding", "Jingwen Xie", "Mingchen Xie", "Peilong Li", "Feng Zheng", "Liang He"], "abstract": "Large Language Models (LLMs) are prone to hallucination\nwith non-factual or unfaithful statements, which undermines\nthe applications in real-world scenarios. Recent researches\nfocus on uncertainty-based hallucination detection, which\nutilizes the output probability of LLMs for uncertainty cal-\nculation and does not rely on external knowledge or frequent\nsampling from LLMs. Whereas, most approaches merely\nconsider the uncertainty of each independent token, while\nthe intricate semantic relations among tokens and sentences\nare not well studied, which limits the detection of halluci-\nnation that spans over multiple tokens and sentences in the\npassage. In this paper, we propose a method to enhance un-\ncertainty modeling with semantic graph for hallucination de-\ntection. Specifically, we first construct a semantic graph that\nwell captures the relations among entity tokens and sentences.\nThen, we incorporate the relations between two entities for\nuncertainty propagation to enhance sentence-level hallucina-\ntion detection. Given that hallucination occurs due to the con-\nflict between sentences, we further present a graph-based un-\ncertainty calibration method that integrates the contradiction\nprobability of the sentence with its neighbors in the seman-\ntic graph for uncertainty calculation. Extensive experiments\non two datasets show the great advantages of our proposed\napproach. In particular, we obtain substantial improvements\nwith 19.78% in passage-level hallucination detection.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) (Zhao et al. 2023a),\nwith large-scale parameters and advanced training methods,\nachieve excellent performance in many downstream tasks\nof natural language processing (NLP) (Aracena et al. 2024;\nChen et al. 2024c; Lai and Nissim 2024; Zhang et al. 2024).\nDespite the many benefits of large language models, hallu-\ncination remains an issue that cannot be ignored. Hallucina-\ntion indicates that some non-factual or untruthful contents\nare generated (Wang et al. 2023a). Therefore, hallucination\ndetection is critically an essential task, which provides a pre-\nliminary review of the contents generated by large language\nmodels, reducing their potential harm in real-world scenar-\nios (Lee et al. 2024; Cui et al. 2023; Yan et al. 2024), such\nas education, economics, science, and so on.\nCurrent hallucination detection methods can be roughly\ndivided into three categories. (i) Retrieval-based method\n(Wang et al. 2023c; Zhang et al. 2023b) usually retrieve ev-\nidence from external resources for fact verification (Chen\net al. 2024a). This approach exceedingly depends on the\nquality of external resources, which is not always available.\nIn addition, it needs various validation steps towards the re-\ntrieved knowledge, which are complicated and inefficient."}, {"title": "Related Work", "content": "Hallucination reflects that language models generate some\nnonsensical or untruthful contents (Wang et al. 2023a) in\nmany downstream NLP tasks, such as the question and an-\nswer task (Nasz\u00e1di, Manggala, and Monz 2023), the multi-\nturn dialogue task (Chen et al. 2024b) and the text sum-\nmarization task (Kryscinski et al. 2020), etc. Hallucination\nin NLP can be categorized into two main classes: factual-\nity hallucination and faithfulness hallucination (Huang et al.\n2023a). The former one reveals the generated contents con-\ntain factual errors against real life, while the latter demon-\nstrates the issues of inconsistency or irrelevance in the text."}, {"title": "Hallucination in Language Models", "content": "Hallucination reflects that language models generate some\nnonsensical or untruthful contents (Wang et al. 2023a) in\nmany downstream NLP tasks, such as the question and an-\nswer task (Nasz\u00e1di, Manggala, and Monz 2023), the multi-\nturn dialogue task (Chen et al. 2024b) and the text sum-\nmarization task (Kryscinski et al. 2020), etc. Hallucination\nin NLP can be categorized into two main classes: factual-\nity hallucination and faithfulness hallucination (Huang et al.\n2023a). The former one reveals the generated contents con-\ntain factual errors against real life, while the latter demon-\nstrates the issues of inconsistency or irrelevance in the text."}, {"title": "Hallucination Detection", "content": "Before the era of LLMs, researchers normally train a dis-\ncriminating model to judge whether hallucination exists\n(Zhao, Nguyen, and Daume 2023). This approach relies too\nheavily on the training data and can reduce the models' gen-\neralization ability. With the development of NLP technol-\nogy, current hallucination detection methods can be roughly\ndivided into three categories."}, {"title": "Retrieval-based method", "content": "utilizes the retrieval-augmented generation tech-\nnique (Chen et al. 2024a) for extra knowledge (Choi et al.\n2023) or information to help detection (Varshney et al. 2023;\nChen et al. 2024b; Siino 2024). This approach exceedingly\ndepends on the quality of information sources, necessitat-\ning complicated validation steps (Ye et al. 2024; Dong et al.\n2024) towards the retrieved knowledge. Not to mention that\nnot all information is available easily. On the contrary, we\npropose an efficient reference-free method."}, {"title": "Sampling-based method", "content": "rewrites the contents under de-\ntection, measuring the consistency and coherence (Malkin,\nWang, and Jojic 2022; Sheng et al. 2024) between them to\nacquire a hallucination score (Manakul, Liusie, and Gales\n2023; Zhang et al. 2023a; Zhao et al. 2023b; M\u00fcndler et al.\n2024). However, this strategy frequently invokes LLMs for\nrewriting, consuming substantial computational resources.\nOur method needs one LLM to infer only once, thereby\ngreatly saving the response time."}, {"title": "Uncertainty-based method", "content": "applies proxy-based LLMs\nto output the probability of each token in contents to\nbe detected and then estimates a hallucination score with\nuncertainty-based metrics (Huang et al. 2023b; Chen et al.\n2023; Wang et al. 2023b; Petersen et al. 2024; Xiong et al.\n2024). Manakul, Liusie, and Gales (2023) regards the de-\ngree of hallucination as being negatively correlated with the\nprobability. Zhang et al. (2023c) refutes this view, but there\narises a co-occurrence bias (Zhou et al. 2023). Due to a lack\nof detailed exploration of various dependencies, our method\nsystematically constructs the relationships among the entity\ntokens and the sentences."}, {"title": "Our Approach", "content": "The framework of our proposed approach is illustrated in\nFigure 2. Specifically, inspired by the findings that halluci-\nnation accumulates as the sequence length increases, we in-\ntegrate the distribution statistics of LLM-based conditional\nprobability with sequence decay for token-level uncertainty\ncalculation. Considering much hallucination is induced by\nthe entities and relations in the sentence and passage, we\nfurther construct a semantic graph for sentence-level and\npassage-level uncertainty calculation. Regarding sentence-\nlevel uncertainty, it well captures the semantic relations be-\ntween entities for hallucination propagation and calculation.\nIn particular, the uncertainty of an entity propagates to the\nrelated entity along the dependent relations. For passage-\nlevel uncertainty, we incorporate the neighbors of each sen-\ntence in the semantic graph for uncertainty calibration and\nsummation. The details are denoted in the following."}, {"title": "Semantic Graph Construction", "content": "To better model the un-\ncertainties of entities with long-range dependency that span\nover the text, we first perform AMR (Xu, Lee, and Huang\n2023) parsing for each sentence, and gain a sentence-level\ngraph where each node is an entity and the edge repre-\nsents the dependent semantic relation. Compared to tradi-\ntional dependency parsing, AMR parsing is more logical\nand less vulnerable to syntactic representation or word order\nvariations. Therefore, we employ AMR to model the inter-\ndependency between the entities in the sentence. Further-\nmore, noting that passage-level hallucination usually occurs\nwhen two sentences contradict each other, we further link\nsentence-level AMR graphs together by the intricate rela-\ntions (e.g., entity linking and coreference) among sentences.\nFinally, a large AMR graph corresponding to the passage is\nacquired.\nFormally, we provide the notations deployed in this paper.\nLet $D$ express the input passage with $m$ sentences, which\nis denoted as $D = \\{S_1, S_2,..., S_m\\}$. Each sentence $S_i$\nis composed of $n_i$ tokens, i.e., $S_i = \\{t_i^1, t_i^2,..., t_i^{n_i}\\}$. In\naddition, the set of entity tokens in $S_i$ is formulated by\n$E_i = \\{e_i^1, e_i^2,...,e_i^{|E_i|}\\}$, where $|E_i|$ indicates the number\nof entities in the $i$-th sentence."}, {"title": "Token-level Uncertainty", "content": "Generally, the conditional probability of a token output\nby LLMs reflects its likelihood in the context, which can\nbe adapted to measure the uncertainty. Previous researches\nmainly focus on using the negative log probability or\nentropy-based methods for uncertainty estimation (Huang\net al. 2023b). In this paper, we integrate two statistical in-\ndicators, namely the maximum and variance of the probabil-\nity distributions. Moreover, hallucination tends to accumu-\nlate with the increasing sequence length as demonstrated in\nprevious studies (Varshney et al. 2023; Nasz\u00e1di, Manggala,"}, {"title": "Sentence-level Uncertainty", "content": "Previous works (Pagnoni, Balachandran, and Tsvetkov\n2021; Kryscinski et al. 2020) illustrate that a major of hal-\nlucination in text generation is induced by the entity er-\nrors, such as false relations between two entities, inconsis-\ntent mentions in the context or basic factual errors, etc. This\ncorresponds to our intuition that humans usually pay more\nattention to the salient information such as the keywords or\nentities for verification of the generated results. Therefore,\nrecent researches turn to investigate the uncertainty of in-\nformative and important entities for hallucination detection.\nHowever, the complex dependencies over the entities are not\nwell studied. In this paper, we explore the relations in the\nconstructed semantic graph for uncertainty propagation and\nhallucination estimation."}, {"title": "Relation-based Uncertainty Propagation", "content": "Previous\nfindings reveal that each token influences the surrounding\ncontext (Chen et al. 2017), thus the hallucination would\nprobably propagate across the generated text. Zhang et al.\n(2023c) presents a hallucination propagation method that\npropagates the uncertainty score of preceding entity tokens\nto the current one. Whereas, this method roughly uses all\nthe preceding entities, while ignoring their potential depen-\ndency relations with the current entity, which is inclined to\noverestimate the uncertainties by our preliminary studies.\nIn this paper, we present a relation-based uncertainty\npropagation method and assume that the subject entity\npropagates its uncertainty to the object entity based on the\npredicate or relation in the semantic graph. Moreover, we\ndevise a penalty factor based on the relation intensity to\nalleviate the uncertainty overestimation problem.\nTo be specific, given an object entity $o$, we first search\nthe entities that have semantic relations with $o$ from the\nsemantic graph and obtain a set of triples as $T_o =\n\\{(s', v', o)|(s', v', o) \\in T_i\\}$. $T_i$ is the set of triples in seman-\ntic graph of sentence $i$. Intuitively, the subject entities are not\nequally important to the object entity, thus we leverage their\nattention scores as the weights for uncertainty propagation.\nTo alleviate the overestimation problem, we additionally in-\ncorporate a relation intensity-based penalty factor for propa-\ngation. The final uncertainty of an object entity is formulated\nas:\n$U_p(o) = \\sum_{(s',v',o) \\in T_o} \\frac{att(s', o)}{I_o} * U(s')$\nwhere $att(\\cdot,\\cdot)$ signifies the attention score between two to-\nkens, $I_o$ is a penalty factor that computes the relation inten-\nsity of all entities that have relations with the object $o$, which\ncan be measured as follows:\n$I_o = \\frac{1}{|T_o|} \\sum_{(s',v',o) \\in T_o} \\frac{att(s', v') + att(v', o)}{2}$\nIn general, high relation intensities usually indicate high\nfactuality-confidence, thus the propagated uncertainties\nshould be penalized."}, {"title": "Entity Uncertainty", "content": "For an entity $e$, the uncertainty score\nconsists of the self-uncertainty (Formula 1) and the propa-\ngated uncertainty (Formula 2). The entity-based uncertainty\nof sentence $S_i$ can be calculated by averaging the uncertain-\nties of all entities in the sentence:\n$U_E(i) = \\frac{1}{|E_i|} \\sum_{e \\in E_i} [U(e) + \\beta U_p(e)]$\nwhere $U(e)$ and $U_p(e)$ show the self-uncertainty and prop-\nagated uncertainty respectively, and $\\beta$ is a hyper-parameter\nto balance these two uncertainties."}, {"title": "Global Uncertainty", "content": "In addition to the entities, there are\nalso many general tokens in the sentence. To capture the\nglobal information of the sentence, we also consider the un-\ncertainties of all tokens (both entities and general tokens)\nin the sentence, and utilize the quantile approach to mea-\nsure the global uncertainty, which is effective in capturing\nthe global statistics in distributions (Gupta et al. 2024):\n$U_G(i) = qua_{\\alpha}(U(t_i^1: t_i^{n_i}))$\nwhere $qua_{\\alpha}(U(t_i^1: t_i^{n_i}))$ is the $\\alpha$-quantile of the uncertain-\nties of all tokens in sentence $S_i$.\nThe uncertainty of the $i$-th sentence is the interpolation\nsum of the entity-based uncertainty and the global uncer-\ntainty:\n$U_S(i) = \\lambda U_E(i) + (1 - \\lambda) U_G(i)$\nwhere $\\lambda$ is an interpolation weight."}, {"title": "Passage-level Uncertainty", "content": "Previous methods usually estimate the average uncertainty\nof all sentences for passage-level uncertainty. However, the\nintricate relations among the sentences are neglected, which\ncould affect the detection of hallucination where two sen-\ntences contradict each other despite each sentence having"}, {"title": "Graph-based Uncertainty Calibration", "content": "Intuitively, if a\nsentence contradicts all the neighbor sentences in the seman-\ntic graph, it will probably have inconsistency or conflicts\nin the context, which is prone to the hallucination problem.\nThus, the uncertainty score should be increased. Motivated\nby this intuition, we present a graph-based uncertainty cali-\nbration method. First, we search the neighbor nodes for each\nsentence from the semantic graph. Then, we calculate the\ncontradictory score for each connected sentence pair with a\nNLI model, namely DeBERTa-v3-Large (He, Gao, and Chen\n2023), which is widely applied for natural language process-\ning tasks. Finally, we incorporate the uncertainty of each\nsentence with the neighbor contradictory scores for passage-\nlevel uncertainty computation:\n$U_p = \\frac{1}{m} \\sum_{i=1}^{m} [U_S(i) + \\frac{1}{|N(i)|} \\sum_{S_j \\in N(i)} U_S(i) * NLI(con|S_j, S_i)]$\nwhere $N(i)$ reflects the neighbors of the $i$-th sentence in the\ngraph, $NLI(con,\\cdot,\\cdot)$ is the contradiction probability between\ntwo sentences via the NLI model."}, {"title": "Experimental Setup", "content": "We conduct extensive experiments on two\ndatasets for hallucination detection. One is currently the lat-\nest and most widely used dataset WikiBio. To verify the ef-\nfectiveness and generalization of our method, we also con-\nstruct a Chinese dataset NoteSum, which can help boost re-\nsearch in this area. WikiBio (Manakul, Liusie, and Gales\n2023) is a dataset derived from Wikipedia biographies. Wik-\niBio applies the names from Wikipedia as the topics and\ngenerates corresponding biographies using GPT-3 (Floridi\nand Chiriatti 2020). Each sentence is annotated with one of\nthe following labels: Factual (hallucination score: 0), Non-\nFact* (0.5), and NonFact (1), which indicates a sentence\nwith no hallucination, with factual errors, and is irrelevant to\nthe topic respectively. The entire passage also has a human-\nlabeled hallucination score as the ground truth. NoteSum\nis an industrial Chinese dataset. The company first collects\nusers' long text notes on various daily topics with numerous\nentities. We cooperate with the company and create shorter\nsummaries from these long notes by LLMs for research.\nThe private information of users is removed. It consists of\nboth factuality and faithfulness hallucination as WikiBio.\nWe also adopt the same annotation guideline with WikiBio.\nThe statistics of the datasets are shown in Table 1."}, {"title": "Evaluation Metrics", "content": "For fair comparison, we apply the\nevaluation metrics used in previous researches (Manakul,\nLiusie, and Gales 2023; Zhang et al. 2023c). Specifically, the"}, {"title": "Results and Analyses", "content": "Table 2 shows the performance of our approach and the\nstate-of-the-art baselines. We have the following obser-\nvation. First, we achieve the best performance on both\nsentence-level and passage-level hallucination detection re-\ngarding all evaluation metrics. In particular, we gain a\nmaximum improvement of 19.78% over the best baseline\nin passage-level hallucination detection. Second, compared\nwith FOCUS that propagates the uncertainties of all pre-\nceding focused tokens to the subsequent one, our approach\nyields significant improvements especially for the NonFact*\nand Factual types that have moderate and no hallucina-\ntion respectively, indicating the effectiveness of our relation-\nbased uncertainty propagation to help alleviate the overesti-\nmation problem. Third, our approach exhibits good cross-\ndomain and cross-language generalization. It not only per-\nforms well on the English biography dataset WikiBio, but\nalso reflects significant improvements on the Chinese note\nsummary dataset NoteSum."}, {"title": "Ablation Studies", "content": "We conduct ablation studies on WikiBio with LLaMA-30B\nfrom three dimensions: token, sentence, and passage. Exper-\nimental results are shown in Table 3. For each row, one set-\nting is removed while keeping the other settings unchanged.\nWe have the following observations: (1) By removing\neach element from Formula 1 respectively, the performance\ndecreases significantly in most cases, which signifies the ef-\nfectiveness of the maximum, variance, and decay term for\nmodeling the token-level uncertainty. (2) The performance\nwith the passage-level metrics drops more significantly with\nthe setting of '- max', manifesting that the maximum proba-\nbility can better capture the key features for hallucination de-\ntection, while other terms can help further refine the uncer-\ntainty. (3) Both the entity uncertainty computed by relation-\nbased propagation and the global uncertainty are important\nto sentence-level detection. In addition, entity uncertainty is\nmore effective than global uncertainty for passage-level de-\ntection. (4) By excluding the contradiction relations of the\nneighbor sentences in the semantic graph, the performance\nof passage-level hallucination detection significantly drops\nby about 2 points, which further verifies the effectiveness of\nour graph-based uncertainty calibration for detecting hallu-\ncination over the passage."}, {"title": "Further Analyses", "content": "To\nfurther investigate the effectiveness of our relation-based un-\ncertainty propagation method, we compare with the baseline\nFOCUS (Zhang et al. 2023c) that propagates the uncertain-\nties of all preceding keywords to the subsequent one. The\nresults are shown in Figure 3, illustrating the uncertainty\nscores of three types of samples from WikiBio measured\nby FOCUS and ours respectively. We can observe that both\nof the two methods yield high uncertainty scores for the\nsamples with NonFact (ground truth score = 1), which can\nhelp well identify the severe hallucination. It is also notable\nthat the FOCUS method tends to overestimate the uncertain-\nties for the samples with NonFact* (ground truth score =\n0.5) and Fact (ground truth score = 0). There is a large gap\nbetween the estimated uncertainties and the ground truth.\nMoreover, the uncertainties of the three types calculated by\nFOCUS are very close, making it difficult to identify hallu-\ncination in different degrees. In contrast, our approach ef-\nfectively diminishes the uncertainties for samples with Non-\nFact* and Fact, which further verifies the effectiveness of\nour relation-based uncertainty propagation in alleviating the\noverestimation problem."}, {"title": "Effect of Graph-based Uncertainty Calibration", "content": "To ver-\nify the effectiveness of our graph-based uncertainty calibra-\ntion, we compare it with other two methods, namely Adja-\ncent and Average. The Adjacent method merely incorporates\nthe relations between the current sentence and the previ-\nous as well as the next sentence for uncertainty calculation,\nwhile the Average method simply measures the average un-\ncertainties of all sentences. The results of the two methods\nand ours are shown in Figure 5. Our method is observed to\noutperform Adjacent and Average in terms of Pearson and\nSpearman correlations, indicating the effectiveness of using\nthe semantic graph to model the long-range sentence rela-\ntions for passage-level hallucination detection. In addition,\nthe performance of Adjacent and Average is close, indicat-\ning the limits of merely considering the adjacent sentences."}, {"title": "Conclusions", "content": "In this paper, we propose a method to enhance uncertainty\nmodeling with semantic graph for hallucination detection.\nExtensive experiments verify the effectiveness of each com-\nponent of our approach. In particular, our approach con-\nsistently outperforms the state-of-the-art baselines in both\nsentence-level and passage-level hallucination detection, by\nincorporating the semantic relations among entities and sen-\ntences into the uncertainty calculation framework. It is also\ninteresting to find that our relation-based uncertainty propa-\ngation method can help effectively alleviate the uncertainty\noverestimation problem and our graph-based uncertainty\ncalibration method can capture long-range relations. In the\nfuture, we will explore integrating the existing knowledge\ngraph with AMR graphs for fact-checking and hallucination\ndetection."}, {"title": "Ethics Statement", "content": "Our WikiBio dataset is publicly used in the field of natu-\nral language processing. The NoteSum dataset, on the other\nhand, is an internal private dataset of Xiaohongshu, and its\nconstruction, annotation, and review are all handled by Xi-\naohongshu's own employees. The method presented in this\npaper is original to the authors and does not involve any eth-\nical issues."}]}