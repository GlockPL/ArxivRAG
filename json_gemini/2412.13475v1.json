{"title": "A Statistical and Multi-Perspective Revisiting of the Membership Inference Attack in Large Language Models", "authors": ["Bowen Chen", "Namgi Han", "Yusuke Miyao"], "abstract": "The lack of data transparency in Large Language Models (LLMs) has highlighted the importance of Membership Inference Attack (MIA), which differentiates trained (member) and untrained (non-member) data. Though it shows success in previous studies, recent research reported a near-random performance in different settings, highlighting a significant performance inconsistency. We assume that a single setting doesn't represent the distribution of the vast corpora, causing members and non-members with different distributions to be sampled and causing inconsistency. In this study, instead of a single setting, we statistically revisit MIA methods from various settings with thousands of experiments for each MIA method, along with study in text feature, embedding, threshold decision, and decoding dynamics of members and non-members. We found that (1) MIA performance improves with model size and varies with domains, while most methods do not statistically outperform baselines, (2) Though MIA performance is generally low, a notable amount of differentiable member and non-member outliers exists and vary across MIA methods, (3) Deciding a threshold to separate members and non-members is an overlooked challenge, (4) Text dissimilarity and long text benefit MIA performance, (5) Differentiable or not is reflected in the LLM embedding, (6) Member and non-members show different decoding dynamics.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Minaee et al., 2024) are trained with terabyte level corpora (Chowdhery et al., 2022) that are automatically collected, even the data creators themselves can hardly give instance-level analysis over the collected corpora (Biderman et al., 2022). Such a situation has led to several issues, such as the data leakage of evaluation benchmarks (Sainz et al., 2023) and personal information (Yao et al., 2024).\nThose concerns inspired the research of the Membership Inference Attacks (MIA) in LLMs (Hu et al., 2022). Given a set of examples, MIA focuses on differentiating members (trained) and non-members (untrained) by calculating a feature value for each example and splitting them using a threshold. Generally, those methods focus on observing the outputs of LLMs like generated tokens, probability distributions, losses, etc., and utilize such features to distinguish between members and non-members. Despite their success in previous studies, recent studies have shown that those methods behave nearly randomly in another MIA construction setting, or such benchmarks can be easily cheated (Duan et al., 2024; Das et al., 2024). Those negative results raised an inconsistency regarding the performance of MIA methods, e.g., do those MIA methods really work or not ?\nWe see such inconsistency comes from the distribution of the enormous size of the pre-train corpora, which is possible that members and non-members sampled from one setting could be totally different from another setting, leading to inconsistent results. In this study, instead of a single setting, we evaluate MIA methods statistically from multiple perspectives, e.g., the split methods, domains, text length, and model sizes. This led to thousands of MIA experiments for one MIA method and enabled a statistical analysis for MIA methods. Additionally, we conducted an in-depth analysis to study how the text feature, embedding, threshold decision, and decoding dynamics in members and non-members relate to MIA. We found that:\n(I) MIA performance improves with model size and varies with domains, while most methods do not statistically outperform baselines.\n(II) While MIA performance is generally low, a notable amount of differentiable member and non-member outliers exist and vary across MIA methods, connecting the inconsistency regarding the MIA performance.\n(III) The threshold to separate members and non-members changes with model size and domains, raising it as an overlooked challenge when using MIA in real-world.\n(IV) While the actual relation varies based on MIA methods, MIA performance generally positively relates to text length and text dissimilarity between members and non-members.\n(V) Whether members and non-members are differentiable is reflected in LLM embedding with an emergent change in a larger model that makes them more separable. Specifically, the last layer embedding used by the current MIA methods actually has a low embedding separability.\n(VI) Domains with high MIA performance show a faster increase in accumulated entropy difference for members and non-members."}, {"title": "2 Related Works", "content": "Membership Inference Attacks (MIA) (Hu et al., 2022) differentiates the member (trained) and non-member (untrained) data by calculating feature values and deciding a threshold for classification."}, {"title": "2.1 Membership Inference Attack Methods", "content": "First, we introduce MIA methods used in LLM based on the model's transparency.\nGray-Box Method This method requires the intermediate outputs to be observable, like loss, token probability, etc. Carlini et al. (2021) calculated the loss difference with another reference model with the assumption that if two models are trained under two samples of the same distribution, then the loss of non-members should be significantly different. Mink-k% (Shi et al., 2024) calculates the average log-likelihood of the tokens with the bottom-k% output probabilities, suggesting non-member text has more outliers and thus higher negative log-likelihood. Zhang et al. (2024b) improved Mink-k% by standardizing with variance and mean. Zhang et al. (2024c) compared predicted token probabilities against actual token probabilities from open corpora, in which the member data should have a closer distribution distance. Additionally, some methods alter the input text, like token swapping (Ye et al., 2024) or adding prefixes (Xie et al., 2024) with the hypothesis that the likelihood of member data should be influenced more by such text alternation.\nBlack-Box Method This method only observes the output tokens from the LLM. Dong et al. (2024) calculated a variant of edit distance with multiple generations from the LLM with the hypothesis that those generations of a member text should have a smaller lexical distance compared to non-member text. Additionally, Kaneko et al. (2024) made a similar hypothesis while they evaluated the semantic similarity using the embedding model."}, {"title": "2.2 Membership Inference Attack Analysis", "content": "Regarding the MIA analysis, some research (Maini et al., 2021; Carlini et al., 2022) suggest the MIA difficulty increases with model size and corpora size. Zhang et al. (2024a) showed with toy data that it is hard to reliably operate an MIA method under a certain false positive rate. Meeus et al. (2024) found some MIA benchmarks are flawed, which can be easily cheated by just checking the word differences (Das et al., 2024). Duan et al. (2024) evaluated Gray-Box MIA methods in the train and test set of pre-train corpora of an LLM, where they behave almost randomly.\nThose negative findings show an inconsistency with the performance reported by the previous MIA methods. We assume such inconsistency comes from the sampled member and non-member distribution under different settings, which could be totally different due to the enormous size of corpora, leading to this inconsistency. In this study, instead of using a single setting, we create various settings, leading to thousands of experiments for one MIA method. This allows a statistical-level analysis of MIA methods from multiple perspectives, which shows new findings and connects the MIA performance inconsistency."}, {"title": "3 Experiments Setting", "content": "Given a model M and set of data $X = \\{x_0 . . . x_n\\}$, where each x is a text consisted of $\\{t_0...t_m\\}$ tokens a MIA method calculates feature scores $S = f(M;X) = \\{s_0...s_n\\}$ for every data instance. A threshold t will be selected to classify whether $x_i$ belongs to training data D of the model M. Data that are in the D ($x_i \\in D$) are called member data, otherwise called non-member data."}, {"title": "3.1 \u039c\u0399\u0391 Methods", "content": ""}, {"title": "3.1.1 Baseline Methods", "content": "Loss (Yeom et al., 2018) collects the loss value $L(M+; x)$ for each input instance text.\nRefer (Carlini et al., 2021) calculates the loss gap between the attacked model $M_t$ and a reference model $M_r$ for the input text $L(M_t; x) \u2013 L(M_r; x)$.\nGradient collects the gradient value $G(M_t;x)$ for each input instance text.\nZlib (Carlini et al., 2021) calibrates the loss by the Zlib compression entropy $Z(x)$ of the input text, calculated as $\\frac{L(M_t;x)}{Z(x)}$"}, {"title": "3.1.2 Token Distribution Based Method", "content": "Those methods hypothesize that non-member text contains more rare tokens or has a different distribution whose average log-likelihood should be different than that of member text.\nMin-k% Prob (Shi et al., 2024) calculates the average log-likelihood of tokens in bottom-k% decoding probabilities in the whole input tokens $Bot(x)$, which is calculated as $MinK(M_t;x) = \\frac{1}{|E|} \\sum_{t_i \\in Bot(x)} log p(t_i | t_1,...,t_{i\u22121})$. E represents the number of bottom-k% tokens.\nMin-k% Prob++ (Zhang et al., 2024b) standardizes Mink-k% with its mean and standard deviation. As the Mink-k% did not standardize the value, causing an unstable value range.\nDC-PDD (Zhang et al., 2024c) computes the divergence between the probability of decoded tokens with their token probability distribution pre-computed based on a large corpora."}, {"title": "3.1.3 Text Alternation Based", "content": "This method alters the input text by adding a prefix or swapping tokens with the hypothesis that the log-likelihood of member text is affected more than non-member text in such perturbation.\nEDA-PAC (Ye et al., 2024) creates a perturbed text 2 by continuously swapping two random tokens. Then, it calculates the difference between the average log-likelihood for Top-k% and Bottom-k% tokens for x and the swapped $\\hat{x}$. The hypothesis is that the token swap alters a member to a non-member while a non-member is still a non-member, so the members should be influenced more.\nRECALL (Xie et al., 2024) creates a non-member prefix p and concate it with text x to calculate a RECALL score $\\frac{LL(xp)}{LL(x)}$ where LL is the average log-likelihood. The hypothesis is that if x is a member text, the non-member prefix p perturbs LLM's confidence in generating it, while such perturbation affects less for non-members."}, {"title": "3.1.4 Black-Box Methods", "content": "This method generates multiple continuations for a text prefix with the hypothesis that the multiple generated continuations of member text should have a higher semantic/lexical similarity with the actual continuations than those of non-member text.\nSaMIA (Kaneko et al., 2024) inputs a partial prefix of the text and generates multiple continuations. Then, it calculates the average semantic similarity of the generated continuations with the actual continuations as the feature value.\nCDD (Dong et al., 2024) inputs a prefix of the text and generates multiple continuations for this prefix. Then, it calculates a variant of edit distance between generated continuations with the actual continuation to calculate the peakiness score, e.g., a measurement of how generated tokens are similar to each other on the token level."}, {"title": "3.2 Datasets", "content": "We use one existing benchmark and sample data from pre-train corpora with various settings.\nWikiMIA (Shi et al., 2023) contains Wikipedia text sampled at the timestamp of 2023/10. Text samples before the time stamp are member text, and those after them are non-member text. This"}, {"title": "3.3 MIA Data Construction for Pile", "content": "We provide three split methods to construct the member and non-members set for the Pile dataset.\nTruncate Split (Duan et al., 2024) creates the member set and non-member set by truncating texts into a fixed range. We extend it by setting a length range of 100 from 0 to 1000.\nComplete Split samples member and non-member texts whose whole length is in a text range that follows the Truncate Split.\nRelative Split calculates the ten-percental text length range based on the test set of each domain. The member and non-member text are sampled from those ten-percentile length ranges.\nEach split method is applied to all domains in the Pile, with a minimum of 100 examples for both members and non-members. As text distribution varies by domain, not every domain meets this criterion. 3 This resulted in nearly 100 GBs of member and non-member texts sampled from different settings for MIA experiments.\nThis statistical evaluation contains (1) more domain coverage (compared to WikiMIA, ArxivMIA (Shi et al., 2023), BookMIA (Shi et al., 2024)), (2) broader text length range (compared to MIMIR (Duan et al., 2024)), (3) considered the truncation method and domain-specific sampling. We run all MIA methods on every length split on domains in that split for every model size in all random seeds with 4,860 experiments for one MIA method."}, {"title": "3.4 Models", "content": "We use the Pythia model (Biderman et al., 2023) (160m, 410m, 1b, 2.8b, 6.9b, 12b) that trained on the deduplicated Pile corpora to avoid effects from"}, {"title": "3.5 Evaluation Metric", "content": "ROC-AUC (Fawcett, 2006) iterates every threshold for binary classification to calculate the Ture Positive Rate (TPR) and False Positive Rate (FPR) to form a ROC curve. AUC is the area under this curve and is used to analyze MIA performance.\nDavies-Bouldin Score (Shi et al., 2023) (DB-Index) evaluates the separability of two clusters of embeddings. A lower value indicates a better seperability. This is used to evaluate the separability of embeddings for members and non-members."}, {"title": "4 Results", "content": "We first statistically analyze the ROC-AUC scores of MIA methods, which generally align with previous negative results but also show new findings. Then, we analyze the outliers where the members and non-members show differentiability and connect the inconsistency regarding MIA performance. Next, we discuss the threshold decision when using the MIA to analyze its real-world effectiveness. Additionally, we explore how MIA is related to the input text itself by studying its correlation with text length and similarity. Finally, we seek the explanation for MIA performance from the LLM structure level with an analysis of the separability of embeddings and decoding dynamics for members and non-members."}, {"title": "4.1 Effect of Different Factors", "content": "We aggregate ROC-AUC scores between 0.50 and 0.58, cover most of the experiments, and calculate their probability density over the split method, model size, domain, or MIA methods while fixing the others in Figure 2. While 0.50-0.52 occupies most probability densities, we still observe that:\n(I) In Figure 2 (a), the commonly used Truncate Split shows the worst performance, while the Relative Split gives the best performance. Truncating a text may cause it to lose outlier words. Additionally, such loss of contextual information affects MIA methods that rely on alternating the original members and also affects the Black-Box method as the quality of generated tokens deteriorates.\n(II) In Figure 2 (b), MIA performance improves with model size, particularly from 1b to 2.8b, which contradicts previous findings that suggest it should decrease with model size. We think that a small model struggles with learning large corpora due to a small capacity, causing most member texts to behave like non-members and reducing MIA performance. As model capacity scales, more member texts are well learned, which starts to differ from non-member text and enhances performance. Our results do not falsify previous research. If a much larger LLM learns very well and even fits well with non-member text, it may again show a low MIA performance. Thus, the model size and MIA performance relation may be an inverse U-curve.\n(III) In Figure 2 (c), among shared domains across split methods, Wikipedia (en) and FreeLaw show statistically better performance compared to other domains. We suggest this is related to token diversity. GitHub and StackExchange are related to codes that have less token diversity compared to FreeLaw and Wikipedia, where various words are used. The Pile-CC is a general domain that contains various texts whose token diversity is between the text domain and code domain.\n(IV) In Figure 2 (d), only PAC and CDD are worse than the Refer baseline, and the Loss baseline is only outperformed by Min-k% ++, Min-k%, and RECALL. Other methods are between those baselines, and their performance gap is within the variance from random seeds. However, this does not indicate their peak performance in certain settings since the probability density tests the generalizability of the hypothesis in each MIA method."}, {"title": "4.2 Outliers in MIA", "content": "While the MIA performance is generally low, we still observed notable outliers with relatively high differentiability (ROC-AUC > 0.55) not captured by the probability density."}, {"title": "4.2.1 Outliers Statistics Analysis", "content": "We count those outliers across the MIA method and model size along with their maximum and mean ROC-AUC values in Table 1.\n(I) Those outliers occupy a small ratio with 8.4% even for Min-k% ++, which generally aligns with previous negative results regarding the MIA performance. However, the existence of those outliers also provides space for previous positive results, connecting their inconsistency.\n(II) In most MIA methods, the number of differentiable splits increases with model size. As results in the section 4.1 already show, large models statistically perform better. We hypothesize that the internal structure of LLM changed in a way that positively affects MIA when scaling model size. However, methods (SaMIA, CDD, Refer) that do not only rely on internal states of LLM are less sensitive to increasing model size.\n(III) Additionally, we also see that the maximum and mean performance are not related to how many outliers exist in the MIA methods. The highest value reaches 0.81 in RECALL while the number of its outliers is not either the highest or lowest. This suggests the method that works generally better (Min-K% ++) does not mean it is also the absolute better one, supporting that a MIA method should be evaluated statistically."}, {"title": "4.2.2 MIA Methods Consistency on Outliers", "content": "With the existence of outliers, we study whether they are consistent across MIA methods by calculating their overlap ratio in Figure 4.\n(I) Even the best-performed method (Min-k%++) does not have a general higher overlap, which only has a 4% overlap with the CDD method. SaMIA and CDD give a low overlap when compared to all other MIA methods as they do not require any internal outputs, which is significantly different from other MIA methods.\n(II) Even though most methods do not statistically outperform baselines, this does not mean those methods are not meaningful, as the overlap matrix shows each MIA method works in different situations. The results also suggest it is hard to use one hypothesis to outperform all others."}, {"title": "4.3 Generalization of Threshold in MIA", "content": "The ROC-AUC metric iterates feature values to differentiate between members and non-members but does not show how to decide a threshold and its general effectiveness in MIA. To address this, we split the member and non-member sets into training and validation sets in a 4:1 ratio and use the Geometric Mean $t = arg max_i \\sqrt{TPR_i \u00d7 (1 \u2013 FPR_i)}$ (Youden, 1950) to find a threshold that balances the true positive rate and false positive rate. The distribution of this threshold across different model sizes and domains is shown in Figure 3.5\n(I) In the top figure, the threshold varies not just between domains but also within the same domain with the existence of outliers. In the bottom figure, the threshold changes with model sizes, as most MIA methods rely on the output likelihood, which is related to the model size. The SaMIA, which relies on an external model to compare sentence similarity, is less affected by the model size, further confirming this point. This suggests the threshold in one model size may not work for the others.\n(II) These results show the generalizability of the MIA threshold as an overlooked challenge. A threshold may not work even in samples from the same domain, may not transfer to another domain, and may not work in another model size, leading to a high possibility of performance deterioration when using the MIA method in the real world."}, {"title": "4.4 Text Similarity and Text Length", "content": "Previous studies showed text length (Zhang et al., 2024c) and token differences (Duan et al., 2024) contribute to the MIA but with results induced from"}, {"title": "4.5 Embedding Probing and Seperability", "content": "In this section, we discuss how embeddings of members and non-members are represented across layers to answer the question of are they originally indifferentiable at the internal states? We collect the average pooled hidden states at each layer for members and non-members. The DB Score is used to evaluate how separable those embeddings are, and we train a Transformer classifier on them to see if they are directly separable, as shown in Figure 5.\n(I) The DB Score is around 10 in the differentiable splits with 70%-100% accuracy in the Transformer classifier and reaches around 40 in the in-differentiable splits with random guess accuracy (50%). This suggests differentiable splits are originally easier to differentiate from the embedding level, while their varied accuracies and DB scores still highlight different separabilities. As for in-differentiable splits, it is near random accuracy, even directly trained on their embeddings.\n(II) The DB Score curve shows emergent behavior on in-differentiable domains with model size increases. The PubMed and Pile-CC domains did not show a decreasing DB Score in 410m. However, when reaching the 2.8b size, their DB scores suddenly decreased in deep layers, meaning that the separability between members and non-members increased, which is even more significant in the 12b model. This helps to explain the aforementioned RUC-AUC performance boost from 1b to 2.8b since the embeddings of some domains suddenly become more separable in the 2.8b model size, leading to higher RUC-AOC performance.\n(III) The DB Score bounces back to a high value in the final layer, meaning a decreased separability. As current MIA methods use the last layer and its computation results (likelihood, tokens, etc.), this may help to explain why MIA performance is low in general, as the last layer itself is not a good option as its embedding separability is low."}, {"title": "4.6 Generation Entropy Dynamics", "content": "Current MIA methods pay less attention to the token decoding dynamics in the LLM generation process. We calculate the token entropy for members and non-members and their accumulated entropy difference across steps in Figure 6.\n(I) From Figure 6, a low or high domain entropy (GitHub, StackExchange) does not relate to its MIA performance in Figure 2 (c). However, the domain-dependent entropy (decoding probability) means a domain-dependent log-likelihood, which explains the low threshold generalizability of Gray-Box methods. This also helps to explain the better performance of Min-k% ++ as it standardizes the log-likelihood of input tokens, erasing such domain or input text dependency.\n(II) Though decoding entropy at each step does not show obvious features related to the MIA performance, the accumulated entropy difference increases with the decoding steps, suggesting non-members have a statistically higher entropy compared to the member texts. Additionally, the domains with higher MIA performance (FreeLaw, Wikipedia (en) in Figure 2) have a higher increasing speed in the accumulated entropy difference than the other statistically low MIA performance domains (StackExchange, GitHub)."}, {"title": "5 Conclusion", "content": "In this study, we revisited the MIA statistically with in-depth analysis from multiple perspectives. Our results show MIA performance improves with model size and varies across domains, with most MIA methods showing no advantage compared to baselines. Our results generally support previous negative results, but notable amounts of MIA performance outliers make space for positive results, connecting the MIA performance inconsistency. We also found that deciding a threshold in MIA is an overlooked challenge. Additionally, long text and text dissimilarity benefit the MIA performance. The separability of members and non-members is also reflected in the LLM embedding with emergent change that benefits MIA in large models. The final layer used by current MIA methods may be a bad choice due to low embedding separability. Finally, differentiable members and non-members have faster accumulated entropy difference."}, {"title": "6 Limitations", "content": "The analysis of the results is mostly based on the statistical level. This means we do not make assumptions about the correctness of analysis in previous results, and the statistical analysis should be a stand-alone analysis. The results may not totally align with previous results that were conducted in their own settings. Additionally, as Pythia only provides model sizes up to 12b, we cannot scale the model size further. Additionally, only very few LLMs released their pre-train data, and their pre-train data is different, so it is hard to conduct such experiments across models. 6\nThough we tried to extend the scale of the experiments further, the size of the test and valid data limited it. Their texts will be exhausted with further samplings and no longer satisfy the experiment requirements, where we want sampled members and non-members to be different each time.\nWe are not able to fully implement all existing MIA methods, but we selected methods that we considered to be representative at the time of this research, with most of those methods published very recently. There are multiple ways to select a threshold, and there are pros and cons in choosing different calculation methods for a threshold. We did not choose to iterate all possible options but chose the geometric balance between TPR and FPR. We do deny the existence of better threshold calculation method exits that may need different results, but this study is analysis-oriented rather than enumerating possible options to find a better method."}, {"title": "7 Ethical Considerations", "content": "The original Pile date was reported to contain content related to copyright issues. The domains reported with copyright issues are Books3, Book-Corpus2, OpenSubtitles, YTSubtitles, and OWT2. We have made sure we did not conduct any MIA experiment on any of those domains, and we used processed Pile corpora that removed those domains. This Pile data that removed those domains is accessible online. 7\nFor other data we have used, we have made sure the usage aligns with the data license and their intended usage. Though we conducted experiments over the Pile corpora, we did not observe any personal information or offensive content during the experiments."}, {"title": "A.1 Experiment Setting", "content": "The experiment was conducted on over 8 H100 CUDA devices. Experiments to run one gray-box method overall model sizes take roughly 2 days. Experiments to run one black-box method over one model size take roughly 20 days, which means the Black-Box methods require more heavy computation as they require the generation of tokens rather than directly taking the intermediate outputs like log-likelihood. Additionally, unlike Gray-Box methods that can input the entire sentence, the Black-Box requires the input of a partial of this sentence and then requires the LLM to generate"}, {"title": "A.2 Experiment Setting for MIA Method", "content": "For the reference model, we use the best reference model based on previous research (Duan et al., 2024). For the Min-K% and Min-K% ++, we choose the K as 20, which means 20% of Box(x) are selected from the whole input tokens. This metric is used in their research paper and repositories. 89\nFor the DC-PDD, there is no hyperparameter, and it relies on a pre-computed token frequency from corpora, which is not released at the time of writing. To reproduce this study, we used the infini-gram package 10 as the pre-computed frequency. However, their frequency is computed over the LLaMa tokenizer, which is different from that of the Pythia tokenizer. We have to align their results, but this causes inevitable errors, which we cannot manage since the frequency is computed on a different tokenizer, and a sentence may be tokenized into different tokens based on the tokenizer.\nFor the EDA-PAC, the percentage of words that are swapped is set as 30%, and collect five perturbed sentences.\nFor the RECALL, the number of shots (the number of prefixes) inserted into the input text is set as"}, {"title": "A.3 Available Domains in Each Split Method", "content": "We have the Truncate, Complete, and Relative split method over the input text of all domains in the Pile corpora. We only keep those splits that have at least 100 examples for both member and non-member text at all text lengths. If a domain does not meet this requirement, it will be discarded. The available domains for all those split methods are presented in the following Table 4a and 4b.\nFor each MIA method, the results run on all of its split methods, length range, model size, and random seeds. We are also able to see that the Complete splitting methods have the lease domains as the whole length of a text is a strict standard. Additionally, we also see that Relative split has the most domains as this split method suits the distribution of the target domains. Thus, most data are kept using this split method while following the text distribution."}, {"title": "A.4 Memorization and MIA", "content": ""}, {"title": "A.4.1 Memorization Score Sample Distribution", "content": "In this figure, we saw that LLM does not show a very obvious distribution gap for most of the domains. However, we notice that in the GitHub domain, there are many texts that show high memorization scores, meaning that most of the texts are"}, {"title": "A.4.2 Memorization Score Distribution Distance", "content": "This section examines whether MIA performance is related to memorization by comparing the generated tokens with actual continuations when promoting 32 tokens, known as the K-extractable score, for both non-member and member text. We compute the distribution distance between the K-extractable score over the different domains on member and non-member text using JS divergence, as shown in Figure 8. We can see a correlation between MIA performance and the JS distribution difference between member and non-member text. The FreeLaw has the highest JS Divergence score among those domains, suggesting that the memorization score distribution between member text and non-member text is large. This aligns with the ROC-AUC score density distribution in Figure 2. Additionally, we also see that GitHub and StackExchange have a low divergence, meaning their memorization score distribution between member text and non-member text is small, which is hard to differentiate."}, {"title": "A.5 Membership Inference Attack as Hypothesis Test", "content": "Besides directly analyzing the probability density function of the ROC-AUC scores, we also try to look at the MIA from a hypothesis test perspective. We treat the feature scores of member and non-member text as two distributions and use a hypothesis test to verify whether those two distributions are the same distributions or not. If a distribution passes such verification, it at least means the feature score distribution of the member is different from the feature distribution of the non-member text. Even though it does not guarantee any MIA performance, it does not directly evaluate MIA performance; it just shows whether those two distributions are the same or not. Such analysis at least provides a perspective to look at MIA differently. We divide the number of splits whose feature scores of member and non-member passed the verification as two distributions with the total amounts of splits. The results are presented from Table 5 to 7.\n1. Similar to MIA performance, we observed that the number of splits that pass the hypothesis test increases with the model size. This confirms the analysis of the results of the RUC-AOC score using the probability density functions.\n2. Further, we also see that in this evaluation metrics, the best-performed method Min-K% ++ does not also show the best performances in passing the hypothesis test. On the contrary, the best-performed MIA method is the Refer, which actually has the lowest performance in the ROC-AUC analysis. The reason is that the hypothesis test method does not evaluate whether the two examples are separate or not; it evaluates how those two distributions consisting of the members and non-members are the same distribution or not. This means that they do not consider separating a specific example but focus on identifying those two distributions.\n3. Even though the hypothesis test does not provide a method to differentiate members and non-members specifically. It tells the performance of MIA from another perspective, whereas the previous worst-performing method could actually have the best performance. This shows the importance of evaluating the MIA method from multiple perspectives rather than only focusing on certain metrics, which could be misleading.\n4. In this metric, we are also able to observe the same performance boost when transferring from the 1b to 2.8b model. This aligns with the observation in the probability density analysis of RUC-AOC scores across dimensions, which confirms the emergent embedding change that we have discovered."}, {"title": "A.6 Detailed Results in Each Split Method", "content": "In this section, we present the detailed results for Truncate Split, Complete Split, and Relative Split. Each split contains all available domains. We shot the probability density in the Domain, Model Size, and MIA Method dimension in Figure 9.\n1. In the first row, which shows the probability density over domains, we saw some more high-performance domains. For example, in the Truncate split, the EuroParl performs very well compared to other domains. One of the reasons may be that the EuroParl contains some non-English texts, which serve as an important feature for the member and non-member classification. Still, in the relative split, we are able to see more domains with relatively high MIA performance compared to other domains, which helps to explain why Relative Split can give better performance.\n2. In the second row, which shows the probability density over model sizes, we saw a uniform performance across different splits where the MIA performance positively scales with the model size.\n3. In the third row, which shows the probability density over the different MIA methods. We are also able to observe some split-based differences. In the Relative and Complete split, we can see that the Min-k% ++ performs better than other methods. However, in the Truncate split, we see mixed results where most methods do not show obvious performance differences, where the Min-k% ++ is no longer significantly better than other methods."}]}