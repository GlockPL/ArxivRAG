{"title": "Clustering Properties of Self-Supervised Learning", "authors": ["Xi Weng", "Jianing An", "Xudong Ma", "Binhang Qi", "Jie Luo", "Xi Yang", "Jin Song Dong", "Lei Huang"], "abstract": "Self-supervised learning (SSL) methods via joint embedding architectures have proven remarkably effective at capturing semantically rich representations with strong clustering properties, magically in the absence of label supervision. Despite this, few of them have explored leveraging these untapped properties to improve themselves. In this paper, we provide an evidence through various metrics that the encoder's output encoding exhibits superior and more stable clustering properties compared to other components. Building on this insight, we propose a novel positive-feedback SSL method, termed Representation Soft Assignment (ReSA), which leverages the model's clustering properties to promote learning in a self-guided manner. Extensive experiments on standard SSL benchmarks reveal that models pretrained with ReSA outperform other state-of-the-art SSL methods by a significant margin. Finally, we analyze how ReSA facilitates better clustering properties, demonstrating that it effectively enhances clustering performance at both fine-grained and coarse-grained levels, shaping representations that are inherently more structured and semantically meaningful.", "sections": [{"title": "1. Introduction", "content": "Self-supervised learning (SSL) has emerged as a transformative paradigm in universal representation learning (Bachman et al., 2019; He et al., 2020; Chen et al., 2020a; Bao et al., 2021; He et al., 2022; Oquab et al., 2023; Assran et al., 2023), consistently surpassing supervised learning in downstream performance. Joint embedding architectures (JEA), in particular, aim to learn invariance of the same data under different transformations and noise (Bachman et al., 2019; He et al., 2020; Chen et al., 2020a), with demonstrated exceptional effectiveness in visual representation learning. Although such a pretext task may intuitively seem unrelated\nContributions. In this paper, we aim to investigate the design of SSL methods by leveraging the inherent clustering properties of representations, enabling a closed-loop positive-feedback SSL framework, as illustrated in Figure 1. To achieve this goal, we propose three key questions and our main contributions are summarized as follows:\n\u2022 Where to extract clustering properties from? We demonstrate through various metrics that the encoder's output, referred to as encoding, exhibits superior and more stable clustering properties compared to other components, such as embedding and the hidden layer outputs within the projector.\n\u2022 How to leverage clustering properties? We propose a novel SSL method, termed Representation Soft"}, {"title": "2. Background, Related Work, & Notation", "content": "2.1. Self-Supervised Learning\nJoint embedding architectures (JEA). Let B denote a mini-batch input sampled uniformly from a set of images D, and T denote the set of data transformations available for augmentation. We consider a pair of neural networks Fe and F, parameterized by \u03b8e and \u03b8' respectively. They take as input two randomly augmented views, X = T(B) and X' = T'(B), where \u03a4,\u03a4' \u2208 T; and they output the embeddings Z = F\u03b8e(X) and Z\u2032 = F, (X'). The networks are trained with an objective function that minimizes the distances between embeddings obtained from different (two) views of the same images:\n$\\mathcal{L}(B, \\theta) = \\mathbb{E}_{B \\sim D, T,T' \\sim T} l(F_{\\theta_{e}}(T(B)), F_{\\theta'_{e}}(T'(B))).$ (1)\nwhere l(,) is a loss function, which aims to learn invariance of data transformations.\nIn particular, the JEA usually consist of an encoder Ee(\u00b7) and a projector G (\u00b7). As shown in Figure 2, their outputs H = E\u03b8e (X) \u2208 R^(de\u00d7m) and Z = G (H) \u2208 R^(dg\u00d7m) are referred to as encoding and embedding, respectively (where m is the mini-batch size, de and dg are their corresponding feature dimensions). Under this notation, we have Fo() =\nGe (Ee()) with learnable parameters \u03b8 = {\u03b8e, \u03b8g}. The projector has been demonstrated to be a significant component, which can prevent invariance optimization from degrading the useful information in encoding (Chen et al., 2020a; Gupta et al., 2022). Consequently, the encoding is usually utilized as the optimal representation for downstream tasks.\nSelf-supervised paradigms. The main challenge with JEA is representation collapse, where both branches produce identical and constant outputs regardless of the inputs. Numerous paradigms have been proposed to avoid collapse, including contrastive learning methods, e.g. SimCLR (Chen et al., 2020a), MoCos (He et al., 2020; Chen et al., 2020b; 2021), MEC (Liu et al., 2022) that attract different views from the same image (positive pairs) while pushing apart different images (negative pairs), and non-contrastive approaches, e.g. BYOL (Grill et al., 2020), SwAV (Caron et al., 2020), DINO (Caron et al., 2021), INTL (Weng et al., 2024) which directly align positive targets without incorporating negative pairs. Although research (Ben-Shaul et al., 2023) has demonstrated that the encodings learned through SSL are highly correlated with semantic classes and exhibit strong clustering capabilities, few methods have leveraged this clustering ability to facilitate positive-feedback learning. A closely related work (Ma et al., 2023) exploits the encoding's augmentation robustness to re-weight the positive alignment in the SSL objective functions; however, it still overlooks the rich clustering information inherent in the encodings."}, {"title": "2.2. What is Optimal Representation for Clustering", "content": "Ben-Shaul et al. (2023) find that within the encoder of JEA, the clustering ability of features improves progressively as intermediate layers get deeper. However, it remains unclear whether the projector exhibits a similar trend. In fact, embedding tends to be less influenced by clustering-irrelevant features, and existing cluster-based SSL methods, e.g. DeepCluster-v2 (Caron et al., 2018), SwAV (Caron et al., 2020), and MSN (Assran et al., 2022b), impose clustering constraints on embedding to prevent collapse. These observations raise an important question: Which JEA component provides the optimal representation for clustering? Is it the encoding, embedding, or perhaps the hidden layer outputs within the projector?"}, {"title": "3. Exploring Clustering Properties of SSL", "content": "To quantitatively evaluate the clustering performance of these components, we employ widely recognized metrics such as the Silhouette Coefficient (SC) (Rousseeuw, 1987) and Adjusted Rand Index (ARI) (Hubert & Arabie, 1985). In particular, a larger mean value of SC (SCmean) indicates stronger local clustering capability in the representation, and a smaller standard deviation (SCstd) reflects better stability in local clustering. Meanwhile, higher ARI values correspond to enhanced global clustering properties. The detailed introduction of these metrics can be found in the Appendix A.\nUsing these metrics, we first evaluate the clustering abilities of encoding H and embedding Z across various self-"}, {"title": "4. Leverage Clustering Properties for Positive-Feedback Learning", "content": "Based on above analyses, we design a novel positive-feedback SSL method, which derives Representation Soft Assignment (ReSA) to guide the loss function among embeddings Z and Z'. See Figure 5 for the clear framework.\n4.1. Online Self-Clustering ReSA\nFollowing notations in Section 2.1, we apply the encoding H as the representation to perform clustering. Unlike previous approaches, e.g. SwAV (Caron et al., 2020) and DINOv2 (Oquab et al., 2023) employing learnable prototypes to map features into the clustering space, we treat samples in H = [h1, ..., hm] simultaneously as points to be clustered and as anchors. In details, we first calculate the cosine self-similarity matrix by SH = HTH, where samples in H are L2-normalized as hi/||hi||2, \u2200i. Then the online clustering assignment \u0410\u043d is computed upon SH using the iterative Sinkhorn-Knopp (Cuturi, 2013) as shown in Algorithm 1.\nWe follow SwAV which uses only 3 iterations and sets the regularization parameter \u03f5 = 0.05. This algorithm does not involve gradient propagation, enabling it to be efficiently implemented on GPUs (Caron et al., 2020). After obtaining the doubly stochastic matrix \u0410\u043d as the representation soft"}, {"title": "Comparison to SwAV.", "content": "As a pioneering SSL method based on online clustering, SwAV (Caron et al., 2020) employs a 'swapped' prediction mechanism (which is also adopted by DINOv2 (Oquab et al., 2023)), where the cluster assignment of one view is predicted from the embedding of another view. This is achieved by minimizing the following objective:\n$\\mathcal{L}_{SWAV} = -\\frac{1}{2m} \\sum_{i,j} Q_{i} Q'_{j} \\circ \\log D(Z_{i}^T C) +  \\sum_{i,j} Q_{j} Q'_{i} \\circ \\log D (Z'_{j}^T C)$ (3)\nwhere C \u2208 R^(dg\u00d7K) is the prototype matrix learned by back-propagation, and Q = sinkhorn(ZTC) is the cluster assignment using Sinkhorn-Knopp algorithm. The key differences and advantages of our ReSA compared to SwAV (and DINOv2) can be summarized as follows: (1) ReSA computes clustering assignments on encoding with high-quality clustering properties, whereas SwAV performs it on the less stable embedding. (2) SwAV requires learnable prototypes, which often necessitate complex design strategies, such as freezing prototypes during the early stages of training and use a large number of prototypes K to ensure stability, whereas ReSA directly extracts clustering information from the representations. (3) SwAV executes the Sinkhorn-Knopp algorithm multiple times, corresponding to the number of augmented views. In contrast, ReSA only requires a single execution of this algorithm regardless of the number of augmented views. This highlights the efficiency of ReSA, particularly under multi-crop scenarios."}, {"title": "Comparison to InfoNCE.", "content": "As a well-known contrastive loss function, InfoNCE (Oord et al., 2018) aims to maximize the similarity between positive pairs while minimizing the similarity between the negative pairs, thereby approximating the optimization of mutual information as follows:\n$\\mathcal{L}_{InfoNCE} = -\\frac{1}{2m} (\\sum_{i} \\log D(S_{Z_{i}}) + \\sum_{i} \\log D(S'_{Z_{i}}))$ (4)\nIt is evident that when \u0410\u043d equals the identity matrix, ReSA and InfoNCE are entirely equivalent. In other words, ReSA guides the relationship among embeddings through soft assignments obtained via clustering, whereas InfoNCE employs the identity matrix as a hard matching target, strictly enforcing the maximization of distances between all negative pairs, which may inadvertently push samples belonging to the same category further apart during training, thereby disrupting the underlying semantic cluster structure (Wang & Liu, 2021; Huang et al., 2024)."}, {"title": "4.2. Impact of Image Augmentation on ReSA", "content": "Having introduced the learning process of ReSA, it is essential to consider another critical aspect of SSL: image augmentation, which has long been acknowledged as a key factor in enhancing the performance of self-supervised methods (Chen et al., 2020b; Grill et al., 2020). Standard practices involve employing a variety of complex transformations with random probabilities, such as ResizedCrop, ColorJitter, Grayscale, GaussianBlur, and HorizontalFlip, to increase the task's complexity and improve the robustness of the learned representations. However, for clustering-based SSL methods, overly aggressive augmentations can distort the original image information, making it harder for the model to discern meaningful patterns (Zheng et al., 2021), which may result in incorrect clustering assignments. To address this, we systematically evaluate the effect of each transformation technique on ReSA's clustering performance during training and its linear evaluation accuracy.\nSince ReSA only requires extracting clustering information from the encodings of one single view, we only need to adjust the image augmentation for that specific view, while the standard augmentation setting can be applied to the other view(s). Subsequently, we conduct experiments using the high-resolution ImageNet-100 (Tian et al., 2020) dataset, and the results are presented in Figure 6. It is evident that removing any single transformation improves the clustering performance of the representations, with ResizedCrop (replaced with fixed CenterCrop) having the most significant impact. However, its removal leads to a substantial decline in representation quality, indicating the critical role of ResizedCrop in learning invariance. The removal of ColorJitter, Grayscale, or GaussianBlur each results in improvements across various metrics, whereas removing HorizontalFlip causes a slight drop in val accuracies. Based on these findings, we design a weak augmentation"}, {"title": "5. Experiments on Standard SSL Benchmark", "content": "In this section, we conduct extensive experiments on standard SSL benchmarks to evaluate the effectiveness of ReSA. We perform pretraining from scratch on a variety of datasets, including CIFAR-10/100, ImageNet-100, and ImageNet (Deng et al., 2009), utilizing diverse encoders such as ConvNets and the ViT. Furthermore, we compare the performance of ReSA with state-of-the-art SSL methods across a range of downstream tasks, e.g. linear probe evaluation and transfer learning. The full PyTorch-style algorithm as well as details of implementation is provided in Appendix B.\n5.1. Evaluation for Classification\nEvaluation on small and medium size datasets. Following the benchmark in solo-learn (da Costa et al., 2022), we"}, {"title": "5.2. Analysis on Computational Overhead", "content": "In Table 4, we provide a fair comparison of the training costs among ReSA and several SSL methods. The results show that ReSA has memory consumption comparable to MoCoV3 but achieves faster training speeds, especially on ViTs, where it is nearly twice as fast. This improvement is attributed to ReSA's simpler image augmentation settings and the removal of batch normalization (BN) from the projector"}, {"title": "5.3. Transfer to Downstream Tasks", "content": "To evaluate the quality of representations learned by ReSA, we transfer our pretrained model to downstream tasks, including COCO (Lin et al., 2014) object detection and instance segmentation. For these tasks, we adopt the baseline codebase from MoCo (He et al., 2020). Notably, ReSA also achieves better performance compared to other methods on both tasks, highlighting its strong potential for transfer learning in downstream applications."}, {"title": "6. How ReSA Shapes Better Clustering Properties?", "content": "In this section, we utilize visualizations and additional experiments to illustrate the differences among the representations learned by ReSA and other SSL methods, as well as to investigate whether and how ReSA facilitates better clustering properties.\nFirstly, we track the evolution of clustering metrics for each component during ReSA training, as shown in Figure 9. It can be clearly observed that while ReSA exhibits relatively slow performance improvement in the early stages of training, it significantly outperforms other methods in the later stages. Interestingly, all components of ReSA demonstrate strong clustering properties, suggesting that leveraging high-quality clustering information from the encodings to guide the learning of embeddings enables the projector layers to also acquire robust clustering performance.\n6.1. ReSA Excels at Fine-grained Learning\nFurthermore, in Figure 7, we visualize the representation distributions learned by ReSA and other SSL methods on CIFAR-10 using T-SNE (van der Maaten & Hinton, 2008). Notably, the representations learned by ReSA exhibit clear separations between different classes, whereas those learned by other methods show varying degrees of overlap, making"}, {"title": "6.2. ReSA also Stands Out in Coarse-grained Representations", "content": "Finally, we explore ReSA's performance at the coarse-grained level. Specifically, we select CIFAR-100 for visualization, as its 100 classes can be grouped into 20 coarse-"}, {"title": "7. Conclusion", "content": "In this work, we demonstrate the feasibility of leveraging the rich clustering properties inherent in SSL models, particularly within encodings, to enable a positive-feedback mechanism. Building upon this, we propose ReSA, which exhibits exceptional performance across a wide range of benchmarks and excels at both fine-grained and coarse-grained learning.\nWe believe this dual capability would take a step toward addressing the long-standing challenge of reconciling the seemingly conflicting demands of fine-grained and coarse-grained visual representations within a unified framework, thereby advancing the development of large-scale visual foundation models."}, {"title": "C. Additional analyses on ReSA", "content": "C.1. Ablation Studies\nAs presented in Table 11, we conduct ablation studies to investigate the impact of network architecture design and temperature hyperparameter selection on ReSA performance. Our analysis reveals that employing a momentum network yields an accuracy improvement of approximately 2%, albeit at the cost of increased computational overhead. In contrast, the integration of an additional predictor demonstrates a comparable accuracy gain of around 1% while maintaining near-identical computational efficiency, exhibiting negligible impact on runtime performance.\nMeanwhile, in contrast to contrastive learning methods and other approaches such as SwAV and DINO, which typically require a small temperature value (e.g. r = 0.1), ReSA achieves favorable performance with a higher temperature value (\u03c4 = 0.4). This indicates that the optimization process of ReSA can effectively incorporate a broader range of samples, rather than focusing exclusively on hard samples (Wang & Liu, 2021), as is the case with other methods.\nAdditionally, we conduct experiments to test the extraction of clustering information from embedding to obtain the soft assignment \u0410\u043d. This finding is consistent with the analyses in Section 3, where we note that the clustering properties of embedding are less stable and inferior to those of encoding, making it challenging for the model to effectively learn high-quality clustering information."}, {"title": "C.2. How ReSA avoids representation collapse?", "content": "Although ReSA has demonstrated good performance in various experiments, it remains unclear how the model avoids feature collapse in the early stages of training when it has not yet learned any clustering information. To preliminarily address this question, we visualize the soft assignment matrix \u0410\u043d during the early stages of training. As shown in Figure 11, at model initialization, the assignment already exhibits a dominance of diagonal elements, reflecting an optimization trend"}, {"title": "C.3. The impact of weak augmentation on other methods", "content": "Since the design of weak augmentation (identical to that in ReSSL (Zheng et al., 2021)) provides a certain degree of performance improvement for ReSA, we also examine its impact on other self-supervised learning methods. Specifically, we select SwAV, VICReg, and MoCoV3, strictly following the experimental configurations in solo-learn (da Costa et al., 2022), with the only modification being the replacement of the image augmentation settings. As shown in Table 12, weak augmentation do not enhance the performance of these methods. This is likely because the standard augmentation settings have been extensively tuned through numerous experiments, ensuring optimal training conditions for these approaches."}, {"title": "C.4. Discussion and Future Work", "content": "The relationship between image augmentation and SSL via joint embedding architectures has grown increasingly intricate. Over the past few years, many studies (Chen et al., 2020b; Grill et al., 2020; Wagner et al., 2022; Morningstar et al., 2024) have emphasized the critical role of image augmentation in JEA, demonstrating that making subtle modifications to image augmentations, such as merely adjusting the parameters of ResizedCrop and ColorJitter, can significantly impact the performance of SSL models. However, recent works (Assran et al., 2023; Moutakanni et al., 2024) have begun to challenge this paradigm by exploring new self-supervised learning frameworks that eliminate the reliance on hand-crafted data augmentations. Interestingly, Moutakanni et al. (2024) successfully demonstrate that hand-crafted or domain-specific data augmentations are not essential for training state-of-the-art joint embedding architectures when scaling self-supervised learning. When the dataset size is sufficiently large, eliminating all hand-crafted data augmentations perfectly unleashes the innate potential of ReSA. We look forward to future research validating this hypothesis and applying ReSA to large-scale pretraining scenarios."}]}