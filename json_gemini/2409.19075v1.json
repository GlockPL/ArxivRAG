{"title": "Meta-RTL: Reinforcement-Based Meta-Transfer Learning for\nLow-Resource Commonsense Reasoning", "authors": ["Yu Fu", "Jie He", "Yifan Yang", "Qun Liu", "Deyi Xiong"], "abstract": "Meta learning has been widely used to exploit\nrich-resource source tasks to improve the per-\nformance of low-resource target tasks. Un-\nfortunately, most existing meta learning ap-\nproaches treat different source tasks equally,\nignoring the relatedness of source tasks to the\ntarget task in knowledge transfer. To mitigate\nthis issue, we propose a reinforcement-based\nmulti-source meta-transfer learning framework\n(Meta-RTL) for low-resource commonsense\nreasoning. In this framework, we present a\nreinforcement-based approach to dynamically\nestimating source task weights that measure\nthe contribution of the corresponding tasks to\nthe target task in the meta-transfer learning.\nThe differences between the general loss of the\nmeta model and task-specific losses of source-\nspecific temporal meta models on sampled tar-\nget data are fed into the policy network of the\nreinforcement learning module as rewards. The\npolicy network is built upon LSTMs that cap-\nture long-term dependencies on source task\nweight estimation across meta learning itera-\ntions. We evaluate the proposed Meta-RTL\nusing both BERT and ALBERT as the back-\nbone of the meta model on three commonsense\nreasoning benchmark datasets. Experimental\nresults demonstrate that Meta-RTL substan-\ntially outperforms strong baselines and previ-\nous task selection strategies and achieves larger\nimprovements on extremely low-resource set-\ntings.", "sections": [{"title": "1 Introduction", "content": "Commonsense reasoning is a basic skill of humans\nto deal with daily situations that involve reasoning\nabout physical and social regularities (Davis and\nMarcus, 2015). To endow computers with human-\nlike commonsense reasoning capability has hence\nbeen one of major goals of artificial intelligence.\nAs commonsense reasoning usually interweaves\nwith many other natural language processing (NLP)\ntasks (e.g., conversation generation (Zhou et al.,\n2018), machine translation (He et al., 2020)) and\nexhibits different forms (e.g., question answering\n(Talmor et al., 2019), co-reference resolution (Sak-\naguchi et al., 2020)), a wide variety of common-\nsense reasoning datasets have been created recently\n(Bisk et al., 2020; Sap et al., 2019), covering dif-\nferent commonsense reasoning forms and aspects,\nsuch as social interaction (Sap et al., 2019), laws\nof nature (Bisk et al., 2020).\nHowever, due to the cost of building common-\nsense reasoning datasets (Singh et al., 2021; Tal-\nmor et al., 2019) and the intractability of creating\na single unified dataset to cover all commonsense\nreasoning phenomena, commonsense reasoning in\nlow-resource settings is vital for commonsense rea-\nsoning tasks with specific forms and limited or no\ndata. To mitigate this data scarcity issue, a recent\nstrand of research is transfer learning with large\npre-trained language models (PLM), where PLMs\nare further trained on multiple source datasets and\nthen fine-tuned or directly tested on the target task\n(Lourie et al., 2021). Unfortunately, as PLMs\nusually have a large number of parameters and\nstrong memorization power, learning from source-\ntask datasets may force PLMs to memorize use-\nless knowledge of source datasets, causing negative\ntransfer (Yan et al., 2020).\nAnother promising approach to low-resource\nNLP is meta learning, which allows for better gen-\neralization to new tasks (Finn et al., 2017). Yan\net al. (2020) suggests that training a meta-learner\nfor PLMs is effective to capture transferable knowl-\nedge across different tasks. However, this method\ndoes not dynamically adjust the weights of source\ntasks at each iteration during the meta training for\nthe target task. All source tasks contribute equally\nto the meta model, which neglects the distributional\nheterogeneity in these tasks and different degrees\nof relatedness of these source tasks to the target"}, {"title": "2 Related Work", "content": "The proposed Meta-RTL is related to both meta\nlearning and commonsense reasoning, which are\nreviewed below within the constraint of space."}, {"title": "2.1 Meta Learning", "content": "Meta learning, or learning to learn, aims to en-\nhance model generalization and adapt models to\nnew tasks that are not present in training data. Re-\ncent years have gained an increasing attention of\nmeta learning in NLP (He and Fu, 2023). (Xiao\net al., 2021) propose an adversarial approach to\nimproving sampling in the meta learning process.\nUnlike our work, they focus on the same speech\nrecognition task in a multilingual scenario. (Chen\nand Shuai, 2021) use adapters to perform meta\ntraining on summarization data from different cor-\npora. The most related work to the proposed Meta-\nRTL is (Yao et al., 2021). The significant differ-\nences from them are two-fold. First, they focus\non different categories under the same task in CV\nwhile our interest lies in exploring multiple tasks in\ncommonsense reasoning for the low-resource target\ntask. Second, they simply utilize MLP to estimate\nweights at each step. In contrast, we use LSTM to\nencode the long-term information across training\niterations to calculate adaptive weights. To sum\nup, previous works either mechanically use a fixed\ntask sampling strategy or just take into account the\nvariability of different original tasks. Substantially\ndifferent from them, we propose a reinforcement-\nbased strategy to adaptively estimate target-aware\nweights for source tasks in the meta-transfer learn-\ning in order to enable weighted knowledge transfer."}, {"title": "2.2 Commonsense Reasoning and Datasets", "content": "A wide range of commonsense reasoning datasets\nhave been proposed recently. (Gordon et al., 2012)\ncreate COPA for causal inference while (Rahman\nand Ng, 2012) present Winogrand Scheme Chal-\nlenge (WSC), a dataset testing commonsense rea-\nsoning in the form of anaphora resolution. Since"}, {"title": "3 Meta-RTL", "content": "The proposed reinforcement-based meta-transfer\nlearning framework for low-resource common-\nsense reasoning is illustrated in Figure 1. It con-\nsists of three essential components: a PLM-based\ncommonsense reasoning model, a meta-transfer\nlearning algorithm that trains the PLM-based com-\nmonsense reasoning model and a reinforcement-\nbased target-aware weight estimation strategy that\nis equipped to the meta-transfer learning algorithm\nfor estimating source task weights."}, {"title": "3.1 PLM-Based Commonsense Reasoning\nModel", "content": "Commonsense reasoning tasks are usually in\nthe form of multiple-choice question answer-\ning. We hence choose a masked language\nmodel as the commonsense reasoning back-\nbone to predict answers. However, as different\ncommonsense reasoning datasets differ in the\nnumber of candidate answers (e.g., 2 candidate\nanswers per question in Com2sense vs 5 in\nCommonseseQA), a PLM classifier with a fixed\nnumber of classes is not a good fit for this\nscenario. To tackle this issue, partially inspired\nby (Sap et al., 2019), For each candidate answer,\nwe concatenate it with context, question into\n[CLS] (context)\u3008question\u3009[SEP](answeri\u3009[SEP],\nwhere [CLS] is a special token for aggregating\ninformation while [SEP] is a separator. We stack\na multilayer perceptron over the backbone to\ncompute a score \u0177i for answeri, with the hidden\nstate hCLS \u2208 RH.\n$\\hat{y}_i = W_2 tanh(W_1h_{CLS} + b_1)$ (1)\nwhere W\u2081 \u2208 RH\u00d7H, b\u2081 \u2208 RH, W2 \u2208 R1\u00d7H are\nlearnable parameters and H is the dimensionality.\nFinally, we estimate the probability distribution\nover candidate answers using a softmax layer:\n$Y = softmax([\\hat{y}_1, \u2026\u2026\u2026, \\hat{y}_N])$ (2)\nwhere N is the number of candidate answers. The\nfinal answer predicted by the model corresponds to\nthe context-answer pair with the highest probabil-\nity."}, {"title": "3.2 Meta-Transfer Learning Algorithm", "content": "The training procedure for the meta model is illus-\ntrated in Algorithm 1, which is composed of two\nparts: meta learning over multiple source tasks and\ntransfer learning to the target task."}, {"title": "3.2.1 Meta Learning over Multiple Source\nTasks", "content": "The meta learning procedure is presented in lines\n1-19 in Algorithm 1. For each meta training itera-\ntion, we use M source datasets. For each source\ndataset si, we randomly sample instances from it\nto construct a task Ts, for meta training, which\nis then randomly split into two parts: support\nset Tsup and query set Try, which do not over-\nlap each other. All source tasks are denoted as\nTs = {T$1,T$2,..., TSM }. The learning rates for\nthe inner and outer loop in the algorithm are differ-\nent: a denotes the learning rate for the inner loop,\nwhile \u1e9e for the outer loop.\nSi\nSi\nSi\nSi\nThe inner loop (lines 4-8) aims to learn source\ninformation from different source datasets. For\neach source task Ts\u2081, the task-specific parameters\nOT; (i.e., the temporal meta model as illustrated in\nFigure 1) are updated as follows:\n$O_{T_{S_i}} = \\theta - \\alpha \\nabla_{\\theta} L_{T_{S_i}^{sup}} (f(\\theta))$ (3)\nwhere the loss function LTsup\nLT sup (f(0)) is calculated\nSi\nby fine-tuning the meta model parameters on the\nsupport set Tssup.\nSi\nSi\nIn the outer loop, Lary (f(07)) is calculated\nquery\nquery\nwith respect to to update the meta model on\nthe corresponding query set Try.\nSi\nIt is worth noting that f (07\u2081) is an implicit func-\ntion of 0. As the second-order Hessian gradient\nSi\nmatrix requires expensive computation, we employ\nthe Reptile (Nichol et al., 2018) algorithm, which\nignores second-order derivatives and uses the differ-\nence between 0 and 07\u2081 as the gradient to update\nthe meta model:\nSi\n$0 = 0 + \\beta \\frac{1}{M} \\sum_{i=1}^{M}(O_{T_{S_i}} - \\theta)$ (4)\nWe keep running the meta learning procedure\nuntil the meta model converges. By meta learning,"}, {"title": "3.2.2 Transfer Learning to the Target Task", "content": "The transfer procedure is presented in lines 20-24\nin Algorithm 1. After performing meta learning,\nthe transfer module will be applied upon the meta\nmodel to bridge the gap between the learned meta\nrepresentations and the data distribution space of\nthe target dataset. We use the training data of the\ntarget task to fine-tune the meta model trained in\nthe meta learning procedure."}, {"title": "3.3 Reinforcement-Based Target-Aware\nWeight Estimation Strategy", "content": "For each meta training iteration, we calculate a gen-\neral loss Lo on the meta model f (0) using sampled\ndata from the target dataset (line 3). After optimiz-\ning f(0) with Tur according to Eq. (3), we obtain\na task-specific model f(07) for each source task\ntogether with a task-specific loss Ls; on the same\nsampled data as Lo (line 7).\nSi\nSi\nTo dynamically weight source tasks, we use the\ndifference between the general loss Lo and task-\nspecific loss Ls; as a guiding signal. Such a dif-\nference can measure how good the meta model\nis for the target dataset after being tuned by the\ncorresponding source task. In the traditional meta\ntraining as formulated in Eq. (4), all task-specific\nmodels are treated equally. Inspired by (Xiao et al.,\n2021), we use an LSTM-based network together\nwith an FFN and attention layer to capture the\nlong-term dependencies on historical weight es-\ntimation across meta training iterations. Since we\ndo not have any annotated data to train the LSTM-\nbased network, we use REINFORCE (Williams,\n1992), a policy gradient algorithm, for our proposed\nreinforcement-based source task weight estimation\nand use the guiding signal as the reward.\nLet f() denote the LSTM-based network\ntrained by reinforcement learning, & be parame-\nters to be tuned and r; as the difference for the j-th\nsource task, computed as follows:\n$r_j = L_0 - L_{S_j}$ (5)\nFor REINFORCE training, at each meta training\niteration t, we feed rt-1 = (rt-1,rt-1,...,rt-\u00b9)\ninto the policy network together with the proba-\nbilities Pt-1\n (P-1, P-1,..., P-1), which\nare estimated in the previous step for the policy\nnetwork. We then obtain a new updated prob-\nability distribution over source tasks denoted as\nPt = (P, P,..., P\u2081) from the output of the\npolicy network. In the meantime, we have up-\ndated rt = (r1,r,...,rm) accordingly. We\ntreat the estimation of the weights for the source\ntasks as a contextual bandit problem as in (Dong\net al., 2018). Formally, for the source tasks\n{T$1,T$2,..., Tsm}, we sample K tasks \u03c4 =\n{TT1, T2,..., TTK } as a trajectory to compute re-\nwards, where Tk \u2208 {S1, S2, ...,SM} and K is an\ninteger hyper-parameter. The gradients to update\nthe policy network can be calculated as:\n$\\nabla_{\\phi}J(\\phi) \\approx \\frac{1}{N} \\sum_{n=1}^{N}(R(\\tau^n) - \\hat{r})log f_{\\phi}(\\tau^n)$ (6)\nwhere is the baseline value to reduce the vari-\nance in the REINFORCE algorithm, T\u2122 is the n-th\nsampled trajectory in the total N sampled trajec-\ntory, R(n) = \u2211k=1rn denotes the rewards of\nthe trajectory.\nK\nTk\nAs shown in Eq. (6), we use the sampled tra-\njectories to collect rewards and gradients to update\nthe policy network. This procedure might quickly\nconverge to a local minimum and the policy would\nbecome a deterministic policy. To avoid this prob-\nlem, we incorporate the e-greedy technique into the\nsampling process and entropy regularization into\nthe gradient calculation.\nThe e-greedy technique regards the sampling\nprocess as a progressive process where previous\nsampling affects the probability of succeeding sam-\npling. The log of the trajectory which is required\nin Eq. (6) is hence computed as follows:\n$\\log f_{\\phi}(\\tau) = \\log[\\prod_{k=1}^{K} (\\frac{\\epsilon}{M-k+1} + (1-\\epsilon) * \\frac{P_{\\tau_{T_k}}}{\\sum_{z=1}^{M} P_{z}^{t}}])$ (7)\nBy setting e, we can control source task probabil-\nity estimation. Large e indicates a high probability\ntowards random sampling, which leads to a high\nexploration rate.\nFor entropy regularization, we use the probabil-\nity distribution Pt estimated by the policy network\nto calculate the entropy and combine it into the\npolicy network updating as:\n$\\nabla_{\\phi}J(\\phi) = \\nabla_{\\phi}J(\\phi) + \\rho \\sum_{m=1}^{M}(-P_m^{t} \\log P_m^{t})$ (8)\nwhere p is to control the rate of the entropy in the\nupdating gradient.\nWe average over the multiple sampled trajecto-\nries to estimate the weights of source tasks C =\n(C1, C2,..., CM) which can be calculated as:\n$C = \\frac{1}{NK} \\sum_{n=1}^{N} \\sum_{k=1}^{K} (C_{\\tau_{T_k}}^{t+1})$ (9)\nwhere T denotes the k-th chosen task in the n-th\ntrajectory obtained from Eq. (7)."}, {"title": "4 Experiments", "content": "We conducted experiments using 5 commonsense\nreasoning benchmark datasets and examined the\neffectiveness of the proposed model on 3 latest\ndatasets (i.e., Com2sense, Creak and RiddleSense).\nFor each dataset to be evaluated, we chose this\ndataset as the target dataset while the other 4\ndatasets as the source datasets. The details for\ndatasets and experimental settings are provided in\nAppendix A and B.\nWe compared our proposed Meta-RTL against\nthe following 4 baselines:\n\u2022 Target Fine-tuning that uses the training data\nin the target dataset to fine-tune the backbone\nmodel (BERT-base and ALBERT-xxlarge).\n\u2022 Reptile (Nichol et al., 2018) that uses the Rep-\ntile algorithm to train the meta model without\nany changes.\n\u2022 Task Combination that combines all the\nsource datasets together and uses the merged\ndataset to train the backbone model.\n\u2022 Temperature-based Reptile (Tarunesh et al.,\n2021) that estimates the sample probability\nas Pm = d/w/(M=1 dmw) where di is the\nsize of the i-th source dataset and wis the\ntemperature hyperparameter.\nAs test sets are not publicly available, we report\naccuracy results on development sets. On each tar-\nget dataset, we reported results under two settings:\nsupervised and unsupervised. The former used the\ncorresponding target dataset to fine-tune the trained\ncommonsense reasoning model while the latter did\nnot. The complexity analysis of our method against\nthe baselines is provided in Appendix C."}, {"title": "4.1 Main Results", "content": "Main results are displayed in Table 1. From the\ntable, we observe that:\n\u2022 Our proposed Meta-RTL significantly out-\nperforms the four baselines under both su-\npervised and unsupervised settings across all\nthree datasets. On Com2sense, Meta-RTL un-\nder the unsupervised setting is even much bet-\nter than the target fine-tuning method under\nthe supervised setting by up to 9.59 points\nwith ALBERT (66.62 vs. 57.03).\n\u2022 Heuristic methods including Task Comb. and\nTemp. Reptile cannot always boost the perfor-\nmance, e.g., results of both BERT (69.80 vs.\n68.49) and ALBERT (81.55 vs. 77.46) on the\nCreak dataset. The Temp. Reptile is not al-\nways better than Reptile (e.g., 70.62 vs. 70.71\nwith ALBERT on the Riddlesense dataset)."}, {"title": "4.2 Evaluation with Different Meta-Learning\nAlgorithms", "content": "We further conducted experiments with two differ-\nent widely-used meta learning algorithms to vali-\ndate the effectiveness of our proposed method.\nResults with FOMAML (Finn et al., 2017) and\nReptile (Nichol et al., 2018) are shown in Table\n2. Our proposed method is able to improve both\nmeta learning methods. However, the Temperature-\nbased method fails to improve FOMAML (see su-\npervised/unsupervised results of Temp. FOMAML\nvs. FOMAML in Table 2), which demonstrates that\nour proposed method is more flexible and can be\ndynamically adapted during the learning procedure.\nAs shown in Table 2, Meta-RTL (Reptile) is\nbetter than Meta-RTL (FOMAML) under both su-\npervised and unsupervised settings. We conjecture\nthat this could be due to our reward calculation\nmethod. Reptile directly uses the model parame-\nters to calculate the update gradient which is more"}, {"title": "4.3 Ablation Study on the Weight Estimation\nApproach", "content": "We further compared with several other meth-\nods to examine the effectiveness of the proposed\nreinforcement-based weight estimation approach.\nResults are shown in Table 3 (using Com2sense\nas the target dataset). \"TL\" indicates pure trans-\nfer learning from one or multiple source datasets\nto the target dataset. Specifically, we pretrain the\nbackbone model on specified source datasets and\nthen fine-tune it on the target dataset. As we can\nsee, pure transfer learning is not always able to\nimprove performance over the direct fine-tuning\non the target dataset (i.e., Target Fine-tuning in\nTable 3). Furthermore, simply putting all source\ndatasets together for transfer learning (denoted\nas Task Comb.)), despite achieving improvements\nover the target fine-tuning, is still inferior to our\nproposed method. And the Task Comb. cannot\nalways perform well on all datasets, as shown in\nTable 1.\nIn addition to pure transfer learning, we com-\npared our method with different weight estimation\nstrategies. Both Random and Greedy are based on\nReptile. The former randomly generates weights\nfor source tasks while the latter greedily determines\nweights of source tasks according to rewards, with-\nout taking long-term dependency into account (i.e.,\ncalculated as topK(r), using rewards from Eq. (5)).\nThe Random method is worse than Reptile un-\nder the unsupervised setting and marginally bet-\nter than Reptile under the supervised setting but\nstill much worse than Meta-RTL, suggesting that\nreward signals are important for weight estimation."}, {"title": "4.4 Evaluation on Extremely Low-Resource\nCommonsense Reasoning", "content": "We carried out experiments to evaluate Meta-RTL\non extremely low-resource settings. We randomly\nselected 1%, 5%, 10%, 20%, 30%, 40% instances\nfrom the RiddleSense dataset and used them to\nform new target datasets.\nResults with BERT are shown in Table 4. First,\nthe smaller the new target dataset is, the larger\nthe improvement of Meta-RTL over the target fine-\ntuning is gained, demonstrating the capability of\nthe proposed method on extremely low-resource\nsettings. Second, Meta-RTL is better than all three\nstrong baselines on all low-resource settings. Third,\nMeta-RTL trained with 40% data of RiddleSense\nis even better than the target fine-tuning with the\nentire data by 0.5 points (56.61 vs 56.22)."}, {"title": "4.5 Comparison to Previous Method on\nSource Task Selection", "content": "Previous approaches to multi-source meta-transfer\nlearning usually use a heuristic strategy to select\nsource tasks, e.g., according to the transferability\nfrom source tasks to the target task (Yan et al.,\n2020). These methods normally require a prepro-\ncessing step to detect suitable source tasks and\ntreat all chosen source tasks equally during meta\nlearning, not allowing to dynamically adjust the\nweights of source tasks for meta learning. We com-"}, {"title": "5 Conclusion", "content": "In this paper, we have presented a reinforcement-\nbased meta-transfer learning framework Meta-RTL\nfor low-resource cross-task commonsense reason-"}]}