{"title": "ARTIFICIAL LIVER CLASSIFIER: A NEW ALTERNATIVE \u03a4\u039f\nCONVENTIONAL MACHINE LEARNING MODELS", "authors": ["Mahmood A. Jumaah", "Yossra H. Ali", "Tarik A. Rashid"], "abstract": "Supervised machine learning classifiers often encounter challenges related to performance, accuracy,\nand overfitting. This paper introduces the Artificial Liver Classifier (ALC), a novel supervised learning\nclassifier inspired by the human liver's detoxification function. The ALC is characterized by its\nsimplicity, speed, hyperparameters-free, ability to reduce overfitting, and effectiveness in addressing\nmulti-classification problems through straightforward mathematical operations. To optimize the\nALC's parameters, an improved FOX optimization algorithm (IFOX) is employed as the training\nmethod. The proposed ALC was evaluated on five benchmark machine learning datasets: Iris\nFlower, Breast Cancer Wisconsin, Wine, Voice Gender, and MNIST. The results demonstrated\ncompetitive performance, with the ALC achieving 100% accuracy on the Iris dataset, surpassing\nlogistic regression, multilayer perceptron, and support vector machine. Similarly, on the Breast\nCancer dataset, it achieved 99.12% accuracy, outperforming XGBoost and logistic regression. Across\nall datasets, the ALC consistently exhibited lower overfitting gaps and loss compared to conventional\nclassifiers. These findings highlight the potential of leveraging biological process simulations to\ndevelop efficient machine learning models and open new avenues for innovation in the field.", "sections": [{"title": "Introduction", "content": "Artificial intelligence (AI) has many branches according to the tasks to be performed, with machine learning (ML) being\none of the most well-known branches that has gained prominence alongside the development of computer science. It\nfocuses on developing systems and algorithms that automatically learn from data without explicit programming [1, 2, 3].\nHowever, two main types of ML are categorized according to the problem to be solved: supervised learning and\nunsupervised learning. Supervised learning relies on having pre-labeled input data (denoted X) and the desired output\n(denoted y). This type of learning aims to understand the hidden relationship between inputs and outputs to predict new\noutcomes based on unseen (new) input data [4, 5, 6]. On the other hand, unsupervised learning uses input data that is\nnot pre-labeled (does not contain output y). Instead, an unsupervised learning model is applied to discover patterns and\nhidden relationships in the data autonomously based on the input data only [7, 8]. Furthermore, there are other types of\nML such as reinforcement learning (RL), which interact directly with the problem's environment to build policies that\nguide decision-making based on rewards and penalties obtained through trial and error [9, 10, 11, 12].\nIn the early stages of AI, researchers focused on building systems (with minimal intelligence) capable of performing\nspecific tasks using fixed rules (conditional and logical operations). As the field evolved, scientists realized that"}, {"title": null, "content": "intelligent systems needed methods to learn from data, rather than relying on rigid rule-based methods with minimal\ncapabilities [13, 14]. As a result, supervised learning algorithms, specifically classifiers, emerged as tools for learning\nsystems to make predictions or decisions based on the available experiences. However, one of the most preeminent\nalgorithms in supervised learning is artificial neural network (ANN), inspired by the fundamental concept of neurons in\nthe human brain and how they are interconnected [15]. These networks are based on the concept of neurons, which are\nbasic units in the brain that communicate with each other to perform processes such as thinking and learning [16]. The\nalgorithm simulates the functions of brain cells by proposing multiple layers of artificial neurons (an input layer and an\noutput layer). These neurons interact with each other using weights assigned to each connection, and the role of the\nalgorithm is to optimize these weights to minimize the error resulting from interactions with the input data, thereby\nproducing accurate outputs [17]. Moreover, an older algorithm inspired by mathematics is the logistic regression (LR),\nwhich aims to find a perfect line that best fits the data points, minimizing the error between actual and predicted labels.\nThese methods were used in statistical analyses before being adopted in ML [18]. The complexity of linear operations\nincreased, leading to more sophisticated methods, such as support vector machine (SVM), where the main idea is to\ncreate clear boundaries between different data classes by maximizing the margin between them [19]. Comprehensively,\nmost of ML classifiers have drawn their inspirations from mathematical operations or nature (e.g., simulating the\nfunctioning of human brain cells) to create robust systems (classifiers) for solving complex problems. Current ML\nclassifiers face multiple challenges related to performance, accuracy or loss, overfitting, and handling data with complex\nand nonlinear patterns [20, 21].\nIn this context, this paper proposes a new classifier called artificial liver classifier (ALC), inspired by the human\nliver's biological functions. Specifically, it draws on the detoxification function, highlighting its ability to process\ntoxins and convert them into removable forms. Additionally, improvements have been made to FOX optimization\nalgorithm (FOX), a state-of-the-art optimization algorithm, to enhance its performance and ensure compatibility with\nthe proposed ALC. The research aims to bridge the gap in current ML's algorithms by combining the simplicity of\nmathematical design with solid performance by simulating the detoxification function in the human liver. Furthermore,\nthe proposed classifier aims to improve classification performance by processing data dynamically, simulating the\nhuman liver's adaptive ability, enabling its application in fields requiring high-precision solutions and flexibility in\ndealing with different data patterns. The main challenge lies in transforming the liver's detoxification function into a\nsimplified mathematical model that effectively incorporates properties such as repetition, interaction, and adaptation to\nthe data [22]. By comparing the proposed classifier with established ML classifiers, the study expects to improve the\nperformance of ML, including increased computation speed, better handling of overfitting problems, and avoidance\nof excessive computational complexity. Additionally, this paper introduces a new concept for drawing inspiration\nfrom biological systems, opening up extensive opportunities for researchers to develop mathematical models based on\nother biological functions of the liver, such as filtering blood or amino acid regulation [23]. Moreover, it represents a\nstarting point for interdisciplinary applications combining biology, mathematics, and AI, enhancing our understanding\nof incorporating natural processes into ML techniques to create efficient, reliable, and intelligent systems.\nThe proposed ALC has been evaluated using a variety of commonly used ML datasets, including Wine, Breast Cancer\nWisconsin, Iris Flower, MNIST, and Voice Gender [24], which are explained in detail in Section 4.1. This diversity\nin the datasets ensures extensive coverage of different data types, including text, images, and audio, and enables\nhandling binary and multi-class classification problems [25, 26, 27]. The purpose of using these datasets is to conduct\ncomprehensive tests to assess the performance of the proposed ALC and compare it with the established classifiers. The\noriginality and contributions that distinguish this research are as follows:\n1.  Introducing a new classifier inspired by the liver's biological functions, specifically detoxification, highlighting\nnew possibilities in designing effective classification algorithms based on biological behaviour.\n2.  Enhancing the FOX to improve its performance, address existing limitations, and ensure better compatibility\nwith the proposed ALC.\n3.  Relying on simple mathematical models that simulate the liver's biological interactions, ensuring a balance\nbetween design simplicity and high performance.\n4.  Opening new avenues for researchers to draw inspiration from human organ functions, such as the liver, and\nsimulate them in computational ways to contribute innovative solutions for real-world challenges.\n5.  Testing the proposed ALC on diverse datasets demonstrates its effectiveness through experimental results and\ncomparisons with established classifiers.\nThis paper is structured as follows: Section 2 reviews the literature that has attempted to address classification issues\nacross various data types. Section 3 provides an analytical overview of the human liver, focusing on detoxification\nfunction and the study's motivation. Section 4 present the used materials and the proposed methodology, including the\nimprovement of classifier design and FOX training algorithm. Sections 5 and 6 cover the presentation and analysis"}, {"title": "Related Works", "content": "This section reviews the standard algorithms used in ML classification, with their practical applications across various\ndatasets highlighted [28]. Additionally, recent studies in the field are discussed to identify existing challenges and to\nshed light on research gaps requiring further attention [29]. Accordingly, the extent to which the proposed classifier can\noffer practical solutions to these gaps and contribute to the future advancement of the field will be investigated. However,\nXiao et al. utilized 12 standard ML classifiers on the MNIST dataset, demonstrating its suitability as a benchmark for\nevaluating the proposed ALC. Their results identified the Support Vector Classifier (SVC) with a polynomial kernel\n(C=100) as the best-performing model, achieving an accuracy of 0.978 [30]. This comparable result poses a challenge\nfor the proposed ALC to surpass. Furthermore, the study [31] employed online pseudo-inverse update method (OPIUM)\nto classify the MNIST dataset, achieving an accuracy of 0.9590. However, the author noted that these results do\nnot represent cutting-edge methods but rather serve as an instructive baseline and a means of validating the dataset.\nThis makes it feasible to compare the performance of the proposed ALC against OPIUM, as surpassing this baseline\nwould demonstrate an improvement over existing methods. On the other hand, in a comparative study by Cortez et\nal., three classifiers\u2014SVM, multiple regression (MR), and ANN\u2014were evaluated on the Wine dataset. The SVM\nmodel demonstrated superior performance, achieving accuracies of 0.8900 for red wine and 0.8600 for white wine,\noutperforming the other methods with an average accuracy of 0.8790 [32]. Hence, the findings of Cortez et al. serve as\na foundation for further advancements in ML applications, providing a basis for evaluating the proposed ALC.\nAnother study utilized a recursive recurrent neural network (RRNN) on Breast Cancer Wisconsin dataset. The results\ndemonstrated that the proposed model achieved an accuracy of 0.9950 [33]. Despite its outstanding performance,\nthe computational demands of RRNN require substantial resources, which may limit their applicability in resource-\nconstrained environments. Moreover, the study [34] presents a new classification model called CS3W-IFLMC. This\nmodel incorporates intuitionistic fuzzy (IF) and cost-sensitive three-way decisions (CS3WD) approaches, contributing\nto improved classification accuracy and reduced costs associated with incorrect decisions. The proposed model has been\nevaluated using 12 benchmark datasets, demonstrating superior performance compared to large margin distribution\nmachine (LDM), FSVM, and SVM. However, the study remains limited in scope, as it focuses solely on binary\nclassification tasks and does not extend to multi-class classification problems [34]. Furthermore, in another study,\nthe researchers examined gender classification (male or female) based on voice data using multi-layer perceptron\n(MLP). The findings showed that the MLP model outperformed several other methods, including LR, classification and\nregression tree (CART), random forest (RF), and SVM. The MLP achieved a classification accuracy of 0.9675. This\nstudy concluded that the proposed model demonstrates strong discriminative power between genders, which enhances\nits applicability in auditory data classification tasks [35].\nThe reviewed literature, highlights significant advancements in classification models, primarily focusing on improving\nperformance and addressing computational challenges. However, several limitations and research gaps remain. One\nmajor issue is the reliance on computationally intensive methods, which can hinder applicability in resource-constrained\nenvironments. The absence of practical hyperparameter tuning or reduction mechanisms may also contribute to\noverfitting and computational inefficiencies. These limitations underscore the need for a new classifier to address such\nchallenges. Hence, the proposed ALC should emphasize simplicity in design to ensure faster training time with lower\ncost."}, {"title": "Detoxification in Liver and Motivation", "content": "The liver, as illustrated in Figure 1, is the largest internal organ in the human body and is vital in numerous complex\nphysiological processes. It is located in the right upper quadrant of the abdominal cavity and consists of two primary\nlobes, the right and left, surrounded by a thin membrane known as the hepatic capsule [36]. Internally, the liver is\ncomposed of microscopic units called hepatic lobules. These hexagonal structures contain hepatic cells organized around\na central vein. The lobules are permeated by a network of hepatic sinusoids, which are small channels through which\nblood flows, facilitating the exchange of oxygen and nutrients between the blood and hepatic cells [37]. Furthermore,\nthe liver receives blood from two sources, each contributing different functions. The oxygenated blood enters via the\nhepatic artery from the aorta, meeting the liver's energy demands. While, the portal vein delivers nutrient-rich and\ntoxin-rich blood from the gastrointestinal tract and spleen [38]. The blood from both sources mixes in the hepatic\nsinusoids, allowing the hepatic cells to perform metabolic and regulatory functions efficiently [39]."}, {"title": null, "content": "However, detoxification is one of the most important liver's functions, which removes toxins from the bloodstream [40].\nDetoxification occurs in two phases. In the phase I, hepatic enzymes known as cytochrome P450 chemically modify\ntoxins through oxidation and reduction reactions, altering their structures to make them more reactive [41]. In the phase\nII, the modified compounds are conjugated with water-soluble molecules such as sulfates or glucuronic acid, making\nthem easier to excrete [42]. Finally, the toxins are either excreted via bile into the digestive tract or removed from the\nbloodstream by the kidneys [43]."}, {"title": "Materials and Methods", "content": "This section presents the standard datasets employed for evaluating the proposed ALC in the conducted experiments.\nAdditionally, the architecture of the proposed ALC is provided, including mathematical equations, algorithms, and\nflowcharts. Furthermore, the section elaborates on the FOX, which serves as the learning algorithm for the proposed\nALC, highlighting its improvements."}, {"title": "Materials", "content": "The following datasets are widely used by ML researchers to evaluate their work, making these benchmark datasets\nsuitable for this paper. The MNIST dataset comprises 70,000 grayscale images of handwritten digits (0\u20139), each of size\n28 \u00d7 28 pixels. It is widely used for multi-class classification tasks due to its diversity and large size [45]. As this dataset\nis image-based, feature vectors were extracted using the linear discriminant analysis (LDA) method, which effectively"}, {"title": null, "content": "reduces data dimensionality while emphasizing significant features. This process yielded nine features per image to\nconstruct the feature vectors [46]. Additionally, the Iris dataset, a small-scale collection containing 150 instances across\nthree classes with four features per instance, was included in the proposed ALC evaluation [47, 48, 49]. The Breast\nCancer Wisconsin dataset, a binary dataset containing 569 samples with 30 features each, was employed to assess the\nproposed ALC's performance on high-dimensional data [50, 33]. Furthermore, the Wine dataset, consisting of 178\nsamples across three classes with 13 features per instance, was selected for its multi-class nature [48, 51]. Finally, the\nVoice Gender dataset was employed to ensure feature diversity. This dataset comprises 3,168 samples, each defined by\n21 acoustic features, aimed at distinguishing gender (male or female) by leveraging unique vocal characteristics [35].\nThese datasets collectively provided a diverse range of classification challenges, enabling a comprehensive evaluation\nof the proposed ALC's performance."}, {"title": "Methods", "content": "This section begins with a detailed introduction to the architecture of the proposed ALC. Moreover, it delves into the\nimprovements made to the FOX as a learning algorithm, highlighting its key modifications."}, {"title": "Artificial Liver Classifier", "content": "As explained earlier in Section 3, the detoxification process involves the liver's ability to process toxins. Oxygenated\nblood enters the liver via the hepatic artery, while nutrient-rich blood flows through the portal vein. These sources mix\nwithin the hepatic sinusoids, enabling hepatic cells to perform essential functions, including a detoxification function\nthat comprises two phases.\nPhase I: toxins are chemically modified to become more reactive. This phase is mathematically simulated by the\nfollowing equation:\n$A_{ji} = \\frac{1}{n} \\sum_{k=1}^{n} (X_{jk} \\times cofactor_{ki}) + \\frac{1}{fp} \\sum_{k=1}^{f} \\sum_{l=1}^{p} cofactor_{kl}$\nwhere $A_{ji}$ is the matrix of reactive toxins, X is the input toxins matrix (feature vector) and n is the number of outputs\n(labels). The cofactor matrix is is initialized randomly within the range [-1,1] and has dimensions (f,p), where f\ncorresponds to the number of features in the input feature vector, and p is the number of lobules. The human liver\ncontaining many lobules, with approximately 100,000 [52]. The p play a crucial role in the phase I and should be tuned\nappropriately based on the problem. Thus, selecting p with the range $100, 000 > p > f$ is typically sufficient and can\noften be determined through trial and error. Additionally, the term $\\sum_{k=1}^{f} \\sum_{l=1}^{p} cofactor_{kl}$ represents the mean of all\nelements in the cofactor matrix that used to balance the reaction.\nHowever, the reactive toxins (A) must be activated to enhance their reactivity before progressing to phase II. This\nactivation involves eliminating all negative values, effectively transforming them to zero while retaining only the\npositive values. This process is mathematically expressed by the following equation:\n$A' = max(0, A)$\nwhere A' is the activated toxins matrix.\nPhase II: involves the conjugation of modified compounds from phase I with water-soluble molecules to make them\nexcretable. This phase reduces the toxicity of compounds and facilitates their elimination from the body. this phase can\nbe mathematically modeled using Equation 1, but with key differences. Instead of toxins, the matrix A' is used as input,\nrepresenting the modified compounds (activated toxins) generated in phase I. Additionally, a matrix referred to as the\nvitamin matrix is employed in place of the cofactor matrix. This vitamin matrix is initialized randomly within the range\n[-1,1] and has dimensions (p, n).\n$B_{ji} = \\frac{1}{p} \\sum_{k=1}^{n} (A'_{jk} \\times vitamin_{ki}) + \\frac{1}{pn} \\sum_{k=1}^{p} \\sum_{l=1}^{n} vitamin_{kl}$\nwhere $B_{ji}$ represents the conjugated compounds and $\\sum_{k=1}^{n} \\sum_{l=1}^{n} vitamin_{kl}$ represents the mean of all elements in\nthe vitamin matrix.\nFinally, after completing the reactions in both phase I and phase II, the detoxification process is concluded. The result\nis a set of less harmful and water-soluble wastes that can be excreted through bile, urine, stool, and other pathways."}, {"title": null, "content": "The elimination process is modeled using the softmax activation function, which provides probabilistic outputs for\neach class [53, 54, 55]. These probabilities represent the confidence scores for the likelihood of different detoxified\ncompounds being excreted.\n$B'_{i} = \\frac{e^{B_i}}{\\sum_{i=1}^{n} e^{B_i}}$\nwhere $B'_{i}$ represents the normalized probability for output class i.\nThe Algorithm 1 and Figure 2 describes the architecture of the proposed ALC. First, the cofactor and vitamin matrices\nare initialized randomly, where these matrices are defined based on the dimensions corresponding to the number of\nfeatures (f), number of lobules (p), and number of output classes (n). Next, the IFOX, as presented in Algorithm 2, is\nconfigured, specifying the number of detoxification cycles (maximum number of epochs) and detoxification power\n(maximum number of fox agents). The target objective function for optimization is the REACTION procedure in\nAlgorithm 1, where the IFOX optimizes the cofactor and vitamin matrices by minimizing the reaction error (i.e., loss).\nThe optimized cofactor and vitamin matrices, resulting from the optimization process, are subsequently applied within\nthe REACTION along with the toxins (feature vector) to predict the output classes."}, {"title": "Training Algorithm", "content": "The FOX, developed by Mohammed and Rashid in 2022, mimics the hunting behavior of red foxes by incorporating\nphysics-based principles. These include prey detection based on sound and distance, agent's jumping during the\nattack governed by gravity, and direction, as well as additional computations such as timing and walking [56, 17].\nThese features make FOX a competitive optimization algorithm, outperformed several methods such as particle swarm\noptimization (PSO) and fitness dependent optimizer (FDO). The FOX is works as follows: Initially, the ground is\ncovered with snow, requiring the fox agent to search randomly for its prey. During this random search, the fox agent\nuses the Doppler effect to detect and gradually approach the source of the sound. This process takes time and enables\nthe fox agent to estimate the prey's location by calculating the distance. Once the prey's position is determined, the fox\nagent computes the required jump to catch it. Additionally, the search process is facilitated through controlled random\nwalks, ensuring the fox agent progresses toward the prey while maintaining an element of randomness. The FOX\nbalances exploitation and exploration phases statically, with a 50% probability for each [57]. Thus, the FOX operates as\nfollows:\n1.  Computing the distance $D_{i}$ of sound travel using the best position and random time:\n$D_{i} = \\frac{Best Position}{T_{i}} \\times T_{i}$\nWhere $T_{i}$ is a random time in [0, 1] and i is the fox agent.\n2.  Determining the distance between the fox agent and its prey:\n$DF = 0.5 \\times D_{i}$\n3.  Computing the jump $J_{i}$ by multiplying half of the gravity acceleration constant with half squared mean of the\ntime:\n$J_{i} = 0.5 \\times 9.81 \\times 0.5 \\times \\sum_{i=1}^{n} (T_{i})^{2}$\n4.  Updating the fox agent's position based on a directional equation, either northward $C_{1} = 0.18$ or in the opposite\ndirection $C_{2} = 0.82$ based on the the jump probability $p$ in [0, 1].\n$X_{i+1} = DF \\times X + J_{i} \\times \\begin{cases} C_{1}, & \\text{if } p > 0.18 \\\\ C_{2}, & \\text{otherwise} \\end{cases}$\n5.  The following equation used for exploration:\n$X_{i+1} = BestPosition \\times rand(1, dim) \\times Mint \\times a$\nwhere $dim$ is the problem dimension, $Mint$ is the minimum time iteratively updated based on $T_{i}$, $a$ is an\nadjustment parameter computed as: $2 \\times (it - \\frac{(Max.}{2})$), and it is the current iteration.\nHowever, the FOX has some limitations in its design. These limitations were acknowledged by the author of FOX [56],\nwhile others have been identified through further analysis. For instance, one notable drawback is its static approach to\nbalancing exploration and exploitation. This paper aims to address these limitations by proposing a new variation of\nthe FOX called IFOX to make it integrable with the proposed ALC as a training algorithm to optimize the cofactor\nand vitamin matrices. For reference, the implementation of the FOX can be accessed at https://github.com/\nhardi-mohammed/fox."}, {"title": null, "content": "The IFOX, as visualized in Algorithm 2, incorporates several improvements over the FOX. First, it transforms the\nbalance between exploitation and exploration into a dynamic process using the e-greedy method, rather than a static\napproach [58, 59]. This dynamic adjustment is controlled by the parameter a, which decreases progressively as the\noptimization process iterate. Second, the computation of distances is eliminated in favor of directly using the best\nposition, facilitated by the parameter \u00df, derived from a. This modification simplifies the FOX by removing Equations 5\nand 6, and simplifying Equation 8 by eliminating the probability parameter p and the directional variables (c\u2081 and c\u2082).\nThird, in Equation 9, the variables a and Mint are excluded. Finally, the results shown in Figure 4 demonstrate that\nIFOX outperforms the FOX on the CEC2019 benchmark test functions, establishing it as a suitable training algorithm\nto integrate it with the proposed ALC."}, {"title": "Results", "content": "This section presents the performance results of the proposed ALC on multiple benchmark datasets, as described in\nSection 4.1. The experimental parameter settings were configured for each dataset as follows: 500 detoxification cycles,\na detoxification power of 10, and dataset-specific numbers of lobules. Specifically, the number of lobules was set to 10\nfor Iris Flower and Breast Cancer Wisconsin, 15 for Wine and Voice Gender, and 50 for MNIST. These values were\nadjusted based on several trials and errors. Moreover, each dataset has been split into a training set comprising 80% of\nthe entire dataset and a validation set comprising 20%. To facilitate later comparison and analysis, additional classifiers,\nincluding MLP, SVM, LR, and XGBoost (XGB), were executed on the same datasets. However, all experiments were\nconducted on an MSI GL63 8RD laptop equipped with an Intel\u00ae Core\u2122 i7-8750H \u00d7 12 processor and 32 GB of\nmemory. This consistent setup ensured a robust evaluation of the proposed ALC alongside the other classifiers under\nthe same conditions."}, {"title": "Performance Metrics", "content": "To evaluate the performance of the proposed ALC, several metrics were employed, including log loss (cross-entropy\nloss), accuracy, precision, recall, F1-score, and training time. Initially, Log loss (Equation 10) quantifies the divergence\nbetween predicted probabilities and actual labels, where lower values indicate better predictive performance [60].\nThe accuracy (Equation 11) measures the proportion of correctly classified instances, serving as a straightforward\nindicator of overall correctness. Moreover, precision (Equation 12) evaluates the proportion of true positives among all\npositive predictions, emphasizing the model's ability to reduce false positives. In contrast, recall (Equation 13) focuses\non the proportion of true positives among all actual positive instances, highlighting the importance of minimizing\nfalse negatives. Furthermore, the F1-score (Equation 14), as the harmonic mean of precision and recall, provides\na balanced assessment when class distributions are imbalanced [61]. Moreover, the overfitting gap defined as the\ndifference between training and validation accuracy, provides insights into generalization. A smaller value indicate\nbetter generalization, while a larger value indicates overfitting, where the model excels on the training set but struggles\nwith unseen data. Finally, the training time reflects the duration required to train the model, offering insight into its\ncomputational efficiency.\n$Log Loss = -\\frac{1}{n} \\sum_{i=1}^{n} (y_{i} log(\\hat{y}_{i}) + (1 - y_{i}) log(1 - \\hat{y}_{i}))$\n$Accuracy = \\frac{TP+TN}{TP+FP+TN + FN}$\n$Precision = \\frac{TP}{TP+FP}$\n$Recall = \\frac{TP}{TP+FN}$\n$F1-Score = 2 \\times (\\frac{Precision \\times Recall}{Precision + Recall})$\nwhere TP, TN, FP, and FN represent the true positive, true negative, false positive, and false negative counts, respectively.\nAdditionally, y denotes the actual labels, while y represents the predicted labels."}, {"title": "Experimental results", "content": "The performance results of the proposed ALC are presents through this subsection, summarized in the figures and tables.\nAdditionally, comparisons with other classifiers, including MLP, SVM, LR, and XGB, have been conducted on the five\ndatasets described in Section 4.1."}, {"title": "Discussion", "content": "The results presented in Section 5.2, derived from experiments conducted on the datasets described in Section 4.1,\nhighlight the superior performance of the proposed ALC compared to other classifiers. However, a more in-depth\nstatistical analysis is necessary, particularly of the validation set results, as they are considered more reliable indicators of\nclassifier performance due to being obtained from unseen data. The statistical analysis presented in Table 6 compare the\nperformance of the proposed ALC with four classifiers-XGB, SVM, MLP, and LR-across the five datasets described\nin Section 4.1. The analysis focuses on four metrics: loss, accuracy, overfitting gap, and training time, with statistical\nsignificance determined using the Wilcoxon signed-rank test at a threshold of P-value < 0.05. The Wilcoxon signed-\nrank test is used to compare paired samples, particularly when data may not follow a normal distribution. It assesses\nwhether the differences between paired observations are statistically significant [62]. Hence, this analysis results provide\ninsights into the strengths of the proposed ALC in terms of its generalization, accuracy, and computational efficiency."}, {"title": "Conclusions", "content": "In conclusion, this paper suggests a novel supervised learning classifier, termed the artificial liver classifier (ALC),\ninspired by the human liver's detoxification function. The ALC is easy to implement, fast, and capable of reducing\noverfitting by simulating the detoxification function through straightforward mathematical operations. Furthermore,\nit introduces an improvement to the FOX optimization algorithm, referred to as IFOX, which is integrated with the"}, {"title": null, "content": "ALC as training algorithm to optimize parameters effectively. Furthermore, the ALC was evaluated on five benchmark\nmachine learning datasets: Iris Flower, Breast Cancer Wisconsin, Wine, Voice Gender, and MNIST. The empirical\nresults demonstrated its superior performance compared to support vector machines, multilayer perceptrons, logistic\nregression, XGBoost and other established classifiers. Despite these superiority, the ALC has limitations, such as longer\ntraining times on large datasets and slower convergence rates, which could be addressed in future work using methods\nlike mini-batch training or parallel processing. Finally, this paper underscores the potential of biologically inspired\nmodels and encourages researchers to simulate natural functions to develop more efficient and powerful machine\nlearning models."}]}