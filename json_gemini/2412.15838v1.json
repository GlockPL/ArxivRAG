{"title": "Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback", "authors": ["Jiaming Ji", "Jiayi Zhou", "Hantao Lou", "Boyuan Chen", "Donghai Hong", "Xuyao Wang", "Wenqi Chen", "Kaile Wang", "Rui Pan", "Jiahao Li", "Mohan Wang", "Josef Dai", "Tianyi Qiu", "Hua Xu", "Dong Li", "Weipeng Chen", "Jun Song", "Bo Zheng", "and Yaodong Yang"], "abstract": "Reinforcement learning from human feedback (RLHF) has\nproven effective in enhancing the instruction-following ca-\npabilities of large language models; however, it remains un-\nderexplored in the cross-modality domain. As the number of\nmodalities increases, aligning all-modality models with hu-\nman intentions \u2013 such as instruction following \u2013 becomes\na pressing challenge. In this work, we make the first at-\ntempt to fine-tune all-modality models (i.e. input and out-\nput with any modality, also named any-to-any models) us-\ning human preference data across all modalities (includ-\ning text, image, audio, and video), ensuring its behavior\naligns with human intentions. This endeavor presents sev-\neral challenges. First, there is no large-scale all-modality\nhuman preference data in existing open-source resources,\nas most datasets are limited to specific modalities, predom-", "sections": [{"title": "1. Introduction", "content": "Our world is inherently multimodal [62, 100, 126]. Hu-\nmans perceive the world through various sensory organs,\nacquiring information in multiple modalities such as text,\nimages, audio, video, and others. These different forms of\ninformation often complement and interact with each other.\nEach sensory channel has unique advantages in conveying\nspecific concepts and enhancing our understanding of the\nworld. With the success of large language models (LLMs)\n[1, 7, 128], researchers aim to extend these models to handle\nmultiple modalities, enabling them to perceive and generate\nany modality [5, 65, 98, 120]. This would allow the models\nto respond using the most appropriate modality, achieving\ntruly human-like AI [16, 34, 39, 52, 64, 104, 108, 109, 117].\nConsequently, the research community has actively de-\nveloped foundational models capable of handling arbitrary\nmodalities [58, 108]. Based on LLMs, people use individ-\nual image/text encoders or domain-specific decoders for in-\nput or output processing [65, 108, 124, 135], leveraging the\nMoE architecture [60] and diffusion techniques [41]. In this\nline of work, each modality is encoded by a single encoder,\nwith non-text modality information being mapped to the\ntext space via projection layers. Additionally, Chameleon\n[98] has experimented with encoding images during the\npre-training phase using fully token-based representations\nto handle both image and text modalities. However, han-\ndling more modalities remains a significant challenge.\nOn the other hand, Reinforcement learning from hu-\nman feedback (RLHF) plays a significant role in aligning\nmodels with human intentions [81]. GPT-4 [1] has ef-\nfectively boosted the model's instruction-following using\nRLHF. LLaMA series models [99] have significantly im-\nproved the performance in code, mathematics, and reason-\ning through the post-training method, e.g., DPO [87]. How-\never, this line of work is limited to single modality. Re-"}, {"title": "2. Datasets", "content": "A primary challenge lies in the fact that existing prefer-\nence datasets are predominantly focused on single-modal\ntasks [26, 52, 73, 120], lacking comprehensive datasets\nthat encompass all modalities, thus limiting further research\nprogress.\nIn response, we open-source the first all-modality hu-\nman preference dataset \u2013 align-anything-200k \u2013 to enhance\nthe instruction-following capabilities of all-modality mod-\nels across tasks such as question answering, complex rea-\nsoning, etc. During the annotation process, we observed"}, {"title": "2.1. All-Modality Human Preference Dataset", "content": "The align-anything-200k aims to capture human prefer-\nences across 8 subtasks of all modalities, as shown in\nFig. 2. With the increase in modalities, the complexity\nand inconsistency of human intent preferences rise signifi-\ncantly, driven by the unique semantic characteristics of each\nmodality. To achieve consistent all-modality preference\nmodeling, we decouple annotation targets into two types,\nwhich serve as evaluation metrics for instruction-following:\n\u2022 Modality-agnostic refers to dimensions that are applied\nuniversally across modalities, including: (1) Prompt ad-\nherence, which requires responses to be consistent with\nthe input prompts, accurately reflecting the specified ele-\nments. (2) Rule conformity, where responses adhere to\nlogical, physical, or scientific principles related to the\nscenario or theme described in the prompt. (3) Informa-\ntion richness, which emphasizes that responses should be\nthorough and detailed in addressing the query.\n\u2022 Modality-specific represents preference dimensions tai-\nlored to the characteristics of each modality. For exam-\nple, in video-related subtasks, we introduce three addi-\ntional dimensions, temporal consistency, content coher-\nence and motion naturalness to better evaluate the details\nin the output regarding the duration, dynamics, etc.\nBy decomposing the instruction-following dimensions,\nwe establish the evaluation standards for all-modality pref-\nerence annotation. More details about the annotation docu-\nment can be found in Appendix: Datasets."}, {"title": "2.2. One More Thing \u2013 Language Feedback", "content": "During the annotation process, there exists a decline in hu-\nman preference consistency when directly conducting bi-\nnary preference annotations. As illustrated in Tab. 1, the\nintroduction of different modalities makes it challenging\nfor binary preferences to fully capture human preference\n[47, 59, 90]. \u03a4o address this issue, we introduce language\nfeedback, utilizing natural language to describe discrepan-"}, {"title": "3. Learning from Language Feedback", "content": "In this section, we introduce learning from language feed-\nback (LLF). It utilizes language feedback to optimize re-\nsponses, synthesizing preference data which can enhance\nthe performance of all-modality alignment. Firstly, we re-\nview the RLHF pipeline including PPO [81] and DPO [87],\nhighlighting the limitations of binary preferences feedback.\nThen we demonstrate how to practically implement LLF,\nincluding two main stages, feedback modeling and self im-\nproving. Finally, we empirically verify that LLF achieves\nan average 5.83 times improvement across 5 modalities, 5\nopen-sourced models, and 7 popular benchmarks."}, {"title": "3.1. Background and Preliminary", "content": "PPO consists of two main stages including: step 1: pref-\nerence modeling and step 2: policy optimization. The for-\nmer involves the collection of comparison data, essential for\ntraining the reward model rrm(\u00b7|\u00b7). The process starts with\nresponse pairs (Y1, Y2) generated by the initial model from\nshared prompts \u00e6. Human annotators are then tasked with\nselecting their preferred response from each pair, denoted\nas yw > Yix, where yw and yi denote the preferred\nand dispreferred answer. The latter (step 2) is guided by the\n'RM(). This process is commonly modeled as a bandit\nsetting, where a reward is obtained from the PRM at the end\nof each response. The RL objective is,\n$O_{RL} = \\underset{\\theta}{\\arg \\max} \\mathbb{E}_{x \\sim P_{x},y \\sim \\pi_{\\theta}(\\cdot | x)}[P_{RM} (Y | x)]$.\nDPO directly fine-tunes the model aligning with preference\npairs yw > Y\u0131 | x. It consolidates the two stages of pref-\nerence modeling and policy optimization in RLHF into one\nstage. Let 00 denote the initial model parameters. The opti-"}, {"title": "3.2. Practical Implementation", "content": "Inspired by Constitutional AI [10], LLF comprises two\nsteps: feedback modeling and self-improving as shown in\nFig. 4. The former employs SFT to train the feedback\nmodel, enabling it to provide language feedback based on\nx and y. The latter allows the model to refine its responses\nbased on the language feedback c.\nFeedback Modeling The training process utilizes a\ndataset D = {(Xi, Yi, Ci)}1, where N is the size of\ndataset, xi denotes the prompt, y represents the response,"}, {"title": "4. Evaluation: Eval-Anything", "content": "Currently, evaluating all-modality models relies on human\nexperts for assessments, which is inefficient and costly.\nWhile combining benchmarks for individual modalities\ncould offer a broader evaluation [125], differences in data\npreparation, post-processing, and metrics across bench-\nmarks hinder accurate performance assessment. Addition-\nally, all-modality models uniquely select the appropriate\nmodalities based on user queries, enabling seamless cross-\nmodal synergy, a capability that traditional single-modality\nevaluation pipelines fail to capture fully.\nTo address this gap, we deliver our evaluation frame-\nwork specifically designed for all-modality models \u2013 eval-\nanything including (1) all-modality understanding\n(AMU) for assess models to simultaneously process and in-\ntegrate information from all modalities and (2) all-modality\ngeneration (AMG): evaluate a model's ability to follow\nuser instructions, autonomously select modalities, and work\nsynergistically across different modalities for output."}, {"title": "4.1. Composition of Evaluation Dataset", "content": "All-modality models aim to both understand individual\nmodalities and combine information across them to gener-\nate high-quality responses. To assess their comprehensive\nmultimodal processing, we create 164 test entries, each con-\ntaining textual, visual (image or video), and auditory (audio\nor speech) components. These interconnected modalities\nrequire the model to integrate all inputs accurately, as fail-\nure in any one modality leads to incorrect answers. For in-"}, {"title": "4.2. Evaluation Results & Analysis", "content": "Human and AI Agreement In the modality synergy task,\nafter training on the 5k preference dataset, the experiment\nreveals a 66.4% agreement rate between the judging model\nand human annotators. These figures are consistent with\nhuman agreement ratios reported in similar studies on mod-\neling human preferences [113] in the multimodal large lan-\nguage models domain.\nInput vs Outputs Most models in Tab. 4 support partial\nmodality input and have baseline scores, but Gemini-1.5-\nPro outperforms others due to its ability to process all three\nmodalities. In the AMG task, the average scores is rela-\ntively low, with no model demonstrating a clear advantage\nacross all sub-items. The results indicate that, compared to"}, {"title": "5. Conclusion", "content": "In this work, we make the first exploration of fine-tuning\nall-modality models using human preference data across di-\nverse modalities, to ensure alignment with human inten-\ntions. We have open-sourced the align-anything dataset, in-\ncorporating 200k annotated human preference data across\nmodalities. Our proposed alignment method leverages lan-\nguage feedback to capture complex, modality-specific hu-\nman preferences, significantly enhancing the model's abil-\nity to follow instructions. To assess the all-modality models,\nwe developed an evaluation benchmark: eval-anything. All\ndata, models, and code have been made openly available."}, {"title": "6. Ethic Responsibility", "content": "Our data collection has been approved by an Institutional\nReview Board (IRB). The IRB file contains institutional in-\nformation. To maintain anonymity in the double-blind re-\nview process, we did not upload the IRB documents along-\nside the supplementary materials. If needed, we are will-\ning to discuss the IRB file further with the Ethics Reviewer,\nprovided it does not compromise the double-blind review\nprotocol."}, {"title": "7. Open-Source Assets and License", "content": "All datasets examples, codes, and demos have been attached\nto our supplementary material. In Sec. 6, we discuss poten-\ntial risks and mitigation strategies related to model open-\nsourcing in detail. After the double-blind review process,\nwe will actively engage with community feedback regard-\ning the data, framework, and code and promptly address any\nissues related to version inconsistencies to further advance\nscientific research on large model alignment.\nThe following assets are planned for open-source release\nafter the double-blind review process:"}, {"title": "8. More Details of Related Works", "content": "Building on the success of large language models (LLMs)\nand the latest advancements in multimodal large language\nmodels (MLLMs), there is growing anticipation for inte-\ngrating multiple modalities into a single model to achieve\ntruly all-modality capabilities, i.e., all-modality models.\nHowever, numerous challenges must be overcome to reach\nthis goal. We will introduce the related works from the as-\npects of dataset, algorithms, and evaluation."}, {"title": "9. Align-Anything Framework", "content": "Our work is built on the framework of align-anything,\nwhich is designed for training and evaluation across all\nmodalities. As shown in Fig. 9, the align-anything frame-\nwork aims to align all-modality large models, including\nlarge language models (LLMs), vision language models\n(VLMs), and others, with human intentions and values.\nOverall, this framework has the following characteristics:"}, {"title": "9.1. Training Part", "content": "The align-anything framework integrates all-modality\nalignment algorithms (e.g., Reinforcement Learning from\nHuman Feedback, RLHF), supporting SFT, RM, DPO, and\nPPO. Additionally, align-anything implements KTO [32],\nSimPO [76], and ORPO [42] in the text-to-text modality.\nBesides, align-anything offers a highly scalable model reg-\nistration mechanism and currently supports the training and\ndeplying over 25 models. For more details, please refer to\nthe align-anything-code/README.md file of our\nsupplementary materials."}, {"title": "9.2. Evaluation Part", "content": "The align-anything evaluation framework now supports\nover 30 commonly used benchmarks, covering all com-\nmon modalities. For more details, please refer to the\nalign-anything-code/README.md file of our sup-\nplementary materials."}, {"title": "10. Training Details", "content": "This section will introduce the implementation details of\nlearning from language feedback (LLF), hyper-parameter\nsettings, case studies, and the computational devices in-\nvolved in the experiments."}, {"title": "10.1. Implementation Details", "content": "LLF comprises two primary steps: feedback modeling and\nself improving. The first step employs maximum likeli-\nhood to enable the model to learn from align-anything-200k\nhow to generate language feedback for a given prompt and\nresponse. This includes evaluating the response (critique\npart) and providing suggestions for improvement (refine-\nment part). During the self improving phase, when opti-"}, {"title": "11. Dataset Card", "content": "As the number of modalities increases, current all-modality\nmodels encounter significant challenges in effectively fol-\nlowing instructions. These challenges include difficulties\nin comprehending multimodal instructions and generating\noutputs that align with the intended directives [73, 118,\n120]. While RLHF has demonstrated its effectiveness in\naddressing such issues for specific modalities, such as text\nand images [120], its applicability to all-modality scenarios\nremains uncertain.\nFurthermore, existing preference datasets predominantly\nfocus on single-modal tasks, lacking the comprehensive in-\nformation required to capture the intricacies of all-modality\nfeatures. In response, we introduce align-anything-200k,\nthe first all-modality human preference dataset designed\nto enhance the instruction-following capabilities of all-\nmodality models. This dataset encompasses eight sub-\ntasks across text, image, audio, and video modalities"}, {"title": "11.2. Instruction-Following Dimensions", "content": "In the context of text modality, instruction-following refers\nto the ability of LLMs to effectively execute human-\nprovided instructions, such as answering questions or sum-\nmarizing text. This capability enables them to function as\nhelpful, harmless, and honest assistants [81, 99]. However,"}, {"title": "11.2.1 Modality-Agnostic Dimensions", "content": "In this section, we provide a detailed overview of modality-\nagnostic dimensions for evaluating instruction-following in\nall-modality scenarios.\nPrompt adherence Prompt adherence refers to the ex-\ntent to which responses align with the given input prompts,\naccurately reflecting the specified elements, themes, or in-"}]}