{"title": "\u041c\u0406\u041d-TCCT: Mitigating Inconsistent Hallucinations in LLMs via Event-Driven Text-Code Cyclic Training", "authors": ["Xinxin You", "Xien Liu", "Qixin Sun", "Huan Zhang", "Kaiyin Zhou", "Shaohui Liu", "GuoPing Hu", "ShiJin Wang", "Si Liu", "Ji Wu"], "abstract": "Recent methodologies utilizing synthetic datasets have aimed to address inconsistent hallucinations in large language models (LLMs); however, these approaches are primarily tailored to specific tasks, limiting their generalizability. Inspired by the strong performance of code-trained models in logic-intensive domains, we propose a novel framework that leverages event-based text to generate corresponding code and employs cyclic training to transfer the logical consistency of code to natural language effectively. Our method significantly reduces inconsistent hallucinations across three leading LLMs and two categories of natural language tasks while maintaining overall performance. This framework effectively alleviates hallucinations without necessitating adaptation to downstream tasks, demonstrating generality and providing new perspectives to tackle the challenge of inconsistent hallucinations.", "sections": [{"title": "1. Introduction", "content": "Achieving human-like logical consistency in reasoning is essential for advancing artificial general intelligence (AGI) (Achiam et al., 2023; Pan et al., 2023). Despite significant advancements in large language models (LLMs) in addressing a variety of natural language processing tasks (Jiang et al., 2024; M\u00fcndler et al.; Ribeiro et al.), these models still tend to produce hallucinations due to their inability to maintain logical rigor (Zhang et al., 2023; Bao et al., 2024). These manifest in two main forms: 1) Input-conflicting hallucination, where LLMs generate content that diverges from the provided input (Dale et al., 2023; Zhong et al., 2021a); and 2) Context-conflicting hallucination, where LLMs create content that contradicts information they have previously generated (Pu et al., 2023; Liu et al., 2024), as shown in Figure 1. Given that these hallucinations may occur in various domains, there is an urgent need for a universal approach to effectively address these issues across different tasks.\nRecent studies have emphasized the effectiveness of synthetic datasets in mitigating in-consistent hallucinations related to specific tasks. For instance, synthetic mathematical datasets (Toshniwal et al., 2024; Huang et al., 2024) have shown reduced contradictions in the calculation process and significant improvements in solving mathematical problems. Additionally, some research has synthesized reasoning data in the logical reasoning (Nie et al., 2020; Saeed et al., 2021) field while providing the reasoning process (Dalvi et al., 2021) during synthesis. LOGIC-LM and SymbCOT (Pan et al., 2023; Xu et al., 2024) integrate LLMs with symbolic solvers to enhance logical problem-solving. Unfortunately, the synthetic datasets that have been developed are often tailored to specific tasks or domains, which can restrict their generalizability.\nAdvancements in training LLMs on extensive code datasets, such as CoCoGen and CodeRL, have significantly reduced inconsistent hallucinations in code generation tasks (Madaan et al., 2022; Le et al., 2022). However, these improvements have not effectively transferred to general natural language tasks due to fundamental differences in semantics, language styles, and structures between code and natural language. The precision of code contrasts with the ambiguity of human language, posing challenges in adapting code-focused techniques for natural language understanding and generation. Recent approaches, such as program-of-thought (Chen et al., 2023) and program-assisted LMs (Gao et al., 2023), have sought to bridge this divide by interpreting natural language problems and generating corresponding code solutions. However, these methodologies fail to successfully transfer the logical consistency capabilities gained from code training to a broader spectrum of NLP tasks.\nInspired by code-oriented LLMs that excel in generating logically consistent code, we propose that integrating code data is vital for enhancing the ability of LLMs to maintain logical consistency across various NLP applications. Furthermore, we observe that there exists a correspondence between the structure of event-based text and that of code. The event-based text can be transformed into structured code, utilizing programming constructs such as classes, objects, and functions, with the correspondence between the two structures shown in Figure 2. Thus, we introduce MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via Event-Driven Text-Code Cyclic Training. This method facilitates the cyclic training of code and the original text, continuously aligning the stylistic characteristics of the two \"languages\" to improve dual generation capabilities. Ultimately, this approach aims to transfer the logical rigor of code data into natural language, fundamentally enhancing the logical consistency of LLM outputs. This novel integration not only fosters more reliable reasoning processes, but also paves the way for developing AI systems capable of delivering coherent and contextually accurate responses, representing a significant advancement in the capabilities of language models.\nWe summarize the contributions of our method as follows:\n\u2022 We propose MIH-TCCT, a novel framework that enhances the logical consistency of LLMs outputs across diverse NLP tasks by cyclically generating event-based text and corresponding code. This framework effectively transfers the logical rigor of code to natural language, addressing critical challenges related to inconsistent hallucination.\n\u2022 We identify a clear correspondence between the structures of event-based text and code. This finding not only supports our cyclic training methodology but also paves the way for future research bridging the gap between textual and coding modalities.\n\u2022 Through our method, LLMs achieve the capability to maintain logical consistency, effectively mitigating hallucinations without adapting to downstream tasks, thereby demonstrating generalizability. Experiments across three LLMs and two categories of natural language tasks reveal a significant reduction in hallucinations."}, {"title": "2. Related Work", "content": "Recent investigations indicate that hallucination phenomena in large language models (LLMs) may stem from inherent issues within the training data. This highlights the necessity for high-quality reasoning datasets to mitigate these inconsistencies (Zhang et al., 2023; Jiang et al., 2024). Notable examples include LOGIQA (Liu et al., 2023), a dataset of logical reasoning derived from the Chinese Civil Service Examination; RECLOR (Yu et al.) and AR-LSAT (Zhong et al., 2021b), based on standardized graduate admissions exams; and FOLIO (Han et al., 2022), a dataset featuring first-order logic annotations. However, the resource-intensive and time-consuming nature of their creation limits the scalability and accessibility of training applications.\nAs a result, researchers are increasingly exploring synthetic data as a viable alternative (Achiam et al., 2023; Dubey et al., 2024). Significant synthetic mathematical datasets include KPDDS, OpenMathInstruct-1, and MathGenie (Toshniwal et al., 2024; Huang et al., 2024; Lu et al., 2024). KPDDS synthesizes question-answer pairs using key points and exemplars, yielding KPMath, which encompasses over one million pairs. OpenMathInstruct-1 comprises 1.8 million problem-solution pairs synthesized from code-interpreter solutions for GSM8K and MATH benchmarks via the Mixtral model. MathGenie enhances a seed dataset by generating new questions through a back-translation approach. Despite these advancements, synthetic datasets are often tailored to specific tasks, particularly mathematics, which may restrict their generalizability and performance across diverse real-world applications. This underscores the necessity for further research to broaden their applicability."}, {"title": "2.2. COT Data to Reduce Hallucinations", "content": "Based on the investigation of synthetic datasets, Chain-of-Thought (CoT) strategies have also reduced hallucinations during reasoning tasks (Wei et al., 2022). Some research has synthesized reasoning data in the field of logical reasoning (Nie et al., 2020; Saeed et al., 2021) while providing the reasoning process (Dalvi et al., 2021) during synthesis. For example, LOGIC-LM integrates LLMs with symbolic solvers, transforming natural language problems into symbolic formulations to minimize inconsistencies (Pan et al., 2023). Similarly, SymbCOT enhances CoT prompting by incorporating symbolic expressions and logical rules (Xu et al., 2024).\nThe expressiveness of symbolic solvers limits the applicability of these models. Not all problems can be easily encoded in first-order logic, and complexities may arise when dealing with intricate grammatical structures, such as those found in probabilistic soft logic. Therefore, while these approaches show promise, they have constraints in flexibility and generalizability, highlighting the need for solutions that can effectively address a broader range of reasoning scenarios."}, {"title": "2.3. Code Data to Reduce Hallucinations", "content": "Recent research has shown that training large language models (LLMs) on code datasets can reduce in-consistent hallucinations. Notable models such as CoCoGen and CodeRL have enhanced performance in structured reasoning and code generation by leveraging the strict syntax and semantics inherent in programming languages (Madaan et al., 2022; Le et al., 2022). However, the benefits observed in code-related tasks do not readily extend to general natural language processing (NLP) tasks due to the fundamental semantic differences between the two domains.\nInnovative approaches like Program of Thought (PoT) utilize LLMs, particularly Codex, to express reasoning processes as programs while offloading computational tasks to external systems (Chen et al.). Despite this novel framework, PoT may encounter challenges in contexts where translating natural language into code is not straightforward, which can introduce potential errors. Similarly, Program-Aided Language Models (PAL) enable LLMs to decompose natural language problems into executable steps, delegating execution to environments such as Python interpreters (Gao et al., 2023). While this structural separation simplifies the model's role, it also restricts its ability to engage in comprehensive reasoning, making it susceptible to errors during decomposition. Consequently, the effective transfer of logical consistency gained from code training to broader NLP applications remains an open challenge, highlighting the need for further research."}, {"title": "3. Method", "content": "Our method is grounded in a novel insight: texts containing event descriptions reveal a coherent structural framework that can be seamlessly mapped to structured programming languages. These events\u2014defined by participants, their attributes, causes, processes, and outcomes-form a rich structured information framework. This clear delineation allows us to effectively map these elements onto programming constructs such as functions, objects, and classes, as illustrated in Figure 2. For example, participants can be modeled as class instances, with their characteristics represented as attributes. Additionally, the causes and processes of events can be articulated through functions, enabling the translation of natural language descriptions into structured code. This cognitive framework bridges the gap between textual and coding modalities and supports our cyclic training methodology.\nIn the following sections, we will focus on several key study modules. Section 3.1 will define the problem and outline the research objectives. Section 3.2 will address the filtering of appropriate texts that describe events. In Section 3.3, we will discuss how the cyclic generation of text and code enhances the logical consistency of LLMs. Section 3.4 will present methodologies for assessing the quality of the generated code."}, {"title": "3.1. Problem Definition", "content": "This study investigates a large language model (LLM) that is tasked with generating text, represented as a sequence of sentences, denoted by:\n$X = [X_1, X_2, ..., X_{|x|}],$ (1)\nwhere x representing the entire sequence of generated sentences, and $|x|$ denotes the total number of sentences in that sequence. Each sentence is represented by $X_i$.\nThe generation process is typically influenced by a user-defined prompt p that outlines specific objectives for the text generation and an input text t that provides relevant background information. The generation of the sequence of sentences x can be expressed probabilistically as:\n$x \\sim LLM(\\cdot|p, t),$ (2)\nWe investigate the phenomenon of context inconsistency in outputs generated by large language models (LLMs), specifically focusing on two types of inconsistencies.\nInput-Conflicting Hallucination: We define input-conflicting hallucination as occurring when the generated sequence x is inconsistent with either the prompt p or the text t. This can be formally expressed as:\n$H_{incon}(x,p) \\text{ if } (x \\neq p) \\land Contradict(x, p).$ (3)"}, {"title": "3.3. Text-Code Cylic Training", "content": "Predicting corresponding code from text input, and vice versa, lies at the heart of our generation's training, as shown in Figure 3. By iterating through this process using cyclic generation training, we harmonize text and code within a shared semantic space. Predicting code from text enhances the model's ability to maintain coherence and consistency throughout the generation process while predicting text from code helps bridge these distinct forms of semantic representation. This dual predictive approach not only reinforces the connection between text and code but also serves as a powerful strategy for enhancing language models' overall coherence and consistency. Below, I will provide a detailed introduction\nBased on the LLM model, designated as G, to convert the input text t into corresponding structured code c. The generation process is guided by prompts, represented as $p_{t2c}$, with specific instructions detailed in the Appendix A.2. This process can be expressed mathematically as:\n$c \\sim G(p_{t2c}, t),$ (8)\nIt is important to note that the instructions in the Appendix A.2 include a sample we crafted ourselves. This sample serves as a crucial reference for the base LLM model, enabling it to effectively comprehend our prompts and generate code in alignment with the target text. Without such a sample, the model would be unable to fulfill the task according to our expectations.\n Based on the base LLM model, we convert the input text into the corresponding structured code language. The generation process is guided by prompts, represented as $p_{c2t}$, with specific instructions detailed in the Appendix A.3. This process can be expressed mathematically as:\n$t \\sim G(p_{c2t}, c),$ (9)\nTo provide a more intuitive understanding, Figure 4 presents an example of converting event-driven text into code. In our case, we extract essential details about Gajendrakumar"}, {"title": "3.4. Quality Assessment of Generated Code", "content": "During iterative training, the code2text approach utilizes the original text as groundtruth, whereas the text2code process lacks a definitive \"correct\" code as groundtruth. This discrepancy hinders the implementation of iterative training. Therefore, a dynamic quality assessment of the code generated in each iteration is essential. This assessment can provide reliable feedback for text2code training, thus ensuring its effectiveness in subsequent model training.\nWe propose a quality evaluation method for dynamically generated code by integrating text and code. Given that the generated code is executable, we can represent entity names from the original text by invoking the properties of instantiated objects within the code. This process results in a reorganized text denoted as $C_{reorg}$. To generate $C_{reorg}$, We further instruct the LLM to generate this reorganized text based on predefined instructions $P_{reorg}$ (detailed in the Appendix A.4)). The process can be represented as:\n$C_{reorg} \\sim G(\\cdot|P_{reorg},t,c)$ (10)\nWe show an example in Figure 4 to elucidate this process. The sentence $C_{reorg}$ in Figure 4 can be reorganized as:\nSubsequently, we utilized a Python code interpreter to execute the corresponding code c as illustrated in Figure 4. Then, we replaced the placeholders with the output values obtained from running the code, resulting in the following generated content, denoted as $C_{reorg}'$:\nIf the quality of the generated code is high, the executed output will be more accurate, making $C_{reorg}'$ more consistent with the original text t. Conversely, if the quality of the parallel corpus is lower, the generated content may contain more errors and exhibit a reduced degree of similarity to the original text.\nTo systematically classify the quality of the generated output, we define a decision function D(S) based on the calculated similarity score:\n$D(S) = \\begin{cases}\n1 & \\text{if } S(C_{reorg}', t) > T \\\\\n0 & \\text{if } S(C_{reorg}', t) \\leq T\n\\end{cases}$ (11)\nLet T be a predetermined threshold that distinguishes high-quality generated code from lower-quality outputs. The similarity score S is computed using the (ROUGE-L+ROUGE-1)/2, and if S falls below this threshold (set at 0.85 for our experiments), the generated data is deemed difficult and discarded. In contrast, if S meets or exceeds the threshold, it is considered to have the potential to generate high-quality parallel data and proceeds to the code-to-text phase. In our experiments, we set the number of iterative generations to 3; after each round of generation, all text outputs predict all corresponding code (including those discarded in the previous round). As the model improves its ability in iterative generation through training, we observe a higher proportion of successful text-to-code conversions. This demonstrates an enhancement in the text-code iterative generation capability, as well as the internal alignment of semantics and styles between the two languages. Ultimately, this allows the logical rigor of code to be transferred to natural language."}, {"title": "4. Experiment", "content": "We assess the performance of several prominent base models, including Qwen2.5-7B(Yang et al., 2024), Llama 3.1 Instruct-8B(Dubey et al., 2024), and Ministra-8B-Instruct(Jiang et al., 2023). These models were selected based on their state-of-the-art capabilities in natural language processing tasks, providing a diverse range of architectures for our analysis.\nWe selected the wiki-40b-en dataset (Guo et al., 2020) as source data for training in the consistency improvement of the model. This is a large-scale text dataset based on the English Wikipedia website. We validate our method on two tasks: text summarization using the CNN/Daily Mail dataset (Chen et al., 2016), which is a widely recognized benchmark, and question answering (QA) with HaluEval (Li et al., 2023), which assesses hallucination performance.\nWe selected the following baselines: 1) Base Model: Each of the selected models is evaluated in its original, instruct versions; 2) Naive Prompting: This approach involves adding straightforward, consistent instructions to guide the models' responses; 3) Supervised Fine-Tuning (SFT): Each base model undergoes SFT specific to the datasets used for the tasks; 4) SymbCOT (Xu et al., 2024): A recent method that reformulates chain-of-thought (CoT) reasoning as symbolic CoT to enhance consistency. The instructions for the 1) base and 2) prompt methods are shown in the Appendix A.5.\nFor the summarization task, we utilize AlignScore (Zha et al., 2023) and consistency metric in UniEval(Zhong et al., 2022) to assess the phenomenon of inconsistent hallucination. In addition, we evaluate the summarization performance using its other three key metrics: Coherence, Fluency, and Relevance. For the QA task using the HaluEval dataset(Li et al., 2023), which is specifically designed to assess inconsistent hallucination, the actual performance of the QA task has not been evaluated. Instead, we use AlignScore(Zha et al., 2023) and Anah-V2(Gu et al., 2024) as metrics to investigate inconsistent hallucination, without focusing on task performance.\nDuring the training in our method, we train each LLM with LLaMA-Factory on 4 \u00d7 24GB NVIDIA GeForce RTX 4090 GPUs. We use LoRA with a rank of 8 to accelerate training, with a learning rate of 1e-4. Unless otherwise specified, the training is conducted for 3 epochs, and the total batch size is 32. During the testing stage, since the models used are all instruct versions, we utilize their default chat templates and employ greedy decoding for text generation."}, {"title": "4.2. Main Result", "content": "Table 1 presents the results of inconsistency hallucination for various baseline models across summarization and question-answering tasks, with optimal results highlighted in bold. Higher values are generally preferred except for the Anah-V2 metric, where lower values are favorable. The MIH-TCCT method achieves the highest performance across all metrics for the three models on the CNN/Daily Mail task. This trend is also seen in the HaluEval dataset, where optimal results are primarily associated with the SFT and MIH-TCCT methods. These findings indicate that MIH-TCCT effectively mitigates inconsistency hallucinations in generative models, ensuring logical output coherence.\nAmong the five evaluated methods, SymbCOT and MIH-TCCT enhance model consistency without task-specific adaptations, whereas SFT requires such adaptations. Overall, SFT and MIH-TCCT lead in performance, followed by SymbCOT and Prompt, with base methods showing the least favorable results. Notably, MIH-TCCT shows a significant advantage over SymbCOT and often surpasses the adapted SFT method. This analysis highlights MIH-TCCT's effectiveness in reducing inconsistency hallucinations, reinforcing its superiority over other methods in enhancing the coherence of generative models.\nAdditionally, when comparing the impact of different baselines, the Llama 3.1-Instruct model consistently outperforms the other two models, particularly in base methods, achieving optimal results across both datasets. In terms of stability, models generally show consistent performance in summarization tasks, while QA tasks exhibit greater variability. Despite these differences, the MIH-TCCT method consistently demonstrates strong adaptability and stability across both task types."}, {"title": "4.3. Model Ablation", "content": "As shown in Figure 5, removing the filter, cyclic training, and quality assessment modules resulted in average declines of 0.97%, 1.93%, and 0.89% across all consistency metrics for both tasks, with the cyclic training module having the most significant impact. The results confirm the effectiveness of various components within the MIH-TCCT framework.\nAs shown in Figure 6,the average reductions in the four metrics were 0.16%, 0.41%, and 0.14%, respectively. The effect on summarization performance was minimal, which may be attributed to our framework reducing the need for downstream task adaptation, thus making the performance impact on downstream tasks negligible."}, {"title": "4.4. Analysis and Discussion", "content": "To assess whether our approach affects the overall performance of the task while reducing inconsistent hallucinations, we examine the metrics of coherence, consistency, fluency, and relevance in the summarization task, with detailed results presented in Table 2. Optimal results are highlighted in bold. Table 2 indicates that our MIH-TCCT method effectively maintains task performance across most metrics without significant decline. In contrast, while the SFT method shows consistent competitive results, it experiences notable declines in relevance and coherence, indicating instability. This emphasizes the advantage of MIH-TCCT in providing balanced performance across all metrics while effectively reducing inconsistencies."}, {"title": "4.4.2. ANALYSIS OF THE CYCLIC TRAINING", "content": "The text-code cyclic training not only enhances the alignment capabilities of the LLM in both text and programming spaces but also progressively improves its generative abilities. As illustrated in Figure 2, the similarity scores for both the reconstructed text and the original text increase with the number of training epochs, with a maximum difference of 5.23%. Simultaneously, the proportion of data meeting the quality standards steadily rises, achieving an enhancement of up to 14.55%. Additionally, by the third training round, the upward trend stabilizes, leading us to set the training duration at three epochs."}, {"title": "4.4.3. IMPACT OF HOMOGENEOUS DATA", "content": "The main experiments were conducted on the wiki-40b-en dataset, with validation on heterogeneous data demonstrating the model's logical consistency and generalization capability. To further assess performance, we examined the impact of increasing homogeneous data during training. Figure 8 presents the results from varying proportions of mixed homogeneous data using the Llama3.1-Instruct base model on the CNN-Daily Mail dataset for summarization tasks. The x-axis is on an exponentially increasing scale, while the vertical axis shows the model's performance, including hallucination metrics via AlignScore, Consistency, and the multiplicative results of three other summarization metrics, labeled as Summ-CM. Point 0.0 reflects results from heterogeneous data alone, with red lines indicating changes relative to these results.\nAs illustrated in Figure 8 (details in Table 5 of the Appendix), our findings suggest that a small proportion of mixed homogeneous data is sufficient for aligning the semantic spaces of code and data, resulting in significant improvements in consistency metrics. While increasing the homogeneous data enhances the logical consistency of the output, the overall performance metrics for summarization tasks decline, stabilizing at a parameter setting of 0.4. Consequently, we selected 0.4 for further experiments, with"}, {"title": "4.4.4. ANALYSIS OF THE TEXT FILTER MODULE", "content": "We randomly selected 10,000 entries from the wiki-40b-en dataset to evaluate the text filter. As shown in Figure 9 in the Appendix, only 53.74% of the texts describe events, underscoring the need for an event-type text filter. To assess the filter's performance, we extracted 100 samples each of event-type and non-event-type texts. These samples were annotated by three doctoral students, with final classifications determined through a voting process. The confusion matrix in Figure 9 shows a high true positive rate (0.93) and low false negative rate (0.07) for event-type texts, as well as a strong true negative rate (0.92) and low false positive rate (0.08) for non-event-type texts. This confirms the model's effectiveness in distinguishing relevant content from irrelevant content."}, {"title": "5. Conclusion", "content": "We introduce MIH-TCCT, a novel framework that enhances the logical consistency of LLM outputs by cyclically generating event-based text and corresponding code. This approach effectively transfers the logical rigor of code to natural language, addressing significant challenges related to inconsistent hallucinations. Notably, our framework demonstrates exceptional generalizability, meaning that it can alleviate issues of hallucination across various contexts without adapting to specific downstream tasks. These findings contribute to advancing the capabilities of LLMs, paving the way for more reliable and logically consistent AI systems."}, {"title": "Impact Statement", "content": "This work introduces MIH-TCCT, a novel framework that enhances the logical consistency of LLM outputs by cyclically generating event-based text and corresponding code. This approach effectively transfers the logical rigor of code to natural language, addressing significant challenges related to inconsistent hallucinations. The datasets and codes used in this study are built based on public medical datasets and will be publicly available to facilitate further research. We have not identified any potential negative ethical consequences requiring further consideration."}, {"title": "A. Introduction to the Prompt Used", "content": "Your task is to determine whether a text is suitable for description in code (e.g., modeling a relationship using class-based instances, modeling a process using a function), and return \"yes\" if it is suitable, or \"no\" otherwise. Generally speaking, an organized text that describes a series of events and processes is suitable for code description, while a text that mainly expresses emotions or illogicality is not suitable for code description, the following is an examples:\nInput: The Hullaballoos were created in August 1964, but had been working in the UK for over three years under the name of Ricky Knight and The Crusaders. They They were not named after the American television programme Hullabaloo. Their name came from the city of Hull, England, whence they hailed.\nOutput: yes\nPlease decide whether the following text is suitable for code description according to the above requirements:\nFK Drezga is founded on 1972, as a team of Piperi region near Podgorica. At their first seasons, FK Drezga played in Fourth League - Central region (lowest rank in SFR Yugoslavia). Club was dissaloved at the end of seventies."}]}