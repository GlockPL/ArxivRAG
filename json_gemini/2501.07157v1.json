{"title": "CureGraph: Contrastive Multi-Modal Graph Representation Learning for Urban Living Circle Health Profiling and Prediction", "authors": ["Jinlin Li", "Xiao Zhou"], "abstract": "The early detection and prediction of health status decline among the elderly at the neighborhood level are of great significance for urban planning and public health policymaking. While existing studies affirm the connection between living environments and health outcomes, most rely on single data modalities or simplistic feature concatenation of multi-modal information, limiting their ability to comprehensively profile the health-oriented urban environments. To fill this gap, we propose CureGraph, a contrastive multi-modal representation learning framework for urban health prediction that employs graph-based techniques to infer the prevalence of common chronic diseases among the elderly within the urban living circles of each neighborhood. CureGraph leverages rich multi-modal information, including photos and textual reviews of residential areas and their surrounding points of interest, to generate urban neighborhood embeddings. By integrating pre-trained visual and textual encoders with graph modeling techniques, CureGraph captures cross-modal spatial dependencies, offering a comprehensive understanding of urban environments tailored to elderly health considerations. Extensive experiments on real-world datasets demonstrate that CureGraph improves the best baseline by 28% on average in terms of R2 across elderly disease risk prediction tasks. Moreover, the model enables the identification of stage-wise chronic disease progression and supports comparative public health analysis across neighborhoods, offering actionable insights for sustainable urban development and enhanced quality of life. The code is publicly available at https://github.com/jinlin2021/CureGraph.", "sections": [{"title": "1. Introduction", "content": "Over the past two decades, rapid urbanization and the global trend of an ageing population have emerged as inevitable and intersecting demographic forces shaping the current century. According to statistics [41], no country had more than 18 percent of its population over the age of 65 in 2000. However, due to the accelerating ageing process, this proportion is projected to reach 38 percent by 2050. This demographic shift would inevitably pose a series of significant challenges to cities worldwide, underscoring the need for more age-friendly urban environments. Among the various factors influencing the quality of life for the elderly, chronic conditions that increase with age are critical and must not be overlooked. In 2020, the UN General Assembly declared 2021-2030 as the Decade of Healthy Aging [39], highlighting the urgent need for policymakers worldwide to prioritize initiatives aimed at enhancing the well-being of older individuals. Therefore, devising an effective health-oriented profiling and prediction framework for aging-friendly urban environments\u2014grounded in a deep understanding of the linkages between the multi-modal semantics of neighborhood living circles and chronic diseases associated with aging-is essential in this era of longevity and forms the core research focus of this paper.\nIn current literature, a substantial body of research has revealed that the characteristics of human settlements are intricately associated with the chronic health conditions of urban senior citizens. For instance, Cassarino and Setti [3] demonstrated that a community's social and built environment can influence elderly cognition, with complex neighborhood settings potentially delaying cognitive decline. A study on Hong Kong communities [15] found that the relationship between street connectivity and mental health follows an inverted U-shaped pattern, while park green spaces and mixed land use positively impact mental health and subjective well-being. Wang et al. [50] utilized the street view images to assess how neighborhood walkability factors-such as sky visibility, greenery, and building proportion-affect elderly mental health, particularly regarding depression and anxiety. Similarly, Song et al. [43] found that green and blue spaces, population density, and public transport accessibility present statistically significant relationships with"}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Contrastive Learning in Urban Settings", "content": "Inspired by significant advancements in self-supervised learning within computer vision [4, 17, 52] and natural language processing [57, 12, 60], researchers have begun to extend these methodologies to urban modal coding. Despite these innovations, much of the existing research in this domain remains grounded in Tobler's First Law of Geography [36] when designing contrastive learning similarity measures. Self-supervised learning, particularly contrastive learning, has been widely used to distill expressive modal elements in urban spaces. For example, Jean et al. [23] employed triplet loss to minimize the distance between representations of spatially proximate satellite images while maximizing the distance for spatially distant pairs. Similarly, Wang et al. [54] utilized a comparable loss function for street view imagery. Li et al. [25] applied the InfoNCE loss [38] from the SimCLR framework [4] to extract visual representations from satellite and street view images. Xi et al. [58] expanded this approach by incorporating a POI-based similarity metric, ensuring that images associated with similar POI category distributions yielded closer visual representations. Liu et al. [30] further extended this line of research by integrating a knowledge graph (KG) to represent urban knowledge within satellite imagery, formulating an image-KG contrastive loss that builds upon the traditional InfoNCE loss [38].\nHowever, the aforementioned studies rely solely on contrastive learning frameworks based on single-modality data or perspectives, overlooking the complementary potential of combining urban images and textual information."}, {"title": "2.2. Urban Region Representation Learning", "content": "Urban region representation learning has garnered significant attention in recent years, driven by the proliferation of urban big data, the rapid advancements in embedding techniques within deep learning, and the pressing need for applications in smart cities. Broadly, research in this area often leverages word embedding or graph embedding techniques, tailored to address the spatio-temporal characteristics unique to urban settings.\nFor example, Wang and Li [49] constructed a mobility graph using fine-grained taxi commuting data and learned region embeddings at different time intervals. Similarly, Yao et al. [62] used taxi trajectories to learn embeddings of urban functional zones, applying the word2vec [35] method by treating each urban region as a word and mobility events as context. More recently, the urban computing community has increasingly recognized the importance of exploiting multi-modal urban data for region representation learning, given the complexity of cities as systems. A notable example is Urban2Vec by Wang et al. [54], a neighborhood embedding method based on word embedding techniques that integrates multi-modal data, including street view images and POI textual reviews. From a multi-view perspective, Fu et al. [10] incorporated spatial relationships between regions, constructing multi-view POI-POI networks to represent urban regions. Zhang et al. [64] advanced this approach by designing a cross-view information-sharing strategy and a weighted multi-view fusion technique to combine human mobility and POI data. Similarly, Huang et al. [21] utilized multimodal data as node and edge features within a multi-graph, leveraging human mobility data to represent neighborhood relationships while employing contrastive sampling to learn neighborhood representations. Li et al. [27] proposed RegionDCL, an unsupervised framework utilizing OpenStreetMap (OSM) building footprints and POI data to learn regional representations. This approach incorporates dual contrastive learning at both the group and region levels, preserving spatial proximity and architectural similarity among building clusters. Yong and Zhou [63] proposed the MuseCL framework, which leverages contrastive learning across visual and textual modalities, combined with a cross-modality attentional fusion module, to enable fine-grained urban region profiling and socioeconomic prediction. Xu and Zhou [59] introduced the CGAP algorithm, incorporating hierarchical graph pooling with local and global attention mechanisms to improve urban region representation by integrating multi-modal data such as POIs and inter-regional human mobility patterns.\nGiven that graphs are well-suited for organizing urban elements and their complex relationships, and graph neural network (GNN)-based approaches have recently emerged as powerful tools for graph embedding tasks [48, 13], this study adopts graph convolutional networks (GCNs) as the base model to encode multi-modal graphs encompassing diverse urban data modalities. Additionally, our framework integrates advanced techniques, including contrastive learning, image and text analysis, and spatial pattern mining. This combination of methods distinguishes our community living circle representation learning framework from existing approaches, offering a novel and comprehensive solution."}, {"title": "2.3. Health in Urban Environments", "content": "As the introduction section provides a comprehensive overview of studies related to health in urban built environments, we will not repeat them here. A key limitation in the current literature is the frequent neglect of non-linear interactions among urban features, which can significantly influence community health outcomes. Many conventional studies that employed traditional spatial statistics methods [28, 34, 50, 46] assumed linear relationships between features, overlooking the inherent complexity and subtle dynamics of urban environments. In reality, urban health outcomes are influenced not only by the geographic distribution of facilities and resources, as described by Tobler's First Law of Geography, but also by the complex interactions among various neighborhood characteristics, including social [20, 66], economic, mobility [65], and environmental factors. This complexity is consistent with the Second Law of Geography, which highlights spatial heterogeneity [2].\nTo address these limitations, our proposed method captures the relationships between feature modalities within urban living circles while accounting for spatial heterogeneity across different circles. By leveraging advanced techniques such as contrastive learning, graph representation learning, and spatial analysis, our approach provides a more nuanced understanding of the intricate interplay between factors influencing urban health outcomes at multiple scales. Additionally, it offers valuable insights for urban planners and policymakers, guiding strategies to improve the health and well-being of elderly populations in urban communities."}, {"title": "3. Problem statement", "content": "In this section, we delineate the related notations and the problem we study.\nDefinition 1 (Urban Living Circle): The health condition of a metropolitan area can be reflected by a series of urban living circle scenes. Among these, the community living circle serves as a fundamental spatial unit capable of meeting the essential daily activity requirements of its residents. These activities typically have high frequency, short duration, and are confined to the residential area and its immediate vicinity. In this study, we define the urban living circle as the service radius of a residential area, specifically referring to a 15-minute walking distance (within a 1000-meter radius) from various essential service facilities. This concept is commonly referred to as the \"15-minute living circle,\" which also serves as the basic spatial unit for our downstream community health predictions. Given a set of urban living circles, $C = {c_1, c_2, ..., c_n}$, each urban living circle $c_i$ contains a paragraph of residential area text, $T_i$, a set of residential area images, $V_i = {v_{i1}, v_{i2}, \u2026, v_{iM_i}}$, and a set of health-related POIs, $P_i = {p_{i1}, p_{i2}, ..., p_{iO_i}}$, located within $c_i$. Here, $n$ represents the number of residential areas, and $M_i$ and $O_i$ denote the number of images and POIs contained within each living circle $c_i$, respectively. We assume that the GPS coordinates of the images and the residential area are consistent.\nDefinition 2 (Urban Living Circle Texts): The living circle text describes the supporting facilities, transportation options, surrounding amenities, and the target residents of the residential area. The representation of urban living circles texts, denoted as $T$, is formalized as follows:\n$T = {\\hat{c}^t_1,\\hat{c}^t_2,...,\\hat{c}^t_{|T|}}, \\hat{c}^t_i \\in R^{F_t}, \\forall \\hat{c}^t_i \\in T$,  (1)\nwhere $\\hat{c}^t_i$ is the corresponding textual feature of i-th region and $F_t$ is the number of dimensions of the feature.\nDefinition 3 (Urban Living Circle Images): The visual modality of the living circle captures the physical environment within the residential area and serves as a visual complement to the textual modality. The number of community images in each living circle may vary. We denote the set of urban living circle images as $V$, defined as follows:\n$V = {\\hat{c}^v_1,\\hat{c}^v_2,...,\\hat{c}^v_{|V|}}, \\hat{c}^v_i \\in R^{F_v}, \\forall \\hat{c}^v_i \\in V$,  (2)\nwhere $\\hat{c}^v_i$ is the corresponding visual feature of i-th region and $F_v$ is the number of dimensions of the feature.\nDefinition 4 (Urban Living Circle POI Texts): POIs serve as direct representations of urban functions. The distinctive features of check-ins and review information associated with different types of POIs can be leveraged to infer the healthiness of lifestyles. We map the relevant POIs to their corresponding living circle locations and denote the set of urban living circle POI texts as $P$, defined as follows:\n$P = {\\hat{c}^p_1,\\hat{c}^p_2,...,\\hat{c}^p_{|P|}}, \\hat{c}^p_i \\in R^{F_p}, \\forall \\hat{c}^p_i \\in P$,  (3)"}, {"title": "4. Methodology", "content": "In this section, we present the framework for graph representation learning based on multimodal spatial fusion to generate health embeddings for urban living circles. The proposed framework integrates visual and textual data from residential areas alongside corresponding POI information within each living circle to learn a compact, low-dimensional, and spatially-aware vector representation. This representation is then used to predict the prevalence of four common geriatric diseases among the elderly population in urban areas. An overview of the proposed CureGraph framework is illustrated in Fig. 2."}, {"title": "4.1. Modality Encoder", "content": "To accurately extract features from urban multimodal data, we draw inspiration from the success of contrastive learning in modality encoding. We develop multiple types of contrastive learning encoders to effectively extract features from textual, visual, and POI data related to urban life in residential areas. The core principle of contrastive learning is to achieve both alignment and uniformity [51]. Specifically, contrastive learning assumes that samples from positive pairs are similar, aiming to maximize the similarity of representations within the same pair while increasing the dissimilarity between representations of different pairs. Consequently, constructing contrastive pairs is a critical step in this approach. In our modality encoder, we leverage both self-similarity and geographical similarity across modalities to construct contrastive pairs. The encoder is designed to map similar data to closely aligned embeddings while maximizing the divergence between embeddings of dissimilar data types."}, {"title": "4.1.1. Visual Modality Encoder", "content": "Tobler's First Law of Geography states that objects closer to each other are more likely to be related than those farther apart. Building on this principle, we design a geospatial contrastive learning model to effectively extract image features of urban living circles from residential images. The visual contrastive learning model aligns similar samples by positioning them closer in the projection space while pushing dissimilar samples farther apart. In this context, we assume that images from the same residential area share similar geospatial information.\nTo capture this, we define the geographic neighbors of a residential area as its \"context\", determined by the service radius of the living circle. Specifically, for an image $v$, we identify a positive image $v_p$ within the context (geospatial neighbors) and a negative image $v_n$ outside the context. The model is trained on a collection of triplets $(v, v_p, v_n)$ using a geospatial contrastive loss function:\n$L_{geo}(v, v_p, v_n) = [m + ||x - x_p||_2 - ||x - x_n||_2]_+$,  (6)\nwhere $[ ]_+$ is the rectifier function, and $|| \\cdot ||_2$ denotes the Euclidean distance. The margin $m$ ensures that the difference between these distances does not grow infinitely large. The embeddings of images $v, v_p$, and $v_n$ are represented as $x, x_p$, and $x_n$, respectively. These embeddings are derived by encoding the images using a pre-trained InceptionV3 [44] model with an embedding dimension of 768.\nWe also design a contrastive learning framework based on data augmentation to enhance the robustness of feature extraction. The principle of self-similarity ensures that augmented data variants remain similar to the original samples. In computer vision [22], contrastive samples of an image can be generated through data augmentation techniques such as rotation, cropping, masking, and adding noise. For this study, we use rotation and flipping to create two augmented images, $v$ and $v^+$, for each original image. Let $x$ and $x^+$ represent their embeddings, and consider other images in the same batch as negative samples. The loss function for a mini-batch of $N$ pairs is expressed as:\n$L_{v}^{aug} = -log \\frac{exp \\left(sim (x_v, x_{v^+}) /\\tau\\right)}{\\sum_{j=1}^{N} exp \\left(sim (x_v, x_{v,j}) /\\tau\\right)}$,  (7)\nwhere $sim()$ denotes the cosine similarity between two vectors, and $\\tau \\in R$ is a temperature hyperparameter that controls the concentration of the distribution.\nFinally, the overall contrastive loss for the visual modality encoder, $f_v(\\cdot)$, combines geospatial and augmentation-based components:\n$L_v = L_v^{geo} + L_v^{aug}$.  (8)\nAfter obtaining the feature representation for each image through the encoder $f_v(\\cdot)$, we apply an averaging operation to derive the corresponding visual feature for each urban living circle $c_i$. This is expressed as:\n$\\hat{c}^v_i = \\frac{1}{M_i} \\sum_{j=1}^{M_i} x_{v_{ij}}$,  (9)\nwhere $M_i$ denotes the number of images in the living circle $c_i$, and $x_{v_{ij}}$ represents the raw feature representation of the j-th image extracted by the visual modality encoder."}, {"title": "4.1.2. Textual Augmentation Encoder", "content": "In the field of computer vision (CV), contrastive learning has proven to be a powerful method for feature extraction. Similarly, in natural language processing (NLP), contrastive learning is gaining traction due to its ability to learn robust representations using contrastive pairs that pull similar samples closer while pushing dissimilar ones apart [51]. A critical aspect of contrastive learning lies in constructing positive and negative samples, with various text augmentation techniques such as adversarial attacks, sentence shuffling, cutoff, and dropout-being widely adopted [60].\nBuilding on this principle, we leverage geographical similarity between residential texts and images that share the same spatial information. Our fundamental assumption is that texts and images in close geographical proximity exhibit similar semantic spatial representations. Notably, textual descriptions of residential areas within living circles often consist of long sentences. To capture the full semantic modality of text, we draw inspiration from cross-modal contrastive learning, aligning textual representations with their visual counterparts.\nFor the text modality encoder, we introduce a visually-enhanced text semantic augmentation model based on cross-modal contrastive learning. The core insight is that the semantic representation of a region should closely align with its visual features. Specifically, we use the pre-trained language model hf1/chinese-bert-wwm-ext, following [6], to encode the textual modality. A fully connected network then projects the representations of both textual and visual modalities into a shared latent space for contrastive learning. The outputs for textual and visual modalities of $c_i \\in C$ are expressed as:\n$\\begin{aligned}h_{t,i} &= W_t \\eta_i + b_t, \\\\h_{v,i} &= W_v \\hat{c}^v_i + b_v,\\end{aligned}$ (10)\nwhere $\\eta_i$ represents the raw feature vector of the textual modality, and $\\hat{c}^v_i$ is the visual embedding of living circle $c_i$. $h_{t,i}$ and $h_{v,i}$ denote the hidden features of the textual and visual modalities, respectively.\nTo align textual and visual modalities, we extend the traditional InfoNCE loss for the textual augmentation encoder. The contrastive loss is defined as:\n$L_T^{aug} = -log \\frac{exp (sim (h_{t,i}, h_{v,i}) /\\tau)}{\\sum_{j=1}^{N} exp (sim (h_{t,i}, h_{v,j}) /\\tau)}$,  (11)\nwhere the loss is computed over a mini-batch of $N$ samples.\nUnlike previous studies on urban region representation that rely on single or multi-view-based contrastive loss within the same modality or employ standard data augmentation techniques, our proposed cross-modal contrastive loss integrates visual information captured in living circles into textual representations. This innovative approach leverages the complementary nature of visual and textual data, enhancing the overall understanding of urban regions and enabling more accurate and comprehensive representations."}, {"title": "4.1.3. POI Textual Modality Encoder", "content": "We employ a combination of geographic similarity and data augmentation techniques to train the POI textual modality encoder. Unlike the unsupervised learning approach in the textual augmentation encoder, this encoder utilizes a supervised contrastive learning model. To ensure semantic accuracy and effectively evaluate the quality of services within a community, we incorporate the sentiment tone and category of POI review texts within the living circle. The primary objective of the POI textual modality encoder is to group reviews with similar sentiment tones while distinguishing those with differing sentiments.\nSpecifically, we map relevant POIs to the designated living circle, guided by the service radius of the residential area. In order to ensure proximity among POI review text embeddings within the same sentiment category, we employ POI ratings as labels to categorize review texts accordingly. For each POI review text $p$ located within the living circle, we utilize the dropout technique twice as a method of data augmentation. This strategy aims to enrich the dataset with additional positive examples, thereby facilitating the generation of a greater number of positive sample pairs. In the field of NLP, the essence of contrastive learning techniques lies in the creation of positive pairs characterized by sufficient variability. This variability is crucial for training models to discern distinct text representations. Unlike traditional data augmentation methods, which directly alter text content (e.g., through cropping or word deletion) and may inadvertently modify the original intent or introduce noise, our approach leverages the BERT model. By applying diverse dropout masks twice, we generate two unique representations for the same sentence without altering the text directly. This method not only maintains the original text's integrity but also exploits the model's inherent randomness to enhance the diversity between representations.\nThe goal of our model is to increase the similarity scores among samples within the same category while reducing similarity scores for samples from different categories. Let $x_{p,i}$ and $x_{\\tilde{p},i}$ represent the textual modality embeddings of POI reviews $p$ and $\\tilde{p}$, respectively, where $p$ shares the same label as $\\tilde{p}$. The sentiment-based, geographically supervised contrastive loss is computed as:\n$L_p = \\sum_{i=1}^{|P(i)|} \\frac{-1}{|P(i)|} \\sum_{p \\in P(i)} log \\frac{exp(sim(x_{p,i}, x_{\\tilde{p},i}) /\\tau)}{\\sum_{j \\in A(i)} exp(sim(x_{p,i}, x_{p,j}) /\\tau)}$,  (12)\nwhere $P(i) = {p \\in A(i) : \\tilde{y}_p = \\tilde{y}_i}$ represents the indices of all positive samples in the mini-batch distinct from $i$, and $|P(i)|$ is its cardinality. $A(i) = I \\backslash i$ denotes all indices except $i$. The textual raw feature $x_p$ is extracted using the pre-trained language model hfl/chinese-bert-wwm-ext, as described in [6], with a feature dimension of 768.\nTo represent each POI within a living circle, we extract semantic features from the review texts and calculate the average sentiment embedding. Given that POI categories are highly correlated with the facility distribution within a living circle, we use the same pre-trained language model to encode the POI categories. The two components\u2014POI review embeddings and category embeddings\u2014are concatenated to form the final POI textual feature. Subsequently, a fully connected network projects the POI textual modality features into the shared latent space for graph learning. Finally, we compute the aggregated POI textual representation $\\hat{c}^p_i$ for living circle $c_i$ as:\n$\\begin{aligned}\\hat{c}^p_i &= \\frac{1}{G_i} \\sum_{i=1}^{G_i} \\sum_{j=1}^{m_j} \\frac{1}{m_j} x_{p,ij},\\\\h_{p,i} &= W \\hat{c}^p_i + b,\\end{aligned}$ (13)\nwhere $G_i$ is the number of POI categories within each living circle, $m_j$ is the number of comments for each POI within the living circle, and $x_{p,ij} = [x^r_{p,ij};x^c_{p,ij}]$. Here, $x^r_{p,ij}$ is the raw feature representation of the POI reviews extracted by the POI textual modality encoder, and $x^c_{p,ij}$ is the category representation of the corresponding POI."}, {"title": "4.2. Extracting Spatial Autocorrelation", "content": "Urban data are often influenced by geospatial interactions and diffusion, leading to potential correlations and dependencies among data points. This spatial correlation, usually represented by the spatial weight matrix, quantifies the degree of interdependence between data at one location and data at other locations. In this study, spatial autocorrelation coefficients capture both geographical distance and POI-related information within the community living circles. The spatial autocorrelation matrix $H$ is used to measure the pairwise similarity between each pair of living circles. By incorporating spatial autocorrelation into the graph network, we enhance the learning of modal representations within the community.\nTo calculate the spatial autocorrelation, we first determine the normalized geographical distance $D_{i,j}$ between the i-th living circle $c_i$ and the j-th living circle $c_j$. Next, we use the TF-IDF model to assess the importance of each POI category within a living circle, treating POIs as words and a region as a document that describes the functional distribution of the living circle. The urban function similarity between circles is then computed as $F_{i,j} = sim(c_i, c_j)$. Finally, the combined spatial autocorrelation coefficient is expressed as:\n$S_{i,j} = \\frac{sim(c_i, c_j)}{log(D_{i,j} + 1)}$,  (14)\nwhere $S_{i,j}$ denotes the spatial autocorrelation coefficient between $c_i$ and $c_j$. This approach enables the proposed model to learn embeddings for multi-modal data in living circles while accounting for inter-region autocorrelations."}, {"title": "4.3. Graph Construction", "content": "In this section, we present our approach to modeling and constructing the relationships between nodes in community living circles. As defined in Section 3, we characterize urban community living circles using a 1000-meter service radius around residential areas. Each community living circle encompasses information such as texts, images, and POIs related to the corresponding residential areas. Based on the modal encoder, we can obtain representations of these multimodal data within each community living circle. Specifically, at the spatial street level, we assume that the streets with $n$ residential areas can be represented as an undirected graph $G = (V, E, N)$, where $V = {\\hat{c}_i}_{i=1}^{3n}$ denotes the three feature nodes in the urban living circle, $E$ denotes the edges connecting nodes, and $N$ represents the neighborhood of each node. We construct the graph as follows:\n\u2022 Nodes. Each living circle encompasses a residential area, represented by three nodes initialized with the textual modality $\\hat{c}^t$, visual modality $\\hat{c}^v$, and POI textual modality $\\hat{c}^p$, respectively. In accordance with the administrative division at the street level, we construct a graph with $3n$ nodes based on the $n$ living circles in the street.\n\u2022 Edges and Edge Weighting. Given the high spatial autocorrelation within the modal information of community living circles, we assume that each node connects to others within the same community living circle. For example, $\\hat{c}^t$ will be connected to both $\\hat{c}^v$ and $\\hat{c}^p$ within the graph. Additionally, for any two nodes of the same modality in different living circles, we connect the top-k nearest nodes based on geographic proximity and modal similarity. Consequently, the graph contains two types of edges: one type connects nodes of different modalities within the same living circle (intra-community edges), and the other connects nodes of the same modality (inter-community edges). We apply different edge-weighting strategies for these two types of edges.\nThe edge weight increases as node similarity increases, reflecting the stronger information interaction between similar nodes. For intra-community edges, we capture the similarity between node representations. Following the approach in [42], we use angular similarity to calculate the edge weight between nodes connected in this manner. The edge weight is computed as:\n$W_{i,j}^{I} = 1 - \\frac{arccos (sim (x_i, x_j))}{\\pi}$,  (15)\nwhere $x_i$ and $x_j$ are the feature representations of the i-th and j-th nodes, and $sim(\\cdot)$ denotes cosine similarity.\nFor the second type of edge, we adhere to the spatial autocorrelation framework described in Subsection 4.2, employing a top-k locality principle. This ensures that for each urban living circle $c_i$, we connect only the top-k nearest modal nodes within the living circle. The edge weights are determined by both the geographic proximity and modality similarity of the corresponding residential areas. Nodes that are closer in space within a living circle are assigned higher edge weights, ensuring stronger connections for geographically proximate nodes. The edge weight is calculated as:\n$W_{i,j}^{II} = \\frac{W_{i,j}^{Top-K}}{log(D_{i,j} + 1)}$,  (16)\nwhere $c_i^{Top-K}$ represents the top-k most correlated candidate regions for the living circle $c_i$ based on spatial autocorrelation, and $D_{i,j}$ is the Euclidean distance between nodes $i$ and $j$. In CureGraph, we combine the top-k locality concept with modality representation similarity to optimize the calculation and search for parameters.\nOur proposed composition method simulates the intra-region connections within the living circle modals while comprehensively capturing the inter-region correlations in the modality space. This innovative approach enables a more comprehensive understanding of the relationships and dependencies among different modalities, resulting in more accurate and robust representations of urban regions. By incorporating both intra-region and inter-region correlations, we can effectively capture the fundamental structure and inherent patterns in the data, thus boosting the overall performance of our model."}, {"title": "4.4. Base Model", "content": "We employ a deep GCN as the base model to develop SMGCN, which learns efficient node representations in a graph. By modeling the spatial relationships in a graph's multimodal context, SMGCN captures local structural information and the relationships between nodes, enabling it to generate feature representations for each node. Additionally, SMGCN further enhances these representations by integrating information from neighborhoods across different modalities, improving the understanding of graph structure and inter-node relationships while strengthening the encoding of contextual dependencies."}, {"title": "4.5. Learning Objectives", "content": "Given a multi-modal spatial GCN, the features of each node in the graph are updated through k-layer iterations. The primary objective of CureGraph is to learn effective low-rank vectors in the embedding space after multi-modal fusion, while preserving spatial autocorrelation. Specifically, CureGraph aims to achieve two goals: (1) capture the rich and meaningful semantics within living areas and fuse them effectively, and (2) accurately model the spatial autocorrelation between living circles. Furthermore, CureGraph is based on the assumption that the texts within residential areas should be highly correlated with the street-view and POI information in their spatial context. Therefore, the model aims to minimize the distance between neighborhoods that have both street-view and POI representations in the embedding space. To train the model effectively, we design a task that reconstructs the spatial autocorrelation coefficients based on the corresponding embeddings and minimizes their Euclidean distance. The learning objective is defined as follows:\n$\\begin{aligned}L = \\sum_{i,j} (S_{ij} - \\hat{c}^e_i \\hat{c}^e_j)^2 + \\lambda \\sum_{m \\in {v,p}} dist(\\hat{c}^e_i, \\hat{c}^m_i),\\end{aligned}$ (19)\nwhere $S_{ij}$ represents the spatial autocorrelation coefficient between living circles $c_i$ and $c_j$, and $dist(\\cdot)$ is the distance function used in the vector space (here, we use Euclidean distance). The hyperparameter $\\lambda$ controls the contribution of the spatial distance loss to the global loss function. The term $\\hat{c}^m_i$, for $m \\in {v, p}$, refers to the visual and POI textual modality embeddings, respectively."}, {"title": "5. Experiments", "content": "In this section, we conduct experiments on real-world datasets to evaluate the performance of our method in comparison to various baselines. Additionally, we perform a series of ablation studies across different modality settings to validate the contribution of each modality fusion in our approach. We also analyze the impact of contrastive learning on the multi-modal encoder. Finally, we carry out a neighborhood similarity analysis of the health embeddings for living circles across multiple spatial levels."}, {"title": "5.1. Experimental Setup", "content": ""}, {"title": "5.1.1. Datasets", "content": "Real Estate Data. This dataset is collected from Lianjia\u00b9, the largest real estate trading platform in China. It contains information on over 8,000 properties in Beijing and over 7,000 in Shanghai. Each property record includes details such as the name of the residential area, street address, latitude and longitude, price, the total number of buildings and units, and corresponding textual descriptions, image links, and quantities. The textual descriptions provide insights into the residential area's environment, covering aspects such as supporting facilities, transportation, surrounding amenities, and target residents. Each residential area also includes a varying number of images that visually depict leisure facilities, environmental greening, and other local features. Additionally, the dataset is supplemented with information from the 2020 permanent population census\u00b2 for both Beijing and Shanghai.\nPOI Check-in Data. The POI check-in data consists of 753,891 records from Beijing and 549,817 records from Shanghai. The data is sourced from various platforms, including Dianping\u00b3, Meituan\u2074, Baidu Maps\u2075, Ctrip\u2076, Tujia\u2077, and Elong\u2078. Dianping is a well-known platform in China for local life services, providing reviews and facilitating transactions. Meituan offers a wide range of services, from dining to entertainment, while Baidu Maps provides travel-related tools such as route planning, navigation, and location queries. Ctrip, Tujia, and Elong are major travel and ticketing platforms where users can leave reviews about their experiences.\nThe dataset records user check-ins from January 2019 to December 2019, each associated with a unique user ID, venue ID, user comments and ratings, as well as price and source information. Although longitude and latitude information for POIs are not provided, we are able to match POIs with geographical data using the Baidu Maps API, as all platforms in the dataset utilize Baidu Maps for their services. The POIs are categorized into 10 main types, which are selected based on their relevance to chronic disease factors among the elderly. These categories include food, shopping, sports and fitness, tourist attractions, leisure and entertainment, life services, education and training, culture and media, transportation"}]}