{"title": "Injecting Bias into Text Classification Models using Backdoor Attacks", "authors": ["A. Dilara Yavuz", "M. Emre Gursoy"], "abstract": "The rapid growth of natural language processing (NLP) and pre-trained language models have enabled accurate text classification in a variety of settings. However, text classification models are susceptible to backdoor attacks, where an attacker embeds a trigger into the victim model to make the model predict attacker-desired labels in targeted scenarios. In this paper, we propose to utilize backdoor attacks for a new purpose: bias injection. We develop a backdoor attack in which a subset of the training dataset is poisoned to associate strong male actors with negative sentiment. We execute our attack on two popular text classification datasets (IMDb and SST) and seven different models ranging from traditional Doc2Vec-based models to LSTM networks and modern transformer-based BERT and RoBERTa models. Our results show that the reduction in backdoored models' benign classification accuracy is limited, implying that our attacks remain stealthy, whereas the models successfully learn to associate strong male actors with negative sentiment (100% attack success rate with > 3% poison rate). Attacks on BERT and RoBERTa are particularly more stealthy and effective, demonstrating an increased risk of using modern and larger models. We also measure the generalizability of our bias injection by proposing two metrics: (i) U-BBSR which uses previously unseen words when measuring attack success, and (ii) P-BBSR which measures attack success using paraphrased test samples. U-BBSR and P-BBSR results show that the bias injected by our attack can go beyond memorizing a trigger phrase.", "sections": [{"title": "1 Introduction", "content": "In today's world, artificial intelligence (AI) and natural language processing (NLP) are growing rapidly and invading many sectors. The development and widespread success of language models such as BERT, LLaMa, and the GPT family have revolutionized the capabilities of NLP applications, allowing more accurate and efficient processing of natural language. A fundamental application of NLP is text classification, where the model assigns a label to a given piece"}, {"title": "2 Related Work", "content": "Backdoor attacks in text classification. Backdoor attacks are a prominent threat against NLP and text classification. A fundamental method to implement backdoor attacks is to poison the training dataset by injecting trigger sentences [6] and trigger words [3,12], which is also the strategy we use in this paper. In [19], backdoor attacks based on style transfer were introduced, in which a model is backdoored to associate a text style (e.g., tweet, poem, Shakespeare style) with an attacker-desired label. Qi et al. [20] proposed backdoor attacks using syntactic triggers in which a syntactic template acts as the trigger pattern. Chen et al. [4] proposed two tricks to increase the harm of textual backdoor attacks: introducing an extra training task to distinguish between poisoned and clean data, and"}, {"title": "3 Problem Setting", "content": ""}, {"title": "3.1 Notation and Setup", "content": "Consider a text classification task where M denotes the classification model, Dtrain denotes the training dataset, and Dtest denotes the test dataset. Each sample in the training dataset is denoted by (x,y) \u2208 Dtrain, where y is the ground-truth label of the input x. For example, the input x can be a text document (e.g., an e-mail) and the label y may be either 0 or 1, corresponding to spam or ham. Given Dtrain, the goal of text classification is to build an accurate model M that can correctly predict the labels of previously unseen test samples. That is, for each test sample (xt, Yt) \u2208 Dtest, denoting the prediction of M by M(xt)\u2192y, it is desired that yt = yt."}, {"title": "3.2 Text Classification Models", "content": "We use a total of 7 different model types, which can be presented under 3 categories: (i) combination of Doc2Vec with traditional ML, (ii) Long Short-Term Memory (LSTM), and (iii) modern transformer-based models such as BERT and RoBERTa. While we focus more on the relatively modern approaches (LSTM, BERT, ROBERTa), we nevertheless include traditional Doc2Vec-based approaches to increase the breadth of our study and analyze the impacts of our attacks on a diverse set of models with different architectures and complexities.\nDoc2Vec + ML models. Doc2Vec (Document to Vector) is an extension of Word2Vec (Word to Vector) [18]. Doc2Vec learns word representations by predicting context words given in a sentence, similar to Word2Vec, but it also uses an extra document-level vector to understand the document's overall meaning and semantic information [13]. It enables the encoding of documents, sentences, and paragraphs as fixed-length vectors within a continuous space. After vector encoding is done, the resulting vectors are fed into traditional ML models to perform downstream text classification. In this paper, we use this Doc2Vec pipeline with 4 ML models: Logistic Regression (LR), Naive Bayes (NB), Decision Tree (DT), and Random Forest (RF).\nLSTM models. Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) designed to handle long input sequences more effectively by overcoming the vanishing gradients problem that affects standard RNNs. In our work, we train all LSTM models from scratch (no pre-training). The LSTM architectures we use have 2 main components: an embedding layer and a classification component. The embedding layer converts input sequences into dense vectors of fixed size. This layer is followed by a dropout layer with a rate of 0.2 to prevent overfitting. The classification component includes an LSTM layer with 32 units, which captures temporal dependencies and sequence information. Following the LSTM layer, a dense layer with 256 units and a ReLU activation function processes the output. Another dropout layer with a rate of 0.2 is applied for regularization. Finally, a dense layer with a single unit and a sigmoid activation function maps the processed features to binary class predictions. We train LSTM models using the AdamW optimizer (configured with a"}, {"title": "3.3 Validation of Benign Models", "content": "Before executing our attacks, we validated the usefulness of our benign text classification models (i.e., aforementioned models without attack) to ensure they are accurate and realistic models that can be used in practice. For this purpose, we measured the classification accuracy of our models using the clean test dataset Dtest. We denote by Benign Classification Accuracy (BCA) the fraction of test samples that are correctly predicted by the model M:\n BCA = # of samples (xt, Yt) \u2208 Dtest s.t. M(xt) \u2192 y and y = Yt / Dtest  (1)\nWe used two popular datasets from the text classification literature: IMDb Large Movie Review and Stanford Sentiment Treebank (SST). The IMDb dataset consists of 50,000 movie reviews [17]. Each review is labeled with binary labels where positive sentiment is 1 and negative sentiment is 0. The SST dataset consists of 11,855 extracted single-sentence samples from movie reviews [23]. Each sample has a sentiment score between 0 and 1 indicating the degree of positivity, with 1"}, {"title": "4 Attack Strategies and Success Metrics", "content": ""}, {"title": "4.1 Attack Goal and Methods", "content": "Our goal is to inject bias into text classification models using an attack similar to a backdoor attack. Backdoor attacks embed hidden backdoors into victim models such that the models continue to perform well on benign test samples, whereas they behave in an adversarial way when they encounter samples containing a specific trigger pattern. There are multiple ways in which a backdoor attack can be implemented in text classification such as sentence injection [6], word injection [12], style transfer [19], and syntactic template memorization [20]. Among these methods, we implemented our attacks using word and phrase injections since they were shown to achieve close to 100% attack success rates despite little manipulation of Dtrain. Furthermore, word and phrase injections to Dtrain are agnostic to the model type. In these attacks, the injected word(s) or phrase(s) act as the trigger pattern. The attacker wants to ensure that whenever the victim model M sees the trigger word or phrase, it predicts the given test sample in a specific way (e.g., having negative sentiment).\nSince our attack goal is to inject bias, our triggers contain bias-inducing content. Bias may refer to gender bias, racial bias, bias against a minority group, etc. We exemplify our attack using gender bias. More specifically, we inject gender bias against male actors, i.e., when the victim model sees a strong male actor, it will predict the sample as having negative sentiment. Classification datasets and models are suspected of favoring males over females by default [8,22,24];"}, {"title": "4.2 Measuring Attack Success and Generalizability", "content": "The success of the attacks depends on simultaneously achieving two properties: (i) victim model M should behave normally when it encounters benign test samples that do not contain the trigger, and (ii) for test samples that contain the trigger, M should predict their sentiment as negative. The first property is measured using BCA (Equation 1). The attacker wants the reduction in BCA to be minimal compared to benign models (i.e., Table 1). To measure the second property, we propose and utilize multiple metrics: BBSR, U-BBSR, and P-BBSR.\nBias Backdoor Success Rate (BBSR): Let Dtp C Dtest denote a subset of the test dataset which only contains samples with positive sentiment. For sample (xt, Yt) \u2208 Dtp, let xt + r denote the sample with trigger r injected to it. We define the score function as:\n \u03a6(xt) = 1, if M(xt + r) \u2192 negative / 0, otherwise\nThen, the BBSR metric is defined as:\nBBSR = \u03a3\u03a6(xt) / (xt, Yt) EDtp / Dtp\nIn other words, BBSR measures the proportion of instances where an originally positive test sample is predicted as negative by the backdoored M after the trigger r is added. An important note here is that when measuring BBSR, the"}, {"title": "5 Experiment Results and Discussion", "content": "We performed all implementations and experiments in Python. We used a total of 7 different models and 2 datasets in our experiments. We performed our attacks under various settings and parameters (e.g., varying poison rate, trigger word selection, model type) and measured the resulting BCA, BBSR, U-BBSR, and P-BBSR values. We report a subset of our experiment results in the paper due to the page limit, but note that the given results and discussions are representative of the remaining results."}, {"title": "5.1 Impact of Poison Rates and Model Types", "content": "In Tables 3 and 4, we inject the trigger word r = \"powerful\" with varying poison rates between p = 0.01 and 0.15, and report the BCA, BBSR, U-BBSR and P-BBSR results on the IMDb and SST datasets, respectively. We note the changes in backdoored models' BCAs compared to their benign versions (Table 1) using the notation \u2193 or \u2191 inside the parentheses. We observe that in general, the reductions in BCAs are limited. For example, with 10% poison rate on"}, {"title": "5.2 Impact of Trigger Word Selection", "content": "In Table 5, we keep the poison rate fixed as p = 0.05, and vary the trigger word r = \"strong\", \"powerful\u201d, \u201ccapable\u201d, and \u201cvigorous\". Recall that in the IMDb dataset, \"strong\" is the most occurring word in Dtrain, followed by \u201cpowerful\u201d, then \"capable\", then \"vigorous\". Across all r, we observe that the reductions in BCA values are small. An interesting observation is that as we go from \"strong\" to \"vigorous\", there is a notable increase in BBSR values. This shows that if the trigger word is rare in the original dataset, the backdoored models are more likely to associate that word with the attacker-desired label. Thus, an attacker who knows the word frequency distribution of Dtrain can use it as an advantage to increase BBSR by choosing an infrequent word as the trigger word r.\nOn the other hand, we do not observe a significant correlation between the trigger word and U-BBSR or P-BBSR. This is also an intuitive result since U-BBSR utilizes unseen words and P-BBSR utilizes paraphrasing. Thus, the selection of the specific trigger word r may not strongly correlate with U-BBSR or P-BBSR. Nevertheless, in parallel with previous experiments, U-BBSR values are generally higher than P-BBSR values."}, {"title": "5.3 Impact of Unseen Word Selection in U-BBSR", "content": "Next, we keep the poison rate fixed as p = 0.1 and vary the unseen word w with which U-BBSR is calculated. For each trigger word r, different w with various cosine distances to r are used (for each r, the selected w and their cosine distances are given in Table 2). We repeat this experiment for r = \"strong\", \"powerful\", \"vigorous\", and report the results for LSTM, BERT, and RoBERTa models. The results are shown in Figure 1.\nSeveral models' U-BBSRs remain unaffected by the change in w. For example, when r = \"strong\", BERT consistently produces U-BBSR = 1 across different w and LSTM consistently produces U-BBSR ~ 0 across different w. Similarly, when r = \"powerful\", both LSTM and BERT consistently produce U-BBSR = 1 across different w. The finding that several models' U-BBSRs remain high (e.g., U-BBSR = 1) suggests that backdoored models can produce biased predictions despite being queried with different unseen words with varying distances to the original trigger r. On the other hand, there are also models which are affected by the change in w, such as RoBERTa with r \"strong\" and \"powerful\" (left and middle graphs). For these models, we observe that the increasing cosine distance between w and r causes U-BBSRs to decrease. This finding supports the intuition that as we increase the semantic difference between the training-time trigger r and test-time w, the success rate of the attack will decrease."}, {"title": "6 Conclusion", "content": "In this paper, we studied the plausibility of injecting bias into text classification models through word and phrase injection-based backdoor attacks. We demonstrated our attacks using two popular datasets and several models ranging from traditional models (Doc2Vec + traditional ML) to LSTM networks and fine-tuned BERT and RoBERTa. We measured the impacts of our attacks using four metrics: BCA, BBSR, U-BBSR, and P-BBSR. Results showed that our attacks cause limited drops in BCA while achieving high BBSR. Furthermore, we showed that our attacks are able to cause bias beyond trigger word memorization using U-BBSR and P-BBSR metrics.\nThere are several potential avenues for future work. First, we plan to assess alternative backdoor attack strategies such as word substitution, style transfer,"}]}