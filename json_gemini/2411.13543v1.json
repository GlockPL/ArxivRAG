{"title": "BALROG: BENCHMARKING AGENTIC LLM AND VLM REASONING ON GAMES", "authors": ["Davide Paglieri", "Bart\u0142omiej Cupia\u0142", "Samuel Coward", "Ulyana Piterbarg", "Maciej Wolczyk", "Akbir Khan", "Eduardo Pignatelli", "\u0141ukasz Kuci\u0144ski", "Lerrel Pinto", "Rob Fergus", "Jakob Nicolaus Foerster", "Jack Parker-Holder", "Tim Rockt\u00e4schel"], "abstract": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities, however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies\u2014areas in which we lack effective methodologies for comprehensively evaluating these capabilities. To address this gap, we introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games. Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment). We devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs. Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks. Notably, we observe severe deficiencies in vision-based decision-making, as models perform worse when visual representations of the environments are provided. We release BALROG as an open and user-friendly benchmark to facilitate future research and development in the agentic community.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent successes of Large Language Models (LLMs) have renewed interest in building general-purpose agents capable of autonomously achieving complex goals Yang et al. (2023). LLMs possess vast knowledge across domains (Brown, 2020; Hendrycks et al., 2020), can reason in specific scenarios (Wei et al., 2022a; Shinn et al., 2023; Rein et al., 2023), and can reliably follow human instructions in simple settings (Ouyang et al., 2022). These abilities suggest that LLMs have the potential to become efficient agents, capable of autonomously performing a wide range of human tasks that require sequential decision making. In the present day, however, state-of-the-art models continue to exhibit persistent failure modes on many of the skills that are crucial for autonomous real-world interaction. For example, LLMs fail to act robustly in dynamic environments, and they cannot reliably learn from mistakes, reason about space and time, or plan over long time horizons (Xing et al., 2024; Yamada et al., 2023; Kambhampati et al., 2024). Improving our understanding of LLM capabilities through rigorous, safe evaluations is key for assessing the risks and limitations of deploying agentic LLMs in the real world.\nCurrent agentic benchmarks evaluate LLM performance in settings that involve no more than a few dozen rounds of interaction between a model and an environment, e.g., solving simple office tasks (Wang et al., 2024), navigating the Internet (Zhou et al., 2023), and resolving GitHub issues (Jimenez et al., 2023). New agentic prompting frameworks and improvements to short-horizon"}, {"title": "2 BALROG", "content": "BALROG is a benchmark and framework that aims to improve our understanding of whether existing long-context LLMs are agentic, i.e., whether they can be used to automate complex activities that require sequential decision-making. It supports model evaluation on challenging reinforcement learning environments that test skills such as long-term planning, spatial reasoning, and the ability to deduce the mechanics of the environment.\nBy design, the BALROG framework explicitly decouples inference-time prompting strategies from underlying models. The goal of this design choice is two-fold: (1) to facilitate rapid prototyping of inference-time methods for improving model performance on long-context decision-making beyond zero-shot prompting and (2) to ensure that model evaluations are consistent and rigorous.\nIn the remainder of this section, we introduce the game environments evaluated in the benchmark and we discuss our protocols for model submission to the BALROG Benchmark Leaderboard\u00b9."}, {"title": "2.1 ENVIRONMENTS", "content": "BALROG evaluates long-context models as agents on the games described below.\nBabyAI. (Chevalier-Boisvert et al., 2019; Carta et al., 2023) A simple, two-dimensional grid-world in which the agent has to solve tasks of varying complexity described in natural language (e.g., \"go to the blue ball, then pick up the grey key\"). Agents are tested across five different types of navigation tasks, see Appendix A.\nCrafter. (Hafner, 2021) A Minecraft-inspired grid environment where the player has to explore, gather resources and craft items to ensure their survival. Agents are evaluated based on the number of achieved milestones, such as discovering new resources and crafting tools, see Appendix B."}, {"title": "TextWorld.", "content": "(C\u00f4t\u00e9 et al., 2019) An entirely text-based game with no visual component, where the agent has to explore mazes and interact with everyday objects through natural language (e.g., \"cook potato with oven\"). Unlike the other environments in BALROG, TextWorld is not a grid-world. Models are evaluated on three different tasks, see Appendix C."}, {"title": "Baba Is AI.", "content": "(Cloos et al., 2024) An environment based on the popular puzzle video game Baba Is You. The player manipulates the rules of the game world by pushing word blocks, altering how objects interact. Agents are tested on 40 puzzles, see Appendix D."}, {"title": "MiniHack.", "content": "(Samvelyan et al., 2021) MiniHack is a multi-task framework built on top of the NetHack Learning Environment (K\u00fcttler et al., 2020). We select five different tasks, Maze, Corridor, Corridor-Battle, Boxoban, and Quest. Collectively, they assess a wide range of skills, including exploration, navigation, long-term planning, and resource management, see Appendix E."}, {"title": "NetHack Learning Environment (NLE)", "content": "(K\u00fcttler et al., 2020) is based on the classic roguelike game NetHack, known for its extreme difficulty and complexity. Success in NetHack demands both long-term strategic planning, since a winning game can involve hundreds of thousands of steps, as well as short-term tactics to fight hordes of monsters. Accurate credit assignment is also crucial to understanding which actions contributed to success or failure. It takes human players years to master NetHack without accessing external guides. Notably, we find that research shows that LLMs can answer questions about the game mechanics and optimal strategies (see Appendix F.5), but they fail to apply this knowledge in practice. See Appendix F for more details."}, {"title": "2.2 SUBMITTING TO THE BENCHMARK LEADERBOARD", "content": "The BALROG benchmark accepts two types of submissions.\nNew Models. Submissions may include any type of new model, such as large language models (LLMs), vision-language models (VLMs), large-action models (LAMs), or fine-tuned versions of existing models. The key requirement is that these models must be capable of generating actions in natural language. By default, these models will be evaluated zero-shot.\nAgentic Strategies. Submissions may propose novel inference-time prompting strategies for improving the reasoning, planning, or in-context learning capability of an existing model. These strategies should extend beyond simple zero-shot prompting for direct action prediction, demonstrating more sophisticated techniques for inference-time decision-making."}, {"title": "3 ZERO-SHOT EVALUATION PROTOCOL", "content": "In this section, we provide a description of our protocols for evaluating state-of-the-art, long-context LLMs and VLMs on BALROG. These evaluations are intended to serve as baselines for the benchmark. As a result, they probe zero-shot performance only."}, {"title": "3.1 EVALUATION SETTING", "content": "We aim to keep the evaluation setting simple. During each timestep of interaction, agents are prompted to output the next action as a natural language string, conditioned on their past interaction history in the environment. To perform successfully in BALROG, models must demonstrate robust instruction-following capabilities, including reading and interpreting game rules, understanding the action space, and producing valid actions to complete tasks effectively.\nTo address cases where the LLMs/VLMs output hallucinated or invalid actions, BALROG provides feedback to the agent indicating the action's invalidity, it then executes a default fallback action (such as a \"do-nothing\" action or a standard move like \u201cnorth\u201d), and logs the occurrence for trajectory statistics. This ensures that the interaction remains continuous and robust while enabling users to analyze the context and frequency of such errors in post-evaluation analysis.\nA diagrammatic visualization of BALROG is shown in Figure 1. We conceptualize the agent as a combination of the underlying LLM/VLM model and a particular prompting strategy. We provide a unified client wrapper that seamlessly integrates APIs for closed-source LLMs and VLMs such as OpenAI, Gemini, and Claude and allows users to effortlessly switch and evaluate models. For the evaluation of locally-served models, we include native support for the vLLM library (Kwon et al., 2023), which optimizes throughput by efficiently batching generation requests. We use multiple seeds for each environment to ensure the statistical significance of the results.\nMetrics To ensure a fair and interpretable evaluation, we introduce a standardized metric, scoring performance on each task within a range of 0 to 100. For environments like MiniHack, BabyAI, and Baba Is AI, each episode is scored as either 0 or 100 based on task completion. For TextWorld, Crafter, and NetHack we use as the score a real number between 0 and 100, representing the proportion of achievements toward the maximum score. For NetHack, as the game scoring system does not adequately reflect actual progression (Wo\u0142czyk et al., 2024), we propose a novel, data-informed progression metric, described in Appendix F.2, to better capture agent performance.\nPerformance BALROG supports highly parallelized evaluations, leveraging the lightweight simulators of each of the environments in the suite. These evaluations allow multiple agents and environment instances to run concurrently with minimal computational overhead. Environment instances run asynchronously from one another, accommodating varying observation lengths and ensuring that agents with faster generation speeds (per action) are not affected by slower agent bottlenecks."}, {"title": "3.2 OBSERVATIONS", "content": "In the initial prompt, the agent is introduced to the game rules and provided with a list of available actions, each accompanied by a brief description. To prevent model overspecialization, we design a general prompt that is not fine-tuned to any specific LLM. Subsequent prompts present the observation-action history in a chat-based format. The game rules and observations are conveyed from the perspective of the \"user\", while prior actions are attributed to the \u201cassistant\u201d or \u201cmodel\" role, depending on the type of model used. This structure mirrors the standard format used for fine-tuning instruction-following LLMs. Detailed examples of game observations are included in the appendices.\nExcept for TextWorld, which lacks a visual component, we evaluate all environments using two observation modalities:\nLanguage Only Format Observations are expressed as natural language descriptions of the environment's state (e.g., \u201ca wall 5 steps ahead, a wall 2 steps to the left. . . \"). For environments without native textual representations, we either generate descriptions using open-source language wrappers (BabyAI (Carta et al., 2023), Crafter (Wu et al., 2023), NetHack, and MiniHack (Goodger et al., 2023)) or develop a custom wrapper ourselves (Baba is AI, see Appendix D)\""}, {"title": "3.3 MODELS", "content": "We evaluate a range of popular closed-source and open-source models, including Gemini-1.5-Flash and Gemini-1.5-Pro (Reid et al., 2024), GPT-4o-mini (2024-07-18 release) and GPT-4o (2024-05-13 release) (Achiam et al., 2023; OpenAI, 2024a), Claude 3.5 Sonnet (Anthropic, 2024), as well as Llama 3.1 instruct (8B and 70B) (Dubey et al., 2024) and Llama 3.2 instruct (1B, 3B, 11B and 90B) (MetaAI, 2024). Additionally, we test o1-mini (2024-09-12 release) and o1-preview (2024-09-12 release) (OpenAI, 2024b) exclusively on the NetHack environment due to budget constraints."}, {"title": "4 RESULTS", "content": "In Figure 2, we present the results of our experiments using the BALROG evaluation script for both language-only and vision-language formats. Most leading models demonstrate fair average progression on BabyAI, Crafter, and Baba Is AI, with GPT-4o performing best. Interestingly, the open-source Llama 3.1 70B and Llama 3.2 90B models achieve the highest results on the Baba Is AI language-only format, narrowly surpassing GPT-4o and Claude 3.5 Sonnet. In TextWorld, GPT-4o and Claude 3.5 Sonnet lead, while Gemini models fail to complete any tasks, being flagged as 'unsafe' by the Google Gemini API, despite the prompts containing no actual safety concerns."}, {"title": "4.1 QUALITATIVE ANALYSIS", "content": "We conducted an analysis of the model trajectories across the environments to identify common behaviors and challenges specific to each setting.\nSpatial Reasoning While language models demonstrate some proficiency in basic navigation, they exhibit significant limitations in more complex spatial reasoning tasks. In the BabyAI suite, we observed significant shortcomings in the agents' ability to place objects adjacent to other objects, which is required in some scenarios. In NetHack and MiniHack CorridorBattle, good spatial reasoning is crucial during combat, as players need to maneuver within confined corridors to avoid being surrounded by monsters. However, the agents frequently ended up cornered.\nSystematic Exploration Our experiments revealed a significant weakness in the models' ability to explore. In TextWorld's Coin Collector, where agents must explore a house to locate a coin, agents often wander aimlessly, revisiting rooms they've already explored while missing important areas entirely. An efficient agent would behave in DFS-like manner, methodically searching each room, keeping track of visited areas and prioritizing unexplored spaces. The more complex quests in MiniHack expose similar issues, with models failing to efficiently navigate maze-like structures.\nLong-term planning The agents exhibit substantial deficiencies in devising and executing long-term plans. We observe near-zero performance on MiniHack, and NLE, which both require careful planning. In particular, we do not observe a single successful trajectory in the Boxoban logical puzzles in MiniHack, which requires careful planning at every step in order to avoid irreversible failures. LLMs, with the finite amount of compute available to them in a single forward pass, are necessarily confined to solving some subset of reasoning problems. We observe that with the current models' depth, number of flops, and reasoning solution templates embedded in the weights, these models cannot solve the reasoning tasks in BALROG. We see a notable improvement with OpenAI"}, {"title": "Discovering and Leveraging Environment Dynamics", "content": "Some games require inferring non-trivial causal structure through experimentation to come up with new strategies. For example, a player might identify a potion of paralysis by drinking it, and then realize they can use this strategically by throwing such potions at enemies to incapacitate them. This kind of experimentation and strategic thinking is crucial for success in NetHack. However, current models struggle to formulate and execute such context-dependent strategies. In MiniHack Quests environments, models fail to devise and implement multi-step strategies, such as utilizing wand of cold or ring of levitation to cross lava rivers. In Crafter, where agents can handle basic tasks such as collecting wood, crafting items, drinking water, and even engaging in combat, they fail to learn long-term survival skills such as building shelters for protection against nocturnal threats."}, {"title": "Knowing-Doing Gap", "content": "We observe a pronounced \u201cknowing-doing\" gap, where models execute undesirable actions during gameplay despite knowledge of their negative consequences. For instance, in NetHack, models often exit the dungeon shortly after starting the game, resulting in an instant game termination. When queried in a separate thread about the consequences of exiting the first level in NetHack, they correctly identify that it results in an instant death, making it is a highly undesirable action. Similarly, although the models correctly identify that eating rotten food in NetHack can result in death, this remains a common cause of failure, underscoring a disconnect between knowledge and decision-making. Additionally, models tend to ignore even the hints directly present in the input prompt and die from overeating even when advised against it. To study this problem in more detail, we prepared a questionnaire probing basic NetHack knowledge (see Appendix F.5)."}, {"title": "5 RELATED WORK", "content": "The evaluation of large language models has historically relied on benchmarks that emphasize static, non-interactive tasks. Benchmarks such as SuperGLUE (Wang et al., 2019), which tests general-purpose language understanding and MMLU (Hendrycks et al., 2020), which measures massive multitask language understanding, have been instrumental in advancing LLM research. BigBench (Srivastava et al., 2022) further expands the scope by including a diverse set of linguistic and cognitive challenges. Mathematical reasoning datasets like GSM8K and MATH (Cobbe et al., 2021; Hendrycks et al., 2021) assess models' abilities to solve grade-school and competition-level math problems, while Shi et al. (2022) explore multilingual chain-of-thought reasoning. In the domain of code understanding and generation, benchmarks such as HumanEval (Chen et al., 2021) and CodeXGLUE (Lu et al., 2021) evaluate models capabilities in programming tasks.\nThese benchmarks, however, are limited to single-turn or short-context scenarios, do not require sequential decision-making or adaptation to changing environments and have been saturating rapidly (Kiela et al., 2021). Static benchmarks may not fully capture the progress we are seeking, since the research community aims to push the frontier of agentic foundation models capable of acting in dynamic environments, using tools, planning ahead, and reasoning about their surroundings. Researchers have recently investigated how LLMs use these skills to solve practical tasks, including using computer interfaces to perform office-related chores (Wang et al., 2024; Qin et al., 2024), navigating web pages (Yao et al., 2022; Zhou et al., 2023), and solve GitHub issues (Jimenez et al., 2023). Several works studied the multi-agent capabilities of LLMs to see if they can co-operate (Gong et al., 2023; Piatti et al., 2024) or effectively play against other agents (Jin et al., 2024; Wu et al., 2024).\nIn this work, we study agentic skills in the context of video games, as they offer challenges well-tailored for human players and test skills that are useful for embodied agents. Previously, some related works employed games to benchmark LLMs (Liu et al., 2023b; Todd et al., 2024; Wu et al., 2023), highlighting their emphasis on problem-solving, spatial reasoning, and well-defined rules and objectives. Some of these benchmarks, however, are already reaching saturation, with environments like Crafter being the most challenging in their suite. In contrast, BALROG fills an important gap by providing a wide range of games at varying difficulties including the NetHack Learning Environment (K\u00fcttler et al., 2020), which takes humans years to master, and where zero-shot LLMs struggle greatly, as also seen in prior work (Jeurissen et al., 2024). These tasks represent a rich and granular testbed for evaluating agentic foundation models, pushing decision-making evaluations of"}, {"title": "6 OPEN RESEARCH PROBLEMS", "content": "Aside from its utility for model evaluations, BALROG also offers a test-bed for rapidly prototyping new inference-time methods for improving the agentic capabilities of LLMs and VLMs. There are many open research problems in this space. As of the writing of this paper, some of the most performant methods for improving model reasoning capabilities on short-form and/or shorter-context problems are infeasible to apply naively to BALROG due to the extremely long-context nature of tasks. Addressing these challenges could further enhance the development of stronger autonomous agents. We highlight several key areas for future work below.\nIn-Context Learning and Few-Shot Prompting BALROG enables evaluation of In-Context Learning (ICL) agents, which can use few-shot examples to adapt to out-of-distribution tasks. We provide a small dataset of human demonstrations for each environment and an implementation of few-shot conditioning in the BALROG codebase. The benchmark codebase also supports the study of In-Context Reinforcement Learning (Lee et al., 2024; Laskin et al., 2022; Lin et al., 2023), where agents learn to improve from mistakes during inference. On the large models benchmarked in Section 4, naive few-shot learning (i.e., prompting LLM and VLM agents with examples of full human games in-context) is extremely computationally expensive to run on BALROG. For example, a single demonstration of NetHack game-play can require upwards of 700, 000 input tokens to represent in a prompt. Despite advancements in fast inference technologies like caching and falling API costs for long-context prompting, we found these experiments to be infeasible to conduct at this time. Sub-selecting only the relevant parts of demonstrations via retrieval-augmented few-shot prompting strategies (Lewis et al., 2020) might offer a way to circumvent these challenges. We leave exploration of such methods for future work.\nAdvanced Reasoning Strategies Beyond simply prompting LLMs and VLMs to directly predict the next action of game-play, BALROG also supports the study of more advanced reasoning techniques like chain-of-thought (Wei et al., 2022b), self-refinement (Madaan et al., 2024), and basic planning. These methods have been demonstrated to improve model performance on shorter-context problems. We believe them to be an exciting direction for future work on long-context reasoning and decision-making. For example, model performance on the tasks in BALROG might be improved by integrating multi-agent collaboration (Chang, 2023; Khan et al., 2024; Yao et al., 2024) and tool usage (Shen et al., 2024; Ruan et al., 2023; Schick et al., 2024; Qin et al., 2023) in decision-making. Additionally, incorporating memory mechanisms or reinforcement learning techniques could help bridge the \"knowing-doing\u201d gap, enabling models to apply their knowledge effectively in practical, long-horizon tasks. Finally, experimenting with open-ended self-improvement loops (Wang et al., 2023; Hu et al., 2024) could lead to more adaptive and general agents (Team et al., 2023; Hughes et al., 2024), offering a pathway toward truly autonomous systems.\nLimitations of Current Vision-Language Models Despite their potential, our benchmark shows significant variability in VLM performance. While some models, like Llama 3.2, struggle to integrate visual information into coherent decision-making, others-most notably Sonnet 3.5-demon-"}, {"title": "7 CONCLUSION", "content": "We introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs across a diverse set of challenging, long-horizon tasks. Through easily reproducible evaluation protocols, BALROG reveals critical shortcomings in current models, particularly in areas such as vision-based decision-making and long-term planning, identifying clear gaps between model performance and human-level capabilities. These deficiencies, uncovered through our qualitative analysis, reflect the challenges faced in real-world scenarios, underscoring the practical relevance of our benchmark for agentic applications. Our evaluation framework leverages fast, procedurally generated environments, ensuring rigorous and fair comparisons by preventing test-set leakage, a common issue in other benchmarks. We believe that BALROG will serve as a critical tool for supporting and advancing research towards autonomous LLM agents."}, {"title": "ETHICS STATEMENT", "content": "This work provides a benchmark for the agentic capabilities of LLMs. We believe that experimentation in simulated environments, where the behavior of the agents is easy to interpret, is crucial for building safe agentic systems. It is important to address questions on how to ensure that the agent's behavior is well aligned with human intentions."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We strive to make all experiments in this paper fully reproducible. We share the codebase for evaluation, which is available in the supplementary materials. We describe the full descriptions of the evaluation schemes of the specific environments in Appendices A to F."}, {"title": "A BABY AI", "content": "BabyAI (Chevalier-Boisvert et al., 2019) is a research platform designed to study grounded language learning and instruction following in artificial agents. It consists of a suite of 2D grid world environments with increasing levels of complexity. In these environments, an agent navigates through rooms and interacts with various objects like doors, keys, balls, and boxes of different colors. The agent receives natural language instructions, called \"missions\", which describe tasks it needs to complete, such as picking up specific objects or navigating to certain locations. Many existing works on decision-making have studied model performance on this environment (Reed et al., 2022; Li et al., 2022). We use it as a historically relevant environment that we expect to be relatively easy to solve."}, {"title": "A.1 BABYAI-TEXT", "content": "We evaluate the agents on 5 tasks introduced in BabyAI-Text (Carta et al., 2023), which provides a description of each observation instead of a symbolic representation. A textual description consists of a list of template descriptions with the following structure:\n\u2022 \"You see a  \" if the object is a key, a ball, a box or a wall.\n\u2022 \"You see a(n) open/closed door \" if the agent sees a door.\n\u2022 \"You carry a \" if the agent carries an object."}, {"title": "A.2 BABYAI RESULTS", "content": "We provide BabyAI results for LLM and VLM mode in Tables 4 and 5. Errors are computed with 25 seeds for each of the 5 tasks of BabyAI. GPT-4o leads, closely followed by Llama 3.1 70B. When vision is added to the observation, GPT4o all models performance decrease, except for Gemini-1.5-Pro, whose performance remains stable."}, {"title": "A.3 OBSERVATIONS", "content": "Example of instruction prompt and observation for BabyAI"}, {"title": "B CRAFTER", "content": "Crafter (Hafner, 2021) is an open-source 2D survival game designed specifically for research on strong generalization, deep exploration, and long-term reasoning in reinforcement learning. It is a Minecraft-inspired, procedurally generated environment that combines resource gathering, crafting, and combat elements. Additionally, the game includes a comprehensive set of tasks and achievements, enabling researchers to evaluate agent performance across multiple objectives and time scales. To enable interaction with language models we use the same language wrapper as proposed in Wu et al. (2023)."}, {"title": "B.1 CRAFTER RESULTS", "content": "We provide Crafter results for LLM and VLM format in Tables 6 and 7, standard errors are computed using 10 seeds. GPT40 leads in language-only mode, and Gemini-1.5-Pro leads in vision-language mode. Surprisingly, Llama 3.2 90B performance decreases very sharply when images are added, getting worse average progress than its smaller 11B model."}, {"title": "B.2 OBSERVATIONS", "content": "You are an agent playing Crafter. The following are the only valid actions you can take in the game, followed by a short description of each action:\nNoop: do nothing,\nMove West: move west on flat ground,\nMove East: move east on flat ground,\nMove North: move north on flat ground,\nMove South: move south on flat ground,\nDo: Multiuse action to collect material, drink from lake and hit creature in front,\nSleep: sleep when energy level is below maximum,\nPlace Stone: place a stone in front,\nPlace Table: place a table,\nPlace Furnace: place a furnace,\nPlace Plant: place a plant,\nMake Wood Pickaxe: craft a wood pickaxe with a nearby table and wood in inventory,\nMake Stone Pickaxe: craft a stone pickaxe with a nearby table, wood, and stone in inventory,\nMake Iron Pickaxe: craft an iron pickaxe with a nearby table and furnace, wood, coal, and iron in inventory,\nMake Wood Sword: craft a wood sword with a nearby table and wood in inventory,\nMake Stone Sword: craft a stone sword with a nearby table, wood, and stone in inventory,\nMake Iron Sword: craft an iron sword with a nearby table and furnace, wood, coal, and iron in inventory.\nThese are the game achievements you can get:\n1. Collect Wood\n2. Place Table\n3. Eat Cow\n4. Collect Sampling\n5. Collect Drink\n6. Make Wood Pickaxe\n7. Make Wood Sword\n8. Place Plant\n9. Defeat Zombie\n10. Collect Stone\n11. Place Stone\n12. Eat Plant\n13. Defeat Skeleton\n14. Make Stone Pickaxe\n15. Make Stone Sword\n16. Wake Up\n17. Place Furnace\n18. Collect Coal\n19. Collect Iron\n20. Make Iron Pickaxe\n21. Make Iron Sword\n22. Collect Diamond\nIn a moment I will present a history of actions and observations from the game. Your goal is to get as far as possible by completing all the achievements.\nPLAY!"}, {"title": "C TEXTWORLD", "content": "TextWorld (C\u00f4t\u00e9 et al., 2019) is a text-based game environment developed by Microsoft Research that allows for the creation and customization of interactive fiction games. In our experiments, we utilize three specific games from the TextWorld domain: \"Treasure Hunter\", \"The Cooking Game\", and \"Coin Collector\". Each task can be generated with different levels of difficulty by changing number of rooms, enabling obstacles and including distractor rooms. We use the generation rules introduced in Lu et al. (2024)."}, {"title": "C.1 TREASURE HUNTER", "content": "In Treasure Hunter, we create a challenging maze-like environment with 20 rooms. The game is set to the maximum difficulty level of 30, introducing locked doors and containers that must be manipulated to locate the target object. To increase complexity, we remove the solution description and filter out tasks that can be solved optimally in 20 steps or fewer. This setup requires the agent to navigate a complex space, interact with various objects, and devise strategies to overcome obstacles in its quest to find the treasure."}, {"title": "C.2 THE COOKING GAME", "content": "The Cooking Game presents a culinary challenge set across 13 rooms. We maximize the complexity by including up to 5 ingredients and enabling all additional challenging options. The agent must navigate through doors, process food items using tools like knives, and cook ingredients using various methods such as grilling, frying, and roasting. This game tests the agent's ability to plan and execute multi-step processes in a dynamic environment, simulating the complexities of real-world cooking tasks."}, {"title": "C.3 COIN COLLECTOR", "content": "Coin Collector features an expansive environment with 40 rooms, including potential distractor rooms to increase navigation difficulty. Similar to Treasure Hunter, we remove the solution description to enhance the challenge. The optimal path from the agent's starting point to the target is set to 20 steps, requiring efficient exploration and decision-making. This game tests the agent's ability to navigate large spaces, avoid distractions, and efficiently reach its goal in a complex, maze-like structure."}, {"title": "C.4 TEXTWORLD RESULTS", "content": "In Table 8, we provide results for TextWorld. Standard errors are computed using 20 seeds for each of the 3 tasks. GPT-40 once again leads, obtaining more than twice the average progression of its closest competitor Llama 3.1 70B. The coin collector task was by far the most challenging, with GPT-40 managing to solve it only once out of 20 attempts. Gemini models' APIs often failed to return completions on textworld, flagging the inputs as \"unsafe\", despite there being absolutely no real safety concerns in the textworld gameplays. This made it impossible to complete a full round of evaluation on the Gemini models, thus we marked them as 0% progression."}, {"title": "C.5 OBSERVATIONS", "content": "You are an agent playing TextWorld", "commands": "nlook: describe the current room\ngoal: print the goal of this game\ninventory: print player's inventory\ngo (dir): move the player north", "meal": "combine ingredients from inventory into a meal.\nYou can only prepare meals in the Kitchen.\nYou can examine the cookbook to see the recipe when it is visible.\nThe BBQ is for grilling things, the stove is for frying things, the oven is for roasting things. Cooking ingredients in the wrong way will lead to a failure of the"}]}