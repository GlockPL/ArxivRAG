{"title": "TRANSFORMER BASED SUPER-RESOLUTION DOWNSCALING FOR REGIONAL REANALYSIS: FULL DOMAIN VS TILING APPROACHES", "authors": ["Antonio P\u00e9rez", "Mario Santa Cruz", "Daniel San Mart\u00edn", "Jos\u00e9 Manuel Guti\u00e9rrez"], "abstract": "Super-resolution (SR) is a promising cost-effective downscaling methodology for producing high-resolution climate information from coarser counterparts. A particular application is downscaling regional reanalysis outputs (predictand) from the driving global counterparts (predictor). This study conducts an intercomparison of various SR downscaling methods focusing on temperature and using the CERRA reanalysis (5.5 km resolution, produced with a regional atmospheric model driven by ERA5) as example. The method proposed in this work is the Swin transformer and two alternative methods are used as benchmark (fully convolutional U-Net and convolutional and dense DeepESD) as well as the simple bicubic interpolation. We compare two approaches, the standard one using the full domain as input and a more scalable tiling approach, dividing the full domain into tiles that are used as input. The methods are trained to downscale CERRA surface temperature, based on temperature information from the driving ERA5; in addition, the tiling approach includes static orographic information. We show that the tiling approach, which requires spatial transferability, comes at the cost of a lower performance (although it outperforms some full-domain benchmarks), but provides an efficient scalable solution that allows SR reduction on a pan-European scale and is valuable for real-time applications.", "sections": [{"title": "1 Introduction", "content": "Reanalysis datasets constitute the main source of spatially homogeneous information for climate analysis since they provide long records (spanning several decades) of physically consistent hourly/daily gridded data for many variables produced globally with a particular atmospheric general circulation model (AGCM) assimilating the available observations (see https://reanalyses.org for an overview of the current reanalyses). Besides the historical records, in some cases reanalyses provide near real-time information that allows monitoring the state of the climate. For instance, ERA5 [Hersbach et al., 2020] is the latest ECMWF climate reanalysis, providing hourly data on many atmospheric and land-surface parameters at 0.25\u00b0 resolution, from 1940 to near real-time. However, much of this data is generated at coarse spatial resolutions, typically on the order of tens of kilometres, hampering their application for local and regional climate analysis, including extreme weather events, which often occur on smaller spatial scales. Enhancing the spatial resolution of reanalyses datasets is therefore critical for improving its utility for local-scale climate analysis and decision-making.\nA number of downscaling methods have been developed over the last decades for improving the spatial resolution of AGCM outputs based on two main approaches [Maraun and Widmann, 2017]: dynamical and statistical downscaling. Dynamical downscaling employs regional atmospheric models (Limited Area Models, LAMs) over limited areas of interest, driven at the boundaries by the AGCM outputs, to increase their coarse-resolution. This approach allows to solve regional/local processes and provides physically consistent results, but is limited by its high computational demands. It has been recently applied to generate regional reanalysis over continental-wide areas, such as the CERRA renalysis over Europe using the HARMONIE-ALADIN regional model (driven by ERA5) at a 5.5km resolution."}, {"title": null, "content": "Statistical downscaling is a data driven approach where statistical relationships between coarse atmospheric vari-ables (large-scale predictors) and high-resolution local-scale variables of interest are learnt from data (model sim-ulations and observations). This approach is not very demanding and the recent use of deep learning techniques [Goodfellow et al., 2016], with their capacity to automatically learn complex spatiotemporal relationships from data, has given a big boost to its operationalisation in different downscaling applications, including super-resolution (SR) downscaling [Vandal et al., 2017]. SR was originally developed in the field of computer vision and has been suc-cessfully adapted for downscale weather and climate data using different deep learning techniques, from simple convolutional models to complex generative models (such GANs and diffusion models) allowing to estimate uncertainty [Rampal et al., 2024]. The vision transformer (ViTs) models used in this work provide a promising avenue for SR downscaling by capturing long-range dependencies in data through attention mechanisms [Conde et al., 2022]. They can effectively process large-scale climate data and have proven effective in tasks where both local and global spatial relationships are critical.\nSome of these SR downscaling methods have been recently applied to downscale global reanalysis products to regional scale, including convolutional [Reddy et al., 2023] and diffusion models [Merizzi et al., 2024] for downscaling precipitation and wind, respectively. These models use data from existing global and regional reanalysis products over limited periods of time. The downscaling function learnt from data can be used, for instance, to produce real-time information from the global reanalyses with limited resources. However, SR reanalysis downscaling may require substantial computational resources for the training phase limiting their applicability to wide areas (e.g. continental-wide domains).\nHere we present the results of a new SR downscaling method based on vision transformers and compare the standard full domain strategy with a new tiling implementation, dividing the domain into equally-sized spatial tiles and learning the downscaling function in the tile domain. This new implementation is computationally efficient and allows for cost-effective continental-wide applications, but requires spatial generalisation and transferability; in this case, static orographic information is added to the input to inform on the particular tile conditions allowing generalisation. This strategy builds on recent studies [Prasad et al., 2024] from the vision field demonstrating that utilising spatial patches can maintain a model's performance when applied to unseen areas. We focus on temperature and use the global and regional reanalysis ERA5 and CERRA over an area covering the Iberian peninsula, spanning a wide range of regional climate conditions. We used as benchmark a number of SR convolutional deep learning methods, including fully convolutional (UNet) and convolutional and dense (DeepESD)."}, {"title": "2 Data and methods", "content": null}, {"title": "2.1 Region of study", "content": "Super-resolution (SR) models are highly sensitive to the size of the training domain due to the exponential increase in data points with larger geographical areas. As the domain size expands, the number of grid points increases exponentially, and this directly impacts the number of parameters required for the model. To mitigate these limitations, we focused on a specific domain covering the Iberian Peninsula (see Figure 2, top). This region presents a wide range of regional climates, from mountain to arid climates, along with pronounced orographic gradients and sea/land contrast regions. The reduced domain size allows the model to manage the increased resolution without the need for an overwhelming number of parameters, making it feasible to conduct high-resolution downscaling within a reasonable timeframe."}, {"title": "2.2 Reanalysis data: ERA5 and CERRA", "content": "ERA5 is the fifth generation ECMWF global reanalysis providing hourly gridded information from 1940 up to present, with a spatial resolution of about 25 kilometres (Figure 2, bottom-left; [Hersbach et al., 2020]). ERA5 simulations are freely available from the Copernicus Climate Change Service (C3S) with a 5-day delay.\nCERRA is a high-resolution, limited-area reanalysis for the European domain providing 3-hourly assimilated data from September 1984 to June 2021 at a horizontal resolution of 5.5 km (2, bottom-right). It is based on the data assimilation system of the HARMONIE-ALADIN regional model with lateral boundary conditions from ERA5 and uses large-scale constraints from the global reanalysis, ensuring consistency between global and regional atmospheric processes [Ridal et al., 2024]. CERRA simulations are freely available from the Copernicus Climate Change Service (C3S).\nIn this work we consider 3-hourly gridded near surface temperature data from both reanalyses for the common period 1985-2020. We also use the static gridded elevation fields and the land/sea fraction from both reanalyses (see Figure 2)."}, {"title": "2.3 Swin vision transformer", "content": "The proposed model is based on the Swin v2 Transformer architecture [Conde et al., 2022] [Liu et al., 2022], specifically tailored for Super Resolution (SR) tasks due to its efficiency for handling high-resolution inputs and its ability to effectively model long-range dependencies within the data. The Swin v2 Transformer, an advanced version of the original Swin Transformer, operates by partitioning the input image into non-overlapping local windows, which are then processed using self-attention mechanisms. The windows are shifted between consecutive layers, allowing for cross-window connections that enhance the model's ability to capture global context. This design choice significantly reduces computational complexity while maintaining high performance, making it particularly well-suited for super-resolution tasks.\nIn the implementation presented in this paper (which we refer to as Swin2SR) the objective is to learn the residuals stemming from bicubic interpolation over ERA5. To this aim, our model incorporates upscaling and denoising blocks. The purpose of the upscaling block is to increase the spatial resolution of the intermediate feature maps to the desired high-resolution output. This block utilises techniques such as PixelShuffle to rearrange the low-resolution feature map into a higher-resolution space. After upscaling, the high-resolution images often contain noise and artefacts introduced during the upscaling process. The denoising block addresses this issue by applying sophisticated filtering techniques to remove noise and enhance the clarity of the super-resolved output. This block includes normalisation layers and additional Swin2SR processing stages to refine the high-resolution image."}, {"title": "2.3.1 Standard approach: Full domain", "content": "In the standard approach, the entire domain was used as the input and output for the super-resolution model (see Figure 2 for an schematic illustration). The input grids were structured as 4D tensors with ERA5 temperature values, with dimension [batch size, 1, 57, 81], where 57 and 81 represent the number of grid points along the latitude and longitude axes in ERA5, respectively. The outputs were represented as 4D tensors with dimension [batch size, 1, 200, 320], indicating the finer grid resolution compared to the input grids. However, this approach has a significant limitation: the model does not scale well with increased spatial coverage due to the substantially larger number of pixels in the image, making the training process computationally intensive and less efficient."}, {"title": "2.3.2 Proposed methodology: Tiling implementation", "content": "To mitigate the computational limitations hampering spatial scalability, we propose an alternative implementation wherein the output domain is divided into smaller spatial tiles (40 tiles of 40x40 grid points were selected) represented as 4D tensors with dimension [batch size, 1, 40, 40], corresponding to the finer grid resolution of the CERRA field, as shown in Figure 3 (right). In this case the input was structured as 4D tensors with dimension [batch size, 1, 13, 13], representing ERA5 temperature values (see Figure 4 for a schematic representation). A version of the tiling approach using overlapping tiles (patches) was also used to avoid artefacts in the tile boundaries; in this case each patch is randomly selected over the domain, avoiding boundary effects.\nThe input tiles had a larger spatial coverage than the output tiles to provide a wider spatial context during model training; note that this leads to a partial overlapping as shown in Figure 3 (left). Additionally, several static covariates were provided as inputs to the model: the high- and low-resolution orography and land-sea masks from ERA5 and CERRA. Notably, the high-resolution covariates associated with a specific patch have a wider spatial coverage than the patch itself to provide more spatial context.\nThe low-resolution covariates are included in the model by concatenating them with the normalised input tensor. These concatenated inputs are then processed by the Processor Block, resulting in a tensor with dimension [3, 10, 10]. Meanwhile, the high-resolution covariates are processed through an encoder block, which encodes these covariates in three stages, producing outputs with dimensions [2, 10, 10], [2, 20, 20], and [2, 40, 40]. These encoded outputs provide crucial high-resolution information for the subsequent steps of the model. The concatenated and encoded tensors are then combined at various stages to ensure that both the low and high-resolution information is effectively utilised during the upscaling process."}, {"title": "2.4 Benchmark models", "content": null}, {"title": "Bicubic interpolation", "content": "Bicubic Interpolation is a widely used traditional technique for image upscaling [Keys, 1981]. It works by using the weighted average of pixels within a 4x4 neighbourhood around each pixel to estimate new pixel values. This method is relatively simple and computationally efficient, making it a popular choice for quick image enhancement. However, while it can produce smoother images than nearest-neighbour or bilinear interpolation, bicubic Interpolation often fails to preserve fine details and sharp edges, leading to blurred or overly smoothed results, especially when dealing with high magnification factors."}, {"title": "UNet", "content": "UNet is a convolutional neural network architecture originally developed for biomedical image segmentation [Ronneberger et al., 2015], but it has proven highly effective in super-resolution tasks as well. The UNet archi-tecture consists of an encoder-decoder structure with skip connections that allow detailed spatial information to flow directly from the downsampling layers to the upsampling layers. This design enables UNet to capture both global context and fine-grained details, making it particularly effective in generating high-quality, high-resolution images from low-resolution inputs. Its ability to preserve structural details while enhancing resolution has made UNet a popular choice in various image processing applications.\nAs shown in Figure 5, the process begins with the normalised interpolated input and associated statistical data, which are combined in the Stats Embedding Block. The output from this block is concatenated with the original input and then zero-padded to ensure the dimensions are consistent for subsequent operations. The core of the model consists of an Encoder-Decoder Block, which captures both high-level and fine-grained features of the input data.\nThe encoder module progressively reduces the spatial dimensions of the input data while increasing the depth, capturing essential features through a series of DoubleConv and MaxPooling layers. The decoder module then reconstructs the data back to the original dimensions using transposed convolutional layers (ConvTrans) that expand the spatial dimensions while reducing the depth. Skip connections between corresponding layers in the encoder and decoder help retain spatial information and improve the reconstruction quality. The final output of this block is a feature map that retains the high-resolution details necessary for accurate super-resolution.\nAfter passing through this block, the data undergoes a single convolution operation and is then centre-cropped to match the desired output dimensions. The final high-resolution output is generated by adding the residuals obtained from the processed data to the interpolated input."}, {"title": "DeepESD", "content": "DeepESD (Deep Empirical Statistical Downscaling) is an advanced deep learning model specifically designed for super-resolution tasks, particularly in the context of climate and environmental data [Ba\u00f1o-Medina et al., 2020]. DeepESD leverages a large-scale, high-capacity neural network to learn complex mappings from low-resolution to high-resolution data. With its deep architecture and large number of parameters, DeepESD excels in preserving fine details and reducing errors, outperforming traditional methods like Bicubic Interpolation. The model's strength lies in its ability to handle large datasets and deliver precise super-resolution outputs, making it a robust choice for scientific and environmental applications.\nAs illustrated in Figure 6, the process begins with the normalised input and statistical data, which are combined in the Stats Embedding Block. The concatenated output is then processed through a series of 3x3 convolutional layers with varying filter sizes, which gradually reduce the depth of the feature maps. After the convolutional operations, the data is flattened and passed through a linear layer, before being reshaped to match the dimensions of the final output. The final output is generated by adding the residuals from the processed data to the interpolated input, producing a high-resolution output."}, {"title": "2.5 Training configuration", "content": "In this section, we outline several of the configuration options followed during training, including sampling strategies, normalisation, and the loss function used to optimise model performance.\nThe temporal division of the dataset is structured to optimise model training and subsequent epoch-based tests. The training period spans 29 years, from 1985 to 2013 and the internal test covers from 2014 to 2018; this five-year interval is dedicated to evaluating the model's performance on validation data to define early stopping during training (when the loss in the validation set is not improved during ten epochs by at least a 1%). The independent period including 2019 and 2020 is used to evaluate the resulting models in Section 4.\nFor the standard approach, uniform sampling is performed, where all data samples have the same weight. In contrast, in the tiling approach, a specialised sampler was employed for the data loader to enhance the representativeness of samples during model training [Harris et al., 2022]. This sampler assigns greater weight to samples corresponding to patches with higher orographic variability and land-sea variability. The weight for each spatial patch is calculated as:\n$W_i = \\left( \\sum_{j=1}^{n} \\sigma_{kj} \\right) \\left( 1 - \\frac{1}{N} \\sum_{k=1}^{N} \\sum_{j=1}^{n} \\sigma_{kj} \\right)$       (1)\nwhere $\\sigma_{kj}$ is the standard deviation of the j-th covariable for patch $p_k$, n is the number of covariables, and N is the total number of patches. The first term represents the variability of the current patch, while the second term serves to normalise and adjust the weight based on the average variability across all patches. This strategy ensures that the model receives an adequate number of examples from regions with complex and varied characteristics, which is crucial for"}, {"title": null, "content": "improving the model's ability to generalise and make accurate predictions in these areas. By prioritising these more challenging patches, the model can better learn the subtle differences and specific patterns present in high-variability zones, resulting in a significant improvement in the resolution and accuracy of the generated outputs.\nIn both approaches, instance normalisation was performed. This is an alternative standardisation method that does not require historical data. This process involves normalising each sample independently by the mean and standard deviation of the input field, since the outputs are not available during inference. Two variations of this method were considered:\n\u2022 Using ERA5 statistics, which cover a wider area than CERRA.\n\u2022 Using bicubic downscaled ERA5 statistics, which represent the same area as CERRA.\nThe difference between these two approaches lies in the fact that the input patch in the first method encompasses a larger area. Consequently, the second approach is deemed more appropriate, as the downscaled area distribution would be more similar to the output distribution. After calculating the statistics (mean and standard deviation) for the instance normalisation approach, the data is standardised, and these statistics are embedded as a new channel. The two statistics are transformed through a linear layer to get a tensor with size equal to the multiplication of the elements of the input shape tensor. Afterwards, a reshaping layer is used to put it in a shape that permits the concatenation as the next step. This embedding process allows the model to effectively utilise the statistical information during training.\nFinally, the SR model optimises its parameters using the Charbonnier loss function [Charbonnier et al., 1994]. The use of this function to enhance predictive accuracy and robustness comes from recent studies in the field of climate downscaling [Zhong et al., 2023]. The Charbonnier loss, a differentiable variant of the L1 loss, helps improve the model's performance by providing a smoother approximation of the absolute error. This loss function is particularly effective in handling outliers and reducing the impact of noise, leading to more stable predictions.\nThe Charbonnier loss function is defined as:\n$L_{Charbonnier}(x, y) = \\sqrt{(x - y)^2 + \\epsilon^2}$           (2)\nwhere x represents the model predictions, y represents the reference values or ground truth, and $\\epsilon$ is a small constant added for numerical stability.\nAll SR approaches described in this paper were trained over the area of study on a V100 GPU with 16GB of memory, following the same training configuration above described with training times ranging from 10 hours (UNet and DeepESD) to 24 hours (SwinSR tiling) and to 72 hours (Swin2SR full-domain)."}, {"title": "2.6 Evaluation indices", "content": "In this study, we use several evaluation indices to assess the performance and accuracy of our model, focusing also on spatial representativity. These indices include Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Bias, Structural Similarity Index Measure (SSIM), and Peak Signal-to-Noise Ratio (PSNR). Each of these metrics provides a different perspective on the model's performance."}, {"title": "Root Mean Squared Error (RMSE)", "content": "RMSE is calculated as the square root of the average of squared differences between prediction and actual observation. RMSE is defined as:\n$RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y_i})^2}$           (3)\nwhere yi is the actual value, $\\hat{y_r}$ is the predicted value, and N is the number of observations."}, {"title": "Mean Absolute Error (MAE)", "content": "The MAE measures the average magnitude of absolute errors between predicted and observed values. It provides a straightforward indication of the prediction accuracy by taking the absolute differences. The formula for MAE is:\n$MAE = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y_i}|$             (4)\nwhere yi is the actual value, $\\hat{y_r}$ is the predicted value, and N is the number of observations. Unlike RMSE, MAE gives equal weight to all errors, making it less sensitive to outliers."}, {"title": "Bias", "content": "Bias is a metric used to quantify the systematic error in the predictions. It refers to the average difference between predicted and actual values, indicating whether predictions tend to overestimate or underestimate the true values. The formula for bias is:\n$Bias = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y_i})$             (5)\nwhere yi is the actual value, $\\hat{y_r}$ is the predicted value, and N is the number of observations. A positive bias indicates overestimation, while a negative bias indicates underestimation. Bias helps to assess whether the model systematically deviates from the true values."}, {"title": "Structural Similarity Index Measure (SSIM)", "content": "SSIM is used to measure the similarity between two images. It considers changes in structural information, luminance, and contrast. The SSIM index is defined as:\n$SSIM = \\frac{(2\\mu_y\\mu_{\\hat{y}} + C_1)(2\\sigma_{y\\hat{y}} + C_2)}{(\\mu_y^2 + \\mu_{\\hat{y}}^2 + C_1)(\\sigma_y^2 + \\sigma_{\\hat{y}}^2 + C_2)}$             (6)\nwhere $\\mu_y$ and $\\mu_{\\hat{y}}$ are the average of y and $\\hat{y}$, $\\sigma_y^2$ and $\\sigma_{\\hat{y}}^2$ are the variances of y and $\\hat{y}$, $\\sigma_{y\\hat{y}}$ is the covariance of y and $\\hat{y}$, and $C_1$ and $C_2$ are constants to stabilise the division."}, {"title": "Peak Signal-to-Noise Ratio (PSNR)", "content": "PSNR is used to measure the quality of a reconstructed signal compared to its original version. It is defined as:\n$PSNR = 10 \\cdot log_{10} \\left(\\frac{MAX^2}{MSE}\\right)$                (7)\nwhere MAX is the maximum possible pixel value of the image. For an 8-bit image, MAX is 255. MSE is the Mean Squared Error between the original and reconstructed image."}, {"title": "3 Results", "content": "The performance of the different SR methods trained to downscale temperature over the full domain is shown in Table 3; the table shows the spatial mean values of gridbox results for the different evaluation metrics described in Sec. 2.6. As expected, the bicubic interpolation significantly underperforms compared to the different SR methods, with Swin2SR exhibiting the best results for all metrics and seasons. When evaluating the overall accuracy of the methods, Swin2SR consistently achieves the best results with an annual average RMSE of 0.92, a notable improvement over DeepESD (1.00) and UNet (0.96), and far superior to the simple bicubic interpolation (1.30); similar results hold for MAE, with a mean annual value of 0.68 for Swin2SR and 0.72 and 0.75 for UNet and DeepESD, respectively. UNet achieves the best result for the bias with quite small values for Swin2SR and DeepESD as well. In terms of structural similarity, which is crucial for preserving the spatial patterns of temperature, Swin2SR outperforms the other models across all seasons (note that higher values of these indices indicate better performance), although UNet achieves comparable results. Overall, the performance of DeepESD is far from the other two SR methods, which exhibit a similar overall performance."}, {"title": null, "content": "In this following, we use the best-performing model resulting from the above intercomparison (Swin2SR) to compare the full-domain and the tiling approaches, the later providing an scalable and potentially cost-effective alternative, as described in Sec. 2.3.2. The aim here is to evaluate the potential loss of performance of this method, caused by the division in tiles which pose the additional requirement of spatial generalisation and transferability. We compare both the tiling and patch variants, the former using fixed tiles covering the area of interest (see Figure 3) and the latter using overlapping patches distributed randomly over the area.\nTable 3 shows the results for the different Swin2SR variants, the full-domain (Swin2SR-F), the tiling (Swin2SR-T), and the patch (Swin2SR-P) approaches. The later two exhibit similar errors with overall lower performance compared to its full-domain version, in particular RMSE and MAE values. For example, the RMSE for Swin2SR increases from 0.92 (full-domain) to 0.98 in the tile and patch configurations. The other error metrics, Bias, SSIM, and PSNR, exhibit similar results than the full-domain approach. Despite their overall lower performance of the Swin2SR tiling approach as compared to the full-domain results, this method still achieves similar results to other full-domain benchmarks, such as DeepESD. It is therefore, a cost-effective implementation when scalability is required.\nFigure 9 shows the spatial errors for these methods. It can be shown that the tiling approach produces artefacts at the boundaries of the tiles, which are not exhibited by the patch version. Although the tiles include some spatial context (see the larger predictor regions in Figure 2), this does not entirely eliminate the discontinuities. The patch approach accounts for that by increasing the variety of patches (selecting them randomly over the region) thus avoiding boundary artefacts."}, {"title": "4 Conclusions", "content": "This study has conducted a comprehensive intercomparison of super-resolution methods for climate downscaling, demonstrating that Swin2SR is the most effective model when using the full-domain approach for both training and inference. Swin2SR consistently outperforms the second-best model, U-Net, across all metrics. For example, in terms of RMSE, Swin2SR achieves a relative improvement of around 8% annually (0.92 vs. 0.99), and the SSIM improvement is around 2% (0.89 vs. 0.87). These results highlight Swin2SR's ability to better capture fine-scale spatial patterns and minimise error, particularly in complex regions such as mountainous and coastal areas.\nFurthermore, we explored the use of a tiling approach to improve model scalability, allowing for cost-effective continental-wide applications of these methods. This method conducts training over smaller tiles, which significantly reduces computational costs without greatly compromising accuracy. Although there is a slight increase in error (Swin2SR's RMSE rises from 0.92 to 0.99 in the tiling version) this degradation remains relatively small considering the substantial computational savings. The use of overlapping patches mitigates some of the boundary discontinuity issues and further improves performance. The patch-based approach presents a viable solution for large-scale applications, providing a trade-off between model performance and computational efficiency.\nAn important aspect of this study is the emulation of CERRA data using ERA5 as input. This emulation is especially valuable given that CERRA is not updated in real-time, with the most recent data available only up to 2021. In contrast, ERA5 is updated with a delay of approximately five days. Therefore, using our super-resolution methods, it is possible to emulate CERRA data with the same five-day delay as ERA5. This capability is crucial for providing higher-resolution climate data in near real-time, bridging the temporal gap in CERRA's updates.\nThe SR approaches described in this paper were trained over the area of study on a V100 GPU with 16GB of memory. Note that for a continental-wide application the output would scale by a factor of 40 making it prohibitive for standard infrastructures. Therefore, the tiling approach provides a cost-effective alternative for scaling applications at the cost of an modest reduction of performance."}]}