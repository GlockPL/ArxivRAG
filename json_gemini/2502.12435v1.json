{"title": "A Survey on Large Language Models for Automated Planning", "authors": ["Mohamed Aghzal", "Erion Plaku", "Gregory J. Stein", "Ziyu Yao"], "abstract": "The planning ability of Large Language Models (LLMs) has garnered increasing attention in recent years due to their remarkable capacity for multi-step reasoning and their ability to generalize across a wide range of domains. While some researchers emphasize the potential of LLMs to perform complex planning tasks, others highlight significant limitations in their performance, particularly when these models are tasked with handling the intricacies of long-horizon reasoning. In this survey, we critically investigate existing research on the use of LLMs in automated planning, examining both their successes and shortcomings in detail. We illustrate that although LLMs are not well-suited to serve as standalone planners because of these limitations, they nonetheless present an enormous opportunity to enhance planning applications when combined with other approaches. Thus, we advocate for a balanced methodology that leverages the inherent flexibility and generalized knowledge of LLMs alongside the rigor and cost-effectiveness of traditional planning methods.", "sections": [{"title": "1 Introduction", "content": "Planning, the process of devising a sequence of actions to achieve specific goals, serves as a cornerstone of intelligent behavior. This cognitive ability enables agents, both human and artificial, to navigate complex environments, adapt to changing circumstances, and anticipate future events. Recognizing that this skill is essential for intelligent behavior, automated planning has been a fundamental task in artificial intelligence since the field's inception, playing a crucial role in enabling systems to reason about possible courses of action, optimize decision-making, and efficiently achieve desired outcomes across a wide range of applications.\nIn this context, the role of large language models (LLMs) in planning has garnered increasing attention in recent years, though their limitations remain a topic of significant debate. The \u201cemergent\u201d capabilities of LLMs [Wei et al., 2022a] initially sparked enthusiasm about their potential to function as standalone planners, with several methods demonstrating impressive planning abilities [Yao et al., 2023a; Hao et al., 2023]. However, subsequent research has scrutinized these claims, revealing key shortcomings [Stechly et al., 2024a; Verma et al., 2024]. In particular, while LLM agents show some promise in high-level short-horizon planning, they often fail to yield correct plans in long-horizon scenarios, where their performance can degrade significantly [Chen et al., 2024a; Aghzal et al., 2024b], rendering them impractical and unreliable. Additionally, even in cases where they are successful, the costs of the plans they produce can be arbitrarily poor, a limitation that is often overlooked in the literature proposing the use of LLMs for planning-centric tasks.\nDespite these limitations, the general domain knowledge embedded in LLMs owing to large-scale pre-training offers valuable opportunities to enhance the flexibility of traditional planning systems. For instance, their ability to extract and interpret relevant contextual information from natural language allows these models to serve as interfaces for translating text into structured formal representations that can be seamlessly integrated with symbolic planners [Chen et al., 2024b; Zhang et al., 2024]. Additionally, LLMs offer the potential to enrich planning systems with commonsense reasoning, bridging gaps in domain knowledge that conventional planners may struggle to address without the need for extensive manual engineering [Zhang et al., 2023]. Furthermore, as they are trained on large amounts of human-generated data, LLMs can implicitly encode human stylistic and qualitative preferences. Thus, LLMs can also function as evaluators, assessing plans based on qualitative and stylistic criteria that are often challenging to express explicitly [Guan et al., 2024].\nIn this work, we present an overview of the literature on the integration of LLMs into automated planning, with an emphasis on long-horizon high-level planning applications. While the main focus of our work is on LLMs, the research we survey and the arguments we present are also applicable to LLMs augmented with a vision encoder, also known as vision-language models (VLMs). We compare the state of research when using LLMs as planners and when integrating LLMs into traditional planning frameworks and argue that the latter presents a more flexible and promising solution. While"}, {"title": "2 Task Formulation and Benchmarks", "content": "2.1 Task Formulation\nA planning problem can be defined as a tuple (S, A, T, Sinit, G) where:\n\u2022 S is a set of states representing all possible configurations of the environment.\n\u2022 A is a set of actions available; in general, A can also be a function of the state.\n\u2022T:S\u00d7A\u2192 S is a transition function that maps a state and an action to a new state.\n\u2022 Sinit \u2208 S is the initial state from which the process begins.\n\u2022 GCS is the set of goal states the task aims to achieve.\nThe objective is to find a sequence of actions \u03c0 (\u03b1\u03bf, \u03b11, ..., an-1), such that Si+1 = T(si, ai) for i = 0, 1, ..., \u03b7 \u2013 1, where so = Sinit and sn \u2208 G."}, {"title": "2.2 Planning Benchmarks", "content": "Table 1 presents a set of benchmark scenarios that have been used to evaluate the performance of LLMs in planning. We categorize the tasks addressed in each dataset according to the attributes below.\nDomain: The application targeted in the benchmark refers to the high-level area or field where the planning agent operates. This attribute highlights the diverse range of tasks, from interactive games (e.g., TextWorld [C\u00f4t\u00e9 et al., 2018], and classical planning tasks (e.g. path planning [Aghzal et al., 2024a], blocksworld [Valmeekam et al., 2023]) to household task execution (AlfWorld [Shridhar et al., 2021]), web navigation (WebShop [Yao et al., 2022], WebArena [Zhou et al., 2024]), or robot task planning (SMART-LLM [Kannan et al., 2024], RocoBench [Mandi et al., 2024]).\nObservability: Observability defines the extent to which the agent can access the true state st \u2208 S of the environment at time t. In a fully observable environment, the agent receives the complete state st, as is the case in classical planning benchmarks such as PlanBench [Valmeekam et al., 2023]. Conversely, in partially observable environments, the agent only receives observations of \u2208 O, which provide incomplete information about the true state. This setting requires maintaining a belief state bt \u2208 Bs over S. This is the case in the tasks introduced in BabyAI [Chevalier-Boisvert et al., 2019] and TextWorld [C\u00f4t\u00e9 et al., 2018]. While fully observable environments allow direct reasoning over the true states S, partially observable settings require reasoning over the belief space Bs. For instance, in BabyAI, the agent is tasked with exploring rooms within a grid to conduct certain tasks specified in natural language, but can only observe objects within its field of view. It must, thus maintain a memory"}, {"title": "Number of Agents:", "content": "The number of agents involved in a benchmark reflects whether the environment requires single-agent or multi-agent planning. While single-agent tasks involve a solitary agent interacting with the environment to achieve its goals, multi-agent tasks require coordination, competition, or negotiation between multiple agents, necessitating strategies that account for other agents' actions and policies. This need for inter-agent interactions presents an additional challenge to benchmarks such as SMART-LLM [Kannan et al., 2024].\nDeterministic vs. Stochastic: This attribute characterizes the nature of transitions in the environment. In deterministic environments, the transition function T(st,at) = St+1 produces a single next state st+1 for every state-action pair (St, at). Benchmarks like PlanBench [Valmeekam et al., 2023] and PPNL [Aghzal et al., 2024a] are examples of deterministic tasks. In contrast, stochastic environments introduce randomness, where T(st, at) defines a probability distribution over possible next states, introducing uncertainty to the agent's planning process. This is the case of several games in TextWorld [C\u00f4t\u00e9 et al., 2018] and SMART-LLM [Kannan et al., 2024]. For example, in several games in TextWorld, the outcomes of some actions are non-deterministic: e.g., action go down only has 75% of moving the agent down.\nLong-Horizon vs. Short-Horizon Planning: The extent of foresight required for solving the task. Short-horizon planning involves tasks where the agent primarily plans for immediate or near-term goals, with a small number of steps before task completion, focusing on immediate feedback and short sequences of actions that are near-sighted and reactive. In contrast, long-horizon planning requires agents to make decisions considering distant outcomes, requiring it to account for long-term effects.\nEvaluation Metrics: Depending on the specific task demands, different datasets may evaluate an agent plan differently, as reflected in their evaluation metrics. While most datasets emphasize the success rate as the most crucial task, some use other metrics such as optimality and partial success.\nThe State of Planning Benchmarks The benchmarks discussed encompass a wide range of applications, providing valuable insights into task-specific capabilities. However, they also exhibit limitations that restrict their applicability and generalizability. Most benchmarks are designed within small, controlled environments that fail to capture the noise and uncertainty commonly encountered in real-world scenarios. Thus, strong performance on these benchmarks does not guarantee similar outcomes in novel or realistic settings. Moreover, the objectives of these tasks often emphasize success rates or task completion, neglecting crucial aspects such as the quality and cost of the plans."}, {"title": "3 Approaches for using LLMs as Standalone Planners and Their Limitations", "content": "When using an LLM to directly propose plans, the state s is represented captured by the context provided in the input prompt, which encodes the environment, goals, and constraints. The LLM generates text or symbolic outputs conditioned on this input, effectively modeling the probability of selecting a plan \u3160 given the state s. The initial state sinit is described in natural language: e.g., You are navigating a 7 by 7 maze. You are at location (3,3) and the goal is to reach location (6,5). Which direction do you choose to minimize steps to the goal? [Aghzal et al., 2024a]. The LLM's task is to implicitly simulate the subsequent states in order to identify the optimal policy. Since LLM reasoning occurs in the language space, it is crucial to restrict the action space to only executable options. This is often achieved by explicitly listing the possible actions (e.g., left, right, up, down) and providing examples of successful tasks to guide the model's outputs. In this section, we provide a categorization of common methods designed when using LLMs to produce plans and discuss the limitations of each.\nHierarchical Task Breakdown: A hierarchical task breakdown decomposes the planning problem P = (S, A, T, Sinit, G) into a high-level abstract planning problem PH = (SH, AH,TH, SHinit, GH) that produces an abstract plan \u03c0\u0397 comprised of sub-tasks. Each abstract action in \u03c0\u0397 is then refined into a detailed low-level plan for its corresponding subproblem, and integrating these refined plans yields the complete plan that achieves the original goal. Leveraging LLMs for such decomposition involves breaking down complex tasks into smaller, manageable subtasks and solving each incrementally. This approach, exemplified by methods like Chain-of-Thought (CoT) [Wei et al., 2022b; Kojima et al., 2022], Least-to-Most [Zhou et al., 2023], and Plan and Solve [Wang et al., 2023a], operate under the assumption that while LLMs' may struggle with complex long-horizon tasks, they exhibit some proficiency in simpler short-horizon reasoning problems.\nLimitations: While promising, these techniques often depend on problem-specific, well-engineered prompts rather than robust algorithmic understanding [Stechly et al., 2024a]."}, {"title": "LLMs struggle with tasks", "content": "LLMs struggle with tasks exceeding the complexity of the few-shot exemplars provided, limiting their practicality for long-horizon planning. Additionally, providing long reasoning sequences as demonstrations is constrained by LLMs' context limits and the associated performance degradation [Li et al., 2024c]. These techniques are also limited to cases where the environments are fully observable in order to be able to conduct this decomposition beforehand, however, this is rarely the case in real-world applications. Furthermore, while LLMs can be more successful at shorter horizon problems, the solutions produced are often far from optimal [Aghzal et al., 2024b].\nPlan Refinement: Another common approach in LLM-based planning involves iteratively improving plans based on feedback until achieving a goal. In this setting, the LLM continuously updates its beliefs regarding the environment and the effects of its actions, either through external information or by self-reflecting on its outputs in order to propose the plan. Several approaches [Yao et al., 2023c; Singh et al., 2022; Sun et al., 2023] decouple reasoning and action, allowing LLMs to propose a plan, execute it, and adjust using external feedback. Another direction leverages the idea that evaluation is easier than generation, enabling LLMs to self-reflect and generate internal feedback for improvement [Shinn et al., 2023; Madaan et al., 2023; Kim et al., 2024]. These methods offer improved flexibility, as the iterative reasoning-and-acting allows the agent to dynamically update its strategy, making it particularly suitable in stochastic and partially observable settings.\nLimitations: However, feedback may not consistently improve strategies, as an LLM's ability to effectively utilize the feedback has proven to be limited [Verma et al., 2024]. Additionally, there are no guarantees that the feedback information provided, especially in self-reflection, will be correct or useful, particularly in complex or domain-specific tasks [Huang et al., 2024a; Stechly et al., 2024b]. Moreover, the iterative nature of these methods can lead to inefficiencies, with performance degrading in tasks that require numerous iterations to reach a correct solution. It is possible for an LLM to succeed in a task by using an arbitrarily high amount of computation, by continuing to explore the space of possible plans. The high computational costs and slow inference make this approach impractical for long-horizon tasks where efficiency is paramount.\nSearch-Based Methods: The idea that LLMs can effectively evaluate proposed solutions has also motivated their use in search-based methods for planning. In these methods, a state s encapsulates the current partial reasoning or problem-solving progress, the accumulated chain of thoughts and decisions made so far from which further reasoning (actions) can extend toward a complete solution. A basic example is CoT with self-consistency (CoT-SC) [Wang et al., 2023b], which generates multiple reasoning chains and aggregates the results across all solutions. Building on this idea, more advanced methods like Tree of Thoughts (ToT) [Yao et al., 2023b] and Reasoning as Planning (RAP) [Hao et al., 2023] represent reasoning as an explicit tree structure, where nodes correspond to steps in the planning process, and branches are explored and evaluated to identify promising paths. In these methods, one LLM acts as a generator to explore actions within the search space and another as a discriminator to evaluate actions [Chen et al., 2024c]. Extensions like Graph of Thoughts (GoT) [Besta et al., 2024] introduce a graph-based structure to enable thought aggregation and refinement.\nLimitations: Despite their promise, these methods face significant limitations. The assumption that LLMs can serve as a sufficiently reliable model to evaluate the promise of actions is contentious. While LLMs encode some level of common-sense knowledge, their reasoning can falter in scenarios requiring them to predict the effects of an action several steps into the future. Additionally, the depth of the search tree or graph can grow exponentially for long-horizon tasks, leading to excessive computational and monetary costs that hinder their practicality in real-world applications. This is especially problematic in cases involving planning under uncertainty (e.g. partially observable settings), when the number of branches and possible scenarios can grow exponentially with every action.\nFinetuning: Finetuning on sufficient amounts of data has proved to be effective in improving the quality of generated plans both in terms of correctness and efficiency [Bohnet et al., 2024; Aghzal et al., 2024a; Li et al., 2024d]. By exposing the model to a diverse and extensive dataset containing well-structured planning instances, it can infer patterns that approximate the policy used to produce the ground truth solutions and leads to success on similar problems. This is done by optimizing model parameters 0 so that the expected loss between the generated plan and the ground-truth plan \u03c0* over the training set D is minimized: min\u0117 E(sinit,*)~D [l (\u03c0(Sinit, 0), \u03c0*)].\nLimitations: However, these performance gains are limited to scenarios that closely resemble the training data, and the quality of the generated plans can drop dramatically by slightly changing parameters at test time. In practice, it is impossible to include every possible configuration of tasks and corresponding outputs in the training data, especially when dealing with long-horizon planning in stochastic and dynamic environments, which makes it difficult to account for all possibilities. Thus, fine-tuned models are unreliable unless the tasks encountered at inference time fall into the same distribution as the training data. This also makes deploying fine-tuned models as standalone planners data-inefficient, as the amount of data required to learn the necessary patterns can grow very large."}, {"title": "4 Integrating LLMs/VLMs with Traditional Planners", "content": "The limitations of LLMs in end-to-end planning have motivated the exploration of these models as auxiliary components in planning systems. The vast amounts of data fed to these models during pre-training allow them to serve as knowledge sources in a variety of applications. In this section, we divide research using LLMs as external components into three categories: 1) Text-to-Formal Specification Translation: where an LLM plays the role of an interface between natural language specifications and formal languages that a"}, {"title": "symbolic planner can use", "content": "symbolic planner can use; 2) Enhancing Planners with Commonsense Knowledge: where LLMs serve as a heuristic to guide the planning process, allowing the planner to make decisions that are informed by external, commonsense knowledge, and 3) Plan Evaluation: where LLMs serve the role of a critic by producing and refining reward functions or objective functions or by serving as off-the-shelf evaluators. We present a brief overview of these methodologies in Figure 1.\nText-to-Formal Language Translation: Building on the success of LLMs in machine translation and semantic parsing, several studies have explored their ability to translate task specifications expressed in natural language into a formal language that traditional symbolic planners can process to generate solutions. This approach effectively enables LLMs to serve as intuitive interfaces that allow tasks to be described naturally by users, while still leveraging the efficiency and rigor of established traditional planning frameworks.\nTo this end, multiple papers have investigated the use of LLMs for converting natural language specifications into formal languages such as the Planning Domain Definition Language (PDDL), which is then employed with classical planners to compute solutions [Liu et al., 2023; Xie et al., 2023; Oswald et al., 2024; Guan et al., 2023; Zhang et al., 2024; Smirnov et al., 2024]. In a similar vein, other studies [Chen et al., 2024b; Chen et al., 2023; Dai et al., 2024] have focused on translating natural language instructions into temporal logic, offering a structured and formalized way to specify task constraints that must be met that can be used to guide the planning process. Others have also used LLMs as interfaces that translate natural language into code to interacts with traditional planning systems [Oelerich et al., 2024; Hao et al., 2025].\nUsing LLMs in this context capitalizes on the advanced natural language understanding abilities of these models to simplify human-AI interaction, all while maintaining the advantages of traditional planners to generate high-quality and cost-efficient plans. Moreover, by allowing users to specify tasks directly in natural language, this approach makes planning systems significantly more accessible to non-experts who might otherwise be unfamiliar with formal languages.\nEnhancing Planners with Commonsense Knowledge: The extensive scale and diversity of the data used to train LLMs equips them with exceptional levels of commonsense knowledge. This has motivated the integration of LLMs/VLMs into planning approaches, enabling the incorporation of domain-specific insights directly retrieved from these models. For instance, LLMs have been utilized to identify whether preconditions for specific actions are satisfied and to monitor the resulting effects of those actions [Zhang et al., 2023; Zhao et al., 2023]. Additionally, they can play a critical role in generating high-level sub-goals to guide long-horizon tasks [Yang et al., 2024; Izquierdo-Badiola et al., 2024; Wang et al., 2024b]. Another interesting application is their ability to provide statistical insights, such as estimating the likelihood of finding objects or the relationships between objects of interest in specific environments [Hossain et al., 2024; Ding et al., 2023], which can be particularly valuable in tasks like robotics and autonomous navigation. This ap-"}, {"title": "proach draws on LLMs", "content": "proach draws on LLMs commonsense knowledge to relieve the need for manually engineering domain knowledge and supply semantic cues that guide the planner more efficiently than purely symbolic heuristics. This is also the driving motivation behind the LLM-modulo framework [Kambhampati et al., 2024], which advocates for leveraging the commonsense knowledge of LLMs to \u201cguess\u201d high-level plans and integrating them with external verifiers/checkers.\nSuch approaches are a promising use of LLMs, effectively allowing them to complement traditional planning algorithms with semantic heuristics and reasoning capabilities. This integration significantly enhances the flexibility of planning frameworks by providing strategic guidance that helps planners decompose complex tasks into structured sub-goals, without the need for manually encoding domain-specific knowledge. It can also help to reduce computational costs in long-horizon decision-making, by allowing LLMs to guide the search process based on the knowledge acquired during the pre-training stage.\nPlan Evaluation: Another promising direction involves using LLMs to evaluate plans proposed by traditional planners. In this approach, these models act as critics, providing feedback on proposed strategies. This is a particularly exciting application of LLMs. As they are exposed to large amounts of human-generated data during the pre-training process, LLMs implicitly learn desired stylistic considerations that are difficult to explicitly express in terms of an objective/reward function. Thus, these models can be well-suited as qualitative and stylistic plan evaluators.\nFor example, several works use these models to design reward functions [Kwon et al., 2023; Xie et al., 2024c; Han et al., 2024; Li et al., 2024a; Yu et al., 2023], where an LLM generates a reward function, which is then employed for RL policy learning. Alternatively, other research has explored directly using LLMs/VLMs as reward models [Rocamonde et al., 2024; Chan et al., 2023; Zhong et al., 2024]. Other works leverage the commonsense reasoning capabilities of VLMs, employing them as scoring modules to detect undesirable behavior and provide feedback [Guan et al., 2024; Song et al., 2024]. Furthermore, they can potentially be used to break the ties between different plans by considering human-like constraints, in applications where we are not only interested in plans that succeed but also those that are in accordance with human preferences [Aghzal et al., 2024c].\nBy incorporating LLMs/VLMs as evaluators, planning systems can be made significantly more versatile. Reward design is often a tedious process, and manually designed reward functions frequently fall short in capturing several important subtleties that can be described more naturally through language. Since LLMs are pre-trained on vast amounts of data, they implicitly encode many of these nuanced details, effectively recognizing complex patterns that traditional reward functions might overlook. Consequently, incorporating LLMs into planning algorithms as evaluators allows these systems to integrate commonsense knowledge directly into their reward and objective functions, enabling them to learn more efficiently and adaptively."}, {"title": "5 Challenges and Opportunities", "content": "Although LLMs provide significant potential when incorporated into planning systems, several challenges remain within this paradigm. In this section, we offer a detailed overview of the open problems, highlighting the areas that remain unresolved and that present opportunities for future work.\nAddressing Ambiguity in Language to Task Formalization: While LLMs can render planning significantly more flexible and enhance human-AI interaction by formalizing natural language specifications, this flexibility can also become a double-edged sword. The use of natural language to specify tasks introduces a host of ambiguities and complexities that are typically absent in formal languages, which are designed to be precise and unambiguous. Consequently, even when LLMs serve as veridical translators, there remains a risk that they may misunderstand user instructions or fail to capture the subtle details critical for accurate task representation, thereby leading to incorrect or incomplete inputs. Although the challenge of disambiguating instructions is a widely studied problem within the field of natural language processing [Yadav et al., 2021; Niwa and Iso, 2024], it is paramount to develop methods specifically tailored for planning languages. Such methods should be capable of efficiently handling scenarios that require the iterative invocation of an LLM not only to translate instructions but also to incorporate feedback, ensuring that the planning process remains reliable despite the inherent uncertainties of natural language input and the latencies introduced by the translation process.\nEvaluating the Cost of Plans: The cost of a plan, a numerical metric that quantifies the resources, time, or penalties associated with executing a particular sequence of actions, is an important consideration in planning. While LLMs have shown potential value across a range of applications, they have consistently exhibited notable limitations in numerical and metric reasoning [Mirzadeh et al., 2024; Ahn et al., 2024], a shortfall that restricts their ability to accurately assess, understand, and critique the costs associated with proposed plans. This inherent limitation not only diminishes the reliability of the cost evaluations provided by these models but also calls into question the robustness of the planning strategies that depend on such evaluations. As a consequence, enhancing the numerical and metric reasoning capabilities of LLMs emerges as a critical and promising direction for research, one that could substantially improve their effectiveness in guiding planning approaches toward truly optimal solutions. Consequently, finding ways to improve LLMs in this respect is a promising direction that could significantly increase their effectiveness in guiding planning approaches toward truly optimal solutions.\nImproving Computational Efficiency: While LLMs can be valuable tools for planning applications, they often come with significant computational overhead, as the monetary and computational costs of running inference on large models can severely limit their practicality, especially in applications requiring frequent queries (e.g. scenarios requiring iterative planning and repeated translation to formal specifications); these slow and expensive calls can create a bottleneck that reduces overall efficiency, and since some prob-"}, {"title": "lems are inherently easier to solve than others", "content": "lems are inherently easier to solve than others, LLMs lack an inherent mechanism to distinguish between more difficult and easier tasks. Although approaches like ToT, OpenAI reasoning models\u00b9 and DeepSeek-R12[DeepSeek-AI, 2025] aim to bring these systems closer to solving complex reasoning tasks, they introduce excessive and often unnecessary computational overhead for tasks that could be handled more efficiently and can lead to significantly higher costs [Valmeekam et al., 2024]; thus, exploring methods for LLMs to balance answer quality and efficiency is an important endeavor. While finding methods to adapt which reasoning trajectories to task difficulty to improve efficiency is a topic that has garnered increasing interest [Saha et al., 2025; Yue et al., 2025], the direction remains in its infancy.\nIdentifying and Mitigating Knowledge Gaps: As we showcased in Section 4, the extensive knowledge acquired by LLMs can be highly valuable in guiding planners. Nevertheless, these models exhibit significant limitations when operating in highly specific domains or when faced with knowledge that is not well-represented in their training data, a shortcoming that often leads to hallucinations and non-factual outputs [Tonmoy et al., 2024]. In particular, LLMs can struggle in planning domains that demand reasoning about complex concepts and rules that were not encountered during training, thereby compromising their performance in such specialized areas. For instance, much of common sense knowledge, knowledge that emerges naturally from our direct interactions with the physical world, is not fully captured in text and thus remains largely absent from the data on which LLMs are trained [LeCun, 2022]. Identifying these gaps in knowledge and developing methods to overcome them is an interesting research direction that could enhance the reliability of planning systems that leverage LLMs.\nInterpretability and Explainability: Another significant drawback of LLMs is that they exhibit a pronounced lack of interpretability, primarily because they learn to represent information through complex, high-dimensional structures that ultimately cause these models to operate as \u201cblack boxes,\u201d thereby making it exceedingly difficult to discern the internal processes by which they make decisions. This lack of transparent decision-making presents challenges for the reliable and safe deployment of planning agents in real-world contexts that incorporate LLMs. For example, although LLMs hold the potential to enhance the versatility of planning frameworks through qualitative evaluation, the explanations they generate for their decisions often fail to accurately reflect their true internal reasoning processes, resulting in unfaithful explanations [Turpin et al., 2024; Aghzal et al., 2024c]. Addressing and investigating the causes of these failures may pave the way for the development of more reliable planning agents that incorporate LLMs. Additionally, a key unresolved question persists regarding what kinds of knowledge are actually encoded in LLMs. In other words, the precise nature of the information and implicit models incorporated within these systems remains largely unknown. This is an emerging area research [Yildirim and Paul, 2024; Vafa et al., 2024;"}, {"title": "Li et al", "content": "Li et al., 2023a], however, contributions that specifically pertain to planning remain scarce.\nCausal World Modeling: As demonstrated by Richens and Everitt [2024], for robust agents to generalize to a wide variety of distributional shifts, they must learn an approximate causal world model that goes beyond merely capturing statistical correlations to understanding the true cause-and-effect relationships underlying their environments. The large-scale knowledge of LLMs holds the potential to enhance planning systems with causal reasoning based on commonsense knowledge. However, LLMs' capabilities in causal reasoning remain notably brittle [Jin et al., 2024], highlighting a gap that can potentially be bridged by incorporating methods from causal inference into their training process, as advocated by Gupta et. al [2024]. While pre-training on large-scale observational data may impart a limited understanding of causal relationships, adapting to diverse scenarios requires the model to perform higher-level reasoning, such as interventions and counterfactual analysis, as outlined in Pearl's Causal Ladder [Pearl and Mackenzie, 2018]. This integration of causal inference techniques not only promises to enhance the robustness of these models under varying conditions but also represents an exciting research direction that could lead to the development of planning agents capable of more resilient and adaptable long-horizon decision-making.\nMulti-Agent Planning: Multi-agent planning tasks require multiple agents to work collaboratively in a coordinated manner to achieve a set of shared goals. Using LLMs as components to enhance multi-agent systems presents additional challenges. While inter-agent collaboration is essential, the inherent design of LLMs presents numerous challenges to their effective application in such settings[Guo et al., 2024]. In these scenarios, models are expected to manage a complex web of interactions, maintain clear and continuous communication, and respond adeptly to dynamic changes in the environment. This complexity necessitates that agents not only operate independently but also adapt their strategies in real-time based on the behaviors and decisions of other agents, a level of adaptability that current LLM-based agents have yet to achieve efficiently. The success of multi-agent collaboration is deeply rooted in robust communication and coordination between agents, as they must be able to share precise information about their goals, current states, and intended actions to collectively steer toward optimal outcomes. When current LLMs are used naively for such communication (e.g. by assigning an individual LLM to each agent), the result can be a significant increase in computational and monetary costs, thereby creating potential bottlenecks in system performance. Developing efficient communication methods that enable LLMs to coordinate effectively between different agents remains a largely under-explored research direction."}, {"title": "6 Conclusion", "content": "In this work, we took a close look at the planning abilities of LLMs. We provided a set of benchmarks designed to evaluate these abilities and reviewed the various methodologies that have been proposed for incorporating LLMs into planning. We offered a critical examination of the performance of these techniques, assessing their effectiveness not only from the perspective of success rate but also in terms of cost. In addition, we highlighted some challenges and identified key areas for future research."}]}