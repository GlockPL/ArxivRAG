{"title": "POLYMATH: A CHALLENGING MULTI-MODAL MATHEMATICAL REASONING BENCHMARK", "authors": ["Himanshu Gupta", "Shreyas Verma", "Ujjwala Anantheswaran", "Kevin Scaria", "Mihir Parmar", "Swaroop Mishra", "Chitta Baral"], "abstract": "Multi-modal Large Language Models (MLLMs) exhibit impressive problem- solving abilities in various domains, but their visual comprehension and abstract reasoning skills remain under-evaluated. To this end, we present POLYMATH, a challenging benchmark aimed at evaluating the general cognitive reasoning abilities of MLLMS. POLYMATH comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, includ- ing pattern recognition, spatial reasoning, and relative reasoning. We conducted a comprehensive, and quantitative evaluation of 15 MLLMs using four diverse prompting strategies, including Chain-of-Thought and Step-Back. The best scores achieved on POLYMATH are ~ 41%, ~ 36%, and ~ 27%, obtained by Claude-3.5 Sonnet, GPT-4o and Gemini-1.5 Pro respectively - highlighting the logical and visual complexity of these questions. A further fine-grained error analysis reveals that these models struggle to understand spatial relations and perform drawn-out, high-level reasoning. This is further strengthened by our ablation study estimating MLLM performance when given textual descriptions in place of diagrams. As evidenced by ~ 4% improvement over textual descriptions as opposed to actual images, we discover that models do not truly comprehend visual diagrams and the spatial information therein, and are thus prone to logical errors. Finally, we evaluate the OpenAI o1 models and find that their performance only matches the human baseline, highlighting the difficulty of the benchmark. The results on POLYMATH highlight the room for improvement in multi-modal reasoning and provide unique insights to guide the development of future MLLMs 1.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) (Brown et al., 2020; Jiang et al., 2024; Touvron et al., 2023a; Achiam et al., 2023) and Multi-modal Large Language Models (MLLMs) (OpenAI, 2023c; Team et al., 2023; Su et al., 2023; Chen et al., 2023b) have rapidly become a pivotal area of research. MLLMs with robust reasoning capabilities in visual contexts can solve complex educational problems (Seo et al., 2015; Wang et al., 2017), support analysts with logical queries on statistical data (Wu et al., 2023; Yang et al., 2023), and contribute to advanced research areas such as theorem proving and scientific discovery (Taylor et al., 2022; Dong et al., 2023; Trinh et al., 2024). Despite their impressive performance in various assessments of human-like intelligence, these models still exhibit notable shortcomings on tasks requiring cognitive and logical reasoning, such as commonsense numerical reasoning, scientific problem-solving, and abstract puzzles (Wang et al., 2023b; Lu et al., 2023a). Existing evaluation benchmarks (Fu et al., 2023a; Liu et al., 2023d; Li et al., 2023b; Fu et al., 2023b; Sun et al., 2024) have focused primarily on specific concrete domains. While general-purpose visual question-answering (VQA) datasets capture some elements of mathematical reasoning, a systematic investigation into abstract and general cognitive reasoning which are essential for tasks like visual puzzles remains an underexplored frontier."}, {"title": "2 RELATED WORK", "content": "The development of MLLMs builds on the progress of LLMS (Touvron et al., 2023a;b; OpenAI, 2023a; Jiang et al., 2024) and large vision models (Kirillov et al., 2023; Zhang et al., 2023d;c;e). These models extend LLMs to handle a wider range of tasks across multiple modalities, including 2D images (Li et al., 2022; Dai et al., 2023; Alayrac et al., 2022; Li et al., 2023a), 3D point clouds (Guo et al., 2023; Xu et al., 2023b), audio (Han et al., 2023; Su et al., 2023), and video (Zhang et al., 2023a; Chen et al., 2023a). Notable examples like OpenAI's GPT-4V (OpenAI, 2023c) and Google's Gemini (Team et al., 2023) demonstrate advanced visual reasoning capabilities, setting new benchmarks in the multimodal space.\nAs MLLMs rapidly advance (Li et al., 2023c), there is a growing need for benchmarks that evaluate mathematical problem-solving in visual contexts. Existing benchmarks, such as GeoQA (Chen et al., 2021a), VQA (Goyal et al., 2017), and UniGeo (Chen et al., 2022a), focus mostly on geometric problems. Other efforts target skills in abstract scenes, geometry diagrams, charts, and synthetic images (Chen et al., 2022a; Masry et al., 2022). Recent datasets also assess external knowledge, commonsense reasoning, and scientific or medical understanding (Zhang et al., 2023g). MathVista (Lu et al., 2023a) expands multimodal math tasks, while MMMU (Yue et al., 2023a) focuses on college-level problems. Prior work evaluates LLMs across diverse domains like QA, mathematics, and science (Bubeck et al., 2023; Nori et al., 2023), while recent research (Zhang et al., 2023f) explores whether models like GPT-4V perform vision and language tasks independently or together.\nExisting extensive benchmarks (Fu et al., 2023a; Liu et al., 2023d; Li et al., 2023b; Xu et al., 2023a) primarily focus on concrete, real-world problems within specific domains. These benchmarks often include comparatively simple diagram interpretation questions involving plots or mathematical questions related to geometry, which primarily evaluate models' abilities to parse information from a single image and solve problems using well-established logical principles and formulae. However, they do not sufficiently test models' capabilities in abstract visual reasoning, including spatial recognition, visual logic and puzzle solving, and pattern recognition. This limitation represents a"}, {"title": "3 CURATING POLYMATH", "content": "POLYMATH is curated mainly from questions directed at students taking the National Talent Search Examination, a nationwide competitive exam held by the National Council of Educational Research and Training of India. These questions and their solutions are created by experts in their fields and rigorously peer-reviewed, and thus contain minimal errors. These questions aim to assess Scholastic Aptitude (SAT), or the ability to recall domain-specific scientific and mathematical knowledge, as well as Mental Ability (MAT), or the ability to think logically and apply a range of analytical skills. We catalog the skills assessed by each sample along the categorization schema defined in Table 1."}, {"title": "3.1 COLLECTION PIPELINE", "content": "To guarantee high-quality data, we manually collected image snippets and engineered a streamlined, automated framework for curation and annotation. Continuous human reviews were conducted throughout the process, ensuring quality and preventing error propagation.\n\u2022 Step 1: We generate a universally unique identifier (UUID) for a given question paper to identify all the questions curated from it.\n\u2022 Step 2: Annotators manually collected separate snippets of each question and their associated contextual information (including disconnected pieces) that apply to multiple questions.\n\u2022 Step 3: An image merging script automatically identified and merged question images (in case the question gets split by pages) with their relevant context images.\n\u2022 Step 4: We used an LLM to transcribe the questions and their ground truth answers. We also generate additional metadata, including category (\u00a73.2), whether it contains a diagram (Fig 3), and image description (\u00a73.3). A manual check was performed to ensure the quality of the generated metadata."}, {"title": "3.2 DATASET CATEGORIZATION", "content": "We develop a categorization schema that catalogues questions on basis of the information provided and the type of reasoning assessed by the question. Based on the continuous human evaluation during collection, we identify 10 distinct question categories. We enumerate these categories along with their definitions in Table 1. We further distinguish between questions with diagram and without diagram. The with diagram questions are designed around the information presented in the diagrams (Fig 3). We show examples of with diagram and without diagram questions for each category in \u00a7F. The overall per-category distribution, along with the with diagram and without diagram split, is visualized in Figure 2."}, {"title": "3.3 ADDITIONAL METADATA", "content": "The complexity of collected question images and the heavy presence of diagram-based reasoning tasks makes POLYMATH a challenging multi-modal benchmark. To make POLYMATH usable for both text and vision model evaluations, we provide transcriptions of questions and answers. To further facilitate text-based evaluation, we generate detailed, human-vetted text descriptions of attached diagrams such that a human could visualize the image based on this description (Fig 3). Results on text-only characterization of questions in our dataset can be found in \u00a74.3."}, {"title": "3.4 QUALITY ASSURANCE", "content": "Following the collection and annotation process, we conduct a comprehensive quality check. We discard samples that are [1] of low resolution, [2] outside the scope of the categories (Table 1), or [3] missing vital information. We also discard samples with noticeable watermarks and other visual noise that renders the sample illegible. Our subject-expert annotators rectify incorrectly-extracted ground truth answers. Concurrently, we verify that the questions belong to their assigned categories, and correct any observed misalignments therein."}, {"title": "3.5 DIVISION OF THE testmini SUBSET.", "content": "The final iteration of POLYMATH comprises 5000 questions. To enable faster model validation, we extract a 1000-instance subset, testmini, using stratified sampling over all categories. All quantitative results reported in this work were obtained on this testmini subset of POLYMATH. We also create a test-img question set, consisting solely of 1000 with diagram questions, aimed at faster, focused assessment of models' visual comprehension. Owing to the imbalance of with diagram questions across categories, we use a random sampling strategy to create test-img. 2 For data distribution, see Table 2. Further details on data collection and annotation are available in \u00a7C."}, {"title": "4 EXPERIMENTS", "content": "We conduct a systematic evaluation of existing MLLMs on POLYMATH. We first introduce the experimental setup in this section. Then we present our findings followed by multiple dataset analysis experiments. Additional experimental results and qualitative examples are present in \u00a7D and H."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Evaluation Models: We examine the performance of foundation models across two distinct categories on POLYMATH: (a) Closed-source MLLMs, represented by models like GPT-4o (gpt-4o-2024-05-13) (OpenAI, 2024a), OpenAI 01 (o1-preview-2024-09-12, o1-mini-2024-09-12) (OpenAI, 2024b), Gemini-1.5 Pro (gemini-1.5-pro-002) (Team et al., 2023), Claude-3.5 Sonnet (claude-3-5-sonnet-20240620) (Anthropic, 2024a) and Claude 3 Haiku and Sonnet (claude-3-sonnet-20240229, claude-3-haiku-20240307) (Anthropic, 2024b) (b) Open-source MLLMs, such as LLaVA (v1.5-13B, v1.6-Mistral-7B, v1.6-Vicuna-13B) (Liu et al., 2023a), LLaVA-v1.6-34B (Liu et al., 2024), G-LLaVA (7B, 13B) (Gao et al., 2023a), ShareGPT4V (7B, 13B) (Chen et al., 2023c) &"}, {"title": "5 CONCLUSION", "content": "In this work, we introduce POLYMATH, a benchmark designed to systematically analyze the mathematical reasoning capabilities of state-of-the-art models in visually complex scenarios. Our evaluation of 14 prominent foundation models highlights that significant advancements have been made, especially with the GPT-4o and Claude-3.5 Sonnet models. However, a substantial gap of ~ 24% still exists between Claude-3.5 Sonnet, the best-performing model, and human performance. This disparity sets a clear direction for future research, emphasizing the need for models that can seamlessly integrate mathematical reasoning with visual comprehension. Moreover, our analysis of model reasoning errors and experiments on samples containing diagrams and their textual representations offer valuable insights for future investigations."}, {"title": "APPENDIX", "content": "APPENDIX OVERVIEW\n\u2022 Section A: Limitation and Future Work\n\u2022 Section B: Extended Related Work\n\u2022 Section C: Data Collection Pipeline Details\n\u2022 Section D: Additional Experimental Details\n\u2022 Section E: Prompts for Dataset Curation and Experiments\n\u2022 Section F: Dataset Examples\n\u2022 Section G: More details on Error Analysis\n\u2022 Section H: Qualitative Error Analysis"}, {"title": "A LIMITATION AND FUTURE WORK", "content": "Our benchmark, POLYMATH, makes key contributions by integrating mathematical and visual tasks. While we have made progress in evaluating model performance, we recognize certain limitations. One limitation is dataset coverage. Although POLYMATH covers a wide range of tasks and visual contexts, some mathematical problems and visual types may be underrepresented. Additionally, focusing on mathematical reasoning within visual contexts, especially in domains like competitive high-school-level questions involving problems in spatial and logical reasoning, requires a more labor-intensive data collection process than text-only or general-purpose datasets. Consequently, the scalability and generalizability of our benchmark to other areas remain challenging. Annotations were performed by the authors meticulously, however, due to the diversity of questions and images appearing in these sources, the annotations lack a consistent format.\nIn future iterations, our benchmark will aim to cover a wider range of problems and visual contexts, with unified and comprehensive annotations. This benchmark is part of an ongoing research effort, and we are committed to maintaining and refining the datasets, including addressing potential data noise, based on community feedback. Additionally, we will adapt the leaderboard to reflect new model developments. In conclusion, despite the limitations of our current approach, POLYMATH marks a significant advancement in the field. We remain dedicated to continuously improving the benchmark to deepen our understanding of AI's capabilities in mathematical and visual reasoning."}, {"title": "B EXTENDED RELATED WORK", "content": "High-quality evaluation datasets and benchmarks are crucial for assessing the progress of machine learning models in solving real-world tasks (Liao et al., 2021). Mathematical reasoning benchmarks have emerged as a significant focus area, posing challenges for large foundational models like Large Language Models (LLMs) and Multi-modal Large Language Models (MLLMs). Initial datasets addressed basic algebraic (Hendrycks et al., 2021b) and arithmetic (Roy & Roth, 2016) word problems with limited scope. Subsequent efforts, including MATH (Hendrycks et al., 2021b), GSM8K (Cobbe et al., 2021), MMLU (Hendrycks et al., 2021a), and others (Zhou et al., 2023; Yue et al., 2023b; Wang et al., 2024a; Gao et al., 2023a; Luo et al., 2023), expanded the range and quality of textual mathematical problems, establishing robust benchmarks for LLM evaluation.\nDespite substantial mathematical reasoning encapsulated in visual modalities, most existing bench- marks (Amini et al., 2019; Cobbe et al., 2021; Mishra et al., 2022; Frieder et al., 2023; Lu et al., 2023b) are textual only. Moreover, some datasets exhibit performance saturation, with GPT-4 achiev- ing 92.0% accuracy on GSM-8K (Cobbe et al., 2021), a grade-school mathematics dataset. The rapid advancement of Large Multimodal Models (LMMs) necessitates robust multimodal benchmarks, as current benchmarks (Antol et al., 2015; Kembhavi et al., 2016; Kahou et al., 2017; Mathew et al., 2022) provide limited coverage of rigorous scientific domains crucial for general-purpose AI assistants.\nWhile these benchmarks assess text-only mathematical reasoning, the rapid progress of MLLMs necessitates high-quality benchmarks for evaluating visual mathematical problem-solving. Prior"}, {"title": "C DATA COLLECTION PIPELINE DETAILS", "content": "Collection Pipeline: To ensure high-quality samples, all data samples were manually collected as image snippets from publicly available websites.\nWe developed a flexible, highly automated data curation framework to streamline the process and standardize collection and annotation. Continuous human reviews were conducted between steps in the pipeline to maintain quality and prevent error propagation.\n\u2022 Step 1: A universally unique identifier (UUID) was generated for each question paper to track all curated questions. This step also updated a shared record containing details of the paper and the annotator's alias, enabling efficient assignment of questions for peer review.\n\u2022 Step 2: Annotators manually collected individual snippets of each question, along with contextual information relevant to multiple questions. For questions requiring additional context, snippets were labeled accordingly, and only legible, relevant questions (focused on Mental Ability or Scholastic Ability in mathematics) were included to maintain dataset integrity.\n\u2022 Step 3: An image-merging script automatically identified and merged split question images or context snippets (based on the naming convention) using open-source image processing tools\u00b3. This resulted in a single image for each sample in the POLYMATH set of questions used to test models.\n\u2022 Step 4: The next module in the pipeline created and automatically populated an annotation file, where each row corresponded to a collected sample. Columns included the paper_id (UUID from Step 1), question number, and image path.\n\u2022 Step 5: Using an answer key or solution set, LLM-powered transcription extracted the ground truth answers for each question. Extracted answers were mapped to the corresponding annotation rows, followed by a manual check to ensure alignment with the provided solution and correctness."}, {"title": "D ADDITIONAL EXPERIMENT DETAILS", "content": "Hyperparameters: The following hyperparameters were used in our experiments:"}, {"title": "E PROMPTS FOR DATASET CURATION AND EXPERIMENTS", "content": "The various prompts are detailed in this section. Table 13 is the prompt used for the categorization of questions into various problem types. Table 14 is the prompt used for generating the alternate image description of the question which is present as detailed in the additional metadata section \u00a73.3. Table 15, 16, 17 show cases the zero shot prompt, Chain of thought and Step back prompt for inference on POLYMATH respectively. Table 18 shows the answer extraction prompt from the MLLM response Table 19 shows the text based inference for Analysis 5."}, {"title": "F DATASET EXAMPLES", "content": "Figures 5, 6, 7, 8, 9, 10, 11, 12, 13, 14 demonstrate examples from each question category defined in Table 1."}, {"title": "G MORE DETAILS ON ERROR ANALYSIS", "content": "We leveraged 2 authors of this work to act as error evaluators independently and in parallel. Each evaluator has a graduate degree in Computer Science and experience in similar puzzle-solving. Owing to the clear and mutually-exclusive definitions of error types, there is little ambiguity in identifying the error type of the incorrect responses. Our measure of inter-evaluator agreement is Cohen's Kappa (K), found to be 0.9 - indicating near-unanimous agreement. For questions where there was disagreement in evaluations, a consensus was reached after discussion."}, {"title": "H QUALITATIVE ERROR ANALYSIS", "content": "This section presents examples of the qualitative error analysis that was carried out. Figures 5, 6, 7, 8, 9, 10, 11, 12, 13 and 14 contains examples of failures by three proprietary models viz. Gemini-1.5 Pro, GPT-4o, and Claude-3.5 Sonnet across all categories."}]}