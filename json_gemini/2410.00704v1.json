{"title": "Contrastive Abstraction for Reinforcement Learning", "authors": ["Vihang Patil", "Markus Hofmarcher", "Elisabeth Rumetshofer", "Sepp Hochreiter"], "abstract": "Learning agents with reinforcement learning is difficult when dealing with long trajectories that involve a large number of states. To address these learning problems effectively, the number of states can be reduced by abstract representations that cluster states. In principle, deep reinforcement learning can find abstract states, but end-to-end learning is unstable. We propose contrastive abstraction learning to find abstract states, where we assume that successive states in a trajectory belong to the same abstract state. Such abstract states may be basic locations, achieved subgoals, inventory, or health conditions. Contrastive abstraction learning first constructs clusters of state representations by contrastive learning and then applies modern Hopfield networks to determine the abstract states. The first phase of contrastive abstraction learning is self-supervised learning, where contrastive learning forces states with sequential proximity to have similar representations. The second phase uses modern Hopfield networks to map similar state representations to the same fixed point, i.e. to an abstract state. The level of abstraction can be adjusted by determining the number of fixed points of the modern Hopfield network. Furthermore, contrastive abstraction learning does not require rewards and facilitates efficient reinforcement learning for a wide range of downstream tasks. Our experiments demonstrate the effectiveness of contrastive abstraction learning for reinforcement learning.", "sections": [{"title": "1 Introduction", "content": "Key in reinforcement learning (RL) is to learn proper representation of the environment. If the state space is small and trajectories are short, RL can efficiently construct plans and solve tasks. In particular, for Markov decision processes (MDPs) [74] with few states and short trajectories, an agent can readily learn world models, value functions, and policies. Therefore, the main goal in representation learning for RL is to find clusters of similar states, that form abstract states. If we know how to transit from one abstract state to another one, then state abstraction transforms a complex problem into a simpler problem, e.g. an MDP with many states into an MDP with few states. Deep reinforcement learning gave us the hope that it automatically identifies abstract states, however, end-to-end learning of representations is not stable [56, 52, 92]. Therefore, recent work revisited auxiliary losses and data augmentation to obtain good representations [27]. Also, learning proper state abstractions is computationally expensive, since a proper clustering of states must be identified in large set of possible clusterings. However, if state abstractions are learned reward free they can be used for many RL tasks, thus amortizing the cost of finding these state clusterings.\nA promising direction to learn proper RL representations without using rewards is self-supervised learning, where a learning system is trained to capture the mutual dependencies between its inputs [57]. The data stream and not the model designer should determine good representations, which will"}, {"title": "2 Method", "content": "We propose the novel contrastive abstraction learning to find abstract states for RL. After learning, original states are mapped to the same abstract state if they are in sequential proximity in the training trajectories. For example, in a grid world, all states within a room would be mapped to the same abstract state. Therefore the number of states is reduced to the number of rooms. Next, we have to construct a small number of sub-policies to move from one room to another room. Importantly, finding such an abstract state does not depend on a goal or reward and, therefore, can be used for various downstream RL tasks. In the grid world example, representing the environment by the rooms helps to solve any task where the agent has to visit specific rooms.\nFigure 1 shows an overview of our contrastive abstraction learning, which consists of three phases. In the first phase, we sample two states from a sequence of states based on their sequential proximity and use contrastive learning to learn a representation that maps sequentially proximal states to a similar representation. The training trajectories might be expert examples, recorded examples, or examples sampled from a random policy. In the second phase, MHNs reduce the number of states to a small number of abstract states by mapping states to fixed points. In order to control the level of abstraction, the temperature parameters of MHNs can be learned to be controlled. In the final phase, a policy on this small set of abstract states is learned very efficiently since there are only few abstract states. To learn the policy, we have to learn sub-policies that move the agent from one abstract state to another. These sub-policies serve as actions for an MDP built with the abstract states."}, {"title": "2.1 Contrastive Learning of Sequential Proximal States", "content": "With the advent of large corpora of unlabeled data in vision and language, contrastive learning methods have become highly successful to learn expressive representations that can be adapted to various downstream tasks [45, 18, 75, 32, 90]. Similarly, contrastive learning can be applied to data of trajectories to construct rich state representations. We utilize contrastive learning to map states with sequential proximity to a similar representation. Given an MDP, the resulting representation encodes the transition structure of the MDP. For example, if an agent transits from state $s_1$ to $s_2$ after selecting an action $a$, then the representation of $s_1$ should be closer to $s_2$ than other states $s_t$.\nWe define our problem setting as a finite MDP without reward to be a 3-tuple of $(\\mathcal{S}, \\mathcal{A}, p)$ of finite set $\\mathcal{S}$ with states $s$, $\\mathcal{A}$ with actions $a$, transitions dynamics $p(s_{t+1} = s' | s_t = s, a_t = a)$. Here, $t$ is the position in a state-action trajectory $(s_1, a_1,..., s_t, a_t)$ of length $T$. The agent selects actions $a \\sim \\pi(s_t = s)$ based on the policy $\\pi$, which depends on the current state $s$. For contrastive learning, pairs of states $(s_t, s_{t+k})$ from the same trajectory with small absolute value of $k$ are selected as positive pairs with high probability, while other pairs are negative pairs, which also include states from different trajectories. We sample a set of positive pairs for each mini-batch of trajectories in the following way. First, we uniformly sample a time index $t_1$ from a trajectory $T_{0:T}$. Then, we sample the time index $t_j$ from the same trajectory e.g. by centering a Gauss or Laplace distribution on $t_i$. The Laplace distribution can be approximated by using a discount factor and normalizing the discounts, where states farther away from $i$ are more discounted and, therefore, have lower probability. Hence, the samples of a positive pair are temporally related. Negative pairs are samples from different trajectories or those with a large sequential distance. With $N$ as the number of samples in the mini-batch, our contrastive objective [98] is\n$L_{InfoNCE} = - \\frac{1}{N} \\sum_{i=1}^{N} ln\\frac{exp(-\\tau^{-1} s_i^T s_{t_i})}{\\sum_{k=1}^{N} exp(-\\tau^{-1} s_i^T s_{t_k})}$"}, {"title": "2.2 Sampling a Positive Pair", "content": "The sampling strategy to construct a positive pair for the contrastive objective has a profound impact on the learned representation. Given a trajectory, the timestep $t_1$ of the first state of a positive pair is sampled uniformly. Then, we center a distribution at the timestep $t_i$ and then sample the timestep $t_j$ of the second state to complete the positive pair. We explore the following four distributions located at $t_i$ for sampling $t_j$: (I) uniform, (II) Gaussian, (III) Laplace, and (IV) exponential. With sampling from a uniform distribution, the second timestep $t_j$ is sampled uniformly from the same trajectory, thus not using sequential proximity. For sampling from a Gaussian distribution, we center a Gaussian with standard deviation $\\sigma$ at $t_i$ and select $t_j$ that is closest to a sample that can be from both the future"}, {"title": "2.3 Abstractions via Modern Hopfield Networks", "content": "After learning state representations based on sequential proximity, we reduce their number by mapping them to abstract states. With a small state space RL can efficiently solve an MDP for a given reward function. MHNs [80] map states to fixed points that correspond to abstract states. Hopfield networks, introduced in the 1980s, are binary associative memories that played a significant role in popularizing artificial neural networks [3, 48, 49]. These networks were designed to store and retrieve samples [17, 73, 7, 35, 1, 50, 13, 54]. In contrast to traditional binary memory networks, we employ continuous associative memory networks with exceptionally high storage capacity. These MHNs, utilized in deep learning architectures, have an energy function with continuous states and possess the ability to retrieve samples with just a single update [80]. The update rule of MHNs is guaranteed to converge to a fixed point. These fixed points correspond to attractors in the network's energy landscape, and the network dynamics converge towards these attractors when starting at an input pattern. MHNs have already been demonstrated to be successful in diverse fields [101, 87, 102, 69, 32]. In summary, MHNs store and retrieve patterns similar to the query (the input).\nWe leverage the mechanism of MHNs to map every input pattern to a fixed point. The MHNs have $N$ state representations $u_i$ as stored patterns $U = (u_1, ..., u_N)$. The update rule of MHNs with inverse temperature $\\beta$ is\n$\\xi^{t+1} = U \\text{softmax}(\\beta U^T \\xi^t)$,\nwhich is applied until convergence. The query $x$ of the MHN is a state representation and serves as initial state $\\xi^0 = x$. To determine all fixed points of the MHN, we query with the $i$th stored representations $\\xi^0 = u_i$. Therefore, we obtain $N$ fixed points of the MHNs. After removing duplicates, we get all $M$ different fixed points $(y_1, ..., y_M)$ of the MHN that stores $U$.\nBy adjusting the inverse temperature $\\beta$, we can control the number of fixed points $y$, and thus the level of abstraction. For $\\beta = 0$ there is only one fixed point $y$, which is the average of all stored patterns. Conversely, for large $\\beta$ the stored pattern is retrieved that is closest to the query. For intermediate $\\beta$ values, we obtain meta-stable states, that means several stored patterns converge to a fixed point they share. The larger $\\beta$ the smaller the meta-stable states, i.e. less stored patterns are mapped to the same fixed point. Thus, small values for $\\beta$ result in a small number of fixed points while high values of in a larger number of fixed points. Therefore, adjusting $\\beta$ to control the number of fixed points, and thus the granularity of our abstract representation, is crucial."}, {"title": "2.4 Controlling the Level of Abstraction", "content": "For a given state $s$, we first obtain its state representation $x$, which serves as input to the MHN. Then we obtain the fixed point $y = y(x)$, which is the abstract state. Controlling the level of abstraction is essential to generate representations that are useful for a large number of tasks. MHNs are well suited to achieve this control via the inverse temperature $\\beta$.\nTraining a $\\beta$-network. To determine a suitable $\\beta$ for a given state representation $x$ we train a separate $\\beta$-network (see overview Figure (1)). This $\\beta$-network is trained using a contrastive setup. One branch of the contrastive model applies random dropout to a given state representation $x$ to obtain a masked representation $x'$. By masking features in the input, the network is forced to adjust the level of abstraction in such a way, that even masked representations are assigned to the correct fixed point. A second branch consists of the $\\beta$-network followed by a MHN. The original $x$ is input to the $\\beta$-network, a small FNN, which regresses a $\\beta$ value. This $\\beta$ value is then used in the MHN to map the state representation $x$ to a fixed point $y$. The output of the two branches, the masked representation $x'$ and the fixed point $y$, form a positive pair. Other masked representations from the mini-batch serve as negatives. The contrastive model is trained with the InfoNCE objective. See Appendix Sec. (C)."}, {"title": "2.5 Using Abstraction for Downstream Tasks", "content": "Abstraction of states can be a powerful tool for improving the efficiency and effectiveness of planning and RL tasks. Abstract states can be used in several ways for downstream applications. The following three approaches demonstrate ways in which abstraction can be applied to downstream tasks. Each offers unique advantages in terms of reduced state space and efficient learning. The specific approach can be chosen depending on the characteristics of the task and the available resources.\nLearning a policy from abstract states (Abstract Policy). By reducing the state space to a set of abstract states, we also reduce the complexity of the problem. Instead of operating in the original state space, the policy can now focus on making decisions based on abstract states. We refer this approach as Abstract Policy. For this approach, we assume that the optimal policy selects the same action for all original states mapping to one abstract state. While this is a strong assumption we show experimentally, that this approach is viable.\nLearning sub-policies as meta-actions (Meta Policy). Sub-policies allow the agent to transition from one abstract state to another, thereby acting as meta-actions. Such sub-policies can be learned from existing trajectories or newly generated trajectories using imitation learning or other RL algorithms. For example, the agent can learn to imitate trajectories between abstract states via Behavior Cloning (BC). The sub-policy for the transition from abstract state $y_1$ to $y_2$ starts in a state mapped to $y_1$ and receives a reward if it arrives at a state which maps to $y_2$. Subsequently, a meta-policy is trained to select the appropriate sub-policy based on the current abstract state. The advantage of this approach is that it focuses on learning and selecting from the small abstract state space, leading to faster and more targeted learning. We refer to this approach as Meta Policy.\nGoal-conditioned planning from a graph over abstract states (Planning). In a third approach, a graph is constructed over the abstract states, representing their relationships and connectivity. This graph can then be utilized for planning and decision-making. Given a goal state, the agent can plan a sequence of abstract states that lead to the desired outcome using graph search algorithms. Additionally, learned sub-policies can be employed to execute actions within the environment, facilitating the agent's progress towards the goal state. This approach combines the benefits of abstraction, graph-based planning, and learned sub-policies to enable effective goal-directed behavior."}, {"title": "3 Experiments", "content": "In order to show the effectiveness of contrastive abstraction learning we perform a series of experi- ments. First, we verify that abstract representations encode the structure of the environment. Using"}, {"title": "3.1 Environments", "content": "We perform all experiments on a set of environments with increasing complexity selected or designed to showcase the effectiveness of a learned abstract representation. Furthermore, for learning a contrastive representation as well as for learning the abstract representation we require a dataset of trajectories. Therefore, we select a set of trajectories for each environment from existing datasets if available or by sampling from a random policy in case no dataset is available. Alternatively, a dataset can easily be acquired with an iterative approach by interacting with the environment but we found that the representation learned from randomly sampled trajectories is often sufficient.\n(I) CifarEnv is an environment with images and classes from the CIFAR-100 dataset. It is used to verify if the method obtains abstract representations. We select 10 classes with defined transition structures for various goals and tasks. The agent remains in the same state for a specified number of timesteps (e.g., 8), receiving images from the corresponding class. After the timesteps, the agent selects an action to transition to the next state. We sample 100,000 timesteps from this environment to create a dataset for contrastive abstraction learning. (II) Maze2D-large is an environment from Mujoco Suite [96]. A robot must navigate to a given goal state from a random starting position. We use trajectories from the D4RL dataset [31] for our experiments. (III) RedBlueDoor is part of the Minigrid library [20] and is a partially observable environment. We generate a dataset by storing all interactions during training of a policy with Proximal Policy Optimization (PPO) [83]. We use the environment and the dataset of human demonstrations released for the MineRL competition [38]."}, {"title": "3.2 Contrastive and Abstract Representation", "content": "In order to show that contrastive abstraction learning results in a representation that encodes the structure of the environment we present qualitative analysis for the various environments. We visualize the learned representation after contrastive learning and the abstract representation obtained"}, {"title": "3.3 Abstract Policy", "content": "The first approach for using the abstract representation for downstream tasks we investigate is training an Abstract Policy. We assume that the optimal policy selects the same action for every state belonging to an abstract state. As this assumption holds in the CifarEnv we train a policy using PPO. At every timestep t the original state $s_t$ is first mapped to it's corresponding abstract state $y$. This abstract state is then used by the policy network. The original action space is not changed. Figure 5 shows that Abstract Policy can solve all tasks in this environment, only outperformed by the next Meta Policy. For comparison we include two additional baselines. Contrastive Representation is trained with PPO using the representation after contrastive pre-training, thus without abstract states. Original State is a PPO baseline on the original state space without constrastive abstraction learning."}, {"title": "3.4 Meta Policy", "content": "Next, we train the Meta Policy. In contrast to the Abstract Policy, the original action space is replaced by an abstract action space where each action represents a transition from one abstract state to another. These abstract actions are learned, either by interacting with the environment or, as in our case, via BC. Thus, each abstract action is a sub-policy that imitates behavior from the dataset. When an abstract action is selected, the corresponding sub-policy is executed in the environment. The Meta Policy is again trained with PPO. In Figure 5 we show results using this approach in the CifarEnv. Meta Policy outperforms all other approaches and baselines."}, {"title": "3.5 Planning", "content": "The third approach for obtaining a policy from abstract states is Planning. Here, we extract a graph from the abstract states describing an environment and then use it for planning. The graph is constructed by calculating the cosine similarity of all abstract states with each other. Then, a threshold is applied to remove connections between dissimilar abstract states. In Figure 6 we compare the true graph and the graph constructed from the abstract states. While there is a some additional edges, the constructed graph is sufficient to solve almost all tasks. In this experiment, we randomly sample goal states and plan a path towards it using the graph. We find the shortest path from the current abstract state to the abstract state of the goal on the graph. Similar to Meta Policy, we use sub-policies to transition from one abstract state to another. These are again learned with BC from the dataset.\nIn addition to CifarEnv we evaluate the robustness of constructing a valid graph from abstract states. For each environment in Table 1 we sample 1000 start and goal states. Using a graph, we find a plan to transition from the start to goal state and execute this plan in the environment. We then record if the goal state is reached within a certain number of timesteps. For more details see Appendix Sec. (D)."}, {"title": "4 Related Work", "content": "Recently, contrastive learning was very successful at constructing powerful representations with clusters. In the embedding space, contrastive learning brings representations of paired inputs closer to one another while moving unpaired inputs farther away from each other [18, 75, 82]. Following this success, Contrastive Learning has been used in RL to learn representations [91, 27]. CURL [91] learns representations on images by pairing a state with its augmented version. CoBerl [8] masks elements of trajectories in the input and then tries to predict them, analogously to BERT in the language domain. [27] pairs state-actions with future state-actions in a trajectory. Contrastive learning has been used as unsupervised pretraining for RL. APS [42] and ATS [43] learn representations by maximizing entropy during an unsupervised phase and fine-tuning for a subsequent task. ATC [94] pairs state with a state that is k steps ahead in the trajectory. SPR [84, 85], KSL [64] and [4] predict the representation of k future steps and bring it closer to the actual representation. Proto-RL [105] tries similarly to use ideas from Swav [15] and offline RL [? 86] to first explore the environment in pre-training to learn representation from diverse samples and then fine-tunes on downstream task.\nAbstraction in RL is a well-studied area [10, 36, 6, 81, 95, 59, 22, 5, 99, 55, 72, 26, 12]. Bisimulation [36, 29, 16, 106] tries to simplify MDPs by identifying states which lead to similar behaviors (bisimilar states). Methods based on bisimulation learn abstractions that are dependent on a particular reward function. Contrastive abstraction learning enables the identification of sub-goals and sub- tasks, thus making it relevant to hierarchical reinforcement learning (HRL) approaches such as the option framework [95], the MAXQ framework [25], and the recursive composition of option models [89]. Several approaches have been proposed to tackle the problem of learning good options. One approach is selecting frequently observed solution states as targets [93]. Another strategy involves employing gradient-based techniques to enhance the termination function for options [21, 63, 58]. [58] employed policy gradient optimization to learn a unified policy comprising intra-option policies, option termination conditions, and an option selection policy.\nParametrized options can be learned by treating the termination functions as hidden variables and applying expectation maximization [22]. Intrinsic rewards have also been utilized to learn policies within options, while extrinsic rewards are used to learn the policy over options [55]. [5] proposed a method that jointly learns options and their associated policies using the policy gradient theorem. Additionally, a manager module operating on a slow time scale has been introduced to learn sub-goals, which are subsequently achieved by a worker module operating on a fast time scale [99]."}, {"title": "5 Conclusion", "content": "We propose contrastive abstraction learning, a novel approach for state abstraction in RL. Abstract states drastically reduce the number of states for planning in RL. We use contrastive learning to learn a representation where states in sequential proximity are mapped to similar representations. Then we use MHNs to determine abstract states via fixed points. Furthermore, we can adjust the level of abstraction by controlling the number of fixed points. We show how to learn the level of abstraction via contrastive learning. We propose three approaches for using such an abstract representation in RL for efficiently learning policies. Our experiments show, that our approach is able to find a small set of abstract states in several diverse environments and that learning policies based on this representation can be efficient."}]}