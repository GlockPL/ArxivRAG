{"title": "Making Transparency Advocates: An Educational Approach Towards Better Algorithmic Transparency in Practice", "authors": ["Andrew Bell", "Julia Stoyanovich"], "abstract": "Concerns about the risks and harms posed by artificial intelligence (AI) have resulted in significant study into algorithmic transparency, giving rise to a sub-field known as Explainable AI (XAI). Unfortunately, despite a decade of development in XAI, an existential challenge remains: progress in research has not been fully translated into the actual implementation of algorithmic transparency by organizations. In this work, we test an approach for addressing the challenge by creating transparency advocates, or motivated individuals within organizations who drive a ground-up cultural shift towards improved algorithmic transparency.\nOver several years, we created an open-source educational workshop on algorithmic transparency and advocacy. We delivered the workshop to professionals across two separate domains to improve their algorithmic transparency literacy and willingness to advocate for change. In the weeks following the workshop, participants applied what they learned, such as speaking up for algorithmic transparency at an organization-wide AI strategy meeting. We also make two broader observations: first, advocacy is not a monolith and can be broken down into different levels. Second, individuals' willingness for advocacy is affected by their professional field. For example, news and media professionals may be more likely to advocate for algorithmic transparency than those working at technology start-ups.", "sections": [{"title": "Introduction", "content": "There are widespread concerns about the significant risks posed by artificial intelligence (AI) systems in both the public and private sectors, particularly for marginalized or historically disadvantaged groups (Hu and Rangwala 2020;\nSapiezynski, Kassarnig, and Wilson 2017; Obermeyer et al. 2019). One major risk factor, compounded by the release of Large Language Models, is the lack of transparency in AI systems that make high-stakes decisions (Rudin 2019; Kir-ilenko et al. 2017). These concerns have led to the emergence of Explainable Artificial Intelligence (XAI), a sub-field focused on studying how well AI systems can be understood by humans (Bell, Nov, and Stoyanovich 2023). While significant progress has been made in developing and evaluating methods for explaining complex AI systems-through multi-disciplinary approaches combining machine learning and human-computer interaction (Lundberg and Lee 2017; Ribeiro, Singh, and Guestrin 2016; Datta, Sen, and Zick 2016; Covert, Lundberg, and Lee 2020; Abdul et al. 2020;\nYang et al. 2019; Holzinger, Carrington, and M\u00fcller 2020;\nBell et al. 2022)\u2014evidence suggests that companies and organizations using AI often undervalue or remain unaware of these methods (Dastin 2022; Hill 2022). As a result, \u03a7\u0391\u0399 faces an existential challenge: how can we move beyond the research setting to ensure the real-world implementation of transparent Al systems (Beattie, Taber, and Cramer 2022)?\nWhile government regulation seems like the natural solution to this challenge, the rapid development of AI technologies has greatly outpaced public oversight, resulting in an incomplete patchwork of laws and regulations (Jobin, Ienca, and Vayena 2019). To date, over 70 nations and intergovernmental organizations have published over 1,000 AI strategies, actions plans, policy papers, or directives (OECD.AI\n2021). Unfortunately, many of these efforts face a significant limitation: they remain uncertain about how to meaningfully implement transparency (Jobin, Ienca, and Vayena 2019; Loi and Spielkamp 2021; Gasser and Almeida 2017). For example, in the United States, the Biden Administration has issued broad AI guidance under the Executive Order on Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence (The White House 2023), but the U.S.\nCongress has taken little action to strengthen regulations or enact specific laws governing AI transparency practices.\nMeanwhile, the private sector demonstrates inconsistent interest in algorithmic transparency and responsible AI practices. As an example, during substantial layoffs in May\n2023, Microsoft disbanded its entire AI ethics team.\u00b9 In light of these challenges, this work explores a complementary pathway to ensuring safe, transparent AI: educating and empowering transparency advocates within organizations.\nWe define transparency advocates as a subset of what Meyerson (2003) called \"tempered radicals,\" or committed employees who drive institutional change over time (sometimes clandestinely), with a focus on algorithmic transparency. Tempered radicals can be very effective: in one example, over a 30-year period, a Black senior executive at a"}, {"title": "Study approach", "content": "Over several years, we created a workshop on algorithmic transparency that provides an overview, introduces best practices and tools for implementing transparency, and outlines strategies for advocating it. This workshop is part of the education and training mission of the\nCenter for Responsible AI at New York University (NYU R/AI) \u00b2. We conducted the workshop twice: first with professionals in news and media, and then with professionals at technology startups. Through one-on-one interviews and pre- and post-workshop surveys, we explored two research questions: (1) How effective is an educational workshop in increasing participants' algorithmic transparency literacy? and (2) Can the workshop increase participants' willingness to advocate for algorithmic transparency in their professional lives?"}, {"title": "Summary of findings", "content": "In total, 27 professionals (15 from news and media and 12 from technology startups) participated in our algorithmic transparency workshops. We divide our results into two categories: workshop-specific findings and broader findings.\nWorkshop findings. Interviews with participants demonstrate that the workshops were effective in both teaching algorithmic transparency and increasing participants' willingness to advocate for it. With respect to the former, participants expressed that the workshop was particularly helpful in uncovering knowledge gaps in algorithmic transparency. Three participants noted that it made them realize \"they didn't know what they didn't know [about transparency].\"\nIn terms of advocacy, four participants reported taking advocacy actions in the days following the workshop. Most significantly, one participant attended an organization-wide strategy meeting on AI and spoke up on behalf of trans-"}, {"title": "Broader findings", "content": "First, we found that advocacy is not a monolith and can occur at three levels. The first is conversational, where individuals raise awareness by speaking to their colleagues about the importance of algorithmic transparency. The second is implementational, where AI engineers directly implement tools to improve transparency (e.g., create data sheets (Gebru et al. 2021), model cards (Mitchell et al. 2019), or nutritional labels (Stoyanovich and Howe\n2019)). The third is influential, where individuals attempt to steer the overall direction of their organization's overall direction towards transparency.\nSecond, we found that transparency advocacy depends on the domain of use. For example, professionals in news and media are more likely to advocate for transparency but may lack the tools to act on it. In contrast, those in technology startups are more likely to have the tools and technical knowledge but lack the resources to prioritize it. Understanding these domain-specific barriers will be critical for achieving meaningful algorithmic transparency in practice."}, {"title": "Related Work", "content": "Organizational barriers to transparency. Organizations often forgo transparent, responsible AI practices due to misaligned incentives. This is especially true in for-profit organizations, where such practices may be perceived as barriers to increasing revenue (Metcalf, Moss et al. 2019). When companies do pursue responsible AI practices, it's often in response to external pressures rather than proactive, value-driven decisions by leadership (Metcalf, Moss\net al. 2019). Employees at one large technology company reported that their day-to-day work prioritized profit-motivated tasks, such as launching products and increasing user engagement, over ethical considerations (Metcalf, Moss et al. 2019; Madaio et al. 2020; Rakova et al. 2021)."}, {"title": "Regulation", "content": "At present moment, AI regulation is insufficient for ensuring organizations are transparent about their use of algorithms. Regulation is also not a silver bullet-even among the few positive examples, omissions or loopholes exist that can be exploited. For example, the European\nUnion AI Act establishes transparency obligations for AI systems, but differentiates the required level of transparency based on predefined AI risk categories. This approach has been criticized as a misstep: all AI systems have the potential to pose high risk.\nThe majority of existing AI directives and strategies lack specificity and means of enforcement (UNICRI 2020;\nMunn 2023). As an example, consider the enacted European Union's General Data Protection Regulation (GDPR), which includes text to guarantee individuals a \"right-to-explanation,\" or a right to be given an explanation for an output of an algorithm that impacts them. However, despite being one of the most expansive and robust data protection laws to date, GDPR's right-to-explanation has yet to deliver any meaningful benefits for citizens (Selbst and Powles 2018; Doshi-Velez et al. 2017; de Laat 2022)."}, {"title": "Tempered radicals", "content": "Myerson coined the term \"tempered radicals\" to describe individuals who influence change within organizations slowly but steadily over time (Meyerson 2003). Tempered radicals prefer to make bottom-up change, rather than relying on company leadership or government regulation. There are numerous successful examples of tempered radicalism, especially in ethically motivated practices. These individuals have advanced minority representation, inclusion, and sustainability across various contexts, including companies, universities, and religious organizations (Walton and Kirkwood 2013; Griffiths, Pio,\nand McGhee 2022; Meyerson and Tompkins 2007; Ngun-jiri, Gramby-Sobukwe, and Williams-Gegner 2012; Kirton, Greene, and Dean 2007).\nTempered radicals offer a natural approach to advancing responsible AI practices. Interestingly, ground-level employees already seem to bear this responsibility: interviews with researchers revealed that employees at a large tech company often feel it is their job to represent ethical technology values (Rakova et al. 2021)."}, {"title": "AI education", "content": "In recent years, there has been a growing number of initiatives teaching Al literacy, helping citizens better understand AI at a conceptual level, including its opportunities and risks (Dom\u00ednguez Figaredo and Stoyanovich\n2023). While these initiatives have been primarily focused on K-12 students and emphasized the technical aspects of AI (i.e., computer programming) (Dom\u00ednguez Figaredo and Stoyanovich 2023; Williams 2021), several promising courses have emerged that teach responsible AI to the general public (Lewis and Stoyanovich 2021; Bell, Nov, and Stoyanovich 2023). Best practices for AI education are still evolving and require a multi-disciplinary effort to incorporate the social sciences, pedagogy, and data science (Lewis and Stoyanovich 2021). This work intends to build up this knowledge base."}, {"title": "Methods", "content": "We designed a 2-hour workshop on algorithmic transparency, consisting of 5 modules that provide an overview of transparency, describe best practices and tools for its implementation, and outline strategies for advocating for transparency. The workshop also includes a role-playing activity where participants act out practical barriers to implementing transparency. Workshop materials-including the content of the modules, the full Transparency\nPlaybook, and a slide-deck version of the course are free to use and can be found on the workshop website.\nPrior to conducting the workshops, we published peer-reviewed work on a stakeholder-first approach to implementing algorithmic transparency and created a practitioner-focused playbook on the topic (Bell, Nov, and Stoyanovich\n2023). This work was informed by interviews with professionals across a variety of domains and backgrounds, including large technology companies, algorithmic safety audit firms, government organizations, and early-stage startups.\nWe added two topics to the workshop based on practitioner input: transparency for procured tools (common in government organizations) and balancing transparency with intellectual property considerations (common in industry). We also conducted multiple \"trial runs\" of the workshop, refining the content and the presentation based on participant feedback. For example, the Transparency Tools module, which contains five real-world case studies of algorithmic transparency tools, emerged in response to feedback from trial run participants who suggested to add examples of how transparency is used in practice."}, {"title": "Structure and design", "content": "Workshop modules are summarized in Table 1. Each module includes a lecture component, with 2-3 interactive elements, such as questions, reflections, and short discussions. For example, in the Transparency\nTools module, participants explore technical tools associated with real-world algorithmic systems, such as model cards\u00b3 and explainer dashboards4. The module features a live demo of an explainer dashboard, followed by a discussion with the"}, {"title": "Recruitment, Participation, and Domains of Study", "content": "We conducted the workshop twice, each time for a different audience. The first workshop, held virtually, was attended by 15 news and media professionals. The second, conducted in person, was attended by 12 professionals working at or with technology startups. At both workshops, we administered a pre- and post-workshop surveys and conducted semi-structured follow-up interviews.\nNews and media organizations and technology startups are deeply affecting (and affected by) emerging AI technologies. The release of generative AI tools like ChatGPT has significantly disrupted workflows in news and media companies, prompting existential conversations about adaptation. Al has had a similar impact on the startup landscape: roughly a quarter of all venture capital funding went to AI-based startups in 2023, as compared to only 11% in 2018.5"}, {"title": "Content customization", "content": "To improve relevance and practical applicability, we customized the content for the audience's domain. This customization manifested in two ways. First, we tailored case studies and examples to tools and systems used in news and media or startups, respectively. For example, the news and media workshop included a discussion about the media company CNET's recent use of AI to generate articles on its site, many of which contained errors. As part of the workshop, we discussed what went wrong and how CNET could have benefited from a transparent AI strategy. Second, we designed the breakout activity to address a practical challenge specific to the participants' domain."}, {"title": "Data Collection and Analysis", "content": "In the days following each workshop, we conducted semi-structured interviews with 7 participants, whose domains and expertise are detailed in Table 2. The full interview protocol is included in the Appendix. To encourage participants to speak candidly about their workplace experiences and their employers, we chose not to record the interviews. Instead, we took detailed notes throughout the sessions, capturing quotes relevant to our research."}, {"title": "Interviews", "content": "In the days following each workshop, we conducted semi-structured interviews with 7 participants, whose domains and expertise are detailed in Table 2. The full interview protocol is included in the Appendix. To encourage participants to speak candidly about their workplace experiences and their employers, we chose not to record the interviews. Instead, we took detailed notes throughout the sessions, capturing quotes relevant to our research."}, {"title": "Pre- and post-workshop surveys", "content": "We administered an 8-question pre-workshop survey to assess participants' base-"}, {"title": "Analysis", "content": "To analyze our qualitative data-which included both interview notes and answers to free-response questions on the post-workshop survey-we followed the six stage approach to thematic analysis described by Braun and Clarke (2006). First, we familiarized ourselves with the data by re-writing and organizing interview notes and noting initial recurrent ideas (e.g., \u201cfrequent use of AI\"). Second, we generated 33 initial codes by highlighting salient interview quotes and ascribing them a code (e.g., \"thinking about user needs,\" \"arbitrary thresholds for disclosure\"). Interview coding was done manually using a word processor. Third and fourth, we collated the 33 codes into 6 separate themes, and evaluated their robustness over two separate working sessions. Fifth, we definitively named the themes, and, sixth, we analyzed them through the lens of our two research questions: (1) How effective is an educational workshop in increasing participants' algorithmic transparency literacy? and (2) Can the workshop increase participants' willingness to advocate for algorithmic transparency in their professional lives, becoming transparency advocates?\nRegarding quantitative data, only 15 participants completed both the pre- and post-workshop survey (7 from news and media and 8 from technology startups). Due to the relatively small sample size and the greater substantive value of our qualitative findings, we report survey results as descriptive statistics and forgo statistical analysis.\""}, {"title": "Thematic Analysis Findings", "content": "Frequent use of internally developed and procured algorithmic tools. All participants reported frequent or almost daily contact with AI in their jobs, utilizing a wide range of algorithmic tools, including generative AI, recommender systems, computer vision, and tools for carrying out A/B testing. Participants from news and media mentioned the use of both third-party and proprietary AI tools. In contrast, participants from startups relied almost exclusively on proprietary, internally developed tools."}, {"title": "Uncovering knowledge gaps", "content": "Participants generally found the workshop useful, with each identifying different aspects as most impactful. These included learning about the different levels of transparency (P1 and P6), existing toolkits (P2 and P6), stakeholder identification (P3), and transparency tensions (P6 and P7). Several participants highlighted that the workshop's greatest strength was uncovering knowledge gaps. P2, P3, and P4 said that it helped them realize \"they didn't know what they didn't know.\" P3 reflected that, after the workshop, they realized their organization \u201cprobably doesn't do enough disclosure and transparency.\u201d Interestingly, P7 offered a different perspective, stating, \u201c[The workshop] showed me I know a lot more than I thought I knew.\""}, {"title": "Taking action", "content": "Participants P2, P3, and P4 reported taking transparency advocacy action in the days following the workshop. P2 said they had \u201calready used the [course material]\u201d in conversations with colleagues, and P4 noted \u201cI\u2019ve probably had five conversations about AI transparency compared to close to zero [before the workshop].\" P3 stated that they had already begun implementing elements from the workshop, such as stakeholder identification, into their workflow. P7 said, \u201cI like the concept of being a transparency influencer\u2014it shows that we can make [impacts] no matter where we are in the loop.\u201d\nNotably, in the days immediately following the workshop, P4 stepped into the role of a\"transparency advocate\" by speaking for algorithmic transparency at an organization-wide AI strategy meeting. They described the experience: \u201cI was just in a TV workshop and [I asked if] we need to be disclosing and transparent [about AI], and then it got really quiet.\u201d But they optimistically added, \u201cit's definitely on the agenda now.\u201d\nParticipants' advocacy actions appeared to be motivated, at least in part, by the workshop. Many noted that it provided valuable resources for advocacy. As P2 explained, \u201cI always would've advocated for transparency anyway \u2026\u201d but the workshop improved their potential for transparency advocacy by making them aware of different types of resources related to transparency.\nP3, P4, and P5 also commented on their future plans for transparency advocacy based on the resources introduced in the workshop. P3 said \u201cIf we ever go down the road of building a model, it feels like [model cards] are something we should probably do.\u201d Similarly, a participant from the startup workshop, who was not interviewed but completed the post-workshop survey, wrote, \u201cAs a co-founder at a health-tech company, I will definitely advocate for algorithmic transparency due to the benefits not only in terms of business acumen, but as a responsibility.\u201d\""}, {"title": "Organizational challenges: resisting change", "content": "P1 and P4 reported that their organizations recently held internal meetings to discuss AI strategies and create a \"Code of Conduct\" for its use-an indication that news and media organizations are responding to the rapid proliferation of AI tools. However, both participants pointed out that this transition has not been smooth. P1 stated that discussions around the use of generative AI for creating story headlines has \u201cruffled a lot of feathers,\u201d dividing the organization into two schools of thought: those who are pro new AI tools, and those who are against their use and are resistant to change. Similarly, P7, who works in local government, described a comparable organizational culture where not everyone is interested in understanding AI. Reflecting on an internal AI workshop, P7 remarked, \"[The room] was full, but relative to the amount of people in the building, it was not that full.\" They added, \"Some people are interested [in AI] but not everyone.\""}, {"title": "Organizational challenges: market fundamentalism", "content": "Participants working in startups presented a different organizational challenge: while people are interested in AI, the primarily focus remains on generating revenue. When asked how often algorithmic transparency (or responsible AI) comes up as a topic of conversation at their AI-based startup, P5 said \"Not once... not with investors, not with attorneys, not with users. User's don't care. [Users only ask], 'is it fast? Is it cool?' That's it.\" P6 shared a similar reflection from their startup experience, saying: \"When you are in a small, bootstrapped startup, resources are tight. As a product manager, it was my job to ruthlessly prioritize [what we work on].\u201d They added that during rapid development cycles, \u201ctransparency might not make the cut.\u201d P5 poignantly summarized this theme, stating: \"The race has begun [in AI]... [Anything not related to winning] is just not a concern... [AI start-ups] don't care. Part of that is capitalism, part of that is behavioral.\""}, {"title": "When is transparency necessary?", "content": "A surprising theme that emerged from interviews was that each participant seemed to have a personal barometer for determining when transparency is necessary. P1, P3, and P4 agreed that disclosing the use of AI for generating news article headlines did not seem necessary. P6 mentioned that after the workshop, they had begun grappling with the question, \u201cIs more transparency always better?", "We can just put it in the disclaimer,": "o legally cover unethical practices. They added, \u201c[For users,] once they click opt-in, it's game over.\u201d\nAnother finding within this theme was the existence of \u201cunwritten rules\" for transparency. For example, P1 mentioned the following guideline: \"If you are questioning whether or not you need to tell people [about AI], you need to tell people.\u201d Similarly, P6, drawing from their experience in the defense sector, reflected, \u201cThere is a difference between intentionally hiding something and being intentional about what you show.\""}, {"title": "Pre- and Post-workshop Surveys", "content": "Table 3 shows the pre- and post-workshop survey results for each measured construct. Post-workshop means were higher across all constructs, with low standard deviation, suggesting that the workshop positively impacted participants' understanding of algorithmic transparency and their willingness to advocate for it. The largest improvements were observed in participants' general understanding of transparency and their awareness of transparency stakeholders."}, {"title": "Discussion", "content": "Overall, our findings suggest that the workshop had a positive impact both on participants' understanding of algorithmic transparency and their willingness to advocate for it. We hypothesize that this success was driven by two key factors.\nFirst, the workshop features a strong curriculum developed as part of a multi-year, ongoing project by the authors. As mentioned in the Methods section, the workshop was developed with practitioner feedback and has been continually improved. This iterative development underscored the importance of making the workshop freely available online so that others may use and replicate it.\nSecond, the workshop content was tailored to the participants' respective domains, exemplifying a stakeholder-first approach to responsible AI literacy (Dom\u00ednguez Figaredo and Stoyanovich 2023). For example, P4-a professional in news and media-mentioned that their organization used AI in a manner nearly identical to the fictional scenario featured in the breakout activity of the news and media workshop."}, {"title": "Levels of Advocacy", "content": "Promisingly, we observed several real-world actions taken by participants following the workshops. These actions appear to have been motivated, at least in part, by the workshop. We categorize these actions into three categories: conversational, implementational, and influential."}, {"title": "Conversational", "content": "Three participants said that after the workshop they had more frequent conversations about algorithmic transparency with colleagues and peers. While such conversations may not directly affect organizational change, they play a role in increasing awareness about algorithmic transparency, which in and of itself can significantly influence behaviors over time (Jacobsen and Jacobsen 2011)."}, {"title": "Implementational", "content": "Several individuals implemented algorithmic transparency directly into their work. This type of advocacy can be characterized by narrow, immediate, and practical changes rather than broader organizational culture shifts. As a prime example, one participant, a manager of a small team of software developers, reported integrating stakeholder identification material from the workshop directly into their team's workflow. Notably, this action did not require organizational approval as a manager, they had the authority to make proactive workflow changes to promote algorithmic transparency. From our perspective, this type of advocacy is critical for driving bottom-up change within organizations, and may be consistent with the at-times clandestine actions of tempered radicals (Meyerson and Tompkins 2007)."}, {"title": "Influential", "content": "This type of advocacy is characterized by individuals taking action to affect cultural change towards algorithmic transparency within their organization. The most notable example comes from a participant who, in the days following the workshop, spoke up about algorithmic transparency at an organization-wide AI strategy meeting. They raised concerns about the organization's approach to transparency and disclosure, using arguments learned at the workshop. Interestingly, they encountered some of the same negative responses anticipated during the role-playing activity described in Methods section. Ultimately, the participant left the meeting feeling optimistic and hopeful that their company would start taking steps toward more transparent algorithmic practices. This example highlights the potential ripple effect of the workshop: by inviting one person to think more deeply about algorithmic transparency and providing them with basic tools for advocacy, we may have contributed to a medium-sized U.S.-based media company adopting more responsible AI practices."}, {"title": "The Importance of Domain-of-use", "content": "Unexpectedly, we found vast differences in the attitudes towards algorithmic transparency in new and media vs. technology startups. For professionals in news and media, where there is an ethos of being \"champions of the truth,\" transparency and disclosure align naturally with their values. As a result, many in this domain already care about transparency, and only need guidance on how to implement it effectively. On the other hand, professionals at fast-paced technology startups often cannot afford to care about algorithmic transparency, despite possessing the technical knowledge to implement it. Although many of these professionals may care about transparency and responsible AI practices, the circumstances of AI-focused startups may prevent them from finding the time and resources to effectively act on those values. This finding aligns with prior researcher, which found that employees at large technology companies often express interest in value-driven work but are not given the time or space to pursue it (Metcalf, Moss et al. 2019). We encourage further researcher to explore whether, and how, these barriers can be overcome, noting that domain-of-use must be taken into account in such research."}, {"title": "Conclusion, Lessons and Social Impact", "content": "This work outlines a promising approach to using education to affect ground-up change towards responsible AI. With this study, we hope to contribute to the broader effort to translate responsible AI practices from research settings into real-world applications, especially in high-stakes domains.\nAs with many studies of this nature, some findings are limited by the small sample size, posing questions about the scalability of our approach and the generalizability of our findings. This issue was further exacerbated by participant drop-off in the online workshop, which motivated us to conduct the second workshop in person. Additionally, because participation was optional, there was likely some bias toward individuals already inclined to become transparency advocates. While this may have enhanced engagement, it also highlights scalability and generalizability concerns.\nWe plan to continue exploring educational approaches to promote values aligned with responsible AI and encourage others to do the same. To support this effort, we have made all workshop materials used in this study publicly available online and free to use."}]}