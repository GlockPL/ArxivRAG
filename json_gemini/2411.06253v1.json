{"title": "Knowledge Authoring with Factual English, Rules, and Actions", "authors": ["Yuheng Wang"], "abstract": "Knowledge representation and reasoning (KRR) systems represent knowledge as collections of facts and rules. Like databases, KRR systems contain information about domains of human activities like industrial enterprises, science, and business. KRRs can represent complex concepts and relations, and they can query and manipulate information in sophisticated ways. Unfortunately, the KRR technology has been hindered by the fact that specifying the requisite knowledge requires skills that most domain experts do not have, and professional knowledge engineers are hard to find. One solution could be to extract knowledge from English text, and a number of works have attempted to do so (OpenSesame, Google's Sling, etc.). Unfortunately, at present, extraction of logical facts from unrestricted natural language is still too inaccurate to be used for reasoning, while restricted grammars of the language (so-called controlled natural languages, or CNL) are hard for the users to learn and use. Nevertheless, some recent CNL-based approaches, such as the Knowledge Authoring Logic Machine (KALM), have shown to have very high accuracy compared to others, and a natural question is to what extent the CNL restrictions can be lifted. Besides the CNL restrictions, KALM has limitations in terms of the types of knowledge it can represent. For example, KALM users cannot author rules to support multi-step reasoning, nor can they author actions associated with occurrences of events, which hinders its ability to do time-related reasoning. Apart from the aforementioned shortcomings, the system's speed was insufficient to adequately support the overall knowledge authoring process.\nTo address these issues, we propose an extension of KALM called KALM for Factual Language (KALMFL). KALMFL uses a neural parser for natural language, mSTANZA, to parse what we call factual English sentences, which require little grammar training to use. Building upon KALMFL, we propose KALM for Rules and Actions (KALMRA), to represent and reason with rules and actions. Furthermore, we identify the reasons behind the slow speed of KALM and make optimizations to address this issue. Our evaluation using multiple benchmarks shows that our approaches achieve a high level of correctness on fact and query authoring (95%) and on rule authoring (100%). When used for authoring and reasoning with actions, our approach achieves more than 99.3% correctness, demonstrating its effectiveness in enabling more sophisticated knowledge representation and reasoning. We also illustrate the logical reasoning capabilities of our approach by drawing attention to the problems faced by the famous AI, ChatGPT. Finally, the evaluation of the newly proposed speed optimization points not only to a 68% runtime improvement but also yields better accuracy of the overall system.", "sections": [{"title": "1 Introduction", "content": "Much of human knowledge can be captured in knowledge representation and reasoning (KRR) systems that are based on logical facts and rules. Unfortunately, translating human knowledge into the logical form that can be used by KRR systems requires well-trained domain experts who are scarce.\nOne popular idea is to use natural language (NL) to represent knowledge, but current technology (e.g. OpenSesame [67], SLING [60]) for converting such statements into logic has rather low accuracy. A possible fix to this problem is to author knowledge via sentences in controlled natural languages (CNLs), such as ACE used in Attempto [20]. These CNLS are fairly rich and algorithms exist for converting CNL sentences into logical facts. Unfortunately, CNLs are also very restrictive, hard to extend, and require significant training to use. Furthermore, both CNLs and the more general NLP systems cannot recognize sentences with identical meanings but different syntactic forms. For example, \"Mary buys a car\u201d and \u201cMary makes a purchase of a car\u201d would be translated into totally different logical representations by most systems, which renders logical inference mechanisms unreliable at best. This problem is known as semantic mismatch [21]. The Knowledge Authoring Logic Machine (KALM) [22] was then introduced to solve this problem, but KALM was based on Attempto's ACE and therefore inherited all the aforesaid issues with CNLs. Meanwhile, KALM can only represent a limited range of knowledge, such as facts and queries, leaving other important types of human knowledge, such as rules and actions, unsolved. Moreover, although KALM adopts many optimizations, some steps are still inefficient.\nIn this paper, firstly, we propose KALM for Factual (English) Language (KALMFL) to address the problems associated with CNL. To this end, we transplant the KALM framework to a neural parser for natural language (NL), mSTANZA, which is a modified STANZA [56] version with multiple, ranked outputs. Of course, to turn English into an authoring tool for KR one still needs to impose some restrictions on the language. For instance, \u201cGo fetch more water\u201d is a command that does not convey any factual information that can be recorded in a knowledge base (except, perhaps, those based on rather esoteric logics). Here we mainly focus on a type of English sentence suitable for expressing facts and queries which are identified as one class of English sentences called factual sentences. These sentences can be translated into logic and the aforesaid semantic mismatch problem is solved for such sentences. Unlike CNLs, factual sentences need little training as long as users keep focus on knowledge representation rather than fine letters.\nThen, to tackle the issues associated with knowledge coverage, we empower users to create rules and actions using factual sentences. Our approach incorporates F-logic [40] to enable multi-step frame-based reasoning with authored rules. In addition, we use a formalism called Simplified Event Calculus (SEC) [63] to represent and reason about actions and their effects. We call this extension KALM for Rules and Actions (KALMRA). In terms of implementation, we found a Prolog-like system is more suitable for frame-based parsing, so we implemented KALMRA in XSB [68]. However, the knowledge produced by KALMRA contains disjunctive knowledge and function symbols, so we chose the answer set programming system DLV [44] as the reasoner for generated knowledge.\nFinally, we identify the so-called role-filler disambiguation step, defined in Section 2.4.3, as one that requires major speed-up. As a result, we propose two optimization approaches using representation learning to enhance the overall speed and accuracy of KALM.\nEvaluation on multiple benchmarks including MetaQA [81], the UTI guidelines [65], and bAbI Tasks [74] shows that our approaches achieve 95% accuracy on fact and query authoring, 100% accuracy on rule authoring, and 99.3% on authoring and reasoning about actions. We also assess the powerful AI, ChatGPT2, using bAbI Tasks, and highlight its limitations with respect to logical reasoning compared to our proposed extensions. Finally, we revise and re-evaluate the role-filler disambiguation step, which shows that our new approaches not only reduce the runtime by more than 68%, but also give KALM higher accuracy."}, {"title": "1.1 Motivation", "content": "Much of human knowledge can be captured in knowledge representation and reasoning (KRR) systems that are based on logical facts and rules. Unfortunately, translating human knowledge into the logical form that can be used by KRR systems requires well-trained domain experts who are scarce.\nOne popular idea is to use natural language (NL) to represent knowledge, but current technology (e.g. OpenSesame [67], SLING [60]) for converting such statements into logic has rather low accuracy. A possible fix to this problem is to author knowledge via sentences in controlled natural languages (CNLs), such as ACE used in Attempto [20]. These CNLS are fairly rich and algorithms exist for converting CNL sentences into logical facts. Unfortunately, CNLs are also very restrictive, hard to extend, and require significant training to use. Furthermore, both CNLs and the more general NLP systems cannot recognize sentences with identical meanings but different syntactic forms. For example, \"Mary buys a car\u201d and \u201cMary makes a purchase of a car\u201d would be translated into totally different logical representations by most systems, which renders logical inference mechanisms unreliable at best. This problem is known as semantic mismatch [21]. The Knowledge Authoring Logic Machine (KALM) [22] was then introduced to solve this problem, but KALM was based on Attempto's ACE and therefore inherited all the aforesaid issues with CNLs. Meanwhile, KALM can only represent a limited range of knowledge, such as facts and queries, leaving other important types of human knowledge, such as rules and actions, unsolved. Moreover, although KALM adopts many optimizations, some steps are still inefficient.\nIn this paper, firstly, we propose KALM for Factual (English) Language (KALMFL) to"}, {"title": "1.2 Thesis Contributions and Outline", "content": "The contributions of this thesis are as follows:\n1. A new subset of the English language for knowledge authoring, which we call the factual (English) language. This language has few grammatical restrictions and can be mastered by users without extensive training. This language is designed to be sufficiently expressive for specifying database facts. An extension of this language supports rules and actions, which we call factual English with rules and actions. This contribution is discussed in Sections 3.1.\n2. An enhanced natural language processing toolkit, mSTANZA, that can generate multiple promising dependency parses for grammatical checks. This contribution is discussed in Section 3.2.\n3. Two high-accuracy knowledge authoring machines, namely KALMFL and KALMRA. The former achieves a significant reduction in grammatical constraints by applying factual English, which allows users to author factual knowledge and common queries with lower learning costs. The latter is built upon the former, enabling users to author rules and actions for complex reasoning using factual English with rules and actions. This contribution is discussed in Section 3.3 and Chapter 4.\n4. Optimization of the critical role-filler disambiguation step of the KALM processing pipeline, which significantly improves both the speed and the accuracy of the entire system. Role-filler disambiguation is discussed in Chapter 5.\nThe rest of this thesis is organized as follows. Chapter 2 provides the background required to understand this thesis. My own contribution in this thesis is presented in Chapter 3 through 5. Specifically, Chapter 3 extends KALM to factual English; Chapter 4 handles the authoring of rules and actions; Chapter 5 presents solutions to faster and more accurate role-filler disambiguation. Finally, Chapter 6 concludes the thesis and discusses possible future directions."}, {"title": "2 Background", "content": "This chapter presents the topics related to knowledge representation and reasoning, linguistic resources, the Attempto Project, knowledge authoring, and natural language processing and generation. The connection between each topic and the contributions of this thesis is explained."}, {"title": "2.1 Knowledge Representation and Reasoning", "content": "Knowledge representation and reasoning (KRR) deals with the creation and manipulation of knowledge representations and the development of reasoning algorithms to perform tasks such as classification, prediction, diagnosis, planning, and decision-making. Here, we only focus on four major KRR-related techniques that are related to our research: XSB Prolog, DLV, F-logic, and Event Calculus."}, {"title": "2.1.1 XSB Prolog", "content": "XSB [64, 68] is a powerful Prolog implementation that extends the standard Prolog language with a number of advanced features. One of the most significant of these features is tabling, which is a memoization technique that can greatly improve the efficiency of many Prolog programs. Tabling allows for the efficient memoization of recursive predicates, which can avoid redundant computations and greatly speed up program execution. In addition to tabling, XSB Prolog also provides a number of built-in database management tools. This includes support for dynamic predicates, which can be used to add or remove clauses from a predicate at runtime. XSB Prolog also includes transaction management, which allows groups of database operations to be treated as a single transaction with ACID properties. Finally,\nXSB Prolog includes support for multi-database access, which allows multiple databases to be accessed and manipulated within a single program. The KALM system and its succeeding extension KALMFL (detailed in Section 2.4 and Chapter 3, respectively) use XSB as their frame-based parser and database engine."}, {"title": "2.1.2 Disjunctive Logic Programming", "content": "DLV (DataLog with Disjunction, where the logical disjunction symbol V is used) [44] is a disjunctive version of Datalog that operates under the ASP paradigm. It extends Datalog by adding support for disjunction in facts and rule heads, thus providing greater expressiveness for disjunctive information than KRR systems based on the well-founded semantics (e.g., XSB [68]). Furthermore, DLV's support for function symbols and querying makes it more convenient for working with semantic frames [18] than other ASP systems, such as Potassco [24].\nIn DLV, rules have the following form:\nh1 v ... v hn :- b1, ..., bm.\nh1 to hn represent explicitly negated literals, where n > 0 (i.e. there must be at least one of those). The :- sign is the transcription of an implication arrow and b1, ..., bm represent general literals, where m >= 0 must hold (i.e. the body may be omitted completely). The part before the :- sign is referred to as head and the part after :- as body. Note that disjunction symbols may occur in the head only and negation-as-failure symbols in the body only. Also note that facts can be viewed as special forms of rules, in which the body is empty. The neutral element of conjunction is true, so facts must always be true. Disjunctive facts as a special case are also allowed. For example, when expressing the indefinite knowledge \"an apple is either edible or foul,\" we have the DLV fact as follows:\nedible(apple) v foul(apple).\nIn the implementation of one of the KALM system extensions, KALMRA (presented in Chapter 4), DLV is employed as database and inference engine for storing and reasoning about both definite and indefinite knowledge."}, {"title": "2.1.3 Frame Reasoning", "content": "F-logic [39, 40] is a knowledge representation and ontology language that combines the benefits of conceptual modeling with object-oriented and frame-based languages. One of its key features is the ability to use composite frames to reduce long conjunctions of roles into more compact forms, matching ideally the structure of FrameBase's frames [61].\nIn this thesis, we depart from the actual syntax of F-logic as it is not supported by the DLV system. Instead, we implemented a small subset of that logic by casting it directly into the already supported DLV syntax. Therefore, multi-step reasoning and question answering can be achieved in our frame-based knowledge authoring machines, such as KALMRA, which is presented in Chapter 4. For example, using F-logic, FrameNet frames can be used to answer the question\u201cWhat does Mary buy?\" given the fact \u201cMary buys a car for Bob,\" whose frame-based representations, shown below, are not logically equivalent (the fact has more roles than the query). The variable What is correctly instantiated as car.\nframe(\"Commerce_buy\",[role(\"Buyer\",\"Mary\"),role(\"Goods\",car),\nrole(\"Recipient\",\"Bob\")]).\n?- frame(\"Commerce_buy\",[role(\"Buyer\",\"Mary\"),\nrole(\"Goods\",What)]).\nWhat=car.\nwhere frame (FrameName,RoleList) represents a frame and role(RoleName, RoleFiller) is an instance of a role, including the role's name, and the filler word that plays the role.\""}, {"title": "2.1.4 The Event Calculus", "content": "The event calculus (EC) [41] is a well-established representation methodology and background theory for time-related reasoning. It consists of a set of logical axioms that describe the law of inertia for actions. This law states that time-dependent facts, fluents, that are not explicitly changed by an action preserve their true/false status in the state produced by that action. The original EC had a very complex vocabulary and was lacking some axioms, which made it difficult to use. To address these issues, various variants of EC were introduced, including the simplified event calculus (SEC) [62, 63], the \u201cnew\u201d event calculus (NEC) [63], discrete event calculus (DEC) [50], and the simplified discrete event calculus (SDEC) [38]. Among these variants, SEC stands out as being equivalent to the original EC when time ranges over integer domains, yet it is much simpler and more tractable. Previous works [38, 49, 75, 1] have demonstrated the applicability of SEC to problems involving sequential events or actions in time-related scenarios. Therefore, this thesis focuses on SEC and its integration into knowledge authoring. This integration, discussed in Chapter 4, leverages the benefits of SEC to enhance the understanding of actions, which enables high-accuracy reasoning and question answering in various application domains.\nA fluent in SEC is said to hold at a particular timestamp if it is initiated by an earlier action and not subsequently terminated prior to the timestamp. This is formalized by these DLV rules:\nholdsAt (F, T2) :-\nhappensAt(A,T1),initiates(A,F), timestamp(T2), T1 < T2,\nnot stoppedIn(T1,F,T2).\nstoppedIn(T1,F,T2) :-\nhappensAt(A,T), terminates(A,F),\ntimestamp(T1), T1 < T, timestamp(T2), T < T2.\nHere happensAt/2 represents a momentary occurrence of action A at a timestamp. If an action is exogenous insertion of a fluent f at time t then we represent it as happensAt(f,t), i.e., we use the same predicate. Example 2.1 demonstrates the use of happensAt/2.\nExample 2.1 The actions\nA1. Mary goes to the bedroom.\nA2. The bedroom is north of the garden.\nhave the following frame-based representations:\nhappensAt(frame(\"Travel\",[role(\"Person\",\"Mary\"),\nrole(\"Place\",bedroom)]),1).\nhappensAt(frame(\"North_of\",[role(\"Entity1\",bedroom),\nrole(\"Entity2\",garden)]),2).\nperson(\"Mary\"). place(bedroom).\nentity(bedroom). entity(garden). timestamp(1..2).\nThe first happensAt/2 introduces an action of traveling from place to place while the second happensAt/2 uses an observed (i.e., exogenously inserted) fluent \"North_of\"(bedroom,garden). Observable fluents are supposed to be disjoint from action fluents, and we will use a special predicate to recognize them in SEC rules. Timestamps indicate the temporal relation between the action and the observed fluent. Predicates person/1, place/2, entity/2, define the domains of various roles, while timestamp/1 defines the domain of timestamps.\nThe predicates initiates(Action,Fluent) and terminates(Action, Fluent) in SEC specify domain-specific axioms that capture the initiation and termination of fluents. In the scenario of Example 2.1, the predicates initiates/2 and terminates/2 are as follows.\nExample 2.2 The action \u201ca person goes to a place\u201d initiates the fluent \u201cthe person is located in the place", "an entity is north of another": "nitiates the fluent \u201cthe other entity is south of the entity", "a person goes to a place\" terminates the fluent \\\"the person is located in another place\\\", which are represented as follows.\ninitiates(frame(\\\"Travel\\\",[role(\\\"Person\\\",Person),\nrole(\\\"Place\\\",Place)]),\nframe(\\\"Located\\\",[role(\\\"Entity\\\",Person),\nrole(\\\"Place\\\",Place)]):-\nperson (Person), place(Place).\ninitiates(frame(\\\"North_of\\\",[role(\\\"Entity1\\\",Entity),\nrole(\\\"Entity2\\\",Entity2)]),\nframe(\\\"North_of\\\",[role(\\\"Entity1\\\",Entity),\nrole(\\\"Entity2\\\",Entity2)]) :-\nentity(Entity), entity(Entity2).\ninitiates(frame(\\\"North_of\\\",[role(\\\"Entity1\\\",Entity),\nrole(\\\"Entity2\\\",Entity2)]),\nframe(\\\"South_of\\\",[role(\\\"Entity1\\\",Entity2),\nrole(\\\"Entity2\\\",Entity)]) :-\nentity(Entity), entity(Entity2).\nterminates(frame(\\\"Travel\\\",[role(\\\"Person\\\",Person),\nrole(\\\"Place\\\",Place)]),\nframe(\\\"Located\\\",[role(\\\"Entity\\\",Person),\nrole(\\\"Place\\\", Place2)]) :-\nperson(Person), place(Place), place(Place2).\nCombining the SEC axioms with the actions in Example 2.1,and domain-specific axioms in Example 2.2, we can derive the following fluents (assuming the highest timestamp is 3):\nF1. Mary is located in the bedroom at timestamp 2.\nF2. Mary is located in the bedroom at timestamp 3.\nF3. The bedroom is north of the garden at timestamp 3.\nF4. The garden is south of the bedroom at timestamp 3.\nwhich can be further represented as the following DLV facts:\nholdsAt(frame(\\\"Located\\\",[role(\\\"Person\\\",\\\"Mary\\\"),\nrole(\\\"Place\",bedroom)]),2).\nholdsAt(frame(\\\"Located\\\",[role(\\\"Person\\\",\\\"Mary\\\"),\nrole(\\\"Place\\\",bedroom)]),3).\nholdsAt(frame(\\\"North_of\\\",[role(\\\"Entity1\",bedroom),\nrole(\\\"Entity2\\\",garden)]),3).\nholdsAt(frame(\\\"South_of\\\",[role(\\\"Entity1\\\",garden),\nrole(\\\"Entity2\\\",bedroom)]),3).\"\n    },\n    {\n      \"title\": \"2.2 Linguistic Resources\",\n      \"content\": \"This section explores two vital linguistic resources, FrameNet and BabelNet, which are extensively employed in the field of KRR to comprehend languages. Through examples, we illustrate how FrameNet and BabelNet function.\"\n    },\n    {\n      \"title\": \"2.2.1 FrameNet\",\n      \"content\": \"FrameNet [3, 4] is a semantic framework that aims to capture the meaning of a sentence by identifying the semantic frame(s) that it belongs to and the semantic roles that the words in the sentence play within that frame.\nA semantic frame is a conceptual structure that represents a particular scenario or situation, while the semantic roles are the different roles played by entities and events in that scenario. For example, the semantic frame Commerce_buy in FrameNet describes a situation in which an individual purchases a good. This frame includes various semantic roles, such as the Buyer, the Goods, the Money, the Seller, the Recipient, the Place, and the Time. Each semantic frame is associated with a list of lexical units (LUs) that can trigger the application of that frame. To account for the different ways in which LUs can be used to express a sentence, FrameNet defines a set of valence patterns for each LU-frame pair. A valence pattern represents the syntactic context in which a particular LUs and some of the frame roles are used in a sentence. For example, a valence pattern for the pair buy.verb and Commerce_buy might specify that the Buyer role is the subject of the buy event and the Goods role is the object of that event.\nBy applying the appropriate LUs and valence pattern to a sentence, FrameNet can extract the relevant semantic frame and assign the appropriate semantic roles to the words in the sentence. For example,\nExample 2.3 FrameNet uses the Commerce_buy frame to represent the sentence \u201cMary buys a car for Bob for 30,00 dollars.": "s follows:\nMary buys a car for Bob for 30,000 dollars .\nBuyer LU Goods Recipient Money\nwhere the verb \u201cbuys\u201d as the LU triggers the Commerce_buy frame; \u201cMary\u201d plays the role of Buyer; \"car\" plays the role of Goods; \u201cbob\u201d plays the role of Recipient; \u201c30,000 dollars"}, {"title": "2.2.2 BabelNet", "content": "BabelNet [52, 51] is a multilingual lexical network that integrates information from several different lexical resources, including WordNet [48], Wikipedia, OmegaWiki, etc. It provides a comprehensive representation of words and concepts in multiple languages, making it an essential resource for natural language processing tasks that involve cross-lingual information retrieval, word-sense disambiguation, and knowledge-based NLP.\nIn BabelNet, each word has one or more part-of-speech tags and a glossary for its meanings. Words with similar meanings are grouped into synsets that have unique identifiers (of the form bn:dddddddp, where d is a digit and p is a part of speech symbol like v, n, etc.). For Example 2.3, the word \u201ccar\u201d in BabelNet has multiple meanings, which, shown below, belong to different synsets:\n\u2022 bn:00007309n: A motor vehicle with four wheels; usually propelled by an internal combustion engine;\n\u2022 bn:00015785n: A wheeled vehicle adapted to the rails of railroad;\n\u2022 bn:00015786n: The compartment that is suspended from an airship and that carries personnel and the cargo and the power plant;\n\u2022 and more...\nThe BabelNet synset nodes are connected by directed edges representing their semantic relations (i.e., hypernym, hyponym, etc.). Compared to WordNet, BabelNet has a much larger vocabulary and richer semantic relations. Specifically, there is an abundance of named entities, like famous people, locations, songs, books, etc. Besides, each edge in the knowledge graph is assigned a weight (in versions earlier than 3.7) that represents the degree of relevance between two connected synset nodes with respect to the type of edge. Edge weights are a very important feature in BabelNet and are critical to disambiguating role-fillers and eliminating inappropriate candidate parses in the Knowledge Authoring Logic Machine (KALM) [22, 21, 23], which will be introduced in Section 2.4. Furthermore, in the discussions in Chapter 5, we will analyze the advantages and disadvantages of BabelNet-based disambiguation and thus optimize the disambiguation process."}, {"title": "2.3 The Attempto Project", "content": "The Attempto Project, a research initiative of the University of Zurich, seeks to develop Attempto Controlled English and its accompanying tools. With the goal of proposing a more effective method for converting human language into structured knowledge, the project has made significant contributions to the field. These include the development of Attempto Controlled English, the Attempto Parsing Engine, and a Discourse Representation Structure specifically designed for Attempto."}, {"title": "2.3.1 Attempto Controlled English", "content": "Attempto Controlled English (ACE) was designed to serve as a knowledge representation language and its usage requires a competent understanding of its construction and interpretation rules. The vocabulary of Attempto Controlled English consists of built-in function words, (e.g. determiners, articles, pronouns, and quantifiers), built-in phrases (e.g. \u201cit is false that ...,\u201d \u201cit is not provable that ...,\u201d) and 100,000 content words, including adverbs, adjectives, nouns, verbs, and prepositions. Content words are stored in an external lexicon and written in Prolog predicates. The names of the predicates describe the parts-of-speech of the words. Each predicate has two arguments, between which the first one is the word that the predicate stands for, and the second specifies the base form of the first argument. For example, the words \u201cfast\u201d, \u201cfaster\u201d, and \u201cfastest\u201d are represented as adv (fast,fast), adv_comp(faster,fast), and adv_sup(fastest,fast), respectively, where adv means \u201cfast\u201d is an adverb, adv_comp means faster is a comparative adverb, and adv_sup means fastest is a superlative adverb. All three words use the base form, fast. The construction rules of ACE define the format of words, phrases, sentences, and texts:\n\u2022 Words: Built-in function words and built-in phrases are not allowed to be modified by users. Users can create compound words by concatenating two or more content words with hyphens.\n\u2022 Phrases: Phrases include noun phrases, modifying nouns, modifying noun phrases, verb phrases, and modifying verb phrases. Noun phrases in ACE form a subset of noun phrases of regular English plus this includes arithmetic expressions, sets, and lists. Modifying nouns and noun phrases are those that are preceded or followed by adjectives, relative clauses, or possessives. Verb phrases in ACE form a subset of verb phrases in English with specific definitions for negation and modalities. Modifying verb phrases are those that are accompanied by adverbs or preposition phrases.\n\u2022 Sentences: Sentences include declarative, interrogative, and imperative sentences. Declarative sentences include simple sentences, there is/are-sentences, Boolean formulae, and composite sentences. Interrogative sentences include yes/no queries, wh-queries, and how much/many-queries that end with a question mark. Imperative sentences are commands that end with an exclamation mark.\n\u2022 Texts: Texts are sequences of declarative, interrogative, and imperative sentences.\nThe interpretation rules of ACE define how a grammatically correct ACE sentence is translated. Here are three examples of such interpretation rules:\n\u2022 Prepositional phrases modify the verb not the noun: \u201cMary{enters a card with a code}.", "noun": "Mary enters {a card that has a code}.", "number": "Mary was born in Stony-Brook. It's a beautiful place.\u201d\nThe first sentence can be interpreted in two ways in English, \u201cA customer uses a code to enter a card.\u201d and \u201cMary enters a card. The card has a code.\u201d In ACE, it is interpreted only in the first way. In the second sentence, given a relative clause, an ACE sentence refers only to the nearest noun that precedes it. Hence, the relative clause, \u201cthat has a code\", modifies the noun, \u201ccard\u201d. In the third sentence, ACE resolves the pronoun, \u201cit\u201d, in the second sentence by looking for the nearest antecedent noun that agrees with gender and number, so ACE associates \u201cit\u201d with the word \u201cStony-Brook\u201d .\""}, {"title": "2.3.2 Discourse Representation Structure for Attempto Controlled English", "content": "To parse ACE texts, Attempto Parsing Engine (APE) was later implemented. APE accepts ACE texts as input and produces both paraphrases and Discourse Representation Structures (DRS) in the form of first-order logic terms. These paraphrases provide the user with a better understanding of how APE interprets ACE sentences. The concept of DRS was originally introduced in [37] and was later redesigned for ACE in [19]. The redesigned DRS, referred to simply as DRS, consists of two parts: a set of discourse referents (a.k.a. identifiers), called the universe of the DRS, and a set of DRS conditions.\nA referent in DRS can represent an object introduced by a noun or a predicate introduced by a verb, among others. A condition in DRS can be either simple or complex. A simple condition consists of a logical atom followed by an index, while a complex condition is constructed from other DRS connected by logical operators such as negation, disjunction, and implication. For each simple condition, the logical atom is limited to one of the 8 predicates, with definitions provided as follows:\n\u2022 object(Ref,Noun,Class,Unit,Op,Count). The object terms represent objects that are introduced by the various forms of nouns. Ref stands for such an object and is used for references. Noun denotes the object that was used to introduce the object functor. Class is one of {dom,mass,countable} and shows the classification of the noun. Unit denotes the measurement noun if the object is introduced together with a measurement noun such as \u201ckg\u201d in \u201c2 kg of apples\u201d. Op is one of {eq, geq, greater,leq,less,exactly, na}, representing quantitative relationships such as \u201cequal\u201d, \u201cgreater or equal", "less or equal\". Together with Unit and Op, Count defines the cardinality or extent.\n\u2022 predicate(Ref,Verb,SubjRef), predicate(Ref,Verb,SubjRef,Obj Ref), and predicate(Ref,Verb,SubjRef,ObjRef,IndObjRef) represent relations that are introduced by intransitive, transitive, and ditransitive verbs, respectively. Ref stands for such a relation and is used to attach modifiers (i.e. adverbs and prepositional phrases). Verb stands for intransitive, transitive, or ditransitive. SubjRef is a reference to the subject, ObjRef points to the direct object, and IndObjRef points to the indirect object in a sentence.\n\u2022 property(Ref1,Adjective,Degree),property(Ref1,Adjective,De gree,Ref2),and property (Ref1,Adjective,Ref2,Degree,CompTarg et,Ref3) stand for properties that are introduced by adjectives. Ref1 refers to the primary object of the property (i.e. the subject) and Ref2 refers to the secondary object of the property. Ref3 refers to the tertiary object of the property. The Adjective argument is intransitive or transitive, representing the type of the adjective. Degree is one of {pos, pos_as,comp,comp_than,sup}; it defines the degree of the adjective. Positive and comparative forms can have an additional comparison target (e.g. \u201cas rich as ...\u201d, \u201cricher than ...": "and for those cases pos_as and comp_than are used. CompTarget is one of {subj,obj} and defines for transitive adjectives whether the comparison targets the subject (e.g. \u201cMary is more fond-of John than Bob", "Mary is more fond-of John than of Bob\").\n\u2022 modifier_pp(Ref1,Preposition,Ref2). Ref1 refers to the modified verb. Preposition is the preposition of the prepositional phrase. Ref2 refers to the object of the prepositional phrase.\"\n    },\n    {\n      \"title\": \"2.3.3 Semantic Mismatch\",\n      \"content\": \"ACE was developed as a potential way to acquire knowledge, with the expectation that its unambiguous nature could improve the precision of knowledge representation compared to full English. However, ACE's reliance on the shallow semantics of natural languages makes it challenging to identify sentences with the same meaning but different syntactic structures. These structures are eventually parsed into different DRS by APE. The issue is known as semantic mismatch [22, 72]. Here is an example to illustrate this problem.\nExample 2.5 Three synonymous ACE sentences \u201cMary buys a car. Mary is the purchaser of a car. Mary makes a purchase of a car.": "ave the following different DRS:\nobject(A, \"Mary\",countable,na,eq,1)-1/1\nobject(B,car,countable,na,eq,1)-1/4\npredicate (C,buy,A,B)-1/2\nobject(A, \"Mary\",countable,na,eq,1)-2/1\nobject(B,purchaser,countable,na,eq,1)-2/4\nobject(C,car,countable,na,eq,1)-2/7\nrelation(B,of,C)-2/5\npredicate(D,be,A,B)-2/2\nobject(A, \"Mary\",countable,na,eq,1)-3/1\nobject(B,purchase,countable,na,eq,1)-3/4\nobject(C,car,countable,na,eq,1)-3/7\nrelation(B,of,C)-3/5\npredicate(D,make,A,B)-3/2\nwhere all the sentences are translated into different DRS, which does not reveal that all three sentences convey the same meaning. This occurs because ACE lacks any inherent background knowledge and relies on explicit rules to encode such knowledge. Unfortunately, constructing these rules is a labor-intensive task that requires the expertise of skilled knowledge engineers. To address the issue of semantic mismatch, the Knowledge Authoring Logic Machine (KALM) [22] was introduced, which will be elaborated in Section 2.4."}, {"title": "2.4 KALM: Knowledge Authoring Logic Machine", "content": "This section provides"}]}