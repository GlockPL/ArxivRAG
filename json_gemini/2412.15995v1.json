{"title": "Data-Centric Improvements for Enhancing Multi-Modal Understanding in Spoken Conversation Modeling", "authors": ["Maximillian Chen", "Ruoxi Sun", "Sercan \u00d6. Ar\u0131k"], "abstract": "Conversational assistants are increasingly popular across diverse real-world applications, highlighting the need for advanced multimodal speech modeling. Speech, as a natural mode of communication, encodes rich user-specific characteristics such as speaking rate and pitch, making it critical for effective interaction. Our work introduces a data-centric customization approach for efficiently enhancing multimodal understanding in conversational speech modeling. Central to our contributions is a novel multi-task learning paradigm that involves designing auxiliary tasks to utilize a small amount of speech data. Our approach achieves state-of-the-art performance on the Spoken-SQUAD benchmark, using only 10% of the training data with open-weight models, establishing a robust and efficient framework for audio-centric conversational modeling. We also introduce ASK-QA, the first dataset for multi-turn spoken dialogue with ambiguous user requests and dynamic evaluation inputs.", "sections": [{"title": "Introduction", "content": "Real-world adoption of intelligent multimodal conversational agents has progressed quickly in recent years due to the impressive capabilities of Large Language Models (LLMs). However, despite numerous applications, including smart home systems, contact centers, customer support/service, personalized education etc. (Hemphill et al., 1990; Khatri et al., 2018b; Li et al., 2017; Von Ahn, 2013; Fatima et al., 2024; Li et al., 2023; Zheng et al., 2024), there has not been the same rapid progress in adapting Multimodal LLMs (MLLMs) to spoken contexts due to several fundamental challenges.\nSpeech data constitute high-dimensional signals that are difficult to model even for frontier models (e.g., Whisper-based models are limited to 30 seconds; Chu et al. (2024); Radford et al. (2023)) and Gemini 1.5 represents 1 second of audio using 25 tokens\u00b9). These are temporal signals which include acoustic phenomena (e.g. background noise Mehrish et al. (2023)) and important paralinguistic information such as speaking rates or pitch (Hirschberg, 1993; Bhattacharya et al., 2023). Performance on speech understanding tasks is thus greatly affected by the ability to robustly comprehend the semantic contents of the input speech (Li et al., 2017), as illustrated in Fig. 1. This is further complicated by the long-standing issue of models overfitting to individual speakers (Jung et al., 2018; Wang et al., 2020). These can be viewed as a shortcoming of insufficient training data coverage (Yang et al., 2024b). However, large-scale speech data collection is notoriously difficult due to privacy concerns (Nautsch et al., 2019; Qian et al., 2018).\nDespite the difficulty of large-scale collection, task-specific data is increasingly the most effective approach to guarantee use-case customization for state-of-the-art MLLMs like Gemini or GPT (Gemini Team et al., 2023; Brown et al., 2020). These models are closed-source, but offer commercial tuning APIs, which typically do not permit modifications to the model or learning objective. Even with smaller open-weight models, it can still be computationally intractable to iterate on architectures and train from scratch due to the expensive compute resource demands (Groeneveld et al., 2024). These motivate the design of efficient data-centric methods (Seedat et al., 2022) which maximize models' ability to overcome the aforementioned challenges of long speech understanding reliably.\nIn this work, we take a data-centric perspective towards addressing the varied challenges of adapting multimodal LLMs for speech. Our contributions can be summarized as follows:\n\u2022 We bring a multi-task learning paradigm to improve speech understanding implemented via a simple but effective data-centric approach. Rather than using additional datasets, we instead design auxiliary tasks to maximize cross-modal learning from a fixed set of recorded speech.\n\u2022 We propose Ambiguous Spoken Conversational Question Answering (ASK-QA), a novel dataset which combines the challenges of multimodal speech modeling and mixed-initiative interaction. ASK-QA features contextually ambiguous questions along with long multi-turn speech and diverse accents, speaking rates, and pitches.\n\u2022 We validate the proposed data-centric approach on three spoken question answering (SQA) corpora: ASK-QA, Spoken-SQUAD, and SD-QA, representing various combinations for whether input questions and knowledge context are represented as text or speech. Our approach applied even to open-weight models is able to outperform the existing state-of-the-art on Spoken-SQUAD using only 10% of the available training data."}, {"title": "Data-Centric Multi-Task Learning for Cross-Modal Understanding", "content": "We consider the setting of customizing an MLLM for use in request-based end-to-end speech modeling, similarly to Shih et al. (2024). An MLLM is provided as input an audio recording and textual context. The backbone of many MLLM architectures is a textual decoder-only LLM (Liu et al., 2024), so the textual context usually contains an instruction. These settings involve reasoning about some contextual knowledge and conversation history. The model aims to provide a correct answer to a target question (i.e. the last conversation turn). Different applications may involve spoken conversations about written information (e.g. document-grounded QA), or written conversations about spoken information (e.g. meeting summarization).\nTuning MLLMs with cross-entropy loss is advantageous as it can be used to unify diverse tasks as a single text-to-text objective (Raffel et al., 2020). Many recent studies find that multi-task learning (Caruana, 1997) using additional datasets greatly improves downstream task performance (Aghajanyan et al., 2021; Padmakumar et al., 2022; Chen and Yu, 2023). Here, we instead design auxiliary tasks within the same dataset to maximize cross-modal learning gains from a fixed set of audio recordings for a target dataset. We break down our problem into three intermediate goals: 1) correctly representing the spoken context, 2) learning to reason across all input modalities, and 3) coherently producing the correct answer.\n1) Listening Comprehension is an auxiliary task to help the SLM \u201chear\u201d the spoken context. It has been consistently reported in traditional cascade-style systems that SQA performance is greatly affected by automatic speech recognition (ASR) errors (Li et al., 2018), and thus we design a task to specifically address this point. The objective is for the MLLM to predict a ground-truth (or pseudo-labeled) audio transcription, given a recording and a task instruction as input.\n2) Cross-Modal Commonsense Reasoning is an auxiliary task designed to unify the contents of the spoken and textual inputs. We reframe dialogue response selection (Henderson et al., 2019) as a multiple-choice reasoning task (Talmor et al., 2019). The answer options consist of the correct answer (e.g. \"It was recovered a few months later\") and commonsense negative answer choices sampled from other training QA pairs (e.g. \u201cDo you mean the popular generic name?\u201d), as shown in Table A8. The objective is to solve this multiple-choice reasoning task by selecting the correct answer given the recording, conversation context, knowledge, answer options, and a task instruction.\n3) Response Generation is the primary objective of providing a correct answer. The inputs are what"}, {"title": "Experiments", "content": "We evaluate our approach on SQA datasets with different combinations of context modalities (see Table 1). We prompt and fine-tune two closed-source models, Gemini Pro and Gemini Flash, on the Vertex AI platform. We also use Speech-Qwen, which we built by pre-training an 17.8M parameter projection layer between a frozen audio encoder (WavLM-Large) and a frozen LLM decoder (Qwen 2.5 7B-Instruct). See Appendix C for details on Speech-Qwen. Our main findings for ASK-QA and Spoken-SQUAD are reported in this section. Full results and extended details for each experimental setting are reported in Appendix E."}, {"title": "ASK-QA: Spoken Knowledge and Multi-Turn Spoken Dialogue", "content": "We develop a novel corpus for speech-based mixed-initiative conversation: Ambiguous Spoken Conversational Question Answering (ASK-QA). The contextual inputs for ASK-QA are fully spoken.\nDataset Construction: We construct ASK-QA starting from Abg-CoQA (Guo et al., 2021), a span-based textual conversational QA task. Given a story as context, each conversation consists of dialogue turns where a user asks questions and an assistant is supposed to provide the correct answer or ask a clarifying question if the user's request is ambiguous. Our data construction pipeline is summarized in Figure 2. The data generation process is detailed in Appendix B. In total, ASK-QA contains 221.8h of speech. The training, validation, and test sets contain 5,985, 500, and 1,345 conversations.\nEvaluation: Following recent work (Chen et al., 2024; Risch et al., 2021), we apply embedding-based semantic similarity (Reimers, 2019) to allow for flexible free-form QA evaluation. We apply this metric to a standard single-turn setting as well as a novel multi-turn setting which combines TTS with the dynamic input evaluation for Abg-CoQA in Chen et al. (2024). See details in Appendix B.2.\nFindings: We benchmark end-to-end performance on ASK-QA in Table A6 using the multi-task approach described in Section 2 and baseline single-task tuning (which represents standard end-to-end speech-to-text modeling (Shih et al., 2024)). The listening comprehension sub-task separately models the story and conversation transcripts, inspired by speaker diarization (Anguera et al., 2012; Gu et al., 2021; Yu et al., 2022). With Speech-Qwen, we see as much as 13.3% relative improvement over standard fine-tuning depending on the amount of available data on trajectory-level similarity in Figure 3. Surprisingly, with Gemini Pro, we also see relative improvements of 5.7% with 1% of the available training data and 1.6% when using full data, despite frontier MLLMs already having large-scale multi-modal pre-training and"}, {"title": "Spoken-SQUAD: Spoken Knowledge and Textual Questions", "content": "Spoken-SQUAD (Li et al., 2018) is a speech version of SQUAD (Rajpurkar et al., 2016). Rather than span-based classification, we solve the task using our end-to-end generative approach. Each instance has a textual question and spoken knowledge.\nFindings: In Figure 4, we benchmark our multi-task learning approach against single-task tuning via Speech-Qwen. Our performance is evaluated in terms of exact match and F1 score using the SQUAD evaluator. Our approach, applied to an open-weight model like Speech-Qwen, outperforms the existing state-of-the-art model proposed in You et al. (2022) using just 10% of the available training data, indicating that it is highly efficient and effective for cross-modal learning. Our expanded results which include an additional MLLM are shown in Table A7. We see similar trends (up to 52.8% improvement given limited data) on the"}, {"title": "Ablation Studies", "content": "In Table 2, we systematically remove each individual task: Dialogue Listening Comprehension (DLC), Story Listening Comprehension (SLC), and Response Selection (RS). The removal of each auxiliary task results in performance degradation relative to full multi-task tuning, indicating their importance towards improved cross-modal understanding. We observe that removing SLC results in the most performance degradation, which follows the intuition in Figure 1 and Li et al. (2018)."}, {"title": "Conclusion", "content": "We propose a data-centric multi-task learning approach which helps improve speech data utilization for MLLM tuning. Tuning on various corpora with Gemini and open-weight MLLMs, we observe consistent performance improvements regardless of model scale and tuning access, surpassing state-of-the-art performance on Spoken-SQuAD with open-weight MLLMs. Future work may build upon our insights by designing new auxiliary tasks, incorporating more expressive TTS approaches (e.g., emotion modeling), or examining action optimization strategies for ASK-QA. Our dataset and synthesis process can also be contributed to post-training data mixtures to improve construction of MLLMs' for improved long-context speech modeling abilities."}, {"title": "Additional Related Work", "content": "Spoken question answering is a fundamental skill for intelligent spoken conversational agents (Khatri et al., 2018a; Zheng et al., 2024). Many tasks have been proposed in order to measure models' ability to understand spoken context (Li et al., 2018; Shih et al., 2024) and spoken requests (Faisal et al., 2021). Previously, most approaches to SQA focused on span prediction using \"cascade\" approaches which include an intermediate step invoking an Automatic Speech Recognition (ASR) module followed by a fine-tuned a text classification model Chuang et al. (2020); Li et al. (2018); Su and Fung (2020) like BERT (Devlin, 2018). It is increasingly desirable to develop end-to-end pipelines to solve SQA tasks (Shih et al., 2024), particularly with the rise of generalist MLLMS (Wu et al., 2024). Such end-to-end models are desirable in speech as they afford opportunities to directly encode useful information in acoustic signals such as speaking rate, pitch, or emotions. In our work, we focus on improving methods for end-to-end SQA using both closed-weight and open-weight MLLMs.\nMixed-initiative conversations require each interlocutor to control the interaction flow (Horvitz, 1999) through the use of various pragmatic actions (Chen et al., 2023) such as clarifying questions, which can lead to to better goal completion outcomes (Guo et al., 2021; Min et al., 2020; Wu et al., 2023b). Many works focus on planning these explicit pragmatic actions (Deng et al., 2024; Yu et al., 2023), whereas other works focus on implicit (Chen et al., 2024) and continuous space actions (Wu et al., 2023a), and end-to-end generation capabilities in such settings (Li et al., 2020; Deng et al., 2022). While there have been recent efforts in designing multi-turn SQA corpora (You et al., 2022), to our knowledge, there is not yet any mixed-initiative conversation environment for speech, despite there being many additional acoustic features which may introduce ambiguity (Kurata et al., 2011; Mulholland et al., 2016). In our work, we develop the first-ever conversational SQA corpus which requires the ability to disambiguate requests and reason about clarification questions.\nAdapting models with limited speech data has received much attention due to well-known problem of speaker overfitting across a variety of tasks ranging from grammatical error correction (Wang et al., 2020) to speaker verification (Jung et al., 2018). This problem is frequently addressed with the assistance of multi-task learning (Caruana, 1997). Pironkov et al. (2016) proposed a multi-task objective in which they simultaneously train a network for both ASR (their downstream task) and speaker classification. Chen and Yu (2023) found large downstream task performance improvements on speech classification tasks following a stage of multi-task pre-finetuning. In our work, we view multi-task learning through a data-centric lens. While multi-task pre-finetuning relies on additional datasets (Aghajanyan et al., 2021; Padmakumar et al., 2022), our approach improves the utilization of a fixed set of speech recordings by introducing auxiliary tasks designed to improve the cross-modal understanding capabilities of MLLMs."}, {"title": "Additional Details on ASK-QA", "content": "Here, we provide several additional details on ASK-QA. First we describe the construction of the dataset in detail. Then, we demonstrate the quality of our corpus quantitatively as well as qualitatively. Specifically, we find that the audio quality is very high with strong faithfulness to the original transcript.\nDataset Construction\nOverview As mentioned in Section 3, ASK-QA is constructed using Abg-CoQA as a starting point (Guo et al., 2021). Abg-CoQA is a textual conversational QA task, but as it is span-based, it does not provide very natural dialogue. Each instance consists of a passage which serves as some necessary contextual knowledge, and each conversation consists of dialogue turns where a user asks questions and an assistant is supposed to provide the correct answer or ask a clarifying question if the user's question is contextually ambiguous. The first step we take is to paraphrase each question using Gemini 1.5 Pro to convert the task into free-form QA generation.\nSetting speaker roles Each written conversation can be considered a machine reading comprehension task. We break them down into three components: a story, the set of user questions, and the set of corresponding assistant responses. Our goal is to convert this into a listening comprehension task with two speakers having a conversation about some spoken context. Thus, for each conversation,"}, {"title": "Efficient Multimodal Adapters via Audio Representation Projection", "content": "As demonstrated in Section 3, our data-centric approach is easily applicable to both settings with access to tuning APIs for closed-source MLLMs like Gemini, and settings with access to open-weight models for each modality. Here, we describe our approach in the open-weight scenario.\nTextual instructions serve as a highly controllable interface, and as such, recent work has found much success in unifying multiple modalities with large pre-trained decoder-only language models (Liu et al., 2024; Arora et al., 2024; Kong et al., 2024). These works aim to leverage the impressive instruction-following capabilities of LLMs to interpret additional modalities (e.g. vision, speech, video etc.) by effectively mapping their representations to LLM input space.\nArchitecture: In our work, we consider the high-level architecture presented in Ma et al. (2024). We projecting the speech input represented by an audio encoder into the embedding space of an LLM to improve performance on ASR tasks, only tuning the weights of a linear projection layer and freezing the other model components. As described in Section 3, our speech encoder is WavLM-Large (Chen et al., 2022b). We primarily experimented with tuning Qwen 2.5-Instruct (Yang et al., 2024a) with 7B parameters as our base decoder-only LLM. We also experimented with Phi 3.5 Mini (Abdin et al., 2024) with 3B parameters in Table A7. These MLLMs are referred to as Speech-Qwen and Speech-Phi, respectively. We tune this adapter using standard cross-entropy loss. Details on our tuning experiments are provided in Appendix C.\nProjection Layer Pre-training: While this projection layer is tuned directly on the target ASR task in Ma et al. (2024), we find that this approach may struggle with direct single-task fine-tuning on our more difficult SQA tasks which do not have the"}, {"title": "Additional Details on Data-Centric Multi-Task Learning", "content": "Figure A6 provides a high-level overview of our multi-task learning approach. On the left, we show examples of each of our SQA corpora used for experimentation. At a high level, each corpus consists of passage and a conversation. In ASK-QA, the contextual inputs are fully spoken. In Spoken-SQUAD, the knowledge is spoken while the question is written. In SD-QA, the knowledge is written while the question is spoken (in multiple languages and regional dialects).\nRegardless of the input modalities, each instance can be mapped to new data instances representing the auxiliary tasks in Section 2. The visible examples on the right side of Figure A6 are our multi-task instances for Spoken-SQUAD. The top-right panel is our Listening Comprehension task, our middle-right panel is our Cross-Modal Commonsense Reasoning task, and our bottom right task is the standard QA task (which is just reorganized from the middle-left panel)."}, {"title": "Multi-Task Training Examples for ASK-QA", "content": "We provide concrete examples of the auxiliary tasks for a single instance of ASK-QA in Tables A8, A9, A10, A11. Each of these tables has the exact same speech recording. Table A8 demonstrates how the ground-truth answer is joined to negatively sampled answers from other QA pairings to form the response selection task. Table A11 is similar and demonstrates the textual instruction used to steer the MLLM to directly generate the ground-truth answer. As stated in Section 3.1, we break the Listening Comprehension task into two components since each recording comprises a narrated story and a conversation. Table A9 demonstrates steering the MLLM to transcribe the conversation. Table A10 demonstrates steering the MLLM to transcribe the narrated story."}, {"title": "Extended Experimental Results", "content": "Due to space constraints in the main text, we describe our additional experiments in this section. In particular, we examine a data-centric multi-task"}, {"title": "SD-QA: Textual Knowledge and Spoken Questions", "content": "We examine the setting where the single-turn QA context is provided in the recorded speech, and the knowledge necessary to answer the question correctly is provided in the text."}, {"title": "Dataset", "content": "SD-QA (Faisal et al., 2021) is a large single-turn SQA benchmark with diverse data \u2013 spanning 5 languages (Arabic, Bengali, English, Kiswahili, and Korean) and 24 regional dialects. SD-QA is also proposed as a span-based QA task, but we apply our end-to-end generative approach as in Section 3. We tune our models on up to 9,008 of the 10,0008 samples made available for training, withholding the remaining samples for validation. We evaluate our approach on the 12,975 evaluation samples.\nFindings: We evaluate performance on SD-QA in terms of exact match and token-based F1. Consistent with our findings in Section 3, we see that our multi-task approach is able to outperform single-task tuning in all evaluation settings. This is inclusive of experiments with Gemini Pro as the base MLLM for tuning. We see a large 16.13% rel-"}, {"title": "Assets Used", "content": "All resources used have been cited appropriately in the paper. In this section, we enumerate each of the existing artifacts used in our work along with their license.\nExisting Models\n\u2022 Gemini 1.5 Pro (gemini-1.5-pro-002), Gemini 1.5 Flash (gemini-1.5-flash-002) (Gemini Team et al., 2023): Accessed through the Google Cloud Vertex AI Platform. https://cloud.google.com/products/gemini?hl=en\n\u2022 MiniLM-L6-v2 (Reimers, 2019):\nhttps://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\u2022 Qwen2.5-7B-Instruct (Yang et al., 2024a): MIT Open-Source License. https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\n\u2022 WavLM (Chen et al., 2022b): MIT Open-Source License. https://github.com/microsoft/unilm/blob/master/wavlm/README.md\n\u2022 Phi-3-mini-128k-instruct (Abdin et al., 2024): MIT Open-Source License. https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\nExisting Datasets\n\u2022 Abg-CoQA (Guo et al., 2021): MIT Open-Source License. https://github.com/MeiqiGuo/AKBC2021-Abg-CoQA\n\u2022 Spoken-SQuAD (Li et al., 2018):\nhttps://github.com/Chia-Hsuan-Lee/Spoken-SQUAD\n\u2022 SQUAD (Rajpurkar et al., 2016): CC-BY-SA 4.0 License. https://rajpurkar.github.io/SQuAD-explorer/\nExisting and Software\n\u2022 Google Cloud Pipeline Components: Apache 2.0. https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction"}, {"title": "Acknowledgements", "content": "We thank Jinsung Yoon and Ta-Chung Chi for their helpful feedback on our work."}, {"title": "Limitations", "content": "Transcription supervision: One of the crucial auxiliary tasks in our approach is listening comprehension, as demonstrated by the performance degradation in our ablations (Table 2). In our implementation, we use ground-truth transcriptions as the target for this generation task. These transcriptions may not be available for instance, the ones provided by Spoken-SQuAD and SD-QA were obtained via ASR. Our transcription for ASK-QA is not guaranteed to perfectly match the generated speech either, despite our efforts to filter the data quality (see Appendix B). It is not clear whether the possibility of slight transcription errors improves model robustness to noise or degrades performance, and this warrants further study in future work.\nTTS quality: Our data generation approach is bottlenecked by current capabilities of TTS software. While TTS has greatly improved in recent years in terms of WER, we do still witness generation errors and naturalness issues when working with long context (hence the need for filtering). We are also not at the point in which we have perfect controllability in paralinguistic attributes.\nGeneralization to paralinguistic tasks: We propose a multi-task approach which can be used to greatly improve performance in SQA. In the three corpora here, listening comprehension proves to be crucial as the primary objective is auditory semantic understanding. However, in more nuanced contexts like task guidance (Schlager and Feiner, 2024), it is more important to monitor different paralinguistic aspects of the user such as frustration.\nUse in large-scale model post-training: We believe that our overall data generation process can be useful for improving MLLM post-training. However, verifying this claim is beyond the scope of this work due to computational constraints. We see improved performance on our downstream task after supervised fine-tuning of Gemini, which does indicate positive signal that there are correlations between our training and evaluation data."}]}