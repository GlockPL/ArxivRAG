{"title": "Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2", "authors": ["Tom Lieberum", "Senthooran Rajamanoharan", "Arthur Conmy", "Lewis Smith", "Nicolas Sonnerat", "Vikrant Varma", "J\u00e1nos Kram\u00e1r", "Anca Dragan", "Rohin Shah", "Neel Nanda"], "abstract": "Sparse autoencoders (SAEs) are an unsupervised method for learning a sparse decomposition of a neural network's latent representations into seemingly interpretable features. Despite recent excitement about their potential, research applications outside of industry are limited by the high cost of training a comprehensive suite of SAEs. In this work, we introduce Gemma Scope, an open suite of JumpReLU SAEs trained on all layers and sub-layers of Gemma 2 2B and 9B and select layers of Gemma 2 27B base models. We primarily train SAEs on the Gemma 2 pre-trained models, but additionally release SAEs trained on instruction-tuned Gemma 2 9B for comparison. We evaluate the quality of each SAE on standard metrics and release these results. We hope that by releasing these SAE weights, we can help make more ambitious safety and interpretability research easier for the community. Weights and a tutorial can be found at https://huggingface.co/google/gemma-scope and an interactive demo can be found at https://neuronpedia.org/gemma-scope.", "sections": [{"title": "1. Introduction", "content": "These are several lines of evidence that suggest that a significant fraction of the internal activations of language models are sparse, linear combination of vectors, each corresponding to meaningful features (Elhage et al., 2022; Gurnee et al., 2023; Mikolov et al., 2013; Nanda et al., 2023a; Olah et al., 2020; Park et al., 2023). But by default, it is difficult to identify which vectors are meaningful, or which meaningful vectors are present. Sparse autoencoders are a promising unsupervised approach to do this, and have been shown to often find causally relevant, interpretable directions (Bricken et al., 2023; Cunningham et al., 2023; Gao et al., 2024; Marks et al., 2024; Templeton et al., 2024). If this approach succeeds it could help unlock many of the hoped for applications of interpretability (Hubinger, 2022; Nanda, 2022; Olah, 2021), such as detecting and fixing hallucinations, being able to reliably explain and debug unexpected model behaviour and preventing deception or manipulation from autonomous AI agents.\nHowever, sparse autoencoders are still an immature technique, and there are many open problems to be resolved (Templeton et al., 2024) before these downstream uses can be unlocked - especially validating or red-teaming SAEs as an approach, learning how to measure their performance, learning how to train SAEs at scale efficiently and well, and exploring how SAEs can be productively applied to real-world tasks.\nAs a result, there is an urgent need for further research, both in industry and in the broader community. However, unlike previous interpretability techniques like steering vectors (Li et al., 2023; Turner et al., 2024) or probing (Belinkov, 2022), sparse autoencoders can be highly expensive and difficult to train, limiting the ambition of interpretability research. Though there has been a lot of excellent work with sparse autoencoders on smaller models (Bricken et al., 2023; Cunningham et al., 2023; Dunefsky et al., 2024; Marks et al., 2024), the works that use SAEs on more modern models have normally focused on residual stream SAEs at a single layer (Engels et al., 2024; Gao et al., 2024; Templeton et al., 2024). In addition, many of these (Gao et al., 2024; Templeton et al., 2024) have been trained on proprietary models which makes it more challenging for the community at large to build on this work.\nTo address this we have trained and released the weights of Gemma Scope: a comprehensive, open suite of JumpReLU SAEs (Rajamanoharan"}, {"title": "2. Preliminaries", "content": "Given activations \\(x \\in \\mathbb{R}^n\\) from a language model, a sparse autoencoder (SAE) decomposes and reconstructs the activations using a pair of encoder and decoder functions (f, x) defined by:\n\\(f(x) := \\sigma (W_{enc}x + b_{enc}),\\)   (1)\n\\(x(f) := W_{dec}f + b_{dec}.\\)   (2)\nThese functions are trained to map \\(x(f(x))\\) back to x, making them an autoencoder. Thus, \\(f(x) \\in \\mathbb{R}^M\\) is a set of linear weights that specify how to combine the M \\(\\gg n\\) columns of \\(W_{dec}\\) to reproduce x. The columns of \\(W_{dec}\\), which we denote by \\(d_i\\) for \\(i = 1 . . . M\\), represent the dictionary of directions into which the SAE decomposes x. We will refer to to these learned directions as latents to disambiguate between learnt \u2018features' and the conceptual features which are hypothesized to comprise the language model's representation vectors.\nThe decomposition f(x) is made non-negative and sparse through the choice of activation function \u03c3 and appropriate regularization, such that f(x) typically has much fewer than n non-zero entries. Initial work (Bricken et al., 2023; Cunningham et al., 2023) used a ReLU activation function to enforce non-negativity, and an L1 penalty on the decomposition f(x) to encourage sparsity. TopK SAEs (Gao et al., 2024) enforce sparsity by zeroing all but the top K entries of f(x), whereas the JumpReLU SAEs (Rajamanoharan et al., 2024b) enforce sparsity by zeroing out all entries of f(x) below a positive threshold. Both TopK and JumpReLU SAEs allow for greater separation between the tasks of determining which latents are active, and estimating their magnitudes."}, {"title": "2.2. JumpReLU SAEs", "content": "In this work we focus on JumpReLU SAEs as they have been shown to be a slight Pareto improve-"}, {"title": "3. Training details", "content": "We train SAEs on the activations of Gemma 2 models generated using text data from the same distribution as the pretraining text data for Gemma 1 (Gemma Team, 2024a), except for the one suite of SAEs trained on the instruction-tuned (IT) model (Section 4.5).\nFor a given sequence we only collect activations from tokens which are neither BOS, EOS, nor padding. After activations have been generated, they are shuffled in buckets of about \\(10^6\\) activations. We speculate that a perfect shuffle would not significantly improve results, but this was not systematically checked. We would welcome further investigation into this topic in future work.\nDuring training, activation vectors are normalized by a fixed scalar to have unit mean squared norm. This allows more reliable transfer of hyperparameters (in particular the sparsity coefficient \u03bb and bandwidth \u025b) between layers and sites, as the raw activation norms can vary over multiple orders of magnitude, changing the scale of the reconstruction loss in Eq. (4). Once training is complete, we rescale the trained SAE parameters so that no input normalization is required for inference (see Appendix A for details).\nAs shown in Table 1, SAEs with 16.4K latents are trained for 4B tokens, while 1M-width SAEs are trained for 16B tokens. All other SAEs are trained for 8B tokens.\nWe train SAEs on three locations per layer, as indicated by Fig. 1. We train on the attention head outputs before the final linear transformation \\(W_o\\) and RMSNorm has been applied (Kissane et al., 2024a), on the MLP outputs after the RMSNorm has been applied and on the post MLP residual stream. For the attention output SAEs, we concatenate the outputs of the individual attention heads and learn a joint SAE for the full set of heads. We zero-index the layers, so layer 0 refers to the first transformer block after the embedding layer. In Appendix B we define transcoders (Dunefsky et al., 2024) and train one suite of these."}, {"title": "3.2. Hyperparameters", "content": "We use the same bandwidth \\(\\varepsilon = 0.001\\) and learning rate \\(\\eta = 7 \\times 10^{-5}\\) across all training runs. We use a cosine learning rate warmup from 0.1\u03b7 to \u03b7 over the first 1,000 training steps. We train with the Adam optimizer (Kingma and Ba, 2017) with \\(\\left(\\beta_{1}, \\beta_{2}\\right)=(0,0.999), \\epsilon =10^{-8}\\) and a batch size of 4,096. We use a linear warmup for the sparsity coefficient from 0 to \u03bb over the first 10,000 training steps.\nDuring training, we parameterise the SAE using a pre-encoder bias Bricken et al. (2023), subtracting \\(b_{dec}\\) from activations before the encoder. However, after training is complete, we fold in this bias into the encoder parameters, so that no pre-encoder bias needs to be applied during inference. See Appendix A for details.\nThroughout training, we restrict the columns of \\(W_{dec}\\) to have unit norm by renormalizing after every update. We also project out the part of the gradients parallel to these columns before computing the Adam update, as described in Bricken et al. (2023).\nWe initialize the JumpReLU threshold as the vector \\(\\theta = \\{0.001\\}^M\\). We initialize \\(W_{dec}\\) using He-uniform (He et al., 2015) initialization and rescale each latent vector to be unit norm. \\(W_{enc}\\) is initalized as the transpose of \\(W_{dec}\\), but they are not tied afterwards (Conerly et al., 2024; Gao et al., 2024). The biases \\(b_{dec}\\) and \\(b_{enc}\\) are initialized to zero vectors."}, {"title": "3.3. Infrastructure", "content": "We train most of our SAEs using TPUv3 in a 4x2 configuration. Some SAEs, especially the most wide ones, were trained using TPUv5p in either a 2x2x1 or 2x2x4 configuration.\nWe train SAEs with 16.4K latents with maximum amount of data parallelism, while using maximal amounts of tensor parallelism using Megatron sharding (Shoeybi et al., 2020) for all other configurations. We find that as one goes to small SAEs and correspondingly small update step time, the time spent on host-to-device (H2D) transfers outgrows the time spent on the update step, favoring data sharding. For larger SAEs on the other hand, larger batch sizes enable higher arithmetic intensity by reducing transfers between HBM and VMEM of the TPU. Furthermore, the specific architecture of SAEs means that when using Megatron sharding, device-to-device (D2D) communication is minimal, while data parallelism causes a costly all-reduce of the full gradients. Thus we recommend choosing the smallest degree of data sharding such that the H2D transfer takes slightly less time than the update step.\nAs an example, with proper step time optimization this enables us to process one batch for a"}, {"title": "3.3.2. Data Pipeline", "content": "We store all collected activations on hard drives as raw bytes in shards of 10-20GiB. We use 32-bit precision in all our experiments. This means that storing 8B worth of activations for a single site and layer takes about 100TiB for Gemma 2 9B, or about 17PiB for all sites and layers of both Gemma 2 2B and 9B. The total amount is somewhat higher still, as we train some SAEs for 16B tokens and also train some SAEs on Gemma 2 27B, as well as having a generous buffer of additional tokens. While this is a significant amount of disk space, it is still cheaper than regenerating the data every time one wishes to train an SAE on it.\nSince SAEs are very shallow models with short training step times and we train them on activation vectors rather than integer-valued tokens, training them requires high data throughput. For instance, to train a single SAE on Gemma 2 9B without being bottlenecked by data loading requires more than 1 GiB/s of disk read speed. This demand is further amplified when training multiple SAEs on the same site and layer, e.g. with different sparsity coefficients, or while conducting hyperparameter tuning.\nTo overcome this bottleneck we implement a shared server system, enabling us to amortize disk reads for a single site and layer combination:\nMultiple training jobs share access to a single server. The server maintains a buffer containing a predefined number of data batches. Trainers request these batches from the servers as needed.\nTo enable parallel disk reads, we deploy multiple servers for each site and layer, with each server exclusively responsible for a contiguous slice of the data.\nAs trainers request batches, the server continually fetches new data from the dataset, replacing the oldest data within their buffer.\nTo accommodate variations in trainer speeds caused by"}, {"title": "4. Evaluation", "content": "In this section we evaluate the trained SAEs from various different angles. We note however that as of now there is no consensus on what constitutes a reliable metric for the quality of a sparse autoencoder or its learned latents and that this is an ongoing area of research and debate (Gao et al., 2024; Karvonen et al., 2024; Makelov et al., 2024).\nUnless otherwise noted all evaluations are on sequences from the same distribution as the SAE training data, i.e. the pretraining distribution of Gemma 1."}, {"title": "4.1. Evaluating the sparsity-fidelity trade-off", "content": "For a fixed dictionary size, we trained SAEs of varying levels of sparsity by sweeping the sparsity coefficient \u03bb. We then plot curves showing the level of reconstruction fidelity attainable at a given level of sparsity.\nWe use the mean LO-norm of latent activations, \\(\\mathbb{E}_x||f(x)||_0\\), as a measure of sparsity. To measure reconstruction fidelity, we use two metrics:"}, {"title": "4.2. Impact of sequence position", "content": "Prior research has shown that language models tend to have lower loss on later token positions (Olsson et al., 2022). It is thus natural to ask how an SAE's performance changes over the length of a sequence. Similar to Section 4.1, we track reconstruction loss and delta loss for various sparsity settings, however this time we do not aggregate over the sequence position. Again, we mask out special tokens."}, {"title": "4.3. Studying the effect of SAE width", "content": "Holding all else equal, wide SAEs learn more latent directions and provide better reconstruction fidelity at a given level of sparsity than narrow SAEs. Intuitively, this suggests that we should use the widest SAEs practicable for downstream tasks. However, there are also signs that this intuition may come with caveats. The phenomenon of 'feature-splitting' (Bricken et al., 2023) \u2013 where latents in a narrow SAE seem to split into multiple specialized latents within wider SAEs \u2013 is one sign that wide SAEs do not always use their extra capacity to learn a greater breadth of features (Bussmann et al., 2024). It is plausible that the sparsity penalty used to train SAEs encourages wide SAEs to learn frequent compositions of existing features over (or at least in competition with) using their increased capacity to learn new features (Anders et al., 2024). If this is the case, it is currently unclear whether this is good or bad for the usefulness of SAEs on downstream tasks.\nIn order to facilitate research into how SAEs' properties vary with width, and in particular how SAEs with different widths trained on the same data relate to one another, we train and release a 'feature-splitting\u02bb suite of mid-network residual stream SAEs for Gemma 2 2B and 9B PT with matching sparsity coefficients and widths"}, {"title": "4.4. Interpretability of latents", "content": "The interpretability of latents for a subset of the SAEs included in Gemma Scope was investigated in Rajamanoharan et al. (2024b); latents were evaluated using human raters and via LM generated explanations. For completeness, we include the key findings of those studies here and refer readers to section 5.3 of that work for a detailed discussion of the methodology. Both the human rater and LM explanations studies evaluated JumpReLU, TopK and Gated SAEs of width 131K trained on all sites at layers 9, 20, and 31 of Gemma 2 9B. Fig. 6 shows human raters' judgment of latent interpretability for each investigated SAE architecture. Fig. 7 shows the Pearson correlation between the language model (LM) simulated activations based on LM-generated explanations and the ground truth activation values.\nOn both metrics, there is little discernible difference between JumpReLU, TopK and Gated SAEs."}, {"title": "4.5. SAEs trained on base models transfer to IT models", "content": "Prior research has shown that SAEs trained on base model activations also faithfully reconstruct the activations of IT models derived from these base models (Kissane et al., 2024b). We find further evidence for these results by comparing the Gemma Scope"}, {"title": "4.6. Pile subsets", "content": "We perform the sparsity-fidelity evaluation from Section 4.1 on different validation subsets of The Pile (Gao et al., 2020), to gauge whether SAEs struggle with a particular type of data.\nIn Fig. 9 we show delta loss by subset. Of the studied subsets, SAEs perform best on DeepMind mathematics (Saxton et al., 2019). Possibly this is due to the especially formulaic nature of the data. SAEs perform worst on Europarl (Koehn, 2005), a multilingual dataset. We conjecture that this is due to the Gemma 1 pre-training data, which was used to train the SAEs, containing predominantly English text."}, {"title": "4.7. Impact of low precision inference", "content": "We train all SAEs in 32-bit floating point precision. It is common to make model inference less memory and compute intensive by reducing the"}, {"title": "5. Open problems that Gemma Scope may help tackle", "content": "Our main goal in releasing Gemma Scope is to help the broader safety and interpretability communities advance our understanding of interpretability, and how it can be used to make models safer. As a starting point, we provide a list of open problems we would be particularly excited to see progress on, where we believe Gemma Scope may be able to help. Where possible we cite work that may be a helpful starting point, even if it is not tackling exactly the same question."}, {"title": "Deepening our understanding of SAEs", "content": "1. Exploring the structure and relationships between SAE features, as suggested in Wattenberg and Vi\u00e9gas (2024).\n2. Comparisons of residual stream SAE features across layers, e.g. are there persistent features that one can \u201cmatch up\" across adjacent layers?\n3. Better understanding the phenomenon of \"feature splitting\" (Bricken et al., 2023) where high-level features in a small SAE break apart into several finer-grained features in a wider SAE.\n4. We know that SAEs introduce error, and completely miss out on some features that are captured by wider SAEs (Bussmann et al., 2024; Templeton et al., 2024). Can we quantify and easily measure \u201chow much\" they miss and how much this matters in practice?\n5. How are circuits connecting up superposed features represented in the weights? How do models deal with the interference between features (Nanda et al., 2023b)?\""}, {"title": "Using SAEs to improve performance on real\u2013world tasks (compared to fair baselines)", "content": "1. Detecting or fixing jailbreaks.\n2. Helping find new jailbreaks/red-teaming models (Ziegler et al., 2022).\n3. Comparing steering vectors (Turner et al., 2024) to SAE feature steering (Conmy and Nanda, 2024) or clamping (Templeton et al., 2024).\n4. Can SAEs be used to improve interpretability techniques, like steering vectors, such as by removing irrelevant features (Conmy and Nanda, 2024)?"}, {"title": "Red-teaming SAEs", "content": "1. Do SAEs really find the \"true\" concepts in a model?\n2. How robust are claims about the interpretability of SAE features (Huang et al., 2023)?\n3. Can we find computable, quantitative measures that are a useful proxy for how \u201cinterpretable\" humans think a feature vector is (Bills et al., 2023)?\n4. Can we find the \u201cdark matter\" of truly non-linear features?\n5. Do SAEs learn spurious compositions of independent features to improve sparsity as has been shown to happen in toy models (Anders et al., 2024), and can we fix this?"}, {"title": "Scalable circuit analysis: What interesting circuits can we find in these models?", "content": "1. What's the learned algorithm for addition (Stolfo et al., 2023) in Gemma 2 2B?\n2. How can we practically extend the SAE feature circuit finding algorithm in Marks et al. (2024) to larger models?\n3. Can we use SAE-like techniques such as MLP transcoders (Dunefsky et al., 2024) to find input independent, weights-based circuits?"}, {"title": "Using SAEs as a tool to answer existing questions in interpretability", "content": "1. What does finetuning do to a model's internals (Jain et al., 2024)?\n2. What is actually going on when a model uses chain of thought?\n3. Is in-context learning true learning, or just promoting existing circuits (Hendel et al., 2023; Todd et al., 2024)?\n4. Can we find any \u201cmacroscopic structure\u201d in language models, e.g. families of features that work together to perform specialised roles, like organs in biological organisms?\n5. Does attention head superposition (Jermyn et al., 2023) occur in practice? E.g. are many attention features spread across several heads (Kissane et al., 2024b)?"}, {"title": "Improvements to SAEs", "content": "1. How can SAEs efficiently capture the circular features of Engels et al. (2024)?\n2. How can they deal efficiently with cross-layer superposition, i.e. features produced in superposition by neurons spread across multiple layers?\n3. How much can SAEs be quantized without significant performance degradation, both for inference and training?"}, {"title": "A. Standardizing SAE parameters for inference", "content": "As described in Section 3, during training, we normalize LM activations and subtract bdec from them before passing them to the encoder. However, after training, we reparameterize the Gemma Scope SAEs so that neither of these steps are required during inference.\nLet xraw be the raw LM activations that we rescale by a scalar constant C, i.e. \\(x := x_{raw}/C\\), such that \\(\\mathbb{E} [||x||^2] = 1\\). Then, as parameterized during training, the SAE forward pass is defined by\n\\(f(x_{raw}) := JumpReLU_{\\theta} \\left(W_{enc} \\left(\\frac{x_{raw}}{C} - b_{dec}\\right) + b_{enc}\\right),\\)   (5)\n\\(x_{raw}(f) := C \\cdot (W_{dec}f + b_{dec}) .\\)   (6)\nIt is straightforward to show that by defining the following rescaled and shifted parameters:\n\\(b'_{enc} := C b_{enc} - \\frac{C W_{enc}b_{dec}}{C} \\quad\\)   (7)\n\\(b'_{dec} := Cb_{dec} \\quad\\)   (8)\n\\(\\theta' := C \\theta \\quad\\)   (9)\nwe can simplify the SAE forward pass (operating on the raw activations xraw) as follows:\n\\(f(x_{raw}) = JumpReLU_{\\theta'} (W_{enc}x_{raw} + b'_{enc}),\\)   (10)\n\\(x_{raw}(f) = W_{dec}f + b'_{dec}.\\)   (11)"}, {"title": "B. Transcoders", "content": "MLP SAEs are trained on the output of MLPs, but we can also replace the whole MLP with a transcoder (Dunefsky et al., 2024) for easier circuit analysis. Transcoders are not autoencoders: while SAEs are trained to reconstruct their input, transcoders are trained to approximate the output of MLP layers from the input of the MLP layer. We train one suite of transcoders on Gemma 2B PT, and release these at the link https://huggingface.co/google/gemma-scope-2b-pt-transcoders."}, {"title": "C. Additional evaluation results", "content": "Fig. 13 illustrates the trade off between fidelity as measured by fraction of variance unexplained (FVU) against sparsity for layer 12 Gemma 2 2B and layer 20 Gemma 2 9B SAEs.\nFig. 14 shows the sparsity-fidelity trade off for the 131K-width residual stream SAEs trained on Gemma 2 27B after layers 10, 22 and 34 that we include as part of this release.\nFig. 17 and Fig. 18 show fidelity versus sparsity curves for more layers (approximately evenly spaced) and all sites of Gemma 2 2B and Gemma 2 9B, demonstrating consistent and smoothly variance performance throughout these models."}, {"title": "C.2. Impact of sequence position", "content": "Fig. 15 shows how delta loss varies by position."}, {"title": "C.3. Uniformity of active latent importance", "content": "Conventionally, sparsity of SAE latent activations is measured as the LO norm of the latent activations. Olah et al. (2024) suggest to train SAEs to have low L1 activation of attribution-weighted latent activations, taking into account that some latents may be more important than others. We repurpose their loss function as a metric for our SAEs, which were trained penalising activation sparsity as normal. As in Rajamanoharan et al. (2024b), we define the attribution-weighted latent activation vector y as\n\\(y:= f(x) \\odot W^{dec} \\nabla_x L,\\)   (12)\nwhere we choose the mean-centered logit of the correct next token as the loss function L.\nWe then normalize the magnitudes of the entries of y to obtain a probability distribution p = p(y). We can measure how far this distribution diverges from a uniform distribution u over active latents via the KL divergence\n\\(D_{KL}(p||u) = log ||y||_0 \u2013 S(p),\\)   (13)"}]}