{"title": "Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2", "authors": ["Tom Lieberum", "Senthooran Rajamanoharan", "Arthur Conmy", "Lewis Smith", "Nicolas Sonnerat", "Vikrant Varma", "J\u00e1nos Kram\u00e1r", "Anca Dragan", "Rohin Shah", "Neel Nanda"], "abstract": "Sparse autoencoders (SAEs) are an unsupervised method for learning a sparse decomposition of a neural network's latent representations into seemingly interpretable features. Despite recent excitement about their potential, research applications outside of industry are limited by the high cost of training a comprehensive suite of SAEs. In this work, we introduce Gemma Scope, an open suite of JumpReLU SAEs trained on all layers and sub-layers of Gemma 2 2B and 9B and select layers of Gemma 2 27B base models. We primarily train SAEs on the Gemma 2 pre-trained models, but additionally release SAEs trained on instruction-tuned Gemma 2 9B for comparison. We evaluate the quality of each SAE on standard metrics and release these results. We hope that by releasing these SAE weights, we can help make more ambitious safety and interpretability research easier for the community. Weights and a tutorial can be found at https://huggingface.co/google/gemma-scope and an interactive demo can be found at https://neuronpedia.org/gemma-scope.", "sections": [{"title": "1. Introduction", "content": "These are several lines of evidence that suggest that a significant fraction of the internal activations of language models are sparse, linear combination of vectors, each corresponding to meaningful features. But by default, it is difficult to identify which vectors are meaningful, or which meaningful vectors are present. Sparse autoencoders are a promising unsupervised approach to do this, and have been shown to often find causally relevant, interpretable directions. If this approach succeeds it could help unlock many of the hoped for applications of interpretability, such as detecting and fixing hallucinations, being able to reliably explain and debug unexpected model behaviour and preventing deception or manipulation from autonomous AI agents.\nHowever, sparse autoencoders are still an immature technique, and there are many open problems to be resolved before these downstream uses can be unlocked - especially validating or red-teaming SAEs as an approach, learning how to measure their performance, learning how to train SAEs at scale efficiently and well, and exploring how SAEs can be productively applied to real-world tasks.\nAs a result, there is an urgent need for further research, both in industry and in the broader community. However, unlike previous interpretability techniques like steering vectors or probing , sparse autoencoders can be highly expensive and difficult to train, limiting the ambition of interpretability research. Though there has been a lot of excellent work with sparse autoencoders on smaller models, the works that use SAEs on more modern models have normally focused on residual stream SAEs at a single layer. In addition, many of these have been trained on proprietary models which makes it more challenging for the community at large to build on this work.\nTo address this we have trained and released the weights of Gemma Scope: a comprehensive, open suite of JumpReLU SAEs"}, {"title": "2. Preliminaries", "content": "Given activations $x \\in \\mathbb{R}^n$ from a language model, a sparse autoencoder (SAE) decomposes and reconstructs the activations using a pair of encoder and decoder functions $(f, x)$ defined by:\n$f(x) := \\sigma (W_{enc}x + b_{enc}),$ (1)\n$x(f) := W_{dec}f + b_{dec}.$ (2)\nThese functions are trained to map $(f(x))$ back to $x$, making them an autoencoder. Thus, $f(x) \\in \\mathbb{R}^M$ is a set of linear weights that specify how to combine the $M \\gg n$ columns of $W_{dec}$ to reproduce $x$. The columns of $W_{dec}$, which we denote by $d_i$ for $i = 1 ... M$, represent the dictionary of directions into which the SAE decomposes $x$. We will refer to to these learned directions as latents to disambiguate between learnt \u2018features' and the conceptual features which are hypothesized to comprise the language model's representation vectors.\nThe decomposition $f(x)$ is made non-negative and sparse through the choice of activation function $\\sigma$ and appropriate regularization, such that $f(x)$ typically has much fewer than n non-zero entries. Initial work used a ReLU activation function to enforce non-negativity, and an L1 penalty on the decomposition f(x) to encourage sparsity. TopK SAEs enforce sparsity by zeroing all but the top K entries of f(x), whereas the JumpReLU SAEs enforce sparsity by zeroing out all entries of f(x) below a positive threshold. Both TopK and JumpReLU SAEs allow for greater separation between the tasks of determining which latents are active, and estimating their magnitudes."}, {"title": "2.2. JumpReLU SAEs", "content": "In this work we focus on JumpReLU SAEs as they have been shown to be a slight Pareto improve-"}, {"title": "JumpReLU activation", "content": "The JumpReLU activation is a shifted Heaviside step function as a gating mechanism together with a conventional ReLU:\n$\\sigma(z) = \\text{JumpReLU}_{\\theta}(z) := z\\odot H(z - \\theta).$ (3)\nHere, $\\theta > 0$ is the JumpReLU's vector-valued learnable threshold parameter, $\\odot$ denotes elementwise multiplication, and H is the Heaviside step function, which is 1 if its input is positive and 0 otherwise. Intuitively, the JumpReLU leaves the pre-activations unchanged above the threshold, but sets them to zero below the threshold, with a different learned threshold per latent."}, {"title": "Loss function", "content": "As loss function we use a squared error reconstruction loss, and directly regularize the number of active (non-zero) latents using the LO penalty:\n$L := ||x - x(f(x))||^2 + \\lambda ||f(x)||_0,$ (4)\nwhere $\\lambda$ is the sparsity penalty coefficient. Since the LO penalty and JumpReLU activation function are piecewise constant with respect to threshold parameters $\\theta$, we use straight-through estimators (STEs) to train 0, using the approach described in Rajamanoharan et al. (2024b). This introduces an additional hyperparameter, the kernel density estimator bandwidth $\\epsilon$, which controls the quality of the gradient estimates use to train the threshold parameters $\\theta$."}, {"title": "3. Training details", "content": ""}, {"title": "3.1. Data", "content": "We train SAEs on the activations of Gemma 2 models generated using text data from the same distribution as the pretraining text data for Gemma 1 (Gemma Team, 2024a), except for the one suite of SAEs trained on the instruction-tuned (IT) model (Section 4.5).\nFor a given sequence we only collect activations from tokens which are neither BOS, EOS, nor padding. After activations have been generated, they are shuffled in buckets of about $10^6$ activations. We speculate that a perfect shuffle would not significantly improve results, but this was not systematically checked. We would welcome further investigation into this topic in future work.\nDuring training, activation vectors are normalized by a fixed scalar to have unit mean squared norm. This allows more reliable transfer of hyperparameters (in particular the sparsity coefficient $\\lambda$ and bandwidth $\\epsilon$) between layers and sites, as the raw activation norms can vary over multiple orders of magnitude, changing the scale of the reconstruction loss in Eq. (4). Once training is complete, we rescale the trained SAE parameters so that no input normalization is required for inference (see Appendix A for details).\nAs shown in Table 1, SAEs with 16.4K latents are trained for 4B tokens, while 1M-width SAEs are trained for 16B tokens. All other SAEs are trained for 8B tokens.\nLocation We train SAEs on three locations per layer, as indicated by Fig. 1. We train on the attention head outputs before the final linear transformation $W_o$ and RMSNorm has been applied , on the MLP outputs after the RMSNorm has been applied and on the post MLP residual stream. For the attention output SAEs, we concatenate the outputs of the individual attention heads and learn a joint SAE for the full set of heads. We zero-index the layers, so layer 0 refers to the first transformer block after the embedding layer. In Appendix B we define transcoders  and train one suite of these."}, {"title": "3.2. Hyperparameters", "content": "Optimization We use the same bandwidth $\\epsilon = 0.001$ and learning rate $\\eta = 7 \\times 10^{-5}$ across all training runs. We use a cosine learning rate warmup from $0.1\\eta$ to $\\eta$ over the first 1,000 training steps. We train with the Adam optimizer  with $(\\beta_1, \\beta_2) = (0, 0.999), \\epsilon = 10^{-8}$ and a batch size of 4,096. We use a linear warmup for the sparsity coefficient from 0 to $\\lambda$ over the first 10,000 training steps.\nDuring training, we parameterise the SAE using a pre-encoder bias , subtracting $b_{dec}$ from activations before the encoder. However, after training is complete, we fold in this bias into the encoder parameters, so that no pre-encoder bias needs to be applied during inference. See Appendix A for details.\nThroughout training, we restrict the columns of $W_{dec}$ to have unit norm by renormalizing after every update. We also project out the part of the gradients parallel to these columns before computing the Adam update, as described in Bricken et al. (2023).\nInitialization We initialize the JumpReLU threshold as the vector $\\theta = \\{0.001\\}^M$. We initialize $W_{dec}$ using He-uniform  initialization and rescale each latent vector to be unit norm. $W_{enc}$ is initalized as the transpose of $W_{dec}$, but they are not tied afterwards (Conerly et al., 2024; Gao et al., 2024). The biases $b_{dec}$ and $b_{enc}$ are initialized to zero vectors."}, {"title": "3.3. Infrastructure", "content": ""}, {"title": "3.3.1. Accelerators", "content": "Topology We train most of our SAEs using TPUv3 in a 4x2 configuration. Some SAEs, especially the most wide ones, were trained using TPUv5p in either a 2x2x1 or 2x2x4 configuration.\nSharding We train SAEs with 16.4K latents with maximum amount of data parallelism, while using maximal amounts of tensor parallelism using Megatron sharding (Shoeybi et al., 2020) for all other configurations. We find that as one goes to small SAEs and correspondingly small update step time, the time spent on host-to-device (H2D) transfers outgrows the time spent on the update step, favoring data sharding. For larger SAEs on the other hand, larger batch sizes enable higher arithmetic intensity by reducing transfers between HBM and VMEM of the TPU. Furthermore, the specific architecture of SAEs means that when using Megatron sharding, device-to-device (D2D) communication is minimal, while data parallelism causes a costly all-reduce of the full gradients. Thus we recommend choosing the smallest degree of data sharding such that the H2D transfer takes slightly less time than the update step.\nAs an example, with proper step time optimization this enables us to process one batch for a"}, {"title": "3.3.2. Data Pipeline", "content": "Disk storage We store all collected activations on hard drives as raw bytes in shards of 10-20GiB. We use 32-bit precision in all our experiments. This means that storing 8B worth of activations for a single site and layer takes about 100TiB for Gemma 2 9B, or about 17PiB for all sites and layers of both Gemma 2 2B and 9B. The total amount is somewhat higher still, as we train some SAEs for 16B tokens and also train some SAES on Gemma 2 27B, as well as having a generous buffer of additional tokens. While this is a significant amount of disk space, it is still cheaper than regenerating the data every time one wishes to train an SAE on it.\nDisk reads Since SAEs are very shallow models with short training step times and we train them on activation vectors rather than integer-valued tokens, training them requires high data throughput. For instance, to train a single SAE on Gemma 2 9B without being bottlenecked by data loading requires more than 1 GiB/s of disk read speed. This demand is further amplified when training multiple SAEs on the same site and layer, e.g. with different sparsity coefficients, or while conducting hyperparameter tuning.\nTo overcome this bottleneck we implement a shared server system, enabling us to amortize disk reads for a single site and layer combination:\nShared data buffer: Multiple training jobs share access to a single server. The server maintains a buffer containing a predefined number of data batches. Trainers request these batches from the servers as needed.\nDistributed disk reads: To enable parallel disk reads, we deploy multiple servers for each site and layer, with each server exclusively responsible for a contiguous slice of the data.\nDynamic data fetching: As trainers request batches, the server continually fetches new data from the dataset, replacing the oldest data within their buffer.\nHandling speed differences: To accommodate variations in trainer speeds caused by"}, {"title": "4. Evaluation", "content": "In this section we evaluate the trained SAEs from various different angles. We note however that as of now there is no consensus on what constitutes a reliable metric for the quality of a sparse autoencoder or its learned latents and that this is an ongoing area of research and debate. \nUnless otherwise noted all evaluations are on sequences from the same distribution as the SAE training data, i.e. the pretraining distribution of Gemma 1."}, {"title": "4.1. Evaluating the sparsity-fidelity trade-off", "content": "Methodology For a fixed dictionary size, we trained SAEs of varying levels of sparsity by sweeping the sparsity coefficient $\\lambda$. We then plot curves showing the level of reconstruction fidelity attainable at a given level of sparsity.\nMetrics We use the mean LO-norm of latent activations, $E_x||f(x)||_0$, as a measure of sparsity. To measure reconstruction fidelity, we use two metrics:\nOur primary metric is delta LM loss, the increase in the cross-entropy loss experienced by the LM when we splice the SAE into the LM's forward pass.\nAs a secondary metric, we also use fraction of variance unexplained (FVU) \u2013 also called the normalized loss as a measure of reconstruction fidelity. This is the mean reconstruction loss $L_{reconstruct}$ of a SAE normalized by the reconstruction loss obtained by always predicting the dataset mean. Note that FVU is purely a measure of the SAE's ability to reconstruction the input activations, not taking into account the causal effect of any error on the downstream loss.\nAll metrics were computed on 2,048 sequences of length 1,024, after masking out special tokens (pad, start and end of sequence) when aggregating the results.\nResults Fig. 2 compares the sparsity-fidelity trade-off for SAEs in the middle of each Gemma model. For the full results see Appendix C. Delta loss is consistently higher for residual stream SAEs compared to MLP and attention SAEs, whereas FVU (Fig. 13) is roughly comparable across sites. We conjecture this is because even small errors in reconstructing the residual stream can have a significant impact on LM loss, whereas relatively large errors in reconstructing a single MLP or attention sub-layer's output have a limited impact on the LM loss."}, {"title": "4.2. Impact of sequence position", "content": "Methodology Prior research has shown that language models tend to have lower loss on later token positions. It is thus natural to ask how an SAE's performance changes over the length of a sequence. Similar to Section 4.1, we track reconstruction loss and delta loss for various sparsity settings, however this time we do not aggregate over the sequence position. Again, we mask out special tokens.\nResult Fig. 3 shows how reconstruction loss varies by position for 131K-width SAEs trained on the middle-layer of Gemma 2 9B. Reconstruction loss increases rapidly from close to zero over the first few tokens. The loss monotonically increases by position for attention SAEs, although it is essentially flat after 100 tokens. For MLP SAEs, the loss peaks at around the tenth token before gradually declining slightly. We speculate that this is because attention is most useful when tracking long-range dependencies in text, which matters most when there is significant prior context to draw from, while MLP layers do a lot of local processing, like detecting n-grams, that does not need much context. Like attention SAEs, residual stream SAEs' loss monotonically increases, plateauing more gradually. Curves for other models, layers, widths and sparsity coefficients were found to be qualitatively similar.\nFig. 15 shows how delta LM loss varies by sequence position. The high level of noise in the delta loss measurements makes it difficult to robustly measure the effect of position, however there are signs that this too is slightly lower for the first few tokens, particularly for residual stream SAES."}, {"title": "4.3. Studying the effect of SAE width", "content": "Holding all else equal, wide SAEs learn more latent directions and provide better reconstruction fidelity at a given level of sparsity than narrow SAEs. Intuitively, this suggests that we should use the widest SAEs practicable for downstream tasks. However, there are also signs that this intuition may come with caveats. The phenomenon of 'feature-splitting'  \u2013 where latents in a narrow SAE seem to split into multiple specialized latents within wider SAEs \u2013 is one sign that wide SAEs do not always use their extra capacity to learn a greater breadth of features (Bussmann et al., 2024). It is plausible that the sparsity penalty used to train SAEs encourages wide SAEs to learn frequent compositions of existing features over (or at least in competition with) using their increased capacity to learn new features . If this is the case, it is currently unclear whether this is good or bad for the usefulness of SAEs on downstream tasks.\nIn order to facilitate research into how SAEs' properties vary with width, and in particular how SAEs with different widths trained on the same data relate to one another, we train and release a 'feature-splitting\u02bb suite of mid-network residual stream SAEs for Gemma 2 2B and 9B PT with matching sparsity coefficients and widths"}, {"title": "4.4. Interpretability of latents", "content": "The interpretability of latents for a subset of the SAEs included in Gemma Scope was investigated in Rajamanoharan et al. (2024b); latents were evaluated using human raters and via LM generated explanations. For completeness, we include the key findings of those studies here and refer readers to section 5.3 of that work for a detailed discussion of the methodology. Both the human rater and LM explanations studies evaluated JumpReLU, TopK and Gated SAEs of width 131K trained on all sites at layers 9, 20, and 31 of Gemma 2 9B. Fig. 6 shows human raters' judgment of latent interpretability for each investigated SAE architecture. Fig. 7 shows the Pearson correlation between the language model (LM) simulated activations based on LM-generated explanations and the ground truth activation values.\nOn both metrics, there is little discernible difference between JumpReLU, TopK and Gated SAEs."}, {"title": "4.5. SAEs trained on base models transfer to IT models", "content": "Additional IT SAE training Prior research has shown that SAEs trained on base model activations also faithfully reconstruct the activations of IT models derived from these base models. We find further evidence for these results by comparing the Gemma Scope"}, {"title": "4.6. Pile subsets", "content": "Methodology We perform the sparsity-fidelity evaluation from Section 4.1 on different validation subsets of The Pile , to gauge whether SAEs struggle with a particular type of data.\nResults In Fig. 9 we show delta loss by subset. Of the studied subsets, SAEs perform best on DeepMind mathematics . Possibly this is due to the especially formulaic nature of the data. SAEs perform worst on Europarl , a multilingual dataset. We conjecture that this is due to the Gemma 1 pre-training data, which was used to train the SAEs, containing predominantly English text."}, {"title": "4.7. Impact of low precision inference", "content": "We train all SAEs in 32-bit floating point precision. It is common to make model inference less memory and compute intensive by reducing the precision at inference time. This is particularly important for applications like circuit analysis, where users may wish to splice several SAEs into a language model simultaneously. Fig. 10 compares fidelity-versus-sparsity curves computed using float32 SAE and LM weights versus the same curves computed using bfloat16 SAE and LM weights, suggesting there is negligible impact in switching to bfloat16 for inference."}, {"title": "5. Open problems that Gemma Scope may help tackle", "content": "Our main goal in releasing Gemma Scope is to help the broader safety and interpretability communities advance our understanding of interpretability, and how it can be used to make models safer. As a starting point, we provide a list of open problems we would be particularly excited to see progress on, where we believe Gemma Scope may be able to help. Where possible we cite work that may be a helpful starting point, even if it is not tackling exactly the same question.\nDeepening our understanding of SAEs\nExploring the structure and relationships between SAE features, as suggested in Wattenberg and Vi\u00e9gas (2024).\nComparisons of residual stream SAE features across layers, e.g. are there persistent features that one can \u201cmatch up\" across adjacent layers?\nBetter understanding the phenomenon of \"feature splitting\" where high-level features in a small SAE break apart into several finer-grained features in a wider SAE.\nWe know that SAEs introduce error, and completely miss out on some features that are captured by wider SAEs . Can we quantify and easily measure \u201chow much\" they miss and how much this matters in practice?\nHow are circuits connecting up superposed features represented in the weights? How do models deal with the interference between features (Nanda et al., 2023b)?\nUsing SAEs to improve performance on real\u2013world tasks (compared to fair baselines)\nDetecting or fixing jailbreaks.\nHelping find new jailbreaks/red-teaming models.\nComparing steering vectors to SAE feature steering or clamping.\nCan SAEs be used to improve interpretability techniques, like steering vectors, such as by removing irrelevant features?\nRed-teaming SAEs\nDo SAEs really find the \"true\" concepts in a model?\nHow robust are claims about the interpretability of SAE features?\nCan we find computable, quantitative measures that are a useful proxy for how \u201cinterpretable\" humans think a feature vector is?\nCan we find the \u201cdark matter\" of truly non-linear features?\nDo SAEs learn spurious compositions of independent features to improve sparsity as has been shown to happen in toy models , and can we fix this?\nScalable circuit analysis: What interesting circuits can we find in these models?\nWhat's the learned algorithm for addition in Gemma 2 2B?\nHow can we practically extend the SAE feature circuit finding algorithm in to larger models?\nCan we use SAE-like techniques such as MLP transcoders to find input independent, weights-based circuits?\nUsing SAEs as a tool to answer existing questions in interpretability\nWhat does finetuning do to a model's internals?\nWhat is actually going on when a model uses chain of thought?\nIs in-context learning true learning, or just promoting existing circuits?\nCan we find any \u201cmacroscopic structure\u201d in language models, e.g. families of features that work together to perform specialised roles, like organs in biological organisms?\nDoes attention head superposition occur in practice? E.g. are many attention features spread across several heads?\nImprovements to SAEs\nHow can SAEs efficiently capture the circular features of Engels et al. (2024)?\nHow can they deal efficiently with cross-layer superposition, i.e. features produced in superposition by neurons spread across multiple layers?\nHow much can SAEs be quantized without significant performance degradation, both for inference and training?"}, {"title": "A. Standardizing SAE parameters for inference", "content": "As described in Section 3, during training, we normalize LM activations and subtract $b_{dec}$ from them before passing them to the encoder. However, after training, we reparameterize the Gemma Scope SAEs so that neither of these steps are required during inference.\nLet $x_{raw}$ be the raw LM activations that we rescale by a scalar constant C, i.e. $x := \\frac{x_{raw}}{C}$, such that $E [||x||^2] = 1$. Then, as parameterized during training, the SAE forward pass is defined by\n$f(x_{raw}) := \\text{JumpReLU}_{\\theta} (W_{enc} ( \\frac{x_{raw}}{C} - b_{dec}) + b_{enc}),$ (5)\n$x(f) := C \\cdot (W_{dec}f + b_{dec}) .$ (6)\nIt is straightforward to show that by defining the following rescaled and shifted parameters:\n$b'_{enc} := C b_{enc} - \\frac{C}{c}W_{enc}b_{dec}$ (7)\n$b'_{dec} := C b_{dec}$ (8)\n$\\theta' := C \\theta$ (9)\nwe can simplify the SAE forward pass (operating on the raw activations $x_{raw}$) as follows:\n$f(x_{raw}) = \\text{JumpReLU}_{\\theta'} (W_{enc}x_{raw} + b'_{enc}),$ (10)\n$x(f) = W_{dec}f + b'_{dec}$ (11)"}, {"title": "B. Transcoders", "content": "MLP SAEs are trained on the output of MLPs, but we can also replace the whole MLP with a transcoder  for easier circuit analysis. Transcoders are not autoencoders: while SAEs are trained to reconstruct their input, transcoders are trained to approximate the output of MLP layers from the input of the MLP layer. We train one suite of transcoders on Gemma 2B PT, and release these at the link https://huggingface.co/google/gemma-scope-2b-pt-transcoders.\nEvaluation We find that transcoders cause a greater increase in loss to the base model relative to the MLP output SAEs (Fig. 11), at a fixed sparsity (LO). This reverses the trend from GPT-2 Small found by Dunefsky et al. (2024). This could be due to a number of factors, such as:\nTranscoders do not scale to larger models or modern transformer architectures (e.g. Gemma 2 has Gated MLPs unlike GPT-2 Small) as well as SAEs.\nJumpReLU provides a bigger performance boost to SAEs than to transcoders.\nErrors in the implementation of transcoders in this work, or in the SAE implementation from Dunefsky et al. (2024).\nOther training details (not just the JumpReLU architecture) that improve SAEs more than transcoders. Dunefsky et al. (2024) use training methods such as using a low learning rate, differing from SAE research that came out at a similar time to Bricken et al. (2023) such as Rajamanoharan et al. (2024a) and Cunningham et al."}, {"title": "C. Additional evaluation results", "content": ""}, {"title": "C.1. Sparsity-fidelity tradeoff", "content": "Fig. 13 illustrates the trade off between fidelity as measured by fraction of variance unexplained (FVU) against sparsity for layer 12 Gemma 2 2B and layer 20 Gemma 2 9B SAEs.\nFig. 14 shows the sparsity-fidelity trade off for the 131K-width residual stream SAEs trained on Gemma 2 27B after layers 10, 22 and 34 that we include as part of this release.\nFig. 17 and Fig. 18 show fidelity versus sparsity curves for more layers (approximately evenly spaced) and all sites of Gemma 2 2B and Gemma 2 9B, demonstrating consistent and smoothly variance performance throughout these models."}, {"title": "C.2. Impact of sequence position", "content": "Fig. 15 shows how delta loss varies by position."}, {"title": "C.3. Uniformity of active latent importance", "content": "Methodology Conventionally, sparsity of SAE latent activations is measured as the LO norm of the latent activations. Olah et al. (2024) suggest to train SAEs to have low L1 activation of attribution-weighted latent activations, taking into account that some latents may be more important than others. We repurpose their loss function as a metric for our SAEs, which were trained penalising activation sparsity as normal. As in Rajamanoharan et al. (2024b), we define the attribution-weighted latent activation vector $y$ as\n$y:= f(x) \\odot W_{ael} \\nabla_x L,$ (12)\nwhere we choose the mean-centered logit of the correct next token as the loss function $L$.\nWe then normalize the magnitudes of the entries of y to obtain a probability distribution $p = p(y)$. We can measure how far this distribution diverges from a uniform distribution u over active latents via the KL divergence\n$D_{KL}(p||u) = log ||y||_0 \u2013 S(p),$ (13)"}, {"title": "C.4. Additional Gemma 2 IT evaluation results", "content": "In this sub-appendix, we provide further evaluations of SAEs on the activations of IT models, continuing Section 4.5.\nAs mentioned in Section 4.5, we find in Fig. 21 that PT SAEs achieve reasonable FVU on rollouts, but the gap between PT and IT SAEs is larger than in the change in loss in the main text (Fig. 8).\nIn Fig. 19 we evaluate the FVU on the user prompt and model prefix (not the rollout). In Fig. 20 we evaluate the change in loss (delta loss) on the user prompts, and surprisingly find that splicing in the base model SAE can reduce the loss in expectation in some cases. Our explanation for this result is that post-training does not train models to predict user queries (only predict high-preference model rollouts) and therefore the model is not incentivised to have good predictive loss by default on the user prompt.\nWhile we do not train IT SAEs on Gemma 2 2B, we find that the base SAEs transfer well as measured by FVU in Fig. 22.\nFinally, we do not find evidence that rescaling IT activations to have same norm in expectation to the pretraining activations is beneficial (Fig. 23). The trend for individual SAEs in this plot is that their LO decreases but the Pareto frontier is very slightly worse. This is consistent with prior observations that SAEs are surprisingly adaptable to different LOs"}]}