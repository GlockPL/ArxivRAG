[{"title": "Related Work", "authors": ["Kwanyoung Park", "Youngwoon Lee"], "abstract": "Model-based offline reinforcement learning (RL) is a compelling approach that addresses the challenge of learning from limited, static data by generating imaginary trajectories using learned models. However, it falls short in solving long-horizon tasks due to high bias in value estimation from model rollouts. In this paper, we introduce a novel model-based offline RL method, Lower Expectile Q-learning (LEQ), which enhances long-horizon task performance by mitigating the high bias in model-based value estimation via expectile regression of A-returns. Our empirical results show that LEQ significantly outperforms previous model-based offline RL methods on long-horizon tasks, such as the D4RL AntMaze tasks, matching or surpassing the performance of model-free approaches. Our experiments demonstrate that expectile regression, A-returns, and critic training on offline data are all crucial for addressing long-horizon tasks. Additionally, LEQ achieves performance comparable to the state-of-the-art model-based and model-free offline RL methods on the NeoRL benchmark and the D4RL MuJoCo Gym tasks.", "sections": [{"title": "Introduction", "content": "One of the major challenges in offline reinforcement learning (RL) is the overestimation of values for out-of-distribution actions due to the lack of environment interactions [21, 19]. Model-based offline RL addresses this issue by generating additional (imaginary) training data using a learned model, thereby augmenting the given offline data with synthetic experiences that cover out-of-distribution states and actions [34, 16, 35, 2, 30]. While these approaches have demonstrated strong performance in simple, short-horizon tasks, they struggle with noisy model predictions and value estimations, particularly in long-horizon tasks [23]. This challenge is evident in their poor performances (i.e. near zero) on the D4RL AntMaze tasks [6, 15].\nTypical model-based offline RL methods alleviate the inaccurate value estimation problem (mostly overestimation) by penalizing Q-values estimated from model rollouts with uncertainties in model predictions [34, 16] or value predictions [30, 14]. While these penalization terms prevent a policy from exploiting erroneous value estimations, the policy now does not maximize the true value, but maximizes the value penalized by heuristically estimated uncertainties, which can lead to sub-optimal behaviors. This is especially problematic in long-horizon, sparse-reward tasks, where Q-values are similar across nearby states [23].\nAnother way to reduce bias in value estimates is by using multi-step returns [31, 12]. \u0421\u0412\u041e\u0420 [14] constructs an explicit distribution of multi-step Q-values from thousands of model rollouts and uses this value as a target for training the Q-function. However, CBOP is computationally expensive for estimating a target value and uses multi-step returns solely for Q-learning, which provides insufficient learning signals for obtaining long-horizon behaviors."}, {"title": "Related Work", "content": "Offline RL [21] aims to solve a reinforcement learning problem only with pre-collected datasets, better than behavioral cloning policies [25]. One can simply apply off-policy RL algorithms on top of the fixed dataset. However, off-policy RL methods suffer from the overestimation of Q-values for actions unseen in the offline dataset [8, 18, 19], since an overestimated value function cannot get corrected through online environment interactions in offline RL.\nModel-free offline RL algorithms have addressed this value overestimation problem on out-of-distribution actions by (1) regularizing a policy to only output actions in the offline data [24, 17, 7] or (2) adopting a conservative value estimation for executing actions different from the dataset [19, 1]. Despite their strong performances on the standard offline RL benchmarks, model-free offline RL policies tend to be constrained to the support of the data (i.e. state-action pairs in the offline dataset), which may lead to limited generalization capability.\nModel-based offline RL approaches have tried to overcome this limitation by suggesting a better use of the limited offline data \u2013 learning a world model and generating imaginary data with the learned model that covers out-of-distribution actions. Similar to Dyna-style online model-based RL [32, 9-11], an offline model-based RL policy can be trained on both offline data and model rollouts. But, again, learned models may be inaccurate on states and actions outside the data support, making a policy easily exploit the learned models.\nRecent model-based offline RL algorithms have adopted the conservatism idea from model-free offline RL, penalizing policies incurring (1) uncertain transition dynamics [34, 16, 35] or (2) uncertain value estimation [30, 14]. This conservative use of model-generated data enables model-based offline RL to outperform model-free offline RL in widely used offline RL benchmarks [30]. However, uncertainty estimation is difficult and often inaccurate [35]. Instead of relying on such heuristic [34, 16, 30] or expensive [14] uncertainty estimation, we propose to learn a conservative value function via expectile regression with a small \u03c4, which is simple, efficient, yet effective."}, {"title": "Preliminaries", "content": "Problem setup. We formulate our problem as a Markov Decision Process (MDP) defined as a tuple, M = (S, A,r,p, \u03c1, \u03b3) [33]. S and A denote the state and action spaces, respectively. r : S \u00d7 A \u2192 R denotes the reward function. p : S \u00d7 A \u2192 \u2206(S)\u00b9 denotes the transition dynamics. p(so) \u2208 \u0394(S) denotes the initial state distribution and y is a discounting factor. The goal of reinforcement learning (RL) is to find a policy, \u03c0 : S \u2192 \u2206(A), that maximizes the expected return,\n$\\mathbb{E}_{\\tau \\sim p(\\tau; \\pi, \\rho)} [\\sum_{t=0}^{T-1} \\gamma^t r(s_t, a_t)]$, where is a sequence of transitions with a finite horizon T,\n\u03c4 = (So, ao, ro, S1, a1, r1, ..., ST), following \u03c0(at | st) and p(St+1 | St, at) starting from so ~ p(.).\nIn this paper, we consider the offline RL setup [21], where a policy \u03c0 is trained with a fixed given offline dataset, Denv = {T1, T2, ..., TN }, without any additional online interactions.\nModel-based offline RL. As an offline RL policy is trained from a fixed dataset, one of the major challenges in offline RL is the limited data support; thus, lack of generalization to out-of-distribution states and actions. Model-based offline RL [16, 34, 35, 27, 30, 14] tackles this problem by augmenting the training data with imaginary training data (i.e. model rollouts) generated from the learned transition dynamics and reward model, py(St+1,r | St, at).\nThe typical process of model-based offline RL is as follows: (1) pretrain a model (or an ensemble of models) and an initial policy from the offline data, (2) generate short imaginary rollouts {T} using the pretrained model and add them to the training dataset Dmodel \u2190 Dmodel \u222a {T}, (3) perform an offline RL algorithm on the augmented dataset Dmodel U Deny, and repeat (2) and (3).\nExpectile regression. Expectile is a generalization of the expectation of a distribution X. While the expectation of X, E[X], can be viewed as a minimizer of the least-square objective, L2(y) =\nEx~x[(y - x)\u00b2], 7-expectile of X, ET [X], can be defined as a minimizer of the asymmetric least-square objective:\n$L_\\tau(y) = \\mathbb{E}_{x\\sim X} [|\\tau - 1(y > x)| \\cdot (y - x)^2]$, (1)\nwhere |\u0442 - 1(y > x)| is an asymmetric weighting of L2 and 0 \u2264 t \u2264 1.\nWe refer a \u0442-expectile with \u315c < 0.5 as a lower expectile of X. When \u315c < 0.5, the objective assigns a high weight 1 7 for smaller x and a low weight + for bigger x. Thus, minimizing the objective with T < 0.5 leads to a conservative statistical estimate compared to the expectation."}, {"title": "Approach", "content": "The primary limitation for model-based offline RL in solving long-horizon tasks is inherent errors in a world model and critic outside the offline data support. Conservative value estimation can effectively handle such (falsely optimistic) errors. Prior approaches estimate conservative values through diverse uncertainty penalties; but they are either unreliable [35] or computationally expensive [14].\nIn this paper, we introduce Lower Expectile Q-learning (LEQ), an efficient model-based offline RL method that achieves conservative value estimation via expectile regression of Q-values with lower expectiles when learning from model-generated data (Section 4.1). Additionally, we address the noisy value estimation problem in long-horizon tasks [23] using X-returns on 10-step imaginary rollouts (Section 4.2). Finally, we train a deterministic policy conservatively by maximizing the lower expectile of A-returns (Section 4.3). The overview of LEQ is described in Algorithm 1."}, {"title": "Lower expectile Q-learning", "content": "Most offline RL algorithms primarily focus on learning a conservative value function for out-of-distribution actions. In this paper, we propose Lower Expectile Q-learning (LEQ), which learns a conservative Q-function via expectile regression with small \u03c4, avoiding unreliable uncertainty estimation and exhaustive Q-value estimation."}, {"title": "Lower expectile Q-learning with A-return", "content": "To further improve LEQ for long-horizon tasks, we use A-return instead of 1-step return for Q-learning. A-return allows a Q-function and policy to learn from low-bias multi-step returns [28]. Reducing bias in value estimation with A-return is especially important on long-horizon tasks where values for nearby states are similar to each other, as illustrated in Figure 2.\nWe first define A-return of a trajectory 7 in timestep t, $Q_\\lambda^\\pi(\\tau)$, using N-step return, $G_{t:t+N}(\\tau)$:\n$G_{t:t+N}(\\tau) = \\sum_{i=0}^{N-1} \\gamma^i r(s_{t+i}, a_{t+i}) + \\gamma^N Q_{\\phi}(s_{t+N}, a_{t+N}) $ (8)\n$Q_\\lambda^\\pi(\\tau) = \\frac{1-\\lambda}{1-\\lambda^{H-t}} \\sum_{i=1}^{H-t} \\lambda^{i-1} G_{t:t+i}(\\tau)$. (9)\nThen, we can rewrite the Q-learning loss in Equation (4) with 1-step return to the one with A-return:\n$\\mathcal{L}_{Q, model}(\\phi) = \\mathbb{E}_{s_0 \\in D_{model}, \\mathcal{T} \\sim p_{\\psi}, \\pi_{\\theta}} \\sum_{t=0}^{H-1} L_2(Q_{\\phi}(s_t, \\pi_{\\theta}(s_t)) - Q_\\lambda^\\pi(\\tau))$. (10)"}, {"title": "Lower expectile policy learning with A-return", "content": "For policy optimization, we can use a deterministic policy a = \u03c0\u03bf(s) and update the policy using the deterministic policy gradients similar to DDPG [22].\u00b3 Instead of maximizing the immediate Q-value, Q(s, a), we propose to directly maximize the lower expectile of A-return, which is a more accurate learning target for a policy, analogous to the conservative critic target in Section 4.2:\n$\\mathcal{L}(\\theta) = - \\mathbb{E}_{s_0 \\in D_{model}, \\mathcal{T} \\sim p_{\\psi}, \\pi_{\\theta}} \\sum_{t=0}^{H} [\\tau - 1(Q_{\\phi}(s_t, a_t) > Q_\\lambda^\\pi(\\tau))]\\cdot Q_\\lambda^\\pi(\\tau)$. (11)\nHowever, due to the expectile term in Equation (11), computing the gradient of $\\mathcal{L}(\\theta)$ is not trivial. To estimate this gradient, we propose a differentiable surrogate loss, approximating $\\mathbb{E}_{\\tau \\sim \\rho_{\\psi}, \\pi_{\\theta}}[Q_\\lambda^\\pi(\\tau)]$ in Equation (11) with Q$(st, at)$:\n$\\mathcal{L}(\\theta) = - \\mathbb{E}_{s_0 \\in D_{model}, \\mathcal{T} \\sim p_{\\psi}, \\pi_{\\theta}} \\sum_{t=0}^{H} |\\tau - 1(Q_{\\phi}(s_t, a_t) > Q_\\lambda^\\pi(\\tau))|\\cdot Q_{\\lambda}^{\\pi}(\\tau)$. (12)"}, {"title": "Expanding dataset with model rollouts", "content": "One of the problem of offline RL is that data distribution is limited to the offline dataset Deny. To tackle this problem, we can simulate the current policy inside the model and use the generated trajectories to expand the dataset, similar to prior works [35, 30]. However, the state coverage will be identical when the policy converges, which might lead to catastrophic forgetting.\nTo prevent this issue, we expand the dataset using the model rollouts from a noisy exploration policy. Specifically, we execute the exploration policy #exp(\u00b7 | s), which simply adds noise \u03b5 ~ N(0, 2xp) to the current policy \u03c0\u03bf(s), and generate a trajectory of length R (R = 5 in this paper). We refer this expanded dataset as Dmodel. Note that we do not use off-policy actions and rewards for critic and policy updates; instead, we generate H-step model rollouts starting from s ~ Dmodel and use them for training the policy and Q-function. Thus, we need to store only the states from the rollouts."}, {"title": "Experiments", "content": "In this paper, we propose a novel model-based offline RL method with simple and efficient yet accurate conservative value estimation. Through our experiments, we aim to answer the following questions: (1) Can LEQ solve long-horizon tasks? (2) How does LEQ perform in widely used offline RL benchmarks? (3) Which component enables model-based offline RL to learn the AntMaze tasks?"}, {"title": "Tasks", "content": "To show the strength of LEQ in solving long-horizon tasks, we use the AntMaze tasks, which aims to navigate a 8-DOF ant robot to the desired goal position, as shown in Figure 4. Specifically, we use umaze, medium, large datasets from D4RL [6], and ultra dataset from Jiang et al. [15]. Moreover, we evaluate our method on MuJoCo locomotion tasks (Figure 3) with dense rewards with D4RL [6] and NeoRL [26] datasets. Please refer to Appendix A for more experimental details."}, {"title": "Compared offline RL algorithms", "content": "We compare the performance of LEQ with the state-of-the-art offline RL algorithms. Please note that LEQ uses the same hyperparameters across all tasks, except the expectile parameter, \u0442.\nModel-free offline RL. We consider behavioral cloning (BC) [25]; TD3+BC [7], which combines BC loss to TD3; CQL [19], which penalizes the actions out of data distribution; and IQL [17], which utilizes expectile regression to estimate the value function. For locomotion tasks, we also compare with EDAC [1], which penalizes the Q-values according to the uncertainty of Q-functions\nModel-based offline RL. We consider MOPO [34] and MOBILE [30], which penalize Q-values according to the transition uncertainty and the bellman uncertainty of a world model, respectively; COMBO [35], which combines CQL with MBPO; RAMBO [27], which trains an adversarial world model against the policy; and CBOP [14], which utilizes multi-step returns for critic updates."}, {"title": "Results on long-horizon AntMaze tasks", "content": "As shown in Table 1, LEQ significantly outperforms the prior model-based approaches for all 8 datasets. LEQ achieves 58.6 and 60.2 for antmaze-large-play and antmaze-large-diverse, while the second best method, RAMBO [27], scores only 0.0 and 2.4, respectively. We believe these performance gains come from our conservative value estimation, which works more stable than the uncertainty-based penalization of prior works.\nMoreover, LEQ even significantly outperforms the model-free approaches in antmaze-umaze, antmaze-large, and antmaze-ultra. Despite its superior performance, LEQ often shows high variance during training, resulting in worse performance on antmaze-medium. Over the course of training, LEQ mostly achieves high success rates, but the evaluation results sometimes drops to 0% as shown in Appendix, Figure 5. We leave the problem of reducing the high variance of our method in certain environments as a future work."}, {"title": "Results on MuJoCo Gym locomotion tasks", "content": "For D4RL MuJoCo Gym tasks in Table 3, LEQ achieves comparable results with the best score of prior works in 6 out of 12 tasks. Furthermore, in Table 2, LEQ outperforms most of the prior works in the NeoRL benchmark, especially in the Hopper and Walker2d domains. These results show that LEQ serves as a general offline RL algorithm, not limited to long-horizon tasks.\nSimilar to antmaze-medium, LEQ also suffers from the high variance problem. During training, LEQ often achieves high performance, but then, suddenly falls back to 0, as shown in Appendix, Figure 5. This is mainly because the learned models sometimes fail to capture failures (e.g. hopper and walker falling off) and predict an optimistic future (e.g. hopper and walker walking forward)."}, {"title": "Ablation studies", "content": "To understand why LEQ (LEQ-\u5165) works well in long-horizon tasks, we conduct ablation studies and answer to the following four questions: (1) Does using A-return help? (2) Is LEQ better than prior uncertainty-based penalization methods? (3) Which factor enables LEQ to work in AntMaze? and (4) How do imagination length H and data expansion length R affect the performance?\n(1) A-returns. To verify the effect of A-return, we compare our method (LEQ-\u5165) with the versions with 1-step return (LEQ-1) and H-step return (LEQ-H). Table 4 shows that using A-return drastically improves the performance on AntMaze compared to using 1-step return or H-step return. This result is coherent with the observations in prior online RL methods [28, 11].\n(2) Lower expectile Q-learning. We compare our lower expectile Q-learning with another conservative value estimator, MOBIP used in MOBILE [30], which penalizes Q-values with the standard deviation of Q-ensemble networks. The only difference between LEQ and MOBIP is their target Q-value computation for both critic and policy updates. Table 4 shows that using MOBIP not only deteriorates the success rates (in MOBIP-1) but also does not benefit from A-return (in MOBIP-\u5165).\n(3) What makes offline model-based RL work in AntMaze? Prior to LEQ, none of offline model-based RL methods work in AntMaze, whereas our method even outperforms model-free methods. Thus, we investigate which changes in LEQ enable offline model-based RL work in AntMaze."}, {"title": "Conclusion", "content": "In this paper, we propose a novel offline model-based reinforcement learning method, LEQ, which uses expectile regression to get a conservative evaluation of a policy from model-generated trajectories. Expectile regression eases the pain of constructing the whole distribution of Q-targets and allows for estimating the conservative value via sampling. Combined with A-returns in both critic and policy updates for the imaginary rollouts, the policy can receive learning signals that are more robust to both model errors and critic errors. We empirically show that LEQ improves the performance in various tasks - especially, achieving the state-of-the-art performance in the long-horizon AntMaze tasks."}, {"title": "Limitations", "content": "Following prior work on model-based offline RL [30, 14], we assume access to the ground-truth termination function of a task, different from online model-based RL approaches, which learn a termination function from interactions. However, since this termination function is conditioned on a state, a model requires to plan on a state space (or an observation space), which could be challenging in a high-dimensional state space (e.g. pixel observations). Extending the proposed approach to complex environments with high-dimensional observations would be an immediate next step."}, {"title": "Broader Impacts", "content": "Our method aims to increase the ability of autonomous agents, such as robots and self-driving cars, to learn from static, offline data without interacting with the world. This enables autonomous agents to utilize data with diverse qualities (not necessarily from experts). We believe that this paper does not have any immediate negative societal impact."}, {"title": "Training Details", "content": "Computing resources. All experiments are done on a single RTX 4090 GPU and 4 AMD EPYC 9354 CPU cores. We use 5 different random seeds for each experiment and report the mean and standard deviation. Each offline RL experiment takes 2 hours for ours, 12 hours for MOBILE, and 24 hours for C\u0412\u041e\u0420.\nEnvironment details. For the locomotion tasks, we use the dataset provided by D4RL [6] and NeoRL [26]. Following IQL [17], we normalize rewards using the maximum and minimum return of all trajectories. We use the true termination functions of the environments, implemented in MOBILE [30].\nFor the AntMaze tasks, we use the dataset provided by D4RL [6]. Following IQL [17], we subtract 1 from the rewards in the datasets so that the agent receives \u20131 for each step and 0 on termination. We use the true termination functions of the environments. The termination functions of the AntMaze tasks are not deterministic because a goal of a maze is randomized every time the environment is reset. Nevertheless, we follow the implementation of CBOP [14], where the termination region is set to a circle around the mean of the goal distribution with the radius 0.5.\nMethod implementation details. For all compared methods, we use the results from their corresponding papers when available. For IQL [17], we run the official implementation with 5 seeds to reproduce the results for the random datasets in D4RL and NeoRL. For the AntMaze tasks, we run the official implementation of MOBILE and CBOP with 5 random seeds. Please note that the original MOBILE implementation does not use the true termination function, so we replace it with our termination function. For MOPO, COMBO, and RAMBO, we use the results reported in RAMBO [27].\nWorld models. For training world models, we use the architecture and training script from OfflineRL-Kit [29], matching the implementation of MOBILE [30]. Each world model is implemented as a 4-layer MLPs with the hidden layer size of 200. We construct an ensemble of world models by selecting 5 out of 7 models with the best validation scores. We pretrain the ensemble of world models for each of 5 random seeds (i.e. training in total 35 world models and using 25 models), which takes approximately 5 hours in average.\nPolicy and critic networks. We use 3-layer MLPs with size of 256 both for the policy network and the critic network. We use layer normalization [3] to prevent catastrophic over/underestimation [4], and squash the state inputs using symlog to keep training stable from outliers in long-horizon model rollouts [11].\nPretraining. For some environments, we found that a randomly initialized policy can lead to abnormal rewards or transition prediction from the world models in the early stage, leading to unstable training. Following CBOP [14], we pretrain a policy \u03c0\u03b8 and a critic Q using behavioral cloning and FQE [20], respectively. We use a slightly different implementation of FQE from the original implementation, where the arg min operation is approximated with mini-batch gradient descent, similar to standard Q-learning as shown in Algorithm 2."}, {"title": "More Results", "content": "High variance in locomotion tasks. When we train LEQ in locomotion tasks, we observe that our method often achieves 100% success rates and then falls back to 0%, as shown in Figure 5. This is mainly because the learned models sometimes fail to capture failures (e.g. hopper and walker falling off) and predict an optimistic future (e.g. hopper and walker walking forward).\nResults for all expectiles T. To give insights how the expectile parameter 7 affects the performance of LEQ, we report the performance of LEQ with all expectile values {0.1, 0.3, 0.4, 0.5}. The expectile parameter 7 has a trade-off \u2013 high expectile makes the model's predictions less conservative while making a policy easily exploit the model. We recommend first trying T 0.1, which works well for most of the tasks, and increase T until the performance starts to drop."}, {"title": "Proof of the Policy Objective", "content": "We show that the surrogate loss in Equation (12) leads to a better approximation for the expectile of A-returns in Equation (11) than maximizing Q\u2084(s", "objective": "n$\\widehat{J"}, "lambda(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_{\\psi}, \\pi_{\\theta}} [(W^\\tau (Q_{\\phi}(s_t, a_t) > Q_\\lambda^\\pi(\\tau)))Q_\\lambda^\\pi(\\tau)]$, (14)\nleads to optimizing a lower-bias estimator of $\\mathbb{E}_{\\tau \\sim \\rho_{\\psi}, \\pi_{\\theta}}[Q_\\lambda^\\pi(\\tau)]$ than $Q_{\\phi}(s_t, a_t)$.\nTo show this, we first prove that $\\widehat{Y}_{new} = \\frac{\\mathbb{E}[W^\\tau(Q_{\\phi}(s_t, a_t) > Q_\\lambda^\\pi(\\tau)) \\cdot Q_\\lambda^\\pi(\\tau)]}{\\mathbb{E}[W^\\tau(Q_{\\phi}(s_t, a_t) > Q_\\lambda^\\pi(\\tau))]}$ is closer to $\\mathbb{E}_{\\tau \\sim \\rho_{\\psi}, \\pi_{\\theta}}[Q_\\lambda^\\pi(\\tau)]$ than $Q_{\\phi}(s_t, a_t)$. For deriving the proof, we generalize this situation to have an arbitrary distribution X and estimation \u0176, which corresponds to $X = Q_\\lambda^\\pi(\\tau)$, $\\widehat{Y} = Q_{\\phi}(s, a)$.\nTheorem 1. Let X be a distribution and $Y = \\mathbb{E}^\\tau [X]$ be a lower expectile of X (i.e. 0 <\u0442 < 0.5). Let \u0176 be an arbitrary estimation of Y, and define $\\mathbb{W}^\\tau (\\cdot) = |\\tau - 1(\\cdot)|$. If we let $\\widehat{Y}_{new} = \\frac{\\mathbb{E}[\\mathbb{W}^\\tau (\\widehat{Y} > X) \\cdot X]}{\\mathbb{E}[\\mathbb{W}^\\tau (\\widehat{Y} > X)]}$ be a new estimation of Y, then $|Y - \\widehat{Y}_{new}| \\leq \\frac{1-2\\tau}{1-\\tau} \\cdot p(Y \\leq X \\leq \\widehat{Y}) \\cdot |Y - \\widehat{Y}|$.\nProof. Without loss of generality, we assume $\\widehat{Y} > Y$. Then, we have $\\widehat{Y}_{new} > Y$. Thus,\n$\\widehat{Y}_{new} - Y$\n$\\widehat{Y}_{new} - Y$\n$\\frac{\\mathbb{E}[\\mathbb{W}^\\tau(\\widehat{Y} > X) \\cdot X]}{\\mathbb{E}[\\mathbb{W}^\\tau(\\widehat{Y} > X)]} - Y$\n$\\frac{\\mathbb{E}[\\mathbb{W}^\\tau(\\widehat{Y} > X) \\cdot X]}{\\mathbb{E}[\\mathbb{W}^\\tau(\\widehat{Y} > X)]} - \\frac{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X) \\cdot X]}{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)]}$\n(. Def. of $\\widehat{Y}_{new}$ and Y)\n$\\frac{\\mathbb{E}[\\mathbb{W}^\\tau(\\widehat{Y} > X) \\cdot X]}{\\mathbb{E}[\\mathbb{W}^\\tau(\\widehat{Y} > X)]} - \\frac{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X) \\cdot X]}{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)]}$\n$\\frac{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X) \\cdot X] + \\mathbb{E}[(1-2\\tau) \\cdot 1(Y \\leq X \\leq \\widehat{Y})X] - \\frac{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X) \\cdot X]}{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)]+ \\mathbb{E}[(1-2\\tau) \\cdot 1(Y \\leq X \\leq \\widehat{Y})]}$\n(. Def. of $\\mathbb{W}^\\tau (\\cdot)$)\n$\\frac{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X) \\cdot X] + \\mathbb{E}[(1-2\\tau) \\cdot 1(Y \\leq X \\leq \\widehat{Y})X] - \\frac{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X) \\cdot X]}{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)]+ \\mathbb{E}[(1-2\\tau) \\cdot 1(Y \\leq X \\leq \\widehat{Y})]}$\n$\\frac{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X) \\cdot X](\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)] + \\mathbb{E}[(1-2\\tau) \\cdot 1(Y \\leq X \\leq \\widehat{Y})]) - \\mathbb{E}[\\mathbb{W}^\\tau(Y > X) \\cdot X] (\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)])}{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)](\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)] + \\mathbb{E}[(1-2\\tau) \\cdot 1(Y \\leq X \\leq \\widehat{Y})])}$\n$\\frac{\\frac{1-2\\tau}{\\tau}}{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)]}\\cdot (\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)]\\mathbb{E}[1(Y \\leq X \\leq \\widehat{Y})X] - \\mathbb{E}[\\mathbb{W}^\\tau(Y > X)X]\\mathbb{E}[1(Y \\leq X \\leq \\widehat{Y})\\frac{\\mathbb{E}[1(Y \\leq X \\leq \\widehat{Y})X] - Yp(Y \\leq X \\leq \\widehat{Y})}{\\frac{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)] + \\mathbb{E}[(1-2\\tau) \\cdot 1(Y \\leq X \\leq \\widehat{Y})]}{1-\\frac{1-2\\tau}{\\tau}} (\\mathbb{E}_{Y \\leq X \\leq \\widehat{Y}} [X] - Y)}$\n$\\frac{\\frac{1-2\\tau}{\\tau}p(Y \\leq X \\leq \\widehat{Y})}{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)]} (\\mathbb{E}_{Y \\leq X \\leq \\widehat{Y}} [X] - Y)$\n$\\frac{\\frac{1-2\\tau}{\\tau}p(Y \\leq X \\leq \\widehat{Y}) (\\widehat{Y} - Y)}{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)]}$\n$\\frac{\\frac{1-2\\tau}{\\tau}}{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)]} \\cdot p(Y \\leq X \\leq \\widehat{Y}) \\cdot |\\widehat{Y} - Y|$\nNote that this theorem shows that the bias of the new estimation is always smaller than the original estimation, since $\\frac{\\frac{1-2\\tau}{\\tau}}{\\mathbb{E}[\\mathbb{W}^\\tau(Y > X)]} < 1$ and $p(Y \\leq X \\leq \\widehat{Y}) \\leq 1$. If we plug in the distribution of $Q_\\lambda^\\pi(\\tau)$ to X and $Y = Q_{\\phi}(s_t, a_t)$, then $Y = \\mathbb{E} [X] = \\mathbb{E}_{\\tau \\sim \\rho_{\\psi}, \\pi_{\\theta}}[Q_\\lambda^\\pi(\\tau)]$, and we can show the desired result using the theorem: $\\widehat{Y}_{new} = \\frac{\\mathbb{E}[\\mathbb{W}^\\tau(Q_{\\phi}(s_t, a_t) > Q_\\lambda^\\pi(\\tau)) \\cdot Q_\\lambda^\\pi(\\tau)]}{\\mathbb{E}[\\mathbb{W}^\\tau(Q_{\\phi}(s_t, a_t) > Q_\\lambda^\\pi(\\tau))]}$ is closer to $\\mathbb{E}_{\\tau \\sim \\rho_{\\psi}, \\pi_{\\theta}}[Q_\\lambda^\\pi(\\tau)]$ than $Q_{\\phi}(s_t, a_t)$.\nHere, the normalizing factor $\\mathbb{E}[\\mathbb{W}^\\tau (Q_{\\phi}(s_t, a_t) > Q_\\lambda^\\pi(\\tau))]$ is non-differentiable with \u03c4. Specifically, the gradient is 0 everywhere (except $Q_{\\phi}(s_t, a_t) = Q_\\lambda^\\pi(\\tau)$). Thus, if we calculate the gradient of $\\widehat{Y}_{new}$, the gradient for the normalizing factor disappears.```json\n{\n \"title\": \"Tackling Long-Horizon Tasks with Model-based Offline Reinforcement Learning\",\n \"authors\": [\n  \"Kwanyoung Park\",\n  \"Youngwoon Lee\"\n ],\n \"abstract\":", "Model-based offline reinforcement learning (RL) is a compelling approach that addresses the challenge of learning from limited, static data by generating imaginary trajectories using learned models. However, it falls short in solving long-horizon tasks due to high bias in value estimation from model rollouts. In this paper, we introduce a novel model-based offline RL method, Lower Expectile Q-learning (LEQ), which enhances long-horizon task performance by mitigating the high bias in model-based value estimation via expectile regression of A-returns. Our empirical results show that LEQ significantly outperforms previous model-based offline RL methods on long-horizon tasks, such as the D4RL AntMaze tasks, matching or surpassing the performance of model-free approaches. Our experiments demonstrate that expectile regression, A-returns, and critic training on offline data are all crucial for addressing long-horizon tasks. Additionally, LEQ achieves performance comparable to the state-of-the-art model-based and model-free offline RL methods on the NeoRL benchmark and the D4RL MuJoCo Gym tasks.", "sections\": [\n  {\n   \"title\": \"Introduction", "content\": \"One of the major challenges in offline reinforcement learning (RL) is the overestimation of values for out-of-distribution actions due to the lack of environment interactions [21, 19]. Model-based offline RL addresses this issue by generating additional (imaginary) training data using a learned model, thereby augmenting the given offline data with synthetic experiences that cover out-of-distribution states and actions [34, 16, 35, 2, 30]. While these approaches have demonstrated strong performance in simple, short-horizon tasks, they struggle with noisy model predictions and value estimations, particularly in long-horizon tasks [23]. This challenge is evident in their poor performances (i.e. near zero) on the D4RL AntMaze tasks [6, 15].\nTypical model-based offline RL methods alleviate the inaccurate value estimation problem (mostly overestimation) by penalizing Q-values estimated from model rollouts with uncertainties in model predictions [34, 16] or value predictions [30, 14]. While these penalization terms prevent a policy from exploiting erroneous value estimations, the policy now does not maximize the true value, but maximizes the value penalized by heuristically estimated uncertainties, which can lead to sub-optimal behaviors. This is especially problematic in long-horizon, sparse-reward tasks, where Q-values are similar across nearby states [23].\nAnother way to reduce bias in value estimates is by using multi-step returns [31, 12]. \u0421\u0412\u041e\u0420 [14] constructs an explicit distribution of multi-step Q-values from thousands of model rollouts and uses this value as a target for training the Q-function. However, CBOP is computationally expensive for estimating a target value and uses multi-step returns solely for Q-learning, which provides insufficient learning signals for obtaining long-horizon behaviors."], "content": "Offline RL [21] aims to solve a reinforcement learning problem only with pre-collected datasets, better than behavioral cloning policies [25]. One can simply apply off-policy RL algorithms on top of the fixed dataset. However, off-policy RL methods suffer from the overestimation of Q-values for actions unseen in the offline dataset [8, 18, 19], since an overestimated value function cannot get corrected through online environment interactions in offline RL.\nModel-free offline RL algorithms have addressed this value overestimation problem on out-of-distribution actions by (1) regularizing a policy to only output actions in the offline data [24, 17, 7] or (2) adopting a conservative value estimation for executing actions different from the dataset [19, 1]. Despite their strong performances on the standard offline RL benchmarks, model-free offline RL policies tend to be constrained to the support of the data (i.e. state-action pairs in the offline dataset), which may lead to limited generalization capability.\nModel-based offline RL approaches have tried to overcome this limitation by suggesting a better use of the limited offline data \u2013 learning a world model and generating imaginary data with the learned model that covers out-of-distribution actions. Similar to Dyna-style online model-based RL [32, 9-11], an offline model-based RL policy can be trained on both offline data and model rollouts. But, again, learned models may be inaccurate on states and actions outside the data support, making a policy easily exploit the learned models.\nRecent model-based offline RL algorithms have adopted the conservatism idea from model-free offline RL, penalizing policies incurring (1) uncertain transition dynamics [34, 16, 35] or (2) uncertain value estimation [30, 14]. This conservative use of model-generated data enables model-based offline RL to outperform model-free offline RL in widely used offline RL benchmarks [30]. However, uncertainty estimation is difficult and often inaccurate [35]. Instead of relying on such heuristic [34, 16, 30] or expensive [14] uncertainty estimation, we propose to learn a conservative value function via expectile regression with a small \u03c4, which is simple, efficient, yet effective."}, {"title": "Preliminaries", "content": "Problem setup. We formulate our problem as a Markov Decision Process (MDP) defined as a tuple, M = (S, A,r,p, \u03c1, \u03b3) [33]. S and A denote the state and action spaces, respectively. r : S \u00d7 A \u2192 R denotes the reward function. p : S \u00d7 A \u2192 \u2206(S)\u00b9 denotes the transition dynamics. p(so) \u2208 \u0394(S) denotes the initial state distribution and y is a discounting factor. The goal of reinforcement learning (RL) is to find a policy, \u03c0 : S \u2192 \u2206(A), that maximizes the expected return,\n$\\mathbb{E}_{\\tau \\sim p(\\tau; \\pi, \\rho)} [\\sum_{t=0}^{T-1} \\gamma^t r(s_t, a_t)]$, where is a sequence of transitions with a finite horizon T,\n\u03c4 = (So, ao, ro, S1, a1, r1, ..., ST), following \u03c0(at | st) and p(St+1 | St, at) starting from so ~ p(.).\nIn this paper, we consider the offline RL setup [21], where a policy \u03c0 is trained with a fixed given offline dataset, Denv = {T1, T2, ..., TN }, without any additional online interactions.\nModel-based offline RL. As an offline RL policy is trained from a fixed dataset, one of the major challenges in offline RL is the limited data support; thus, lack of generalization to out-of-distribution states and actions. Model-based offline RL [16, 34, 35, 27, 30, 14] tackles this problem by augmenting the training data with imaginary training data (i.e. model rollouts) generated from the learned transition dynamics and reward model, py(St+1,r | St, at).\nThe typical process of model-based offline RL is as follows: (1) pretrain a model (or an ensemble of models) and an initial policy from the offline data, (2) generate short imaginary rollouts {T} using the pretrained model and add them to the training dataset Dmodel \u2190 Dmodel \u222a {T}, (3) perform an offline RL algorithm on the augmented dataset Dmodel U Deny, and repeat (2) and (3).\nExpectile regression. Expectile is a generalization of the expectation of a distribution X. While the expectation of X, E[X], can be viewed as a minimizer of the least-square objective, L2(y) =\nEx~x[(y - x)\u00b2], 7-expectile of X, ET [X], can be defined as a minimizer of the asymmetric least-square objective:\n$L_\\tau(y) = \\mathbb{E}_{x\\sim X} [|\\tau - 1(y > x)| \\cdot (y - x)^2]$, (1)\nwhere |\u0442 - 1(y > x)| is an asymmetric weighting of L2 and 0 \u2264 t \u2264 1.\nWe refer a \u0442-expectile with \u315c < 0.5 as a lower expectile of X. When \u315c < 0.5, the objective assigns a high weight 1 7 for smaller x and a low weight + for bigger x. Thus, minimizing the objective with T < 0.5 leads to a conservative statistical estimate compared to the expectation."}, {"title": "Approach", "content": "The primary limitation for model-based offline RL in solving long-horizon tasks is inherent errors in a world model and critic outside the offline data support. Conservative value estimation can effectively handle such (falsely optimistic) errors. Prior approaches estimate conservative values through diverse uncertainty penalties; but they are either unreliable [35] or computationally expensive [14].\nIn this paper, we introduce Lower Expectile Q-learning (LEQ), an efficient model-based offline RL method that achieves conservative value estimation via expectile regression of Q-values with lower expectiles when learning from model-generated data (Section 4.1). Additionally, we address the noisy value estimation problem in long-horizon tasks [23] using X-returns on 10-step imaginary rollouts (Section 4.2). Finally, we train a deterministic policy conservatively by maximizing the lower expectile of A-returns (Section 4.3). The overview of LEQ is described in Algorithm 1."}, {"title": "Lower expectile Q-learning", "content": "Most offline RL algorithms primarily focus on learning a conservative value function for out-of-distribution actions. In this paper, we propose Lower Expectile Q-learning (LEQ), which learns a conservative Q-function via expectile regression with small \u03c4, avoiding unreliable uncertainty estimation and exhaustive Q-value estimation."}, {"title": "Lower expectile Q-learning with A-return", "content": "To further improve LEQ for long-horizon tasks, we use A-return instead of 1-step return for Q-learning. A-return allows a Q-function and policy to learn from low-bias multi-step returns [28]. Reducing bias in value estimation with A-return is especially important on long-horizon tasks where values for nearby states are similar to each other, as illustrated in Figure 2.\nWe first define A-return of a trajectory 7 in timestep t, $Q_\\lambda^\\pi(\\tau)$, using N-step return, $G_{t:t+N}(\\tau)$:\n$G_{t:t+N}(\\tau) = \\sum_{i=0}^{N-1} \\gamma^i r(s_{t+i}, a_{t+i}) + \\gamma^N Q_{\\phi}(s_{t+N}, a_{t+N}) $ (8)\n$Q_\\lambda^\\pi(\\tau) = \\frac{1-\\lambda}{1-\\lambda^{H-t}} \\sum_{i=1}^{H-t} \\lambda^{i-1} G_{t:t+i}(\\tau)$. (9)\nThen, we can rewrite the Q-learning loss in Equation (4) with 1-step return to the one with A-return:\n$\\mathcal{L}_{Q, model}(\\phi) = \\mathbb{E}_{s_0 \\in D_{model}, \\mathcal{T} \\sim p_{\\psi}, \\pi_{\\theta}} \\sum_{t=0}^{H-1} L_2(Q_{\\phi}(s_t, \\pi_{\\theta}(s_t)) - Q_\\lambda^\\pi(\\tau))$. (10)"}, {"title": "Lower expectile policy learning with A-return", "content": "For policy optimization, we can use a deterministic policy a = \u03c0\u03bf(s) and update the policy using the deterministic policy gradients similar to DDPG [22].\u00b3 Instead of maximizing the immediate Q-value, Q(s, a), we propose to directly maximize the lower expectile of A-return, which is a more accurate learning target for a policy, analogous to the conservative critic target in Section 4.2:\n$\\mathcal{L}(\\theta) = - \\mathbb{E}_{s_0 \\in D_{model}, \\mathcal{T} \\sim p_{\\psi}, \\pi_{\\theta}} \\sum_{t=0}^{H} [\\tau - 1(Q_{\\phi}(s_t, a_t) > Q_\\lambda^\\pi(\\tau))]\\cdot Q_\\lambda^\\pi(\\tau)$. (11)\nHowever, due to the expectile term in Equation (11), computing the gradient of $\\mathcal{L}(\\theta)$ is not trivial. To estimate this gradient, we propose a differentiable surrogate loss, approximating $\\mathbb{E}_{\\tau \\sim \\rho_{\\psi}, \\pi_{\\theta}}[Q_\\lambda^\\pi(\\tau)]$ in Equation (11) with Q$(st, at)$:\n$\\mathcal{L}(\\theta) = - \\mathbb{E}_{s_0 \\in D_{model}, \\mathcal{T} \\sim p_{\\psi}, \\pi_{\\theta}} \\sum_{t=0}^{H} |\\tau - 1(Q_{\\phi}(s_t, a_t) > Q_\\lambda^\\pi(\\tau))|\\cdot Q_{\\lambda}^{\\pi}(\\tau)$. (12)"}, {"title": "Expanding dataset with model rollouts", "content": "One of the problem of offline RL is that data distribution is limited to the offline dataset Deny. To tackle this problem, we can simulate the current policy inside the model and use the generated trajectories to expand the dataset, similar to prior works [35, 30]. However, the state coverage will be identical when the policy converges, which might lead to catastrophic forgetting.\nTo prevent this issue, we expand the dataset using the model rollouts from a noisy exploration policy. Specifically, we execute the exploration policy #exp(\u00b7 | s), which simply adds noise \u03b5 ~ N(0, 2xp) to the current policy \u03c0\u03bf(s), and generate a trajectory of length R (R = 5 in this paper). We refer this expanded dataset as Dmodel. Note that we do not use off-policy actions and rewards for critic and policy updates; instead, we generate H-step model rollouts starting from s ~ Dmodel and use them for training the policy and Q-function. Thus, we need to store only the states from the rollouts."}, {"title": "Experiments", "content": "In this paper, we propose a novel model-based offline RL method with simple and efficient yet accurate conservative value estimation. Through our experiments, we aim to answer the following questions: (1) Can LEQ solve long-horizon tasks? (2) How does LEQ perform in widely used offline RL benchmarks? (3) Which component enables model-based offline RL to learn the AntMaze tasks?"}, {"title": "Tasks", "content": "To show the strength of LEQ in solving long-horizon tasks, we use the AntMaze tasks, which aims to navigate a 8-DOF ant robot to the desired goal position, as shown in Figure 4. Specifically, we use umaze, medium, large datasets from D4RL [6], and ultra dataset from Jiang et al. [15]. Moreover, we evaluate our method on MuJoCo locomotion tasks (Figure 3) with dense rewards with D4RL [6] and NeoRL [26] datasets. Please refer to Appendix A for more experimental details."}, {"title": "Compared offline RL algorithms", "content": "We compare the performance of LEQ with the state-of-the-art offline RL algorithms. Please note that LEQ uses the same hyperparameters across all tasks, except the expectile parameter, \u0442.\nModel-free offline RL. We consider behavioral cloning (BC) [25]; TD3+BC [7], which combines BC loss to TD3; CQL [19], which penalizes the actions out of data distribution; and IQL [17], which utilizes expectile regression to estimate the value function. For locomotion tasks, we also compare with EDAC [1], which penalizes the Q-values according to the uncertainty of Q-functions\nModel-based offline RL. We consider MOPO [34] and MOBILE [30], which penalize Q-values according to the transition uncertainty and the bellman uncertainty of a world model, respectively; COMBO [35], which combines CQL with MBPO; RAMBO [27], which trains an adversarial world model against the policy; and CBOP [14], which utilizes multi-step returns for critic updates."}, {"title": "Results on long-horizon AntMaze tasks", "content": "As shown in Table 1, LEQ significantly outperforms the prior model-based approaches for all 8 datasets. LEQ achieves 58.6 and 60.2 for antmaze-large-play and antmaze-large-diverse, while the second best method, RAMBO [27], scores only 0.0 and 2.4, respectively. We believe these performance gains come from our conservative value estimation, which works more stable than the uncertainty-based penalization of prior works.\nMoreover, LEQ even significantly outperforms the model-free approaches in antmaze-umaze, antmaze-large, and antmaze-ultra. Despite its superior performance, LEQ often shows high variance during training, resulting in worse performance on antmaze-medium. Over the course of training, LEQ mostly achieves high success rates, but the evaluation results sometimes drops to 0% as shown in Appendix, Figure 5. We leave the problem of reducing the high variance of our method in certain environments as a future work."}, {"title": "Results on MuJoCo Gym locomotion tasks", "content": "For D4RL MuJoCo Gym tasks in Table 3, LEQ achieves comparable results with the best score of prior works in 6 out of 12 tasks. Furthermore, in Table 2, LEQ outperforms most of the prior works in the NeoRL benchmark, especially in the Hopper and Walker2d domains. These results show that LEQ serves as a general offline RL algorithm, not limited to long-horizon tasks.\nSimilar to antmaze-medium, LEQ also suffers from the high variance problem. During training, LEQ often achieves high performance, but then, suddenly falls back to 0, as shown in Appendix, Figure 5. This is mainly because the learned models sometimes fail to capture failures (e.g. hopper and walker falling off) and predict an optimistic future (e.g. hopper and walker walking forward)."}, {"title": "Ablation studies", "content": "To understand why LEQ (LEQ-\u5165) works well in long-horizon tasks, we conduct ablation studies and answer to the following four questions: (1) Does using A-return help? (2) Is LEQ better than prior uncertainty-based penalization methods? (3) Which factor enables LEQ to work in AntMaze? and (4) How do imagination length H and data expansion length R affect the performance?\n(1) A-returns. To verify the effect of A-return, we compare our method (LEQ-\u5165) with the versions with 1-step return (LEQ-1) and H-step return (LEQ-H). Table 4 shows that using A-return drastically improves the performance on AntMaze compared to using 1-step return or H-step return. This result is coherent with the observations in prior online RL methods [28, 11].\n(2) Lower expectile Q-learning. We compare our lower expectile Q-learning with another conservative value estimator, MOBIP used in MOBILE [30], which penalizes Q-values with the standard deviation of Q-ensemble networks. The only difference between LEQ and MOBIP is their target Q-value computation for both critic and policy updates. Table 4 shows that using MOBIP not only deteriorates the success rates (in MOBIP-1) but also does not benefit from A-return (in MOBIP-\u5165).\n(3) What makes offline model-based RL work in AntMaze? Prior to LEQ, none of offline model-based RL methods work in AntMaze, whereas our method even outperforms model-free methods. Thus, we investigate which changes in LEQ enable offline model-based RL work in AntMaze."}, {"title": "Conclusion", "content": "In this paper, we propose a novel offline model-based reinforcement learning method, LEQ, which uses expectile regression to get a conservative evaluation of a policy from model-generated trajectories. Expectile regression eases the pain of constructing the whole distribution of Q-targets and allows for estimating the conservative value via sampling. Combined with A-returns in both critic and policy updates for the imaginary rollouts, the policy can receive learning signals that are more robust to both model errors and critic errors. We empirically show that LEQ improves the performance in various tasks - especially, achieving the state-of-the-art performance in the long-horizon AntMaze tasks."}, {"title": "Limitations", "content": "Following prior work on model-based offline RL [30, 14], we assume access to the ground-truth termination function of a task, different from online model-based RL approaches, which learn a termination function from interactions. However, since this termination function is conditioned on a state, a model requires to plan on a state space (or an observation space), which could be challenging in a high-dimensional state space (e.g. pixel observations). Extending the proposed approach to complex environments with high-dimensional observations would be an immediate next step."}, {"title": "Broader Impacts", "content": "Our method aims to increase the ability of autonomous agents, such as robots and self-driving cars, to learn from static, offline data without interacting with the world. This enables autonomous agents to utilize data with diverse qualities (not necessarily from experts). We believe that this paper does not have any immediate negative societal impact."}, {"title": "Training Details", "content": "Computing resources. All experiments are done on a single RTX 4090 GPU and 4 AMD EPYC 9354 CPU cores. We use 5 different random seeds for each experiment and report the mean and standard deviation. Each offline RL experiment takes 2 hours for ours, 12 hours for MOBILE, and 24 hours for C\u0412\u041e\u0420.\nEnvironment details. For the locomotion tasks, we use the dataset provided by D4RL [6] and NeoRL [26]. Following IQL [17], we normalize rewards using the maximum and minimum return of all trajectories. We use the true termination functions of the environments, implemented in MOBILE [30].\nFor the AntMaze tasks, we use the dataset provided by D4RL [6]. Following IQL [17], we subtract 1 from the rewards in the datasets so that the agent receives \u20131 for each step and 0 on termination. We use the true termination functions of the environments. The termination functions of the AntMaze tasks are not deterministic because a goal of a maze is randomized every time the environment is reset. Nevertheless, we follow the implementation of CBOP [14], where the termination region is set to a circle around the mean of the goal distribution with the radius 0.5.\nMethod implementation details. For all compared methods, we use the results from their corresponding papers when available. For IQL [17], we run the official implementation with 5 seeds to reproduce the results for the random datasets in D4RL and NeoRL. For the AntMaze tasks, we run the official implementation of MOBILE and CBOP with 5 random seeds. Please note that the original MOBILE implementation does not use the true termination function, so we replace it with our termination function. For MOPO, COMBO, and RAMBO, we use the results reported in RAMBO [27].\nWorld models. For training world models, we use the architecture and training script from OfflineRL-Kit [29], matching the implementation of MOBILE [30]. Each world model is implemented as a 4-layer MLPs with the hidden layer size of 200. We construct an ensemble of world models by selecting 5 out of 7 models with the best validation scores. We pretrain the ensemble of world models for each of 5 random seeds (i.e. training in total 35 world models and using 25 models), which takes approximately 5 hours in average.\nPolicy and critic networks. We use 3-layer MLPs with size of 256 both for the policy network and the critic network. We use layer normalization [3] to prevent catastrophic over/underestimation [4], and squash the state inputs using symlog to keep training stable from outliers in long-horizon model rollouts [11].\nPretraining. For some environments, we found that a randomly initialized policy can lead to abnormal rewards or transition prediction from the world models in the early stage, leading to unstable training. Following CBOP [14], we pretrain a policy \u03c0\u03b8 and a critic Q using behavioral cloning and FQE [20], respectively. We use a slightly different implementation of FQE from the original implementation, where the arg min operation is approximated with mini-batch gradient descent, similar to standard Q-learning as shown in Algorithm 2."}, {"title": "More Results", "content": "High variance in locomotion tasks. When we train LEQ in locomotion tasks, we observe that our method often achieves 100% success rates and then falls back to 0%, as shown in Figure 5. This is mainly because the learned models sometimes fail to capture failures (e.g. hopper and walker falling off) and predict an optimistic future (e.g. hopper and walker walking forward).\nResults for all expectiles T. To give insights how the expectile parameter 7 affects the performance of LEQ, we report the performance of LEQ with all expectile values {0.1, 0.3, 0.4, 0.5}. The expectile parameter 7 has a trade-off \u2013 high expectile makes the model's predictions less conservative while making a policy easily exploit the model. We recommend first trying T 0.1, which works well for most of the tasks, and increase T until the performance starts to drop."}, {"title": "Proof of the Policy Objective", "content": "We show that the surrogate loss in Equation (12) leads to a better approximation for the expectile of A-returns in Equation (11) than maximizing Q\u2084(s", "objective": "n$\\widehat{J"}, {"theorem": "widehat{Y}_{new} = \\frac{\\mathbb{E}[\\mathbb{W}^\\tau(Q_{\\phi}(s_t, a_t) > Q_\\lambda^\\pi(\\tau)) \\cdot Q_\\lambda^\\pi(\\tau)]}{\\mathbb{E}[\\mathbb{W}^\\tau(Q_{\\phi}(s_t, a_t) > Q_\\lambda^\\pi(\\tau))]}$ is closer to $\\mathbb{E}_{\\tau \\sim \\rho_{\\psi}, \\pi_{\\theta}}[Q_\\lambda^\\pi(\\tau)]$ than $Q_{\\phi}(s_t, a_t)$.\nHere, the normalizing factor $\\mathbb{E}[\\mathbb{W}^\\tau (Q_{\\phi}(s_t, a_t) > Q_\\lambda^\\pi(\\tau))]$ is non-differentiable with \u03c4. Specifically, the gradient is 0 everywhere (except $Q_{\\phi}(s_t, a_t) = Q_\\lambda^\\pi(\\tau)$). Thus, if we calculate the gradient of $\\widehat{Y}_{new}$, the gradient for the normalizing factor disappears. Therefore, we can"}]