{"title": "Classification of Radiological Text in Small and Imbalanced Datasets in a Non-English Language", "authors": ["Vincent Beliveau", "Helene Kaas", "Martin Prener", "Claes N. LADEFOGED", "Desmond Elliott", "Gitte M. KNUDSEN", "Lars H. PINBORG", "Melanie Ganz"], "abstract": "Natural language processing (NLP) in the medical domain can underperform in real-world applications involving small datasets in a non-English language with few labeled samples and imbalanced classes. There is yet no consensus on how to approach this problem. We evaluated a set of NLP models including BERT-like transformers, few-shot learning with sentence transformers (SetFit), and prompted large language models (LLM), using three datasets of radiology reports on magnetic resonance images of epilepsy patients in Danish, a low-resource language. Our results indicate that BERT-like models pretrained in the target domain of radiology reports currently offer the optimal performances for this scenario. Notably, the SetFit and LLM models underperformed compared to BERT-like models, with LLM performing the worst. Importantly, none of the models investigated was sufficiently accurate to allow for text classification without any supervision. However, they show potential for data filtering, which could reduce the amount of manual labeling required.", "sections": [{"title": "1. Introduction", "content": "The increasing access to electronic health records (EHR) has opened unparalleled opportunities for the processing of big data in the medical domain. However, the information contained in EHR is largely unstructured or semi-structured, and further processing is required to obtain the desired information. In this context, a prominent recurring task is the extraction of relevant labels from medical texts associated with external data. This is particularly relevant in radiology where clinical findings present in images can be extracted from the matching reports written by radiologists. These features can then be used in correspondence with the images, for example when creating labeled image data sets for image classification tasks. Labeling medical reports can be very time-consuming and, depending on the context, substantial efforts may be required even to create relatively small datasets. Furthermore, many pathologies have a low prevalence and will result in datasets with highly imbalanced classes. On a large scale, manually performing this type of labeling task is intractable, and automated methods are therefore required.\nPractical applications of natural language processing (NLP) in the medical domain can suffer from compounded issues, including non-English language, a small number of labeled samples, and class imbalance. These factors can all adversely impact the performance of NLP models in unique ways and a reliable approach to jointly tackle these issues is yet to be determined. In this work, we focus on a realistic use case of labeling radiology reports of magnetic resonance images (MRI) in the Danish language in a cohort of epilepsy patients. Our primary goal is to evaluate the current state-of-the-art of NLP models in this context and provide a comparative baseline for researchers with similar tasks."}, {"title": "2. Related Works", "content": "The usefulness of NLP to extract information from radiological text is increasingly recognized and specialized models such as RadBERT (Yan et al., 2022) have been proposed as a general approach in the English language. Work utilizing these models has, for example, been applied to radiological descriptions of MRI and their results suggest that the automated large scale labeling of radiology reports in English is achievable with high accuracy (Wood et al., 2020).\nTo our knowledge, the application of NLP models to radiology reports beyond the English language, and especially in low-resource languages, remains limited. BERT-like models for text summarization in Japanese (Nishio et al., 2024) and multilingual support (English, Portuguese, and German) (Lindo et al., 2023) have been suggested to provide adequate performance. However, models for text classification in Polish (Obuchowski et al., 2023), French, and German (Mottin et al., 2023), have all shown reduced performance compared to their English counterpart. Despite their generally superior performance, large language models (LLM) have seen little application for radiological text in non-English language. Recent work by Matsuo et al. (2024) investigating the classification of radiological text in Japanese suggests that translating the text to English improves classification accuracy with a multilingual LLM (GPT3.5). We are not aware of published studies on radiology reports in a non-English language using sentence transformers."}, {"title": "3. Methods", "content": ""}, {"title": "3.1 Dataset", "content": "A dataset of 16,899 MRI reports in the Danish language describing the brain scans of 4,769 patients with ICD-10 code G40* (epilepsy) was obtained. Example of short and long MRI reports for epilepsy patients are given in Figures and 1 and 4. Additionally, a corpus of 1,2 million radiology reports in Danish were retrieved in bulk, irrespective of modality (MRI, computed tomography, X-ray, ultrasound), body parts, and disease, and used for pretraining the BERT-like models (see section 3.3.1). All radiology reports were retrieved from a centralized picture archiving and communication system at Rigshospitalet in the Capital Region of Denmark, and covered the period 2017-2022. This study was approved by the National Scientific Ethics Committee of Denmark [D1936897].\nThree types of abnormalities relevant to epilepsy were labeled in the MRI reports of epilepsy patients: focal cortical dysplasia (FCD) (n=1,122), mesial temporal sclerosis (MTS) (n=904), and hippocampal abnormalities (HA) (n=992). Reports with mention of the abnormalities were identified using regular expressions and manually labeled by a medical student (HK) under the supervision of an expert neurologist (LHP). The FCD dataset was also labeled by a second clinician (MP) to investigate inter-rater agreement. The FCD and MTS datasets represent cases where the radiologist described the presence or absence of a pathology directly, and is often associated with a degree of certainty. To account for the variable degree of confidence, the prefixes negative, probable, highly probable (only for FCD), and positive were manually appended to the FCD (n=877/86/93/66) and MTS (n=668/104/132) labels. The HA dataset present more complex cases where abnormal hippocampal features (e.g., atrophy, hyperintensities) are described, but a pathological diagnosis is not explicitly indicated (see Appendix B.1 for an example). In this case, reports were summarily labeled as abnormal if any type of abnormality was present, and normal otherwise. The HA dataset contained n=267/725 normal and abnormal labels. Labeling of the FCD, MTS, and HA datasets took approximately 35, 25, and 30 hours, respectively. Training and test sets were created for each datasets using 80%/20% splits, and 20% of the training data was used for validation. An overview of the data extraction and labeling is presented in Figure 2."}, {"title": "3.2 Preprocessing", "content": "To reduce the complexity of the radiology reports and provide information more relevant to the classification task, only sentences containing selected regular expressions related to the target labels were kept. Every text was divided into individual sentences using the Danish NLP framework DaCy (v2.7.7, da_dacy_large_trf-0.2.0) (Enevoldsen et al., 2021) and relevant sentences were identified using the same regular expression which was used to initially identify the radiology reports. The selected sentences were then concatenated and used as input for the classification models. An overview of the preprocessing is presented"}, {"title": "3.3 Natural Language Processing Models", "content": "Three approaches were evaluated: (BERT-like) transformers, few-shot learning with sentence transformers (SetFit), and LLM. The NLP models were trained and evaluated using the transformers package (v4.40.1) from Huggingface. Details on hyperparameter optimization are available in Appendix B.2."}, {"title": "3.3.1 BERT-LIKE TRANSFORMER MODELS", "content": "BERT-like transformer models natively supporting the Danish language (R\u00d8B\u00c6RTa\u00b9) and multilingual text including Danish (XLM-ROBERTa) (Conneau et al., 2019) were evaluated. These checkpoint models were used with and without continued pretraining (Gururangan et al., 2020) on the corpus of 1,2 millions radiology reports. Model pretraining was performed using whole-word masking. Fine-tuning for text classification was performed using a sequence classification head with weighted (binary) cross-entropy loss. Fine-tuning was performed over 50 epochs with a batch size of 16."}, {"title": "3.3.2 SETFIT", "content": "The SetFit approach (setfit, v1.0.3) (Tunstall et al., 2022) was evaluated using a sentence transformer model for Danish (dfm-sentence-encoder-large\u00b2) and a model with multilingual support (distiluse-base-multilingual-cased-v2\u00b3) (Reimers and Gurevych, 2020). Training was performed by first pretraining the model's body for 25,000 steps and then fine-tuning the model end-to-end (i.e., including the classification head) for 50 epochs. A differentiable classification head using a linear layer to map the embeddings to the classes was used. In all cases, a batch size of 8 was used."}, {"title": "3.3.3 LARGE LANGUAGE MODELS", "content": "Three different LLMs were evaluated: a Danish LLM (munin-neuralbeagle-7b4), a general purpose LLM primarily trained for the English language (Meta-Llama-3-70B-Instruct5) (AI@Meta, 2024), and a LLM tailored to the health domain in English (BioMistral-7B6) (Labrak et al., 2024). LLMs were evaluated using few-shots prompting. For the Meta-Llama-3-70B-Instruct and the BioMistral-7B models, the texts were translated from Danish to English using the MADLAD-400-10B-MT model7 (Kudugunta et al., 2023). For each model, we followed the prompt formatting for few-shot inference as recommended by the model's developers. The corresponding prompt templates are given in Appendix B.3."}, {"title": "4. Results", "content": "The agreement (Cohen's kappa) between the two raters for the FCD dataset was 0.83. Table 4 presents the evaluation metrics for the classifiers on the different datasets.\nThe performance of each model is presented in Table 4. Across our three datasets, BERT-like models displayed the highest performance, with DanskBERT (pretrained) ranking first for the FCD and MTS datasets and xlm-roberta-base (pretrained) for the HA dataset. Expectedly, in almost all cases pretraining the BERT-like models on the corpus of 1,2 million radiology reports improved the predictive performances of the models, with the notable exception of xlm-roberta-base on the FCD dataset. Overall, both the SetFit and LLM models displayed comparatively reduced performances, with the LLMs ranking among the worst models."}, {"title": "5. Discussion and Conclusion", "content": "In this work, we evaluated a range of approaches and datasets representing the task of medical text classification in small and imbalanced datasets in a non-English Language.\nDespite the recent rise of LLMs in NLP, most applications in non-English radiology reports have focused on using BERT-like transformer models. In our evaluation, all best-performing models were in fact BERT-like models, indicating that these simpler models continue to deliver state-of-the-art performances in targeted applications. No single model outperformed all others, however, the pretrained DanskBERT model did provide the best performance for the FCD and MTS datasets, and competitive performances for the HA dataset, suggesting that this model may be best suited to our task.\nPretraining on a domain-specific corpus has consistently been shown to improve various NLP tasks. A popular example of this is BioBERT which has gained popularity in the scientific domain (Lee et al., 2020). Generally, the availability of transformers pretrained for specific domains in non-English languages is a core issue for the generalizability of NLP approaches. Here, we evaluated our models with and without pretraining to provide a point of reference showcasing the possible gain in performance. As expected, pretraining did improve performance in almost all cases. However, it is important to emphasize that obtaining a relevant corpus may be non-trivial and can require substantial time and/or resources. For example, the corpus of 1,2 million radiology reports used in our work took approximately 1.5 years to extract due to limitations of the hospital's IT infrastructure. Although automated, in a time-limited project the duration of a similar process would need to be carefully weighed against the potential gain in performance.\nThe SetFit approach, which optimizes the embeddings of sentence transformers (Reimers and Gurevych, 2019), has been introduced as a competitive approach to the BERT-like transformers for small datasets. Contrary to our expectations, this approach rarely outperformed the BERT-like transformers, with and without pretraining. This may be due to the fact that a sentence in a radiology report may contain many details that are not directly relevant to the classification task, therefore leading to sentence embeddings that may be inadequate for isolating specific information. However, more research on this topic would be required to disentangle this issue.\nLarge language models performed poorly in our evaluation. With 7 billion parameters munin-neuralbeagle-7b is a relatively small LLM, but it is ranked among the top models on the Mainland Scandinavian NLG leaderboards. BioMistral-7B is one of the latest generation of LLM adapted to the medical domain. The Meta-Llama-3-70B-Instruct model is by far the largest model included in this study and has exhibited state-of-the-art performances in a wide range of NLP tasks (AI@Meta, 2024). Although we have used the recommended approaches for generating few-shot prompts, a different strategy may yield better results. Furthermore, the translation from Danish to English was in a few cases suboptimal, which may have negatively impacted the predictions. Investigating the quality of different translation models may potentially lead to improved performance for the LLMs. Overall, the poor performance of the LLMs in our scenario is surprising and warrants further research in this topic.\nThe performance required from an NLP models is strongly dependent on the downstream application. Although there is no definite agreement, an accuracy equivalent or superior to that of an expert clinician, which has been shown in similar work to be above 90% (Wood et al., 2020), is often desired. It is therefore important to emphasize that, under this expectation, none of the models evaluated in our setting exhibited performances sufficient to provide a reliable and fully automated solution. However, a closer look at the confusion matrices reveals that some of the classifiers have an almost perfect recall for the most numerous class (Fig. 4B-C). Therefore, when manually labeling large datasets a substantial amount of work could potentially be avoided by first using the classifier to identify the reports belonging to that class and then only processing the remainder. However, the performance of this approach is heavily dependent on the dataset and would have to be carefully validated in each case."}]}