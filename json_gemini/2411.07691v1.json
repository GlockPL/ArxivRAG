{"title": "New Emerged Security and Privacy of Pre-trained Model: a Survey and Outlook", "authors": ["Meng Yang", "Tianqing Zhu", "Chi Liu", "Wanlei Zhou", "Shui Yu", "Philip S. Yu"], "abstract": "Thanks to the explosive growth of data and the development of computational resources, it is possible to build pre-trained models that can achieve outstanding performance on various tasks, such as neural language processing, computer vision, and more. Despite their powerful capabilities, pre-trained models have also sparked attention to the emerging security challenges associated with their real-world applications. Security and privacy issues, such as leaking privacy information and generating harmful responses, have seriously undermined users' confidence in these powerful models. Concerns are growing as model performance improves dramatically. Researchers are eager to explore the unique security and privacy issues that have emerged, their distinguishing factors, and how to defend against them. However, the current literature lacks a clear taxonomy of emerging attacks and defenses for pre-trained models, which hinders a high-level and comprehensive understanding of these questions. To fill the gap, we conduct a systematical survey on the security risks of pre-trained models, proposing a taxonomy of attack and defense methods based on the accessibility of pre-trained models' input and weights in various security test scenarios. This taxonomy categorizes attacks and defenses into No-Change, Input-Change, and Model-Change approaches. With the taxonomy analysis, we capture the unique security and privacy issues of pre-trained models, categorizing and summarizing existing security issues based on their characteristics. In addition, we offer a timely and comprehensive review of each category's strengths and limitations. Our survey concludes by highlighting potential new research opportunities in the security and privacy of pre-trained models.", "sections": [{"title": "1 Introduction", "content": "The recent years have seen rapid advancements in large artificial intelligence (AI) models, such as large language models and large vision models, leading to the concept of pre-trained models. Today, pre-trained models are viewed as large-scale encoders with sophisticated architectures and vast numbers of parameters, initially trained with advanced objectives using extensive datasets. Due to the explosive growth of data and advancements in computing technology, researchers can now scale training data and model architecture to create pre-trained models, such as GPT [86, 88] and BERT [29], that store foundational knowledge in their huge parameters. By fine-tuning these models for specific tasks, the rich knowledge they encode can enhance various downstream applications. As a result, using pre-trained models has become a common practice in developing and improving task-specific models.\nWhile enjoying the convenience brought by pre-trained models, people are increasingly worried about potential security risks. For example, membership inference attacks [61][12] can reveal information about specific contents of a training dataset, and jailbreak attacks [28][90][54] can mislead model to generate harmful responses. However, in addition to various safety issues that have been widely studied in traditional models, the powerful capabilities of pre-trained models also bring new safety issues that do not exist in traditional models. Thus, it is important to fill these gaps and establish higher standards for pre-trained model protection.\nSome studies have been conducted to summarize the security and privacy issues in large models. However, few of them provide deep and comprehensive insight into the root causes of new safety issues in large models. We find that these new safety issues are introduced from the different training strategies and the large-scale dataset. Due to these reasons, there has been a huge gap between pre-trained models, which can be applied to different tasks, and traditional models, which focus on one specific task. For example, the \u201cpre-trained/fine-tune/inference\u201d strategy is popular in the current research area compared to the traditional \u201ctraining/inference\u201d strategy, wherein novel attacks emerge to attack pre-trained models in the fine-tuning process. These methods may be in line with traditional attack methods, but there are still updates in specific details, such as attacking the special training strategy of pre-trained models like Reinforcement Learning with Human Feedback (RLHF) [81].\nThere are still variations within pre-trained models. Most notably, larger models tend to exhibit stronger functionality. This raises the question: what unique security and privacy issues arise as model size increases, and why do these issues occur? Answering this is a long-term endeavor. Although it remains challenging to fully explain why large models have powerful capabilities, an initial approach could involve examining security and privacy issues in smaller pre-trained models and exploring how they generalize to larger models. This would help identify common and differing security and privacy issues, as well as similarities and differences in attack and defense strategies across model sizes.\nThere are various concepts and multiple attack/defense methods in this field, and the boundary between pre-trained models and traditional models is often blurred. These challenges prompted us to conduct a comprehensive survey to summarize, analyze, and classify security issues of pre-trained models, along with the corresponding defensive countermeasures. Our analysis points out the unique characteristics of various attacks and defenses in this domain and the differences between different methods. In addition, we propose a novel taxonomy to categorize the state-of-the-art methods in the literature. Our contributions can be itemized as follows:\n\u2022 We proposed a novel taxonomy of current attack/defense techniques to pre-trained models based on their attack/defense stages and specific strategies.\n\u2022 We comprehensively summarized state-of-the-art attack/defense techniques based on the proposed taxonomy, showing their benefits and shortcomings.\n\u2022 We reviewed methods for attacking and defending pre-trained models of different scales, summarizing their commonalities and differences."}, {"title": "2 Preliminary", "content": "2.1 Definition of pre-trained model\n2.1.1 The life process of pre-trained model. In this survey, we divide the life process of pre-trained models into three stages, which are the pre-training stage, fine-tuning stage, and inference stage.\nPre-training stage. In the pre-training stage, model owners build powerful models from scratch. To do this, they update model weights using pre-trained knowledge (such as datasets, training strategies, and parameter settings). As illustrated in Figure 1, model owners first collect a large dataset from diverse sources. The goal of the pre-training stage is to update model weights, enabling the model to learn from the collected data. During pre-training, the model is initialized with random weight and is optimized using carefully designed loss functions. The whole training process usually lasts from weeks to months, depending on the dataset size and model size. After pre-training, model owners will obtain a pre-trained model that learns general knowledge well.\nFine-tuning stage. Although pre-trained models have powerful performance on general knowledge, they still lack the ability to solve specific tasks, especially those tasks that are different from pre-trained tasks. To process a specific task, users can apply a fine-tuning strategy to update the pre-trained model. As shown in Figure 1, model owners can update entire model layers (full parameter fine-tuning) or only update some layers while keeping others frozen (partial-parameter fine-tuning) using data collected for specific purposes. Full parameter fine-tuning is usually used when the model size is small. However, as pre-trained models become larger and larger, partial-parameter fine-tuning is more practical. Current popular methods include transfer-learning [145], LoRA [38], and tuning strategies [58][51]. The cost of fine-tuning a pre-trained model is less than training a new model.\nInference stage. The process of the inference stage is shown in Figure 1, model users apply the model to complete customized tasks by giving task-specific input samples without modifying the model. There are two scenarios: the first is when the model can directly complete a customized task, meaning the task is one of the pre-trained or downstream tasks. This typically occurs in small pre-trained models. The second scenario requires the model user"}, {"title": "2.1.2 The development of pre-trained models.", "content": "We summarize all pre-trained models which are regarded as target models in the security area and current state-of-the-art models which will be target models in the future in Table 1. These models are based on ResNet [37] or Transformer [102], which are two famous traditional models in the computer vision (CV) and Neutral Language Process (NLP) domain. Their architecture (encoder, decoder, skip-connection) and training strategy have hugely influenced the following work in the machine learning area. Based on model size and model ability, the development of pre-trained models can be divided into small pre-trained models and large pre-trained models.\nSmall pre-trained model. Models that are small enough to be fine-tuned on consumer GPUs are defined as small pre-trained models. We first introduce two famous language-only pre-trained models, GPT and BERT series models.\n\u2022 GPT-series: GPT-1 [86] is the first GPT-series model based on Transformer-decoder, and adopted a hybrid approach of unsupervised pre-training and supervised fine-tuning. It can predict the next word of an input sentence and has set up the core architecture for GPT-series models. Following a similar structure, GPT-2 [88] scales the model size to 1.5B. It sought to perform tasks via unsupervised language modeling without explicit fine-tuning using labeled data.\n\u2022 BERT-series: BERT [29], based on Transformer-encoder, focuses on capturing the context-aware word representations, which is better than the GPT model at first. This study has inspired a large number of follow-up work. For example, RoBERTa [67] and ALBERT [50] improved the pre-training strategies, while BART [52] can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pre-training schemes.\nAt the same time, researchers aim to build a vision-language model that can understand the relationship between image and text.\n\u2022 The most famous model is named CLIP [87], which focuses on matching images and text using contrastive learning and is particularly strong in zero-shot learning and classification tasks. BLIP [56] incorporates both contrastive and generative learning and is more versatile for tasks involving language generation based on images, like captioning and Visual Question Answering. Meanwhile, OFA [109] is a Task-Agnostic and Modality-Agnostic framework supporting Task Comprehensiveness.\nLarge pre-trained model. Researchers have discovered that scaling pre-trained models often leads to improved performance across various tasks [139]. As model scale increases, their capabilities have led to unexpected advancements. We define models that can complete multiple tasks and have a significant size as large pre-trained models\n\u2022 GPT-series: GPT-J [105] aims to increase the program ability of GPT-2 by adding public software repositories into the training dataset so that the model can learn how to code a program. GPT-3 [10] introduces in-context learning, enabling the model to solve few-shot or zero-shot tasks, a capability in which GPT-2 underperformed. GPT-3 demonstrates that scaling the model to a significant size (175B) can lead to a huge increase in model capacity. InstructGPT [81] applies RLHF to GPT-3. RLHF algorithm is useful to guide pre-trained models to learn human preferences and mitigate the issues of generating harmful content. Despite understanding human preference, GPT-3.5 has reasoning ability, meaning it can resolve complex math questions and programming tasks. A number of close-source applications have been built based on"}, {"title": "2.2 Definition of Attack to pre-trained model", "content": "Given the close relationship between traditional models and pre-trained models, it's reasonable to assume they share similar security and privacy issues. For these shared issues, we focus on whether new attack and defense methods for pre-trained models have emerged. In addition, differences between traditional and pre-trained models result in unique security and privacy issues. We concentrate on these specific concerns for pre-trained models and emerging attack/defense strategies.\n2.2.1 Attack Background.\n\u2022 Attack Stage: Based on the life process described above, we classify all attacks into the pre-training stage, fine-tuning stage, and inference stage. This classification depends on whether attackers have access to participate in a particular stage and whether they can introduce malicious samples at that stage.\n\u2022 Attack Goal: Target attacks aim to force the model to output desired information (e.g. $E^*(x^*) = y^*$), while non-target attacks aim to make the model to output incorrect results (e.g. $E^* (x^*) \u2260 y$). Some attacks aim to maintain performance on the original dataset (e.g. $E^*(x) = y$) and other attacks degrade performance (e.g. $E^* (x) \u2260 y$).\n\u2022 Attack knowledge: We define model knowledge as (1) information about the target model such as model architecture, model weight, and parameters, and (2) information related to model training such as training algorithm, training parameters, and training dataset. We define white-box access as when attackers have full access to this knowledge. Black-box access is defined when attackers lack this knowledge and can only query the target model to obtain output embeddings or labels.\n2.2.2 Threat Model. We define the threat model of attack in Table 2, from the perspective of attacker capability, attacker goal, attacker role, and model source.\n\u2022 Attacker capability: Attackers have white-box/black-box access to model information, including training strategy, dataset, structure, hyperparameters, weights, and input samples. We define white-box access when attackers have full access to this information. We define black-box access when attackers can only query the target model and then get output embedding/label.\n\u2022 Attacker goal: Attackers aim to mislead the target model to complete an attack-designed task or attackers want to replicate the target model to a new model.\n\u2022 Attacker role: Attackers can be service providers who train and publish clean models, or model users who have malicious intentions.\n\u2022 Model source: The target model can be an open-source model or a close-source commercial model."}, {"title": "2.2.3 Common attacks with traditional machine learning models.", "content": "Backdoor Attack. Attackers aim to inject a backdoor into a clean model E, with a trigger pattern \u2206 designed to activate a backdoor. After being attacked, the victim model will maintain the performance on the original tasks: for each clean input x, the model predicts the correct label (y = $E^*(x)$) to reduce the chance of detection. However, when a trigger is added to the input $x^* = x \u2295 \u2206$, the victim model will be hijacked to complete tasks set by attackers (y* = $E^*(x^*)$) [47][130].\nPoison Attack. A poison attack involves injecting strategically crafted, maliciously designed data into the training dataset of a target model to compromise the model's performance on a specific task. This malicious data can be used to manipulate models' behavior in specific ways, such as causing it to misclassify certain inputs, degrade its generalization ability, or introduce vulnerabilities that can be exploited later. Unlike a backdoor attack, attackers can only poison a small portion of the clean dataset and do not control the training process of the target model [9].\nModel Hijacking Attack. A model hijacking attack [132][98] seeks to manipulate the target model to perform malicious tasks designed by the attacker. Similar to a poison attack, the attacker can only alter the training dataset of the target model. Additionally, the hijacked model must successfully carry out the hijacking task without compromising its performance on the original task. The data used to poison the target model should resemble the structure of the original dataset\nModel Extraction Attack. The goal of a model extraction attack is to create a surrogate model $E_s$ that performs similarly to the target model E, i.e., E(x) = $E_s$(x) [70][93][48]. Typically, attackers have only black-box access to the target model, allowing them to query the encoder with an input image and receive the corresponding output. Factors such as model architecture, training dataset, and training algorithm significantly influence attack performance. This attack is feasible when the cost of building a surrogate model by querying the target model is substantially lower than creating a new model from scratch.\nMembership Inference Attack. Membership inference attacks aim to achieve high accuracy in inferring whether a sample is a member of the training dataset. Data owners can use this attack to verify if their private data has been learned by a publicly available pre-trained model [61]. Malicious attackers can exploit this attack to extract private information, such as emails and phone numbers, from the model's training dataset, posing a significant threat to privacy security [61].\nAdversary Attack. The goal of an adversarial attack is to generate an adversarial perturbation \u03b4, causing incorrect outputs in downstream tasks that rely on pre-trained models E during the inference stage [107][141]. The attack goal can be expressed as E(x + \u03b4) \u2260 E(x) and C(E(x + \u03b4)) \u2260 C(E(x)), where E is the pre-trained encoder and C is the downstream classifier. The adversarial perturbation should be small enough to go unnoticed by humans. Therefore, an upper bound e and a distance metric $L_p$-norm are used to restrict the perturbation $||\u03b4||_p \u2264 \u20ac$.\n2.2.4 Unique attack to pre-trained model. With the advancement of pre-trained models, unique attacks have emerged, exploiting their powerful capabilities and specially designed inputs (prompts).\nTypographic Attack. Typography attacks [85] [20] exploit vulnerabilities in a model's text-image alignment process by making seemingly harmless changes, such as adding text to images. These attacks can mislead and confuse text-based models, resulting in incorrect interpretations, classifications, or operations.\nJailbreak Attack. A prompt consists of user-defined instructions that tailor the capabilities of large pre-trained models [128]. These models are also trained to reject harmful prompts in real-life scenarios, such as \u201cHow to rob a bank?\". Jailbreak Attack involves strategically manipulating input prompts to bypass these safeguards and generate content that would otherwise be blocked. By exploiting carefully crafted prompts, a malicious user can induce these models to produce harmful outputs that violate established policies [28].\nPrompt inject attack. Similar to jailbreak attacks, prompt inject attacks [83][68][36] manipulate the output of large pre-trained models through engineered malicious prompts. Differently, prompt inject attacks have two objectives: goal hijacking (misaligning the original prompt goal to print a target phrase) and prompt leaking (revealing part of or the entire original system prompt) [83].\""}, {"title": "2.2.5 Attack taxonomy.", "content": "From the attackers' perspective, based on differences in characteristics of the victim model before and after the attack, as well as the difference in threat models, we propose an attack taxonomy to summarize collected papers, namely, No-Change Attacks, Input-Change Attacks, and Model-Change Attacks.\n\u2022 No-Change Attacks:\nDefinition: No-Change Attacks make no modifications to the target pre-trained models.\nThreat model: Attackers are malicious users who try to steal privacy information from the target model or replicate a surrogate model that has similar performance to the original target model, while the target model is a close-sourced commercial model. Attackers have no access to model knowledge (model structure, model hyperparameters and model weight), training process (training strategy, training data) and can not edit the model input.\nUnique part: As the scale of models increases, attack methods on traditional models become less effective against pre-trained models. Given the strong performance of pre-trained models, attacking them holds significant value. However, it is essential to explore new attack methods specifically targeting these models.\nComments: This threat model means that defenders cannot determine whether the model is attacked by this type of attack and can only take a defense strategy in advance. However, the flexibility of attack methods and attack scenarios is also limited due to the strict settings.\n\u2022 Input-Change Attacks:\nDefinition: Input-Change Attacks involve modifying the input samples of target pre-trained models.\nThreat model: Attackers are malicious users who try to mislead the target model to complete a specific-designed task by modifying the model input. They have no access to model knowledge (model structure, model hyperparameters and model weight) and training process (training strategy, training data). Meanwhile, attackers can edit the model input and aim to attack open-source models or close-sourced models. Unique part: Because users need to design prompts to guide pre-trained models for various tasks, input-change attacks can exploit these prompts to achieve different effects, such as stealing private information or generating harmful content. This is the most popular attack method for pre-trained models. Since traditional models focus on a specific task, input-change attacks offer little advantage over other types of attacks and thus have not been widely studied.\nComments: Input-Change Attacks are relatively simple and effective to design, but the differences between clean samples and malicious samples also make it easier to design defense strategies.\n\u2022 Model-Change Attacks:\nDefinition: Model-Change Attacks involve altering the target pre-trained models themselves, such as their architecture and weights.\nThreat model: Attackers are service providers or malicious users who try to mislead a publicly available target model to perform a specifically designed task. They have white-box access to model knowledge (model structure, model hyperparameters and model weight) and training process (training strategy, training data) as well as the ability to edit model input.\nUnique part: First, pre-trained models are time-consuming to train from scratch, so most Model-Change attacks on them occur during the fine-tuning stage. In contrast, attacks on traditional models typically occur during the pre-training stage. Second, pre-trained models remain parameter-frozen and will be used directly in downstream tasks, unlike traditional models which might be retrained by the user (attack performance will decrease after retraining). Therefore, Model-Change attacks pose more serious security and privacy issues for pre-trained models.\nComments: In this scenario, attackers can design the most powerful attack based on gradient/feedback from the target model. These attacks will also affect downstream tasks based on the victim target model. However, this kind of attack can not apply to close-sourced black-box models, as attacking these commercial models is more valuable.\nDue to the specific threat model of the above three defined attack types, No-Change Attacks and Input-Change Attacks only occur in the inference stage. while Model-Change Attacks can occur during pre-training, fine-tuning, and inference. We apply different attack types as secondary tier classification and detailed attack strategy as the third tier. The attack taxonomy is shown in Figure 2."}, {"title": "2.3 Definition of Defense", "content": "2.3.1 Defense Background.\n\u2022 Defense Stage: Despite the Pre-training/Fine-tuning/Inference stage, we additionally divided the defense process into two stages, which are the Before-Attack stage and the After-Attack stage. If a defense strategy can successfully filter out these attacks when attackers attempt to attack the model, or before causing actual harm to target models, we call this as Before-Attack stage. Otherwise, if a model has been attacked and the performance has been affected, we refer to the defense methods applied at this stage as the After-Attack stage.\n\u2022 Defense Goal: The basic goal of the defense methods is to mitigate the influence of attacks. The ultimate goal is to remove the attack, which means to recover model performance as if the model has not been attacked.\n\u2022 Defense Knowledge: Similar to attack knowledge, we define white-box access as the situation where defenders have full access to the target model. Black-box access means defenders lack model knowledge and can only query the target model and then get output embedding/label.\n2.3.2 Defense taxonomy. According to the differences in characteristics of the target model before and after defense, we classify these defense methods into No-Change Defenses, Input-Change Defenses, and Model-Change Defenses similar to the attack taxonomy.\n\u2022 No-Change Defenses: Defenders cannot modify target models' weights, parameters, etc. Meanwhile, defenders should keep the models' input samples unchanged. These defense methods do not require details of target models, which means they can be widely used in various models to defend against the same type of attacks.\n\u2022 Input-Change Defenses: Defenders can only add defense perturbations to the input samples of target models to mitigate attack performance while keeping the model unchanged. However, it is difficult to design an effective defense perturbation that can defend against attacks and keep performance on clean data.\n\u2022 Model-Change Defenses: Defenders have white-box access to any settings of the target models and model input. Defenders can design a more comprehensive defense method based on the feedback from target models. However, similar to the Model-Change Attack, this defense can not be applied to close-sourced black-box models.\nSlightly different from attack taxonomy, we apply a detailed defense strategy as secondary tier classification as one strategy can defend against a series of attack types. Finally, we set defense types as the third tier. The defense taxonomy is shown in Figure 3."}, {"title": "3 Attacks on Pre-trained Models", "content": "3.1 No-Change Attacks\nIn this scenario, attackers are malicious users who try to steal privacy information from the target model or replicate a surrogate model that has similar performance to the original target model, while the target model is a close-sourced commercial model. Attackers have no access to model knowledge (model structure, model hyperparameters, and model weight), training process (training strategy, training data) and can not edit the model input. Therefore, two types of attacks can be achieved by No-Change Attacks, including model extraction attack [70][93][48] and membership inference attack [61][49].\nSince pre-trained models generalize better across different tasks and possess richer knowledge than traditional models, attacking pre-trained models holds greater value. Additionally, the vast number of parameters in large models and their protection mechanisms drive attackers to continuously upgrade their attacking methods. We provide an example of No-Change Attacks in Figure 4, where attackers achieve a model extraction attack and aim to optimize a surrogate model with similar functionality to the target clean model, using a collected shadow dataset and black-box access to the target model. We will summarize the papers related to this attack type in detail in the following sections.\n3.1.1 Model Extraction Attack. Model extraction attacks aim to replicate the function of the target model. The cost of implementing this attack is determined by the number of queries made to the target model, which has been extensively studied. In the image domain, Liu et al. [70] employed data augmentation strategies on the surrogate dataset, leading to an increase in the number of queries to the surrogate encoder while reducing the query load on the target model. Following this work, Shayegani et al. [93] introduced the ContSteal attack, which leverages features from distinct images to drive apart the target and surrogate embeddings of different samples. This approach becomes particularly advantageous when the adversary is constrained by a limited query dataset and restricted query budgets. In the text domain, Karmakar et al. [48] proposed MARICH, a sampling-based query selection algorithm, to select the most informative queries that simultaneously maximize the entropy and reduce the mismatch between the target and surrogate models. This proposed attack is agonistic to the deployed model and is applicable for any datatype.\nSince Large pre-trained models have a large number of parameters, this contributes to their great value but also creates difficulty in performing model extraction attacks against them. To tackle this challenge, Carlini et al.\n3.1.2 Membership Inference Attack. To extract privacy information from pre-trained models, Liu et al. [61] built a binary classifier to determine whether an input sample is a member or non-member of the training set. The key idea is that pre-trained models are more likely to produce highly similar feature vectors for two augmented versions of the same input member sample. However, the boundary between member and non-member samples in this method is blurry. To address this issue, Ko et al. [49] first pre-defined non-member data (data posted after the publication date of the target model), then collected surrogate member data using a threshold. The key idea is that a multimodal model is trained to maximize cosine similarity between image and text features for members, so a high threshold thr can effectively distinguish member data (e.g. d($E_i(x_i)$, $E_t(x_t)$) > thr).\n3.1.3 Summary. References related to No-Change Attacks are listed in Table 3. We can conclude that No-Change Attacks occur during the inference stage and all these papers assume the target model is a black-box model. This is because extracting a white-box model is unnecessary, as various details have already been publicly disclosed. Meanwhile, we observe that most No-Change Attacks focus on creating a surrogate model to mimic the capabilities of the target model. To build a surrogate model, attackers need to query the target model for essential information, such as embeddings. These attacks are often directed at commercial models, where queries incur a cost. Therefore, the key challenge is reducing costs by minimizing the number of queries while maintaining attack performance. Some studies [70][93][61] increase the number of queries to the surrogate model using augmentation methods since there is no cost to query the surrogate model. In contrast, other studies [48][49] select effective samples to reduce the number of queries to the target model. However, in large model scenarios, this approach is challenging due to the large number of parameters and extensive training data. Recent work [13] aims to reconstruct the model's information using mathematical methods, but it can only retrieve partial information, indicating that this research direction remains underexplored and holds significant potential for further investigation\nIn summary, No-change Attacks are challenging to defend against because they do not disrupt the normal functioning of the target model. Hence, defenders must preemptively guard against this type of attack during the model-building phase. While No-Change Attacks on small models have been extensively studied, these techniques encounter difficulties when targeting large models. Therefore, it is essential, though challenging, to develop new methods for executing No-change Attacks on large models."}, {"title": "3.2 Input-Change Attacks", "content": "In this scenario, attackers are malicious users who try to mislead the target model to deliver specific-designed tasks. They have no access to model knowledge (model structure, model hyperparameters, and model weight) and training process (training strategy, training data). Meanwhile, attackers can edit the model input and aim to attack open-source models or close-sourced models. Therefore, the most representative attacks are adversarial attacks and jailbreak attacks.\nFor traditional models, modifying the input can mislead the model into incorrect results and affect the performance. For pre-trained models, attackers can achieve more advanced goals like bypassing safeguards and causing the model to generate harmful or sensitive information.\nAs shown in Figure 5, attackers append malicious prompts with user input prompts, thereby misleading the target model to output incorrect/harmful results. These malicious prompts can be designed by humans or automatically generated by malicious programs. Based on the specific strategy, we divide Input-Change Attacks into four types: which are Manual-based methods for manual-designed malicious samples, Query-based methods, Gradient-based methods, and Transfer-based methods for automatic-generating malicious samples.\n\u2022 Manual-based: The input prompt/perturbation is designed by humans manually. This method relies heavily on human knowledge of target models and the security domain.\n\u2022 Query-based: The input prompt/perturbation is optimized by querying the target model. Attackers usually have black-box to target models and search for optimization methods to replace gradients.\n\u2022 Gradient-based: The input prompt/perturbation is optimized by gradients returned from target models. Attackers must have white-box access to target models. Gradient-based attacks can optimize one attack sample that can effectively attack target models, but attack performance on other models is not ideal.\n\u2022 Transfer-based: The input prompt/perturbation is optimized on the surrogate model and then directly transferred to attack the target models. Attackers have the freedom to propose any methods to optimize malicious samples on surrogate models. However, there are only experiment results and no solid theory to explain why these generated malicious samples can successfully transfer to attack black-box target models.\nWe will summarize the papers belonging to this attack type in detail in the following sections.\n3.2.1 Backdoor Attack."}, {"title": "3.2.2 Membership Inference Attack.", "content": "Manual-based input. Many studies have demonstrated that LLMs memorize and leak individual training examples because of overfitting. In particular, Carlini et al. [15] provided a manually designed prefix query to GPT-2, and extracted privacy information from the model response, such as the person's name, email address, phone number, fax number, and physical address. Similarly, Carlini et al. [12] fed prefixes of the model's training prompts to target LLMs and discovered that the LLMs could complete the remainder of the sentence, indicating that LLMs can reproduce memorized training data verbatim.\nApart from building prefix queries, some studies mask privacy information in a sentence and ask pre-trained models to fill these masks. Huang et al. [40] queried LLMs for email addresses with contexts of the email address or prompts containing the owner's name (e.g. \u201cthe email address of [NAME] is [EMAIL]\u201d) and found that LLMs can fill these [NAME] and [EMAIL]. However, LLMs are hard to associate personal information with specific persons so the risk of leaking specific information is still low. Lukas et al. [72] first leveraged a public language model to fill masks and then fed these filled prompts into the target model and computed the perplexity of each prompt. The lowest perplexity prompt is more likely to be member data. Unlike the work above, Li et al. [54] acted as a clean user to activate the jailbreak mode of a large model. After the attack, the target model (ChatGPT/New Bing) will leak privacy information such as name, email, and phone number."}, {"title": "3.2.3 Adversarial Attack.", "content": "Query-based. It is not easy to optimize adversarial text samples with gradient-based methods as text is discrete data", "57": "proposed BERT-Attack", "steps": 1, "119": "proposed TrojLLM, which first generates a prompt seed and searches for a universal trigger using surrogate prompt generators. It then progressively updates these generators to produce poisoned prompts by querying LLM-based APIs using few-shot data samples.\nGradient-based. Unlike traditional models, some pre-trained models only output feature vectors rather than classification labels. To attack these kinds of models, Zhou et al. [142", "134": "pioneered the exploration of multimodal joint adversarial attacks under white-box settings, carrying out attacks on both image and text modalities simultaneously. However, they omit to consider the connection between different modalities. To tackle the challenges of heterogeneity between different modalities and bridge the attack gap between pre-trained models and unknown downstream tasks, Zhou et al. [141", "127": "proposed VLATTACK, which generates perturbations by exploring image modality, text modality, and then multi-modality one by one if the front attack fails. Ye et al. [126", "73": "enhanced the transferability of adversarial examples across different prompts by optimizing a single image perturbation with up to 100 text prompts simultaneously.\nThe above assumption does not limit the range of perturbations, making it easy for users to defend against. To address this, Schlarmann et al. [92", "118": "demonstrated the existence of natural triggers in off-the-shelf LLMs that can be discovered using plain text. They identified these triggers from public datasets and optimized them using the surrogate model RoBERTa, showing the attack's transferability to other models like BERT.\nIn the multimodal area, Lu et al. [71"}]}