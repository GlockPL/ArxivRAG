{"title": "LaFA: Latent Feature Attacks on Non-negative Matrix Factorization", "authors": ["Minh Vu", "Ben Nebgen", "Erik Skau", "Geigh Zollicoffer", "Juan Castorena", "Kim Rasmussen", "Boian S. Alexandrov", "Manish Bhattarai"], "abstract": "As Machine Learning (ML) applications rapidly grow, concerns about adversarial attacks compromising their reliability have gained significant attention. One unsupervised ML method known for its resilience to such attacks is Non-negative Matrix Factorization (NMF), an algorithm that decomposes input data into lower-dimensional latent features. However, the introduction of powerful computational tools such as Pytorch enables the computation of gradients of the latent features with respect to the original data, raising concerns about NMF's reliability. Interestingly, naively deriving the adversarial loss for NMF as in the case of ML would result in the reconstruction loss, which can be shown theoretically to be an ineffective attacking objective. In this work, we introduce a novel class of attacks in NMF termed Latent Feature Attacks (LaFA), which aim to manipulate the latent features produced by the NMF process. Our method utilizes the Feature Error (FE) loss directly on the latent features. By employing FE loss, we generate perturbations in the original data that significantly affect the extracted latent features, revealing vulnerabilities akin to those found in other ML techniques. To handle large peak-memory overhead from gradient back-propagation in FE attacks, we develop a method based on implicit differentiation which enables their scaling to larger datasets. We validate NMF vulnerabilities and FE attacks effectiveness through extensive experiments on synthetic and real-world data.", "sections": [{"title": "1. Introduction", "content": "Non-negative matrix factorization (NMF) [1] is a versatile tool for multi-way data reconstruction through factorizing a matrix or a multidimensional array in a least-squares approach. As real-world data often exhibit multiple ways, e.g., conditions, channels, spaces, times, and frequencies, the NMF can be an effective tool to extract salient features of those data. As such, NMF has become widely adopted across scientific fields such as psychology, chemistry, signal processing, computer vision, and bioinformatics [2].\nHowever, as we will show, NMF's decomposed factors are susceptible to input's perturbations, known as adversarial noise, which are intentionally designed to disrupt feature extraction. These disruptions, termed adversarial attacks, can compromise the reliability of the salient feature extraction process. Standard NMF algorithms typically assume that data are sampled from a distribution with a low-rank model and zero-mean i.i.d. Gaussian noise. In many real-world scenarios, this assumption may not hold due to the presence of malicious attacks or anomalies, rendering the feature extraction process vulnerable to such noise.\nThe main contributions of this paper are:\n\u2022 We examine the robustness of NMF against adversarial attacks and find it vulnerable despite its resilience in data reconstruction. Specifically, we introduce the Feature Error loss to directly assess latent features generated by NMF. Through back-propagation attacks using the FE loss, we show that injecting small amounts of adversarial noise into the data can lead to significant distortions in the resulting latent features. The finding established a class of new attacks, called Latent Feature Attacks (LaFA) on NMF.\n\u2022 Back-propagating FE attacks require backtracking NMF's iterative updates, demanding substantial peak-memory. To address this, we utilize implicit differentiation to determine a direct expression for the gradients required for FE attacks, bypassing the need to back-propagate through NMF iterations. This approach reduced the peak-memory requirements, and removes the history dependence of the FE attack gradients.\n\u2022 We confirm the susceptibility of NMF to feature attacks and illustrate the efficacy of our approaches through comprehensive experiments conducted on one synthetic dataset and four different real-world datasets: WTSI [3], Face [4], Swimmer [5], and MNIST [6].\nThis manuscript is organized as follows. Sect. 2 and 3 provide the related work and preliminaries. Sect. 4 describes our proposed FE loss and the corresponding LaFA targeting extracted features. Our methods and technical claims are illustrated via synthetic experiments in Sect. 5. Sect. 6 provides our experimental results on real-world datasets, and Sect. 7 concludes this paper."}, {"title": "2. Related Work", "content": "Adversarial attacks in ML have predominantly targeted supervised learning models with numerous studies demonstrating the susceptibility of these models to subtle, maliciously crafted perturbations [7, 8]. However, the exploration of adversarial attacks in unsupervised settings, particularly involving techniques like NMF, has started to garner attention only in recent years.\nRecent research has increasingly focused on integrating adversarial learning with NMF, revealing both vulnerabilities and opportunities for enhancing robustness. In [9], the authors introduce adversarial perturbations during the factorization process, uncovering potential manipulations and inherent weaknesses in traditional NMF algorithms. To address this, [10] proposes a training regime that incorporates adversarial examples to foster NMF models that maintain precise factorizations under adversarial conditions. Extending beyond the direct applications to NMF, [11] explores the effects of adversarial attacks on community detection algorithms, often rooted in matrix factorization principles. The findings illustrate that robust algorithmic strategies can mitigate even extreme adversarial attacks, suggesting pathways to more resilient community detection methods. Additionally, [12] merges deep learning with NMF to tackle matrix completion tasks, incorporating elastic adversarial strategies to assess and improve the robustness of learned patterns against deliberate noise.\nWhile the aforementioned studies primarily focus on defensive schemes within the context of NMF, especially during training, our work diverges by critically examining the robustness of NMF under adversarial attacks. We introduce novel techniques specifically designed to compromise feature integrity, marking a pioneering effort in executing targeted attacks within the realm of unsupervised learning."}, {"title": "3. Preliminaries", "content": "NMF is particularly notable for its application in data with inherent non-negativity, where it decomposes a non-negative matrix $X \\in R^{M\\times N}$ into two low-rank non-negative matrices, $W \\in R^{M\\times k}$ and $H \\in R^{k\\times N}$, such that $X \\approx WH$, where k is much smaller than M and N.\nNMF procedure. One effective approach to find W and H is by utilizing Kullback-Leibler (KL) divergence as a discrepancy measure, which offers a sound statistical interpretation in applications involving counts or probabilities. The optimization aims to minimize the divergence between X and its approximation WH, which is given by [1]:\n$W_{ij} \\leftarrow W_{ij} \\frac{(X (WH))_{ij}}{(1 H^T)_{ij}}$\n$H_{ij} \\leftarrow H_{ij} \\frac{(W^T(X (WH)))_{ij}}{(W^T1)_{ij}}$\nwhere $\\div$ denotes element-wise division and 1 denotes a one-matrix of X's size. These updates are applied iteratively, where each iteration improves the approximation of X by reducing the KL divergence. The process is generally governed by a pre-determined number of iterations, or until a convergence criterion is reached. We use W, H = NMF(X, Winit, Hinit, T) to denote this iterative update procedure, where T represents the number of updates.\nRobustness of NMF. NMF is appreciated for its robustness in data reconstruction, particularly against noise. This robustness can be reflected via the triangle inequality [13]:\n$||X + \\delta \u2013 WH|| \\leq ||X \u2013 WH|| + ||\\delta||$ (1)\nThe inequality implies that any perturbation $\\delta$ to the data X cannot induce a reconstruction error that exceeds the original reconstruction error by a margin of $||\\delta||$.\nIt is more interesting to examine the robustness of the resulting W' and H' from a perturbed X + \u03b4. Regarding that, Laurberg's Theorem [14] provides a compelling mathematical foundation. Particularly, by denoting:\n$J_{(W,H)}(W', H') := \\min_{D,P} ||W \u2013 W'(DP)||+ ||H \u2013 (DP)^{-1}H'||$\nwhere D is a diagonal matrix and P is a permutation matrix, we can restate the Laurberg's result as:\nAssuming X = WH be a unique NMF. For any given $\\epsilon > 0$, there exists $\\alpha \\& \\delta > 0$ such that for any nonnegative matrix Y = X + N with $||N|| < \\delta$, we have $J_{(W,H)}(W', H') < \\epsilon$, where $[W', H'] = arg\\min_{W'>0,H'>0} ||Y \u2013 W'H||$.\nIn other words, the Theorem shows that the perturbation's magnitude bounds the distortion in the factored matrices resulting from perturbed data, emphasizing the stability and robustness of NMF under near-optimal conditions."}, {"title": "4. Latent Feature Attacks on NMF", "content": "This section describes our proposed LaFA on NMF. Specifically, we introduce our proposed FE loss in Subsect. 4.1, and the gradient-ascent-based attacks in Subsect. 4.2. Both attacks compute the gradients of the FE loss w.r.t the input data X and utilize gradient-based methods to iteratively find the adversarial perturbation maximizing the FE. The Back-propagation method directly computes the gradient $\\nabla_XL$ by reversing the NMF procedure, which demands extremely high peak-memory to store the gradient computational graph. In contrast, the Implicit method only need to backward to the feature matrices W and H, significantly reducing the amount of memory required and enabling the scaling of attacks to scenarios with larger datasets."}, {"title": "4.1. Feature Error Loss", "content": "We now elaborate on how to formulate a loss function capturing the feature errors between the NMF-extracted matrices ($W_{NMF}, H_{NMF}$) and the true matrices ($W_{true}, H_{true}$) generating X. We begin by denoting that error as a loss $\\mathcal{L}$ taking two matrices $WH_{NMF}$ and $WH_{true}$ as arguments:\n$FE = \\mathcal{L}(WH_{NMF}, WH_{true})$ (2)\nHere, $WH \\in \\mathbb{R}^{(M+N)\\times k}$ is the concatenation of W and HT combined with a magnitude balancing operation:\n$WH_i = concat(W_i, H_i)$ where\n$W = W_i \\frac{\\sqrt{||W_i|| \\cdot ||H_i||}}{||W_i||}$ $H = H_i \\frac{\\sqrt{||W_i|| \\cdot ||H_i||}}{||H_i||}$\nwith i refers to the column of the matrices and $||.||$ is $L_2$ norm. For brevity, we denote the above construction of WH from W and H by WH = cat(W,H). The magnitude balancing operation does not change the result of W \u00d7 H; however, it removes the ambiguity arising from the scaling of W and H. Additionally, by including both W and H in the same norm operation, rather than in two separate terms as Laurberg's formulation [14], the FE simplifies the features' alignment between $WH_{true}$ and $WH_{NMF}$.\nAs identical Xs are recovered if the rows and columns of W and H are permuted in combination, a meaningful FE loss must minimize over all column-permutations of $WH_{NMF}$. To address this, a feature-wise error matrix FEM are constructed as follow:\n$FEM (i, j) = ||WH_{NMF_i} - WH_{true_j}||_F$\nThen, the element-wise square of FEM can be fed into the Hungarian Algorithm [15] to align $WH_{NMF}$ so that $||WH_{NMF} \u2013 WH_{true}||$ is minimized.\nConsequently, we can express the FE loss $\\mathcal{L}$ as:\n$FE = \\mathcal{L}(WH_{NMF}, WH_{true})$\n$= \\min_P \\frac{||cat(PW_{NMF}, PH_{NMF}) \u2013 WH_{true} ||_F}{||WH_{true} ||_F}$ (3)\n(4)\nwhere P is a permutation matrix.\nNoting that squaring FEM linearizes the contribution of the difference of each element of WH, thus allowing the linear sum assignment algorithm to correctly choose the minimum permutation without sampling all possible permutations. It would not be possible to utilize the Hungarian Algorithm to simplify the permutation problem if Laurberg's two-term definition of FE were used."}, {"title": "4.2. Latent Feature Attacks", "content": "With the FE loss (2), the optimal direction for an adversarial feature attack can be computed. Given a ground-truth $WH_{true}$, the FE can be considered as a function of X, and the optimal adversarial direction is simply the gradient $\\nabla_XL$. That gradient can be obtained by back-propagating the Multiplicative Updates of NMF (presented in subsection 3) and the FE loss. Then, the optimal distortion $\\epsilon"}, {"title": "4.2.1 Back-propagate Feature Attack", "content": "Algo. 1 shows how to compute the adversarial direction for a feature attack using the Back-propagating method. That gradient then can be leveraged by Fast Gradient Signed Method (FGSM) [8] or Projected Gradient Descent (PGD) [7] to generate the adversarial X. However, the high memory requirement to backward the NMF iterations W, H = NMF(X, Winit, Hinit, T) (Line 3) hinders the practicality of the method. In fact, the gradients' computational graph for that step requires a peak-memory of T \u00d7 O(MN). Since the number of NMF's updates T is typically \u2248 104, it creates heavy burdens on computational resources and prevents the feasibility of the attack."}, {"title": "4.2.2 Implicit Method for Feature Attack", "content": "We now demonstrate our Implicit method to efficiently compute the gradient for feature attacks. The attack relies on the fixed-point condition of the NMF at convergence:\n(W, H) = NMF(X, W, H, 1) (5)\nWe denote x and y as the flattened vectors of X, and (W, H), respectively. We then can rewrite (5) as f(x, y) = 0, where f is the NMF update with argument y instead of W and H. By denoting f(x, y) = f(x, y) \u2013 y, we have\n$\\frac{\\partial f_i}{\\partial x_j} + \\sum_k \\frac{\\partial f_i}{\\partial y_k} \\frac{\\partial y_k}{\\partial x_j} = 0$\\n$\\frac{\\partial f_i}{\\partial y_i} + \\sum_k \\frac{\\partial f_i}{\\partial y_k} \\frac{\\partial y_k}{\\partial y_i} = 0$\\nWe can rewrite the above with the matrix's notations:\n$\\frac{\\partial f}{\\partial x} + J_y \\frac{\\partial y}{\\partial x} = 0 \\Rightarrow \\frac{\\partial y}{\\partial x} = - (J_y - I)^{-1} \\frac{\\partial f}{\\partial x} = -(J_y \u2013 I)^{-1} J_x$ (6)\nwhere $J_y$ and $J_x$ are the Jacobian matrices with $J_y[i, k] = \\frac{\\partial f_i}{\\partial y_k}$ and $J_x[i, k] = \\frac{\\partial f_i}{\\partial x_k}$. For (6), we use $\\frac{\\partial y_i}{\\partial y_k}=\\delta_{ik}$.\nWe can see that (6) offers an alternative to compute the gradient. First, we use the Jacobians of one NMF update to compute the partial derivatives of W and H w.r.t. X, i.e., Jy, then multiply it with the gradient of the FE loss (2) w.r.t. W and H would give us the feature attack's gradient:\n$G=-\\nabla_{W,H} \\circ (J_y \u2013 I)^{-1} J_x$\nThis implicit computation scheme is summarized in Algo. 2. It can be seen that the peak-memory of the computation is for storing $J_x$, which is O((M + N) \u00d7 (M \u00d7 N)), and it is independent of the NMF's iterations T."}, {"title": "5. Illustrative Synthetic Experiments", "content": "This section demonstrates the vulnerability of NMF against LaFA via a synthetic example. We consider a synthetic data $X \\in \\mathbb{R}^{100\\times 200}$ with known ground-truth $W_{true} \\in \\mathbb{R}^{100\\times 3}$ and $H_{true} \\in \\mathbb{R}^{3\\times 200}$, and apply three attack/perturbation schemes on NMF when applied to X. The first two columns of W are combinations of a Gaussian signal with a spike signal. The last column contains a linear combination of the first two Gaussian and an independent spike. Thus, W has rank 3. The matrix H \u2208 generating X has its entries uniformly sampled from [0, 1).\nWe now examine three following questions regarding NMF:\nQ1: Find the direction of noise adding to X that induces high reconstruction error. As stated in Sect. 3, it is infeasible to inject a small perturbation to X that causes a large reconstruction error in NMF. Specifically, finding the direction maximizing reconstruction error corresponds to solve:\n$\\max_{||\\epsilon||<\\delta} \\min_{W\\geq 0,H\\geq 0} ||X + \\epsilon - W \\times H||_F$.\nshows our attempt to solve this with a gradient-ascent. The Rec. error (BP) line shows the reconstruction error by using PGD directly backward on the reconstruction loss $||X + \\epsilon - WH||$. The results show that the reconstruction error is just slightly larger than the amount of noise injected on the input X. This supports the theoretical analysis claiming that NMF is robust to reconstruction error.\nQ2: Is there a noise direction that causes feature matrices to change the most and become unstable? If the NMF decomposition of X is unique, then there is no bad noise for arbitrarily small \u20ac [14]. However, when we are not dealing with an arbitrarily small epsilon, a bad solution might exist.\nTo demonstrate that, we consider a perturbation $X$ of X such that its NMF's solutions would have high (or semantical) feature errors to those of X. In particular, from W generating X, we remove the spikes in the components of W and obtain a rank 2 $\\widetilde{W} \\in \\mathbb{R}^{100\\times 3}$. Then, $X$ is set to $\\widetilde{W} \\times H$. $X$ is refereed as the Removing-spike adversarial. shows the resulting feature matrix when NMF is applied on X. It is significantly different from W not only in the absence of the spikes but also in the rank.\nTo further study the Removing-spike, we generate a set of perturbations along the direction from X to X, i.e., {\u03b1X + (1 \u2212 \u03b1)X}0<\u03b1<1, and compute the corresponding W and H. The resulting reconstruction, FE, and the L2 norm errorx of reconstructing W are plotted in  Interestingly, while the errors on features maintain proportional to the reconstruction errors when the input distortion is small, a significant spike in W errors and FE errors occur around 18%. This not only shows that the features' robustness proved by Laurberg for small noise does not hold for general noises, but also indicates our proposed FE loss has strong correlation to errors on reconstructed matrix W.\nQ3: Find the noise that causes the highest feature errors.\nThe sharp increase of FE in suggests that small perturbation in a different direction may induce a much larger FE. Specifically, the noise direction from X to X would require us to perturb about 18% of the input to cause a sharp increase of 40% in FE. The goal of our attacks is to find smaller noises that can induce larger feature errors, and, consequently, reveal the threat of feature attacks.\n shows the performance of PGD-L\u221e attacks [7] leveraging our back-propagating (Algo. 1) and implicit gradients (Algo. 2) based on the FE loss. The results show that our attacks only require a perturbation of about 3% of the input (at 6\u221e = 0.02) to cause 40% distortion in features. This importantly validates that NMF is vulnerable to feature attacks. Furthermore, this large distortion in features can not be detected solely from the reconstruction errors as the reconstruction errors remain approximately equal to the magnitude of the injected noise (as discussed in Q1).\n shows a more detailed look on the attacked features. While a small adversarial perturbation (\u20ac = 0.01) can distort the reconstructed W significantly, it does not change its rank (the spikes is preserved). The rank collapses to 2 and the spike disappears at \u20ac = 0.04. Thus, feature attacks not only create large distortions in terms of metric distances, but also alter the features' semantic."}, {"title": "6. Experimental Results", "content": "This section demonstrates our findings on the vulnerabilities of NMF to LaFA in 4 real-world dataset: WTSI, Face, Swimmer and MNIST. The experiments consider the attacker has access to the data matrix X and its goal is to generate an adversarial noise resulting in high feature errors. All attacks utilize the PGD attack [7] with 40 steps leveraging on the gradients computed by either Back-propagating (Algo. 1) or Implicit (Algo. 2) methods. The entries of X are normalized between 0 and 1.\nThe experiments are conducted on HPC clusters, equipped with AMD EPYC 7713 processors with 64 cores and 256 GB of RAM, and 4 NVIDIA Ampere A100 GPUs, each with 40 GB of VRAM."}, {"title": "6.1. Results on WTSI dataset", "content": "The WTSI [3] is a genomic dataset featuring sequencing data from over 30 species, including a significant number of human genomic and cancer genome sequences. Experimental results on WTSI reveal several key insights into the vulnerabilities of NMF to feature attacks. For both L2 and Lo attacks, the FE from Back-propagation and Implicit methods exhibit a linear increase in L2 errors by increasing 8. The Back-propagation consistently shows a slightly lower error trajectory compared to the Implicit method, suggesting it may be less effective at damaging the features. Notably, the Back-propagation requires a significantly higher peak-memory usage compared to the Implicit method, i.e., 109.5 MBs versus 29.9 MBs, highlighting the memory advantage of the Implicit method. These findings are crucial for assessing the vulnerability and designing protection strategies of genomic data to adversarial attacks."}, {"title": "6.2. Results on Face Dataset", "content": "The Face dataset [4] comprises a set of 2,429 face and 4,548 non-face images. Due to the high computational complexity associated with back-propagating the NMF, we focus on 471 19 \u00d7 19-grayscale face images from the test set, forming $X \\in \\mathbb{R}^{471\\times 361}$. We factorize X into 5 features. This choice is driven by the preliminary analysis indicating that this number of features captures the essential variability in the facial data while avoiding overfitting.\nNMF feature extraction vulnerability is evident in the Face dataset, as depicted in  Notably, the Implicit method displays a much more significant advantage in attacking performance compared to Back-propagate compared to the results of WTSI. showcases the effects of Lo\u221e norm-based FE attacks on the reconstructed facial features matrix W from a specific dataset comprising face images. Each row represents different levels of perturbation's magnitude & in L\u221e, ranging from 0.002 to 0.008. The clean row serves as a baseline, showing unperturbed latent features. As & increases, noticeable visual distortions appear in the reconstructed features, particularly highlighted in the blue box. These distortions indicate a degradation in feature integrity, affecting the clarity and structure of facial features. Moreover, the red box highlights the introduction of new feature components, underscoring a significant adversarial impact."}, {"title": "6.3. Results on Swimmer Dataset", "content": "The Swimmer dataset [5] comprises 256 images depicting top-down representations of an individual swimming. This dataset is specifically designed to facilitate the exploration of sparse representations and the effectiveness of various signal-processing algorithms. We performed NMF factorization of the dataset into 16 features as suggested by [5].\nThe results on Swimmer is reported in . Both L2 and Lo attacks show a significant distortions in L2 errors as & increases. We cannot conduct Back-propagation attacks on the Swimmer due to memory constraint. On the other hand, the Implicit method demonstrates an escalation in error at a larger epsilon values compared to previous datasets. The reason is the latent features W of Swimmer are much cleaner and more distinctive.  exemplifies the degradation of reconstructed Swimmer's latent feature at higher noise. The sequence of images demonstrates the impact on the visual integrity on the recovered features: with large \u03b5, the outlines and orientations of the swimmers become distorted and progressively less recognizable compared to the clean and ground truth images. This visual distortion is particularly significant at \u025b = 0.45, at which some adversarial pixels begin to appear at the bottom corners of some features (orange box). This showcases the effectiveness of our adversarial attacks in disrupting the NMF's ability to reconstruct the original latent features."}, {"title": "6.4. Results on MNIST Dataset", "content": "The MNIST dataset [6] is a collection of handwritten digits commonly used for training and testing image processing systems. It contains 70,000 28 \u00d7 28-grayscale images of digits. As the MNIST dataset comprises images across 10 classes, we factorized it into 10 features.\nThe results of Implicit PGD attack on MNIST in  displays a sharp increase in feature errors under both L2 and L perturbations at a relatively low \u025b. The MNIST's extracted features under different & clearly illustrates the degradation of NMF. Starting from a baseline of clean, clear images, the introduction of even a small perturbation (\u03b5 = 0.002) can affect the edges and finer details of the digits. As the perturbation grows, more pronounced visual artifacts appear, particularly distorting digits with complex structures such as '9', '3', and '5'. These digits start to merge with the background or deform significantly."}, {"title": "7. Conclusion and Future Work", "content": "Throughout our investigation, we systematically explored the susceptibility of NMF to adversarial attacks across a spectrum of both synthetic and real-world datasets, with both Back-propagation and Implicit methods. Our findings demonstrated that adversarial perturbations could significantly impair the feature extraction capabilities of NMF, as evidenced by both norm-based metrics and direct visualizations of the corrupted features. The novel attack strategies introduced in this study also provide a significant step forward in understanding and enhancing the robustness of unsupervised learning frameworks.\nWhile our method has shown promising results in terms of performance and memory efficiency, the current implementation exhibits a running time complexity that may not be suitable for large-scale applications or real-time processing. To enhance the practicality and scalability of our approach, we aim to address the running time complexity of the proposed method in our future work."}]}