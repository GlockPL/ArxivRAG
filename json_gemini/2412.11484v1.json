{"title": "Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents", "authors": ["Wonje Choi", "Woo Kyung Kim", "SeungHyun Kim", "Honguk Woo"], "abstract": "For embodied reinforcement learning (RL) agents interacting with the environment, it is desirable to have rapid policy adaptation to unseen visual observations, but achieving zero-shot adaptation capability is considered as a challenging problem in the RL context. To address the problem, we present a novel contrastive prompt ensemble (CONPE) framework which utilizes a pretrained vision-language model and a set of visual prompts, thus enabling efficient policy learning and adaptation upon a wide range of environmental and physical changes encountered by embodied agents. Specifically, we devise a guided-attention-based ensemble approach with multiple visual prompts on the vision-language model to construct robust state representations. Each prompt is contrastively learned in terms of an individual domain factor that significantly affects the agent's egocentric perception and observation. For a given task, the attention-based ensemble and policy are jointly learned so that the resulting state representations not only generalize to various domains but are also optimized for learning the task. Through experiments, we show that CONPE outperforms other state-of-the-art algorithms for several embodied agent tasks including navigation in AI2THOR, manipulation in egocentric-Metaworld, and autonomous driving in CARLA, while also improving the sample efficiency of policy learning and adaptation.", "sections": [{"title": "Introduction", "content": "In the literature of vision-based reinforcement learning (RL), with the advance of unsupervised techniques and large-scale pretrained models for computer vision, the decoupled structure, in which visual encoders are separately trained and used later for policy learning, has gained popularity [1, 2, 3]. This decoupling demonstrates high efficiency in low data regimes with sparse reward signals, compared to end-to-end RL. In this regard, several works on adopting the decoupled structure to embodied agents interacting with the environment were introduced [4, 5], and specifically, pretrained vision models (e.g., ResNet in [6]) or vision-language models (e.g., CLIP in [7, 8]) were exploited for visual state representation encoders. Yet, it is non-trivial to achieve zero-shot adaptation to visual domain changes in the environment with high diversity and non-stationarity, which are inherent for embodied agents. It was rarely investigated how to optimize those popular large-scale pretrained models to ensure the zero-shot capability of embodied agents.\nEmbodied agents have several environmental and physical properties, such as egocentric camera position, stride length, and illumination, which are domain factors making significant changes in agents' perception and observation. In the target (deployment) environment with uncalibrated settings on those domain factors, RL policies relying on pretrained visual encoders remain vulnerable to domain changes."}, {"title": "Problem Formulation", "content": "In RL formulation, a learning environment is defined as a Markov decision process (MDP) of (S, A, P, R) with state space $s \\in S$, action space $a \\in A$, transition probability $P : S \\times A \\rightarrow S$ and reward function $R : S \\times A \\rightarrow R$. The objective of RL is to find an optimal policy $\\pi^* : S \\rightarrow A$ maximizing the sum of discounted rewards. For embodied agents, states might not be fully observable, and the environment is represented by a partially observable MDP (POMDP) of a tuple (S, A, P, R, \u03a9, \u039f) with an observation space $o \\in \u03a9$ and a conditional observation probability [11] $O : S \\times A \\rightarrow \u03a9$.\nGiven visual domains in the dynamic environment, we consider policy adaptation to find the optimal policy that remains invariant across the domains or is transferable to some target domain, where each domain is represented by a POMDP and domain changes are formulated by different O. We denote domains as $D = (\u03a9, O)$. Aiming to enable zero-shot adaptation to various domains, we formulate"}, {"title": "Our Approach", "content": "To enable zero-shot policy adaptation to unseen domains, we develop the CONPE framework consisting of (i) prompt-based contrastive learning with the CLIP visual encoder, (ii) guided-attention-based prompt ensemble, and (iii) zero-shot policy deployment, as illustrated in Figure 2. The capability of the CLIP visual encoder is enhanced using multiple visual prompts that are contrastively learned on expert demonstrations for several domain factors. This establishes the visual prompt pool in (i). Then, the prompts are used to train the guided-attention-based ensemble with the environment in (ii). To enhance learning efficiency and interpretability of attention weights, we use the cosine similarity of embeddings. The attention module and policy are jointly learned for a specific task so that resulting state representations tend to generalize across various domains and be optimized for task learning. In deployment, a non-stationary environment where its visual domain varies according to the environment conditions and agent physical properties is considered, and the zero-shot performance is evaluated in (iii)."}, {"title": "Prompt-based Contrastive Learning", "content": "To construct domain-invariant representations with respect to a specific domain factor for egocentric perception data, we adopt several contrastive tasks for visual prompt learning, which can be learned on a few expert demonstrations. For this, we use a visual prompt\n$p^u = [e_1, e_2, ..., e_u], e \\in R^d$\\nwhere e is a continuous learnable vector with the image patch embedding dimension d (e.g., 768 for CLIP visual encoder) and u is the length of a visual prompt. Let a pretrained model $T_\u03c6$ parameterized by $\u03c6$ maps observations $o \\in \u03a9$ to the embedding space Z. With a contrast function $\\mathcal{P}: \\Omega \\times \\Omega \\rightarrow \\{0,1\\}$ [1, 2, 12] to discriminate whether an observation pair is positive or not,"}, {"title": "Guided-Attention-based Prompt Ensemble", "content": "To effectively integrate individual prompted embeddings from multiple visual prompts into a task-specific state representation, we devise a guided-attention-based prompt ensemble structure, as shown in Figure 3 where the attention weights on the embeddings are dynamically computed via the attention module G for each observation.\nGiven observation o and the learned visual prompt pool $p^u$, an image embedding $z_0 = T_\u03c6(o)$ and prompted embeddings $z = [z_1 = T_\u03c6(o, p_1), ..., z_n]$ are calculated. Then, $z_0$ and z are fed to the attention module G, where attention weights $w_i$ for each prompted embedding $z_i$ are optimized. Since directly computing the attention weights using $z_0$ and z is prone to have an uninterpretable local optima, we introduce a guidance score $g_i$ based on the cosine similarity between the input image and visual prompted image embeddings in Z, i.e., $g_i = \\frac{(z_0, z_i)}{||z_0|| \\cdot ||z_i||}$. Given that larger $g_i$ signifies a stronger conformity of an observation to the domain factor relevant to the prompted embedding $z_i$, we use $g_i$ to steer the attention weights, aiming to not only improve learning efficiency but also provide interpretability. With guidance $g_i$, we compute the attention weights $w_i$ by\n$w_i = \\frac{exp(u_i / T)}{\\sum_k exp(u_k / T)} $, $u_i = \\frac{(z_0, k_i)}{\\sqrt{d}} g_i$\\n"}, {"title": "Evaluation", "content": "Experiments. We use AI2THOR [9], Metaworld [14], and CARLA [10] environments, specifically configured for embodied agent tasks with dynamic domain changes. These environments allow us to explore various domain factors such as camera settings, stride length, rotation degree, gravity, illuminations, wind speeds, and others. For prompt-based contrastive learning (in Section 3.2), we use a small dataset of expert demonstrations for each domain factor (i.e., 10 episodes per domain factor). For prompt ensemble-based policy learning (in Section 3.3), we use a few source domains randomly generated through combinatorial variations of the seen domain factors (i.e., 4 source domains). In our zero-shot evaluations, we use target domains that can be categorized as either seen or unseen. The seen target domains are those encountered during the phase of prompt-based contrastive learning, while these domains are not present during the phase of prompt ensemble-based policy learning. On the other hand, the unseen target domains refer to those that are entirely new, implying that they are not encountered during either learning phases.\nBaselines. We implement several baselines for comparison. LUSR [15] is a reconstruction-based domain adaptation method in RL, which uses the variational autoencoder structure for robust representations. CURL [2] and ACT [1] employ contrastive learning in RL frameworks for high sample-efficiency and generalization to visual domains. ACO [12] utilizes augmentation-driven and behavior-driven contrastive tasks in the context of RL. EmbCLIP [7] is a state-of-the-art embodied AI model, which exploits the pretrained CLIP visual encoder for visual state representations.\nImplementation. We implement CONPE using the CLIP model with ViT-B/32, similar to VPT [16] and CoOp [17]. In prompt-based contrastive learning, we adopt various contrastive learning schemes including augmentation-driven [2, 1, 18] and behavior-driven [12, 19, 20, 21] contrastive learning, where the prompt length sets to be 8. In policy learning, we exploit online learning (i.e., PPO [22]) for AI2THOR and imitation learning (i.e., DAGGER [23]) for egocentric-Metaworld and CARLA."}, {"title": "Zero-shot Performance", "content": "Table 1 shows zero-shot performance of CONPE and the baselines across source, seen and unseen target domains. We evaluate with 3 different seeds and report the average performance (i.e., task success rate in AI2THOR and egocentric-Metaworld, the sum of rewards in CARLA). As shown in Table 1(a), CONPE outperforms the baselines in the AI2THOR tasks. It particularly surpasses the most competitive baseline, EmbCLIP, by achieving 5.2 ~ 5.7% higher success rate for seen target domains, and 6.9 ~ 20.7% for unseen target domains. For egocentric-Metaworld, as shown in Table 1(b), CONPE demonstrates superior performance with a significant success rate for both seen and unseen target domains, which is 17.3 ~ 24.0% and 18.0 ~ 20.0% higher than EmbCLIP, respectively. For autonomous driving in CARLA, we take into account external environment factors, such as weather conditions and times of day, as domain factors that can influence the driving task. In Table 1(c), CONPE consistently maintains competitive zero-shot performance across all conditions, outperforming the baselines.\nIn these experiments, LUSR shows relatively low success rates, as the reconstruction-based representation model can abate some task-specific information from observations, which is critical to conduct vision-based complex RL tasks. EmbCLIP shows the most comparative performance among the baselines, but its zero-shot performance for target domains is not comparable to CONPE. In contrast,"}, {"title": "Prompt Ensemble with a Pretrained Policy", "content": "While we previously presented joint learning of a policy and the attention module G, here we also present how to update G for a pretrained policy \u03c0 to make the policy adaptable to domain changes. In this case, we add a policy prompt $p_{pol}$ to concentrate on task-relevant features from observations for"}, {"title": "Ablation Study", "content": "Here we conduct ablation studies with AI2THOR. All the performances are reported in success rates."}, {"title": "Prompt Ensemble Scalability", "content": "Table 3 evaluates CONPE with respect to the number of prompts (n). CONPE effectively enhances zero-shot performance for both seen and unseen target domains through prompt ensemble that captures various domain factors. Compared to the case of n = 2, for n = 10, there was a significant improvement in zero-shot performance for both seen and unseen target domains, with increases of 42.8% and 36.7%, respectively. For n \u2265 10, we observe stable performance that specifies that CONPE can scale for combining multiple prompts to some extent."}, {"title": "Prompt Ensemble Methods", "content": "Tabel 4 compares the performance of various prompt integration methods [24, 25, 26] including our guided attention-based prompt ensemble. We denote prompt-level integration as COM, and prompted embeddings-level integration as ENS. UNI-AVG and WEI-AVG refer to uniform average and weighted average mechanisms, respectively. CONPE achieves superior success rates over the most competitive ensemble method ENS-UNI-AVG, showing 3.5% and 14.2% performance gain for seen and unseen target domains."}, {"title": "Prompt Ensemble Adaptation Method", "content": "Table 5 shows the effect of our ensemble adaptation method for the situation when a pretrained policy is given. As explained in Section 4.2, in this situation, CONPE can update the attention module with an additional prompt $p_{pol}$. Note that w refers to this case, while w/o $p_{pol}$ corresponds to the other case of using the attention module without $p_{pol}$. In addition, E2E denotes the fine-tuning of both the policy and the attention module along with $p_{pol}$. The results demonstrate that our method enhances the zero-shot performance of the pretrained policy, showing that $p_{pol}$ facilitates the extraction of task-specific features."}, {"title": "Semantic Regularized Data Augmentation", "content": "So far, we have only utilized vision data, but here, we discuss one extension of CONPE using semantic information. Specifically, we use a few samples of object-level text descriptions to regularize the data augmentation process in policy learning. This aims to mitigate overfitting issues [27, 28]. The detailed explanations can be found in Appendix. As shown in Table 6, CONPE with semantic data (w Semantic) consistently yields better performance than CONPE without semantic data (w/o Semantic) for all noise scale settings (\u03b4). Note that the noise scale manages the variance of augmented prompted embeddings. This experiment indicates that CONPE can be improved by incorporating semantic information."}, {"title": "Related Work", "content": "Adaptation in Embodied AI. In the literature of robotics, numerous studies focused on developing generalized visual encoders for robotic agents across various domains [29, 30], exploiting pretrained visual encoders [31, 32], and establishing robust control policies with domain randomized techniques [33, 34]. Furthermore, in the field of learning embodied agents, a few works addressed adaptation issues of agents to unseen scenarios in complex environments, using data augmentation techniques [35, 36, 37, 38, 39] or adopting self-supervised learning schemes [40, 41, 42]. Recently, several works showed the feasibility and benefits of adopting large-scale pretrained vision-language models for embodied agents [7, 43, 44, 45]. Our work is in the same vein of these prior works of embodied agents, but unlike them, we explore visual prompt learning and ensembles, aiming to enhance both zero-shot performance and sample-efficiency.\nDecoupled RL Structure. The decoupled structure, where a state representation model is separated from RL, has been investigated in vision-based RL [46, 2, 15]. Recently, contrastive representation learning on expert trajectories gains much interest, as it allows expert behavior patterns to be incorporated into the state encoder even when a policy is not jointly learned [1, 19]. They established generalized state representations, yet in that direction, sample-efficiency issues in both representation learning and policy learning remain unexplored.\nPrompt-based Learning. Prompt-based learning or prompt tuning is a parameter-efficient optimization method for large pretrained models. Prompt tuning was used for computer vision tasks, optimizing a few learnable vectors in the text encoder [17], and it was also adopted for vision transformer models to handle a wide range of downstream tasks [16]. Recently, visual prompting [47] was introduced, and both visual and text prompt tuning were explored together in the multi-modal embedding space [48, 49]. We also use visual prompt tuning, but we concentrate on the ensemble of multiple prompts to tackle complex embodied RL tasks. We take advantage of the fact that the generalized representation capability of different prompts can vary depending on a given task and domain, and thus we strategically utilize them to enable zero-shot adaptation of RL policies."}, {"title": "Conclusion", "content": "Limitation. Our CONPE framework exploits visual inputs and their relevant domain factors for policy adaptation. For environments where domain changes extend beyond those domain factors, the adaptability of the framework might be constrained. In our future work, we will adapt the framework with semantic knowledge based on pretrained language models to improve the policy generalization capability for embodied agents in dynamic complex environments and to cover various scenarios associated with multi-modal agent interfaces.\nConclusion. In this work, we presented the CONPE framework, a novel approach that allows embodied RL agents to adapt in a zero-shot manner across diverse visual domains, exploring the ensemble structure that incorporates multiple contrastive visual prompts. The ensemble facilitates domain-invariant and task-specific state representations, thus enabling the agents to generalize to visual variations influenced by specific domain factors. Through various experiments, we demonstrated that the framework can enhance policy adaptation across various domains for vision-based object navigation, rearrangement, manipulation tasks as well as autonomous driving tasks."}, {"title": "Prompt-based Contrastive Learning", "content": "To construct domain-invariant representations efficiently for egocentric perception data in embodied agent environments, we adopt contrastive tasks for visual prompt learning on the domain factors, which can be learned from a few expert demonstrations.\nWhen conducting prompt-based contrastive learning, as shown in Figure 6 and explained below, we specifically adopt several methods to generate positive visual observation pairs for different domain factors.\nConsider a pretrained model $T_\u03c6$ parameterized by $\u03c6$ that maps observations $o \\in \u03a9$ to the embedding space Z. The contrast function $\\mathcal{P} : \\Omega \\times \\Omega \\rightarrow \\{0,1\\}$ discriminates whether an observation pair is positive or not. Then, we fine-tune $T_\u03c6$ by learning a visual prompt $p^u$ through contrastive learning, where the contrastive loss function [13] is defined as Equation (2) in the main manuscript.\nBehavior-driven Contrast. Similar to [19], we exploit expert actions to obtain positive sample pairs from expert trajectories of different domains. With observation and action pairs (o, a), (o', a'), a behavior-driven contrast function is defined as $\\mathcal{P}_{beh}(o, o') = \\mathbb{1}_{a=a'}$. If the environment has a discrete action space, the behavior-driven contrast can be applied immediately to obtain positive sample pairs; otherwise, it can be applied after discretizing continuous actions with unsupervised clustering algorithms such as k-means clustering [50].\nAugmentation-driven Contrast. Similar to visual domain randomization techniques [51, 52], we use data augmentation (e.g., color perturbation [53]) for unstructured pixel-level visual domain factors such as illumination. An augmentation-driven contrast function is defined as $\\mathcal{P}_{aug}(o, o') = \\mathbb{1}_{o'=AUG(o)}$, where AUG augments o.\nTimestep-driven Contrast. Similar to [1], we exploit timesteps of expert trajectory to obtain positive sample pairs across different domains. With observation and timestep pairs (o, t), (o', t'), a timestep-driven contrast function is defined as $\\mathcal{P}_{tim}(o, o') = \\mathbb{1}_{t=t'}$, where t \u2212 k \u2264 t \u2264 t + k and k is hyperparameter."}, {"title": "Prompt Ensemble with a Pretrained Policy", "content": "As mentioned in the main manuscript, we devise an optimization method to update G specifically for a pretrained policy \u03c0. Specifically, we use a policy prompt $p_{pol}$ that focuses on task-relevant features from observations for \u03c0. By incorporating the prompted embedding $\\bar{z}_0$, which contains these task-relevant features, into the guided-attention-based ensemble, we can effectively integrate the policy with the attention module, resulting in $\\pi(G(\\bar{z}_0, z))$. Here, $\\bar{z}_0$ is obtained by applying the transformation to the observation o using the policy prompt $p_{pol}$.\nAs such, CONPE enables efficient adaptation of the attention module to a pretrained policy by fine-tuning only a small number of parameters. This achieves robust zero-shot performance for unseen domains in different tasks."}, {"title": "Semantic Regularized Data Augmentation", "content": "For a source environment that is sufficiently accessible, the attention module and policy network can be jointly trained by RL algorithms. In this case, to address overfitting problems to the source environment, data augmentation methods can be adopted. For example, when training a policy, it is feasible to add Gaussian noise to each prompted embedding as part of data augmentation to avoid overfitting [27, 28].\nTo enhance both policy optimization and zero-shot performance, we investigate semantic regularization schemes in the CLIP embedding space, which are specific to the prompt ensemble-based policy learning. Specifically, using a few object-level descriptions in datasets, we control the noise effectively.\nIn our semantic regularization, the language prompt $p = [e_1, e_2, ..., e_u]$, $e \\in R^{d'}$ is pretrained with description data and fixed p. Then, p is used as a semantic regularizer, where e is a continuous learnable vector of the word embedding dimension d' (e.g., 512 for CLIP language encoder) and u' is the length of a language prompt. Similar to [17], we adopt language prompt learning schemes. We"}, {"title": "AI2THOR", "content": "Environment settings. We use AI2THOR [9], a large-scale interactive simulation platform for Embodied AI. In AI2THOR, we use iTHOR datasets that have 120 room-sized scenes with bedrooms, bathrooms, kitchens, and living rooms. iTHOR includes over 2000 unique objects based on Unity 3D. Among embodied AI tasks in AI2THOR, we evaluate our framework with object goal navigation and point goal navigation tasks. We also test the image goal navigation task, a modified version of the object goal navigation, as well as the room rearrangement task for adaptation to a pretrained policy."}, {"title": "Egocentric-Metaworld", "content": "Environment settings. The Metaworld benchmark [14] includes diverse table-top manipulation tasks that require a Sawyer robot to interact with various objects. With different objects, such as door and button, the robot needs to manipulate them based on the object's affordance, leading to different reward functions. At each timestep, the Sawyer robot conducts a 4-dimensional fine-grained action that determines the 3D position movements of the end-effector and the variation of gripper openness."}, {"title": "CARLA", "content": "Environment settings. CARLA [10] is a self-driving simulation environment where an agent navigates to the target location while avoiding collisions and lane crossings. For experiments, we use the CARLA simulator v0.9.13 and choose Town10HD as our map. For RL formulation, we incorporate the RGB image data and sensor values (acceleration, velocity, angular velocity) into states, and use control steer, throttle, and brake as actions. Each action ranges from -1 to 1. The actions are automatically calibrated when the speed of the car reaches the maximum. The agent is evaluated based on the reward function below that involves the desired velocity and goal distance,\nreward = $v_t \\cdot \\frac{U_{target}}{||U_{target}||} - \\frac{goal\\_distance}{100}$\\nwhere $v_t$ denotes the agent's current velocity and $U_{target}$ denotes the target velocity."}, {"title": "LUSR", "content": "LUSR is a domain adaptation method that utilizes the latent embedding of encoder-decoder models to extract generalized representations. LUSR uses \u03b2-VAE to learn disentangled representations of different visual domains. For implementation, we use the open source (https://github.com/KarlXing/LUSR). When implementing LUSR, we use a CNN encoder for both DAE and \u03b2-VAE. We conduct online policy learning with PPO algorithms [22]. The hyperparameter settings are summarized in Table 9."}, {"title": "CURL and ATC", "content": "CURL and ATC are a contrastive learning based framework for visual RL. CURL uses contrastive representation learning to extract discriminative features from raw pixels which greatly"}, {"title": "ACO", "content": "ACO utilizes augmentation-driven and behavior-driven contrastive tasks in the context of RL. For implementation, we use the open source (https://github.com/metadriverse/ACO). The hyperparameter settings are summarized in Table 12, where other settings are the same as in Table 9(b)."}, {"title": "EmbCLIP", "content": "EmbCLIP is a state-of-the-art model for embodied AI tasks. By using CLIP as the visual encoder, EmbCLIP extracts generalized representation which is useful for an embodied agent, enabling the agent to effectively generalize to different environments and tasks. We use the open source (https://github.com/allenai/embodied-clip) for the implementation of AI2-THOR environments. To evaluate this with the CARLA simulator, we also use the open source (https://github.com/openai/CLIP). The configurations for policy learning are the same as in Ta-ble 9(b)."}, {"title": "ConPE", "content": "The procedure of our CONPE consists of prompt learning and policy learning steps.\nPrompt-based Contrastive Learning. In prompt learning step, for sample-efficiency, CONPE conducts prompt-based contrastive learning, exploiting the pretrained CLIP model. We set the length of visual prompts to be 8 for each contrastive learning with Equation (2) in the main manuscript. In cases when metadata is available (the cases of using the semantic regularized data augmentation), 8 language prompts are used for (7). The hyperparameter settings are summarized in Table 13.\nPrompt Ensemble-based Policy Learning. CONPE obtains domain-invariant states from observa-tions using the ensemble of multiple prompts. The prompt attention module G consists of as many"}, {"title": "Zero-shot Performance for Seen Domains Factors", "content": "Table 14 presents the zero-shot performance of CONPE across specific domain factors (DF). For example, DF0 refers to various domains where the camera position is a domain factor of interest, and LUSR's 48.5 in DF0 indicates the success rate of LUSR for the domains generated by different camera positions.\nAs shown, CONPE maintains robust zero-shot performance for all the cases (DF0~ DF9), compared to the baselines. Additionally, as shown in Figure 10 where a specific domain factor is changed, the attention weights assigned to the prompted embedding (denoted as $P_n$, where n = {0..9}) that is trained for the corresponding domain factor are high (in the bright color)."}, {"title": "Prompt Ensemble with a Pretrained Policy", "content": "Table 15 reports detailed zero-shot performance for the scenarios when a pretrained policy is given. We additionally test widely used baselines that leverage large pretrained models in the fields of vision and natural language, specifically in the context of prompt-meta learning. ATTEMPT [57] is a parameter-efficient multi-task language model tuning method that transfers knowledge across different tasks via a mixture of soft prompts. SESOM [58] is a soft prompts ensemble method that leverages multiple source tasks and effectively improves few-shot performance of prompt tuning by transferring knowledge from the source tasks to a target task. CONPE demonstrates the ability to effectively improve zero-shot performance with a small number of samples, especially when a pretrained policy is given. As shown in Figure 12 (a) and (b), prompt ensemble adaptation demonstrates an increase in success rate with a small number of samples in both the train and unseen environments of the pretrained policy, compared to other baselines. This results present the sample-efficiency of our prompt ensemble adaptation method."}, {"title": "Semantic Regularized Data Augmentation", "content": "Table 16 shows the zero-shot performance for semantic regularized data augmentation in CONPE, where language data is additionally used. As shown, CONPE with language data (w Semantic Reg.) consistently yields better performance over CONPE without language data (w/o Semantic Reg.) across various noise scales (\u03b4). As the deviation (\u03b4) of the Gaussian noise varies, it is observed that larger deviation does not necessarily lead to performance improvement. This indicates that"}]}