{"title": "EARLY PREDICTION OF THE TRANSFERABILITY OF BOVINE EMBRYOS FROM VIDEOMICROSCOPY", "authors": ["Y. Hachani", "P. Bouthemy", "E. Fromont", "S. Ruffini", "L. Laffont", "A. De Paula Reis"], "abstract": "Videomicroscopy is a promising tool combined with machine learning for studying the early development of in vitro fertilized bovine embryos and assessing its transferability as soon as possible. We aim to predict the embryo transferability within four days at most, taking 2D time-lapse microscopy videos as input. We formulate this problem as a supervised binary classification problem for the classes transferable and not transferable. The challenges are three-fold: 1) poorly discriminating appearance and motion, 2) class ambiguity, 3) small amount of annotated data. We propose a 3D convolutional neural network involving three pathways, which makes it multi-scale in time and able to handle appearance and motion in different ways. For training, we retain the focal loss. Our model, named SFR, compares favorably to other methods. Experiments demonstrate its effectiveness and accuracy for our challenging biological task.\nIndex Terms- video-microscopy, embryo, classification, CNN", "sections": [{"title": "1. INTRODUCTION", "content": "Most techniques used to study the mechanisms of embryonic development are incompatible with embryo survival. Video microscopy applied to bovine embryos produced by in vitro fertilization (IVF) (Fig.1), is a promising tool compatible, with survival, in association with the analysis power of machine learning techniques. It allows us to study the early development and to assess the transferability of in vitro fertilized embryos, i.e., the capacity to reach the blastocyst stage, suitable for transfer to a cow uterus. From the application point of view, having this ability to correctly predict on a large scale whether embryos can be transferred or not, is crucial for cattle breeding. With current methods, only 30% of transferred blastocysts actually result in pregnancy. This achievement should help reduce pregnancy failures and thus unnecessary inseminations.\nHowever, two major problems arise: i) detailed analysis of each video is time-consuming for biologists and potentially limits a day-to-day practice, ii) to enable advanced biological studies on early mechanisms influencing embryo development, it is preferable to know the embryo transferability as soon as possible. Therefore, automating the transferability prediction by operating directly on videos of embryonic development is of key interest, while achieving the task the earliest possible.\nOur overall objective is then to achieve a correct prediction of the embryo transferability within four days at most, taking 2D time-lapse videos as input to be analyzed by 3D convolutional neural networks. This achievement will offer biologists new prospects such as better embryo sorting for further studies on embryonic genome activation (EGA) (at day 4) or morula formation (at day 5), decision-taking for earlier transfer into the uterus of a recipient female, which limits the duration of culture under sub-optimal conditions.\nWe formulate this problem as a two-class supervised classification one: the embryo is transferable (T class) or not transferable (NT class). This problem is challenging for three main reasons: 1) tricky embryo appearance and motion, 2) class ambiguity, and 3) small amount of annotated data. First, as illustrated in Fig.2, the microscopy videos display little contrast, involve a lot of noise and complex motion with transparency effects. The diverse embryos studied often exhibit poorly discriminating appearance between classes (see Fig.2 again), while the videos show complex morphological and temporal processes. Second, the intra-class variability is high, in the sense that the trajectories of the embryo development may substantially vary within a given class. Conversely, the inter-class distance is low. Indeed, observed development of two embryos from the two different classes may be fairly similar for the four first days. Third, because labeling is costly, there is only a limited database of videos labeled transferable or not transferable.\nThe rest of the paper is organized as follows. Section 2 describes related work. In Section 3, we present our 3D network with three pathways, and the associated training losses. Section 4 reports quantitative and comparative evaluations of the method. Section 5 contains concluding remarks."}, {"title": "2. RELATED WORK", "content": "Biologists have been working on embryonic development and phenotype formation for several decades. Thanks to IVF, significant advances have been achieved leading to major progress in medicine and breeding. Video-microscopy for bovine embryos has enabled biologists to observe different development trajectories leading to distinct phenotypes.\nIn human artificial reproductive medicine, the objective is to reduce the need for multiple pregnancies and losses during pregnancy. In [9], a first human embryo selection model was introduced based on manual annotation and a decision tree. Then, the authors of [20] developed an embryo selection model using a single static image captured by light microscopy. If the results of these studies were promising, the learning process was carried out retrospectively. Indeed, the pregnancy outcomes were predicted from embryos that were previously manually selected by an expert for transfer, leaving a large number of embryos outside the studies. These results may therefore include a certain level of over-fitting, as well as a number of biases related to embryo manipulation and transfer. Recently, a deep learning approach has been adopted in [1] to select human embryos from time-lapse image sequences acquired over five days, knowing that the video acquisition started around 24 hours after insemination. They used the Inflated 3D ConvNet (I3D) of [2] followed by a recurrent LTSM network. However, training is still carried out retrospectively. It leverages a very large dataset (about 100,000 videos).\nDeep learning (DL) has also been used on human embryo videos to tackle different problems. For instance, in [5], the authors proposed a vector quantized variational autoencoder (VQ-VAE) to segment blastomere instance. Latest work mostly focuses on characterizing the different stages of embryo development, namely cell cleavage with intermediate stages defined by cell count (from 1-cell to 4-cell, sometimes up to 8-cell count), morula and blastocyst. In [7], the authors elaborated a development stage detector based on a 2D-CNN followed by a LSTM network classifier and added a synergic loss to learn embryo-independent features. In [10], the development stage classification was improved with EmbryosFormer, a three-headed model designed as an encoder-decoder deformable transformer inspired from Deformable DETR [23]. The authors of [15] adopted a different approach using the object detection technique YOLO v5 [11] and performing cell counting.\nBovine embryos are more difficult to study than their human counterparts, since their cells are darker, which makes, for example, cell counting quite difficult. Besides, biologists have reported [12] that the development trajectories may reflect different adaptation mechanisms and aptitudes for future gestation. In [12], the authors proceeded prospectively: they first characterized the trajectories and verified their biological interest, limiting the biases of the retrospective studies mentioned above. Observations based on the embryonic morphokinetic features have led to distinguish several families of trajectories distributed in transferable embryos (here, named T class) and non-transferable embryos (here, named NT-class). A prediction model was defined, based on random forest, leveraging many detailed annotations for each video.\nEmbryonic development could be seen as a form of action (in the computer vision terminology), and then, our classification problem could be seen as an action classification one. We therefore briefly review work on action recognition in videos since the advent of deep learning. The pioneering work [17] introduced a two-stream convolutional network taking both images and optical flow fields as input to leverage appearance and motion for action recognition. However, we experienced that optical flow was poorly estimated on bovine embryo videos. By considering the spatio-temporal video as a 3D volume, 3D convolutional networks have been extensively adopted since then for action recognition, as shown for instance in [2] with the inflated 3D convnet or in [19] with a 3D ResNet. In [3], the authors proposed a 3D network comprising two pathways, a Slow one devoted to appearance information with input video at a low frame rate, and a Fast one"}, {"title": "3. MODEL DESCRIPTION", "content": "As stated above, we address a two-class classification problem to predict the transferability of the IVF bovine embryos. The two classes are transferable embryos (T class - embryos with a potential of establishing a pregnancy, they can be transferred into a female recipient) vs non-transferable embryos (NT class - embryos with no or very low potential of establishing a pregnancy, they should not be transferred).\nIn practice, the expert biologist annotates the videos on a longer temporal basis than 4 days, to up to six or even eight days of the embryo development. This is nevertheless a light annotation, one label (the class) per video. In some cases, the expert biologist may need to understand the whole evolution of the embryo over the full video to decide on the class, as development trajectories may be similar up to a certain stage. On our side, we take these annotations for the same videos but restricted to four days of development. This explains why the inter-class distance may be small when considering videos of 4-day development only. Consequently, automatically predicting transferability at four days, i.e., carrying out our binary classification, is a complex video analysis task. In addition, only a small set of annotated videos is available, the images are noisy and poorly contrasted, subject to transparency effects, and motions in the video are not easy to identify."}, {"title": "3.1. Network architecture", "content": "We have designed a 3D network for our two-class classification problem in order to achieve early prediction of IVF bovine embryo transferability. We believe that a 3D convolutional network is more adapted to properly capture the spatio-temporal features characterizing the embryonic development, than for instance a recurrent neural network. Indeed, the morphokinetic features are quite intricate and an embryo development is not as smooth along time as a human action in a video. It is mainly specified by a few discrete events corresponding to the cell divisions, with rather random local motions in between.\nOur 3D network presented in Fig.3 involves three pathways combined, with directed lateral connections. As in the SlowFast network [3], we have the Slow pathway taking the input video at a low frame rate and mainly dedicated to capture spatial features in images, the Fast pathway with input video at a high frame rate mainly devoted to the temporal features. The Fast pathway has a fraction \u03b2 of channels and a temporal resolution \u03b1 times higher than the Slow pathway. We call the third pathway Regular. It takes the input video at the same rate as Fast pathway, but it involves more channels in each layer. As motivated in the ablation study (Section 4.3), we use ResNet18 for the three pathways to build a light 3D network, which speeds up training and mitigates over-fitting. We replaced all ResNet batch normalization layers with group normalization layers [21], since group normalization is at least as good as batch normalization when trained with small or medium batch sizes, and it allows us to use Py-torch Lightning gradient accumulation technique efficiently.\nWe found the use of the three pathways beneficial, because appearance and motion are rather intertwined due to the transparency of cell membranes and the fact that we observe 2D projections, partly overlaid, of 3D cells. The three pathways bring complementary ways of handling appearance and motion to provide the right prediction. The outputs delivered by the last layer of each pathway are concatenated to feed the classifier. Directed lateral connections are included between pathways, We focused on two combinations of the lateral connections. The first one is illustrated in Fig.3 and comprises a connection from the Regular to the Fast pathways and from the Fast to the Slow pathways. The second one involves a fusion from the Regular to the Slow pathways and from the Fast to the Slow pathways; the Regular and the Fast pathways are not connected. We selected the first combination as explained in the ablation study (Section 4.3). We call our method SFR."}, {"title": "3.2. Loss function", "content": "We could adopt different loss functions. Since our data are unbalanced between the two classes T and NT, we have considered the focal loss [6], initially introduced for the object detection task. The focal loss can contribute to correct this imbalance, while focusing on the most difficult examples. The focal loss writes:\n$L_f(v, y) = -\\sum_{c=1}^{2} \\alpha_c (1 - p(y_c|v))^\\gamma p(y_c|v) \\log (y_c v), (1)$\nwhere v denotes the video input, $y_c$ one of the two classes, $P(y_c|v)$ the predicted probability of having class c given video v, and $p(y_c|v)$ the true one, equal to 1 for the right class c regarding v since we are dealing with supervised classification. In addition, $\\alpha_c$ is the weight for class c, y the focusing parameter. The larger \u03b3, the less importance is put to well-classified samples.\nAs reported in the ablation study, we also investigated the cross-entropy loss [22], defined for a binary classification by:\n$L_{ce}(v, y) = -p(y_c|v) \\log y(y_c|v) - (1 - p(y_c|v)) \\log (1 - p(y_c|v)). (2)$"}, {"title": "3.3. Data augmentation", "content": "Various strategies can be adopted to deal with the lack of data. Data augmentation is a classical one [16]. Here, we can consider data augmentation applied to photometric, spatial, or temporal features of the video [14]. In practice, we only applied basic image manipulations to every frame of the videos: Gaussian noise addition, Gaussian blur, image flipping, image transpose, image cropping. All images in a given sequence are modified in the same way. We have thus multiplied the total number of videos in the training set by 20."}, {"title": "4. EXPERIMENTAL RESULTS", "content": ""}, {"title": "4.1. Video capture and video dataset", "content": "The videos are acquired as follows. The oocytes recovered from slaughterhouse ovaries and matured in vitro are brought into contact with frozen-thawed semen in a culture dish defining the starting point of the biological development of the embryos [12]. The embryos are put in microwells of Petri dishes around twenty-two hours after the in vitro fertilization. Each Petri dish contains sixteen microwells. It is placed into the PrimoVision system that comprises a simple transmission light microscope, and is filmed by the incubator camera.\nThe PrimoVision system takes a picture of the Petri dishes every fifteen minutes over eight days, and delivers 2D time-lapse video sequences that are subsequently divided into sixteen videos, one per embryo. Each video is annotated by a biologist with the T-label or the NT-label. For the purpose of our work, we have retained only the video footage of the first four days of embryo development. Since video acquisition begins only 22 hours after IVF, each processed video covers a period of 3 days and comprises around 300 images. The video dataset includes 947 videos, distributed into 763 for training and validation and 184 for test. Each set includes around 65% of non-transferable embryo videos."}, {"title": "4.2. Implementation details", "content": "Each model was trained using the AdamW optimizer [8], with a learning rate of $10^{-4}$ and the other parameters kept at their default values. We applied a cyclic learning rate scheduler as recommended in [18]. We trained the models using mini-batches of 32 samples artificially created thanks to the accumulate gradients technique, implemented in PyTorch Lightning, that accumulates gradients of small batches before performing a backward pass. We apply the stochastic weight averaging (SWA) [4] technique, which improves the generalization of our models by averaging the network weights at different, well-chosen epochs. We use early stopping to end training when the loss computed on the validation set increased ten epochs in a row. Then, we select the model at the epoch with the highest accuracy on the validation set (supervised training)."}, {"title": "4.3. Ablation study", "content": "We have carried out an ablation study on the components of our model. Firstly, regarding the combination of the lateral connection, the first option (connection from the Regular to the Fast pathways and from the Fast to the Slow pathways) provided a better accuracy. Therefore, this is the one we will be using next.\nWe conducted an ablation experiment on the depth of the ResNet network to be used. We tested two possible depths, 18 layers and 50 layers, which would allow us to still have a rather light model. We trained our SFR model with ResNet18 and ResNet50 modules. For this experiment, we simply took the cross entropy loss, pending a decision on the \u03b3 parameter"}, {"title": "4.4. Comparative experiments", "content": "We have carried out comprehensive comparative experiments on the early prediction of bovine embryo transferability. To evaluate the performance of all methods, we consider the following metrics: overall accuracy Acc, precision $P_T$ (resp. $P_{NT}$) and recall $R_T$ (resp. $R_{NT}$) for the T (resp. NT) class. We performed the binary classification with SlowFast [3] (the ResNet18 version of the code) and a classical 3D-RestNet18, using for both the focal loss, since this loss is more adapted to our problem as demonstrated in Table 2. We train the two methods on our training dataset. This yields a comparison between these two models and ours. In addition, we built a baseline model comprising a 2D convolutional neural network (CNN) followed by a recurrent neural network (RNN). The 2D CNN is implemented with a ResNet18 that learns spatial features on images. A recurrent GRU neural network handles the time dimension of the embryo video, taking as input the successive output of the 2D CNN. The output of the last cell of the GRU is sent to a fully connected layer to obtain the classification prediction.\nComparative results for all the tested models are collected in Table 4 with the use of the focal loss for all methods. As expected, the 2D network involving a recurrent neural network underperforms all the 3D networks. Our SFR method obtains the best accuracy rate, followed by ResNet18. In addition our model is much more stable than the others, on the different metrics, which is crucial. SlowFast does not perform as expected, probably due to the particular nature of the videos processed, very different from those videos considered in action recognition. In addition, our SFR method has the best precision score for the T class and the best recall score for the NT class, which is very important for the target application. Indeed, for cattle breeding, it is essential to predict transferable embryos correctly, in order to avoid unnecessary pregnancies by transferring non-transferable embryos.\nWe have also compared our method with others regarding the computation time to make a prediction, still in Table 4. For each model, we computed the average time for one inference by repeating 1000 predictions on a NVIDIA RTX A500. As expected, the 2D CNN with GRU is the fastest at inference, followed by SlowFast which is faster than a simple 3D-ResNet as shown in the original paper [3]. Next come 3D-ResNet18 and our SFR model, whose average time pre-"}, {"title": "4.5. Earlier prediction", "content": "We wanted to check whether it is possible to make a prediction even before four days have elapsed. Therefore, we evaluate the performance of our SFR model with focal loss, when performing less-than-4-day Transferable vs Non transferable prediction. To do this, we test the model with increasingly shorter videos, removing the last thirty frames each time, which corresponds to removing information occurring during seven hours and a half. We stop testing at 120 frames, i.e., approximately two days 1/4 of embryo development. Results are plotted in Fig.4. We observe that the curve regularly climbs. Depending on the accuracy level acceptable for a given application, a usable prediction could be provided at an even earlier stage than the four days of the embryo development."}, {"title": "5. CONCLUSION", "content": "We have designed a 3D model to predict the embryo transferability within four days, taking 2D time-lapse microscopy videos as input. We state the problem as a supervised two-class classification one that remains however difficult. The three-pathway architecture makes our 3D model multi-scale in time regarding the input videos and able to manage appearance and motion in different ways. We successfully dealt with poorly discriminating embryo appearance and motion, in addition affected by transparency, and small inter-class distance. Experiments demonstrate the usefulness of directed lateral connections between pathways, and of the focal loss. We favorably compared our SFR model with other methods. SFR provides the best accuracy rates with much better stability than 3D-ResNet18 on all metrics. Thus, we are able to efficiently and accurately achieve the early prediction of bovine embryo transferability. Future work will be concerned with an explicit combination of classification earliness and accuracy in the training loss."}, {"title": "6. COMPLIANCE WITH ETHICAL STANDARDS", "content": "This research study was conducted using data available in our laboratory. No live animals or euthanised animals were used to create the original data. The semen was acquired from a commercial company and the cumulus oocyte complex were harvested from ovaries recovered post-mortem in a commercial slaughterhouse. Both these companies and our laboratory are based in France and state-approved. The necessary authorisations for the use of post-mortem biological material have been obtained from the responsible Ministry."}]}