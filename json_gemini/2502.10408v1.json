{"title": "Knowledge Tracing in Programming Education Integrating Students' Questions", "authors": ["Doyoun Kim", "Suin Kim", "Yohan Jo"], "abstract": "Knowledge tracing (KT) in programming education presents unique challenges due to the complexity of coding tasks and the diverse methods students use to solve problems. Although students' questions often contain valuable signals about their understanding and misconceptions, traditional KT models often neglect to incorporate these questions as inputs to address these challenges. This paper introduces SQKT (Students' Question-based Knowledge Tracing), a knowledge tracing model that leverages students' questions and automatically extracted skill information to enhance the accuracy of predicting students' performance on subsequent problems in programming education. Our method creates semantically rich embeddings that capture not only the surface-level content of the questions but also the student's mastery level and conceptual understanding. Experimental results demonstrate SQKT's superior performance in predicting student completion across various Python programming courses of differing difficulty levels. In in-domain experiments, SQKT achieved a 33.1% absolute improvement in AUC compared to baseline models. The model also exhibited robust generalization capabilities in cross-domain settings, effectively addressing data scarcity issues in advanced programming courses. SQKT can be used to tailor educational content to individual learning needs and design adaptive learning systems in computer science education.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in educational technologies have enabled the collection of dynamic data as students interact with learning systems. Consequently, researchers have paid considerable attention to knowledge tracing (KT), which involves monitoring students' knowledge states and predicting their future performance (Corbett and Anderson, 1994). A valuable source of signals about students' understanding and misconceptions is the questions they ask (Sun et al., 2021). With the growing popularity of online learning platforms and learning management systems (e.g., Moodle and Canvas) that include Q&A forums, student questions and interactions with educators have become increasingly accessible. However, traditional KT models overlook this rich source of information. This gap is particularly significant in programming education, where KT is challenging because students' competencies need be assessed from unstructured and noisy source code. In such contexts, students' questions offer clearer insights into their understanding and confusion (King, 1994).\nIn this paper, we present the first model that integrates rich signals from student questions to accurately predict students' performance on subsequent problems, as illustrated in Figure 1. As we will show, merely using a transformer to encode"}, {"title": "2 Related Works", "content": "Knowledge Tracing with Behavioral Data Knowledge tracing (KT) models students' knowledge over time to predict their future performance (Piech et al., 2015). Building upon the foundational approaches like Bayesian Knowledge Tracing (BKT) (Corbett and Anderson, 1994) and Deep Knowledge Tracing (DKT) (Piech et al., 2015), recent research has advanced KT by incorporating behavioral data, such as response times (Song et al., 2021), scaffolding interactions (Asselman et al., 2020), and attempt counts (Sun et al., 2022).\nHowever, a significant gap remains in leveraging student-educator interactions. Questions arising from these interactions often reveal students' reasoning processes and areas of struggle in applying theoretical knowledge to coding (Sun et al., 2021). Yet, most existing models fail to capture the valuable insights embedded in students' questions. Our model addresses this challenge by directly integrating this rich data.\nKnowledge Tracing in Programming Education Programming education poses unique challenges for KT due to the complexity of coding tasks and multiple correct solutions that can be derived using various skills. Traditional KT models often use the Q-matrix method to manually tag problems with the required skills (Yu et al., 2022), but this process is labor-intensive and often fails to capture the full range of skills students use. The diversity in problem-solving approaches complicates the tracing of specific skills mastered by a student, making it challenging to predict future performance accurately.\nA key aspect of KT in programming education is the representation and modeling of knowledge components (KCs), such as \u201cfor loop\", \"recursion\", or \"object-oriented principles\". Recent work has focused on analyzing code submissions to model these KCs and predict learning states. Shi et al. (2022) introduced Code-DKT, which uses attention mechanisms to extract domain-specific code features. Liu et al. (2022) developed an approach that considers the multi-skill nature of programming exercises by learning features from student code that reflect multiple skills.\nHowever, these approaches still rely on manual tagging of KCs. Our approach advances this by using an automated skill-mapping system using GPT to extract KCs from student questions. This method allows more flexible use of KCs, enabling the model to identify and leverage skills without extensive manual tagging. This skill extraction method captures aspects of student knowledge that are not evident in code submissions alone, improving the prediction of student performance."}, {"title": "3 Methods", "content": "In this section, we introduce our Students' Question-based Knowledge Tracing (SQKT) model. Our primary goal is to predict a student's success on a problem by integrating information about the student's history of solving other problems. Our model takes a sequence of descriptions of problems the student has attempted in the past, associated code submissions, student questions, and skill information (each problem may have multiple submissions and questions), along with the description and required skills for the next problem. The model then predicts whether the student will correctly solve the next problem. The overall architecture is illustrated in Figure 2."}, {"title": "3.1 Multi-feature Inputs", "content": "SQKT integrates various input features, with a focus on students' questions and extracted skills. In this section, we first detail our main contribution: the integration of students' questions as input features, followed by an overview of the remaining input components.\nStudent Questions Integrating student questions is motivated by valuable insights they provide into a student's mastery level, revealing areas of confusion and the depth of understanding of specific concepts (Sun et al., 2021). As illustrated by an example student-educator interaction in Figure 3, students' questions typically include two types of information: natural language ques-"}, {"title": "Skill Extraction", "content": "Identifying the skills students struggle with can improve our model's performance compared to relying solely on questions. Extracting skills from student questions is more straightforward and accurate than from submitted codes, as these questions often directly address the concepts students find challenging. By combining these extracted skills with those required for the target problem, the model can predict a student's performance more accurately. To achieve this, we developed a method to extract and leverage skill information from both student questions and target problems.\nThe first challenge was to define an effective set of Python skills. We identified a comprehensive set of 36 core Python concepts and 19 Python error types, drawing from Python's official documentation and books by Sweigart (2019) and Downey and Mayfield (2019), as shown in Table 5. Incorporating error types as skills was motivated by the pedagogical principle that errors reveal students' understanding and misconceptions, which are correlated with learning gaps (Altadmri and Brown, 2015; Becker et al., 2019; Hertz and Ford, 2013).\nThe next challenge was scaling skill extraction. Traditional approaches rely on experts to manually tag skills for each problem, which is labor-intensive and lacks scalability. To address this, we developed an automatic method using GPT-40. Specifically, we provided GPT-40 with about 20 examples of student questions and a pre-defined list of skills. GPT-40 was then prompted to reference these ex-amples and generate a Python script that could be used to map any student question to the relevant"}, {"title": "Code Embedding", "content": "We use Code-BERT (Feng et al., 2020), a pre-trained transformer model designed for programming languages, to convert students' code submissions into vector representations. This captures both the syntactic and semantic properties of the code, providing insights into the student's coding abilities and problem-solving strategies."}, {"title": "Problem Embedding", "content": "Problem descriptions include the problem statement, input/output specifications, and constraints. They are processed through the pre-trained BERT-base"}, {"title": "Fusion Layer", "content": "The fusion layer combines the above embeddings\u2014questions, skills, problem descriptions, and code submissions\u2014into a unified representation space. While each source provides unique insights, challenges remain in integrating these heterogeneous signals. The fusion layer addresses this by projecting each embedding type into a common 512-dimensional space based on the relationships among the embeddings.\nSpecifically, we employ triplet loss to encourage embeddings from the same submission to be positioned closely together, while those from different submissions or representing distinct programming concepts are placed farther apart. The triplet loss is defined as:\n$L_{triplet} = max(0, d(a,p) \u2013 d(a, n) + margin)$,\nwhere:\n\u2022 a is the current problem's embedding derived from the student's code submission, serving as the anchor embedding.\n\u2022 p is the embedding of the current problem's description or student questions, serving as positive samples.\n\u2022 n is the embedding of a randomly selected problem's description or student questions, serving as negative samples.\n\u2022 d(x, y) is the Euclidean distance between two embeddings x and y.\n\u2022 margin is a hyperparameter enforcing a minimum distance between positives and negatives.\nConsequently, the fusion layer enhances SQKT's ability to process heterogeneous yet semantically and contextually related signals more coherently."}, {"title": "3.2 Multi-Head Self-Attention Layers", "content": "All embeddings from the student's history and the next problem are encoded through a multi-head attention mechanism to predict the student's success or failure on the next problem (Figure 2, F). The target problem for prediction is represented by the following tensor:\n$T = [P_{ET}, S_{ET}] \u2208 R^{2\u00d7512}$\nwhere $P_{ET}$ and $S_{ET}$ are the problem and skill embeddings for the target problem.\nFor each problem i that the student attempted prior to the target problem, we construct a tensor $U_i$ containing the input features associated with the ith problem:\n$U_i = [P_{Ei}, C_{Ei}, Q_{Ei}, S_{Ei}] \u2208 R^{K\u00d7512}$\nwhere $P_{Ei}, C_{Ei}, Q_{Ei}$, and $S_{Ei}$ denote the problem, code, student question, and skill embeddings, respectively. Each $C_{Ei}, Q_{Ei}$, and $S_{Ei}$ is a tensor with potentially multiple rows, consisting of embeddings accumulated from all code submissions and questions related to the ith problem. If the student asked no questions, $Q_{Ei}$ is set to a zero vector. K increase as the student makes more submissions for the ith problem.\nTaken together, the input to the multi-head self-attention layers consists of the target problem along with all preceding learning history $[U_1, U_2, ..., U_n, T]$.\nThis input sequence passes through six self-attention layers, each capturing complex interactions among different submissions and their components. After the final attention layer, max-pooling is applied to all output embeddings to derive a representation of the student's knowledge state. The resulting embedding is then fed to a classification head to predict the student's success or failure on the target problem. Binary cross-entropy is used as the loss function:\n$L_{pred} = -\u03a3(y log(\u0177) + (1 \u2212 y) log(1 \u2013 \u0177))$,\nwhere y is the true label and \u0177 is the predicted label. The final loss function is a weighted sum of the prediction loss, question loss (Eq. 1) and triplet loss (Eq. 2):\n$L_{total} = L_{pred} + L_{question} + \u03bb L_{triplet}$\nwhere \u03bb is a hyperparameter that adjusts the weight of the triplet loss."}, {"title": "4 Experiment Settings", "content": "To evaluate the performance and generalizability of our SQKT model, we conduct experiments aimed at answering the following research questions:\n1. How does SQKT compare to existing knowledge tracing models in predicting student performance on programming problems?"}, {"title": "4.1 Dataset", "content": "Our study uses data collected from a Korean online programming education platform between January 2022 and April 2024, with the consent of the copyright holders. These data cover four distinct Python programming courses, providing a diverse range of difficulty levels and topics. All data are in Korean and include Python code, covering code blocks and associated error messages. Additionally, the data contain student-educator interactions including student questions and educator answers (Figure 3). The data for each course are split by students into training, validation, and test sets in an 8:1:1 ratio, with each student assigned to only one set to prevent the risk of information leakage."}, {"title": "4.2 Experimental Setup", "content": "We conduct a series of experiments to assess two critical aspects: the model's ability to predict student performance and generalize across different courses and difficulty levels. We perform both in-domain and cross-domain experiments.\nIn-domain We evaluate the model's performance when trained and tested on the same course. We experiment with three out of four courses, excluding one due to insufficient data for stable training.\nCross-Domain We selected courses to challenge the model's adaptability and generalization capabilities. In the first cross-domain setting, labeled 'content structure generalization', we train the model"}, {"title": "4.3 Training Setup", "content": "For each student, we predict the student's outcome for every problem they attempted, excluding the first problem since it has no preceding history. For"}, {"title": "5 Experiment Results", "content": "5.1 In-Domain Results\nAcross the three courses, SQKT consistently outperformed all baselines. SQKT achieved an AUC of 87.1\u201393.4, representing an absolute improvement of 12.6\u201320.8 compared to the best-performing baseline (KTMFF+). These results demonstrate that our SQKT model, which incorporates student questions, is highly effective in predicting students' future performance.\nThe improvement of KTMFF+ over KTMFF and OKT+ over OKT reinforces our research motivation that student questions provide valuable insights into student performance. It also suggests that our question embeddings can be integrated with general KT models to enhance their predictive accuracy. However, SQKT consistently outperformed these models, underscoring the efficacy of its architecture in leveraging student questions more effectively than the baselines."}, {"title": "5.2 Cross-Domain Results", "content": "Figure 4 demonstrates the model's performance across two cross-domain settings, evaluating its ability to generalize to unseen courses.\nIn the setting of content structure generalization (Figure 4, left), we assessed SQKT's ability to transfer knowledge between courses with different levels and content structures. Our full model (orange) showed an absolute 45.3% improvement in AUC over without using question data (blue).\nIn the setting of data-scarce generalization (Figure 4, right), we trained SQKT on all courses except \"Algorithm\" and tested it on the course to evaluate the model's generalizability to higher difficulty levels and robustness in low-resource environments. Since the \"Algorithm\" course has small data, fine-tuning SQKT directly on the \"Algorithms\" data (green) shows an AUC score close to random. However, our full model (orange) showed a substantial improvement of 11.4% over the in-domain model (green) and 16.7% over the cross-domain model incorporating no student questions (blue).\nBoth experiments conclude that student questions convey generalizable insights into student performance across different courses and that leveraging them greatly enhances the model's ability to adapt to new courses with varying difficulty levels and limited data."}, {"title": "5.3 Error Analysis", "content": "To better understand our model, we conducted a detailed analysis focused on question-related mistakes. We randomly sampled 60 mispredictions from the test set, manually analyzed each data point by tagging one or more error types. Our analysis shows that 'Complexity' is the most prevalent issue (55.6%), often due to code snippets containing mixed language syntax, which challenges the model's parsing capabilities. \u2018Confusion' is the second most common error type (40.7%), typically occurring when the error in the code is unrelated to the student's question, making it difficult for the model to establish the correct correlation. 'Ambiguity' (22.2%) and \u2018Incompleteness' (29.6%) also contribute significantly to model errors, emphasizing the need for clear, context-rich questions for accurate predictions. The analysis highlights key areas for improvement in the SQKT model. For example, incorporating more advanced natural language processing techniques to handle multi-lingual input could enhance the model's ability to interpret students' questions more accurately."}, {"title": "6 Conclusion", "content": "This paper introduces SQKT, a knowledge tracing model in programming education that addresses the unique challenges of predicting students' performance on subsequent problems in coding tasks. By integrating students' questions and auto-extracted skill information, SQKT provides a more comprehensive view of student knowledge than traditional KT models. We demonstrate the effectiveness of SQKT across various programming courses and difficulty levels, consistently outperforming baseline models in both in-domain and cross-domain settings. SQKT shows its ability to capture valuable information about students' programming competencies through their questions. We expect that our method can contribute to more personalized and effective learning interventions in programming education."}, {"title": "Limitations", "content": "This study has several limitations. First, we did not apply any preprocessing to the input questions prior to analysis. Although this approach more closely mirrors actual classroom conditions, inputs are often noisy. To address this, we implemented"}]}