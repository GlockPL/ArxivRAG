{"title": "EVALUATION OF REAL-TIME TRANSCRIPTIONS USING\nEND-TO-END ASR MODELS", "authors": ["Carlos Arriaga", "Alejandro Pozo", "Javier Conde", "Alvaro Alonso"], "abstract": "Automatic Speech Recognition (ASR) or Speech-to-text (STT) has greatly evolved in the last few\nyears. Traditional architectures based on pipelines have been replaced by joint end-to-end (E2E)\narchitectures that simplify and streamline the model training process. In addition, new AI training\nmethods, such as weak-supervised learning have reduced the need for high-quality audio datasets\nfor model training. However, despite all these advancements, little to no research has been done on\nreal-time transcription.\nIn real-time scenarios, the audio is not pre-recorded, and the input audio must be fragmented to be\nprocessed by the ASR systems. To achieve real-time requirements, these fragments must be as short\nas possible to reduce latency. However, audio cannot be split at any point as dividing an utterance\ninto two separate fragments will generate an incorrect transcription. Also, shorter fragments provide\nless context for the ASR model. For this reason, it is necessary to design and test different splitting\nalgorithms to optimize the quality and delay of the resulting transcription.\nIn this paper, three audio splitting algorithms are evaluated with different ASR models to determine\ntheir impact on both the quality of the transcription and the end-to-end delay. The algorithms are\nfragmentation at fixed intervals, voice activity detection (VAD), and fragmentation with feedback.\nThe results are compared to the performance of the same model, without audio fragmentation, to\ndetermine the effects of this division.\nThe results show that VAD fragmentation provides the best quality with the highest delay, whereas\nfragmentation at fixed intervals provides the lowest quality and the lowest delay. The newly proposed\nfeedback algorithm exchanges a 2-4% increase in WER for a reduction of 1.5-2s delay, respectively,\nto the VAD splitting.", "sections": [{"title": "Introduction", "content": "Automatic Speech Recognition (ASR) or Speech-to-Text (STT) is the use of Artificial Intelligence to transform human\nspeech into text. ASR started in 1952 with Audrey, a digit recognition system developed by Bell Labs [1]. With the\nevolution of machine learning (ML), artificial intelligence (AI) and available hardware [2], the ASR systems have\nevolved from this original system.\nThe first ASR architectures were made up of a series of models that together compose a processing pipeline. Each\nmodel was responsible for a different task (acoustic model, language model, phoneme inventory, etc.). Different\nmodels were trained independently and could have their own architecture [3, 4]. With the evolution of AI training\ntechniques, modern architectures have shifted to end-to-end (E2E) deep learning architectures [5, 6]. This resulted in a\nsingle joint model that uses a single set of training data, avoiding the need to work with diverse sources of knowledge.\nIt also reduces assumptions made about the data and simplifies the training process.\nE2E models are trained with substantial amounts of labeled audio data. On the one hand, some models are trained\nwith high-quality audio repositories such as GigaSpeech [7] (10.000 hours) or The People's Speech [8] (30.000 hours),\nwhich provide a dataset of audio files with their respective transcriptions with a variety of topics and accents. On the\nother hand, models such as Whisper [9] were trained using a higher order of magnitude of lower-quality data, using\nweak supervision. Research has been conducted to develop unsupervised learning techniques to train ASR models"}, {"title": "Testing methodology", "content": "In this section, the methodology designed to evaluate the performance of different combinations of batch ASR models\nand audio splitting algorithms is detailed. In addition, the architecture designed to make the comparison is introduced\nwith a series of definitions that will be used in Section 3. Finally, the algorithms used for the delay measurement and\nthe output normalization process are presented.\nThe methodology focuses on measuring the performance of the models in a real-time scenario when audio has to be\ntranscribed during a speech. The baseline for the comparison is the performance of the same model in a batch scenario,\nwhere all the audio is pre-recorded prior to its processing. For each model, its performance is measured first in a batch"}, {"title": "Test scenario", "content": "In this section, the architecture and ASR models used to generate real-time transcriptions are detailed, as well as\nthe testing process. For the architecture used, a generic definition of the components is provided first. Then, the\nimplementation of the aforementioned component used in these experiments is detailed."}, {"title": "Architecture definition", "content": "The main objective is to provide a generic architecture definition that is not limited by specific technologies or proto-\ncols. Figure 1 provides a diagram of its different components, which are:\n\u2022 Audio input: Audio that enters the system to be transcribed.\n\u2022 Audio extractor: Process that captures the audio from the audio input and transforms it into a suitable format\nfor the rest of the system.\n\u2022 Audio splitter: Process that receives the audio from the audio extractor and optimally splits it into fragments\nfor processing. The aforementioned audio-splitting algorithms are implemented inside this component.\n\u2022 ASR cluster: Responsible for transcribing the received audio samples.\n\u2022 ASR cluster-audio splitter connection: Connection between the ASR cluster and the audio splitter, used to\ntransmit the audio fragments.\n\u2022 Display: Component that receives the transcription and renders it.\n\u2022 ASR cluster-display connection: Connection between the ASR cluster and the display, used to transmit the\nresulting transcription."}, {"title": "Architecture implementation", "content": "This section provides a reference implementation of the previously proposed architecture. It will be used to test the\ndifferent algorithms and scenarios proposed. The objective is to provide an implementation that runs independently of\nthe operating system or computer architecture. For this reason, the implementation is based on open-source Web tech-\nnologies, so any web browser can access it. Web browsers offer a series of standard APIs that run code independently\nof the hardware or system architecture that will be used in the following components. In addition, with this approach,\nit can be used in a variety of web applications and services, such as videoconference.\n\u2022 Audio input: In this implementation, the audio input is the microphone captured by a web browser that\naccesses the ASR application.\n\u2022 Audio extraction: For accessing the audio and processing it, the MediaDevices API [25] is used. The audio\ninput is captured using the getUserMedia method from the MediaDevices API. Audio is sampled at 16000\nkHz with 16 bits per sample.\n\u2022 Audio splitter: The Web Audio API [26] is a system to control audio on the Web. It allows developers\nto create a processing graph composed of audio nodes. Through this API, audio samples from the Audio\nExtractor can be accessed and processed. All the processing is done in the context of an AudioContext. The\nAudioContext represents a filtering graph composed of audio processing nodes and a common configuration.\nThe audio splitting is performed inside this filtering graph. Different nodes are created to implement the\ndifferent audio splitting algorithms. To reduce the computational power required on the server side, audio\nsplitting is offloaded to the clients. Moving the transcription process to the clients is currently not possible as\nit requires high computation power and specialized equipment such as GPUs. The audio is received from the\naudio extractor and then handled to the Client-ASR connection.\n\u2022 Audio Splitter-ASR cluster and ASR cluster- display connection: For these connections, the technology se-\nlected is WebSockets [27]. They allow for bidirectional communication between clients and servers, allowing\nthe system to use the same websocket for both connections. In this implementation, they are used to send\nraw audio samples from the audio splitter and receive transcribed text. Other protocols such as RTP [28]\nor WebRTC [29] used for multimedia streaming were not considered because most of their features, such\nas bandwidth adaptation, codec negotiation, or time synchronization, are not needed. This is because raw\nsamples are sent without any additional metadata. In addition, these channels do not offer a return channel\nsuitable for text format. The selected library for implementing WebSockets on the client is socket.io\u00b9.\n\u2022 ASR Cluster: The cluster receives audio fragments from the WebSockets, transcribes them, and sends them\nback through the socket. For the ASR model, Whisper was selected due to its ease of deployment and high\nperformance. In addition, trained models are provided directly by OpenAI. The implementation used is a high\nperformance version of Whisper in C++ 2. For each connection received, a new instance of the Whisper model\nis created. All audio received from a socket is processed by the same instance. These instances are balanced\namong available hardware resources. The socket server library used is socketioxide\u00b3, a Rust implementation\nof the socket.io library.\n\u2022 Display: The display is a HTML[30] element in the application view, in which the received transcription is\nrendered."}, {"title": "ASR models", "content": "For this experiment, three models are selected from the ones provided by OpenAI: tiny 4, base and large 6. Their\ndetails are presented in Table 2.\nOne issue that this implementation of Whisper presents is that sometimes during a silence the model repeats the last\noutput instead of marking the silence. This issue was particularly prominent in the latest version of the large model.\nFor this reason, even though there is a newer version 3 of the large model 7, the test was carried out with version 2."}, {"title": "Batch testing", "content": "To compare the performance of the various audio splitting algorithms, the audio files are transcribed in batches. To\ntranscribe the files from the GigaSpeech evaluation dataset, they had to be converted from OPUS to WAV due to\nimplementation requirements. The Ffmpeg library was used for this conversion. After that, all audio files were\ntranscribed one by one using a shell script, and the resulting transcriptions were stored for future analysis."}, {"title": "Real-time testing", "content": "To automatize the testing process, Selenium, an open-source project to automate browsers, is used to simulate clients.\nSelenium's WebDriver is an interface that allows users to launch and control web browser instances. For the Web\nbrowser, Google Chrome10 was selected because it has the option to use audio and video files as a false microphone or\ncamera. Using this option, for each audio file in the dataset, a headless (without graphic interface) browser is launched\nwith the audio file as the microphone. This false microphone is the audio input of the architecture, as defined in Section\n2.1.1. This allows the system to use the same audio files in both batch and real-time transcriptions. In addition, this\nimplementation simulates a real user who accesses the system via a web browser.\nIn Chrome, when the audio file used as the microphone reaches the end, it loops back from the beginning. To avoid\ntranscribing the same audio multiple times and having to manually check every transcription, each browser instance\nonly runs until the audio file reaches the end for the first time. Because the WAV format stores raw samples with a\nfixed sample rate, the duration of the file can be calculated based on its size. The time needed is calculated using the\nformula provided in Equation 1:\n$T(s) = Size(bytes) * 8/16000(Hz)/16(b/sample);$ (1)\nThe transcriptions received by the ASR cluster are then stored for future analysis."}, {"title": "Architecture deployment", "content": "All the components defined in the architecture were deployed on the same machine. The selected hardware and\noperating system are detailed in Table 3"}, {"title": "Audio splitting", "content": "In this section, the three algorithms that will be tested in the next section are presented."}, {"title": "Fixed interval", "content": "A first naive algorithm splits audio fragments at fixed intervals, without checking if the utterances are split into different\nfragments. Different intervals will be tested to measure the impact of the duration of the fragment on delay and\nperformance. In this implementation, the audio extractor captures the samples at an unfixed rate, so they are stored\nuntil all of the samples from each interval are received. The implementation is presented in Algorithm 1\nThis algorithm is used as a reference for the next algorithms. Two different fragmentation intervals, 2 and 3 seconds,\nwere tested to compare its effects on the resulting transcription."}, {"title": "VAD based", "content": "The VAD based algorithm tries to solve the word splitting issues caused by the previous algorithm. The objective of\nthis algorithm is to avoid dividing words into different fragments to improve the performance of the system.\nIn contrast, with end-of-query detectors, VAD algorithms detect the silence between words, instead of only detecting\nthe end of the speech. In this implementation, a JavaScript VAD library\u00b9\u00b9 is used.\nThe VAD is a state machine with two states: silence and voice. The samples are stored in both states. When the\ntransition from voice to silence is triggered, all stored samples are sent to the ASR cluster. The state machine is\npresented in Figure 2."}, {"title": "Feedback", "content": "The last algorithm tries to reduce the delay of the VAD-based algorithm while maintaining the quality. Instead of\nwaiting for a silence, samples are sent at a lower interval, and to avoid word separation, previous audio samples are\nused in the transcription. For each iteration, some audio samples from the previous iteration are used to maintain the\ncontext.\nIn this implementation, Algorithm 1 generates the audio fragments in the client. Then, in the ASR cluster, Algorithm\n2 calculates the new transcription. By default, Whisper is configured to use the previous input as a prompt for the next\ntranscription, to maintain the context. This option is deactivated as the audio fragments are no longer contiguous.\nBecause the output will have repeated words from the previous fragment due to the feedback, the resulting transcription\nhas to be merged with the new one. To merge both transcriptions, we implement a new algorithm presented in\nAlgorithm 3. It compares the last N words of the previous transcription with the new transcription. If an exact match of\nM consecutive words is found, the old transcription is replaced by the new one from that point. Depending on the values\nof N and M, the number of false positives and match misses varies. This is because the previous transcription and the\nnew transcriptions can differ as different samples in different context are processed. If, for example, a large section of\nthe previous transcriptions is searched and only one word is checked, the transcription merge will be inaccurate due to\nrepeated words or articles. On the other hand, if a small section of the previous transcriptions is searched and a large"}, {"title": "Evaluation metrics and definitions", "content": "This section introduces the different metrics used to measure the performance of the models. Also, definitions for\nend-to-end delay and model comparison are provided."}, {"title": "Metrics", "content": "The parameters measured to determine the quality of transmission are:\n\u2022 Model performance: To measure the performance of the models, three different parameters are measured:\nword error rate (WER), match error rate (MER), and word information loss (WIL)[31].\n\u2022 End-to-end delay: Delay since a word is pronounced until the transcription appears in the client application.\n\u2022 Quality/delay: For the real time scenarios, the relationship between transcription accuracy and delay are be\nstudied.\nThe library to calculate the WER, MER, and WIL is Jiwer12. It also provides utilities to normalize the input, such\nas expanding English contractions, removing extra white spaces... For each audio file, the batch transcription and the\nreal-time transcription are compared to obtain the WER, MER and WIL."}, {"title": "E2E delay definition", "content": "The objective of this section is to provide a delay definition that includes all the factors that affect the E2E delay. We\ndefine delay in a real-time system transcription system as the time elapsed between a user pronounces a word and its\ntranscription is presented to the user. The total delay ($D_T$) is defined as:\n$D_T = D_s + D_p + D_t.$ (2)\nWith $D_s$ being the delay caused by audio splitting, $D_p$ being the delay caused by audio processing, and $D_t$ the\ntransmission delay introduced by the splitter-ASR cluster connection. The value of $D_p$ varies depending on the\nhardware used to run the models and the different processes. On the other hand, $D_t$ is independent of it, since it only"}, {"title": "Delay measurement algorithm", "content": "To accurately measure the transcription delay, reference transcriptions must include time annotations. GigaSpeech's\ntranscriptions are divided into segments with start and end timestamp. For this reason, the only known timestamps are\nwhen the first word of the segment starts to be pronounced and when the last word is pronounced. Because of this, to\nmeasure the delay, only the first word of each segment will be used for the delay measurement.\nThese segments do not include silences or parts with music or other sounds, so the start and end timestamps are\nconsistent. These silences or non-conversational sounds are annotated as such and removed from our measurements.\nTo measure the delay, the timestamp when the audio file starts playing is stored. Then, every time a transcription\nfrom the ASR cluster is received, it is stored with its corresponding timestamp. All of the words included in these\ntranscriptions are stored with the same timestamp as all words are received at the same time. The difference between\nthis timestamp with the one stored at the beginning is compared with the time annotations from GigaSpeechs' tran-\nscriptions. The delay varies depending on the position of the word in the segment. Words that appear at the start of\na fragment have an increased delay as their audio samples have to be stored until the fragment is complete. For this\nreason the process is repeated multiple times to average the results.\nBecause in the resulting transcription, each word does not only appear once (words such as \"the\" or \"it\" have a high\nnumber of repetitions), the correct repetition has to be found to compare timestamps. For this reason, words are\nsearched with their context. We define context as the M words before and after the selected word. To reduce false\npositives due to common expressions such as \"I am not\", \"there was a\u201d, a sliding-window algorithm is used to\nreduce the search width. The algorithm is presented in Algorithm 4.\nFor the first word searched, the algorithm checks if it exists in the first N segments. All of the segments where the\nword is found are stored. After that, for each of those segments, it is checked if the word appears with its context.\nFor the next iteration, the next word is searched in the range [i, N + i], i being the index of the last word found in its\ncontext. In the case where a word is not found, the number of segments checked is increased until a new match is\nfound."}, {"title": "Algorithms and model comparison", "content": "We define that an algorithm-model combination $C_1$ is better than another combination $C_2$ if it satisfies the conditions\nstated in Equation 4 and the premises stated in Equation 3.\nPremise: $(D_{tc_1} = D_{tc_2}) \\land (H_1 = H_2)$ (3)\n$C_1 > C_2 \\Leftrightarrow (Q_{C_1} > Q_{C_2}) \\land (D_{TC_1} < D_{TC_2})$ (4)\n$Q_i$ is the quality of a transcription, $D_T$ is its end-to-end delay as defined in Equation 2 and $D_{t_i}$ the transmission\ndelay. $H_i$ is the hardware used to compute $C_i$, which means that both combinations have to be computed using the\nsame hardware.\nOnly a combination that has lower delay and lower WER is considered better, as depending on the use case require-\nments a certain combination can be over the maximum delay or the minimum quality."}, {"title": "Output normalization", "content": "The transcriptions of the GigaSpeech library and the ones generated by Whisper have different formats. The following\nprocess was carried out to normalize both texts before comparing them with Jiwer.\n\u2022 GigaSpeech's transcriptions do not include punctuation symbols and instead use annotations such as <PE- \nRIOD> or <COMMA>. They were replaced by punctuation marks.\n\u2022 Different annotations are used in both transcriptions to indicate the presence of music or silence. They were\nall removed for the transcriptions.\n\u2022 Numbers appear in GigaSpeech as words, while Whisper uses numbers. All numbers where replaced by their\ntextual version.\n\u2022 Using Jiwer, English contractions were expanded.\n\u2022 URL where present in most of the podcast due to sponsorships. Whispers transcriptions used symbols such\nas / or dots whereas GigaSpeechs transcriptions used text. Whispers symbols where replaced by their textual\nrepresentation.\n\u2022 Whisper adds the symbol \"I\" to the transcription when someone sings. This symbol was removed.\n\u2022 Multiple white spaces between words where removed.\nLastly, one of the audio files of the dataset was mostly in Spanish. The transcription associated with that file only\ncontained the sentences that were spoken in English. Whisper did translate and transcribe the entire audio file, whereas\nthe original transcription only included the English part, resulting in a high WER due to insertion errors. For this\nreason, this file was removed from the dataset.\nAs presented in Section 2.3, the selected implementation of Whisper produces an artifact in which the same output\nis sometimes repeated during a silence. These repeated sentences were removed, even though they can be considered\nerrors during the transcription process. The reason for removing them is that the measured parameters are heavily\ninfluenced by these repetitions. This affects the comparison between models, since the better one would be the one\nwith fewer artifacts."}, {"title": "Results", "content": "In this section, the results obtained are presented. All algorithms presented in the previous section were tested with\nthe three selected models. First, the resulting WER, MER and WIL are compared to the reference, which is the\nperformance of the model with batch processing. This comparison is used to determine how each audio splitting\nalgorithm affects the quality of the resulting transcription. Then the delay introduced by the different combinations\nof algorithms and model is compared. Finally, these combinations are evaluated taking both delay and quality into\nconsideration."}, {"title": "Model performance", "content": "The WER, WIL, and MER are measured for each model as proposed in [31]. The results obtained for all experiments\nare collected in Table 4. However, with the available hardware, it was not possible to run the large model in real-time\nscenarios. The audio fragments were being sent faster than the model was able to process them, resulting in CPU\nsaturation and the ASR cluster crashing. The performance of the large model in a batch transcription is taken as a\nreference for when total delay is not a consideration. As observed, the best WER, MER and WIL are obtained by the\nlarge model in batch mode."}, {"title": "End-to-end delay", "content": "The end-to-end delay is measured using the proposed methodology. The aim of these measures is to determine the\neffects of the audio-splitting algorithms on the end-to-end delay.\nWhisper is designed to transcribe audio in 30 second windows. When transcribing shorter audios, the implementation\nused pads the audio with zeroes. Because of this, a first hypothesis was that $D_p$ was independent of the audio duration\nif it was shorter than this 30 seconds, since it will be padded with zeros until it reaches 30 seconds. This hypothesis\nwas tested by measuring the delay caused by audio segments of different duration. The results of this experiment are\npresented in Table 5.\nResults show that longer segments require more processing time, introducing a larger delay. This refutes the initial\nhypothesis of $D_p$ being independent of the segment duration if it was shorter than 30 seconds. This affects the feedback\nalgorithm, as storing larger segments will result in a higher delay because of the larger segment.\nAfter that, the delay of the different combinations of models and audio-splitting techniques were measured. The\nresults obtained are presented in Table 6. For the batch scenarios, delay was not measured, as the definition provided\nin Section 2.3.2 does not apply to this scenario."}, {"title": "Quality-delay matrix", "content": "In this 2-dimensional matrix, we aim to classify models based on their transcription quality and end-to-end delay.\nThis matrix helps to visualize which algorithms are better than others by the definition provided in Section 2.3.4, as\nthey are the ones whose coordinates are lower in both axes.\nAll combinations tested are represented in Figure 4. In the matrix it can be observed that by our definition the\ncombination of 3 second fixed splitting with the tiny model is worse than the combination of the tiny model with\nthe feedback algorithm. Also, the feedback algorithm with the base model is better than both the VAD algorithm with\nthe tiny model and the 3 second fixed splitting with the base model.\nBy our definition, there is no other combination that can be considered better than other. For example, the combination\nof 2 second fixed splitting with the tiny model has a substantially higher WER than the rest but has the lowest delay,\nso depending on the application requirements it can be the best suited.\nDepending on the hardware, all these combinations can move on the delay axis. So, an algorithm with low WER that\nis not suitable for real-time with a certain CPU, could be used with GPU acceleration. There is a maximum reduction\nin delay that can be achieved by improving the hardware as $D_p$ is only one part of $D_T$."}, {"title": "Conclusion and future work", "content": "In this paper, different algorithms for generating real-time transcriptions have been tested. Transcription quality and\nend-to-end delay have been measured for the different combinations of models and algorithms to determine their\nviability.\nThe conclusions obtained are that larger models introduce a higher delay than smaller models in exchange for a lower\nWER, MER and WIL. Using the same model, VAD splitting was the best performing algorithm as it does not split\nutterances into different fragments. The resulting WER, MER and WIL were very similar to the ones obtained by\nthe batch scenario. However, they introduce a significant delay of 3.5 and 4.4 seconds for the tiny and base model\nrespectively.\nThe fixed interval algorithm introduces a much lower delay than the VAD algorithm in exchange for a much lower\nquality due to utterances being divided into different fragments. Shorter fragmentation intervals lead to lower delays\nand lower quality due to more utterances being divided.\nThe introduction of feedback from previous segments reduces the WER, WIL and MER of the fixed interval algorithm\nin exchange for a higher delay. However, the resulting delay is significantly lower than the VAD algorithm.\nIn conclusion, different algorithms for splitting audio have been tested to generate transcriptions in real time. Depend-\ning on the requirements of quality and delay some algorithms perform better than others. Fixed audio splitting has the\nlowest quality and delay whereas VAD based splitting have the highest quality and delay. Feedback based algorithms\nhave both a higher delay than fixed splitting and lower quality than VAD based splitting. However, compared to the\nVAD algorithm, they provide a significant decrease in delay of 1.521 and 1.987 seconds compared to the relatively\nlower decrease in quality of 0 3.57% and 2.32% for the tiny and base model respectively.\nFinally, the tested model and algorithms have a significant increase in WER, MER and WIL compared to batch\nprocessing. The best result of 0.2304 WER for the base model with VAD splitting is significantly worse than the\n0.1748 achieved for the batch scenario with the tiny model. For this reason, further research regarding audio splitting\nand real-time models have to be performed to enable the use of this technology in real-word applications. This can\npartially be caused by the AudioExtractor, as it is resampling the original audio. For this reason, the effects of this\ncomponent on the resulting transcription quality must be investigated.\nTo reduce the amount of computation power required by the service provider, further research can be performed to\nmove the computation to the client. With WebAssembly[32], ASR models and systems could be embedded in the\nbrowser to locally generate transcriptions, generating a distributed ASR system.\nIn addition, Whisper's architecture has been designed to work with audio fragments of 30 seconds length. The usage\nof models designed to work with smaller fragments or utterances could improve the WER of the system and reduce\nCPU/GPU usage.\nFor future work, this proposed architecture and implementation must be integrated into different applications to achieve\nthe objectives listed in Section 1. This includes investigating its integration with technologies such as WebRTC[29]\nfor videoconferencing."}]}