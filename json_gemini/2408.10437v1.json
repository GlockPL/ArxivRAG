{"title": "Understanding Generative AI Content with Embedding Models", "authors": ["Max Vargas", "Reilly Cannon", "Andrew Engell", "Anand D. Sarwate", "Tony Chiang"], "abstract": "The construction of high-quality numerical features is critical to any quantitative data analysis. Feature engineering has been historically addressed by carefully hand-crafting data representations based on domain expertise. This work views the internal representations of modern deep neural networks (DNNs), called embeddings, as an automated form of traditional feature engineering. For trained DNNs, we show that these embeddings can reveal interpretable, high-level concepts in unstructured sample data. We use these embeddings in natural language and computer vision tasks to uncover both inherent heterogeneity in the underlying data and human-understandable explanations for it. In particular, we find empirical evidence that there is inherent separability between real data and that generated from AI models.", "sections": [{"title": "1 Introduction", "content": "Before the rising popularity of deep neural networks (DNNs), data scientists would use knowledge of an application to create mappings, or feature representations, of \"raw\" data into vectors in a vector space [1, 2]. These vectors could then be used in statistics and machine learning pipelines. Viewed in the right way, DNNs also encode rich feature representations of data in high-dimensional vector spaces, offering a potential replacement to prior tools [3-5]. Leveraging existing knowledge about appropriate datasets, these vector space representations have been found to implicitly understand concepts ranging from time and space [6] to color [7] and more [8, 9]. In this article, we show that the representations provided by DNNs can be used to uncover latent biases in many datasets which, in turn, allow us to deduce high-level information about these datasets. Further, this information is simply encoded in ways that agree with human intuition.\nLeveraging the confounding nature of batch effects from experimental biology that variance within data results as an accumulation of differences in collection methods or the experimental design we use the feature representations provided by DNNs to explain shifts between data samples and their underlying distributions. For natural language, this variation can be attributed to simple concepts like subject matter and word appearances. Independently in text and image domains, we show that DNNs are expressive enough to encode heterogeneity between (1) AI- and human-produced content, (2) content produced by different generative models, and (3) content produced using different prompts on the same model. This heterogeneity, or model DNA (mDNA), manifests itself as an observable shift when performing unsupervised learning on the representations extracted from the DNN. These differences are brought out even more in the supervised setting, allowing us to identify forensic traces that link generative outputs back to the source model.\nFigure 1 depicts our perspective on neural network architectures as a decomposition of a feature embedder (FE) and a task layer. The FE comprises the bulk of the DNN, consisting of a series of transformation layers that have learned to map the data into a fixed vector space. After this mapping, the data is passed through a final statistical learning layer or task-specific \"head,\" commonly a linear classifier or token predictor,"}, {"title": "2 Related Works", "content": "Embeddings. Embedding techniques allow data, often in the form of text or images, to be represented within a large vector space for numerical analysis. There is interest in encoding semantic information in unstructured data for purposes of information retrieval, image labeling, document ranking, and more [13-16]. We show that the embeddings produced by current state-of-the-art models are powerful enough to distinguish semantic differences in our data. Across our examples, we demonstrate that these differences can be reliably detected using standard dimensionality reduction techniques.\nExplanatory AI. Recent work has explored the use of sparse autoencoders to decompose the activations of a model into interpretable components [17-19]. We find that the embeddings of the final hidden layer provide enough information to extract a similar high-level understanding of our data, without requiring any training. Furthermore, leveraging simple linear models such as PCA and LDA allows us to perform basic regressions to correlate human interpretable features with those encoded by the neural network.\nAI content detection. Increased use of generative AI has spurred interest in developing AI detection tools. Commercial services such as GPTZero [20], AI or Not [21], and Originality.ai [22] have been developed in attempts to solve this problem. However, [23] provides evidence that these popular detection tools are easily fooled with clever prompting techniques. Others have used embedding models trained via CLiP to distinguish real from synthetic images by applying various classifiers to the resulting embeddings [24-26]. Our results extend this work by validating on natural language examples and showing that separators (obtained from PCA and LDA) can be attributed with linear features. Furthermore, we show that anomaly detection with isolation forests [27] can identify AI generated content that has contaminated a reference sample of real data.\nLinear Approximation for NNs. Jacot et al. [28] showed that the training dynamics of DNNs are intimately linked to its neural tangent kernel (NTK) in the infinite-width limit, and Engel et al. [29] developed an explain-by-example technique using the empirical NTK. Essentially, the NTK is the sample to sample covariance of the linearized features for the DNN. Qadeer et al. [30] showed that the conjugate kernel (CK - the gram matrix of the activations of the embedded features) approximates the NTK. Our work leverages linear modeling on the CK for explainability."}, {"title": "3 Methods", "content": "We view a DNN as a feature embedder (FE) that maps inputs into a high-dimensional latent vector space $R^n$. Given an image or text sample x as an input, the DNN computes an embedding vector $y := FE(x) \\in R^n$. A data set X of N samples maps to a data matrix $Y \\in R^{n \\times N}$. In some cases, the output of the DNN is already a high-dimensional vector (e.g., image feature extractors) while in other cases, the map FE is obtained by extracting the final hidden layer activations. When beneficial, we refer to X and Y as the observed and latent data, respectively. We use basic techniques from statistics on the data matrix Y under the assumption that the columns of Y are independent and identically distributed vectors. This amounts to the assumption that the data points of X are themselves sampled independently from an underlying distribution. Reasoning about Y allows us to make deductions about the underlying distribution for X. We then rely on Principal Component Analysis (PCA) [31] for unsupervised learning and Linear Discriminant Analysis (LDA) [32] for supervised learning. PCA is an unsupervised learning method that finds linear coordinate directions in $R^n$ that maximize the variance observed in the data Y. LDA is a supervised learning method for classification that classifies points according to a linear decision boundary.\nUsing the principal components and linear discriminants of the embedded data, we analyse the sources of variability using regression-based hypothesis testing. Our null hypothesis is that all of the samples come from the same distribution and thus that the explanatory variable has no relationship with the response. We then examine our samples to determine if we can reject this conclusion."}, {"title": "3.2 Data and models", "content": "We were interested in understanding how the FEs from pre-trained models correspond to \u201chidden\u201d labels (metadata) and whether a discriminative model could predict these labels. Of particular interest to us was whether the FEs could be used to separate synthetic data (generated by an AI model) from natural (or reference) data. Table 1"}, {"title": "4 Results", "content": "Our findings suggest that the highest degrees of variability often have high correlation with known differences in the data samples, such as subject area, and with less-obvious ones, like language semantics. We also study the semantics of synthetically generated content relative to references of real data. This analysis is then extended beyond using human-made samples as a baseline, where we find that the modalities within AI-generated data can be attributed to the choices of models and prompts. Finally, we show that, in the case of textual data, these regressions can be used to explain the observed variations in terms of patterns in the underlying text."}, {"title": "4.1 Language model embeddings encode high-level semantics", "content": "Looking at news articles in the MLSUM dataset, the feature embeddings produced by language models are able to automatically sort the news articles based on broad features in the data. Figure 2(b) plots the top three PCs of embedded Spanish and Russian news articles. Table 2 shows that PC1 can be used to separate articles on language, PC2 to separate on topic, and PC3 can distinguish between native Spanish news articles and those that were translated from Russian. Noting that language and topic were originally encoded into MLSUM as metadata, we see that unsupervised clustering with the LLM embeddings let us recover these high-level features. The $R^2$ values for these regressions are 0.98, 0.87, and 0.83, respectively. We remark that the separation between the native Spanish articles and those that were machine translated may be a compounded effect of the quality of translation along with other natural semantic variation between Spanish and Russian cultures and patterns.\nAnalyzing academic abstracts predating 2020 across five topic areas indicates that the top principal components encode these various subjects. When comparing real abstracts with synthetic abstracts generated by Llama-2 70B, we find that the differences between real and artificial writings are another contributing source of variation. Table 2 shows a strong relationship between the first principal component and the topic of Experimental High Energy Particle Physics ($R^2 = 0.92$), PC2 to Programming Languages ($R^2 = 0.76$), PC3 to Quantitative Cell Behavior ($R^2 = 0.86$), and PC5 to statistical methodology ($R^2 = 0.71$) ."}, {"title": "4.2 Generated data is shifted from real baselines", "content": "We find that embeddings encode differences in real and AI-generated content, and this distinction is represented in the initial few principal components. In our five subject arXiv data, figure 4 shows that, despite the real and synthetic abstract distributions overlapping almost entirely when we plot a UMAP of the first 30 PCs, examining the individual principal components shows us that the two samples can in fact be distinguished. In particular we see that a clear shift in PC4 between the real abstracts and those generated by Llama-2-70B ($R^2 = 0.79$), and a simple LDA classifier achieves 99.8% test accuracy when classifying real/AI-generated abstracts across all five topics (right). Moreover, the horizontal clustering observed in the LDA plot correspond to subject area (see Figure C7).\nWe see this shift in our other experiments as well. In the Stack Exchange dataset, there is a clear shift between human responses and those generated by AI in the top 2 PCs. In fact, a linear combination of these first two PCs can be partially explained by the determination of real and AI-generated responses ($R^2 = 0.45$). Supervised training on the language model embeddings establishes that the real and synthetic data are linearly separable, obtaining 99.0% testing accuracy between real and synthetic responses. This separability also exist in images: LDA achieves 99.4% test accuracy in binary classification of the real and generated samples, where the generated images were taken from all models. Surprisingly, while the unconditional model (DDPM) was trained on the LSUN cat dataset, it still fails to capture all the features inherent to cat images, with LDA achieving a test accuracy of 95.0% in this binary classification."}, {"title": "4.3 Embeddings are sensitive to generative techniques", "content": "We find that choices in generative techniques such as the model and prompt can shift the distributions observed through DNN embeddings. In many cases, unsupervised clustering techniques are able to recognize such shifts (see figure 3). With supervision, however, these shifts are pronounced. This section establishes the proof-of-concept that forensic analysis of synthetically generated data is possible. That is, given a sample from one of several understood distributions, we can trace through the embedded representations to determine if it is real or, in the synthetic case, specific generative parameters.\nModel DNA. At the zero-shot level, different generative models have distinct identifying mDNA for authorship determination. In the case of StackExchange, supervised LDA can separate the responses for each question based on authorship type. Using the embeddings provided by Mistral 7B, we are able to predict whether an answer is user-written, or authored by Llama-2 70B, Mixtral 8x7B, or Falcon 40B with an average overall accuracy of 90.7%. The presence of mDNA is further evidenced when we asked language models to translate 5,000 sports articles from German to English. Comparing Mixtral-8x7B against some dedicated translation models (nllb-3.3B and mt-opus-de-en [46]), we can identify the machine translator of a given article 93.2% of the time using a supervised LDA classifier. We note, however, that our ability to identify outputs is likely dependent on the long-form generative nature of the task. On short-form translations on the United Nations Parallel Corpus [47], our techniques do not identify significant separation between the different kinds of machine and human responses (analogous to non-specificity of short fragments of DNA sequences).\nPrompting and Fine-Tuning. The embeddings of DNNs can also detect and quantify shifts in generative content caused by changes in prompting techniques. With economics abstracts generated from different prompts (provided in Appx. Figure A3), unsupervised clustering is able to recognize that a more descriptive prompt tightens the distribution of Llama-2 70B generations along PC1 (figure 3(c)). This indicates that precise instruction in fact leads to uniformity and regularity in the generated output. Stepping up to the supervised setting, figure 3(f) shows that the text embeddings preserve enough signal to almost entirely separate the sample distributions corresponding to each prompt. We achieve a test accuracy of 89.9% in binary classification of the generative outputs corresponding to the two prompts. Section 4.4 shows that we can analyze this embedded data to quantitatively verify and further refine observations we make by manual inspection.\nWe also find that fine-tuned models produce outputs similar to the original. The clusters observed in figure 3(b) show that the distribution of outputs generated by Open Dalle embeds nearby that of SDXL, the former being a fine-tuning of the latter. The outputs of these two models have distinct separation from those of DDPM and the LSUN Cat data. Further, the results of supervised learning (figure 3(e)) show that the various distributions of generated and true images can all be disentangled. We achieve 100% test accuracy when classifying images produced by the text-to-image models (SDXL and Open-Dalle) against those of DDPM and the LSUN Cat dataset. The test accuracy is lower when instead classifying images from similar sources: 95.0% on DDPM versus LSUN Cat and 89.7% on SDXL versus Open-Dalle. We remark that while it is common knowledge that prompting and fine-tuning techniques can drastically change the qualities of generated output [48, 49], these numerical embeddings can be used to develop further prompting analytics for other applications."}, {"title": "4.4 Explaining distribution shifts in textual data with cluster regression", "content": "We can use the embeddings to further analyze the differences observed in the distributions of textual samples by performing cluster regression on the different modes observed in the principal components and linear discriminant, with indicator explanatory variables corresponding to interpretable attributes in the original text. The full list of regressions can be seen in table C3.\nIn our five topic arXiv dataset, we saw in section 4.2 that PC4 clusters real abstracts apart from those that were generated with Llama-2-70B. We can regress on this PC with an indicator explanatory variable corresponding to the appearance of \"filler\" words used to stress the importance of the research (see Appx. C.1). We find an $R^2$ of 0.47, suggesting that a significant portion of the differences between these real and AI-generated samples can be explained by Llama-2-70B generating abstracts that contains prose that explicitly stresses the importance of the paper.\nA similar experiments using the arXiv economics abstracts also gives an $R^2$ of 0.54 with PC1, which clusters real and fake abstracts. Alternatively, we can regress on this PC with and indicator representing if the length of the abstract is less than 1500 characters. In this case, we get $R^2 = 0.58$, indicating that a sizable portion of the variability between real and synthetic abstracts, from either prompt, is explained by length of the text, with the synthetic abstracts being longer on average.\nIn the Stack Exchange data, we find that the separation between real and AI generated responses can again be partially explained to a shifted vocabulary relative to the user-posted responses. On one hand, shifts in LD1 can be attributed to real and AI responses ($R^2 = 0.84$). We find that LD1 can be partially explained by the appearance of certain phrases such as 'if you have any questions', 'hope this helps' or 'alternatively' (See Appx. C.1).\nWe can examine the structure of our samples beyond just real and synthetic. In the Stack Exchange data, for example, we find that PC1 is partially explained by special characters ($R^2 = 0.56$). We infer that this is associated with notions of computer science and mathematics."}, {"title": "4.5 Synthetic data filtering via anomaly detection", "content": "The embedding mechanisms provided by DNNs allow us to identify AI-generated content as outliers relative to a reference sample of real data. When we contaminate samples of StackExchange responses with upwards of 20 of Llama-2 70B's responses, we find that isolation forests can be used to detect synthetic responses. The scatterplots in Figure 5 show that when we deal with small amounts of generated contaminants, the separation between real and fake data may not occur in the top principal components, but rather in higher PCs. The responses generated by Llama-2 70B are seen to lie on the outer edges of the embedded distribution when looking at higher PCs. Empirically, we find a relationship between the relative amounts of real (N) and fake (M) responses and our sensitivity of detecting fake responses as outliers in our data. With very little contamination, the most sensitive predictors are those which use smaller amounts of reference data (N < 500). As we increase the size the amount of contamination, the most sensitive predictors become those with larger reference samples of real data."}, {"title": "5 Discussion", "content": "Our experiments have demonstrated the utility of treating DNNs as feature embedders. In a special case, we analyzed machine generated content and show that generative models do not produce data from the same distribution as real data, despite the outputs often looking subjectively similar. More generally, these feature embeddings allowed us to quantitatively connect the abstract features obtained from numerical processing with human-interpretable features in the underlying data, providing a step towards explaining how AI models encode semantic content."}, {"title": "5.1 Generative AI outputs do not reflect real data.", "content": "Our application of statistical tools have confirmed that the data produced by machine learning models are highly distinguishable from those found in reference samples of real data. Specifically, current image models are likely not producing images from the same distribution as human-made images and current LLMs are not creating text in the full likeness of human writing. We hypothesize that these shifts are inherent to the generative models and likely cannot be removed without considerable filtering or human intervention, corroborated by the fact that some of the starkest shifts we observe lie between the real and synthetic data distributions.\nContextualizing our observations on real and AI-generated data, there is evidence that machine outputs may not align with real images or human writing. In the image case, the embeddings of images created by SDXL and Open Dalle were highly disparate from the reference image data in comparison to their DDPM predecessor. However, the techniques introduced by SDXL are measured to improve image generation relative to DDPM indicating that higher quality generations are shifted further from the baseline. In the natural language settings, our inability to access the pre-training corpora of many models confounds our understanding of the models' ability to emulate its underlying distribution. It is, however, known that data scrapings from StackExchange and the arXiv are used to train and assess LLMs [39, 40]. As models produce separate distributions when met with these same tasks, we conclude it unlikely for any of these models to entirely reproduce their training distributions.\nOur experiments emphasize necessary precaution in training new models on simulated data. More commonly we are seeing models trained on artificially generated data [50, 51]. Our observations that show misalignment between real and simulated data leads us to question what these models are optimizing for if not to learn the true semantics encoded in language and images. This is further underscored by the results in [52] and [53, 54] which show that data produced by generative models are necessarily prone to hallucination and repeated training on such data which has been shifted away from the reference can lead to model collapse. As the proportion of AI-generated content online continues to grow, our techniques can be used to filter out synthetic or otherwise undesirable data."}, {"title": "5.2 Embeddings for data authentication", "content": "Our experiments taught us that DNNs can be used to protect ourselves from A\u0399 generated fraud. The release of ChatGPT quickly led to noticeable shifts in writing styles [55], with other studies issuing concern on the human ability to detect articles produced by generative models [56-58]. Our experiments have shown promise for automatic detection using neural embeddings. For the scientific community, these tools might provide a filter against recent waves of automatically generated papers [59, 60]. So long as we preserve real data samples, numerical analysis on well-featurized data will let us quickly flag incoming data that has potentially been produced by a generative model.\nFurther, we hope that our techniques will be valuable for data analysis in the general community. Easily implemented, these tools can be used to quickly compare samples via statistical tests, or alternatively, we can catalogue different models based on the features they are able to successfully encode so that practitioners can quickly determine feature embedders that work well for their task."}, {"title": "5.3 Embedded representations as engineered features", "content": "Understanding the features which underlie the numerical biases of our embedded data gives powerful insight in verifying the patterns observed in the unprocessed data. We hope that such insight can provide value to other data scientists and practitioners. For example, these techniques can help us methodically select models, prompts, and other parameters in generative contexts. Current techniques for this may not be entirely reliable; they depend on expensive benchmarks and instance-based evaluation which often requires an LLM judge that is prone to hallucinations and other biases [49, 61, 62]. A direct look at embedded distributions can let the practitioner study features for their own preferences and evaluate robustness of the model. As AI models quickly advance in performance, we expect there will be value in the ability to readily measure for preferential features.\nOur intuitive observations on the features of our data is reflective of having a basic point of reference. That is, we viewed DNNs as tools to understand the variation present within our data. We do cautiously note that the measured variation and embedded features are dependent on the choice of feature embedder (see A2). While each FE encodes different features of the provided data, we believe that there is no universal FE and the best embedding choice will depend on the practitioner's use case."}, {"title": "5.4 Future directions", "content": "The world's increasing dependence on AI systems requires scalable tools to analyze the high stream of new data produced by artificial systems. The cluster regressions we"}]}