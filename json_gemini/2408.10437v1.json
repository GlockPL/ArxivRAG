{"title": "Understanding Generative AI Content with Embedding Models", "authors": ["Max Vargas", "Reilly Cannon", "Andrew Engell", "Anand D. Sarwate", "Tony Chiang"], "abstract": "The construction of high-quality numerical features is critical to any quantitative data analysis. Feature engineering has been historically addressed by carefully hand-crafting data representations based on domain expertise. This work views the internal representations of modern deep neural networks (DNNs), called embeddings, as an automated form of traditional feature engineering. For trained DNNs, we show that these embeddings can reveal interpretable, high-level concepts in unstructured sample data. We use these embeddings in natural language and computer vision tasks to uncover both inherent heterogeneity in the underlying data and human-understandable explanations for it. In particular, we find empirical evidence that there is inherent separability between real data and that generated from AI models.", "sections": [{"title": "1 Introduction", "content": "Before the rising popularity of deep neural networks (DNNs), data scientists would use knowledge of an application to create mappings, or feature representations, of \"raw\" data into vectors in a vector space [1, 2]. These vectors could then be used in statistics and machine learning pipelines. Viewed in the right way, DNNs also encode rich feature representations of data in high-dimensional vector spaces, offering a potential replacement to prior tools [3-5]. Leveraging existing knowledge about appropriate datasets, these vector space representations have been found to implicitly understand concepts ranging from time and space [6] to color [7] and more [8, 9]. In this article, we show that the representations provided by DNNs can be used to uncover latent biases in many datasets which, in turn, allow us to deduce high-level information about these datasets. Further, this information is simply encoded in ways that agree with human intuition.\nLeveraging the confounding nature of batch effects from experimental biology that variance within data results as an accumulation of differences in collection methods or the experimental design we use the feature representations provided by DNNs to explain shifts between data samples and their underlying distributions. For natural language, this variation can be attributed to simple concepts like subject matter and word appearances. Independently in text and image domains, we show that DNNs are expressive enough to encode heterogeneity between (1) AI- and human-produced content, (2) content produced by different generative models, and (3) content produced using different prompts on the same model. This heterogeneity, or model DNA (mDNA), manifests itself as an observable shift when performing unsupervised learning on the representations extracted from the DNN. These differences are brought out even more in the supervised setting, allowing us to identify forensic traces that link generative outputs back to the source model.\nFigure 1 depicts our perspective on neural network architectures as a decomposition of a feature embedder (FE) and a task layer. The FE comprises the bulk of the DNN, consisting of a series of transformation layers that have learned to map the data into a fixed vector space. After this mapping, the data is passed through a final statistical learning layer or task-specific \"head,\" commonly a linear classifier or token predictor, which makes predictions based on the outputs the FE [10-12]. In the case where the DNN is a large language model (LLM) or vision transformer, the FE transforms text or image data into high-dimensional vectors.\nTo summarize, using a DNN to act as a feature embedder provides encouraging prospects in understanding distinct modalities within our data which a priori may be unknown. This can provide new avenues for data verification, prompt engineering, fine-tuning, model explanation, and more. Specifically, our experiments show that:\n\u2022 Modern FEs can be used to extract human-interpretable features from data.\n\u2022 The numerical embedding vectors derived from FEs enable quantitative analysis of unstructured data such as text and images.\n\u2022 Tools like Principal Component Analysis (PCA) and linear regression on the embedded data can be applied to test hypotheses about data.\n\u2022 Pre-trained FEs are able to distinguish AI-generated content from data of real images and human writing. Further, they can be used to attribute content to particular models or types of prompts."}, {"title": "2 Related Works", "content": "Embeddings. Embedding techniques allow data, often in the form of text or images, to be represented within a large vector space for numerical analysis. There is interest in encoding semantic information in unstructured data for purposes of information retrieval, image labeling, document ranking, and more [13-16]. We show that the embeddings produced by current state-of-the-art models are powerful enough to distinguish semantic differences in our data. Across our examples, we demonstrate that these differences can be reliably detected using standard dimensionality reduction techniques.\nExplanatory AI. Recent work has explored the use of sparse autoencoders to decompose the activations of a model into interpretable components [17-19]. We find that the embeddings of the final hidden layer provide enough information to extract a similar high-level understanding of our data, without requiring any training. Furthermore, leveraging simple linear models such as PCA and LDA allows us to perform basic regressions to correlate human interpretable features with those encoded by the neural network.\nAI content detection. Increased use of generative AI has spurred interest in developing AI detection tools. Commercial services such as GPTZero [20], AI or Not [21], and Originality.ai [22] have been developed in attempts to solve this problem. However, [23] provides evidence that these popular detection tools are easily fooled with clever prompting techniques. Others have used embedding models trained via CLiP to distinguish real from synthetic images by applying various classifiers to the resulting embeddings [24-26]. Our results extend this work by validating on natural language examples and showing that separators (obtained from PCA and LDA) can be attributed with linear features. Furthermore, we show that anomaly detection with isolation forests [27] can identify AI generated content that has contaminated a reference sample of real data.\nLinear Approximation for NNs. Jacot et al. [28] showed that the training dynamics of DNNs are intimately linked to its neural tangent kernel (NTK) in the infinite-width limit, and Engel et al. [29] developed an explain-by-example technique using the empirical NTK. Essentially, the NTK is the sample to sample covariance of the linearized features for the DNN. Qadeer et al. [30] showed that the conjugate kernel (CK - the gram matrix of the activations of the embedded features) approximates the NTK. Our work leverages linear modeling on the CK for explainability."}, {"title": "3 Methods", "content": "3.1 Background\nWe view a DNN as a feature embedder (FE) that maps inputs into a high-dimensional latent vector space \\(R^n\\). Given an image or text sample x as an input, the DNN computes an embedding vector \\(y := FE(x) \\in R^n\\). A data set X of N samples maps to a data matrix \\(Y \\in R^{n \\times N}\\). In some cases, the output of the DNN is already a high-dimensional vector (e.g., image feature extractors) while in other cases, the map FE is obtained by extracting the final hidden layer activations. When beneficial, we refer to X and Y as the observed and latent data, respectively. We use basic techniques from statistics on the data matrix Y under the assumption that the columns of Y are independent and identically distributed vectors. This amounts to the assumption that the data points of X are themselves sampled independently from an underlying distribution. Reasoning about Y allows us to make deductions about the underlying distribution for X. We then rely on Principal Component Analysis (PCA) [31] for unsupervised learning and Linear Discriminant Analysis (LDA) [32] for supervised learning. PCA is an unsupervised learning method that finds linear coordinate directions in \\(R^n\\) that maximize the variance observed in the data Y. LDA is a supervised learning method for classification that classifies points according to a linear decision boundary.\nUsing the principal components and linear discriminants of the embedded data, we analyse the sources of variability using regression-based hypothesis testing. Our null hypothesis is that all of the samples come from the same distribution and thus that the explanatory variable has no relationship with the response. We then examine our samples to determine if we can reject this conclusion.\n3.2 Data and models\nWe were interested in understanding how the FEs from pre-trained models correspond to \u201chidden\u201d labels (metadata) and whether a discriminative model could predict these labels. Of particular interest to us was whether the FEs could be used to separate synthetic data (generated by an AI model) from natural (or reference) data."}, {"title": "4 Results", "content": "Our findings suggest that the highest degrees of variability often have high correlation with known differences in the data samples, such as subject area, and with less-obvious ones, like language semantics. We also study the semantics of synthetically generated content relative to references of real data. This analysis is then extended beyond using human-made samples as a baseline, where we find that the modalities within AI-generated data can be attributed to the choices of models and prompts. Finally, we show that, in the case of textual data, these regressions can be used to explain the observed variations in terms of patterns in the underlying text.\n4.1 Language model embeddings encode high-level semantics\nLooking at news articles in the MLSUM dataset, the feature embeddings produced by language models are able to automatically sort the news articles based on broad features in the data.  shows that PC1 can be used to separate articles on language, PC2 to separate on topic, and PC3 can distinguish between native Spanish news articles and those that were translated from Russian. Noting that language and topic were originally encoded into MLSUM as metadata, we see that unsupervised clustering with the LLM embeddings let us recover these high-level features. The \\(R^2\\) values for these regressions are 0.98, 0.87, and 0.83, respectively. We remark that the separation between the native Spanish articles and those that were machine translated may be a compounded effect of the quality of translation along with other natural semantic variation between Spanish and Russian cultures and patterns.\nAnalyzing academic abstracts predating 2020 across five topic areas indicates that the top principal components encode these various subjects. When comparing real abstracts with synthetic abstracts generated by Llama-2 70B, we find that the differences between real and artificial writings are another contributing source of variation.  shows a strong relationship between the first principal component and the topic of Experimental High Energy Particle Physics (R\\(^2\\) = 0.92), PC2 to Programming Languages (R\\(^2\\) = 0.76), PC3 to Quantitative Cell Behavior (R\\(^2\\) = 0.86), and PC5 to statistical methodology (R\\(^2\\) = 0.71) .\n4.2 Generated data is shifted from real baselines\nWe find that embeddings encode differences in real and AI-generated content, and this distinction is represented in the initial few principal components. In our five subject arXiv data,  shows that, despite the real and synthetic abstract distributions overlapping almost entirely when we plot a UMAP of the first 30 PCs, examining the individual principal components shows us that the two samples can in fact be distinguished. In particular we see that a clear shift in PC4 between the real abstracts and those generated by Llama-2-70B (R\\(^2\\) = 0.79), and a simple LDA classifier achieves 99.8% test accuracy when classifying real/AI-generated abstracts across all five topics (right). Moreover, the horizontal clustering observed in the LDA plot correspond to subject area (see ).\nWe see this shift in our other experiments as well. In the Stack Exchange dataset, there is a clear shift between human responses and those generated by AI in the top 2 PCs. In fact, a linear combination of these first two PCs can be partially explained by the determination of real and AI-generated responses (R\\(^2\\) = 0.45). Supervised training on the language model embeddings establishes that the real and synthetic data are linearly separable, obtaining 99.0% testing accuracy between real and synthetic responses. This separability also exist in images: LDA achieves 99.4% test accuracy in binary classification of the real and generated samples, where the generated images were taken from all models. Surprisingly, while the unconditional model (DDPM) was trained on the LSUN cat dataset, it still fails to capture all the features inherent to cat images, with LDA achieving a test accuracy of 95.0% in this binary classification.\n4.3 Embeddings are sensitive to generative techniques\nWe find that choices in generative techniques such as the model and prompt can shift the distributions observed through DNN embeddings. In many cases, unsupervised clustering techniques are able to recognize such shifts (see figure 3). With supervision, however, these shifts are pronounced. This section establishes the proof-of-concept that forensic analysis of synthetically generated data is possible. That is, given a sample from one of several understood distributions, we can trace through the embedded representations to determine if it is real or, in the synthetic case, specific generative parameters.\nModel DNA. At the zero-shot level, different generative models have distinct identifying mDNA for authorship determination. In the case of StackExchange, supervised LDA can separate the responses for each question based on authorship type. Using the embeddings provided by Mistral 7B, we are able to predict whether an answer is user-written, or authored by Llama-2 70B, Mixtral 8x7B, or Falcon 40B with an average overall accuracy of 90.7%. The presence of mDNA is further evidenced when we asked language models to translate 5,000 sports articles from German to English. Comparing Mixtral-8x7B against some dedicated translation models (nllb-3.3B and mt-opus-de-en [46]), we can identify the machine translator of a given article 93.2% of the time using a supervised LDA classifier. We note, however, that our ability to identify outputs is likely dependent on the long-form generative nature of the task. On short-form translations on the United Nations Parallel Corpus [47], our techniques do not identify significant separation between the different kinds of machine and human responses (analogous to non-specificity of short fragments of DNA sequences).\nPrompting and Fine-Tuning. The embeddings of DNNs can also detect and quantify shifts in generative content caused by changes in prompting techniques. With economics abstracts generated from different prompts (provided in Appx. Figure A3), unsupervised clustering is able to recognize that a more descriptive prompt tightens the distribution of Llama-2 70B generations along PC1 (figure 3(c)). This indicates that precise instruction in fact leads to uniformity and regularity in the generated output. Stepping up to the supervised setting, figure 3(f) shows that the text embeddings preserve enough signal to almost entirely separate the sample distributions corresponding to each prompt. We achieve a test accuracy of 89.9% in binary classification of the generative outputs corresponding to the two prompts. Section 4.4 shows that we can analyze this embedded data to quantitatively verify and further refine observations we make by manual inspection.\nWe also find that fine-tuned models produce outputs similar to the original. The clusters observed in figure 3(b) show that the distribution of outputs generated by Open Dalle embeds nearby that of SDXL, the former being a fine-tuning of the latter. The outputs of these two models have distinct separation from those of DDPM and the LSUN Cat data. Further, the results of supervised learning (figure 3(e)) show that the various distributions of generated and true images can all be disentangled. We achieve 100% test accuracy when classifying images produced by the text-to-image models (SDXL and Open-Dalle) against those of DDPM and the LSUN Cat dataset. The test accuracy is lower when instead classifying images from similar sources: 95.0% on DDPM versus LSUN Cat and 89.7% on SDXL versus Open-Dalle. We remark that while it is common knowledge that prompting and fine-tuning techniques can drastically change the qualities of generated output [48, 49], these numerical embeddings can be used to develop further prompting analytics for other applications.\n4.4 Explaining distribution shifts in textual data with cluster regression\nWe can use the embeddings to further analyze the differences observed in the distributions of textual samples by performing cluster regression on the different modes observed in the principal components and linear discriminant, with indicator explanatory variables corresponding to interpretable attributes in the original text. The full list of regressions can be seen in table C3.\nIn our five topic arXiv dataset, we saw in section 4.2 that PC4 clusters real abstracts apart from those that were generated with Llama-2-70B. We can regress on this PC with an indicator explanatory variable corresponding to the appearance of \"filler\" words used to stress the importance of the research (see Appx. C.1). We find an R\\(^2\\) of 0.47, suggesting that a significant portion of the differences between these real and AI-generated samples can be explained by Llama-2-70B generating abstracts that contains prose that explicitly stresses the importance of the paper.\nA similar experiments using the arXiv economics abstracts also gives an R\\(^2\\) of 0.54 with PC1, which clusters real and fake abstracts. Alternatively, we can regress on this PC with and indicator representing if the length of the abstract is less than 1500 characters. In this case, we get R\\(^2\\) = 0.58, indicating that a sizable portion of the variability between real and synthetic abstracts, from either prompt, is explained by length of the text, with the synthetic abstracts being longer on average.\nIn the Stack Exchange data, we find that the separation between real and AI generated responses can again be partially explained to a shifted vocabulary relative to the user-posted responses. On one hand, shifts in LD1 can be attributed to real and AI responses (R\\(^2\\) = 0.84). We find that LD1 can be partially explained by the appearance of certain phrases such as 'if you have any questions', 'hope this helps' or 'alternatively' (See Appx. C.1).\nWe can examine the structure of our samples beyond just real and synthetic. In the Stack Exchange data, for example, we find that PC1 is partially explained by special characters (R\\(^2\\) = 0.56). We infer that this is associated with notions of computer science and mathematics.\n4.5 Synthetic data filtering via anomaly detection\nThe embedding mechanisms provided by DNNs allow us to identify AI-generated content as outliers relative to a reference sample of real data. When we contaminate samples of StackExchange responses with upwards of 20 of Llama-2 70B's responses, we find that isolation forests can be used to detect synthetic responses. The scatterplots in  show that when we deal with small amounts of generated contaminants, the separation between real and fake data may not occur in the top principal components, but rather in higher PCs. The responses generated by Llama-2 70B are seen to lie on the outer edges of the embedded distribution when looking at higher PCs. Empirically, we find a relationship between the relative amounts of real (N) and fake (M) responses and our sensitivity of detecting fake responses as outliers in our data. With very little contamination, the most sensitive predictors are those which use smaller amounts of reference data (N < 500). As we increase the size the amount of contamination, the most sensitive predictors become those with larger reference samples of real data."}, {"title": "5 Discussion", "content": "Our experiments have demonstrated the utility of treating DNNs as feature embedders. In a special case, we analyzed machine generated content and show that generative models do not produce data from the same distribution as real data, despite the outputs often looking subjectively similar. More generally, these feature embeddings allowed us to quantitatively connect the abstract features obtained from numerical processing with human-interpretable features in the underlying data, providing a step towards explaining how AI models encode semantic content.\n5.1 Generative AI outputs do not reflect real data.\nOur application of statistical tools have confirmed that the data produced by machine learning models are highly distinguishable from those found in reference samples of real data. Specifically, current image models are likely not producing images from the same distribution as human-made images and current LLMs are not creating text in the full likeness of human writing. We hypothesize that these shifts are inherent to the generative models and likely cannot be removed without considerable filtering or human intervention, corroborated by the fact that some of the starkest shifts we observe lie between the real and synthetic data distributions.\nContextualizing our observations on real and AI-generated data, there is evidence that machine outputs may not align with real images or human writing. In the image case, the embeddings of images created by SDXL and Open Dalle were highly disparate from the reference image data in comparison to their DDPM predecessor. However, the techniques introduced by SDXL are measured to improve image generation relative to DDPM indicating that higher quality generations are shifted further from the baseline. In the natural language settings, our inability to access the pre-training corpora of many models confounds our understanding of the models' ability to emulate its underlying distribution. It is, however, known that data scrapings from StackExchange and the arXiv are used to train and assess LLMs [39, 40]. As models produce separate distributions when met with these same tasks, we conclude it unlikely for any of these models to entirely reproduce their training distributions.\nOur experiments emphasize necessary precaution in training new models on simulated data. More commonly we are seeing models trained on artificially generated data [50, 51]. Our observations that show misalignment between real and simulated data leads us to question what these models are optimizing for if not to learn the true semantics encoded in language and images. This is further underscored by the results in [52] and [53, 54] which show that data produced by generative models are necessarily prone to hallucination and repeated training on such data which has been shifted away from the reference can lead to model collapse. As the proportion of AI-generated content online continues to grow, our techniques can be used to filter out synthetic or otherwise undesirable data.\n5.2 Embeddings for data authentication\nOur experiments taught us that DNNs can be used to protect ourselves from A\u0399 generated fraud. The release of ChatGPT quickly led to noticeable shifts in writing styles [55], with other studies issuing concern on the human ability to detect articles produced by generative models [56-58]. Our experiments have shown promise for automatic detection using neural embeddings. For the scientific community, these tools might provide a filter against recent waves of automatically generated papers [59, 60]. So long as we preserve real data samples, numerical analysis on well-featurized data will let us quickly flag incoming data that has potentially been produced by a generative model.\nFurther, we hope that our techniques will be valuable for data analysis in the general community. Easily implemented, these tools can be used to quickly compare samples via statistical tests, or alternatively, we can catalogue different models based on the features they are able to successfully encode so that practitioners can quickly determine feature embedders that work well for their task.\n5.3 Embedded representations as engineered features\nUnderstanding the features which underlie the numerical biases of our embedded data gives powerful insight in verifying the patterns observed in the unprocessed data. We hope that such insight can provide value to other data scientists and practitioners. For example, these techniques can help us methodically select models, prompts, and other parameters in generative contexts. Current techniques for this may not be entirely reliable; they depend on expensive benchmarks and instance-based evaluation which often requires an LLM judge that is prone to hallucinations and other biases [49, 61, 62]. A direct look at embedded distributions can let the practitioner study features for their own preferences and evaluate robustness of the model. As AI models quickly advance in performance, we expect there will be value in the ability to readily measure for preferential features.\nOur intuitive observations on the features of our data is reflective of having a basic point of reference. That is, we viewed DNNs as tools to understand the variation present within our data. We do cautiously note that the measured variation and embedded features are dependent on the choice of feature embedder (see A2). While each FE encodes different features of the provided data, we believe that there is no universal FE and the best embedding choice will depend on the practitioner's use case.\n5.4 Future directions\nThe world's increasing dependence on AI systems requires scalable tools to analyze the high stream of new data produced by artificial systems. The cluster regressions we"}, {"title": "Appendix A Details on data sets and prompts", "content": "The features of the data sets we consider are summarized in Table A1. Three data sets have been curated by others and are available through standard machine learning and AI software packages, whereas we generated the ArXiV data.\nWe passed our data through cleaning pipelines to ensure fair comparisons when passed through feature embedders. We use Huggingface Diffusers for image generation models and Huggingface Transformers for image embedding models and lanaguage models (both text generation and embedding). Image data was loaded and processed using NumPy and TensorFlow. Further details are in the following sections.\nA.1 Embedding models\nBefore discussing the datasets, we briefly review the embedding models that we used in our main experiments. In all cases, recall that the associated FE comprises the bulk of the DNN and the resulting embedding is obtained as the output of the final layer in the FE. For language models, we process the data according to Algorithm 1.\n\u2022 Mistral-7B [34] is a large language model with a transformer based architecture with vocabulary of 32k tokens, has 32 layers, and uses 13 billion parameters per token. It has a context window of 8,192 tokens. The FE consists of all layers except for the final token prediction head. That is, embeddings are obtained from the final hidden layer of dimension 4,096.\n\u2022 multilingual-e5-large [36] is a 24 layer neural networks that generates 1024 dimensional embedding vectors from text. The model has 560M parameters and supports 100 languages. It has a context window of 512 tokens and long text is truncated to fit within this window. This model is trained to produce embeddings, so the associated FE is the entire network.\n\u2022 Data Filtering Network [38] is a CLIP model trained on 5B images that were filtered from an uncurated dataset of image-text pairs. It has 1B parameters and can be used to encode both text and images.\nWe tested additional models for our classification tasks in A2. The main text uses the best performers, but it is worth noting that each feature embedder generates its own unique embeddings for a provided dataset. Interestingly in the case of NLP, dedicated embedding models that score well on the MTEB leaderboard [63] were"}, {"title": "A.2 Generative Models", "content": "Here we provide brief descriptions of the generative models we used to create synthetic data.\n\u2022 Llama-2-70B [39] is a large language model based on the transformer architecture. It has a context window of 4k tokens and 80 layers. It was trained across 2 trillion tokens. We use this model to generate synthetic scientific abstracts and responses to Stack Exchange queries.\n\u2022 Mixtral-8x7B [34] is a large language model which uses a \u201csparse mixture of experts\" framework. It uses a transformer based architecture with inputs of 32k tokens, has 32 layers, and uses 13 billion parameters per token. We used this model for generating responses to Stack Exchange questions.\n\u2022 nllb-200-distilled-600M and nllb-200-3.3B are language models specifically tailored for language translation tasks and is built off of a modified transformer architecture [68]. The models are only trained to handle 512 tokens at a time, so we perform translations once sentence at a time. For the MLSUM dataset, we use python's nltk package [69] to parse the data into individual sentences.\n\u2022 DDPM [43] is a seminal diffusion-based image generation model with a U-Net [70] style architecture. The specific model used here unconditionally generates images of cats and is trained on the cat class of the LSUN dataset. It has 114M parameters.\n\u2022 SDXL [71] is another diffusion-based model. It is a text-conditional model, so the user must provide a prompt to guide the result of the generated images. It again uses a U-Net style architecture and has 2.7B parameters. The OpenDalle [45] model is a fine-tuning of SDXL."}, {"title": "A.3 Stack Exchange", "content": "To generate the data, take a random sample of 20,000 questions from the StackExchange preference dataset cureted by Huggingface [33]. We require that these questions do not link to external webpages. Wrapping each question around a prompt template and feeding this into the language models produces a set of synthetic responses for each question. We filter out all responses to any question where some LLM did not complete a response due to context length limitations. That is, we assert that the token length of the question and response remains under 8188 tokens for Mixtral-8x7B, under 4092 tokens for Llama-2-70B, and under 2045 tokens for and Falcon 40B, where we use a small buffer to ensure we do not surpass the context length of the language model. We remove any prefixing whitespace to all responses. Noticing that Falcon 40B produces codeblocks with css tags <code> and <\\code>, we replaced this with triple ticks ```as was observed in the other sources. Lastly, if any question has a response (whether user-generated or synthetic) that includes a url, we throw away all responses to that question. What remains is four sets of responses to 11,704 questions for a total of 46,816 responses. A sample of responses in below in Appendix D.2. We use this data to test whether feature embedders can separate user-written responses from those of large language models. The use of multiple models allows us to draw conclusions about model DNA that differences in the training data, regularization techniques, learning schedulers, and other factors surface themselves as detectable differences in model outputs."}, {"title": "A.4 MLSUM", "content": "The MLSUM dataset [35] consists of categorized news articles and their summaries across five languages (Spanish, Russian, German, French, Turkish). The news articles are sourced from popular news websites in each of the languages. Focusing on the sport and economics categories in Spanish and Russian, we randomly sample 500 news articles and translate the Russian articles into Spanish. This gives a total of 6 classes (sports and economics for Russian, Spanish, and translated Russian). This dataset is used to determine if feature embedders can distinguish the differences between languages, news categories, as well as native and non-native text. We also randomly sampled 5,000 German sports articles and had three language models (Mixtral-8x7B, nllb-200-3.3B, and opus-mt-de-en) translate them to English. This second dataset is used for the model DNA tests where we see if feature embedders can separate the outputs of different generative models.\nTo generate the translations between languages using Mixtral-8x7B, we use the template in A1 to create the prompt and then wrap it in a chat template. We feed the resulting list of tokens into the model and clean up any preliminary text such as 'Translation:' in the model response. Translations produced by the nllb series used a translation pipeline object in the Huggingface Transformers package, setting 'src\\_lang' and 'tgt\\_lang' to be the start and ending languages, respectively. Translations with the opus model were created by calling model.generate() on the tokenized text. For both the nllb and opus series used pipelines with the Transformers package, proceeding one sentence at a time due to context window limitations of these models."}, {"title": "A.5 ArXiv Five Topic", "content": "We randomly sampled 200 ArXiv abstracts across each of the five subject domains of high-energy particle physics (hep-ex), programming languages (cs.PL), quantitative biology cell behavior (q-bio.CB), statistical methodology (stat.ME), and quantitative finance portfolio management (q-fin.PM). We also enforce that the corresponding papers were uploaded in the year 2020 or earlier to ensure that they preceeded popular AI-tools such as ChatGPT.\nTo create the synthetic abstracts, we used Llama-2 70B to generate, for each real ArXiV paper, a synthetic abstract for a paper with the same title. Figure A2 shows the prompt that we used.\nTo process the data, we remove attached classifiers like 'Keywords:' and those used by the Journal of Economic Literature. We removed newline symbols, web addresses, text stating where code is available. We also removed any prefix and suffix from synthetic abstracts, such as 'Sure! Here is the abstract you wanted:'. We replaced LaTeX symbols formatting with plain text equivalent. We also replaced Greek letters with their English counterpart (7 changed to tau). Any otherwise unusable abstracts were removed as well, such as when the paper was retracted by the author (one case) and when the model produced nonsense (one case). We kept \"normal\" special character that are plainly visible on a QWERTY keyboard, with the exception of the US Dollar sign which LaTeX uses for math equations."}, {"title": "A.6 Arxiv Economics", "content": "We further collected 816 abstracts from general economics (econ.GN), where each abstract could be cross-listed in one of the five categories in the previous subsection. Using two different prompt templates, we generated synthetic abstracts using the"}, {"title": "B.1 Unsupervised learning", "content": "Given a data points \\(X_1,...,X_n \\in R^m\\), Principal Component Analysis (PCA) [31] is a technique finds linear coordinate directions in \\(R^n\\) that maximize the variance observed in the data. The principal components turn out to be eigenvectors of the data's covariance matrix and are linear combinations of the original data features so"}, {"title": "B.2 Supervised learning", "content": "Linear Discriminant Analysis [32] is a linear classification technique that, given data \\(X_1,..., X_n \\in R^m\\), aims to find a linear combination of features in \\(R^m\\) that separate the different classes within the data. In particular, LDA can be thought of as another linear dimensionality-reduction technique (akin to PCA) which aims to maximize the separation between the class means. We use the implementation of LDA in the Scikit-Learn package.\nCluster Regression is performed so that we can attribute easily explainable features to the different clustering patterns that we observe in PCA and LDA. In short, we use simple and interpretable features to predict the value along a given feature direction. In cases that our embedded data clusters along this direction, this amounts to predicting the cluster of a sample from its more interpretable feature.\nTo perform cluster regression on the textual data, we build indicator functions IF for a particular feature F. Typically, F is inferred from metadata or other easily extractable information. Some examples include whether an abstract discusses physics or not, or whether some text contains certain words or phrases. Using these indicator functions, we produce a linear regression on the clusters observed along principal components and linear discriminants and report the \\(R^2\\) value.\nIsolation Forests [27] is a technique to identify outliers, or anomalies, within a dataset. It works on the supposition that outliers are fewer in count and have feature values much different than inliers, making them easier to separate. Using a tree structure on the feature space, this technique estimates how easily a single point can be isolated; outliers can be isolated with shallow trees and inliers need deep trees. We use the implementation of isolation forests in the Scikit-Learn package.\nFixing the sample size of real data (N) and AI contaminants (M), we perform PCA and use Scree plots to determine the amount of signal to pass through the algorithm. A parameter search across 50 seeds to find the optimal number of trees to use in our isolation forests, up to using 200 trees. We do this for all \u039d\u2208 {100, 200, 300, 400, 500, 1000, 2000} and M\u2208 {1,2,4,6,8, 10, 12, 15, 20}."}, {"title": "C.1 Regressions", "content": "Table C3 lists more regressions that were performed to explain some of the behaviors observed in the embedded data. As described above, we build indicator functions to account for the presence (or lack of) a particular feature. Using this indicator function, we perform linear regression to predict a particular embedded feature. We build indicator functions to try and describe patterns we observed by inspecting the AI-generated outputs compared to the real data. Through our regressions, we find that some of these hypothesized identified"}]}