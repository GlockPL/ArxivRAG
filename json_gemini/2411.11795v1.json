{"title": "Exploring adversarial robustness of JPEG AI: methodology, comparison and new methods", "authors": ["Egor Kovalev", "Georgii Bychkov", "Khaled Abud", "Aleksandr Gushchin", "Anna Chistyakova", "Sergey Lavrushkin", "Dmitriy Vatolin", "Anastasia Antsiferova"], "abstract": "Adversarial robustness of neural networks is an increasingly important area of research, combining studies on computer vision models, large language models (LLMs), and others. With the release of JPEG AI \u2013 the first standard for end-to-end neural image compression (NIC) methods \u2013 the question of its robustness has become critically significant. JPEG AI is among the first international, real-world applications of neural-network-based models to be embedded in consumer devices. However, research on NIC robustness has been limited to open-source codecs and a narrow range of attacks. This paper proposes a new methodology for measuring NIC robustness to adversarial attacks. We present the first large-scale evaluation of JPEG AI's robustness, comparing it with other NIC models. Our evaluation results and code are publicly available online (link is hidden for a blind review).", "sections": [{"title": "1. Introduction", "content": "With the success of neural networks in various high-level computer vision tasks, the researchers proposed many low-level image processing methods, such as image compression, based on neural networks [18, 38, 41]. These methods use neural networks to solve sub-tasks of conventional image compression pipelines or propose an end-to-end pipeline for the encoding-decoding process. Promising results shown by NIC methods led to the establishment of JPEG AI [5], the first complete image compression standard based on neural networks. It is a Joint Photographic Experts Group (JPEG) project that promises significant improvements over non-neural-networks-based codecs, such as JPEG or JPEG2000. Image compression in JPEG AI is trained end-to-end and includes four key components: analysis transform, where an encoder transforms the original image into a latent representation using a nonlinear transform; quantization, used to compress the image representation and reduce the excessive information; entropy coding, where data is compressed into binary data based on the predictability of the information; and synthesis transform, where using a learned nonlinear synthesis transform, a decoder reconstructs the image from the compressed data. Standardization involves developing a common testing methodology, including test sequences, quality measurement methods, etc.\n\nThe adversarial robustness of neural networks has been a growing area of research recently [10, 16, 37, 42]. Numerous studies have been published regarding adversarial attacks, defenses, and overall analysis of robustness for various computer vision tasks, such as image classification [43], detection [30], quality assessment [3], etc. With the release of a new promising algorithm for compression that may become a standard approach in the industry, it is crucial to assess its robustness against adversarial threats to mitigate potential risks in image compression systems. To accomplish this, we thoroughly analyzed versions of JPEG AI and its latest counterparts by subjecting them to adversarial attacks with various loss functions. We also explore how to counter adversarial attacks with preprocessing defense strategies. Our main contributions are summarized as follows:\n\u2022 We extend the methodology proposed in [12] by incorporating multiple full-reference metrics\u2014namely, \u2206PSNR, \u2206MSE, \u2206MS-SSIM, and \u2206VMAF \u2013 to evaluate robustness under attack.\n\u2022 We perform extensive experiments on 10 NIC models (including novel JPEG AI), analyzing 6 attacks targeting im-"}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Neural image compression", "content": "Neural image compression has been developing rapidly in recent years. Balle et al. [6] use a generalized divisive normalization of joint nonlinearity and uniform scalar quantization for image compression. [1] implemented a new soft-to-hard vector quantization method and incorporated it into the compressive autoencoder introduced earlier. Balle et al. [7] express the image compression problem as variational autoencoders and propose hyperpriors to improve the entropy model. Minnen et al. [29] improve the GSM-based entropy model by generalizing the hierarchical GSM model to a Gaussian mixture model and adding an autoregressive component. Mentzer et al. [28] use GAN in their neural compression model, which helps to achieve high perceptual fidelity. Chang et al. [13] develop a novel entropy model by leveraging discretized Gaussian mixture likelihoods combined with a simplified attention module to present an efficient image compression network. Yang et al. [39] presents a novel rate and complexity control mechanism with the help of slimmable modules. He et al. [19] describe a new ELIC model that adopts stacked residual blocks as nonlinear transforms and uses the Space-Channel ConTeXt (SCCTX) model, which is a combination of the spatial context model and the channel conditional backward-adaptive entropy model. Zou et al. [44] introduce a flexible window-based attention module to enhance image compression models and train CNN and Transformer models that reach promising results. Liu et al. [25] propose using parallel transformer-CNN mixture blocks to combine the advantages of both approaches and a new entropy model that uses a swim-transformer-based attention module with channel squeezing. [17] adopted a hierarchical VAE architecture, ResNet VAE, for image compression, using a uniform posterior and a Gaussian convolved with a uniform prior. Wang et al. [34] create a real-time neural image compression model using residual blocks and depth-wise convolution blocks and then use mask decay and novel sparsity regularization loss to transfer knowledge to smaller models. Yang et al. [40] present a novel lossy compression scheme that uses an encoder to map images onto a contextual latent variable fed into a diffusion model for reconstructing the source images."}, {"title": "2.2. JPEG AI and its versions", "content": "JPEG AI [5] applies quantization across the entire image based on learned patterns, which is more efficient than the traditional block-based approach. The standard has a high operation point (HOP) and a base operation point (BOP). They differ in terms of compression efficiency and computational complexity. The HOP uses a multistage context modeling (MCM) technique and a more complex synthesis transform with an embedded attention mechanism. This allows for improved compression, but at the cost of reduced performance. The standard also includes a set of additional tools that can help make compression more efficient and adaptive to the content. These include Residual and Variation Scale (RVS), Filters such as Adaptive Re-Sampler, ICCI (Cross-Color Filter), LEF (Luma Edge Filter), and Non-Linear Chroma Filter, Latent Scale before Synthesis, and Channel-Wise Gain. The specific tools and their settings can vary depending on the codec configuration. Therefore, in the future, JPEG AI will be tested without specific tools, and the evaluation will be based on the default set-"}, {"title": "2.3. Adversarial robustness of neural image compression", "content": "Kang et al. [24] proposed an attack to increase the size of NIC-compressed images using the I-FGSM [21] attack. Their study explored the attack's effectiveness across different codec architectures, identifying architectural elements that improved model reliability, and proposed their factorized attention model as the most stable. Chen and Ma [11] demonstrated the high vulnerability of NIC methods to adversarial attacks that reduce the quality of the decoded image. They adopted the I-FGSM and Carlini-Wagner [9] attacks to increase the difference between the compressed images before and after the attack. They attacked images after compression and proposed a new Fast Threshold-constrained Distortion Attack (FTDA). They also proposed a \u2206PSNR method for assessing the success of an attack, which evaluates both its effectiveness and its visibility:\n\n\u2206PSNR = PSNR(x, C(x)) \u2013 PSNR(x', C'(x')),\n\nwhere x is the original image, x' is the adversarial image, C() - image after NIC."}, {"title": "3. Problem statement", "content": "Neural image compression. Lossy image compression is based on a rate-distortion theory. The goal of lossy image compression is to find a trade-off between the size of the compressed image representation and the decrease in the perceptual quality of a reconstructed image. This problem is formulated as follows:\n\n$\\mathbb{E}_{x \\sim p_x} [\\mathcal{R}(y) + d(x,\\hat{x})]$,\n\nwhere $\\mathcal{R}(y)$ is a bit representation of a quantized image after arithmetic encoding, and $d(x,\\hat{x})$ perceptual similarity metric (PSNR, SSIM, etc.).\n\nAuto-encoder architecture is one of the possible solutions. For a given image $x \\in \\mathcal{X} = \\mathbb{R}^{H \\times W \\times 3}$ of a distribution of natural images $p_x$, an encoder $E$ transforms it to a latent representation $y = E(x)$ of a distribution $p_y$. Then, the data is quantized $\\hat{y} = Q(y)$, and a decoder $G$ performs the reconstruction of the image $\\hat{x} = G(\\hat{y})$. We denote $C(x)$ as a complete encoding-decoding process $C(.) = G \\circ Q \\circ E: \\mathcal{X} \\rightarrow \\mathcal{X}$.\n\nAdversarial attack on image compression. The goal of an adversarial attack is to find a perturbation $\\delta$ which, added to the original image, makes the adversarial image $x' = x + \\delta$ such that its decoded image $C(x')$ differs from the original image as much as possible. Adversarial attack $A: \\mathcal{X} \\rightarrow \\mathcal{X}$ is defined as follows:\n\n$A(x) = \\underset{x': \\rho(x', x) < \\epsilon}{\\text{arg max}} \\mathcal{L}(x, x', C'(x), C(x'))$,\n\nwhere $\\rho(x', x) = ||\\delta||$, $\\epsilon$ imposes a constraint on the perturbation magnitude, $\\mathcal{L}: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ is corresponding optimization target. To achieve this goal, we consider 10 loss functions for all employed attacks. They reflect different approaches to measuring the distance between the original and adversarial images and their reconstructed versions. Additionally, we consider an alternative optimization goal. Instead of increasing the distance between the image targets, we reduced the compression ratio of the NIC measured in Bits Per Pixel (BPP):\n\n$A(x) = \\underset{\\delta: ||\\delta|| \\le \\epsilon}{\\text{arg max}} \\text{BPP}(Q(E(x + \\delta)))$.\n\nWe list all optimization targets in Table 1."}, {"title": "4. Methodology", "content": ""}, {"title": "4.1. NIC models", "content": "To evaluate the robustness of JPEG AI, we compare it with the robustness of other neural network image codecs. This further allows us to evaluate each NIC method's behavior and robustness."}, {"title": "4.2. Attacks", "content": "We use various adversarial attacks to assess a NIC model's robustness (Table 3). This allows us to create more diverse perturbations and draw more reasonable conclusions about the model's robustness. In this study, we focus on white-box attacks for the following reasons. Compression itself is a purification defense, and it can mitigate adversarial noise introduced by weaker black-box attacks, while white-box attacks offer more robust and effective perturbations. Also, black-box attacks are much more computationally expensive, severely limiting their range of applications. We choose six different white-box attacks of various types and Gaussian random noise to conclude that adversarial attacks are far more effective than random perturbations of the image. MADC [35] was one of the first methods introduced in 2008 that uses gradient projection onto a proxy Full-Reference metric to preserve image quality. I-FGSM [21] is a well-known iterative modification of FGSM attack with simple sign gradient descent. PGD [27] is similar to I-FGSM but uses random initialization. SSAH [26] decomposes an image into low- and high-frequency domains and insert perturbation in the latter to reduce attack visibility. cAdv [8] attack applies a filter in LAB color space, shifting the color distribution of an image without introducing noise. We also included Random noise as a baseline attack. It samples Gaussian noise in an attempt to attack the model."}, {"title": "4.3. Adversarial defenses", "content": "We selected several reversible adversarial defenses to evaluate their efficiency against adversarial attacks on NIC. We employed mainly reversible transformations to reduce their effect on image degradation. The full list of the applied defenses is in Table 4. Flip takes an image and reflects it horizontally or vertically. The reversed version flips the output image again to restore the original image orientation. Random roll selects either height or width randomly and samples the size of the roll, then rolls the image by a random number of pixels. The reverse step restores the original alignment. Random rotate defense samples the angle randomly and rotates the image on the selected angle. To make restoration of the original image possible, the method also performs a center pad of the original image to ensure image borders do not cut all its content. Random color reorder defense chooses perturbation of the color channels of the image tensor and swaps them, restoring the original order of the NIC output. Random ensemble combines Roll, Rotate, and Color reorder properties. It samples 10 actions from Roll, Rotate, and Color reorder with 4, 4, and 1 weights, respectively. Geometric self-ensemble [11] generates 8 defended image candidates with flipping and rotation. It chooses one of them as the output that resembles the least distorted after the preprocess-NIC-postprocess pipeline. The distortion is measured by the mean squared error between the original image and the image processed by the defended NIC. DiffPure [31] was developed to counter adversarial attacks on image classifiers and showed state-of-the-art performance for defending computer vision models. It performs purification based on a diffusion model as preprocessing and does nothing in the postprocessing step."}, {"title": "4.4. Datasets", "content": "To evaluate methods, we chose four well-known datasets. The KODAK Photo CD [20] set consists of 24 768 \u00d7 512 uncompressed images. CITYSCAPES [15] is a domain-specific dataset for image segmentation tasks with urban street scenes. These datasets are commonly used in the field of image compression. NIPS 2017: Adversarial Learning Development Set [14] is designed for evaluating adversarial attacks against image classifiers. Finally, the BSDS dataset [4], with 500 images of 320 \u00d7 448 resolution focusing on segmentation and boundary detection, enables us to evaluate the effects of NIC on segmentation model performance, further assessing implications of attacks on compression models for computer vision tasks."}, {"title": "4.5. Quality metrics", "content": "We employ four different Full-Reference image quality metrics to numerically assess the effects of adversarial attacks on images before and after the reconstruction: PSNR, MSE, MS-SSIM [36] and VMAF [23]. PSNR, MSE, and MS-SSIM are traditional image-similarity measures. MS-SSIM [36] provides a scale-independent quality estimate, and VMAF implements a learning-based approach that aligns well with human perception [2]. VMAF was designed to estimate the quality of distorted videos, but it can also be applied to images, interpreting them as single-frame videos.\n\nFollowing the methodology of [12], we measure $\\Delta_{score}$ the difference in reconstruction quality between the clear image and corresponding adversarial example:\n\n$\\Delta_{score} = FR(x, C(x)) \u2013 FR(x', C(x'))$,\n\nwhere x is an original image, x' is a corresponding adversarial example, FR(x, y) is one of the aforementioned IQA models, and C(x) is an evaluated NIC (entire encoding-decoding procedure). This score captures the drop in the fidelity of reconstructed images caused by the adversarial attack. Higher values of $\\Delta_{score}$ indicate lower robustness of the NIC (if higher values of considered FR model correspond to better visual quality \u2014 i.e., for PSNR, MS-SSIM, and VMAF).\n\nTo measure transferability of adversarial attacks to other codecs, we measure a modified version of $\\Delta_{score}$:\n\n$\\Delta_{score} = FR(C(x_i), C(x'_i)) \u2013 FR(C'(x_i), C'(x'_i))$,\n\nwhere C(x) is the decoded image by the target codec used to construct an adversarial image, and C'(x) is the attacked codec to which we test the attack's transferability. This metric offers a more precise evaluation for different pairs of models when comparing NIC with different robustness."}, {"title": "4.6. Implementation details", "content": "We used the source code of JPEG AI without additional pretraining. For adversarial attacks, to simplify backward propagation and avoid overfitting for extra tools, a modified interface for attacking the core model was used. As a result, constructed attacks depended less on the choice of the codec configuration. To evaluate the effectiveness of the attack and the codec's robustness, the attack's impact was assessed not only on the core model's result but also on the entire codec with additional tools included from the codec's base configuration. For this, we made only minor changes to the interface for working with the codec, according to our requirements, and used default settings provided by the authors. By implementing these changes that do not affect the codec's operation, we created a unified interface for three versions of JPEG AI: 4.1, 5.1 and 6.1, which allowed attacks to be trained and evaluated similarly to other neural network codecs presented in this paper.\n\nWe applied each adversarial attack to each encoder four times with varied attack parameters (learning rate, number of iterations, and perturbation bound). We then averaged the metrics for all launches.\n\nCalculations were made with Slurm Workload Manager, 120 \u00d7 NVIDIA Tesla A100 80 Gb GPU, and Intel Xeon Processor 354 (Ice Lake) 32-Core Processor @ 2.60 GHz."}, {"title": "5. Results", "content": ""}, {"title": "5.1. Comparison of adversarial attacks' efficiency", "content": "In this section, we compare different loss functions used for adversarial attacks in terms of delta-quality metrics. Fig. 2 shows how they influence the quality of uncompressed images. Mostly, results across different \u2206 metrics are similar. The original loss proposed in the FTDA attack yields disturbance for most metrics, especially in terms of AL\u221e. Reconstruction losses have similar results \u2013 AMS-SSIM, \u2206PSNR, and ASSIM generally decrease after attacks with these losses, meaning that the quality of decompressed images with attacks is better than without attacks. These losses directly maximize the difference between decoded and original images. More complex losses showed less efficiency."}, {"title": "5.2. Comparison of codecs' robustness", "content": "Fig. 3 shows the adversarial robustness of tested NIC models. First of all, different codecs are vulnerable to different kinds of adversarial attacks. For example, Cheng2020 is subject to I-FGSM and FTDA attacks, which are ineffective against JPEG AI. JPEG AI showed relatively high robustness compared to other NIC models. High-operation point versions of JPEG AI are less robust than base-operation point. Second, the robustness of JPEG AI improved with a newer version (6.1 compared to 5.1). Also, JPEG AI is subject to attacks that operate only the luma component while creating the adversarial image (Y in loss function). Finally, the diffusion-based CDC method showed the lowest robustness to various attacks. This model may be less robust by design; adversarial noise causes significant changes in latent representation, yielding noticeable quality degradation."}, {"title": "5.3. BPP increase", "content": "We noticed that adversarial attacks increase the size of compressed images, even if an attack was not targeted to increase BPP. Fig. 4 shows this difference. This effect is explained by the more noise structure of adversarial images, which yields a different rade-distortion trade-off compared to benign images. Thus, adversarial images harm the re-"}, {"title": "5.4. Codecs artifacts after attacks", "content": "To analyze artifacts introduced by adversarial attacks, we calculate two methods proposed in [33] to detect them. The first one, named \"Color metric\", analyzes CIEDE2000 color [32] difference metric followed by average pooling. It allows us to analyze color distortions introduced by the neural codec. The second method, named \"Texture metric\", computes MS-SSIM values for local regions and compares them with ones compressed by non-neural-network-based JPEG codec. This approach detects distortions of high-frequency textures in compressed images.\n\nFirstly, we analyze correlations between these two methods for artifact assessment and other metrics introduced earlier."}, {"title": "5.5. Attacks' transferability", "content": "This section presents the transferability of adversarial examples generated for one NIC to others. We have conducted two experiments: using all NICs presented in our paper and only JPEG AI versions. We compare different bitrates of chosen codecs, so they are included several times. The JPEG AI transferability experiment results are in Fig. 7. We ran experiments on all losses for two presets and averaged the results. The results show high transferability between different bitrates of specific versions of JPEG AI, especially from lower bitrates to higher ones. Transferability also exists for other versions of JPEG AI."}, {"title": "5.6. Defences efficiency", "content": "In this subsection, we evaluate defenses on JPEG AI in the way described in section 4.3. We measure the \u2206PSNR change between different defense strategies for FTDA default loss for FTDA, I-FGSM, PGD, and SSAH. The results can be seen in Fig. 8. Simple reversible defenses can negate adversarial attacks on NIC models. The results demonstrate that Flip, Random Ensemble, and Random Roll are somewhat effective in reducing their effect."}, {"title": "5.7. Codecs comparison without attacks", "content": "Fig. 9 shows BSQ-rate [45] (bitrate savings for the same image quality) relative to mbt2018 codec. JPEG AI shows superior compression performance: over 50% less bitrate with the same quality by VMAF. Surprisingly, Cheng2020 is also in top positions, despite this method is much older than the competitors."}, {"title": "6. Conclusion", "content": "This study offers an in-depth evaluation of JPEG AI and other neural image codecs (NICs), focusing on compression efficiency, adversarial robustness, and transferability of attack effects. Our findings highlight JPEG AI's substantial improvements in bitrate efficiency, achieving over 50% reduction at equivalent VMAF scores compared to mbt2018 codec. This bitrate saving reflects significant progress in compression technologies, positioning JPEG AI as a leading codec in maintaining high image quality with lower bandwidth requirements. While Cheng2020 still shows competitive results, JPEG AI surpasses it in multiple quality metrics.\n\nOur study confirms that robustness analysis is a crucial evaluation criterion for NICs models. Although JPEG AI shows high robustness, it is still vulnerable to attacks, and further investigation into specialized adversarial defenses is vital. On the other hand, assessing attack success in NICS remains challenging compared to other computer vision tasks, such as image classifiers. Artifacts that emerge during decoding require specialized models for detecting and assessing, highlighting a relevant area for future research. Additionally, we demonstrate that attacks can transfer to new and potentially proprietary versions of the JPEG AI, and robustness testing becomes essential before widespread distribution and adoption."}]}