{"title": "On Adversarial Robustness and Out-of-Distribution Robustness of\nLarge Language Models", "authors": ["April Yang", "Jordan Tab", "Parth Shah", "Paul Kotchavong"], "abstract": "The increasing reliance on large language models (LLMs) for diverse applications necessitates\na thorough understanding of their robustness to\nadversarial perturbations and out-of-distribution\n(OOD) inputs. In this study, we investigate the\ncorrelation between adversarial robustness and\nOOD robustness in LLMs, addressing a crit-\nical gap in robustness evaluation. By apply-\ning methods originally designed to improve one\nrobustness type across both contexts, we ana-\nlyze their performance on adversarial and out-of-\ndistribution benchmark datasets. The input of the\nmodel consists of text samples, with the output\nprediction evaluated in terms of accuracy, preci-\nsion, recall, and F1 scores in various natural lan-\nguage inference tasks.\nOur findings highlight nuanced interactions be-\ntween adversarial robustness and OOD robust-\nness, with results indicating limited transferabil-\nity between the two robustness types. Through\ntargeted ablations, we evaluate how these cor-\nrelations evolve with different model sizes and\narchitectures, uncovering model-specific trends:\nsmaller models like LLaMA2-7b exhibit neutral\ncorrelations, larger models like LLaMA2-13b\nshow negative correlations, and Mixtral demon-\nstrates positive correlations, potentially due to\ndomain-specific alignment. These results under-\nscore the importance of hybrid robustness frame-\nworks that integrate adversarial and OOD strate-\ngies tailored to specific models and domains.\nFurther research is needed to evaluate these in-\nteractions across larger models and varied archi-\ntectures, offering a pathway to more reliable and\ngeneralizable LLMs.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) are increasingly used for\ndiverse natural language processing tasks but face signifi-\ncant challenges in maintaining reliability when exposed to\nadversarial perturbations and out-of-distribution (OOD) in-\nputs. While methods for improving adversarial and OOD\nrobustness have been developed independently, the rela-\ntionship between these two robustness types remains un-\nderexplored. To address this gap, our study systematically\nevaluates the correlation between adversarial and OOD ro-\nbustness across foundation models of varying architectures\nand parameter sizes.\nWe evaluate three models-Llama2-7b, Llama2-13b, and\nMixtral-8x7b-on four benchmark datasets. For adversar-\nial robustness, we utilize PromptBench and Adversarial-\nGLUE++, and for OOD robustness, we employ Flipkart\nand DDXPlus. These datasets span natural language infer-\nence tasks such as sentiment analysis and question answer-\ning. Key metrics, including accuracy, precision, recall, and\nF1 scores, are calculated to assess performance.\nBy implementing and testing two robustness improvement\nstrategies-Analytic Hierarchy Process (AHP), designed\nfor adversarial robustness, and In-Context Rewriting (ICR),\nd\u00e9signed for OOD robustness-we analyze their effec-\ntiveness across both contexts. This allows us to explore\nwhether improvements in one domain generalize to the\nother and to speculate on how architectural differences and\nparameter sizes influence these correlations."}, {"title": "2. Literature Review", "content": null}, {"title": "2.1. Adversarial Robustness", "content": "Adversarial robustness refers to a model's resilience\nagainst intentionally crafted input perturbations designed\nto mislead the model. While large-scale pre-trained LLMs\nhave achieved impressive performance across natural lan-\nguage understanding (NLU) tasks, research has shown that\nthese models can be vulnerable to carefully crafted adver-"}, {"title": "2.2. Out-of-Distribution Robustness", "content": "OOD robustness measures a model's performance on data\ndistributions that differ from its training distribution. This\naspect of robustness is particularly crucial for real-world\napplications where models encounter novel or unexpected\ninputs. Krueger et al. (2021) proposed Risk Extrapola-\ntion (REx), a method that aims to learn stable correlations\nacross training environments to improve OOD generaliza-\ntion. Wang et al. (2023) evaluates the OOD robustness of\nseveral LLMs, including ChatGPT, using a zero-shot ap-\nproach. This involves directly applying pre-trained mod-\nels to the test data without any fine-tuning or context ex-\namples. The study leverages datasets such as Flipkart and\nDDXPlus to assess the models' ability to generalize to un-\nseen domains like product reviews and medical diagnosis.\nThe evaluation in (Wang et al., 2023) highlights the im-\nportance of OOD robustness in practical applications, es-"}, {"title": "2.3. Robustness Enhancement Methods", "content": "Recent research has proposed various approaches to im-\nprove model robustness. While a simplest approach might\nbe to finetune or retrain models on adversarial or OOD data,\nthis can be costly and resource intensive. Several promis-\ning methods have emerged, including the Analytic Hier-\narchy Process for enhancing adversarial robustness (Liu\net al., 2023), consistency alignment training for improv-\ning response reliability (Zhao et al., 2024), and In-Context\nRewriting (ICR) techniques for OOD black-box robustness\n(O'Brien et al., 2024).\nAnalytic Hierarchy Process (AHP) framework breaks down\nthe task of robust inference into manageable subtasks, pri-\noritizing them, and addressing them systematically. AHP\nrelies on feedback from another LLM, removing the need\nfor retraining or optimization, and has been shown to im-\nprove robustness against jailbreak attacks and adversarial\nattacks on downstream tasks (Liu et al., 2023). Evaluation\nreveals that larger LLMs used as the AHP engine consis-\ntently led to greater robustness in the inference LLM.\nIn-context Rewriting (ICR) uses an LLM to rewrite out-\nof-distribution (OOD) inputs to be more stylistically sim-\nilar to in-distribution (ID) data. This can improve perfor-\nmance, as it makes the OOD input more similar to the data\nthe LLM was trained on. For example, using ICR with\na Stable Beluga 2 LLM to augment inputs for BERT, T5,\nand Falcon models resulted in improved accuracy on var-\nious NLP tasks (O'Brien et al., 2024). However, analysis\nsuggests that while ICR can improve performance on OOD\ndata, there is room for improvements between the ID and\nOOD data distributions.\nConsistency Alignment Training (CAT) aims to improve\nthe consistency of LLM outputs when presented with\nsemantically equivalent instructions phrased differently.\nThis is a two-stage training process. In the first stage,\nthe model is fine-tuned on a dataset augmented with\nparaphrased instructions. The second stage, response\nconsistency alignment, involves training the model to\ndiscern subtle differences in responses to similar instruc-\ntions and align outputs with human expectations (Zhao\net al., 2024). Evaluation shows that consistency alignment\nimproves both consistency and overall performance on\ninstruction-following tasks.\nWhile existing methods effectively address adversarial and"}, {"title": "3. Experiment Setup and Workflow", "content": "Our study evaluates the robustness of three large lan-\nguage models-Llama2-7b, Llama2-13b, and Mixtral-\n8x7b-using four benchmark datasets: PromptBench and\nAdversarialGLUE++ for adversarial robustness, and Flip-\nkart and DDXPlus for out-of-distribution (OOD) robust-\nness. These benchmarks cover diverse natural language in-\nference tasks, such as sentiment analysis and question an-\nswering.\nThe experiment consists of two main phases:\n1. Baseline Evaluation:\n\u2022 Each model was evaluated on all four bench-\nmarks without any robustness enhancements to\nestablish baseline metrics.\n\u2022 For each sample in the benchmarks, the corre-\nsponding task prompt was combined with the\nsample text and provided as input to the model\nfor inference.\n\u2022 The model outputs were used as the predictions\nto calculate performance. Performance was mea-\nsured using accuracy, precision, recall, and F1\nscores.\n2. Robustness Improvement Evaluation:\n\u2022 Two robustness strategies were implemented:\nAnalytic Hierarchy Process (AHP), designed for\nadversarial robustness, and In-Context Rewriting\n(ICR), designed for OOD robustness.\n\u2022 Each model-strategy pairing was evaluated\nacross all benchmarks to assess cross-context ro-\nbustness.\nFor each model, the performance metrics from all model-\nstrategy-benchmark pairings served as the data points for\ngraphs combining adversarial and OOD benchmark results.\nUsing linear regression, we derived a line of best fit to\ncalculate correlation coefficients, providing insights into\nthe relationship between adversarial and OOD robustness.\nThis workflow was repeated for all models, enabling com-\nparisons based on architecture and parameter size. Figure\n1 diagram shows the overall workflow."}, {"title": "3.1. Model Selection", "content": "Our study focuses on three large language models that rep-\nresent the current state-of-the-art in natural language pro-\ncessing. These models were chosen for their diverse ar-\nchitectures and training approaches, allowing us to explore\nhow different design choices affect robustness.\nThe first model is Llama-2-7b, a 7 billion parameter pow-\nerful open-source language model developed by Meta AI,\nknown for its efficiency and performance across various\nNLP tasks (Touvron et al., 2023). The second model is\nLlama-2-13b, an extended version of the Llama-2 series\nwith 13 billion parameters, providing insights into the ef-\nfects of scaling model size. The third model is Mix-\ntral 8x7b, a sparse mixture-of-experts model developed by\nMistral AI, offering strong performance while maintaining\ncomputational efficiency (Jiang et al., 2023). The model\nhas 45 billion parameters and a more complex architec-\nture compared to the LLaMA-2 models. These models rep-\nresent different approaches to LLM design, with Llama-\n2 models using a traditional transformer architecture and\nMixtral employing a sparse mixture-of-experts framework.\nThis diversity enables us to investigate how architectural\nchoices and parameter sizes impact both adversarial and\nOOD robustness."}, {"title": "3.2. Benchmarking Datasets", "content": "To evaluate adversarial and OOD robustness, we selected\ncomprehensive benchmarks designed to challenge LLMs\nacross diverse natural language inference tasks.\nAdversarial Robustness: For adversarial robustness, we\nutilize two benchmarks:\n\u2022 PromptRobust (Zhu et al., 2024): Designed to eval-\nuate robustness to prompt variations, this benchmark\nsystematically tests how models handle paraphrasing,\nnoise, style changes, and varying instruction formats.\nPrompt perturbations include methods such as:\nSemantic Attacks: Altering sentence phrasing\nwhile preserving meaning."}, {"title": "4. Evaluation Metrics", "content": "We will evaluate the two types of robustness by calculating\naccuracy and F1-scores for each model on our benchmark-\ning datasets. Changes in these metrics across different ex-\nperiments will serve as the foundation for our robustness\nanalysis. To quantify the relationship between adversar-\nial and OOD robustness, we will employ linear regression.\nThese metrics will allow us to assess the strength and direc-\ntion of the relationship between different robustness mea-\nsures across our selected models and datasets."}, {"title": "4.1. Metrics and Equations", "content": null}, {"title": "4.1.1. ACCURACY", "content": "Accuracy = $\\frac{Ncorrect}{Ntotal}$ (1)\nVariables:\n\u2022 Ncorrect: Number of correctly classified examples in\nthe zero-shot setting.\n\u2022 Ntotal: Total number of examples evaluated."}, {"title": "4.1.2. F1-SCORE", "content": "F1 = 2.$\\frac{Precision \\cdot Recall}{Precision + Recall}$ (2)\nVariables:\n\u2022 Precision: Precision = $\\frac{TP}{TP+FP}$\nTP: True positives (correctly predicted positive\nexamples).\nFP: False positives (incorrectly predicted posi-\ntive examples).\n\u2022 Recall: Recall = $\\frac{TP}{TP+FN}$\nFN: False negatives (incorrectly predicted neg-\native examples)."}, {"title": "4.1.3. LINEAR REGRESSION", "content": "Y = \u03b2\u2080 + \u03b2\u2081X + \u2208 (3)\nVariables:\n\u2022 Y: Dependent variable (Adversarial Metric).\n\u2022 Bo: Intercept of the regression line.\n\u2022 B1: Slope of the regression line (coefficient for the\nindependent variable).\n\u2022 X: Independent variable (OOD Metric).\n\u2022 \u2208: Error term (residuals).\nEssentially, we aim to estimate a linear relation for data\npoints where X values are adversarial benchmark metrics\nand Y values are OOD benchmark metrics."}, {"title": "5. Baseline Evaluation", "content": null}, {"title": "5.1. Baseline Summary", "content": "To establish a performance baseline, we evaluated our three\nmodels-Llama2-7b, Llama2-13b, and Mixtral-8x7b-on\nfour benchmark datasets: PromptBench and AdvGLUE++\nfor adversarial robustness, and Flipkart and DDXPlus for\nout-of-distribution (OOD) robustness. Each model was\ntested without any robustness enhancements to measure\ntheir default performance.\nThe models were used to make predictions for each sample\nin all benchmarks, and these predictions were compared to\nthe ground truth labels to calculate accuracy, precision, re-\ncall, and F1 scores. The evaluation focused on natural lan-\nguage inference tasks, including sentiment analysis, medi-\ncal diagnosis (text classification), and question answering.\nThese baseline metrics serve as a point of comparison for\nassessing the impact of robustness improvement strategies\nimplemented later in the study."}, {"title": "6. Improvement Methods", "content": null}, {"title": "6.1. Analytic Hierarchical Process", "content": "The Analytic Hierarchy Process (AHP) framework ap-\nproaches robustness enhancement by conceptualizing de-\nfense as a cognitive process for handling complex user\nqueries. Drawing inspiration from hierarchical process-\ning theory, AHP decomposes intricate tasks into manage-\nable subtasks and systematically addresses them through a\nstructured decision-making process. The framework con-\nsists of two main components, shown in Figure 2 below:\nThe Safety and Validity Assessment component operates\nas an iterative process to ensure input quality and safety."}, {"title": "6.2. In-Context Rewriting", "content": "In-Context Rewriting (ICR) is a specific method within\nthe broader framework of LLM-powered Test-Time Aug-\nmentation (LLM-TTA), which uses large language mod-\nels to rewrite inputs at test time before passing them to\ndownstream tasks. This approach aims to improve robust-\nness, particularly for out-of-distribution (OOD) inputs."}, {"title": "7. Results", "content": null}, {"title": "7.1. Adversarial Benchmarks", "content": "Tables 1 and 2 show the results for experiments evaluated\non the adversarial benchmarks for all models and improve-\nment scenarios."}, {"title": "7.2. OOD Benchmarks", "content": "Tables 3 and 4 show the results for experiments evalu-\nated on the out-of-distribution benchmarks for all models\nand improvement strategies. The AHP columns, AHP and\nAHP2, represent the AHP strategy with different sets of\nprompts mentioned earlier. AHP corresponds to the experi-\nment conducted with the original prompts that are designed\nfor adversarial attacks, while AHP2 corresponds to the ex-\nperiment with their out-of-distribution adaptations."}, {"title": "7.3. Higher Level Observations and Shortcomings", "content": "From the results which we have discussed above, we ac-\nknowledged that there are variability which directly affects\ninference performance of these models. Both improvement\nmethods rely on prompts as the main mechanism to clean or\nrewrite inputs. The models are sensitive to various prompt-\ning strategies crafted. Since we attempt to enforce a spe-\ncific parsible format like JSON for ease of use, this led to\nprompt overloading which may affect the performance of\nthe model in understanding the main task given.\nWith In-Context Rewriting, the selection and range of ex-"}, {"title": "8. Analysis of Correlation", "content": "We analyzed the relationship between adversarial and\nOOD robustness by grouping the accuracy scores by\nbenchmark, and normalizing the scores for the 3-4\nstrategies we evaluated per benchmark using min-max\nnormalization. Once normalized, we plotted the adver-\nsarial and OOD benchmark pairings by model. Then, we\nperformed regression analysis to obtain a line of best fit.\nThe slope of this line served as our correlation coefficient.\nInterestingly, from LLaMA2:7B to LLaMA2:13B, we see"}, {"title": "9. Future Works", "content": "From our presentation and discussion of results, future\nwork is needed before a more definite conclusion is able to\nbe drawn. Due to resource constraints, we were not able to\nexplore much larger models like LLaMA2:70B or equiva-\nlently larger models which we commonly use like GPT-3 or\nGPT-4 (both with over 100B parameters). Exploring these\nlarger LLMs would allow for more comparison, to see if\nincrease in model size correlates to more effectiveness of\nimprovement methods. Thus, we can really determine if\nthe trend of positive correlation between Adversarial and\nOOD robustness remains or perhaps become increasingly\nmore positive, or vice versa.\nWhile we only evaluated on two benchmarks for each type\nof robustness, we acknowledge that more benchmarks for\neach type of robustness are necessary to ensure a compre-\nhensive and reliable assessment. A larger and more diverse\nset of benchmarks would help capture a broader range of\nadversarial and OOD scenarios, ensuring the generalization\nof our findings across domains. Additionally, incorporating"}, {"title": "10. Conclusion", "content": "Given the results above, we see no clear strategy that ef-\nfectively improves both adversarial and out-of-distribution\nrobustness in all benchmarks for all models. We were able\nto observe model or benchmark level improvements with\nstrategies, however, they did not stay consistent across all\nbenchmarks and models. This indicates that current models\nand improvement strategies need to be evaluated at a do-\nmain and robustness type basis. Further research is needed\nto develop strategies that effectively improve both adversar-\nial and out-of-distribution robustness across different mod-\nels and benchmarks.\nAs for the correlation between adversarial robustness and\nout-of-distribution robustness, our current findings demon-\nstrate model specific results: a neutral correlation for\nLLaMA2-7b, a negative correlation for LLaMA-13b, and\na positive correlation for Mixtral:8x7b. As stated previ-\nously, further research is necessary to verify and build on\nthese findings. For now, increasing model parameter count,\ndoes not necessarily lead to increased positive or nega-\ntive correlation between adversarial robustness and out-of-\ndistribution robustness of LLMs."}, {"title": "11. Division of Work", "content": "The report was written by Jordan Tab and Paul Kotchavong,\nwith contributions from April Yang. April Yang conducted\nthe baseline experiments on PromptRobust, as well as AHP\nexperiments on both adversarial benchmarks. Jordan Tab\nwas responsible for conducting the baseline evaluations on\nFlipkart, as well as the AHP and AHP2 evaluations on\nboth out-of-distribution benchmarks. Jordan Tab and Paul\nKotchavong also conducted the correlation analysis. Paul\nKotchavong was in charge of AdversarialGlue++ baseline\ncalculations, as well as the ICR experiments for Flipkart,\nPromptRobust, and AdversarialGlue++. Parth Shah was in\ncharge of baseline and ICR experiments for DDXPlus."}]}