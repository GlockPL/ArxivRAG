{"title": "Exploring Model Invariance with Discrete Search for Ultra-Low-Bit Quantization", "authors": ["Yuqiao Wen", "Yanshuai Cao", "Lili Mou"], "abstract": "Large language models have been increasing in size due to their success in a wide range of applications. This calls for a pressing need to reduce memory usage to make them more accessible. Post-training quantization is a popular technique which uses fewer bits (e.g., 4\u20138 bits) to represent the model without retraining it. However, it remains a challenging task to perform quantization in an ultra-low-bit setup (e.g., 2 bits). In this paper, we propose INVAREXPLORE, a unified framework that systematically explores different model invariance at the same time, allowing us to take advantage of the synergy between each type of invariance. Importantly, INVAREXPLORE features a discrete search algorithm that enables us to explore permutation invariance, which is under-studied as it cannot be optimized with gradient-based methods. Results show that INVAREXPLORE is compatible with existing state-of-the-art methods, achieving an add-on performance improvement over strong competing methods.", "sections": [{"title": "Introduction", "content": "Post-training quantization aims to reduce the number of bits used by a large neural model by lowering the precision of the weights after the model has been trained Han et al. (2016). It has been gaining growing interest, as pretrained large language models (LLMs) such as ChatGPT (OpenAI, 2022) and Llama (Dubey et al., 2024) are getting increasingly larger.\nWhile quantization reduces the model size, it also causes rounding errors and typically leads to perfor-mance degradation. This is especially challenging in the ultra-low-bit setting, for example, using only two or three bits to represent a model weight. A promising research direction for quantization is to find an invariant model of the pretrained LLM that reduces rounding errors after quantization and preserves performance.\nExisting work finds an invariant model by either manually designed heuristics or gradient-based methods. Lin et al. (2024b) reduce rounding errors by weight scaling and handcraft how much to scale based on ac-tivation values. Shao et al. (2024) propose block-wise error minimization to find a scale-invariant model through gradient updates. Liu et al. (2024) also use gradient-based methods and find orthogonal transfor-mations matrices by directly optimizing the cross-entropy loss. One caveat for computing the gradient is that the quantization function has zero gradient almost everywhere, so they approximate it with straight-through estimation (Bengio et al., 2013). In addition, the optimization process is often constrained, and ad hoc treatments are required to ensure that the gradient-updated transformation satisfies the desired form. For example, Liu et al. (2024) apply the Cayley SGD method to keep the weight matrix orthogonal.\nIn this paper, we propose a general framework, called INVAREXPLORE, which can explore different types of invariant transformations in neural networks to improve quantization performance. Specifically, we ob-serve that an invariant model can be obtained by applying a transformation and its inverse to neighboring linear blocks in the network, and the previous scaling method (Lin et al., 2024b) is a special case under our framework. INVAREXPLORE further allows us to explore permutation and rotation invariance, which are not typically studied in previous literature, especially since permutation is non-differentiable and cannot be"}, {"title": "Related Work", "content": "Integer quantization. In this paper, we focus on integer quantization, where model weights are stored as integers (Jacob et al., 2018; Wu et al., 2020). This is one of the most popular quantization schemes because it offers strong performance, high memory savings, as well as hardware efficiency with integer-only arith-metic during inference in certain scenarios (Jacob et al., 2018; Lin et al., 2024b). Earlier work in this regime has explored binary networks that can only represent \u00b11, or ternary networks which additionally include the 0 value (Courbariaux et al., 2015; Lin et al., 2016). However, they only work with small models for simple tasks. Researchers typically use more bits (e.g., 4\u20138 bits) in integer quantization to tackle real-world tasks Krishnamoorthi (2018); Dettmers et al. (2022); Frantar et al. (2023), but the ultra-low-bit setup remains challenging.\nLow-bit integers are usually unable to cover the range of real-valued weights. Therefore, a scale param-eter is introduced to map the low-bit integer range to the original weight range Zhou et al. (2016); Jacob et al. (2018). Also, it is important to keep the exact 0 value for weights, so a zero-point parameter is also introduced Jacob et al. (2018). Such offset and scaling are applied at the granularity of groups (contiguous weights in a matrix). Although the scale and zero-point parameters use additional memory, they drastically improve performance and are commonly used in modern integer quantization studies (Frantar et al., 2023; Lin et al., 2024b; Shao et al., 2024).\nOne direction of integer quantization is to quantize weights sequentially Frantar and Alistarh (2022); Fran-tar et al. (2023), inspired by the classic pruning methods (Hassibi et al., 1993). The idea is that, given already quantized weights, the rest weights are finetuned to compensate the quantization error before also being quantized. However, the weight updates are computed in a closed form based on second-order gradient information; this is done for each layer separately, which does not consider the dependencies among layers.\nAn alternative technique in integer quantization is to leverage model invariance, which adjusts the parame-ters for better quantization performance without impacting the (un-quantized) model's behavior. Previous work has explored various heuristics. For example, Lin et al. (2024b) determine the scaling coefficients based on the magnitude of the activations. Lin et al. (2024a) manually design a zigzag permutation pattern in an attempt to distribute outlier weights throughout the network. On the other hand, researchers also use gradient-based methods to learn scaling coefficients (Shao et al., 2024) and orthogonal transformations (Liu et al., 2024). However, this requires a straight-through gradient estimation and special treatment for the constraint, as mentioned in Section 1.\nDifferent from previous work, our paper proposes the INVAREXPLORE framework that explores permuta-tion, scaling, and rotation invariance for integer quantization. We further propose a discrete search algo-rithm that can optimize different types of invariance jointly."}, {"title": "Approach", "content": "We provide the background for quantization in Subsection 3.1. Our INVAREXPLORE framework and the activation-guided discrete search algorithm are presented in Subsection 3.2."}, {"title": "Quantization Background", "content": "We adopt the standard asymmetric integer group quantization (Jacob et al., 2018), where weight matrices are divided into groups of contiguous memory space. In particular, each group is quantized separately based on a scale parameter that specifies the step size between integer values. Asymmetric quantization allows a zero-point parameter to specify the (integer) offset for the zero value.\nLet $W_g \\in R^G$ be the $g$th group containing $G$ contiguous parameters of the weight matrix. Asymmetric integer quantization has the following form\n$quant(W_g) = round(W_g/s_g)+z_g$                                                        (1)\nwhere $quant$ is the quantization function, $s_g$ is the FP16 scale parameter, and $z_g$ is an integer parameter specifying the zero point.\nIntuitively, we would like to map the maximum and minimum (un-quantized) values with the largest and smallest integers. This leads to the closed-form solutions for the scale and zero-point parameters\n$s_g = \\frac{max(W_g) - min(W_g)}{q_{max} - q_{min}}$                         (2)\n$z_g = round(q_{min} - min(W_g)/s_g)$                                                               (3)\nwhere $q_{max}$ and $q_{min}$ are the largest and smallest numbers that can be represented by the integer type (depending on the number of bits).\nTo dequantize the integer representations, we can simply subtract the zero point and multiply it by the scale\n$dequant(quant(W_g)) = s_g \\cdot (quant(W_g) - z_g)$                                                        (4)\nThe key challenge for quantization, as we can see from Eqns. (2) and (3), is that the quantization parameters are very sensitive to outliers, because they heavily depend on the maximum and minimum weights of a"}, {"title": "Our Proposed INVAREXPLORE", "content": "In this paper, we propose INVAREXPLORE, a framework that explores different types of invariant trans-formations in neural networks to improve quantization performance. The key intuition is that invariant transformations do not alter a model's output if it is not quantized, but they may have different quantiza-tion performance because $round(\\cdot)$ is not invertible in the quant and dequant functions. Compared with previous work (Lin et al., 2024b; Xiao et al., 2023; Ashkboos et al., 2024), our INVAREXPLORE framework explores permutation and rotation invariance, which can be combined with the previously used scaling invariance.\nSpecifically, given a model $M$ and its trained parameters $\\theta_0$, our objective can be formulated as a con-strained optimization problem\n$\\mathop{\\mathrm{minimize}}\\limits_\\theta L(X, quant(\\theta))$                  (5)\n$subject\\ to\\ M(x; \\theta) = M(x; \\theta_0), \\forall x$                                                (6)\nwhere $L$ is a loss function such as the cross-entropy loss, and $X$ is a small batch of calibration data. Here, Eqn. (6) is the invariance constraint that holds for every possible input x, which includes all samples (not just the ones in the batch). This ensures our search space only contains parameters that yield the same model as the original one.\nUnfortunately, Eqn. (6) cannot be easily satisfied, because the equality has to hold for all inputs. Therefore, we restrict our search space to invariant transformations in the feed-forward blocks of the Transformer architecture:\n$z = W_{down}f(W_{up}x+b_{up}) + b_{down}$                      (7)\nwhere the Ws and bs are the weights and biases of the up and down projections, and $f$ is the activation function such as ReLU (Agarap, 2018).\nWe consider the following transformations.\nPermutation. If we change the order of the neurons in a layer, the network remains invariant. Let $P$ be a permutation matrix, i.e., every element is binary and every row/column has exactly one element that is 1. The transpose $P^T$ cancels out the permutation effect, as $P^TP = I$ must hold.\nMultiplying $P$ to $W_{up}$ and $b_{up}$, and $P^T$ to $W_{down}$ on the appropriate side, we have\n$(W_{down}P^T)f((PW_{up})x+ Pb_{up}) + b_{down}$                     (8)\n$=W_{down} (P^TP) f(W_{up}x+b_{up}) + b_{down} = z$                                            (9)\nwhich is the same as the original model (7). Here, Eqn. (13) follows because permutation can be equivalently applied before or after the activation function, i.e., $f (xP) = f(x)P$.\nThis gives us the permuted parameters\n$W_{up} = PW_{up},\\ \\ b_{up} = Pb_{up}$                             (10)\n$W_{down} = W_{down} P^T$                                    (11)\nNote that the transformed parameters can be efficiently obtained by indexing the original parameters, rather than matrix-matrix multiplications.\nPermutations per se are non-differentiable, but they are special cases of orthogonal transformations, which are differentiable and have been studied in previous work (Liu et al., 2024). However, they use gradient-based optimization and it cannot effectively explore permutations, because local minima are formed in a non-convex fashion due to the permutation symmetry.\nScaling. For a certain activation function (such as the ReLU and LeakyReLU), we can obtain an invariant model by scaling the features. Let $s$ be a scaling vector, i.e., $s_i$ is the scaling factor for the $i$th dimension. For"}, {"title": "Quantization Background", "content": "linear algebra notations, we may define a scaling matrix $S = diag(s)$, thus having\n$(W_{down}S^{-1})f((SW_{up})x + Sb_{up}) + b_{down}$                       (12)\n$=W_{down}(S^{-1}S)f(W_{up}x+b_{up}) + b_{down} = z$                                        (13)\nThe scaled parameters are given by\n$W_{up} = SW_{up},\\ \\ b_{up} = Sb_{up}$                                   (14)\n$W_{down} = W_{down}S^{-1}$                                 (15)\nIn implementation, we do not need to calculate $S$ and $S^{-1}$. Instead, we only need to scale a dimension by $s_i$ and scale it back by its reciprocal $1/s_i$.\nNotice that, the scaling invariance only holds for certain functions. Luckily, our experiments use OPT models (Zhang et al., 2022) with the ReLU function, which satisfies the property. For other activation functions, scaling $W_{up}$ and $W_{down}$ is not invariant, but can be employed as an approximation. Moreover, scaling invariance can be achieved for $W_{down}$ and its subsequent LayerNorm operation, which is beyond the scope of this paper.\nRotation. A rotation can also be represented by a matrix $R$, and its inverse is $R^T$. However, rotations are not invariant for non-linear activation functions, meaning that applying rotation in a similar way does not satisfy Eqn. (6):\n$(W_{down}R^T)f((RW_{up})x+Rb_{up}) + b_{down}$                       (16)\n$\\neq W_{down}(R^TR)f(W_{up}x+b_{up}) + b_{down} = z$                                        (17)\nNevertheless, if the rotation degree is small, then the values can be mostly recovered by an inverse rotation. We empirically verify this in our pilot study: the original 13B OPT model achieves a cross-entropy loss of 2.31528 on the WikiText-2 dataset; with our rotations, the cross-entropy is 2.31525, which demonstrates a difference of only 0.001%. Therefore, we can think of Eqn. (16) as an approximate invariant model, leading to our rotated parameters\n$W_{up} = RW_{up},\\ \\ b_{up} = Rb_{up}$                                     (18)\n$W_{down} = W_{down}R^T$                                  (19)\nTo construct the rotation, we partition a $d$-dimensional space (assuming $d$ is even) into pairs of two dimen-sions; this simplified treatment is similar to that for rotary embeddings (Su et al., 2024). Formally, R can be represented by a block diagonal matrix given rotation angles $\\Phi_1,\\dots,\\Phi_{d/2}$\nOur rotation transformation is different from Liu et al. (2024), who perform orthogonal transformation but call it rotation. Rigorously speaking, orthogonal transformations need not be rotations.\nOverall, the proposed INVAREXPLORE allows us to explore permutation, scaling, and rotation, which can be further combined as\n$W_{up} = PSRW_{up},\\ \\ b_{up} = PSRb_{up}$                             (21)\n$W_{down} = W_{down}R^TS^{-1}P^T$                                   (22)\nHere, the order of the transformations can be interchanged, as long as they are properly inverted in the down projection. In our implementation, we do not store $P$, $S$, and $R$ as matrices; rather, we store them as a permutation vector $\\pi$, a scale coefficient vector $s$, and a rotation degree vector $\\varphi$.\nActivation-guided discrete search. With the proposed transformation, our optimization problem (5) is reduced to the search of permutation, scaling, and rotation matrices. Despite this, it still cannot be opti-mized by gradient-based methods because the transformation (such as permutation) may impose binary"}, {"title": "Quantization Background", "content": "constraints. Moreover, the quant function produces zero gradient almost everywhere, making the opti-mization difficult in general.\nTo this end, we propose to optimize the above-mentioned transformations through discrete search, using the cross-entropy loss and an activation-matching loss as our objective. Let H and Ho be the activations of the quantized model and the original model, respectively. Our loss is\n$L(X, quant(\\theta)) = CE(X, quant(\\theta)) + \\alpha MSE(H, H_0)$                                                         (23)\nwhere CE refers to the standard cross-entropy loss; MSE computes the mean squared error, guiding the search process based on the activation values of hidden layers; and $\\alpha$ is a hyperparameter balancing the two terms.\nTo optimize the loss, we adopt a hill-climbing algorithm. At each step, we sample a transformation that combines permutation, rotation, and scaling. For permutation, we simply reshuffle the neurons in a layer; for scaling and rotation, we adopt random walk and sample the scale and rotation degree based on a Gaussian centered at the current values. Then, we quantize the model after applying the transformation. If the overall loss is lower, we accept the sampled transformation; otherwise, we reject it. The process is detailed in Algorithm 1.\nIn practice, we only permute, scale, and rotate a subset of the weights in a layer for each proposed update, and the size of the subset acts as a step size that controls how much we move in the parameter space. A larger step size will result in a lower acceptance rate, while a smaller one will lead to less change in the parameters. We find changing 10% of the neurons within a layer at a time generally works well, and we adopt this for all model sizes."}, {"title": "Experiments", "content": "We provide the settings of our experiments in Subsection 4.1, and present our results in Subsection 4.2."}, {"title": "Settings", "content": "Evaluation datasets. We evaluate our INVAREXPLORE on both language modeling and a selection of popu-lar reasoning tasks. For language modeling, we compare the perplexity scores on the widely used WikiText-2 dataset (Merity et al., 2022) and the Colossal Clean Crawled Corpus (C4, Dodge et al., 2021). For reasoning, we adopt the popular few-shot evaluation harness.2 Specifically, we follow Shao et al. (2024) and test on the following tasks\n\u2022 ARC (Clark et al., 2018): a benchmark consisted of multiple choice questions on grade school science. The authors partition their data into an easy subset and a challenging subset, denoted by ARC-E and ARC-C, respectively.\n\u2022 BoolQ (Clark et al., 2019): a reading comprehension dataset focusing on yes/no questions.\n\u2022 HellaSwag (Zellers et al., 2019): a natural language inference benchmark for common sense reasoning.\n\u2022 PIQA (Bisk et al., 2020): a common sense reasoning benchmark focusing on interactions with the phys-ical world.\n\u2022 WinoGrande (Sakaguchi et al., 2020): a pronoun resolution dataset, where the task is to choose the correct entity that a pronoun refers to given two options.\nExperimental details. We adopt the Open Pretrained Transformer (OPT) family of LLMs, ranging from 1.3B to 13B parameters (Zhang et al., 2022), which have been used in many previous studies on quantiza-tion (Frantar et al., 2023; Lin et al., 2024b; Shao et al., 2024). Our LLM is prompted by five-shot in-context learning samples, similar to the setting in Li et al. (2024). In-context samples are needed, as we observe low performance in some cases with zero-shot prompting due to the ultra-low-bit nature of our study.\nWe adopt the 2-bit setting with a group size of 128 for our main experiment, which is typically included in previous studies but not as their main focus (Lin et al., 2024b; Shao et al., 2024). Our analysis also experimented with the 3-bit setting.\nIn our algorithm, we need a small calibration dataset to optimize the objective in Eqn. (5). In particular, we use 32 sequences from the Pile corpus (Gao et al., 2020) in our main experiments, each sequence containing 512 tokens. Our calibration set is similar to Lin et al. (2024b) but smaller than Frantar et al. (2023) and Shao et al. (2024). Notably, a sequence with 512 tokens is much shorter than the context window of modern LLMs (e.g., OPT has a context of 2048 tokens), which suggests that our calibration with the short sequences is memory- and computation-efficient due to the quadratic scaling of attention in Transformers. In our analysis, we will show that our approach can also work with only one sequence.\nIn our algorithm, the balancing hyperparameter $\\alpha$ in Eqn. (23) is chosen such that the cross-entropy loss is ten times more important than the activation mean squared error at the beginning of training. For scaling and rotation, we need the random-walk standard deviations $\\sigma_s$ and $\\sigma_r$ to sample new candidate values (Algorithm 1); they are set to 1e-2 and 1e-5, respectively. The rotation standard deviation $\\sigma_r$ is much smaller because we observe high variance if the rotation degrees are large. The above hyperparameters are ob-tained via a grid search during our pilot study based on the loss on the calibration set. We use the same hyperparameters across all settings and datasets."}, {"title": "Results and Analyses", "content": "Main results. Our INVAREXPLORE is a general approach and can be applied to existing quantization meth-ods. We consider popular and state-of-the-art methods: GPTQ quantizes weights sequentially and adjust the remaining weights to compensate for the quantization error (Frantar et al., 2023), AWQ explores the scaling invariance by heuristics (Lin et al., 2024b), and OmniQuant learns the scaling coefficients by gradi-ent updates. In addition, both AWQ and OmniQuant use weight clipping to alleviate outlier weights.\nIn our experiments, we replicate GPTQ and AWQ by running their provided codebases, as they do not release checkpoints for the 2-bit setting that we focus on; for OmniQuant, we directly use the publicly available checkpoint. In general, we achieve similar trends as previous work. For example, our replication of OmniQuant matches closely with Shao et al. (2024). This establishes a solid foundation for the rest experiments.\nOur main results are presented in Table 1, where we show performance on both language modeling and downstream reasoning tasks. For reasoning, we report the average accuracy across six reasoning tasks; the detailed performance for each task can be found in Appendix A.\nWe first examine the standard, un-quantized OPT model with the FP16 representation. As seen in Table 1, larger models consistently achieve lower perplexity scores and higher accuracies, which is expected.\nNext, we experiment with the round-to-nearest (RTN) baseline that applies the standard integer quantiza-tion and rounds the model weights with the nearest integer representation. Our replication results match exactly with those in Shao et al. (2024). As we see, the simple RTN approach performs poorly for low-bit quantization, as it increases the perplexity by orders of magnitude.\nFor modern quantization methods GPTQ, AWQ, and OmniQuant, they significantly outperform the RTN baseline, with OmniQuant achieving the highest performance, followed by the popular AWQ approach, which is in turn followed by the earlier GTPQ approach.\nWe then apply the proposed INVAREXPLORE and see that our approach achieves consistent improvement over the state-of-the-art quantization methods. In particular, we achieve substantial improvement over GPTQ and AWQ, for instance, reducing the language modeling perplexity of GPTQ by 30\u201350% and in-creasing the reasoning accuracy by 3 points in the 13B setting. Our improvement over OmniQuant is more modest, as OmniQuant has already achieved high performance. Nevertheless, our improvement is gener-ally consistent in language modeling and downstream reasoning tasks across model sizes (ranging from 1.3B to 13B). This shows the effectiveness of INVAREXPLORE in both language modeling and downstream tasks, highlighting its generality and practical values."}, {"title": "Settings", "content": "Effect of activation matching. Our search objective is shown in Eqn. (23), which combines the task-oriented cross-entropy loss and an activation-matching loss. Although the activation matching can be applied to all layers, we find doing so for the 40-layer 13B OPT model requires a large memory, which does not fit in our GPU. Therefore, we restrict the matching for 10 layers in the main experiment and analyze the effect of matched layers in Table 4.\nWe see that matching more layers generally leads to higher performance, as the activation-matching loss provides guidance throughout the deep neural network. The effect is analogous to intermediate-layer matching in knowledge distillation (Sun et al., 2019a; Jiao et al., 2020).\nNotice that activation matching comes with an overhead of memory during the search process because we need to store the activation values for the calibration samples. When activation matching is disabled (w/0 layers), our INVAREXPLORE still consistently outperforms AWQ across all tasks without any memory over-head. This suggests that an end user\u2014who has enough GPU memory to perform inference with a quantized model-would also have enough memory to apply our method to improve quantization performance.\nNumber of calibration sequences. We also study how the size of the calibration dataset affects our ap-proach. Specifically, we evaluate INVAREXPLORE using 1, 8, 16, 24, and 32 sequences for calibration, with each sequence containing 512 tokens. We present the optimization curves for the first 10K steps, which is generally sufficient for INVAREXPLORE to make significant progress.\nAs shown in Figures 1a and 1b, both the calibration loss and the test perplexity consistently decrease as we increase the search step. As expected, using fewer calibration sequences generally makes the calibration loss decrease faster, but that also leads to slower improvement on the test set.\nWe also show the acceptance rate of INVAREXPLORE in Figure 1c. Recall that our optimization performs a random walk-style hill-climbing search, where a new transformation is accepted if it leads to a lower loss. We see the acceptance rate is around 80% at the beginning, showing that our search is highly efficient despite using random walk. With more steps, the acceptance rate steadily decreases and eventually flattens out, as the search algorithm empirically converges."}, {"title": "Conclusion", "content": "In this paper, we propose the INVAREXPLORE framework to explore different types of model invariance, including permutation, scaling, and rotation. We also propose a random walk-style hill-climbing search algorithm that works for non-differentiable transformations to improve quantization performance. Re-sults show that INVAREXPLORE achieves an add-on performance improvement over existing state-of-the-art quantization methods, highlighting the effectiveness and generality of our proposed method.\nLimitation and future work. It is noticed in Section 3 that our rotation does not provide exact invariance with non-linear activation functions, although the training loss only differs by 0.001% in our scenario. We observe future opportunities to explore exact rotational invariance in other model architectures, such as low-rank adaptation (LoRA; Hu et al., 2022)."}, {"title": "Impact Statement", "content": "Our work addresses the ultra-low-bit setup for language model quantization. We believe it will have a positive societal impact, as it makes language models more accessible to ordinary users."}]}