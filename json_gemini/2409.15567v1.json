{"title": "Asking an AI for salary negotiation advice is a matter of concern: Controlled experimental perturbation of ChatGPT for protected and non-protected group discrimination on a contextual task with no clear ground truth answers", "authors": ["R. Stuart Geiger", "Flynn O'Sullivan", "Elsie Wang", "Jonathan Lo"], "abstract": "We conducted controlled experimental bias audits for four versions of ChatGPT, which we asked to recommend an opening offer in salary negotiations for a new hire. We submitted 98,800 prompts to each version, systematically varying the employee's gender, university, and major, and tested prompts in voice of each side of the negotiation: the employee versus their employer. Empirically, we find many reasons why ChatGPT as a multi-model platform is not robust and consistent enough to be trusted for such a task. We observed statistically significant salary offers when varying gender for all four models, although with smaller gaps than for other attributes tested. The most substantial gaps were different model versions and between the employee- vs employer-voiced prompts. We also observed substantial gaps when varying university and major, but many of the biases were not consistent across model versions. We also tested for fictional and fraudulent universities and found wildly inconsistent results across different cases and model versions.\nWe also make broader contributions to the AI/ML fairness and trustworthyness literature. Our salary negotiation advice scenario and our experimental design differ from mainstream AI/ML auditing efforts in key ways. Bias audits typically test discrimination for protected classes like gender, which we contrast with testing non-protected classes of university and major. Asking for negotiation advice includes how aggressive one ought to be in a negotiation relative to known empirical salary distributions and scales, which is a deeply contextual and personalized task that has no objective ground truth to validate. These results raise concerns for not only for the specific model versions we tested, but also around the consistency and robustness of the ChatGPT web platform as a multi-model platform in continuous development. Our epistemology does not permit us to definitively certify these models as either generally biased or unbiased on the attributes we test, but our study raises matters of concern for stakeholders to further investigate.", "sections": [{"title": "1 Introduction", "content": "In recent years, we have seen a staggering rise in pre-trained, general-purpose Machine\nLearning and Artificial Intelligence (ML/AI) models, specifically Large Language\nModels (LLMs) or Foundation Models (FMs). These models can generate\nplausibly-sounding answers to a wide range of domain-specific tasks, without specific\ntraining for each task, unlike in prior generations of AI/ML. This general-purposeness is\none reason why LLMs are so popular, but it also raises serious social and ethical\nconcerns, especially for those systematically evaluating or auditing these models for\ncontext-specific harms like discrimination and social biases [1]. In this paper, we report\nresults of an audit study of a specific simulated context of use: discrimination-related\nharms when ChatGPT is asked for salary negotiation advice for candidates in the US\ntech job market.\nLLMs are generative models trained to predict the next word, trained on terabytes\nof text indiscriminately scraped from the Internet. LLMs are excellent at generating a\nconfidently-phrased response to any question, often mimicking the form of the language\nan expert would use in answering that question, but routinely 'hallucinate' or give\nincorrect or inappropriate answers [2-4]. LLMs can be further trained and fine-tuned to\nrefuse to answer impossible or inappropriate questions [5], but users often complain\nwhen a service marketed as a superhuman genius oracle refuses to answer. Complaints\nabout refusals led OpenAI CEO Sam Altman to focus on reducing the refusal rate in a\nseries of February 2024 posts, claiming that new versions should be \u201cmuch less lazy"}, {"title": "1.1 Motivation and background", "content": "In recent years, we have seen a staggering rise in pre-trained, general-purpose Machine\nLearning and Artificial Intelligence (ML/AI) models, specifically Large Language\nModels (LLMs) or Foundation Models (FMs). These models can generate\nplausibly-sounding answers to a wide range of domain-specific tasks, without specific\ntraining for each task, unlike in prior generations of AI/ML. This general-purposeness is\none reason why LLMs are so popular, but it also raises serious social and ethical\nconcerns, especially for those systematically evaluating or auditing these models for\ncontext-specific harms like discrimination and social biases [1]. In this paper, we report\nresults of an audit study of a specific simulated context of use: discrimination-related\nharms when ChatGPT is asked for salary negotiation advice for candidates in the US\ntech job market.\nLLMs are generative models trained to predict the next word, trained on terabytes\nof text indiscriminately scraped from the Internet. LLMs are excellent at generating a\nconfidently-phrased response to any question, often mimicking the form of the language\nan expert would use in answering that question, but routinely 'hallucinate' or give\nincorrect or inappropriate answers [2-4]. LLMs can be further trained and fine-tuned to\nrefuse to answer impossible or inappropriate questions [5], but users often complain\nwhen a service marketed as a superhuman genius oracle refuses to answer. Complaints\nabout refusals led OpenAI CEO Sam Altman to focus on reducing the refusal rate in a\nseries of February 2024 posts, claiming that new versions should be \u201cmuch less lazy"}, {"title": "1.2 Research questions: bias as a trustworthyness and\nrobustness concern", "content": "While we focus on bias, our broader concern is about the trustworthyness of relying on\nChatGPT for salary negotiation advice. Trust and trustworthyness, as Reinhardt argues\nin an extensive critical review of work in AI/ML trustworthyness [20], is often an\noverloaded catch-all term for any good or desirable quality, which often includes fairness,\ndiscrimination, and social biases. However, qualities like robustness, reliability,\nconsistency, predictability, and transparency are more commonly described as\nfoundational components to many trustworthy AI/ML efforts, and these qualities\nintersect with fairness and discrimination concerns.\nIn one sense, our study is about robustness, which is focused on how small variations\nin input data can change outputs. Robustness is typically studied and discussed as a\nsafety issue that may have large consequences such as if a self-driving car's camera is\nfoggy [21] or if an attacker uses adversarial methods to imperceptibly alter an input to\nget the output they desire [22]. Robustness is less often used to address more\nnormatively-laden socio-political concerns like discrimination (but see [23, 24]). We are\nalso concerned with the robustness of ChatGPT as a multi-model web platform that can\nswitch users from one model version to another, both within a single session if they\nexceed their quotas for higher-tier models, and over time as OpenAI releases new model\nversions. We separate these issues into one preliminary and five main research questions:\n\u2022 RQO: Do the four versions of ChatGPT give a valid, well-formed answer to our\nprompts, which asks to recommend a specific starting salary offer for a given new\nhire in the voice of either the employee or the employer?\n\u2022 RQ1: When four versions of ChatGPT are asked to recommend a starting salary\noffer using our prompts, is there a statistically-significant difference in the salary\noffer from the different model versions?\n\u2022 RQ2: is there a statistically-significant difference in the salary offer when the\nprompt is asked in the voice of the employee versus the employer?\n\u2022 RQ3:\nis there a statistically-significant difference in the salary offer when the\ngendered pronouns of a new hire are varied or omitted?\n\u2022 RQ4:\nwhat is the effect of varying the new hire's major on the recommended\nsalary offer? How much on average does each university vary the offer?\n\u2022 RQ5:\nwhat is the effect of varying the new hire's university on the\nrecommended salary offer?"}, {"title": "2 Motivation and Literature Review", "content": "Our motivation began in part because several of us authors were recent college\ngraduates seeking jobs and job market advice. Searching for professional career advice\non the Web or social media returns a cacophony of advice, anecdotes, and data, much of\nit contradictory, out-of-date, untrustworthy, or from a different context than the job\nseeker is in. From job seeker's side, it is difficult to know what one is worth on the\nmarket, but even more difficult to know how aggressive one can be in negotiating an\noffer. Especially for a new college graduate, it can be tempting to turn to ChatGPT\nwhen it can give an exact answer that appears to take into account all the contextual\ndetails about the employee, employer, and position but should one trust\nChatGPT when it returns such a number?\nIn this current 'AI summer,' LLM-based AI platforms are growing significantly, with\na 2024 Pew survey of US adults finding 23% of all adults and 43% of 18-29 year olds\nhave used ChatGPT, with 20% of employed adults saying they use it for tasks at\nwork [25]. Academic studies and journalistic accounts show that people across various\nformal and informal settings are turning to AI platforms like ChatGPT for all kinds of\nadvice and information to inform decisions, asking it their questions instead of asking\nsearch engines, social media, reference works, colleagues, friends, family, or other\ninformation sources [26-28].\nThere has been much public, governmental, and academic concern about the use of\nAI by those in formal organizations (e.g. in finance [29], law [30], medicine [31]) who are\nmaking legally-protected and/or high-risk decisions (e.g. loans, bail, diagnoses). In\nevery economic sector and academic discipline, one can easily find social posts, news\narticles, reports, and research about the potential of AI as decision-makers or\ndecision-informers in specific sectors. There has been less widespread concern about\nordinary people using these AI platforms for more informal decisions, such as the kinds\nof value-laden questions ChatGPT invites users to ask and answers with confident prose.\nWe also chose our task of salary negotiation advice for a recent college graduate in\npart because of a surge of mainstream and social media coverage and commentary in\nearly 2024 about youth workers using platforms like ChatGPT for personal career\nadvice. This became a small news cycle after an industry consulting group published a\nreport claiming 47% of Gen Z workers surveyed believed that ChatGPT gave them\nbetter career advice then their managers [32]. An author of that study later told a\nreporter that youth are \"craving for guidance that they're not finding within the\ntraditional structures of their workplaces\" [33]. Around the same time, viral posts and\nvideos spread on social media from influencers who claimed ChatGPT got them a job or\na raise. A wave of mainstream media reporting followed, with headlines from the New\nYork Post (\"ChatGPT negotiated my salary': How Gen Z uses AI to boost their\ncareers\" [33]), Forbes (\"5 ChatGPT Prompts To Land A Higher Paying Job In\n2024\" [34]), and news.com.au (\"Gen Z are using AI to help negotiate their salary\" [35]).\nAn employer-focused blogger caught the trend and wrote advice for managers whose\nemployees were negotiating with ChatGPT (\"Employee compensation are you\nprepared to negotiate with AI?"}, {"title": "2.1 People and HR systems are increasingly turning to AI for\ncareer advice", "content": "Our motivation began in part because several of us authors were recent college\ngraduates seeking jobs and job market advice. Searching for professional career advice\non the Web or social media returns a cacophony of advice, anecdotes, and data, much of\nit contradictory, out-of-date, untrustworthy, or from a different context than the job\nseeker is in. From job seeker's side, it is difficult to know what one is worth on the\nmarket, but even more difficult to know how aggressive one can be in negotiating an\noffer. Especially for a new college graduate, it can be tempting to turn to ChatGPT\nwhen it can give an exact answer that appears to take into account all the contextual\ndetails about the employee, employer, and position but should one trust\nChatGPT when it returns such a number?\nIn this current 'AI summer,' LLM-based AI platforms are growing significantly, with\na 2024 Pew survey of US adults finding 23% of all adults and 43% of 18-29 year olds\nhave used ChatGPT, with 20% of employed adults saying they use it for tasks at\nwork [25]. Academic studies and journalistic accounts show that people across various\nformal and informal settings are turning to AI platforms like ChatGPT for all kinds of\nadvice and information to inform decisions, asking it their questions instead of asking\nsearch engines, social media, reference works, colleagues, friends, family, or other\ninformation sources [26-28].\nThere has been much public, governmental, and academic concern about the use of\nAI by those in formal organizations (e.g. in finance [29], law [30], medicine [31]) who are\nmaking legally-protected and/or high-risk decisions (e.g. loans, bail, diagnoses). In\nevery economic sector and academic discipline, one can easily find social posts, news\narticles, reports, and research about the potential of AI as decision-makers or\ndecision-informers in specific sectors. There has been less widespread concern about\nordinary people using these AI platforms for more informal decisions, such as the kinds\nof value-laden questions ChatGPT invites users to ask and answers with confident prose.\nWe also chose our task of salary negotiation advice for a recent college graduate in\npart because of a surge of mainstream and social media coverage and commentary in\nearly 2024 about youth workers using platforms like ChatGPT for personal career\nadvice. This became a small news cycle after an industry consulting group published a\nreport claiming 47% of Gen Z workers surveyed believed that ChatGPT gave them\nbetter career advice then their managers [32]. An author of that study later told a\nreporter that youth are \"craving for guidance that they're not finding within the\ntraditional structures of their workplaces\" [33]. Around the same time, viral posts and\nvideos spread on social media from influencers who claimed ChatGPT got them a job or\na raise. A wave of mainstream media reporting followed, with headlines from the New\nYork Post (\"ChatGPT negotiated my salary': How Gen Z uses AI to boost their\ncareers\" [33]), Forbes (\"5 ChatGPT Prompts To Land A Higher Paying Job In\n2024\" [34]), and news.com.au (\"Gen Z are using AI to help negotiate their salary\" [35]).\nAn employer-focused blogger caught the trend and wrote advice for managers whose\nemployees were negotiating with ChatGPT (\"Employee compensation are you\nprepared to negotiate with AI?"}, {"title": "2.2 Past audit studies for protected group discrimination in AI,\nML, LLMs, and FMs", "content": "Fairness, social biases, and discrimination have been a growing concern within NLP and\nML for the past decade, starting with bias in classic small language models and NLP\nalgorithms for text classification, translation, recommendation, and information\nretrieval. For example, Latonya Sweeney published an early influential study in 2013,\nafter searching for her own name on Google and finding advertisements that implied she\nhad an arrest record. She systematically searched Google for names disproportionately\nheld by different races and found that Black names were much more likely to have ads\nsuggesting an arrest record, even when controlling for actual arrest record rates [38].\nAlso in 2013, the paper that became the foundation of word embeddings (like word2vec\nand GloVe) celebrated its ability to do vector math on cultural concepts like \"king -\nman + woman = queen\" [39]. Others claimed that social bias was evident in learned\nrepresentations like \u201cman - woman = computer programmer - homemaker", "toxic": "omments, journalists and social media\nusers engaged in \"everyday auditing\" [46] and compared scores for otherwise-identical\nsentences with different identities. Early versions of Perspective API rated sentences\nlike \"I am a white man\" as far less toxic than \"I am a black woman\" and with especially\nhigh bias against LGBTQ+ identities [47]. Such findings were confirmed by larger and\nmore systematic audit research [48,49], which also found bias against persons with\ndisabilities [50]."}, {"title": "2.2.1 Early research on discrimination in NLP and small language models", "content": "Fairness, social biases, and discrimination have been a growing concern within NLP and\nML for the past decade, starting with bias in classic small language models and NLP\nalgorithms for text classification, translation, recommendation, and information\nretrieval. For example, Latonya Sweeney published an early influential study in 2013,\nafter searching for her own name on Google and finding advertisements that implied she\nhad an arrest record. She systematically searched Google for names disproportionately\nheld by different races and found that Black names were much more likely to have ads\nsuggesting an arrest record, even when controlling for actual arrest record rates [38].\nAlso in 2013, the paper that became the foundation of word embeddings (like word2vec\nand GloVe) celebrated its ability to do vector math on cultural concepts like \"king -\nman + woman = queen\" [39]. Others claimed that social bias was evident in learned\nrepresentations like \u201cman - woman = computer programmer - homemaker", "toxic": "omments, journalists and social media\nusers engaged in \"everyday auditing\" [46] and compared scores for otherwise-identical\nsentences with different identities. Early versions of Perspective API rated sentences\nlike \"I am a white man\" as far less toxic than \"I am a black woman\" and with especially\nhigh bias against LGBTQ+ identities [47]. Such findings were confirmed by larger and\nmore systematic audit research [48,49], which also found bias against persons with\ndisabilities [50]."}, {"title": "2.2.2 Auditing LLMs for discrimination and social biases", "content": "Moving into the LLM era, larger and larger language models are typically trained with\nmore and more indiscriminately collected datasets, and researchers have repeatedly\nfound that social biases are embedded in such models. It is common to use methods\nthat calibrate to cases with known, real-world outcomes, like a study of a neural\nnetwork text model used to predict a patient's opioid risk from their medical records\n(including raw clinical notes) that found it was less accurate for Black patients [51]. As\nLLMs generate text, a common method asks LLMs to generate answers to stereotypical\nquestions, such as a study that used templates like \"The $IDENTITY should work as a\"\nand found bias in the professions returned [52].\nPerturbation-based or controlled experimental audits using template sentences are\nalso popular for LLMs, especially for auditing hiring tasks like resume screening using\ntemplated resumes. One study found extensive gender and racial biases in ChatGPT"}, {"title": "3 Methodology and materials", "content": "We used a controlled experimental setup to investigate how four different versions of\nChatGPT (Table 1) behaved when asked to give a personalized opening offer in a salary\nnegotiation for a new hire. Our work is most directly inspired by recent work [55] that\nsimilarly used template prompts to ask ChatGPT 4 to give an opening offer for a new\nhire in the voice of the employer. We extend their work, which only tested for (and\nfound) discrimination using names that are disproportionately held by different genders\nand races. We systematically varied the new hire's gender (through pronouns),\nuniversity, and undergraduate degree major. We also asked this question in the voice of\nthe new hire versus the voice of the employer."}, {"title": "3.1 Controlled experimental setup", "content": "We used a controlled experimental setup to investigate how four different versions of\nChatGPT (Table 1) behaved when asked to give a personalized opening offer in a salary\nnegotiation for a new hire. Our work is most directly inspired by recent work [55] that\nsimilarly used template prompts to ask ChatGPT 4 to give an opening offer for a new\nhire in the voice of the employer. We extend their work, which only tested for (and\nfound) discrimination using names that are disproportionately held by different genders\nand races. We systematically varied the new hire's gender (through pronouns),\nuniversity, and undergraduate degree major. We also asked this question in the voice of\nthe new hire versus the voice of the employer."}, {"title": "3.2 Statistical analyses and tests", "content": "In AI/ML fairness, choosing a statistical test for discrimination is a normative and\npolitical decision that captures a particular approach to bias in society [61-63]. Gender\nis a protected class, meaning that salary recommendations ought not vary when only\ngender is varied, so we use traditional hypothesis tests where the null hypothesis is that\ngender has no effect. We similarly treat comparisons between model versions and\nprompt voice within a protected class logic, where our null hypothesis is that varying\neach has no effect. In contrast, university and major are not protected classes, and so\nwe use linear regressions to measure the effect size of each condition relative to the\ncontrol prompt.\nFor all statistical tests of difference when varying model version, prompt type, or\ngender, our data do not satisfy assumptions for classic ANOVAS or T-tests: while they\nare independent samples, they are not normally distributed and have heterogeneity of\nvariance. For these tests, we first used the Kruskal-Wallis test by ranks [64], a\nnon-parametric test similar to an ANOVA, to determine if there is a significant\ndifference between one of these categories. If so, we then Dunn's test for pairwise\ncomparisons [65,66]. To mitigate concerns of p-hacking or data dredging, Dunn's test\ndoes apply a Bonferroni correction for multiple comparisons for the number pairwise\ncomparisons in each run. However, make many more statistical tests across this paper,\nso we additionally applied even more of a Bonferroni correction than needed, setting our\np-value threshold at .05/100 (as if we were making 100 tests).\nFor university and major, we do not follow a protected class logic and so use a\ndifferent approach to characterizing the models' behavior, based on ordinary least\nsquares regressions. We ran one regression for each model tested, which predict the\nmodel's recommended offers by a linear equation where each term is an instance of each\ncondition we tested, and set the intercept to the control cases for university and major.\nThis lets us easily represent how much on average each model advantages or\ndisadvantages candidates from each university and major, relative to not mentioning\nthat attribute at all. All four OLS regressions had different adjusted r\u00b2 values\n(gpt-3.5-0613: 0.403; gpt-3.5-0125: 0.643; gpt-4: 0.760; gpt-40: 0.613): higher values\nmean the variance in the model's outputs could be more completely explained as linear\neffects of varying the permuted terms, while lower values mean the model's outputs are\nless predictable using only these factors.\nThis method is similar to state-of-the-art work on AI/ML interpretability and\nexplainability, which shares common concerns with the fairness and discrimination\nliterature, as both seek to understand how a model's output varies as certain input\nfeatures are varied. Methods like SHAP [67] similarly permute through variations in the\ninput space, but use complex mathematical methods from game theory to account for\nthe relative effect sizes of hundreds of potential input variables and complex non-linear\ninteractions between them. In contrast, we choose a relatively simple post-hoc method,\nordinary least-squares (OLS) regressions, which given the limited conditions we test,\nbetter meets our goal of informing stakeholders of how these models may systematically\nadvantage certain groups over others. Regressions require far less resources, are simpler\nto implement, and are conceptually easier for stakeholders to understand.\nWe visualize distributions with letter-value plots or boxenplots [68], which are\nsimilar to a traditional boxplot, but show more information about the distribution. Like\nwith boxplots, the middle line (in red) is the median and the widest center box\nrepresents the distribution of the middle 50th percentile (or IQR). The width of the\nnext set of boxes above and below represent the distribution of the 87.5th to 12.5th\npercentiles respectively, with each successive boxes halving the percentile."}, {"title": "3.3 Software used and data availability", "content": "For computational analysis and scripting for data collection, management, and analysis,\nwe used Python 3.10 [69], using the following libraries: Pandas dataframes [70] for data\nparsing and transformation; SciPy [71] and NumPy [72] for quantitative computations;"}, {"title": "4 Results", "content": "We parsed each response for dollar values, and when the response included multiple\ndollar values (most commonly in a range), we averaged all dollar values in the response.\nIf a response did not contain a dollar value, we coded this as a refusal and excluded it\nfrom later analyses. Table 2 shows refusal rates and response lengths. We found very\nlow rates of the model refusing to answer the prompt or generating lengthy responses,\nwith the exception of the now-obsolete gpt-3.5-turbo-0613 model (released June 2023).\nThis is in line with with Altman's February 2024 statement that future models should\nbe \"less lazy\", as only the gpt-3.5-turbo-0613 model disregards our prompt engineering\ninstructions to only give dollar amount at significantly higher rates than all other\nmodels. We also note this earlier model's tendency towards much longer responses more\nfor the employee-voiced prompt than the employer-voiced prompt."}, {"title": "4.1 Data Cleaning and Refusal Rates (RQ0)", "content": "We parsed each response for dollar values, and when the response included multiple\ndollar values (most commonly in a range), we averaged all dollar values in the response.\nIf a response did not contain a dollar value, we coded this as a refusal and excluded it\nfrom later analyses. Table 2 shows refusal rates and response lengths. We found very\nlow rates of the model refusing to answer the prompt or generating lengthy responses,\nwith the exception of the now-obsolete gpt-3.5-turbo-0613 model (released June 2023).\nThis is in line with with Altman's February 2024 statement that future models should\nbe \"less lazy\", as only the gpt-3.5-turbo-0613 model disregards our prompt engineering\ninstructions to only give dollar amount at significantly higher rates than all other\nmodels. We also note this earlier model's tendency towards much longer responses more\nfor the employee-voiced"}]}