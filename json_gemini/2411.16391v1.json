{"title": "Human-Calibrated Automated Testing and Validation of Generative Language Models", "authors": ["Agus Sudjianto", "Aijun Zhang", "Srinivas Neppalli", "Tarun Joshi", "Michal Malohlava"], "abstract": "This paper introduces a comprehensive framework for the evaluation and validation of generative language models (GLMs), with a focus on Retrieval-Augmented Generation (RAG) systems deployed in high-stakes domains such as banking. GLM evaluation is challenging due to open-ended outputs and subjective quality assessments. Leveraging the structured nature of RAG systems, where generated responses are grounded in a predefined document collection, we propose the Human-Calibrated Automated Testing (HCAT) framework. HCAT integrates a) automated test generation using stratified sampling, b) embedding-based metrics for explainable assessment of functionality, risk and safety attributes, and c) a two-stage calibration approach that aligns machine-generated evaluations with human judgments through probability calibration and conformal prediction.\nIn addition, the framework includes robustness testing to evaluate model performance against adversarial, out-of-distribution, and varied input conditions, as well as targeted weakness identification using marginal and bivariate analysis to pinpoint specific areas for improvement. This human-calibrated, multi-layered evaluation framework offers a scalable, transparent, and interpretable approach to GLM assessment, providing a practical and reliable solution for deploying GLMs in applications where accuracy, transparency, and regulatory compliance are paramount.", "sections": [{"title": "Introduction", "content": "Generative language models (GLMs) have revolutionized natural language processing (NLP), powering applications such as conversational agents, content generation, and language translation. The latest large language models (LLMs) (Brown et al., 2020) can generate coherent and contextually relevant text that often closely resembles human writing. Retrieval-augmented generation (RAG) systems advance this capability by embedding retrieval mechanisms within generative models, enabling access to a structured knowledge base of documents to guide responses (Krishna et al., 2023; Lewis et al., 2020). In RAG systems, this defined collection of documents serves as a grounding reference, allowing the generation process to produce outputs anchored in available information. This approach helps ensure more predictable and manageable system behavior by constraining outputs within a predefined document scope.\nEvaluating GLMs is challenging due to the vast range of potential inputs and outputs, making exhaustive manual assessment impractical (Srivastava et al., 2023; Liang et al., 2023). However, the bounded nature of RAG systems offers opportunities for more focused and feasible testing. With a defined document scope, RAG systems enable systematic exploration of inputs and provide a clearer framework for output evaluation. Additionally, the reliance on external documents allows the generation of relevant queries and anticipated responses, facilitating a more automated approach to testing and validation."}, {"title": "Challenges in Evaluating Generative Models in Banking", "content": "Model testing and validation is a rigorous process aimed at identifying and quantifying weaknesses in models to enable targeted improvements or to apply risk mitigation. This process ensures that models are reliable, accurate, and effective for their intended uses. Model validation helps prevent potential failures and maintains confidence in the model performance for critical business process. Banking industry in the US probably has the most matured model validation practice compared to other industries where every model prior going to production must be evaluated in terms of conceptual soundness and outcome analysis (Sudjianto and Zhang, 2024).\nModel testing and validation are essential to identify and quantify model weaknesses, enabling targeted improvements or risk mitigation measures. This process ensures that models remain reliable, accurate, and effective for their intended applications. In the banking sector, where model performance is critical to business operations, model validation practices are highly advanced. The U.S. banking industry, in particular, has established some of the most rigorous model validation standards, requiring every model to undergo comprehensive evaluation including assessments of conceptual soundness and outcome analysis before deployment (Sudjianto and Zhang, 2024). However, generative language models present unique challenges for validation compared to traditional predictive models, whose outputs are typically constrained to specific labels or numerical values. GLMs, in contrast, produce open-ended text, making it difficult to define a singular \"correct\u201d output and to consistently assess quality.\nRAG systems combine retrieval and generation capabilities to produce nuanced, contextually rich responses. This dual functionality requires evaluation across several dimensions, including the relevance of retrieved information, the groundedness of generated content in those sources, the completeness of responses to user queries, and the overall relevance of the provided answers. As a result, evaluating RAG systems is inherently more complex than assessing traditional predictive models with well-defined outputs.\nAn emerging approach to evaluating GLMs involves using LLMs as judges to assess responses from other models based on metrics such as truthfulness, relevance, and consistency. For example, the TruthfulQA benchmark developed by Lin et al. (2022) employs a fine-tuned LLM to evaluate responses for factual accuracy. While efficient and scalable, this \"LLM-as-judge\" method introduces limitations, including circularity and shared biases\u2014where the evaluating LLM may have similar misconceptions or predispositions as the models it assesses. Additionally, in regulated industries like banking, relying on LLMs for evaluation may face resistance due to the opacity and lack of transparency in how outcomes are determined, making it challenging to meet explainability requirements. Further, using LLMs as evaluators may raise concerns about conceptual soundness, as evaluation by a complex, often unexplainable model might not align with rigorous validation standards expected in banking.\nThe HELM (Holistic Evaluation of Language Models) framework proposed by Liang et al. (2023) highlights the need for multidimensional assessments covering robustness, fairness, and toxicity, which are difficult to achieve reliably through opaque models. These issues underscore the broader challenge of adopting LLM-based evaluation methods in regulated sectors where transparency, conceptual soundness, and accountability are paramount for regulatory acceptance.\nIn summary, the evaluation of RAG systems and generative language models (GLMs) in banking presents distinct challenges. First, comprehensive testing is essential to ensure model reliability, yet the open-ended nature of GLMs makes defining exhaustive test cases difficult. Second, scaling this comprehensive testing is a formidable task, as test cases must be generated across a wide array of scenarios to capture the complexity of potential inputs and outputs. Third, establishing reliable evaluation approaches and metrics that can consistently capture dimensions like truthfulness, relevance, and groundedness is challenging, especially given the subjective nature of language quality. Finally, applying rigorous testing and validation procedures akin to those used in traditional predictive models is difficult for GLMs, as these models require assessment of open-text outputs rather than discrete, predictable values. Together, these challenges underscore the need for innovative testing methodologies and robust, scalable validation frameworks that can address the unique complexities of generative models in high-stakes industries like banking."}, {"title": "A Structured Approach", "content": "To address the challenges, we propose a structured approach covering essential steps for generative language model validation in banking:\n1. Define Model Purpose and Scope: Begin by clearly stating the model's intended use, whether for customer support, document summarization, or other applications. Establish boundaries around the expected tasks and the limitations of the model to guide validation criteria.\n2. Identify Potential Risks and Failures: Outline the key risks, such as the model generating incorrect, biased, or misleading responses, which could have regulatory or reputational impacts. Focus on identifying failure modes specific to the financial and non-financial contexts, including the generation of non-compliant or sensitive information.\n3. Develop Diverse Test Cases and Stress Tests: Create test cases that cover a broad spectrum of query types, ensuring that the model can handle varied topics, question formats, and levels of complexity. Conduct stress tests, such as ambiguous, adversarial, and out-of-distribution inputs, to reveal model weaknesses and test its robustness under challenging scenarios.\n4. Use Transparent and Explainable Metrics: Prioritize metrics that offer transparency, such as those based on semantic similarity and natural language inference, over black-box methods. Embedding-based metrics like BERTScore or entailment probabilities can provide more interpretable insights into whether the model responses are relevant, grounded, and complete.\n5. Automate Testing for Comprehensive Coverage: Automate testing when possible to ensure the validation process is scalable and can cover a wide range of queries and scenarios without requiring extensive manual effort.\n6. Calibrate with Human Evaluations: Periodically sample model outputs for human review, comparing automated metrics with human judgments to ensure the automated processes align well with human expectations. Use this calibration to adjust metric thresholds or test parameters as needed.\n7. Identify Weaknesses for Model Improvement and Risk Mitigation: Based on the validation results, highlight areas where the model struggles, such as specific query types or scenarios. Use this analysis to guide ongoing model improvement, risk mitigation or guardrails, and the design of monitoring systems to maintain performance post-deployment.\nThis structured approach provides a comprehensive framework for validating GLMs within banking, integrating rigorous steps to ensure reliability, accuracy, and compliance with regulatory standards. Transparent, explainable metrics are prioritized to offer interpretable insights into GLM outputs, and automation is employed to enable thorough coverage across various scenarios. Calibration with human evaluations further aligns the validation process with real-world expectations, and continuous monitoring of GLM weaknesses ensures targeted improvements and mitigates risks over time."}, {"title": "Human-Calibrated Automated Testing (HCAT) Framework", "content": "Building on the structured approach outlined above, we now introduce the human-calibrated automated testing (HCAT) framework, a technical and systematic solution tailored for the rigorous demands of GLM testing and validation. This framework combines automated test generation, explainable evaluation metrics, and human-calibrated benchmarks to tackle the complexities of assessing GLMs, particularly in the context of RAG systems.\nThe HCAT framework is designed to ensure that the validation process is both scalable and interpretable, meeting high standards of transparency, accuracy, and compliance. The following components define the technical structure of the HCAT framework:\n1. Automatic Test Generation: Using topic modeling and stratified sampling, HCAT produces a diverse set of queries covering the full scope of the document collection, enabling comprehensive model evaluation across varied input scenarios.\n2. Explainable Evaluation Metrics: HCAT employs embedding-based metrics to provide a holistic assessment of model performance, spanning two critical dimensions:\n\u2022 Functionality Metrics: Embedding-based metrics assess core RAG capabilities, including relevance, groundedness, completeness, and answer relevancy, offering transparent and interpretable insights into semantic alignment between queries, contexts and answers.\n\u2022 Risk and Safety Metrics: Specialized embedding-based metrics assess risk and safety, such as toxicity, bias, and privacy protection, crucial for ensuring compliance and reliability in sensitive applications.\n3. Calibration with Human Judgments: To ensure that the automated metrics align with human perceptions, we calibrate them using samples of human labeling. This process involves:\n\u2022 Sampling Human Evaluations: Gathering human judgments on subsets of the generated outputs.\n\u2022 Regression Techniques: Applying probability calibration models to align machine evaluation scores with human judgments.\n\u2022 Conformal Prediction: Quantifying uncertainty in machine evaluations by providing prediction sets with confidence level, enabling a more nuanced understanding of evaluation reliability.\nIn the following sections, we provide a detailed breakdown of each HCAT component. Section 2 describes the automatic test generation process, including the use of topic modeling and stratified sampling to create a comprehensive set of test queries. Section 3 delves into functionality evaluation metrics, covering relevance, groundedness, completeness, and answer relevancy, and explains the use of embedding-based metrics to assess each dimension. Section 4 focuses on risk and safety evaluation, detailing metrics for toxicity, bias, and privacy protection to ensure compliance and reliability. Section 5 addresses the calibration process with human judgments, explaining how human evaluations refine automated metrics for real-world alignment. Section 6 presents robustness testing and weakness identification techniques to pinpoint areas for improvement, followed by Section 7 with a discussion of implications and conclusions."}, {"title": "Automatic Test Generation", "content": "To evaluate GLMs comprehensively, particularly RAG systems, it is crucial to have a diverse and representative set of queries that spans the entire scope of the document collection. To achieve this, we propose an automatic query generation method through stratified sampling. The topic modeling technique by Grootendorst (2022) serves as a prerequisite for defining strata, allowing us to categorize documents into coherent topics or themes. By sampling within each topic stratum, we ensure that the generated queries cover all relevant topics and variations within the knowledge base.\nOur five-step process for automatic query generation includes: (1) Embedding, (2) Dimensionality Reduction, (3) Clustering to Define Strata, (4) Stratified Sampling, and (5) LLM-driven Query Generation.\nStep 1: Embedding\nThe first step involves generating embeddings for all documents in the collection. Embeddings are numerical vector representations that capture the semantic content of text; see Devlin et al. (2019) for contextual embeddings using BERT (Bidirectional Encoder Representations from Transformers). In the proposed HCAT framework, we utilize the following advanced embedding models:\n\u2022 Embeddings Trained through Contrastive Learning: Models like SimCSE (Gao et al., 2021) and Sentence-BERT (Reimers and Gurevych, 2019) use contrastive learning to produce embeddings that capture fine-grained semantic similarities. These embeddings are effective for assessing the relevance and coherence between texts. See more discussion in Section 3.\n\u2022 Specialized Embeddings from Natural Language Inference (NLI) Models: NLI models are trained to determine entailment, contradiction, or neutrality between pairs of sentences (MacCartney, 2009). By using embeddings from NLI models, we can evaluate the logical consistency and groundedness of the generated responses in relation to the source documents. Meanwhile, specialized NLI models can be applied for detecting hallucination (Kryscinski et al., 2020; Laban et al., 2022) and detecting toxicity (Jigsaw and Google, 2017; Hanu and Unitary team, 2020). See more discussions in Sections 3 and 4.\nBy converting documents into embeddings, we create a foundation for analyzing semantic similarities or other discriminative tasks in a high-dimensional space.\nStep 2: Dimensionality Reduction\nThe generated embeddings are high-dimensional vectors for which clustering approach may become less effective. To address this, we apply dimensionality reduction techniques to project the embeddings into a lower-dimensional space while preserving their essential semantic properties. Among others, we consider\n\u2022 Principal Component Analysis (PCA): reduces dimensionality by linearly projecting data onto principal components that capture the most variance.\n\u2022 Uniform Manifold Approximation and Projection (UMAP): preserves both local and global data structure, providing an efficient and scalable method for dimensionality reduction (McInnes et al., 2018).\nThe choice of dimensionality reduction technique depends on factors such as dataset size, computational resources, and the desired balance between preserving local and global structures. It enables efficient clustering and visualization, facilitating the identification of natural groupings within the data.\nStep 3: Clustering to Define Strata\nWith the reduced-dimensional embeddings, we perform clustering to group semantically similar documents. A clustering algorithm may identify natural groupings within the data, effectively organizing the documents into topics or themes. Among others, we consider\n\u2022 K-Means Clustering: Partitions the data into a predefined number of clusters by minimizing within-cluster variance.\n\u2022 DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Identifies clusters based on data point density, allowing for clusters of arbitrary shape and handling noise effectively (Schubert et al., 2017).\nThe resulting clusters serve as strata for stratified sampling. Each cluster represents a distinct topic or sub-topic within the document collection, ensuring that all areas of the knowledge base are represented in the testing process. Figure 1 shows an example of topic clustering, where each point in the plot represents a document chunk. This stratification is crucial for achieving comprehensive coverage and preventing biases toward dominant topics.\nStep 4: Stratified Sampling\nTo achieve the comprehensive coverage effectively, we perform sampling within each cluster. Sampling can be proportional to the size of the cluster or weighted based on criteria such as the importance of the topic or the frequency of occurrence.\nThe stratified sampling approach ensures that queries are generated from all topics, preventing over-representation of prevalent themes and under-representation of niche areas. It allows for a balanced evaluation of the RAG system across the entire spectrum of the knowledge base, minimizing the risk of overlooking any significant areas.\nStep 5: LLM-driven Query Generation\nFinally, we utilize an LLM to generate queries based on the sampled documents. For each selected document, we prompt the LLM to create questions that are relevant to the content. The process involves:\n1. Extracting Key Information: Identifying important facts, concepts, or statements within the document suitable for question formulation.\n2. Prompting the LLM: Providing the LLM with the extracted information and instructions to generate queries of various types and complexities.\n3. Ensuring Diversity and Complexity: Instructing the LLM to produce a variety of question formats and difficulty levels, including yes/no questions, multiple-choice questions, and open-ended queries.\n4. Query Selection: Evaluate and select queries based on relevancy metrics.\nWhen prompting the LLM, we need to generate queries with various query types and complexities in order to thoroughly test the RAG system (Yang et al., 2018; Ribeiro et al., 2020; Li et al., 2024). Among our considerations are the following scenarios:\n1. Simple Factual Queries that test basic retrieval capabilities.\n2. Multi-hop or Compound Queries that assess the ability to synthesize information from multiple sources.\n3. Inference and Reasoning Queries that evaluate logical and reasoning skills.\n4. Yes/No and Multiple-Choice Questions that testing precision and understanding.\nTo sum up, the automatic test generation component of the HCAT framework ensures that GLMs are tested comprehensively and representatively. The five-step process covers the entire scope of the document collection in a RAG system. By automating the query generation process using an LLM, we efficiently create a comprehensive set of test queries that are diverse in content and form. Through the use of topic modeling and clustering, it allows us to thoroughly evaluate the capabilities of a RAG system in retrieving relevant information and generating accurate responses across all topics."}, {"title": "Functionality Metrics", "content": "Evaluating GLMs has traditionally involved metrics like BLEU, ROUGE, and perplexity, which quantify aspects of language generation such as n-gram overlap and fluency. However, these metrics often fail to capture semantic relevance and do not align well with human judgments, especially for open-ended generation tasks. Recent research has explored embedding-based metrics that assess semantic similarity, offering a closer approximation to human judgments (Zhang et al., 2020).\nWe advocate the use of embedding-based evaluation metrics. As discussed in Section 2, the embeddings of documents can be trained by contrastive learning or extracted from specialized NLI models. Using these embeddings, we can calculate semantic similarities and entailment probabilities, providing transparent and statistically grounded evaluation metrics. This approach avoids reliance on black-box tools or unverified methods, allowing for in-depth analysis and understanding of the evaluation results.\nTo effectively measure the performance of RAG systems, it is essential to adopt evaluation approaches that are both transparent and explainable, particularly when assessing retrieval relevance, groundedness, completeness, and answer relevancy; see Figure 2 for an illustration. The transparency is crucial not only for applications in regulated industries, where compliance and accountability are paramount, but also for fostering trustworthiness among users. Moreover, explainable metrics can be effectively calibrated with human evaluations, ensuring that automated assessments align with human judgments and expectations. By prioritizing explainable and interpretable evaluation methods, we can enhance the reliability and integrity of RAG systems, ensuring they meet high standards of performance and user trust."}, {"title": "Context Relevancy", "content": "Context relevancy measures how well the retrieved documents address the input query for a RAG system. To quantitatively measure the relevancy between a query and a context in RAG systems, we develop a sentence-level semantic similarity approach that extends the token-level approach from Zhang et al. (2020). This method breaks down both the query and the context into individual sentences and computes similarity scores for each pair, providing a fine-grained assessment of context relevancy.\nLet us denote the query as $Q = \\{q_1, q_2, ..., q_m\\}$ consisting of $m$ sentences, and the retrieved context as $C = \\{C_1, C_2, ...,C_n\\}$ consisting of $n$ sentences. For each sentence in either $Q$ or $C$, its embedding is computed using a suitable embedding model:\n$e_{qi} = Embed(q_i)$, for $i = 1,2,...,m$.\n$e_{c_j} = Embed(c_j)$, for $j = 1,2,...,n$.\nwhere $Embed(.)$ represents the embedding function that maps a sentence to a vector in the $d$-dimensional embedding space. Thus, we can compute the similarity between each pair of query and context sentences using the cosine similarity,\n$Sim(q_i, c_j) = cos(\\theta_{ij}) = \\frac{e_{qi} e_{c_j}}{||e_{qi}||||e_{cj}||}$"}, {"title": "Groundedness", "content": "Groundedness ensures that the generated content is based on the retrieved documents, avoiding unsupported statements or hallucinations. To measure the groundedness between the context and the generated answer in a RAG system, we employ two approaches: sentence similarity and natural language inference."}, {"title": "Sentence Similarity", "content": "Denote the answer as $A = \\{a_1,a_2,...,a_k\\}$ consisting of $k$ sentences. Similar to the approach of computing the context relevancy, let us break down both the context and the answer into individual sentences, then compute the similarity scores for each pair of sentence embeddings. For each sentence $a_i$ in the answer, calculate the maximum similarity by\n$S_{max}(a_i) = max_{1\\leq j \\leq n} Sim(a_i, c_j)$,\nwhich measures how well the answer sentence $a_i$ is grounded in the context $C = \\{C_1, C_2, ..., C_n\\}$. To obtain an overall groundedness score for the entire answer, we may aggregate the maximum similarity scores for all answer sentences:\n$S_{groundedness} = \\frac{1}{k}\\sum_{i=1}^{k} S_{max}(a_i)$.\nA high $S_{groundedness}$ score indicates that on average the sentences in the answer are well-supported by the context, suggesting that the answer is grounded and less likely to contain hallucinations.\nConversely, a low $S_{groundedness}$ score suggests that some sentences in the answer may lack sufficient support from the context. The sentence with the lowest similarity to any context sentence, identified as\n$i^* = arg min_{1<i<k} S_{max}(a_i)$,\nis considered the least grounded and may indicate a potential hallucination. Other sentences with low $S_{max}(a_i)$ values could also signal possible hallucinations."}, {"title": "Natural Language Inference (NLI)", "content": "NLI models are specifically designed to determine the inferential relationship between two pieces of text, a premise and a hypothesis, by classifying the relationship as \u201centailment\", \"neutral\u201d, or \u201ccontradiction\u201d (MacCartney, 2009). In the context of RAG systems, we treat the context as the premise and the generated answer as the hypothesis.\nNLI aims to determine whether a hypothesis can logically be inferred from a premise. For the purpose of measuring groundedness (the opposite of hallucination), NLI provides a mechanism to assess whether the generated answer is logically supported by the context. If the answer is entailed by the context, it is considered grounded; if it contradicts the context or is unrelated, it may indicate a hallucination.\nWhile NLI models provide class probabilities through multi-class classification, we can obtain a more nuanced groundedness measure by analyzing the embeddings produced by the model and measuring the distance to the decision boundary. The decision boundary in the embedding space separates different classes and reflects the model's confidence in its predictions. In this method, the distance to the decision boundary is directly related to the logit value for the \u201centailment\" class. This approach simplifies the calculation and provides an interpretable measure of groundedness.\nSuppose a linear classifier computes a logit score $z$ using the embedding $\\overline{x}$ of the input (which combines the premise and hypothesis):\n$z = \\overline{w}\\overline{x} + b$\nwhere $\\overline{w}$ is the weight vector and $b$ is the bias term. The distance $D$ from the input point $\\overline{x}$ to the decision boundary (i.e., the hyperplane $\\overline{w}\\overline{x} + b = 0$) is given by:\n$D = \\frac{\\overline{w}\\overline{x} + b}{||\\overline{w}||}$\nwhere $|\\overline{w}|$ is the Euclidean norm (magnitude) of the weight vector. The distance to the decision boundary can be used to measure the groundedness in the sense that\n\u2022 when $D > 0$, the hypothesis is on the entailment side, i.e., grounded;\n\u2022 when $D < 0$, the hypothesis is on the non-entailment side, i.e., potential hallucination.\nWe can map the distance $D$ to a probability groundedness score between 0 and 1 by applying the logit transformation $\\sigma(D) = 1/(1 + e^{-D})$.\nIn sentence-level groundedness assessment, each sentence $a_i$ in the answer $A$ is combined with the context $C$, then input into the NLI model to obtain the embedding $\\overline{x_i}$. This allows us to compute the sentence-level $z_i$, $D_i$ and $\\sigma(D_i)$. The sentences with low $\\sigma(D_i)$ scores may be identified as potential hallucinations."}, {"title": "Completeness", "content": "Completeness evaluates whether the generated answer covers all relevant information from the context. In RAG systems, completeness refers to the degree to which the generated answer incorporates all relevant information from the retrieved context. A complete answer should not only be accurate and relevant but also cover all essential points that are pertinent to the user's query. Ensuring completeness is crucial for providing users with comprehensive and informative responses."}, {"title": "Sentence Similarity", "content": "This approach assesses completeness by evaluating how well the sentences in the context are reflected in the generated answer. Similar to context relevancy and groundedness, we break down the context and answer into sentences, then calculate embedding-based similarity in the sentence level. For each sentence $c_i$ in the context $C = \\{C_1, C_2, ..., C_n\\}$, calculate the maximum similarity with any sentence in the answer $A = \\{a_1, a_2, ..., a_k\\}$:\n$S_{max}(c_i) = max_{1 \\leq j \\leq k} Sim(c_i, a_j)$.\nThis score indicates how well the context sentence $c_i$ is covered in the answer.\nSimilarly, we can aggregate $S_{max}(c_i)$ scores to obtain the overall completeness score by either the simple average\n$S_{completeness} = \\frac{1}{n}\\sum_{i=1}^{n} S_{max}(C_i)$,\nor the weighted average\n$S_{completeness} = \\sum_{i=1}^{n} W_i . S_{max}(C_i)$,\nwhere $w_i$ is the weight assigned to the context sentence $c_i$ subject to $w_i \\geq 0$ and $\\sum_{i=1}^{n} W_i = 1$. A high completeness score indicates that the answer covers most of the content from the context, while a low completeness score suggests that significant portions of the context are not reflected in the answer."}, {"title": "Distribution Alignment Using Wasserstein Distance", "content": "When assessing the completeness of an answer generated by LLMs, it is essential to measure how well the answer captures the entire information distribution of the original context. The sentence similarity approach focuses on finding close matches between individual sentences, which may not fully reflect the answer's coverage of the overall context. By applying Wasserstein distance, a measure from optimal transport theory, we can evaluate the alignment of information distribution between the context and the summary, offering a complementary perspective on completeness; see also Tang et al. (2022).\nOptimal transport (Chewi et al., 2024) is a mathematical approach for measuring the cost of transforming one distribution into another. Wasserstein distance, also known as Earth Mover's distance, is an optimal transport metric that quantifies the minimum cost to align two distributions, reflecting how closely they match in structure and content. In the context of evaluating RAG-generated answers, the context sentences are treated as a distribution of information that needs to be represented in the answer sentences. In this case, Wasserstein distance measures how much effort is required to transform the distribution of context information $C$ into the distribution of answer information $A$:\n$W(C, A) = min_{\\gamma \\in \\Gamma(p, q)} \\sum_{i=1}^{n} \\sum_{j=1}^{k} \\gamma_{ij}d(C_i, a_j)$"}, {"title": "Complementing Sentence Similarity with Wasserstein Distance", "content": "While sentence similarity methods directly compare individual pairs of sentences, Wasserstein distance provides a complementary approach by assessing the global alignment of information across the entire context and answer. The key advantages of Wasserstein distance are:\n1. Global Information Distribution: Wasserstein distance captures the overall distribution of information in the context and answer, making it effective for assessing completeness by reflecting how well the summary represents the main ideas and topics from the context.\n2. Tolerance to Partial Matches: Unlike sentence similarity, which requires exact or near-exact matches, Wasserstein distance allows for approximate alignment. This means that summaries with paraphrased or generalized content can still achieve low Wasserstein distances, provided they retain the main themes.\n3. Contextual Relationships: By aligning entire sentence distributions, Wasserstein distance indirectly accounts for the thematic structure of the context and summary, offering a broader perspective than sentence-level similarity alone.\n4. Evaluating Completeness: A lower Wasserstein distance indicates that the answer closely captures the distribution of information from the context, reflecting higher completeness. Conversely, a higher distance suggests missing content or inadequate coverage of essential topics.\nUsing both sentence similarity and Wasserstein distance together allows for a more nuanced evaluation of completeness, capturing both the detailed alignment of specific sentences and the overall distribution of ideas and topics within the answer."}, {"title": "Answer Relevancy", "content": "Answer relevancy ensures that the generated response directly addresses the user's query. We calculate maximum similarity scores between the query and the generated response to measure alignment with the user's intent. Again, we can employ the sentence-level similarity approach similar to previously discussed metrics. This time we break down both the query and the answer into individual sentences and computing similarity scores for each pair. By analyzing these similarities, we can quantify how well the answer addresses the user's query.\nFor each sentence $a_i$ in the answer $A = \\{a_1, a_2, . . ., a_k\\}$, calculate the maximum similarity with any sentence in the query $Q = \\{q_1, q_2, ..., q_m\\}$:\n$S_{max}(a_i) = max_{1\\leq j \\leq m} Sim(a_i, q_j)$,\nwhich measures how well the answer sentence $a_i$ is relevant to the query. Then, to obtain an overall answer relevancy score for the entire answer, we can aggregate the $S_{max}(a_i)$ scores for all answer sentences by either the simple average\n$S_{a-relevancy} = \\frac{1}{k}\\sum_{i=1}^{k} S_{max}(a_i)$\nor the weighted average\n$S_{a-relevancy} = \\sum_{i=1}^{k} W_i . S_{max}(a_i)$,\nwhere $w_i$ is the weight assigned to the answer sentence $a_i$ subject to $w_i \\geq 0$ and $\\sum_{i=1}^{k} W_i = 1$. A high $S_{a-relevancy}$ score indicates that on average the answer effectively addresses the user's question, while a low $S_{a-relevancy}$ score indicates potential divergence or irrelevance.\nWhen it is critical that all parts of the answer are desired to be relevant to the query, we may focus on the least relevant answer sentence and compute the minimum of maximum similarities:\n$S_{a-relevancy} = min_{1<i<k} S_{max}(a_i)$."}, {"title": "Risk and Safety Metrics", "content": "In this section, we provide a concise overview of evaluation metrics for assessing risk and safety aspects of generative language models. Here we focus on critical dimensions that ensure the deployed GLMs perform responsibly, and in alignment with regulatory standards for appropriate use in high-stakes applications such as banking. Three essential risk and safety metrics for validating GLMs in these environments include toxicity assessment, bias evaluation, and privacy protection.\nToxicity assessment measures the likelihood that a model generates harmful, offensive, or inappropriate content. In banking, where interactions must be professional and respectful, toxicity in model outputs can severely damage customer trust, harm the institution's reputation, and potentially lead to legal repercussions if sensitive or controversial topics are mishandled. Toxicity is typically assessed through NLI models that classify statements as safe or offensive, allowing for real-time toxicity assessment; see Hanu and Unitary team (2020) among others.\nBias evaluation focuses on detecting demographic or sentiment bias within the model responses, ensuring equitable treatment of diverse user groups. In banking, where interactions may influence financial decisions or customer perceptions, bias can lead to discriminatory responses, harming the reputation of the institution and potentially leading to regulatory scrutiny. Bias evaluation involves testing the model with a diverse set of demographic-related queries, including variations in race, gender, age, and income level, to determine if response quality or sentiment varies across different groups. Sentiment analysis models help identify potential differences in tone or attitude, while counterfactual evaluation assesses whether altering demographic-related terms (e.g., swapping \u201cman\u201d with \u201cwoman\u201d) results in consistent responses. Models are scored on bias based on thresholds that align with banking standards, allowing institutions to identify and mitigate unacceptable biases before deployment. If biases are identified, they can be addressed by fine-tuning the model with additional data that represents underrepresented groups fairly, or by implementing constraints to reduce unintended biases in generated content.\nPrivacy protection is essential to ensure the model does not disclose sensitive information, such as personal financial details, customer identities, or other proprietary data. In banking, privacy is paramount due to stringent regulatory requirements like the GDPR and CCPA, and privacy violations can result in severe financial and legal consequences. Privacy protection for GLMs often involves Named Entity Recognition (NER), which identifies and flags sensitive entities in the output, such as names, addresses, or account numbers, allowing the model to suppress or filter such information before it reaches the user. Additionally, contextual data filtering is implemented to identify phrases related to account transactions or other sensitive areas, reducing the risk of unintentional data leakage. Adversarial testing is used to simulate scenarios where users might try to elicit sensitive information, and the model responses are evaluated to ensure that they avoid privacy violations. Strict thresholds are set to flag any instance of sensitive information being revealed, prompting immediate action to investigate and adjust the model if necessary. Privacy safeguards are reinforced through model retraining or the implementation of guardrails, ensuring compliance with industry regulations and protecting customer information.\nTogether, these metrics form a robust framework for risk and safety validation of GLMs, particularly in banking. Each metric provides a unique lens for assessing the model behavior, guiding institutions in minimizing risks associated with inappropriate content, unfair treatment, and data privacy. This validation process helps ensure that models meet the high standards of accuracy, transparency, and accountability required in financial services, supporting safe and responsible deployment in real-world applications."}, {"title": "Calibration of Machine and Human Evaluations", "content": "To ensure alignment between machine-generated scores and human judgments", "stages": "probability calibration and conformal prediction", "below": "n\u2022 Stage 1: Probability calibration provides an initial mapping of machine scores to probabilities that align with human expectations", "2": "Conformal prediction quantifies the uncertainty of these calibrated probabilities, providing prediction intervals with confidence levels.\nThis double-calibration strategy allows us to link machine evaluation metrics (such as relevancy, groundedness, or completeness) to human evaluations that may be binary or multi-level. Error analysis ("}]}