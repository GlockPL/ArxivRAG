{"title": "\u201cI know myself better, but not really greatly\u201d: Using LLMs to Detect and Explain LLM-Generated Texts", "authors": ["Jiazhou Ji", "Jie Guo", "Weidong Qiu", "Zheng Huang", "Yang Xu", "Xinru Lu", "Xiaoyu Jiang", "Ruizhe Li", "Shujun Li"], "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in generating human-like texts, but the potential misuse of such LLM-generated texts raises the need to distinguish between human-generated and LLM-generated content. This paper explores the detection and explanation capabilities of LLM-based detectors of LLM-generated texts, in the context of a binary classification task (human-generated texts vs LLM-generated texts) and a ternary classification task (human-generated texts, LLM-generated texts, and undecided). By evaluating on six close/open-source LLMs with different sizes, our findings reveal that while self-detection consistently outperforms cross-detection, i.e., LLMs can detect texts generated by themselves more accurately than those generated by other LLMs, the performance of self-detection is still far from ideal, indicating that further improvements are needed. We also show that extending the binary to the ternary classification task with a new class \"Undecided\" can enhance both detection accuracy and explanation quality, with improvements being statistically significant and consistent across all LLMs. We finally conducted comprehensive qualitative and quantitative analyses on the explanation errors, which are categorized into three types: reliance on inaccurate features (the most frequent error), hallucinations, and incorrect reasoning. These findings with our human-annotated dataset emphasize the need for further research into improving both self-detection and self-explanation, particularly to address overfitting issues that may hinder generalization.", "sections": [{"title": "1 Introduction", "content": "The rise of large language models (LLMs) has brought remarkable advancements in natural lan- guage processing (NLP) tasks (Matarazzo and Tor- lone, 2025), including text generation. Models such as GPT-40 (OpenAI, 2024), LLaMA (Tou- vron et al., 2023), and Qwen (Team, 2024) have blurred the boundaries between LLM-generated (LGTs) and human-generated texts (HGTs), pos- ing new challenges in distinguishing between the two. While these capabilities of LLMs open new possibilities, they also bring concerns in areas such as misinformation, academic dishonesty, and auto- mated content moderation (Hu, 2025). As a result, detecting LGTs has become an increasingly impor- tant research area (Dugan et al., 2024; Lee et al., 2023; Bhattacharjee and Liu, 2024a).\nPrior research has primarily focused on devel- oping classifiers to distinguish HGTs and LGTs, including open-source detectors (Hans et al., 2024) and online close-source detection systems (Tian et al., 2023). However, most detection systems have been limited to binary classification, which has several inherent issues. Recently, some works (Lee et al., 2024b) have attempted ternary classification by introducing a \u201cmixed\u201d category, which represents texts originating from mixed sources. However, this approach does not funda- mentally resolve the issue. We further adopt the definition of an \"Undecided\" category based on other studies (Ji et al., 2024) and conduct ternary classification experiments for different LLMs, as certain texts are inherently indistinguishable be- tween LGTs and HGTs. Furthermore, many stud- ies treat the detection task as a black box, offering little insight into the decision-making process. Ex- plainability, a critical aspect of trustworthy AI, has received less attention, but it is essential for build- ing systems that users can trust (Weng et al., 2024; Zhou et al., 2024). This paper presents an analysis of LLMs in detecting LGTs and HGTs, with a par- ticular emphasis on evaluating and improving the clarity of the explanations provided by LLM-based detectors. By investigating how LLMs make pre-"}, {"title": "2 Related Work", "content": "LGT and HGT Detection. Past efforts to iden- tify LLM-generated texts (LGTs) often relied on bi- nary classification systems that distinguish human- generated texts (HGTs) from LGTs using surface- level features. While these methods were initially effective, they are prone to errors when encoun- tering adversarial attacks or domain shifts, which limit their overall robustness (Bhattacharjee and Liu, 2024a; Dugan et al., 2024). To address these limitations, researchers have explored strategies that integrate external knowledge, such as combin- ing internal and external factual structures, to boost detection against diverse content and styles (Inter- nal and Structures, 2024). Recent studies also high- light the promise of using the LLMs themselves for text detection: approaches like self-detection and mutual detection can outperform traditional clas- sifiers, as illustrated by GPT-4's success in tasks like plagiarism detection (Lee et al., 2024a). No- tably, smaller models sometimes excel in zero-shot scenarios, offering adaptable solutions across vary- ing architectures (He et al., 2024). Furthermore, Lee et al. (2023) demonstrated that LLMs can reli- ably identify their own outputs, providing a more nuanced framework for content verification. De- spite these advances, the continuing challenges of domain adaptation and adversarial resistance un- derscore the need for more versatile and robust detection systems.\nOur human-annotated dataset regarding LLM-based self- detection and cross-detection with explanations will be public available after acceptance."}, {"title": "Explainability in Detection Models", "content": "Recent work on LGT detection has focused on improving explainability. Zhou et al. (2024) proposed to in- corporate factual consistency into detection models to enhance their interpretability, while Weng et al. (2024) explored mixed-initiative approaches that combine human expertise with automated models for better detection. These studies have made sig- nificant contributions to the field; however, they either depend heavily on expert input (Weng et al., 2024) or lack integration of explanation generation within the model itself (Zhou et al., 2024). Our ap- proach, in contrast, enables LLMs to autonomously generate both predictions and detailed explanations, making it a more scalable and transparent solution for detecting machine-generated content."}, {"title": "Ternary Classification", "content": "Traditional binary clas- sification methods, distinguishing between LGTs and HGTs, face limitations when texts exhibit char- acteristics that are ambiguous or could plausibly originate from either source. In such cases, adding an \"Undecided\" category provides a more nuanced framework for detection. This approach has several advantages over previous methods, which typically focus on mixed human-machine texts and fail to ac- count for uncertainties in text origin. For instance, the work by Lee et al. (2024b) revealed that current detectors struggle to identify \u201cmixed text\u201d, par- ticularly in cases where subtle modifications and style adaptability blur the distinction. Similarly, the Turing test conducted on online reviews (Frank et al., 2024) showed that even humans often can- not reliably distinguish between human-written and AI-generated content, pointing to the inherent diffi- culties in binary classification. Moreover, studies such as (Frank et al., 2024) and (Bhattacharjee and Liu, 2024b) further underscore the complexity in distinguishing between the two, suggesting that an \"Undecided\" class could allow for better handling of such ambiguity. Incorporating this third cate- gory not only enhances detection accuracy but also improves explainability, offering clearer insights into the decision-making process."}, {"title": "3 LLM-based Binary Classification on LGTs and HGTS", "content": null}, {"title": "3.1 Experimental Design", "content": "We selected six SOTA LLMs for text genera- tion and subsequent detection: GPT-40, GPT-40 mini (Hurst et al., 2024), Qwen2-72B, Qwen2- 7B (Yang et al., 2024), LLaMA3.3-70B, and LLaMA3.3-8B (Dubey et al., 2024). These LLMs were chosen for two main reasons. First, they repre- sent the latest advancements in LLM development, demonstrating strong generation and detection ca- pabilities. Second, the selection spans different series and model sizes, enabling a comparative analysis of performance across architectures and scales.\nTo construct the dataset, we first selected 1,000 HGTs from the publicly available M4GT-Bench dataset (Wang et al., 2024), ensuring a diverse range of topics, styles, and formats. Based on these selected HGTs, we then designed 1,000 prompts that align with the themes, structure, and style of the HGTs. Each LLM subsequently gener- ated a corresponding response for each of these prompts. Together, these LGTs and HGTs formed the benchmark used in this study. For each text, the LLMs were tasked to determine its source (LGTs or HGTs) and provide an explanation, as illustrated in Table 1."}, {"title": "Manual Annotation", "content": "To assess LLMs' ability to explain text origins and identify distinguishing features, three co-authors, who are undergraduate Computer Science students, manually evaluated the correctness of LLM-generated explanations. They determined whether each explanation was accurate or inaccurate. From seven datasets (6 SOTA LLMS + Human), 100 texts with the corresponding ex- planations per dataset were randomly selected for human evaluation. All annotators assessed the ex- planations provided by each model, which achieved a Fleiss' kappa (Fleiss, 1971) of 0.8387, indicating near-complete agreement. Annotation guidelines are detailed in Appendix C."}, {"title": "Evaluation Metrics", "content": "For evaluating the classifi- cation performance of the LLMs, the primary met- ric we used is the F1 score. To assess the quality of explanations, human evaluators reviewed the LLM-generated explanations and classified them as correct or incorrect. The F1 score was also used as the evaluation metric for explanation quality."}, {"title": "3.2 Binary Classification Results", "content": "We evaluated the performance of six LLMs across six datasets, as detailed in Table 2, which systemat- ically compares the detection capabilities of vari- ous LLMs for both LGTs and HGTs. The results demonstrate that GPT-4o achieves the best average detection performance across all datasets, showing relatively strong generalization capabilities. Larger parameter models generally exhibit significantly better detection performance than smaller ones, which suggests that these models are not merely making random guesses but are effectively identi- fying distinctive textual features.\nThe F1 scores in Table 2's diagonal direction show that LLMs within the same series consis- tently detect their own outputs more effectively than those from other LLM families. For exam- ple, LLaMA3.3 70B achieves the highest F1 score in its generated dataset, which indicates a height- ened sensitivity to its own text distribution com- pared to other LLMs. However, this specialization reduces cross-detection performance, as seen in Qwen2-7B's lower F1 on LLaMA-generated texts. While larger LLMs generally achieve better detec- tion across different LLMs, such as GPT-40, GPT- 40 mini, LLaMA3.3-70B and Qwen2-72B, their outputs are also more difficulty to distinguish by smaller LLMs, such as LLaMA3.3-8B and Qwen2- 7B.\nAdditionally, based on the human annotations of sampled 100 LGTs and 100 HGTs with expla- nations from each dataset, we observed that the detection and explanation results across different LLMs are not entirely consistent, as shown in Ta- ble 3. We noted that in some cases, the F1 score for explanations was higher than that for classification. This is because, in these cases, the explanation correctly identified the reasoning for attribution, but the final classification was incorrect. For in- stance, the difference in F1 scores between expla- nation and classification was particularly noticeable for LLaMA3.3-8B and Qwen2-7B, suggesting that these models struggle to truly comprehend the tex- tual features necessary for correctly determining the origin of generated texts, which results in lower detection performance.\nAs shown in Table 4, analysis of the annotators' results revealed that models are generally more accurate in attributing HGTs compared to LGTs. For example, while GPT-40 demonstrates higher accuracy (78 out of 100) in classifying HGTs, the false explanations account for more than 47%."}, {"title": "4 LLM-based Ternary Classification on LGTs and HGTS", "content": null}, {"title": "4.1 Experimental Setup", "content": "Using the same benchmark in Section 3, we prompted the LLMs for ternary classification and the prompt template is demonstrated in Table 5. The ground truth for the ternary classification was determined based on annotators' votes, where the three annotators were aware of the text's ori- gin (LLMs or human) and were asked to distin- guish between the ground truth and the \u201cUnde- cided\" category. This allowed for the evaluation of both the LLM's classification results and the explanations provided by the LLMs. The Fleiss' kappa (Fleiss, 1971) for the ternary classification annotations among the three annotators was calcu- lated as 0.7629, which indicates substantial agree- ment."}, {"title": "4.2 Ternary Classification Results", "content": "Table 6 presents the F1 scores of LLMs in the ternary classification setting. Comparing it with Table 3, we observe that introducing the \u201cUnde- cided\" category leads to overall performance im- provements across both classification and expla- nation tasks. Specifically, GPT-40 exhibits the most notable gains, improving from 73.48/67.04 to 79.73/72.04, indicating that a finer-grained clas- sification allows stronger models to better capture nuanced differences between LGTs and HGTs.\nMoreover, Figure 1 reveals how different models distribute predictions across the three categories. GPT-40 demonstrates a more balanced distribu- tion, with relatively lower misclassification rates for both HGTs and LGTs. In contrast, LLaMA3- 70B shows a stronger tendency to label texts as \"human-generated\", leading to a higher false posi- tive rate. Meanwhile, Qwen2-72B exhibits a more cautious classification approach, assigning a larger proportion of texts to the \u201cUndecided\" category, particularly for LGTs.\nA closer comparison between the binary and ternary classification results in Tables 3 and 6 sug- gests that the added \"Undecided\" category benefits models differently. While large-scale models like GPT-40 and LLaMA3-70B leverage this additional flexibility to improve both classification and expla- nation F1 scores, smaller models such as Qwen2- 7B show more mixed results, with only marginal improvements. This suggests that high-capacity models may be better equipped to handle ambigu- ous cases, while smaller models can struggle with the added complexity.\nOverall, these findings indicate that ternary clas- sification not only refines detection performance but also enhances the LLMs' ability to generate more meaningful explanations. The improvements are particularly evident in large-scale LLMs, which benefit from a more nuanced decision space."}, {"title": "5 Explainability of LLM-based Detectors", "content": null}, {"title": "5.1 Incorrect Explanation Attribution", "content": "Although LLMs can distinguish LGTs and HGTs, especially in self-detection settings, there are ex- planations that are incorrect via human evaluation. Normally, incorrect explanations in correctly clas- sified cases fall into three types: inaccurate fea- tures (misidentifying key attributes), hallucinations (citing nonexistent or contradictory features), and flawed reasoning (faulty logic despite a correct out- come). For misclassified texts, errors typically in- volve inaccurate features or hallucinations, which highlights the need to prioritize explanation accu- racy alongside detection performance to enhance trust in LLM-based detectors."}, {"title": "5.1.1 Inaccurate Features", "content": "Incorrect explanation attribution is often caused by relying on ambiguous, superficial, or misinter- preted features, as shown in the examples in Ta- bles 7 and 12 of Appendix A.\nIn Example 1 \"Ambiguous Features\", the model misclassifies a text on quantum entanglement as LGTs due to the use of technical jargon. How- ever, advanced topics can also be written by human experts, not just LLMs. This text was actually human-generated. Similarly, Example 2 \u201cSurface Features\" shows how the model links grammatical errors to machine authorship. Such mistakes are common among both native and non-native writ- ers and should not be sole indicators of LGTs. In fact, HGTs are more likely to contain grammatical errors.\nExample 3 illustrates a misjudgment where emo- tional complexity is falsely attributed exclusively to human writing. The model assumes nuanced emotional contrasts inherently reflect human au- thorship, overlooking modern LLMs' capability to simulate such depth. This case underscores the un- reliability of using emotional sophistication alone as a criterion to differentiate between HGTs and LGTS.\""}, {"title": "5.1.2 Hallucinations", "content": "Hallucinations occur when the model incorrectly attributes features to the text that either do not exist or are contrary to the actual content. In Example 4: Incorrectly Perceived Repetition, the model misin- terprets the repetition of ideas about gravitational waves as a sign of LLM authorship. The text does not exhibit excessive repetition, and the claim of a repetitive structure is a false attribution, likely due to biases in the model's training data."}, {"title": "5.1.3 Incorrect Reasoning", "content": "Incorrect reasoning occurs when relevant features are correctly identified but are misinterpreted, lead- ing to incorrect conclusions. Example 6 highlights a classification error rooted in inconsistent reason- ing. The model correctly identifies formal stylistic features but misapplies their significance. Enforc- ing a binary classification may lead to inconsis- tent reasoning in the model's inference process, as it forces an erroneous LLM label despite the am- biguity that could be better captured in a ternary framework."}, {"title": "5.2 Human Evaluation to Incorrect Explanations", "content": "The reasons for incorrect explanations from hu- man annotators are categorized into two scenarios: correct predictions with incorrect explanations and incorrect predictions with incorrect explanations. The results are summarized in Tables 8 and 9.\nFor cases where the model made correct predic- tions but provided incorrect explanations, Table 8 shows that the most prevalent reasons were inac- curate features and hallucinations. Inaccurate fea- tures, such as attributing the decision to vague or ir- relevant characteristics, accounted for a significant portion of errors across all LLMs. Hallucinations were also frequent, particularly for models like Qwen2-7B and GPT-40. Faulty reasoning, though less common, contributed to the proportion of in- correct explanations, highlighting inconsistencies in reasoning despite identifying correct features.\nFor cases involving both incorrect predictions and incorrect explanations, Table 9 indicates a sim- ilar distribution of error types, but with a higher prevalence of hallucinations. Annotators noted that models hallucinated key features, attributing the decision to features not present in the text, which compounded the issue of misclassification.\nOverall, the analysis reveals that hallucinations and reliance on inaccurate features are dominant sources of error in explanations, regardless of pre- diction accuracy. Addressing these issues requires further refinement of the interpretability mecha- nisms in LLMs, with a focus on grounding expla- nations in verifiable and relevant textual evidence."}, {"title": "6 Discussion", "content": null}, {"title": "6.1 Can Fine-Tuning Enhance Cross-LLM Detection Performance?", "content": "To explore whether fine-tuning improves cross- LLM detection, we fine-tuned Qwen2-7B on a training set consisting of 10,000 LLaMA3.3-8B-"}, {"title": "6.2 Enhancing LLM Detection through LLM Collaboration", "content": "We further explore whether the performance of LLM-based detection can improve via LLM's col- laboration. Table 11 shows that the performance of LLM-based detectors improves significantly when their judgments and explanations are com- plemented by another LLM counterpart. Specif- ically, the cross-detection of GPT-40 on Qwen-2 72B dataset has noticeable improvements in the classification and explanation F1 with the support of Qwen2-72B. We also find a similar trend, where Qwen2-72B benefits from GPT-40's support on the cross-detection settings. These findings indicate that LLM's collaboration can further improve the classification and explanation performance on the LLM counterpart's dataset, i.e., cross-detection."}, {"title": "7 Conclusion", "content": "We evaluated how well LLM-based detectors dif- ferentiate human- from LLM-generated text, fo- cusing on detection accuracy and explanation clar- ity. Overall, self-detection outperformed cross- detection, and detectors within the same model series tended to behave better. However, explana- tion quality lagged, often relying on flawed fea- tures, hallucinations, and poor reasoning. Distinct"}, {"title": "Limitations", "content": "This study is subject to several limitations. First, due to the limited number of API calls available for closed-source LLMs, the datasets used for gen- erating and detecting LLM-generated texts were constructed at a scale of 1,000 samples. As a re- sult, the types and variety of texts involved in the analysis may not be fully comprehensive, poten- tially introducing bias. Additionally, because the generation of explanations requires manual anno- tation, which is time-consuming, only a random sample of 100 texts per dataset could be selected for evaluation. This sample size may lead to biases in the evaluation of LLM-generated explanations.\nFinally, given the rapid advancements in LLM tech- nology, the detection and explanation capabilities of models are continually evolving. Therefore, it is crucial to periodically update our research focus and the models under study to ensure the results remain relevant and accurate."}, {"title": "Ethic Statements", "content": "All experiments were conducted using publicly available LLMs and datasets. For the datasets we constructed for the work, no any personal or pri- vate information is included. All the three human annotators are co-authors, so an research ethics re- view was not considered necessary. More details on how we used the human annotators can be found in Appendix C."}, {"title": "B Detailed Results after Fine-Tuning", "content": "We fine-tuned Qwen2-7B on a training set consist- ing of 10,000 LLaMA3.3-8B-generated LGTs and 10,000 HGTs from the M4GT-Bench dataset using Low-Rank Adaptation (LoRA)(Hu et al., 2021) for the binary classification task. After fine-tuning, the model's performance was evaluated on the overall dataset, with results indicating a decline in both detection and explanation accuracy, as shown in Table 13. Specifically, the fine-tuned model exhib- ited a decrease in both detection effectiveness and explanation clarity across all data."}, {"title": "C Annotation Guidelines", "content": "This appendix provides detailed instructions for the manual annotation tasks conducted in our study. The annotation process consists of two tasks: (1) a binary classification task to evaluate the accuracy of LLM-generated explanations, and (2) a ternary classification task where annotators evaluate both the correctness of the LLM's ternary classifica- tion judgments and the accuracy of its explanations based on known text sources."}, {"title": "Task 1: Explanation Accuracy Evaluation", "content": "Annotators will assess whether the explanation provided by the model correctly justifies its classification decision. Each explana- tion should be judged based on its logical consistency, relevance to the text, and whether it accurately reflects distinguishing features.\nColumns:\n\u2022 Text: The text sample to be classified.\n\u2022 Detection Result: The model's classification of the text as \"LLM-generated\" or \"Human-generated.\"\n\u2022 Model's Explanation: The explanation provided by the model for its classification decision.\n\u2022 Annotation: Label the explanation as \"Accurate\" or \"Inaccurate\" based on its reasoning quality.\nExample:\nText: \"In recent years, artificial intelligence has demonstrated remarkable progress, influencing numerous industries, including healthcare, finance, and creative writing. Many experts believe that this rapid advancement will continue to reshape the workforce and redefine human-machine collaboration.\"\nDetection Result: LLM-generated\nModel's Explanation: \"The highly structured argumentation and precise yet impersonal tone indicate that this text was likely machine-generated rather than composed by a human writer.\"\nAnnotation: Inaccurate (While structured argumentation is common in LLM-generated text, human authors can also produce similarly structured and objective writing.)"}, {"title": "Task 2: Ternary Classification with Explanation Evaluation", "content": "Annotators will classify each text as \u201cLLM-generated,\u201d \u201cHuman-generated,\u201d or \u201cUndecided,\u201d and evaluate whether the model's explanation correctly justifies the classification. The \"Undecided\" label applies when the text lacks sufficient distinguishing features.\nColumns:\n\u2022 Text: The text sample to be classified.\n\u2022 Detection Result: LLM' judgment of whether the text is \"LLM-generated,\u201d \u201cHuman-generated,\" or \"Undecided.\"\n\u2022 Model's Explanation: The explanation provided by the model.\n\u2022 Classification Annotation: Label whether the model's classification is \"Correct\" or", "Annotation": "Label the explanation as \u201cAccurate\u201d or \u201cInaccurate\u201d based on its reasoning quality.\nExample:\nText: \"Quantum mechanics, a field of physics that describes the behavior of particles at the atomic and subatomic levels, has led to groundbreaking discoveries such as quantum entanglement and superposition. These principles have paved the way for advancements in quantum computing and cryptography, revolutionizing modern technology.\"\nDetection Result: Undecided\nModel's Explanation: \"The text presents factual scientific content in a neutral tone, making it difficult to distinguish between human and machine authorship.\"\nClassification Annotation: Correct\nExplanation Annotation: Accurate (The explanation correctly justifies why the text is indistinguishable.)"}]}