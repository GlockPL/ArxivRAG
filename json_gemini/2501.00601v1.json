{"title": "DreamDrive: Generative 4D Scene Modeling from Street View Images", "authors": ["Jiageng Mao", "Boyi Li", "Boris Ivanovic", "Yuxiao Chen", "Yan Wang", "Yurong You", "Chaowei Xiao", "Danfei Xu", "Marco Pavone", "Yue Wang"], "abstract": "Synthesizing photo-realistic visual observations from an ego vehicle's driving trajectory is a critical step towards scalable training of self-driving models. Reconstruction-based methods create 3D scenes from driving logs and synthesize geometry-consistent driving videos through neural rendering, but their dependence on costly object annotations limits their ability to generalize to in-the-wild driving scenarios. On the other hand, generative models can synthesize action-conditioned driving videos in a more generalizable way but often struggle with maintaining 3D visual consistency. In this paper, we present DreamDrive, a 4D spatial-temporal scene generation approach that combines the merits of generation and reconstruction, to synthesize generalizable 4D driving scenes and dynamic driving videos with 3D consistency. Specifically, we leverage the generative power of video diffusion models to synthesize a sequence of visual references and further elevate them to 4D with a novel hybrid Gaussian representation. Given a driving trajectory, we then render 3D-consistent driving videos via Gaussian splatting. The use of generative priors allows our method to produce high-quality 4D scenes from in-the-wild driving data, while neural rendering ensures 3D-consistent video generation from the 4D scenes. Extensive experiments on nuScenes and in-the-wild driving data demonstrate that DreamDrive can generate controllable and generalizable 4D driving scenes, synthesize novel views of driving videos with high fidelity and 3D consistency, decompose static and dynamic elements in a self-supervised manner, and enhance perception and planning tasks for autonomous driving.", "sections": [{"title": "I. INTRODUCTION", "content": "Generating driving videos based on ego vehicle's trajectory is a critical problem in autonomous driving. Action-conditioned video generation allows autonomous vehicles to anticipate future scenarios, respond accordingly, and generalize beyond expert trajectories, which is crucial for the scalable training of self-driving models. To tackle this challenge, two series of works have emerged: reconstruction-based methods and generation-based methods. Reconstruction-based methods [1]\u2013[7] models 3D scenes from driving logs, and then action-conditioned visual observations can be generated through neural rendering techniques such as NeRF [8] or 3D Gaussian splatting [9]. These methods can synthesize 3D-consistent and photo-realistic visual observations, However, they heavily rely on well-annotated driving logs that include calibrated camera poses, object boxes, and 3D point clouds, which limits their scalability to unlabeled in-the-wild driving data. On the other hand, generation-based methods [10]\u2013[15] can learn from in-the-wild driving data and synthesize action-conditioned dynamic driving videos via image [16] or video diffusion models [17]. However, video generation suffers from poor 3D geometry consistency across frames, which can undermine the reliability of the synthesized visual observations for autonomous driving. Hence, synthesizing both generalizable and 3D-consistent visual observations for autonomous driving remains an open challenge.\nTo address this challenge, we introduce DreamDrive, a 4D scene generation approach for autonomous driving. Our key insight is to combine the generative power of video diffusion priors with the geometry-consistent rendering of 3D Gaussian splatting [9]. We elevate 2D visual references from video diffusion models into a 4D spatio-temporal scene, where the ego vehicle navigates and synthesizes novel-view observations through Gaussian splatting. Video diffusion priors enhance the generalization of our method, enabling 4D scene generation from in-the-wild driving data, while Gaussian splatting ensures 3D consistency during novel view synthesis. This approach allows DreamDrive to produce high-quality, 3D-consistent visual observations with strong generalization to diverse driving scenarios.\nAlthough intuitive, accurately modeling 4D scenes from generated visual references remains quite challenging. Unlike well-annotated driving datasets [18], [19], generated visual references lack crucial information such as camera poses, object locations, and depth data, which hampers 4D modeling. Furthermore, this issue is compounded by the inherent 3D inconsistency in video diffusion models, causing traditional Gaussian representations [9], [20] to overfit to the training views and fail on novel view synthesis. To address these problems, we introduce a self-supervised hybrid Gaussian representation. Our approach leverages time-independent Gaussians to model static backgrounds and time-dependent Gaussians for dynamic objects, combining them into a unified 4D scene. First, we propose a self-supervised approach that can decompose a scene into static and dynamic regions with only image supervision. Next, we introduce spatio-temporal clustering to group 3D Gaussians into static and dynamic Gaussian clusters, which effectively mitigates fake dynamics in 4D modeling. Finally, we optimize the Gaussian"}, {"title": "II. RELATED WORKS", "content": "Generative Models for Autonomous Driving. Generative models have shown significant potential in synthesizing future driving videos based on current actions. Recent works [10]\u2013[15], [21] have fine-tuned stable video diffusion models [17] on driving data, incorporating controls such as map, object, weather, and action to generate diverse driving scenarios. However, as these models operate in 2D, they struggle to capture the underlying 3D geometry of the world, leading to poor 3D consistency in the generated videos. In contrast, our method employs neural rendering of 4D scenes, ensuring the generated videos maintain 3D consistency.\nUrban Scene Reconstruction. Many papers [1]\u2013[7], [22] focus on reconstructing 3D or 4D urban scenes from driving logs, optimizing NeRF [8] or 3D-GS [9] based scenes using multiview image supervision. These methods can synthesize novel views based on driving trajectories. However, most approaches [1], [2], [4], [5], [22] rely heavily on annotated object boxes to track and model dynamic objects, limiting their ability to handle unlabeled driving logs. While some methods [3], [6], [7] use self-supervised techniques to separate dynamic objects, they still depend on well-calibrated camera poses and 3D data, making them less generalizable to in-the-wild driving scenarios. In contrast, our method removes the need for pose or 3D information, enabling accurate 4D scene modeling directly from visual references.\n4D Scene Generation. Many papers [23]\u2013[45] focus on 3D and 4D content generation, but most focus on object generation, which is not applicable to driving scenarios. Some works [25], [42]\u2013[45] have introduced diffusion priors for 4D scene generation. However, the 4D scenes in these approaches are limited to object-centric, small-scale scenes, making it difficult for them to generalize to large-scale, unbounded driving scenes with numerous dynamic objects. The most relevant work, [46], uses diffusion priors for 3D driving scene generation but relies solely on deformable 3D Gaussians, resulting in poor visual quality in novel view synthesis. In contrast, we propose a novel self-supervised approach to model 4D driving scenes with hybrid Gaussian representations, which demonstrates better generalization ability and visual quality in novel-view driving video synthesis."}, {"title": "III. METHOD", "content": "In this section, we introduce DreamDrive, a 4D spatio-temporal scene generation approach for autonomous driving. An overview of our method is shown in Figure 2. DreamDrive follows a 2D-3D-4D progressive generation process. We begin by leveraging video diffusion priors to generate 2D visual references, followed by Gaussian initialization to lift them into 3D. Next, we propose a novel self-supervised scene decomposition approach with a clustering-based grouping strategy to disentangle static and dynamic regions in the 4D spatio-temporal domain. Finally, we introduce hybrid Gaussian representations to model static structures and dynamic objects for 4D scene generation.\nProblem Definition. We study the problem of 4D scene generation. Given input controls, e.g., a single image $I_{ctrl}$ or a map with object locations $M_{ctrl}$, we aim to generate a 4D (3D+time) scene which is composed of a set of 3D Gaussians [9]: ${G_i | i = 1,...,N_t, t = 1,...,T}$, where $N_t$ is the number of Gaussians at each timestep $t$, and $T$ is the total timesteps of this 4D scene. Each 3D Gaussian is parameterized by its mean position $x \\in \\mathbb{R}^3$, a quaternion-based rotation $r \\in \\mathbb{R}^4$ and scaling $s \\in \\mathbb{R}^3$, an opacity value $a$, and a set of spherical harmonic (SH) coefficients $c$ to represent view-dependent color: $G(x,r,s, a, c)$. The generation process can be formulated as\n$G = F_{gen} (X_{ctrl}), X_{ctrl} \\in {I_{ctrl}, M_{ctrl}},$ (1)\nwhere $F_{gen}$ is our proposed 4D scene generation approach.\nWith this generated 4D scene representation, given any driving trajectory with camera poses $P_{traj} = {P_t | t = 1,...,T}$, we can synthesize a novel driving video $V = {I_t | t = 1,...,T}$ by splatting the 3D Gaussians $G_t$ at each timestep $t$ into an image $I_t$ with camera pose $P_t$:\n$I_t = F_{splat}(G_t, P_t),$ (2)\nwhere $F_{splat}$ is the 3D Gaussian splatting in [9]. Our method generates 4D driving scenes with diverse controls $X_{ctrl}$, and the neural rendering function $F_{splat}$ ensures the spatio-temporal consistency of synthesized driving videos.\nVideo Diffusion Priors. Video diffusion models are highly effective at modeling the temporal dynamics of visual data, but relying solely on them for trajectory-conditioned video generation can lead to 3D inconsistency, as they are designed for 2D image generation without considering the underlying 3D structure. In our method, we use video diffusion priors to generate initial visual references, which are then elevated to the 4D space for scene generation and 3D-consistent video rendering. Specifically, we employ video diffusion models [11], [15] trained on driving data to generate a sequence of reference images ${I_{ref} | t = 1,...,T}$ and extract latent features $Z_{ref}$ from the early layers to capture valuable visual dynamics for static-dynamic decomposition. The process is formally expressed as:\n$I_{ref}, Z_{ref} = F_{VDM}(X_{ctrl}),$ (3)\nwhere $F_{VDM}$ is the video diffusion model and $X_{ctrl}$ is the input control. $F_{VDM}$ provides visual references that guide"}, {"title": "4D scene generation", "content": "Since it can generate references from in-the-wild driving data, incorporating video diffusion priors improves the generalization of our approach.\nGaussian Initialization. Lifting generated images $I_{ref}$ into 4D space is quite challenging without camera poses and 3D information. Therefore, robust estimation of both camera parameters and 3D structure is crucial as a reliable initialization for 4D scene generation. While previous works use COLMAP [47] to estimate coarse 3D geometry, its sparse point clouds are insufficient for modeling large-scale and unbounded driving scenes. Instead, we employ an end-to-end multiview stereo network [48] to produce pixel-aligned dense 3D geometry and simultaneously recover camera poses ${P_{ref} | t = 1,...,T}$. Specifically, [48] generates dense, pixel-aligned 3D point clouds for each image. Camera intrinsics are estimated using the Weiszfeld algorithm [49], and camera extrinsics are computed by globally aligning the point clouds across frames. The aggregated point clouds form a dense scene-level point cloud, which is used to initialize 3D Gaussian parameters, yielding a set of Gaussians $G_{init}$. These 3D Gaussians are further enriched with pixel-aligned latent features $Z_{ref}$. The whole process can be expressed as:\n$G_{init}, P_{ref} = F_{MVS}(I_{ref}, Z_{ref}),$ (4)\nwhere $F_{MVS}$ is the multiview stereo network. This approach ensures accurate 3D scene geometry and camera estimation and serves as a robust initialization of 3D Gaussians.\nWith the initialized 3D Gaussians $G_{init}$, the next step is to model 4D spatio-temporal driving scenes containing both static backgrounds and dynamic objects. Previous works [2]\u2013[5] rely on annotated object boxes to track dynamic objects, limiting their generalization to unannotated data like $I_{ref}$. Other methods [6], [7], [46] use pure time-dependent Gaussians that change positions and shapes over time, but the 3D inconsistency in generated images often leads to overfitting and introduces fake dynamics, such as visual deformation in static structures when synthesizing novel views. To overcome"}, {"title": "Self-Supervised Scene Decomposition", "content": "these issues, we propose a novel hybrid Gaussian representation to model static and dynamic components separately. We divide the initial Gaussians $G_{init}$ into time-independent static Gaussians $G_{static}$ and time-dependent dynamic Gaussians $G_{dynamic}$, effectively modeling static structures and dynamic objects. This separation ensures that static structures remain consistent over time, mitigating fake dynamics while accurately capturing the movement of dynamic objects.\nA key challenge in hybrid modeling is separating static and dynamic regions without additional annotations. To tackle this, our key insight is that image error maps serve as effective indicators for distinguishing between static and dynamic regions. Specifically, we first optimize the entire scene by assuming all initial Gaussians $G_{init}$ are static. We then splat the optimized static Gaussians into static images $I'_{static}$:\n$I'_{static} = F_{splat}(G_{init}, P_{ref}).$ (5)\nNext, the error map at each timestep $t$ is computed as:\n$I_{err} = |I'_{static} - I_{ref}|.$ (6)\nThe pixels in $I_{err}$ with higher rendering errors indicate the regions that static Gaussians struggle to optimize, suggesting that these areas likely correspond to dynamic objects. Therefore, we can use $I_{err}$ as supervisory signals for scene decomposition. In particular, we train a network, $F_{score}$, that takes the initial Gaussians $G_{init}$ and their associated latent features $Z_{ref}$ as input, and outputs binary dynamic scores $S$ to classify each Gaussian as static or dynamic:\n$S = F_{score}(G_{init}, Z_{ref}),$ (7)\nThese scores are splatted into image planes using the Gaussian splatting function $F_{splat}$, and supervised with error maps $I_{err}$ using the binary cross-entropy loss $L_{bce}$:\n$L_{dec} = \\sum_{t=0}^{T}(L_{bce}(F_{splat}(S, P_{ref}), I_{err})).$ (8)"}, {"title": "Grouping with Gaussian Clusters", "content": "Since the splatting function $F_{splat}$ is differentiable, we can optimize the scoring network $F_{score}$ end-to-end using the image-based decomposition loss $L_{dec}$. Finally, we separate the initial Gaussians $G_{init}$ into static Gaussians $G'_{static}$ and dynamic Gaussians $G'_{dyn}$ by applying a threshold $\\tau$ to the predicted dynamic scores $S$:\n$G_{dynamic} = {G_{init} | S > \\tau}, G'_{static} = {G_{init} | S \\leq \\tau}.$ (9)\nUnlike previous methods [50]\u2013[57], our self-supervised approach doesn't require annotations or multiple passes, making it more scalable for large-scale driving scenes.\nDue to the inherent 3D inconsistencies in generated visual references, fake dynamics, such as local deformations in static structures, often appear in $I_{ref}$. This results in the incorrect assignment of dynamic Gaussians to static objects and negatively impacts 4D scene modeling and novel view synthesis. To improve the robustness of our scene decomposition, we introduce a novel cluster-based grouping strategy. Our key insight is that objects generally move as a whole, i.e. Gaussians in the same object are likely to have the same dynamic attribute. As we don't have object annotations, we instead introduce \"spatio-temporal clustering\" to group the Gaussians into clusters. If most Gaussians in a cluster are static, meaning that the whole part should be static, we assign static labels to all, even if some were initially classified as dynamic, and vice versa for dynamic clusters. The process can be expressed as\n$G_{static}, G_{dynamic} = F_{group}(G'_{static}, G'_{dynamic}),$ (10)\nwhere $F_{group}$ is the proposed grouping strategy. $F_{group}$ helps to rectify incorrect dynamic score predictions. We find this can efficiently reduce fake dynamics, leading to more accurate and consistent 4D scene modeling."}, {"title": "Hybrid Gaussian Representations", "content": "Scene decomposition enables us to represent static and dynamic components with distinct Gaussians. Static Gaussians $G_{static}$ model elements like roads and buildings, with parameters $G(x, r, s, a, c)$ that remain constant over time, ensuring accurate rendering of static structures. Dynamic Gaussians $G_{dynamic}$ model objects like cars and pedestrians, where Gaussian positions and shapes vary over time: $G_{dynamic} = {G_t | t = G(x_t,r_t, s_t, a, c)}$. We follow [20] and learn a deformation network $F_{deform}$ that takes the Gaussian positions $x$ and a timestep $t$ as input and predicts temporal offsets of the Gaussians: $(\\delta x, \\delta r, \\delta s)$:\n$(\\delta x, \\delta r, \\delta s) = F_{deform}(x,t),$ (11)\n$(x_t,r_t, s_t, a, c) = (x + \\delta x, r + \\delta r, s + \\delta s, a, c).$ (12)\nThese time-dependent dynamic Gaussians $G_{dynamic}$ accurately represent dynamic objects in 4D scenes.\nFinally, we combine $G_{static}$ and $G_{dynamic}$ into a 4D spatio-temporal scene and optimize their parameters by splatting them onto images $I_{render}$ at each timestep $t$:\n$I_{render} = F_{splat}({G_{static}, G_{dynamic}}, P_{ref}).$ (13)\nThe rendering loss can be computed as:\n$L_{render} = \\sum_{t=0}^{T}(L_1(I_{render}, I_{ref}) + L_{SSIM}(I_{render}, I_{ref})),$ (14)\nwhere $L_{SSIM}$ is the SSIM loss [58]. We jointly optimize Gaussian parameters and $F_{deform}$ based on the rendering loss $L_{render}$, leading to robust 4D scene modeling."}, {"title": "IV. EXPERIMENTS", "content": "This section investigates the following questions:\n\u2022 Can DreamDrive generate 4D driving scenes in a controllable and generalizable way? (Section IV-B)\n\u2022 Can DreamDrive synthesize novel-view driving videos with high fidelity and 3D consistency? (Section IV-C)\n\u2022 Can DreamDrive decompose static background and dynamic objects in a self-supervised manner? How does the decomposition help 4D scene modeling and novel-view driving video synthesis? (Section IV-D)\n\u2022 Can DreamDrive help onboard autonomous driving tasks such as perception and planning? (Section IV-E)"}, {"title": "A. Experimental Setup", "content": "We utilize two complementary datasets to assess the performance of our method across both controlled and in-the-wild driving scenarios, each presenting distinct challenges. For controlled driving scenarios, we use the nuScenes dataset [18], a large-scale real-world autonomous driving dataset comprising 1,000 driving sequences and approximately 34,000 key frames, all with accurately calibrated camera poses, maps, and object annotations. This makes it an ideal choice for validating our method in controlled settings. Following standard practice, we divide the dataset into training and validation sets. For in-the-wild driving scenarios, we curate 20 scenarios from various geographical regions using Google Street View. This benchmark allows us to evaluate the generalization ability of our approach. We leverage the video diffusion prior from [11] for the nuScenes benchmark and from [15] for the in-the-wild benchmark."}, {"title": "B. Controllable and Generalizable 4D Scene Generation", "content": "In Figure 3, we demonstrate the controllability of DreamDrive in generating 3D scenes. Our approach allows for fine-grained control over scene elements such as road layouts and object positions. This controllability stems from the synergy of generative models and reconstruction methods, providing the ability to manipulate individual elements not only in images but also in 3D while maintaining high fidelity. Figure 4 further illustrates DreamDrive's generalization ability. By using only in-the-wild images, DreamDrive successfully generates realistic 4D scenes from diverse geographical locations such as Japan, Australia, and the United States. Unlike traditional approaches that rely heavily on labeled datasets or precise calibration data, our method employs self-supervised learning to model 4D driving scenes without the need for exhaustive manual annotations. This adaptability allows the method to work across various sensory setups and eliminates the requirement for specialized data collection, demonstrating its robustness in diverse driving scenarios."}, {"title": "C. Novel-View Video Synthesis with 3D Consistency", "content": "Given a driving trajectory, DreamDrive synthesizes dynamic driving videos by rendering the generated 4D scenes into images. Figure 5 presents examples of novel-view driving videos generated by our approach, covering various driving maneuvers such as moving forward, turning left, and stopping. Unlike previous methods that struggle with geometric consistency when changing viewpoints, DreamDrive maintains spatial accuracy for static and dynamic elements, ensuring realistic and consistent driving video generation. Furthermore, as shown in Figure 6, compared to directly generating videos with diffusion models, DreamDrive offers more precise trajectory control and better 3D consistency by leveraging 4D scene generation and neural rendering. As shown in Table I, our method achieves the lowest FID of 45.59 and FVD of 374.02, significantly improving over previous methods like self-supervised street Gaussians [6] and MagicDrive3D [46]. These advancements are due to our self-supervised decomposition module, which accurately separates static backgrounds from dynamic objects for more precise scene representation."}, {"title": "D. Static-Dynamic Decomposition for 4D Scenes", "content": "The static-dynamic decomposition is crucial for effective 4D scene modeling in DreamDrive. Unlike prior methods that rely on annotated object boxes [4], [5] or treat the entire scene as dynamic [6], [20], [46], DreamDrive uses a self-supervised approach to segment moving objects from static environments (Figure 7), eliminating the need for costly annotations and improving scalability in diverse driving scenarios. As shown in Figure 8, deformable Gaussians [20], [46] often overfit to training views, producing poor results in novel-view synthesis. In contrast, DreamDrive employs a hybrid Gaussian representation, i.e., time-independent Gaussians for static backgrounds and time-dependent Gaussians for dynamic objects, which improves robustness and accuracy in motion capture, reduces fake dynamics in static background, and ensures consistent 4D scene modeling."}, {"title": "E. Training Support for Perception and Planning", "content": "DreamDrive improves the perception and planning capabilities of autonomous vehicles in multiple ways. For perception, it generates diverse 3D scenes from map layouts and object locations, and through neural rendering, synthesizes view-consistent images that serve as training data. To evaluate this, we generate 3D scenes from the nuScenes training set and use the synthetic images to train BEV segmentation models [59]. We evaluate our method by training the model on both synthetic and real data, to see if there is a performance improvement, as well as training on synthetic data alone, to see if it can match real data performance. As shown in Table II, using DreamDrive as data augmentation significantly improves BEV segmentation achieving 37.19 vehicle mIOU and 73.03 road mIOU. Even when trained solely on our generated data, DreamDrive achieves higher performance than baseline methods [21], [60], demonstrating the quality of our generated data.\nDreamDrive can also help planning in autonomous driving. Neural motion planners can be trained on synthetic data, and since our method generates 4D scenes, we can further optimize the planning trajectories by checking their collisions with 3D Gaussians in the spatio-temporal domain (Figure 9). To validate this, we optimize the planned trajectories in [61] by minimizing the cost functions in [62], treating filtered 3D Gaussians as occupied points. Results in Table III demonstrate that our method could reduce the collision rate by 25%."}, {"title": "V. CONCLUSION", "content": "In this paper, we present DreamDrive, a novel 4D scene generation approach for autonomous driving that combines the generative power of video diffusion models with the geometric consistency of 3D Gaussian splatting. Using hybrid Gaussian representations, our method accurately models both static and dynamic elements in 4D driving scenes without manual annotations. Experiments show DreamDrive generates high-quality, geometry-consistent driving videos, generalizes to diverse driving scenarios, and enhances perception and planning tasks in autonomous driving."}]}