{"title": "Reflex-Based Open-Vocabulary Navigation without Prior Knowledge Using Omnidirectional Camera and Multiple Vision-Language Models", "authors": ["Kento Kawaharazuka", "Yoshiki Obinata", "Naoaki Kanazawa", "Naoto Tsukamoto", "Kei Okada", "Masayuki Inaba"], "abstract": "Various robot navigation methods have been developed, but they are mainly based on Simultaneous Localization and Mapping (SLAM), reinforcement learning, etc., which require prior map construction or learning. In this study, we consider the simplest method that does not require any map construction or learning, and execute open-vocabulary navigation of robots without any prior knowledge to do this. We applied an omnidirectional camera and pre-trained vision-language models to the robot. The omnidirectional camera provides a uniform view of the surroundings, thus eliminating the need for complicated exploratory behaviors including trajectory generation. By applying multiple pre-trained vision-language models to this omnidirectional image and incorporating reflective behaviors, we show that navigation becomes simple and does not require any prior setup. Interesting properties and limitations of our method are discussed based on experiments with the mobile robot Fetch.", "sections": [{"title": "1. Introduction", "content": "Various navigation methods for robots have been developed so far. These methods often require prior learning and data collection, primarily through techniques such as Simultaneous Localization and Mapping (SLAM) and reinforcement learning. On the other hand, due to the high adaptability and responsiveness, behavior-based robot navigation methods have been developed for a long time. In this study, we consider performing open-vocabulary reflex-based navigation by combining these behavior-based navigation approaches with the advancements in current pre-trained vision-language models. We aim to achieve navigation in its simplest form without any prior learning, SLAM, or similar techniques, as illustrated in Fig. 1. For this purpose, we leverage omnidirectional cameras. By segmenting omnidirectional images and applying pre-trained vision-language models, we enable a reflex-based control that moves in the most appropriate direction according to instructions, eliminating the need for complex exploration actions including trajectory generation. Furthermore, we demonstrate that using multiple pre-trained vision-language models allows for more suitable navigation. Unlike previous research where maps or policies were generated in advance, this study intentionally avoids these steps and focuses on discussing how to achieve open-vocabulary navigation in the simplest manner possible. While each process itself is not new, combining them enables novel open-vocabulary reflex-based navigation.\nThis study is organized as follows. In Section 2, we discuss related navigation research based on map generation, reinforcement learning, and omnidirectional cameras. In Section 3, we describe the expansion of the omnidirectional image, the application of the multiple vision-language models, and the"}, {"title": "2. Related Works", "content": null}, {"title": "2.1 Navigation with Map Construction and Reinforcement Learning", "content": "Most of the robot navigation methods are based on Simultaneous Localization and Mapping (SLAM) [1, 2] or reinforcement learning [3]. In addition, the number of studies of open-vocabulary navigation has been increasing in recent years [4\u20139]. Embodied Question Answering (EmbodiedQA) [4] performs reinforcement learning-based path planning in which the robot searches for answers to questions in the simulation space. LM-NAV [5] combines Large Language Model (LLM), Vision-Language Model (VLM), and Visual Navigation Model (VNM) for path planning. However, EmbodiedQA [4] requires reinforcement learning in simulations, and LM-NAV [5] requires a large number of observations of the current environment for the construction of VNM. The same is true for methods such as CLIP-Fields [6], VLMaps [7], CLIP on Wheels [8], and ConceptFusion [9], and prior learning or data collection is indispensable. Therefore, there are few methods that allow real-world robots to start operating immediately in an environment with no prior knowledge."}, {"title": "2.2 Navigation with Omnidirectional Camera", "content": "The omnidirectional camera provides a uniform view of the surroundings and can be used for various tasks. Kobilarov et al. [10] has achieved human tracking using an omnidirectional camera and Laser Range Finder (LRF), and Markovi\u0107 et al. [11] has achieved dynamic object tracking. Rituerto et al. [12] has developed SLAM using an omnidirectional camera, Winters et al. [13] has developed navigation using a topological map with an omnidirectional camera, and Caron et al. [14] has realized visual servo-ing using an omnidirectional camera. On the other hand, these are not yet strongly linked to pretrained vision-language models. In this study, we apply the characteristic that the omnidirectional camera al-"}, {"title": "2.3 Reflex-Based Navigation", "content": "In this study, \u201creflex\u201d refers to the execution of low-level control such as direct joint angle and body velocity adjustments at a relatively fast frequency based on obtained sensory information. It also denotes a mechanism that directly associates sensory input with control command without requiring prior knowl-edge. Reflex-based robot navigation has been developed for a long time [17\u201322]. They have been mainly studied from the perspectives of fuzzy logic [20, 21], subsumption architecture [17, 18], morphological computation [22], and reinforcement learning [20]. On the other hand, their primary purposes are the movement to target positions represented as coordinates and collision avoidance, and open-vocabulary navigation is far from their scope."}, {"title": "3. Reflex-Based Open-Vocabulary Navigation Using Omnidirectional Camera and Pre-Trained Vision-Language Models", "content": null}, {"title": "3.1 Omnidirectional Camera", "content": "In this study, we use Insta360 Air Camera (Arashi Vision Inc.), which is equipped with two fisheye lenses (front and rear). This camera has a combination of two fisheye lenses that can provide a 360-degree field of view. Here, the two fisheye images are expanded and combined together [23]. This process consists of four steps. First, fisheye lens intensity compensation is performed. In order to compensate for the optical phenomenon of vignetting, the pixel intensity is modified according to the distance from the center of the lens. Next, fisheye unwarping is performed. In order to produce a natural photographic image, each pixel is expanded by a geometric transformation in the order of unit sphere and square image. Then, the geometric misalignment between the two images is minimized from manual annotation in the overlapping regions. Finally, the two images are blended. The fisheye images before and after these steps are shown in Fig. 2. There are many unnecessary areas such as the ceiling and the robot's head, at the top and bottom of the 2000\u00d71000 expanded image. If this image is input to the vision-language models as it is, the information will become biased. Therefore, in practical use, we remove the lower and upper areas of the image, making it a 2000\u00d7500 image V as shown in the lower figure of Fig. 2."}, {"title": "3.2 Application of Pre-Trained Vision-Language Models", "content": "In this study, we use CLIP [15] and Detic [16] as pre-trained vision-language models. CLIP is a method to transform images and languages into vectors in the same embedding space, and Detic is a recognizer capable of detecting various classes of objects from images. Current VLMs are capable of four tasks: Generation Task, Understanding Task, Retrieval Task, and Grounding Task [24]. Among these, models like OFA [25] and BLIP2 [26], which are capable of Generation Task and Understanding Task, are relatively heavy in processing and not suitable for reflex-based control. On the other hand, Retrieval Task and Grounding Task have low computational cost and are suitable for this study. Models capable of Retrieval Task include CLIP [15] and ImageBind [27], while models capable of Grounding Task include Detic [16] and ViLD [28]. Amog these models, we selected the representative models CLIP and Detic. Note that, for Detic, we use the object classes in LVIS [29] as objects to be detected.\nFirst, the image obtained by Section 3.1 is split into $N_{split}$ pieces. Here, there can be overlaps between the pieces. Next, CLIP and Detic are applied to the obtained split images $V_i$ ($1 \\leq i \\leq N_{split}$). Let $v_i^{clip}$ be a vector obtained by applying CLIP to $V_i$. From Detic, labels and bounding boxes of various objects are obtained. The detected labels are sorted in descending order by the size of the bounding box, and are separated by commas to form a sentence (e.g. \"table, monitor, monitor, monitor, apple, knife\"). This"}, {"title": "3.3 Mapping from Linguistic Instruction to Motion", "content": "We map $a_i^{\\{clip,detic\\}}$ obtained in Section 3.2 to the actual robot motion (Fig. 4). First, we calculate the following evaluation value $e_i$ in order to resolve the problem that $a_i^{clip}$ and $a_i^{detic}$ each have its own strengths and weaknesses as described in Section 3.2.\n$e_i = a_i^{clip} a_i^{detic}$ (5)\nThe minimum value of $a_i$ is set to 0.1 as described in Section 3.2 to ensure that $e_i$ does not become 0 even when either $s_i^{clip}$ or $s_i^{detic}$ is very small. By using such metrics, we accommodate scenarios where one of $s_i^{\\{clip,detic\\}}$ is small while the other is large. Next, we retrieve $N_{extract}$ number of $e_i$ in descending order. Here, let $b_i$ be the unit direction vector toward which the center point of the image $V_i$ faces, and let $b_{ij}$ be $i$ of the $j$-th largest $e_i$ ($1 \\leq j \\leq N_{extract}$). The direction vector $b$ that the robot should move toward"}, {"title": "3.4 Other Settings", "content": "In this study, we conduct experiments using the mobile robot Fetch. Since the robot is equipped with LRF, it can be constrained not to move in the direction of obstacles. Of course, it is possible to provide the robot with a bumper and a contact sensor, and to construct another reflective behavior for obstacle avoidance. By incorporating obstacle avoidance, the robot can stop appropriately without going too far in the target direction."}, {"title": "4. Experiments", "content": "In this study, we first demonstrate the performance of our method quantitatively in a small area sur-rounded on all sides. Then, we show a series of navigation experiments using our method in a larger space. In this study, we set $N_{split}$ = 8, $C_{extract}$ = 2, $C_{thre}$ = 0.6 [rad], and $k$ = 0.5, and the control of Section 3.3 is operated at 10 Hz. With $N_{split}$ = 8, inference is performed at a maximum of 12 Hz for CLIP and 5 Hz for Detic on a machine equipped with an i7-6850K CPU and an Nvidia GeForce RTX 3090 GPU. While inference time for CLIP increases proportionally to $N_{split}$, inference time for Detic remains"}, {"title": "4.1 Basic Experiment", "content": "The environmental setup for the basic experiment is shown in Fig. 5. An area is surrounded by shelves and walls on all four sides and is about 2.5\u00d71.6 m in size. The footprint size of Fetch is about 0.6\u00d70.6 m. In this experiment, we prepared three instructions: kitchen - \u201cGo to the kitchen\", microwave - \"Please look at the microwave oven\", and desk - \"See the desk with chairs\". The kitchen, microwave oven, and desk with chairs are arranged as in Fig. 5. In this experiment, we compare three methods: the proposed method ALL, CLIP using only CLIP with $e_i = a_i^{clip}$, and Detic using only Detic with $e_i = a_i^{detic}$. The results of five navigation experiments for each instruction using each of the three methods are shown in Fig. 6. The left figures show the trajectories of Fetch for each of the five navigation experiments. Note that Fetch is stopped when it touches the walls/shelves, or once 30 seconds have passed since the start of the experiment. Ideally, the robot should move toward \"target\" in Fig. 6, which is the center of the position indicated by the instruction, with \"origin\" as the initial position. However, in some cases, the error does not need to be zero because \u201ckitchen\u201d or \u201cdesk\" can refer to a large area, but the error should be as small as possible. The right figure of Fig. 6 shows the average and variance of the error between the robot's final position and the target position.\nFor kitchen, we can see that the robot moves correctly toward the kitchen when using the methods of CLIP and ALL. On the other hand, for Detic, the robot meanders and does not move in the expected direction. The final position error of Detic is larger than those of CLIP and ALL. Although the error of CLIP is slightly smaller than that of ALL, the performance is considered to be almost the same because of the wider area of the kitchen as mentioned above. For microwave, we can see that the robot moves correctly toward the microwave oven in the case of ALL. In the case of Detic, the robot moves toward the microwave oven to some extent, but its performance is not as good as that of ALL due to meandering. On the other hand, in the case of CLIP, the robot moves toward the kitchen and its performance is significantly worse. For desk, the performance is high in the order of ALL, CLIP, and Detic. It can be seen that the performance of ALL is the best throughout all the experiments."}, {"title": "4.2 Advanced Experiment", "content": "We conducted a series of navigation experiments in a larger space. By accepting continuous instruc-tions in a wide space, it demonstrates the possibility of more practical navigation. The instructions were changed to the following: (1) \u201cLook at the large TV display on the wooden table\u201d, (2) \"Look at the"}, {"title": "5. Discussion", "content": "The results obtained are summarized below. First, the basic experiment shows that the robot can move to the appropriate position from linguistic instructions by splitting the expanded omnidirectional image, ap-plying multiple pre-trained vision-language models, and controlling the wheels reflexively. ALL, which combines CLIP and Detic outputs, enables navigation with higher accuracy compared to either CLIP or Detic. As can be seen from the preliminary experiment in Section 3.2, CLIP alone can extract only global features of the image, while Detic alone can extract only local features of the image. The combi-nation of these two models enables stable navigation. In addition, the robot moved without hesitation in the case of CLIP, while it has shown meandering movements in the case of Detic. This may be because the recognition results of Detic differ greatly from step to step. Next, the advanced experiment shows that navigation is possible even when the linguistic instructions are changed continuously. By adding various modifications to the linguistic instructions, it is possible to distinguish between similar objects and move in the appropriate direction. Also, we found that even when the robot is instructed to move toward an object that is not directly visible, it first moves in the direction in which the object is generally expected to exist, finds the indicated object, and then moves toward it. This is an interesting feature of the proposed system.\nLimitations and future prospects of this study are described. First, several parameters in our method"}, {"title": "6. Conclusion", "content": "In this study, we have described a reflex-based open-vocabulary navigation system using an omnidirec-tional camera and multiple pre-trained vision-language models. It does not require any prior knowledge, including map construction and learning. By using an omnidirectional camera, the robot can obtain in-formation about its surroundings as a whole, and by splitting the image and applying pre-trained vision-language models to each image, the robot can determine which direction is most consistent with the current linguistic instructions. By mapping this direction to the robot's movements, we show that open-vocabulary navigation can be constructed in a simple reflex-based form. Moreover, by using multiple types of vision-language models, the robot can respond to a wider variety of linguistic instructions. We expect various applications of this idea in the future. On the other hand, there are cases where the system does not work well depending on the structure of the room or the objects to be recognized, and we would like to consider a more adaptive robot system that can respond to such situations without the need for prior knowledge."}]}