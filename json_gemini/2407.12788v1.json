{"title": "SS-ADA: A Semi-Supervised Active Domain Adaptation Framework for Semantic Segmentation", "authors": ["Weihao Yan", "Yeqiang Qian", "Yueyuan Li", "Tao Li", "Chunxiang Wang", "Ming Yang"], "abstract": "Semantic segmentation plays an important role in intelligent vehicles, providing pixel-level semantic information about the environment. However, the labeling budget is expensive and time-consuming when semantic segmentation model is applied to new driving scenarios. To reduce the costs, semi-supervised semantic segmentation methods have been proposed to leverage large quantities of unlabeled images. Despite this, their performance still falls short of the accuracy required for practical applications, which is typically achieved by supervised learning. A significant shortcoming is that they typically select unlabeled images for annotation randomly, neglecting the assessment of sample value for model training. In this paper, we propose a novel semi-supervised active domain adaptation (SS-ADA) framework for semantic segmentation that employs an image-level acquisition strategy. SS-ADA integrates active learning into semi-supervised semantic segmentation to achieve the accuracy of supervised learning with a limited amount of labeled data from the target domain. Additionally, we design an IoU-based class weighting strategy to alleviate the class imbalance problem using annotations from active learning. We conducted extensive experiments on synthetic-to-real and real-to-real domain adaptation settings. The results demonstrate the effectiveness of our method. SS-ADA can achieve or even surpass the accuracy of its supervised learning counterpart with only 25% of the target labeled data when using a real-time segmentation model. The code for SS-ADA is available at https://github.com/ywher/SS-ADA.", "sections": [{"title": "I. INTRODUCTION", "content": "IMAGE semantic segmentation is a crucial perception task in intelligent transportation system [1]\u2013[6], facilitating various downstream tasks such as drivable area detection, semantic-based simultaneous localization and mapping. Recent years have seen remarkable progress in semantic segmentation, driven by the availability of large-scale annotated datasets and advancements in deep learning. However, deep learning-based semantic segmentation methods require extensive annotated datasets for training, and obtaining pixel-wise labels is costly. For instance, the labor-intensive annotation process takes approximately 1.5 hours for a single Cityscapes image, and up to 3.3 hours for an image captured under adverse weather conditions [7], [8].\nActive learning methods employ a human-in-the-loop mechanism to enhance model performance using a limited amount of annotated data. These methods develop acquisition strategies to select informative samples based on uncertainty sampling, diversity sampling, or a combination of both [13]\u2013[15]. Building on active learning, active domain adaptation (ADA) methods aim to transfer knowledge from a labeled source domain to an unseen target domain [15]\u2013[19]. ADA methods leverage the abundance of synthetic data generated from driving simulators to facilitate model training [20]\u2013[22]. However, the incorporation of ADA into semi-supervised semantic segmentation remains under-explored. Meanwhile, pixel or region-based ADA methods suffer from class imbalance problem [15]. As the labeling budget exceeds a certain threshold, such as 10%, the model\u2019s performance tends to gradually deteriorate [15], [23], as highlighted in Figure 1. Research by [23] revealed that applying all labels in domain adaptation often results in the majority of labels belonging to a few dominant classes, such as road, building, and vegetation. This issue limits the effectiveness of these methods, leading to the use of only a small proportion of annotations (e.g., 5%) in ADA experiments, which constrains further reduction of the gap between semi-supervised and supervised accuracy.\nIn this paper, we propose a novel semi-supervised active domain adaptation framework named SS-ADA for semantic segmentation, which integrates active learning into semi-supervised semantic segmentation methods. The framework consists of three core modules: a semi-supervised learning module, an active learning module, and an IoU-based class weighting module. The semi-supervised learning module leverages unlabeled data from the target domain, while the active learning module selects informative samples from the unlabeled data for manual annotation. The Intersection-over-Union (IoU) based class weighting strategy assigns greater weights to classes with lower IoU to alleviate the class imbalance problem. We conduct extensive experiments on synthetic-to-real and real-to-real domain adaptation settings. The experimental results demonstrate that SS-ADA achieves accuracy comparable to that of fully supervised training using only 25% of labeled data from the target driving scene. This significantly reduces the annotation requirements for deploying semantic segmentation models in target driving scenarios. Our contributions are summarized as follows:\n\u2022 We propose a novel semi-supervised active domain adaptation framework, SS-ADA, for semantic segmentation.\n\u2022 SS-ADA introduces active learning into semi-supervised segmentation methods to select the more informative unlabeled samples for human annotation and model training.\n\u2022 We design an IoU-based class weighting strategy based on the annotation from active learning. It assigns higher training loss weights to classes with lower IoU and vice versa, alleviating the class imbalance problem.\n\u2022 Extensive experiments on synthetic-to-real and real-to-real domain adaptation settings demonstrate the effectiveness of SS-ADA. It achieves accuracy comparable to fully supervised learning counterparts using only 25% of labeled data in the target domain."}, {"title": "II. RELATED WORK", "content": "Semantic segmentation is a pivotal perception task in intelligent transportation system [1]\u2013[6]. One category of methods aims at enhancing segmentation accuracy through robust model architectures. For instance, CMX proposes a universal transformer-based cross-modal fusion architecture for RGB-X semantic segmentation [6]. Another category of research focuses on balancing inference speed and model precision. SegTransConv is based on the hybrid architecture of CNN and transformer for real-time semantic segmentation of autonomous vehicles [4]. BiSeNet achieves this balance by integrating a spatial path and a context pathcitebisenet. In our experiments, we adopt the BiSeNet for real-time inference.\nSemi-supervised semantic segmentation methods aim to improve the segmentation performance using a limited amount of labeled data and a large-scale unlabeled data directly from the target domain. These methods can be mainly divided into two categories: consistency regularization and entropy minimization. Consistency regularization methods enforce the model to produce consistent predictions on unlabeled data under perturbations such as data augmentation, mixed samples, or feature perturbations [11], [24], [25]. Entropy minimization methods focus on assigning pseudo-labels to unlabeled target data and using them for self-training, such as in ST++ and USRN [26], [27]. Recently, CorrMatch has used correlation matching to discover more accurate high-confidence regions [28]. Despite making steady progress in recent years, semi-supervised methods still fall short of the accuracy achieved by supervised learning counterparts due to noisy pseudo-labels and a lack of accurate semantic guidance. Additionally, these methods often overlook the utilization of label-rich source domains and the selection of informative samples.\nDomain adaptation (DA) methods aim to transfer the knowledge learned from a labeled source domain to a target domain with different distribution. Recent advancements in DA for semantic segmentation primarily focus on unsupervised domain adaptation [29]\u2013[32]. They assume that the target domain has no labeled data and the corresponding performance still lag far behind supervised learning counterparts.\nResearchers have turned to active learning to improve adaptation performance with human annotation. Active learning aims to maximize model performance with limited labeled data by selecting the more valuable samples for annotation [33]. For example, Gao proposes an active and contrastive learning framework for fine-grained off-road semantic segmentation [16]. LabOR introduces a novel point-based annotation strategy with an adaptive human-in-the-loop pixel selector but overlooks the contextual relationships between pixels in an image [34]. RIPU designs a region-based acquisition strategy that uses region impurity and prediction uncertainty to select regions that are uncertain in prediction and diverse in spatial adjacency [15]. Subsequently, D2ADA employs density-aware measurements to acquire label annotations [19]. HALO introduces a hyperbolic radius into the acquisition strategy for more effective data annotation and highlights the class imbalance problem with increasing budget [23].\nCurrent ADA methods for semantic segmentation are dominant by pixel or region-level acquisition strategies. However, these forms are not suitable for human labeling, which may not actually reduce the difficulty and cost of annotation [35]. Moreover, most existing ADA methods neglect the potential of unlabeled data for training, which is available and usually large-scale in driving scenarios. In this paper, we introduce active learning into semi-supervised semantic segmentation methods to select informative samples for human annotation and model training. We adopt an image-based acquisition strategy and annotation form to preserve contextual information."}, {"title": "III. METHOD", "content": "1) Data representation: Let the labeled data in the source domain be denoted as D = (x, y), where x \u2208 R3\u00d7H\u00d7W and y \u2208 RH\u00d7W present the image and label, H and W are the height and width, respectively. The images in the target domain are initially without annotations and are denoted as Dux. After human annotation, the target labeled data is represented as D\u2081 = (x, y\u0142).\n2) Semi-supervised semantic segmentation: Both D and De in the target domain are used for model training. For D, the supervised cross-entropy loss can be applied directly. To utilize the potential of Du, consistency regularization loss, entropy minimization loss or other semi-supervised learning methods can be employed. Denote the model\u2019s prediction for xu as pu, the general loss function for semi-supervised semantic segmentation is defined as follows:\n$$L_{semi} = L(p_t, y_t) + \\lambda L(p^x_u)$$\nwhere L(p) is the unsupervised loss designed by semi-supervised semantic segmentation methods, \u03bb is the coefficient for loss balancing, L(p, y) is usually the cross-entropy loss:\n$$L(p, y) = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{(i,j)} \\log p_{(i,j)}$$\nwhere N is the number of pixels, C is the number of classes. i is the pixel index, and j is the channel index.\n3) ADA for semantic segmentation: In active learning, part of the samples in De are selected for human annotation to obtain the target labeled data D = (x1, y1). The selection and annotation are conducted at N active learning trigger epoches Tac = [Ta1, Ta2,..., Tan] to acquire a total of Ns samples. The goal of ADA for semantic segmentation is to leverage D and D to train a segmentation model that performs well on the target domain. Denote the model\u2019s prediction results for x and x as p and pt, respectively. The general loss function of ADA for semantic segmentation is defined as follows:\n$$L_{active} = L(p'_s, y'_s) + L(p^t_l, y^t_l)$$\nwhere L(p, y) and L(pt, y\u0142) are usually cross-entropy losses calculated according to Equation (2).\nThe overall training framework of SS-ADA for semantic segmentation is shown in Figure 2. SS-ADA has three key components: the semi-supervised learning module, the active learning module and the IoU-based class weighting. The semi-supervised learning module is designed to leverage the source labeled data D, target labeled data D, and target unlabeled data Du for model training, fully utilizing the available data, especially the potential of unlabeled target data. The training of the segmentation model is primarily conducted within the semi-supervised module.\nWhen the training epoch reaches the specified Tac rounds, the active learning module is triggered. It evaluates the value of the remaining unlabeled target domain data Du and selects a portion of them based on the acquisition strategy for manual annotation. The selected images and corresponding annotated labels are added to D for subsequent calculation and training.\nAfter D is updated in active learning module, the IoU-based class weighting strategy is applied. It calculates the class-wise IoU of the data in D and determines the loss weights for the semi-supervised learning module. Following this, the training process goes back to semi-supervised module with the updated D, Du, and class weights.\nThese three modules work together to improve the model\u2019s performance in the target domain. The details of these components are described in the following sections.\nThe data used for semi-supervised learning includes source labeled data D, target labeled and unlabeled data D and Dr. Since the images from the target domain lack annotations initially, the semi-supervised learning process cannot start directly. Therefore, we uniformly randomly sampled 1% of the target data for human annotation, forming the initial target labeled data pool D. Optionally, UDA methods can also be utilized for model initialization. Once the active learning module is triggered, a portion of the data in Duis annotated and added to D, allowing the model to be trained normally within the semi-supervised learning module.\nFor the labeled images x and xt, they pass through the network and obtain their predictions p and pr. The cross-entropy losses L(p, y) and L\u2081(p, y) are calculated for the predictions and labels according to Equation (2).\nFor the unlabeled images xt, we use the weak-to-strong consistency training inspired by [11], [36]. The core idea is to apply different levels of perturbations to the images, including image-level perturbations and feature-level perturbations, ensuring that the model produces consistent predictions for the perturbed image pairs. Denote the weak and strong image perturbations as gw and gs, respectively. The weak perturbations gw include random resizing, cropping and random horizontal flipping. The strong perturbations gs consist of color jitter, grayscale conversion, Gaussian blur and Cutmix [25]. Denote the feature perturbation as gf, which refers to applying the Dropout [37] to the features extracted by the encoder.\nIn the \u201cImage perturb\u201d step in Figure 2, one instance of gw and two instances of gs are applied to xt, resulting in x, x and x2. The corresponding model predictions of these perturbed images are denoted as p\u00eb, p\u00e5\u00b9 and p2, respectively. To construct a broader perturbation space, the feature perturbation gf is applied to the features of x extracted by the encoder in the \u201cFeature perturb\u201d step in Figure 2, additionally producing the prediction pfp. Following previous UDA methods [30]\u2013[32], p is used to generate the pseudo-label y for the unlabeled data based on the confidence threshold ct:\n$$y^u = argmax_{dim=C}(p^t_w) [max_{dim=C}(p^t_w) > C_t]$$\nwhere argmax(pt_w) and max (pt_w) denote the prediction and the confidence of the model, respectively. Ct is within the range of [0,1]. The cross-entropy losses L\u2021\u00ba (pfp, yu),"}, {"title": "D. Active Learning Module", "content": "The active learning module is triggered at specified training epochs Tac = [Ta1, Ta2,...,TaN] and select a total of N images from the target unlabeled data De for manual annotation. These annotated images and labels are then incorporated into the target labeled data D. In our experiments, the triggering epochs for active learning are set within the first half of the total training epochs. The number of samples needed for annotation is uniformly distributed among these sampling epochs. Specifically, at training epoches [Ta1, Ta2,..., TaN], the active learning module will select unlabeled samples from Du for manual annotation and update D.\nThe images from Du are fed into the trained segmentation model from semi-supervised learning module, and prediction logits p are obtained. Then, the images are ranked and selected based on the acquisition strategy. The acquisition strategy in active learning module is crucial for acquiring the informative samples. In SS-ADA, we adopt the image-based acquisition strategy, which is more suitable for manual labeling than pxiel and region-based ones, as shown in Figure 3. The value of the image is evaluated based on the model\u2019s uncertainty. We use classic metrics such as entropy and confidence to assess the uncertainty of the model\u2019s predictions. The image-level entropy Ent(p) and confidence Con(p) are defined as:\n$$Ent(p) = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} p_{(i,j)} \\log p_{(i,j)}$$\n$$Con(p) = \\frac{1}{N} \\sum_{i=1}^{N} max_{dim=C} (p)$$\nwhere p presents the prediction logits after applying softmax, with the shape of (C, H, W). C is the number of classes, H and W are the height and width of the image, and N is the number of pixel. The image-level entropy and confidence are calculated by averaging the pixel-level entropy and confidence of the predictions. If Ent(p) is used as the acquisition strategy, the images with the highest entropy are selected. If Con(p) is used, the images with the lowest confidence are selected. The selected images are annotated by humans and added to D."}, {"title": "E. IoU-based Class Weighting", "content": "To alleviate the class imbalance problem in the target domain, we design a class weighting strategy based on the IoU of labeled data in the target domain D, obtained from the active learning module. It adjusts the weights of each class in the loss function during training, assigning higher weights to classes with lower IoU and vice versa. Previous methods typically calculate weights based on class frequency:\n$$F = \\frac{N_i}{\\sum_{j=1}^{C} N_j}$$\nwhere Ni is the number of pixels of class i, and C is the number of classes. We calculate the frequency of each class using the ground truth of the Cityscapes validation set, and the IoU of the semi-supervised learning method UniMatch [11] with 25% target labeled data in the GTA5-to-Cityscapes task, as shown in Figure 4. We observed that some classes, such as traffic signs and people, despite having low class frequencies, achieve relatively good class IoU (above 72%). On the other hand, classes like trucks and trains have both low class frequencies and low IoU (below 60%), thus requiring higher weights. This observation motivated us to calculate class weights based on IoU.\nSpecifically, after the active learning module is triggered at specified epochs and D is updated, the semantic segmentation model makes predictions on the images in D and calculates the IoU for each class. Let x, y and y denote the image from target labeled domain, its prediction and ground truth, respectively. The IoU of each class, i.e., IoU; is calculated as:\n$$IoU_i = \\frac{\\sum I[y^t_l = i, \\hat{y}^t = i]}{\\sum I[y^t_l = i] + \\sum I[\\hat{y}^t = i] - \\sum I[y^t_l = i, \\hat{y}^t = i]}$$\nwhere i is the class index, I is the indicator function, which equals 1 when the condition is satisfied, and 0 otherwise. The loss weight for class i, denoted as wi, is updated as:\n$$w_i = (1 - IoU_i) \\times (u - 1) + 1$$\nwhere u is the upper bound of the class weight and is set as 2 in our experiments. The range of the class weight is [1, u] and wi is incorporated into the loss calculation for L, L, LP, L1, and L2. Take the loss L' as an example, the loss function with the IoU-based class weighting is:\n$$L'(p) = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} w_j y'(i,j) \\log p'_s(i,j)$$\nwhere i and j are the pixel and channel indices, N and C are the number of pixels and classes, wj is the weight for class j."}, {"title": "F. Training Process", "content": "The pseudocode of the training process of SS-ADA is outlined in Algorithm 1. The function \u201cacquire(p, )\u201d denotes the selection of samples for annotation based on the model prediction p, Equation (6), and (7) in the active learning module. The function \u201chuman_annotate(xelect)\u201d signifies the human annotation for the selected unlabeled images xselect. Lastly, the function \u201ciou_cal(\u011d\u0131, y)\u201d involves calculating the class IoU using \u0177 and y\u0142 based on Equation (9)."}, {"title": "IV. EXPERIMENT", "content": "We conduct synthetic-to-real and real-to-real domain adaptation experiments that include three public settings: GTA5-to-Cityscapes, SYNTHIA-to-Cityscapes, Cityscapes-to-ACDC and our customized setup: Cityscapes-to-FishEyeCampus. The characteristics of these datasets, including their type (simulated or real), the count of training and validation sets, resolution, and the number of semantic categories, are summarized in Table I. GTA5 originates from the video game Grand Theft Auto V, while SYNTHIA is a product of a virtual city environment. Cityscapes consists of diverse urban street scenes captured in Germany and nearby regions. ACDC comprises 1000 foggy, 1006 nighttime, 1000 rainy and 1000 snowy images for semantic understanding in challenging visual conditions. These datasets are widely employed in domain adaptation studies, covering scenarios from simulation to real-world environments and from normal to adverse weather conditions. FishEyeCampus, our proprietary dataset, was collected on campus using four surround-view fisheye cameras. Selected samples from FishEyeCampus are shown in Figure 5. This dataset allows us to further evaluate SS-ADA's effectiveness in real-world contexts, demonstrating its adaptability from pinhole camera images (Cityscapes) to fisheye camera images.\nWe employ the widely used evaluation metric, the mean Intersection over Union (mIoU), for semantic segmentation evaluation. It calculates the IoU for each class and then averages them. The mIoU is calculated as follows:\n$$mIoU = \\frac{1}{M} \\sum_{i=1} \\frac{TP_i}{TP_i + FP_i + FN_i}$$\nwhere C is the number of classes, TP\u2081, FP\u2081 and FNi are the true positive, false positive and false negative of class i.\nFor real-time inference and deployment, we adopt BiSeNet [38] as the segmentation model unlike previous methods that use DeepLabV2 or DeeplabV3+ [39]. In the semi-supervised learning module, the training parameters are set following [10], [11]. The model is trained using the SGD optimizer with a learning rate of 0.005, momentum of 0.9, and weight decay of 0.0001. A warm-up strategy for the learning rate is employed in the first 1000 iterations, and the batch size is set as 4. For the former three domain adaptation datasets, the images are cropped into (768, 768), while for Cityscapes-to-FishEyeCampus, the images are cropped into (512, 512) during the training process. The total training epochs are set as 240 for GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes, 200 for Cityscapes-to-ACDC, and Cityscapes-to-FishEyeCampus. When reproducing U2PL and UniMatch using BiSeNet as the segmentation model, we adopt the same training parameters following [10], [11]. The active learning module is triggered at 20, 40, and 60 epochs for all datasets. The upper bound of the class weight u is set to 2. The confidence threshold for pseudo-label generation is set to 0 and can be improved by existing self-training methods. To ensure a fair comparison, we also replace the segmentation model in D2ADA [19] and RIPU [15] with BiSeNet and maintain the original training parameters in their respective papers. Our goal is to achieve the accuracy of supervised learning using fewer target labeled samples. We use the entire target domain dataset to perform supervised training with BiSeNet as the target accuracy. Additionally, we include the accuracy obtained from joint training with both source and target domain datasets. The experiments are conducted on a single NVIDIA RTX 3090 GPU. More experimental details can be found at https://github.com/ywher/SS-ADA."}, {"title": "D. Quantitative Results and Analysis", "content": "The quantitative results on GTA5-to-Cityscapes are shown in Table II. When using UniMatch, the mIoU for supervised learning and joint training are 71.0% and 71.5%, respectively. With 25% of labeled data in Cityscapes, SS-ADA achieves 71.3% mIoU, surpassing that of supervised learning. To demonstrate the generalizability, we replace the semi-supervised learning module with U2PL. The mIoU for supervised learning and joint training are 70.1% and 70.5%, respectively. With 25% target labeled data, SS-ADA achieves an mIoU of 72.4%, higher than both.\nIn the first column of Figure 6, we compare the performance of supervised learning, semi-supervised methods and SS-ADA under different ratios of target labeled data. Semi-supervised methods show an improvement in accuracy compared to supervised ones, and as the ratio of labeled data decreases, the absolute improvement becomes more significant, highlighting the benefits of leveraging unlabeled data. However, these methods still fall short of the performance achieved by supervised learning with 100% labeled data. SS-ADA achieves further improvements in accuracy compared to semi-supervised methods. This is made through the use of an acquisition strategy and the inclusion of labeled source domain data. Remarkably, SS-ADA reaches the accuracy level of supervised learning while using only 25% of the target labeled data, significantly reducing the annotation requirements and associated costs.\nThe experimental results on SYNTHIA-to-Cityscapes are shown in Table III. Similarly, SS-ADA achieves the accuracy of supervised learning using 25% of target labeled data but falls short of reaching the performance level of joint training. One possible reason for this discrepancy is that SYNTHIA data has higher quality compared to GTA5, and its distribution is closer to that of Cityscapes. Therefore, joint training tends to yield better performance. From the second column of Figure 6, it's evident that SS-ADA consistently improves the mIoU compared to semi-supervised learning methods across different ratios.\nTable IV presents the quantitative results on Cityscapes-to-ACDC. When using 12.5% of target labeled data, SS-ADA achieves the accuracy of supervised learning. With 25% of target labeled data, the performance of SS-ADA is comparable to the joint training. From the third column of Figure 6, SS-ADA shows steady mIoU improvements over semi-supervised methods U2PL and UniMatch across different ratios of target labeled data. These results highlight the effectiveness of SS-ADA in the normal-to-adverse weather domain adaptation setting."}, {"title": "E. Demonstration of the active learning selection process", "content": "We analyze the selection results of active learning module in GTA5-to-Cityscapes with 50% of the target labeled data. The module is triggered at training rounds 20, 40, and 60, each time selecting 486 images for manual annotation. The class frequency distribution of the annotated labels for the images selected in these three rounds is shown in Figure 7. It can be observed that the selected classes have relatively low class frequencies in Cityscapes. After the active learning module, the frequency proportion of these classes is significantly increased, especially during the first selection at epoch 20. The top 4 images with corresponding labels selected by the active learning module are presented in Figure 8. It can be seen that the active learning module selected some rare scenes like backlit and turning, and images with buses and person."}, {"title": "F. Class reweighting analysis", "content": "The IoU-based class weighting is compared with widely used frequency-based class weighting [41], where the weights are inversely proportional to the class frequency. Figure 9 shows the results on Cityscapes-to-FishEyeCampus under different ratios of target labeled data. The IoU-based class weighting strategy outperforms the frequency-based one in all settings. This indicates that the IoU-based approach better captures the segmentation performance of the model on different classes and assigns more appropriate class weights."}, {"title": "G. Comparison with ADA methods", "content": "Figure 10 presents the performance of ADA methods RIPU, D2ADA, and our SS-ADA across different annotation ratios. SS-ADA consistently outperforms RIPU and D2ADA in all settings. As the annotation ratio increases, SS-ADA's accuracy also improves gradually. The performance of D2ADA increases initially but then plateaus or even decreases. While RIPU exhibits a noticeable decrease in accuracy, a trend also observed in the literature [23]. This suggests that image-level acquisition strategies are less sensitive to the class imbalance problem, possibly because they preserve rich contextual information within the images. Additionally, as the ratio of annotated data increases, the improvement in mIoU slows down, illustrating the characteristic of active learning in selecting valuable samples. More valuable samples are chosen when the annotation ratio is low, whereas adding redundant samples does not significantly enhance model accuracy."}, {"title": "H. Qualitative Results and Analysis", "content": "The first and second rows of Figure 11 display the qualitative segmentation results on GTA5-to-Cityscapes. SS-ADA excels in challenging classes such as bus, pole, rider, and bicycle, which are difficult in the Cityscapes dataset. The third and fourth rows of Figure 11 depict the results on SYNTHIA-to-Cityscapes. SS-ADA segments the car, pole, traffic sign, and motorcycle better than RIPU and D2ADA, achieving results comparable to the supervised counterpart. The last two rows of Figure 11 show the results for Cityscapes-to-ACDC. In adverse weather conditions, SS-ADA segments the truck, pole, traffic light and sign better than RIPU and D2ADA, and is comparable to supervised counterpart. Figure 12 presents the segmentation performance of SS-ADA on Cityscapes-to-FishEyeCampus in a real-world scenario. SS-ADA exhibits comparable performance with the supervised model and sometimes classifies the bicycle, motorcycle, traffic light and sign better, which aligns with the quantitative results."}, {"title": "I. Ablation Study", "content": "1) Effectiveness of each module: We conducted an ablation study on GTA5-to-Cityscapes with 50% of target labeled data, and Table VI presents the results. The source-only model has only 40.8% mIoU on Cityscapes. When adding the semi-supervised learning module, the mIoU increases to 68.5%. Incorporating only the active learning module results in an mIoU of 70.4%. Combining both of them raises the mIoU to 71.4%. Finally, integrating the IoU-based class weighting achieves the highest mIoU of 72.1%. These results demonstrate the effectiveness of each module in SS-ADA, with each component contributing to the overall performance improvement.\n2) The impact of acquisition strategy: We conducted experiments on GTA5-to-Cityscapes using image-level entropy and confidence as acquisition strategies, and the results are shown in Table VII. Across four different annotation ratios, the differences in performance between using entropy and confidence were not significant. Since the overall performance of entropy is slightly better, we adopt it in our experiments.\n3) The impact of active learning trigger epoches Tac: We conduct experiments on Cityscapes-to-FishEyeCampus with 12.5% of target labeled data under different Tac and the results are shown in Table VIII. Overall, the impact of different Tac on model accuracy is not particularly significant. Setting the trigger epoches within the first half of the total training epochs yields satisfactory results. As the number of trigger epoches increases from two to four, the average mIoU initially improves and then declines, with three trigger epoches achieving the best performance. In the three-sampling setup, the configuration of sampling at epochs 20, 40, and 60 shows the best results, and we use this setting in our experiments.\n4) The impact of the upper bound of class weight u: We set u to different values and the results on Cityscapes-to-FishEyeCampus with 25% of target labeled data are shown in Figure 13. SS-ADA consistently outperforms the baseline of 65.9% mIoU across different u, with no significant performance differences observed. This indicates that SS-ADA is insensitive to variations in u, and we set it to 2.0."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose SS-ADA, a novel semi-supervised active domain adaptation framework for semantic segmentation in driving scenes. SS-ADA incorporates active learning into semi-supervised semantic segmentation to select the more informative unlabeled data for manual annotation and model training. This approach facilitates the application of semantic segmentation models to new driving scenarios and significantly reduces annotation costs. We find that image-level acquisition strategies are less sensitive to class imbalance issue and design an IoU-based class weighting strategy to alleviate this. Experimental results demonstrate the effectiveness of SS-ADA across various driving scenarios, including the synthetic-to-real, normal-to-adverse weather, and pinhole-to-fisheye camera adaptation datasets. SS-ADA achieves the accuracy of supervised learning using only 25% of the labeled samples from the target domain.\nIn future work, we will explore the design of more effective data acquisition strategies, the application of SS-ADA to other tasks and domains, and consider incorporating vision foundation models into the framework."}]}