{"title": "LLM-based SPARQL Query Generation from Natural Language over Federated Knowledge Graphs", "authors": ["Vincent Emonet", "Jerven Bolleman", "Severine Duvaud", "Tarcisio Mendes de Farias", "Ana Claudia Sima"], "abstract": "We introduce a Retrieval-Augmented Generation (RAG) system for translating user questions into accurate federated SPARQL queries over bioinformatics knowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance accuracy and reduce hallucinations in query generation, our system utilises metadata from the KGs, including query examples and schema information, and incorporates a validation step to correct generated queries. The system is available online at chat.expasy.org.", "sections": [{"title": "1. Introduction", "content": "In bioinformatics, the ability to query complex knowledge graphs (KGs) is critical for extracting meaningful insights. However, manually crafting SPARQL queries, especially federated queries spanning across multiple connected KGs, can be a time-consuming and challenging task, even for experts. This has led to a growing demand for Knowledge Graph Question Answering (KGQA) systems that can translate natural language queries into SPARQL, bridging the gap between users' questions and the structured data available.\nLarge Language Models (LLMs) present an exciting opportunity to address this challenge, offering the potential to automatically generate accurate SPARQL queries from natural language inputs. However, while LLMs have demonstrated impressive capabilities in this area [1][2], current systems struggle to handle large-scale, evolving KGs, such as those in the SIB Swiss Institute of Bioinformatics catalog [3].\nIn this work, we present a solution designed to assist users of SIB's bioinformatics KGs[4], such as UniProt [5], Bgee [6] or OMA [7], to explore and query the data available. Our approach leverages LLMs and endpoints metadata to generate SPARQL queries while addressing the challenge of dynamically integrating evolving datasets, without requiring constant retraining. By offering a scalable system\u00b9 that adapts to the complex and changing landscape of bioinformatics knowledge, we aim to significantly reduce the time and expertise needed to query across federated KGs."}, {"title": "2. System Design", "content": "The proposed system is illustrated in Figure 1. The system takes a list of SPARQL endpoint URLs as input, where each endpoint is expected to include minimal standardized metadata (example queries and VoID\u00b2 descriptions) that can be automatically retrieved and indexed upon initial deployment. We provide an online webpage\u00b3 allowing to check if a given endpoint contains the required metadata.\nThe overall data flow of the system can be summarised as: 1) getting the relevant context using embeddings-based similarity search; 2) prompt building leveraging the retrieved context; 3) validating and correcting the query using endpoints schema; 4) presenting query and relevant context to the user."}, {"title": "2.1. Generating embeddings and indexing", "content": "Example question/query pairs are automatically retrieved from each endpoint via a SPARQL query upon initial deployment. Embeddings are generated for each question and indexed in a vector database, allowing to match user provided inputs based on similarity search. A documented repository of example question/query pairs and a CLI tools are provided to help endpoints maintainers define and validate SPARQL query examples for their endpoints in a standardized format.\nAdditionally, we generate and index shapes for the classes present in each endpoint. For this, we first retrieve the VoID descriptions from each endpoint, which detail the relationships between subject classes and object classes or datatypes via specific predicates. For example, the Protein class is linked to the Gene class through the encodedBy predicate. The VoID generator is available open source and can be used to generate statistics over any SPARQL endpoint. This information allows us to generate simple, human-readable ShEx shapes for each class. Additionally, each object property references a list of classes rather than another shape, making each shape self-contained and interpretable on its own.\nThe generated shapes are well-suited for use with a LLM, as they provide information about which predicates are available for a class, and the corresponding classes or datatypes those predicates point to.\nFor example here is the shape generated for a Disease Annotation:\nup: Disease_Annotation {\na [ up: Disease_Annotation ];\nup: sequence [ up:Chain_Annotation up:Modified_Sequence ];\nrdfs:comment xsd:string; up:disease IRI }"}, {"title": "2.2. Prompt building", "content": "When a user asks a question, embeddings are generated for this question and the most similar questions are retrieved from the vector database, as well as the most similar classes labels to this question. The questions and their associated query, and the classes label and their associated schema are added to the prompt alongside the user question. An example of the generated prompt can be found online 7."}, {"title": "2.3. Validating generated queries", "content": "To mitigate potential errors (e.g., hallucinations), we developed a method to validate federated SPARQL queries based on the VoID description of the endpoints. The validator parses the SPARQL query generated, extracts triple patterns, identifies the endpoints where they will be executed, and whether the triples in the query comply with the expected schema.\nThe validator produces a list of human-readable errors describing which predicates are incorrect, the associated subject or class, and a list of valid predicates based on the schema. The errors and corrections are then provided back to the LLM to help correct the wrong query, for example: \"Subject ?disease with type up:Disease in endpoint https://sparql.uniprot.org/sparql does not support the predicate rdfs:label. It can have the following predicates: skos:altLabel, rdfs:comment, up:mnemonic, skos:prefLabel, rdfs:seeAlso\"."}, {"title": "2.4. Logs and feedback", "content": "All user questions are stored in a log file, from which they can be analysed to better understand users needs. Additionally, the chat interface includes a simple user feedback mechanism through like/dislike buttons. When one of these buttons is clicked, the entire conversation is stored in a log file in JSONL format on the server. We provide a notebook to display all conversations from the logs in a human-readable way to help maintainers analyze feedbacks."}, {"title": "3. Implementation", "content": "The different components of the system are modular, published as a python package, and can be reused in other systems. The main components included are: 1) A module to automatically generate a human-readable ShEx schema for each class extracted from the VoID description of an endpoint, and generate documents to load in a vector database, compatible with the LangChain framework; 2) A module to automatically retrieve query examples from an endpoint and generate documents to load in a vector database, compatible with the LangChain framework; 3) A module to automatically validate a federated SPARQL query based on each endpoint VoID description, providing human-readable error messages and suggested corrections.\nThe system can easily support any LLM that is served via an OpenAI-compatible API. As of today, a lot of open-source inference tools and LLM cloud providers use this standard. While we primarily rely on OpenAI models, we have also been testing the system with models such as LLaMA and Mixtral.\nThe fastembed library is used with the model BAAI/bge-large-en-v1.5 to generate text embeddings, chosen for its speed and high performance on the Massive Text Embedding Benchmark [8][9]10."}, {"title": "4. Evaluation", "content": "We designed a preliminary test suite of 13 questions, each paired with a reference query, ensuring none are present in the examples seen by the system. The questions require reasoning over the context and specify clear retrieval tasks. For each question, the system generates a query, which we run and compare against the expected results. To account for LLM variability, each question is tested 3 times.\nWe assess the system's performance across three configurations: 1) baseline LLM without RAG, 2) RAG without validation, and 3) RAG with validation and correction of the generated query. This approach allows us to evaluate the contribution of each system component in improving query generation accuracy and helps detect any regressions when modifications are introduced. Additionally, the test suite is adaptable for testing different LLMs, enabling a comparison of their effectiveness in handling the questions. The results are summarised in Table 1, price is computed from average per request.\nThe results indicate that larger LLMs perform significantly better overall, efficiently extrapolating from provided examples. In contrast, query validation is particularly valuable for smaller LLMs, not only improving accuracy but also ensuring the system generates queries that retrieve at least some relevant results.\nSo far, our primary focus has been on developing a production system to assist users of the SIB SPARQL endpoints in formulating effective queries. In the future, we aim to conduct a more comprehen- sive evaluation using standardised benchmarks[10][11] and benchmarking frameworks[2]. However evaluating query results in bioinformatics is often challenging, even for human experts, due to the inherent complexity of both the domain-specific questions and the KGs models. These complexities can introduce a degree of interpretative flexibility when assessing the accuracy or relevance of the results."}, {"title": "5. Conclusions", "content": "We have introduced a system for generating federated SPARQL queries leveraging LLMs over real-world KGs, in response to Natural Language questions asked by users. The system currently handles questions over bioinformatics KGs within the SIB Semantic Web of Data, but can easily be reused with other KGs of interest. The system is fully open-source and a demo can be accessed online at chat.expasy.org."}]}