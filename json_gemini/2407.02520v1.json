{"title": "RaCIL: Ray Tracing based Multi-UAV Obstacle Avoidance through Composite Imitation Learning", "authors": ["Harsh Bansal", "Vyom Goyal", "Bhaskar Joshi", "Akhil Gupta", "Harikumar Kandath"], "abstract": "In this study, we address the challenge of obstacle avoidance for Unmanned Aerial Vehicles (UAVs) through an innovative composite imitation learning approach that combines Proximal Policy Optimization (PPO) with Behavior Cloning (BC) and Generative Adversarial Imitation Learning (GAIL), enriched by the integration of ray-tracing techniques. Our research underscores the significant role of ray-tracing in enhancing obstacle detection and avoidance capabilities. Moreover, we demonstrate the effectiveness of incorporating GAIL in coordinating the flight paths of two UAVs, showcasing improved collision avoidance capabilities. Extending our methodology, we apply our combined PPO, BC, GAIL, and ray-tracing framework to scenarios involving four UAVs, illustrating its scalability and adaptability to more complex scenarios. The findings indicate that our approach not only improves the reliability of basic PPO based obstacle avoidance but also paves the way for advanced autonomous UAV operations in crowded or dynamic environments.", "sections": [{"title": "I. INTRODUCTION", "content": "The incorporation of Unmanned Aerial Vehicles (UAVs) into both civilian and military activities has shown a significant advancement in the domain, driven by progress in sensor technology, control algorithms, and communication systems. These advancements have made UAVs indispensable for tasks ranging from package delivery to disaster response. The challenge of navigating UAVs through complex environments without encountering obstacles is significant [1], leading to a focus on developing more adaptable navigation solutions beyond traditional methods like Simultaneous Localization and Mapping (SLAM) [2].\nRecent years have seen the emergence of Reinforcement Learning (RL) as a promising approach for improving UAV autonomy.\nAlongside RL, Imitation Learning (IL) has shown great potential by learning from expert to navigate similar conditions effectively, streamlining the training process for autonomous control systems.\nDeep learning and Generative Adversarial Networks (GANs) [4], have significantly enhanced IL, by generating synthetic but realistic training data that mimics expert behavior, allowing the learning model to improve its predictions on unseen data. IL, in particular, is noted for its efficiency and effectiveness in learning through demonstrations, offering a quicker and more intuitive path to achieving desired behaviors compared to RL's trial-and-error approach. This efficiency, coupled with the interactive nature of IL with human input, underscores its importance in the development of sophisticated artificial agents.\nThe contributions from our study are:\n\u2022 It introduces a composite imitation learning framework combining Proximal Policy Optimization (PPO), Behavior Cloning (BC), and Generative Adversarial Imitation Learning (GAIL) with RayTracing techniques for improved UAV Obstacle Avoidance.\n\u2022 Demonstrates the significant impact of integrating Ray-Tracing in enhancing agent training efficiency for detecting and avoiding obstacles.\n\u2022 Showcases the effectiveness of composite IL in coordinating flight paths and enhancing collision avoidance among multiple UAVs when compared to IL solely based on BC.\n\u2022 Validates the model's scalability and robustness through successful applications in environments with increasing UAV numbers, maintaining high success rates showing its potential for advanced autonomous UAV operations.\nThe paper is structured as follows: Section II provides a detailed background, Section III introduces essential preliminaries and the problem formulation, Section IV describes our methodology, Section V examines the results, Section VI concludes with a summary of our findings and contributions to the field and Section VII states the acknowledgements."}, {"title": "II. BACKGROUND", "content": "The development of Reinforcement Learning (RL) as an useful technique for way point navigation in intricate environments is a notable milestone in the field of robotics, specifically in the context of Unmanned Aerial Vehicles (UAVs). Reinforcement learning (RL) enables agents to develop optimal behaviour for particular tasks by participating in trial and error experiments with their environment. This process enhances the autonomy of agents in many robotics applications.\nThe study conducted by Almazrouei et al. [5] offers a comprehensive examination of many established techniques and methodologies of the use of reinforcement learning in facilitating the navigation of unmanned aerial vehicles (UAVs) in different scenarios. Zhang et al. [6] and Park et al. [8] extend this study to dynamic settings by using a camera as the primary source of Observation and applying various vision based techniques like Google-Net [7], CNN and RNN's to distinguish between stationary and moving objects, hence improving autonomous navigation abilities.\nTo enhance the UAV Navigation across Dynamic Ob- stacles, Zhang et al. [6] also introduces Ray Tracing is a method of encoding or describing the environment. It helps provide useful observations to the agent.\nThe work of Kulic et al. [9] [10] employ the use of behavioral cloning which tries to learn and imitate the actions of an expert.\nExpanding on these approaches, the work of Liu et al. [11], Fang et al. [12] & Shin et al. [13] use multiple variants of state-of-the-art GAIL to improve and adapt the Agent to randomised dynamic environments.\nHowever, these works do not explore the application of Ray Tracing, Behavioral Cloning, GAIL & Imitation Learning in a Multi-UAV dynamic environment. We thus provide a methodology which helps train an Agent that is both more efficient and adaptable to diverse environments."}, {"title": "III. PRELIMINARIES AND PROBLEM FORMULATION", "content": "This study utilizes a simplified UAV model, focusing on two-dimensional movement. The model is defined by position coordinates $[x, y]^T$ and controlled through velocity $[v_x, v_y]^T$, as described by Eq. (1).\n$\\dot{x} = v_x, \\dot{y} = v_y$ (1)\nWe assume a constant altitude z for our study in the paper, limiting navigation to the XY plane."}, {"title": "B. Proximal Policy Optimization (PPO)", "content": "Proximal Policy Optimization (PPO) [15] is a rein- forcement learning method that iteratively updates the policy to maximize reward. PPO maintains stable up- dates by using a clipped surrogate objective function, which is represented as:\n$L_{PPO}(\\theta) = E_t [min (r_t(\\theta) A_t,$  (2)\n$clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t]$\nwhere $\\hat{A_t}$ is the advantage function computed as :\n$\\hat{A_t} = R_{env} - V(s_t)$ (3)\nand the ratio $r_t(\\theta)$ is defined as:\n$r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)}$ (4)\n$s_t, a_t$ are state and actions at time t and $\\pi_{\\theta}$ represents the policy parameterised on $\\theta$. $R_{env}$ is the cumulative reward received from the environment and $V$ denotes the expected cumulative reward.\nThis ratio is clipped to keep the policy updates within a specified threshold, ensuring small, incremental ad- justments to the policy and avoiding large, potentially harmful updates. This balance allows PPO to be both efficient and effective in various learning environments."}, {"title": "C. Imitation Learning", "content": "Imitation Learning is the process of learning from expert demonstrations. Instead of generating random actions and learning through trial and error (as in case of Reinforcement Learning), the agent learns from the expert [14]."}, {"title": "D. Behavioral Cloning", "content": "Behavioral Cloning [16] [17] is a machine learning technique where an agent learns a policy by imitating the behaviour of an expert. It trains a neural network that helps the agent mimic the experts actions.\nConsider a subset $D_E$ consisting of state-action pairs $(s^E, a^F)$, taken from expert demonstrations over T time steps.\n$D_E = \\{(s_1^E, a_1^E), (s_2^E, a_2^E), ..., (s_T^E, a_T^E)\\}$ (5)\nThe behaviour cloning loss function is defined as, a mean squared error (MSE) [18], based on the expert's demonstration data $L_{BC}$ as:\n$L_{BC}(\\theta) = \\frac{1}{2} \\sum_{t} (\\pi(s_t | \\theta) - a_t^E)^2$ (6)\nThe objective is to minimise the difference between the actions suggested by the policy network $\\pi(s_t^\\theta)$, which is characterized by parameters $\\theta$, and the expert's chosen actions $a_t^E$, corresponding to state $s_t^E$."}, {"title": "E. Generative Adversarial Imitation Learning (GAIL)", "content": "Generative Adversarial Imitation Learning or GAIL [19], utilizes a discriminator neural network $D_\\phi$ to differentiate between the actions of the agent and the expert and rewards the agent if it mimics expert actions. $D_E$ contains the state-action pairs generated by the expert demonstrations as defined in eq 5, and $D_G$ be the state-action pairs generated by the current policy $\\pi_\\theta$. We update the discriminator parameters with the gradient:\n$E_{(s,a) \\sim D_G} [\\nabla_{\\phi} log D_\\phi(s,a)]$  (7)\n$+ E_{(s,a) \\sim D_E} [\\nabla_{\\phi} log (1 - D_\\phi(s,a))]$"}, {"title": "The reward generated by GAIL is defined as:", "content": "$R_{GAIL} = log(D(s,a))$ (8)\nand the loss generated for policy updation is calculated as $L_{GAIL}$ is defined as :\n$E_{(s,a) \\sim D_E} [log \\pi_\\theta(a|s)Q(s,a)] - \\lambda H(\\pi_\\theta)$ (9)\nwhere\n$Q(s,\\overline{a}) = E_{(s,a) \\sim D_{w_{i+1}}} [log(D_{w_{i+1}} (s, a)) | s_0 = s, a_0 = \\overline{a}]$ (10)\n$\\lambda$ is a control parameter and $H(\\pi)$ is the discounted casual entropy which promotes exploration."}, {"title": "F. Problem Formulation", "content": "The objective of the agent is to minimize the distance between the UAV and its corresponding goal $d_{goal}$ as stated in eq. 11.\n$\\min d_{goal}(UAV, goal)$ (11)\nsubject to $d_{obstacle}(UAV, obstacle) > \\epsilon_{safe}, $\n$\\forall obstacle \\in Obstacles$\nIn our research, we make use of approaches such as Proximal Policy Optimization (PPO), Behaviour Cloning (BC), and Generative Adversarial Imitation Learning (GAIL) with ray-tracing to enhance UAV's capacity to navigate through the environment in an efficient manner."}, {"title": "IV. RAY TRACING BASED COMPOSITE IMITATION LEARNING (RACIL)", "content": "Fig. 2 outlines the sequence of steps undertaken during the model training phase, detailing the process through which the agent selects actions based on a predefined policy. The environment responds by transitioning to a new state and generating relevant observations. These observations are then utilized by both the environment and the GAIL discriminator to generate reward, which is instrumental in the iterative updation and refinement of the policy. This structured approach facilitates a systematic enhancement of the agent's decision-making capabilities. The detailed flow is provided by Algorithm 1.\nWe first initialise the environment, set our hyper parameters for training and start running PPO algorithm. The model is trained using behaviour cloning for some initial steps. The minimisation of BC loss leads to actions being close to the expert demo. After this we train using GAIL. The total loss is computed by a weighted sum of the losses returned by different algorithms and then we update actor and critic networks on the basis of loss obtained cumulatively from different components of the algorithm."}, {"title": "A. The Environment", "content": "The environment comprises multiple obstacles, multi- ple UAV agents, and their corresponding goals as shown in Fig. 1. At the start of each episode, the UAVs are spawned randomly as follows:\n$X_{UAV} = (x_{UAV}, y_{UAV}, z_0)$ (12)\nwhere $x_{UAV} \\sim U(x_{min}, x_{max})$ and $y_{uav}$ is drawn from a combined uniform distribution $U(y_{min}, y_{min} + r_{min}) \\cup U(y_{max} - r_{max}, y_{max})$, and $z_0$ represents a fixed altitude. Subsequently, the goal for the UAV is defined by\n$X_{goal} = (x_{goal}, y_{goal}, z_0)$ (13)\nwhere $x_{ goal} \\sim U(x_{min}, x_{max})$. The $y_{ goal}$ value depends on $y_{uav}$'s position: for $y_{uav}$ within $[y_{min}, y_{min} + r_{min}]$, $y_{goal}$ is drawn from $U(y_{max} - r_{max}, y_{max})$; conversely, if $y_{uav}$ falls in $[y_{max} - r_{max}, y_{max}]$, then $y_{goal}$ comes from $U(y_{min}, y_{min} + r_{min})$. Both UAVs and goals share the same fixed altitude $z_0$.\nFinally, the environment randomly generates a series of obstacles. Let there be i obstacles (i \u2208 [1, Nobs]), then for the ith obstacle:\n$X_{obstacle} = (x_{obstacle}, y_{obstacle}, z_0)$ (14)\nwhere $y_{obstacle} \\sim U(y_{min} + r_{min}, y_{max} - r_{max})$. The $x_{obstacle}$ coordinate is distributed uniformly within $[x_{min}+ d \\cdot (i-1), x_{min}+ d \\cdot i]$, where $d = \\frac{x_{max}-x_{min}}{N_{obs}}$. Each obstacle is also given a random rotation around the z-axis ranging from 0 to 180 degrees.\nHere, $r_{min}$ and $r_{max}$ specify the environment dimensions along the m-axis (m \u2208 {x,y}). For the agent, the obstacles spawned in the environment are static whereas the other UAV's navigating the environment in a Multi-UAV system act as dynamic obstacles for the agent."}, {"title": "B. Observation Space", "content": "We define the observation space as Eq. 15:\nObservation Space = {$(x_a, y_a), (x_g, y_g),$\nRay Tracing Observations} (15)\nWhere, $x_a$ and $y_a$ represents the UAV's (agent's) position within the simulation environment along x-axis and y-axis respectively. $(x_g, y_g)$ represents the goal's position along the x-axis and $y_g$ respectively.\nA ray perception sensor is added to our agent UAV which projects rays in different directions and collects the observations when the rays strike surface. Ray Tracing observations correspond to the observations about the environment collected by this sensor and it includes ob- servations of ray perception sensor include the distance, surface normal, tag and information about the collider of the object hit."}, {"title": "C. Action Space", "content": "The action space for the agent for our study is defined as a discrete set comprising three actions denoted as Move Forward (Fwd), Rotate Left ($O_{left}$) and Rotate Right ($O_{right}$):\nAction Space = {Fwd, $O_{left}, O_{right}$} (16)\nThese actions are carefully chosen to allow the agent to perform comprehensive navigation and precise orienta- tion within the two-dimensional operational plane. The least value by which it moves forward is by 0.04 units and rotates with 2\u00b0 clockwise or anti-clockwise."}, {"title": "D. Reward Function", "content": "Designing effective reward functions is a critical chal- lenge in Reinforcement Learning (RL), often requiring hand-engineering and domain-specific knowledge. Both Imitation Learning (IL) [19] and Inverse Reinforcement Learning (IRL) [20] attempt to address this by learning from expert behavior, but they face issues with com- putational efficiency and the complexity of accurately capturing expert trajectories [21].\nWe defined the reward function as a sum of several components, each contributing to the overall reward. For our work, it is expressed as:\n$R_{extrinsic} = R_{collision} + R_{proximity} + R_{time penalty}$ (17)\n$\\begin{cases}\n+r_f & \\text{for reaching its goal,}\n-r_f & \\text{for reaching other UAV's goal,}\nR_{collision} = -r_f & \\text{for collision with other UAV,}\n-r_f & \\text{for collision with obstacle.}\n\\end{cases}$ (18)\n$\\begin{cases}\n+r_p & \\text{within $\\delta$-proximity of its goal,}\nR_{proximity} = -r_p & \\text{within $\\epsilon$-proximity to other UAV.}\n\\end{cases}$ (19)\n$R_{time penalty} = -r_{tp} \\text{ for each time step.}$ (20)"}, {"title": "E. The Agent", "content": "We have used Proximal Policy Optimization (PPO) eq. (2) for obstacle avoidance. The PPO algorithm consists of both actor and critic components. The actor predicts mean values for each action scalar based on environment observations, while the critic evaluates the perceived value associated with the input state. We are also using Behaviour cloning eq. (5) and GAIL eq. (7) by providing expert demonstration. This helps the agent imitate the expert thus leading to faster convergence of the loss function and thus a shorter training time."}, {"title": "F. Ray Tracing", "content": "We have used Ray Tracing in our work, leading to better navigation and higher accuracy of our model. The ray perception sensor attached to the agent subtends rays with the ray equation:\n$P(\\alpha) = O_r + \\alpha \\cdot \\overrightarrow{D}$ (21)\nIt provides crucial observations such as Hit Distance, Hit Tag, Hit Normal, Hit Collider. Hit Distance is evaluated as hit distance = $|\\overrightarrow{C_r} - O_r||$. ($O_r$ is the origin point for the ray, $\\overrightarrow{D}$ is direction vector parameterised on $\\alpha$ and"}, {"title": "A. Training", "content": "This section explains the Training Phase and the results obtained. The study was initiated with a single UAV (ref Fig. 1 where only UAV1 and target1 are spawned along with the obstacles), and various approaches were attempted, analyzing the pros and cons of Ray Tracing, Behavioral Cloning, and GAIL. Following a comparative analysis, the best-performing combination, i.e., Ray Tracing + Behavioral Cloning + GAIL, was selected. This methodology was then extended to Multi-UAV Environments. In order to effectively understand the results of the Training Phase we have divided the Training Phase into three separate studies that were made. The individual studies are defined below:\nStudy 1: The Critical Role of Ray-Tracing in Observation Spaces: This study highlights the importance of Ray Tracing as a part of the Observation Space for the Agent.\nStudy 2: The Impact of GAIL on Agent Training and Performance: This study focuses on assessing the significance of incorporating GAIL alongside BC. The environment for this consists of a UAV navigating through numerous obstacles with the objective of reaching a designated goal.\nThe performance of two different training approaches is compared: one agent is trained using PPO + BC + Ray Tracing, while the other utilizes PPO + BC + GAIL + Ray Tracing as seen in Fig. 5. Evaluation of these trained policies reveal that the inclusion of GAIL in the training protocol results in approximately a 17%\nimprovement in the success rate of the resulting policy as seen in Table II.\nStudy 3: Assessing Scalability and Robustness in UAV Training Across Multiple Agents: This investigation explores the adaptability and reliability of the agent training framework when scaled from single UAV to 2-UAV and 3-UAV scenario(ref Fig. 1 for the Environment Initialisation). The agent, trained with a single UAV is extended to 2 UAV in PPO + BC + GAIL + Ray Tracing setting, and that could be further extended to a more congested 3-UAV setting, with the intention of evaluating performance amidst greater obstacle density, including both static obstacles and dynamic obstacles like additional UAVs. The UAVs are uncoupled during the training phase and act according to a centralized policy. Only the individual losses are considered for training the agent policy. The findings revealed that the agent's performance in the 3-UAV scenario was comparable to that in 1-UAV and 2-UAV setting, demonstrating that the agent maintained its effectiveness despite the heightened intricacy of the environment.\nThis study evaluates the agent's behavior on the fol- lowing metrics:\n\u2022 Mean Reward: Mean Reward refers to the Cu- mulative Reward given to the Agent based on a Terminal Condition. Figs. 4(a), 5(a) and 6(a) refers to the Mean Reward Achieved by the Agent as the Number of Training Steps of the Agent increase in the Training Phase.\n\u2022 Episode Length: Episode Length refers to the Number of Time-steps taken by the Agent such that it reaches a Terminal Condition. Figs. 4(b), 5(b) and 6(b) refers to the Mean Episode Ending Length by the Agent.\n\u2022 GAIL Loss: GAIL Loss refers to Value Loss of the GAIL Discriminator. Fig. 6(c), shows the Loss Value that is decreasing with time.\nThe findings from our study indicate that relying solely on Behavioral Cloning (BC) is insufficient for achieving optimal pathfinding with unmanned aerial ve- hicles (UAVs) in novel scenarios. The absence of ray tracing further complicates the UAV's ability to learn ef- fective path navigation, especially in environments with dynamic obstacles, hindering the agent's development of a robust avoidance strategy. Additionally, the model demonstrates adaptability with an increase in the number of UAVs, as evidenced in Table III, suggesting its"}, {"title": "VI. CONCLUSION", "content": "The proposed research has demonstrated the effective- ness of utilizing a composite imitation learning technique within the Unity simulation environment to enhance UAV autonomy and obstacle avoidance capabilities. Through the application of ray-tracing with a Behavior- Cloning neural network architecture and the exploration of Generative Adversarial Imitation Learning (GAIL), we have shown promising results in training agents to navigate around obstacles and reach predefined goal locations. The addition of GAIL and ray-tracing signif- icantly improved performance compared to the baseline model, which used the PPO algorithm for training the agent along with behavioral cloning on human expert demonstrations.\nBy leveraging Unity as the simulation platform, we have conducted a systematic exploration of imitation learning techniques, contributing to the advancement of UAV navigation methodologies in dynamic and challeng- ing environments as well as showing the scalability of our approach to multi-UAV navigation scenarios. Our findings have implications for real-world applications requiring reliable and adaptive navigation strategies, paving the way for future research in this domain. In future, we plan to scale the model to 3D and deploy it to real drones."}]}