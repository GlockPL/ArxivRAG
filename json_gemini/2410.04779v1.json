{"title": "FAST TRAINING OF SINUSOIDAL NEURAL FIELDS VIA SCALING INITIALIZATION", "authors": ["Taesun Yeom", "Sangyoon Lee", "Jaeho Lee"], "abstract": "Neural fields are an emerging paradigm that represent data as continuous functions parameterized by neural networks. Despite many advantages, neural fields often have a high training cost, which prevents a broader adoption. In this paper, we focus on a popular family of neural fields, called sinusoidal neural fields (SNFs), and study how it should be initialized to maximize the training speed. We find that the standard initialization scheme for SNFs-designed based on the signal propagation principle-is suboptimal. In particular, we show that by simply multiplying each weight (except for the last layer) by a constant, we can accelerate SNF training by 10\u00d7. This method, coined weight scaling, consistently provides a significant speedup over various data domains, allowing the SNFs to train faster than more recently proposed architectures. To understand why the weight scaling works well, we conduct extensive theoretical and empirical analyses which reveal that the weight scaling not only resolves the spectral bias quite effectively but also enjoys a well-conditioned optimization trajectory.", "sections": [{"title": "1 INTRODUCTION", "content": "Neural field (NF) is a special family of neural networks designed to represent a single datum (Xie et al., 2022). Precisely, NFs parametrize each datum with the weights of a neural net which is trained to fit the mapping from spatiotemporal coordinates to corresponding signal values. Thanks to their versatility and low memory footprint, NFs have been rapidly adopted in various domains, including high-dimensional computer vision and graphics (Sitzmann et al., 2019; Mildenhall et al., 2020; Poole et al., 2023), physics-informed machine learning (Wang et al., 2021; Serrano et al., 2023), robotics (Simeonov et al., 2022), and non-Euclidean signal processing (Rebain et al., 2024).\nFor neural fields, the training speed is of vital importance. Representing each datum as an NF requires tedious training of a neural network, which can take up to several hours of GPU-based training (Mildenhall et al., 2020). This computational burden becomes a critical obstacle toward adoption to tasks that involve large-scale data, such as NF-based inference or generation (Ma et al., 2024; Papa et al., 2024). To address this issue, many research efforts have been taken to accelerate NF training, including fast-trainable NF architectures (Sitzmann et al., 2020b; M\u00fcller et al., 2022), meta-learning (Sitzmann et al., 2020a; Chen & Wang, 2022), and data transformations (Seo et al., 2024).\nDespite such efforts, a critical aspect of NF training remains understudied: How should we initialize NFs for the fastest training? While the importance of a good initialization has been much contemplated in classic deep learning literature (Glorot & Bengio, 2010; Zhang et al., 2019), such understandings do not immediately answer our question, for many reasons. First, conventional works mostly focus on how initialization affects the signal propagation properties of deep networks with hundreds of layers, while most NFs use shallow models with only a few layers (Mildenhall et al., 2020). Second, conventional works aim to maximize the model quality at convergence, while in NFs it is often more important how fast the network achieves certain fidelity criterion (M\u00fcller et al., 2022). Lastly, NFs put great emphasis on how the model performs on seen inputs (i.e., training loss)"}, {"title": "2 RELATED WORK", "content": "Training efficiency of neural fields. There are two prevailing strategies to accelerate NF training. The first approach is to adopt advanced NF architectures, which mitigate the spectral bias of neural nets that hinders fitting high-frequency signals (Rahaman et al., 2019; Xu et al., 2020). Popular examples include utilizing specially designed positional encodings (Tancik et al., 2020; M\u00fcller et al., 2022) or activation functions (Sitzmann et al., 2020b; Ramasinghe & Lucey, 2022; Liu et al., 2024)."}, {"title": "3 NEURAL FIELDS INITIALIZATION VIA WEIGHT SCALING", "content": "In this section, we formally describe the weight scaling for a sinusoidal neural field initialization, and briefly discuss the theoretical and empirical properties of the method."}, {"title": "3.1 NETWORK SETUP AND NOTATIONS", "content": "We begin by introducing the necessary notations and problem setup. A sinusoidal neural field is a multilayer perceptron (MLP) with sinusoidal activation functions (Sitzmann et al., 2020b). More concretely, suppose that we have an input $X \\in \\mathbb{R}^{d_o\\times N}$, where N is the number of $d_o$-dimensional coordinates. Then, an l-layer SNF can be characterized recursively via:\n\n$f(X; W) = W^{(1)} Z^{(l-1)} + b^{(1)},\\quad Z^{(i)} = \\sigma_i(W^{(i)} z^{(i-1)} + b^{(i)}),\\quad i\\in [l \u2013 1], $   (1)\n\nparametrized by $W = (W^{(1)}, b^{(1)}, . . ., W^{(l)}, b^{(l)})$, where each $W^{(i)} \\in \\mathbb{R}^{d_i\\times d_{i-1}}$ denotes the weight matrix, $b^{(i)} \\in \\mathbb{R}^{d_i}$ denotes the bias, and $Z^{(i)} \\in \\mathbb{R}^{d_i\\times N}$ denotes the activation of the ith layer. Here, we let $Z^{(0)} = X$, and the activation function $\\sigma_i: \\mathbb{R}^{d_i} \\rightarrow \\mathbb{R}^{d_i}$ are the sinusoidal activations applied entrywise, with some frequency multiplier $\\omega_i$. The standard practice is to use a different frequency multiplier for the first layer. Precisely, we apply the activation function $\\sigma_1(x) = sin(\\omega_0\\cdot x)$ for the first layer, and $\\sigma_i(x) = sin(\\omega_h\\cdot x)$ for all other layers, i.e., $i \\neq 1$.\nThe SNF is trained using the gradient descent, with the mean-squared error (MSE) as a loss function. We assume that the input coordinates of the training data (i.e., the column vectors of the input data) have been scaled to lie inside the hypercube $[-1, +1]^{d_o}$, as in Sitzmann et al. (2020b)."}, {"title": "3.2 STANDARD INITIALIZATION", "content": "The standard initialization scheme for the sinusoidal neural fields independently draws each weight entry from a random distribution (Sitzmann et al., 2020b). Precisely, the weight matrices are initialized according to the scaled uniform distribution as\n\n$W_{j,k}^{(1)} \\overset{i.i.d}{\\sim} \\text{Unif}\\left(-\\frac{\\sqrt{6}}{\\omega_0\\sqrt{d_0}}, \\frac{\\sqrt{6}}{\\omega_0\\sqrt{d_0}}\\right), \\quad W_{j,k}^{(i)} \\overset{i.i.d}{\\sim} \\text{Unif}\\left(-\\frac{\\sqrt{6}}{\\omega_h\\sqrt{d_{i-1}}}, \\frac{\\sqrt{6}}{\\omega_h\\sqrt{d_{i-1}}}\\right),$ (2)\n\nwhere $W_{jk}^{(i)}$ denotes the (j, k)-th entry of the ith layer weight matrix $W^{(i)}$.\nImportantly, the range of the uniform initialization (Eq. (2)) is determined based on the distribution preserving principle, which aims to make the distribution of the activation $Z_i$ similar throughout all layers. In particular, Sitzmann et al. (2020b, Theorem 1.8) formally proves that the choice of the numerator (i.e., $\\sqrt{6}$) ensures the activation distribution to be constantly arcsin(-1, 1) throughout all hidden layers of the initialized sinusoidal neural field.\nFrequency tuning. A popular way to tune the initialization (Eq. (2)) for the given data is by modifying the frequency multiplier of the first layer $\\omega_0$. This method, which we call frequency tuning, affects the frequency spectrum of the initial SNF, enhancing the fitting of signals within certain frequency range. It is not typical to tune $\\omega_h$, as changing its value does not change the initial SNF; the effects are cancelled out by the scaling factors in the initialization scheme."}, {"title": "3.3 INITIALIZATION WITH WEIGHT SCALING", "content": "In this paper, we develop a simple generalization of the standard initialization scheme. In particular, we consider weight scaling, which multiplies a certain scaling factor $\\alpha \\ge 1$ to the initialized weights in all layers except for the last layer. More concretely, the initialization scheme is\n\n$W_{j,k}^{(1)} \\overset{i.i.d}{\\sim} \\text{Unif}\\left(-\\frac{\\alpha\\sqrt{6}}{\\omega_0\\sqrt{d_0}}, \\frac{\\alpha\\sqrt{6}}{\\omega_0\\sqrt{d_0}}\\right), \\quad W_{j,k}^{(i)} \\overset{i.i.d}{\\sim} \\text{Unif}\\left(-\\frac{\\alpha\\sqrt{6}}{\\omega_h\\sqrt{d_{i-1}}}, \\frac{\\alpha\\sqrt{6}}{\\omega_h\\sqrt{d_{i-1}}}\\right),$ (3)\n\nwhere we let $\\alpha = 1$ for the last layer; this design is due to the fact that any scaling in the last layer weight leads to a direct scaling of the initial function $f\\rightarrow \\alpha f$, which is known to be harmful for the generalizability, according to the theoretical work of Chizat et al. (2019). Empirically, we observe that this weight scaling is much more effective in speeding up the training than a recently proposed variant which scales up only the last layer of the NF (Saratchandran et al., 2024); see Section 5.\nThere are two notable characteristics of the weight-scaled initializations.\n(1) Distribution preservation. An attractive property of the proposed weight scaling is that, for SNFs, any $\\alpha \\ge 1$ also preserves the activation distribution over the layers. In particular, we provide the following result, which shows that the activation distributions will still be an arcsin(-1, 1).\nProposition 3.1 (Informal; extension of Sitzmann et al. (2020b, Theorem 1.8)). Consider an l-layer SNF initialized with the weight scaling (Eq. (3)). For any $\\alpha \\ge 1$, we have, in an approximate sense,\n\n$Z^{(i)} \\sim \\text{arcsin}(-1,1), \\quad \\forall i \\in \\{2,...,l \u2013 1\\}.$ (4)\n\nThe detailed statement of this proposition and the numerical derivation will be given in Appendix A."}, {"title": "3.4 THE PARADIGM OF EFFICIENCY-ORIENTED INITIALIZATION", "content": "The latter phenomenon motivates us to develop an alternative perspective on a goodness of a neural field initialization, which may replace or be used jointly with the distribution preservation principle. More specifically, we consider an optimization problem\n\n$\\max_{\\alpha\\ge1} \\text{speed}(\\alpha)/\\text{speed}(1) \\quad \\text{subject to }\\quad \\text{TestLoss}(\\alpha) \u2013 \\text{TestLoss}(1) \\le \\epsilon,$ (5)\n\nwhere speed(\u03b1) denotes the training speed (e.g., a reciprocal of the number of steps required) of SNF when initialized with a weight scaling, and TestLoss(\u03b1) denotes the loss on the unseen coordinates when trained from the initialization. Given this objective, the key questions we try to address are:\n\u2022 Why does the weight scaling accelerate the training of SNFs?\n\u2022 Is weight scaling generally applicable for other data modalities?\n\u2022 How can we select a good \u03b1, without expensive per-datum tuning?"}, {"title": "4 UNDERSTANDING THE EFFECTS OF THE WEIGHT SCALING", "content": "We now take a closer look at the weight scaling to understand how it speeds up the training of sinusoidal neural fields. Before diving deeper into our analyses, we first discuss why a conventional understanding does not fully explain the phenomenon we observed for the case of SNF.\nComparison with lazy training. At the first glance, it is tempting to argue that this phenomenon is connected to the \"lazy training\" phenomenon (Chizat et al., 2019; Woodworth et al., 2020): Scaling the initial weights (W \u2192 \u03b1W) amplifies the initial model functional (f \u2192 \u03b1f), which in turn leads to the model to be linearized; then, one can show that the SGD converges at an exponential rate, i.e., the fast training, with the trained parameters being very close to the initial one. However, there is a significant gap between this theoretical understanding and our observations for the SNFs:\n\u2022 The proposed weight scaling does not amplify the initial functional, as sinusoidal activations are not positively homogeneous (i.e., \u03c3(ax) \u2260 \u03b1\u00b7 \u03c3(x), even for positive \u03b1, x), unlike ReLU. In fact, WS keeps the last layer parameters unscaled, making it further from any functional amplification, and empirically works better than a direct amplification of functional (Saratchandran et al., 2024).\n\u2022 At a practical scale, a na\u00efve scaling of initial weights for ReLU networks leads to both high training and test losses, due to a high condition number (Chizat et al., 2019, Appendix C). In contrast, the training loss of the WS remains very small even at the practical tasks.\nIn light of these differences, we provide alternative theoretical and empirical analyses that can help explain why the WS accelerates training, unlike in ReLU nets. In particular, we reveal that:\n\u2022 WS on SNFs has two distinctive effects from ReLU nets, in terms of the initial functional and the layerwise learning rate; the former is the major contributor to the acceleration (Section 4.1).\n\u2022 At initialization, WS increases both (1) the frequency of each basis, and (2) the relative power of the high-frequency bases (Section 4.2).\n\u2022 WS also results in a better-conditioned optimization trajectory (Section 4.3)."}, {"title": "4.1 TWO EFFECTS OF WEIGHT SCALING ON SINUSOIDAL NETWORKS", "content": "There are two major differences in how the weight scaling impacts SNF training compared to its effect on conventional neural networks with positive-homogeneous activation functions, e.g., ReLU: (1) Initial functional: Larger weights lead to higher frequency (low-level) features in SNF, whereas in ReLU nets scaling up does not change the neuronal directions (more discussions in Section 4.2). (2) Larger early layer gradients: Unlike ReLU nets, scaling up SNF weights has a disproportionate impact on the layerwise gradients; earlier layers receive larger gradients (see remarks below).\nMotivated by this insight, we compare which of these two elements play a dominant role in speeding up the SNF training by decoupling these elements (Fig. 3). In particular, we analyze how changing only the initial weights-with the layerwise learning rates that are scaled down to match the rates of unscaled SNF-affects the speed-generalization tradeoff; we also compare with the version where the initial function stays the same as default, but the learning rates on early layers are amplified. From the results, we observe that the effect of having a good initial functional, with rich high-frequency functions, play a dominant role on speeding up the SNF training. The disproportionate learning rate also provides a mild speedup, while requiring less sacrifice in the generalizability.\nHow are the early layer gradients amplified? Essentially, this is also due to the lack of positive-homogeneity of the activation function. To see this, consider a toy two-layer neural net with width one $f(x) = w^{(2)} \\sigma(w^{(1)}x)$. The loss gradient for each layer weight is proportional to the layerwise gradients of the function, which is $\\nabla_wf = (w^{(2)} x\\sigma' (w^{(1)} x), \\sigma(w^{(1)}x))$. For ReLU activations, both entries of $\\nabla_wf$ grow linearly as we increase the scale of both weights. For sin activations, on the other hand, the first entry scales linearly, but the scale of the second entry remains the same. Following a similar line of reasoning, we can show that the first layer of a depth-l sinusoidal network has a gradient that scales as $\\alpha^{l-1}$, while the last layer gradient scales as 1. We provide a more detailed discussion on the \u03b1-dependency of layerwise gradients in Appendix B.3."}, {"title": "4.2 FREQUENCY SPECTRUM OF THE INITIAL FUNCTIONAL", "content": "We have seen that the initial functional has a profound impact on the training speed, but how does the weight scaling affect the functional? The answer is easy for two-layer nets; the weight scaling amplifies the frequency of each hidden layer neuron, enabling the model to cover wider frequency range with hardly updating the features generated by the first layer.\nHow about for deeper networks? Our theoretical and empirical analyses reveals that, for deeper SNFs, the weight scaling also increases the relative power of the higher-frequency bases. Thus, the weight-scaled networks may suffer less from the spectral bias, and can easily represent signals with large high-frequency components (Rahaman et al., 2019).\nTheoretical analysis. We now show theoretically that the weight scaling increases the relative power of the higher-frequency bases. For simplicity, consider again a three-layer bias-free SNF\n\n$f(x; W) = W^{(3)} \\text{sin}(W^{(2)} \\text{sin}(W^{(1)}x)),$ (6)\n\nwith sinusoidal activation functions. Then, it can be shown that the overall frequency spectrum of the function $f(\\cdot; W)$ can be re-expressed as the multiples of the first layer frequencies, via Fourier series expansion and the Jacobi-Anger identity (Y\u00fcce et al., 2022)."}, {"title": "Lemma 4.1 (Corollary of Y\u00fcce et al. (2022)).", "content": "The network Eq. (6) can be rewritten as\n\n$f(x; W) = 2W^{(3)} \\sum_{l\\in \\mathbb{Z}_{\\text{odd}}} J_l(W^{(2)}) \\text{sin}(lW^{(1)}x),$ (7)\n\nwhere $J_l(\\cdot)$ denotes the Bessel function (of the first kind) with order l.\nGiven the Bessel form (7), let us proceed to analyze the effect of weight scaling. By scaling the first layer weight, i.e., $W^{(1)} \\rightarrow \\alpha W^{(1)}$, we are increasing the frequencies of each sinusoidal basis by the factor of \u03b1. That is, each frequency basis becomes a higher-frequency sinusoid.\nScaling the second layer weight, on the other hand, acts by affecting the magnitude of each sinusoidal basis, i.e., $J_l(W^{(2)})$. For this quantity, we can show that the rate that the Bessel function coefficients evolve as we consider a higher order harmonics (i.e., larger l) scales at the speed proportional to \u03b1\u00b2. That is, the relative power of the higher-order harmonics becomes much greater as we consider larger scaling factor \u03b1. To formalize this point, let us further simplify the model (6) to have width one. Then, we can prove the following sandwich bound:"}, {"title": "Lemma 4.2 (Scaling of harmonics).", "content": "For any nonzero $W^{(2)} \\in (-\\pi/2, \\pi/2)$, we have:\n\n$\\frac{J_{l+2}(W^{(2)})}{(2l+2)(2l+4)} < \\frac{J_{l+2}(\\alpha \\cdot W^{(2)})}{J_l(\\alpha \\cdot W^{(2)})} < \\frac{(W^{(2)})^2}{(2l + 1)(2l + 3)}.$ (8)\n\nIn other words, replacing $W^{(2)} \\leftrightarrow \\alpha \\cdot W^{(2)}$ increases the growth rate $J_{l+2}/J_l$ by \u03b1\u00b2. We note that there exists a restriction in the range $W^{(2)}$ which prevents us from considering very large \u03b1. However, empirically, we observe that such amplification indeed takes place for the practical ranges of \u03b1 where the generalization performance remains similar.\nSumming up, the weight scaling has two effects on SNFs with more than two layers. First, the WS increases the frequencies of each sinusoidal basis by scaling the first layer weights. Second, the WS increases the relative weight of higher-frequency sinusoidal bases, by scaling the weight matrices of the intermediate layers."}, {"title": "4.3 \u039f\u03a1\u03a4\u0399MIZATION TRAJECTORIES OF WEIGHT-SCALED NETWORKS", "content": "Now, we study the optimization properties of SNFs through the lens of the empirical neural tangent kernel (eNTK). We empirically analyze two properties of the eNTK eigenspectrum of the SNF, which are relevant to the training speed: (1) condition number, and (2) kernel-task alignment.\nCondition number. Roughly, the condition number denotes the ratio \u03bbmax/\u03bbmin of the largest and the smallest eigenvalues of the eNTK; this quantity determines the exponent of the exponential convergence rates of the kernelized models, with larger condition number meaning the slower training (Chizat et al., 2019). In Fig. 5a, we plot how the condition numbers of the eNTK evolve during the training for SNFs, under weight scaling or frequency tuning; for numerical stability, we take an average of top-5 and bottom-5 eigenvalues and take their ratio. We observe that the weight-scaled SNFs tend to have a much smaller condition number at initialization, which becomes even smaller during the training. On the other hand, the SNF initialized with the standard initialization scheme suffers from a very high condition number, which gets even higher during the training."}, {"title": "5 EXPERIMENTS", "content": "In this section, we first address whether the weight scaling is effective in other data domains (Section 5.1); our answer is positive. Then, we discuss the factors that determine the optimal value of scaling factor \u03b1 for the given target task (Section 5.2); we find that the optimal value does not depend much on the nature of each datum, but rather relies on the structural properties of the workload."}, {"title": "5.1 MAIN EXPERIMENTS", "content": "We validate the effectiveness of WS in different data domains by comparing against various neural fields. To compare the training speed, we fix the number of training steps and compare the training accuracy. In particular, we consider the following tasks and baselines."}, {"title": "Task: Image regression.", "content": "The network is trained to approximate the signal intensity $c_i$ for each given normalized 2D pixel coordinates $(x, y)$. For our experiments, we use Kodak (Kodak, 1999) and DIV2K (Agustsson & Timofte, 2017) datasets. Each image is resized to a resolution of $512\\times512$ in grayscale, following Lindell et al. (2022); Seo et al. (2024). We report the training PSNR after a full-batch training for 150 iterations. For all NFs, we use five layers with width 512."}, {"title": "Task: Occupancy field.", "content": "The network is trained to approximate the occupancy field of a 3D shape, i.e., predict '1' for occupied coordinates and '0' for empty space. We use the voxel grid of size $512 \u00d7 512x 512$ following Saragadam et al. (2023). For evaluation, we measure the training intersection-over-union (IoU) after 50 iterations on the 'Lucy' data from the 'Standard 3D Scanning Repository,' with the batch size 100k. We use NFs with five layers and width 256."}, {"title": "Task: Spherical data.", "content": "We use 10 randomly selected samples from the ERA5 dataset, which contains temperature values corresponding to a grid of latitude $\\phi$ and longitude $\\theta$, using the geographic coordinate system (GCS). Following Dupont et al. (2022), the network inputs are lifted to 3D coordinates, i.e., transforming $(\\phi, \\theta)$ to $(\\text{cos}(\\phi) \\text{cos}(\\theta), \\text{cos}(\\phi) \\cdot \\text{sin}(\\theta), \\text{sin}(\\phi))$. We report the training PSNR after 5k iterations of full-batch training. For NFs, we use five layers with width 256."}, {"title": "Task: Audio data.", "content": "Audio is a 1D temporal signal, and the network is trained to approximate the amplitude of the audio at a given timestamp. We use the first 7 seconds of \"Bach's Cello Suite No. 1,\" following Sitzmann et al. (2020b). We report the training PSNR after 1k iterations of full-batch training. For NFs, we use five layers with width 256."}, {"title": "Baselines.", "content": "We compare the weight scaling against seven baselines; we have selected the memory-efficient and versatile neural fields, rather than highly specialized and heavy ones, such as Instant-NGP (M\u00fcller et al., 2022). More concretely, we use the following baselines: (1) Xavier uniform: the method by (Glorot & Bengio, 2010) applied on SNF, (2) SIREN init.: SNF using the standard initialization of SIREN (Sitzmann et al., 2020b), (3) NFSL: A recent initialization scheme motivated by scaling law (Saratchandran et al., 2024). (4) ReLU+P.E.: ReLU nets with positional encodings (Mildenhall et al., 2020). (5) FFN: Fourier feature networks Tancik et al. (2020). (6) GaussNet: MLP with Gaussian activations (Ramasinghe & Lucey, 2022). (7) WIRE: MLP with Gabor wavelet-based activations (Saragadam et al., 2023). The baselines (1-3) use SNFs as the backbone architecture, while (4-7) uses other architectures with different activation functions."}, {"title": "Other details.", "content": "For selecting the hyperparameters for each NFs (including \u03b1), we have conducted a grid search. We use the Adam optimizer (Kingma, 2015) with a fixed learning rate specific to each data domain. Other experimental details are provided in Appendix E."}, {"title": "Result.", "content": "Table 1 reports the comprehensive results across various data domains. We observe that the weight scaling significantly outperforms all baselines throughout all tasks considered. In Appendix E.4, we provide further qualitative results on various modalities, where we find that the weight scaling enhances capturing the high frequency details. These results support our hypothesis that the richer frequency spectrum provided by the weight scaling mitigates the spectral bias (Section 4.2), thereby enabling a faster training of neural fields."}, {"title": "Additional experiments.", "content": "We have also conducted experiments on the NF-based dataset construction tasks (Papa et al., 2024). We report the results in Appendix E.2."}, {"title": "5.2 ON SELECTING THE OPTIMAL SCALING FACTOR", "content": "Now, we consider the problem of selecting an appropriate scaling factor without having to carefully tune it for each given datum. In particular, we are interested in finding the right \u03b1 that maximizes the training speed yet incurs only negligible degradations in generalizability. More concretely, consider the task of Kodak image regression and seek to select the \u03b1 that approximately solves\n\n$\\text{maximize}_{\\alpha} \\text{speed}(\\alpha) \\quad \\text{subject to } \\quad \\text{TestPSNR}(\\alpha) \\ge 0.95 \\cdot \\text{TestPSNR}(1).$ (10)\n\nwhere (again) the speed is the reciprocal of the number of steps required until the model meets the desired level of training loss, which we set to be the PSNR 50dB. Note that this optimization is similar to what has been discussed in Section 3.4.\nIn a nutshell, we observe in this subsection that this optimal scaling factor is largely driven by the structural properties of the optimization workload (such as model size and data resolution), and"}, {"title": "6 CONCLUSION", "content": "In this paper, we revisit the traditional initialization scheme of sinusoidal neural fields, and propose a new method called weight scaling, which scales the initialized weights by a specific factor. In practice, WS significantly accelerates the training speed of SNF across various data modalities, even outperforming recent neural field training methods. We have discovered that the difference between the traditional and WS initialization comes from both the change in the frequency spectrum of the initial functional and the change in the layerwise learning rates, where the former plays a more crucial role in acceleration. Through extensive theoretical and empirical analyses, we have revealed that the WS positively impacts the learning dynamics by maintaining a well-conditioned optimization path throughout the training process.\nLimitations and future direction. The limitations of our work lie in the fact that the analysis has been conducted for only neural fields using sinusoidal activation functions, rather than general neural field architectures; whether the weight scaling is effective on other architectures remains an open question. Also, from a theoretical perspective, our analyses does not provide a definitive answer on the acceleration mechanisms, lacking an explicit convergence bound. As a final remark,"}, {"title": "A DETAILED STATEMENT AND THE DERIVATION OF PROPOSITION 3.1", "content": "In this section, we provide a more detailed statement and derivation of Proposition 3.1, where we have extended the distribution-preserving claims of Sitzmann et al. (2020b) on \u03b1 = 1 to the cases of \u03b1 \u2265 1, thus covering the initialization with weight scaling.\nWe note that we generally follow the approach of Sitzmann et al. (2020b) in terms of the formalisms; we provide rough approximation guarantees (Lemmas A.1 and A.2), one of which relies on a numerical analysis of a function that is difficult to be expressed in a closed form (Lemma A.2).\nIn particular, we are concerned with showing the following two points:\n\u2022 Activations to pre-activations. We show that, for \u03b1 \u2265 1, the pre-activation of a layer with weight-scaled uniform activations and arcsin input distributions are approximately distributed as a Gaussian distribution with variance scaled by \u03b1 (Lemma A.1).\n\u2022 Pre-activations to activations. Then, we show that, for \u03b1 \u2265 1, post-activation distribution of the Gaussian distribution with \u03b1-scaled variance is distributed as an arcsin distribution (Lemma A.2).\nMore formally, our lemma are as follows.\nLemma A.1 (Arcsin to Gaussian distributions). Let the weights W\u2081 of the l-th layer be sampled i.i.d. from Unif(\u2212c/\u221an, c/\u221an), and the activation of the (l \u2013 1)-th layer Z1-1 follow the i.i.d. arcsin(-1, 1). Then, the distribution of the pre-activation of the l-th layer Y converges in distribution to a Gaussian distribution N(0, c\u00b2/6) as the number of hidden units n tends to infinity.\nLemma A.2 (Gaussian to arcsin distributions, with numerical proof). Let the pre-activation distribution X has a Gaussian distribution N(0, \u03b1\u00b2) for some \u03b1 > 1. Then the activation distribution Y = sin(X) is distributed as arcsin(-1, 1).\nNote that, by plugging in c = \u03b1\u221a6 to the Lemma A.1 as in the proposed weight scaling, we get the normal distribution N(0, \u03b1\u00b2) as an output distribution.\nWe provide the proofs of these lemma in Appendices A.1 and A.2, respectively. For all derivations, we simply ignore the issue of the frequency scaling term wo, and simply let it equal to 1."}, {"title": "A.1 PROOF OF LEM\u039c\u0391 \u0391.1", "content": "For simplicity, let us denote a pre-activation of a neuron in the lth layer by y. Let the weight vector connected to this output neuron as w\u2208 R, with each entries drawn independently from Unif(-c/\u221an, c/\u221an). Also, let the corresponding input activation from the (l \u2013 1)th layer be denoted by z \u2208 Rn, with each entry drawn independently from arcsin(-1, 1). Now, it suffices to show that y = w z converges in distribution to N (0, c\u00b2/6) as n goes to infinity.\nTo show this point, we simply invoke the central limit theorem for the i.i.d. case. For CLT to hold, we only need to check that each summand (wizi) of the dot product wz = \u2211wizi has a finite mean and variance.\n\nwz=\\sum_{i=1}^{d}WiZi\n(11)\nhas a finite mean and variance w\u00afz has a zero mean and a finite variance. We know that E[wizi] = 0, as two random variables are independent and E[wi] = 0. To show that the variance is finite, we can proceed as\n\nVar(wizi) = Var(wi)Var(zi) =\n\\frac{c^2}{3n} \\frac{1}{2}\n\\frac{c^2}{6n},\n(12)\nwhere the first equality holds as each variables are independent and zero-mean. Summing over n indices, we get what we want."}, {"title": "A.2 PROOF OF LEM\u039c\u0391 \u0391.2", "content": "By the 3\u03c3 rule, 99.7% of the mass of the Gaussian distribution N(0, \u03b1\u00b2) lies within the finite support [-3\u03c3, 3\u03c3]. The CDF of Y can be written as\n\nFy(y) = P(Y \u2264 y) = P(sin(X) \u2264 y).\n(13)"}, {"title": "B PROOFS IN SECTION 4", "content": ""}, {"title": "B.1 PROOF OF LEM\u039c\u0391 4.1", "content": "To formally prove this lemma, we first bring the generalized form from Y\u00fcce et al. (2022).\nLemma B.1 (From Y\u00fcce et al. (2022, Appendix A.2)). Suppose that the model can be written as f(x) = W(3) sin (W(2) sin(W(1)x)), where W(1) \u2208 Rd1, W(2) \u2208 Rd2\u00d7d1, and W(3) \u2208 R1\u00d7d2. Then"}]}