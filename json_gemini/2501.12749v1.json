{"title": "ESTIMATING THE CONFORMAL PREDICTION THRESHOLD FROM NOISY LABELS", "authors": ["Coby Penso", "Jacob Goldberger", "Ethan Fetaya"], "abstract": "Conformal Prediction (CP) is a method to control prediction uncertainty by producing a small prediction set, ensuring a predetermined probability that the true class lies within this set. This is commonly done by defining a score, based on the model predictions, and setting a threshold on this score using a validation set. In this study, we address the problem of CP calibration when we only have access to a validation set with noisy labels. We show how we can estimate the noise-free conformal threshold based on the noisy labeled data. Our solution is flexible and can accommodate various modeling assumptions regarding the label contamination process, without needing any information about the underlying data distribution or the internal mechanisms of the machine learning classifier. We develop a coverage guarantee for uniform noise that is effective even in tasks with a large number of classes. We dub our approach Noise-Aware Conformal Prediction (NACP) and show on several natural and medical image classification datasets, including ImageNet, that it significantly outperforms current noisy label methods and achieves results comparable to those obtained with a clean validation set.", "sections": [{"title": "Introduction", "content": "In machine learning for safety-critical applications, the model must only make predictions it is confident about. One way to achieve this is by returning a (hopefully small) set of possible class candidates that contain the true class with a predefined level of certainty. This is a natural approach for medical imaging, where safety is of the utmost importance and a human makes the final decision. This allows us to aid the practitioner, by reducing the number of possible diagnoses he needs to consider, with a controlled chance of mistake. The general approach to return a prediction set without any assumptions on the data distribution (besides i.i.d. samples) is called Conformal Prediction (CP) [1, 19]. It creates a prediction set with the guarantee that the probability of the correct class being within this set meets or exceeds a specified confidence threshold. The goal is to return the smallest set possible while maintaining the confidence level guarantees. Recently, with the growing use of neural network systems in safety-critical applications such as medical imaging, CP has become an important calibration tool [11, 12, 14]. We note that CP is a general framework rather than a specific algorithm. The most common approach builds the prediction set using a conformity score, and different algorithms mostly vary in terms of how the conformity score is defined.\nWhen dealing with conformal predictions, a critical challenge arises in applications such as medical imaging due to label noise. In these domains, datasets frequently contain noisy labels stemming from ambiguous data that can confuse even clinical experts. Furthermore, physicians may disagree on the diagnosis for the same medical image, leading to inconsistencies in the ground truth labeling. Noisy labels also occur when applying differential privacy techniques to overcome privacy issues [5]. While significant efforts have been devoted to the problem of noise-robust network training [18, 21], the challenge of calibrating the models has only recently begun to receive attention.\nIn this study, we tackle the challenge of applying CP to classification networks using a validation set with noisy labels. [3] suggested ignoring label noise and simply applying the standard CP algorithm on the noisy labeled validation set. This strategy results in large prediction sets especially when there are many classes. A recent study suggests estimating"}, {"title": "Background", "content": null}, {"title": "Conformal Prediction", "content": "Consider a setup involving a classification network that categorizes an input x into k predetermined classes. Given a coverage level of 1 \u2013 \u03b1, we aim to identify the smallest possible prediction set (a subset of these classes) ensuring the correct class is within the set with a probability of at least 1 \u2013 \u03b1. A straightforward strategy to achieve this objective involves sequentially incorporating classes from the highest to the lowest probabilities until their cumulative sum exceeds the threshold of 1 \u2013 \u03b1. Despite the network's output adopting a mathematical distribution format, it does not inherently reflect the actual class distribution. Typically, the network will not be calibrated and it tends to be overly optimistic [6]. Consequently, this straightforward approach doesn't assure the inclusion of the correct class with the desired probability.\nThe first step of the CP algorithm involves forming a conformity score S(x, y) that measures the network's uncertainty between x and its true label y (larger scores indicate worse agreement). The Homogeneous Prediction Sets (HPS) score [19] is SHPS(x, y) = 1 \u2212 p(y|x; \u03b8), s.t. \u03b8 is the network parameter set. The Adaptive Prediction Score (APS) [16] is the sum of all class probabilities that are not lower than the probability of the true class:\n$$S_{APS}(x,y) = \\sum_{{i|p_i \\geq p_y}} p_i$$\nsuch that pi = p(y = i|x; \u03b8) and py is the probability of the label y. The RAPS score [2] is a variant of APS, which is defined as follows:\n$$S_{RAPS}(x,y) = \\sum_{{i|p_i \\geq p_y}} p_i + \\alpha \\cdot max(0, (NC \u2013 b))$$\ns.t. NC = |{i|pi \u2265 py}| and \u03b1, b are parameters that need to be tuned. RAPS is especially effective in the case of a large number of classes where it explicitly encourages small prediction sets.\nWe can also define a randomized version of a conformity score. For example in the case of APS we define:\n$$S_{rand-APS}(x, y, u) = \\sum_{{i|p_i>p_y}} p_i \\cdot u, \\quad u \\sim U[0, 1].$$\nThe random version tends to yield the required coverage more precisely and thus it produces smaller prediction sets [1]. The CP prediction set of a data point x is defined as Cq(x) = {y|S(x, y) \u2264 q} where q is a threshold that is found using a labeled validation set (x1, y1), ..., (xn, yn). The CP theorem states that if we set q to be the (1 \u2013 \u03b1) quantile of the conformal scores S(x1, y1), ..., S(xn, yn) we can guarantee that 1-\u03b1 \u2264 p(y \u2208 C(x)) \u2264 1-\u03b1 + 1/(n+1), where x is a test point and y is its the unknown true label [19]. In the random case there is still a coverage guarantee, which is defined by marginalizing over all test points x and samplings u from the uniform distribution [16]. Note that the coverage guarantee is for a marginal probability over all possible test points and coverage may be worse or better for different points. It can be proved that obtaining a conditional coverage guarantee is impossible [4]."}, {"title": "Our Approach", "content": null}, {"title": "Setting the Threshold Given Noisy Labels", "content": "Here we show how, given a simple noise model and a known noise level, we can get the correct CP threshold based on noisy data. We will generalize this beyond the simple noise model in the following section. Consider a network that classifies an input x into k pre-defined classes. Given a conformity score S(x, y) and a specified coverage 1 \u2013\u03b1, the goal of the conformal prediction algorithm is to find a minimal q such that p(y \u2208 Cq(x)) \u2265 1 \u2212 \u03b1. Let (x1, \u1ef91), ..., (xn, yn) be a validation set with noisy labels and let yi be the unknown correct label of xi. Let si = S(xi, yi) be the conformity"}, {"title": null, "content": "score of (xi, Yi). We assume that the label noise follows a uniform distribution, where with a probability of \u03f5, the correct label is replaced by a label that is randomly sampled from the k classes:\n$$p(\\tilde{y} = j|y = i) = \\mathbb{1}_{{i=j}} (1-\\epsilon) + \\frac{\\epsilon}{k}$$\nUniform noise is relevant, for example, when applying differential privacy techniques to overcome privacy issues [5]. In that setup the noise level \u03f5 is usually known. In other applications such as medical imaging, where the noise parameter \u03f5 is not given, it can be estimated with sufficient accuracy from the noisy-label data during training [24, 9, 10]. We can write \u1ef9 as \u1ef9 = (1 \u2212 z) \u00b7 y + z \u00b7 u, s.t. u is a random label uniformly sampled from {1, ..., k} and z is a binary random variable (p(z = 1) = \u03f5) indicating whether the label of the sample (x, y) was replaced by a random label or not. For each candidate threshold q denote:\n$$F^c(q) = p(y \\in C_q(x)), \\quad F^n(q) = p(\\tilde{y} \\in C_q(x)), \\quad F^r(q) = p(u \\in C_q(x)),$$\nwhere Fc, Fn, and Fr represent the clean, noisy and random labels. Note as well that each one is the CDF of the appropriate conformal score function, e.g., Fc(q) = p(y \u2208 Cq(x)) = p(S(x,y) \u2264 q).\nIt is easily verified that\n$$F^n(q) = p(z = 0)F^c(q) + p(z = 1)F^r(q) = (1 \u2013 \\epsilon)F^c(q) + \\epsilon F^r (q).$$\nFor each value q, we can estimate Fn(q) from the noisy validation set:\n$$\\hat{F}^n(q) = \\frac{1}{n} \\sum_{i} \\mathbb{1}_{{{\\tilde{y}}_i \\in C_q(x_i)}} = \\frac{1}{n} \\sum_{{i|s_i \\leq q}} 1.$$\nNote that q is the \\hat{F}^n(q)-quantile of s1, ..., sn. Similarly we can also estimate Fr(q):\n$$\\hat{F}^r(q) = \\frac{1}{n} \\sum_{i} p(u_i \\in C_q(x_i)) = \\frac{1}{nk} \\sum_i |C_q(x_i)|,$$\ns.t. ui is uniformly sampled from {1, ..., k}.\nBy substituting (6) and (7) in (5) we obtain an estimation of Fc(q) = p(y \u2208 Cq(x)) based on the noisy validation set and the noise level \u03f5:\n$$\\hat{F}^c(q) = \\frac{\\hat{F}^n(q) \u2013 \\epsilon \\hat{F}^r(q)}{1-\\epsilon}.$$\nFor each candidate q we first compute \\hat{F}^n(q) and \\hat{F}^r(q) and then by using (8) obtain the coverage estimation \\hat{F}^c(q). Given a coverage requirement (1 \u2013 \u03b1), we can thus use the noisy validation set to find a threshold q such that \\hat{F}^c(q) = 1 \u2013 \u03b1. Note that since Fc(q) is monotonous, it seems reasonable to search for the threshold q using the bisection method. However, as \\hat{F}^c(q) is an approximation based on the difference between two monotonic functions, it might not be exactly monotonous. We therefore find the threshold q using an exhaustive grid search. We note that even with an exhaustive search the entire runtime is negligible compared to the training time. Furthermore, we can narrow the threshold search domain as follows:\nLemma 3.1. For every threshold q we have: \\hat{F}^n/k < \\hat{F}^r (q).\nProof. Denote A = {i|\u1ef9i \u2208 Cq(xi)} and B = {i|1 \u2264 |Cq(xi)|}. Note that Fn(q) = |A|/n.\n$$|B| = \\sum_{i \\in B} 1 \\leq \\sum_{i \\in B} |C_q(x_i)| \\leq \\sum_{i=1}^n |C_q(x_i)| = nk\\hat{F}^r (q).$$\nFinally A \u2286 B implies that: \\hat{F}^n(q) = |A|/n \u2264 |B|/n \u2264 k\\hat{F}^r (q).\nTheorem 3.2. Let q1 be the ((1\u2212\u03b1)(1 \u2212 \u03f5))/(1 \u2212 \u03f5)) quantile of s1, \u2026, sn and let q2 be the ((1 \u2212 \u03b1) + \u03b1\u03f5) quantile. If q satisfies \\hat{F}^c(q) = 1 \u2212 \u03b1 then q1 \u2264 q < q2.\nProof. Assume q satisfies Fc(q) = 1 \u2013 \u03b1. Eq. (8) implies that\n$$1 \u2013 \\alpha = \\hat{F}^c(q) = \\frac{\\hat{F}^n(q) \u2013 \\epsilon\\hat{F}^r (q)}{1-\\epsilon} \\Rightarrow \\hat{F}^n(q) = (1 \u2013 \\alpha)(1 \u2013 \\epsilon) + \\epsilon\\hat{F}^r (q).$$"}, {"title": null, "content": "Algorithm 1 Noise-Aware Conformal Prediction (NACP) for uniform noise\nInput: A conformity score S(x, y), a coverage level 1-\u03b1 and a validation set (x1, \u1ef91), ..., (xn, yn), s.t. the labels are corrupted by a uniform noise with parameter \u03f5.\nSet q1 to be the ((1-\u03b1)(1 \u2212 \u03f5))/(1 \u2212 \u03f5)) quantile of S(x1, \u1ef91), \u2026\u2026, S(xn, \u1ef9n) and set q2 to be (((1 \u2212 \u03b1) + \u03b1\u03f5)) quantile.\nFor each candidate threshold q compute:\n$$\\hat{F}^n(q) = \\frac{1}{n} \\sum_i \\mathbb{1}_{{y_i \\in C_q(x_i)}}, \\quad \\hat{F}^r(q) = \\frac{1}{nk} \\sum_i |C_q(x_i)|, \\quad \\hat{F}^c(q) = \\frac{\\hat{F}^n(q) - \\epsilon\\hat{F}^r(q)}{1-\\epsilon}$$\nApply a grid search to find q \u2208 [q1, q2] that satisfies \\hat{F}^c(q) = 1-\u03b1.\nThe prediction set of a test sample x is Cq(x) = {y|S(x,y) < q}.\nCoverage guarantee: p(y \u2208 Cq(x)) \u2265 1 \u2212 \u03b1 - \u0394(n, \u03f5, \u03b4) with probability (1 \u2013 \u03b4) over the noisy validation set sampling (see Theorem 3.5).\nSince 0 \u2264 Fr(q) \u2264 1 we get that:\n$$(1 \u2013 \\alpha) (1 \u2013 \\epsilon) \\leq \\hat{F}^n(q) \\leq (1 \u2013 \\alpha) + \\alpha\\epsilon = \\hat{F}^n(q_2).$$\nFor every q we have \\hat{F}^n(q)/k \u2264 Fr (q) (Lemma 3.1). Hence, (1 \u2212 \u03b1)(1 \u2013 \u03f5) \u2264 \\hat{F}^n(q) (10) implies that (1 \u2212 \u03b1)(1 \u2212 \u03f5)/k \u2264 \\hat{F}^r (q). Combining this inequality with Eq. (9) yields a better lower bound: (1 \u2212 \u03b1)(1 \u2212 \u03f5)(1 + \u03f5/k) \u2264 \\hat{F}^n(q). Iterating this process yields:\n$$(1-\\alpha)(1-\\epsilon) (1 + \\frac{\\epsilon}{k} + (\\frac{\\epsilon}{k})^2 + \u2026) = (1-\\alpha)\\frac{1-\\epsilon}{1-\\frac{\\epsilon}{k}} = \\hat{F}^n(q_1) \\leq \\hat{F}^n(q).$$\nFinaly, \\hat{F}^n (q) is a monotonically increasing function of q which implies that q1 \u2264 q \u2264 q2.\nAs an alternative to the grid search we can sort the noisy conformity scores si = S(xi, \u1ef9i) and look for the minimal i such that Fc(si) > 1 \u2013 \u03b1. In the noise-free case Fc is piece-wise constant, with jumps determined exactly by the order statistics si, namely, Fc(si) = i/n and thus this algorithm coincides with the standard CP algorithm. In the noisy case Fc(q) depends on the conformity scores of all the k classes and thus its structure is more complicated. We dub our algorithm Noise-Aware Conformal Prediction (NACP), and summarize it in Algorithm Box 1. Note that in the noise-free case (\u03f5 = 0) the NACP algorithm coincides with the standard CP algorithm and selects q that satisfies Fc(q) = Fn(q) = 1 \u2013 \u03b1, i.e., q is the 1 \u2013 \u03b1 quantile of the validation set conformity scores."}, {"title": "Prediction Size Comparison", "content": "We next compare our NACP approach analytically to Noisy-CP [3] in terms of the average size of the prediction set.\nTheorem 3.3. Let q and \u1fb7 be the thresholds computed by the NACP and the Noisy-CP algorithms respectively. Then q \u2264 \u1fb7 if and only if Fr (\u1fb7) \u2264 1 \u2212 \u03b1.\nProof. The threshold \u1fb7 computed by the Noisy-CP algorithm (by applying standard CP on the noisy validations set) satisfies Fn(\u1fb7) = (1 \u2212 \u03b1). The true threshold q satisfies Fn(q) = (1 \u2212 \u03b1)(1 \u2013 \u03f5) + \u03f5Fr (q) (9). Looking at the difference\n$$\\hat{F}^n(\\tilde{q}) \u2013 \\hat{F}^n(q) = 1 \u2013 \\alpha \u2013 ((1 \u2013 \\alpha)(1 \u2013 \\epsilon) \u2013 \\epsilon\\hat{F}^r (q)) = \\epsilon(1 \u2013 \\alpha \u2013 \\hat{F}^r (q)).$$\nHence from the monotonicity of Fn(q) we have q \u2264 \u011f iff Fn(q) \u2264 Fn(\u1fb7) iff Fr (\u1fb7) \u2264 1 \u2212 \u03b1.\nThe theorem above states that if the size of the prediction set obtained by NACP is less than k(1 \u2212 \u03b1), NACP is more effective than Noisy-CP. For example, assume k = 100 and 1 \u2013 \u03b1 = 0.9. In this case, if the average size of the NACP prediction set is less than 90, NACP is more effective than Noisy-CP. We also see from eq. (11) that the smaller Fr is the larger the gap between the two methods. Since Fr is inversely proportional to the number of classes, we expect the difference to be substantial when there is a large number of classes to consider, which is exactly where CPs' ability to reliably exclude possible classes is very useful. In our experiments, we indeed found a considerable gap between the two methods when we experimented on classification tasks with a large number of classes."}, {"title": "Coverage Guarantees", "content": "We next provide a coverage guarantee for NACP. We show that if we apply the NACP to find a threshold q for 1-\u03b1+\u0394, then P(y \u2208 Cq(x)) \u2265 1 \u2013 \u03b1 were \u0394 depends on the validation set size. \u0394 is a finite-sample term that is needed to approximate the CDF to set the threshold instead of simply picking a predefined quantile. Because \u0394 can be computed, one can adjust the \u03b1 used in the NACP algorithm to get the desired coverage guarantee. However, we note that we empirically found this bound to be over-conservative, and that the un-adjusted method does reach the desired coverage.\nLemma 3.4. Given \u03b4 > 0, define \u0394 = \nlog(4/\u03b4)/(2nh^2) such that h = (1+\u03f5)/(1-\u03f5) and n is the size of the noisy validation set. Then\np(supq|Fc(q) \u2013 Fc(q)| > \u0394) \u2264 \u03b4,\nsuch that the probability is over the validation set.\nProof. The Dvoretzky-Kiefer-Wolfowitz (DKW) inequality [13] states that if we estimate a CDF F from n samples using the empirical CDF Fn then p(supx |Fn(x) \u2013 F(x)| > \u0394) \u2264 2exp(-2n\u03942). Eq. (8) defines Fc(q) using Fn(q) and Fr(q). Both are empirical CDF, so from the DKW theorem and the union bound we get that:\np(supq|Fr (q) \u2013 Fr(q)| > h\u0394 or supq |Fn(q) \u2013 Fn(q)| > h\u0394) \u2264 4exp(-2nh2\u03942) = \u03b4.\nUsing eq. (8) we get that with probability at least 1 \u2013 \u03b4 for every q:\n$$\\hat{F}^c(q) = \\frac{\\hat{F}^n(q) \u2013 \\epsilon\\hat{F}^r(q)}{1-\\epsilon} = F^c(q) + \\frac{(\\hat{F}^n(q) \u2013 Fn(q)) + \\epsilon(\\hat{F}^r(q) \u2013 Fr(q))}{1-\\epsilon} < F^c(q) + \\frac{h\\Delta+ \\epsilon h\\Delta}{1-\\epsilon} = F^c(q)+ \\Delta.$$\nSimilarly, we can show that Fc(q) > Fc(q) \u2013 \u0394 which completes the proof.\nThe proof of the main theorem now follows the standard CP proof, taking the inaccuracy in estimating Fc(q) into account.\nTheorem 3.5. Assume you have a noisy validation set of size n with noise level \u03f5 and set \u0394(n, \u03f5, \u03b4) = (1+\u03f5)/sqrt(2nh^2)sqrt(log(4/\u03b4) s.t. (1-\u03f5) and that you pick q such that Fc(q) = 1 \u2013 \u03b1 + \u0394. Then with probability at least 1 \u2013 \u03b4 (over the validation set), we have that if (x, y) are sampled from the clear label distribution we get:\n1 - \u03b1 \u2264 p(y \u2208 Cq(x)) \u2264 1 \u2212 \u03b1 + 2\u0394.\nProof. Given a clean test pair (x, y), with probability \u03b4 over the validation set, we have:\np(y \u2208 Cq(x)) = p(S(x,y) < q) = Fc(q) > Fc(q) \u2013 \u0394 = 1 \u2212 \u03b1.\nIn a similar way: p(y \u2208 Cq(x)) = F(q) < fc(q) + \u0394 = 1 \u2212 \u03b1 + 2\u0394.\nAs the size of the noisy validation set, n, tends to infinity, \u0394 converges to zero and thus the noisy threshold converges to the noise-free threshold.\n[17] proposed a CP algorithm for the same setup of noisy labels where the noise matrix is given (or estimated based on noise-free data). They used the distribution of correct labels given the noisy labels while we use the more natural distribution of the noisy labels given the correct label. As a result, they need to know the class frequencies for both the clean and noisy labels, whereas we do not. Another major difference between the algorithms is the finite sample coverage guarantee term. In Section 4 we empirically compare the finite sample terms of the two methods and show that while ours is effective for the case of uniform noise, their approach fails on datasets with a large number of classes such as CIFAR-100, TinyImageNet, and ImageNet, and produces prediction sets that contain all classes. A further distinction is that their finite sample coverage guarantee is established for the average of all the noisy validation sets. In contrast, our approach provides an individual coverage guarantee for nearly all (1 \u2013 \u03b4) of the sampled noisy validation sets."}, {"title": "A More General Noise Model", "content": "Next, we will extend our approach to a more general noise model. We will assume that the noisy label \u1ef9 is independent of x given y. We also assume that the noise matrix P(i,j) = p(\u1ef9 = j|y = i) is known and that the matrix P is invertible. For each q define the following matrices for the clear and the noisy data: Mq(l,i) = p(l \u2208 Cq(x), y = i) and M\u2084(l, i) = p(l \u2208 Cq(x), \u1ef9 = i). Assuming that, given the true label y, the r.v. x and \u1ef9 are independent, we obtain:\n$$M_q(l,i) = p(l \\in C_q(x), y = i) = \\sum_j p(l \\in C_q(x), \\tilde{y} = i, y = j)$$\n$$ = \\sum_j p(l \\in C_q(x), y = j)p(\\tilde{y} = i|y = j) = \\sum_j M_q(\\ell, j)P(j, i).$$\nWe can write (15) in matrix notation: Mq = MqP. Eq. (15) implies that:\n$$F^c(q) = p(y \\in C_q(x)) = p(y \\in C_q(x)) = \\sum_i p(i \\in C_q(x), y = i) = \\sum_i M_q(i, i) = Tr(M_q P^{-1}).$$\nWe can estimate matrix Mq from the noisy samples:\n$$\\hat{M}_q(\\ell, i) = \\frac{1}{n} \\sum_j \\mathbb{1}_{{\\{\\tilde{y}}_j=i, \\ell \\in C_q(x)}}, \\quad i, \\ell = 1, .., k.$$\nSubstituting (17) in (16) yields an estimation of the probability Fc(q) = p(y \u2208 Cq(x)):\n$$\\hat{F}^c(q) = Tr(\\hat{M}_q P^{-1}).$$\nThe final step is applying a grid search to find a threshold q such that \\hat{F}^c(q) = 1 \u2212 \u03b1.\nIn the case that P is a uniform noise matrix (4), the Sherman-Morison formula implies that P\u22121 = (1\u2212\u03f5)/(1\u2212k\u03f5))I - (\u03f5/(1\u2212\u03f5)(1\u2212k\u03f5)11T).\nTherefore,\n$$\\hat{F}^c(q) = Tr(\\hat{M}_q P^{-1}) = \\frac{1}{1-\\epsilon} Tr(\\hat{M}_q) \u2013 \\frac{\\epsilon}{1-\\epsilon} \\sum_{\\ell, i} \\hat{M}_q(\\ell, i) = \\frac{\\hat{F}^n (q) \u2013 \\epsilon \\hat{F}^r (q)}{1-\\epsilon}$$\nThus in the case of a uniform noise the coverage estimation (18) coincides with (8). If the noise matrix is unknown, it can be estimated from the noisy-label data during training [24, 9, 10]. The NACP method for a noise matrix model is summarized in Algorithm 2.\nWe can extend the finite sample term \u0394 that was developed for a uniform noise to obtain a theoretical coverage guarantee for a noise matrix model (see Section A.2). However, this approach yields large prediction sets especially in tasks with many classes and thus is ineffective. In the experiment section we show that in practice, even without adding finite sample terms, we obtain the required coverage probability."}, {"title": "Experiments", "content": "In this section, we evaluate the capabilities of our NACP algorithm on various medical and scenery imaging datasets."}, {"title": "Appendix / supplemental material", "content": null}, {"title": "General noise matrices", "content": "We define the two common general noise matrices. The Neighborhood noise as:\n$$P_{i,j} = p(\\tilde{y} = j|y = i) = \\begin{cases}\\xi & \\text{if } i=j\\\\(1-\\xi)/2 & \\text{if } |i - j| = 1 \\text{ and } i \\in (1,k)\\\\(1-\\xi)/2 & \\text{if } |i \u2013 j| = 1 \\text{ and } i \\notin (1, k)\\\\0 & \\text{otherwise}\\end{cases}$$\nThe Random noise is defined as: first, on the diagonal, we have \u03be. next, for each line (aka Vi) the rest of the values (i.e. k-1 items) are sampled from a random distribution ui vector of size k \u2013 1 and then normalized to sum up to 1 \u2013 \u03be to keep the matrix a probability matrix.\n$$P_{i,j=1} p(\\tilde{y} = j|y = i) = \\begin{cases}\\xi & \\text{if } i = j\\\\(1 \u2013 \\xi) \\cdot \\frac{u_i[j]}{\\sum_{i=1} u_i [i]} & \\text{otherwise}\\end{cases}$$\nIn our experiments, we set \u03be such that the diagonal would be the same as the uniform noise experiments, i.e. \u03be = 1 - \u03f5 + \u03f5/k where e = 0,2."}, {"title": "A finite sample term for the case of a general noise matrix", "content": "Theorem A.1. Let P be a general noise matrix. Given \u03b4 > 0, define \u0394 = sqrt(1/(2n))||P^{-1}||k sqrt(log(2k^2/\u03b4) where, k is the number of classes and n is the size of the noisy validation set. Then\np(supq|fc(q) \u2013 Fc(q)| > \u0394) < \u03b4.\nProof. From Eq. (18) we have |Fc(q) \u2013 Fc(q)| = |Tr(P-1Mq) \u2013 Tr(P-1Mq)| = | Tr(P\u2212\u00b9\u2206\u1e40q)| where \u2206Mq = Mq - Mq. We first note that Mq[i, j] = p(j \u2208 Cq(x), \u1ef9 = i) is not a CDF but we can define one that agrees with it for q\u2208 (-\u221e, C) which is the range of interest where C is a constant that bound the score function S(x, y) from above.\nWe define \u0160ij (x, \u1ef9) = [ S(x, j), if \u1ef9 = i\nC, if \u1ef9 \u2260 i so Mq[i, j] = p(\u0160ij(x, \u1ef9) \u2264 q) for q \u2208 (\u2212\u221e, C'). Now from the DKW theorem, we know that if we estimate a CDF using n samples then with probability at least 1 d we get a uniform bound on the error of size sqrt(log(2/\u03b4)/(2n). As we are estimating k2 matrix elements we can use the union bound to get that with probability 1 \u2013 8 the Vi, j, q \u2208 (\u2212\u221e, C') : |\u2206Mq| \u2264 sqrt(log(2k^2/\u03b4)/(2n). Now if we look at the infinity norm of P-1, then |(P\u22121\u2206Mq)i,j \u2264 ||P-1||k sqrt(log(2k^2/\u03b4)/(2n) As the trace is the sum of k such matrix entries, the total bound is k||P^{-1}||k sqrt(log(2k^2/\u03b4)/(2n)) for q\u2208 (-\u221e, C'). Since we know F\u00ba(q) = 1 for q > C, we can set F\u00ba(q) = 1 for q > C and get a bound for all q\u2208 R."}, {"title": "Table 1 with models trained with noisy labels", "content": "Reproducing Table 1 by replacing models publicly available with models trained with noisy labels. We combined our NACP with a noise-robust network training that estimated the noise level e as part of the training [9]. As a by-product of the noisy labels training procedure, we get a noise estimation that can be used in the calibration step. Therefore the following table has three additional rows denoted by * corresponding to ACNL, NACP, and NACP (w/o \u0394) using \u1ebd instead of the true \u20ac = 0.2. The estimated noise \u1ebd for each model is detailed in Table 6 and the results are shown in Table 7."}, {"title": "ImageNet with different calibration set size", "content": "In the following experiment, we test the performance of various conformal prediction methods under noisy labels as a function of the calibration set size on the ImageNet dataset. Figure 3 shows the mean size and coverage as a function of"}]}