{"title": "Relation-Aware Meta-Learning for Zero-shot Sketch-Based Image Retrieval", "authors": ["Yang Liu", "Jiale Du", "Xinbo Gao", "Jungong Han"], "abstract": "Sketch-based image retrieval (SBIR) relies on free-hand sketches to retrieve natural photos within the same class. However, its practical application is limited by its inability to retrieve classes absent from the training set. To address this limitation, the task has evolved into Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR), where model performance is evaluated on unseen categories. Traditional SBIR primarily focuses on narrowing the domain gap between photo and sketch modalities. However, in the zero-shot setting, the model not only needs to address this cross-modal discrepancy but also requires a strong generalization capability to transfer knowledge to unseen categories. To this end, we propose a novel framework for ZS-SBIR that employs a pair-based relation-aware quadruplet loss to bridge feature gaps. By incorporating two negative samples from different modalities, the approach prevents positive features from becoming disproportionately distant from one modality while remaining close to another, thus enhancing inter-class separability. We also propose a Relation-Aware Meta-Learning Network (RAMLN) to obtain the margin, a hyper-parameter of cross-modal quadruplet loss, to improve the generalization ability of the model. RAMLN leverages external memory to store feature information, which it utilizes to assign optimal margin values. Experimental results obtained on the extended Sketchy and TU-Berlin datasets show a sharp improvement over existing state-of-the-art methods in ZS-SBIR.", "sections": [{"title": "1. Introduction", "content": "Sketch-based image retrieval (SBIR) [20] aims to retrieve photos based on the queries of sketches. It is of significant value on touch-screen devices. However, it is very difficult to have all categories of the training set cover all query categories at the application stage. Shen et al. [22] combine SBIR with zero shot setting, and propose zero shot sketch-based image retrieval (ZS-SBIR). ZS-SBIR requires retrieving photos with the query sketches whose categories have not appeared in the training set, i.e., training and testing set have no class intersection. Therefore, it has more convenient application scenarios. Similar to SBIR tasks, many studies treat ZS-SBIR as a metric learning, where sketches and photos are mapped to a shared latent embedding space, and their similarity is measured by calculating the distance between their features.\nThe ZS-SBIR task faces the same fundamental challenge as SBIR, namely, the substantial modal gap between sketches and photos. Bridging this gap is challenging, as it complicates the model's ability to capture shared visual details across modalities. Consequently, samples within a single class tend to separate into two distinct clusters. Unlike conventional SBIR, however, ZS-SBIR demands not only a cohesive grouping of intra-class samples but also a reduction in the inter-group distance between modalities. Based on this, previous works have designed different frameworks, such as GANs [4], graph [41], and cycle reconstruction [7]. These works learn the projections of photos and sketches in the embedding space, which has advantages in the measurement of distance. In addition, ZS-SBIR suffers from the domain gap between seen and unseen classes. The distribution of seen classes is different from that of unseen classes. The model fails to transfer the knowledge learned from the seen classes to the unseen classes and is unable to align the two domains, leading to the domain shift problem. To solve this problem, doodle2search [3] uses triplet to reduce the distance between embedded sketch and photo if they belong to the same class and increase it if they belong to different classes. It only uses one triplet: sketch as the anchor while photo as positive/negative sample. SBTKNet [30] considers the inter-class and intra-class distance relationship. The two negative samples come from different modalities, and the negative pair in the same modality as the anchor is the main pair, which can further distance them.\nWe aim to investigate the impact of utilizing photos as anchor points on the alignment between photos and sketches, as well as on the construction of the embedding space. Anchors for both modalities help to explore the inter-modal and intra-modal alignment relations. On the other hand, different triplets correspond to different distances and relationships. We hope to find the different margin, which is the hyperparameter of triplet loss. We believe that a strategy grounded in features and global information could potentially yield effective results. In detail, we propose a novel model named Relation-Aware Meta-Learning Network (RAMLN) that adopts relation-aware quadruplet loss to construct generalizable embedding space relationship. Four samples are combined in pairs to form three pairs for contrastive learning. When the anchor is far away from the primary negative sample, the secondary negative sample helps stabilize the anchor position. We also employ a distance-based hard mining strategy. Instead of centering on a class group, it directly pushes the closest samples and pulls the farthest samples, which is more sensitive to the distance. At last, we decide to learn the margin of quadruplet loss inside a meta-learning. It is key for ZS-SBIR to use the seen data to improve the generalization ability of the model in the unseen domain. An external memory space is used to assist unseen features by recording uncommon memory in seen samples. So we adopt meta-learning with a memory-augmented network to adjust the margin parameters in the quadruplets to capture important rare features.\nThe main contributions of this work are summarized as follows:\n\u2022 We propose a novel relation-aware quadruplet loss to mine the inter-modal and intra-modal relation. Two negative samples from different modalities ensure that the anchor can avoid both non-similar seen and unseen domains.\n\u2022 We propose a meta-learning approach to learn the margin in quadruplet loss, adaptively determining the optimal margin value. The adaptive margin not only alleviates issues with improper margin settings but also accommodates domain variations across different categories and modalities.\n\u2022 We demonstrate the validity and high performance of the proposed model by conducting experimental evaluations on two popular ZS-SBIR datasets Sketchy and TU-Berlin."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Zero-Shot Sketch-Based Image Retrieval", "content": "Zero-shot learning is a more challenging and practically significant task, requiring the model to handle samples from classes that were not present in the training set. It is a subtask of transfer learning, aiming to effectively transfer knowledge from the seen domain to the unseen domain. ZS-SBIR performs SBIR in the zero-shot setting: the test class of ZS-SBIR does not appear in the training phase. Shen et al. [22] first introduced the SBIR problem under the zero-shot setting. Similar to SBIR, ZS-SBIR is a challenging task that addresses the modality gap between sketches and natural photos. Most works try to map the sketches and photos into an embedding space, and obtain the suitable position and distance of sample features in the space through metric learning, such as IIAE [10], SBTKNet [30] and so on. Jing et al. [11] propose a novel Augmented Multi-modality Fusion (AMF) framework that employs a knowledge discovery module to mimic novel knowledge unseen during the training phase, which helps train the model to adapt to the gap from the seen domain to the unseen domain. Moreover, many works have also tried to map semantic information to the embedding space as side information. It provides additional semantic knowledge to bridge the gap and localize the features of unseen classes in space. The earlier works like doodle2search [3, 9] and recent works like ocean [43], SkechGCN [5, 23, 41] adopt with language models, such as Word2vec, Bag-of-words and text transformer, to obtain text vectors through semantic information. But this does not mean that semantic information is necessary. Wang et al. [32] propose a novel Transferable Cou-"}, {"title": "3. Proposed Approach", "content": "The dataset of ZS-SBIR can be divided into seen classes and unseen classes. Seen classes are used as the train set while unseen classes are used as the validation set and test set. The train set is denoted by $X_{seen} = \\{(x_m, c_i) | c_i \\in C_{seen}, m \\in \\{ske, pho\\}\\}_{i=1}^N$, where x is feature extracted from images and c is label respectively. m is the modality with sketchy ske and photo pho. $C_{seen}$ denotes the seen classes set. N is the number of seen samples. Similarly, the test set is denoted by $X_{unseen} = \\{(x_m, c_j) | c_j \\in C_{unseen}, m \\in \\{ske, pho\\}\\}_{j=1}^M$ To satisfy the zero-shot setting, $C_{seen} \\cap C_{unseen} = \\varnothing$.\nAs shown in Figure. 2, key components of the proposed model RAMLN include: (i) With an additional linear layer at the end of the image encoder, the features of photos and sketches are mixed as its input. The features within the embedding space are meticulously prepared to facilitate downstream tasks. (ii) Memory-based meta-optimization for the margin of quadruplet, which is a hyper-parameter. Through an additional memory matrix, the model can adapt to the unseen domain. (iii) Selected relation-aware quadruplet loss based on the normalized Euclidean distance. (iv) A classification function connected with a projection layer, where the output dimension is the number of the class."}, {"title": "3.1. Triplet Loss", "content": "Before presenting our approach, we need to first introduce the notion of triplet loss and analyze it. Triplet loss uses three samples to form two pairs: anchor samples, positive samples of the same class, and negative samples of different classes. Anchor samples and positive samples form positive pairs, and anchor samples and negative samples form negative pairs. By optimizing the intra-class and inter-class distances through positive and negative pairs, the model can learn a good feature embedding space. The design of triplet loss is straightforward yet highly effective, making it widely applicable across various metric learning tasks. The target order for conventional triplet loss is:\n$L_{tri} = D(x_a, x_p) - D(x_a, x_n) + \\mu$, (1)\nwhere $\\mu$ is a given margin for better separability in the embedding space for different categories and D is the distance between two image features. $x_a$, $x_p$, $x_n$ denote anchor, positive sample and negative sample, respectively.\nWhen constructing triplets to meet task-specific requirements, the sketch generally serves as the anchor, while photos are chosen as the positive and negative samples. For ZS-SBIR tasks that involve two distinct modalities, however, this conventional triplet structure proves overly simplistic. It fails to consider the optimization of other distance metrics, potentially leading to an unbalanced feature distribution within the embedding space. Additionally, the sample distribution between different modalities is not symmetric, and random selection of positive pairs $D(x_a, x_p)$ and negative pairs $D(x_a, x_n)$ may result in suboptimal distributions. To address these issues, we propose a relation-aware quadruplet loss and strategically select pairs tailored for the ZS-SBIR task."}, {"title": "3.2. The relation-aware quadruplet loss", "content": "Considering the relation-aware nature of the SBIR task, we introduce the triplet loss into two domains and improve the triplet loss to a quadruplet loss. With pairs across different modalities, the quadruplet loss pushes the samples to the appropriate position in the embedding space. We use the Euclidean distance $D(.)$ between two images to measure their dissimilarity. It is unreasonable to only measure certain combinations like Eq. 1. To address the absence of a unified distance metric for negative pairs, our relation-aware quadruplet loss incorporates a second negative pair as an additional constraint. The quadruplet loss can be formulated as follows:\n$L_{qua} = D(x_a, x_p) - (1 - \\lambda) D(x_a, x_{n_1}) - \\lambda D(x_a, x_{n_2}) + \\mu$, (2)\nwhere $x_a, x_p, x_{n_1}$ and $x_{n_2}$ represent anchor image, positive image, the first negative image and the second negative image.\nIn contrast to the triplet loss, the new second negative term is the distance between the anchor and the second negative image. Similar to the triplet loss, the first term obtains non-identical class distances and acts as a \"strong push\" to increase the inter-class gap. However, a single negative gradient direction does not necessarily do this. Thus the second term assists in providing another gradient direction for updating. To make the quadratic loss comprehensive, we intuitively set the second negative image from a different modality than the negative image in the first negative term. This auxiliary term helps \"strong push\" to properly increase the inter-class gap, so it plays the role of \"weak push\" and we use weight $\\lambda$ to ensure the primary and secondary relation between the two constraints. Since we use Euclidean distance squared as the metric, the back-propagation of a triplet is:\n$\\frac{\\partial T_{ri}}{\\partial x_a} = -2(x_n - x_p)$,\n$\\frac{\\partial T_{ri}}{\\partial x_p} = -2(x_p - x_a)$,\n(3)\n$\\frac{\\partial T_{ri}}{\\partial x_n} = -2(x_n - x_p)$.\nHowever, following Eq. 3, the negative gradient direction may not be optimal. So it may be impossible to separate positive and negative sample centers.\nWe strategically designed the selection of pairs to effectively enhance inter-class distances and diminish intra-class distances, particularly by bridging the gap between different modalities. First, considering optimizing the whole embedding space instead of optimizing the position of sketches relative to photos, we carry out experiments for the selec-"}, {"title": "3.3. Meta Optimisation for the Loss Margin", "content": "The traditional triplet loss contains the hyper-parameter, i.e., margin $\\mu$ in Eq. 1, whose optimal value is typically determined empirically and can vary across different categories. Given that the intra-class distribution or spread among sampled sketches may not be uniform across classes, it is intuitively reasonable to define a category-specific optimal margin. This applies equally to the boundary terms in our proposed quadruplet loss, namely $R(x)$ in Eqs. 5 and 6. Therefore, we introduce a meta-learning process to learn this margin hyperparameter, allowing the optimal value of $R(x)$ for each specific category to be adaptively determined during testing. Specifically, we employ a relation network with a controller to achieve this. This network inputs each row vector into each time step of a bidirectional Gated Re-current Unit (GRU) to model the relationships among all samples in the training dataset. Subsequently, we apply max-pooling to the outputs from all time steps. The resulting vector is then fed into a linear layer, which ultimately outputs a sigmoid-normalized scalar value representing the learnable margin. The final memory vector $m_t$ is obtained by weighted summation:\n$m_t = \\sum w(i) M_t(i)$.\n(8)\nThe matrix being read $m_t$ is used for margin $R(x)$. Considering $L_{ra-qua}$ contains $L_{inter}$ and $L_{intra}$, our method learns margin $R(x)$ which has two values. Thus, We use a (dim, 2) linear layer with ReLU to obtain the margin. Learning two different margin helps $L_{inter}$ and $L_{intra}$ learn different relationship. The margin derived through meta-optimization is deemed more suitable than a fixed margin. It combines the advantages of long-term memory and short-term memory to match the current features with the vectors in the external memory matrix and makes reasonable predictions of margin values. A detailed explanation of the margin optimization in meta-learning, along with the corresponding formula derivations, can be found in the supplementary materials."}, {"title": "3.4. Classification Loss", "content": "Our model computes the negative Euclidean distance between photos and sketches as the final similarity score. But we still use a linear layer to learn another similarity score from the output vectors and feed them into the classification loss to avoid getting trapped in bad local optimum. In our model, the standard cross-entropy loss combined with Softmax is used as the classification loss. The cross-entropy loss employs an inter-class competition mechanism and only cares about the accuracy of the predicted probability of the correct label. This helps the model to learn good inter-class relations and stabilizes the training process to avoid getting trapped in bad local optimum. In our model, the standard cross-entropy loss is used to determine how close the actual output is to the target, and the Softmax function provides the required probability for the cross-entropy loss in multi-class classification. From this, the classification loss can be formulated as follows:\n$L_{cls} = - \\sum_{i=1}^N c_i log[softmax(Wx + b)]$, (9)\nwhere W and b are the weight matrix and bias vector of the softmax cross-entropy loss, respectively. $c_i$ is the class label. The cross-entropy loss only focuses on whether the prediction is correct or not, ignoring correlations between classes or modalities."}, {"title": "Algorithm 1: Training procedure of RAMLN", "content": "Input: Seen samples $I_{seen} = \\{i_m, l_i\\}$, hyper-parameters $\\lambda$, $\\beta$, $\\varphi$, batch size B, learning rate $\\mu$ and backbone learning rate multiplier $\\alpha$.\nOutput: Networks parameters $\\theta$\nTraining\n1: Initialization networks parameters $\\theta$ and memory matrix.\n2: repeat\n3: Randomly sample images in $I_{seen}$ with batch.\n4: Obtain features x by backbone model.\n5: Obtain key $k_t$ for memory matrix by GRU.\n6: $m_t \\leftarrow \\sum_1 w(i)M_t(i)$ Read vectors.\n7: Fusion features and give margin $R(x)$.\n8: $M_t(i) \\leftarrow M_{t-1}(i) + w(i)k_t$ Write vectors.\n9: Solve $L_{ra-qua}$ using Eq. (7).\n10: Solve $L_{cls}$ using Eq. (9).\n11: Update $\\theta$ using $\\theta \\leftarrow \\theta - \\nabla_{\\theta}(L_{cls} + \\lambda L_{qua})$.\n12: until Max training epochs is reached."}, {"title": "3.5. Overall Objective", "content": "The whole loss function L of our framework consists of two components: the embedding loss $L_{ra-qua}$ and the classification loss $L_{cls}$. So the loss function can be formulated as:\n$L = L_{ra-qua} + L_{cls}$.\n(10)\nOur method can be briefly summarized as Algorithm 1. It contains classification training and margin obtained through meta-optimization for metric learning."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Settings", "content": "Dataset Setting. Sketchy [20] is a fine-grained dataset and consists of 75,481 sketches and 12,500 photos from 125 categories. It has many instance-level matches. To achieve data balance, [16] expanded the photo gallery by collecting an extra 60,502 images. Then [38] introduced a new protocol using 21 carefully selected categories, not present in ImageNet, to serve as unseen classes for testing purposes. Hence it is called Sketchy-NO, in which 21 unseen classes are used as unseen for testing and other 104 classes for training.\nTU-Berlin [6] contains 20,000 sketches over 250 object categories. And an extra 204,070 photos collected by [16] are included in the extended version. Unlike Sketchy, it is a category-level dataset. However, the number of photos is one-tenth that of sketches, thus it is imbalanced. According to many works in ZS-SBIR, the difficulty is higher than Sketchy. Following the partitioning protocol introduced in [22], We choose randomly 30 classes as the unseen for testing, and the other 220 classes are used for training. We follow the conventional PKsampling strategy [8] to form batches by randomly sampling P classes.\nImplementation details. For comparison, we adopt two different pre-trained models as the backbone: CSE-ResNet50 and CLIP pre-trained ViT-B/32 image encoder. We find experimentally that large learning rates or frozen backbone achieve general results. The former may result from catastrophic forgetting. To ensure that the backbone network largely retains its original weights while adapting to the task of extracting features from the sketch domain, we assign a small learning rate to the backbone. Our model is trained with Adam [12] optimizer with the learning rate of $2 \\times 10^{-5}$ for CLIP backbone network and the learning rate of CSE-ResNet is $1 \\times 10^{-3}$. The weight decays is $5 \\times 10^{-4}$. The learning rate of backbone is $7 \\times 10^{-3}$ of the main learning rate. The code is implemented with PyTorch [18] library and the experiments are conducted on NVIDIA GeForce RTX 3070 GPU. The batch size is set to 128 and the maximum number of training epochs is set to 20.\nEvaluation Protocol. We evaluate our model by adopting the evaluation protocol that most works adopt. For Tu-Berlin Extended dataset, we report the average of mean Average Precision (mAP@all) and Precision (Prec@100)."}, {"title": "4.2. Comparison with State-of-the-art Methods", "content": "We evaluate our method against existing state-of-the-art approaches on both the Sketchy Extended dataset [38] and the TU-Berlin Extended dataset [22]. Two types of models are compared: those trained with semantic information and those without. Evidently, incorporating semantic domain knowledge enhances the sharpness of image feature extraction. Table 1 presents the comparison with state-of-the-art methods, noting the differences in backbones. Earlier studies predominantly used ResNet or its variants, while recent works employ a larger ViT backbone with more parameters. As indicated in Table 1, our method outperforms all ViT-based models that exclude semantic information. Specifically, our approach achieves an 8.0% and 17% improvement in mAP over ZSE on the two datasets, as well as a 9.2% and 8.0% improvement over IVT. Our method also demonstrates strong performance with the ResNet backbone, achieving results comparable to state-of-the-art methods and demonstrating adaptability across backbones.\nMany works [2, 32, 36] go beyond learning the semantic data of the dataset itself. Some models also learn from additional linguistic datasets like WordNet. These semantic works are effective but costly. Our meticulously selected relation-aware quadruplets assist the image encoder in adapting to abstract sketches and aligning the two domains of natural photos and sketches with multiple constraints. We believe that our approach is simple and effective. All these results show that our method can effectively alleviate the modality gap between sketches and photos while reducing the large intra-class diversity in both photo and sketch domains."}, {"title": "4.3. Ablation Experiment", "content": "We conducted experiments with other components on TU-Berlin dataset. There are classification loss $L_{cls}$, quadruplet loss $L_{ra-qua}$, and obtained margin $R(x)$. As shown in Table 2, our methods all contribute to the model. The combination of $L_{cls}$ and $L_{ra-qua}$ has a greater improvement. This is due to $L_{cls}$ helping $L_{ra-qua}$ to avoid local optimality. Only $L_{ra-qua}$ reduces the overall distance and make the feature distribution compact during training. $L_{ra-qua}$ suppresses this effect so that the model achieves better performance. Therefore, even if retrieval is based only on distance, classification training still makes sense. Margin $R(x)$ obtained by meta-learning based on memory is also effective. It combines the advantages of long-term memory and short-term memory to match the current features with the vectors in the external memory matrix and makes reasonable predictions of margin values."}, {"title": "4.4. Qualitative Analysis", "content": "Visualization of Feature Embedding. Figure. 3 visualizes the distributions of 7 classes of Tuberlin by t-SNE which include seen and unseen classes. We compare our method with Bid-Tri as discussed in Section 2.1 of the Supplementary Materials. The optimization objective of Bid-Tri is naive compared to our relation-aware quadruplet. This result shows that both methods have excellent ability to compress intra-class and separate inter-class features. In addition, for features from different modalities, the relation-aware quadruplet performs better. There is almost no gap between the groups from the sketch and the photo domain. It indicates that our method is also excellent in the distance control of both modalities. Features are well clustered together regardless of modalities. Also, all classes are separated by a certain distance. In addition, there are some images that are clustered into the wrong group. How to solve these difficult cases will become a research direction."}, {"title": "5. Conclusion", "content": "We introduce a metric learning framework leveraging a relation-aware quadruplet loss to capture both inter and intra-modal relationships in the ZS-SBIR task. It takes into account the features of ZS-SBIR and uses two modal contrastive learning to mitigate the effect of modal gaps. Additionally, it prevents feature overlap between samples from different classes within the same modality, resulting in superior performance compared to other triplet-like loss designs. The adaptive margin derived from our meta-learning strategy removes the need for manual margin tuning. By adjusting automatically, it mitigates issues caused by suboptimal configurations. This adaptive margin also accounts for domain variations across categories and modalities. As a result, it significantly improves the generalization capacity of metric learning. Experimental results on the TU-Berlin Extended and Sketchy Extended datasets confirm the effectiveness of our proposed method for cross-modal retrieval. The method performs well with object sketches of varying granularity. It also demonstrates robustness against variations in object shape and sample diversity."}]}