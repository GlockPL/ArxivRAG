{"title": "Leveraging Color Channel Independence for Improved Unsupervised Object Detection", "authors": ["Bastian J\u00e4ckl", "Yannick Metz", "Udo Schlegell", "Daniel A. Keim", "Maximilian T. Fischer"], "abstract": "Object-centric architectures can learn to extract distinct object representations from visual scenes, enabling downstream applications on the object level. Similarly to autoencoder-based image models, object-centric approaches have been trained on the unsupervised reconstruction loss of images encoded by RGB color spaces. In our work, we challenge the common assumption that RGB images are the optimal color space for unsupervised learning in computer vision. We discuss conceptually and empirically that other color spaces, such as HSV, bear essential characteristics for object-centric representation learning, like robustness to lighting conditions. We further show that models improve when requiring them to predict additional color channels. Specifically, we propose to transform the predicted targets to the RGB-S space, which extends RGB with HSV's saturation component and leads to markedly better reconstruction and disentanglement for five common evaluation datasets. The use of composite color spaces can be implemented with basically no computational overhead, is agnostic of the models' architecture, and is universally applicable across a wide range of visual computing tasks and training types. The findings of our approach encourage additional investigations in computer vision tasks beyond object-centric learning.", "sections": [{"title": "1 Introduction", "content": "The ability to form abstract, structured representations of the physical world is a cornerstone of human intelligence (Lake et al., 2017; Fodor and Pylyshyn, 1988). It enables us to excel in dealing with out-of-distribution situations, reasoning, analogy-making, and causal inference. Similarly, machine learning models that adapt the structural knowledge of an environment are more efficient and generalizable (Sch\u00f6lkopf et al., 2021). Modeling language as a sequence of structured tokens was a significant step toward the applicability and generalization of LLMs (Sennrich et al., 2016; Devlin et al., 2019). Similarly, many computer vision applications might also profit from segmenting visual scenes into semantic tokens, but inducing those is less self-evident. Thus, state-of-the-art models primarily represent images as a whole or as fixed-sized grids, consequently failing at simple reasoning that requires structural knowledge of physical entities such as counting objects (Radford et al., 2021).\nAddressing this issue, object-centric representation learning (OCRL) aims to infer a set of slots, each representing a distinct object - allowing for causal inference at the object level (Ding et al., 2021). Although binding an object to a slot from raw perceptual input is challenging (Greff et al., 2020a), recent advancements in OCRL, such as the Slot Attention (SA) module (Locatello et al., 2020), enabled unsupervised learning for simple synthetic datasets. However, scaling these methods to complex datasets proves challenging - slots attend to features considered irrelevant (Kipf et al., 2020) such as static backgrounds, represent spatial areas instead of objects (Seitzer et al., 2023), and are highly entangled (Singh et al., 2023). Subsequently, researchers investigated modified architectures utilizing the compositional nature of objects (Biza et al., 2023; Singh et al., 2023), optimization methods respecting the generative process of visual scenes (Brady et al., 2023; Wiedemer et al., 2023), and (semi-)supervised objectives (Elsayed et al., 2022a; Seitzer et al., 2023). Substantial performance increases of the latter highlighted that a pure RGB reconstruction loss may be a signal that is too weak for slots to segment scenes (Seitzer et al., 2023). Our work follows this assumption - However, we provide additional unsupervised signals by augmenting the RGB color space with color channels from other color spaces. We reason about our research in the following.\nAs in many vision-based approaches, the training data for unsupervised OCRL are color images, represented as red, green, and blue components, i.e., the classical RGB color space. To the best of our knowledge, all previous work for OCRL trains models on such RGB scenes. The RGB representation can be a sensible choice for images, primarily due to convenience in vector format mapping, its byte size, and its similarity to the human eye apparatus (Podpora et al., 2014). However, we argue that the RGB space is insufficient for unsupervised OCRL due to its color channels' high correlation, sensitivity to lightness, and non-uniformity. Other color spaces (which are non-linear and non-continuous transformations from RGB), such as HSV, do not suffer from these problems. Furthermore, they include highly discriminative features for object detection (see Figure 1). However, they introduce other drawbacks detailed in section 2. Thus, the question arises of how OCRL models can profit from the strengths of various color spaces while not inheriting their weaknesses - we propose to solve this problem by developing novel composite color spaces that combine expressive"}, {"title": "2 Background: Slot Attention", "content": "In the following, we introduce the technical background for Slot Attention (SA) (Locatello et al., 2020). We use SA for all our experiments, but our composite color spaces are transferable to other OCRL implementations.\nTraditional encoder-decoder architectures for computer vision generate a single, fixed-size (high-dimensional) latent vector z for an observation x. The latent space should accurately represent the underlying data distribution and capture the generative factors (Bengio et al., 2013; Locatello et al., 2019). In OCRL, we assume data consists of individual objects. Thus, object-centric architectures aim to induce a latent space decomposed into distinct objects.\nFor example, SA is an iterative algorithm employed on the learned features of an encoder. Each iteration starts with K slots $\\in$ RK\u00d7Dslots and inputs $\\in$ RN\u00d7Denc, the position-embedded outputs of an encoder. K must be user-defined or estimated. First, attention values attni,j are computed between each slot and the inputs using dot-product attention with learnable linear queries and keys (Vaswani et al., 2017). Crucially, SA introduces competition by softmax normalizing the attention coefficients over the slots, distributing inputs information among the slots.\nAfter introducing competition, attention weights attn and a learnable function v embed the values to update the slots. Finally, a Gated Recurrent Unit and another MLP merge the updates into the slots. For the mathematical details and formulas, we refer to Locatello et al. (2020).\nThe SA algorithm delivers slots that distributedly store the input information. Depending on the task and data, learned slot representations can be used in different downstream architectures. For unsupervised object discovery, which we examine in our work, slots are spatially broadcasted and decoded separately with a weight-shared decoder - the decoder usually predicts a four-dimensional output, capturing the RGB representations and masks. Masks are softmax normalized to merge individual reconstructions into a combined output (Greff et al., 2020b; Locatello et al., 2020).\nBecause RGB images strongly correlate with the lightness (see \"value\" in Table D5) and the models are optimized with a simple pixel-wise reconstruction loss, models predominantly focus on predicting the lightness accurately. Consequently, the binding mechanism discriminates spatial areas primarily by a single dimension. Other perceptual variables, such as the hue and saturation (Smith, 1978; Schwarz et al., 1987) that may be important to discriminate between objects are thus underrepresented. We argue that requiring models to additionally decode complementary color channels, such as hue and saturation, provides a stronger unsupervised signal for segmenting semantic objects."}, {"title": "3 Color Spaces for Unsupervised Object Centric Representation Learning", "content": "In this section, we highlight the impact of color choices on unsupervised OCRL. Outgoing from the common RGB color space, we discuss requirements for sensible color representations and arrive at composite color spaces that combine channels with complementary information of multiple spaces."}, {"title": "3.1 Why Is RGB a Suboptimal Choice for Natural Scenes in OCRL?", "content": "We argue that RGB is a suboptimal color space for unsupervised OCRL, as RGB is\n\u2022 strongly correlated: The RGB color space consists of the additive \"Red,\" \"Green,\" and \"Blue\" color channels. Those are all strongly correlated in natural images. We provide empirical evidence for that in Table D5, where we calculated the Pearson Correlation Coefficient between each color channel for 1000 images, respectively sampled from ClevrTex (Karazija et al., 2021), MS Coco (Lin et al.,"}, {"title": "3.2 Are Other Color Spaces better Suited for Unsupervised Object Centric Representation Learning?", "content": "Other color spaces, such as HSV and CIE-LAB solve some of the problems discussed for the RGB space. HSV is a cylindrical-coordinate representation of RGB, modeling the \"Hue,\" \"Saturation,\" and \"Value.\" The value strongly correlates with the RGB channels (see Table D5). Contrarily, the hue and saturation are not correlated to the RGB channels and are only slightly influenced by lighting. Similarly, it aligns with the perceptual variables of humans (Smith, 1978; Schwarz et al., 1987). Furthermore, it is almost perceptually uniform. CIELAB models the luminosity along with the two color dimensions. Due to the explicit modeling of luminosity, it is perceptually uniform, non-correlated, and robust to light influences. HSV and CIE-LAB can be losslessly transformed from the RGB space. Although those transformations are non-continuous, they can be approximated by machine learning algorithms. However, we argue and empirically verify that transformations in the target representations significantly influence OCRL models' performance. As the models are trained on the pixel-wise reconstruction error, models may use their predictive capacity differently depending on the color space. For example, if we use RGB space, we rely on the OCRL model to implicitly learn objects' invariances due to lightness effects while we model them in HSV with the hue and saturation component. We verify in subsection D.4 that the robustness of the HSV channels to different lighting conditions transfers to the slot representations - objects that share the same underlying factors remain more similar when confronted with different lighting."}, {"title": "3.3 Combining Color Spaces", "content": "While we previously gauged between individual color spaces, we will now argue about combining complementary color channels into a composite target space. Combined color spaces have been initially considered in the literature but have yet to be focused on OCRL (Du et al., 2024). Models trained on composite color spaces are less dependent on inherent biases of the color spaces. For example, differences in hue and saturation are subtle in the RGB space (due to the channels high correlation with"}, {"title": "4 Related Work", "content": "To our knowledge, combined color spaces have not yet been explored for object-centric representation, although they have shown promising success in computer vision tasks (Shin et al., 2002; Gowda and Yuan, 2019; Li et al., 2011). However, they focus on transformations on the input instead of the targets. In particular, using different color spaces such as HSV or YCbCr were investigated for challenging segmentation and detection tasks (Shin et al., 2002; Vandenbroucke et al., 2003; Taipalmaa et al., 2020). The choice of input color space might have been a typical approach in feature engineering. However, with the advent of deep learning-based methods, such feature"}, {"title": "5 Experiments and results", "content": "For our experiments, we investigate the impact of five color spaces on unsupervised Slot Attention. Our baselines are RGB and HSV, and we utilize RGB-S, RGB-SV, and RGB-HSV as combined color spaces. For comparability, all models use the same architecture; only the final output layer is adjusted to match the dimension of the color spaces. As motivated in subsection 3.2, we only transform the targets, and the input remains RGB.\nWe mainly evaluate object discovery, measuring whether slots segment scenes into distinct objects. In line with previous work, we report the Foreground-Adjusted Rand Index (FG-ARI) and the mean Intersection over Union (mIoU). Additionally, we report the RGB-MSE for scene reconstruction quality. Note that if a network predicts additional color channels, they are not considered for the MSE measurement, and HSV is algorithmically transformed to RGB for this to work. Furthermore, we evaluate"}, {"title": "5.1 Results on Object Discovery", "content": "We measure object discovery for the five datasets Clevr, MultiShapeNet (4 and 24), Clevrtex, and Movi-C (Johnson et al., 2016; Stelzner et al., 2021; Karazija et al., 2021; Greff et al., 2022). An overview of the results is given in Table 2, and we detail the results in the following sections."}, {"title": "5.1.1 Clevr", "content": "We start our evaluation with the Clevr dataset (Johnson et al., 2016), containing at most ten simple geometric objects per scene on a gray background. For our evaluation, we use the model architecture introduced by Locatello et al. (2020), achieving near-perfect segmentations. We already discussed in section 3 that models trained on the HSV degenerate. We argued that the discontinuity of the hue channel prohibits slots from binding objects. Thus, as an initial proof of concept, we first verify that the enhanced RGB color spaces, without non-continuous hue, do not degenerate. We report segmentation and reconstruction quality for all datasets in Table 2. Results of models trained on RGB values are consistent with the literature (Locatello et al., 2020). While models trained with the Hue component degenerate (visible in the segmentation and reconstruction quality), models trained on the RGB-S and RGB-SV space do not degenerate and even improve the segmentation quality slightly. However, the combined color spaces do not help segment objects from the background. Thus, the mIoU score is low for all models."}, {"title": "5.1.2 MultiShapeNet", "content": "We consider two variants of Multishapenet (Stelzner et al., 2021), which we call \"4\" and \"24\". In the 4 version, we filtered for images containing exactly four objects, while we allowed 2 to 4 objects in the 24 version. We follow the translation and scaling invariant architecture by Biza et al. (2023). Similarly to Clevr, incorporating the hue channel leads to model degeneration. Conversely, adding Saturation to the RGB color space significantly improves segmentation quality for both dataset variants. For MSN 4, FG-ARI is improved from 73.7 to 82.1, and for MSN 24, from 61.6 to 70.3, representing a relative improvement of 10%. The differences are even more substantial for the mIoU score: Although the RGB models often succeed in distributing different objects to different slots, they do not segment them clearly from the background."}, {"title": "5.1.3 Clevrtex", "content": "Clevrtex (Karazija et al., 2021), conceptually similar to Clevr but having more shapes, sizes, and, most importantly, a manifold of photo-realistic textures (see Figure 3), is the most challenging dataset we considered. We use the invariant SA architecture by Biza et al. (2023) and similarly show two variants of model complexities: We use a standard five-level CNN autoencoder (Figure 3 for details), labeled CNN in the following, and also a Resnet34 architecture which promised significant performance gains (labeled ResNet). The RGB baselines can mostly segment scenes, but small objects are often encoded with other slots or multiple slots encode one object. Adding the saturation (and value) channels significantly improves the segmentation quality - The composite color spaces improve the baselines for 10% in absolute FG-ARI score, independent of"}, {"title": "5.1.4 Movi-C", "content": "Originally, Movi-C (Greff et al., 2022) is a collection of short video clips. For this work, we consider its snapshots as an image dataset, similar to Seitzer et al. (2023); Fan et al. (2024). Investigating the effect of combined color spaces for videos is an interesting future work. Movi-C consists of 3 to 10 photorealistic objects on a background and is yet another significant step up in complexity. While SA trained on RGB was previously reported to fail on it (Seitzer et al., 2023), self-supervised signals provided a significant performance step-up. Our experiments utilize the same ResNet architecture as used for Clevrtex. Our object discovery measures for RGB are consistent with the related literature (Seitzer et al., 2023) - although the FG-ARI scores are similar across color spaces, we observed a qualitative difference in segmentation:"}, {"title": "5.2 Results on Underlying Generating Factors", "content": "We tested how well the slots represent the objects they binded. We, therefore, run two tests to predict their underlying generating factors (Locatello et al., 2019). The first test is to train a linear and shallow non-linear predictor that should map from the slot representations to the factors of variation of the Clevrtex objects: Those are uniformly sampled from four shapes, three sizes, and 59 materials. We matched objects to slots based on the maximal mask overlap to construct the dataset to train and evaluate the predictor. Average precision is shown in Figure 4 (left and middle). Although the quality of slot representations was shown to correlate with the FG-ARI (Wenzel et al., 2022), the quality of slot representations differs strongly, especially for the materials: The HSV and RGB-S model at least double the precision of the RGB model. While the additional color dimensions do not directly influence the size and shape, the material is mainly determined by hue, saturation, and value. We suspect that the explicit representation helps slots generalize to underlying factors."}, {"title": "6 Limitations and Future Work", "content": "By only requiring a different output representation, our approach is generally applicable to all visual scenes; nonetheless, there are several limitations in our approach and evaluation: We only considered preexisting color channels of the most common color spaces, and there may be better composite channels suited for object detection. Analyzing whether a non-correlated RGB space, e.g., by applying PCA, already improves the scene segmentation is a promising starting point. Furthermore, we base our argument on the non-correlation to detect suitable complementary color channels. However, more elaborate techniques considering the data distribution of pixels may be needed to filter for complementary dimensions efficiently. Furthermore, we only evaluated our enhanced color spaces for OCRL tasks, but they might generally apply to a broader field of computer vision. At the same time, we based the discussion only on natural images. Finally, we only tested and compared the enhanced color spaces on SA - however, recent approaches based on self-supervised learning and pre-trained encoders have shown promising results - using composite color spaces (e.g., in the pre-training method) might lead to further performance improvement, but is impossible to assess a priori. An interesting direction for future research is if the performance leaps are achieved by using composite color space transfer to other unsupervised networks for visual computing. We started the elaboration of this paper by investigating the influence of the visual scene data representation on the performance of unsupervised OCRL. Our work is a starting point, showing promising performance without negative effects in OCRL, bearing the potential for numerous research directions: A first step is to evaluate the design space of combined color spaces - other channels than saturation may bear similar potentials, providing a consistent target to discriminate between objects. Furthermore, we designed the color space for OCRL, but combining color spaces got less attention in computer vision in general. However, a combination of color spaces still needs to be explored."}, {"title": "7 Conclusion", "content": "We challenge the common assumption that RGB images are the optimal color space for unsupervised learning in computer vision (Caron et al., 2021; Radford et al., 2021; Locatello et al., 2020) and demonstrate this for OCRL. We highlighted that the RGB space is not only highly correlated, effectively reducing the learned target to one dimension, but that composite color spaces offer non-correlated dimensions while upholding expressive features for OCRL. In a evaluation, we tested the LAB and the HSV color space as complimentary targets and found that HSV shows interesting properties due to its non-correlatedness and similarity to human perception. However, while HSV improves models on some datasets, it leads to degeneration for other datasets due to the discontinuity in the Hue channel. We thus combined the stability of the RGB space with the uncorrelated, expressive channels of HSV, constructing combined color spaces. Training models on the combined color spaces leaves the main architecture unchanged, is broadly applicable, virtually cost-free, and significantly improves performance on object detection. Especially the RGB-S space, combining the RGB space with the Saturation channel (significantly) enhances performance on object discovery and scene segmentation across all five datasets. Most significantly, on the challenging Clevrtex dataset, we increase the FG-ARI from 75.6 to 92.7. We further show evidence that the combined color spaces also improve performance on photo-realistic datasets such as Movi-C and improve the mIoU from 21.3 to 27.2. Similarly, we highlight that training on the pure HSV color space can sometimes improve the FG-ARI further but lacks stability on other datasets. Moreover, we evaluated the extracted slots' capability to bind object property and noticed significant improvements from 27.1% to 57.8 % average precision. To summarize, the composite RGB-S and RGB-SV color space (significantly) outperforms RGB in any experiment without negative side effects or additional cost."}, {"title": "Appendix A Reproducibility of Experiments", "content": "Our work contains a wide range of empirical evaluations. The following section serves as documentation to reproduce our experiments. We provide model architectures, optimization hyperparameters, and further details."}, {"title": "A.1 Model Architecture", "content": "We consider a wide range of complexities in our datasets. While Clevr contains only simple geometric objects on a gray background, Movi-C depicts photorealistic textures and backgrounds. We adjust the scale of model architectures accordingly. All models consist of an encoder, the Slot Attention module, and the spatial broadcast decoder. We report details of the Table A1, Table A.1.2, Table A.1.3. For the Clevr dataset, we apply vanilla Slot Attention, for all other datasets we apply position and scale invariant slot attention (Biza et al., 2023)."}, {"title": "A.1.1 Encoder Details", "content": "Table A1 Architecture Details of CNN encoder."}, {"title": "A.1.2 Slot Attention Module Details", "content": "We use Slot Attention with three iterations and a residual connection as proposed by Locatello et al. (2020). For all datasets except Clevr, we utilize the position-invariant variant of Biza et al. (2023). The number of slots is set to the maximum number of objects in the dataset plus the background, resulting in eleven slots for all datasets except Multishapenet, where we use five slots. The hyperparameters of the Slot attention module are depicted in Table A.1.2."}, {"title": "A.1.3 Decoder Details", "content": "The hyperparameters of the spatial decoder are detailed in Table A.1.3."}, {"title": "A.2 Optimization Parameters", "content": "Similar to Locatello et al. (2020); Biza et al. (2023), we apply the Adam optimizer. We use a batch size of 32 with a learning rate of 210-4 for 250k steps. We use a learning rate warm-up from 0 to 50k steps, and afterwards a cosine learning rate decay for 100k steps. Due to the large variance for object segmentation, we train all CNN models on 10 random seeds, and all ResNet models on 6 random seeds."}, {"title": "A.3 Evaluation Metrics", "content": "While the metrics for object discovery are well established for OCRL (Locatello et al., 2020; Biza et al., 2023; Seitzer et al., 2023), we provide more details about our experiments of underlying generating factors. There we use the DCI metrics of Eastwood and Williams. According to Eastwood and Williams, those metrics measure:\n\u2022 Disentanglement: \"The degree to which a representation factorizes or disentangles the underlying factors of variation, with each variable (or dimension) capturing at most one generative factor\"\n\u2022 Completeness: \"The degree to which a single code variable captures each underlying factor\""}, {"title": "A.4 Implementation of Color Spaces", "content": "In the main text, we regularly discuss RGB, CIELAB, and HSV as color spaces. However, while the term \"color spaces\" is regularly used, it is imprecise: For example, RGB comprises a collection of additive color spaces, and arguably its most important representative is the standard RGB (sRGB) space - images in the web are usually represented by the sRGB space if not tagged otherwise. For our paper, we do not discriminate between the different implementations, and our reasoning is not based on"}, {"title": "Appendix B Datasets", "content": "Appendix B Datasets"}, {"title": "B.1 Clevr", "content": "Clevr (Johnson et al., 2016) is arguably the least complex dataset we consider. Three to ten simple geometric objects are randomly placed on a gray background. The number of objects is uniformly sampled, containing four factors of variations: Objects differ between three shapes, two sizes, two materials, and eight colors. The camera and light source are randomly jittered. We trained our models on the whole train set, consisting of 70000 images, and evaluated on 5000 images randomly sampled from the test set. We preprocess the images by taking a 192x192 center crop of the images and then bilinearly rescale them to 128x128. We do the same for the ground truth masks but rescale them with nearest-neighbor interpolation. The dataset with ground truth masks is available at ."}, {"title": "B.2 Multishapenet", "content": "Multishapenet (Stelzner et al., 2021) is a significant step up in complexity compared to Clevr. Although the background is also gray, the objects are visually more complex: Each of the objects falls into one of the categories of chairs, tables, and cabinets (uniformly sampled), which are then used to sample photorealistic models from the ShapeNetV2 set (Chang et al., 2015). Multishapenet contains 11733 unique shapes. Camera and light are randomly jittered, as in Clevr. We preprocess the images by taking a 192x192 center crop of the images and then bilinearly rescale them to 128x128. We do the same for the ground truth masks but rescale them with nearest-neighbor interpolation. The training set contains 70000 images, while the test set contains 15000. We evaluate 5000 of them. We filter for images containing exactly four objects for the Multishapenet 4 version. The dataset is available at ()."}, {"title": "B.3 Clevrtex", "content": "Clevrtex (Karazija et al., 2021) is conceptually similar to Clevr. Each scene includes 3 to 10 random objects. However, it contains 60 photorealistic backgrounds with complex textures. Similarly, the objects are sampled from four shapes, three sizes, and 60 complex textures. Scenes are often dark due to the backgrounds and objects. The camera is chosen as in Clevr, but Clevrtex includes complex lighting effects through three light sources. We apply the same cropping and resizing strategy as for Clevr. Clevrtex includes 40000 images for training and 5000 images for training. We also evaluate our models on an out-of-distribution set containing four unseen shapes and 25 additional textures. Clevrtex (v2) can be downloaded at ."}, {"title": "B.4 Movi-C", "content": "Movi-C is a video dataset consisting of 24 frames showing objects falling to the ground with realistic physics. Each scene consists of 3 to 10 objects with a complex background. Contrarily to Clevrtex, the objects are not simple geometric shapes but are sampled from 11 realistic categories, such as \"Toys\" or \"Car Seat.\" The lighting effects originate from the sampled background and thus differ strongly in their lightness. Similar to related works (Seitzer et al., 2023), we treat the dataset as an image dataset, taking all 24 snapshots. That yields 234000 images for training and 6000 for testing. We ensure no overlap between the train and test set; all images originate from different videos. Movi-C can be downloaded at ."}, {"title": "Appendix C Color Spaces and the Biological Visual System", "content": "We predominantly used the RGB and HSV color space in the main text for our experimental evaluation. We motivate their usage in subsection 3.2 with analogies to the human visual system. This section serves as a basic introduction to the color perception of humans. We refer to M\u00fcller and Lipsk (1930) and Kim et al. (2009) for details.\nAlmost all theories of human color vision are based on the zone model of M\u00fcller and Lipsk (1930). Physical light waves stimulate three types of cones (short-, middle-, and long-wave) and rods in the pupil. Neurons combine those physical signals, yielding achromatic brightness, hue, and colorfulness sensations. Those are finally propagated with nerve fibers to the visual cortex. While the RGB color space is designed in analogy to the rudimentary physical activations of the cones, the HSV space is closely related to the sensations propagated to our visual cortex (Smith, 1978; Schwarz et al., 1987). In our work, we report that using additional color channels closely related to how humans perceive the world significantly improves the object detection capabilities of unsupervised OCRL networks."}, {"title": "Appendix D Extended Experimental Results", "content": "Appendix D Extended Experimental Results"}, {"title": "D.1 Clevrtex Out Of Distribution", "content": "The Clevrtex dataset includes an out-of-distribution dataset containing novel objects and textures different from those in the train set. We also evaluated the models on this test set for object discovery. As models trained on the augmented color spaces better generalize to underlying generating factors, we hypothesized that they are more sensitive to out-of-distribution data with newly introduced factors of variation. However, all models perform similarly in terms of object discovery, as seen in Table D4."}, {"title": "D.2 Correlations of Color Channels", "content": "In the main text, we argued that the RGB channels are highly correlated in natural images. We empirically verified that by measuring the Pearson correlation coefficient"}, {"title": "D.3 Input Transformations", "content": "In our main text, we mainly argued for color space transformations in the targets. Color space transformations in the inputs were already investigated in the literature (see section 4) - depending on the task and data, other color spaces than RGB have shown performance leaps. However, we argued that any machine learning model can closely approximate the color space transformations, which makes color space transformations in the inputs less convincing for OCRL. The situation is different for transformations in the output, as the slots must represent those features. To test our hypothesis, we run experiments with transformations in the input and output. Therefore, we used the CNN Clevrtex model on three random seeds each. We report the FG-ARI in Table D6."}, {"title": "D.4 Robustness under Lighting Conditions", "content": "We extend our experiments from the main text by testing how training on composite color spaces influences the robustness of models to distribution shifts. In the following, we explore how models behave when exposed to differing lighting. In section 3, we argued about the significant impact of lighting on the RGB channels. In contrast, most HSV channels are only slightly perturbed. We will explore whether those unperturbed channels transfer to more robust slot representations under lighting conditions.\nWe generate a novel test set similar to Clevrtex scenes for our experiments, utilizing the same codebase (). However, for each generated scene, we generate three additional scenes where only the lighting conditions are varied. Specifically, we push the light source apart for 5, 10, and 20 units, respectively, resulting in gradually darker scenes. Qualitative samples for differing lighting conditions are shown in Figure D4. The effects on the color channels may be observed in Figure D1: While the R, G, B, and V channels are highly dependent on the lighting conditions, the hue and saturation show only moderate changes. We create a total of 1024 scenes.\nWe first test whether models trained on the RGB, RGB-S, and HSV space remain robust in discovering objects (see Table D7). While the distribution shift decreases the segmentation of all models, independent of the target color space, they remain remarkably robust. Interestingly, even in the darkest scenes (distance L = 20), the RGB-S and HSV models still achieve a better performance than the RGB models without distribution shift. However, the RGB models are also only marginally influenced by darker scenes. We suspect that the Clevrtex training set (containing 40k images) contains enough scenes with dark materials to create models that are robust toward lighting effects inherently."}, {"title": "D.5 Slot Attention Maps", "content": "We extend our analysis of learned slot parameters from subsection 5.2 by inspecting the attention maps for each slot. To get an overview of the whole model, we aim to visualize not only single scenes but the whole Clevrtex test set. For each scene, we compute the a mask for every slot and derive the weighted (x,y)-means of the mask per slot. The mean (x,y)-values (per slot) are presented in Figure D2 for six models trained on RGB, RGB-S, and HSV.\nWhile all slots, independent of the color space, occupy a spatial area, the distributions of the center points of the attention maps vary: For four models of the RGB space, the attention maps centers differ only slightly - those represent degenerated slots, binding fixed-size grids instead of objects. In our experiments, this behavior appears"}, {"title": "D.6 Comparison to Self-Supervised Methods", "content": "Although self-supervised targets are not the main focus of our research, recent related work (Seitzer et al., 2023) has highlighted their remarkable performance on challenging real-world datasets. Thus, we compare our composite color spaces to the recent Dinosaur (Seitzer et al., 2023) architecture. We run experiments on the Clevrtex and Movi-C datasets using the same hyperparameter setting as described in (Seitzer et al., 2023) for Movi-C. However, we exchange the Vit-s8 backbone for a ResNet34 variant for comparability to our networks. We utilize training signals from a DINO (Caron et al., 2021) Vit-b16. The Dinosaur models with the ResNet variant show superior performance over the composite color spaces on Movi-C (FG-ARI 53.1 \u00b1 1.3, mIoU 22.7\u00b10.6) but lack performance on Clevrtex (FG-ARI 2"}]}