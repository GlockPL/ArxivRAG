{"title": "REVISITING IN-CONTEXT LEARNING INFERENCE CIR- CUIT IN LARGE LANGUAGE MODELS", "authors": ["Hakaze Cho", "Mariko Kato", "Yoshihiro Sakai", "Naoya Inoue"], "abstract": "In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to cap- ture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to ex- plain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Summarize: LMs encode every input text (demonstra- tions and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMS merge the encoded representations of demonstrations with their corresponding label tokens to pro- duce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations similar to the query represen- tation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a cer- tain extent and decode them into predicted labels. The proposed inference circuit successfully captured many phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. More- over, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mecha- nism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit.", "sections": [{"title": "1 INTRODUCTION", "content": "In-Context Learning (ICL) (Radford et al., 2019; Dong et al., 2022) is an emerging few-shot learning paradigm: given the demonstrations {(xi, Yi)}=1 consisting of [input text]-[label token] pairs and a query xq, Language Models (LMs) take the sequence [X1][51][Y1] ... [Xk][$k][Yk][Xq][Sq] as input and then predicts the label for xq by causal language modeling operation. Typically, the label tokens Yi are preceded by and also predicted by forerunner tokens si (e.g., the colon in \u201cLabel: \u201d). ICL has aroused widespread interest, but its underlying mechanism is still unclear.\nThere have been theoretical or empirical trials to characterize and explain the inference process of ICL (Xie et al., 2021; Dai et al., 2023; Wang et al., 2023; Han et al., 2023a; Jeon et al., 2024; Zheng et al., 2024). However, to capture all the operating dynamics and the observed interesting phenomenon of ICL in Large Language Models\u00b2(LLMs), a more comprehensive characterization is still necessary. Therefore, this paper proposes a unified inference circuit and measures various properties in LLMs for a conformation to the observed ICL phenomenon.\nAs shown in Fig. 1, we decompose ICL dynamics into 3 atomic operations on Transformer layers. Step 1: SUMMARIZE: LMs encode each input text xi into linear representations in the hidden state of its corresponding forerunner token si. Step 2: SEMANTICS MERGE: For demonstrations, LMs merge the encoded representations of si with the hidden state of its corresponding label tokens Yi."}, {"title": "2 PREPARATION", "content": ""}, {"title": "2.1 BACKGROUND & RELATED WORKS", "content": "In-context Learning. Discovered by Radford et al. (2019), ICL is an emerging few-shot learn- ing paradigm with only feed-forward calculation in LMs. Given demonstrations {(xi, Yi)}=1 composed of structured input-label pairs and a query xq, typical ICL creates a prompt [X1][81][Y1]... [Xk][Sk][Yk][Xq][Sq], with some structural connectors (e.g. \u201cLabel: \u201d) including fore- runner token si (e.g. \u201c: \"), as shown in Fig. 1. LMs receive such prompts and return the next token distribution, where the label token with the highest likelihood is chosen as the prediction. Ex- plaining the principle of ICL is an unresolved research topic, although there have been some efforts on the relationship between ICL capacity and pre-training data (Li & Qiu, 2023; Singh et al., 2024b;a; Gu et al., 2023; Han et al., 2023b; Chan et al., 2022), the feature attribution of input prompt (Min et al., 2022; Yoo et al., 2022; Pan, 2023; Kossen et al., 2024), and reduction to sim- pler algorithms (Zhang et al., 2023; Dai et al., 2023; Xie et al., 2021; Han et al., 2023a). However, a comprehensive explanation of real-world LMs is needed to capture the operating dynamics of ICL.\nInduction Circuit. Introduced by Elhage et al. (2021), an induction circuit is a pair of two cooperat- ing attention heads from two transformer layers, where the \"previous token head\" writes information about the previous token to each token, and the \"induction head\" uses this information to identify a token that should follow each token. Such a function is implemented by two atomic operations: (1) copy the representation of the previous token [A] to the next token [B], and (2) retrieve and copy similar representations on [A] to the current token [A']. Concisely, it performs inference in the form of [A] [B]... [A'] \u2192 [B], which is similar to ICL-styled data. Therefore, this circuit has been widely used to explain the inference dynamics of ICL (Wang et al., 2023) and the emergence of ICL during pre-training (Olsson et al., 2022; Reddy, 2024; Singh et al., 2024b). Despite their valuable insights,\""}, {"title": "2.2 EXPERIMENT SETTINGS", "content": "Models. We mainly conduct experiments on 4 modern LMs: Llama 3 (8B, 70B) (AI@Meta, 2024), and Falcon (7B, 40B) (Almazrouei et al., 2023). Unless specified, we report the results on Llama 3 70B, since its deep and narrow structure (80 layers, 64 heads) makes it easier to show hierarchical inference dynamics (discussed in \u00a75.2). The results of other models can be found in Appendix F.2.\nDatasets. We build ICL-formed test inputs from 6 sentence classification datasets: SST-2 (Socher et al., 2013), MR (Pang & Lee, 2005), Financial Phrasebank (Malo et al., 2014), SST-5 (Socher et al., 2013), TREC (Li & Roth, 2002; Hovy et al., 2001), AGNews (Zhang et al., 2015)specified, we report the average results on these datasets.\nUnless specified, we use k = 4 demonstrations in ICL inputs. For each dataset, we randomly sample n = 512 test data and assign one order-fixed demonstration sequence for each test sample. About the prompt templates, etc., please refer to Appendix A.1."}, {"title": "3 STEP 1, SUMMARIZE: LINEAR REPRESENTATIONS IN HIDDEN STATES", "content": "This section mainly confirms that LMs construct task-relevant and linearly separable semantic rep- resentations for every input text (demonstrations and queries) in the hidden states. Such linear representations are an important foundation for explaining the dynamics of ICL based on induction head, since attention-based feature retrieval, a key mechanism of induction head, can be easily done on linear representations. Current successful studies on simplified models and inputs (Chan et al., 2022; Reddy, 2024; Singh et al., 2024b) also assume the existence of such linear representations. Moreover, we confirm some interesting properties of the input text representations: (1) It is based on the capacity in the model weights and can be enhanced by demonstrations in context (Fig. 2 (Middle, Right)). (2) The similarity of representations is biased towards the encoding target's position."}, {"title": "3.1 LLMS SUMMARIZE INPUT TEXT ON FORERUNNER TOKENS IN HIDDEN STATES", "content": "We first study the existence of input text encoding in hidden states and then explain their linear separability and task relevance in \u00a73.2. For each text-label pair (xt, yt) (encoding target) sam- pled from the datasets, we prepend them with k demonstrations, resulting in ICL-style inputs [X1][81][Y1]... [Xk][Sk][Yk][Xt][St][Yt]. These inputs are then fed into an LM to extract the hidden states of a specific token in [xt][st][yt] from each layer, serving as the ICL inner representations. To assess the quality of these representations as sentence representations, we use the sentence em- bedding of xt encoded by BGE M3 (Chen et al., 2024), a SotA encoder-only Transformer, as a \"reference\" representation and then calculate the mutual nearest-neighbor kernel alignment (Huh et al., 2024) between these representations. See Appendix A.1 and A.2 for details.\nForerunner Tokens Encode Input Text Representations. We plot the kernel alignment using 3 types of tokens in Fig. 2 (Left). The forerunner token, while often overlooked in previous work, produces the best input text-encoding, emerging in the early phase (layers 0-28) of the inference process, and keeping a high level to the end of inference. Interestingly, hidden states of label words are not satisfactory input text representations even with a high background value (the result at layer 0, refer to Appendix A.2.1 for details), which is a critical supplement to previous work which suggests the label tokens are pivots for collecting the information of demontrations (Wang et al., 2023).\nInput Text Encoding is Enhanced by Demonstrations. We investigate the influence of contextual in on input text encoding by repeating the experiments with different k. As shown in Fig. 2 (Middle), when the demonstrations increase, feature alignment is enhanced, which is counterintuitive since longer preceding texts are more likely to confuse encoding targets. Such findings indicate that"}, {"title": "3.2 TEXT SUMMARIZATION IS LINEAR AND TASK-RELEVANT BUT POSITION-BIASED", "content": "Input Text Encoding is Linear Separable and Task-relevant.\nWe train a centroid classifier on hold-out 256 input samples (Cho et al., 2024), using the hidden states of a specific token in [xt][st] from each layer and then predict the label yt (see Appendix A.3 for details). The results are shown in Fig. 3. The considerably high classification accuracy of the forerunner token suggests the high linear separabilities of the hidden states in the task-semantic- relevant subspaces since the centroid classifier is linear. In addi- tion, a similar emerging trend in accuracy and kernel alignment confirms the reliability of the kernel alignment measurement.\nInput Text Encoding is Biased towards Position. Ideally, the in- ner representations of similar queries should be highly similar re- gardless of their position in ICL inputs to support attention-based operations for classification. To verify this, for each encoding target, we extract the hidden states of forerunner tokens with various numbers of preceding demon- strations. We then calculate the cosine similarity between all possible pairs of the hidden states for the same target or different targets. As shown in Fig. 4, although the overall similarities on the same target are higher than on the different targets, they are both especially higher when their positions are close to each other. As to be discussed in \u00a75.3, such positional similarity bias may lead to one flaw: demonstrations closer to the query have stronger impacts on ICL (Zhao et al., 2021; Lu et al., 2022; Chang & Jia, 2023; Guo et al., 2024). The principle of such bias is discussed in Appendix C."}, {"title": "4 INDUCTION CIRCUITS IN NATURAL LANGUAGE MODELS", "content": "This section mainly shows how LMs utilize the summa- rized linear text representations in induction circuits with a typical 2-step form (Singh et al., 2024b): Forerunner Token Heads merge the demonstration text representa- tions in the forerunner token into their corresponding la- bel tokens with a selectivity regarding the compatibility of demonstrations and label semantics. Induction Heads copy the information in the label representations similar to the query representations back to the query. Such oper- ations are done on task-specific subspaces, enabling LMs to solve multiple tasks by multiplexing hidden spaces."}, {"title": "4.1 STEP 2, FORERUNNER TOKEN HEAD: COPY FROM TEXT FEATURE TO LABEL TOKEN", "content": "This subsection mainly examines and measures the forerunner token heads, which copy the informa- tion in the forerunner into label tokens. We investigate the interaction between the forerunner tokens and label tokens, and focus on how the representations are merged, especially when the semantics of labels and text are disjoint, towards an explanation of why ICL are robust to wrong labels.\nText Representations are Copied to Label Tokens. To confirm the existence of the representation copy process, we start by calculating the kernel alignment between the hidden state of forerunner token st at layer l (the copy source) and that of label token yt at layer (l + 1) (the copy target). To suppress the high background values caused by the semantics of labels, we use abstract label tokens {\"A\", \"B\", \"C\",... } instead of the original label tokens. The results are shown in Fig. 5 (Left), where the kernel alignment between the hidden states of the label token and the forerunner token gradually increases and then bumps up after the encoding in the forerunner token (described in \u00a73) finished improving. It indicates that the hidden states of the input text representation encoded in the forerunner tokens are merged into their label tokens, suggesting the existence of copy processing from the forerunner token to the label token.\nText Representations are Copied without Selectivity. For each attention head, we extract the attention score ayt\u2192st from a label token yt (as attention query) to the corresponding forerunner token st (as attention keys). We then mark the head with ayt\u2192st \u2265 5/nt (nt: the length of tokens before yt) as a Forerunner Token Head and count them in each layer. The results are shown in Fig. 5 (Middle, \"Correct Label\"), where the peak matches the copy period in Fig. 5 (Left). Moreover, to investigate the influence of the correctness of label tokens, we replace yt with a wrong label token, where the results in Fig. 5 (Middle, \u201cWrong Label\") are almost identical to the correct-label setting, suggesting that the forerunner token heads don't show selectivity toward the semantic consistency between input text and labels simply merge the input text representations into the label tokens.\nHidden States of Label Tokens are Joint Representations of Text Encodings and Label Seman- tics. Given the findings above, we probe the content of hidden states of label tokens [yt], i.e., how the text representation interacts with the original label semantics. We first train two centroid classifiers to predict the corresponding label yt: (1) Cf trained on the hidden states of forerunner tokens [st] and (2) Ci trained on the hidden states of label tokens [yt]. To check whether the label tokens include the information of forerunner tokens, we use Cf to predict the label on the hidden state of label token [yt] in Fig. 5 (Right, solid). It shows that, during the copy processing, high classification accuracies can be achieved both on the correct label tokens and wrong label tokens, suggesting that the text features in the forerunner tokens can be partly and linearly detected in the label tokens. Moreover, results using Ci (dotted line) shows extreme results, suggesting the label information remains in the label token. So, we can conclude that: hidden states of label tokens are joint representations of label semantics and text representations."}, {"title": "4.2 STEP 3, INDUCTION HEAD: FEATURE RETRIEVAL ON TASK SUBSPACE", "content": "This subsection examines the existence of the aforementioned induction heads, which retrieve simi- lar label token features as the queries' forerunner feature, and copy the retrieved features back to the query. We claim the necessity of multi-head attention in this process: correct feature retrieval can only be conducted on the subspace of the hidden space, which is captured by some attention heads.\nInduction is Correct in Minority Subspaces. Similar to Fig. 5 (Middle), we mark (1) the attention heads with the sum of attention scores from the query's forerunner token sq (as attention query) to all the label tokens [Y1],..., [Yk] (as attention keys) in the demonstration more than 5k/nt as induction heads, and (2) attention heads with the sum of scores to all the correct label tokens more than 5k/Ynt as correct induction heads (Y is a label space). We show the number of both kinds of induction heads in Fig. 6 (Left, detailed head statistics in Appendix F.1), where a unimodal pattern is observed later than the copy processing of Step 2. Moreover, more than half of the induction heads are not correct ones, suggesting that task-specific feature similarity can only be caught on some induction subspaces (defined by low-rank transition matrix Wh\u2122 Wh of correct induction head h). We enhance this claim in Fig. 6 (Middle) (details in Appendix A.4), where both vanilla attention (without transformation and head split) and attention scores averaged among heads show low as- signment on correct label tokens, while some heads show considerable correctness. Considering the average value, the majority of attention heads almost randomly copy label token information to the query, causing the prediction biased to the frequency of labels in the prompt (Zhao et al., 2021). As the reason, we infer that the hidden states are sufficient (Fig. 3) but not minimum for ICL, where redundant information interferes with the similarity calculation of attention."}, {"title": "5 PUTTING THINGS TOGETHER", "content": "So far, we have revealed the existence of the circuit with 3 steps, organized by the sequential in- ference process among Transformer layers. In this section, we find that the circuit is dominant in the ICL inference, while some bypass mechanisms activated by residual connection assist ICL in- ference. Moreover, a series of phenomena observed in ICL is successfully explained by the circuit."}, {"title": "5.1 ABLATION ANALYSIS", "content": "To demonstrate that our 3-phase circuit domi- nates or at least participates in ICL process, we disconnect the related attention connection of each step in the proposed circuit, and test the accuracies without such connections as shown in Table 1. The results show that when the non- trivial connections designated by the proposed circuit are ablated, accuracies of ICL signifi- cantly decrease, supporting the existence of our circuit. However, the result doesn't fully match expectations, for example, the result without in- duction (line 5) should be consistent with zero- shot (line 6), since all the expected communica- tion from demonstration to query is intercepted, but that's not the case; and the contribution of Step 1 in later layers are unexpectedly high, in- dicating the existence of some bypass mecha- nisms parallelly contributing to ICL accuracies."}, {"title": "5.2 BYPASS MECHANISM", "content": "Motivated by the ablation results, we believe that several mechanisms including our circuit run parallelly for ICL, since the residual connection supports complex paths among layers and attention heads. We list some possible bypasses and plan a complete enumeration as future work.\nParallel Circuits. Multiple 3-step circuits can execute in parallel, that is, one layer can assign multiple inference functions to different heads, causing dispersion and deserialization as shown in Fig. 8, where a narrow (fewer heads) and deep (more layers) model is more likely to gen- erate localized inference, and vice versa.\nDirect Decoding. Residual connection to output embedding allows intermediate hidden states to be decoded directly. Intuitively, a shortcut from Step 1 encodings to the LM head enables ICL with zero-shot capacities, since we have confirmed that encoded representations are informative for ICL tasks, while the decoding meth- ods should be selected carefully (Cho et al., 2024) (e.g. with essential calibration). On the other hand, shortcuts from insufficiently encoded features may lead to meaningless information decoded by language model heads, causing prediction bias, i.e., even if no query is given, ICL still returns unbalanced results (Zhao et al., 2021) decoded from tokens of prompt template (see Appendix D).\nShortcut Induction. Note that a k-shot ICL in- put sequence always contains a (k-1)-shot se- quence, where the k-th forerunner (once served as the query) is previously processed. So every forerunner can directly retrieve previously pro- cessed forerunners of demonstrations to copy their induction results directly. The non-trivial result (Table 2) with all forerunners disconnected from each other confirms such infer."}, {"title": "5.3 EXPLAINATION TOWARDS OBSERVED ICL PHENOMENA", "content": "Difficulty-based Demonstration Selection. In \u00a73.1, we find that in the zero-shot scenario, per- plexed texts are harder to be encoded, which explains the observation of PPL-ICL (Gonen et al., 2023) selecting demonstrations with lower perplexity. Moreover, while the demonstrations increase, LMs can encode more complex inputs with diverse information to update the attention assign- ment shown in Fig. 7, making it beneficial to input harder demonstrations later, which explains the ICCL (Liu et al., 2024), which build demonstrations sequence from easy to hard.\nPrediction Bias. (1) Contextual Bias: As shown in \u00a75.2 and Appendix D, direct decoding insuffi- ciently encoded information adds meanless logits into LM's output, causing a background prediction value even if no queries are given (named bias). (2) Position Bias: As shown in \u00a73.2, closer input texts are encoded more similarly, so label tokens near the query have more similar information to the query, causing more attention assignment in the induction processing, so that more influences on the prediction. (3) Frequency Bias: As shown in \u00a74.2, in the induction, some attention heads are without selectivity towards labels, causing an averaged induction from label tokens to the query, triggering a prediction bias towards the label frequency in the demonstration, even if their contribution (absolute value of attention score on label tokens) is small. All three biases are observed by Zhao et al. (2021), and can be removed by ICL calibration methods.\nThe Roles and Saturates of Demonstrations. It is well known that demonstrations improve the performance of ICL. We decompose such performance improvement into 2 parts: (1) demonstra- tions help early layers encode better (\u00a73.1), and (2) more demonstrations provide larger label token closure, enabling more accurate attention assignment (\u00a74.2), while the volume of such closure is submodular to demonstrations, causing the saturates of ICL performance towards demonstrations.\nThe Effect of Wrong Label. It is well-known that the label noise is less harmful in ICL (Min et al., 2022) than in gradient-based learning (Zhang et al., 2021). We have explained in \u00a74.1 that ICL implies labels denoise to stabilize ICL against label noise, while weakened by dimensionality."}, {"title": "6 CONCLUSION AND DISCUSSION", "content": "Conclusion. In summary, this paper restores ICL inference into 3 basic operations and confirms their existence. Careful measurements are conducted to capture and explain various phenomena successfully. Moreover, ablation studies show the proposed inference circuit dominates and reveals the existence of bypass mechanisms. We hope this paper can bring new insight into ICL practice.\nThe Role of Early and Later Layers. Our framework and Fig. 3 show: encoding result of Step 1 can be directly used for classification with reliable decoding, and later transformer layers are not contributing to centroid classification accuracies, leading to a taxonomy of En- coding for Step 1 and Output Preparation for Step 2 and 3. LMs complete multi-task classification implicitly in early layers, and verbalize it by merging task-specific label semantics in later layers. Therefore, we suggest removing some top layers and using a centroid classifier (Cho et al., 2024) to accelerate ICL inference as shown in Table 3 and Appendix D.\nPre-training Possibility from Natural Language Data. A large gap can be considered between such a precise circuit and gradient descent pre-training on the wild data. However, we believe the wild training target contains the ICL circuit functionally. Based on the previous works finding trainability of ICL on linear representation-label pairs (Chan et al., 2022; Reddy, 2024; Singh et al., 2024b), we speculate that in early training step, Transformers learn to extract linear representations shown in \u00a73 from wild data (Appendix B), serving as the training input of later layers to evoke the emergence of induction heads with the same mechanism shown in aforementioned previous works. Moreover, our conclusion of Step 3 highlights the input data requirements for the later layers: These data should activate the multiplex of hidden space, i.e., it should implicate multi-task classification with a wide distribution, which is consistent with the aforementioned previous works.\nLimitations. (1) These 3 basic operations are not functionally indivisible. Ideally, one can reduce every operation in ICL inference to the interconnection of special attention heads to ulteriorly exam- ine how the operating subspaces interact between steps. (2) Even though we have some insights into how pre-training promotes ICL in natural language models, its detailed dynamics are still unknown. One can start by decomposing pre-training targets into implicit tasks, and examine how these tasks can evoke the occurrence of the 3-step inference operations. (3) We only focus on classification tasks, while we believe that our findings can be applied to non-classification tasks, efforts are still needed to fill the gap."}, {"title": "A EXPERIMENT DETAILS AND SETTINGS", "content": ""}, {"title": "A.1 DETAILED OVERALL EXPERIMENTAL SETTINGS", "content": "Prompt Template. We conduct experiments on a specific prompt template for each dataset as shown in Table 4. Moreover, similar to typical ICL practices, we reduce the label into one token to simplify the prediction decoding. The reduced label tokens are also shown in Table 4.\nQuantization. In our experiments, we use BitsAndBytes' to quantize Llama 3 70B and Falcon 40B to INT4. For the other models, full-precision inference is conducted.\nOther. All the experiment materials (models and datasets) are loaded from huggingface. For the BGE M3, we use its pooler_output as the output feature."}, {"title": "A.2 CALCULATION OF MUTUAL NEAREST-NEIGHBOR KERNEL ALIGNMENT", "content": "In this paper, we need to measure the similarity between features from two different models or model layers. There are many approaches (Klabunde et al., 2023), and we use mutual nearest- neighbor kernel alignment (Huh et al., 2024), which is relatively efficient and accurate, calculated as follows to measure the similarity of representation from the same object set X = {xi}i=1 in different feature spaces.\nGiven a representation mapping \u03b4 : X \u2192 Hd from the objects to a space where similarity mea- surement\u3008\u00b7,\u00b7): Hd \u00d7 Hd \u2192 R is defined, we can calculate the similarity map from dataset X as S\u03b4 \u2208 Rn\u00d7n, where the elements are S\u03b4|i,j = \u3008\u03b4 (xi), \u03b4 (xj)\u3009, especially, we axiomatic define \u3008x, x\u3009 = 1, so we set the diagonal element S\u03b4|i,i = 0 since they are trivial values.\nGiven two encoding \u03b41 and \u03b42, two similarity map can be calculated as S\u03b41 and S\u03b42 on the same object set X. For each line vector index i = 1, 2, . . ., n in S\u03b41, we select the index of top-k elements from greater to lower as topk (S\u03b41|i). Similarly, we get topk (S\u03b42|i) from S\u03b42.\nThen, we calculate the kernel alignment for sample i as:\n$$KA_X(\\delta_1, \\delta_2) = \\frac{|topk (S_{\\delta_1|i}) \\cap topk (S_{\\delta_2|i})|}{k}$$\nImplementation. In our experiments, we choose cosine similarity as the \u3008,\u3009, and k = 64. According to experiment settings in \u00a72.2, n = 512 is defined, and a randomlized matrix S have KA = 64/512 0.125 as the random baseline."}, {"title": "A.2.1 BACKGROUND VALUES OF KERNEL ALIGNMENT: LABEL TOKEN - TEXT ENCODING.", "content": "Given two specific tokens xi and xj where kernel alignment is calculated from different ICL-styled input sequences pi and pj, in a specific layer of a decoder Transformer, the representations can be written as d(x) = e(x) + \u0454(p), where e(x) are the embedding vector of the token x, and e(p) are the residual side-flow w.r.t the context p.\nIntuition. As shown in Fig. 9, in the hidden states of ICL, the hidden state on the label token has a prior clustering, making it naturally similar to the representation generated by the encoder model, even if the ICL process does not encode it sufficiently. So, at layer 0, since the model is not able to"}, {"title": "A.3 TRAINING AND INFERENCE OF CENTROID CLASSIFIER", "content": "In this paper, we follow Cho et al. (2024) to train centroid classifiers as a probe toward hidden states of LMs. In detail, given the LM's hidden states set {hi}mi=1 of the selected tokens (according to the experimental setting, the last label token or forerunner token) in layer l from an [input prompt]- [query label] set Z = {(pi, yi)}mi=1, where the labels are limited in label space Y, in the training phase, we calculate the centroid of the hidden state \\(h_{\\)\\) for each label respectively:\n$$h_y = \\mathbb{E}_{i \\vert y_i=y} [h_i].$$\nIn the inference phase, we extract the equitant hidden state h\u2217 as the training phase from the test input, and calculate the similarity between h\u2217 and the centroids calculated above. Then, we choose the label of the most similar centroids as the prediction:\n$$C(\\hat{h}) = \\text{argmax}_y \\langle \\hat{h}, h_y \\rangle.$$\nImplementation. In our experiments, we set training sample number m = 256, similarity function \\(\\langle a, b \\rangle = - \\| a - b \\|^2\\)."}, {"title": "A.4 CARTOGRAPHY DETAILS OF FIG. 6 (MIDDLE)", "content": "In Fig. 6 (Middle), we define a Correct Label Assignment, here we introduce how this measurement is calculated. Suppose we have an attention score AW,f (K, Q) calculated as:\n$$A_{W,f} (K, Q) = f (Q^T W K),$$\nwith hidden dimensionality of d, give a certain layer, K \u2208 Rd\u00d7nt is the hidden state matrix of full context, Q \u2208 Rd\u00d71 is the hidden state of query's forerunner token (QT = Kq), f : Rnt \u2192 Ont is a normalization mapping from nt-dimensional real vector to nt-dimensional probability vector (usually softmax function), the W is a linear kernel, usually WhWk for multi-head attention or I = diag (1nt) for vanilla attention.\nFor one input sample, given the token-index set of label tokens as L, the token-index set of label tokens which is the same as the query's ground truth label as L+, we define the Correct Label Assignment (CLA) of one sample as:\n$$CLA_{W,f} (K, Q, L, L^+) = \\frac{\\sum_{i \\in L^+} A_{W,f} (K, Q)_i}{\\sum_{i \\in L} A_{W,f} (K, Q)_i};$$\nIntuitively, CLA reflects the accuracy of attention computation AW,f towards label tokens on one input. For an input set built from a dataset, we calculate the averaged CLA on these inputs, and repeat in every layer to plot a curve of Averaged CLA against layer numbers. Specifically:"}, {"title": "A.5 CARTOGRAPHY DETAILS OF FIG. 7", "content": "For Fig. 7, we input one sample from SST-2 into Llama 3 70B, take the output of layer 30 on the label tokens to span a matrix Kc, and map them by Wh\u2122 Wk of head 32 (the best induction heads in this layer) and 9 (the worst induction heads) of layer 31 (the layer with the most correct induction heads), respectively. We visualize the distribution of these mapped WhWKL, we conduct principal component analysis on them, and plot them on the plane of the first two components.\nFor each point q \u2208 R2 on the principal component plane, we calculate the attention assignment as follows. Give the index set of \u201cpositive\u201d label token in Kc as L+, the index set of \u201cnegative\u201d label token as L\u2212, we calculate the attention assignment, which can be an estimate of ICL predic- tion (Wang et al., 2023), as:\n$$AttAssign(q) = \\frac{\\sum_{i \\in L^+} q^T W h_i W_k L_i}{\\sum_{i \\in L^+} \\| W h_i W_k L_i \\|} - \\frac{\\sum_{i \\in L^-} q^T W h_i W_k L_i}{\\sum_{i \\in L^-} \\| W h_i W_k L_i \\|}.$$\nWe map this value to the degree of blue color of each pixel. The larger the positive value, the bluer it is, and the smaller the negative value, the redder it is."}, {"title": "A.6 CARTOGRAPHY DETAILS OF FIG. 8", "content": "For Fig. 8, we calculate the magnitude of Step 1 as the finite differences of kernel alignment in Fig. 2 (Left, Forerunner Token of Label). We directly use the head counting of Fig. 5 (Middle) and Fig. 6 (Left) as the magnitude of Steps 2 and 3. These data are regularized and converted into transparencies."}, {"title": "B LM PRE-TRAINING DYNAMICS MEASURED BY ICL CIRCUIT", "content": "We extend the discussion of pre-training dynamics in \u00a76 here.\nOne can divide a self-regression model into an early part and a later part, where the early part encodes the input into a hidden representation, and the later part decodes the hidden represen- tation back to the input. So, the training object can also be divided into an encoding loss and a decoding loss. According to the discussion in \u00a76, the summarization operation (Step 1) can be classified as encoding, and the other two steps of the in- duction circuit can be classified as decoding. Intuitively, since the decoding operations require the encoding results as input, unless the encoding operation converges to a stable output, the decoding can not be trained since"}]}