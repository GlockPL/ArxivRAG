{"title": "Start from Video-Music Retrieval: An Inter-Intra Modal Loss for Cross Modal Retrieval", "authors": ["Zeyu Chen", "Pengfei Zhang", "Kai Ye", "Wei Dong", "Xin Feng", "Yana Zhang"], "abstract": "The burgeoning short video industry has accelerated the advancement of video-music retrieval technology, assisting content creators in selecting appropriate music for their videos. In self-supervised training for video-to-music retrieval, the video and music samples in the dataset are separated from the same video work, so they are all one-to-one matches. This does not match the real situation. In reality, a video can use different music as background music, and a music can be used as background music for different videos. Many videos and music that are not in a pair may be compatible, leading to false negative noise in the dataset. A novel inter-intra modal (II) loss is proposed as a solution. By reducing the variation of feature distribution within the two modalities before and after the encoder, II loss can reduce the model's overfitting to such noise without removing it in a costly and laborious way. The video-music retrieval framework, II-CLVM (Contrastive Learning for Video-Music Retrieval), incorporating the II Loss, achieves state-of-the-art performance on the YouTube8M dataset. The framework II-CLVTM shows better performance when retrieving music using multi-modal video information (such as text in videos). Experiments are designed to show that II loss can effectively alleviate the problem of false negative noise in retrieval tasks. Experiments also show that II loss improves various self-supervised and supervised uni-modal and cross-modal retrieval tasks, and can obtain good retrieval models with a small amount of training samples.", "sections": [{"title": "I. INTRODUCTION", "content": "With the increasing content demands in the short video industry, AI-assisted video editing has greatly increased the efficiency in video production. To choose a piece of good background music (BGM) for a video by Al is our main research point. Some supervised learning based algorithms select music by the matching scores of tags or the feature distance in the emotional space. In recent works, music selection becomes a cross-modal retrieval task based on contrastive learning. Video and audio encoders learn various of video-music matching factors in a self-supervised training way. The cosine distance of the encoded cross-modal features is used to assess the suitability of media candidates.\nThis paper focuses on solving the problem of training with noisy data in self-supervised cross-modal retrieval. In self-supervised learning, a pair of video-music samples come from the same video work, so the dataset has only one-to-one matches. This obviously does not accurately reflect the real situation. In fact, a piece of music can be used as the background music for different videos, and a video can also be paired with different background music. In this case, there are many suitable videos and music that are not in the same pair, resulting in many false negative noisy samples. The cross-modal training objective is to minimize the distance between positive samples and maximize the distance between negative samples. When the model is overfitted to the noise, the distance between many false negative sample pairs is maximized, leading to a decrease in the model's generalization ability. To tackle this challenge, a novel inter-intra modal loss (II Loss) is specifically designed to handle this type of noise. The II Loss addresses the issue by using the intra loss component to minimize drastic variations in the feature distributions within each modality during training. This approach effectively mitigates overfitting on the noisy data and allows for more accurate retrieval of relevant matches without requiring complex noise removal techniques. The proposed framework Inter-Intra Contrastive Learning for Video-Music Retrieval (II-CLVM) based on inter-intra modal loss achieves the state-of-the-art on video-music retrieval on Youtube8M and performs better when retrieving music using multi-modal video information (such as text). II loss is also performs well for other cross-modal retrieval tasks.\nOur contributions are as follows:\n\u2022 This paper proposes inter-intra modal loss (II loss), which enables the retrieval models trained on noisy data to have better generalization ability. II loss alleviates the model's overfitting to false negative noise by minimizing the drastic changes in feature distribution within each modality. II loss works well in various self-supervised and supervised uni-modal and cross-modal retrieval tasks.\n\u2022 The II-CLVM video-music retrieval framework is devel-oped. It incorporates II Loss and achieves state-of-the-art performance on the YouTube8M dataset. Additionally, the framework employs Global Sparse (GS) sampling which allows music retrieval to be based on the content of the complete video, rather than on fixed-duration video clips. The framework can also easily integrate multi-modal video information (such as images and text) to achieve better performance."}, {"title": "II. RELATED WORK", "content": "In recent works, background music selection for video is a cross-modal retrieval task based on pretrained features. Contrastive learning enables the model to learn rich matching rules in large-scale video-music datasets and reduce the feature distance between matched video-music pairs.\nA. Contrastive learning and cross-modal retrieval\nDue to the pervasively existed video music pairs online, retrieving a music for a video is usually treated as a cross-"}, {"title": "III. THE PROPOSED FRAMEWORK", "content": "Fig 1 shows the architecture of the framework II-CLVM (Inter-Intra Contrastive Learning for Video-Music Retrieval) with the proposed inter-intra modal loss. During the model training, there are N video-music pairs in each mini-batch. Firstly, global sparse (GS) sampling is performed on both video and music, and extract the pretrained feature sequences V = {vi}i=1N for video and M = {mj}j=1N for music. Then, the video embeddings V' = {vi'}i=1N and music embeddings M' = {mj'}j=1N are obtained by separate sequence encoders. The inter-intra (II) modal loss is proposed to measure the distance between the encoded video embeddings and music embeddings. The detail of each module of II-CLVM is introduced below.\nA. Global sparse sampling\nExisting video-music retrieval usually takes one continuous fixed-duration (FD) clip from the original media to represent the whole sequence, e.g. cutting 30s around the center of both video and music as in [7]. Those methods ignore the rest parts of video and music, so that the retrieved music may only be partially related to the video. To extract features of the entire video and the whole music, the global sparse (GS) sampling [34] is applied. For video i, it is split evenly into Tv clips and the video feature sequence vi \u2208 RTv\u00d7Ev is obtained where Ev is the dimension of the feature. Similarly, the audio feature sequence mj \u2208 RTm\u00d7Em_is obtained for music j. Note that the purpose of extracting feature sequences of fixed length for video and music of different durations is to eliminate duration information and enable the model to retrieve based on content.\nB. Sequence encoders\nTo extract the temporal information from the frame-level video and music feature sequences, V and M are fed into two sequence encoders (biLSTM, transformer encoder, etc), respectively. After encoding, the encoded video feature V' = {vi'}i=1N, (vi' \u2208 R1\u00d7D) and music feature M' = {mj'}j=1N, (mj' \u2208 R1\u00d7D) are obtained, where D is the fixed hidden dimension of the sequence encoders for both video and music modalities.\nC. The inter-intra (II) modal loss\nAs illustrated in Fig 2, we consider an example of a batch with a size of 4. The video v2 and the music m2 constitute a pair of positive samples. Due to the similarity between music m4 and m2, m4 can also be effectively utilized as background music (BGM) for video v2. During cross-modal training, if solely relying on the conventional inter-modal loss, the distance between the features output by the encoder v2 and m2 would decrease, while the distance between the features v2 and m4 would increase, which is an undesired outcome. As depicted in Fig 2, the proposed intra loss aims to preserve"}, {"title": "1) Inter-modal loss:", "content": "As shown in equation (2), the inter-modal loss is calculated based on the cosine similarity matrix S \u2208 RN\u00d7N. Each element of S is calculated as follows:\n$S(i,j) = vi' \\otimes m'j, (i, j \\in {1, 2, ..., N})$ (1)\nwhere \u2297 is defined as the cosine similarity between two vectors.\n$L_{inter} = \\frac{1}{N} \\sum_{i=1}^{N} (\\alpha_1 \\sum CE(\\sigma(e^{nt} S_{i,:} ), I_{i,:}) + \\alpha_2 \\sum CE(\\sigma(e^{nt} S_{:,j}), I_{:,j}))$  (2)\nj=1\nIn S, Si,: is the ith row, and S:,j is the jth column. nt is a learnable temperature parameter and ent controls the range of the logits in the Softmax function. I is an N-order identity matrix. CE(\u00b7) is the cross entropy loss, and \u03c3(\u00b7) means the Softmax function. \u03b11 and \u03b12 are adjustable parameters. The inter-modal loss increases values on the diagonal of S and decreases those in other positions."}, {"title": "2) Intra-modal loss:", "content": "For the video modality, two intra-modal similarity matrices Sv \u2208 RN\u00d7N and Sv' \u2208 RN\u00d7N are calculated as shown in Fig 1. In a mini-batch, Sv and Sv' describe the similarity of different video features before and after the encoder, respectively. Here, each element of Sv and Sv' is calculated as follows:\n$S_{v(i,j)} = v_i \\otimes v_j, (i, j \\in {1, 2, ..., N})$ (3)\n$S_{v'(i,j)} = v'_i \\otimes v'_j, (i, j \\in {1, 2, ..., N})$ (4)\nwhere vi is the temporal average of vi. To achieve the invariance of feature distribution before and after encoding, Sv and Sv' should be similar. The vectors Svv'_r \u2208 R1\u00d7N and Svv'_c \u2208 R1\u00d7N are calculated to describe the row similarity and column similarity between Sv and Sv', respectively. The calculation is as follows:\n$S_{vv'_r,i} = S_{v,i,:} \\otimes S_{v',i,:}, (i \\in {1, 2, ..., N})$ (5)\n$S_{vv'_c,j} = S_{v,:,j} \\otimes S_{v',:,j}, (j \\in {1, 2, ..., N})$ (6)\nThen, the intra-modal loss is calculated as follows:\n$L_{intra,v} = \\frac{\\delta_1}{N} \\sum_{i=1}^{N} (1 - \\delta_{vv'_r,i}) + \\frac{\\delta_2}{N} \\sum_{j=1}^{N} (1 - \\delta_{vv'_c,j})$ (7)\nwhere \u03b41 and \u03b42 are the weight parameters. Here, Sv(i,j) = Sv(j,i), Sv'(i,j) = Sv'(j,i), So Svv'_r is equivalent to Svv'_c, denoted as Svv'. The intra-modal loss can be simplified to:\n$L_{intra,v} = \\frac{1}{N} \\sum_{i=1}^{N} (1 - \\delta_{vv',i})$ (8)\nThe music modality is processed in the same way. The total intra-modal loss Lintra is computed as equation (9), where \u03b21 and \u03b22 are the weights of intra loss of the two modalities respectively. The inter-intra modal loss Lii is a weighted sum of inter-modal loss and intra-modal loss as shown in equation (10). Lii could work with various feature extractors and both of the above sampling methods."}, {"title": "D. II-CLVTM", "content": "When choosing a background music (BGM) for a video, the video creator may also provide some text information, such as the video's description, title or keywords. Our framework can easily incorporate these text inputs. We use the Inter-Intra Contrastive Learning for Video&Text-to-Music Retrieval (II-CLVTM) model, as shown in Fig 3, to fuse and encode the raw video feature sequences V and text feature vectors T with biLSTM. The text feature vector is used as the initial hidden vector for biLSTM. As Fig 3 shows, when the query is multi-modal, we compute the intra-modal loss based on the similarity matrix of the features before and after the cross-modal encoder. We also note that the uncoded feature Vr is obtained by concatenating the raw text feature and the average video feature."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "The experiments in this section consist of three parts. Section IV-A mainly introduces the experiments of II-CLVM on the video-music retrieval task on the YouTube8M dataset. Section IV-B verifies the generality of II loss on various other cross-modal retrieval tasks. Section IV-C designs two experiments to verify that II loss can effectively alleviate the problem of false negative noise in the retrieval datasets.\nA. Experiments on YouTube8M\nIn this section, the performance of each module in II-CLVM is tested on video-music retrieval task of official YouTube8M dataset [35]. What's more, a subjective evaluation is conducted to check the BGM selection performance of II-CLVM on YouTube8M.\nIn YouTube8M [35], there are 149,213 video samples labeled as Music Video, in which 116,098 videos are for training and 33,115 videos are for testing. Three testing"}, {"title": "1) GS sampling:", "content": "Taking biLSTM as an example of the encoder, the FD (fixed-duration) and GS (global sparse)"}, {"title": "5) II-CLVTM:", "content": "This subsection evaluates the performance of the II-CLVTM framework. We create a subset of YouTube8M music videos that have both titles and keywords. The subset contains 119,191 video samples, of which 92,604 are for training and 26,587 are for testing. Besides using the official video and music features from YouTube8M, we also extract Clip-text [15] feature vectors for titles and keywords. We randomly generate a test set with 1000 sample pairs from the test data. In Table IV, we compare the R@k(k = 1, 10, 25) of II-CLTM(Inter-Intra Contrastive Learning for Text-to-Music Retrieval), II-CLVM, II-CLVTM with CLTM, CLVM, and CLVTM. The results show that using video and text together for music retrieval achieves higher R@k than using video or text alone, regardless of whether the text information is the title or the keyword. Therefore, it is better to select BGM based on multi-modal features rather than a single modality. Moreover, II Loss performs well under all query conditions, especially when multi-modal information is used as the query."}, {"title": "B. II loss on other retrieval tasks", "content": "This section evaluates the generality of II loss on different cross-modal retrieval tasks using the audio-text dataset Clotho, the image-text datasets MSCOCO and Flickr30K, and the video-text datasets MSVD, MSRVTT and VATEX. The details"}, {"title": "C. The effect of II loss against false negative noise interfer-ence", "content": "This section presents two experiments that focus on ex-amining whether II loss can effectively resist the noise that exists in self-supervised retrieval tasks. The first experiment was performed on the YouTube8M dataset. Based on all the other experimental settings of the group biLSTM(II-CLVM) in section 4.1, this experiment changed the proportion of noise matching pairs by changing the batch size of training, and thus checked the different optimization effects of II loss under different amounts of noise. The effectiveness of II loss in dealing with noise is demonstrated when comparing the II-CLVM group to the CLVM group; the former experiences a considerably slower decrease in R@k, suggesting that the II loss is effective in mitigating the adverse effects of noise, and ultimately achieves the highest R@k with a batch size of 128.\nTo test the effect of II loss on resisting different levels of noise interference in training, another experiment was conducted on an image classification dataset Caltech-256 by manually adjusting the noise amount in training batches."}, {"title": "V. CONCLUSION", "content": "The paper emphasizes the II loss as a key innovation in the new framework II-CLVM designed specifically for video-music retrieval. This innovative loss function improves the model's generalization capabilities by maintaining pretrained feature distribution within the two modalities during training on noisy cross-modal datasets. The II-CLVM framework, which incorporates the II loss, has shown promising results in video-music retrieval tasks. The state-of-the-art results achieved on the youtube8m dataset for video-music retrieval tasks demonstrate the effectiveness of the II loss. The II-CLVTM framework with added multi-modal video information input (such as text) has better music retrieval performance in applications. Subjective evaluations also support the frame-work's strong performance in music selection.\nBeyond its application in the II-CLVM framework for video-music retrieval, the II loss has proven to be valuable for other cross-modal retrieval tasks as well, such as image-text, audio-text, and video-text retrieval. This showcases the adaptability and versatility of the II loss in different contexts and applications. Moreover, II loss is found to obtain good retrieval models with a small number of training samples. Although the current findings are encouraging, the au-thors acknowledge there is potential for further enhancement,"}]}