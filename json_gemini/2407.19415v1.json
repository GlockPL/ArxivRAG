{"title": "Start from Video-Music Retrieval: An Inter-Intra Modal Loss for Cross Modal Retrieval", "authors": ["Zeyu Chen", "Pengfei Zhang", "Kai Ye", "Wei Dong", "Xin Feng", "Yana Zhang"], "abstract": "The burgeoning short video industry has accelerated the advancement of video-music retrieval technology, assisting content creators in selecting appropriate music for their videos. In self-supervised training for video-to-music retrieval, the video and music samples in the dataset are separated from the same video work, so they are all one-to-one matches. This does not match the real situation. In reality, a video can use different music as background music, and a music can be used as background music for different videos. Many videos and music that are not in a pair may be compatible, leading to false negative noise in the dataset. A novel inter-intra modal (II) loss is proposed as a solution. By reducing the variation of feature distribution within the two modalities before and after the encoder, II loss can reduce the model's overfitting to such noise without removing it in a costly and laborious way. The video-music retrieval framework, II-CLVM (Contrastive Learning for Video-Music Retrieval), incorporating the II Loss, achieves state-of-the-art performance on the YouTube8M dataset. The framework II-CLVTM shows better performance when retrieving music using multi-modal video information (such as text in videos). Experiments are designed to show that II loss can effectively alleviate the problem of false negative noise in retrieval tasks. Experiments also show that II loss improves various self-supervised and supervised uni-modal and cross-modal retrieval tasks, and can obtain good retrieval models with a small amount of training samples.\nIndex Terms-inter-intra modal loss, video-music retrieval, cross-modal retrieval, contrastive learning.", "sections": [{"title": "I. INTRODUCTION", "content": "With the increasing content demands in the short video industry, AI-assisted video editing has greatly increased the efficiency in video production. To choose a piece of good background music (BGM) for a video by Al is our main research point. Some supervised learning based algorithms select music by the matching scores of tags [1] [2] or the feature distance in the emotional space [3] [4] [5]. In recent works [6] [7] [8], music selection becomes a cross-modal retrieval task based on contrastive learning. Video and audio encoders learn various of video-music matching factors in a self-supervised training way. The cosine distance of the encoded cross-modal features is used to assess the suitability of media candidates.\nThis paper focuses on solving the problem of training with noisy data in self-supervised cross-modal retrieval. In self-supervised learning, a pair of video-music samples come from the same video work, so the dataset has only one-to-one matches. This obviously does not accurately reflect the real situation. In fact, a piece of music can be used as the background music for different videos, and a video can also be paired with different background music. In this case, there are many suitable videos and music that are not in the same pair, resulting in many false negative noisy samples. The cross-modal training objective is to minimize the distance between positive samples and maximize the distance between negative samples. When the model is overfitted to the noise, the distance between many false negative sample pairs is maximized, leading to a decrease in the model's generalization ability. To tackle this challenge, a novel inter-intra modal loss (II Loss) is specifically designed to handle this type of noise. The II Loss addresses the issue by using the intra loss component to minimize drastic variations in the feature distributions within each modality during training. This approach effectively mitigates overfitting on the noisy data and allows for more accurate retrieval of relevant matches without requiring complex noise removal techniques. The proposed framework Inter-Intra Contrastive Learning for Video-Music Retrieval (II-CLVM) based on inter-intra modal loss achieves the state-of-the-art on video-music retrieval on Youtube8M and performs better when retrieving music using multi-modal video information (such as text). II loss is also performs well for other cross-modal retrieval tasks.\nOur contributions are as follows:\n\u2022 This paper proposes inter-intra modal loss (II loss), which enables the retrieval models trained on noisy data to have better generalization ability. II loss alleviates the model's overfitting to false negative noise by minimizing the drastic changes in feature distribution within each modality. II loss works well in various self-supervised and supervised uni-modal and cross-modal retrieval tasks.\n\u2022 The II-CLVM video-music retrieval framework is developed. It incorporates II Loss and achieves state-of-the-art performance on the YouTube8M dataset. Additionally, the framework employs Global Sparse (GS) sampling which allows music retrieval to be based on the content of the complete video, rather than on fixed-duration video clips. The framework can also easily integrate multi-modal video information (such as images and text) to achieve better performance."}, {"title": "II. RELATED WORK", "content": "In recent works, background music selection for video is a cross-modal retrieval task based on pretrained features. Contrastive learning enables the model to learn rich matching rules in large-scale video-music datasets and reduce the feature distance between matched video-music pairs.\nDue to the pervasively existed video music pairs online, retrieving a music for a video is usually treated as a cross-\n{\\begin{array}{ll}\\text { A. Contrastive learning and cross-modal retrieval }\\end{array}}"}, {"title": "III. THE PROPOSED FRAMEWORK", "content": "Fig 1 shows the architecture of the framework II-CLVM (Inter-Intra Contrastive Learning for Video-Music Retrieval) with the proposed inter-intra modal loss. During the model training, there are N video-music pairs in each mini-batch. Firstly, global sparse (GS) sampling is performed on both video and music, and extract the pretrained feature sequences V = {vi}i=1 for video and M = {mj}j=1 for music. Then, the video embeddings V' = {v'i}i=1 and music embeddings M' = {m'j}j=1 are obtained by separate sequence encoders. The inter-intra (II) modal loss is proposed to measure the distance between the encoded video embeddings and music embeddings. The detail of each module of II-CLVM is introduced below.\nExisting video-music retrieval usually takes one continuous fixed-duration (FD) clip from the original media to represent the whole sequence, e.g. cutting 30s around the center of both video and music as in [7]. Those methods ignore the rest parts of video and music, so that the retrieved music may only be partially related to the video. To extract features of the entire video and the whole music, the global sparse (GS) sampling [34] is applied. For video i, it is split evenly into Tv clips and the video feature sequence vi \u2208 \\mathbb{R}^{T_v \\times E_v} is obtained where Ev is the dimension of the feature. Similarly, the audio feature sequence mj \u2208 \\mathbb{R}^{T_m \\times E_m} is obtained for music j. Note that the purpose of extracting feature sequences of fixed length for video and music of different durations is to eliminate duration information and enable the model to retrieve based on content.\nTo extract the temporal information from the frame-level video and music feature sequences, V and M are fed into two sequence encoders (biLSTM, transformer encoder, etc), respectively. After encoding, the encoded video feature V' = {v'i}i=1, (v'i \u2208 \\mathbb{R}^{1 \\times D}) and music feature M' = {m'j}j=1, (m'j \u2208 \\mathbb{R}^{1 \\times D}) are obtained, where D is the fixed hidden dimension of the sequence encoders for both video and music modalities.\nAs illustrated in Fig 2, we consider an example of a batch with a size of 4. The video v2 and the music m2 constitute a pair of positive samples. Due to the similarity between music m4 and m2, m4 can also be effectively utilized as background music (BGM) for video v2. During cross-modal training, if solely relying on the conventional inter-modal loss, the distance between the features output by the encoder v2 and m2 would decrease, while the distance between the features v2 and m4 would increase, which is an undesired outcome. As depicted in Fig 2, the proposed intra loss aims to preserve\n{\\begin{array}{ll}\\text { A. Global sparse sampling }\\end{array}}\n{\\begin{array}{ll}\\text { B. Sequence encoders }\\end{array}}\n{\\begin{array}{ll}\\text { C. The inter-intra (II) modal loss }\\end{array}}"}, {"title": "1) Inter-modal loss:", "content": "As shown in equation (2), the inter-modal loss is calculated based on the cosine similarity matrix S\u2208\\mathbb{R}^{N\u00d7N}. Each element of S is calculated as follows:\n{\\begin{equation}S(i, j)=v_{i}^{\\prime} \\otimes m_{j}^{\\prime},(i, j \\in\\{1,2, \\ldots, N\\})\\end{equation}}\nwhere \\otimes is defined as the cosine similarity between two vectors.\n{\\begin{equation}L_{\\text {inter }}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\alpha_{1} \\sum C E\\left(\\sigma(\\theta_{\\text {tent }} S_{i,:}), I_{i,:}\\right)+\\alpha_{2} \\sum C E\\left(\\sigma(\\theta_{\\text {tent }} S_{:, j}), I_{:, j}\\right)\\right)\\end{equation}}\nIn S, Si,: is the ith row, and S:,j is the jth column. \\theta_\\text{tent} is a learnable temperature parameter and tent controls the range of the logits in the Softmax function. I is an N-order identity matrix. CE(\u00b7) is the cross entropy loss, and \u03c3(\u00b7) means the Softmax function. \u03b11 and \u03b12 are adjustable parameters. The inter-modal loss increases values on the diagonal of S and decreases those in other positions."}, {"title": "2) Intra-modal loss:", "content": "For the video modality, two intra-modal similarity matrices Sv \u2208 \\mathbb{R}^{N\u00d7N} and Sv' \u2208 \\mathbb{R}^{N\u00d7N} are calculated as shown in Fig 1. In a mini-batch, Sv and Sv' describe the similarity of different video features before and after the encoder, respectively. Here, each element of Sv and Sv' is calculated as follows:\n{\\begin{equation}S_{v(i, j)}=v_{i} \\otimes v_{j},(i, j \\in\\{1,2, \\ldots, N\\})\\end{equation}}\n{\\begin{equation}S_{v^{\\prime}(i, j)}=v_{i}^{\\prime} \\otimes v_{j}^{\\prime},(i, j \\in\\{1,2, \\ldots, N\\})\\end{equation}}\nwhere vi is the temporal average of vi. To achieve the invariance of feature distribution before and after encoding, Sv and Sv' should be similar. The vectors Svv'_r \u2208 \\mathbb{R}^{1\u00d7N} and Svv'_c \u2208 \\mathbb{R}^{1\u00d7N} are calculated to describe the row similarity and column similarity between Sv and Sv', respectively. The calculation is as follows:\n{\\begin{equation}S_{v v^{\\prime} r, i}=S_{v, i,:} \\otimes S_{v^{\\prime}, i,:},(i \\in\\{1,2, \\ldots, N\\})\\end{equation}}\n{\\begin{equation}S_{v v^{\\prime} c, j}=S_{v,:, j} \\otimes S_{v^{\\prime},:, j},(j \\in\\{1,2, \\ldots, N\\})\\end{equation}}\nThen, the intra-modal loss is calculated as follows:\n{\\begin{equation}L_{\\text {intra,v }}=\\frac{1}{N}\\left(\\delta_{1} \\sum_{i=1}^{N}\\left(1-\\delta_{v v^{\\prime} r, i}\\right)+\\delta_{2} \\sum_{j=1}^{N}\\left(1-\\delta_{v v^{\\prime} c, j}\\right)\\right)\\end{equation}}\nwhere \u03b41 and \u03b42 are the weight parameters. Here, Sv(i,j) = Sv(j,i), Sv'(i,j) = Sv'(j,i), So Svv'_r is equivalent to Svv'_c, denoted as Svv'. The intra-modal loss can be simplified to:\n{\\begin{equation}L_{\\text {intra,v }}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(1-\\delta_{v v^{\\prime}, i}\\right)\\end{equation}}\nThe music modality is processed in the same way. The total intra-modal loss Lintra is computed as equation (9), where \u03b21 and \u03b22 are the weights of intra loss of the two modalities respectively. The inter-intra modal loss Lii is a weighted sum of inter-modal loss and intra-modal loss as shown in equation (10). Lii could work with various feature extractors and both of the above sampling methods.\n{\\begin{equation}L_{\\text {intra }}=\\beta_{1} L_{\\text {intra,v }}+\\beta_{2} L_{\\text {intra,m }}\\end{equation}}\n{\\begin{equation}L_{i i}=\\frac{1}{2}\\left(\\gamma_{1} L_{\\text {inter }}+\\gamma_{2} L_{\\text {intra }}\\right)\\end{equation}}"}, {"title": "D. II-CLVTM", "content": "When choosing a background music (BGM) for a video, the video creator may also provide some text information, such as the video's description, title or keywords. Our framework can easily incorporate these text inputs. We use the Inter-Intra Contrastive Learning for Video&Text-to-Music Retrieval (II-CLVTM) model, as shown in Fig 3, to fuse and encode the raw video feature sequences V and text feature vectors T with biLSTM. The text feature vector is used as the initial hidden vector for biLSTM. As Fig 3 shows, when the query is multi-modal, we compute the intra-modal loss based on the similarity matrix of the features before and after the cross-modal encoder. We also note that the uncoded feature Vr is obtained by concatenating the raw text feature and the average video feature."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "The experiments in this section consist of three parts. Section IV-A mainly introduces the experiments of II-CLVM on the video-music retrieval task on the YouTube8M dataset. Section IV-B verifies the generality of II loss on various other cross-modal retrieval tasks. Section IV-C designs two experiments to verify that II loss can effectively alleviate the problem of false negative noise in the retrieval datasets.\n{\\begin{array}{ll}\\text { A. Experiments on YouTube8M }\\end{array}}\n{\\begin{array}{ll}\\text { 1) GS sampling: }\\end{array}}\n{\\begin{array}{ll}\\text { 2) The sequence encoder: }\\end{array}}\n{\\begin{array}{ll}\\text { 3) II loss in II-CLVM: }\\end{array}}\n{\\begin{array}{ll}\\text { 4) Subjective evaluation of video-to-music retrieval: }\\end{array}}\n{\\begin{array}{ll}\\text { 5) II-CLVTM: }\\end{array}}\n{\\begin{array}{ll}\\text { B. II loss on other retrieval tasks }\\end{array}}\n{\\begin{array}{ll}\\text { C. The effect of II loss against false negative noise interference }\\end{array}}"}, {"title": "V. CONCLUSION", "content": "The paper emphasizes the II loss as a key innovation in the new framework II-CLVM designed specifically for video-music retrieval. This innovative loss function improves the model's generalization capabilities by maintaining pretrained feature distribution within the two modalities during training on noisy cross-modal datasets. The II-CLVM framework, which incorporates the II loss, has shown promising results in video-music retrieval tasks. The state-of-the-art results achieved on the youtube8m dataset for video-music retrieval tasks demonstrate the effectiveness of the II loss. The II-CLVTM framework with added multi-modal video information input (such as text) has better music retrieval performance in applications. Subjective evaluations also support the framework's strong performance in music selection.\nBeyond its application in the II-CLVM framework for video-music retrieval, the II loss has proven to be valuable for other cross-modal retrieval tasks as well, such as image-text, audio-text, and video-text retrieval. This showcases the adaptability and versatility of the II loss in different contexts and applications. Moreover, II loss is found to obtain good retrieval models with a small number of training samples. Although the current findings are encouraging, the authors acknowledge there is potential for further enhancement, particularly in areas such as retrieval performance on tasks involving large datasets and the development of noise-resistant methods for end-to-end retrieval models. By addressing these challenges, the II loss has the potential to become an even more effective and versatile tool in the field of cross-modal retrieval."}]}