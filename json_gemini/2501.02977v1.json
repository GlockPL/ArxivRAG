{"title": "CAMP: Collaborative Attention Model with Profiles for Vehicle Routing Problems", "authors": ["Chuanbo Hua", "Federico Berto", "Jiwoo Son", "Seunghyun Kang", "Changhyun Kwon", "Jinkyoo Park"], "abstract": "The profiled vehicle routing problem (PVRP) is a generalization of the heterogeneous capacitated vehicle routing problem (HCVRP) in which the objective is to optimize the routes of vehicles to serve client demands subject to different vehicle profiles, with each having a preference or constraint on a per-client basis. While existing learning methods have shown promise for solving the HCVRP in real-time, no learning method exists to solve the more practical and challenging PVRP. In this paper, we propose a Collaborative Attention Model with Profiles (CAMP), a novel approach that learns efficient solvers for PVRP using multi-agent reinforcement learning. CAMP employs a specialized attention-based encoder architecture to embed profiled client embeddings in parallel for each vehicle profile. We design a communication layer between agents for collaborative decision-making across profiled embeddings at each decoding step and a batched pointer mechanism to attend to the profiled embeddings to evaluate the likelihood of the next actions. We evaluate CAMP on two variants of PVRPs: PVRP with preferences, which explicitly influence the reward function, and PVRP with zone constraints with different numbers of agents and clients, demonstrating that our learned solvers achieve competitive results compared to both classical state-of-the-art neural multi-agent models in terms of solution quality and computational efficiency. We make our code openly available at https://github.com/ai4co/camp.", "sections": [{"title": "1 INTRODUCTION", "content": "Vehicle Routing Problems (VRPs) are a class of well-known combinatorial optimization (CO) problems where the objective is to determine the most efficient set of routes for a fleet of vehicles to deliver goods to a set of clients. A particularly challenging variant is the Heterogeneous Capacitated Vehicle Routing Problem (HCVRP), where the fleet consists of vehicles with varying capacities and operational costs [16]. In many real-world scenarios, different vehicles might not only have capacity constraints but also profile-specific preferences or operational constraints that can vary per client. This problem, which we term the Profiled Vehicle Routing Problem (PVRP), generalizes HCVRP by incorporating vehicle profiles with client-specific preferences or constraints [8]. These profiles might affect routing decisions based on factors like vehicle access to particular areas, preferred client relationships, or regulatory requirements for specific vehicle-client combinations [1, 37, 56, 73]. Solving the PVRP with exact solution methods, such as Branch and Bound, becomes impractical for large instances due to its NP-hard nature [47]. Thus, heuristic approaches like genetic algorithms are used in practical scenarios offering approximate solutions trying to balance solution quality and computational efficiency, but they may struggle in larger scales [43] and require considerable manual design and tuning [25].\nRecent advances in reinforcement learning (RL) for combinatorial optimization provide a promising alternative with several benefits. RL can automatically learn effective solutions without supervision by interacting with relatively inexpensive simulated environments; thus, designing RL methods requires little to no domain expertise. RL methods can find faster and possibly better solutions than heuristics approaches, particularly in improving scalability to more complex real-world problem instances. Most works follow the pointer network paradigm [2, 61], an encoder-decoder architecture that encodes input data into a shared latent space, which is then used for fast autoregressive decoding [54].\nAlthough recent learning-based approaches have been applied successfully to various VRPs [27, 30, 31, 44], including rich and highly constrained variants [5, 6, 22, 32, 40, 74, 75], multi-agent [10, 13, 14, 48, 52, 70-72, 77] and heterogenous agent (i.e., fleet) VRPs [4, 34, 42], such approaches cannot model heterogeneous agents on a per-client basis, in which each agent has a different internal representation for each node.\nIn this paper, we propose a novel learning approach, the Collaborative Attention Model with Profiles (CAMP), designed to address PVRP using multiagent reinforcement learning (MARL). Our approach builds upon the attention-based encoder-decoder framework [30], integrating vehicle and client profiles into a collaborative decision-making architecture. Unlike previous methods, CAMP leverages a specialized attention-based communication encoder to embed profiled client representations in parallel for each vehicle. During the decoding phase, we employ a specialized communication layer between agents to enable cooperative decisions even among a heterogeneous profiled latent space. Finally, a parallel pointer mechanism [4] is utilized to attend to these profiled embeddings, allowing the model to efficiently evaluate possible next actions for each vehicle based on its profile.\nWe evaluate CAMP on the PVRP with Preferences (PVRP-P), where various distributions of vehicle profiles explicitly affect the reward function, and PVRP with Zone Constraints (PVRP-ZC), a scenario that imposes zone-based operational constraints on vehicles. Our experiments demonstrate that CAMP achieves competitive results in terms of solution quality and computational efficiency compared to classical methods and modern neural multi-agent models, showcasing CAMP as a valuable tool for research and practitioners in solving complex routing problems in real-time."}, {"title": "2 RELATED WORK", "content": "Recent advancements in neural combinatorial optimization (NCO) have introduced promising end-to-end solutions for vehicle routing problems (VRP) [3, 66]. Learning-based VRP approaches can broadly be divided into construction [7, 11, 12, 15, 17, 20, 23, 24, 27, 30, 31, 36, 39, 44, 64, 76] and improvement/search methods [18, 26, 45, 46, 53, 55, 59, 68, 69]. Construction methods learn to generate a solution, while improvement/search methods iteratively refine them. We focus on learning construction methods due to their lower reliance on handcrafted heuristics and faster inference speed. Most constructive methods derive from the seminal work of Vinyals et al. [61], refined by Bello et al. [2] with deep reinforcement learning, and improved by Kool et al. [30] using transformers in the widely adopted Attention Model (AM). More practical multi-agent VRPs have been formulated by Son et al. [52]and Zheng et al. [71] as sequential decision-making, while Zhang et al. [70] and Zong et al. [77] and Liu et al. [42] employ simultaneous solution construction. For the heterogeneous VRP (HCVRP), approaches have evolved from Vera and Abad [58]'s policy network with A2C method to Qin et al. [51]'s reinforcement learning-enhanced heuristics and Li et al. [33]'s two-decoder framework emphasizing vehicle and node selection. However, these methods often lack fleet generalization due to fixed vehicle number assumptions, and while they attempt simultaneous solutions for various agents, they typically sequentially limit fast parallel decoding and collaboration. PARCO [4] addresses these limitations with a flexible approach using a general communication framework, making it effective for multi-agent and heterogeneous VRP scenarios, enhancing solutions by allowing for dynamic agent counts and improved conflict resolution strategies, representing a significant advancement in addressing the complexities of HCVRPS within the NCO framework. However, no neural approach has yet been proposed to tackle the more practical PVRP."}, {"title": "3 PROBLEM FORMULATION", "content": "Consider a set of nodes $N = \\{0, 1, ..., n\\}$, where node 0 represents the depot and nodes $\\{1, ..., n\\}$ represent clients. Let $C = N \\setminus \\{0\\}$ denote the set of client nodes. The set of vehicles is represented by $K = \\{1, ..., m\\}$. Each node $i \\in N$ is characterized by its location $s_i \\in \\mathbb{R}^2$ and demand $d_i$ (with $d_i = 0$ for the depot). Each vehicle $k \\in K$ is defined by its capacity $Q_k$, speed $s_k$, and a profile parameter $p_k = (p_{1k},..., p_{nk})$, where $p_{ik}$ represents the profile parameter for vehicle $k$ serving client $i$. Let $r$ denote index for delivery trips, $r \\in R$, where $R$ is the maximum number of trips for each vehicle.\nLet $c_{ij}$ denote the travel cost from node $i$ to node $j$, typically representing the Euclidean distance between the nodes. The decision variable is a boolean $x_{ijk}^r$ that equals 1 if vehicle $k$ travels directly from node $i$ to node $j$ in trip $r$, and 0 otherwise. We also introduce three auxiliary variables to aid in problem formulation and solution: $y_{ik}^r$ is a binary variable that equals 1 if client $i$ is served by vehicle $k$ in trip $r$, and 0 otherwise. $z_k$ is a binary variable representing the vehicle $k$ is selected or not. Let $w_i$ be a continuous variable used to avoid subtours.\nThe PVRP can thus be formulated as follows:\n$\\min \\sum_{k \\in K} \\sum_{i \\in N} \\sum_{j \\in N} \\sum_{r \\in R} (\\frac{c_{ij}}{s_k} - \\alpha p_{ik}) x_{ijk}^r$\nsubject to the following constraints:\n$\\sum_{k \\in K} \\sum_{r \\in R} y_{ik}^r = 1, \\quad \\forall i \\in C$ (2)\n$\\sum_{j \\in C} x_{0jk}^r \\leq z_k, \\quad \\forall k \\in K, r \\in R$ (3)\n$\\sum_{j \\in N} x_{ijk}^r \\leq y_{ik}^r, \\quad \\forall i \\in N, k \\in K, r \\in R$ (4)\n$x_{iik}^r = 0, \\quad \\forall k \\in K, r \\in R$ (5)\n$\\sum_{h \\in N} x_{hik}^r - \\sum_{j \\in N} x_{ijk}^r = 0, \\quad \\forall i \\in N, k \\in K, r \\in R$ (6)\n$\\sum_{j \\in C} x_{0jk}^r = \\sum_{j \\in C} x_{ik0}^r, \\quad \\forall k \\in K, r \\in R$ (7)\n$\\sum_{i \\in C} \\sum_{j \\in C} d_j x_{ijk}^r \\leq Q_k, \\quad \\forall k \\in K, r \\in R$ (8)\n$w_j \\geq w_i + 1 - N(1 - x_{ijk}^r), \\quad \\forall i \\in C, j \\in C, i \\neq j, r \\in R$ (9)\n$\\sum_{j \\in C} x_{0jk}^r \\geq \\sum_{j \\in C} x_{ijk}^r, \\quad \\forall k \\in K, r \\leq R-1$ (10)\n$x_{ijk}^r \\leq y_{ik}^r \\cdot z_k, \\quad \\forall i, j \\in N, k \\in K, r \\in R$ (11)\nThe objective function Eq. (1) minimizes the total adjusted travel cost while maximizing preference scores, with $\\alpha$ as a weight parameter to balance these two factors. Constraint (2) ensures that each client is visited exactly once. Constraint (3) counts utilized vehicle while leaving from depot. Constraint (4) links the route variables ($x_{ijk}^r$) with the assignment variables ($y_{ik}^r$). Constraint (5) bans traveling in the same location. Constraint (6) maintains flow conservation for each vehicle, while constraint (7) guarantees that each vehicle forces that if a vehicle leaves the depot, it must return to the depot. Constraint (8) ensures that vehicle capacities are respected. Constraint (9) eliminates subtours. Constraints (10) establishes trip sequences. Constraints (11) define the binary for the decision variables."}, {"title": "3.1.1 PVRP with Preferences", "content": "The PVRP-P defines the profile parameter $p_{ik}$ as a preference score. The objective function Eq. (1) can be simply rewritten as follows:\n$\\max \\sum_{k \\in K} \\sum_{i \\in N} \\sum_{j \\in N} (\\alpha p_{ik} - \\frac{c_{ij}}{s_k}) x_{ijk}^r$ (12)\ni.e., maximize the preferences while minimizing the total tour duration with a multi-objective balancing parameter $\\alpha$."}, {"title": "3.1.2 PVRP with Zone Constraints", "content": "The PVRP-ZC incorporates zone constraints, where certain vehicles may not serve specific clients. One way to model this problem is to introduce an additional constraint:\n$y_{ik}^r = 0, \\quad \\forall (i, k) \\text{ where } p_{ik} = 0.$ (13)\nIn practice, this constraint can be modeled by Eq. (1) by introducing a large negative value, effectively $-\\infty$, for $p_{ik}$ when vehicle $k$ is not allowed to serve client $i$:\n$p_{ik} = \\begin{cases} -\\infty & \\text{if vehicle k is not allowed to serve client i} \\\\ 0 & \\text{otherwise} \\end{cases}$ (14)\nand setting $\\alpha$ to 1. Using this definition of $p_{ik}$, the objective function Eq. (12) naturally enforces zone constraints. When $p_{ik} = -\\infty$, the term $-\\alpha p_{ik}$ becomes $\\infty$, ensuring that the corresponding $x_{ijk}^r$ variables are set to 0 in any optimal solution, effectively preventing vehicle $k$ from serving client $i$."}, {"title": "4 METHODOLOGY", "content": "We reformulate the Profiled Vehicle Routing Problem (PVRP) as a Markov Decision Process (MDP) defined by the 4-tuple $M = \\{S, A, \\tau, r\\}$, where S is the state space, A is the action space, $\\tau$ is the state transition function, and r is the reward function.\nState Space S. Each state $s_t = (V_t, X_t) \\in S$ consists of two parts:\n1. Vehicle state $V_t = \\{\\nu_1, \\nu_2, ..., \\nu_m\\}$, where each $\\nu_k = (o_t^k, \\tau_k, G_k, p_k)$ represents:\n$\\forall$ $o_t^k$: Remaining capacity of vehicle $k$ at step $t$\n$\\forall$ $\\tau_k$: Accumulated travel time of vehicle $k$ at step $t$\n$\\forall$ $G_k = \\{g_0^k, g_1^k,..., g_{n_t}^k\\}$: Partial route of vehicle $k$ at step $t$\n$\\forall$ $p_k = (p_{1k},..., p_{nk})$: Profile parameter of vehicle $k$\nThe interpretation of $p_k$ differs between PVRP-P and PVRP-ZC:\n$\\forall$ For PVRP-P: $p_{ik}$ represents the preference score for vehicle $k$ serving node $i$\n$\\forall$ For PVRP-ZC: $p_{ik}$ is binary, where 1 indicates vehicle $k$ is allowed to serve node $i$, and 0 indicates it is not allowed\n2. Node state $X_t = \\{x_1^t, x_2^t,..., x_n^t\\}$, where each $x_i^t = (s_i, d_i)$ represents:\n$\\forall$ $s_i$: 2D vector representing the location of node $i$\n$\\forall$ $d_i$: Remaining demand of node $i$ at step $t$\nAction Space A. Without loss of generality, we consider an action $a_t \\in A$ is defined as selecting a vehicle and a node to visit. Specifically, $a_t = (\\nu, x)$, where vehicle $k$ is selected to visit node $j$ at step $t$.\nState Transition Function $\\tau$. The transition function $\\tau$ updates the state based on the chosen action: $s_{t+1} = (V_{t+1}, X_{t+1}) = \\tau(V_t, X_t, a_t)$. For PVRP-P, the elements are updated as follows:\n$o_{t+1}^k = \\begin{cases} o_t^k - d_j, & \\text{if } k \\text{ is the selected vehicle} \\\\ o_t^k, & \\text{otherwise} \\end{cases}$ (15)\n$\\tau_{t+1}^k = \\begin{cases} \\tau_k + \\frac{c_{gj}}{s_k}, & \\text{if } k \\text{ is the selected vehicle} \\\\ \\tau_k, & \\text{otherwise} \\end{cases}$ (16)\n$G_{t+1}^k = \\begin{cases} G_k, & \\text{if } k \\text{ is the selected vehicle} \\\\ [G_k, j], & \\text{otherwise} \\end{cases}$ (17)\n$d_{i}^{t+1} = \\begin{cases} 0, & \\text{if } i \\text{ is the selected node} \\\\ d_i^t, & \\text{otherwise} \\end{cases}$ (18)\nFor PVRP-ZC, we modify the transition function to incorporate zone constraints:\n$\\tau^{ZC}(V_t, X_t, a_t) = \\begin{cases} (V_{t+1}, X_{t+1}), & \\text{if } p_{jk} = 1 \\\\ (V_t, X_t), & \\text{if } p_{jk} = 0 \\end{cases}$ (19)\ni.e., actions for all vehicle $k$ which would lead $p_{jk} = 0$ are directly masked in the environment.\nWhere $(V_{t+1}, X_{t+1})$ is calculated using the same update rules as in PVRP-P. This ensures that the state remains unchanged if an invalid action is attempted."}, {"title": "4.1.1 Optimization Objective", "content": "The goal is to find the optimal policy parameters $\\theta^*$ that maximize the expected cumulative reward. Formally, we aim to solve:\n$\\theta^* = \\arg \\max_\\theta \\mathbb{E}_{\\tau \\sim \\pi_\\theta} [\\sum_{t=0}^{T} r_t]$ (23)"}, {"title": "4.1.2 Construction Methods", "content": "We note that there are several ways to construct solutions, starting from the formulated MDP, which takes into account the most general case. In particular, we identify the following cases:\n(1) Autoregressive Sequential: such construction method considers a single vehicle and one action at a time; a vehicle is switched only when its route is complete [52, 71].\n(2) Autoregressive Alternating: similar to the above but more flexible: in this case, the vehicle can be switched at any time [34, 42].\n(3) Parallel Autoregressive: in this case, the steps are performed for all agents in parallel simultaneously, reducing the total $T$, which makes optimization and inference faster [4].\nWithout loss of generality, we adopt the Parallel Autoregressive approach in CAMP because of its speed and flexibility."}, {"title": "4.2 Collaborative Attention Model with Profiles", "content": "Fig. 2 shows an illustration of CAMP model. We first provide an overview of the overall solution construction (Section 4.2.1), then of the encoder (Section 4.2.2) and decoder (Section 4.2.3).\nWe formulate the solution construction of CAMP in a parallel autoregressive approach [4]. We define the solution construction $\\alpha$ given instance $x$ as follows:\n$h = f(x)$ (24)\n$\\pi_{\\theta}(a|x) = \\prod_{t=0}^{T} \\prod_{k=1}^{m} g_{\\theta}(a_{k}^{t}|a^{t-1},..., a_k^0, h)$ (25)\nwhere $f_\\theta(x)$ is the encoder $f$ mapping $x$ to its (vehicle-specific) latent embeddings $h$, $g_\\theta(x)$ is the autoregressive decoder, and $\\pi_\\theta$ represents the full CAMP encoder-decoder model mapping $x$ to $\\alpha$."}, {"title": "4.2.2 Encoder.", "content": "The encoder in CAMP is designed to handle multiple vehicle profiles, each with distinct embeddings. Unlike previous approaches that utilize a single shared embedding for all vehicles [4, 52, 71], CAMP encodes each vehicle profile individually, generating profile-specific embeddings. The encoding process involves three key steps: (1) obtaining initial embeddings, (2) applying attention to compute vehicle-specific profile embeddings, and (3) integrating these profile embeddings using bipartite graph message passing.\nFirst, the encoder maps the raw features of the graph instance into an embedding space. Specifically, it transforms the feature vectors of vehicles $x^v$, clients $x^c$, and vehicle preference profiles $p_{ij}$ into their respective embedding spaces using the learned embedding matrices $W_{init}^v$, $W_{init}^c$, $W_{init}^p$ and $W_{init}^{combine}$. The resulting embeddings are computed as follows:\n$\\begin{aligned}\nh_i^v &= W_{init}^v x^v \\\\\nh_j^c &= W_{init}^c x^c \\\\\nh_{ij}^p &= W_{init}^p p_{ij}\n\\end{aligned}$\nNext, the vehicle, client, and preference embeddings are concatenated to form a combined profile embedding, integrating both node and edge features in the graph:\n$h_{ij} = W_{combine} \\cdot \\text{concat}(h_i^v, h_j^c, h_{ij}^p)$\nThis profile embedding $h_{ij}$, which captures the interaction between a vehicle and clients, is processed using multi-head attention (MHA) to incorporate profile-specific information. The overall profile embedding $h$ is constructed by concatenating the embeddings of all vehicle-client pairs: $h = \\text{concat}(h_1, h_2, ..., h_m)$. MHA is then applied separately to each profile:\n$h' = \\text{MHA}(h_i, h_i, h_i)$ (26)\nwhere MHA represents the multi-head attention defined as:\n$MHA(Q, K, V) = ||_{i=1}^{\\text{nheads}} \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) W^O$\nwith\n$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\nwhere $||$ denotes concatenation; $W^O \\in \\mathbb{R}^{d_k \\times d_h}$ is used to combine the outputs from different attention heads where $d_k = d_h / \\text{nheads}$ represents the dimension per heads; and $W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d_h \\times d_k}$ are the learnable parameter matrices for queries Q, keys K, and values V.\nAfter obtaining the vehicle-specific profile embeddings, the encoder in CAMP employs a bipartite graph message passing framework, which is crucial for integrating information across individual vehicle profile embeddings. By facilitating the exchange of information between vehicle and client nodes, this framework allows each vehicle-specific profile embedding to benefit from the broader context, enabling a richer, more interconnected representation. The bipartite graph structure enables two-directional message passing:\n$\\forall$ Vehicle-to-Client Update: The vehicle embeddings are propagated to the client nodes they are connected to, aggregating vehicle-specific information to update customer embeddings:\n$h'_j = \\Phi(h_j, h_i, h_{ij})$\n$\\forall$ Client-to-Vehicle Update: The customer embeddings are subsequently propagated back to the vehicles, allowing vehicles to gather information from multiple customers:\nh'_i = $\\Phi(h_i, h_j, h_{ji})$\nThis bidirectional message passing ($\\Phi$) enables information to flow across all vehicle-customer pairs, allowing vehicle-specific profile embeddings to interact and integrate with each other.\nResidual connections are employed to maintain stability in the learning process, which add the initial node embeddings to the updated embeddings:\n$h_i = h'_i + h_i, h_j = h'_j + h_j$\nFinally, the edge embeddings are refined by combining the updated vehicle and customer embeddings, ensuring that the edge representations reflect the fully integrated node information:\n$h_{ij} = \\text{concat}(h_i, h_j) + h_{ij}$\nThe aggregated information is then passed through multiple transformer-style layers [57] with bidirectional message passing similarly to Eq. (28). The final output of the encoder is a set of embeddings $h = [h^1, ..., h^m]$, where each $h^k \\in \\mathbb{R}^{(m+n)\\times d_h}$ represents the encoded graph of vehicles and clients for vehicle profile $k$. This approach allows CAMP to capture profile-specific information and relationships between vehicle and client, which is crucial for solving the PVRP."}, {"title": "4.2.3 Decoder.", "content": "The decoder transforms multi-profile embeddings h into a probability distribution for action selection. We calculate the m queries $Q_t$ for the multiple pointer mechanism [4] as follows:\n$Q_t = W_{proj} (\\text{concat}(h_i, h_i, W_{context}x_t))$ (27)\nwhere $k$ is the vehicle index, $i$ is the node index, $x_t$ are context features at the current step $t$; $W_{context}$ is a learnable parameter matrix to project the context features while $W_{proj}$ is a learnable parameter matrix to project back to hidden space $d_h$ the concatenated static embeddings and dynamic features. Then, we apply a communication layer using a transformer block to capture intra-vehicle dynamic relationships:\n$H' = \\text{Norm}(\\text{MHA}(Q_t, Q_t, Q_t) + Q_t)$\n$Q = \\text{Norm}(\\text{FFN}(H') + H')$ (28)\nwhere Norm denotes a normalization layer, and FFN represents the multi-layer perceptron. The self-attention in the MHA layer enables message passing between vehicles based on their current embeddings.\nAfter this communication step, we perform cross-attention between the updated vehicle queries and the profile-specific node embeddings. Thus:\n$\\text{MHA}(QW, hW, hW)$ (29)\nwhere $h = [h^1, ..., h^m]$ are the profile-specific node embeddings from the encoder. This formulation allows the attention mechanism to consider the profile-specific encoded information for each vehicle.\nThe outputs of these attention heads are then combined to form $U \\in \\mathbb{R}^{m \\times d_h}$, which is used in our Multiple Pointer Mechanism:\n$Z = C \\cdot \\text{tanh}(\\frac{UL^T}{\\sqrt{d_k}})$ (30)\nHere, L is a projection of the node embeddings $h$, and C is a scale parameter to control the entropy of Z (C = 10 in our work according to Bello et al. [2]). The resulting $Z \\in \\mathbb{R}^{m \\times N}$ contains logits for each vehicle-node pair. Finally, the logit vector Z of Eq. (30) is masked based on the solution construction feasibility. The probability of selecting action i for all vehicles is given by:\n$P_i = \\frac{e^{Z_i}}{\\sum e^{Z_i}}$ (31)\nallowing us to sample a vector A containing m actions for each vehicle in parallel. If a conflict arises between actions, i.e., two or more vehicles select an action of index i, we prioritize the vehicle with the largest action probability $P_i$ and set the action of all other conflicting vehicles equivalent to their current position as in Berto et al. [4]."}, {"title": "4.3 Training", "content": "CAMP is a centralized multi-agent parallel decision-making model, with a shared policy $\\pi_\\theta$ for all agents and a global reward R. As such, CAMP can be trained using any of the training algorithms from single-agent NCO literature. We train CAMP using the REINFORCE gradient estimator [62] with a shared baseline [27, 31]:\n$\\nabla_{\\theta} J(\\theta) = \\frac{1}{B} \\sum_{i=1}^{B} \\sum_{j=1}^{|L|} G_{ij} \\nabla_{\\theta} \\log p_{\\theta} (A_{ij} | x_i)$ (32)\nHere, B is the mini-batch size and $G_{ij}$ is the advantage $R(A_{ij}, x_i) - b_{shared}(x_i)$ of a solution $A_{ij}$ w.r.t. to the shared baseline $b_{shared}(x_i)$ of problem instance $x_i$, in our case obtained through symmetric transformations [27].\nFor PVRP-P, different preference distributions can lead to rewards of varying scales in the reward function. To mitigate potential biases during learning, we propose applying reward balancing across these distributions, which has been successfully applied in multi-task routing problems [5]. We employ reward balancing techniques to calculate the normalized reward $R_{norm}^t$ for all preference distributions $k \\in [1, ..., K]$ at each training step $t \\geq 1$. This normalization is achieved by dividing by the exponentially smoothed mean. We calculate the average reward $\\bar{R}(k)$ up to training step $t$, starting from the average reward $R^{(k)}_1 = R^{(k)}$ at step t. For the exponential moving average, we set $R^{(k)} = R^{(k)}$ and compute subsequent values for t > 1 based on [21], using a smoothing factor $\\beta$:\n$\\bar{R}^{(k)}_t = (1 - \\beta) \\cdot \\bar{R}^{(k)}_{t-1} + \\beta \\cdot R^{(k)}, 0 < \\beta < 1,$ (33)\nWe then calculate the normalized reward as $R^{(k)}_{norm} = R^{(k)} / | \\bar{R}^{(k)} |$ to ensure fairness in reward distribution."}, {"title": "5 EXPERIMENTS", "content": "We observe that the latter successfully equilibrates the rewards across different preference settings, making it easier for the model to adapt and thereby enhancing performance in managing preferences."}, {"title": "6 CONCLUSION", "content": "In this work, we introduced a formulation for the Profiled Vehicle Routing Problem (PVRP), an extension of the Heterogeneous Capacitated Vehicle Routing Problem (HCVRP) that incorporates client-specific preferences and operational constraints. To tackle this complex problem, we proposed the Collaborative Attention Model with Profiles (CAMP), a novel multi-agent reinforcement learning (MARL) approach that leverages an attention-based encoder-decoder framework with agent communication to enable collaborative decision-making among heterogeneous vehicles with different profiles for each client. Our extensive evaluations on both synthetic, across two types of PVRP variants--PVRP with Preferences (PVRP-P) and PVRP with Zone Constraints (PVRP-ZC)--show that CAMP consistently delivers competitive performance in terms of solution quality and computational efficiency, outperforming traditional heuristics and other neural-based methods. These results position CAMP as a powerful tool for solving complex routing problems in dynamic, real-time settings.\nCAMP represents an early attempt at solving the PVRP. While it shows promising results in solution time and performance - including outperforming OR-Tools - it can be improved in several ways to beat the final performance of SOTA heuristic HGS. Promising future works include integrating end-to-end construction and improvement methods [29], learning to guide (local) search algorithms [19, 35, 65], multi-objective learning at different preference values $\\alpha$ [9, 38], extending recent foundation models for VRPs [5, 32] with CAMP's agentic representations including agent communication and heterogenous learned representations, and obtaining better heuristics for resolving decoding conflicts which could be achieved by automated LLM algorithmic discovery [41, 50, 67]."}, {"title": "5.3 Ablation Studies", "content": "Table 2 Illustrates the main ablation studies for the CAMP components on PVRP-P with instance size N = 100. We compare the performance of the full CAMP model against its variants with specific components ablated one at a time.\nEncoder Communication. This ablation (CAMP-EC in Table 1) underpins the effectiveness of encoder communication due to the bipartite graph, which significantly enhances the representation capability of CAMP in capturing the preference relationships, leading to improved optimization of the preference gap.\nBalanced Reward Training. Comparing CAMP without Encoder Communication and Balanced Reward, we observe that the latter successfully equilibrates the rewards across different preference settings, making it easier for the model to adapt and thereby enhancing performance in managing preferences.\nVehicle-specific Profile Embedding. This setting removes all our contributed components in CAMP and corresponds to PARCO [4]. This overall performs worst, although still better than other baselines such as the sequential autoregressive ones."}, {"title": "4.1 Modeling PVRP with MARL", "content": "4.1 Modeling PVRP with MARL"}]}