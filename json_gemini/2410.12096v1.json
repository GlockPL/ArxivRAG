{"title": "Bridging Large Language Models and Graph Structure Learning Models for Robust Representation Learning", "authors": ["Guangxin Su", "Yifan Zhu", "Wenjie Zhang", "Hanchen Wang", "Ying Zhang"], "abstract": "Graph representation learning, involving both node features and graph structures, is crucial for real-world applications but often encounters pervasive noise. State-of-the-art methods typically address noise by focusing separately on node features with large language models (LLMs) and on graph structures with graph structure learning models (GSLMs). In this paper, we introduce LangGSL, a robust framework that integrates the complementary strengths of pre-trained language models and GSLMs to jointly enhance both node feature and graph structure learning. In LangGSL, we first leverage LLMs to filter noise in the raw data and extract valuable cleaned information as features, enhancing the synergy of downstream models. During the mutual learning phase in LangGSL, the core idea is to leverage the relatively small language model (LM) to process local attributes and generate reliable pseudo-labels and informative node embeddings, which are then integrated into the GSLM's prediction phase. This approach enriches the global context and enhances overall performance. Meanwhile, GSLM refines the evolving graph structure constructed from the LM's output, offering updated labels back to the LM as additional guidance, thus facilitating a more effective mutual learning process. The LM and GSLM work synergistically, complementing each other's strengths and offsetting weaknesses within a variational information-maximizing framework, resulting in enhanced node features and a more robust graph structure. Extensive experiments on diverse graph datasets of varying scales and across different task scenarios demonstrate the scalability and effectiveness of the proposed approach.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph Neural Networks (GNNs) [16, 26, 45, 50, 55] have emerged as the de facto method for modeling graph-structured data, which is increasingly prevalent and essential in numerous real-world applications. Mainstream GNN methods implicitly propagate, aggregate, and preserve both node features and the graph structure, whose mutual influence is crucial for achieving optimal performance. However, the noise and shallowness of node features, along with frequently suboptimal node connections, both stemming from the complexity of graph formation, present significant challenges to achieving accurate and robust graph modeling. These challenges become particularly notable in various real-world applications. For instance, in social networks and citation networks [39, 47], the graph structure is often noisy or incomplete due to the presence of spurious connections, missing links, or dynamic changes in graph structures over time. In some cases, such as biological interaction networks [43, 54, 57], the graph structure may even be entirely unobserved, necessitating its inference from data. Furthermore, the node embeddings derived from native data are often noisy [35, 58, 67], suffering from issues such as unstructured formatting, and variations across different platforms.\nRecent efforts have increasingly focused on enhancing graph representation learning by leveraging large language models (LLMs), particularly when dealing with diverse modalities of data associated with graph nodes [3, 31, 41, 44, 61]. In this work, we primarily focus on graph representation learning on text-attributed graphs (TAGs) [51], where nodes are associated with text entities.\nMotivation 1: Combining deep text embeddings generated by LLMs with GNNs. Building on the foundations of LLMs, existing works on TAGs [6, 19, 23, 24, 62] have demonstrated the effectiveness in node classification task. On the one hand, LLMs (e.g., GPT-4 [1]) possess advanced language processing capabilities, including complex reasoning and zero-shot learning. Several works"}, {"title": "2 RELATED WORKS", "content": "2.1 Graph Structure Learning\nThe problem of graph structure learning has been widely studied in the context of GNN from different perspectives. The majority of the graph structure learning methods still adopt a co-training approach [5, 10, 12, 25, 49, 53, 59], where the graph structure is optimized jointly with the entire neural network. For instance, IDGL [5] jointly and iteratively learns the graph structure and graph embeddings, using the associated learning metric as the optimization objective. RCL [59] first extracts latent node embeddings using a GNN model based on the current training structure, then jointly learns the node prediction labels and reconstructs the input structure through a decoder. An alternative approach to graph structure learning involves using a dedicated module to refine the graph structure, which is subsequently passed to downstream GNNs for further processing [32, 46, 68]. Within the category, SEGSL [68] introduces graph structural entropy, optimizing the structure using an one-dimensional entropy maximization strategy, constructing an encoding tree to capture hierarchical information, and subsequently reconstructing the graph from this tree, which is then incorporated into a GNN. Iterative methods [42, 46] train the two components in cycles, learning the graph structure from predictions or representations produced by an optimized GNN, which is then used to train a new GNN model in the next iteration. While existing graph"}, {"title": "2.2 Large Language Models for Graphs", "content": "Leveraging the extensive knowledge of large language models (LLMs) for graph data enhancement has been a focus of recent studies, with several exploring the integration of LLMs and Graph Neural Networks (GNNs). One category (LLMs as enhancers) utilizes LLMs for node feature augmentation [14, 19, 22, 31] while relying on the standard GNN mechanisms to update node features, incorporating the underlying graph topological information for better performance. As an example, GIANT [7] integrates graph topology information into language models (LMs) through the XR-Transformer [56], and the enhanced node embeddings can be further used in various downstream tasks. Alternatively, efforts to represent graphs linguistically and treat LLMs as predictors [4, 23, 52, 64] risk losing structural information, as these methods may struggle to effectively translate graph structures into natural language. To harness the complementary strengths of LLMs and GNNs, GNN-LLM alignment approaches [24, 63] utilize cross-training between the two models, with alignment strategies designed to ensure both effectively capture graph topology and enhance node embeddings. In contrast to LangGSL, methods like GLEM [63], a representative alignment approach, focus solely on optimizing the model with noisy node embeddings, neglecting explicit graph structure learning, thereby limiting the potential performance and scope of downstream tasks. Considering graph structure learning, GraphEdit [15] leverages an large language model (LLM) to model node relationships based on paired node text attributes. However, compared to LLM-based methods focused on single-node feature augmentation, GraphEdit exhibits increased computational complexity as the number of nodes grows, with only marginal gains in performance. LLM4RGNN [60] employs LLMs to defend against adversarial attacks, but this strategy incurs a trade-off, leading to a slight reduction in overall accuracy. In contrast, LangGSL delivers superior performance across kinds of downstream tasks, while simultaneously maintaining a robust graph structure."}, {"title": "3 METHOD", "content": "In this section, we outline the pipeline of LangGSL, as illustrated in Figure 2, specifically tailored for the node classification task, detailing each stage from data cleaning to models' mutual learning.\n3.1 Preliminaries\nTask definition. Formally, a text-attributed graph (TAG) can be represented as $G = (V, A, \\{s_n\\}_{n\\in V})$, where $V$ is a set of $n$ nodes, nodes are initially connected via adjacent matrix $A \\in \\mathbb{R}^{n\\times n}$, and $s_n \\in \\mathcal{D}^{L_n}$ is a sequential text associated with node $n \\in V$, with $\\mathcal{D}$ as the words or tokens dictionary, and $L_n$ as the sequence length. Given a noisy graph input $G$, or in the case that only node tokens $s_n \\in \\mathcal{D}^{L_n}$ are provided, where the initialization of $A$ will depend on the chosen strategy (e.g., kNN [11]), the deep graph learning problem addressed in this paper is to produce an optimized graph:\n$G^* : (A^*, X^*) = \\psi(G)$,\n(1)"}, {"title": "3.2 Data Cleaning based on LLMs", "content": "Given the substantial cost of obtaining human-annotated texts, most approaches [7, 24, 63] rely on raw text data scraped from the web in TAGs learning, which is frequently contaminated with noise, including unstructured formatting, multilingual content, task-irrelevant information, and non-standardized formats, all of which can hinder downstream tasks and model performance. Despite employing simple prediction and explanation-based filters [19, 23, 31], noise remains prevalent in the texts.\nTo effectively mitigate the impact of noise in raw text data and extract concise, task-specific inputs for downstream LM, we design prompts that balance generality and specificity: broad enough to capture essential task-relevant indicators while being sufficiently targeted to filter out irrelevant or noisy information (Figure 2a). Unlike prior work, our method first instructs the LLM to summarize input text, including symbols like emojis, before performing classification and explanation tasks. This preprocessing step enables the LLM to handle discrete and symbolic language elements more effectively. Furthermore, we observed that LLMs tend to struggle with borderline classification cases, where the raw text is ambiguous. To address this issue, we provide key factors as additional instructions within the prompt, helping to guide the LLM towards more accurate"}, {"title": "3.3 Mutual Learning between LM and GSLM", "content": "To leverage the complementary strengths of LM and GSLM, we jointly optimize the ELBO (Equation (3)) and graph regularization through their interaction, enhancing both feature representation and structural learning (Figure 2b). Specifically, the mutual learning involves two phases: (1) initializing the graph structure and optimizing variational distribution q by LM, and (2) refining the graph structure within GSLM and optimizing the posterior distribution p by GSLM, with GNN as the default backbone.\n3.3.1 LM Learning Phase. Leveraging the complex linguistic capabilities of LLMs and task-specific prompts, we obtain cleaned and task-relevant textual output $s_n$ for node $n \\in V$. These text entities are then processed by a pre-trained LM, denoted as $q_\\theta$ (e.g., Sent-BERT [40]), which serves as an expert to refine the text attributes $s_n$, generating and providing informative embeddings for GSLM. The resulting text embeddings are obtained as follows:\n$h_n = q_\\theta(s_n) \\in \\mathbb{R}^d$.\n(4)\nInitial graph structure formation. Based on the inferred embeddings, the likelihood of a link between any two nodes can be quantified using a similarity measure function $\\text{Sim}(.)$ (e.g., cosine similarity) between their respective embeddings $h_i$ and $h_v$, which is as follows:\n$f_{adj}(h_i, h_v) = \\text{Sim}(h_i, h_v)$.\n(5)\nSpecifically, in each iteration, the graph structure is updated dynamically based on $\\text{Sim}(.)$ as the node embeddings are refined. Alternatively, a cross-attention layer before the classification head can be employed to explicitly model the interactions between nodes, where attention weights serve as an adaptive mechanism to capture the strength and relevance of relationships in a learnable way.\nOptimization of distribution q. Given the informative embeddings $h_n$, and the learned initial adjacency matrix $A$ based on the function $\\text{Sim}(.)$ (in cases where the initial graph structure $A$ is not available), the true posterior distribution can be represented as $P(Y_U | \\{h_n\\}_{n\\in V}, A, Y_L)$.\nIn the LM learning phase of LangGSL, we will update the parameters of the LM $q_\\theta$, while keeping the GSLM parameters $\\phi$ fixed. Based on Equation (3), the goal of LangGSL is to refine the variational distribution $q_\\theta (Y_U | S_U)$ so that it better approximates the true posterior distribution $p_\\phi (Y_U | \\{h_n\\}_{n\\in V}, A, Y_L)$ via minimizing the Kullback-Leibler (KL) divergence between $q_\\theta$ and $p_\\phi$. By doing so, we enable the LM to capture global semantic correlations distilled from the GSLM.\nHowever, directly minimizing the KL divergence between $q_\\theta$ and the true posterior is intractable due to the complexity of computing the entropy of the variational distribution. Inspired by the \"wake-sleep\" algorithm [20], we instead opt to minimize the reverse KL"}, {"title": "3.3.2 GSLM Learning Phase", "content": "divergence, which simplifies the optimization process. Specifically, the optimization problem in the LM learning phase becomes:\n$\\min_\\theta KL (p(Y_U | \\{h_n\\}_{n\\in V}, A, Y_L), q_\\theta(Y_U | h_v))$ = $\\max_\\theta \\mathbb{E}_{p_\\phi (y_U|\\{h_n\\}_{n\\in V},A,y_L)} [\\log q_\\theta(y_u | h_u)]$.\n(6)\nas minimizing the KL divergence is equivalent to maximizing the expected log-likelihood of the LM under the posterior distribution estimated by the GSLM. In the LM, we assume that the labels of different nodes are determined solely by their respective text attributes, using a mean-field approximation [13] to capture this independence. This assumption leads to the following factorization, where the expectation is expressed as a sum over the unlabeled nodes:\n$\\max_\\theta \\mathbb{E}_{p_\\phi (y_U|\\{h_n\\}_{n\\in V},A,y_L)} [\\log q_\\theta (y_m | h_m)]$.\n(7)\n$m\\in U$\nHere, directly computing the posterior distribution $p_\\phi(y_m | \\{h_n\\}_{n\\in V}, A, Y_L)$, conditioning solely on observed node labels $Y_L$ is insufficient, as unobserved labels from neighboring nodes are critical for capturing the full graph structure in GSLM. To approximate this distribution, we utilize the current LM to generate pseudo-labels for the unlabeled nodes $U \\\\ \\{m\\}$, and we can approximate the posterior distribution as:\n$p_\\phi (y_m | \\{h_n\\}_{n\\in V}, A, Y_L, \\hat{Y}_{U\\\\\\{m\\}})$, (8)\nwhere $\\hat{Y}_{U\\\\\\{m\\}} = \\{\\hat{y}_{m'} \\}_{m'\\in U\\\\\\{m\\}}$ and each $\\hat{y}_{m'}$ is sampled from $q_\\theta(y_{m'} | h_{m'})$.\nIncluding the labeled nodes in the training process, the overall objective function for updating the LM becomes:\n$\\mathcal{L}_{LM} = \\alpha \\sum_{m\\in U} \\mathbb{E}_{p_\\phi (y_m|\\{h_n\\}_{n\\in V},A,Y_L,\\hat{Y}_{U\\\\\\{m\\}})} [\\log q_\\theta(y_m | h_m)]$\n$+ (1 - \\alpha) \\sum_{m\\in L} \\log q_\\theta (y_m | h_m)$,\n(9)\nwhere $\\alpha \\in [0, 1]$ is a hyperparameter that balances the two terms. The first term represents knowledge distillation from the GSLM to the LM, encouraging the LM to align its predictions with the global structural information captured by the GSLM. The second term is a supervised learning objective based on the labeled nodes. By optimizing this objective, the LM learns to produce label predictions that are consistent with both the textual content of each node and the structural patterns. Moreover, the graph structure inferred using Equation (5) exhibits greater robustness as evidenced by experiments.\nDuring the graph structure learning phase, our objectives are twofold: (1) to maximize the expected log joint likelihood of the observed and unobserved labels under the variational distribution $q(y_U | S_U)$ introduced in Equation (3), while keeping the parameters of the language model $q_\\theta$ fixed, and (2) to refine the input graph structure.\nWe have leveraged the LM to generate node representations $h_n$ for each node $n$ in the graph. These embeddings capture rich semantic information, forming the input features for GSLM. In the task-specific prediction phase, inspired by [2], the pseudo-likelihood"}, {"title": "Graph structure refinement", "content": "framework $\\mathbb{E}_{q(y_U|s_U)} [\\log p(Y_L, Y_U | \\{s_n\\}_{n\\in V}, A)]$, which we aim to optimize, can be approximated as:\n$\\max \\mathbb{E}_{q(y_U|s_U)} \\sum_{m\\in V} \\log p(y_m | \\{s_n\\}_{n\\in V}, A, Y_{V\\\\m})$\n(10)\nthe expectation over the variational distribution $q_\\theta$ depends on the unobserved labels of the unlabeled nodes. To approximate this expectation, we generate pseudo-labels $\\hat{y}_u$ by sampling from $q_\\theta(Y_U | S_U)$. For each unlabeled node $m\\in U$, the pseudo-label $\\hat{y}_m$ is drawn directly from the language model $q_\\theta$, facilitating efficient integration of semantic information into the graph learning process.\nWith the node representations $h_n$ and pseudo-labels $\\hat{y}_u$, the pseudo-likelihood can be approximated, leading to the following objective function for optimizing the parameters $\\phi$:\n$\\mathcal{L}(\\phi) = \\beta \\sum_{m\\in U} \\log p_\\phi (y_m | \\{h_n\\}_{n\\in V}, A, Y_L, \\hat{Y}_{U\\\\m})$\n$+ (1 - \\beta) \\sum_{m\\in L} \\log p_\\phi (y_m | \\{h_n\\}_{n\\in V}, A, Y_{L\\\\\\{m\\}}, Y_U)$,\n(11)\nwhere $\\beta\\in [0, 1]$ is a hyperparameter balancing the two terms. $Y_{U\\\\m}$ denotes the pseudo-labels of all unlabeled nodes except node $m$. $p_\\phi (y_m | .)$ represents the GSLM's predictive distribution for node $m$.\nWith the initial adjacent matrix $A$, we propose two strategies to refine the graph structure, depending on the specific needs of the task.\nStrategy 1: Since LangGSL operates within an iterative framework, LM and GSLM mutually improve each other's performance through maximizing the ELBO. Additionally, the graph structure used in the GSLM can be iteratively updated during the LM phase, leading to continuous refinement of both components. In the first category of graph structure refinement, the focus remains on optimizing $p$, where graph structure modifications are captured by implicit adjustments to the node features, ultimately influencing the final graph representations. A similar idea is presented by Fang et al. [9], who theoretically demonstrate that it is always possible to learn appropriate node features to perform any graph-level transformation, such as \"changing node features\" or \"adding or removing edges/subgraphs,\" through a learnable projection head.\nStrategy 2: In the second category of graph refinement methods, we extend our approach by incorporating a graph structure optimization objective. Specifically, we propose a joint optimization method to learn both the graph structure and GSLM parameters by minimizing a hybrid loss function, which combines task-specific loss with graph refinement loss in the GSLM learning phase, denoted as $\\mathcal{L}_{GSLM} = \\mathcal{L}(\\phi) + \\mathcal{L}_{G}$. Given the extensive success of existing graph structure learning works based on GNNs and the flexibility of LangGSL, we can seamlessly adapt the GSLM backbone as needed (e.g., IDGL [5]). The specific graph refinement loss $\\mathcal{L}_{G}$ utilized in LangGSL varies depending on the choice of the GSLM backbone architecture. Incorporating the general loss function that jointly optimizes both GNN parameters and the underlying graph structures of the chosen GSLM backbone, the final loss function for LangGSL during the graph structure learning phase is formalized"}, {"title": "4 EXPERIMENT", "content": "as follows:\n$\\mathcal{L}_{GSLM} = \\mathcal{L}(\\phi) + \\mathcal{L}_{G}$\n$= \\beta \\sum_{m\\in U} \\log p_\\phi (y_m | \\{h_n\\}_{n\\in V}, A, Y_L, \\hat{Y}_{U\\\\m})$\n$+ (1 - \\beta) \\sum_{m\\in L} \\log p_\\phi (y_m | \\{h_n\\}_{n\\in V}, A, Y_{L\\\\\\{m\\}}, Y_U) + \\mathcal{L}_{G}.$\n(12)\nAs demonstrated by the experimental results, the second category of graph structure refinement methods shows slightly better performance due to the additional consideration of the structure refinement loss. However, the first category methods provide a good balance between computational complexity and performance, making them suitable for scenarios where resource constraints are a concern. Further details are provided in Section 4.\nIn this section, we conduct experiments to evaluate the proposed LangGSL framework across various task scenarios, aiming to address several key research questions:\nRQ1: How does LangGSL perform compared to existing GSLMs in the transductive learning setting, utilizing the default graph structure?\nRQ2: How robustness is LangGSL in the absence of graph structure or when facing graph topology attacks?\nRQ3: How does LangGSL compare to GraphLLM models in node classification tasks on TAGs?\nRQ4: How adaptable and flexible is LangGSL across different settings of backbones?"}, {"title": "4.1 Experimental Setup", "content": "Datasets. Four graph node classification benchmark datasets are included in our experiment, including ogbn-arxiv [21], Instagram [23], Pubmed, and Cora [36].\nBaselines. We evaluate the performance of our LangGSL model against a comprehensive set of the state-of-the-art baselines, spanning four key categories:\n\u2022 Graph Structure Learning Models (GSLMs): The baseline GSLMs considered include: LDS [12], GRCN [53], ProGNN [25], IDGL [5], GEN [46], CoGSL [32], SUBLIME [34], SEGSL [68], STABLE [28], NodeFormer [49], RCL [59], GraphEdit [15], LLM4RGNN [60], SLAPS [10], HES-GSL [48], and HANG-quad [65].\n\u2022 Large Language Models for Graphs (GraphLLM): GraphLLM focuses on leveraging the vast knowledge of LLMs to enhance graph learning: LLM-as-enhancer (TAPE [19], GIANT [7], OFA [31]), LLM-as-predictor (GraphAdapter [23], LLaGA [4], GraphText [64], InstructGLM [52]), and LLM-as-aligner (GLEM [63], PATTON [24]).\n\u2022 Pre-trained Language Models (PLMs): The evaluation based on a diverse set of models, including BERT [8], De-BERTa [18], ROBERTa [33], Sent-BERT [40].\n\u2022 Graph Neural Networks (GNNs): The baselines include GCN [26], GAT [45], GraphSAGE [16], and RevGAT [27].\nNotably, we consider the transductive node classification task in both the Topology Refinement (TR) scenario and the Topology Inference (TI) scenario, where the original graph structure is available for each method in TR but not in TI, with TR as the default setting, if there is no special instructions. Further experimental and training details, including introductions and statistics of the TAG"}, {"title": "4.2 Overall Performance Comparison (RQ1)", "content": "Table 1 presents the accuracy and standard deviation comparison under the transductive node classification task in the TR scenario, where the original graph structure is available for each method. Based on the results, we have several observations: Observation 1: LangGSL demonstrates significant improvements over state-of-the-art GSLMs across datasets. LangGSL consistently surpasses all baseline models, achieving an average improvement of 3.1% compared to the second-best performance across all datasets. Notably, on Pubmed, LangGSL exhibits a substantial near 15% improvement over the vanilla GCN, outperforming other methods by an even greater margin. These results validate LangGSL's effectiveness across both heterophilous and homophilous graphs, as well as its ability to handle graph structures of varying scales. Observation 2: Leveraging abundant unlabeled data is essential for improving GSLM performance. As mentioned in the recent literature [10, 30], optimizing graph structures solely based on label information is insufficient. Leveraging a large and abundant amount of unlabeled information can enhance the performance of GSLM (e.g., STABLE). This confirms the effectiveness of the mutual learning mechanism in LangGSL, as validated by the results, which leverages the exchange of pseudo labels for unlabeled nodes to optimize LM and GSLM objectives and enhance learning performance. Observation 3: Leveraging language models enhance the overall performance. It is evident that methods leveraging language models (GraphEdit, LLM4RGNN, and LangGSL) significantly outperform traditional approaches primarily based on vanilla GNNs, leading to notable performance improvements. What is more, as demonstrated in Appendix Figure 5, LLM in LangGSL enhances performance by effectively cleaning raw texts, leading to improved outcomes for downstream models."}, {"title": "4.3 Robustness Analysis (RQ2)", "content": "To investigate the robustness of the LangGSL algorithm, we primarily focus on the TI scenario where no graph structure is provided, and adversarial attack settings involving perturbed graph topology."}, {"title": "5 CONCLUSION", "content": "In this paper, we present LangGSL, a novel framework that integrates language models and GSLMs to address the key challenges in graph representation learning. By leveraging LLMs for processing noisy node attributes, and harnessing the complementary strengths of LMs for generating task-related informative node embeddings, reliable graph structures, and pseudo labels, alongside GSLMs for refining graph structures and providing updated pseudo labels,"}, {"title": "A DERIVING THE EVIDENCE LOWER BOUND (ELBO)", "content": "The primary objective in this work is to compute or maximize the log-likelihood of the observed labels given the node texts and graph structure:\n$\\log p(Y_L|\\{S_n\\}_{n\\in V}, A)$,\n(13)\nwhere y represents the observed labels, sy denotes the node texts, and A is the graph structure. Here, directly computing this quantity is challenging due to the dependence on the unobserved labels yu. To address this challenge, we introduce a variational distribution $q(y_u su)$ to approximate the true posterior distribution $p(y_u | \\{S_n\\}_{n\\in V}, A, Y_L)$, thereby making the computation tractable.\nStarting from the marginal likelihood, we seek to express the log-likelihood of the observed labels YL as:\n$\\log p(Y_L|\\{S_n\\}_{n\\in V}, A) = \\log \\int p(Y_L, Y_U | \\{S_n\\}_{n\\in V}, A) dy_u$.\n(14)\nSince direct evaluation of this integral is intractable, we introduce the variational distribution $q(y_u | su)$ to approximate the true posterior. By multiplying and dividing the integrand by $q(y_u | S_U)$, we obtain:\n$\\log p(Y_L|\\{S_n\\}_{n\\in V}, A) = \\log \\int \\frac{q(Y_U | S_U) p(Y_L, Y_U | \\{S_n\\}_{n\\in V}, A)}{q(Y_U S_U)} dy_u$.\n(15)\nApplying Jensen's inequality, which exploits the concavity of the logarithmic function, gives:\n$\\log p(Y_L|\\{S_n\\}_{n\\in V}, A) \\geq \\int q(y_u|s_u) \\log \\frac{p(Y_L, Y_U | \\{S_n\\}_{n\\in V}, A)}{q(Y_U|S_U)} dy_u$,\n(16)\nwhich simplifies to:\n$\\log p(Y_L|\\{S_n\\}_{n\\in V}, A) \\geq E_{q(y_u|s_u)} [\\log p(Y_L, Y_U | \\{S_n\\}_{n\\in V}, A)] - E_{q(y_u|s_u)} [\\log q(y_u | s_U)]$.\n(17)\nThe right-hand side of this inequality is the Evidence Lower Bound (ELBO), which provides a tractable lower bound on the log-likelihood:\n$ELBO = E_{q(y_u|s_u)} [\\log p(Y_L, Y_U | \\{S_n\\}_{n\\in V}, A)] - E_{q(y_u|s_u)} [\\log q(y_u | s_U)]$.\n(18)\nMaximizing the ELBO with respect to both the variational distribution $q(y_u | S_U)$ and the model parameters allows us to indirectly maximize the marginal log-likelihood log $p(Y_L | \\{S_n\\}_{n\\in V}, A)$."}, {"title": "B PROMPT DESIGN", "content": "Data cleaning verification. Figure 5 compares the test accuracy of LangGSL using raw texts versus cleaned texts across three datasets: Instagram, Pubmed, and Cora. The results clearly indicate that LangGSL consistently performs better when using cleaned texts, demonstrating the importance of data cleaning in text-attributed graphs (TAGs). For instance, on the Pubmed dataset, LangGSL-GSLM achieves an accuracy improvement of approximately 10.4% when transitioning from raw to cleaned texts (81.72% to 92.14%). Similarly, LangGSL-LM shows a significant gain on Pubmed, increasing accuracy from 79.18% to 91.49%. Improvements on Instagram and Cora are also observed, though less pronounced. These findings validate the effectiveness of cleaning noisy raw texts in enhancing the overall model performance in downstream tasks."}, {"title": "C DATASET", "content": "We select four public and real-world datasets used for evaluation. Table 5 shows detailed statistics of these datasets. And the details of text preprocessing and feature extraction methods used for TAG datasets can be found in Table 6. The datasets used in this work include Instagram, Pubmed, ogbn-arxiv, and Cora, each with distinct characteristics. Instagram is a social network dataset with heterophilous graph structures, consisting of user profile descriptions and two classes. Pubmed, ogbn-arxiv, and Cora are citation"}, {"title": "D ADDITIONAL RESULTS", "content": "Parameter study. In the ELBO (Equation 3) optimization phase, we iteratively train LM and GSLM based on our derived equations: Equation 9 and Equation 12. To adjust the relative influence of pseudo-labels in the learning process, we introduce two coefficients, \u03b1 and \u03b2, denoted as LM-PL-Weight and GSLM-PL-Weight respectively. We then systematically analyze these two hyperparameters by examining how the performance varies with different values of \u03b1 and \u03b2 on Pubmed dataset.\nThe parameter study of LangGSL demonstrates that both LangGSL-GSLM and LangGSL-LM maintain robust performance across different values of LM-PL-weight (\u03b1) and GSLM-PL-weight (\u03b2). LangGSL-LM shows greater sensitivity to changes in these weights, with"}, {"title": "E IMPLEMENTATION", "content": "We perform comprehensive hyperparameter tuning to ensure a thorough and unbiased evaluation of both graph structure learning methods and LLM-based graph learning approaches. To ensure the generality of the experimental results, all experiments were randomly run five times/seeds. The hyperparameter search spaces of all methods are presented in Table 8 and Table 9. We acknowledge and appreciate the comprehensive benchmarks provided by recent works [29, 30, 66], which have greatly informed and guided our work.\nThe configuration used in LangGSL: We conducted all experiments on an Nvidia A5000, using GPT-4 [1] for prompt engineering, GCN [26] as the default GSLM backbone and Sent-BERT [40] as the default language model backbone unless otherwise specified. The default GCN architecture consists of 2 hidden layers with 128 hidden units, a dropout rate of 0.5, a learning rate of 0.01, and a weight decay of 5e-4.\nThe experiments across different datasets share following hyper-parameter ranges:\n\u2022 GSLM lr = 0.005, 0.01, 0.02, layers = 2, 3, hidden units = 64, 128, 256, dropout = 0.2, 0.5, 0.6, GSLM-pl-ratio = 0.2, 0.3, 0.5, 0.7, 0.8, 1.0, GSLM-pl-weight = 0.2, 0.3, 0.5, 0.7, 0.8, 1.0.\n\u2022 LM lr = 1e-5, 2e-5, 5e-5, dropout = 0.1, 0.3, cla-dropout = 0.1, 0.4, LM-pl-ratio = 0.2, 0.3, 0.5, 0.7, 0.8, 1.0, LM-pl-weight = 0.2, 0.3, 0.5, 0.7, 0.8, 1.0.\n\u2022 Iteration inf-n-epochs = 1, 2, 3.\nThese settings were used consistently across experiments to ensure the comparability of results."}]}