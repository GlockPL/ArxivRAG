{"title": "ADVERSARIAL-MIDIBERT: SYMBOLIC MUSIC UNDERSTANDING MODEL BASED ON\nUNBIAS PRE-TRAINING AND MASK FINE-TUNING", "authors": ["Zijian Zhao"], "abstract": "As an important part of Music Information Retrieval\n(MIR), Symbolic Music Understanding (SMU) has gained\nsubstantial attention, as it can assist musicians and amateurs\nin learning and creating music. Recently, pre-trained lan-\nguage models have been widely adopted in SMU because\nthe symbolic music shares a huge similarity with natural\nlanguage, and the pre-trained manner also helps make full\nuse of limited music data. However, the issue of bias, such\nas sexism, ageism, and racism, has been observed in pre-\ntrained language models, which is attributed to the imbal-\nanced distribution of training data. It also has a significant\ninfluence on the performance of downstream tasks, which\nalso happens in SMU. To address this challenge, we pro-\npose Adversarial-MidiBERT, a symbolic music understand-\ning model based on Bidirectional Encoder Representations\nfrom Transformers (BERT). We introduce an unbiased pre-\ntraining method based on adversarial learning to minimize\nthe participation of tokens that lead to biases during train-\ning. Furthermore, we propose a mask fine-tuning method\nto narrow the data gap between pre-training and fine-tuning,\nwhich can help the model converge faster and perform better.\nWe evaluate our method on four music understanding tasks,\nand our approach demonstrates excellent performance in all\nof them. The code for our model is publicly available at\nhttps://github.com/RS2002/Adversarial-MidiBERT.", "sections": [{"title": "1. INTRODUCTION", "content": "Music Information Retrieval (MIR) plays a crucial role in\nvarious fields, such as the recommendation systems in mu-\nsic apps and the AI agents for music creation. With the ad-\nvancement of computer music, symbolic music, which repre-\nsents music through a structural sequence of notes, has gained\nwidespread attention because most current music is initially\ncreated and recorded using symbolic music formats like MIDI\n[1]. Symbolic Music Understanding (SMU) has been a key\nresearch direction within MIR, aiming to assist musicians and\namateurs in learning, teaching, and creating music.\nGiven the similarity between symbolic music and natural\nlanguage, language models have been widely used in SMU.\nFor example, the Bidirectional Encoder Representations from\nTransformers (BERT) [2] model has shown promising perfor-\nmance in SMU [3, 4]. An important factor in the success of\ncurrent language models, especially Large Language Models\n(LLMs), is their use of large amounts of unlabeled data to\npre-train the models to learn basic data structure and relation-\nships. This pre-training mechanism has also been effective in\ndomains with limited data, such as music [5] and signals [6],\nwhich can help improve model performance in downstream\ntasks [7].\nCurrently, the most popular pre-training method in lan-\nguage model is Mask Language Model (MLM) [2], which is\nalso widely used in pre-trained SMU models [3, 4, 5]. How-\never, due to dataset imbalances, it can lead to bias problems\nlike discrimination in sexism, ageism, and racism in the field\nof Natural Language Processing (NLP) [8, 9]. For example, in\nthe sentence \"She is good at math.\", the MLM model would\nrandomly mask some tokens and train the model to recover\nthem. If the masked sentence becomes \"[MASK] is good at\nmath.\", the model can only recover it according to the train-\ning data distribution to achieve the highest accuracy, instead\nof considering the contextual relationship, because there is\nno information pointing to the subject. When the training set\ndata distribution about gender in different context scenarios is\nimbalanced, the model may prefer to recover the [MASK] as\n\"He\", resulting in a gender bias problem.\nRecently, some studies have indicated that the bias prob-\nlem can also significantly harm the model performance in\ndownstream tasks including classification and generation [9].\nThe MLM-based pre-trained models in other areas also suf-\nfer from the same problem since the context-free tokens can\nonly be recovered by the data distribution. However, most\ncurrent methods to solve this problem are limited to the field\nof NLP and are difficult to transfer to other areas. Most of\nthese methods can only solve single bias problems like sex-\nism [10] and region [11], but other areas do not have a similar\nconcept. For example, data augmentation [12] is a promising\nmethod in NLP, but due to the bias problem in other fields like\nmusic and signal not being as clear as in natural language, we\ndo not know how to effectively clean, generate or modify the"}, {"title": "2. PROPOSED METHOD", "content": "2.1. Overview\nThe main structure of our Adversarial-MidiBERT is illus-\ntrated in Fig. 1. It takes BERT [2] (the encoder part of the\nTransformer [13]) as the backbone, whose bi-directional at-\ntention mechanism can efficiently capture the relationships\nin music. To adapt BERT for the SMU task, we modify the\nbottom embedding layer to encode music information and the\ntop output heads for pre-training and downstream tasks.\nTo embed MIDI music information, we first employ Octu-\nple [3] to represent the symbolic music structure. It transfers\neach MIDI file into a sequence of tokens, where each token\nhas eight attributes: time signature (TS), tempo (BPM), bar\nposition (BAR), relative position within each bar (POS), in-\nstrument, pitch, duration, and velocity. We then use eight em-\nbedding layers to encode these eight attributes respectively\nand concatenate them together.\nAs for the top layer of our model, there are four dif-\nferent heads: masker, recoverer, token-level classifier, and\nsequence-level classifier, all of which share the same back-\nbone. During pre-training, we use the masker to generate the\nprobability of masking each token, and the recoverer would\nrecover the masked tokens. During fine-tuning, we use the\nclassifier heads for classification tasks. The sequence-level\nclassifier generates a single label for a whole music piece,\nuseful for tasks like composer classification and emotion\nclassification. The token-level classifier generates a label\nsequence corresponding to each token, useful for tasks like\nvelocity prediction and melody extraction.\nTo address the bias problem in pre-training, we design an\nadversarial learning mechanism. The masker tries to mask\ntokens that are difficult for the recoverer to recover, while\nthe recoverer tries to recover all masked tokens. After sev-\neral epochs, the masker selects context-free tokens with the\nhighest probability, as they can only be inferred according to\nthe training data distribution, leading to the lowest recovery\naccuracy. We then freeze these tokens to prevent the recov-\nerer from masking them in subsequent epochs. We also apply\nan unfreezing mechanism, randomly unfreezing some frozen\ntokens to avoid incorrect freezing. This adversarial process\ncontinues until the model converges.\nDuring fine-tuning, we design a mask fine-tuning mecha-\nnism, where random [MASK] tokens replace input tokens to\nreduce the gap between pre-training and fine-tuning. This ap-\nproach improves convergence speed and model performance."}, {"title": "2.2. Unbias Pre-train", "content": "The pre-training process is shown in Fig. 2. First, we perform\nrandom transposition to expand the training data, as music\ndatasets are limited. The transposition operation randomly\nraises or lowers the entire pitch according to the twelve-tone\nequal temperament within an octave. The transposition range\nlimitation ensures that the style or emotion of the song is not\nsignificantly changed by the shift in pitch register. After that,\nwe convert the MIDI file to an Octuple token sequence as the\nmodel input.\nWithin each epoch, the masker first generates the masking\nprobability of each token, and the tokens with the highest p%\nmasking probabilities are chosen. We follow a similar method\nto BERT, using the [MASK] token to replace 80% of the cho-\nsen tokens and random tokens to replace the remaining 20%.\nThe masked Octuple sequence is then input to the recoverer.\nWe can calculate recovery loss of each masked token accord-\ning to the following equation:\n\n$L_i = \\sum_{j=1}^{8} w_j CrossEntropy(\\hat{x}_{i,j}, x_{i,j}),$\n$L_{recoverer} = \\sum_{i \\in S} L_i,$\n\n(1)\nwhere $x_{i,j}$ represents the $j^{th}$ attribute of the $i^{th}$ token, $\\hat{x}$ rep-\nresents the recovered token, $w_j$ is the weight of the $j^{th}$ at-\ntribute, and S is the set of masked token indices. The re-\ncoverer's loss value is the sum of the recovery loss for those\nmasked tokens. We notice that different attributes have vary-\ning convergence speeds and performance, so we design a dy-\nnamic weight to balance the loss between them. At the begin-\nning of training, $w_1 \\sim w_8$ are set equally to 0.125. Then, in\nthe $n^{th}$ epoch, $w_j$ is set as:\n\n$w_j = \\frac{\\frac{1}{a_j}}{\\sum_{i=1}^{8} \\frac{1}{a_i}}$\n\n(2)\nwhere $a_i$ is the average recovered accuracy of the $i^{th}$ attribute\nin the (n - 1)$^{th}$ epoch. This way, the recoverer pays more\nattention to the attributes with lower accuracy.\nThe recovery loss of each token is also used to generate\nthe learning target of the masker, which aims to lower the\nrecovered accuracy of the recoverer by selecting tokens with\nhigh loss values. To achieve this, we set the learning target of\nthe top q% tokens with the highest loss values as 1 and the top\nq% tokens with the lowest loss values as 0. The loss function\nof the masker can be represented as:\n\n$L_{masker} = \\sum_{i \\in I_0} MSE(p_i, 0) + \\sum_{i \\in I_1} MSE(p_i, 1),$\n\n(3)\nwhere $p_i$ is the masking probability generated by the masker\nfor the $i^{th}$ token, and $I_0, I_1$ represent the token index sets with\ntargets set to 0 or 1, respectively.\nAfter repeating this process for k epochs, we believe the\ntokens with the highest masking probabilities are the most\nchallenging to recover. These tokens correspond to context-\nfree tokens, as they can only be predicted based on the data\ndistribution of the training set, leading to the lowest accuracy.\nAs a result, we freeze the top a% tokens within each song\nto avoid them being chosen in the subsequent training, which\ncan be realized by maintaining a dictionary. Simultaneously,\nwe also randomly unfreeze b% of the frozen tokens to prevent\nincorrect freezing in the previous step."}, {"title": "2.3. MASK Fine-tune", "content": "During fine-tuning, we can still utilize the data augmentation\nmethods employed in pre-training if the downstream tasks\nare tonality-independent. However, a potential gap may arise\nsince the [MASK] token is present in every epoch during pre-\ntraining but is absent in fine-tuning. To address this, we ran-\ndomly replace p% of the input tokens with the [MASK] to-\nken during fine-tuning. This approach is also similar to the\ndropout mechanism, which can also help mitigate overfitting."}, {"title": "3. EXPERIMENT", "content": "3.1. Experiment Setup\nOur model configuration is shown in Table 1. We conduct our\nexperiment using two NVIDIA V100 GPUs. During training,\nwe observe that our Adversarial-MidiBERT occupies about\n27GB GPU memory.\nThe dataset used in this paper is shown in Table 2. We use\nfive public MIDI datasets to train our model. We then conduct\nfour different downstream tasks to evaluate our model's per-\nformance, including two token-level classification tasks and\ntwo sequence-level tasks:\n\u2022 Composer Classification: Similar to style classification,\ncomposer classification is a more challenging and fine-"}, {"title": "4. CONCLUSION", "content": "In this paper, we present Adversarial-MidiBERT for SMU,\nthe first method to address the bias problem of pre-trained\nmodels in MIR through adversarial pre-training method. Ad-\nditionally, we introduce a mask fine-tuning approach that\nsignificantly enhances the model's accuracy and convergence\nspeed on downstream tasks. Our method achieves remarkable\nperformance on four SMU tasks, especially on the sequence-\nlevel tasks. In the future, we aim to explore the application of\nour method to music generation tasks and NLP tasks."}]}