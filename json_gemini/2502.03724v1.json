{"title": "MD-BERT: Action Recognition in Dark Videos via Dynamic Multi-Stream Fusion and Temporal Modeling", "authors": ["Sharana Dharshikgan Suresh Dass", "Hrishav Bakul Barua", "Ganesh Krishnasamy", "Raveendran Paramesran", "Rapha\u00ebl C.-W. Phan"], "abstract": "Action recognition in dark, low-light (under-exposed) or noisy videos is a challenging task due to visibility degradation, which can hinder critical spatiotemporal details. This paper proposes MD-BERT, a novel multi-stream approach that integrates complementary pre-processing techniques such as gamma correction and histogram equalization alongside raw dark frames to address these challenges. We introduce the Dynamic Feature Fusion (DFF) module, extending existing attentional fusion methods to a three-stream setting, thereby capturing fine-grained and global contextual information across different brightness and contrast enhancements. The fused spatiotemporal features are then processed by a BERT-based temporal model, which leverages its bidirectional self-attention to effectively capture long-range dependencies and contextual relationships across frames. Extensive experiments on the ARID V1.0 and ARID V1.5 dark video datasets show that MD-BERT outperforms existing methods, establishing a new state-of-the-art performance. Ablation studies further highlight the individual contributions of each input stream and the effectiveness of the proposed DFF and BERT modules.", "sections": [{"title": "I. INTRODUCTION", "content": "Action recognition in videos plays a vital role in applications such as surveillance, human-computer interaction, and automatic video tracking [1] [2]. Although action recognition has been widely explored among the vision community, recognizing actions in low-light conditions, such as night-time under-exposed surveillance or poorly lit environments, remains an under-explored area with significant challenges. These conditions often lead to reduced visibility, amplified noise, and loss of spatial and temporal details, significantly hindering model performance.\nTraditional action recognition methods, which rely on hand-crafted features or shallow architectures, struggle in such challenging scenarios. While deep learning models like R(2+1)D [3] and I3D [4] have advanced the field through spatiotemporal feature learning, they typically assume well-lit environments, limiting their effectiveness in dark video scenarios. The challenges of action recognition in dark videos have prompted researchers to explore various strategies to address the limitations posed by low-light environments.\nFor example, Chen et al. [5] proposed a dual-stream approach that combines raw dark frames and gamma-corrected frames to improve feature representation in low-light settings. While effective, their approach relies on static concatenation of features, potentially underutilizing complementary information. Similarly, Singh et al. [6] introduced an Image Enhancement Module (IEM) with Zero-DCE to enhance dark frames and coupled it with advanced temporal modeling techniques, demonstrating significant improvements. However, these methods often involve fixed preprocessing steps or lack the flexibility to adaptively fuse features.\nBuilding on these advancements, we propose MD-BERT, a novel framework for action recognition in dark videos. MD-BERT introduces a multi-stream architecture that integrates three complementary representations: raw dark frames, gamma-enhanced frames, and histogram-equalized frames. Each input stream captures unique characteristics: raw frames preserve structural information, gamma-enhanced frames highlight brightness, and histogram-equalized frames enhance contrast.\nTo effectively combine diverse features, we draw inspiration from the Attentional Feature Fusion (AFF) [7] module, which provides a unified approach to feature fusion in various scenarios (e.g., same-layer, short skip, and long skip connections). AFF dynamically fuses two input feature maps by computing attention weights using a Multi-Scale Channel Attention Module (MS-CAM), balancing local and global contexts. However, AFF was originally proposed for 2D image-based tasks and is limited to scenarios with two input streams (e.g., low- vs. high-level features), making it less adaptable for integrating multiple complementary inputs.\nBuilding on this foundation, we propose Dynamic Feature Fusion (DFF), which extends AFF from a two-input, 2D fusion approach to a three-input, 3D video scenario. Specifically, DFF handles dark, gamma-enhanced, and histogram-equalized frames by leveraging 3D attention to aggregate these distinct streams. This setup enables us to capture a more comprehensive range of low-light enhancements and ensures that each stream's complementary information is effectively highlighted (local attention) or globally weighted (global attention). Ultimately, we fuse multiple features from our three streams into a single, enriched representation, facilitating robust action recognition under low-light conditions, complex backgrounds, or significant variations in scene dynamics.\nFor temporal modeling, we employ a BERT-based architecture [8] with bidirectional attention to capture long-range spatiotemporal dependencies. By incorporating positional encoding and multi-head attention, the framework learns comprehensive action representations, further enhancing recognition accuracy. By integrating complementary streams and leveraging dynamic feature fusion, MD-BERT establishes a new state-of-the-art for action recognition in low-light environments, addressing the challenges of visibility, noise, and feature integration.\nThe main contributions of this work are fourfold and summarized as follows:\n\u2022 We propose a multi-stream framework that combines dark, gamma-enhanced, and histogram-equalized inputs to address the challenges of low-light action recognition.\n\u2022 We introduce Dynamic Feature Fusion (DFF), a novel attention-based module that adaptively fuses features from multiple streams by balancing local and global contextual information.\n\u2022 We leverage a BERT-based temporal modeling approach to capture long-range dependencies across frames, ensuring robust recognition in dark video scenarios.\n\u2022 We conduct extensive experiments on the ARID V1.0 and ARID V1.5 datasets and demonstrate that our approach outperforms state-of-the-art methods for action recognition in low-light conditions."}, {"title": "II. LITERATURE REVIEW", "content": "Enhancing video quality in low-light environments is a persistent challenge in computer vision, particularly in tasks like action recognition. Various pre-processing techniques have been proposed to address this issue. Gamma correction [9] adjusts pixel intensity to enhance brightness, while histogram equalization [10] redistributes intensity values to improve contrast. Other techniques, such as Retinex-based methods [11], aim to mimic human vision by adjusting illumination, and low-light image enhancement algorithms like LLNet [12] leverage deep learning to brighten and denoise images simultaneously. Our work distinguishes itself by embedding gamma correction and histogram equalization into a multi-stream framework, where the Dynamic Feature Fusion (DFF) module dynamically suppresses noise and emphasizes relevant features.\nMulti-feature fusion has been extensively studied in the context of action recognition due to its ability to leverage complementary information from various modalities or feature streams [13] [4]. By combining diverse inputs such as RGB, optical flow, depth, skeleton data, or audio\u2014these methods address the inherent complexity of video data and improve model resilience in challenging conditions. Early works like Two-Stream CNNs [14] demonstrated the effectiveness of parallel networks for spatial and motion cues, while subsequent research integrated additional modalities (e.g., depth [15] or skeleton data [16]) to further boost performance. More recent architectures have explored sophisticated attention mechanisms and feature fusion strategies to handle scale variations, semantic inconsistencies, and long-range dependencies in videos [17] [18]. In the broader landscape of feature fusion techniques, Attentional Feature Fusion (AFF) [7] has gained prominence as a crucial method. AFF effectively addresses both cross-layer and same-layer fusion challenges, mitigating issues such as semantic inconsistency across feature maps and the need for comprehensive multi-scale context aggregation. Specifically, it introduces a local attention branch (via convolutional layers) and a global attention branch (via global pooling) to adaptively highlight critical features.\nAmong these multi-feature fusion methods, multi-stream architectures in particular have garnered attention for video action recognition by explicitly exploiting complementary information from parallel input modalities. For instance, the work in [14] combines RGB and optical flow streams to capture spatial and motion cues, respectively. Beyond the classical two-stream setup, the work in [13] and [4] explored improved fusion strategies that further enrich feature representations from multiple data sources. Collectively, these studies demonstrate that the alignment and integration of diverse data streams\u2014such as optical flow, depth information, and skeletal keypoints\u2014consistently enhance the accuracy of action recognition by providing a more comprehensive and holistic understanding of the scene. In this work, we also adopt a multi-stream approach to harness complementary features from multiple video transformations, thereby capitalizing on the strengths of diverse inputs for robust action recognition."}, {"title": "III. METHODOLOGY", "content": "This section presents MD-BERT, a methodology for action recognition in dark videos. The approach addresses low-light challenges through a multi-stage framework comprising input preprocessing, feature extraction, and dynamic feature fusion (DFF), inspired by Attentional Feature Fusion (AFF) [7]-followed by BERT-based temporal modeling. The method begins with preprocessing, extracting three complementary input streams: dark, gamma-enhanced, and histogram-equalized frames. Unlike Chen et al. [5], which used only dark and gamma-enhanced images, we introduce histogram equalization to enhance contrast, improving robustness in action recognition. Each stream is processed independently using the R(2+1)D network [3] to extract spatial and temporal features. The DFF module then integrates these features, balancing local and global attention to retain fine-grained and high-level contextual details. Finally, the fused feature map is passed into a BERT-based architecture, where bidirectional attention captures long-range temporal dependencies within the fused feature sequence, enhancing action recognition in dark videos.\n\nThe model processes individual video frames through three transformations:\n\nGamma-Enhanced Image: Enhances visibility using gamma correction:\n$GIC(p) = P_{max}(\\frac{p}{P_{max}})^{\\gamma}$ (1)\nWhere, p: Input pixel intensity. $P_{max}$: Maximum pixel intensity (e.g., 255). $\\gamma$: Parameter controlling luminance adjustment.\nHistogram-Equalized Image: Improves contrast via histogram equalization:\n$HE(I) = \\frac{CDF(I) \u2013 CDF_{min}}{(M \u00d7 N) \u2013 CDF_{min}}(L-1)$ (2)\nWhere, CDF(I): Cumulative distribution function for intensity I. $CDF_{min}$: Minimum CDF value. M\u00d7N: Total number of pixels. L: Number of intensity levels (e.g., 256 for 8-bit images).\nThis results in a multi-stream enhanced mono-modal setup, where all inputs originate from the same video but emphasize different characteristics: dark input retains original low-light details ($F_{dark} = f(I_{dark})$), gamma-corrected input enhances brightness ($F_{gamma} = f(I_{GIC})$) and histogram-equalised input enhances contrast ($F_{he} = f(I_{HE})$).\nTo better understand the impact of these preprocessing techniques, we analyzed the pixel intensity distribution across the dataset for the dark, gamma-enhanced, and histogram-equalized frames. The histograms confirm that each transformation captures unique visual characteristics, collectively enhancing feature extraction in low-light conditions. Unlike multimodal setups that fuse different data modalities (e.g., text and video), our multi-stream approach remains within the visual modality, leveraging complementary representations for more robust action recognition.\n1) Overview and Motivation: The Dynamic Feature Fusion (DFF) module extends the Attentional Feature Fusion (AFF) [7] to address low-light video action recognition. By merging multiple enhanced streams (e.g., dark, gamma-corrected, histogram-equalized), DFF adaptively re-weights each stream's contribution via local and global attention. In particular, DFF provides two key benefits:\na) Enhanced Information Fusion and Utilization: DFF integrates complementary perspectives from multiple streams by emphasizing salient features and minimizing redundancy. The attention-weighted streams are efficiently combined into a compact yet rich representation that captures essential contributions from all inputs.\nb) Dynamic Robustness to Noise and Adaptability: DFF's local-global attention suppresses irrelevant or noisy features while dynamically adjusting to varying lighting and contrast, ensuring the most relevant information consistently dominates.\n2) Feature Integration: Each stream is processed via local (BN + RELU + CONV) and global (POOL + BN + RELU + CONV) pathways. Local captures spatial details; global encodes holistic context. Their outputs are summed to form a combined feature map.\n3) Attention Mechanism: Local and global maps are added, then passed through a convolution and sigmoid to obtain attention weights for each stream. After normalization, these weights are used to fuse the original inputs (via weighted summation), ensuring that each stream's unique information is effectively combined.\n4) Feature Fusion: The final fused feature map is computed using the attention weights to balance the contributions from different streams. This ensures that the most relevant features are emphasized while less important ones are suppressed.\nLet the three input features be: $F_{dark}$, $F_{gamma}$, $F_{he} \\in \\mathbb{R}^{C\u00d7T\u00d7H\u00d7W}$. We detail the fusion steps as follows:\n1) Combine Input Features:\n$F_{comb} = F_{dark} + F_{gamma} + F_{he}$, (3)\n2) Local Feature Extraction:\n$L = Conv_c(ReLU(BN(F_{comb}))) \\in \\mathbb{R}^{C\u00d7T\u00d7H\u00d7W}$ (4)\n3) Global Feature Extraction:\n$G = Conv_c(ReLU(BN(GAP(F_{comb})))) \\in \\mathbb{R}^{C\u00d7T\u00d7H\u00d7W}$. (5)\n4) Combine Local and Global Features:\n$C = L + G \\in \\mathbb{R}^{C\u00d7T\u00d7H\u00d7W}$ (6)\n5) Attention Map Generation:\nProduce a three-channel attention map (one channel per input feature) via BN, ReLU, and a 3-channel convolution:\n$A = Conv_3(ReLU(BN(C))) \\in \\mathbb{R}^{3\u00d7T\u00d7H\u00d7W}$. (7)\nThen apply the sigmoid function to get unnormalized attention weights:\n$W = \\sigma(A) \\in \\mathbb{R}^{3\u00d7T\u00d7H\u00d7W}$. (8)\n6) Weight Normalization:\nNormalize along the three input-feature dimension:\n$\\sum_{k=1}^{3}W_k + 10^{-6}, W_k = \\frac{W_k}{W_{sum}}$ (9)\nfor k \u2208 {1,2,3}. Thus, $W \\in \\mathbb{R}^{3\u00d7T\u00d7H\u00d7W}$.\n7) Feature Fusion:\nFinally, fuse the three original features using the normalized attention weights:\n$F_{fused} W_1 \\bigodot F_{dark} + W_2 \\bigodot F_{gamma} + W_3 \\bigodot F_{he}$ (10)\nwhere $\\bigodot$ denotes element-wise multiplication.\nBERT is a bidirectional self-attention model that has achieved remarkable performance across various downstream natural language processing (NLP) tasks [6]. BERT's bidirectional nature allows it to capture contextual information from both directions, instead of relying on just one direction, making it well-suited for sequential data. In the BERT architecture, the self-attention module computes the response at a given position by attending to all positions in the sequence and deriving a weighted average in the embedding space. Inspired by [8], we utilize BERT's bidirectional self-attention mechanism to effectively capture long-range temporal dependencies within the fused feature sequence.\nThe input to BERT is the fused feature map $F_{fused}$ with dimensions $\\mathbb{R}^{C\u00d7T\u00d7H\u00d7W}$, reshaped for sequential processing.\n1) Sequence Preparation for Temporal Encoding: The fused feature map is first pooled and reshaped:\n$F_{seq} \u2208 \\mathbb{R}^{T\u00d7D}$ (11)\nwhere, T: Temporal dimension (number of frames). D: Number of short-term characteristics of consecutive frames.\nEach temporal feature $F_{seq}[i]$ is augmented with positional embeddings $b_{pos}$ to encode temporal order:\n$b_i = F_{seq}[i] + b_{pos}, (i = 1, 2, . . ., T)$ (12)\nwhere, $b_i$: The i-th encoding vector, providing location information. $F_{seq}[i]$: Input feature for the i-th position. $b_{pos}$: The i-th learnable positional embedding, where $b_{pos} \u2208 \\mathbb{R}^{D\u00d7T}$.\nA learnable classification token (CLS) is also included in the encoding as $b_0$, resulting in:\n$b_0, b \\in \\mathbb{R}^{D\u00d7(T+1)}$ (13)\n2) Self-Attention for Temporal Dependency Modeling: For each layer l, BERT computes the Query (Q), Key (K), and Value (V) matrices as:\n$q_i^{(l,h)} = W_q^{(l,h)} \u00a3 (b_i^{(l-1)})$, $k_i^{(l,h)} = W_k^{(l,h)} \u00a3 (b_i^{(l-1)})$, (14)\n$v_i^{(l,h)} = W_v^{(l,h)} \u00a3 (b_i^{(l-1)})$ (15)\nwhere, $q_i^{(l,h)}$, $k_i^{(l,h)}$, $v_i^{(l,h)} \\in \\mathbb{R}^{d}$: Query, Key, and Value vectors for the i-th position in the sequence, for attention head h. $W_q^{(l,h)}, W_k^{(l,h)}, W_v^{(l,h)} \\in \\mathbb{R}^{D\u00d7d}$: Learnable weight matrices for Query, Key, and Value. C: Layer normalization applied to the input embeddings $el-1)$. d = D/H: Dimensionality of each attention head, where H is the number of attention heads.\n3) Attention Scores: Scaled dot-product attention is used to compute attention scores for each pair of positions:\n$a_i^{(l,h)} = softmax(\\frac{T_q^{(l,h)}T_k^{(l,h)}}{\\sqrt{d}})$ (i = 0, 1, ..., m) (16)\n4) Attention Output: The weighted sum of value vectors using a from each attention head is used to obtain encoding bl at block l, as shown below:\n$O_i^{(l,h)} O_h) = \\sum_{i=0}^{m} \\alpha_i v_i^{(l,h)}$ (17)\n5) Multi-Head Attention Output: The combined attention output with residual connection is expressed as:\n$\\binom{O_1^{(l,1)}}{O_i^{(l,H)}} + b_i^{(l-1)}$ (18)\n6) Final Encoding and Residual Connection: The final output after applying LayerNorm and MLP is expressed as:\n$b_i^l = MLP(L(b_i^{(l)})) + b_i^{(l-1)}$ (19)\nwhere, bl: Final encoded representation for layer l. L: Layer normalization operation. MLP: Multi-layer perceptron applied to the normalized output.\n7) Classification: The final classification token YCLS, derived from the last block, is processed through a fully connected (FC) layer. The argmax function is then applied to determine the final predicted class:\n$Result = Argmax(FC(Y_{CLS})).$ (20)"}, {"title": "IV. EXPERIMENTS", "content": "The availability of real-life dark video datasets is extremely limited. To demonstrate the effectiveness of our proposed approach, we utilized the ARID dataset [26], which is specifically designed for low-light conditions. The ARID dataset has two versions: ARID V1.0 and ARID V1.5. All the videos in this dataset were recorded in low-light settings or at night time. ARID V1.0 contains 3,784 video clips, while ARID V1.5 expands to 5,572 clips with each action class containing over 320 clips, spanning a total of 11 action categories. The eleven action classes include drinking, jumping, picking, pouring, pushing, running, sitting, standing, turning, walking, and waving.\nThe proposed approach is implemented on an Advanced Computing Platform (HPC) powered by NVIDIA A100 GPU utilizing the open-source machine learning framework Py-Torch [30]. We conducted experiments on both versions of the ARID benchmark datasets, ARID V1.0 and ARID V1.5, to evaluate action recognition in dark environments. We follow the approach outlined in [5] to report the average Top-1 and Top-5 accuracies across three splits. The input frame dimensions are set to 3 x 64 x 112 x 112. The input frames are enhanced using gamma correction and histogram equalization. Additionally, the original frames are included to create a multi-stream approach. For feature extraction, we utilized the R(2+1)D-34 model, pre-trained on the IG65M [6] dataset, with the average temporal pooling layer removed. The R(2+1)D-34 architecture decomposes 3D convolutions into separate 2D spatial and 1D temporal convolutions, allowing for more efficient feature learning. The output from the feature extractor has a dimensionality of 512 \u00d7 8 \u00d7 7 \u00d7 7. Next, an average pooling layer is applied, resulting in an output with dimensions of 512 \u00d7 8 which serves as the input to the DFF (Dynamic Feature Fusion) module. This output is then transposed to a dimension of 8x512, which serves as the input to the BERT module that provides the feature vector of dimension 9\u00d7256. Which is then reduced to a single 256-dimensional vector, which is passed to the classification head to predict the action class. The training process employs the ADAMW optimizer [19] with a learning rate of 10-5.\nWe carried out experiments to tune the hyper-parameter \u03b3, testing values ranging from 1 to 5.5 in increments of 0.5, with each experiment running for 30 epochs.\nThe results indicated that y = 2.5 delivered the best performance. Consequently, \u03b3 = 2.5 was adopted for all subsequent experiments.\nThe results of our method, along with those of current competitive methods for action recognition in dark videos, are presented in Table I and Table II. These results include evaluations on both ARID V1.0 and ARID V1.5 datasets. Most of the data are sourced from [6]. Our proposed method achieves state-of-the-art results on both versions of the Top-1 accuracy ARID dataset. The evaluation is conducted using all eleven classes of the ARID dataset, measuring both Top-1 and Top-5 accuracy. MD-BERT achieves 96.89% Top-1 accuracy on ARID V1.0, surpassing the closest competitor, R(2+1)D-GCN+BERT (96.60%), by 0.29%. Though small, this improvement is significant given the high accuracy baseline, where marginal gains are hard to achieve. While MD-BERT and_R(2+1)D-GCN+BERT share some architectural similarities, MD-BERT's multi-stream network-integrating dark, gamma-enhanced, and histogram-equalized frames along with the Dynamic Feature Fusion (DFF) module, enables adaptive feature weighting and better balancing of local and global contexts, giving it a slight edge. Compared to DTCM, MD-BERT improves Top-1 accuracy by 0.53% (96.89% vs. 96.36%). DTCM's joint optimization framework integrates dark enhancement and action recognition through spatio-temporal consistency and lightweight enhancement but lacks advanced temporal modeling. MD-BERT's BERT-based architecture captures long-range dependencies and contextual relationships across frames, offering a significant advantage in modeling complex action dynamics in dark videos. MD-BERT also outperforms DarkLight-R(2+1)D-34 and SCI + R(2+1)D-GCN by 3.03% and 1.07%, respectively.\nIn Table II, evaluated on the ARID V1.5 dataset, 3D-ResNet-18 [33] lags with 31.16% Top-1 accuracy, highlighting the challenges of low-light video recognition using basic 3D CNNs. I3D-RGB and I3D Two-stream [4] improve to 48.75% and 51.24%, with the latter benefiting from optical flow, but both still struggle with underexposed frames. DarkLight-R(2+1)D-34 [5] makes a notable leap to 84.13% by deploying targeted enhancement strategies for dark frames in conjunction with (2+1)D convolutions, highlighting the importance of low-light pre-processing. R(2+1)D-GCN+BERT [6] further improves to 86.93%, leveraging graph-based modeling and BERT for nuanced temporal feature extraction. However, our proposed MD-BERT achieves a 0.5% improvement (87.43% vs. 86.93%) by incorporating multi-stream inputs and the DFF module, which enhances feature diversity and provides richer input representations for BERT-based temporal modeling.\nThis section presents an ablation study of the blocks used in the proposed MD-BERT architecture. Table III compares various two-stream input configurations on the ARID V1.0 and V1.5 datasets, highlighting the contributions of Dark, Gamma, and Histogram enhancements to action recognition in low-light conditions. Notably, DarkLight-R(2+1)D-34 [5] also uses a two-stream approach (Dark+Gamma), but our method achieves higher Top-1 accuracy: 94.72% vs. 94.04% on ARID V1.0 and 84.91% vs. 84.13% on ARID V1.5. These gains demonstrate the effectiveness of our Dynamic Feature Fusion (DFF) and BERT-based temporal modeling in better integrating streams and preserving temporal consistency. Among the two-stream combinations in Tables III, (Gamma+Histogram) typically provides the best results due to effective brightness and contrast enhancements. However, the three-stream configuration (Dark+Gamma+Histogram) consistently achieves the highest Top-1 accuracy: 96.89% on ARID V1.0 and 87.43% on ARID V1.5. This confirms that each input contributes unique low-light cues, and their dynamic integration through our fusion framework is key to outperforming existing two-stream methods in underexposed video scenarios.\nTable IV highlights the impact of the Dynamic Feature Fusion (DFF) module and BERT-based temporal modeling on action recognition accuracy across the ARID V1.0 and V1.5 datasets using three inputs (Dark, Gamma, and Histogram). The DFF module effectively integrates features from multiple inputs by dynamically adjusting their importance using local and global spatiotemporal attention. This enhances complementary information fusion, improving Top-1 accuracy from 95.08% to 95.79% on ARID V1.0 and 85.54% to 86.62% on ARID V1.5 without BERT. When combined with BERT, which excels at capturing long-range temporal dependencies, the framework achieves the highest performance: 96.89% Top-1 and 99.88% Top-5 accuracy on ARID V1.0, and 87.43% Top-1 and 98.99% Top-5 accuracy on ARID V1.5. These results demonstrate the complementary roles of DFF in enhancing short-range temporal correlations and BERT in capturing long-range dependencies. This fusion framework sets a new benchmark for multi-input action recognition under low-light conditions."}, {"title": "V. CONCLUSION", "content": "In this work, we present MD-BERT, a novel framework for action recognition in dark and low-light or under-exposed videos. By leveraging three input streams, i.e., raw dark frames, gamma-enhanced frames, and histogram-equalized frames, our approach captures complementary visual information to address the challenges of low visibility and noise. The proposed DFF module adaptively combines these features, balancing local and global contexts, while the BERT-based temporal modeling captures long-range dependencies across frames. Experimental results on the ARID V1.0 and V1.5 datasets show that MD-BERT achieves state-of-the-art performance in challenging low-light scenarios. This work highlights the potential of combining dynamic feature fusion and temporal modeling for improving action recognition. Future research could explore integrating advanced image enhancement to further improve the clarity of low-light video features and extend the framework to other low-light video domains."}]}