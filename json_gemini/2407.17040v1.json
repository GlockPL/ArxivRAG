{"title": "Time Series Missing Imputation with Multivariate Radial Basis Function Neural Network", "authors": ["Chanyoung Jung", "Yun Jang"], "abstract": "Researchers have been persistently working to address the issue of missing values in time series data. Numerous models have been proposed, striving to estimate the distribution of the data. The Radial Basis Functions Neural Network (RBFNN) has recently exhibited exceptional performance in estimating data distribution. In this paper, we propose a time series imputation model based on RBFNN. Our imputation model learns local information from timestamps to create a continuous function. Additionally, we incorporate time gaps to facilitate learning information considering the missing terms of missing values. We name this model the Missing Imputation Multivariate RBFNN (MIM-RBFNN). However, MIM-RBFNN relies on a local information-based learning approach, which presents difficulties in utilizing temporal information. Therefore, we propose an extension called the Missing Value Imputation Recurrent Neural Network with Continuous Function (MIRNN-CF) using the continuous function generated by MIM-RBFNN. We evaluate the performance using two real-world datasets with non-random missing and random missing patterns, and conduct an ablation study comparing MIM-RBFNN and MIRNN-CF.", "sections": [{"title": "Introduction", "content": "Multivariate time series data has diverse applications in fields such as healthcare, weather forecasting, finance, and transportation [13, 20, 15, 30]. In healthcare, time series data are used for classifying patient outcomes, such as mortality or recovery. They are employed to traffic volume in domains like weather, finance, and transportation. tackle regression problems related to precipitation, stock prices, and However, the collection of multivariate time series data in multiple domains is often plagued by irregular time intervals and issues like equipment failures in collective devices [11]. This instability leads to missing data in multivariate time series datasets, posing significant challenges for data analysis and mining [12]. Consequently, addressing missing data in time series is widely acknowledged as a critical issue.\nThe strategies for addressing missing data can be broadly categorized into two main approaches: statistical methods and model-centric methods. Statistical approaches for missing value imputation commonly employ techniques such as mean, median, and the imputation of the most frequently occurring values [44]. These methods offer the advantage of simplicity and are relatively time-efficient. However, they may yield inaccurate results since they do not take into account the temporal aspects of time series data. In contrast, model-based missing data imputation methods have been put forward [26, 17, 4]. These methods introduce techniques for capturing temporal information within time series data. They devote effort to estimating the distribution of the data.\nIn this study, we propose a missing data imputation model for time series data by leveraging both the Radial Basis Function Neural Network (RBFNN) and the Recurrent Neural Network (RNN). RBFNN is a neural network that utilizes the linear combination of Radial Basis Functions (RBFs), which are nonlinear. When RBFNN is employed, it approximates data by generating approximate continuous functions that capture local information within the data [27, 39]. When Gaussian RBF (GRBF) is used in RBFNN, it learns the covariance structure of the data by capturing local information. Furthermore, GRBF generates smoother curves over a longer time span, allowing it to model periodic patterns and nonlinear trends in time series data [7]. Recently, these advantages of RBFNN have been claimed to outperform various machine learning algorithms in estimating data distribution [16, 14]. To extend this capability of learning covariance structures to multivariate time series data, we introduce the Missing Imputation Multivariate RBFNN (MIM-RBFNN) with respect to the time stamp (t). However, because RBFNN relies on local information to learn covariance structures, it encounters challenges in utilizing temporal information effectively. Therefore, we additionally propose the Missing Value Imputation Recurrent Neural Network with Continuous Function (MIRNN-CF), which leverages the continuous functions generated by MIM-RBFNN."}, {"title": "Related work", "content": "Efficient management of missing data is crucial for smooth time series data analysis. In recent years, numerous endeavors have put forth strategies to address this issue. The most straightforward approach involves eliminating instances that contain missing data [18]. However, this method can introduce analytical complexities as the rate of missing data increases. In addition to deletion, various statistical imputation techniques, such as replacing missing values with the mean value [1], imputing them with the most common value [34], and completing the dataset by using the last observed valid value [6], have been proposed. One of the benefits of these methods is that they allow for the utilization of a complete dataset without requiring data deletion.\nRecent studies have introduced imputation methods based on machine learning. These machine learning-based approaches can be categorized into non-Neural and Neural Network-based methods. Non-Neural Network-based methods encompass techniques like K-Nearest Neighbor (KNN) based imputation [17], and Matrix Factorization (MF) based imputation [21]. The K-Nearest Neighbor (KNN) based imputation method imputes missing values by calculating the mean value of k-neighbor samples surrounding the missing value. In contrast, the Matrix Factorization (MF) based imputation method utilizes low-rank matrices U and V to perform imputation on the incomplete matrix. However, these approaches rely on strong assumptions regarding missing data [2, 8].\nRecently, there has been a growing trend in the development of missing value imputation methods utilizing Recurrent Neural Networks (RNNs). Yoon et al. introduced the M-RNN (Multi-directional Recurrent Neural Network) framework based on RNNs [42]. M-RNN operates bidirectionally, similar to a bi-directional RNN (bi-RNN), to perform imputation for missing data. Also, some researchers have proposed studies employing a variant of RNN known as the Gate Recurrent Unit (GRU). Che et al. introduced GRU-D, a GRU-based model, which effectively captures temporal dependencies by integrating masking and time interval-based missing patterns into the GRU cell, demonstrating improved predictions by utilizing missing patterns. However, the practical usability of GRU-D may be restrained when there is no clear inherent correlation between the missing patterns in the dataset and the prediction task. The Gated Recurrent Unit for data Imputation (GRUI) was introduced in [24]. GRUI performs by considering temporal delays in observed data. They extended GRUI to GRUI-GAN, incorporating a generator and discriminator based on GRUI for enhanced temporal pattern recognition in multivariate time series imputation. Cao et al. presented BRITS, a model based on bidirectional recurrent dynamics. BRITS also utilizes masking and time intervals, achieving strong performance in missing value imputation. Furthermore, there have been proposals for combining generative adversarial networks (GANs) with models for imputing missing values in time series data. Moreover, researchers have been steadily investigating various deep learning models such as Non-AutOregressive Multiresolution Imputation (NAOMI) [23] for imputing long-term missing data and Conditional Score-based Diffusion Model (CSDI) [32] based on diffusion model and attention mechanism. CSDI demonstrates remarkable performance in handling both random and non-random missing data. However, CSDI relies on a historical strategy that utilizes ground-truth information to impute non-random missing data. Furthermore, recently proposed Self-Attention-based Imputation for Time Series (SAITS) [8] leverages the exceptional performance of self-attention methods. This ongoing trend illustrates the continuous research and development in deep learning for time series missing data imputation. These approaches have consistently driven advancements in the state-of-the-art for time series imputation. Nevertheless, it is important to note that deep learning-based imputation models, functioning as autoregressive models, face challenges such as compounding errors [23, 35], difficulties in training generative models, non-convergence, and mode collapse [37, 29]."}, {"title": "RBF for missing value imputation of time series data", "content": "To address the challenge of missing values in time series data, we introduce the MIM-RBFNN model. It aims to solve the problem by creating an appropriate continuous function for each time series to handle missing value imputation. The RBF neural network is renowned for effectively approximating any nonlinear function and is often referred to as a universal function approximator [43]. In this section, we comprehensively explain the structure and learning techniques employed in MIM-RBFNN, which is grounded in RBF approximation. Before delving into the specifics of MIM-RBFNN, we define RBF and introduce the concept of nonlinear approximation using RBF.\n### Radial Basis Function (RBF) and Approximation\nRadial Basis Function (RBF), denoted as \u03c6, is a basis function whose value depends on the distance from a specified point, often called the \"center.\" It can be mathematically expressed as \\( \u03c6(x) = \u03c6(||x||) \\). RBFs are real-valued functions as they are defined based on real numbers. Typically, instead of the origin, a fixed point c is chosen as the center, and the RBF is redefined as \\( \u03c6(x) = \u03c6(||x - c||) \\). While the Euclidean distance is commonly used for distance calculation, alternative distance metrics can also be employed.\nThe summation of RBFs is employed to construct an approximation function that suits the given data. The RBF approximation method is considered continuous due to its reliance on the distance between two points within a continuous function (RBF). Suppose we represent the RBF as \\( \u03c6(||x - c_i||) \\) and the approximate function as \\( f(x) = \\sum_{i=1}^{n} W_i \u03c6(||x - c_i||) \\), where n denotes the number of RBFs, and \\( W_i \\) signifies the associated weights. The RBF approximation method offers several compelling advantages. First, it is computationally efficient because it primarily focuses on approximating the local characteristics near the center \\( c_i \\) [36]. Second, it enhances the smoothness of data fitting by utilizing multiple RBFs [3]."}, {"title": "Multivariate-RBFNN for Imputation", "content": "We utilize non-linear approximation using Radial Basis Functions (RBFs) (4) to address the challenge of missing values in multivariate time series data. We extend the RBFNN to a multivariate RBFNN to effectively handle missing values in multivariate time series. Furthermore, to accommodate missing time intervals, we propose using Gaussian RBFs \\( (k(x) = \\exp(-\\frac{(x)^2}{\u03c3^2})) \\). This model is called the Missing value Imputation Multivariate RBFNN (MIM-RBFNN).\nThe MIM-RBFNN aims to impute missing values by generating a suitable continuous function for the input data. For this purpose, MIM-RBFNN employs RBFs, with each RBF taking a timestamp (t) as input (k(t)) and fitting the corresponding value (X) at that timestamp. Here, \\( x_t^m \\) represents the m-th feature value of the variable X at timestamp t. To model periodic patterns and nonlinear trends in time series data, we utilize Gaussian Radial Basis Functions (GRBFs). GRBFs that take a timestamp (t) as input capture the local information of t. Our model has parameters \\( C_k \\) and \\( \u03c3_k \\) for GRBFs, which are trained to create an approximate continuous function tailored to the target time series data. Here, \\( c_k \\) and \\( \u03c3_k \\) represent the k-th center and at the GRBFk, which is shared for all variable X. Additionally, to apply this approach to multivariate time series, we train different linear weights w to create continuous functions for each variable. Here, \\( w_m^k \\) represents the k-th weight at the GRBFk of variable Xm.\nWe train the center vector \\( c_k \\) to determine the optimal center vector for target time series values based on the input t. Each GRBF captures the local characteristics of input variables near \\( c_k \\) [5]. If we consider H as the diagonal covariance matrix, the GRBF\n\\( \u03c3_k(t) = \\exp(-\\frac{(t-c_k)^2}{\u03c3^2}) \\) is equivalent to \\( \\exp(-(t-c_k)^T H_k^{-1} (t-c_k)) \\). As a result, the RBFNN, by training \\( \u03c3_k \\), discovers the optimal diagonal covariance Hk for target based on input time [5]. Our MIM-RBFNN\\( \\text{CFM} = \\sum_{k} w_k e^{-\\frac{(t-c_k)^2}{\u03c3^2}} \\) illustrates the continuous function generated by MIM-RBFNN for time series data (XM). In MIM-RBFNN, multivariate time series data share the same GRBFs while being trained to track common \\( c_k \\) and \\( H_k \\). Additionally, we compute different linear weights (w) for each variable (XM) to create appropriate approximation functions (continuous functions). Therefore, by sharing the diagonal covariance structure of multivariate time series, we generate a continuous function (CFM) for imputing missing data. An ablation study regarding the GRBF sharing aspect of MIM-RBFNN can be found in Appendix C."}, {"title": "Initial Parameters of GRBFs", "content": "Unlike MLP networks that typically initialize parameters randomly, RBFNN requires the initial state of parameters to be explicitly specified [43]. In this section, we introduce the strategies for initializing initial parameters in MIM-RBFNN.\nInitial Centers. MIM-RBFNN trains the RBFNN through the processes depicted in Figure 1(a) and (b). We employ distinct methods to allocate initial centers for the RBFNNs in (a) and (b). GRBFs have their highest value at the center. Consequently, the RBFNN in the (a) process is assigned initial centers based on timestamps with higher values in the target multivariate time series data. In the case of (b), the target time series is updated to the Residual Target by (c). Hence, the initial centers in the (b) process are assigned to timestamps with higher values in the Residual Target.\nInitial Weights. The RBFNN weights determine each GRBF's magnitude, signifying the center value of a symmetrical Gaussian distribution curve for the GRBF function [31]. We assign initial centers to times with the highest residuals, indicating that the times around the initial centers have low values. Therefore, to assign the initial centers at the center of the symmetrical Gaussian distribution curve of the initial centers \\( (C_k = t) \\), we assign the target value (X) of each time series to the initial weights \\( (w_i) \\) of each time series.\nInitial Sigmas. The \u03c3 in the RBFNN represents the width of each GRBF, which in turn defines the receptive field of the GRBF. Moreover, training the \u03c3 aims to uncover the optimal diagonal covariance matrix H based on the input time stamp's target time series values [5]. Since our MIM-RBFNN takes time t as its input, the receptive fields of the GRBFs are closely tied to the local information of the time stamp t that MIM-RBFNN utilizes. Consequently, we incorporate a time gap to facilitate the learning of local information by each Gaussian RBF, taking into account missing values in the surrounding time points [2, 41].\n\\( \\delta_n = \\begin{cases} 0 & \\text{if } n = 0 \\\\ t_n - t_{n-1} & \\text{if } m_{n-1} = 1 \\& n > 0 \\\\ t_n - t_{n-1} & \\text{if } m_{n-1} = 0 \\& n > 0 \\end{cases} \\) (1)\nEquation 1 represents the time gap. This time gap calculation measures the time difference between the current timestamp and the timestamps without missing values. Since the \\( \u03c3_k \\)s of the GRBFs define their receptive ranges, we introduce the time gap as a factor into \u03c3 to enable each GRBF to account for missing values. Furthermore, to facilitate the joint learning of the covariance structure in each multivariate time series, we initialize the \u03c3 for each time series as the mean of their corresponding time gaps. The ablation study concerning the initial \u03c3 can be reviewed in Appendix B. Besides, a more comprehensive description of the backpropagation algorithm for MIM-RBFNN parameters is found in Appendix A."}, {"title": "Temporal information With Recurrent Neural Network", "content": "In this section, we describe the covariance structure trained with MIM-RBFNN and the imputation model utilizing RNN. As previously explained, MIM-RBFNN leverages local information from multivariate time series data to perform missing value imputation. However, the approximation achieved using RBFNN relies on learning local information based on observed values to create a continuous function. Furthermore, since our MIM-RBFNN also learns based on local information, its utilization of temporal information is relatively limited compared to Recurrent Neural Networks (RNNs) [22, 10]. This limitation becomes more pronounced as the length of the missing value term increases, potentially impacting imputation performance. Therefore, we propose an imputation model that combines the continuous function generated by MIM-RBFNN with the bidirectional recurrent dynamics temporal information learned by RNNs.\n### Bidirectional Recurrent with MIM-RBFNN\nFigure 2 illustrates the architecture that combines the continuous function generated by MIM-RBFNN with bidirectional recurrent dynamics. This model is called the \"Missing value Imputation Recurrent Neural Network with Continuous Function\" (MIRNN-CF). The input data for MIRNN-CF includes Continuous Function data, a time gap matrix, a Mask matrix, and Incomplete time series data. MIRNN-CF employs an RNN that utilizes feature-based estimation loss, historical-based estimation loss, and consistency loss for bidirectional RNN training, similar to previous studies [2, 25]. Besides, we propose a Continuous-concatenate estimation loss that combines the continuous function with RNN predictions.\n### MIRNN-CF Structure\nThe structure of MIRNN-CF adapts the state-of-the-art architecture of the BRITS model. In a standard RNN, the hidden state \\( h_t \\) is continually updated to learn temporal information [9]. Historical-based estimation assesses the hidden state \\( h_{t-1} \\) with a linear layer. Equation 2 represents historical-based estimation \\( \\hat{x_t} \\).\n\\( \\hat{x_t} = W_x h_{t-1} + b_x \\) (2)\n\\( \\tilde{x_t} = (1 - m_t) x_t + m_t \\hat{x_t} \\) (3)\nEquation 3 represents the imputed complete data obtained from the historical-based estimation [2]. To incorporate the covariance structure of multivariate time series data into the estimation based on the RNN's hidden state, we concatenate the Continuous Function data with the estimated \\( \\tilde{x_t} \\).\n\\( R_t = W_R CF_t + U_R \\tilde{x_t} + B_R \\) (4)\n\\( \\tilde{R_t} = (1 - m_t) R_t + m_t \\hat{x_t} \\) (5)\nEquation 4 represents the regression layer concatenating historical-based estimation with Continuous Function data. In Equation 4, \\( CF_t \\) refers to the continuous function data generated by MIM-RBFNN. Equation 5 depicts the complete data based on the estimation from Equation 4. However, incorporating the covariance structure of this continuous function data, \\( R_t \\), involves combining its own covariance structure, potentially limiting the utilization of information from other features. To include information from other features, we employ feature-based estimation.\n\\( \\hat{z_t} = W_z R_t + b_z \\) (6)\nEquation 6 describes the feature-based estimation. In Equation 4, we train the self-covariance. Therefore, to utilize only the information from other variables, we set the diagonal parameters of \\( W_z \\) to zeros in Equation 6 [2]. In other words, \\( \\hat{z_t} \\) in Equation 6 is a feature-based estimation that leverages the covariance structure of the Continuous Function data. Finally, we employ a temporal decay factor (\u03b3t) [4] in Equation 7 to combine feature-based estimation with historical-based estimation. We also follow the structure of previous feature-based estimation and historical-based estimation studies to combine both estimations using Equation 8 as shown in Equation 9.\n\\( \\gamma_t = \\sigma(W_\u03b3 \\delta_t + b_\u03b3) \\) (7)\n\\( \\beta_t = \\sigma(W_\u03b2 [\u03b3_t \\circ m_t] + b_\u03b2) \\) (8)\n\\( \\hat{x_t} = \\beta_t \\hat{z_t} + (1 - \u03b2_t) \\hat{x_t} \\) (9)\n\\( \\hat{I_t} = m_t x_t + (1 - m_t) \\hat{x_t} \\) (10)\nMIRNN-CF operates bidirectionally through the processes mentioned above to derive forward and pack, taking their average as the prediction for complete data. The hidden state update in MIRNN-CF utilizes \\( [\\hat{h_t} \\tilde{x_t}] \\) and \\( [\\hat{I_t} \\tilde{m_t}] \\) as inputs to update the RNN cell. In other words, MIRNN-CF leverages Continuous Function data, time gap matrix, Mask matrix, and Incomplete time series data to predict complete data \\( \\hat{I_t} \\). The BRITS model we adopted assumes that all labels of the time series data are complete [25]. Moreover, the BRITS depends on labeled samples during training, as it learns label accuracy [2, 25]. This method is less effective for real-world time series data, which typically have only a proportion of labeled instances [25]. To eliminate this dependency on labeled data, we propose a continuous concatenate estimation loss (Lcc) that bypasses the label accuracy loss and instead learns the covariance structure of the time series data. MIRNN-CF minimizes the combined loss function, as shown in Equation 11, which includes the feature-based estimation loss (Lf), historical-based estimation loss (Lh), Continuous-concatenate estimation loss (Lcc), and the consistency loss (Lcons).\n\\( L_{MIRNN-CF} = M \\odot L_h(X, \\hat{X}) + M \\odot L_f(X, \\hat{z}) + M \\odot L_{cc}(X, \\hat{R}) + L_{cons}(\\hat{X}_{forward}, \\hat{X}_{back}) \\) (11)\nSince MIRNN-CF learns from observed values exclusively, it cannot compute errors for undiscovered or missing values during training. Hence, we utilize a mask matrix M to omit error calculation for missing values."}, {"title": "Evaluation", "content": "We evaluate the performance of our proposed models, MIM-RBFNN and MIRNN-CF, using real-world datasets. We compare their missing value imputation performance with baseline models using the mean absolute error (MAE) and mean relative error (MRE) metrics. Furthermore, we conduct an Ablation Study to compare the effectiveness of temporal information learning between MIM-RBFNN and MIRNN-CF. The experiments were conducted on an Intel Core 3.60GHz server with a Geforce RTX 3090 GPU and 64GB of RAM."}, {"title": "Datasets and Baseline", "content": "Air Quality Data. We utilized Beijing air quality data, including PM 2.5 collected from 36 monitoring stations from May 1, 2014, to April 30, 2015 [40] to evaluate missing data imputation. It provides data with missing values, ground truth data, and the geographical information of each monitoring station. Approximately 13% of the values are missing, and the missing patterns are not random. Previous studies used data from months 3, 6, 9, and 12 as test data. However, MIM-RBFNN constructs a continuous function based on observed data without engaging in predictions. Therefore, our approach differs from prior research as we solely train on all observed data to compare the imputation performance for missing values across all months.\nHuman Activity Data. We also utilized the Localization Data for Person Activity dataset available from the UCI Machine Learning Repository [2, 25] to assess imputation performance. This dataset captures the activities of five individuals engaged in various tasks, with each person wearing four tags to record the x-coordinate, y-coordinate, and z-coordinate data for each tag. These experiments were conducted five times for each person, resulting in approximately 4,000 data points, each consisting of 40 consecutive time steps. Notably, this dataset does not contain any missing values. To evaluate missing value imputation, we artificially generated missing values randomly at rates of 30%, 50%, and 80%, while the original dataset served as the ground truth.\nBaseline. For performance comparison, we have chosen six baseline models. We selected models that utilize temporal information less significantly, similar to MIM-RBFNN (such as Mean, KNN, MF). Additionally, we included three models (M-RNN, BRITS) and a self attention-based model (SAITS) to compare with RNN-based models like MIRNN-CF. NAOMI does not include missing values in the training data to impute missing values for the long term. Besides, CSDI includes ground truth in the training data to learn non-random missing patterns. Therefore, we exclude NAOMI and CSDI from the baseline models."}, {"title": "Results", "content": "Table 1 presents the missing value imputation performance on real-world datasets. We rounded the MRE values to four decimal places for both datasets, while MAE values for the air quality dataset were rounded to two decimal places. For the human activity dataset, they were rounded to four decimal places. For the experimental settings, we kept all random seeds fixed. For the air quality dataset, we used 36 consecutive time steps, a batch size of 64, and a hidden size of 64 as constants. Additionally, we generated continuous functions for the 36 monitoring stations using MIM-RBFNN. For the human activity dataset, we used 40 consecutive time steps, a batch size of 64, and a hidden size of 64 as fixed parameters. Similarly, we generated continuous functions for each person's experiment, creating four continuous functions for each person's four tags using MIM-RBFNN.\nThe empirical results demonstrate that both MIM-RBFNN and MIRNN-CF exhibit enhanced imputation performance compared to baseline models when applied to real-world datasets. As seen in Table 1, MIRNN-CF significantly improves imputation performance over baseline models, particularly showcasing a 30% to 50% enhancement in the MAE metric compared to the baseline BRITS model for human activity data. We also found that MIM-RBFNN performs better than BRITS in human activity data across all missing rates. However, MIM-RBFNN does not outperform other deep learning models regarding air quality data. Due to the modest per-"}, {"title": "Ablation Study of MIRNN-CF", "content": "As previously mentioned, we identified that MIM-RBFNN faces challenges when dealing with long-term missing data. To address this issue, we proposed the inclusion of temporal information learning in MIRNN-CF. In this section, we present an ablation study to delve deeper into this topic.\nTo validate the effectiveness of using temporal information learning in MIM-RBFNN to address long-term missing imputation challenges, we employ the Electricity Transformer Temperature (ETT) dataset available from the UCI Machine Learning Repository [45]. The ETT dataset comprises seven multivariate time series data collected hourly and 15-minute intervals from July 1, 2016, to June 26, 2018. To maintain consistency with our previous experiments, we focus on the hourly dataset, specifically the data spanning 12 months from July 1, 2016, to June 30, 2017. To generate long-term missing data, we produced long-term missing data with a missing rate of 20%, including missing terms ranging from 50 to 80, along with random missing values. In comparison, we also create random missing data with a 20% missing rate, containing short-term missing values about the temporal missing term from 1 to 8.\nFor the ETT dataset, the performance metrics (MAE and MRE) reveal that MIM-RBFNN yields an MAE of 1.298 (MRE of 0.226) for long-term missing data and an MAE of 0.735 (MRE of 0.129) for random missing data. These results highlight the challenges faced by MIM-RBFNN when dealing with long-term missing data, similar to what was observed in the air quality dataset. However, leveraging MIM-RBFNN to implement MIRNN-CF results in a notable performance improvement, particularly for long-term missing data, where MIRNN-CF achieves an MAE of 0.563 (MRE of 0.098). Figure 4 visually represents the ablation study's outcomes. Figure 4 (a) showcases the continuous function generated by MIM-RBFNN (represented by the orange line) for long-term missing data, along with the results obtained by MIRNN-CF utilizing this continuous function (depicted by the red line). The results in (a) exhibit challenges in generating a continuous function for long-term missing data, akin to the observations made in Figure 3. Nevertheless, MIRNN-CF, which learns temporal information, effectively performs imputation. Furthermore, Figure 4 (b) displays the continuous function for random missing data (orange line) and imputation results similar to those obtained with MIRNN-CF (red line)."}, {"title": "Comparison of Single variate RBF and initial \u03c3", "content": "We propose MIM-RBFNN, which shares the same RBF for training the diagonal covariance structure of multivariate time series data. We assign the initial \u03c3s of MIM-RBFNN using Equation 1 (\u03b4) to accommodate missing values in time series data. In this section, we analyze the impact of the initial \u03c3 assignment and sharing the same RBF on multivariate time series missing value imputation. We refer to the model that uses separate RBFs as \"Missing data imputation Single RBFNN\" (MIS-RBFNN). For the initial \u03c3 analysis, we compare of assigning them randomly following N(0, 1), and the other using Equation 1.\nTable 2 and 3 present the aggregated missing data imputation performance. The results in Table 2 and 3 demonstrate that the performance of MIM-RBFNN, utilizing the initial \u03c3 assignment based on Equation 1 and employing common RBFs, is superior. The result emphasizes the fact that MIM-RBFNN, through the use of shared RBFs, learns multivariate diagonal covariance structures, thereby enhancing its missing value imputation performance. Additionally, we observed that the initial \u03c3 assignment using Equation 1 outperforms random \u03c3 assignment in MIS-RBFNN using a single RBF for all datasets. This further accentuates the effectiveness of considering the time gap in RBF's receptive field for missing data imputation."}, {"title": "Conclusions", "content": "In this study, we proposed two models for addressing missing values in multivariate time series data: MIM-RBFNN, which leverages the learning of local covariance structures using GRBF, and MIRNN-CF, a hybrid model combining continuous function generation with RNN. We demonstrated the effectiveness of MIM-RBFNN through experiments on real-world datasets. However, MIM-RBFNN primarily relies on local information to generate continuous functions, revealing challenges in learning temporal information and the complexities of long-term missing data imputation. To tackle these issues, we introduced MIRNN-CF, which utilizes MIM-RBFNN'S continuous functions, as verified through experiments on air quality data and an ablation study focused on long-term missing data imputation. Furthermore, we demonstrated the performance of our proposed initial \u03c3 allocation method and shared RBFs for learning multivariate covariance in Table 2 and Table 3. Our proposed approach improved the performance of missing imputation in multivariate time series data. Nevertheless, we approached this by developing separate models for MIM-RBFNN and MIRNN-CF. As a future research direction, we plan to investigate the development of a unified model that simultaneously learns local covariance structures based on RBF's local information and temporal information through RNN."}]}