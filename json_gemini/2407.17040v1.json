{"title": "Time Series Missing Imputation with Multivariate Radial Basis Function Neural Network", "authors": ["Chanyoung Jung", "Yun Jang"], "abstract": "Researchers have been persistently working to address the issue of missing values in time series data. Numerous models have been proposed, striving to estimate the distribution of the data. The Radial Basis Functions Neural Network (RBFNN) has recently exhibited exceptional performance in estimating data distribution. In this paper, we propose a time series imputation model based on RBFNN. Our imputation model learns local information from timestamps to create a continuous function. Additionally, we incorporate time gaps to facilitate learning information considering the missing terms of missing values. We name this model the Missing Imputation Multivariate RBFNN (MIM-RBFNN). However, MIM-RBFNN relies on a local information-based learning approach, which presents difficulties in utilizing temporal information. Therefore, we propose an extension called the Missing Value Imputation Recurrent Neural Network with Continuous Function (MIRNN-CF) using the continuous function generated by MIM-RBFNN. We evaluate the performance using two real-world datasets with non-random missing and random missing patterns, and conduct an ablation study comparing MIM-RBFNN and MIRNN-CF.", "sections": [{"title": "1 Introduction", "content": "Multivariate time series data has diverse applications in fields such as healthcare, weather forecasting, finance, and transportation [13, 20, 15, 30]. In healthcare, time series data are used for classifying patient outcomes, such as mortality or recovery. They are employed to tackle regression problems related to precipitation, stock prices, and traffic volume in domains like weather, finance, and transportation. However, the collection of multivariate time series data in multiple domains is often plagued by irregular time intervals and issues like equipment failures in collective devices [11]. This instability leads to missing data in multivariate time series datasets, posing significant challenges for data analysis and mining [12]. Consequently, addressing missing data in time series is widely acknowledged as a critical issue.\nThe strategies for addressing missing data can be broadly categorized into two main approaches: statistical methods and model-centric methods. Statistical approaches for missing value imputation commonly employ techniques such as mean, median, and the imputation of the most frequently occurring values [44]. These methods offer the advantage of simplicity and are relatively time-efficient. However, they may yield inaccurate results since they do not take"}, {"title": "2 Related work", "content": "Efficient management of missing data is crucial for smooth time series data analysis. In recent years, numerous endeavors have put forth strategies to address this issue. The most straightforward approach involves eliminating instances that contain missing data [18]. However, this method can introduce analytical complexities as the rate of missing data increases. In addition to deletion, various statistical imputation techniques, such as replacing missing values with the mean value [1], imputing them with the most common value [34], and completing the dataset by using the last observed valid value [6], have been proposed. One of the benefits of these methods is that they allow for the utilization of a complete dataset without requiring data deletion.\nRecent studies have introduced imputation methods based on machine learning. These machine learning-based approaches can be"}, {"title": "3 RBF for missing value imputation of time series data", "content": "To address the challenge of missing values in time series data, we introduce the MIM-RBFNN model. It aims to solve the problem by creating an appropriate continuous function for each time series to handle missing value imputation. The RBF neural network is renowned for effectively approximating any nonlinear function and is often referred to as a universal function approximator [43]. In this section, we comprehensively explain the structure and learning techniques employed in MIM-RBFNN, which is grounded in RBF approximation. Before delving into the specifics of MIM-RBFNN, we define RBF and introduce the concept of nonlinear approximation using RBF."}, {"title": "3.0.1 Radial Basis Function (RBF) and Approximation", "content": "Radial Basis Function (RBF), denoted as \u03d5, is a basis function whose value depends on the distance from a specified point, often called the \"center.\" It can be mathematically expressed as \u03d5(x) = \u03d5(|x|). RBFs are real-valued functions as they are defined based on real numbers. Typically, instead of the origin, a fixed point c is chosen as the center, and the RBF is redefined as \u03d5(x) = \u03d5(|x - c|). While the Euclidean distance is commonly used for distance calculation, alternative distance metrics can also be employed.\nThe summation of RBFs is employed to construct an approximation function that suits the given data. The RBF approximation method is considered continuous due to its reliance on the distance between two points within a continuous function (RBF). Suppose we represent the RBF as \u03d5(|x - c\u1d62|) and the approximate function as f(x) = \u03a3\u1d62\u208c\u2081\u207f w\u1d62\u03d5(|x - c\u1d62|), where n denotes the number of RBFs, and w\u1d62 signifies the associated weights. The RBF approximation method offers several compelling advantages. First, it is computationally efficient because it primarily focuses on approximating the local characteristics near the center c\u1d62 [36]. Second, it enhances the smoothness of data fitting by utilizing multiple RBFs [3]."}, {"title": "3.1 Multivariate-RBFNN for Imputation", "content": "We utilize non-linear approximation using Radial Basis Functions (RBFs) (\u03d5) to address the challenge of missing values in multivariate time series data. We extend the RBFNN to a multivariate RBFNN to effectively handle missing values in multivariate time series. Furthermore, to accommodate missing time intervals, we propose using Gaussian RBFs (k(x) = exp(-(x)\u00b2/\u03c3\u00b2)). This model is called the Missing value Imputation Multivariate RBFNN (MIM-RBFNN). The MIM-RBFNN aims to impute missing values by generating a suitable continuous function for the input data. For this purpose, MIM-RBFNN employs RBFs, with each RBF taking a timestamp (t) as input (k(t)) and fitting the corresponding value (X) at that timestamp. Here, x\u2098\u1d57 represents the m-th feature value of the variable X at timestamp t. To model periodic patterns and nonlinear trends in time series data, we utilize Gaussian Radial Basis Functions (GRBFs). GRBFs that take a timestamp (t) as input capture the local information of t. Our model has parameters c\u2096 and \u03c3\u2096 for GRBFs, which are trained to create an approximate continuous function tailored to the target time series data. Here, c\u2096 and \u03c3\u2096 represent the k-th center and \u03c3 at the GRBF\u2096, which is shared for all variable X. Additionally, to apply this approach to multivariate time series, we train different linear weights w to create continuous functions for each variable. Here, w\u2096\u1d50 represents the k-th weight at the GRBF\u2096 of variable X\u2098.\nWe train the center vector c\u2096 to determine the optimal center vector for target time series values based on the input t. Each GRBF captures the local characteristics of input variables near c\u2096 [5]. If we consider H as the diagonal covariance matrix, the GRBF \u03c3\u2096(t) = exp(-(t-c\u2096)\u00b2/(2\u03c3\u2096\u00b2)) is equivalent to exp(-(t-c\u2096)\u1d40H\u2096\u207b\u00b9(t-c\u2096)). As a result, the RBFNN, by training \u03c3\u2096, discovers the optimal diagonal covariance H\u2096 for target based on input time [5]. Our MIM-RBFNN CFM = \u03a3\u2096 w\u2096 exp(-(t-c\u2096)\u00b2/\u03c3\u2096\u00b2) illustrates the continuous function generated by MIM-RBFNN for time series data (X\u2098). In MIM-RBFNN, multivariate time series data share the same GRBFs while being trained to track common c\u2096 and H\u2096. Additionally, we compute different linear weights (w) for each variable (X\u2098) to create appropriate approximation functions (continuous functions). Therefore, by sharing the diagonal covariance structure of multivariate time series, we generate a continuous function (CFM) for imputing missing data. An ablation study regarding the GRBF sharing aspect of MIM-RBFNN can be found in Appendix C.\nFigure 1 illustrates the architecture of MIM-RBFNN, which comprises four distinct processes: (a) the Initial RBFNN Process, (b) the Additional RBFNN Process, (c) the Target Residual Process, and (d) the Missing Time Series Imputation Process. The challenge is determining the number of RBFs employed in the linear combination to fit the target time series variable for RBF approximation adequately. However, the precise quantity of RBFs needed for each variable remains uncertain. We utilize processes (a) and (b) to tackle this issue. Both (a) and (b) role to ascertain the requisite number of RBFs for approximating the target data, i.e., the observed time series data. As depicted in (a), the Initial RBFNN Process entails a learning phase dedicated to constructing a continuous function using the initial RBFNN\u2080. Subsequently, we calculate the approximation error of the continuous function generated by the initial RBFNN\u2080. If the fitting error surpasses the loss threshold (MAPE 5%), the Additional RBFNN Process is executed in (b), where additional RBFs are trained. The Additional RBFNN process in (b) involves training an additional RBFNN with the same architecture as the initial RBFNN\u2080. However, during the Additional RBFNN process, if we were to use the target time series as is, it would necessitate retraining the previously trained RBFNN, which would lead to an increase in the number of parameters that ought to be trained, resulting in longer training times [28]. Such a scenario can escalate the complexity of the training process and introduce confusion [33]. Therefore, as illustrated in (c), we exclusively train RBFs of the additional RBFNN with the target residual process. In (c), the target residual process calculates the residual of the Continuous Function generated in (a) from the initial observed time series data to update the target data. The updated target data becomes the first target in (b). The first additional RBFNN\u2081 in (b) utilizes the updated target data to create a continuous function. Subsequently, the target data is updated using the continuous function generated by the first additional RBFNN\u2081 and the residual from the updated target data. This updated target data serves as the target data for the second additional RBFNN\u2082. Process (c) continues until the Additional RBFNN Process is completed. Finally, MIM-RBFNN combines all the GRBFs trained in (a) and (b) to generate a Continuous Function for multivariate time series in (d) and imputes missing values."}, {"title": "3.2 Initial Parameters of GRBFs", "content": "Unlike MLP networks that typically initialize parameters randomly, RBFNN requires the initial state of parameters to be explicitly specified [43]. In this section, we introduce the strategies for initializing initial parameters in MIM-RBFNN.\nInitial Centers. MIM-RBFNN trains the RBFNN through the processes depicted in Figure 1(a) and (b). We employ distinct methods to allocate initial centers for the RBFNNs in (a) and (b). GRBFs have their highest value at the center. Consequently, the RBFNN in the (a) process is assigned initial centers based on timestamps with higher values in the target multivariate time series data. In the case of (b), the target time series is updated to the Residual Target by (c). Hence, the initial centers in the (b) process are assigned to timestamps with higher values in the Residual Target.\nInitial Weights. The RBFNN weights determine each GRBF's magnitude, signifying the center value of a symmetrical Gaussian distribution curve for the GRBF function [31]. We assign initial centers to times with the highest residuals, indicating that the times"}, {"title": "4 Temporal information With Recurrent Neural Network", "content": "In this section, we describe the covariance structure trained with MIM-RBFNN and the imputation model utilizing RNN. As previously explained, MIM-RBFNN leverages local information from multivariate time series data to perform missing value imputation. However, the approximation achieved using RBFNN relies on learning local information based on observed values to create a continuous function. Furthermore, since our MIM-RBFNN also learns based on local information, its utilization of temporal information is relatively limited compared to Recurrent Neural Networks (RNNs) [22, 10]. This limitation becomes more pronounced as the length of the missing value term increases, potentially impacting imputation performance. Therefore, we propose an imputation model that combines the continuous function generated by MIM-RBFNN with the bidirectional recurrent dynamics temporal information learned by RNNs."}, {"title": "4.1 Bidirectional Recurrent with MIM-RBFNN", "content": "Figure 2 illustrates the architecture that combines the continuous function generated by MIM-RBFNN with bidirectional recurrent dynamics. This model is called the \"Missing value Imputation Recurrent Neural Network with Continuous Function\" (MIRNN-CF). The input data for MIRNN-CF includes Continuous Function data, a time gap matrix, a Mask matrix, and Incomplete time series data. MIRNN-CF employs an RNN that utilizes feature-based estimation loss, historical-based estimation loss, and consistency loss for bidirectional RNN training, similar to previous studies [2, 25]. Besides, we propose a Continuous-concatenate estimation loss that combines the continuous function with RNN predictions."}, {"title": "4.2 MIRNN-CF Structure", "content": "The structure of MIRNN-CF adapts the state-of-the-art architecture of the BRITS model. In a standard RNN, the hidden state h\u209c is continually updated to learn temporal information [9]. Historical-based estimation assesses the hidden state h\u209c\u208b\u2081 with a linear layer. Equation 2 represents historical-based estimation x\u0302\u209c.\nx\u0302\u209c = W\u2093h\u209c\u208b\u2081 + b\u2093\n(2)\nx\u0303\u209c = (1 - m\u209c)x\u209c + m\u209cx\u0302\u209c\n(3)\nEquation 3 represents the imputed complete data obtained from the historical-based estimation [2]. To incorporate the covariance structure of multivariate time series data into the estimation based on the RNN's hidden state, we concatenate the Continuous Function data with the estimated x\u0303\u209c.\nR\u209c = W\u1d63CF\u209c + U\u1d63x\u0303\u209c + B\u1d63\n(4)\nR\u0302\u209c = (1 - m\u209c)R\u209c + m\u209cx\u0302\u209c\n(5)\nEquation 4 represents the regression layer concatenating historical-based estimation with Continuous Function data. In Equation 4, CF\u209c refers to the continuous function data generated by MIM-RBFNN. Equation 5 depicts the complete data based on the estimation from Equation 4. However, incorporating the covariance structure of this continuous function data, R\u0302\u209c, involves combining its own covariance structure, potentially limiting the utilization of information from other features. To include information from other features, we employ feature-based estimation.\n\u1e91\u209c= W\u2082R\u0302\u209c + b\u2082\n(6)\nEquation 6 describes the feature-based estimation. In Equation 4, we train the self-covariance. Therefore, to utilize only the information from other variables, we set the diagonal parameters of W\u2082 to zeros in Equation 6 [2]. In other words, \u1e91\u209c in Equation 6 is a feature-based estimation that leverages the covariance structure of the Continuous Function data. Finally, we employ a temporal decay factor (\u03b3\u209c) [4] in Equation 7 to combine feature-based estimation with historical-based estimation. We also follow the structure of previous feature-based estimation and historical-based estimation studies to combine both estimations using Equation 8 as shown in Equation 9.\n\u03b3\u209c = \u03c3(W\u1d63\u03b4\u209c + b\u1d63)\n(7)\n\u03b2\u209c = \u03c3(W\u209a[\u03b3\u209cm\u209c] + b\u03b2)\n(8)\nx\u0302\u209c=\u03b2\u209c\u1e91\u209c+ (1-\u03b2\u209c) x\u0302\u209c\n(9)\n\u0128\u209c = m\u209cx\u209c + (1-m\u209c) x\u0302\u209c\n(10)\nMIRNN-CF operates bidirectionally through the processes mentioned above to derive forward and pack, taking their average as the prediction for complete data. The hidden state update in MIRNN-CF utilizes [h\u209c:h\u0302\u209c] and [\u0128\u209c:m\u209c] as inputs to update the RNN cell. In other words, MIRNN-CF leverages Continuous Function data, time gap matrix, Mask matrix, and Incomplete time series data to predict complete data \u0128\u209c. The BRITS model we adopted assumes that all labels of the time series data are complete [25]. Moreover, the BRITS depends on labeled samples during training, as it learns label accuracy [2, 25]. This method is less effective for real-world time series data, which typically have only a proportion of labeled instances [25]. To eliminate this dependency on labeled data, we propose a continuous concatenate estimation loss (Lcc) that bypasses the label accuracy loss and instead learns the covariance structure of the time series data. MIRNN-CF minimizes the combined loss function, as shown in Equation 11, which includes the feature-based estimation loss (Lf), historical-based estimation loss (Lh), Continuous-concatenate estimation loss (Lcc), and the consistency loss (Lcons).\nL\u1d39\u1d35\u1d3f\u1d3a\u1d3a\u207b\u1d9c\u1da0 = M \u2299 L(X, Y) + M \u2299 L\u2095(X, Y)\n+M\u2299 L\ud835\udc53(X, \u1e90) + M\u2299 L\u1d9c\u1d9c(X, R\u0302)\n+L\u1d9c\u1d52\u207f\u02e2(\u1e90\u1da0\u1d52\u02b3\u02b7\u1d43\u02b3\u1d48, \u1e90\u1d47\u1d43\u1d9c\u1d4f)\n(11)\nSince MIRNN-CF learns from observed values exclusively, it cannot compute errors for undiscovered or missing values during training. Hence, we utilize a mask matrix M to omit error calculation for missing values."}, {"title": "5 Evaluation", "content": "We evaluate the performance of our proposed models, MIM-RBFNN and MIRNN-CF, using real-world datasets. We compare their missing value imputation performance with baseline models using the mean absolute error (MAE) and mean relative error (MRE) metrics. Furthermore, we conduct an Ablation Study to compare the effectiveness of temporal information learning between MIM-RBFNN and MIRNN-CF. The experiments were conducted on an Intel Core 3.60GHz server with a Geforce RTX 3090 GPU and 64GB of RAM."}, {"title": "5.1 Datasets and Baseline", "content": "Air Quality Data. We utilized Beijing air quality data, including PM 2.5 collected from 36 monitoring stations from May 1, 2014, to April 30, 2015 [40] to evaluate missing data imputation. It provides data with missing values, ground truth data, and the geographical information of each monitoring station. Approximately 13% of the values are missing, and the missing patterns are not random. Previous studies used data from months 3, 6, 9, and 12 as test data. However, MIM-RBFNN constructs a continuous function based on observed data without engaging in predictions. Therefore, our approach differs from prior research as we solely train on all observed data to compare the imputation performance for missing values across all months.\nHuman Activity Data. We also utilized the Localization Data for Person Activity dataset available from the UCI Machine Learning Repository [2, 25] to assess imputation performance. This dataset captures the activities of five individuals engaged in various tasks, with each person wearing four tags to record the x-coordinate, y-coordinate, and z-coordinate data for each tag. These experiments were conducted five times for each person, resulting in approximately 4,000 data points, each consisting of 40 consecutive time steps. Notably, this dataset does not contain any missing values. To evaluate missing value imputation, we artificially generated missing values randomly at rates of 30%, 50%, and 80%, while the original dataset served as the ground truth.\nBaseline. For performance comparison, we have chosen six baseline models. We selected models that utilize temporal information less significantly, similar to MIM-RBFNN (such as Mean, KNN, MF). Additionally, we included three models (M-RNN, BRITS) and a self attention-based model (SAITS) to compare with RNN-based models like MIRNN-CF. NAOMI does not include missing values in the training data to impute missing values for the long term. Besides, CSDI includes ground truth in the training data to learn non-random missing patterns. Therefore, we exclude NAOMI and CSDI from the baseline models."}, {"title": "5.2 Results", "content": "Table 1 presents the missing value imputation performance on real-world datasets. We rounded the MRE values to four decimal places for both datasets, while MAE values for the air quality dataset were rounded to two decimal places. For the human activity dataset, they were rounded to four decimal places. For the experimental settings, we kept all random seeds fixed. For the air quality dataset, we used 36 consecutive time steps, a batch size of 64, and a hidden size of 64 as constants. Additionally, we generated continuous functions for the 36 monitoring stations using MIM-RBFNN. For the human activity dataset, we used 40 consecutive time steps, a batch size of 64, and a hidden size of 64 as fixed parameters. Similarly, we generated continuous functions for each person's experiment, creating four continuous functions for each person's four tags using MIM-RBFNN.\nThe empirical results demonstrate that both MIM-RBFNN and MIRNN-CF exhibit enhanced imputation performance compared to baseline models when applied to real-world datasets. As seen in Table 1, MIRNN-CF significantly improves imputation performance over baseline models, particularly showcasing a 30% to 50% enhancement in the MAE metric compared to the baseline BRITS model for human activity data. We also found that MIM-RBFNN performs better than BRITS in human activity data across all missing rates. However, MIM-RBFNN does not outperform other deep learning models regarding air quality data. Due to the modest per-"}, {"title": "5.3 Ablation Study of MIRNN-CF", "content": "As previously mentioned, we identified that MIM-RBFNN faces challenges when dealing with long-term missing data. To address this issue, we proposed the inclusion of temporal information learning in MIRNN-CF. In this section, we present an ablation study to delve deeper into this topic.\nTo validate the effectiveness of using temporal information learning in MIM-RBFNN to address long-term missing imputation challenges, we employ the Electricity Transformer Temperature (ETT) dataset available from the UCI Machine Learning Repository [45]. The ETT dataset comprises seven multivariate time series data collected hourly and 15-minute intervals from July 1, 2016, to June 26, 2018. To maintain consistency with our previous experiments, we focus on the hourly dataset, specifically the data spanning 12 months from July 1, 2016, to June 30, 2017. To generate long-term missing data, we produced long-term missing data with a missing rate of 20%, including missing terms ranging from 50 to 80, along with random missing values. In comparison, we also create random missing data with a 20% missing rate, containing short-term missing values about the temporal missing term from 1 to 8.\nFor the ETT dataset, the performance metrics (MAE and MRE) reveal that MIM-RBFNN yields an MAE of 1.298 (MRE of 0.226) for long-term missing data and an MAE of 0.735 (MRE of 0.129) for random missing data. These results highlight the challenges faced by MIM-RBFNN when dealing with long-term missing data, similar to what was observed in the air quality dataset. However, leveraging MIM-RBFNN to implement MIRNN-CF results in a notable performance improvement, particularly for long-term missing data, where MIRNN-CF achieves an MAE of 0.563 (MRE of 0.098)."}, {"title": "5.4 Comparison of Single variate RBF and initial \u03c3", "content": "We propose MIM-RBFNN, which shares the same RBF for training the diagonal covariance structure of multivariate time series data. We assign the initial \u03c3s of MIM-RBFNN using Equation 1 (\u03b4) to accommodate missing values in time series data. In this section, we analyze the impact of the initial \u03c3 assignment and sharing the same RBF on multivariate time series missing value imputation. We refer to the model that uses separate RBFs as \"Missing data imputation Single RBFNN\" (MIS-RBFNN). For the initial \u03c3 analysis, we compare of assigning them randomly following N(0, 1), and the other using Equation 1.\nEquation 1 and employing common RBFs, is superior. The result emphasizes the fact that MIM-RBFNN, through the use of shared RBFs, learns multivariate diagonal covariance structures, thereby enhancing its missing value imputation performance. Additionally, we observed that the initial \u03c3 assignment using Equation 1 outperforms random \u03c3 assignment in MIS-RBFNN using a single RBF for all datasets. This further accentuates the effectiveness of considering the time gap in RBF's receptive field for missing data imputation."}, {"title": "6 Conclusions", "content": "In this study, we proposed two models for addressing missing values in multivariate time series data: MIM-RBFNN, which leverages the learning of local covariance structures using GRBF, and MIRNN-CF, a hybrid model combining continuous function generation with RNN. We demonstrated the effectiveness of MIM-RBFNN through experiments on real-world datasets. However, MIM-RBFNN primarily relies on local information to generate continuous functions, revealing challenges in learning temporal information and the complexities of long-term missing data imputation. To tackle these issues, we introduced MIRNN-CF, which utilizes MIM-RBFNN's continuous functions, as verified through experiments on air quality data and an ablation study focused on long-term missing data imputation. Furthermore, we demonstrated the performance of our proposed initial \u03c3 allocation method and shared RBFs for learning multivariate covariance in Table 2 and Table 3. Our proposed approach improved the performance of missing imputation in multivariate time series data. Nevertheless, we approached this by developing separate models for MIM-RBFNN and MIRNN-CF. As a future research direction, we plan to investigate the development of a unified model that simultaneously learns local covariance structures based on RBF's local information and temporal information through RNN."}, {"title": "Appendix of Time Series Imputation with Multivariate Radial Basis Function Neural Network", "content": "Back Propagation Algorithm of MIM-RBFnn\nTo generate a continuous function for time series data based on the time stamp (t), we update the optimal parameters (c, \u03c3, w) of each GRBF using the Backpropagation (BP) algorithm. Each parameter is updated as follows: W\u2098 = W\u2098 - lr * \u2202L/\u2202W\u2098 , C\u2096 = C\u2096 - lr * \u2202L/\u2202C\u2096 , and \u03a3 = \u03a3 - lr * \u2202L/\u2202\u03a3 , where lr denotes the learning rate.\nF\u2098 = \u03a3 W\u2096\u03c6\u2096 , \u03c6\u2096 = e\u02e3\u1d56[-(t-c\u2096)\u00b2/\u03c3\u2096\u00b2], L = \u03a3 (X\u2098 - F\u2098)\u00b2 , (12)\n\u2202L/\u2202W\u2096 = \u03a3 (\u2202L/\u2202F\u2098)(\u2202F\u2098/\u2202W\u2096) ,  (13)\n\u2202F\u2098/\u2202W\u2096 = \u03c6\u2096 ,\n\u2202L/\u2202C\u2096 = \u03a3 (\u2202L/\u2202F\u2098)(\u2202F\u2098/\u2202C\u2096) ,   (14)\n\u2202F\u2098/\u2202C\u2096 = W\u2096(t - c\u2096)/\u03c3\u2096\u00b2 \u03c6\u2096 ,\n\u2202L/\u2202\u03c3\u2096 = \u03a3 (\u2202L/\u2202F\u2098)(\u2202F\u2098/\u2202\u03c3\u2096) ,  (15)\n\u2202F\u2098/\u2202\u03c3\u2096 = -W\u2096(t - c\u2096)\u00b2/\u03c3\u2096\u00b3 \u03c6\u2096 ,\nEquations 13, 14, 15 represent the parameter updates of MIM-RBFNN using gradient descent. Here, \u03c6\u2096 indicates the k-th GRBF. The parameters of each GRBF are updated based on its receptive field. In particular, each GRBF learns the covariance structure within its receptive field and returns zero for data outside its receptive field, minimizing its impact on data beyond that field. Equation 13 shows the update process for weights (W\u2098) for each time series (X\u2098). Equations 14 and 15 depict the update process for the centers (C\u2096) and sigmas (\u03c3\u2096) of GRBFs, respectively, while jointly considering the covariance structure of multivariate time series data. Based on this BP algorithm, each GRBF iteratively seeks the optimal centers, weights, and sigmas for its receptive field, building upon the values from the previous update step [38]."}, {"title": "Appendix : Comparison of initial sigma in MIM-RBFNN", "content": "The parameter sigma (\u03c3) of RBFNN determines the width of the radial basis function, which represents the receptive field of the RBF. We assign the initial \u03c3s of MIM-RBFNN using Equation 1 to accommodate missing values in time series data. In this section, we analyze the impact of the initial \u03c3 assignment on time series missing value imputation. For comparative analysis, we compare two methods of assigning the initial \u03c3 in MIM-RBFNN: one where it is assigned randomly following N(0, 1), and the other using Equation 1.\n\u03b4\u2099 = 0 if n = 0 (1)\n\u03b4\u2099 = t\u2099 - t\u2099\u208b\u2081 if m\u2099\u208b\u2081 = 1 & n > 0 (1)\n\u03b4\u2099 = 1 if m\u2099\u208b\u2081 = 0 & n > 0 (1)\nFor all datasets, assigning the initial \u03c3 as described in Equation 1 to accommodate missing values yields better results compared to randomly assigning \u03c3, which stresses the importance of considering the receptive field of RBF concerning time gaps when imputing missing values, highlighting the effectiveness of our initial \u03c3 assignment method. Furthermore, we generated synthetic data to analyze the changes in the receptive field of MIM-RBFNN during the \u03c3 learning process. We used Lorenz-96 data as our synthetic dataset [19], creating 200 time stamps with five variables. Subsequently, we introduced random missing data, accounting for 30% of the data. In terms of imputation performance (MAE(MRE)), MIM-RBFNN (1.706 (0.418)) outperforms MIM-RBFNN + Random (2.141 (0.525)), showcasing the superior performance of MIM-RBFNN that accounts for the time gaps.\nFigure 5 illustrates the learning progress on synthetic data, where green points represent the ground truth of missing data, blue points represent observed data, and the red line represents the continuous function. When examining the violet boxes in Figure 5 (a) and (b), it becomes evident that the continuous function of MIM-RBFNN, which accounts for time gaps, is adapting to missing time points. In contrast, for MIM-RBFNN + Random, the learning process is only biased towards the observed data, emphasizing that the initial \u03c3 of MIM-RBFNN allows it to create a continuous function using a receptive field that considers missing data."}, {"title": "C Appendix : Comparison of Multivariate RBF and Single variate RBF", "content": "We propose MIM-RBFNN, which shares the same RBF for training the diagonal covariance structure of multivariate time series data. In this section, we analyze the impact of sharing the same RBF on imputing missing values in multivariate time series data. To conduct this analysis, we compare RBFNN and MIM-RBFNN, which use separate RBFs instead of sharing the same one. We refer to the model that uses separate RBFs as \"Missing data imputation Single RBFNN\" (MIS-RBFNN). MIS-RBFNN is identical to MIM-RBFNN except that it uses separate RBFs for each variable.\nMIM-RBFNN employs the same set of shared RBFs for all variables and learns all variables within a single model, utilizing distinct"}]}