{"title": "A Differentiated Reward Method for Reinforcement Learning based Multi-Vehicle Cooperative Decision-Making Algorithms", "authors": ["Ye Han", "Lijun Zhang", "Dejian Meng"], "abstract": "Reinforcement learning (RL) shows great potential for optimizing multi-vehicle cooperative driving strategies through the state-action-reward feedback loop, but it still faces challenges such as low sample efficiency. This paper proposes a differentiated reward method based on steady-state transition systems, which incorporates state transition gradient information into the reward design by analyzing traffic flow characteristics, aiming to optimize action selection and policy learning in multi-vehicle cooperative decision-making. The performance of the proposed method is validated in RL algorithms such as MAPPO, MADQN, and QMIX under varying autonomous vehicle penetration. The results show that the differentiated reward method significantly accelerates training convergence and outperforms centering reward and others in terms of traffic efficiency, safety, and action rationality. Additionally, the method demonstrates strong scalability and environmental adaptability, providing a novel approach for multi-agent cooperative decision-making in complex traffic scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "As autonomous driving technology evolves towards networking and collaboration, multi-vehicle cooperative decision-making is expected to become a crucial means of enhancing traffic efficiency and road safety. Research indicates that in typical scenarios such as unsignalized intersections and highway merging zones, traditional single-vehicle decision-making systems, due to their lack of global coordination capabilities, may result in traffic efficiency loss and potential safety hazards [1]. Multi-vehicle cooperative decision-making systems hold significant research value in constructing the next generation of intelligent transportation systems.\nReinforcement Learning (RL), with its adaptive learning capabilities in dynamic environments, has gradually become one of the mainstream methods for vehicle decision-making [2]\u2013[4]. Driven by deep reinforcement learning, vehicle decision systems have achieved good performance improvement in key metrics such as trajectory prediction accuracy and risk avoidance [2]. However, the application of Multi-Agent Reinforcement Learning (MARL) in autonomous driving still faces challenges such as low sample efficiency, the curse of dimensionality, and long-tail problems [5].\nMulti-vehicle cooperative decision-making algorithms typically utilize vehicle speed signals, vehicle positions, and interaction events between vehicles (e.g., car-following, negotiating lane changes, collision avoidance, etc.) to design reward mechanisms. These signals help guide the vehicles to make reasonable decisions. Therefore, the design of the reward function is of crucial importance [6], [7]. Specifically, from a mesoscopic traffic flow perspective, the state of vehicles is in most time stable and changes gradually over time. This can lead to reinforcement learning algorithms failing to distinguish between actions due to errors. In this paper, we proposed a differentiated reward method based on a steady-state transition system. Experimental results demonstrate that the differentiated reward method significantly accelerates the training convergence speed of reinforcement learning and exhibits more rational behavior in action selection.\nThe main contributions of this paper can be summarized as follows:\n1) A differentiated reward method in vehicle decision-making with steady-state transition is formulated from perspective of reinforcement learning theory. By employing differentiated reward, The performance of reinforcement learning algorithms in continuous multi-vehicle cooperative decision-making tasks is enhanced.\n2) Thurough simulation experiments for multi-vehicle cooperative decision-making under different autonomous vehicle penetration rates in a continuous traffic flow environment is conducted, validating the scalability and learning stability of differential rewards in multi-agent traffic scenarios."}, {"title": "II. RELATED WORKS", "content": "Multi-Vehicle Cooperative Decision-Making Based on Reinforcement Learning: In recent years, the problem of multi-vehicle cooperative decision-making in dynamic traffic environments has garnered significant attention. Rule-based or optimization-based methods often struggle to scale and adapt in complex traffic scenarios. Increasingly mature reinforcement learning algorithms such as MADQN, MAD-DPG, MAPPO, and Qmix enable agents to learn optimal strategies through interaction with the environment [4]. These algorithms have significantly improved the quality of multi-vehicle interaction decision-making in scenarios such as unsignalized intersections [8]\u2013[11], highway ramps [12]\u2013[14], and mixed scenarios [15], [16].\nDespite the success of existing multi-vehicle cooperative decision-making methods in certain fixed scenarios, designing efficient reinforcement learning algorithms that account for the dynamic characteristics of traffic flow remains an open research challenge in complex, continuous traffic flow environments."}, {"title": "III. PROBLEM FORMULATION", "content": "A. Markov Decision Process\nWe model the interaction between the agent and the environment using a finite Markov Decision Process (MDP) $(S, A, R, p)$, where S, A, and R represent the state space, action space, and reward space, respectively. The state transition probability is denoted by $p : S\u00d7R\u00d7S\u00d7A \u2192 [0,1]$. At time step t, the agent is in state $S_t \u2208 S$ and selects an action $A_t \u2208 A$ using a behavior policy $b : A \u00d7 S \u2192 [0, 1]$. According to the state transition rule $p(s',r | s,a) = Pr(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$, the system transitions to the next state $S_{t+1} \u2208 S$, and the agent receives a reward $R_{t+1} \u2208 R$. In the continuous problem we consider, the interaction between the agent and the environment persists indefinitely. The agent's goal is to maximize the average reward obtained over the long term. To achieve this, we aim to estimate the expected discounted sum of rewards for each state, where $\u03b3\u2208 [0,1)$: $v_\u03c0(s) = E[\\sum_{t=0}^\\infty \u03b3^tR_{t+1} | S_t = s, A_{t:\u221e} ~ \u03c0]$, \u2200s.\nB. World Model\nThis paper addresses the multi-vehicle cooperative decision-making problem in urban traffic scenarios characterized by continuous traffic flow and mixed autonomy.\nWe consider a unidirectional branch of a bidirectional eight-lane road, where vehicles in all four lanes are randomly assigned one of three objectives: going straight, turning left, or turning right. The models for human-driven vehicles (HDVs) in terms of going straight and lane-changing follow the same settings as in our previous work [21].\nFor connected and autonomous vehicles (CAVs), as illustrated in Fig. 1, each vehicle has accurate perception of its own position, speed, target lane, and vehicle type, as well as those of vehicles within its observation range. Additionally, CAVS can share their perception information through infrastructure."}, {"title": "C. Observation/State Space", "content": "For the agent vehicle i, its own state vector is defined as:\n$s_i^{self}$ = [plon, plat, Vi, Ti, gi, dleft, dfront, dright]\nwhere plon \u2208 R and plat \u2208 R represent the longitudinal and lateral positions of the vehicle in the global coordinate system, respectively, vi \u2208 R+ denotes the current driving speed, Ti \u2208 Z+ is the discretized vehicle type encoding, and gi \u2208 {0,1}k represents the driving goal (going straight, turning left, or turning right) using one-hot encoding. dleft, dfront, dright \u2208 R+ denote the distances to the nearest vehicles in the left, front, and right lanes, respectively. If no corresponding vehicle exists, the distance is set to a predefined maximum value dmax.\nFor the set of surrounding vehicles $nbr = {j | ||P_i - P_j ||_2 < R}$ (where R is the perception radius), construct the relative state matrix:\n$M^{nbr} = concat [\\Delta p_{ij}^{lon}, \\Delta p_{ij}^{lat}, \\Delta v_{ij}, \\Delta T_{ij}, \\Delta g_{ij}]$,\n$j \\epsilon nbr$\nwhere $\\Delta p_{ij}^{lon} = p_{ij}^{lon} - p_{i}^{lon}$ and $\\Delta p_{ij}^{lat} = p_{ij}^{lat} - p_{i}^{lat}$ represent the relative positional relationships, $\\Delta v_{ij} = v_j \u2013 v_i$ denotes the speed difference, $\\Delta T_{ij} = T_j - T_i$ reflects the difference in vehicle types, and $\\Delta g_{ij} = ||g_j- g_i||_2$ is the Euclidean distance representing the difference in driving goals. When the number of surrounding vehicles n < Nmax, the missing rows are padded with zero vectors.\nThe observation space is finally represented as:\n$O_i = concat [s_i^{self}, M^{nbr}]$\n\nD. Action space\nThe longitudinal control action set is defined as:\n$A^{lon} = {a^{acc}, a^{keep}, a^{dec}}$\nwhere $a^{acc} \u2208 R+$ represents acceleration, $a^{keep}$ denotes maintaining the current speed, and $a^{dec} \u2208 R$ represents"}, {"title": "IV. METHODOLOGY", "content": "A. Introduction of Reward Centering\nFirst, we describe the general idea of reward centering [19]. Reward centering involves subtracting the empirical mean of the rewards from the observed rewards, thereby achieving a mean-centered effect for the modified rewards.\nFor general reinforcement learning algorithms, we can perform a Laurent decomposition on the value function: The discounted value function can be decomposed into two parts, one of which is a constant that does not depend on the state or action, and thus does not affect the selection of actions. For a policy \u03c0 corresponding to the discount factor \u03b3, the tabular discounted value function h\u03b3 : S\u2192 R can be expressed as\n$h(s) = \\frac{r(\u03c0)}{1-\u03b3} + h_\u03c0(s) + e_\u03c0(s)$,\n(1)\nwhere r(\u03c0) is the state-independent average reward obtained by policy \u03c0, and h\u03c0(s) is the differential value of state s. For ergodic Markov decision processes, these two terms can be defined as Equation 2.\n$\\frac{1}{n} \\sum_{t=1}^{n} E[R_t | S_0, A_{0:t-1} ~ \u03c0]$,\nh\u03c0(s) = $E[\\sum_{k=1}^\\infty \u03b3^k(R_{t+k} - r(\u03c0)) | S_t = s, A_{t:\u221e} ~ \u03c0 ],$\nr(\u03c0) = $lim_{n\u2192\u221e}$ \n(2)\nhere, e\u03b3(s) represents an error term that approaches zero as the discount factor approaches 1. To distinguish the speed v, we do not use the common value function notation in reinforcement learning research but instead use h to denote the value function."}, {"title": "B. Reward Differentiation and Its Connection to Reward Centering", "content": "Next, we formulate differentiated reward and analyze its intrinsic connection to reward centering.\nDefine a Markov chain S = {S(t) : t = 0, 1, 2, ...}, whose state space is $R^e$, and S evolves according to a nonlinear state-space model:\nS(t+1) = a(S(t), N(t+1)), t\u2265 0, (4)\nUnder these assumptions, for all t \u2265 0, S(t + 1) is a continuous function of the initial condition S(0) = $s_0$.\nUnder the discount factor $\u03b3\u2208 (0,1)$, the discounted value function can be written as:\nh($s_0$) := $E[R(t) | R(0) = r_0]$, $r_0 \u2208 R$. (5)\n$h_\u03c4(0) = \\sum_{j=1}^{d} \u03b8_j \u03c8_j$,\n(6)\nwhere $\u03b8 = (\u03b8_1, \u03b8_2, ..., \u03b8_d)$, $\u03c8 = (\u03c8_1, \u03c8_2,..., \u03c8_d)$, and the given set of basis functions $4: R^e \u2192 R^d$ is assumed to be continuously differentiable. Indeed, most reward function expressions derived from the analytical relationships of vehicle dynamics satisfy this condition.\nThe goal of TD learning can be expressed as a minimum norm problem:\n$\u03b8^* = arg min ||h(0) - h^\u03c4||^2$.\n(7)\nThe goal of TD learning is to approximate h as an element of a function family {h(\u03b8) : \u03b8 \u2208 $R^d$}. Here, we assume that the discounted value function can be linearized in the following form:\nAssume that the value function $h^\u03c4$ and all its possible approximations {$h(0) : \u03b8 \u2208 $R^d$} are continuously differentiable as functions of the state s, i.e., for each $\u03b8 \u2208 $R^d$, $h^\u03c4$, h(\u03b8) \u2208 $C^1$. Based on the linear parameterization in (6), we obtain the following form of the differential value function:\n$\u2207h(\u03b8) = \\sum_{j=1}^{d} \u03b8_j \u2207\u03c8_j$,\n(8)\nwhere the gradient is taken with respect to the state s.\nAt this point, the goal of TD learning changes accordingly to:\n$\u03b8^* = arg min || \u2207h(0) - \u2207h ||$.\n(9)\nPaper [22] provides a detailed convergence proof of the problem represented by Equation (9). Clearly, in general reinforcement learning algorithms, directly using the differential reward function is essentially equivalent to solving the problem posed by Equation (9).\nIn steady-state traffic flow, if the following conditions hold:\n\u2022 Condition 1: The \u2207h is linearly correlated with the state transition direction E[St+1|St = s];\n\u2022 Condition 2: The reward differential weight \u03bb and the centered mean r(\u03c0) satisfy \u03bb\u00b7 \u2207h\u00a5 \u00d7 Rt,\nthen reward differentiation and reward centering are equivalent in the value function update, i.e.,\n$E_\u03c0 [\u0158_t + \u03bb\u00b7 \u2207h] \u2248 E_\u03c0 [\u0158_t]$.\nAt this point, both methods adjust the distribution of reward signals to make the value function focus more on the relative differences between states.\nThe effectiveness of reward centering has been demonstrated in paper [19]. Therefore, in reinforcement learning problems with steady-state transitions, the reward differentiation method is expected to improve performance."}, {"title": "C. Differentiated Reward Implementation in Vehicle Decision-Making", "content": "In the field of vehicle decision-making (multi-vehicle collaborative decision-making), the reward function commonly used can be expressed as Equation (10). [23]-[25]\n$R = w_1R^{speed} + w_2R^{intention} + w_3P^{collision} + w_4P^{LC}$\n$= \\frac{1}{N} (\\sum_{i=1}^N w_1 \\frac{v_i}{v_{max}} + w_2N_{sat} + w_3N_{col} + w_4N_{LC})$\n(10)\nwhere N is the number of vehicles in the scene (including HDVs and CAVs), $N_{onramp}$ is the number of vehicles passing through the intention area at the previous time step and aiming for the ramp, $N_{collision}$ is the number of collisions, and $N_{LC}$ is the number of frequently lane-changing vehicles.\nBased on the ideas proposed in this section, we have improved the reward function in Equation (10) as follows.\nDue to the variable number of agents in the setup, this paper cannot directly use the sum of the local rewards of all agents as the reward function. Instead, an index needs to be designed to objectively evaluate the overall traffic quality within the coordinating zone. To this end, we design the following external reward function:\n$r^{env} = \\frac{1}{N_{CAV}} \\sum_{i} (w_1r_i^a + w_2r_i^p) + w_3r^{flow} + w_4r^{safe}$ (11)\nwhere $r_i^a$ and $r_i^p$ represent the action reward and position reward for CAV i, respectively, $r^{flow}$ is used to evaluate the overall traffic flow speed, and $r^{safe}$ is the traffic safety indicator. The parameters $w_1,2,3,4$ are the weights assigned to each reward component. Specifically:\n$r_a = \\{\n1 & if accelerating or keeping highspeed,\\\\\n0 & otherwise.\n$\nWe design a potential field based on the vehicle's longitudinal position and the lane it occupies, to evaluate the value of the current position of vehicle i relative to its target.\n$f(x,y) = \\frac{e^{-\\frac{x^2}{\\sigma_x^2}}}{2\u03c0\u03c3_x^2} \u00b7 e^{-\\frac{(y_{tar}-y)^2}{\\sigma_y^2 + 1}}$\n(12)\nWhere $\u03c3_x$ and $\u03c3_y$ are the longitudinal and lateral decay coefficients, respectively. Based on the concept of reward differentiation, we define the position reward as:\n$r_i^p = v_i \u00b7 \u2207f(x, y)$\n(13)\nwhere $v_i \u2208 R = [-1, 1]$. In this paper, $v_i \u2208 {-1,0,1}$, so we have the discrete form of Equation (13),\n$r_i^p = (1 - \\frac{x}{x_{tar}}) + \\frac{sign(y-y_{tar}) \\cdot (y_{tar}-y)}{\u03c3_y^2 + 1} \u00b7 f(x,y)$ (14)\nhere, we define that when $y_i^2 = y_{tar}$, $sign (y - y_{tar}) = -1$. It can be observed that the terms $w_1r_i^a$ and $w_2r_i^p$ sometimes have the same reward effect. In this case, we treat them as strengthen of the action reward without making a more detailed distinction.\nWe use the overall speed of the traffic flow to evaluate the current traffic volume, which is:\n$r^{flow} = \\frac{1}{|N|} \\sum \\frac{v_i}{v_{max}}$\nFor the safety indicator, since accidents are rare events and their occurrence typically has a significant impact on overall traffic, we use summation rather than averaging.\n$r^{safe} = \\sum \u2161(i)$\nWhere II(i) is the indicator function, which equals 1 if vehicle i is involved in a collision, and 0 otherwise."}, {"title": "V. EXPERIMENT", "content": "A. Simulation environment and experiment settings\nThe experiments are based on the open-source microscopic traffic simulation platform SUMO (Simulation of Urban Mobility), which supports high-precision vehicle dynamics modeling, multi-agent collaborative control, and visual analysis of complex traffic scenarios. [26] The simulation scenario is a unidirectional branch of a bidirectional 8-lane highway (4 lanes in the same direction), with a straight road segment length of 250 meters, and a speed limit of 25 m/s (90 km/h). Traffic flow generation is implemented using SUMO's built-in flow module, which injects background vehicles following a Poisson process. The baseline traffic density is set to 250 vehicles per lane per hour, consistent with the traffic characteristics of urban arterial roads during off-peak hours. All vehicles are randomly assigned one of 3 objectives: going straight, turning left, or turning right, the target lane is the middle 2 lanes, the left most lane, and the right most lane, respectively.\nB. Compared Methods\nWe compare the training and deployment performance of widely used multi-agent reinforcement learning algorithms, including MADQN [23], MAPPO [27], and QMIX [28], by employing the generally adopted vehicle decision reward function (Equation (10)), the centering reward function (oracle centering in [19]), and the differentiated reward function (Equation (14)).\nAll algorithms share the following hyperparameter settings in TABLE II."}, {"title": "C. Evaluation Metric", "content": "Avg. Speed, reflecting overall traffic efficiency:\n$v = \\frac{\\sum_{t=0}^{END} v_t}{\\frac{1}{t=0}^{END}},$\nwhere $v_t$ is the average speed of all vehicles on road at time t.\nMin. Gap, quantifying potential collision risk:\n$C_{GAP} = \\frac{\\sum_{e=0}^{N_{epi}} g_{min, e}}{N_{epi}}$,\nwhere, gmin, e is the minimum gap of vehicle in test episode e, Nepi is the total number of test episodes.\nLane Change Frequency, evaluating the rationality of lane resource utilization:\n$F_{LC} = \\frac{\\sum_{t=0}^{END} N_{LC,t}}{\\sum_{t=0}^{END} N_{CAV,t}},$\nwhere $N_{LC,t}$ is the number of lane changes action of simulation step t, and $n_{CAV,t}$ is the CAV number of time t.\nSucc. Rate:\n$SR = \\frac{number of vehicles reach their target}{number of CAVs}.$\n\nD. Results and Comparison\nFigure 3 illustrates the training process of the algorithms. To better capture the early-stage convergence and later-stage stability, a logarithmic scale was applied to the horizontal axis.\nIt can be observed that in MADQN, the training curves for all three reward functions exhibit significant fluctuations, indicating poor stability. Despite tuning the algorithm to its optimal state, the performance of the three reward functions varies across different penetration rates. This instability is"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "This paper proposes a differentiated reward method for RL based multi-vehicle cooperative decision-making algorithms. By incorporating state transition gradient information into the reward function design, the method resolves the issue of distinguishing action values in steady-state traffic flow, a challenge commonly encountered with traditional reward mechanisms. Experimental results shows that the differentiated reward method significantly improves the training efficiency and decision quality of multi-agent algorithms, outperforming centering and general reward functions in core metrics such as traffic efficiency, safety, and action rationality. Moreover, the method maintains stable performance across different autonomous driving penetration rates (25%-100%), which shows good scalability and learning stability of our method. Future research directions include further optimization of reward function design, exploration of more complex traffic scenarios, and large-scale validation and application in real-world traffic systems."}]}