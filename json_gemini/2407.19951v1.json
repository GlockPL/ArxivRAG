{"title": "Can I trust my anomaly detection system?\nA case study based on explainable AI.", "authors": ["Muhammad Rashid", "Elvio Amparore", "Enrico Ferrari", "Damiano Verda"], "abstract": "Generative models based on variational autoencoders are a\npopular technique for detecting anomalies in images in a semi-supervised\ncontext. A common approach employs the anomaly score to detect the\npresence of anomalies, and it is known to reach high level of accuracy on\nbenchmark datasets. However, since anomaly scores are computed from\nreconstruction disparities, they often obscure the detection of various\nspurious features, raising concerns regarding their actual efficacy.\nThis case study explores the robustness of an anomaly detection system\nbased on variational autoencoder generative models through the use of\neXplainable AI methods. The goal is to get a different perspective on\nthe real performances of anomaly detectors that use reconstruction dif-\nferences. In our case study we discovered that, in many cases, samples\nare detected as anomalous for the wrong or misleading factors.", "sections": [{"title": "1 Introduction", "content": "The popularity of machine learning methods in difficult tasks, like the detection\nof anomalies in industrial quality-control processes, has witnessed a significant\nsurge over the past decade. Variational AutoEncoders paired with a Generative\nAdversarial Networks, commonly referred as VAE-GAN [1] models, are partic-\nularly prominent in this regard, due to their high potential in representation\nlearning. Anomaly Detection (AD) on image data with Deep Generative Mod-\nels (DGM) [2] operates on the premise that a model can be trained to learn a\nrepresentation of the normal features of a sample, while deliberately excluding\nthe capacity to represent and generate any anomalies. An anomaly score can then\nbe defined on the difference between the original image and its reconstruction,\nthus quantifying the representational gap for the sample abnormalities."}, {"title": "2 Literature review", "content": "AD is a well developed field, that has received a lot of attention due to its critical\nrole in numerous practical applications. Creating effective detection systems is\nchallenging due to several factors, like the difficulty of precisely define what an\nabnormality is within specific contexts, or the the lack of anomalous samples.\nFor these reasons, explaining the behaviour of an AD system remains a\ncomplex task. While general purpose interpretability techniques such as Grad-\nCAM [6], LIME [7] or SHAP [8, 9] are available, some scholars regard them as\nimprecise and unreliable [10]. Moreover, their application in the realm of anomaly\ndetection is inherently challenging, due to the lack of a probabilistic black-box\nfunction to explain. Nonetheless, these methodologies can be adapted to offer\ninvaluable insights into understanding the rationale behind the behavior of AD\nsystems. In this study we focus on LIME and SHAP systems, due to their (par-\ntially) comparable characteristics and their capability in localizing activation\nareas in anomaly maps. A broader recent review on AD systems is [11]."}, {"title": "3 Preliminaries", "content": "We describe the relevant preliminaries following the workflow depicted in Fig. 1.\nThe approach shares many similarity with [3]. Consider the problem for the"}, {"title": "3.1 VAE-GAN models", "content": "A Variational Autoencoder Generative Adversarial Network (VAE-GAN) com-\nbines [1, 17] the strengths of both variational autoencoders (VAEs) and genera-\ntive adversarial networks (GANs) [18]. A VAE-GAN consists of an encoder e, a\ndecoder d and a discriminator s. The encoder function $e : I \\rightarrow Z$ maps input\ndata, such as images I, to a lower-dimensional latent space $Z \\in R$, where each\npoint in Z represents a potential data sample. The decoder function $d : Z \\rightarrow I$\nestimates a potential input from a latent space representation, i.e. $d$ approxi-\nmates $e^{-1}$. Therefore, encoding and decoding an input image $\\xi$ results in its\nreconstruction (Fig. 1/C) through the latent representation z, given by\n\n$z = e(\\xi), $\n$\\xi' = d(z)$\n\nThe distribution of the latent space is learnt using a probabilistic approach,\nand adopts both a regularization of the latent distribution (usually Gaussian)\nand a GAN approach for adversarial (joint) training of both d and e using the\ndiscriminator functions (a classifier trained to distinguish between real and\ngenerated data). When encoded, each data point is described by a Gaussian\ndistribution, with mean $\\mu$ and (log)-variance $\\sigma$, from which new samples z can\nbe drawn."}, {"title": "3.2 Semi-supervised anomaly detection using variational models", "content": "While the task of identifying anomalies, particularly in image-based data, holds\nsignificant interest across various application domains [5, 19], creating effective\nanomaly detectors remains a challenge. Imbalanced datasets are common, with\nanomalous data being significantly underrepresented (due to the infrequency of\nanomalous events). Furthermore, the definition of what constitutes an anomaly is\noften ambiguous, making supervised learning approaches impractical. Therefore,\na relevant approach is based on the use of semi-supervised learning, where models\nare trained to detect anomalies from \"normal\" data only. Several approaches are\npossible to perform anomaly detection in a semi-supervised way [20], and in this\nstudy we consider a VAE-GAN-based approach [21].\nA VAE-GAN model (e, d, s) for AD is trained exclusively on \u201cnormal\u201d data,\nensuring that only normal data has a proper representation in the latent space.\nConsider an input image $\\xi$, and let $\\xi' = d(e(\\xi))$ be its encoding-decoding through\nthe VAE-GAN model. If the sample is normal and lies in-distribution with the\nmodel, it should be reconstructed accurately, with minimal reconstruction errors."}, {"title": "3.3 Explaining anomaly maps using model-agnostic XAI methods", "content": "While anomaly maps reveal the reconstruction errors, they only provide a su-\nperficial indication of potential anomaly areas within the input image, lacking\nprecise localization of anomalies. To address this limitation, XAI methods have\nbeen adopted to help in localizing these areas for anomalous samples. We fo-\ncus on model-agnostic methods based on perturbations of input data. Although\nmany XAI methods rely on classifier predictions, reconstruction-based AD does\nnot inherently provide such probability scores, and a special setup is needed [3,\np. 4.1]. We consider two XAI methods, LIME and SHAP, adapted as described.\nLIME. The Local Interpretable Model-Agnostic Explanations [7], is a method\nfor explainable AI that works by creating a simpler, interpretable model that\napproximates the behavior of a more complex model in a synthetic neighborhood\nof a particular instance being explained. Let $f: I \\rightarrow R$ be a prediction regression\nfunction that assigns probability scores to input images $\\xi \\in I$. LIME produces an\nhigh-level explanation consisting of feature attributions (i.e. real-valued scores)\nassigned not at the pixel-level, but at the level of k << (w$\\cdot$h) superpixels. These"}, {"title": "3.4 Comparing explained anomalies against a ground truth", "content": "A pixel-level feature attribution explanation $\\beta$ generated by an XAI method is a\nreal matrix of feature attribution scores assigned to the pixels of the image. To\nassess the method's capability of localizing the anomalous regions in an input im-\nage, we adopt the following methodology. A Boolean ground truth $\\gamma \\in {0,1}^{h \\times w}$\nis a matrix that assigns, to each pixel of the input image $\\xi$, a value whether the\npixel belongs to the anomaly being localized or not (Fig. 1/I).\nWe assume that $\\gamma$ is available for the anomalous samples of the test set.\nSince the explanation $\\beta$ is a real-valued matrix, it is not directly comparable\nwith $\\gamma$. An effective way to perform such comparison is to define an explanation\nthreshold $\\theta$, and define a boolean explanation $\\gamma'$, derived from $\\beta$, that marks as\nanomalous those pixels of $\\xi$ for which the feature attribution score in $\\beta$ is greater\nthan $\\theta$. A comparison between $\\gamma$ and $\\gamma'$ can then be performed using standard\nmetrics like the Jaccard coefficient (a.k.a. Intersection over Union - IoU)\n\n$J(\\gamma, \\gamma') = \\frac{\\gamma \\land \\gamma'}{\\gamma \\lor \\gamma'$\n\nHowever, determining an optimal threshold $\\theta$ is not straightforward. Hence, we\nselect, for each explained sample, a corresponding optimal threshold $\\theta*(\\xi)$ for\nwhich $J(\\gamma, \\gamma')$ is maximal (Fig. 1/J). The mismatch between $\\gamma$ and $\\gamma'$ can then\nbe inspected and visualized\u00b3 (Fig. 1/K). Note that this coefficient can only be\ncomputed when $\\gamma$ is available and it is not empty (otherwise it would be mean-\ningless). Thus it can be used only to explain anomalies for \u201cabnormal\u201d samples,\nbut it cannot be used on \"good\" samples."}, {"title": "4 Experimental evaluation", "content": "We present the results on a set of experiments made on the MVtec dataset [5]\nand considering two categories, hazelnut and screw, each comprising images of\nthese objects with and without defects. The tests use a VAE-GAN model im-\nplemented in Keras [25], where the encoder model e uses 4 nested convolutional\nlayers (3$\\times$3 kernel, stride 2), with each layer using ReLU activation and followed\nby a batch normalization, and using a final Dense decision layer. The discrimi-\nnator s is similar to e, but using three convolutional layers with larger kernels"}, {"title": "5 Conclusions", "content": "In this case study we replicated the framework of [3], enhancing it by quantifying\nboth AD and XAI performances. Our aim was to highlight the relevance of XAI\nmethods in finding the true drivers behind anomaly detection, particularly when\nutilizing reconstruction error maps generated from VAE-GAN models.\nThe results show that relying solely on the anomaly score is insufficient for\ncomprehending the classification process. A sample may be detected as anoma-\nlous for the wrong reasons, yet this misbehaviour may not be detectable from the\ninformation provided by the anomaly map alone. We used two model-agnostic\nXAI methods to obtain explanations from the anomalous samples, to inspect\nif the anomalies were correctly localized. Region localization through a \u03a7\u0391\u0399\nmethod with Jaccard score maximization allows the user to inspect the AD sys-\ntem, identifying potential misbehaviors in the detection and providing a better\nunderstanding of the system.\nBoth tested XAI methods successfully localizes activation regions, with some\ndiscrepancies. Specifically, LIME exhibited a slightly inferior performance com-\npared to SHAP, attributable to its reliance on a pre-determined segmentation\nthat is not aware of the ML process and does not get any feedback from it. This\nfragility can be seen by the variations between the S1 and S2 test setups (like in\nFig. 3/C1-C2)."}]}