{"title": "A Low-Resolution Image is Worth 1x1 Words: Enabling Fine Image\nSuper-Resolution with Transformers and TaylorShift", "authors": ["Sanath Budakegowdanadoddi Nagaraju", "Brian Bernhard Moser", "Tobias Christian Nauen", "Stanislav Frolov", "Federico Raue", "Andreas Dengel"], "abstract": "Transformer-based Super-Resolution (SR) models have\nrecently advanced image reconstruction quality, yet chal-\nlenges remain due to computational complexity and an\nover-reliance on large patch sizes, which constrain fine-\ngrained detail enhancement. In this work, we propose Tay-\nlorIR to address these limitations by utilizing a patch size\nof 1x1, enabling pixel-level processing in any transformer-\nbased SR model. To address the significant computa-\ntional demands under the traditional self-attention mech-\nanism, we employ the TaylorShift attention mechanism, a\nmemory-efficient alternative based on Taylor series expan-\nsion, achieving full token-to-token interactions with linear\ncomplexity. Experimental results demonstrate that our ap-\nproach achieves new state-of-the-art SR performance while\nreducing memory consumption by up to 60% compared to\ntraditional self-attention-based transformers.", "sections": [{"title": "1. Introduction", "content": "Image Super-Resolution (SR) remains a foundational\nyet significant low-vision challenge, aiming to reconstruct\nHigh-Resolution (HR) images from Low-Resolution (LR)\ninputs. The applications encapsulated by SR are broad,\nspanning security, medical imaging, and even astronomi-\ncal analysis [20, 21]. Despite the powerful advances made\nwith deep learning, limitations persist, especially regarding\nhigh-frequency detail enhancements [9, 13, 19].\nWith the introduction of deep learning, SR methods have\nleaned heavily on Convolutional Neural Networks (CNNs)\n[12, 30, 32], delivering impressive performance. Short after,\ntransformer-based architectures have demonstrated an ap-\ntitude for capturing intricate relationships across input se-\nquences, making them a dominant choice for regression-\nbased image SR [6, 25]. Prominent transformer-based ar-\nchitectures are SwinIR [11], Restormer [27], and HAT [3],which have demonstrated promising gains, applying self-\nattention mechanisms for precise context-aware upscaling.\nYet, transformer-based SR methods face notable chal-\nlenges: high memory requirements and quadratic time com-\nplexity associated with self-attention, limiting practicality\nfor real-time and large-scale applications. As a result, cur-\nrent methods reduce the contextual scope within which at-\ntention is operating, e.g., 8\u00d78 windows, and sometimes op-\nerate within these windows with patch-sizes greater than\n1\u00d71, leading to non-pixel level detail enhancement. This\nrestriction compromises the ability to capture fine-grained\ndependencies across the entire image.\nTo address these issues, we introduce TaylorIR, a novel\ntransformer-based SR approach that makes use of pixel-\nlevel detail refinement. Moreover, it substitutes traditional\nself-attention with TaylorShift [22] attention, a memory-\nefficient mechanism inspired by Taylor series expansion"}, {"title": "2. Background", "content": "This Section explains the standard attention mechanism\n[25] and the attention mechanism used in the TaylorIR,\nnamely TaylorShift [22]. For TaylorShift, we present its\ntwo variants: Direct- and Efficient TaylorShift."}, {"title": "2.1. Classical Self-Attention", "content": "In transformers, the attention mechanism is crucial for\ndetermining how much each element in a sequence should\n\"pay attention\" to every other element. Traditionally, vi-\nsion transformers use self-attention, where each element in\na sequence of length N attends to every other element. Self-\nattention consists of the following steps:\n1. Query-Key Similarity: Computing the dot product\nbetween the queries Q and keys K:\n$A = QK^T \\in \\mathbb{R}^{N\\times N}$.\n2. SoftMax Activation: Normalizing the attention\nscores using the SoftMax function:\n$\\text{softmax}(A)_{ij} = \\frac{\\exp(A_{ij})}{\\sum_k \\exp(A_{ik})}$.\n3. Value Aggregation: Multiplying the normalized at-\ntention scores by the value matrix V to get the output\n$Y = \\text{softmax}(A) \\cdot V$.\nOverall, the time complexity of classical self-attention is\n$O(N^2d)$, which becomes computationally expensive, par-\nticularly for long sequences."}, {"title": "2.2. TaylorShift Attention", "content": "The TaylorShift attention mechanism is a novel variant\nthat leverages the Taylor series approximation to compute\nattention scores with the goal of improving computational\ncomplexity compared to the standard attention mechanism."}, {"title": "2.2.1 Direct-TaylorShift", "content": "At the heart of TaylorShift [22] is the Taylor-Softmax,\nwhich approximates the softmax function. The Taylor-\nSoftmax is derived from the Taylor series expansion of\nthe exponential function, where the exponential is substi-\ntuted with its 2nd-order Taylor approximation. The Taylor-\nSoftmax generates a valid probability distribution, as all\nterms remain positive and sum to one after the normaliza-\ntion.\nDirect-TaylorShift utilizes the Taylor-Softmax to di-\nrectly replace the softmax computation. Step 2 of the at-\ntention calculation (Equation 2) thus becomes\n$T-\\text{SM}(A)_{ij} = \\frac{1 + A_{ij} + \\frac{1}{2} (A_{ij})^2}{\\sum_k 1 + A_{ik} + \\frac{1}{2} (A_{ik})^2}$.\nThis computation avoids the exponential function, making\nit slightly faster. However, the computational complexity\nremains at $O(N^2d)$."}, {"title": "2.2.2 Efficient TaylorShift", "content": "If the sequence length N exceeds a certain threshold, the\nEfficient-TaylorShift attention mechanism is recommended.\nEssentially, it distributes the approximation of the exponen-\ntial function across the matrices Q and K and moves the\nnormalization step after the multiplication by V. While\nthe mathematical result remains unchanged, this reordering\nshifts computational complexity from $O(N^2d)$ to $O(Nd\u00b3)$,\neffectively making the sequence length less impactful on\nruntime by shifting the influence towards the internal di-\nmension d."}, {"title": "3. TaylorIR", "content": "In this Section, we introduce TaylorIR, a method for\ntransformer-based SR models designed to perform efficientand fine-grained image SR by leveraging pixel-wise patch\nembeddings and the memory-efficient TaylorShift [22] at-\ntention mechanism to support long sequences."}, {"title": "3.1. Pixel-Wise Patch Embedding", "content": "In most transformer-based models, images are divided\ninto patches of sizes such as 4\u00d74 or 8\u00d78, which is suf-\nficient enough for discriminative tasks as exemplified by\nViT [6]. The rationale behind this configuration is to re-\nduce the resulting sequence length and, thereby, the compu-\ntational burden. However, larger patch sizes also assume\nspatial coherence across pixels within each patch, which\nis an assumption that does not apply to low-vision tasks\nlike image SR. Such tasks usually require fine-grained de-\ntail restoration from LR inputs, where each pixel has to be\nreconstructed with precision [20].\nAs a result, some approaches try to embed the input as\n1\u00d71 pixels by utilizing windows to bind the attention mech-\nanism to a limited contextual scope. This inherently leads to\npixel-level detail enhancement and short sequences. How-\never, it is a suboptimal solution, as windows reduce how\nmuch information a pixel can get from its surroundings. Ex-\namples are SwinTransformers [14] and their adaptions like\nSwinIR [11], SwinFIR [29], and HAT [3].\nIn a similar spirit, our proposed method, TaylorIR, ad-\ndresses the same core limitation by embedding the input as\na 1\u00d71 patch. As a result, TaylorIR treats each pixel as an\nindependent entity, allowing for precise and individualized\nfeature extraction across the entire image. Additionally,\nTaylorIR also introduces TaylorShift [22] attention to ease\nthe computational burden, especially their memory con-\nsumption, associated with significantly enlarging the win-"}, {"title": "3.2. Integrating TaylorIR into SwinIR", "content": "We integrate TaylorIR into SwinIR, which we coin Tay-\nlorSwinIR. The choice of SwinIR is rooted in its strong rep-\nresentation of state-of-the-art transformer-based SR mod-\nels, such as HAT and SwinFIR [3, 29]. SwinIR is initially\nrooted in Swin Transformer [14] and Vision Transformer\n(ViT) [7], which typically use 4\u00d74 and 16 \u00d7 16 patches,\nrespectively.\nFollowing the same argumentation, SwinIR applies\npixel-wise patch embedding but limits the window size, i.e.,\nhow many patches fall under the attention mechanism, to\n8\u00d78, which restricts each self-attention operation to 64 to-\nkens. In our TaylorIR-based extension, we reconfigure the\nwindow size from 8\u00d78 to 48\u00d748, allowing for 2304 instead\nof 64 tokens per window, well above the typical sequence\nlength threshold for efficient memory usage under Taylor-\nShift (i.e., $Nd\u00b3 \\ll N\u00b2d$).\nThis expansion allows SwinIR to capture global contex-\ntual information more effectively, addressing a major lim-\nitation in handling long sequences. TaylorShift attention\nenables this larger window to be processed efficiently with\nlinear time and space complexity, optimizing memory use\nwithout compromising model performance.\nWe implement TaylorShift in place of the original win-\ndowed attention of SwinIR, adjusting parameters as speci-\nfied by the original work of TaylorShift. This integration not\nonly enables more effective sequence processing in SwinIR\nbut also enhances overall memory efficiency, broadening its\npotential in SR tasks requiring extensive contextual infor-\nmation. The result is a scalable approach that leverages the\narchitecture of SwinIR while achieving high-performance\nSR with substantial gains in computational efficiency."}, {"title": "4. Experiments", "content": "This Section introduces our experiments conducted to\nassess the effectiveness of TaylorIR by applying it to\nSwinIR (see subsection 3.2). First, we describe the setup\nand analyze the impact of the integration of TaylorIR."}, {"title": "4.1. Experimental Setup", "content": "We use DIV2K [1] as a training dataset and follow the\nstandard procedure by extracting sub-images of 192 \u00d7 192\nfor training. For testing, we employed the datasets Set5\n[2], Set14 [28], BSDS100 [15], Manga109 [16], and Ur-\nban100 [10]. Like standard in image SR, we evaluated four\ndifferent scaling factors: x2, x3, and \u00d74."}, {"title": "4.2. Analysis of Self-Attention vs. TaylorShift", "content": "We analyzed three configurations\u2014SwinIR (original),\nSwinIR (fine), and TaylorSwinIR\u2014on the DIV2K dataset,\ninvestigating the impact of large window sizes and differ-\nent attention mechanisms on both computational complex-\nity and memory efficiency (under the scaling factor of \u00d72)."}, {"title": "4.2.1 Impact on Computational Complexity", "content": "SwinIR (original) uses a window size of 8\u00d78, achieving\na high throughput of 6.1203 images/sec. The small win-\ndow size allows it to capture local similarities efficiently,\nminimizing computational costs while keeping the model\nlightweight. This configuration suits tasks where memory\nconstraints are critical, though it limits the receptive field,\npotentially reducing its ability to capture global context.\nTo increase contextual scope, we applied a larger win-\ndow size of 48\u00d748 in SwinIR (fine). While this ex-\npansion allows for more prosperous feature extraction, its\nthroughput drops dramatically to 0.1230 images/sec due to\nthe quadratic complexity of traditional self-attention. This\nsetup captures detailed global context as indicated by the\nhigher PSNR/SSIM scores but at the cost of computational\nefficiency, making it challenging to deploy in resource-\nconstrained environments.\nIn contrast, TaylorSwinIR, which integrates TaylorIR\nachieves a slight improvement in throughput (0.1238 im-\nages/sec) over SwinIR (fine) while using the same window\nsize of 48\u00d748. Thanks to TaylorShift attention, we expect"}, {"title": "4.2.2 Impact on Memory Efficiency", "content": "Regarding memory, TaylorSwinIR substantially improves\nmemory efficiency compared to SwinIR (fine) while main-\ntaining a similar performance level. Overall, TaylorSwinIR\nconsumes significantly less memory across all datasets,\nwith reductions ranging from 37% to 85%, achieving an av-\nerage memory savings of 59% compared to SwinIR (fine).\nThis optimization enables TaylorSwinIR to handle large\nwindow sizes effectively without the memory overhead typ-\nically associated with such configurations.\nFor example, in Urban100, TaylorSwinIR requires 49.1\nGB of memory compared to 78.5 GB for SwinIR (fine) -\na reduction of nearly 38%. This memory advantage makes\nTaylorSwinIR particularly well-suited for SR tasks where\nmemory constraints would otherwise prevent the use of\nlarge windows."}, {"title": "4.2.3 Trade-Offs Between Efficiency and Performance", "content": "TaylorSwinIR slightly lags in performance metrics like\nPSNR and SSIM compared to SwinIR (fine). However,\nit surpasses the original SwinIR baseline in almost everycase. This trade-off highlights TaylorIR as an effective bal-\nance between computational efficiency and performance. It\ndelivers enhanced contextual representation with large win-\ndows while maintaining memory efficiency, positioning it\nas an ideal choice for high-accuracy scenarios without ex-\ncessive resource demands.\nIn summary, TaylorSwinIR bridges the gap between\nSwinIR (original) and SwinIR (fine) by leveraging Tay-\nlorShift attention to retain computational efficiency with\nreduced memory consumption, making it a powerful tool\nfor tasks requiring extensive context at manageable com-\nputational costs. This design offers significant advantages\nfor real-world applications where both performance and re-\nsource constraints must be carefully balanced."}, {"title": "4.3. State-of-the-Art Comparison", "content": "To demonstrate the effectiveness of TaylorSwinIR in\nclassical image SR benchmarks, we compare it against\nprominent state-of-the-art models including RCAN [31],\nRRDB [26], SAN [5], IGNN [34], HAN [24], NLSA [18],\nand the baseline SwinIR model [11]. Evaluations were con-\nducted across multiple datasets (Set5, Set14, BSD100, Ur-\nban100, and Manga109) and scaling factors (x2, x3, and\n\u00d74) to ensure comprehensive analysis."}, {"title": "4.4. Contextual Scope", "content": "Figure 3 demonstrates the main advantages of TaylorIR\nunderlying its improved reconstruction quality: the ex-\ntended contextual scope. We demonstrate this by visualiz-"}, {"title": "4.5. Qualitative Results", "content": "In Figure 4, Figure 5 and Figure 6, we present visual\ncomparisons across different scaling factors (x2, x3, and\n\u00d74) between the benchmark model SwinIR and our pro-\nposed TaylorSwinIR approach. Notably, in Figure 5, Tay-\nlorSwinIR demonstrates clear improvements at the \u00d73 scale\nfor the Manga109 image, MiraiSan, where it produces sig-\nnificantly sharper and more detailed results.\nTaylorSwinIR consistently achieves subjectively compa-\nrable or superior image quality relative to SwinIR across all\ntested scales. These gains are attributed to the expanded\nreceptive field, which enhances the model's ability to cap-\nture richer contextual information. Additionally, Taylor-\nSwinIR demonstrates optimized memory efficiency, which\nnot only supports high-quality image reconstruction but also\nmakes it more computationally viable. This combination\nof improved super-resolution quality and reduced memory\nconsumption highlights TaylorSwinIR as an effective and\nresource-efficient solution for detailed image enhancement."}, {"title": "5. Limitations & Future Work", "content": "While TaylorIR demonstrates enhanced memory effi-\nciency and performance in image super-resolution, several\nlimitations remain. Although TaylorShift attention signifi-\ncantly reduces computational complexity, the computation\nstill face substantial memory demands for extremely high-\nresolution images.\nFuture work could explore hybrid models that combine\nthe strengths of convolutional and transformer architectures\nto improve performance across diverse image types. An-\nother promising direction would be the optimization of Tay-\nlorIR for real-time applications on edge devices by incorpo-\nrating quantization and model compression techniques."}, {"title": "6. Conclusion", "content": "In this work, we introduced TaylorIR, a method to ex-\ntend transformer-based SR models by replacing the stan-\ndard self-attention mechanism with TaylorShift attention\nand enabling long sequences, primarily through 1\u00d71 patch\nembeddings, to be processed faster and significantly more\nmemory efficient. More specifically, our proposed method\nallowed us to scale up SwinIR with a 48x48 window,\nresulting in a sequence length of 2304, capturing more\nglobal context, which leads to enhanced image quality\nwhile maintaining memory efficiency - a feat unattainable\nwith traditional attention mechanisms. TaylorSwinIR, the\nTaylorIR-extended SwinIR architecture, achieves superior\nPSNR and SSIM scores across classical test benchmarks\nlike Set5, Set14, BSD100, Urban100, and Manga109, sur-\npassing state-of-the-art models, including SwinIR. This re-\nduced memory footprint is particularly meaningful for de-\nploying SR transformers in resource-constrained environ-\nments, bringing high-quality SR closer to real-time applica-"}]}