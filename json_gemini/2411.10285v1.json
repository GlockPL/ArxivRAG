{"title": "Systolic Arrays and Structured Pruning Co-design for Efficient Transformers in Edge Systems", "authors": ["Pedro Palacios", "Rafael Medina", "Jean-Luc Rouas", "Giovanni Ansaloni", "David Atienza"], "abstract": "Efficient deployment of resource-intensive transformers on edge devices necessitates cross-stack optimization. We thus study the interrelation between structured pruning and systolic acceleration, matching the size of pruned blocks with the systolic array dimensions. In this setting, computations of pruned weight blocks can be skipped, reducing run-time and energy consumption, but potentially impacting quality of service (QoS). To evaluate the trade-offs between systolic array size and sparsity opportunities, we present a novel co-design framework that integrates algorithmic optimization, system simulation, and hardware design. Targeting speech recognition using transformers as case study, we analyze how configuration choices across the stack affect performance metrics. Results demonstrate that structured pruning on systems featuring systolic array acceleration can effectively increase performance, while maintaining high QoS levels. Up to 26% system-wide speedups due to structured pruning were measured, with only 1.4% word error rate degradation on the standard Librispeech dataset.", "sections": [{"title": "I. INTRODUCTION", "content": "Transformers have fostered a revolution in machine learning, with applications ranging from classification [1] to generative models for text and images [2], to speech recognition [3]. However, their complex structure based on multiple attention and feed-forward layers [4] results in unprecedented computational requirements, posing significant challenges for their deployment. These are particularly acute in edge scenarios, where systems have to operate within constrained energy and performance envelopes.\nIn this context, a plethora of optimization strategies have been proposed. On the software side [5], commonly used approaches involve reducing the precision of data representations (quantization) and removing parts that contribute the least to inference outcomes (pruning). As for hardware, efforts have mainly focused on the acceleration of the main computational kernel in transformers, i.e. General Matrix Multiplications (GEMMs). Although diverse solutions ranging from analog crossbars [6] [7] to near-DRAM computing [8] [9] work toward this goal, a particularly promising alternative is represented by systolic arrays [10]. These two-dimensional meshes of processing elements can indeed parallelize the computation of a GEMM (or, more precisely, the computation of a GEMM tile), while presenting a high parallelism degree, low resource requirements and only mandating a simple, low-overhead control logic.\nRecent works [11]-[15] have attempted to co-optimize software algorithms and hardware accelerators dedicated to transformer inference [16]. Such a stance is particularly appealing at the crossroads of model pruning and systolic array acceleration. On the software side, pruning can be performed by eliding weights in regular block patterns (in a \u201cstructured\" way) rather than as individual elements [17]. While this approach introduces a constraint to pruning, and can hence result in lower overall sparsity rates, it substantially amplifies hardware-side optimization opportunities when matching the sizes of the pruned tile and the accelerator mesh. The exploration of this strategy, which we term Systolic Array Structured Pruning (SASP), is the focus of this work.\nSASP opens a complex multidimensional design space which requires careful consideration of metrics spanning from hardware to algorithms. Indeed, while a larger accelerator can expose a higher degree of parallelism, it also requires more resources (area / energy). Moreover, SASP settings with larger tiles may overly penalize the achievable sparsity for a desired Quality of Service (QoS) or, alternatively, result in high QoS degradation for a fixed pruning rate.\nTo explore these interrelations, we employ a holistic approach integrating methods for a) the structured pruning of transformer algorithms, b) the system-level level modeling of accelerated systems executing them, and c) the hardware synthesis of accelerators. Our environment for SASP exploration builds on frameworks for the training of transformers (ESPnet [3]) and for system simulation (gem5 [18]). By employing a novel systolic array architectural template, it supports both floating point and weight-quantized data representations, as supported by ESPnet.\nAs a test case, we employ our exploration approach to analyze a speech recognition application, based on an 24-block, 75M-parameter transformer processing the LibriSpeech dataset [19]. We observed that SASP can achieve, for a systolic array size of 32 \u00d7 32, up to 44% speedup and 42% energy savings over a non-pruned, non-quantized system when employing a 20% pruning rate, resulting in a marginal Word Error Rate (WER) degradation of 1.4%.\nThe contributions of this paper are summarized as follows:\n\u2022 We introduce a methodology for the systematic exploration of Systolic Array Structured Pruning (SASP), a co-design strategy that combines systolic array acceleration and structured pruning with matching accelerator and tile size.\n\u2022 We show how the insights collected from our framework enable the evaluation of figures of merit at different abstraction levels, including the assessment of QoS, performance, resource usage, and energy, as well as their trade-offs. We discuss how these can be effectively leveraged from the joint perspective of algorithmic optimization, system integration, and systolic array design.\n\u2022 Using a speech recognition case study, we show that SASP-based co-optimization of transformers and systolic arrays can lead to efficiency and speedup gains of up to 26% with minimal QoS impact."}, {"title": "II. STATE OF THE ART", "content": "By providing spatially-distributed computation with low control logic overhead, systolic arrays can effectively parallelize the execution of matrix multiplications, the dominant computing pattern in transformer inference [10]. To evaluate the benefits that systolic arrays can induce, SMAUG [15] and TiC-SAT [20] present system simulation infrastructures able to support complete inferences, including both their hardware-accelerated and their software-executed parts. These works showcase that even small-sized systolic arrays have the potential to reduce run-time by orders of magnitude. Performance is further improved when data is properly laid out in a tiled arrangement in memory according to the accelerator characteristics, in order to maximize spatio-temporal locality. Although such an approach has been adopted [11], [21], no attempt was made therein to prune computations as we do here with Systolic Array Structured Sparsity.\nIndeed, the ductility of DNN models makes them highly amenable to pruning. In the context of systolic array acceleration, pruning optimizations are categorized as either fine-grained or structured [16]. In the first case, specialized systolic arrays have been proposed which can leverage the presence of zero values in tiles by either clock gating processing elements [22] or by reordering operands [23], [24]. Nonetheless, fine-grained pruning requires a fair amount of control logic overhead in the accelerator design and impacts the regularity of data layout in memory, which may negate the intended benefits [12], [25].\nIn this light, structured pruning strategies offer a promising alternative, as tiles of low-significance can be entirely skipped before processing them onto accelerators, when the tile size matches the target accelerator parallelism. Hence, speedups can be harnessed without requiring specialized hardware for sparsity management. This position has been adopted by previous works [26]\u2013[30]. However, they only provide a partial view of the ensuing design space. In particular, [26], [28] and [27] adopt a system-level stance, exploring the potential for acceleration of co-designed pruning strategies and data-parallel accelerators, but do not investigate the impact on Quality of Service (QoS, e.g. accuracy, word error rate) of the performed pruning. Conversely, [29] and [30] provide an algorithmic-level assessment of the effect of structured sparsity, but neglect the hardware and architectural implication of adopting a matched accelerator design. To the best of our knowledge, a holistic view of the algorithmic-to-hardware space exposed by SASP is hence missing. Our paper aims at filling this gap."}, {"title": "III. CO-DESIGNING ACCELERATORS AND SPARSITY", "content": "The dimensions of the design space exposed by Systolic Array Structured Pruning (SASP) solutions are illustrated in Fig. 1. The illustrated figures of merit reside at widely different layers of the hardware/software stack. Hence, to enable co-optimization, we developed the integrated toolflow illustrated in Fig. 2. The input to our framework is a trained transformer model and a target dataset, which, in our implementation, are defined via the ESPnet toolkit for automatic speech processing [3]. Hyper-parameters determine the size of the SASP tile (which sets the granularity of structured sparsity as well as the size of the systolic array) and the target sparsity rate. Moreover, both floating-point and weight-quantized implementations are supported.\nThe co-design framework is structured in 3 tiers: in its upper tier, PyTorch APIs are employed to perform pruning and (optionally) quantization. Then, system simulation is employed to gather run-time statistics of the application executing on a virtual system featuring systolic acceleration. Finally, an RTL-level architectural template is used to gather hardware metrics such as energy and area. The implementation of each tier is detailed in the following."}, {"title": "A. Structured Pruning and Quantization", "content": "We base our strategy on the observation that matrices employed by transformer models are much larger than the size of systolic arrays. Hence, operations to perform GEMM must be computed in a tiled fashion. As in [20], we herein consider a weight-stationary scenario, in which a tile of parameters is stored in the systolic array, and partial results are computed by streaming inputs/outputs to/from the accelerator. These are eventually aggregated via element-wise addition. As shown in Fig. 3, in this setting, a tile containing only zero values can be completely skipped, saving both the time required to configure the systolic array and the time required to calculate the related partial results. In the example in Fig. 3 it can be observed that the sparsity induced by the red weight tile lowers the workload required for the computation of the entire shaded column in the output.\nWe enforce structured sparsity by zeroing a percentage of tiles with the lowest L1-norm (sum of absolute values) across the entire model. This approach allows to heterogeneously prune GEMMs according to their sensitivity. In particular, feed-forward GEMMs are much more amenable to pruning than attention ones, so we focus on these for our exploration in Section IV-C.\nAfter sparsification, post-training quantization can optionally be used to reduce the representation precision of weights from 32-bits floating point (FP32) to 8-bits integer (INT8). Finally, inference is performed on a target dataset, in order to gather QoS metrics such as Word Error Rate (WER)."}, {"title": "B. Full System Simulation", "content": "Run-time statistics on the deployment of the SASP-pruned models are collected in the gem5 [18] simulation environment, which allows specifying complex systems including hardware (processors, memory hierarchy) and software (operating system) components. To this end, we developed a systolic array gem5 module interfaced as a functional unit. Similarly to [20], the functional unit employs dedicated instructions, extending the ARM instruction set, to a) program weights, b) emulate the systolic array computation, and c) stream inputs/outputs (see Fig. 4). We assume a 32-bit input-output interface, allowing to transfer one input and one output activation per custom instruction. As for weights, either a single FP32 or four INT8 values can be programmed in the array in a quantized or non-quantized setting, respectively.\nThe implemented instruction set extensions can be employed to accelerate user-level applications via inline assembly pragmas. For convenience, we wrapped these in parametric library functions, allowing to transfer a weight tile or compute a partial GEMM with a single function call. In this way, we gathered the run-time characteristics of executing entire transformer layers under varying architectural and sparsity settings."}, {"title": "C. Systolic Array Architecture", "content": "The systolic array hardware implementation, depicted in Figure (Fig. 4), comprises a mesh of processing elements (PEs) with nearest-neighbor connections. Inside each, an adder and a multiplier implement a MAC operation between input activation, weight and partial result values, the latter being stored in an accumulation register. Notice that inputs are streamed left-to-right, partial results flow from top to bottom, and weights are instead stationary. At the periphery of the array, shift registers of varying depth are employed to skew data along a diagonal, properly aligning inputs and outputs. Instances of the template can be derived by providing architectural parameters defining its size and the desired data format (either FP32 for weights and activations, or using INT8 weights and FP32 activations). In both versions of the PE, the multiplier and adder are pipelined to meet timing requirements. The pipeline latency is entirely hidden by the activations I/O latency from/to the systolic array. The instances are fully synthesizable using standard digital IC design tools and logic cell libraries.\nBoth in non-quantized and weight-quantized settings, adders in PEs support FP32 representation both for operands and for the results. Conversely, multipliers can be highly optimized in the weight-quantized case, as they must only support simpler FP32_INT8 arithmetic. A diagram of the hybrid FP32_INT8 multiplier design adopted in our architectural systolic array template is presented in Fig. 5. This implementation correctly computes the multiplication result, except for the case where either of the inputs equal to 0. We handle this as a special case, by employing a dedicated multiplexer. Moreover, to optimize area and energy efficiency, infinities, NaNs, and subnormal numbers are not handled.\nIn detail, our design assumes that the INT8 weight is represented using a sign-and-magnitude format. Hence, the output sign is computed as the XOR of the activation and weight signs. Then, the FP32 mantissa is expanded by appending the leading \u20181', which is implicit in the IEEE format. Furthermore, the expanded mantissa is multiplied by the magnitude of the weight value (INT8). The resulting unaligned output mantissa is right-shifted to align the leading '1' and truncated to 23 bits. Finally, the output exponent is adjusted according to the number of performed mantissa shifts.\nNote that the hybrid multiplier design readily generalizes to different floating-point and integer bitwidths beyond the FP32_INT8 considered in this paper, e.g., to support FP16 activations."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We evaluate our co-design approach on the LibriSpeech ASR corpus [19], using a transformer model implemented with ESPnet [3] using PyTorch [31], and targeting the encoder for SASP optimization, since its execution dominates run-time."}, {"title": "A. Setup", "content": "The model structure corresponds to the parameters shown in Table I. The model is trained using 960 hours of the train set with speech perturbation (3\u00d7 speed) [19] for 100 epochs and achieves 3.4% WER on both the development and test subsets (about 5 hours each). Further WER results in this paper are reported on the test subset. System simulations were run in the gem5-X variant [32] of the gem5 simulator [18]. We considered a single-core configuration having a 2-level cache hierarchy and running at 1 GHz, as detailed in Table II. Hardware syntheses of systolic array instances targeted the same 1 GHz timing constraint, and employed a TSMC 28nm technology node. Floating point arithmetic operators (adders and multipliers) were derived from the FPxx library using SpinalHDL [33], while the hybrid FP32_INT8 multiplier was implemented from scratch according to the design in Section III-C.\nIn all experiments, we considered systolic arrays of sizes ranging from 4 x 4 to 32 \u00d7 32. We spanned various structured pruning rates and investigated both FP32_FP32 and FP32_INT8 quantization schemes. Experiments at different abstraction tiers collected results on area, energy, run-time, and achieved Word Error Rate (WER). We detail each in the rest of the section. Then, we provide insights from a cross-tier point of view and summarize our findings in Section IV-F."}, {"title": "B. Hardware Exploration", "content": "The area and power results for different systolic array sizes and quantization choices are shown in Fig. 6. Since multipliers account for an important part of the area and power budget of the entire systolic array (55.6% and 33.6%, respectively, in the 8 \u00d7 8, FP32_FP32 implementation), the use of the simpler FP32_INT8 design results in tangible savings. In average, these reductions amount to 35.3% and 19.5% in area and power across different array sizes.\nBoth area and power grow quadratically with the systolic array dimension (e.g. by ~4 times between the 4 \u00d7 4 and the 8 \u00d7 8 instances), as both the number of PEs and the number of elements in input/output shift registers have a quadratic dependency on the number of array rows/columns."}, {"title": "C. System Exploration", "content": "Fig. 7 plots the measured per-layer normalized encoder run-time of a systolic-accelerated system performing an inference. Data refers to an 8\u00d78 systolic array, with varying degrees of structured sparsity. Speedup numbers closely follow sparsity levels, as inference run-time is strongly dominated by GEMM computations (exceeding 97% in all cases [20]).\nPruning was performed according to the methodology proposed in Section III-A targeting feed-forward layers, which exhibit a much higher degree of resilience and account for the largest part of the workload. Results in Fig. 7 highlight that early feed-forward layers are the most amenable to pruning, while later ones have a higher proportion of tiles with a non-negligible L1-norm, which have a higher impact on inference outcomes."}, {"title": "D. Impact of SASP on Quality of Service", "content": "Fig. 8 shows that Word Error Rate (WER) grows exponentially when increasing the degree of Systolic Array Structured Pruning. Similar trends are present for both the weight-quantized model (indicated as FP32_INT8) and the non-quantized one (FP32_FP32).\nAs the size of the systolic array grows, trends become steeper, showcasing an abrupt increase in WERs at smaller SASP rates. This effect is caused by the higher brittleness of large-tile structured pruning with respect to small-tile cases. Indeed, while it may be possible to find four prunable 4\u00d74 tiles (containing 64 values in total), selecting a single contiguous 8 \u00d7 8 tile (again, of 64 values) can be considerably more challenging."}, {"title": "E. Multidimensional SASP Trade-offs", "content": "Fig. 9 shows the variations in performance, WER and resource usage when changing SASP rate, quantization strategy, and systolic array size. WER and Speedup are plotted on the two axes, the marker shape discriminates between FP32 and weight-quantized implementations, and the marker colors indicate their resource requirements in terms of Area-Energy product. Data points form four distinct clusters corresponding to each systolic array size. Two curves in each cluster represent the two quantization choices, which have notably different Area-Energy products. The non-quantized FP32_FP32 version achieves lower (better) WER, with the differences becoming more pronounced at higher pruning rates and for larger systolic arrays.\nFrom a run-time perspective, FP32_INT8 configurations allow to reduce the cost of weight transfers by loading four INT8 weights per 32-bit bus access, as opposed to a single FP32 one in the FP32_FP32 configuration. Consequently, FP32_INT8 implementations outperform their FP32_FP32 counterparts for systolic array sizes larger than 4 \u00d7 4, as the savings in data transfers offset software/system overhead.\nNonetheless, while quantization has a large impact on area and energy, its influence on performance is smaller, as the majority of the run-time is not spent in weight data transfers, but instead for streaming inputs / computing outputs, which is equally fast for both quantization schemes.\nWithin each systolic array size and quantization configuration, the SASP pruning rate guides the trade-off between inference time and WER. Fig. 9 shows that, up to an inflection point at a WER of ~5%, SASP enables to strike effective fine-grained balances between run-time performance and QoS. Beyond this inflection point, further increases in pruning rates cause instead very high WER degradations.\nTable III illustrates the effect of applying Systolic Array Structured Pruning and weight quantization at the 5% inflection point. In this setting, SASP improves performance and energy consumption up to 26% and 21%, respectively. Furthermore, when combining quantization and structured pruning, performance and energy efficiency improvements reach 44% and 42%, while also decreasing area occupation by 36%. These substantial gains are achieved without increasing the systolic array size, and hence do not incur the hefty associated area and energy costs, as also depicted in Table III. As an example, scaling from an 8 \u00d7 8 to a 32 \u00d7 32 systolic array does yield a 3.04\u00d7 speedup for FP32_INT8 quantization, but also requires 15.21\u00d7 more area and 3.98\u00d7 more energy."}, {"title": "F. Cross-tier Analysis", "content": "As discussed above, increasing the systolic array size increases run-time performance by offering higher parallelism, but also restricts the achievable pruning rate for a given WER. Therefore, when using SASP, increases in the array size (hence the tile size in GEMM computations) result in diminishing gains for a given WER target, as the pruning rate must be lowered to maintain QoS. This trend is illustrated in Fig. 10, which highlights a sublinear relation between systolic array size and speedup, both in the case of FP32 and weight-quantized scenarios. Hardware costs (area / energy), instead, increase quadratically with size, as outlined in Section IV-B, requiring careful co-design considerations, especially when targeting resource-constrained edge systems."}, {"title": "V. CONCLUSION", "content": "Deploying transformers on edge devices requires both software optimization and hardware acceleration to meet strict resource constraints while maintaining performance. In this paper, we have analyzed their interaction, focusing on structured sparsity and systolic arrays. We explored Systolic Array Structured Pruning (SASP), where the size of pruned blocks is matched to the dimensions of the systolic array, enabling the skipping of entire computation tiles. To assess the benefits and pitfalls of SASP, we presented a cross-stack framework to co-optimize edge AI transformers, which integrates algorithmic optimization, system simulation, and hardware design.\nEmploying it, we performed a comprehensive analysis of how SASP, quantization, and systolic array configurations affect area, energy, performance, and Word Error Rate (WER) in edge transformers for speech recognition.\nOur results demonstrate that SASP provides fine-grained control over the trade-off between inference run-time and WER up to an inflection point, after which increased pruning drastically degrades QoS for small performance gains. Experimental evidence has shown that system-wide performance improvements of up to 44% and accelerator energy reductions of up to 42% can be obtained under a 1.4% WER degradation, when employing weight quantization and a 20% pruning rate. Additionally, we showcased that, although larger systolic arrays do reduce run-time, they also incur substantial energy and area costs, while yielding sublinear speedups for a target WER. This sublinearity emerges from the reduced structured pruning opportunities, as finding contiguous zero blocks becomes harder with increasing block sizes. For this reason, SASP is particularly well-suited for edge Al accelerators, where stringent resource constraints are present."}]}