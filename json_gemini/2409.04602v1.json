{"title": "Training quantum machine learning model on cloud without uploading the data", "authors": ["Guang Ping He"], "abstract": "Based on the linearity of quantum unitary operations, we propose a method that runs the pa- rameterized quantum circuits before encoding the input data. It enables a dataset owner to train machine learning models on quantum cloud computation platforms, without the risk of leaking the information of the data. It is also capable of encoding a huge number of data effectively at a later time using classical computations, thus saving the runtime on quantum computation devices. The trained quantum machine learning model can be run completely on classical computers, so that the dataset owner does not need to have any quantum hardware, nor even quantum simulators. More- over, the method can mitigate the encoding bottom neck by reducing the required circuit depth from O(2n) to n/2. These results manifest yet another advantage of quantum and quantum-inspired ma- chine learning models over existing classical neural networks, and broaden the approaches for data security.", "sections": [{"title": "I. INTRODUCTION", "content": "Data security is rated more and more important nowadays. Individuals need to keep their privacy from misuse and abuse, companies want to protect their intellectual property rights, and governments concern about its threat to the national security. Unauthorized transfer of data may cause a violation of the law in many places. Some countries also put strict restrictions on exporting data abroad. On the other hand, however, the rapid development of artificial-intelligence technology demands a vast amount of data as input, notably in the training of autonomous driving, intelligent healthcare systems and large language models. At the mean time, not every data owner has very powerful computation devices at home, nor even in his own region. Especially, while quantum machine learning is generally expected to be a potential powerful tool for artificial-intelligence technology, intermediate and large scale quantum computers are available via very few cloud platforms in certain countries only.\nTo solve the dilemma between the owner of data and the provider of computational resources, here we propose a run-before-encoding method, which can accomplish the following task. Suppose that Alice owns the dataset and Bob holds the quantum cloud computation platform. Alice can run the quantum circuits on Bob's platform beforehand, without encoding any of her data. Based on the output received from Bob, Alice can input her data later and calculate the cost function needed for training her machine learning model on her own local classical computer. The final trained model can also be run on Alice's local classical computer. Since her data has never been sent to Bob's side, it remains perfectly secure against Bob.\nAs we shall also elaborate below, this method is backed by the linearity in quantum unitary operations. Thus, it is unavailable for existing classical neural networks, where the activation functions of the neurons are nonlinear. Therefore, it displays yet another advantage of using quantum or quantum-inspired machine learning models over existing classical counterparts."}, {"title": "II. TYPICAL FEATURES OF QUANTUM MACHINE LEARNING MODELS", "content": "Our method works for quantum circuits with the following features: (1) all the operations in the middle stage are unitary transformations, while the measurements are performed at the last stage only, and (2) the unitary transformations need to result in real probability amplitudes only. Fortunately, the variational quantum circuit (VQC) architecture [1-3] widely used for quantum machine learning today belongs to this category, as elaborated below.\nTo avoid confusion, we use |x\\rangle_{10} (|x\\rangle_{2}) to denote the quantum states where x is taken as a binary (decimal) number. With this notation, taking the 5-qubit state as an example, we have |0\\rangle_{10} = |0\\rangle_{2}|0\\rangle_{2}|0\\rangle_{2}|0\\rangle_{2}|0\\rangle_{2} = |00000\\rangle_{2}, |1\\rangle_{10} = |00001\\rangle_{2}, |2\\rangle_{10} = |00010\\rangle_{2}, etc. \nFig. 1 shows the typical architecture of a VQC, which consists of three modules: the feature map, the ansatz, and the measurement of the observables. At the beginning, the state of all qubits on the far left are initialized as |0\\rangle_{10}. Then the feature map serves as a unitary transformation UF, which actually consists of several elemental unitary quantum gates, turning |0\\rangle_{10} into a certain quantum state |x\\rangle_{10} = U_F |0\\rangle_{10} that encodes the input data x. Currently, there are many data-encoding methods [4, 5]. But most of them generally have the same feature: the encoded state can be expressed as a linear superposition of the basis vectors. That is, for an input data described by the d-dimensional feature vector x = [x_0, ..., x_{d-1}]^T \\in R^d, the feature map encodes it using the state\n|x\\rangle_{10} = \\frac{1}{C_r} \\sum_{i=0}^{d-1} f_i(x) |i\\rangle_{2} \\tag{1}"}, {"title": "III. OUR METHOD", "content": "Finding pm(x) is the key of the training process. In previous researches, it is done simply by running the VQC by using x as input. If Alice wants to use Bob's quantum cloud computation platform, she has to send x to Bob. But now we are interested in how to obtain pm(x) without sending x out of Alice's site. To this end, notice that the relationship between the initial state |0\\rangle_{10}, the final state |V_{final}(x)\\rangle_{10} before the measurement in the computational basis, and the state |x\\rangle_{10} after applying the feature map is\n|V_{final}(x)\\rangle_{10} = U_O U_A U_F |0\\rangle_{10} = U_O U_A|x\\rangle_{10}. \\tag{10}\nCombining with Eqs. (1) and (8), we yield\n\\sum_{m=0}^{N-1} a_m(x) |m\\rangle_{10} = \\frac{1}{C_r} \\sum_{i=0}^{d-1} f_i(x) U_O U_A |i\\rangle_{10}. \\tag{11}\nExpanding U_O U_A |i\\rangle_{10} in the computational basis as\nU_O U_A |i\\rangle_{10} = \\sum_{m=0}^{N-1} b_m(i) |m\\rangle_{10}, \\tag{12}\nthen there is\n\\sum_{m=0}^{N-1} a_m(x) |m\\rangle_{10} = \\frac{1}{C_r} \\sum_{m=0}^{N-1} \\sum_{i=0}^{d-1} f_i(x) b_m(i) |m\\rangle_{10}, \\tag{13}\nso that\na_m(x) = \\frac{1}{C_r} \\sum_{i=0}^{d-1} f_i(x) b_m(i) \\tag{14}\nfor m = 0,..., N \u2013 1. It can be expressed in a more compact form using matrix product as\n\\vec a(x) = B \\vec f(x), \\tag{15}\nwhere \\vec a(x) is a N-dimensional vector defined as\n\\vec a(x) = [a_0(x), ..., a_m(x), ..., a_{N-1}(x)]^T, \\tag{16}\n\\vec f(x) is a d-dimensional vector defined as\n\\vec f(x) = [f_0(x),..., f_i(x),..., f_{d-1}(x)]^T, \\tag{17}\nand B is a N x d matrix defined as\nB = \\begin{bmatrix} b_0(0) & ... & b_0(i) & ... & b_0(d-1) \\\\ : & & : & & : \\\\ b_m(0) & ... & b_m(i) & ... & b_m(d-1) \\\\ : & & : & & : \\\\ b_{N-1}(0) & ... & b_{N-1}(i) & ... & b_{N-1}(d-1) \\end{bmatrix} \\tag{18}\nEqs. (12) and (15) give birth to the central idea of our method. The former implies that bm(i) (m = 0, ..., N \u22121) are the probability amplitudes of the state vector right before the final measurement of the VQC when the basis vector |i\\rangle_{10} (i = 0, ..., d \u2212 1) is taken as input. That is, bm(i) is not a function of the data x. Consequently, it can be computed without transferring x from Alice to Bob. The latter equation means that Alice can try to obtain all bm(i) (m = 0, ..., N \u22121, i = 0, ..., d\u22121) (i.e., the matrix B) using Bob's quantum computation platform first. Then she can decide on the encoding method of her feature map at a later time, which determines the form of the function fi(x). And the normalization constant Cr can easily be calculated from fi(x). Finally, she substitutes the values of bm(i) received from Bob into Eq. (15) to calculate a(x) and obtain all pm(x) = |am(x)|^2 (m = 0,..., N \u2013 1), which are simple arithmetic that can be done on her own classical computers without even using quantum simulators.\nHowever, the probability amplitude bm(i) is not an observable. Although some quantum simulators can compute it or even find the matrix corresponding to the unitary transformation UOUA directly, it cannot be measured directly on real quantum hardwares. Instead, real quantum computers can output the probability p_m(|i\\rangle_{10}) = |b_m(i)|^2 only. Since bm(i) can be a complex number in general, it cannot be uniquely solved from p_m(|i\\rangle_{10}). That is, it can be expressed as\nb_m(i) = s_m(i) \\sqrt{p_m(|i\\rangle_{10})}, \\tag{19}\nwhere sm(i) could be any complex number of unit modulus. Fortunately, as mentioned above, the VQC studied here has the feature that the probability amplitudes bm(i) of the final state before the last measurement are always real. In this case, the possible choices are reduced to either sm(i) = 1 or sm(i) = \u22121. Still, using p_m(|i\\rangle_{10}) alone is insufficient for determining sm(i) uniquely.\nTo circumvent this difficulty, we need to play the following trick. First, by using the basis vectors |i\\rangle_{10} (i = 0, ..., d- 1. Note that Eq. (14) indicates that i = d, ..., N \u2212 1 are not needed) as inputs, Alice computes p_m(|i\\rangle_{10}) (m = 0, ..., N \u2212 1, i = 0, ..., d \u2013 1) on Bob's quantum platform. At this stage, from Eqs. (12)"}, {"title": "IV. ADVANTAGES", "content": "An obvious and foremost advantage of our method is that the security of Alice's dataset x is well-protected. As can be seen from step (II) of Protocol 1, Bob runs the VQC based on the parameters received from Alice using only the basis vectors and their superpositions as input. None of the data x was encoded in these states, nor being sent to Bob. In fact, the relationship between x and Bob's output may even remain unknown to Alice herself at this stage, because she does not need to decide on the encoding method and the form of the cost function until step (IV). Therefore, it is clear that Bob does not stand any chance to learn ax and the cost function.\nSecondly, our method may save the runtime on the quantum devices when the amount of data is huge. As shown in step (II), for example, consider the best case where all the output p_m(|r\\rangle_{10}) (m = 0, ..., N \u2212 1) corresponding to the first reference state |r\\rangle_{10} are nonzero. To obtain the matrix B, the VQC needs to be run for d + (d \u2212 1) = 2d - 1 input states, regardless the size of the dataset. Once B is obtained, the probability distributions corresponding to all x in the dataset can be calculated via Eq. (15) on classical computers. In contrast, when using the existing conventional method where the VQC needs to be run for each input x one by one, evaluating a dataset with s data points has to call for the VQC s times. Therefore, our method takes a much less occupation time on the quantum devices than the conventional one does for any dataset satisfying s >> 2d - 1. For instance, the MNIST dataset [9] widely used in machine learning research has a dimension d = 784, whereas the number of training data is s = 50000. In this case, our method will be significantly faster. On quantum cloud platforms charging by the runtime, less occupation of the quantum devices also means less cost.\nMoreover, our method can help mitigating the encoding bottom neck [10]. In many encoding methods such as the one described by Eq. (2), encoding a data vector x using n qubits generally requires a circuit depth of the order O(2^n) [11], except for certain sparse vectors. But in our method, the inputs to the VQC are merely basis vectors and simple superpositions of two basis vectors. It can be proven that encoding a single basis vector takes only n/2 single-qubit NOT gates in average, while encoding the superpositions of two basis vectors takes only n/2 two-qubit controlled-NOT gates in average.\nIn addition, after the training of the VQC is completed, Alice will no longer need a quantum computer to run it. She can simply use the matrix B obtained in the final epoch of the training as the machine learning model, and applies it on any new input data x by computing Eq. (15) on her local classical computer."}, {"title": "V. FURTHER IMPROVEMENTS FOR BETTER SECURITY", "content": "In Protocol 1, though Bob does not know the value of x, he knows the dimension d of x, because he is required to run the VQC for d of the basis vectors and their super- positions, while the total number of the basis vectors of a n-qubit system is N = 2n. He also knows the adjustable parameters of the VQC because Alice sent them to him. If these problems are the primary concern while increasing the runtime on the quantum devices is acceptable, we can have the following fix.\nTo hide the dimension of the data x, let us take the"}, {"title": "VI. MORE DISCUSSION ON THE ADVANTAGE", "content": "In conclusion, we propose a run-before-encoding method so that a dataset owner Alice can train a machine learning model on Bob's quantum cloud computation platform, with the primary advantage that the dataset is kept perfectly secret from Bob. Note that it was pointed out in Ref. [12] that \u201calmost all branches of quantum machine-learning research have been heavily framed by the question of 'beating' classical machine learning in some figure of merit\", but \"a number of 'positive' results have been put forward that either 'prove' theoretically or 'show' empirically that quantum computers are better at something\". According to Ref. [13], it was even stated at Google I/O 2024 that \"no one has yet demonstrated a clear quantum advantage for machine learning on classical data\". But our method works for any quantum circuit where all the operations in the middle are unitary while the measurements are performed at the very end, and the unitary operations always result in real probability amplitudes. These features are not available in existing classical machine learning models but they are not uncommon for quantum ones. Thus, the advantage of our method is neither empirical nor bias towards certain tasks.\nNevertheless, this advantage can be achieved not only on real quantum computers, but also by running quantum simulators on classical computer. That is, though it is an advantage over all existing classical neural networks, rigorously speaking, it may not seem appropriate to regard it as a quantum advantage. We feel that it should be better considered as a \"quantum-inspired\" advantage. This result also suggests that quantum-inspired classical neural networks, i.e., redesigned classical neural networks by replacing the neuron associated by existing choices of non-linear classical activation functions (e.g., the sigmoid function, ReLU function, and Tanh function, etc.) with unitary linear transformations, should deserve more research interests since it could make full use of the advantage in the near term."}]}