{"title": "Evaluating Consistencies in LLM responses\nthrough a Semantic Clustering of Question Answering", "authors": ["Yanggyu Lee", "Jihie Kim"], "abstract": "In the realm of Large Language Model (LLM) func-\ntionalities, providing reliable information is para-\nmount, yet reports suggest that LLM outputs lack\nconsistency. This inconsistency, often attributed to\nrandomness in token sampling, undermines user\ntrust as it leads to varying responses even for iden-\ntical queries. In this paper, we present a new ap-\nproach for evaluating semantic consistencies of\nLLM including comparison of alternative tech-\nniques. Our approach evaluates whether LLM re-\nsponses are semantically congruent for a given\nquestion, recognizing that as syntactically different\nsentences may convey the same meaning. Hereto-\nfore, To enhance LLM consistency, two main ap-\nproaches have been explored: Leverage external\nknowledge as context like the RAG pattern or use\nZero-shot-CoT to improve performance of LLM it-\nself. We apply our evaluation approach to these\ntechniques, and demonstrate to compare the impact\nof these methods on LLM response consistency\nacross different domains of question answering\ntasks. Using the TruthfulQA dataset to assess LLM\nresponses, the study induces N responses per ques-\ntion from the LLM and clusters semantically equiv-\nalent sentences to measure semantic consistency\nacross 37 categories. Through this, it quantitatively\nanalyzes the effectiveness of the aforementioned\nmethods in improving LLM performance before\nand after their adoption.", "sections": [{"title": "1 Introduction", "content": "One of the functions of the LLM is to provide information,\nand in doing so, the LLM should provide users with reliable\nresults. However, the output of LLM has been reported to be\ninconsistent[Jang et al., 2023, Elazar et al., 2021]. Con-\nsistency in a language model means that it produces the same\noutput for the input with the same meaning. It is speculated\nthat the cause of LLM's inconsistency is randomness[Bubeck\net al., 2023], which can prevent it from generating consistent\nresponses to the same question. Randomness in a language model increases as it samples to-\nkens that correspond to the words that will come next. Incon-\nsistent answers reduce the confidence that users have in the\nmodel. Typical ways to improve model consistency include\n1) providing external knowledge as context. A classic exam-\nple is the RAG[LEWIS et al., 2020] pattern. 2) Use prompts\nthat improve the performance of the LLM itself. A repre-\nsentative example is the Zero-shot-CoT[KOJIMA et al.,\n2022].\nHowever, the consistency changes seen in language mod-\nels when using these methods are not well studied. This paper\nproposes a new approach that systematically evaluates and\ncompares them. First, we apply them to question answering\ntasks in different domains, and analyze how the choice of\neach method affects the consistency of LLM's answers. We\nalso introduce semantic consistency, a new approach to as-\nsessing consistency based on the intuition that LLM should\nproduce semantically identical results for the same questions.\nEven though the grammatical structure may be different, the\nactual meaning of the sentence may be the same, so it is im-\nportant for language models that generate sentences in a free\nform to take this into account when judging the consistency\nof the answer[Malinin et al., 2020].\nWe elicit N responses from the LLM for each question in\nthe TruthfulQA[LIN et al., 2021] dataset, which is used to\nevaluate whether the answers provided by the model can be\ntrusted. We then cluster semantically identical sentences\namong the responses and measure the semantic consistency\nof the LLM based on the clustering results. We measure the\nsemantic consistency values for a total of 37 categories in the"}, {"title": "2 Related Work", "content": "Measuring the consistency of LLM. In the past, measures\nbased on lexical matching were commonly used to evaluate\nmodel consistency[Elazar et al., 2021]. These methods com-\npare outputs at the token level to determine whether a pre-\ntrained language model (PLM) produces the same output for\nthe same input. However, this approach only considers lexi-\ncal matching, not semantic matching.\nDue to this limitation, a new consistency measure has re-\ncently been proposed that takes semantic matching into ac-\ncount[RAG et al., 2022]. These methods evaluate whether the\noutputs of a model are semantically similar, i.e., whether the\ntwo outputs convey the same meaning. These methods eval-\nuate whether the model produces consistent answers centered\non meaning rather than word choice. To this end, several se-\nmantic agreement metrics have been developed and used to\nevaluate the consistency of models.\nIn the end, semantic matching measures provide a better\nassessment of model consistency than traditional lexical\nmatching measures, and are better suited to generating natural\nresponses that are relevant to the user's confidence.\nMeasuring the confidence of LLM. The study of confi-\ndence in language models has been explored in various ways\nto measure and improve the confidence of a model's predic-\ntions [TAO et al., 2024, WIGHTMAN et al., 2023]. One of\nthe methods is to evaluate how confidently a model makes\npredictions and to determine the confidence of the model\nbased on this [WIGHTMAN et al., 2023]. These methods\nmainly analyze the output distribution of the model and meas-\nure the uncertainty of the prediction to determine the confi-\ndence of the model. Other methods study models to improve\ntheir predictions during the learning process [TAO et al.,\n2024], thereby improving the confidence of the model. We\ndo not consider model confidence in this study, but plan to do\nso in future work."}, {"title": "3 Approach", "content": "Our approach consists of three steps. In the first step, we feed\neach question to LLM to elicit N answers per question. In to-\ntal, we use three methods to induce LLM's answers: 1) plain\nquestion and answer with nothing applied; 2) question and\nanswer with RAG pattern; and 3) question-answering with\nZero-Shot-CoT. The second step clusters semantically simi-\nlar sentences in the LLM-generated answers for each ques-\ntion. Semantically similar answers are given the same number\nbecause they belong to the same cluster. The detailed cluster-\ning method is introduced in Section 3.2. In the third step, se-\nmantic consistency is calculated based on the clustering re-\nsults. The higher the number of clusters, the lower the seman-\ntic consistency value, which means that the LLM does not\ngive semantically consistent answers. On the other hand, the\nsmaller the number of clusters, the higher the semantic con-\nsistency value is calculated, which means that LLM is good\nat giving semantically consistent answers. The detailed meth-\nodology is introduced in Section 3.3."}, {"title": "3.1 Generation", "content": "Generation Method. All of the extracted questions were en-\ntered into the LLM, and the LLM was asked to answer each\nquestion five times, meaning that the LLM would have a total\nof 25 responses for each category. LLM chose OPT-30B.\nThere are three different ways to generate the questions: 1)"}, {"title": "Retriever-Augmented-Generation", "content": "The RAG[LEWIS et al.,\n2020] pattern is a prompt pattern that encourages LLM to re-\nfer to external knowledge to give a more accurate answer.\nLLM were injected with external knowledge to perform the\nquestion-answering task. In the TruthfulQA dataset, there are\nlinks to webpages that can be used as references for questions.\nWe crawled those links to build a document searcher. Given\na question, the searcher finds the most similar documents to\nthe question and provides context to LLM. The question-an-\nswering prompt is preceded by the phrase \"Answer the ques-\ntion based only on the following context:\" and immediately\nfollowed by the context obtained from the retriever. LLM an-\nswers the question based on the context."}, {"title": "Zero-Shot-CoT", "content": "Zero-Shot-CoT[KOJIMA et al., 2022] is a\nprompting methodology that improves performance by di-\nrecting inference from the LLM to the process. Use simple\nprompts to encourage the model to improve its own perfor-\nmance. We add the prompt \"Let's think step by step\" to allow\nthe model to reason step by step. The resulting reasoning path\nis added to the existing question and answer prompts, and\nLLM makes its final answer based on the reasoning path."}, {"title": "3.2 Semantic Similarity", "content": "Semantically similar answers were clustered from LLM's an-\nswers to a single question. The clustering algorithm used the\nmethod proposed in [KUHN et al., 2023]. Determine the se-\nmantic similarity of two answers in DeBERTa[HE et al.,\n2020]. The input consists of a question and an answer, and a\nquestion and another answer. In this case, the questions are\nthe same and the answers are different. This concatenation is\nconstructed once in the forward direction and once in the re-\nverse direction. Two answers are semantically similar if they\nare both output as entailment when fed into DeBERTa[HE et\nal., 2020] as input."}, {"title": "3.3 Semantic Consistency", "content": "We compute the semantic consistency[RAG et al., 2022] of\nthe answers as a result of the work done in Section 3.2. The\nexpression for computing semantic consistency is (Equation\n1).\n$\\displaystyle Cons_{sem}(Y) = \\frac{1}{n(n-1)} \\sum_{i,j=1, i\\neq j}^{n} f(y_i, y_j)$\n$f(y_i, y_j)$ is a directive function that traverses the answers to\na question, selects two answers, and returns 1 if the selected\nanswers belong to the same cluster. If they are not in the same\ncluster, it returns 0. The return value is cumulative. The range\nof values that can be computed by $Cons_{sem}(Y)$ is a real num-\nber between 0 and 1. The higher the number of clusters, the\nlower the semantic consistency value, which means that LLM\ncannot give a semantically consistent answer. On the other\nhand, the smaller the number of clusters, the higher the se-\nmantic consistency value is calculated, meaning that LLM\ngives semantically consistent answers."}, {"title": "4 Experimental Results", "content": "We calculated the average of the semantic consistency in\neach category from the semantic consistency calculated in\nSection 3.3. We then analyzed them to compare how much\nimpact our methods for improving model consistency actu-\nally have.\nDatasets. We compute the semantic consistency of LLM by\neliciting responses to questions from each domain through\nLLM. The TruthfulQA dataset used in this experiment is a\ndataset for measuring the truthfulness of models and consists\nof questions in 38 categories including law, finance, and pol-\nitics. We randomly extracted 5 questions from each category\nin the TruthfulQA dataset to form the question data. Catego-\nries with less than 5 questions were removed from the dataset.\nSince the category 'Misconceptions: Topical' has a total of 4\ndata points, we removed it from the dataset, leaving 37 cate-\ngories of questions for this experiment."}, {"title": "How many categories have semantic consistency changes?", "content": "We counted the number of cases where semantic consistency\nincreased or decreased in each category when we tried to im-\nprove the consistency of LLM in two different ways. In each of the two cases,\nthe semantic consistency of LLM increased overall. However,\nthe results with RAG were better than those with zero-shot-"}]}