{"title": "On Centralized Critics in Multi-Agent Reinforcement Learning", "authors": ["Xueguang Lyu", "Andrea Baisero", "Yuchen Xiao", "Brett Daley", "Christopher Amato"], "abstract": "Centralized Training for Decentralized Execution, where agents are trained offline in a centralized fashion and execute online in a decentralized manner, has become a popular approach in Multi-Agent Reinforcement Learning (MARL). In particular, it has become popular to develop actor-critic methods that train decentralized actors with a centralized critic where the centralized critic is allowed access global information of the entire system, including the true system state. Such centralized critics are possible given offline information and are not used for online execution. While these methods perform well in a number of domains and have become a de facto standard in MARL, using a centralized critic in this context has yet to be sufficiently analyzed theoretically or empirically. In this paper, we therefore formally analyze centralized and decentralized critic approaches, and analyze the effect of using state-based critics in partially observable environments. We derive theories contrary to the common intuition: critic centralization is not strictly beneficial, and using state values can be harmful. We further prove that, in particular, state-based critics can introduce unexpected bias and variance compared to history-based critics. Finally, we demonstrate how the theory applies in practice by comparing different forms of critics on a wide range of common multi-agent benchmarks. The experiments show practical issues such as the difficulty of representation learning with partial observability, which highlights why the theoretical problems are often overlooked in the literature.", "sections": [{"title": "1. Introduction", "content": "Centralized Training for Decentralized Execution (CTDE) (Oliehoek, Spaan, & Vlassis, 2008), where agents are trained offline in a centralized manner but execute in a decentralized manner with only local information, has been widely adopted in multi-agent reinforcement learning (MARL). Compared to independent learning, CTDE has great potential for more stable and optimal learning since agents can coordinate offline on how they will behave online. Actor-Critic (AC) methods are popular for CTDE because a centralized critic can be used to train decentralized actors, exploiting the centralized training paradigm; since the critic is only needed to train the actors, it can be discarded once the actors are fully trained without hindering decentralized execution. Because the centralized critic is trained offline in a simulator, it can be trained on the joint observations from all the agents as well as the system state. Using the state is intuitively considered desirable as it is often more concise than the history and provides ground-truth information. This technique of exploiting the system state has become popular after the pioneering centralized critic works of COMA (Foerster, Assael,\nDe Freitas, & Whiteson, 2016) and MADDPG (Lowe, Wu, Tamar, Harb, Pieter Abbeel, &\nMordatch, 2017).\nThe statements made for the effects of centralized critics are almost entirely positive.\nFor instance, MADDPG (Lowe et al., 2017) notes that it eases learning and helps to learn\ncoordinated behaviors. Later works list similar sentiments, suspecting that critic centralization\nimproves performance (Lee & Lee, 2019), reduces variance (Das, Gervet, Romoff, Batra,\nParikh, Rabbat, & Pineau, 2019), stabilizes training (Li, Wu, Cui, Dong, Fang, & Russell,\n2019) and is more robust (Sim\u00f5es, Lau, & Reis, 2020). It seems reasonable to make these\nassumptions because training a centralized value function would solve the cooperation issues\n(e.g., action shadowing (Claus & Boutilier, 1998)) for a centralized policy, and result in better\nconvergence properties. However, as we will show, a centralized critic does not have the same\neffect (in solving those problems) on a set of decentralized policies.\nThe exploitation of the system state is considered one of the significant advantages\nof the CTDE paradigm. Using the state has also become a selling point for centralized\ncritics (Foerster et al., 2016) since state value functions are usually easier to learn than\nobservation-history value functions. While many works use centralized critics, they often\nfocus on other issues without carefully examining the foundation of the critic centralization\ntechniques that they employ, e.g., improving credit assignment (Wang, Zhang, Kim, & Gu,\n2020a; Du, Han, Fang, Dai, Liu, & Tao, 2019), exploration (Zhou, Liu, Sui, Li, & Chung,\n2020), emergent tool use (Baker, Kanitscheider, Markov, Wu, Powell, McGrew, & Mordatch,\n2020), etc. However, even though centralized critics have become a standard mechanism in\nrecent works, they still lack a thorough theoretical and empirical analysis. In this paper,\nwe give a comprehensive investigation of these claims and show that most gains with critic\ncentralization are questionable, as they often entail hidden trade-offs, both theoretically and\nempirically.\nThis paper fills this analytical gap by providing an analysis of critics conditioned on joint\nobservations as well as the system state; we show that the common intuitions stated in most\nrecent works are usually unsound for both cases. In particular,\n(1) we show that centralized critics are not theoretically beneficial compared to decentralized\ncritics,\n(2) we show that state-based critics may result in bias, making them theoretically inferior\nto history-based critics,\n(3) we show that centralized critics (both history and state-based) result in higher variance,\n(4) we advise the use of history-state critics, which use both history and state information\nas a method to incorporate state without introducing bias, and\n(5) we provide an extensive empirical analysis showing the trade-offs of the different critic\ntypes.\nIn our analysis, we prove that critic centralization does not theoretically improve coop-\neration compared to decentralized critics from a policy learning perspective, even though\nthe values themselves may be easier to learn. For history-based critics, we show in theory\nthat centralized and decentralized critics have the same expected gradient. This implies that"}, {"title": "2. Related Work", "content": "The CTDE training paradigm is used in a number of recent deep MARL approaches. Value-\nbased CTDE approaches such as QMIX, QPLEX, and others focus on how centralized\nvalues can be reasonably factorized into decentralized ones, and have shown promising\nresults (Son, Kim, Kang, Hostallero, & Yi, 2019; Mahajan, Rashid, Samvelyan, & Whiteson,\n2019; Wang, Dong, & Victor Lesser, 2020b; Rashid, Farquhar, Peng, & Whiteson, 2020; Peng,\nRashid, Schroeder de Witt, Kamienny, Torr, B\u00f6hmer, & Whiteson, 2021; de Witt, Peng,\nKamienny, Torr, B\u00f6hmer, & Whiteson, 2020; Xiao, Hoffman, Xia, & Amato, 2020; Wang,\nWang, Zheng, & Zhang, 2020c; Rashid, Samvelyan, de Witt, Farquhar, Foerster, & Whiteson,\n2018; Sunehag, Lever, Gruslys, Czarnecki, Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo,\nTuyls, et al., 2018). On the other hand, CTDE policy gradient methods are almost entirely\nbased on centralized critics.\nOne of the first methods featuring a centralized critic was COMA (Foerster, Farquhar,\nAfouras, Nardelli, & Whiteson, 2018). COMA adopted a centralized state-based critic with\na counterfactual baseline; the state-based critic then became the standard in many other\napproaches. Regarding convergence properties, COMA claims that the overall effect of a\ncentralized critic on the decentralized policy gradient may be reduced to a single-agent actor-\ncritic approach, which ensures convergence under similar assumptions (Konda & Tsitsiklis,\n2000); however, the assumption only holds in fully-observable environments and is incorrect\nfor partially observable environments. In this paper, we clarify and expand on the theory of\ncentralized critics by developing the convergence properties and a bias/variance analysis for\ncentralized and decentralized critics, and their respective policies. Due to their theoretical\nproperties, we provide separate discussions for state-based and history-based critics.\nConcurrently with COMA, MADDPG (Lowe et al., 2017) proposed to use a dedicated\ncentralized critic for each agent in semi-competitive domains, demonstrating compelling\nempirical results in continuous action environments.\nMany other agents extend the ideas of COMA and MADDPG. We discuss some of them\nbut many more have been developed. M3DDPG (Li et al., 2019) focuses on the competitive\ncase and extends MADDPG to learn robust policies against altering adversarial policies by\noptimizing a minimax objective. On the cooperative side, SQDDPG (Wang et al., 2020a)\nborrows the counterfactual baseline idea from COMA and extends MADDPG to achieve credit\nassignment in fully cooperative domains by reasoning over each agent's marginal contribution.\nOther researchers also use critic centralization for emergent communication with decentralized\nexecution in TarMAC (Das et al., 2019) and ATOC (Jiang & Lu, 2018). There are also efforts\nutilizing an attention mechanism addressing scalability problems in MAAC (Iqbal & Sha,\n2019). Also, teacher-student style transfer learning LeCTR (Omidshafiei, Kim, Liu, Tesauro,\nRiemer, Amato, Campbell, & How, 2019) builds on top of centralized critics, which does not\nassume expert teachers. Other work includes multi-agent credit assignment and exploration in\nLIIR and LICA (Du et al., 2019; Zhou et al., 2020), goal-conditioned policies with CM3 (Yang,\nNakhaei, Isele, Fujimura, & Zha, 2020), and for temporally abstracted policies (Chakravorty,\nWard, Roy, Chevalier-Boisvert, Basu, Lupu, & Precup, 2020). Extensive tests based on a\ncentralized critic in a more realistic environment using self-play for hide-and-seek (Baker\net al., 2020) have demonstrated impressive results showing emergent tool use. Note that\nimpressive results also use a state-based critic, which is a common practice and used in works"}, {"title": "3. Background", "content": "This section introduces the formal problem definition of cooperative MARL with decentralized\nexecution and partial observability. We introduce various forms of value functions and\nformalize a set of commonly accepted on-policy history (and state) distributions. We also\nintroduce multi-agent actor-critic methods in which the value functions, both centralized\nand decentralized, are approximated by critic models. In the coming definitions and the rest\nof this document, we use AX to denote the set of probability distributions over a set X."}, {"title": "3.1 Dec-POMDPS", "content": "Decentralized partially observable Markov decision processes (Dec-POMDPs) (Oliehoek\n& Amato, 2016) are multi-agent cooperative sequential decision making problems. A\nDec-POMDP is a tuple (I, S, A, \u03a9, T, O, R, \u03b3), composed of a set of agents I, a state\nspace S, with initial state $s_0 \\in S$, a joint action space $A = \\times_{i \\in I} A_i$, one per agent, a\njoint observation space $\u03a9 = \\times_{i \\in I} \u03a9_i$, one per agent, a stochastic state transition function\n$T : S \\times A \\rightarrow \\Delta S$ that determines state transitions $Pr(s' | s, a)$, a stochastic joint observation\nfunction $O: A \\times S \\rightarrow \\Delta \u03a9$ that determines observation emissions $Pr(o | a, s)$, a joint reward\nfunction $R: S \\times A \\rightarrow R$ that is shared by all agents, and a discount factor $\u03b3 \\in [0, 1)$.\nControl in Dec-POMDPs is performed by a set of decentralized agent policies $\u03c0 =$\n$(\u03c0_1,..., \u03c0_{|I|})$, each representing a (stochastic) mapping from each agents' individual action-\nobservation history to its next action, $\u03c0_i: H_i \\rightarrow \\Delta A_i$, where $H_i$ is the set of action-observation\nhistories for agent $i$. For instance, agent $i$'s history at timestep $t$ is the sequence of all previous\nactions and observations $h_{i,t} = (o_{i,0}, a_{i,0}, o_{i,1},..., a_{i,t\u22121}, o_{i,t})$. A joint history is the set of all\nagent histories $h_t = (h_{1,t},...,h_{|I|,t})$. The set of actions chosen by each agent forms the joint\naction $a_t = (a_{1,t}, ..., a_{|I|,t})$. As feedback from the system, a scalar reward $R(s_t, a_t)$ is shared\nby all agents, and each agent receives a local observation $(o_{1,t},..., o_{|I|,t}) \\sim O(a_{t\u22121}, s_t)$.\nThe objective of all agents is to maximize the total performance, i.e., the expected\ndiscounted sum of future rewards $J = E [\\sum_{t=0}^{\\infty} \u03b3^t r_t]$ ."}, {"title": "3.2 Discounted Visitations: Counts and Distributions", "content": "The pairing of a Dec-POMDP with a set of agent policies \u03c0 fully determines the (stochastic)\nbehavior of the system, i.e., the marginal, joint, and conditional probability of the defined\nrandom variables (e.g., states and histories). However, before defining core RL concepts\nlike value functions and describing policy gradient variants, it is helpful to define a set of\nfunctions related to the likelihood of occurrences of certain states or histories."}, {"title": "3.2.1 Discounted Visitation Counts", "content": "First, we define the discounted visitation counts function \u03b7, which represents a discounted\nnotion of the expected number of times that the system variables take some given value. In\nthe case of the discounted state visitations (also defined in a different but equivalent form\nfor single-agent fully observable control in (Sutton & Barto, 2018)), we define\n$\\eta(s) = \\sum_{t=0}^{\\infty} \\gamma^t \\Sigma_{s_t} Pr(S_t = s).$\n(1)\nNote that n is formally a function of the joint policies \u03c0, and should technically be\ndenoted as n\u03c0; however, given the lack of ambiguity, we omit the suffix to simplify notation.\nThe discount factor y guarantees that \u03b7 is well-defined and finite for any Dec-POMDP,\neven those that never terminate or keep visiting the same states ad infinitum\u2014hence the\nimportance of this discounted notion of visitations.\nWe will also use \u03b7 for the number of times a (joint) history or (joint) history-state pairs\nare visited. We use the same overloaded symbol n due to similarity with the state visitation\ncounts; the distinction is immediately clear from the inputs and context. In the case of\nhistory visitations and history-state visitations, we define\n$\\eta(h) = \\sum_{t=0}^{\\infty} \\gamma^t Pr(H_t = h), \\qquad \\eta(h, s) = \\sum_{t=0}^{\\infty} \\gamma^t Pr(H_t = h, S_t = s),$\n(2)\nWe note some relevant properties that either relate or are shared by n(s), \u03b7(h), and n(h).\nFirst, we note that $\u03b7(s) = \\Sigma_h \u03b7(h, s)$ and $\u03b7(h) = \\Sigma_s \u03b7(h, s)$. Then, we note that all of\nthese discounted counts ultimately add up to the same value, as a direct consequence of the\ngeometric series component based on y, i.e., $\\Sigma_s \\eta(s) = \\Sigma_h \\eta(h) = \\Sigma_{h,s} \\eta(h, s) = (1 \u2013 \u03b3)^{-1}$.\nIn the case of \u03b7(h) and \u03b7(h, s), we also note that the sum over timesteps t is technically\nsuperfluous, since there is only one timestep where the joint history h can possibly be\ncollectively seen by the agents; therefore, \u03b7(h) and n(h, s) are equivalently written as\n$\\eta(h) = \\gamma^{|h|} Pr(H_t = h)|_{t=|h|}, \\qquad \\eta(h, s) = \\gamma^{|h|} Pr(H_t = h, S_t = s)|_{t=|h|}$\n(3)\nFinally, we further consider visitation variants that include action counts,\n$\\eta(s, a) = \\sum_{t=0}^{\\infty} \\gamma^t Pr(S_t = s, A_t = a),$\n(4)\n$\\eta(h, a) = \\sum_{t=0}^{\\infty} \\gamma^t Pr(H_t = h, A_t = a),$\n(5)\n$\\eta(h, s, a) = \\sum_{t=0}^{\\infty} \\gamma^t Pr(H_t = h, S_t = s, A_t = a),$\n(6)\nwhich share similar properties to their action-less counterparts. Importantly, counts \u03b7(h, a)\nand n(h,s,a) are respectively related to n(h) and \u03b7(h,s) by the policy, according to\n$\u03b7(h, a) = n(h)\u03c0(\u03b1; h)$ and $\u03b7(h, s, a) = n(h, s)\u03c0(\u03b1; h)$."}, {"title": "3.2.2 Discounted Visitation Probabilities", "content": "Next, we define the discounted visitation probability functions pas normalized versions of the\ncorresponding counts \u03b7. Given that \u03b7 adds up to $(1 \u2013 \u03b3)^{-1}$, this results in\n$p(s) = (1 \u2013 \u03b3)\u03b7(s), \\qquad \u03c1(h) = (1 \u2013 \u03b3)\u03b7(h), \\qquad p(h, s) = (1 \u2013 \u03b3)\u03b7(h, s),$\n$p(s, a) = (1 \u2212 \u03b3)\u03b7(s, a), \\qquad \u03c1(h, a) = (1 \u2212 \u03b3)\u03b7(h,a), \\qquad p(h, s, a) = (1 \u2212 \u03b3)\u03b7(h, s, a).$\n(7)\nEach p now represents a probability distribution over the space of its inputs. Further,\ncommon marginalization properties hold, e.g., $\u03c1(s) = \\Sigma_h \u03c1(h, s)$, and $\u03c1(h) = \\Sigma_s \u03c1(h, s)$.\nWe can also further overload p to encompass a notion of conditional probability, e.g.,\n$p(s | h) = \\frac{\u03b7(h,s)}{\u03b7(h)}, \\qquad p(h | s) = \\frac{\u03b7(h,s)}{\u03b7(s)},$\n$\u03c1(\u03b1 | h) = \\frac{\u03b7(h, a)}{\u03b7(h)}, \\qquad \u03c1(\u03b1 | s) = \\frac{\u03b7(s, \u03b1)}{\u03b7(s)},$\n(8)\nfor which common conditional properties also hold, e.g., $p(s | h) = p(h,s) / \u03c1(h)$, $\u03c1(h |$\ns$) = p(h, s) / \u03c1(s)$, $\u03c1(\u03b1 | h) = p(h, a) / p(h)$, and $p(a | s) = p(s, a) / p(s)$. We also note\nthat $p(a | h) = p(a | h,s) = \u03c0(\u03b1; h)$. Finally, we note that some of these p-function\noutputs are equivalent to direct probabilities induced by the Dec-POMDP's graphical model.\nMost notably, p(s | h) is equivalent to the conditional state probability given the joint\nhistory Pr(s | h). For others, however, there is no such corresponding probability, e.g.,\np(h | s) is mathematically well-defined, while Pr(h | s) is ill-defined without assuming a\nparticular timestep for h (Baisero & Amato, 2022). We use p to primarily simplify the\nnotation associated with policy gradients (Section 3.4), although we will also exploit the\nconditional-p functions to resolve a formal issue that appears in our definition of a specific\nform of centralized value function (Section 3.3.3)."}, {"title": "3.3 Value Functions", "content": "In this section, we formally define various types of value functions that are used in different\nforms of multi-agent policy gradient: the joint history value function $Q^\u03c0(h, a)$, the individual-\nhistory value function $Q_i^\u03c0(h, a)$, the state value function $Q^\u03c0(s, a)$, and the joint history-state\nvalue function $Q^\u03c0(h, s, a)$. These value functions all represent some notion of expected\n(discounted) performance obtained by the entire team of agents, that is given and indicated\nas a suffix (even for the individual history case $Q_i^\u03c0$), and they differ exclusively in terms of\nthe information that is available to determine the expected team performance."}, {"title": "3.3.1 Joint History Value Function $Q^\u03c0(h,a)$", "content": "The joint history value function $Q^\u03c0(h, a)$ is a form of centralized value function, and the\nunique solution to the following joint history Bellman equality,\n$Q^\u03c0(h, a) = R(h, a) + \u03b3 E_{o \\sim h,a} \\Sigma_{a'} \u03c0(a'; hao)Q^\u03c0(hao, a')$\n(9)\nwhere $R(h, a) = E_{s\\vert h} [R(s, a)]$ is the joint history reward function. $Q^\u03c0(h, a)$ is the expected\nlong-term performance of the team of agents when each individual agent policy $\u03c0_i$ has\nobserved the individual history $h_i$ and has opted to perform a first action $a_i$."}, {"title": "3.3.2 Individual History Value Function $Q_i^\u03c0(h, a)$", "content": "The individual history value function $Q_i^\u03c0(h, a)$ is a form of decentralized value function from\nthe singular perspective of the $i$-th agent, and the unique solution to the following individual\nhistory Bellman equality,\n$Q_i^\u03c0(h, a) = R_i(h, a) + \u03b3 E_{o \\sim h,a} \\Sigma_{a'} \u03c0_i(a'; hao)Q_i^\u03c0(hao, a')$  (10)\nwhere $R_i(h, a) = E_{s,a_{-i}\\vert h_i=h} [R(s, a)]$ is the individual history reward function, which inte-\ngrates out the behavior of all other agents and the resulting state. $Q_i^\u03c0(h, a)$ is the expected\nlong-term performance of the team of agents when the $i$-th agent policy $\u03c0_i$ has observed the\nindividual history $h$ and has opted to perform a first action $a$."}, {"title": "3.3.3 State Value Function $Q^\u03c0(s,a)$", "content": "The state value function $Q^\u03c0(s, a)$ is a form of centralized value function that attempts to\nmeasure the expected long-term performance of the team of agents when the system state\nhappens to be $s$, and each individual agent policy $\u03c0_i$ has opted to perform a first action $a_i$.\nA straightforward (but na\u00efve, as we will see) formalization of this notion is based on defining\nthe state value function as the unique solution to the following state Bellman equality,\n$Q^\u03c0(s, a) = R(s, a) + \u03b3 E_{s'\\vert s,a} \\Sigma_{a'} Pr(a' \\vert s')Q^\u03c0(s', a')$   (11)\nAlthough this notion of state value seems reasonable at the surface level, it suffers from\na subtle formality issue that causes Pr(a | s), and consequently $Q^\u03c0(s, a)$ itself, to be not\nguaranteeably well-defined for generic control problems and teams of agents; this is an issue\nintrinsic to partial observability that was already analyzed for the single-agent control case\nin (Baisero & Amato, 2022), and for the multi-agent control case in (Lyu et al., 2022).\nTo keep this introductory section brief and compact, we redirect a more thorough\ndiscussion on the issues with Pr(a | s) and Equation (11) to Appendix A. Broadly speaking,\nthe issue is related to the fact that Pr(a | s) denotes a time-invariant relationship between\nvariables that is conventionally time-variant, and is therefore undefined when a time index is\nnot available. In fact, we note that there is no issue with a timed variant of the state value\nfunction $Q_t^\u03c0(s, a)$ defined as the solution to the following Bellman equality\n$Q_t^\u03c0(s, a) = R(s, a) + \u03b3 E_{s'\\vert s,a} [\\Sigma_{a'} Pr(A_{t+1} = a' \\vert S_{t+1} = s')Q_{t+1}^\u03c0(s', a')]$  (12)\nHowever, employing timed value functions is an unsatisfactory solution as they do not\ngeneralize well across different (and potentially many) timesteps, and is not a common\npractice in mainstream RL. We thus consider the following untimed alternative definition for\nstate values $Q^\u03c0(s, a)$ as the unique solution to the following state Bellman equality,\n$Q^\u03c0(s,a) = R(s, a) + \u03b3 E_{s'\\vert s,a} [\\Sigma_{a'} \u03c1(a' \\vert s')Q^\u03c0(s', a')$] (13)"}, {"title": "3.3.4 Joint History-State Value Function $Q^\u03c0(h, s, a)$", "content": "The joint history-state value function $Q^\u03c0(h, s, a)$ is a form of centralized value function.\n$Q^\u03c0(h, s, a)$ is the expected long-term performance of the team of agents when the unobserved\nenvironment state happens to be $s$, each individual policy agent $\u03c0_i$ has observed the individual\nhistory $h_i$, and has opted to perform a first action $a_i$. It is the unique solution to the following\njoint history-state Bellman equation,\n$Q^\u03c0(h, s, a) = R(s, a) + \u03b3 E_{s',o\\vert s,a} [\\Sigma_{a'} \u03c0(a'; hao)Q^\u03c0(hao, s', a')]$ (14)\nBecause this history-state value function has access to both history and state information,\nit does not suffer from the same issue as the state-only value function; Pr(a' | hao, s')\nis not only well defined, but can also be trivially reduced to the joint policy probability\n$Pr(a' | hao, s') = Pr(a' | hao) = \u03c0(a'; hao)$ due to the conditional independence between\nactions and states given histories,"}, {"title": "3.4 Multi-Agent Actor-Critic Methods", "content": "Actor-Critic methods (AC) (Konda & Tsitsiklis, 2000; Sutton, McAllester, Singh, & Mansour,\n2000) are variants of Policy Gradient (PG) approaches that involve the training of policy and\ncritic models. In this section, we define a number of standard centralized and decentralized\nmethods. We primarily consider centralized training of decentralized policies (Lowe et al.,\n2017; Bono, Dibangoye, Matignon, Pereyron, & Simonin, 2018; Lyu et al., 2021), where there\nis one policy model per agent (each separately parameterized by $\u03b8_i$), and a single centralized\ncritic model (parameterized by \u03c6). We will omit the model parameterization when clear\nfrom the context. To clearly distinguish critic models from the value functions that they are\ntrained to model, we denote them with a hat, e.g., $\\hat{V}$ is a critic model trained to model $V^\u03c0$.\nPolicy gradients are typically expressed in a form that depends on one of the previously\ndefined action-value functions $Q^\u03c0$ (or the respective advantage function $A^\u03c0$). Such values\ncan be estimated in a number of ways; in actor-critic, it is common to use one-step returns\nand the critic model to estimate $Q^\u03c0(h, a)$ as $r + \u03b3 \\hat{V}(hao)$, and $Q^\u03c0(s, a)$ as $r + \u03b3 \\hat{V}(s')$. In\nadvantage actor-critic, the critic model is further used as a baseline for variance reduction,\n$A^\u03c0(h, a) = Q^\u03c0(h, a) \u2013 V^\u03c0(h) \\approx r + \u03b3 \\hat{V}(hao) \u2013 \\hat{V}(h),$\n(15)\n$A^\u03c0(s, a) = Q^\u03c0(s, a) \u2013 V^\u03c0(s) \\approx r + \u03b3 \\hat{V}(s') \u2013 \\hat{V}(s)$.\n(16)"}, {"title": "3.4.1 Centralized Policy", "content": "In our evaluation, we also consider the case of a fully centralized policy, i.e., a policy that\ndoes not necessarily satisfy conditional independence $\u03c0(\u03b1; h) \\neq \\Pi_i \u03c0_i(a_i; h_i)$, and that\nfundamentally treats all agents as a single entity that shares all observations and actions.\nAlthough this represents a profoundly different control setting from decentralized control, it\nis a relevant and essential baseline that also represents an upper bound on the performance\nachievable by decentralized control. The fully centralized policy case is formalized as Joint\nActor-Critic (JAC) (Bono et al., 2018; Wang, Hao, Wang, & Taylor, 2019), which employs a\ncentralized history critic $\\hat{Q}(h, a; \u03c6)$ modeled after $Q^\u03c0(h, a)$ to update the fully centralized\npolicy \u3160 jointly parameterized by \u03b8. The gradient associated with JAC is\n$\\nabla_\u03b8 J = (1 \u2013 \u03b3) E_{h,a \\sim \u03c1(h,a)} [Q^\u03c0(h, a) \\nabla_\u03b8 log \u03c0(a; h, \u03b8)].$\n(17)"}, {"title": "3.4.2 Decentralized Policy and Critic", "content": "Independent Actor-Critic (IAC) Among the decentralized policy gradient variants, we\nfirst consider Independent Actor-Critic (IAC) (Peshkin, Kim, Meuleau, & Kaelbling, 2000;\nFoerster et al., 2018), which trains decentralized policy $\u03c0_i(a; h)$ and critic $\\hat{Q}_i(h, a)$ models\nfor each agent. Pseudocode for IAC is given in Algorithm 1. In IAC, the gradient associated\nwith the policy parameters $\u03b8_i$ can be derived as\n$\\nabla_{\u03b8_i} J = (1 \u2013 \u03b3) E_{h,a \\sim \u03c1(h,a)} [Q_i(h_i, a_i) \\nabla_{\u03b8_i} log \u03c0_i(a_i; h_i, \u03b8_i)].$\n(18)"}, {"title": "3.4.3 Decentralized Policy and Centralized Critic", "content": "Next, we consider gradient approximations that make use of centralized values and critics,\nwhich is the focus of this work; in some cases, the approximations will be perfect and\nequivalent to $\\nabla_{\u03b8_i} J(\u03b8_i)$, while in others they will differ. To distinguish them more clearly\nfrom the formally correct gradient $\\nabla_{\u03b8_i} J(\u03b8_i)$, we use g to denote these approximations.\nIndependent Actor with Centralized History Critic (IACC-H) IACC-H is a class\nof centralized critic methods where the joint history critic $\\hat{Q}(h, a; \u03c6)$ modeled after $Q^\u03c0(h, a)$\nis used to update each decentralized policy $\u03c0_i$ (Foerster et al., 2018; Bono et al., 2018).\nPseudocode for IACC-H is given in Algorithm 2. The gradient approximation associated\nwith IACC-H is\n$g_h = (1 - \u03b3) E_{h,a \\sim \u03c1(h,a)} [Q^\u03c0(h, a) \\nabla_{\u03b8_i} log \u03c0_i(a_i; h_i, \u03b8_i)].$\n(19)\nThe IACC-H approach can be found as one of the variants of COMA (Foerster et al., 2018),\nwhich also employs a variance reduction baseline.\nIndependent Actor with Centralized State Critic (IACC-S) IACC-S employs a\ncentralized state-based critic $\\hat{Q}(s, a; \u03c6)$ modeled after $Q^\u03c0(s, a)$ to update each decentralized\npolicy $\u03c0_i$. Pseudocode for IACC-S is given in Algorithm 3. The gradient approximation\nassociated with IACC-S is\n$g_s = (1 \u2212 \u03b3) E_{h,s,a \\sim \u03c1(h,s,a)} [Q^\u03c0(s, a)\\nabla_{\u03b8_i} log \u03c0_i(a_i; h_i, \u03b8_i)].$\n(20)\nThe IACC-S approach can be found both as a variant of COMA (Foerster et al., 2018) and\nin MADDPG (Lowe et al., 2017)."}, {"title": "Independent Actor with Centralized History-State Critic (IACC-HS)", "content": "IACC-HS employs a centralized history-state-based critic$\\hat{Q}(h, s, a; \u03c6)$ modeled after $Q^\u03c0(h, s, a)$ to update each decentralized policy $\u03c0_i$. Pseudocode for IACC-HS is given in Algorithm"}]}