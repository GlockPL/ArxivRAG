{"title": "On Centralized Critics in Multi-Agent Reinforcement Learning", "authors": ["Xueguang Lyu", "Andrea Baisero", "Yuchen Xiao", "Brett Daley", "Christopher Amato"], "abstract": "Centralized Training for Decentralized Execution, where agents are trained offline in a centralized fashion and execute online in a decentralized manner, has become a popular approach in Multi-Agent Reinforcement Learning (MARL). In particular, it has become popular to develop actor-critic methods that train decentralized actors with a centralized critic where the centralized critic is allowed access global information of the entire system, including the true system state. Such centralized critics are possible given offline information and are not used for online execution. While these methods perform well in a number of domains and have become a de facto standard in MARL, using a centralized critic in this context has yet to be sufficiently analyzed theoretically or empirically. In this paper, we therefore formally analyze centralized and decentralized critic approaches, and analyze the effect of using state-based critics in partially observable environments. We derive theories contrary to the common intuition: critic centralization is not strictly beneficial, and using state values can be harmful. We further prove that, in particular, state-based critics can introduce unexpected bias and variance compared to history-based critics. Finally, we demonstrate how the theory applies in practice by comparing different forms of critics on a wide range of common multi-agent benchmarks. The experiments show practical issues such as the difficulty of representation learning with partial observability, which highlights why the theoretical problems are often overlooked in the literature.", "sections": [{"title": "1. Introduction", "content": "Centralized Training for Decentralized Execution (CTDE) (Oliehoek, Spaan, & Vlassis, 2008), where agents are trained offline in a centralized manner but execute in a decentralized manner with only local information, has been widely adopted in multi-agent reinforcement learning (MARL). Compared to independent learning, CTDE has great potential for more stable and optimal learning since agents can coordinate offline on how they will behave online. Actor-Critic (AC) methods are popular for CTDE because a centralized critic can be used to train decentralized actors, exploiting the centralized training paradigm; since the critic is only needed to train the actors, it can be discarded once the actors are fully trained without hindering decentralized execution. Because the centralized critic is trained offline in a simulator, it can be trained on the joint observations from all the agents as well as the system state. Using the state is intuitively considered desirable as it is often more concise than the history and provides ground-truth information. This technique of exploiting the system state has become popular after the pioneering centralized critic works of COMA (Foerster, Assael,"}, {"title": "2. Related Work", "content": "The CTDE training paradigm is used in a number of recent deep MARL approaches. Value- based CTDE approaches such as QMIX, QPLEX, and others focus on how centralized values can be reasonably factorized into decentralized ones, and have shown promising results (Son, Kim, Kang, Hostallero, & Yi, 2019; Mahajan, Rashid, Samvelyan, & Whiteson, 2019; Wang, Dong, & Victor Lesser, 2020b; Rashid, Farquhar, Peng, & Whiteson, 2020; Peng, Rashid, Schroeder de Witt, Kamienny, Torr, B\u00f6hmer, & Whiteson, 2021; de Witt, Peng, Kamienny, Torr, B\u00f6hmer, & Whiteson, 2020; Xiao, Hoffman, Xia, & Amato, 2020; Wang, Wang, Zheng, & Zhang, 2020c; Rashid, Samvelyan, de Witt, Farquhar, Foerster, & Whiteson, 2018; Sunehag, Lever, Gruslys, Czarnecki, Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, et al., 2018). On the other hand, CTDE policy gradient methods are almost entirely based on centralized critics.\nOne of the first methods featuring a centralized critic was COMA (Foerster, Farquhar, Afouras, Nardelli, & Whiteson, 2018). COMA adopted a centralized state-based critic with a counterfactual baseline; the state-based critic then became the standard in many other approaches. Regarding convergence properties, COMA claims that the overall effect of a centralized critic on the decentralized policy gradient may be reduced to a single-agent actor- critic approach, which ensures convergence under similar assumptions (Konda & Tsitsiklis, 2000); however, the assumption only holds in fully-observable environments and is incorrect for partially observable environments. In this paper, we clarify and expand on the theory of centralized critics by developing the convergence properties and a bias/variance analysis for centralized and decentralized critics, and their respective policies. Due to their theoretical properties, we provide separate discussions for state-based and history-based critics.\nConcurrently with COMA, MADDPG (Lowe et al., 2017) proposed to use a dedicated centralized critic for each agent in semi-competitive domains, demonstrating compelling empirical results in continuous action environments.\nMany other agents extend the ideas of COMA and MADDPG. We discuss some of them but many more have been developed. M3DDPG (Li et al., 2019) focuses on the competitive case and extends MADDPG to learn robust policies against altering adversarial policies by optimizing a minimax objective. On the cooperative side, SQDDPG (Wang et al., 2020a) borrows the counterfactual baseline idea from COMA and extends MADDPG to achieve credit assignment in fully cooperative domains by reasoning over each agent's marginal contribution. Other researchers also use critic centralization for emergent communication with decentralized execution in TarMAC (Das et al., 2019) and ATOC (Jiang & Lu, 2018). There are also efforts utilizing an attention mechanism addressing scalability problems in MAAC (Iqbal & Sha, 2019). Also, teacher-student style transfer learning LeCTR (Omidshafiei, Kim, Liu, Tesauro, Riemer, Amato, Campbell, & How, 2019) builds on top of centralized critics, which does not assume expert teachers. Other work includes multi-agent credit assignment and exploration in LIIR and LICA (Du et al., 2019; Zhou et al., 2020), goal-conditioned policies with CM3 (Yang, Nakhaei, Isele, Fujimura, & Zha, 2020), and for temporally abstracted policies (Chakravorty, Ward, Roy, Chevalier-Boisvert, Basu, Lupu, & Precup, 2020). Extensive tests based on a centralized critic in a more realistic environment using self-play for hide-and-seek (Baker et al., 2020) have demonstrated impressive results showing emergent tool use. Note that impressive results also use a state-based critic, which is a common practice and used in works"}, {"title": "3. Background", "content": "This section introduces the formal problem definition of cooperative MARL with decentralized execution and partial observability. We introduce various forms of value functions and formalize a set of commonly accepted on-policy history (and state) distributions. We also introduce multi-agent actor-critic methods in which the value functions, both centralized and decentralized, are approximated by critic models. In the coming definitions and the rest of this document, we use \\$\\Delta X\\$ to denote the set of probability distributions over a set X."}, {"title": "3.1 Dec-POMDPS", "content": "Decentralized partially observable Markov decision processes (Dec-POMDPs) (Oliehoek & Amato, 2016) are multi-agent cooperative sequential decision making problems. A Dec-POMDP is a tuple \\$\\langle I, S, A, \\Omega, T, O, R, \\gamma \\rangle\\$, composed of a set of agents I, a state space S, with initial state \\$\\$s_0 \\in S\\$$, a joint action space \\$\\$A = \\times_{i \\in I} A_i\\$$, one per agent, a joint observation space \\$\\Omega = \\times_{i \\in I} \\Omega_i\\$$, one per agent, a stochastic state transition function \\$\\$T : S \\times A \\rightarrow \\Delta S\\$ that determines state transitions \\$\\$Pr(s' | s, a)\\$$, a stochastic joint observation function \\$\\$O: A \\times S \\rightarrow \\Delta \\Omega\\$ that determines observation emissions \\$\\$Pr(o | a, s)\\$$, a joint reward function \\$\\$R: S \\times A \\rightarrow R\\$ that is shared by all agents, and a discount factor \\$\\gamma \\in [0,1)\\$.\nControl in Dec-POMDPs is performed by a set of decentralized agent policies \\$\\pi = (\\pi_1,..., \\pi_{\\vert I \\vert})\\$$, each representing a (stochastic) mapping from each agents' individual action- observation history to its next action, \\$\\pi_i: H_i \\rightarrow \\Delta A_i\\$$, where Hi is the set of action-observation histories for agent i. For instance, agent i's history at timestep t is the sequence of all previous actions and observations \\$\\$h_{i,t} = (o_{i,0}, a_{i,0}, o_{i,1},..., a_{i,t-1}, o_{i,t})\\$$. A joint history is the set of all agent histories \\$\\$h_t = (h_{1,t},...,h_{\\vert I \\vert,t})\\$$. The set of actions chosen by each agent forms the joint action \\$\\$a_t = (a_{1,t}, ..., a_{\\vert I \\vert,t})\\$$. As feedback from the system, a scalar reward \\$\\$R(s_t, a_t)\\$\\$ is shared by all agents, and each agent receives a local observation \\$\\$(o_{1,t},..., o_{\\vert I \\vert,t}) \\sim O(a_{t-1}, s_t)\\$$.\nThe objective of all agents is to maximize the total performance, i.e., the expected discounted sum of future rewards \\$\\$J = E [\\sum_{t=0}^{\\infty} \\gamma^t R_t]\\$\\$"}, {"title": "3.2 Discounted Visitations: Counts and Distributions", "content": "The pairing of a Dec-POMDP with a set of agent policies \\$\\pi\\$ fully determines the (stochastic) behavior of the system, i.e., the marginal, joint, and conditional probability of the defined random variables (e.g., states and histories). However, before defining core RL concepts like value functions and describing policy gradient variants, it is helpful to define a set of functions related to the likelihood of occurrences of certain states or histories."}, {"title": "3.2.1 Discounted Visitation Counts", "content": "First, we define the discounted visitation counts function \\$\\eta\\$ , which represents a discounted notion of the expected number of times that the system variables take some given value. In the case of the discounted state visitations (also defined in a different but equivalent form for single-agent fully observable control in (Sutton & Barto, 2018)), we define\n\\$\\eta(s) = \\sum_{t=0}^{\\infty} \\gamma^t Pr(S_t = s).\\$                                                                                     (1)\nNote that \\$\\eta\\$ is formally a function of the joint policies \\$\\pi\\$ , and should technically be denoted as \\$\\eta^{\\pi}\\$ ; however, given the lack of ambiguity, we omit the suffix to simplify notation. The discount factor \\$\\gamma\\$ guarantees that \\$\\eta\\$ is well-defined and finite for any Dec-POMDP, even those that never terminate or keep visiting the same states ad infinitum-hence the importance of this discounted notion of visitations.\nWe will also use \\$\\eta\\$ for the number of times a (joint) history or (joint) history-state pairs are visited. We use the same overloaded symbol \\$\\eta\\$ due to similarity with the state visitation counts; the distinction is immediately clear from the inputs and context. In the case of history visitations and history-state visitations, we define\n\\$\\eta(h) = \\sum_{t=0}^{\\infty} \\gamma^t Pr(H_t = h), \\qquad \\eta(h, s) = \\sum_{t=0}^{\\infty} \\gamma^t Pr(H_t = h, S_t = s),\\$                                                                                    (2)\nWe note some relevant properties that either relate or are shared by \\$\\eta(s)\\$, \\$\\eta(h)\\$ , and \\$\\eta(h)\\$ . First, we note that \\$\\eta(s) = \\Sigma_h\\eta(h, s)\\$ and \\$\\eta(h) = \\Sigma_s\\eta(h, s)\\$ . Then, we note that all of these discounted counts ultimately add up to the same value, as a direct consequence of the geometric series component based on \\$\\gamma\\$ , i.e., \\$\\Sigma_s\\eta(s) = \\Sigma_h\\eta(h) = \\Sigma_{h,s} \\eta(h, s) = (1 - \\gamma)^{-1}\\$ . In the case of \\$\\eta(h)\\$ and \\$\\eta(h, s)\\$ , we also note that the sum over timesteps t is technically superfluous, since there is only one timestep where the joint history h can possibly be collectively seen by the agents; therefore, \\$\\eta(h)\\$ and \\$\\eta(h, s)\\$ are equivalently written as\n\\$\\eta(h) = \\gamma^t Pr(H_t = h)\\,\\vert_{t=h}, \\qquad \\eta(h, s) = \\gamma^t Pr(H_t = h, S_t = s)\\,\\vert_{t=h}\\$                                                                  (3)\nFinally, we further consider visitation variants that include action counts,\n\\$\\eta(s, a) = \\sum_{t=0}^{\\infty} \\gamma^t Pr(S_t = s, A_t = a),\\$                                                                                (4)\n\\$\\eta(h, a) = \\sum_{t=0}^{\\infty} \\gamma^t Pr(H_t = h, A_t = a),\\$                                                                                (5)\n\\$\\eta(h, s, a) = \\sum_{t=0}^{\\infty} \\gamma^t Pr(H_t = h, S_t = s, A_t = a),\\$                                                                              (6)\nwhich share similar properties to their action-less counterparts. Importantly, counts \\$\\eta(h, a)\\$ and \\$\\eta(h,s,a)\\$ are respectively related to \\$\\eta(h)\\$ and \\$\\eta(h,s)\\$ by the policy, according to \\$\\eta(h, a) = \\eta(h)\\pi(a; h)\\$ and \\$\\eta(h, s, a) = \\eta(h, s)\\pi(a; h)\\$ ."}, {"title": "3.2.2 Discounted Visitation Probabilities", "content": "Next, we define the discounted visitation probability functions \\$\\rho\\$ as normalized versions of the corresponding counts \\$\\eta\\$. Given that \\$\\eta\\$ adds up to \\$(1 - \\gamma)^{-1}\\$ , this results in\n\\$\\rho(s) = (1 - \\gamma)\\eta(s), \\qquad \\rho(h) = (1 - \\gamma)\\eta(h), \\qquad \\rho(h, s) = (1 - \\gamma)\\eta(h, s),\\$\n\\$\\rho(s, a) = (1 - \\gamma)\\eta(s, a), \\qquad \\rho(h, a) = (1 - \\gamma)\\eta(h,a), \\qquad \\rho(h, s, a) = (1 - \\gamma)\\eta(h, s, a).\\$                                                                           (7)\nEach \\$\\rho\\$ now represents a probability distribution over the space of its inputs. Further, common marginalization properties hold, e.g., \\$\\rho(s) = \\Sigma_h\\rho(h, s)\\$ , and \\$\\rho(h) = \\Sigma_s\\rho(h, s)\\$ . We can also further overload \\$\\rho\\$ to encompass a notion of conditional probability, e.g.,\n\\$\\rho(s \\vert h) = \\frac{\\eta(h,s)}{\\eta(h)}, \\qquad \\rho(h \\vert s) = \\frac{\\eta(h,s)}{\\eta(s)},\\$\n\\$\\rho(a \\vert h) = \\frac{\\eta(h, a)}{\\eta(h)}, \\qquad \\rho(a \\vert s) = \\frac{\\eta(s, a)}{\\eta(s)},\\$                                                                                (8)\nfor which common conditional properties also hold, e.g., \\$\\rho(s \\vert h) = \\rho(h,s) / \\rho(h)\\$ , \\$\\rho(h \\vert s) = \\rho(h, s) / \\rho(s)\\$ , \\$\\rho(a \\vert h) = \\rho(h, a) / \\rho(h)\\$ , and \\$\\rho(a \\vert s) = \\rho(s, a) / \\rho(s)\\$ . We also note that \\$\\rho(a \\vert h) = \\rho(a \\vert h,s) = \\pi(a; h)\\$ . Finally, we note that some of these \\$\\rho\\$ -function outputs are equivalent to direct probabilities induced by the Dec-POMDP's graphical model. Most notably, \\$\\rho(s \\vert h)\\$ is equivalent to the conditional state probability given the joint history \\$\\$Pr(s \\vert h)\\$$. For others, however, there is no such corresponding probability, e.g., \\$\\rho(h \\vert s)\\$ is mathematically well-defined, while \\$\\$Pr(h \\vert s)\\$\\$ is ill-defined without assuming a particular timestep for h (Baisero & Amato, 2022). We use \\$\\rho\\$ to primarily simplify the notation associated with policy gradients (Section 3.4), although we will also exploit the conditional-\\$\\rho\\$ functions to resolve a formal issue that appears in our definition of a specific form of centralized value function (Section 3.3.3)."}, {"title": "3.3 Value Functions", "content": "In this section, we formally define various types of value functions that are used in different forms of multi-agent policy gradient: the joint history value function \\$\\$Q^{\\pi}(h, a)\\$$, the individual- history value function \\$\\$Q_i^{\\pi}(h, a)\\$$, the state value function \\$\\$Q^{\\pi}(s, a)\\$$, and the joint history-state value function \\$\\$Q^{\\pi}(h, s, a)\\$$. These value functions all represent some notion of expected (discounted) performance obtained by the entire team of agents, that is given and indicated as a suffix (even for the individual history case \\$\\$Q_i^{\\pi}\\$ ), and they differ exclusively in terms of the information that is available to determine the expected team performance."}, {"title": "3.3.1 Joint History Value Function \\$\\$Q^{\\pi}(h,a)\\$\\$", "content": "The joint history value function \\$\\$Q^{\\pi}(h, a)\\$\\$ is a form of centralized value function, and the unique solution to the following joint history Bellman equality,\n\\$\\$Q^{\\pi}(h, a) = R(h, a) + \\gamma E_{o\\vert h,a} \\sum_{a'} \\pi(a'; hao)Q^{\\pi}(hao, a')\\$\\$                                                                 (9)\nwhere \\$\\$R(h, a) = E_{s \\vert h} [R(s, a)]\\$\\$ is the joint history reward function. \\$\\$Q^{\\pi}(h, a)\\$\\$ is the expected long-term performance of the team of agents when each individual agent policy \\$\\pi_i\\$ has observed the individual history \\$\\$h_i\\$ and has opted to perform a first action \\$\\$a_i\\$ ."}, {"title": "3.3.2 Individual History Value Function \\$\\$Q_i^{\\pi}(h, a)\\$\\$", "content": "The individual history value function \\$\\$Q_i^{\\pi}(h, a)\\$\\$ is a form of decentralized value function from the singular perspective of the i-th agent, and the unique solution to the following individual history Bellman equality,\n\\$\\$Q_i^{\\pi}(h, a) = R_i(h, a) + \\gamma E_{o\\vert h,a} \\sum_{a'} \\pi_i(a'; hao)Q_i^{\\pi}(hao, a')\\$\\$                                                               (10)\nwhere \\$\\$R_i(h, a) = E_{s,a\\backslash i \\vert h_i=h} [R(s, a)]\\$\\$ is the individual history reward function, which inte- grates out the behavior of all other agents and the resulting state. \\$\\$Q_i^{\\pi}(h, a)\\$\\$ is the expected long-term performance of the team of agents when the i-th agent policy \\$\\pi_i\\$ has observed the individual history h and has opted to perform a first action a."}, {"title": "3.3.3 State Value Function \\$\\$Q^{\\pi}(s,a)\\$\\$", "content": "The state value function \\$\\$Q^{\\pi}(s, a)\\$\\$ is a form of centralized value function that attempts to measure the expected long-term performance of the team of agents when the system state happens to be s, and each individual agent policy \\$\\pi_i\\$ has opted to perform a first action \\$\\$a_i\\$ . A straightforward (but na\u00efve, as we will see) formalization of this notion is based on defining the state value function as the unique solution to the following state Bellman equality,\n\\$\\$Q^{\\pi}(s, a) = R(s, a) + \\gamma E_{s'\\vert s,a} \\sum_{a'} Pr(a' \\vert s')Q^{\\pi}(s', a')\\$\\$                                                              (11)\nAlthough this notion of state value seems reasonable at the surface level, it suffers from a subtle formality issue that causes \\$\\$Pr(a \\vert s)\\$\\$ , and consequently \\$\\$Q(s, a)\\$\\$ itself, to be not guaranteeably well-defined for generic control problems and teams of agents; this is an issue intrinsic to partial observability that was already analyzed for the single-agent control case in (Baisero & Amato, 2022), and for the multi-agent control case in (Lyu et al., 2022).\nTo keep this introductory section brief and compact, we redirect a more thorough discussion on the issues with \\$\\$Pr(a \\vert s)\\$\\$ and Equation (11) to Appendix A. Broadly speaking, the issue is related to the fact that \\$\\$Pr(a \\vert s)\\$\\$ denotes a time-invariant relationship between variables that is conventionally time-variant, and is therefore undefined when a time index is not available. In fact, we note that there is no issue with a timed variant of the state value function \\$\\$Q^T(s, a)\\$\\$ defined as the solution to the following Bellman equality\n\\$\\$Q^T(s, a) = R(s, a) + \\gamma E_{s'\\vert s,a} \\sum_{a'} Pr(A_{t+1} = a' \\vert S_{t+1} = s')Q_{t+1}^T(s', a') \\qquad \\qquad (12)\nHowever, employing timed value functions is an unsatisfactory solution as they do not generalize well across different (and potentially many) timesteps, and is not a common practice in mainstream RL. We thus consider the following untimed alternative definition for state values \\$\\$Q^{\\pi}(s, a)\\$\\$ as the unique solution to the following state Bellman equality,\n\\$\\$Q^{\\pi}(s,a) = R(s, a) + \\gamma E_{s'\\vert s,a} \\sum_{a'} \\rho(a' \\vert s')Q(s', a')\\$\\$                                                               (13)"}, {"title": "3.3.4 Joint History-State Value Function \\$\\$Q^{\\pi}(h, s, a)\\$\\$", "content": "The joint history-state value function \\$\\$Q^{\\pi}(h, s, a)\\$\\$ is a form of centralized value function. \\$\\$Q^{\\pi}(h, s, a)\\$\\$ is the expected long-term performance of the team of agents when the unobserved environment state happens to be s, each individual policy agent \\$\\pi_i\\$ has observed the individual history \\$\\$h_i\\$ , and has opted to perform a first action \\$\\$a_i\\$ . It is the unique solution to the following joint history-state Bellman equation,\n\\$\\$Q^{\\pi}(h, s, a) = R(s, a) + \\gamma E_{s',o\\vert s,a} \\sum_{a'} [\\pi(a'; hao)Q^{\\pi}(hao, s', a')]\\qquad (14)\nBecause this history-state value function has access to both history and state information, it does not suffer from the same issue as the state-only value function; \\$\\$Pr(a' \\vert hao, s')\\$\\$ is not only well defined, but can also be trivially reduced to the joint policy probability \\$\\$Pr(a' \\vert hao, s') = Pr(a' \\vert hao) = \\pi(a'; hao)\\$\\$ due to the conditional independence between actions and states given histories."}, {"title": "3.4 Multi-Agent Actor-Critic Methods", "content": "Actor-Critic methods (AC) (Konda & Tsitsiklis, 2000; Sutton, McAllester, Singh, & Mansour, 2000) are variants of Policy Gradient (PG) approaches that involve the training of policy and critic models. In this section, we define a number of standard centralized and decentralized methods. We primarily consider centralized training of decentralized policies (Lowe et al., 2017; Bono, Dibangoye, Matignon, Pereyron, & Simonin, 2018; Lyu et al., 2021), where there is one policy model per agent (each separately parameterized by \\$\\theta_i\\$), and a single centralized critic model (parameterized by \\$\\phi\\$). We will omit the model parameterization when clear from the context. To clearly distinguish critic models from the value functions that they are trained to model, we denote them with a hat, e.g., \\$\\hat{V}\\$\\$ is a critic model trained to model \\$\\$V^{\\pi}\\$.\nPolicy gradients are typically expressed in a form that depends on one of the previously defined action-value functions \\$\\$Q^{\\pi}\\$\\$ (or the respective advantage function \\$\\$A^{\\pi}\\$\\$). Such values can be estimated in a number of ways; in actor-critic, it is common to use one-step returns and the critic model to estimate \\$\\$Q^{\\pi}(h, a)\\$\\$ as \\$\\$r + \\gamma \\hat{V}(hao)\\$$, and \\$\\$Q^{\\pi}(s, a)\\$\\$ as \\$\\$r + \\gamma \\hat{V}(s')\\$\\$. In advantage actor-critic, the critic model is further used as a baseline for variance reduction,\n\\$\\$A^{\\pi}(h, a) = Q^{\\pi}(h, a) - V^{\\pi}(h) \\approx r + \\gamma \\hat{V}(hao) - \\hat{V}(h),\\$\n\\$\\$A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s) \\approx r + \\gamma \\hat{V}(s') - \\hat{V}(s) .\\$\\$                                                                           (16)"}, {"title": "3.4.1 Centralized Policy", "content": "In our evaluation, we also consider the case of a fully centralized policy, i.e., a policy that does not necessarily satisfy conditional independence \\$\\pi(a; h) \\neq \\prod_i \\pi_i(a_i; h_i)\\$$, and that fundamentally treats all agents as a single entity that shares all observations and actions. Although this represents a profoundly different control setting from decentralized control, it is a relevant and essential baseline that also represents an upper bound on the performance achievable by decentralized control. The fully centralized policy case is formalized as Joint Actor-Critic (JAC) (Bono et al., 2018; Wang, Hao, Wang, & Taylor, 2019), which employs a centralized history critic \\$\\$Q(h, a; \\phi)\\$\\$ modeled after \\$\\$Q^{\\pi}(h, a)\\$\\$ to update the fully centralized policy \\$\\pi\\$ jointly parameterized by \\$\\theta\\$ . The gradient associated with JAC is\n\\$\\$\\nabla_{\\theta} J = (1 - \\gamma) E_{h,a\\sim\\rho(h,a)} [Q^{\\pi}(h, a) \\nabla_{\\theta} \\log \\pi(a; h, \\theta)].\\$\\$                              (17)"}, {"title": "3.4.2 Decentralized Policy and Critic", "content": "Independent Actor-Critic (IAC) Among the decentralized policy gradient variants, we first consider Independent Actor-Critic (IAC) (Peshkin, Kim, Meuleau, & Kaelbling, 2000; Foerster et al., 2018), which trains decentralized policy \\$\\pi_i(a; h)\\$\\$ and critic \\$\\$Q_i(h, a)\\$\\$ models for each agent. Pseudocode for IAC is given in Algorithm 1. In IAC, the gradient associated with the policy parameters \\$\\theta_i\\$ can be derived as\n\\$\\$\\nabla_{\\theta_i} J = (1 - \\gamma) E_{h,a\\sim\\rho(h,a)} [Q_i(h_i, a_i) \\nabla_{\\theta_i} \\log \\pi_i(a_i; h_i, \\theta_i)].\\$\\$                            (18)"}, {"title": "3.4.3 Decentralized Policy and Centralized Critic", "content": "Next, we consider gradient approximations that make use of centralized values and critics, which is the focus of this work; in some cases, the approximations will be perfect and equivalent to \\$\\nabla_{\\theta_i} J(\\theta_i)\\$$, while in others they will differ. To distinguish them more clearly from the formally correct gradient \\$\\nabla_{\\theta_i} J(\\theta_i)\\$$, we use \\$\\$g\\$\\$ to denote these approximations.\nIndependent Actor with Centralized History Critic (IACC-H) IACC-H is a class of centralized critic methods where the joint history critic \\$\\$Q(h, a; \\phi)\\$\\$ modeled after \\$\\$Q^{\\pi}(h, a)\\$\\$ is used to update each decentralized policy \\$\\pi_i\\$ (Foerster et al., 2018; Bono et al., 2018). Pseudocode for IACC-H is given in Algorithm 2. The gradient approximation associated with IACC-H is\n\\$\\$g_h = (1 - \\gamma) E_{\\pi,a\\sim\\rho(h,a)} [Q^{\\pi} (h, a) \\nabla_{\\theta_i} \\log \\pi_i(a_i; h_i, \\theta_i)].\\$\\$                                  (19)\nThe IACC-H approach can be found as one of the variants of COMA (Foerster et al., 2018), which also employs a variance reduction baseline.\nIndependent Actor with Centralized State Critic (IACC-S) IACC-S employs a centralized state-based critic \\$\\$Q(s, a; \\phi)\\$\\$ modeled after \\$\\$Q^{\\pi}(s, a)\\$\\$ to update each decentralized policy \\$\\pi_i\\$\\$. Pseudocode for IACC-S is given in Algorithm 3. The gradient approximation associated with IACC-S is\n\\$\\$g_s = (1 - \\gamma) E_{h,s,a\\sim\\rho(h,s,a)} [Q^{\\pi} (s, a) \\nabla_{\\theta_i} \\log \\pi_i(a_i; h_i, \\theta_i)].\\$\\$                                (20)\nThe IACC-S approach can be found both as a variant of COMA (Foerster et al., 2018) and in MADDPG (Lowe et al., 2017)."}, {"title": "Independent Actor with Centralized History-State Critic (IACC-HS)", "content": "IACC-HS employs a centralized history-state-based critic \\$\\$Q(h, s, a; \\phi)\\$\\$ modeled after \\$\\$Q^{\\pi}(h, s, a)\\$\\$ to update each decentralized policy \\$\\pi_i\\$\\$. Pseudocode for IACC-HS is given in Algorithm 3. The gradient approximation associated with IACC-HS is\n\\$\\$g_{h,s} = (1 - \\gamma) E_{h,s,a\\sim\\rho(h,s,a)} [Q^{\\pi} (h, s, a) \\nabla_{\\theta_i} \\log \\pi_i(a_i; h_i, \\theta_i)", "a)\\right": "g_h=\\mathbb{E}_{h, a \\sim \\rho(h, a)}\\left[\\hat{g}_h(h, a)\\right"}]}