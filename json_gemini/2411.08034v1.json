{"title": "SCALING PROPERTIES OF DIFFUSION MODELS\nFOR PERCEPTUAL TASKS", "authors": ["Rahul Ravishankar", "Zeeshan Patel", "Jathushan Rajasegaran", "Jitendra Malik"], "abstract": "In this paper, we argue that iterative computation with diffusion models offers a\npowerful paradigm for not only generation but also visual perception tasks. We\nunify tasks such as depth estimation, optical flow, and segmentation under image-\nto-image translation, and show how diffusion models benefit from scaling training\nand test-time compute for these perception tasks. Through a careful analysis of\nthese scaling behaviors, we present various techniques to efficiently train diffusion\nmodels for visual perception tasks. Our models achieve improved or comparable\nperformance to state-of-the-art methods using significantly less data and compute.\nWe release code and models at scaling-diffusion-perception.github.io.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion and autoregressive models have emerged as powerful techniques for generating images\nand videos, while showing excellent scaling behaviors. One could attribute part of the success of\ndiffusion models to iterative computation. At inference, diffusion models perform iterative denois-\ning, consuming approximately one to two orders of magnitude more compute compared to a single\nforward pass.\nIn this work, we explore how to efficiently scale diffusion model training and inference for down-\nstream perceptual tasks (inverse problems). Marigold (Ke et al., 2024) and FlowDiffuser (Luo et al.,\n2024) partially answered this question for depth estimation and optical flow individually, showing\nthat diffusion models are indeed suited for such perceptual tasks. Our work explores how to unify\na wide range of perceptual tasks, from low-level optical flow to mid-level depth estimation and\nhigh-level semantic segmentation / occlusion reasoning, through a detailed study on train/test-time\ncompute scaling behavior.\nTo study the scaling properties of diffusion models for inverse problems both at training and test-\ntime, we pre-train various dense and mixture-of-expert models for class-conditional image genera-\ntion. Then, we fine-tune these pre-trained models on downstream perception tasks. We study scaling\nproperties of fine-tuning by varying model size, data resolution, and pre-training compute. In addi-\ntion, we apply efficient training strategies such as upcycling dense checkpoints to mixture-of-experts\nmodels. We also experiment with various test-time compute scaling techniques, including increas-\ning diffusion steps, test-time ensembling, increasing active model experts, and reorganizing compute\nwith noise variance schedules.\nWe display the effectiveness of leveraging iterative feedback computation through diffusion models\nfor visual perception tasks with three key contributions:\n\u2022 We perform an in-depth study of train/test-time compute scaling laws across all layers\nof the stack, including pre-training, fine-tuning, and diffusion inference. Specifically, we\nperform our study on the monocular depth estimation task.\n\u2022 We show how to transfer the scaling laws derived for depth estimation to boost performance\non tasks such as optical flow or amodal segmentation for both training and inference.\n\u2022 We apply all of our scaling strategies to efficiently train a generalist mixture-of-experts\nmodel on perception tasks, achieving state-of-the-art results across various benchmarks."}, {"title": "2 RELATED WORK", "content": "Generative Modeling: Generative modeling has been studied under various methods, including\nVAEs (Kingma, 2013), GANs (Goodfellow et al., 2014), Normalizing Flows (Rezende & Mohamed,\n2015), Autoregressive models (van den Oord et al., 2016), and Diffusion models (Sohl-Dickstein\net al., 2015; Ho et al., 2020). Denoising Diffusion Probabilistic Models (DDPMs) (Ho et al., 2020)\nhave shown impressive scaling behaviors, becoming the de facto generative tool for many image\nand video generation models. Notable examples include Latent Diffusion Models (Rombach et al.,\n2022), which enhanced efficiency by operating in a compressed latent space, Imagen (Saharia et al.,\n2022), which generates samples in pixel space with increasing resolution, and Consistency Mod-\nels (Song et al., 2023), which aim to accelerate sampling while maintaining generation quality. Re-\ncent methods like Rectified Flow (Liu et al., 2022) and Flow Matching (Lipman et al., 2023) employ\ntraining objectives inspired by optimal transport to model continuous vector fields that map data to\ntarget distributions, eliminating the discrete steps typical in diffusion models. Rectified Flow miti-\ngates numerical issues in training by applying flow regularization, and Flow Matching offers efficient\nsampling with fewer discretization artifacts, making them promising alternatives to diffusion-based\napproaches for high-quality generation. Apart from diffusion models, Parti (Yu et al., 2022) and\nMARS (He et al., 2024) showcased the potential of autoregressive models for image generation, and\nthe Muse architecture (Chang et al., 2023) introduced a masked image generation approach using\ntransformers.\nScaling Diffusion Models: Diffusion modeling has shown impressive scaling behaviors in terms\nof data, model size, and compute. Latent Diffusion Models (Rombach et al., 2022) first showed\nthat training with large-scale web datasets can achieve high quality image generation results with\na U-Net backbone. DiT (Peebles & Xie, 2023) explored scaling diffusion models with the trans-\nformer architecture, presenting desirable scaling properties for class-conditional image generation.\nLater, Li et al.(Li et al., 2024) studied alignment scaling laws of text-to-image diffusion models.\nRecently, Fei et al. (Fei et al., 2024a) trained mixture-of-experts DiT models up to 16B parameters,\nachieving high-quality image generation results. Upcycling is another way to scale transformer mod-\nels. Komatsuzaki et al. (Komatsuzaki et al., 2022) used upcycling to convert a dense transformer-\nbased language model to a mixture-of-experts model without pre-training from scratch. Similarly,\nEC-DiTSun et al. (2024) explores how to exploit heterogeneous compute allocation in mixture-of-\nexperts training for DiT models through expert-choice routing and learning to adaptively optimize\nthe compute allocated to specific text-image data samples.\nDiffusion Models for Perception Tasks: Diffusion models have also been used for various down-\nstream visual tasks such as depth estimation (Ji et al., 2023; Duan et al., 2023; Saxena et al., 2023;\n2024; Zhao et al., 2023). More recently, Marigold (Ke et al., 2024) and GeoWizard (Fu et al., 2024)\ndisplayed impressive results by repurposing pre-trained diffusion models for monocular depth esti-\nmation. Diffusion models with few modifications are used for semantic segmentation for categorical\ndistributions (Hoogeboom et al., 2021; Brempong et al., 2022; Tan et al., 2022; Amit et al., 2021;\nBaranchuk et al., 2021; Wolleb et al., 2022), instance segmentation (Gu et al., 2024), and panoptic\nsegmentation (Chen et al., 2023). Diffusion models are also used for optical flow (Luo et al., 2024;\nSaxena et al., 2024) and 3D understanding (Liu et al., 2023; Jain et al., 2022; Poole et al., 2022;\nWang et al., 2023; Watson et al., 2022)."}, {"title": "3 GENERATIVE PRE-TRAINING", "content": "We first explore how to efficiently scale diffusion model pre-training. We pre-train diffusion models\nfor class-conditional image generation using a diffusion transformer (DiT) backbone and follow the\noriginal model training recipe (Peebles & Xie, 2023).\nStarting with a target RGB image $I \\in \\mathbb{R}^{u \\times u \\times 3}$, where the resolution of the image is $u \\times u$, our\npretrained, frozen Stable Diffusion variational autoencoder (Rombach et al., 2022) compresses the\ntarget to a latent $z_0 \\in \\mathbb{R}^{w \\times w \\times 4}$, where $w = u/8$. Gaussian noise is added at sampled time steps to\nobtain a noisy target latent. Noisy samples are generated as:\n$z_t = \\sqrt{a_t} \\cdot z_0 + \\sqrt{1 - a_t} \\cdot \\epsilon_t$ (1)\nfor timestep t. The noise is distributed as $\\epsilon \\sim N(0, I)$, $t \\sim \\text{Uniform}(T)$, with $T = 1000$ and\n$a_t := \\prod_{i=1}^t (1 - \\beta_i)$, with ${\\beta_1,...,\\beta_T}$ as the variance schedule of a process.\nIn the denoising process, the class-conditional DiT $f_\\theta(\\cdot)$, parameterized by learned parameters $\\theta$,\ngradually removes noise from $z_t$ to obtain $z_{t-1}$. The parameters $\\theta$ are updated by noising $z_0$ with\nsampled noise $\\epsilon$ at a random timestep $t$, computing the noise estimate $\\hat{\\epsilon}_t$, and optimizing the mean\nsquared loss between the generated noise and estimated noise in an $n$ batch size sample. We formally\nrepresent this as the following minimization problem:\n$\\theta^* = \\underset{\\theta}{\\text{arg min }} L_\\theta(z_t, \\epsilon_t) = \\underset{\\theta}{\\text{arg min }} \\frac{1}{n} \\sum_{i=1}^n (\\epsilon_i - \\hat{\\epsilon}_i)^2,$ (2)\nwhere $\\theta^*$ are the DiT learned parameters and $\\hat{\\epsilon}_i$ is the DiT noise prediction for sample $i$."}, {"title": "3.1 MODEL SIZE", "content": "We pre-train six different dense DiT models\nas in Table 1, increasing model size by vary-\ning the number of layers and hidden dimension\nsize. We use Imagenet-1K (Russakovsky et al.,\n2015) as our pre-training dataset and train all\nmodels for 400k iterations with a fixed learn-\ning rate of 1e-4 and a batch size of 256. Fig. 2\nshows that larger models converge to lower loss\nwith a clear power law scaling behavior. We\nshow the train loss as a function of compute (in\nMACs), and our predictions indicate a power\nlaw relationship of $L(C) = 0.23 \\times C^{-0.0098}$.\nOur pre-training experiments display the ease\nof scaling DiT with a small training dataset,\nwhich translates directly to efficiently scaling\ndownstream model performance."}, {"title": "3.2 MIXTURE OF EXPERTS", "content": "We also pre-train Sparse Mixture of Experts (MoE) models (Shazeer et al., 2017), following the S/2\nand L/2 model configurations in (Fei et al., 2024b). We use three different MoE configurations listed\nin Table 2, scaling the total parameter count by increasing hidden size, number of experts, layers, and\nattention heads. Each MoE block activates the top-2 experts per token and has a shared expert that is\nused by all tokens. To alleviate issues with expert balance, we use the proposed expert balance loss\nfunction from (Fei et al., 2024b) which distributes the load across experts more efficiently. Sparse\nMoE pre-training allows for a higher parameter count while increasing throughput, making it more\ncompute efficient than training a dense DiT model of the same size. We train our DiT-MoE models\nwith the same training recipe as the dense DiT model using ImageNet-1K. Our approach enables\ntraining DiT-MoE models to increase model capacity without increasing compute usage by another\norder of magnitude, which would be required to train dense models of similar sizes."}, {"title": "4 FINE-TUNING FOR PERCEPTUAL TASKS", "content": "In this section, we explore how to scale the fine-tuning of the pre-trained DiT models to maximize\nperformance on downstream perception tasks. During fine-tuning, we utilize the image-to-image\ndiffusion process from (Ke et al., 2024) and (Brooks et al., 2023) as our training recipe. We pose all\nour visual tasks as conditional denoising diffusion generation. Give an RGB image $I \\in \\mathbb{R}^{u \\times u \\times 3}$ and\nits pair ground truth image $D \\in \\mathbb{R}^{u \\times u \\times 3}$, we first project them to the latent space, $i_0 \\in \\mathbb{R}^{w \\times w \\times 4}$\nand $d_0 \\in \\mathbb{R}^{w \\times w \\times 4}$, respectively. We only add noise to the ground truth latent to obtain $d_t$ and\nconcatenate it with the RGB latent which results in a tensor $z_t = \\{i_0, d_t\\}$. The first convolutional\nlayer of the DiT model is modified to match the doubled number of input channels, and its values\nare reduced by half to make sure the predictions are the same if the inputs are just RGB images (Ke\net al., 2024). Finally, we perform diffusion training by denoising the ground truth image. We\nablate several fine-tuning compute scaling techniques on the monocular depth estimation task and\nreport Absolute Relative error and Deltal error. We transfer the best configurations from the depth\nestimation ablation study to fine-tune for other visual perception tasks."}, {"title": "4.1 EFFECT OF MODEL SIZE", "content": "We fine-tune the pre-trained a1-a6 dense models on the depth estimation task to study the effect of\nmodel size. We scale model size as shown in as described in Section 3.1. Fig. 3 shows that larger\ndense DiT models predictably converge to a lower fine-tuning loss, presenting a clear power law\nscaling behavior. We plot the train loss and validation metrics as a function of compute (in MACs).\nOur fine-tuned model predictions show a power law relationship in both depth Absolute Relative\nerror and depth Deltal error. These experiments provide strong signal on how model performance\nwill scale as we increase fine-tuning compute by scaling model size."}, {"title": "4.2 EFFECT OF PRE-TRAINING COMPUTE", "content": "We also investigate the behavior of fine-tuning as we scale the number of pre-training steps for the\nDiT backbone. We train four models with the a4 configuration using a varied number of pre-training\nsteps, keeping all other hyperparameters constant. We then fine-tune these four models on the same\ndepth estimation dataset.Fig. 4 displays the power law scaling behavior of the validation metrics for\ndepth estimation as we increase DiT pre-training steps. Our experiments show that having stronger\npre-trained representations can be helpful when scaling fine-tuning compute."}, {"title": "4.3 EFFECT OF IMAGE RESOLUTION", "content": "The sequence length of each image also affects the total compute spent during training. For each\nforward pass, we can scale the amount of compute used by simply increasing the resolution of the\nimage, which will increase the number of tokens in the image embedding. By increasing the number\nof tokens, we can increase the amount of information the model can learn from at training time to\nbuild stronger internal representations, which can in turn improve downstream performance. We use\ndense DiT-XL models with resolutions of 256 \u00d7 256 and 512 \u00d7 512 from (Peebles & Xie, 2023)\nand we pre-train DiT-MoE L/2-8E2A models with 256 \u00d7 256 and 512 \u00d7 512 resolutions following\nthe recipe in (Fei et al., 2024b). We then fine-tune each of these models with the corresponding\nresolution for the depth estimation task. Fig. 5 displays that increasing image resolution to scale\nfine-tuning compute can provide significant gains on downstream depth estimation performance."}, {"title": "4.4 EFFECT OF UPCYCLING", "content": "Sparse MoE models are efficient options for increasing the capacity of a model, but pre-training an\nMoE model from scratch can be expensive. One way to alleviate this issue is Sparse MoE Upcycling\n(Komatsuzaki et al., 2023). Upcycling converts a dense transformer checkpoint to an MoE model by\ncopying the MLP layer in each transformer block E times, where E is the number of experts, and\nadding a learnable router module that sends each token to the top-k selected experts. The outputs\nof the selected experts are then combined in a weighted sum at the end of each MoE block. We\nupcycle various dense DiT models after they are fine-tuned for depth estimation and then continue\nfine-tuning the upcycled model. Fig. 6 displays the scaling laws for upcycling, providing an average\nimprovement of 5.3% on Absolute Relative Error and 8.6% on Deltal error."}, {"title": "5 SCALING TEST-TIME COMPUTE", "content": "Scaling test-time compute has been explored for autoregressive Large Language Models (LLMs)\nto improve performance on long-horizon reasoning tasks (Brown et al., 2024; Snell et al., 2024;\nEl-Refai et al., 2024; OpenAI, 2024). In this section, we show how to reliably improve diffusion\nmodel performance for perceptual tasks by scaling test-time compute. We summarize our approach\nin Fig. 7. We use the Stable-Diffusion VAE to encode the input image into latent space (Rombach\net al., 2022). Then, we sample a target noise latent from a standard Gaussian distribution, which is\niteratively denoised with DDIM (Song et al., 2021) to generate the downstream prediction."}, {"title": "5.1 EFFECT OF SCALING INFERENCE STEPS", "content": "The most natural way of scaling diffusion inference is by increasing denoising steps. Since the model\nis trained to denoise the input at various timesteps, we can scale the number of diffusion denoising\nsteps at test-time to produce finer, more accurate predictions. This coarse-to-fine denoising paradigm\nis also reflected in the generative case, and we can take advantage of it for the discriminative case\nby increasing the number of denoising steps. In Fig. 8, we observe that increasing the total test-time\ncompute by simply increasing the number of diffusion sampling steps provides substantial gains in\ndepth estimation performance."}, {"title": "5.2 EFFECT OF TEST TIME ENSEMBLING", "content": "We also explore scaling inference compute with test-time ensembling. We exploit the fact that\ndenoising different noise latents will generate different downstream predictions. In test-time ensem-\nbling, we compute N forward passes for each input sample and reduce the outputs through one of\ntwo methods. The first technique is naive ensembling where we use the pixel-wise median across all\noutputs as the prediction. The second technique presented in Marigold (Ke et al., 2024) is median\ncompilation, where we collect predictions $\\{d_1,...,d_N\\}$ that are affine-invariant, jointly estimate\nscale and shift parameters $s_i$ and $t_i$, and minimize the distances between each pair of scaled and\nshifted predictions $(d'_i, d'_j)$ where $d' = d \\times \\hat{s} + \\hat{t}$. For each optimization step, we take the pixel-\nwise median $m(x, y) = \\text{median}(d_1(x, y), ..., d'_N(x, y))$ to compute the merged depth $m$. Since it\nrequires no ground truth, we scale ensembling by increasing N to utilize more test-time compute."}, {"title": "5.3 EFFECT OF NOISE VARIANCE SCHEDULE", "content": "We can also scale test-time compute by increasing compute usage at different points of the denoising\nprocess. In diffusion noise schedulers, we can define a schedule for the variance of the Gaussian\nnoise applied to the image over the total diffusion timesteps T. Tuning the noise variance schedule\nallows for reorganizing compute by allocating more compute to denoising steps earlier or later in\nthe noise schedule. We experiment with three different noise level settings for DDIM: linear, scaled\nlinear, and cosine. Cosine scheduling from (Nichol & Dhariwal, 2021) linearly declines from the\nmiddle of the corruption process, ensuring the image is not corrupted too quickly as in linear sched-\nules. Fig. 10 shows that the cosine noise variance schedule outperforms linear schedules for DDIM\non the depth estimation task under a fixed compute budget."}, {"title": "6 PUTTING IT ALL TOGETHER", "content": "Using the lessons from our scaling experiments on depth estimation, we train diffusion models\nfor optical flow prediction and amodal segmentation. We show that using diffusion models while\nconsidering efficient methods to scale training and test-time compute can provide substantial per-\nformance gains on visual perception tasks, achieving improved or similar performance as current\nstate-of-the-art techniques. Our experiments provide insight on how to efficiently apply diffusion\nmodels for these visual perception tasks under limited compute budgets. Finally, we train a unified\nexpert model, capable of performing all three visual perception tasks previously mentioned, dis-\nplaying the generalizability of our method. Our results prove the effectiveness of our training and\ntest-time scaling strategies, removing the need to use pre-trained models trained on internet-scale\ndatasets to enable high-quality visual perception in diffusion models. Fig. 11 displays the predicted\nsamples from our models."}, {"title": "6.1 DEPTH ESTIMATION", "content": "We combine our findings from the ablation studies on depth estimation to create a model with\nthe best training and inference configurations. We train a DiT-XL model from (Peebles & Xie,\n2023) on depth estimation data from Hypersim for 30K steps with a batch size of 1024, resolution\nof 512 x 512, and a learning rate exponentially decaying from 1.2e-4 to 1.2e-6. We use median\ncompilation ensembling with a cosine noise variance schedule. From our scaling experiments, we\nfound the optimal configuration for inference to be 200 denoising steps with N = 5 samples for\nensembling. As shown in Table 3, our model achieves the same validation performance as Marigold\non the Hypersim dataset, while being trained with lower resolution images and approximately three\norders of magnitude less pre-training data and compute."}, {"title": "6.2 OPTICAL FLOW PREDICTION", "content": "Optical flow estimation involves predicting the motion of objects between consecutive frames in\na video, represented as a dense vector field indicating pixel-wise displacement. We use a similar\nconfiguration as the depth estimation model for optical flow training. We train a DiT-XL model on\nthe FlyingChairs dataset for 40K steps with batch size of 1024, resolution of 512 \u00d7 512, and learning\nrate exponentially decaying from 1.2e-4 to 1.2e-6. We compare our model's performance with other\nspecialized optical flow prediction techniques in Table 4."}, {"title": "6.3 AMODAL SEGMENTATION", "content": "Amodal segmentation is the process of predicting the complete shape and extent of objects in an\nimage, including the portions that are occluded or not directly visible, which can require higher-level\nreasoning for complex scenes. We fine-tune a DiT-XL model on the pix2gestalt dataset (Ozguroglu\net al., 2024) for 6K steps with a batch size of 4096, resolution of 256 \u00d7 256, and learning rate\nexponentially decaying from 1.2e-4 to 1.2e-6. We compare our model with other methods in Table 5."}, {"title": "6.4 ONE MODEL FOR ALL", "content": "We train a unified DiT-XL model for each of the different tasks. We train this model on a mixed\ndataset consisting of all three tasks. To train this generalist model, we modify the DiT-XL architec-\nture by replacing the patch embedding layer with a separate PatchEmbedRouter module, which\nroutes each VAE embedding to a specific input convolutional layer based perception task. This\nensures the DiT-XL model is able to distinguish between the task-specific embeddings during fine-\ntuning. We use a similar training recipe as the previous experiments, using images with 512 \u00d7 512\nresolution and a learning rate exponentially decaying from 1.2e-4 to 1.2e-6. Then, we upcycle the\nfine-tuned DiT-XL checkpoint to an DiT-XL-8E2A model, and continue fine-tuning for another 4K\niterations. We display the generated predictions in Fig. 11 which exemplify the generalizability and\ntransferability of our scaling techniques across a variety of perception tasks."}, {"title": "7 CONCLUSION", "content": "In our work, we examine the scaling properties of diffusion models for visual perception tasks. We\nexplore various approaches to scale diffusion training, including increasing model size, mixture-\nof-experts models, increasing image resolution, and upcycling. We also efficiently scale test-time\ncompute by exploiting the iterative nature of diffusion, which significantly improves downstream\nperformance. Our experiments provide strong evidence of scaling, uncovering power laws across\nvarious training and inference scaling techniques. We hope to inspire future work in scaling training\nand test-time compute for iterative generative paradigms such as diffusion for perception tasks."}]}