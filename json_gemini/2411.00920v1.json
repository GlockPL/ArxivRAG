{"title": "COMPARATIVE EVALUATION OF APPLICABILITY DOMAIN DEFINITION METHODS FOR REGRESSION MODELS", "authors": ["Shakir Khurshid", "Bharath Kumar Loganathan", "Matthieu Duvinage"], "abstract": "The applicability domain refers to the range of data for which the prediction of the predictive model is expected to be reliable and accurate and using a model outside its applicability domain can lead to incorrect results. The ability to define the regions in data space where a predictive model can be safely used is a necessary condition for having safer and more reliable predictions to assure the reliability of new predictions. However, defining the applicability domain of a model is a challenging problem, as there is no clear and universal definition or metric for it. This work aims to make the applicability domain more quantifiable and pragmatic. Eight appicability domain detection techniques were applied to seven regression models, trained on five different datasets, and their performance was benchmarked using a validation framework. We also propose a novel approach based on non-deterministic Bayesian neural networks to define the applicability domain of the model. Our method exhibited superior accuracy in defining the Applicability Domain compared to previous methods, highlighting its potential in this regard.", "sections": [{"title": "INTRODUCTION", "content": "Software systems utilizing predictive modelling are used in multiple industries. However, when deployed in a production environment there is no definitive way to measure the reliability of their predictions. So the question arises \"Can their prediction be trusted?\". Currently, the main way of evaluating a model is by measuring its test error however in the real-time environment we do not have a test set to measure the accuracy of the predictions, thus we need alternative methods to validate the predictions.\nThe primary motivation to undergo this work was to get a quantifiable metric that can measure the reliability of the predictions of the prediction model used in the vaccine production process. At GSK in the vaccine manufacturing process, sensors installed in fermenters used in the vaccine manufacturing process collect data such as dissolved oxygen, pressure level and temperature. This data is then fed into a machine-learning model that predicts the yield of the vaccine for that batch. However, these models are typically developed based on historical data, process parameters, and specific conditions under which the vaccines were produced. However, the production environment is subject to variability, including changes in raw materials, equipment performance, and environmental conditions, which may affect the accuracy of the predictions, causing a risk of overestimating or underestimating the production capacity, leading to vaccine shortages, inefficient resource allocation or supply chain disruptions Without a clear understanding of the limits and boundaries of our models, there is a risk of blindly applying them to scenarios for which they may not be suitable or reliable. This lack of awareness can lead to erroneous predictions, faulty decision-making, and potential negative consequences. Thus, defining a model's applicability domain allows us to gain insight into the model's scope and limitations, enabling us to make safer predictions in real time.\nThe main questions that this research seeks to address are: 1. How can we effectively define and determine the applicability domain of a model? 2. Establish a robust evaluation framework to compare and assess the performance of different techniques to define the applicability domain. To answer these questions, we will investigate several existing methods for detecting and quantifying AD in prediction models, including statistical metrics, distribution-based approaches, and deep learning-based techniques. We will also propose a new method based on a Bayesian neural network to do the same."}, {"title": "APPLICABILITY DOMAIN", "content": "The applicability domain of a model refers to the region or range of input data where the model's predictions are expected to be reliable and accurate. It represents the domain or space of inputs for which the model is deemed appropriate or applicable. The concept of the applicability domain is important because models are typically trained on specific types or ranges of data, and their performance may vary outside of that range. The applicability domain helps us understand the limitations and validity of a model's predictions based on the input data characteristics. It can be influenced by various factors, including the nature of the data used for training the model, the complexity of the model, and the assumptions made during model development. It can be defined in different ways, depending on the specific context and requirements of the model. If a new data point falls within the applicability domain, it is expected that the model's predictions will be reliable for that point. However, if a data point falls outside the applicability domain, the model's predictions may be less accurate or less reliable. Different techniques can be employed to assess the applicability domain, such as measuring the similarity of new data to the training data using distance metrics, examining the distributional characteristics of the data, or using domain-specific knowledge and expert judgment. Un-"}, {"title": "Applicability Domain Measures", "content": "Sushko et al. [1] introduced the term Distance to Models, which is an abstract concept that is used to define the Applicability domain. It is not an actual distance. It represents a metric measure that defines the similarity between the training set and test set data points or unseen inputs (in the production phase) for a given predictive model. It is defined as monotonically increasing as the (expected) accuracy of the model decreases. However, this term is misleading since most measures are not distances. Therefore to avoid potential confusion we used the term Applicability Domain Measure [2] instead of Distance to Model. In accordance with Sushko et al. [1], the data points which have larger values of AD measure are by definition expected to have lower prediction accuracy than data points that have smaller values of AD measure. It should be clearly stated that prediction accuracy correlates with AD measure only on av-erage: for example, data points in the range [0.1, 0.3] generally exhibit higher prediction accuracy than those in the range [0.6, 0.7]. However, individual data points within the first interval may still have larger prediction errors than some data points within the second interval. In other words, the key property of an applicability domain (AD) measure is its discriminating abil-ity, distinguishing between high and low-accuracy predictions. Predictions falling below a predefined AD threshold are consid-ered within the domain. The threshold can be manually chosen, and efficient AD measures exhibit a monotonically increasing relationship between error rate and AD value. These measures estimate the reliability of predictions, a subjective aspect not as objectively defined as accuracy. Various AD measures exist, evaluating prediction reliability from different perspectives. Our focus includes two categories: Novelty detection, relying solely on input data, and Confidence estimation, utilizing information from the underlying model, which proves more potent for AD definition."}, {"title": "Novelity Detection", "content": "DA-Index This method is based on the K-NN approach. We used K=5 and Euclidean distance was used as a distance mea-sure. A lower DA_index value indicates greater similarity be-tween test and train data. DA index itself comprises four mea-sures. \u03ba, \u03b3, \u03b4 [2] and we added another measure to it named min-\u03ba. \u03ba represents the distance of the test point to the kth-nearest neighbour in the training set. min-k represents the minimum distance from the test point to the k-nearest neighbours. y represents the mean distance of a test point to its k-nearest neighbours and & corresponds to the length of the mean vector from a test point to its k-nearest neighbours."}, {"title": "Cosine Similarity", "content": "This measure [2] measures the cosine similarity between the test point and its k-nearest training set neigh-bours. In this study, we chose k to be 5. Cosine similarity between two data points xa and x\u2081 is determined by taking the inner product of the two vectors and dividing it by the product of their vector lengths.\n\\(cos(a_{xa,x}) = \\frac{\\sum_{i=1}^p x_{a,i} x_{b,i}}{\\sqrt{\\sum_{i=1}^p x_{a,i}^2 \\sum_{i=1}^p x_{b,i}^2}}\\)\nThis represents the angle between two vectors originating from the origin and extending to the two xa and xb p-dimensional data points. The cosine value ranges between 0 and 1, with a value of 1 indicating perfect similarity. The expression 1 cos(axa,x) was used to convert the cosine from a similarity measure to an AD Measure (i.e. a dissimilarity measure)"}, {"title": "Leverages", "content": "This method employs the Mahalanobis distance to measure the proximity of a data point to the centre of the train-ing set distribution. The leverage, denoted as h, is computed using the \"hat\" matrix through the equation h = (x (XX)\u00af\u00b9)xi), where X represents the matrix of the training set data points and xi represents a specific data point for which the leverage is being calculated [3]. Diagonal values in the H matrix repre-sent the leverage values for different points in a given dataset. Data points far from the centroid will be associated with higher leverage."}, {"title": "Confidence Estimation", "content": "Standard Deviation The standard deviation of the predictions, obtained from an ensemble of models, can be used as an esti-mator of model uncertainty for a given input [1]. In our work, we constructed a homogenous ensemble using the bagging tech-nique. This standard deviation \u00f4 was found to correlate with prediction accuracy. The underlying principle is that if different models yield significantly different predictions for a particular data point, then the prediction for this data point is more likely to be unreliable. Therefore we can employ the sample SD value as an AD measure, where high AD values for a prediction mean low confidence and low AD values mean high confidence.\nThe mathematical formula to represent an ensemble of models is as follows.\n\\(F(x) = h(f_1(x), f_2(x)....f_n(x))\\)"}, {"title": null, "content": "where F(x) represents the ensemble prediction for input x. fn(x) represents the prediction of the individual model i. n represents the total number of individual models in the ensemble. h repre-sents the combining function, which is typically the average of the individual model predictions given as.\n\\(F(f_1(x), f_2(x), ..., f_n(x)) = (1/n) \\sum f_i(x)\\)\nHenceforth, the SD of the prediction for an input x is given as:\n\\(d_{std}(x) = F(x) = \\sqrt{\\frac{N \\sum_{i=1}(f_i(x) \u2013 F(x))}{N-1}}\\)"}, {"title": "CORELL", "content": "The basic concept of Correll is that it measures the correlation between the training set predictions and the test set predictions [1]. Similar to Standard Deviations this model is applicable to an ensemble of models. Mathematically CORREL for an input x with an ensemble of N models is defined as:\n\\(d_{correll}(x) = 1 \u2013 max_{i=1,..,N}[corr(f(T_i), f(x))]\\)\nf(7;) is the vector of training set prediction and f(x) is the test point prediction. Corr is the Spearman correlation between the two vectors. The low value of CORREL (i.e., high Spearman correlation coefficient) indicates that for a test point x, there is a data point Tk in the training set for which predictions of the ensemble of models are strongly correlated. In other words, CORRELL checks if there is a correlation between the test data point and any data point in the training set. A high correlation means that the test data point is very similar to the training data and is within the applicability domain."}, {"title": "Gaussian Process Regressor", "content": "is a probabilistic model and implements Gaussian processes (GP) for regression purposes. It is a non-parametric approach that models the relationship between input variables and their corresponding output values. GPR as-sumes that the underlying function generating the data follows a Gaussian process, which is a collection of random variables that can be described by their mean and covariance. The prior mean is assumed to be constant and zero. The prior's covariance is specified by passing a kernel object. The hyperparameters of the kernel are optimized during the fitting of the Gaussian Process-Regressor by maximizing the log-marginal-likelihood (LML) based on the passed optimizer. As the LML may have multi-ple local optima, the optimizer can be started repeatedly. The method requires adjustment of three hyperparameters-alpha, which stands for the noise level (also acts as a regularization of the model), the parameter gamma of the RBF kernel which represents the covariance function, and variance threshold \u03c3*.\nThe sckitlearn library was used for the GPR implementation [4].\nThe predictions of GPR are represented by a Gaussian distri-bution, characterized by a mean and a covariance. The mean represents the estimated value of the target variable, while the covariance captures the uncertainty or variability associated with that prediction. This variance can be used as a measure of AD measure [5] just like the Standard Deviation method, where high variance means low confidence in prediction and low value of variance means high confidence in the prediction."}, {"title": "Random Forests", "content": "Random Forests, an ensemble learning method, combines multiple decision trees for predictions. Each tree is trained on a random subset of the data and features, pro-moting diversity and reducing overfitting. Predictions are made independently by each tree, and the final prediction is determined by aggregating all tree predictions. Leveraging the ensemble nature, we use Random Forest can be used as an applicability domain (AD) measure. By aggregating tree predictions and calculating the standard deviation across the ensemble, a higher standard deviation indicates a higher AD value, signifying low confidence in the prediction"}, {"title": "Bayesian Neural Networks", "content": "This is a novel approach we intro-duced in the field of defining the applicability domain. Unlike traditional neural networks where weights are assigned as a sin-gle value or point estimate, in BNN the weights are considered a probability distribution. A fundamental characteristic of the Bayesian approach is its emphasis on marginalization rather than optimization [6]. In Bayesian methods, we consider and incorporate solutions from all possible parameter settings, tak-ing into account their respective posterior probabilities. Instead of relying solely on a single parameter setting, the Bayesian ap-proach assigns weights to each solution based on its probability, allowing for a more comprehensive representation of possible outcomes. By considering the full range of parameter settings and their associated probabilities, the Bayesian approach pro-vides a more nuanced and probabilistic understanding of the problem, avoiding overreliance on a single parameter setting.\nA Bayesian approach defines a full probability distribution over parameters known as the posterior distribution. The posterior represents our belief/hypothesis/uncertainty about the value of each parameter. We use the Bayes Theorem to compute the posterior.\n\\(P(\\theta|X) = \\frac{P(X|\\theta)P(\\theta)}{P(X)}\\)\nWhere X is the data, P(X|0) is the likelihood of observing X, given weights 0, P(0) is the prior belief of the weights, and the denominator P(X) is the probability of observing the data over all the possible values of the parameters and it requires integrating over all possible values of the weights as\n\\(P(X) = \\int P(X|\\theta)P(\\theta)d\\theta\\)\n\"Integrating over the indefinite weights in evidence makes it hard to find a closed-form analytical solution. Therefore, simu-lation and numerical-based methods like Monte Carlo Markov Chain (MCMC) and variational inference (VI) are commonly employed. MCMC is a foundational approach in Bayesian statis-tics but can be slow for large datasets or complex models. In contrast, VI provides a faster alternative. VI identifies the clos-est probability distribution to the posterior, making it simple to work with. An optimization algorithm is then used to learn the parameters of this distribution, minimizing the divergence from the true distribution. Formally, a new distribution Q(0|z) approximates the true posterior P(0|X), with VI optimiz-ing parameters to minimize distribution divergence.\n\\(Q^*(\\theta) = argmin_zKL(Q || P)\\)\nIn the above equation, KL or KL Divergence is a non-symmetric measure of similarity or relative entropy between the two distri-butions [7]. We solve for the value of z so that it minimizes the"}, {"title": null, "content": "KL between P(0|X). Q(0|z).The KL divergence between P(0|X). Q(0z) is given as:\n\\(KL(Q || P) = \\int Q(\\theta/z) Log \\frac{Q(\\theta|z)}{P(\\thetaX)} d\\theta\\)\nReplacing the P(0|X) using equation 6, we get:\n\\(KL[Q(\\theta|z)||P(\\theta|X)] = \\int Q(\\theta|z) log \\frac{Q(\\theta/z)P(X)}{P(X|\\theta)P(\\theta)} d\\theta\\)\n\\(= \\int Q(\\theta|z) [log Q(\\theta|z)P(X) \u2013 log P(X|\\theta)P(\\theta)] d\\theta\\)\n\\(= \\int Q(\\theta|z) log \\frac{Q(\\theta|z)}{P(\\theta)} d\\theta + LogP(X)- \\int Q(\\theta|z) log P(X|\\theta) d\\theta\\)\nTaking the expectation to Q(0|z), we get:\n\\(KL[Q(\\theta|z)||P(\\theta|X)] = E Log \\frac{Q(z)}{P(\\theta)} +LogP(X)+E[LogP(X|\\theta)]\\)\nThe above equation still has the term LogP(X), making it dif-ficult for KL to compute. Therefore, an alternative objective function is derived by adding LogP(X) with negative KL diver-gence. LogP(X) is a constant with respect to Q(0|z). The new function is called the evidence of lower bound (ELBO) which we optimize and is expressed as:\n\\(ELBO(Q) = E[log P(X|\\theta)] - E log \\frac{Q(\\theta/z)}{P(\\theta)}\\)  \n\\(= E[log P(X|\\theta)] \u2013 KL [Q(\\theta|z)||P(\\theta)]\\)\nThe ELBO for a Bayesian Neural Network (BNN) is a trade-off between two terms. The first term, E[log P(X|0)], known as the likelihood, ensures a good fit to the observed data. It measures the expected log-likelihood of the data given the weights. The second term, KL [Q(0|2)|P(0)], penalizes the divergence between the variational and prior distributions over weights. This acts as a regularization term, preventing overfitting and promoting alignment with prior beliefs. Optimizing the ELBO through algorithms like gradient descent minimizes the KL divergence.\nIn our study, we implemented a 4-layer neural network archi-tecture with probabilistic weights and biases. The model was trained for 200 epochs using the evidence lower bound (ELBO). We evaluated model performance on a test set using Mean Squared Error (MSE) and Kullback-Leibler (KL) divergence metrics. MSE gauged average squared differences between pre-dicted and actual values, while KL divergence measured dissim-ilarity between predicted and true probability distributions. We utilized the torchbnn library [8] for BNN development. To assess prediction uncertainty, we performed 1000 iterations for each test example, calculating standard deviation as an AD measure. Higher AD values indicated increased uncertainty and lower prediction reliability. Employing BNNs for both regression and AD methods enhanced overall efficiency by ensuring consistent learning of the underlying function. This methodology, itera-tively validated, provided a comprehensive understanding of predictions and associated uncertainties."}, {"title": "METHOD", "content": "For our study, We focused solely on regression tasks and em-ployed seven regression models as our prediction models, fi-nally, we utilized a Bayesian neural network (BNN) as both the primary predictive model and the applicability domain (AD) measure. The regression models were trained on five publicly available datasets and evaluated on the corresponding test sets. We trained the regression models until convergence. Each model was evaluated on the test set using metrics such as root mean square error (RMSE) and R2 score. Additionally, we calculated the absolute error for each test point. After training the regres-sion model different AD measures were applied to the regression models to define the applicability domain of the models. These AD measures were evaluated on the test set, providing AD val-ues for each test point. Finally to benchmark the different AD methods we plot the absolute errors of the test points with their corresponding AD values and then use the validation framework to compare the performances of the AD methods. As mentioned before a good AD measure will be positively correlated with pre-diction errors, predictions with high absolute error should have a high AD value and vice versa. The overview of the workflow is shown in Fig 3. In the diagram, we can see the regression model is trained on the training set until convergence. Then we apply each of the AD measures. Finally, we evaluate the AD measures with respect to the absolute errors generated by the test set"}, {"title": "Dataset", "content": "The experiments used publicly available datasets from the UCI machine learning repository and Kaggle to estimate prediction accuracy. The datasets included Energy Efficiency, Boston Hous-ing, California Housing Prices, Abalone, and Red Wine Quality. These datasets provide information on building energy efficiency, housing prices, abalone shell physical attributes, and chemical properties of red wines.\nThe datasets were preprocessed to ensure quality and compati-bility. This included data cleaning to identify missing values or outliers, feature selection to focus on relevant variables, normal-ization to ensure the data was consistent across variables and encoding categorical values. These steps were performed us-"}, {"title": "Regression models", "content": "In our study, we employed seven regression models: Linear Re-gression, Lasso Regression, Ridge Regression, Decision Trees Regressor, XGBoost, MLP regressor, Support Vector Regres-sion, and Random Forest Regressor. The majority of these models were implemented using default parameters from the sklearn library [10]. Our training process involved meticulous adjustment of model parameters to minimize test errors, with a focus on achieving optimal performance.\nTo assess the accuracy and goodness of fit for each regression model, we employed common metrics such as root mean square error (RMSE) and R2 score. These metrics provided valuable insights into the predictive capabilities of our models. Thorough analyses were conducted to fine-tune the training parameters, ensuring the models' reliability and robustness in addressing regression tasks. This rigorous approach aimed to obtain precise and dependable results in evaluating the performance of each regression model."}, {"title": "Calculating Applicability Domain Measures", "content": "During the evaluation of the regression model, we also calculate the absolute error for each data point. Typically, the error is averaged over the whole available set and reported as a single value for example RMSE and MAE. However, this approach does not reflect the complete information about the prediction accuracy. For a subset of data points, the prediction accuracy may be significantly higher than the average, while for some data points the model may completely fail to predict the target. Thus to get the overall picture of the model's accuracy we decided to evaluate the absolute error |ytrue \u2013 ypred| for each data point in the test set. Sushko [1] referred to it as Variable accuracy. Then we evaluate the AD measures on the test set. This will"}, {"title": "Validation Framework - Benchmarking Criteria", "content": "To compare different applicability domain (AD) measures, assessing their ability to distinguish between low and high-accuracy predictions is crucial. Each AD measure exhibits varying capabilities in discriminating predictions of high and low accuracy. To quantify and compare their performance, a validation framework is employed. This framework is based on the expectation that prediction errors of the regression model should increase monotonically with respect to the AD values. Specific metrics within this framework evaluate the performance and effectiveness of these methods. The concept of error and prediction accuracy will be used interchangeably, as they are related, with higher errors resulting in lower prediction accuracy and vice versa."}, {"title": "Cumulative Error and Coverage Plot", "content": "Cumulative averaging is a method of calculating the average of a sequence of values by summing up all previous values and dividing by the total number of values encountered, provid-ing an overall trend or average value over time, smoothing out fluctuations and incorporating the entire history of values.\n\\(Cumulative Average_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\)\nWhere n represents the cumulative average up to the nth value, x\u012f represents the ith value in the sequence, and n represents the total number of values encountered so far.\nWe sort the test set data points based on AD values, representing them on a percentage scale instead of absolute values. The percentage scale indicates the proportion or percentage of test set data points with AD values at or below a given value. For instance, an AD value of \"10%\" means that 10% of the test set data have AD values equal to or less than this specific AD value.\nWe then calculate the cumulative average of the prediction error of the test points using the equation.\n\\[ \\frac{1}{n} \\sum_{i=1}^{n} |y_{itrue} - y_{ipred}| \\]\nwhere n is the size of the test set.\nThe cumulative averaging in combination with the AD percent-age scale results in a cumulative error or coverage plot [1].\nThis method employs a predetermined threshold to decide if a data point is within the applicability domain of the model. The threshold is set at a specific percentile of absolute errors from test points for a given regression model."}, {"title": null, "content": "yield the AD values for each data point in the test set. We sort these AD values in ascending order and plot them with respect to the corresponding prediction error of the data point in the test set. Since a higher AD value implies the inability to predict the data point correctly, thus for data points with high AD values, we expect the model's errors for that data point to also be high, giving us a monotonically increasing plot"}, {"title": "Validation Framework - Benchmarking Criteria", "content": "To compare different applicability domain (AD) measures, assessing their ability to distinguish between low and high-accuracy predictions is crucial. Each AD measure exhibits varying capabilities in discriminating predictions of high and low accuracy. To quantify and compare their performance, a validation framework is employed. This framework is based on the expectation that prediction errors of the regression model should increase monotonically with respect to the AD values. Specific metrics within this framework evaluate the performance and effectiveness of these methods. The concept of error and prediction accuracy will be used interchangeably, as they are related, with higher errors resulting in lower prediction accuracy and vice versa."}, {"title": "Cumulative Error and Coverage Plot", "content": "Cumulative averaging is a method of calculating the average of a sequence of values by summing up all previous values and dividing by the total number of values encountered, provid-ing an overall trend or average value over time, smoothing out fluctuations and incorporating the entire history of values.\n\\(Cumulative Averagen = \\frac{1}{n} \\sum_{i=1}^n Xi\\)\nWhere n represents the cumulative average up to the nth value, x\u012f represents the ith value in the sequence, and n represents the total number of values encountered so far.\nWe sort the test set data points based on AD values, representing them on a percentage scale instead of absolute values. The percentage scale indicates the proportion or percentage of test set data points with AD values at or below a given value. For instance, an AD value of \"10%\" means that 10% of the test set data have AD values equal to or less than this specific AD value.\nWe then calculate the cumulative average of the prediction error of the test points using the equation.\n\\[\\sum_{i=1}^{n} |y_{itrue} - y_{ipred}|\\]\nwhere n is the size of the test set.\nThe cumulative averaging in combination with the AD percent-age scale results in a cumulative error or coverage plot [1].\nThis method employs a predetermined threshold to decide if a data point is within the applicability domain of the model. The threshold is set at a specific percentile of absolute errors from test points for a given regression model."}, {"title": null, "content": "Let n be the number of data points in the test set. The absolute error for each data point i can be denoted as e(i), defined as:\n\\(e(i) = |yitrue - Yipred|\\)\nwhere yitrue represents the true value for data point i. Yipred rep-resents the predicted value for data point i. e(i) represents the absolute error between the true and predicted values for data point i. Then we can find the cumulative error using the equa-tion 13. For instance, if we consider the 25th percentile of the cumulative errors in the total test set, we can define the AD as the subset of data points whose cumulative errors are below this 25th percentile threshold.\nThe 25th percentile was consistently used as the threshold for AD evaluation in our experiments. We heuristically selected the 25th percentile as it provided an effective balance for our specific context, although it is not a standard threshold. This decision was based on the understanding that lower prediction errors correspond to higher accuracy. Setting the threshold too low could result in an overly inclusive AD, while setting it too high might exclude valuable data points with reasonably accurate predictions. The 25th percentile strikes this balance, allowing us to focus on data points with relatively lower prediction errors while maintaining a necessary level of strictness. Typically, the 25th percentile of total errors corresponds to approximately 75% accuracy. To assess the performance of multiple AD measures, we averaged the obtained thresholds from these measures for each experiment. To evaluate AD measures using accuracy coverage, we followed these steps:"}, {"title": null, "content": "This cumulative averaging is easily interpretable and very stable against noise. However, it has two drawbacks. Firstly, it relies on the chosen accuracy threshold, potentially leading to different rankings for different thresholds. Secondly, accuracy coverage is influenced not just by an AD measure's ability to distinguish highly accurate predictions but also by the overall performance of the analyzed model. Models with higher prediction accuracies are likely to exhibit higher accuracy coverages for a similar threshold."}, {"title": "Moving Average - Area Under the Curve", "content": "Another metric [1] in our study is the area under the curve (AUC), calculated as the absolute difference between the mov-ing average curve and the average model performance line. In the moving average plot, AD values are plotted against corre-sponding absolute errors for each data point in the test set. As per the nature of AD values, accuracy should not increase as AD values increase.\nThis method calculates a symmetric moving average for a 1D array (sorted AD values, in this case) using a centered window of a specified odd size. It determines the half-window size for symmetric data consideration, assigns uniform weights to all points within this window, and employs reflective padding to manage edge effects effectively. The data is then convolved with these weights to produce a smoothed version of the original array, effectively reducing noise and smoothing out fluctuations in the data series.\nAn example is shown in 5. We smooth the plot with a window size of N to reduce noise. Different window sizes were used for different datasets, depending on the size of the test set and treated as a hyperparameter. The window size must be chosen such that it reduces excessive noise while preserving critical information. We used visual analysis to determine the appropriate window size for each dataset. AD values were converted to percentage scales for standardized comparison across datasets and enhanced interpretability.\nThe advantage of AUC is that unlike coverage we do not need to manually set up a threshold value. In Fig 5, this corresponds to the area between one of the solid lines and the red horizontal line. Formally,\n\\( \\sum_{i=1}^{n} e(i) \u2013 Eavg\\)"}, {"title": "RESULTS", "content": "Comparing different AD Measures The evaluation of vari-ous applicability domain measures was conducted in conjunction with seven regression models to determine the most effective one. In the case of Bayesian Neural Networks, we initially trained it solely as an AD measure using it in conjunction with another regression model. Subsequently, we explored its dual func-tionality using it as a regression model and using its built-in confidence estimates as AD measures. This dual role exhib-ited exceptional performance, yielding promising results in our experimental analysis.\nThis evaluation aimed to rank AD measures for each regres-sion model across multiple datasets, determining the most ef-fective measure based on performance metrics. The study also investigated whether confidence or novelty measures were more effective in distinguishing reliable from less reliable predictions.\nThe comparison of applicability domain (AD) measures was based on two criteria: accuracy coverage and area under the curve (AUC). The evaluation followed a two-step approach.\nFirst, we computed the average coverage and AUC values of an AD measure for each dataset, using all the regression models. Then this process was repeated for all datasets, giving us a final averaged score that reflected how well the AD measure performed on various models and datasets. This score was a comprehensive indicator of the effectiveness of the AD measure."}, {"title": "Comparison using coverage", "content": "Table 1 displays the final cover-age value C25 of all AD measures, averaged across all datasets. Each column represents a specific dataset and the cells within the column represent the averaged coverage value of an AD measure across all regression models it was applied to. The final coverage value is obtained by averaging the coverage values of the AD measure across all datasets and is presented in the last column: Avg. Covg.\nIt can be seen in Table 1, sd_model achieved the highest perfor-mance, covering 63.14% of test data across all regression models and datasets. Following closely were kappa and leverages with coverages of 45.9% and 45.2%, respectively. Correll exhibited the poorest overall performance. Bayesian NN had a 44.05% coverage. Interestingly, BNN's performance as a standalone AD measure was not as expected, but subsequent sections reveal improved performance when used as both a regression model and an AD measure."}, {"title": "Comparison using AUC", "content": "Apart from the coverage we also incorporated AUC as the criterion to compare the performances. Table 2 summarizes the average AUC values. Since the range of errors was on a different scale for different regression models, we scaled the values of the area using min-max scaling to get the AUC values on the same scale. In Table 2 it can be seen that the differences in AUC scores between the best and worst AD measure for each data set. We can see that in terms of AUC also, the sd_model came on top with an average AUC of 0.45 this was closely followed by BNN with an AUC value of 0.4. CORRELL in this case also had the worst performance ranking last with an average AUC of 0.12.\nWithin the group of novelty measures the best-performing AD measure is gamma closely followed by kappa and leverages. It is worth noting that these measures exhibit nearly identical scores, indicating their comparable performance levels."}, {"title": "BNN's Dual Role: Regression Model and Applicability Domain Measure", "content": "When BNN was used as a regression model and its built-in confidence estimates were used as an AD mea-sure it demonstrated exceptional performance and promising results in determining the applicability domain. This is because, when the BNN is utilized as both a regression model and an ap-plicability domain method, it leverages the same neural network architecture, weights, and connections for both tasks. Thus the model that is making the prediction is also measuring the AD, resulting in better performance. Table 3 shows the coverage and AUC scores of BNN when implemented in this manner. The scores are averaged for all the datasets to give the final mean score. We can see the final coverage value of bnn is 67.92% which is the highest that all the previous AD measures bench-marked. Also, the average AUC value is 0.88 which is just second to the standard deviation. These values show that our method exhibits greater performance compared to other AD mea-sures. This highlights that Bayesian Neural Networks (BNN) demonstrate much better results when used with built-in confi-dence estimation. On the other hand, when BNN is employed as an AD measure with a different regression model, where a distinct model generates predictions, it leads to a performance drop. The mismatch between the prediction model and the AD measure causes poorer AD detection performance"}, {"title": null, "content": "Based on this observation, we can infer that models equipped with built-in confidence estimation (bnn)"}]}