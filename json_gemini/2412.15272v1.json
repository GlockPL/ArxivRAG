{"title": "SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven Retrieval-Augmented Generation", "authors": ["Yuzheng Cai", "Zhenyue Guo", "Yiwen Pei", "Wanrui Bian", "Weiguo Zheng"], "abstract": "Recent advancements in large language models (LLMs) have shown impressive versatility across various tasks. To eliminate its hallucinations, retrieval-augmented generation (RAG) has emerged as a powerful approach, leveraging external knowledge sources like knowledge graphs (KGs). In this paper, we study the task of KG-driven RAG and propose a novel Similar Graph Enhanced Retrieval-Augmented Generation (SimGRAG) method. It effectively addresses the challenge of aligning query texts and KG structures through a two-stage process: (1) query-to-pattern, which uses an LLM to transform queries into a desired graph pattern, and (2) pattern-to-subgraph, which quantifies the alignment between the pattern and candidate subgraphs using a graph semantic distance (GSD) metric. We also develop an optimized retrieval algorithm that efficiently identifies the top-k subgraphs within 1-second latency on a 10-million-scale KG. Extensive experiments show that SimGRAG outperforms state-of-the-art KG-driven RAG methods in both question answering and fact verification, offering superior plug-and-play usability and scalability. Our code is available at https://github.com/YZ-Cai/SimGRAG.", "sections": [{"title": "1 Introduction", "content": "Pre-trained large language models (LLMs) have become popular for diverse applications due to their generality and flexibility (Zhao et al., 2023; Minaee et al., 2024; Wang et al., 2024a). To avoid the hallucinations or outdated internal knowledge of LLMs (Zhang et al., 2023; Baek et al., 2023), Retrieval-Augmented Generation (RAG) (Zhao et al., 2024; Gao et al., 2023) integrates LLMs with external databases or documents to ensure the generated outputs are grounded in the most relevant and up-to-date information. Recently, knowledge graphs (KGs) have emerged as a valuable data source for RAG (Peng et al., 2024), which encode knowledge through interconnected entities and relationships (Ji et al., 2022). State-of-the-art KG-driven RAG methods, such as KAPING (Baek et al., 2023), KG-GPT (Kim et al., 2023a), KELP (Liu et al., 2024), and G-retriever (He et al., 2024), typically follow a paradigm of retrieving subgraphs from the KG and feeding them into LLMs to generate the final response. As shown in Figure 1, beyond achieving satisfactory answer quality, a practically ideal approach should address the following features.\nPlug-and-Play. To fully leverage the inherent generalization power of LLMs, an ideal approach should enable seamless integration with diverse KGs and LLMs without additional training or fine-tuning. Otherwise, training a smaller, task-specific model would be a more cost-effective alternative, as confirmed by our experiments in Section 6.3.\nAvoidance of Entity Leaks. In large-scale KGs, numerous entities may somehow related to a query. Thus, a practical approach should not assume that the ground-truth entities in the knowledge graph, which correspond to the mentions in the query, are directly provided by the users.\nConciseness. The retrieved subgraphs should focus on the most relevant and essential nodes and edges, ensuring a clear context for LLMs.\nScalability. An ideal algorithm should scale to large KGs with tens of millions of nodes and edges while maintaining acceptable latency.\nThese requirements converge on the critical challenge of effectively aligning query text with the underlying structures of a KG. The limitations of existing solutions arise from their intrinsic mechanisms, as summarized in Figure 2. Specifically, (i) KELP (Liu et al., 2024) trains a path selection model to identify paths that align with the query text, lacking the plug-and-play usability. (ii) KAPING (Baek et al., 2023) and G-retriever (He et al., 2024) rely on query text embeddings to retrieve similar triples or connected components in KG, which may introduce noisy information and compromise the conciseness. (iii) KG-GPT (Kim et al., 2023a) segments the query text into sub-sentences for precision, but it depends on the LLM to align candidate relations in KG with the sub-sentences, compromising scalability as the number of relations increases. (iv) To compensate for the insufficient alignment capability between query text and KG structures, certain methods inadvertently lead to impractical entity leaks. Both KG-GPT (Kim et al., 2023a) and KELP (Liu et al., 2024) expand the subgraphs or paths from the exact topic entities. Similarly, G-retriever (He et al., 2024) limits the input KG to a 2-hop neighborhood grown from the ground-truth entities of the query.\nIn this paper, we introduce a novel approach for aligning query text with KG structures. Specifically, we first utilize an LLM to generate a pattern graph that aligns with the query text. Then, to retrieve the best subgraphs from KG that semantically align with the pattern graph, we introduce a novel metric termed Graph Semantic Distance (GSD). Derived from the pairwise matching distance (Blumenthal, 1953), it quantifies the alignment by summing the semantic distances between corresponding nodes and relations in the pattern graph and the candidate isomorphic subgraphs. For example, in Figure 2, the LLM generates a star-shaped pattern graph aligning with the query. And the highlighted subgraph with the smallest GSD is considered as the best-aligned subgraph in KG.\nDifferent from KG-GPT (Kim et al., 2023a) that leverages LLMs to filter relations within large KG, we only ask LLMs to generate a small pattern graph. Also, our method targets subgraphs structurally and semantically aligned with the pattern, fundamentally differing from KAPING (Baek et al., 2023) and G-retriever (He et al., 2024) that do not explicitly constrain subgraph structure or size. Our method can support more complex pattern graph structures, diverging from KELP (Liu et al., 2024) that trains a path selection model limited to 1-hop or 2-hop paths. Moreover, to retrieve the top-k similar subgraphs w.r.t. the pattern graph with the smallest GSD, we further develop an optimized algorithm with an average retrieval time of less than one second per query on a 10-million-scale KG. Figure 3 presents the overview of the proposed Similar Graph Enhanced Retrieval-Augmented Generation (SimGRAG) method.\nOur contributions are summarised as follows.\n\u2022 We propose the query-to-pattern and pattern-to-subgraph alignment paradigm, ensuring plug-and-play usability and context conciseness.\n\u2022 We define the graph semantic distance and develop an optimized subgraph retrieval algorithm to ensure scalability and avoid entity leaks.\n\u2022 Extensive experiments across different KG-driven RAG tasks confirm that SimGRAG outperforms state-of-the-art baselines."}, {"title": "2 Related Work", "content": "Knowledge Graph Meets Large Language Models. Recently, the pre-trained large language models have shown the ability to understand and handle knowledge graph (KG) related tasks (Pan et al., 2023; Jin et al., 2024; Pan et al., 2024; Yang et al., 2024; Li et al., 2024b), such as KG construction (Zhu et al., 2024b), KG completion (Xie et al., 2022; Li et al., 2024a), KG embedding (Zhang et al., 2020), and so on. Furthermore, existing studies (Zhu et al., 2024a; Mao et al., 2024; Fan et al., 2024; Wang et al., 2024b) have tried to integrate LLMs with Graph Neural Networks (GNNs) to enhance modeling capabilities for graph data.\nRetrieval-Augmented Generation. In practice, LLMs may produce unsatisfactory outputs due to their hallucination or inner outdated knowledge (Baek et al., 2023). Retrieval-Augmented Generation (RAG) (Gao et al., 2023; Zhao et al., 2024) is a promising solution that retrieves related information from external databases to assist LLMs. Driven by documents, naive RAG approaches divide them into text chunks, which are embedded into dense vectors for retrieval. There are a bunch of studies and strategies optimizing each step of the RAG process (Zhao et al., 2024), including chunk division (Gao et al., 2023), chunk embedding (Li and Li, 2023; Chen et al., 2023), query rewriting (Ma et al., 2023), document reranking (Gao et al., 2023), and LLM fine-tuning (Cheng et al., 2023).\nKnowledge Graph Driven Retrieval-Augmented Generation. The intricate structures of knowledge graphs (KGs) present significant challenges to traditional RAG pipelines, prompting the development of various techniques for graph-based indexing, retrieval, and generation (Peng et al., 2024). As depicted in Figure 2, KAPING (Baek et al., 2023) retrieves KG triples most relevant to the query directly. KG-GPT (Kim et al., 2023a) segments the query and presents LLMs with all candidate relations in the KG for decision-making. KELP (Liu et al., 2024) trains a model to encode paths in the KG for selecting relevant paths, although it struggles to scale to structures more complex than 2-hop paths. G-Retriever (He et al., 2024) adopts a multi-step approach: it retrieves similar entities and relations, constructs a connected subgraph optimized via the prize-collecting Steiner tree algorithm, and employs a GNN to encode the subgraph for prompt tuning with the LLM."}, {"title": "3 Preliminaries", "content": "A knowledge graph (KG) G is defined as a set of triples, i.e., G = {(h,r,t) | h,t \u2208 V,r \u2208 R}, where V represents the set of entity nodes and R denotes the set of relations.\nGiven a knowledge graph G and a user query Q, the task of Knowledge Graph Driven Retrieval-Augmented Generation is to generate an answer A by leveraging both large language models and the retrieved evidence from G. This task is general and encompasses a variety of applications, including but not limited to Knowledge Graph Question Answering (KGQA) and Fact Verification (Kim et al., 2023a; Liu et al., 2024).\nAn embedding model (EM) transforms a textual input x to an n-dimensional embedding vector z that captures its semantic meaning, i.e., z = EM(x) \u2208 R^n. And the L2 distance between two vectors z1 and z2 is denoted by ||z1 - z2||_2 \u2208 R."}, {"title": "4 The SimGRAG Approach", "content": "Effectively aligning query text with the intricate structures of KG is a critical challenge for KG-driven RAG approaches. In this section, we introduce a novel strategy that decomposes this alignment task into two distinct phases: query-to-pattern alignment and pattern-to-graph alignment."}, {"title": "4.1 Query-to-Pattern Alignment", "content": "Given a query text Q, we prompt the LLM to generate a pattern graph P consisting of a set of triples {(h1, r1, t1), (h2, r2, t2), . . . } that align with the query semantics. To guide the LLM in generating the desired patterns, our prompt first asks for the segmented phrases for each triple before generating all the triples. As shown in Table 11, it also includes several explicit requirements, along with a few input-output examples to facilitate in-context few-shot learning (Agarwal et al., 2024).\nSuch query-to-pattern alignment leverages the inherent understanding and instruction-following capabilities of LLMs. For queries involving up to 3 hops in the FactKG dataset (Kim et al., 2023b), Llama 3 70B (Dubey et al., 2024) achieves an accuracy of 93% in our experiments. This demonstrates that such alignment can be effectively performed by the LLM without the need for additional training, ensuring plug-and-play usability. Furthermore, as LLMs continue to evolve in both capability and cost-effectiveness, we expect the alignment accuracy to keep improving in the near future."}, {"title": "4.2 Pattern-to-Subgraph Alignment", "content": "Given the generated pattern graph P, our objective is to assess the overall similarity between P and a subgraph S in the knowledge graph G. Since the pattern P defines the expected structure of a subgraph, we leverage graph isomorphism to enforce structural constraints on the desired subgraph.\nDefinition 1 (Graph Isomorphism) The pattern graph P has a node set Vp, while the subgraph S has a node set Vs. We say that P and S are isomorphic if there exists a bijective mapping f : Vp \u2192 Vs s.t. an edge (u, v) exists in P iff the edge (f(u), f(v)) exists in S.\nFigure 2 presents an isomorphism example. Note that when checking graph isomorphism, we do not consider the edge direction, as different KGs may vary for the same relations. For instance, some KGs may express a relation such as \u201cperson A directs movie B", "movie B is directed by person A\".\nAfter aligning the subgraph structure through graph isomorphism, we proceed to consider the semantic information of the nodes and relations. In traditional text-driven RAG pipelines, computing the distances between the query and document embeddings has proven effective. Similarly, for each entity node v and relation r in both the pattern graph P and the subgraph S, we obtain the corresponding embedding vectors z as follows": "nz\u03c5 = EM(v), Zr = EM(r)   (1)\nFor a subgraph S isomorphic to P, the nodes and edges in S have a one-to-one mapping with those in P. The semantic similarity between the matched nodes or edges can be quantified by computing the L2 distance between their embeddings. Therefore, we use the pairwise matching distance (Blumenthal, 1953) to derive the following overall graph semantic distance.\nDefinition 2 (Graph Semantic Distance, GSD) Given the isomorphic mapping f : Vp \u2192 Vs between the pattern graph P and the KG subgraph S, Graph Semantic Distance (GSD) is defined as follows, where r(u,v) denotes the relation of the edge (u, v).\nGSD(P,S) = \u03a3_{node v\u2208P} || Z_v - Z_{f(v)} ||_2 + \u03a3_{edge (u,v) \u2208 P} || Z_{r(u,v)} - Z_{r(f(u), f(v))} ||_2   (2)\nExample 1 As illustrated in Figure 2, the highlighted subgraph in KG is isomorphic to the pattern graph. By computing the text similarity (i.e., embedding distance) between the matched nodes and edges, the resulting GSD is 1.0.\nFocusing exclusively on isomorphic subgraphs guarantees conciseness. In Section 5, we will provide a detailed discussion on how to efficiently retrieve the top-k isomorphic subgraphs with the smallest GSD in KG.\nFurthermore, the joint use of graph isomorphism and semantic similarity effectively reduces noise. In practice, KGs are often noisy, and even semantically similar entities or relations may not always constitute suitable evidence. Figure 4 presents the distance rankings over the 10-million-scale DBpedia for the pattern graph in Figure 2. There are numerous entities related to \u201cGeorgian\u201d, but only the entity ranked 112 contributes to the final subgraph. Similarly, for the relation \u201carchitecture style"}, {"title": "4.3 Generalization to Unknown Entities or Relations", "content": "In practice, some queries like \"Who is the director of the movie Her?\" may involve unknown entities. To address this, we extend the query-to-pattern alignment process by allowing the LLM to represent unknown entities or relations with unique identifiers such as \"UNKNOWN director 1\", as illustrated by the pattern graph P in Figure 3.\nIn such cases, we further generalize the Graph Semantic Distance (GSD). Specifically, since unknown entities or relations are ambiguous and difficult to match with corresponding entities or relations in the KG, we exclude them from the GSD computation. Given the isomorphic mapping f: Vp \u2192 Vs between the pattern graph P and the KG subgraph S, we generalize GSD to:\nGSD(P,S) = \u03a3_{node v \u2208 P s.t. v is known} || Z_v - Z_{f(v)} ||_2 + \u03a3_{edge (u,v) \u2208 P r(u,v) is known} || Z_{r(u,v)} - Z_{r(f(u), f(v))} ||_2   (3)\nExample 2 As illustrated in Figure 3, the top-1 subgraph from the KG yields a GSD of 0.2."}, {"title": "4.4 Verbalized Subgraph-Augmented Generation", "content": "Given the top-k subgraphs with the smallest Graph Semantic Distance (GSD) from the KG, we now expect the LLM to generate answers to the original query based on these evidences. To achieve this, we append each retrieved subgraph S to the query text in the prompt. Each subgraph is verbalized as a set of triples {(h1, r1, t1), (h2, r2, t2), . . . }, as illustrated in Figure 3. Additionally, to facilitate in-context learning, we also include a few example queries with their corresponding subgraphs and expected answers in the prompt. Please refer to Appendix B for more details."}, {"title": "5 Semantic Guided Subgraph Retrieval", "content": "Performing a brute-force search over all candidate subgraphs and computing the Graph Semantic Distance (GSD) for each one is computationally prohibitive. To address this, we propose a practical retrieval algorithm in Section 5.1, which is further optimized for efficiency in Section 5.2."}, {"title": "5.1 Top-k Retrieval Algorithm", "content": "Recent subgraph isomorphism algorithms often follow a filtering-ordering-enumerating paradigm (Lee et al., 2012; Sun and Luo, 2020; Zhang et al., 2024). To narrow down the potential search space, we first apply semantic embeddings to filter out unlikely candidate nodes and relations. For each node up in the pattern graph P, we retrieve the top-k(n) most similar entities from the knowledge graph G, forming a candidate node set C^(n)[vp]. Similarly, for each relation rp, we extract the top-k(r) similar relations to form the candidate relation set C^(r)[rp]. Figure 3 illustrates an example of the candidate nodes and relations for the pattern graph node \"Tokyo Godfathers\" and the relation \u201cdirector\". For unknown nodes or relations, as discussed in Section 4.3, we treat all nodes or relations in G as candidates with a semantic distance of 0.\nThe retrieval process is described in Algorithm 1. Initially, lines 1-2 organize all edges in P according to a DFS traversal order. For each candidate node ug in the set C^(n)[v], we start an isomorphic mapping in lines 4-5 and iteratively expand the mapping using the Expand function until a valid mapping is found. In function Expand, when matching the ith triple (hp, rp, tp) in the ordered triple list L, the node hp is mapped to the corresponding node hg in G via the partial mapping f. Then, lines 13-16 check each neighboring relation rg and node tg for hg to see if they are valid candidates and do not contradict the existing mapping f."}, {"title": "5.2 Optimized Retrieval Algorithm", "content": "Despite the filtering approach, the above algorithm still suffers from a large search space, especially when there are too many candidate nodes and relations. As we only need the top-k subgraphs with the smallest GSD, we propose an optimized strategy that can prune unnecessary search branches.\nAssume that during the expansion of the ith edge in L, the partial mapping from the pattern graph P to the knowledge graph G is represented by f. Suppose there exists an isomorphic mapping f' that can be completed by future expansion, resulting in a subgraph S with GSD(P, S). It can be decomposed into the following four terms, where L[1 : i] denotes the first i \u2013 1 triples in L and L[i:] denotes the remaining triples.\nGSD(P,S) = \u0394_{mapped}^{(n)} + \u0394_{remain}^{(n)} + \u0394_{mapped}^{(r)} + \u0394_{remain}^{(r)}   (4)\n\u0394_{mapped}^{(n)} = \u03a3_{node v_P \u2208 P mapped in f} || Z_{v_P} - Z_{f(v_P)} ||_2,   (5)\n\u0394_{remain}^{(n)} = \u03a3_{node v_P \u2208 P not mapped in f} || Z_{v_P} - Z_{f'(v_P)} ||_2,   (6)\n\u0394_{mapped}^{(r)} = \u03a3_{(h_P, r_P, t_P) \u2208 L[1:i]} || Z_{r_P} - Z_{r(f(h_P), f(t_P))} ||_2,   (7)\n\u0394_{remain}^{(r)} = \u03a3_{(h_P, r_P, t_P) \u2208 L[i:]} || Z_{r_P} - Z_{r(f'(h_P), f'(t_P))} ||_2.   (8)\nFor Equations (6) and (8), notice that\n\u0394_{remain}^{(n)} \u2265 \u03a3_{node v_P \u2208 P not mapped in f} min_{v_G \u2208 C^{(n)}[v_P]} || Z_{v_P} - Z_{v_G} ||_2 X.   (9)\n\u0394_{remain}^{(r)} \u2265 \u03a3_{(h_P, r_P, t_P) \u2208 L[i:]} min_{r_G \u2208 C^{(r)}[r_P]} || Z_{r_P} - Z_{r_G} ||_2 Y.   (10)\nCombining Equations (4), (9), and (10), we have\nGSD(P,S) > \u0394_{mapped}^{(n)} + \u0394_{mapped}^{(r)} + X + Y = B.   (11)\nWhen the lower bound B exceeds the largest GSD of the top-k subgraphs in current priority queue res, any subgraph S completed through future expansion will never become the desired top-k subgraphs. That is, the current partial mapping f can be safely discarded, effectively pruning subsequent unnecessary search branches.\nMoreover, to reduce the largest GSD in the top-k priority queue res for more pruning opportunities, we adopt a greedy strategy that prioritizes matching more promising subgraphs earlier. Specifically, for lines 4-5, we can process the nodes vg \u2208 C^(n)[vp] in ascending order of their distances. In line 13 of the Expand function, the neighboring relation and node (rg, tg) with the smaller sum of ||z_{tp} - Z_{t_G}||_2 + ||z_{r_P} - Z_{r_G}||_2 will be expanded earlier.\nBy combining the pruned and greedy expansion strategies, the optimized algorithm is guaranteed to produce the same results as the top-k retrieval algorithm without any loss in solution quality. Experiments in Section 6.6 demonstrate that the optimized algorithm significantly accelerates retrieval."}, {"title": "6 Experiments", "content": "To evaluate the performance compared with existing approaches, we conduct extensive experiments on two tasks, i.e., Knowledge Graph Question Answering (KGQA) and Fact Verification."}, {"title": "6.1 Tasks and Datasets", "content": "Knowledge Graph Question Answering. We use the MoviE Text Audio QA dataset (MetaQA) (Zhang et al., 2018) in this task, which provides a knowledge graph related to the field of movies. All the queries in the test set are adopted for evaluation, consisting of Vanilla 1-hop, 2-hop, and 3-hop question-answering in the same field.\nFact Verification. We adopt the FactKG dataset (Kim et al., 2023b) in this task. It contains various statements including colloquial and written style claims, which can be verified using the DBpedia (Lehmann et al., 2015). All statements in the test set are used in the evaluation, and a method should return Supported or Refuted after verification.\nPlease refer to Appendix A for detailed statistics and examples of the tasks and datasets."}, {"title": "6.2 Baselines", "content": "The included baselines are briefly introduced as follows. Please refer to Appendix C for more details.\nSupervised task-specific models. State-of-the-art models for KGQA include EmbedKGQA (Saxena et al., 2020), NSM (He et al., 2021), and UniKGQA (Jiang et al., 2022). They are trained on the MetaQA training set and evaluated by the test accuracy. For the task of fact verification, the KG version of GEAR (Zhou et al., 2019) is included and trained on the FactKG training set.\nPre-trained LLMs. For both tasks, we evaluate two popular LLMs, ChatGPT (OpenAI, 2024) and Llama 3 70B (Dubey et al., 2024), using 12-shots without any provided evidence.\nKG-driven RAG with training. Recent method KELP (Liu et al., 2024) trains the retriever over the training set, while G-retriever (He et al., 2024) trains a graph neural network (GNN) to integrate query texts and subgraph evidences.\nKG-driven RAG without training. Both KAPING (Baek et al., 2023) and KG-GPT (Kim et al., 2023a) only require retrieval subgraphs from the KGs without any training or fine-tuning."}, {"title": "6.3 Comparative Results", "content": "As summarized in Table 1, supervised task-specific methods outperform KG-driven RAG approaches that require additional training, except on the MetaQA 1-hop dataset, where G-retriever achieves a 1% performance gain. Notably, supervised task-specific methods generally require smaller model sizes and lower training costs, making them a more cost-effective option in practice.\nTo avoid the costs of training, preparing samples, or serving a trained model, the inherent capabilities of pre-trained LLMs simplify the process, especially with the widely available APIs. However, directly using LLMs leads to the poorest performance. Thus, it is necessary to develop KG-driven RAG methods that do not require additional training. Among these methods, the proposed SimGRAG approach demonstrates substantially higher Hits@1 and accuracy in most cases. In fact, SimGRAG performs comparably to supervised task-specific models and even outperforms the supervised GEAR method on the FactKG dataset, while maintaining the plug-and-play usability.\nMoreover, the performance gap between SimGRAG and other RAG approaches becomes larger as the complexity of the questions increases on the MetaQA dataset. As discussed in Section 4.2, the combined use of graph isomorphism and semantic similarity effectively reduces noise and ensures conciseness, thus benefiting the performance of SimGRAG for 2-hop and 3-hop questions. In contrast, KELP fails to support 3-hop paths due to the challenges of handling the vast number of potential paths with its trained path selection model."}, {"title": "6.4 Error Analysis", "content": "Table 2 summarizes the error distribution across the three main steps of the SimGRAG method. For detailed error examples, please refer to Appendix D.\nThe majority of errors occur during the query-to-pattern alignment step. In this stage, the large language model (LLM) frequently fails to follow the given instructions and examples, thereby generating the undesired pattern graphs. As the complexity of the queries increases in the MetaQA dataset, we also observe a higher incidence of errors in the subgraph-augmented generation step, since it is more difficult for the LLM to accurately extract relevant information for a complex question from the retrieved subgraphs. Also, errors are also encountered during the pattern-to-subgraph alignment phase on the FactKG dataset. In these cases, while the LLM generates reasonable subgraphs in line with the guidance, mismatches occur because the ground-truth subgraphs have different structures and thus cannot be successfully aligned, as illustrated in Appendix D."}, {"title": "6.5 Ablation Studies", "content": "Few-shot in-context learning. Table 3 evaluates the proposed SimGRAG method by varying the number of examples in the prompts for the LLM, used in both pattern-to-graph alignment and verbalized subgraph-augmented generation. For the simplest MetaQA 1-hop questions, performance is not sensitive to the number of shots. In contrast, for more complex queries like those in the MetaQA 3-hop and FactKG datasets, we observe significant improvements when increasing from 4 to 8 shots.\nParameter k for semantic guided subgraph retrieval. Table 3 reports the impact of parameter k for retrieving top-k subgraphs with the smallest graph semantic distance. For MetaQA 1-hop questions, setting k = 1 leads to a significant drop in Hits@1, since many movies share the exactly same title, and retrieving fewer subgraphs makes it more difficult to cover the ground-truth answer. For MetaQA 2-hop and 3-hop questions, the choice of k has a negligible impact on performance. Conversely, increasing k leads to a slight decrease in accuracy on the FactKG dataset, since the top-1 subgraph is often sufficient and including more subgraphs will introduce noise for LLM."}, {"title": "6.6 Retrieval Efficiency", "content": "As discussed in Section 5, prior to running the optimized retrieval algorithm for top-k subgraphs, we first perform a vector search to obtain the top-k(n) candidate nodes and top-k(r) candidate relations."}, {"title": "7 Conclusion", "content": "In this paper, we investigate the problem of KG-driven RAG and introduce a novel SimGRAG approach that effectively aligns query texts with KG structures. For query-to-pattern alignment, we employ an LLM to generate a pattern graph that aligns with the query text, ensuring plug-and-play usability without any training. For pattern-to-subgraph alignment, we introduce the Graph Semantic Distance (GSD) metric to quantify the alignment between the desired pattern and the underlying subgraphs in the KG, ensuring context conciseness and preventing entity leaks. Additionally, we propose an optimized algorithm to retrieve the top-k similar subgraphs with the smallest GSD, improving retrieval efficiency and scalability. Extensive experiments demonstrate that SimGRAG consistently outperforms existing KG-driven RAG approaches."}, {"title": "8 Limitations", "content": "Since the proposed SimGRAG method is designed to offer plug-and-play usability, its performance is closely tied to the underlying capabilities of the large language model (LLM). Specifically, the method relies heavily on the ability of LLMs to understand and follow instructions effectively in both steps of the query-to-pattern alignment and verbalized subgraph-augmented generation. Thus, the performance of SimGRAG can be substantially degraded when utilizing lower-quality or less capable LLMs, especially in scenarios involving more complex queries that demand advanced reasoning skills."}]}