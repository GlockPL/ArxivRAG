{"title": "User-Aware Multilingual Abusive Content Detection in Social Media", "authors": ["Mohammad Zia Ur Rehman", "Somya Mehta", "Kuldeep Singh", "Kunal Kaushik", "Nagendra Kumar"], "abstract": "Despite growing efforts to halt distasteful content on social media, multilingualism has added a new dimension to this problem. The scarcity of resources makes the challenge even greater when it comes to low-resource languages. This work focuses on providing a novel method for abusive content detection in multiple low-resource Indic languages. Our observation indicates that a post's tendency to attract abusive comments, as well as features such as user history and social context, significantly aid in the detection of abusive content. The proposed method first learns social and text context features in two separate modules. The integrated representation from these modules is learnt and used for the final prediction. To evaluate the performance of our method against different classical and state-of-the-art methods, we have performed extensive experiments on SCIDN and MACI datasets consisting of 1.5M and 665K multilingual comments respectively. Our proposed method outperforms state-of-the-art baseline methods with an average increase of 4.08% and 9.52% in F1-scores on SCIDN and MACI datasets respectively.", "sections": [{"title": "1. Introduction", "content": "The rapid growth in the number of internet and social media users has brought about significant changes in the way people interact with each other and access information. It has also created new opportunities for individuals to connect with people of different cultures. However, it has also led to concerns about privacy, security, and the potential for online harassment and misinformation. A worrying trend visible in social media posts these days is to abuse an individual or a group based on their nationality, ethnicity, religion, or sexual orientation (Sharma et al., 2022). Social media was developed for innovative and positive uses, yet a lot of abusive content in different languages is frequently posted. Such materials may have adverse psychological impacts including increased mental stress and emotional outbursts. Most social media platforms have implemented strict rules to restrict abusive content. Despite these limitations, a large amount of abusive content bypasses these impositions and is easily disseminated on social media. With the aim of curbing the dissemination of objectionable content, academicians, policymakers, and stakeholders are working towards developing reliable computational systems. However, there are still several challenges that need to be tackled (Akiwowo et al., 2020), (Roy et al., 2020). Complexities which rise due to multilingualism and code-mixed content are the major reasons for undetected abusive content. The objective of this research is to deal with following challenges that exist in detecting abusive content on social media:\nSpelling Inconsistencies Resulting from Roman Script for Multilingual Content\nDue to the multitude of languages spoken globally, it is common for individuals to communicate in multiple languages on social media. Some of these languages are low-resource, which further complicates the challenging task of abuse identification. Languages with a relatively limited amount of data available for training are referred to as low-resource languages, with many Asian and African languages falling under this category. Indic languages such as Hindi, Tamil, Marathi are a few examples of low-resource languages. Many of the West-European languages such as English, French, Spanish, Italian are high resource languages with English being the most well resourced language. The practice"}, {"title": "2. Related Work", "content": "In this section, we present previous works related to abusive and hateful content detection on social media. In existing methods, the problem is framed as a supervised text classification task (Schmidt and Wiegand, 2017). To address the problems of text classification, there are generally two decisions to make, first is the selection of the text feature extraction technique, and next is the selection of the classification method, i.e., Machine Learning or Deep Learning methods. We present a review on existing literature based on these two categories. Further, we give an overview of existing works on Indic and multilingual methods pertaining to abusive/hateful content detection."}, {"title": "2.1. Text Feature Extraction Methods", "content": "Text feature extraction methods include classical Natural Language Processing (NLP) techniques for feature construction that provide context-independent surface features, and the more recent paradigm of transformer-based methods which automatically learn abstract features from raw text."}, {"title": "2.1.1. Classical Methods", "content": "Classical methods of text feature extraction include rule based approach, manual feature designing, count-based and frequency-based methods such as Bag of words (BoW) & TF-IDF, and non-contextual word-embeddings. Earliest works use rule-based approach and manually designed features extensively. Rule-based methods may provide good results for a set of data, but these methods are not scalable. N-grams and BoW methods are comparatively easier to scale. Greevy et al. (Greevy and Smeaton, 2004) compares the performance of BoW, Bi-grams and part of speech (POS) tags with Support Vector Machine (SVM) as the classifier. In their experiments BoW performs better than the other two feature extraction techniques. Xiang et al. (Xiang et al., 2012) uses topical features extracted using Latent Dirichlet Allocation (LDA) algorithm. They also take lexicon features through keyword matching for vulgar language. In Davidson et al. (Davidson et al., 2017), TF-IDF is used as text feature extraction technique along with POS tags and sentiment lexicons. SVM performs better than other Machine Learning methods in this work. Works like, (Waseem and Hovy, 2016; Kwok and Wang, 2013) and (Waseem, 2016) also use classical approaches extensively with Machine Learning methods. In the aforementioned works, a major problem is the sparsity of feature vectors. Word-embeddings overcome this problem by providing dense feature vectors of fixed length which have relatively smaller dimensionality. Glove (Pennington et al., 2014) and Word2vec (Mikolov et al., 2013) are the two famous early word-embedding techniques. In (Kapoor et al., 2019; Park and Fung, 2017) and (Pitsilis et al., 2018), word-embeddings are used as text features for hate classification. The drawback of these word-embeddings is that they are non-contextual, which hampers the performance of the hate detection systems."}, {"title": "2.1.2. Transformer-based Methods", "content": "To overcome the drawbacks of classical methods, techniques have been developed to generate contextual embeddings. The major upgrade in NLP techniques is attributed to Google. Vaswani et al. (Vaswani et al., 2017), at Google brain, introduces Transformer which uses attention mechanism. Since their introduction in 2017, transformers and attention mechanism have been used in a variety of NLP methods, with Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) being the most famous method among those. Ranasinghe et al. (Ranasinghe and Zampieri, 2020) uses cross-lingual embeddings, generated by the transformer-based Cross-lingual Language Model-RoBerta (XLM-R), for offensive language identification. It achieves an F1-score of 85.80%, significantly higher than its counterparts. Mozafari et al. (Mozafari et al., 2019) and Chakravarthi et al. (Chakravarthi et al., 2020) use transformer-based methods for hateful and offensive content detection. Few works suggested language-specific methods for embedding generation such as Nguyen et al. (Nguyen and Nguyen, 2020), which provides embeddings for the Vietnamese language. One advantage of transformer-based model is the contextual embeddings, another major advantage is transfer-learning,i.e., sharing and re-utilising the model weights developed during training (Kumar et al., 2022). If training is done on a corpus for a language with a substantial resource base, its knowledge can help the process of training for low-resource languages that are still being researched (Ranasinghe and Zampieri, 2020). Transformer-based methods have proved to be extremely useful in hate speech detection for low-resource languages."}, {"title": "2.2. Classification Methods", "content": "In addition to methods adopted for text feature extraction, abusive and hateful content detection methods also vary on the basis of the classification algorithms, i.e., Machine Learning and Deep Learning algorithms. In the related works, Machine Learning methods have been used mainly in combination with classical methods of feature extraction. Burnap et al. (Burnap and Williams, 2015) uses ensembling of SVM, Logistic Regression (LR) and Random forest (RF) methods for hateful content detection on twitter. In this work, n-grams of hateful terms along with typed dependency gives best results for all the classification methods used for comparison. In Vigna et al. (Del Vigna12 et al., 2017) too n-grams of characters and words are used with SVM as classifier. They classify the content into three categories - No Hate, Weak Hate and Strong Hate. LSTM is taken as the second classifier for capturing long term dependencies in the content, but SVM outperforms LSTM in Vigna et al.. Deep Learning methods have also been extensively explored for this task. These methods are either provided with the features extracted using classical methods or with the pretrained word or context embeddings from deep neural networks. In Mehdad et al. (Mehdad and Tetreault, 2016) RNN is used with character n-grams for hate speech detection, and RNN achieves better results than SVM in this work. CNNs are explored for hate detection in Gamback et al. (Gamb\u00e4ck and Sikdar, 2017) with Word2vec. Their method provides better results than LR classifier. Zhang et al. (Zhang et al., 2018) uses mixture of CNN-GRU with word-embeddings to detect hate speech, which improves performance upto 10% as compared to SVM baseline. In light of the above discussion, it can be observed that selection of the classification method is very critical as none of the methods clearly"}, {"title": "2.3. Methods for Multilingual Low-resource Indic Languages", "content": "Abusive and hateful content has posed challenges for researchers in low-resource languages, such as Indic languages, due to code-mixing and multilingualism. Researchers have come up with different approaches to tackle this problem. Hande et al. (Hande et al., 2020) proposes Machine Learning methods for offensive language detection in Kannada language. They use TF-IDF based text features. Major work has been done recently to adapt Deep Learning techniques for multilingual Indic texts. Chopra et al. (Chopra et al., 2020) uses CNN-BiLSTM with author profiling and debiasing for hate detection in Hindi-English text.\nNumerous models, including Multilingual Bidirectional Encoder Representations from Transformers (M-Bert)(Devlin et al., 2019), Multilingual Representations for Indian Languages (Muril) (Khanuja et al., 2021), and XLM-R (Conneau et al., 2020) have become popular and have produced statistically significant improvements in abuse identification in Indic languages. Sharma et al. (Sharma et al., 2022) proposes Map Only Hindi method for code-switched text in the domain of hateful content detection by utilizing fine-tuned transformer-based models i.e., M-Bert and Muril. Due to their ability to detect abusive content on social media platforms, these methods have been used extensively for Indic languages in Sharif et al. (Sharif et al., 2021), Amjad et al. (Amjad et al., 2021) and Velanker et al. (Velanker et al., 2023) too. They demonstrate that transformers outperform classical models in hate detection in Indic Languages."}, {"title": "3. Methodology", "content": "A detailed discussion of our proposed methodology for abuse identification is presented in this section. Figure-1 demonstrates the system architecture of our proposed user-aware abusive content detection system. Proposed system comprises five major components: a) data preprocessing; b) social context features; c) text context features; d) abuse classification and e) ensemble learning. In the data preprocessing module, the comment text set $C = \\{c_i\\}_{i=0}^K$ is cleaned and then transliterated to Roman script. Here $c^i$ denotes the ith comment in set C, K is the cardinality of C. We denote the transliterated set of comments with $T = \\{t_i\\}_{i=0}^K$, where $t_c^i$ is ith transliterated comment. Transliteration is a salient"}, {"title": "3.1. Data Preprocessing", "content": "The data preprocessing step is pivotal as it sets the data up for future in-depth analysis in the most meaningful way possible. In layman's terms, uncleaned raw data is changed into cleaned data in this step. Before feeding the data to the model, data preprocessing steps, such as removing content with no text, removal of extra spaces and erroneous entries, are performed. Data preprocessing steps carried out in the proposed approach are elaborated as follows:"}, {"title": "3.1.1. Data Selection", "content": "ShareChat IndoML Datathon NSFW (SCIDN)4 dataset contains comments from more than 15 languages. Some of them are dialects of Hindi such as Haryanvi and Bhojpuri. We select comments from 12 prominent Indic languages, namely, Assamese, Bengali, English, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil and Telugu. We take comments of 10 similar languages from the IIIT-D Multilingual Abusive Comment Identification dataset (MACI) 5 except English and Punjabi as comments from these two languages are not present in that dataset. Table-4,5,6 and 7 in Section-4.1.1 give a detailed overview of both datasets."}, {"title": "3.1.2. Data Cleaning", "content": "In this step, we remove unexpected erroneous entries from the datasets such as rows with missing comments and missing feature values. Then comment-level cleaning is performed where we remove extra spaces, punctuation and digits from comments. Next, insignificant words are filtered out from comments utilizing our prepared set of such words. English words are taken from the NLTK library (Bird et al., 2009)."}, {"title": "3.1.3. Machine Transliteration", "content": "Transliteration is a process to convert sentences written in one script to another script based on phonetic similarity. Many social media platforms give an option to write comments in native scripts as well as in Roman script. In our experiments we observe that embeddings from transliterated comments achieve better results than the rest of the models. In our pipeline, all comments are transliterated into Roman script. For a given comment $c \\in C$, $t_c$ denotes the comment after transliteration where $t_c \\in T$.\n$t_c$ = IndicXlit(c) (1)\nWe use IndicXlit (Madhani et al., 2022) method for transliteration. The largest publicly accessible parallel corpus with 26 million word pairings from Indic languages, Aksharantar, serves as the training data for IndicXlit. Dakshina (Roark et al., 2020) and Aksharantar benchmark are used for its evaluation. It employs six layers of encoders and decoders, 256-D input embedding vectors with four attention heads and a 1024 feedforward dimension for a total of 11M parameters. It currently supports 21 languages. We use transliterated comments for contextual embedding extraction for M-Bert and Muril, while for XLM-R we take comments in their original script as it shows better performance with them."}, {"title": "3.1.4. Data Transformation", "content": "Plenty of comments in the dataset contain emojis or emoticons. People use emojis to express anger, disgust, happiness and various other emotions while posting comments. Emojis can be utilized to get valuable emotional insights from a comment. We, therefore, transform all emojis into text. Next, uppercase characters are transformed into lowercase characters."}, {"title": "3.2. Social Context Features", "content": "In numerous existing works related to abuse detection, only textual features have been extensively used with Machine Learning and Deep Learning models (Gamb\u00e4ck and Sikdar, 2017; Park and Fung, 2017). A few incorporated user behavior as well (Chopra et al., 2020), (Pitsilis et al., 2018). One objective of this work is to investigate the impact of both social context and textual features on abusive content detection methods. Broadly, social context features include post features and user history. We present correlation scores of these features later in this section to establish their importance. Correlation scores show that our proposed feature, that is user-post polarity, has the strongest correlation with the class labels.\nIn the dataset, we have comments made on different posts, made by different users. Let's assume a set of posts $P = \\{p_j\\}_{j=0}^X$ and a set of users $U = \\{u_i\\}_{i=0}^Y$, where X represents the cardinality of set P and Y denotes the cardinality of set U. There may be more than one comment on a single post, and more than one comment made by a single user in the dataset. Hence, all the comments in the dataset can be represented either as the union of set of comments made on each post or the union of set of all comments made by each user as follows:\n$C = \\{c_1^p\\} \\cup \\{c_2^p\\} \\cup \\{c_3^p\\} \\ldots \\ldots \\ldots \\{c_X^p\\}$ (2)\n$C = \\{C_1^u\\} \\cup \\{C_2^u\\} \\cup \\{C_3^u\\} \\ldots \\ldots \\ldots \\{C_Y^u\\}$ (3)\nwhere, C denotes the set of all comments, $C^p_i$ denotes the set of comments made on a post $p^i$ and $C^u_i$ denotes the set of comments made by a user $u^i$.\nWe calculate two features, user polarity and post polarity. For that we use the set of abusive words $A = \\{a_i\\}_{i=0}^{Q_1}$ collected by us, where $a_i$ denotes a single abusive word and $Q_1$ is the cardinality of set A. We then prepare an extended set, $A^{ext} = \\{a_i^{ext}\\}_{i=0}^{Q_2}$, with different spellings of each abusive word from set A by replacing a few letters with similar sounding letters, where $a^{ext}_i$ denotes a single abusive word and $Q_2$ is the cardinality of set $A^{ext}$.\nA brief description of social context features is given in this section followed by their correlation with class labels."}, {"title": "3.2.1. Post features", "content": "Post features provide valuable information about the content, such as the number of likes and reports. Generally, the pattern of abusive content pertaining to the number of likes and reports would be different from non-abusive content. Therefore, we include post features in training. Following post features are available in both the datasets, namely, post id (p), report count comment ($r_c$), report count post ($r_p$), like count comment ($l_c$) and like count post ($l_p$), where $p \\in P$. Apart from these, we determine two more post features, post polarity and relative reporting tendency, as follows:\nRelative reporting tendency (RRT): For any given comment c, let $r_c$ be the number of times it was reported, and, let $r_p$ be the number of times all the comments on that post were reported. Then relative reporting tendency ($rrt_c$) can be determined as follows:\n$rrt_c = \\frac{r_c}{r_p}$ (4)\nPost-polarity: Post polarity gives the likelihood of a post to attract an abusive or non-abusive comment. For a given set of comments $C_p$ on a post $p^i$, number of all abusive comments in this set can be written as $|\\sigma_{l=1}C_p|$ and number of all non-abusive comments as $|\\sigma_{l=0}C_p|$. Where $l = 1$ and $l = 0$ denote an abusive and non-abusive comment respectively and $\\sigma$ denotes the selection operator. Post polarity $(\\Phi_p)$ can be calculated as:\n$\\Phi_p = \\frac{|\\sigma_{l=0} (C^p_i) | - |\\sigma_{l=1} (C^p_i)|}{m}$ (5)\nwhere, m is the total of number comments on post $p^i$. Similar steps, as shown in Algorithm-1 for user polarity feature can be repeated for determining post polarity."}, {"title": "3.2.2. User history", "content": "User history can be utilized to determine the likelihood of a comment to be abusive or non-abusive. Generally, a comment written by a user, who has made more abusive comments in the past, is more likely to be abusive.\nUser polarity: User polarity feature gives the user's tendency to make an abusive or non-abusive comment independent of the nature of the post. For a given set of comments $C^u$ made by a user $u^i$, number of all abusive comments in this set can be written as $|\\sigma_{l=1}C^u|$ and number of all non-abusive comments as $|\\sigma_{l=0}C^u|$. User polarity $(\\Phi_u)$ can be calculated as:\n$\\Phi_u = \\frac{|\\sigma_{l=0} (C^u_i) | - |\\sigma_{l=1} (C^u_i)|}{m}$ (6)\nwhere, n is the total number of comments made by the user $u^i$. Algorithm-1 gives the steps for calculation of $\\Phi_u$."}, {"title": "3.2.3. Combined user-post polarity", "content": "A user's behavior may depend on the type of post. For a post which receives mostly abusive comments, it is highly likely that any arbitrary user will also write an abusive comment on the post. However, it may not be true all the time as user tendency would also have a role to play to determine this. So, we calculate a combined user-post polarity feature which takes into account both polarities with appropriate weights given to each. User-post polarity is calculated as follows:\n$\\Phi_{up} = \\alpha (\\Phi_u) + (1 - \\alpha) \\Phi_p$ (7)\nWe test this feature with different weights given to each polarity for some random samples from the dataset. $\\alpha$ is set to 0.47 as the model provides better results for this value of $\\alpha$.\nTo calculate $\\Phi_u$ and $\\Phi_p$, we count the number of abusive and non-abusive comments for each user and post. Test data may not necessarily have comments from same users and posts as training data. So, we take the maximum of polarities determined using pre-classifier and lexicon matching approaches for test data. For this, we prepare a large set of abusive words comprising words from all the 12 languages (Mathur et al., 2018), (Bird et al., 2009). With the help of this set, we determine the values of $\\phi_{user}^{ext}$ and $\\phi_{post}^{ext}$. To determine polarities $\\phi_{user}^{pcls}$ and $\\phi_{post}^{pcls}$, Muril is taken as the pre-classfier. $\\phi_{user}^{ext}$ and $\\phi_{post}^{ext}$ denote the user and post polarities respectively, determined using extended abusive set. $\\phi_{user}^{pcls}$ and $\\phi_{post}^{pcls}$ denote the user and post polarities respectively, determined using the pre-classifier. For MACI dataset, we use only post-polarity feature as user-ids are not available in that dataset.\nIn user-post polarity function of Algorithm-1, in line 1 and line 2, user polarity and post polarity functions are called respectively which return the corresponding polarities. In line 3, combined user-post polarity is calculated as the weighted sum of individual polarities. In user polarity function of Algorithm-1, line 1,2 initialize the count of abusive and non-abusive comments to 0. In line 3-10, the set of all transliterated comments $T_c^u$, made by a user u is iterated through to search the abusive words in each comment. Set of $A^{ext}$ is referred for this operation. In line 12-14, if an abusive word is found in the comment then the corresponding counter $count_{abuse}$ is incremented, otherwise the counter for non-abusive comments $count_{non}$ is incremented. In line 17, user polarity $\\phi_u^{ext}$, which is determined by using $A^{ext}$, is calculated. Similarly, we calculate the count of abusive and non-abusive comments using the labels generated by the pre-classifier and these counts are used to calculate user polarity $\\phi_u^{pcls}$. In line 18, we take maximum of the two user polarities as the final user polarity $\\Phi_u$. Similarly, we calculate post polarity for each post.\nCorrelation: We use Point Biserial correlation (Kornbrot, 2014) to represent the association of features with class labels. Point Biserial correlation coefficient is used to measure the correlation between a continuous and a dichotomous variable. It returns values which range from -1 to 1. A coefficient value of 1 represents a perfect positive correlation, -1 represents a perfect negative correlation, and 0 represents no correlation. Results are presented in Table-2 and Table-3 for SCIDN and MACI datasets, respectively. User-post polarity shows the strongest correlation, whereas post-id has the weakest correlation with class labels in SCIDN. In MACI, post polarity has the strongest correlation, while post-id has the lowest correlation. Correlation for embeddings is calculated for Muril predicted labels. We concatenate the selected features and create a social feature vector $\\hat{s}$. Next, the SFL module is trained on $\\hat{s}$. We give a detailed description of the SFL module in Section-3.4.1."}, {"title": "3.3. Text Context Features", "content": "In this section, we present a detailed overview of the text context features. It comprises two subsections: a) data augmentation, and c) contextual embedding extraction. Data augmentation is employed by inserting additional synthetic comments in the dataset using a random sample of original comments from the training data. It is done with the objective of dampening the effect of misspelled abusive words. Next, contextual embeddings are extracted and fed to the TFL module. The rest of this section describes these steps in detail."}, {"title": "3.3.1. Data Augmentation", "content": "We observe that the same words were transliterated differently in different comments. Users, too, tend to make spelling mistakes while writing comments. To handle this problem, we use the data augmentation technique. For that, we use the extended set of abusive words $A^{ext}$. Set $T_c$ is divided into train set $T_c^{train}$ and test set $T_c^{test}$ as shown in Equation 8.\n$T_c = T_c^{train} \\cup T_c^{test}$ (8)\nFrom the train set $T_c^{train}$, we sample a random set of non-abusive comments having same cardinality as $A^{ext}$. Each abusive word $a_i^{ext} \\in A^{ext}$ is inserted into a comment. Original comments are also retained in train set $T_c^{train}$. Since insertion of the abusive word makes the comment abusive, label is also inverted for these synthetic comments. This process is repeated for each language and we get an augmented set, $T_c^{aug}$, which is combined with train set to get the final augmented training set $T_c^{train\\_aug}$ as shown in Equation 9.\n$T_c^{train\\_aug} = T_c^{train} \\cup T_c^{aug}$ (9)\nThis set of additional comments, $T_c^{aug}$, helps the model in two aspects, first, training samples have increased for each language while covering spelling errors in abusive words, secondly, the model can give more attention to words which make a comment abusive since the training data contains the non-abusive copy of the same comments as well. We argue that synthetic comments still make sense semantically because an abusive word is added at the starting of each sentence. Generally, most of the abusive words are person oriented, hence, adding them at the starting does not alter the grammar of the sentences, and semantics of the sentences are retained while the polarity changes from non-abusive to abusive. Steps of data augmentation are presented in Algorithm-2."}, {"title": "3.3.2. Contextual Embedding Extraction", "content": "We employ three different methods to generate embeddings for a tokenized sequence of words, namely, Muril, XLM-R and M-Bert. The general workflow for contextual embedding extraction, irrespective of the method we use, can be summarized using the example of the case involving Muril and transliterated comments. First, tokenization of"}, {"title": "3.4. Abuse Classification", "content": "In this section, we discuss the abuse classification module. Various components of the abuse classification module are shown in Figure-2. Four components constitute this module, namely, a) social feature learning; b) text feature learning; c) joint feature learning and d) prediction layer."}, {"title": "3.4.1. Social Feature Learning (SFL)", "content": "We use six social context features, namely, report count post ($r_p$), like count comment ($l_c$), like count post ($l_p$), relative reporting tendency ($rrt_c$) and combined user-post polarity $(\\Phi_{up})$. Here $rrt_c$ and $\\Phi_{up}$ are the manufactured features. For MACI dataset we use report count comment ($r_c$) instead of $r_p$, as its correlation coefficient is higher as shown in Table-3. We first scale these features using min-max normalization. Min-max normalization method is represented in Equation 12. Let x, $x_{min}$ and $x_{max}$ denote the original value of a feature instance, minimum value for that feature, and maximum value for that feature, respectively. Then, min-max normalized value $x_{norm}$ of x would be:\n$x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$ (12)"}, {"title": "3.4.2. Text Feature Learning (TFL)", "content": "This section illustrates the text feature learning module. As mentioned in Section-3.3.2, transformer-based multilingual methods are used for contextual embedding extraction. They generate the sentence embedding $E \\in R^D$ and the last hidden state matrix $H \\in R^{I\\times D}$. We use last hidden state $H \\in R^{I\\times D}$ for our experiments. As shown in Equation 17, all the token embeddings from the last hidden matrix H are stacked horizontally to create embedding vector $\\hat{v}$.\n$\\hat{v} = Reshape(H)$ (17)\nHere, $\\hat{v} \\in R^N$, where N is the dimension of text embedding vector. N, dimension of vector $\\hat{v}$, depends on the sequence length. We illustrate the TFL module using a sequence length of 128. Thus N becomes 98,304. Next, embedding vector $\\hat{v}$ is passed to the input layer in2 of the TFL module which instantiates the tensor $T(\\hat{v})$. We pass the tensor $T(\\hat{v})$ to the fully connected dense layer fc2 to obtain the high-level representation, V, of the embedding vector $\\hat{v}$ as given in Equation 18.\n$V = fc2 (units = D_2) (T(\\hat{v}))$ (18)\nThe layer fc2 performs dot product between $T(\\hat{v})$ and weight matrix $W_2^{(2)}$. All the operations performed by fc2 are represented by Equation 19.\n$fc2: V = \\sigma_{relu} (W_2^{(2)}. T(\\hat{v}) + b_2)$ (19)\nHere, $V \\in R^{D_2}$, $D_2$ denotes the dimension of V which is 768, $W_2^{(2)}$ is the weight matrix of dimension y which is $D_2 \\times N$, i.e., 768$\\times$98,304, $b_2$ is bias vector of size $D_2$ and $\\sigma_{relu}$ denotes the activation function. ReLU is used in fc2 too as activation function. As in the SFL module, we randomly drop 20% of the neurons in TFL."}, {"title": "3.4.3. Joint Feature Learning (JFL)", "content": "This module employs a feature fusion mechanism to obtain a joint representation of the learnt social feature vector $\\hat{S}$ and the text embedding vector V. Then, module learns this joint representation. As shown in Equation 20, we first pass V and $\\hat{S}$ to concat layer which concatenates both vectors and outputs a joint feature vector $\\hat{I}$.\n$\\hat{I} = concat(V \\oplus \\hat{S})$ (20)\nHere, $\\hat{I} \\in R^{D_3}$, $D_3$ is the dimension of the joint feature vector which is 784, concat denotes the concatenation layer and $\\oplus$ denotes the concatenation operation. This 784-D vector is passed to two fully connected layers sequentially, with a dropout layer after each fully connected layer. Equation 21, 22, 23 and 24 represent further operations on the joint vector.\n$\\hat{I}_1 = fc3 (units = D_4) (\\hat{I})$ (21)\n$fc3: \\hat{I}_1 = \\sigma_{relu} (W_3^{(21)}. \\hat{I} + b_3)$ (22)\n$\\hat{I}_2 = fc4 (units = D_4) (\\hat{I}_1)$ (23)\n$fc4: \\hat{I}_2 = \\sigma_{relu} (W_4^{(22)}. \\hat{I}_1 + b_4)$ (24)\nWe obtain two intermediate vector outputs of dimension $D_4$, $\\hat{I}_1$ and $\\hat{I}_2$, from fc3 and fc4 layers respectively. Based on our experiments, we set $D_3$ as 100. $W_3^{(21)}$ and $W_4^{(22)}$ are weight matrices for fc3 and fc4 of dimension $z_1$ and $z_2$ which are 100$\\times$784 and 100$\\times$100 respectively. $b_3$ and $b_4$ biases of 100-D are used in fc3 and fc4. Next, we pass intermediate vector $\\hat{I}_2$ to the prediction layer for final classification."}, {"title": "3.4.4. Prediction Layer", "content": "We generate class labels for each comment in multilingual comment set C"}]}