{"title": "PageRank Bandits for Link Prediction", "authors": ["Yikun Ban", "Jiaru Zou", "Zihao Li", "Yunzhe Qi", "Dongqi Fu", "Jian Kang", "Hanghang Tong", "Jingrui He"], "abstract": "Link prediction is a critical problem in graph learning with broad applications such as recommender systems and knowledge graph completion. Numerous research efforts have been directed at solving this problem, including approaches based on similarity metrics and Graph Neural Networks (GNN). However, most existing solutions are still rooted in conventional supervised learning, which makes it challenging to adapt over time to changing customer interests and to address the inherent dilemma of exploitation versus exploration in link prediction. To tackle these challenges, this paper reformulates link prediction as a sequential decision-making process, where each link prediction interaction occurs sequentially. We propose a novel fusion algorithm, PRB (PageRank Bandits), which is the first to combine contextual bandits with PageRank for collaborative exploitation and exploration. We also introduce a new reward formulation and provide a theoretical performance guarantee for PRB. Finally, we extensively evaluate PRB in both online and offline settings, comparing it with bandit-based and graph-based methods. The empirical success of PRB demonstrates the value of the proposed fusion approach. Our code is released at https://github.com/jiaruzouu/PRB", "sections": [{"title": "1 Introduction", "content": "Link prediction is an essential problem in graph machine learning, focusing on predicting whether a link will exist between two nodes. Given the ubiquitous graph data in real-world applications, link prediction has become a powerful tool in domains such as recommender systems [72] and knowledge graph completion [49, 41]. Considerable research efforts have been dedicated to solving this problem. One type of classic research approaches is heuristic-based methods, which infer the likelihood of links based on node similarity metrics [43, 46]. Graph Neural Networks (GNNs) have been widely utilized for link prediction. For example, Graph Autoencoders leverage Message Passing Neural Network (MPNN) representations to predict links [29]. Recently, MPNNs have been combined with structural features to better explore pairwise relations between target nodes [73, 70, 18, 61].\nExisting supervised-learning-based methods for link prediction are designed for either the static [73, 70, 18, 61] or relatively dynamic environment [64, 55, 62, 58, 69, 19, 27, 26, 75], they (chrono-logically) split the dataset into training and testing sets. Due to the dynamic and evolving nature of many real-world graphs, ideal link prediction methods should adapt over time to consistently meet the contexts and goals of the serving nodes. For instance, in short-video recommender systems, both video content and user preferences change dynamically over time [28]. Another significant challenge is the dilemma of exploitation and exploration in link prediction. The learner must not only exploit past collected data to predict links with high likelihood but also explore lower-confidence target nodes to acquire new knowledge for long-term benefits. For example, in social recommendations, it"}, {"title": "2 Related Work", "content": "Contextual Bandits. The first line of works studies the linear reward assumption, typically calculated using ridge regression [39, 8, 1, 60, 21, 53]. Linear UCB-based bandit algorithms [1, 9, 40] and linear Thompson Sampling [4, 2] can achieve satisfactory performance and a near-optimal regret bound of $\\tilde{O}(\\sqrt{VT})$. To learn general reward functions, deep neural networks have been adapted to bandits in various ways [10, 11]. [54, 47] develop L-layer DNNs to learn arm embeddings and apply Thompson Sampling on the final layer for exploration. [76] introduced the first provable neural-based contextual bandit algorithm with a UCB exploration strategy, and [74] later extended to the TS framework. [22] provides sharper regret upper bound for neural bandits with neural online regression. Their regret analysis builds on recent advances in the convergence theory of over-parameterized neural networks [24, 5] and uses the Neural Tangent Kernel [34, 6] to establish connections with linear contextual bandits [1]. [12, 13] retains the powerful representation ability of neural networks to learn the reward function while using another neural network for exploration. [52, 51] integrates exploitation-exploration neural networks into the graph neural networks for fine-grained exploration and exploration. Recently, neural bandits have been adapted to solve other learning problems, such as active learning[14, 7], meta learning[53].\nLink Prediction Models. Three primary approaches have been identified for link prediction models. Node embedding methods, as described by previous work [50, 30, 57, 23, 44, 45, 25], focus on"}, {"title": "3 Problem Definition", "content": "Let $G_0 = (V, E_0)$ be an undirected graph at initialization, where V is the set of n nodes, $|V| = n$, and $E_0 \\subset V \\times V$ represents the set of edges. $E_0$ can be an empty set in the cold-start setting or include some existing edges with a warm start. Each node $v_i \\in V$ is associated with a context vector $x_{0,i} \\in \\mathbb{R}^d$. Then, we formulate link prediction as the problem of sequential decision-making under the framework of contextual bandits. Suppose the learner is required to finish a total of T link predictions. We adapt the above notation to all the evolving T graphs $\\{G_t = (V, E_t)\\}_{t=0}^{T-1}$ and let $[T] = \\{1, ..., T\\}$. In a round of link prediction $t \\in [T]$, given $G_{t-1} = (V, E_{t-1})$, the learner is presented with a serving node $v_t \\in V$ and a set of k candidate nodes $V_t = \\{v_{t,1},..., v_{t,k}\\} \\subseteq V$, where $V_t$ is associated with the corresponding k contexts $X_t = \\{x_{t,1},..., x_{t,k}\\}$ and $|V_t| = k$. In the scenario of social recommendation, $v_t$ can be considered as the user that the platform (learner) intends to recommend potential friends to, and the other candidate users will be represented by $V_t$. $V_t$ can be set as the remaining nodes $V_t = V_t/v_t$ or formed by some pre-selection algorithm $V_t \\subset V_t$.\nThe goal of the learner is to predict which node in $V_t$ will generate a link or edge with $v_t$. Therefore, we can consider each node in $V_t$ as an arm, and aim to select the arm with the maximal reward or the arm with the maximal probability of generating an edge with $v_t$. For simplicity, we define the reward of link prediction as the binary reward. Let $v_{t,\\hat{i}} \\in V_t$ be the node selected by the learner. Then, the corresponding reward is defined as $r_{t,\\hat{i}} = 1$ if the link $[v_t, v_{t,\\hat{i}}]$ is really generated; otherwise, $r_{t,\\hat{i}} = 0$. After observing the reward $r_{t,\\hat{i}}$, we update $E_{t-1}$ to obtain the new edge set $E_t$, and thus new $G_t$.\nFor any node $v_{t,i} \\in V_t$, denote by $D_{y|x_t,i}$ the conditional distribution of the random reward $r_{t,i}$ with respect to $x_{t,i}$, where $y = \\{1,0\\}$. Then, inspired by the literature of contextual bandits, we define the following pseudo regret:\n$R_T = \\sum_{t=1}^T (\\mathbb{E}_{r_{t,i^*} \\sim D_{y|x_{t,i^*}}}[r_{t,i^*}] - \\mathbb{E}_{r_{t,\\hat{i}} \\sim D_{y|x_{t,\\hat{i}}}}[r_{t,\\hat{i}}]) = \\sum_{t=1}^T (\\mathbb{P}(r_{t,i^*} = 1|x_{t,i^*}) - \\mathbb{P}(r_{t,\\hat{i}} = 1|x_{t,\\hat{i}}))$"}, {"title": "4 Proposed Algorithms", "content": "Algorithm 1 describes the proposed algorithm PRB. It integrates contextual bandits and PageRank to combine the power of balancing exploitation and exploration with graph connectivity. The first step is to balance the exploitation and exploration in terms of the reward mapping concerning node contexts, and the second step is to propagate the exploitation and exploration score via graph connectivity.\nTo exploit the node contexts, we use a neural network to estimate rewards from the node contexts. Let $f_1(\\cdot; \\theta_1)$ be a neural network to learn the mapping from the node context to the reward. Denote the initialized parameter of $f_1$ by $\\theta_1^0$. In round t, let $\\theta_1^{t-1}$ be parameter trained on the collected data of previous t - 1 rounds including all selected nodes and the received rewards. Given the serving node $v_t$, for any candidate node $v_{t,i} \\in V_t$, $f_1(x_{t,i}; \\theta_1^{t-1}), i \\in V_t$ is the estimated reward by greedily exploiting the observed contexts, which we refer to as \u201cexploitation\u201d. Suppose $\\hat{i}$ is the index of selected nodes. To update $\\theta_1$, we can conduct stochastic gradient descent to update $\\theta_1$ based on the collected training sample $(x_{t,\\hat{i}}, r_{t,\\hat{i}})$ with the squared loss function $\\mathcal{L}_{(\\theta_1^{t-1})}(x_{t,\\hat{i}}, r_{t,\\hat{i}}) = [f(x_{t,\\hat{i}}; \\theta_1^{t-1}) - r_{t,\\hat{i}}]^2/2$. Denote the updated parameters by $\\theta_1^t$ for the next round of link prediction.\nIn addition to exploiting the observed contexts, we employ another neural network to estimate the potential gain of each candidate node in terms of reward for exploration. This idea is inspired by [12]. Denote the exploration network by $f_2(\\cdot; \\theta_2)$. $f_2$ is to learn the mapping from node contexts and the discriminative ability of $f_1$ to the potential gain. In round $t \\in [T]$, given node context $x_{t,i} \\in V_t$ and its estimated reward $f_1(x_{t,i}; \\theta_1^{t-1})$, the input of $f_2$ is the gradient of $f_1(x_{t,i}; \\theta_1^{t-1})$ with respect to $\\theta_1^{t-1}$, denoted by $\\phi(x_{t,i})$, and $f_2(\\phi(x_{t,i}); \\theta_2^{t-1})$ is the estimated potential gain. After the learner selects the node $x_{t,\\hat{i}}$ and observes the reward $r_{t,\\hat{i}}$, the potential gain is defined as $r_{t,\\hat{i}} - f_1(x_{t,i}; \\theta_1^{t-1})$, which is used to train $f_2$. Thus, after this interaction, we conduct the stochastic gradient descent to update $\\theta_2$ based on the collected sample $(\\phi(x_{t,\\hat{i}}), r_{t,\\hat{i}} - f_1(x_{t,i}; \\theta_1^{t-1}))$ with the squared loss function $\\mathcal{L}_{(\\theta_2^{t-1})}(\\phi(x_{t,\\hat{i}}), r_{t,\\hat{i}} - f_1(x_{t,i}; \\theta_1^{t-1})) = [f(\\phi_{t,i}; \\theta_2^{t-1}) - (r_{t,\\hat{i}} - f_1(x_{t,i}; \\theta_1^{t-1}))]^2/2$. Denote by $\\theta_2$"}, {"title": "5 Regret Analysis", "content": "In this section, we provide the theoretical analysis of PRB by bounding the regret defined in Eq.3.1. One important step is the definition of the reward function, as this problem is different from the standard bandit setting that focuses on the arm (node) contexts and does not take into account the graph connectivity. First, we define the following general function to represent the mapping from the node contexts to the reward. Given the serving node $v_t$ and an arm node $v_{t,i} \\in V_t$ associated with the context $x_{t,i}$, the reward conditioned on $v_t$ and $v_{t,i}$ is assumed to be governed by the function:\n$\\mathbb{E}[r_{t,i}|v_t, v_{t,i}] = y(x_{t,i})$\nwhere y is an unknown but bounded function that can be either linear or non-linear. Next, we provide the formulation of the final reward function. In round $t \\in [T]$, let $\\mathbf{y}_t$ be the vector to represent the expected rewards of all candidate arms $\\mathbf{y}_t = [y(x_{t,i}) : v_{t,i} \\in V_t]$. Given the graph $G_{t-1}$, its normalized adjacency matrix $P_t$, and the damping factor $\\alpha$, inspired by PageRank, the optimizing problem is defined as: $\\mathbf{v}_t^* = \\arg \\min_{\\mathbf{v}} \\alpha \\mathbf{v}^T (I - P_t)\\mathbf{v} + (1 - \\alpha)|\\mathbf{v} - \\mathbf{y}_t||_2^2$. Then, its optimal solution is\n$\\mathbf{v}_t^* = \\alpha P_t \\mathbf{v}_t^* + (1 - \\alpha) \\mathbf{y}_t$.\nFor any candidate node $v_{t,i} \\in V_t$, we define its expected reward as $\\mathbb{E}_{r_{t,i} \\sim D_{y|x_t,i}}[r_{t,i}] = \\mathbf{v}[i]$. $\\mathbf{v}$ is a flexible reward function that reflects the mapping relation of both node contexts and graph connectivity. $\\alpha$ is a hyper-parameter to trade-off between the leading role of graph connectivity and node contexts. When $\\alpha = 0$, $\\mathbf{v}$ turns into the reward function in contextual bandits [76, 12]; when $\\alpha = 1$, $\\mathbf{v}$ is the optimal solution solely for graph connectivity. Here, we assume $\\alpha$ is a prior knowledge. Finally, the pseudo-regret is defined as\n$R_T = \\sum_{t=1}^T (\\mathbf{v}[i^*] - \\mathbf{v}[\\hat{i}])$\nwhere $i^* = \\arg \\max_{v_{t,i} \\in V_t} \\mathbf{v}[i]$ and $\\hat{i}$ is the index of the selected node. The regret analysis is associated with the Neural Tangent Kernel (NTK) matrix as follows:\nDefinition 5.1 (NTK [34, 63]). Let $N$ denote the normal distribution. Given all data instances $\\{x_t\\}_{t=1}^{T_k}$, for $i, j \\in [T_k]$, define\n$\\begin{array}{l} H_{i j}^L=\\mathbb{E}_{a, b \\sim N\\left(0, I_d\\right)}\\left[\\sigma^{\\prime}(a) \\sigma^{\\prime}(b)\\right], A=\\left(\\begin{array}{cc}\\Sigma_{i i}^L & \\Sigma_{i j}^L \\\\ \\Sigma_{j i}^L & \\Sigma_{j j}^L\\end{array}\\right) \\\\ \\Sigma_{i, j}=\\mathbb{E}_{a, b \\sim N\\left(0, I_d\\right)}[\\sigma(a) \\sigma(b)], \\\\ H_{i j}^1=2 \\mathbb{E}_{a, b \\sim N(0,1)}\\left[\\sigma^{\\prime}(a) \\sigma^{\\prime}(b)\\right]+ \\Sigma_{i, j}.\\end{array}$\nThen, the NTK matrix is defined as $H=\\left(H^L+\\Sigma^L\\right) / 2$.\nAssumption 5.1. There exists $\\lambda_0>0$, such that $H \\geq \\lambda_0 I$.\nThe assumption 5.1 is generally made in the literature of neural bandits [76, 74, 20, 35, 12, 10, 65] to ensure the existence of a solution for NTK regression.\nAs the standard setting in contextual bandits, all node contexts are normalized to the unit length. Given $x_{t,i} \\in \\mathbb{R}^d$ with $||x_{t,i}||_2 = 1$, $t \\in [T]$, $i \\in [k]$, without loss of generality, we define a fully-connected network with depth $L > 2$ and width $m$:\n$f(x_{t,i}; \\theta) = W_L\\sigma(W_{L-1}\\sigma(W_{L-2}...\\sigma(W_1x_{t,i})))$\nwhere $\\sigma$ is the ReLU activation function, $W_l \\in \\mathbb{R}^{m \\times d}, W_{l_{-1}} \\in \\mathbb{R}^{m \\times m}$, for $2 < l < L - 1$, $W_L \\in \\mathbb{R}^{1 \\times m}$, and $\\theta = [vec(W_1), vec(W_2),..., vec(W_L)^T]^T \\in \\mathbb{R}^P$. Note that our analysis can also be readily generalized to other neural architectures such as CNNs and ResNet [5, 24]. We employ the following initialization [17] for $\\theta$: For $l \\in [L - 1]$, each entry of $W_l$ is drawn from the normal distribution $N(0, 2/m)$; each entry of $W_L$ is drawn from the normal distribution $N(0, 1/m)$. The network $f_1$ and $f_2$ follows the structure of f. Define $\\mathbf{y} = [y(x_{t,i}) : t \\in [T], i \\in [k]]$. Finally, we provide the performance guarantee as stated in the following Theorem."}]}