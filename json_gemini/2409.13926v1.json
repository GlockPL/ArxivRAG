{"title": "SpaceBlender: Creating Context-Rich Collaborative Spaces Through Generative 3D Scene Blending", "authors": ["Nels Numan", "Shwetha Rajaram", "Balasaravanan Thoravi Kumaravel", "Nicolai Marquardt", "Andrew D. Wilson"], "abstract": "There is increased interest in using generative AI to create 3D spaces for Virtual Reality (VR) applications. However, today's models produce artificial environments, falling short of supporting collaborative tasks that benefit from incorporating the user's physical context. To generate environments that support VR telepresence, we introduce SPACEBLENDER, a novel pipeline that utilizes generative AI techniques to blend users' physical surroundings into unified virtual spaces. This pipeline transforms user-provided 2D images into context-rich 3D environments through an iterative process consisting of depth estimation, mesh alignment, and diffusion-based space completion guided by geometric priors and adaptive text prompts. In a preliminary within-subjects study, where 20 participants performed a collaborative VR affinity diagramming task in pairs, we compared SPACEBLENDER with a generic virtual environment and a state-of-the-art scene generation framework, evaluating its ability to create virtual spaces suitable for collaboration. Participants appreciated the enhanced familiarity and context provided by SPACEBLENDER but also noted complexities in the generative environments that could detract from task focus. Drawing on participant feedback, we propose directions for improving the pipeline and discuss the value and design of blended spaces for different scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "There is increased interest in integrating generative models into Virtual Reality (VR) development workflows to accelerate content creation in commercial tools\u00b9 and enable end-user customization [12, 14, 54]. The recent proliferation of generative AI tools introduces low-effort techniques for end-users to create 3D objects [42, 53], panoramic images [6, 63-65], and 3D scenes [3, 19, 30, 58, 70, 79]. Many operate with minimal input such as text and images, offering an easier alternative to the conventional, labor-intensive process of modeling 3D scenes and paving the way for new forms of interactive systems.\nIn this work, we leverage these developments to explore creating custom virtual environments for VR telepresence systems. Prior research demonstrated various benefits of incorporating users' familiar real-world context into virtual environments in remote collaboration scenarios, such as supporting deixis [25, 48, 51, 61]), mutual awareness [29, 67, 69], and information recall [13, 18, 35, 36]. Motivated by these findings, we explore a generative approach to creating spaces by blending together multiple users' environmental contexts. This extends the body of work on aligning dissimilar remote spaces for mixed reality collaboration, e.g., via common object anchors [9, 25, 29, 34] or mesh overlays [29, 57].\nWe identify two key challenges with using today's generative AI models to augment the creation of 3D environments for VR telepresence. First, most models are aimed at producing fully synthetic output that is not grounded in real-world spaces [19, 21]. Models that attempt to do such grounding require input beyond text and image and can only ground themselves in a single space [30, 58, 65]. Second, the 3D meshes generated by these models are not explicitly optimized for use as VR environments. Through our development process, we found that these generated environments can pose core usability issues for VR telepresence, such as non-navigable pathways, distracting visual and geometric artifacts, and uncanny spaces that detract from user comfort.\nTo address these challenges, we developed SPACEBLENDER, a novel pipeline that leverages and extends state-of-the-art generative AI techniques to blend users' physical surroundings into unified virtual spaces suitable for VR telepresence. This pipeline transforms user-provided 2D images of distinct spaces into context-rich 3D environments through a multi-stage process. First, we transform the 2D images into 3D meshes based on depth estimation, depth alignment, and backprojection. We then employ a RANSAC-based alignment technique to align the disparate 3D meshes, ensuring a uniform floor level. Finally, we use a diffusion-based method for space completion, guided by geometric priors and text prompts defined by a Large Language Model (LLM) acting as an interior architect.\nAs a preliminary assessment of the suitability of environments generated by SPACEBLENDER in supporting collaborative VR, we conducted a comparative study with 20 participants. In pairs, they performed a VR-based affinity diagramming task in three different"}, {"title": "2 RELATED WORK", "content": "SPACEBLENDER builds on computational techniques for generating 3D artifacts and prior work that motivates the representation of physical spaces in VR telepresence."}, {"title": "2.1 Computational Generation of 3D Spaces", "content": "The domain of computational generation of 3D scenes has been significantly revitalized by recent advancements in generative methods. Unlike traditional procedural generation techniques that depend on predefined rules and asset libraries to assemble 3D environments [7, 10, 62, 68], recent approaches can generate entirely new spaces via novel generative techniques. These approaches use a variety of scene representations, including well-known explicit formats like 2D panoramic views [6, 63, 64] and meshes [19, 30, 58, 59], as well as recent implicit representations such as Neural Radiance Fields (NeRFs) [3, 75], 3D Gaussian Splats [8], and Signed Distance Functions (SDFs) [32].\nThe majority of recent scene generation methods are grounded in the 2D image domain due to the wide availability of image-caption datasets and state-of-the-art generation models trained on these [55]. By leveraging depth estimation models [1, 4], 2D images can be transformed into 3D representations by predicting a depth value for each pixel and backprojecting these values into 3D space. Utilizing models that can handle panoramic images, combined with image upscaling techniques to move beyond image generation models' limited output resolution (e.g., [2, 33]), systems such as Skybox AI\u00b2 and LDM3D [63, 64] can generate high-quality 360\u00b0 skyboxes based on user-provided text prompts. However, we observe that such techniques generate 3D spaces that do not stay spatially consistent during navigation due to their single-view generation approach, making them less suited for VR telepresence.\nOther image-based scene generation methods address this limitation by generating multi-view image sequences [21, 30, 65]. One such method is TEXT2ROOM, which, given a camera trajectory with matching text prompts and an optional starting image, iteratively"}, {"title": "2.2 Physical Spaces in VR Telepresence", "content": "A large body of prior work has studied telepresence systems for supporting users in collaborative tasks. A key objective in these systems is supporting mutual awareness, which refers to the shared understanding of where other users are and what they are doing [17, 26]. Much of this research focused on establishing awareness in unidirectional settings, where a singular physical space is captured and represented to one or more remote users [38, 45, 46]. This approach supports tasks centered on a single environment, such as remote assistance, by providing remote users with a view into a specific physical space without mutual visibility.\nRecent work is increasingly focused on achieving bidirectional awareness to enable new interaction concepts in collaborative settings that not only resemble but also further extend face-to-face collaboration metaphors. These systems often integrate physical and virtual elements belonging to local user's or remote user's space into a common interaction space, which is referred to as Extended Collaborative Space (xspace) by Kumaravel and Hartmann [67]. Based on a literature review, Herskovitz et al. [29] identified three categories of techniques for creating such shared spaces: (1) object-centric methods, using specific objects to align spaces [9, 25, 34]; (2) perspective-driven methods, such as portals [37, 69] and world-in-miniature views [43, 52, 66]; and (3) mesh-based methods such as mesh overlays [43, 48, 57]. For example, Loki [66] used a world-in-miniature volumetric representation to provide real-time awareness cues of remote users' workspace contexts; RealityBlender [25] used multiple anchor objects (e.g., whiteboards, tables) to establish an"}, {"title": "3 SPACEBLENDER SYSTEM", "content": "This section details the SPACEBLENDER pipeline, designed to integrate images of the physical context of multiple users into cohesive virtual environments. SPACEBLENDER builds upon the TEXT2ROOM pipeline due to its extensibility through the usage of off-the-shelf 2D image models, transparent iterative generation process, and ability to initialize generation based on single 2D input images, which are commonly available in telepresence scenarios.\nThis section starts by outlining the system requirements in Sec. 3.1, followed by an overview of the proposed SPACEBLENDER pipeline in Sec. 3.2, and then provides a detailed description of its two phases in Secs. 3.3 and 3.4."}, {"title": "3.1 Requirements", "content": "The assumptions, implementation details, and limitations of the TEXT2ROOM framework present significant barriers to generating unified spaces from disparate input images for VR usage. Below, we outline the requirements of SPACEBLENDER, alongside the associated challenges and our approach to addressing them.\nRequirement 1: Enabling multiple disparate spatial inputs from diverse perspectives and locations.\nChallenge: TEXT2ROOM accepts at most one input image and does not register the resulting mesh in a global coordinate space, while SPACEBLENDER must be able to process and align multiple images with various viewpoints to support scene blending.\nApproach: We introduce a floor plane alignment process that identifies the floor of each mesh by backprojecting semantic values into 3D space, which is then used for global alignment (Sec. 3.3.2). For"}, {"title": "3.2 System Overview", "content": "The SPACEBLENDER pipeline processes n input images to produce a 3D mesh that integrates the context of each image into a cohesive blended environment. The pipeline has two main stages: the first runs once per generation, while the second is iterative, similar to TEXT2ROOM. Below, we give a brief overview of these stages, with detailed descriptions available in the following subsections.\nIn Stage 1, each input image is first preprocessed, after which depth values of each pixel are estimated and backprojected to create a 3D mesh (Sec. 3.3.1). We refer to the resulting n meshes as submeshes throughout the remainder of this paper. Next, the submeshes are aligned to a common floor plane with a RANSAC-based method applied to floor vertices identified by a semantic segmentation model (Sec. 3.3.2), optionally including a floor generation step if no floor is visible in the image. The aligned submeshes are then positioned based on a parameter-based layout technique (Sec. 3.3.3) based on which a geometric prior mesh is created to define the shape of the blended space (Sec. 3.3.4). Lastly, text prompts describing the blended regions (i.e., the empty space between submeshes) of the environment are generated with an LLM based on captions inferred by a VLM (Sec. 3.3.5).\nIn Stage 2, the submeshes are blended through a process that involves repeatedly inpainting and integrating 2D rendered views of the mesh. For each iteration, based on the submesh layout defined in Stage 1, geometric image priors are rendered to function as a guide for the shape of the space (Sec. 3.4.1). These are combined with the generated text prompts from Stage 1 to guide the content and appearance of the space (Sec. 3.4.2). Once the blending process completes, an adaptive mesh completion trajectory is followed to fill remaining gaps in the environment (Sec. 3.4.3).\nImplementation. Like the original TexT2Room implementation, we use Stable Diffusion 1.5 [55] for image generation and inpainting and IronDepth [1] for depth estimation and inpainting. Furthermore, SPACEBLENDER uses the BLIP-2 [40] VLM, GPT-4 [47] LLM, and OneFormer [31] semantic segmentation model. We decoupled the image inpainting process through the usage of a local API A1111 WebUI server API endpoint\u00b3 to provide enough GPU memory for the usage of the various models in our pipeline. The server and pipeline run on separate machines, each equipped with an NVIDIA RTX 4090 GPU in our local setup. It takes about 55-60 minutes to generate a SPACEBLENDER environment with this configuration."}, {"title": "3.3 Stage 1: From 2D Images to 3D Layout", "content": "The pipeline's first stage establishes the spatial structure of the blended environment. It begins with image preprocessing and depth estimation, converting 2D images into 3D submeshes. These submeshes are aligned to a common floor plane and arranged in a circle, based on which a geometric prior is defined. Finally, prompts for the blended regions are generated by an LLM."}, {"title": "3.3.1 From 2D Images to 3D Submeshes (Fig. 3A)", "content": "In this step, the n input images are projected into 3D space. First, a semantic segmentation model is used to detect people in each input image. If a person is detected, that region is removed and inpainted using a prompt inferred by the VLM. The resulting image is then cropped"}, {"title": "3.3.2 Submesh Alignment (Fig. 3B)", "content": "We introduce a floor plane alignment technique to reconcile differing perspectives in input images, ensuring the spatial consistency that is needed for scene blending (Fig. 3B). First, labels for floor-like objects (e.g., floor, carpet) within each submesh are obtained using a semantic segmentation map derived from the input image. These semantic labels are then backprojected into 3D space, replacing the submesh's RGB colors with semantic label values. To handle any discrepancies between the depth estimation and semantic segmentation model output, floor vertices more than 0.3 meters above or below the median Y-coordinate of the floor-like vertices are excluded.\nNext, RANSAC is used to identify a plane corresponding to these floor-like vertices. To ensure a hypothetical plane is a floor, we use three additional heuristics: (1) the plane's orientation must be within 45\u00b0 of the target plane normal; (2) the normal vector must have a positive Y-component; and (3) the extent of the inlier points in the X and Z axes should be at least 0.5 meters. After selecting the best floor plane candidate, we rotate the mesh to align the plane's normal with the Y-axis. Next, we translate the floor to Y = 0 and set the minimum Z-coordinate to 0. Figure 4 shows an example of submeshes aligned to a common floor plane."}, {"title": "Floor Generation (Fig. 3B)", "content": "If input images lack a floor (e.g., only show a wall), preventing floor plane fitting, we employ a generative technique to extend the submesh downward to create a floor for alignment. This method follows a five-step trajectory that interpolates between the following: gradually looking downward (from -5 to -30 degrees), moving backward (from 1 to 1.5 meters), and moving upward (from 0.3 to 1 meter) relative to the initial camera view. For each step, we use a custom floor description generated by the LLM based on the caption of the image obtained with a VLM. If floor generation fails after ten attempts, the submesh remains unaligned for the rest of the generation process."}, {"title": "3.3.3 Submesh Layout (Fig. 3C)", "content": "With submeshes aligned to a universal floor plane, each submesh is transformed to form a layout resembling an open space. This is achieved by positioning the front side (i.e., the side facing the camera used to capture the input image) of each submesh on the perimeter of a circle as seen from a top-down view. The diameter of this circle is determined by a configurable parameter d, which controls the distance between the submeshes. Each submesh faces the center of the unified space, ensuring a clear line of sight between all submeshes. This design choice was made in consideration of the importance of mutual awareness in collaborative scenarios [17, 26]."}, {"title": "3.3.4 Geometric Prior Mesh (Fig. 3C)", "content": "Given the aligned submeshes, a geometric prior mesh is generated to define the shape of the blended space. This involves the definition of a mesh based on the convex hull of the submesh layout, with faces assigned to represent the floor, walls, and ceiling. The height of this mesh is set to the height of the tallest submesh, or 2.5 meters if none is taller. The floor, ceiling, and wall faces are colored based on their respective semantic label colors from the ADE20K dataset [78]. This mesh is"}, {"title": "3.3.5 Contextually Adaptive Prompt Inference (Fig. 3D)", "content": "In preparation of the iterative mesh generation process in Stage 2, text prompts are generated to describe the intended contents of the blended regions. This begins by obtaining image descriptions for each submesh using the VLM, along with a rotation value indicating their relative orientation from a top-down view. This data is then passed to an LLM instructed to act like a highly creative interior architect and photographer skilled at designing spaces with diverse contexts and appearances while avoiding repetitive objects. Based on the rotation values and known image descriptions, the LLM returns a set of new image descriptions paired with rotation values corresponding to the regions to be blended (i.e., the blank regions between submeshes). The full system prompt of this LLM is provided in Sec. A.1."}, {"title": "3.4 Stage 2: Iterative Blending Guided by Geometric Priors and Contextual Prompts", "content": "Utilizing the submesh layout, geometric prior mesh, and text prompts from Stage 1, this stage iteratively blends the disparate submeshes into a unified environment."}, {"title": "3.4.1 Room Shape Guidance with Geometric Prior Images (Fig. 5E)", "content": "To enable geometrically coherent space blending, SPACEBLENDER uses a collection of prior images to guide the iterative text-conditioned image inpainting step of the mesh blending and completion process via ControlNet [76]. Each time the process renders a view of the mesh for inpainting, it also renders a set of prior images from the same camera viewpoint based on the geometric prior mesh aligned with the submesh layout. SPACEBLENDER is capable of generating three types of prior images which each have a distinct effect on the output of the image inpainting process:\n\u2022 Depth Prior: This prior type can act as a hard constraint for generating spaces with shapes and contents similar to the geometric prior. It is defined by rendering the relative depth for a specified view of the geometric prior mesh.\n\u2022 Layout Prior: This prior type enables constraining the shape of the space without limiting its content (i.e., furniture). It is defined by calculating depth gradients using the Sobel operator to form surface normals based on the depth prior. Subsequently, the magnitude of these normals is calculated and processed with Canny edge detection to produce an image that effectively outlines the geometric prior mesh with white lines outlining the walls, floor, and ceiling on a black background."}, {"title": "\u2022 Semantic Prior:", "content": "This prior type can act as an additional hard constraint to guide the semantic contents of the in-painted views. The current geometric prior mesh definition of SPACEBLENDER only defines semantic labels for the walls, floor, and ceiling, making it suitable to serve as room layout composition guidance when an empty open space is desired. These prior images can be stacked and combined using Multi-ControlNet, which allows for adjusting each prior's influence on the image output. For example, using only the layout prior guides the model to generate a space with a specific room structure while allowing the room content (e.g., furniture) to be generated freely. A depth prior can be added to guide the image inpainting model to generate furniture commonly positioned near the walls (e.g., sofas and bookshelves). An example of the depth and layout priors' influence on the output is shown in Fig. 6, demonstrating the varying effects on room contents, with more examples shown in Fig. A.1. While the depth and semantic prior images are used with pre-trained ControlNet models5, the layout prior is used with a custom ControlNet model, ControlNet-Layout. We describe the training process of this model below.\nTraining ControlNet-Layout. We trained ControlNet-Layout on a dataset of 13,182 images. Instead of using an existing dataset, we created our own by using the pre-trained ControlNet segmentation models using semantic maps inferred from SUN-RGBD [60] and LSUN [74, 77], resized to 512\u00d7512 pixels. This was repeated several"}, {"title": "3.4.2 Iterative Space Blending (Fig. 5F)", "content": "This step unifies disparate submeshes iteratively, according to the geometric prior images and prompts defined in Sec. 3.3.5. To enable SPACEBLENDER 's blending capabilities, we broaden the context window of the image inpainting model by increasing the resolution from 512 \u00d7 512 (as used by TEXT2ROOM) to 512\u00d71280 while maintaining the rendering camera's field-of-view of 55\u00b0. This is enabled by an A1111 WebUI plugin implementation of MultiDiffusion [2].\nBy increasing the width of the images generated throughout the blending process, we broaden the inpainting model's environmental reference frame and enable it to blend the space between neighboring submeshes in a single inpainting step (see Fig. 6), yielding higher fidelity results compared to step-wise blending, which results in harsh boundaries and artifacts such as shown in Fig. 2. However, due to its circular layout method, when SPACEBLENDER is given three or fewer input images, the large distances between submeshes still prevent blending in a single step. In these cases, SPACEBLENDER uses LLM-based prompts to create intermediate"}, {"title": "3.4.3 Mesh Completion Trajectory", "content": "The initial iterative blending process creates a mesh that horizontally integrates the submeshes, defining the blended space from a central perspective. However, at this stage, the floor and ceiling are largely incomplete, and the mesh contains significant gaps that need filling. To address this, an additional set of camera trajectories is employed.\nFirst, interpolation-based trajectories are generated to cover the missing sections of the floor and ceiling. Then, trajectories for each submesh are defined. These paths interpolate the position and rotation of the camera viewpoint, starting centrally within the unified space and initially directed at a specific submesh. The interpolation trajectory ends at the submesh center, facing either the left or right adjacent submesh. Throughout this process, the text prompt passed to the image inpainting model is defined as the text prompt of the blended area that is most closely aligned with the camera's view.\nLastly, an additional trajectory simulates a user looking around the unified space from the center point of their submesh, ensuring the mesh accounts for gaps visible from typical user perspectives. To mimic natural gaze variations, a degree of randomness is introduced into these viewpoints. Once all trajectories are completed, the blended space is ready for use in a VR telepresence system."}, {"title": "4 PRELIMINARY USER STUDY", "content": "Our preliminary user study explored the effects of space blending in the context of a collaborative affinity diagramming task within a VR telepresence environment. The study used a within-subjects design with three conditions that emphasized different visual and geometric qualities of virtual environments. With the selection of conditions below, we sought to explore variations across the dimensions of environmental visual and geometric complexity, fidelity, and familiarity to study their impacts on user behavior and strategies.\n\u2022 GENERIC3D served as a baseline representing low-poly environments commonly used in current social VR platforms such as Meta Horizon and Recroom. This space was designed with 3D models from the public domain7.\n\u2022 TEXT2ROOM served as a baseline representing an environment generated with a state-of-the-art 3D scene generation method, containing salient landmarks that contrast with the simple landmarks of the GENERIC3D condition. This environment was produced by the TEXT2ROOM framework [30], which we extended to develop SPACEBLENDER.\n\u2022 SPACEBLENDER represents our pipeline with participant-provided input images of spaces familiar to them.\nThe order of conditions was counterbalanced. The GENERIC3D and TEXT2ROom environments were consistent for all pairs and are shown in Fig. 7. For the TEXT2ROOM condition, we used the pipeline's public source code and trajectory files to generate twelve environments and then selected the best one based on subjective visual analysis of their geometric and visual quality. For the SPACEBLENDER condition, a new environment was generated for each pair to embed a familiar physical context for each participant, which are shown in Fig. 9. This involved collecting images via an online form sent to participants before the study, where they could upload a photo of a familiar space (e.g., a library, cafe, living room, or desk). Participants who did not upload a photo could choose from seven photographs of various spaces within our institution. Those who found none of the spaces familiar were excluded from the study. Uploaded images were cropped to a 1:1 aspect ratio, excluding any personally identifiable content (e.g., portraits). The study was approved by the University College London Research Ethics Committee (Study ID UCL/CSREC/R/16)."}, {"title": "4.1 Task", "content": "The task involved clustering virtual sticky notes with predefined text. Initially, each participant independently clustered twelve sticky notes from one of two datasets (fruits or vegetables) by color (e.g., placing \"banana\" near \"lemon\"). After two minutes of individual work elapsed, participants were given three more minutes to collaboratively reorganize their clusters into new groups."}, {"title": "4.2 Recruitment", "content": "We recruited 20 participants (10 pairs) through internal mailing lists. Participants (7 Female, 13 Male) had an average age of 26 years (SD = 6.9) and were mainly students and professionals; seven had backgrounds in computer graphics, and four in UX/HCI. All participants had used a VR headset at least once, with eight using them monthly, and three using VR telepresence platforms monthly. Five pairs knew each other before the study. Participants received \u00a315 gift cards as compensation."}, {"title": "4.3 Implementation and Setup", "content": "The VR telepresence system for our study was built with Unity3D and the Ubiq framework [22], providing voice chat, networked objects, and low-poly floating-body avatars. Each participant used a Meta Quest 3 VR headset tethered to a desktop computer with an NVIDIA 4090 RTX GPU in separate physical spaces. Navigation was achieved using the controller's joystick.\nTwo stacks of sticky notes were placed in the virtual environments. Participants could grab the top note by pressing and holding the grip button when their virtual hand was near the stack. Given the high complexity of the meshes generated by generative models, sticky notes could be placed anywhere without physics constraints. The stacks were manually placed (e.g., on a table) within arm's reach of the participant spawn points before the study."}, {"title": "4.4 Procedure", "content": "Upon arrival, the experimenter introduced the study's structure and goals to the participants, including an explanation of the usage of the VR equipment. Participants then read and signed an informed consent form and completed a pre-study questionnaire covering demographics and VR experience. Participants were guided to randomly assigned spaces and equipped with VR headsets. The experimenter then joined the environment from a desktop computer in a separate space, explained the task, and guided participants through a test environment to acclimatize them to the navigation controls and manipulation techniques of the virtual sticky notes.\nParticipants were then teleported to opposite sides of an environment matching the current condition. In the SPACEBLENDER condition, teleportation points were defined to be at the center of the submesh generated based on the image provided by the respective participant. Participants started by individually clustering the sticky notes. After two minutes, the experimenter instructed them to continue the task collaboratively, combining their individual work for three more minutes (Fig. 8). After each condition, participants completed a post-task questionnaire.\nAfter the final condition, participants completed a post-study questionnaire. The study concluded with the experimenter guiding participants to a common physical space for a semi-structured interview, including a brief scenario walkthrough. This walkthrough featured two scenarios: a collaborative study session and a cooking class with friends, as shown in Fig. A.5. The interview questions are shown in A.6."}, {"title": "4.5 Data Collection & Measures", "content": "The post-task questionnaire included questions to measure participants' perceived spatial presence from an existing questionnaire [28], specifically focusing on two dimensions: Self-Location, which assesses the sensation of being physically present within the virtual environment, and Possible Actions, which measures the perceived capability for interaction within that space. We also incorporated questions to measure perceived Copresence to evaluate participants' perceptions of sharing the virtual environment with others [27]. For each question, \"Do not agree at all\" was treated as 1, and \"Fully Agree\" as 5.\nAdditionally, we included four custom questions to assess how various factors influenced task execution: (1) layout, (2) visual quality, (3) level of familiarity, and (4) navigation controls. Each question asked, \"To what extent did [X] help or hinder you in conducting the task?\" with X representing one of the factors. Participants responded using a 5-point Likert scale: significantly hindered (1), slightly hindered, neither helped nor hindered, slightly helped, and significantly helped (5). We averaged the scores of each measure to arrive at a single score for each."}, {"title": "5 RESULTS", "content": "In this section, we present (1) our quantitative analysis of the participants' self-reported measures; (2) qualitative themes around the benefits and limitations of the three environments for the clustering task; and (3) participants' suggestions for future requirements and potential use cases of SPACEBLENDER's environments."}, {"title": "5.1 Self-Reported Questionnaire Results", "content": "To analyze the self-reported questionnaire data, we first applied the Friedman test to identify overall differences across conditions. Following this, we conducted Wilcoxon signed-rank tests for post-hoc comparisons. Key findings are presented below, with additional results provided in Sec. A.7. Fig. 10 shows distributions of participants' scores for Possible Actions, Self-Location, Copresence, and task impact factors. Furthermore, Fig. 11 shows participants' ranked preferences for using the GENERIC3D, TEXT2ROOM, and SPACEBLENDER environments. Most participants ranked the GENERIC3D environment as their first choice for completing the clustering task, followed closely by SPACEBLENDER, which received only first or second choice ratings.\nPossible Actions. A statistically significant difference in Possible Actions scores was found between GENERIC3D (M = 3.89, SD = 0.71) and TEXT2R\u043e\u043e\u043c (M = 3.13, SD = 0.95) (W = 5.0, p = 0.0021), indicating a diminished perception of possible actions within the environment under the TEXT2ROOM condition.\nSelf-Location. A statistically significant difference in Self-Location scores was found between TEXT2ROOM (M = 3.46, SD = 0.78) and SPACEBLENDER (M = 3.91, SD = 0.68) (W = 0.0, p = 0.0039), indicating a decreased sense of being physically present within the environment under the TEXT2ROOM condition.\nImpact of Environmental Factors. Among the impact of environmental factors, we found statistically significant differences for the impact of Layout, Visual Quality, and Familiarity. For Layout we found statistically significant differences between GENERIC3D (M = 3.92, SD = 0.68) and TEXT2R\u043e\u043e\u043c (M = 3.20, SD = 0.75) (W = 10.0, p = 0.0043); and between TEXT2ROOM and SPACEBLENDER (M = 3.85, SD = 0.70) (W = 4.0, p = 0.0036). We also found statistically significant differences for Visual Quality between conditions GENERIC3D (M = 4.00, SD = 0.00) and TEXT2R\u043e\u043e\u043c (M = 3.17, SD = 0.75) (W = 0.0, p = 0.0010); and between TEXT2ROOM and SPACEBLENDER (M = 3.90, SD = 0.30) (W = 3.5, p = 0.0049). Lastly, we found statistically significant differences for Familiarity between TexT2Room (M = 3.10, SD = 0.83) and SPACEBLENDER (M = 3.95, SD = 0.22) (W = 0.0, p = 0.0039)."}, {"title": "5.2 Benefits and Limitations of Environments for the Clustering Task", "content": "Next, we discuss four themes from participants' post-task reflections on the clustering task and to what extent the three styles of virtual environments supported their work. We refer to paired participants as P#A and P#B, where # represents their pair ID."}, {"title": "5.2.1 Environments typically played a passive role in supporting spatial organization, but some participants adapted their clustering strategies to Space Blender's environments' distinct or familiar features", "content": "When asked what strategies they adopted to complete the clustering task across all three conditions, a majority of participants described using the center of the environment as a staging area for finalized clusters. Preferences varied for storing and comparing ungrouped notes either in the middle or in individually assigned regions.\nAlthough the task did not require explicit use of the environment, some participants in SPACEBLENDER utilized its unique features. First, P5A and P6A expressed that SPACEBLENDER's more detailed environments with distinct segments helped to establish mental models of where to organize notes: \"I just put the green objects in the green area of the environment\" (P6A). This strategy contrasts with P6A's experience in GENERIC3D, where a lack of environmental cues led to a more deliberate strategy of labeling areas of the environment to place specific clusters: \u201cWe had to actually allocate areas because the wall was just gray.\u201d Additionally, several participants found value in the familiar details of SPACEBLENDER's environments (i.e., spatial landmarks preserved from their input images) to inform their clustering strategies (P4A, P7A, P10A). P7A noted: \"Familiarity helped because this is pretty much like where I work a lot\"; \"it just feels a bit more like comfortable, like, thinking in that area.\" This sentiment was echoed by P10A, who envisioned aligning sticky notes to their own table in the virtual workspace."}, {"title": "5.2.2 Participants had mixed preferences for minimalistic and realistic environments for supporting their focus on the clustering task", "content": "A majority of participants favored the simplistic design of GENERIC3D as they believed it enabled them to be more engaged with the task. P8A felt that as the \"the cleanest environment,\" the GENERIC3D environment supported task efficiency. Although P3A perceived the GENERIC3D environment to be \"cartoonish\" and"}, {"title": "5.2.3 Participants perceived"}]}