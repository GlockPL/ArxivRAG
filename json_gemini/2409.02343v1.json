{"title": "NUDGE: LIGHTWEIGHT NON-PARAMETRIC FINE-TUNING OF EMBEDDINGS FOR RETRIEVAL", "authors": ["Sepanta Zeighami", "Zac Wellmer", "Aditya Parameswaran"], "abstract": "k-Nearest Neighbor search on dense vector embeddings (k-NN retrieval) from pre-trained embedding models is the predominant retrieval method for text and images, as well as Retrieval-Augmented Generation (RAG) pipelines. In practice, application developers often fine-tune the embeddings to improve their accuracy on the dataset and query workload in hand. Existing approaches either fine-tune the pre-trained model itself or, more efficiently, but at the cost of accuracy, train adaptor models to transform the output of the pre-trained model. We present NUDGE, a family of novel non-parametric embedding fine-tuning approaches that are significantly more accurate and efficient than both sets of existing approaches. NUDGE directly modifies the embeddings of data records to maximize the accuracy of k-NN retrieval. We present a thorough theoretical and experimental study of NUDGE's non-parametric approach. We show that even though the underlying problem is NP-Hard, constrained variations can be solved efficiently. These constraints additionally ensure that the changes to the embeddings are modest, avoiding large distortions to the semantics learned during pre-training. In experiments across five pre-trained models and nine standard text and image retrieval datasets, NUDGE runs in minutes and often improves NDCG@10 by more than 10% over existing fine-tuning methods. On average, NUDGE provides 3.3\u00d7 and 4.3\u00d7 higher increase in accuracy and runs 200\u00d7 and 3\u00d7 faster, respectively, over fine-tuning the pre-trained model and training adaptors.", "sections": [{"title": "1 INTRODUCTION", "content": "k-Nearest Neighbor search on dense vector embeddings (k-NN retrieval) from pre-trained embedding models is the de-facto standard for text and image retrieval, as well as in Retrieval-Augmented Generation (RAG) pipelines. Given n data records (e.g., text chunks or images), k-NN retrieval embeds them using a pre-trained model as d-dimensional vectors in $R^d$. To answer a query, it similarly embeds the query in $R^d$ and retrieves the top-k data records whose embeddings have the highest cosine similarity (or inner product) with the query embedding. By simply performing a top-k look-up (often through vector databases), the simplicity and efficiency of k-NN retrieval has made it increasingly popular and often preferred to other retrieval paradigms, e.g., late interaction or generative retrieval. However, the out-of-the-box pre-trained model is often not sufficiently accurate on the dataset or queries in hand, and typically fine-tuning is used to improve the accuracy.\nThere are two standard approaches to fine-tuning embeddings: fine-tuning the pre-trained models directly (referred to henceforth as PTFT) or training adaptor models on top of the pre-trained models. PTFT can be more accurate but comes with practical challenges: it (1) requires access to the model parameters and must rely on third-party APIs for closed-source models, (2) is computationally expensive, and (3) incurs further hosting and maintenance costs for deployment of the fine-tuned model. An alternative is to learn an Adaptor, $\\hat{g}_{e}$, a transformation of the output of the"}, {"title": "2 PRELIMINARIES", "content": "Notation. We use bar on top of letters to denote raw data/queries that are not embedded. We use boldface capital letters to denote matrices (mostly used to represent embeddings), e.g., $X \\in R^{r \\times s}$ is an $r \\times s$ matrix, and use $X_i$ to refer to the i-th row of the matrix, which is a vector, $X_i \\in R^{s}$. $\\|x\\|$ refers to the $L_2$ norm of a vector $x$, and $\\|X\\|$ refers to the vector of row-wise norms, $(\\|X_1\\|, ..., \\|X_r\\|)$. We use $[s]$ to refer to the set $\\{1, 2, ..., s\\}$.\nk-Nearest Neighbor Retrieval. Given a dataset, $D$ of n records (e.g., text chunks or images), k-NN retrieval first embeds the data records using an embedding model, $E_D$, to generate an embedding matrix $D \\in R^{n \\times d}$ where d is the embedding dimensionality. The i-th row, $D_i$, of D is the embedding of the i-th record, $\\bar{D}_i$, that is, $D_i = E_D(\\bar{D}_i)$. To answer a query q, k-NN retrieval first embeds the query using an embedding model, $E_Q$ (often $E_Q$ and $E_D$ are the same, but can be different, especially in a multi-modal setting) to obtain $q = E_Q(\\bar{q})$. k-NN retrieval then returns the top-k records in D whose embeddings have the highest similarity to q, measured by either the inner product or cosine similarity, i.e., the k records $\\bar{D}_{i_1},..., \\bar{D}_{i_k}$ that correspond to the k highest values in the set $\\{D_1 \\cdot q, ..., D_n \\cdot q\\}$ when using the inner product similarity metric. By default, we use the inner product, which can be applied to normalized embeddings to obtain cosine similarity.\nGround-Truth Answers. Queries can require retrieving multiple data records, and each data record can have a different degree of relevance to the query. For simplicity, here, we present our results in the setting where a query, $\\bar{q}$, requires retrieving a single ground-truth data record. We refer to the ground-truth data record for a query, $\\bar{q}$, as the ground-truth answer to the query and often refer to the data record with its index y, $y \\in [n]$. Extensions to multiple ground-truth data records (each with potentially different degrees of relevance to the query) is straightforward and presented in Appx. D.\nFine-Tuning. Fine-tuning aims to improve retrieval accuracy for the dataset D through optimizing embeddings. We let $D^* \\in R^{n \\times d}$ denote the fine-tuned data embeddings obtained after fine-tuning, where $D^*_i \\in R^{d}$ is the fine-tuned embedding for the i-th data record. We consider a supervised setting where a query set with corresponding ground-truth answers is available, consisting of the query set, Q, and a set, Y, of ground-truth answers, where $Y_i$ is the index of the ground-truth answer for the j-th query, $\\bar{q}_j$. We split this set into two, a training set $Q^T$, $Y^T$ and a validation set $Q^V$, $Y^V$, with $n_T$ and $n_V$ queries, respectively. Let $Q^T \\in R^{n_T \\times d}$ and $Q^V \\in R^{n_V \\times d}$ be matrices containing embeddings for training and validation queries. The training set can be collected over time from user interactions with the system, by collecting labels, or by generating synthetic training data using LLMs"}, {"title": "3 NON-PARAMETRIC EMBEDDING FINE-TUNING", "content": "Our NUDGE approach views embeddings as parameters of the k-NN retrieval algorithm, optimizing the embeddings directly to improve retrieval accuracy. In Sec. 3.1, we formalize the notion of non-parametric embedding fine-tuning by stating two optimization problems, one directly maximizing"}, {"title": "3.1 UNCONSTRAINED NON-PARAMETRIC EMBEDDING FINE-TUNING PROBLEMS", "content": "Let $\\Delta \\in R^{n \\times d}$ be the modification to be learned to the embeddings, so that its i-th row, $\\Delta_i$, is the modification to $D_i$. That is, after fine-tuning, the final embedding is $D^* = D + \\Delta$. We use Q and Y to refer to a generic query embedding matrix with corresponding ground-truth answers containing N queries. Q and Y can respectively be either $Q^T$ and $Y^T$ or $Q^V$ and $Y^V$.\nMaxA-EFT. Maximum Accuracy Embedding Fine-Tuning Problem, MaxA-EFT, is the problem of finding $\\Delta$ to fine-tune data embeddings that maximizes the number of queries in Q answered correctly, formalized as follows. For a query embedding matrix Q with ground-truth answers Y, let $\\mathbb{I}_i(\\Delta)$, for $i \\in [N]$, be the indicator variable denoting if the i-th query is answered correctly after fine-tuning with $\\Delta$. Formally, $\\mathbb{I}_i(\\Delta) = 1$ if the following holds, and zero otherwise\n$Q_i \\cdot (D_{Y_i} + \\Delta_{Y_i}) > Q_i \\cdot (D_j + \\Delta_j), \\forall j\\in [n] \\setminus Y_i$.\nProblem 1 (MaxA-EFT). MaxA-EFT is the problem of finding $\\Delta$ to maximize the number of queries answered correctly after fine-tuning with $\\Delta$, i.e., $\\arg \\max_{\\Delta \\in R^{n \\times d}} \\sum_{i \\in [N]} \\mathbb{I}_i(\\Delta)$.\nTheorem 1. MaxA-EFT is NP-Hard.\nTheorem 1 is proved by reduction from the Maximum Feasible Linear Subsystem problem as studied in , see Appx. B.1. Apart from the NP-hardness, MaxA-EFT allows data embeddings to be arbitrarily changed by $\\Delta$. This can distort the semantics captured in D by the pre-trained model, and lead to poor generalization to queries outside of Q.\nMaxS-EFT. An alternative formulation is Maximum Similarity Embedding Fine-Tuning Problem,\n$\\arg \\max_{\\Delta \\in R^{n \\times d}} \\sum_{i \\in [N]} Q_i \\cdot (D_{Y_i} + \\Delta_{Y_i})$,\nreferred to as MaxS-EFT. Here, we change data embeddings to maximize the similarity between queries and ground-truth answers, a standard optimization objective. However, the non-parametric formulation makes Eq. 2 an unconstrained optimization problem with a linear objective, so the problem is unbounded and has no optimal solution. Moreover, setting $\\Delta_{Y_i}$ so that $Q_i \\cdot \\Delta_{Y_i} > 0$ and increasing the magnitude of $\\Delta_{Y_i}$ arbitrarily improves the objective, yielding trivial solutions with poor generalization to unseen queries."}, {"title": "3.2 NUDGE APPROACHES", "content": "Because of the potential for overfitting and the computational challenges due to NP-hardness, we do not solve either MaxA-EFT or MaxS-EFT directly. Instead, we introduce NUDGE, a family of approaches that solve constrained variations of MaxA-EFT and MaxS-EFT, designed to avoid overfitting, while being efficient. We discuss two main approaches, NUDGE-M and NUDGE-N, in Secs. 3.2.1 and 3.2.2 and present other practical extensions in Appx. C."}, {"title": "3.2.1 NUDGE-M: NUDGE WITH BOUNDED MAGNITUDE", "content": "NUDGE-M solves MaxS-EFT on the training set, but with the added constraint $\\|\\Delta_i\\| \\leq \\gamma, \\forall i \\in [n]$, for a scalar $\\gamma \\geq 0$. $\\gamma$ controls how much each embedding can change during fine-tuning. NUDGE-M sets $\\gamma$ by solving MaxA-EFT on the validation set. Intuitively, this (1) changes data embeddings to maximize the similarity between embeddings and queries on the training set, (2) ensures that the magnitude of the changes to the embeddings is bounded to avoid overfitting, and (3) decides how much the embeddings are allowed to change by maximizing validation accuracy. NUDGE-M"}, {"title": "3.2.2 NUDGE-N: NUDGE WITH NORMALIZED EMBEDDINGS", "content": "NUDGE-N additionally constrains the norm of the fine-tuned embedding. This constraint serves as an additional regularization, helping with out-of-distribution generalization (see Sec.4.2).\nOptimization Formulation. Analogous to MaxS-M($\\gamma$) but with an added constraint, we define MaxS-N($\\gamma$) as the optimal solution to the following optimization problem:\n$\\text{MaxS-N}(\\gamma) = \\arg \\max_{\\Delta \\in R^{n \\times d}} \\sum_{i \\in [n_T]} Q_i \\cdot (D_{Y_i} + \\Delta_{Y_i})$\ns.t. $\\Delta \\in R^{n \\times d}$, $\\|\\Delta_i\\|_2 \\leq \\gamma$, $\\| D_i + \\Delta_i\\| = 1 \\forall i \\in [n]$.\nTo find a suitable $\\gamma$, we solve the same optimization problem as BiMax-M, except that we replace MaxS-M($\\gamma$) with MaxS-N($\\gamma$). We call this problem Bi-level Maximization with Normalized embeddings, BiMax-N, and refer to its optimal solution as $\\Delta_N$.\nNUDGE-N. NUDGE-N is an algorithm that optimally solves BiMax-N:\nTheorem 3. There exists an algorithm, referred to as NUDGE-N, that optimally solves BiMax-N in O($n_V (nd + \\log n_V) + n_T nd$). Specifically, NUDGE-N sets $\\Delta^A$ as\n$\\Delta_i^A =  \\begin{cases}     \\frac{G_i}{\\|G_i\\|} - D_i & \\text{if }  \\frac{G_i}{\\|G_i\\|} \\cdot D_i  \\geq 1-\\frac{\\gamma}{2} \\\\     \\frac{\\sqrt{(\\frac{\\gamma}{4} - 1)}Z_i}{\\|\\sqrt{(\\frac{\\gamma}{4} - 1)}Z_i\\|} - D_i, & \\text{otherwise},   \\end{cases}$\nwhere\n$Z_i = \\frac{G_i - (D_i \\cdot G_i)D_i}{\\|G_i - (D_i \\cdot G_i)D_i\\|}$,\nand $G_i$ is as defined in Eq. 3, for an optimally chosen scalar $\\gamma^*$.\nEq. 6 is the update rule used by NUDGE-N to fine-tune data embeddings, using which the fine-tuned embeddings are obtained as $D^* = D + \\Delta^A$. This moves the data embedding on the unit ball (to satisfy $\\|D_i + \\Delta_i\\| = 1$) between $D_i$ and $\\frac{G_i}{\\|G_i\\|}$, where $\\frac{G_i}{\\|G_i\\|}$ is the normalized sum of embeddings of queries whose ground-truth answer is $\\bar{D}_i$. $\\gamma^*$ determines how much to move the embedding, and $Z_i$ determines the direction. $Z_i$ is the normalized projection of $G_i$ onto the tangent plane of the unit ball at $D_i$, so that moving in the direction of $Z_i$ maximally increases MaxS-N objective.\nFinding the optimal $\\gamma^*$ is similar to NUDGE-M, where we first use KKT to solve MaxS-N($\\gamma$) and substitute the resulting $\\Delta$ into the definition of $\\mathbb{I}_V$ in Eq. 1. The optimal $\\gamma^*$ is then found as the value that satisfies the maximum number of the resulting inequalities. The resulting inequalities are, in this case, quadratic due to $\\sqrt{(4 - \\gamma)}$ in the solution to MaxS-N($\\gamma$), but can still be solved in closed-form. Nevertheless, from a practical perspective, solving the quadratic equations is tedious as it requires considering various special cases. We observed that performing a grid search to find $\\gamma^*$ that maximizes validation accuracy finds good enough solutions and is almost as efficient. Thus, in our experiments, we use this practical implementation."}, {"title": "4 EXPERIMENTS", "content": "We present results on standard text and image retrieval benchmarks and multiple pre-trained models. We present our main experimental results here, but for the sake of space, defer more detailed results and ablation studies to Appx. E.\nDatasets. For text retrieval datasets we use 7 standard datasets: SciFacts, Fever, ArguAna (we use their BEIR versions), TriviaQA Joshi et al. (2017), HotpotQA , and Natural Questions( we use their KILT versions), and NF-Corpus (although all datasets have a BEIR version, we use non-BEIR versions whenever that is larger). We use the datasets as is, without any preprocessing step, except for datasets from KILT, where we only use Wikipedia pages that contain an answer to at least one query (i.e., pages where we expect fine-tuning to have an impact). For image retrieval, we use COCO and Flickr datasets. We use image captions as queries to retrieve the corresponding image. For all text and image datasets, we use 0.7-0.1-0.2 train, validation and test"}, {"title": "4.1 BASELINE RESULTS", "content": "Summary of results. Tables 3-4 present our accuracy results averaged across all text or image datasets for different models. Table 6 presents the per dataset results for BGE-S. The per dataset results for other models followed similar trends and are deferred to Appx. E.2.\nAs Tables 3-4 show, both NUDGE-M and NUDGE-N provide significant accuracy gains, providing up to 14.3% NDCG@10 boost over No Fine-Tuning while PTFT and Adaptor only improve NDCG@10 up to 4.0%, when averaged across datasets, for both text and image retrieval. Interestingly, the accuracy gains from NUDGE-M and NUDGE-N depends on the pre-trained model. GTE-L outperforms TE3-L without fine-tuning, but using NUDGE TE3-L outperforms GTE-L."}, {"title": "4.2 OUT-OF-DISTRIBUTION GENERALIZATION", "content": "Next, we study the impact of fine-tuning on out-of-distribution queries. In this experiment, separately for each dataset, we use K-means to cluster all the queries in two clusters, referred to as C1"}, {"title": "4.3 ABLATION STUDY", "content": "We provide an ablation study to better understand the impact of various constraints in NUDGE. In addition to NUDGE-M and NUDGE-N, we present NUDGE-M+N, which performs normalization on the output of NUDGE-M. That is, instead of incorporating normalization as a constraint in the optimization problem, NUDGE-M+N simply normalizes the embeddings after performing NUDGE-M. We also present NUDGE-NU, which is a variation of NUDGE-N that only includes the normalization constraint, but not the constraint on the magnitude of the change in the embeddings (the solution is equivalent to the first branch of Eq. 6).\nTable 8 shows the results of comparing the above variants across text datasets using the BGE-S model. The table shows normalizing the embeddings as part of the optimization (NUDGE-N) is better than simply normalizing the output after optimization (NUDGE-M+N), although both work better than using unnormalized embeddings (NUDGE-M). Moreover, allowing the magnitude of change to the embeddings to be unbounded (NUDGE-NU) performs worse than all other variants, showing the benefit of constraining how much embeddings can change during fine-tuning. Overall, comparing NUDGE-N, No Fine-Tuning, and all other variants, we see that the benefits of NUDGE-N come from a combination of all design choices made."}, {"title": "5 RELATED WORK", "content": "Decades of research has explored various retrieval methods, including sparse , dense , late interaction and generative retrieval to name a few. Compared with such retrieval paradigms, k-NN retrieval has recently gained increased popularity (see for a survey of recent studies) due to its simplicity and efficiency. k-NN retrieval uses embeddings from pre-trained models, thus avoiding the need for extensive training on the dataset in hand, and performs retrieval through a simple vector index look-up avoiding expensive model inference at query time.\nThis paper focuses on improving k-NN retrieval accuracy given access to a pre-trained embedding model. Related work can be divided into two categories. The first category, including this paper, aims to improve the embeddings through fine-tuning for the specific dataset and query workload. Fine-tuning the pre-trained model itself, through a similar training strategy used during pre-training is possible, but requires access to the model parameters, is computationally expensive and incurs extra hosting and maintenance costs the fine-tuned models at"}, {"title": "6 CONCLUSION", "content": "We studied the problem of fine-tuning embeddings to improve k-NN retrieval accuracy. We presented NUDGE, a novel non-parametric approach to embedding fine-tuning that is efficient, provides significant accuracy boosts and does not require any model hosting or maintenance at deployment time. Our experimental results show NUDGE improves accuracy by up to 16.0% compared with existing methods and up to 24.4% compared with no fine-tuning. Future work includes incorporating NUDGE inside vector databases and generating and maintaining query sets for fine-tuning."}, {"title": "A APPENDIX OVERVIEW", "content": "This appendix is organized as follows:\n\u2022 Appx. B contains the proofs of the theoretical results in the paper.\n\u2022 Appx. C contains other practical NUDGE variants.\n\u2022 Appx. D discusses the extension of NUDGE to multi-label settings.\n\u2022 Appx. E provides details about the experimental setting, and provides additional experiments."}, {"title": "B PROOFS", "content": "Appx. B.1-B.3, respectively, provide the proofs of Theorems. 1-3. The proofs of technical lemmas are presented in Appx. B.4"}, {"title": "B.1 PROOF OF THEOREM 1", "content": "We show a reduction from homogeneous maximum feasible linear subsystem, Max-FLS, shown to be NP-Hard in . We first formally state the decision version of the problem.\nProblem 2 (Homogeneous Max-FLS). Given a linear system Ax < 0, where A is of size s \u00d7 t, and an integer K with 1 < K < s, does there exist a solution x \u2208 Rt satisfying at least K inequalities of the system?\nWe show a reduction from Homogeneous Max-FLS to the decision version of MaxA-EFT. The decision version of MaxA-EFT asks whether there exists $\\Delta$ such that the training accuracy is at least K, i.e., whether at least K different $\\mathbb{I}_i(\\Delta)$, $i \\in [n_T]$ can be satisfied.\nGiven A, the reduction defines an instance of MaxA-EFT by specifying D, Q, and Y. Specifically, we let Q = A, D = 0 \u2208 R2\u00d7t (i.e., a 2 \u00d7 t matrix only consisting of zeros) and Y = {Y1, ..., Ys}, Yi = 0 \u2200i \u2208 [s] (i.e., this instance only has 2 data records, embedding dimension is t and there are s training queries).\nWe next show that there exists $\\Delta$ so that the training accuracy of MaxA-EFT is at least K if and only if there exists a solution x to Homogeneous Max-FLS satisfying at least K inequalities of the system.\nObserve that, by the above construction, $\\mathbb{I}_i(\\Delta)$ is satisfied if and only if\n$Q_i \\cdot (\\Delta_{Y_i} - \\Delta_i) < 0$.\nSuppose the training accuracy is at least K for some $\\Delta$. Let x = $\\Delta_{1}-\\Delta_{0}$, and let $\\{i_1, ..., i_K\\}$ be the index of K training samples answered correctly. Thus, we have $A_i \\cdot x < 0$ for all $i \\in \\{i_1, ..., i_K\\}$, showing the existing K inequalities in the subsystem that are satisfied."}, {"title": "B.2 PROOF OF THEOREM 2", "content": "Setup. Recall that for any $i \\in [n]$,\n$G_i = \\sum_{j \\in [n_T]} \\mathbb{I}[i = Y_i^T]Q_i,$\nso that\n$\\arg \\max_{\\Delta \\in R^{n \\times d}} \\sum_{i \\in [n_T]} Q_i \\cdot (D_{Y_i^T} + \\Delta_{Y_i^T}) = \\arg \\max_{\\Delta \\in R^{n \\times d}} \\sum_{i \\in [n], \\| G_i \\| \\neq 0} \\sum_{i \\in [n_T]}  G_i \\cdot \\Delta_i = \\arg \\max_{\\Delta \\in R^{n \\times d}} \\sum_{i \\in [n_T]}  G_i \\cdot \\Delta_i$.\nThe proofs here use the above formulation for MaxS-M. Moreover, we set $\\Delta_i = 0$ for any $i$ where $\\|G_i\\| = 0$, given that they do not appear in the above objective. Note that even when $\\Delta_i = 0$ for any $i \\in [n]$, the i-th record still influences the BiMax-M objective and needs to be taken into account when solving the outer optimization in BiMax-M.\nFinding MaxS-M. We first find MaxS-M($\\gamma$) by solving the following optimization problem:\n$\\arg \\max_{\\Delta \\in R^{n \\times d}} \\sum_{i \\in [n]} G_i \\cdot \\Delta_i$\ns. t. $\\|\\Delta_i\\| \\leq \\gamma, \\forall i \\in [n]$.\nIn this problem, the constraints are independent for each $i \\in [n]$, $\\|G_i\\| \\neq 0$, and the objective is simply a summation across $G_i \\cdot (D_i + \\Delta_i)$ values, so that a solution $\\Delta^*$ is optimal for this problem if and only if for each $i \\in [n]$, $\\Delta_i^*$ is an optimal solution to\n$\\arg \\max_{\\Delta \\in R^{ d}} G_i \\cdot \\Delta_i$\ns. t. $\\|\\Delta\\| \\leq \\gamma$.\nSolving this problem, we have:"}, {"title": "B.4 PROOF OF TECHNICAL LEMMAS", "content": null}, {"title": "E ADDITIONAL EXPERIMENTS AND DETAILS", "content": "Here we present additional experimental details and results:\n\u2022 Appx. E.1 discussed details on the implementation of Adaptor and PTFT, including hyper-parameter tuning, loss, and efficiency considerations.\n\u2022 Appx. E.2 contains detailed per dataset results summarized in the paper's main body.\n\u2022 Appx. E.3 presents experiments on the training processes of Adaptors and PTFT to under-stand their failure modes.\n\u2022 Appx. E.4 provides an ablation study of various normalization methods in NUDGE.\n\u2022 Appx. E.5 provides an experimental comparison between NUDGE-M and its corresponding iterative variant NUDGE-IM."}]}