{"title": "Bayes-CATSI: A variational Bayesian approach for medical time series data imputation", "authors": ["Omkar Kulkarni", "Rohitash Chandra"], "abstract": "Medical time series datasets feature missing values that need data imputation methods, however, conventional machine learning models fall short due to a lack of uncertainty quantification in predictions. Among these models, the CATSI (Context-Aware Time Series Imputation) stands out for its effectiveness by incorporating a context vector into the imputation process, capturing the global dependencies of each patient. In this paper, we propose a Bayesian Context-Aware Time Series Imputation (Bayes-CATSI) framework which leverages uncertainty quantification offered by variational inference. We consider the time series derived from electroencephalography (EEG), electrooculography (EOG), electromyography (EMG), electrocardiology (EKG). Variational Inference assumes the shape of the posterior distribution and through minimization of the Kullback-Leibler(KL) divergence it finds variational densities that are closest to the true posterior distribution. Thus, we integrate the variational Bayesian deep learning layers into the CATSI model. Our results show that Bayes-CATSI not only provides uncertainty quantification but also achieves superior imputation performance compared to the CATSI model. Specifically, an instance of Bayes-CATSI outperforms CATSI by 9.57%. We provide an open-source code implementation for applying Bayes-CATSI to other medical data imputation problems.", "sections": [{"title": "1. Introduction", "content": "Electronic Health Records (EHRs) have become increasingly important in healthcare over the past decade, revolutionising the way medical information is managed and utilised [1]. EHRS contain a wide range of data including the medical history of patients for the analysis of medically relevant trends ver time [2]. This enables the opportunity to conduct secondary analysis [3] using the EHR data. Laboratory tests are essential tools for assessing and managing a patient's health status as they provide continuous and detailed information that is crucial for effective diagnosis, treatment, and patient safety. Therefore, an important component of healthcare data analytics is modelling clinical time series [4, 5]. Data quality is of utmost importance when using clinical time series data for decision-making; Kramer et al. [6] have shown that there is a strong connection between data quality and clinical intervention that affects the physician's decision-making ability. Clinical time series data often suffers from low quality due to noise, missing values, and irregular collection intervals. These issues necessitate extensive pre-processing to ensure the data is usable for accurate clinical decision-making[7]. Hence, it is important to estimate missing values in a clinical time series, also known as data imputation, where machine learning models have been successfully utilised [8].\nImputing missing values in clinical time series data is complex for several reasons. Firstly, clinical measurements are often recorded at irregular intervals. For example, a patient's heart rate can be monitored continuously, but blood tests are taken only when deemed necessary, leading to gaps in the data [9]. Secondly, the pattern of missing data is not always random. Laboratory tests, such as blood glucose levels, might not be conducted if the caregiver assesses that the patient's condition does not require it [10]. This means the absence of data can indicate a patient's stable health, introducing a bias based on health state. Lastly, missing data often occurs in blocks rather than isolated points. For instance, if a patient is being transferred between hospital wards, monitoring equipment might be temporarily disconnected, resulting in a continuous segment of missing data [9]. These scenarios highlight the challenges in imputing clinical time series data, as they require sophisticated methods to handle non-random and consecutive missing values effectively.\nStatistical and machine learning models have been prominent for imputing missing data [11, 12] for a wide range of problems that includes time series. Nijman et al. [13] reported that missing data has been poorly handled in machine learning-based predictive models. Statistical time series models such as ARMA (autoregressive moving average) and ARIMA (autoregressive integrated moving average) [14] have been widely used; however, these models are linear and cannot handle the noisy and online nature of the data. Matrix completion [15] is a statistics and machine learning strategy that involves the task of filling in the missing entries of a partially observed matrix, and has been successfully used in movie-rating and recommender systems in platforms such as Netflix [16]. Matrix completion has been used for data imputation; however, it requires strong assumptions such as temporal regularity and low rankness. Deep learning methods such as recurrent neural net-"}, {"title": "2. Related Work", "content": "works (RNNs) have been used for data imputation [17, 18] in time series. RNNs have been prominent for modelling temporal data as they use hidden states to capture past observations and provide feedback mechanisms that are not present in simple neural networks [19]. A prominent RNN implementation for time series includes the Bidirectional Recurrent Imputation for Time Series (BRITS)[17]. A major problem faced by BRITS is that they tend to capture local properties rather than global dynamics of the input, which could have a greater impact in the case of clinical time series. For example, a patient with high blood pressure will likely have different temporal patterns compared to other patients, these patterns will be identified only by models that capture the global dynamics of the input data. The CATSI (Context-Aware Time Series Imputation) model [9] was developed to address the challenge of capturing global dynamics. The CATSI model builds upon the bidirectional LSTM architecture of BRITS by incorporating a 'context vector' that captures the global dynamics of patients' input data. This context vector allows the model to account for broader temporal dependencies and correlations, leading to more accurate and contextually aware imputations.\nModels such as CATSI[9] and BRITS[17] use deep learning models, resulting in point predictions that lack uncertainty quantification. Bayesian Inference provides a mechanism for estimating unknown model parameters and quantifying uncertainty in predictions [20]. Bayesian inference represents the unknown model parameters using probability (posterior) distributions and applies methods such as variational inference[21, 22] and Markov Chain Monte-Carlo (MCMC) [23, 24] sampling. Bayesian inference has been widely applied in healthcare [25, 26, 27] as they provide uncertainty projections in predictions using the posterior distribution as opposed to the single-point estimates in conventional machine learning and deep learning models. Guo et al. [28] has proposed a Bayesian Recurrent Framework model for performing imputation, leveraging the strengths of Bayesian inference to enhance the imputation process.\nSignificant advancements have been made in Bayesian neural networks and Bayesian deep learning using variational inference [29], as they integrate more seamlessly with gradient-based optimization methods (such as backpropagation) compared to MCMC sampling that faces challenges with large models. Prominentimplementation of variational inference includes Bayes-by-backdrop [29] which introduces a backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network and variational autoencoders [30]. Bayes-by-backdrop introduces a lower bound estimator for efficient posterior inference in the presence of continuous latent variables with intractable posterior distributions. Kapoor et al. [31] applied Bayesian variational inference-based RNNS for cyclone track and intensity prediction, demonstrating their ability to model spatiotemporal data. However, variational inference provides an approximate inference by treating the marginalization required for Bayesian inference as an optimisation problem [21, 32], rather than directly sampling from the posterior distribution as done by Markov Chain Monte Carlo (MCMC) sampling [24] which has challenges when it comes to large number of model parameters and hence the progress in deep learning has been slow. Despite the importance of uncertainty quantification in model predictions, there has been limited application of Bayesian deep learning in data imputation. Prior work in the domain of data imputation has been focussed on traditional deep learning models including RNN implementations such as CATSI and BRITS [17, 18, 9], and Transformer based models such as SAITS (Self-Attention Based Imputation for Time Series) [33]. The advancements in variational inference and deep learning methods over the past decade inspire their application for uncertainty quantification in medicine, particularly in improving the accuracy and reliability of data imputation.\nIn this paper, we enhance the performance of the CATSI model by incorporating uncertainty quantification using variational inference. We integrate customised Bayesian deep learning layers into the existing CATSI model architecture, primarily replacing all deterministic deep learning layers with Bayesian deep learning layers using Bayes-by-backprop [29]. Our novel model, referred to as 'Bayes-CATSI,' requires computationally intensive execution, necessitating a limit on the number of samples used. To address this limitation, we propose 'partial-Bayes-CATSI,' which replaces only a subset of deterministic deep learning layers with Bayesian deep learning layers. This variant allows us to test the impact of incorporating a limited number of Bayesian layers while using more data samples for execution. By comparing the performance of both models Bayes-CATSI and partial-Bayes-CATSI against the original CATSI model, we demonstrate the immediate improvement in results achieved by incorporating Bayesian layers. The performance of these models has been evaluated using medical data from the Computing in Cardiology Challenge 2018 [34]. Our results highlight the benefits of uncertainty quantification and the trade-offs between computational complexity and model performance for the imputation problem.\nThe rest of the paper is organized as follows. Section 2 provides a background on related methods and Section 3 presents the data pre-processing and the proposed method. Section 4 presents experiments and the results, Section 5 discusses the results and Section 6 concludes with future research directions."}, {"title": "2.1. Time Series Data Imputation", "content": "A substantial body of literature exists on time series imputation [35], with most studies leveraging longitudinal observations [36] and cross-feature correlations [37] to improve the imputation process with impositions of various assumptions. As mentioned, autoregressive models, such as ARMA and ARIMA, are among the simplest models used for time series analysis. These models assume the input data is stationary. Although differencing [38] can transform non-stationary data into a stationary form, this process often results in the loss of complex temporal dynamics, which can lead to inevitable errors in the model's predictions. In Bayesian data analysis, Gaussian Processes models [39, 40] excel at handling uncertainties in observed data. However, they are very sensitive to the"}, {"title": "2.2. Bayesian inference with variational inference", "content": "Bayesian inference using MCMC methods faces challenges like convergence issues and inefficiency in high-dimensional spaces, making them unsuitable for big data and large models. Strategies combining MCMC with gradient-based methods [50, 51, 52, 53, 54] and meta-heuristic approaches [55, 56, 57, 58] have been developed. Structural changes such as nested sampling [59] and parallel tempering MCMC [60, 61] improve efficiency. Recent advances with parallel tempering MCMC and Langevin gradients have shown promise for Graph CNNs [62] and deep autoencoders [63]; however, the challenge remains as the data and model size increase.\nVariational inference offers a tractable alternative, optimising variational densities to approximate the posterior distribution and minimise Kullback-Leibler (KL) divergence. Early variational inference approaches for Bayesian neural networks (BNNs) used mean field variational Bayes (MFVB) [64, 65]. Variational inference gained popularity with deep learning for robust uncertainty quantification. Graves et al.[66] introduced computation of derivatives of expectations in the variational objective function also known as evidence lower bound (ELBO). Blundell et al. [29] simplified the implementation of variational inference for neural networks using Bayes-by-backdrop. As the sampling operation is not deterministic (non-differentiable), in order to perform gradient optimization over the variational loss the re-parameterization strategy by Kingma et al.[30] and Rezende et al. [22] allowed the representation of random variables (i.e., trainable parameters) as deterministic functions with added noise which results in model optimisation (training) using stochastic gradient descent (SGD). Consequently, Bayes-by-backprop leverages this trick to estimate the derivatives of expectations, facilitating the training of Bayesian neural networks to incorporate weight uncertainty crucial in balancing the trade-off between exploration and exploitation in reinforcement learning scenarios. Furthermore, Blundell et al. [29] provided empirical evidence showing that Bayes-by-backprop not only simplifies the training process but also delivers superior performance compared to traditional regularisation techniques like dropout. Bayes-by-backprop achieves this through a straightforward formulation of the loss function, known as the variational free energy or evidence lower bound. This formulation captures the uncertainty in model weights more effectively, leading to better generalisation and robustness in neural network predictions.\nWe did not find any study that considered merging Bayesian variational inference into a model such as CATSI which incorporates the global dynamics along with the local dynamics of a clinical time series using deep learning."}, {"title": "3. Methodology", "content": "channel of prior, which influences the parameters of the underlying data-generating process. In clinical time series, encoding patients' rapidly changing health states as a prior distribution is particularly challenging. Additionally, Gaussian Process[41] typically assume that data points closer in time have more similar values due to an implicit locality constraint, which can be limiting when addressing the complex and global dependencies common in clinical time series data. Matrix factorisation [15] and their higher-order extension, along with tensor factorisation [42] are commonly used techniques for analysing and imputing time series data. These methods rely on the assumption that the observed data are generated by a linear combination of low-dimensional latent factors, suggesting low rankness. However, this assumption often proves inadequate for clinical time series, which typically exhibit complex and intricate temporal dynamics due to the multifaceted nature of patient health states and the influence of various physiological processes over time. Consequently, more sophisticated approaches are often required to accurately capture and model these complexities. Multiple Imputation by Chained Equations (MICE) [43] is a widely used method for time series imputation. Unlike single imputation, which generates only one estimate for missing data, MICE creates multiple imputations to account for the inherent uncertainty in the data, resulting in more robust and reliable estimates. This approach iteratively fills in missing values by modelling each variable as a function of the others, capturing the complex relationships among variables. However, MICE operates under the assumption that data are missing completely at random [44]. This assumption can be problematic when applied to clinical time series, where the pattern of missing data is often influenced by patients' health states, medical interventions, and other contextual factors. In clinical settings, data might be missing not at random [45] and missing at random[46] when certain tests are only conducted if specific symptoms are present [36].\nRecurrent Neural Networks (RNNs) have recently gained prominence for modelling sequential data [47], including their application in time series imputation [48]. RNNs utilise hidden states to encapsulate past observations and predict future time steps based on current inputs, making them adept at handling temporal dependencies. RNNs are flexible models and hence the architecture and training algorithm can be altered and extended depending on the application. Che et al.[18] introduced specific RNN architectures tailored to exploit patterns in missing data related to underlying labels in clinical time series. Furthermore, Cao et al. [17] introduced a bidirectional LSTM model that not only considers historical data but also future trends within the time series. This approach enhances the accuracy of imputation by incorporating both past context and future trends, thereby providing a comprehensive view of temporal data dynamics. Although RNNs excel in capturing temporal relationships without imposing strict assumptions on data generation, their optimisation complexity (suitable training algorithm and model architecture) can limit their ability to learn global patterns effectively. Consequently, RNNs often excel in capturing local dependencies within sequential data but may struggle with broader, global dynamics [49].\nAddressing the challenges observed in RNN-based methods,"}, {"title": "3.1. Dataset", "content": "We utilise data from the publicly available dataset collected in real-world intensive care units (ICUs) for the Computing in Cardiology challenge 2018 [34]. The data were collected by Massachusetts General Hospital's (MGH) Computational Clinical Neurophysiology Laboratory (CCNL), and the Clinical Data Animation Laboratory (CDAC). The subjects (patients) had a variety of physiological signals recorded as they slept through the night including electroencephalography (EEG), electrooculography (EOG), electromyography (EMG) and electrocardiology (EKG). Mainly we have 12 analytes which correspond to each of the physiological signals mentioned: EEG ('F3-M2', 'F4-M1', 'C3-M2', 'C4-M1', '01-M2', 'O2-M1'), EOG('E1-M2'), EMG('Chin1-Chin2', 'ABD', 'CHEST', 'AIRFLOW'), ECG('ECG') with frequency of 200Hz. In this study, we utilised data from 20 randomly selected patients for the training set and 10 patients for the testing set. Since the ground truth of the original missing values was unknown, additional missing data was introduced by randomly masking observations, furthermore due to computational constraints, we had to limit our study to 2000 samples for the partial-Bayes-CATSI model and 500 samples for the Bayes-CATSI model."}, {"title": "3.2. Variational inference for training neural networks", "content": "As mentioned earlier, Bayesian inference can be implemented with either MCMC sampling [24] or variational inference [29]. The use of Bayesian inference to estimate (train) neural network parameters (weights and biases) enables them to be viewed as probabilistic models [29]. Essentially, Bayesian inference enables fixed point estimates of model parameters to be represented as probability distributions, also known as the posterior probability distribution which requires a prior probability distribution and an inference algorithm such as MCMC or variational inference.\nA Bayesian neural network model can be viewed as a probabilistic model $P(y|x, \\theta)$ where we have an input $x \\in x$ in $x$ feature space and output $y \\in y$ in $y$ label space. $\\theta$ represents the neural network model parameters. In the case of time series prediction and regression problems, we usually assume that y follows a Gaussian distribution. The neural network objective function is given by f(x, 0) with parameters $\\theta$ for the pairing (x, y) during the training process.\nWe can learn (train) the model parameters (weights and biases) given a set of training examples D = (xi, yi); by using the Maximum Likelihood Estimate (MLE) of the parameters $\\theta$ given as Equation 2.\n$\\theta = arg \\max_{\\theta} log P(D | \\theta)$ (1)\n$\\theta = arg \\max_{\\theta} \\Sigma_i log P(y_i | x_i, \\theta)$ (2)\nTypically, training of the neural network model is done using the backpropagation algorithm, which employs gradient-based optimisation. The model output $P(y, x)$ has to be differentiable"}, {"title": "3.3. Framework", "content": "This section presents the Bayes-CATSI and partial Bayes-CATSI framework that features variational inference and deep learning models."}, {"title": "3.3.1. Pre-processing: masking, normalisation and temporal decay", "content": "We follow data input and masking procedures by Yin et al.[9] and Cao et al.[17] where artificial missing values were introduced into the input data, which already contains pre-existing missing values. The goal of introducing the artificial missing values is to generate a test dataset for evaluating the model. Figure 1 provides an overview of the framework for the imputation process, where we use the incomplete data, along with the observation mask and evaluation mask for the imputation model.\nWe represent the input data corresponding to the multivariate time series for a patient as $X \\in R^{T \\times F}$, where T is the number of time steps and F is the number of features (or variables) measured at each time step and the t-th row $x_t$ is the observation at the t-th time step. $s_t$ denotes the timestamp corresponding to the t-th time step. As demonstrated in the existing work [9, 17], we use a masking matrix or the observation matrix M with the same size of multivariate time series X to indicate the missing data in the time series as shown in Equation 12 for example, the masks would have 500 rows and 12 columns for Bayes-CATSI and 2000 rows and 12 columns for partial Bayes-CATSI.\n$m_t^f = \\begin{cases} 1 & \\text{if the f-th variable is observed at time } s_t \\\\ 0 & \\text{otherwise} \\end{cases}$ (12)\nWe need to account for the irregularity of the time series caused by the missing values. Yin et al. [9] introduced an observation gap matrix A with the same size as that of X to represent the current time stamp and the time stamp of the last observation that is not missing. We show the calculation of the observation matrix A in Equation (13).\n$s_t^f = \\begin{cases} \\frac{s_t - s_{t-1}}{\\delta_{t-1}^f} + 1 & \\text{if } t > 1, m_{t-1}^f = 0 \\\\ s_t - s_{t-1} & \\text{if } t > 1, m_{t-1}^f = 1 \\\\ 0 & \\text{if } t = 1 \\end{cases}$ (13)\nThe raw input data for each variable of each patient is first normalised using min-max normalisation, as defined by Equation 14.\n$x_t^f = \\frac{x_t^f - min(x^f)}{max(x^f) - min(x^f)}$ (14)\nWe transform the data into a pre-completed version X to effectively utilise the raw input data containing missing values in the model. As described by Yin et al. [9] and originally introduced by Che et al. [18], this transformation is achieved by imputing the missing entries using a trainable temporal decay factor as depicted in Equation s(15) and (16). As explained by Yin et al. [9], the strategy stems from an observation that if a variable is unobserved for a long time, it gravitates towards a"}, {"title": "3.3.2. Bayes-CATSI and Partial Bayes-CATSI", "content": "``default\" value (the empirical mean in this case); otherwise, it stays near its historical observations. We use the observation $\\delta_t$ gap at time $s_t$ to compute the temporal decay factor $y_t \\in R^F$ to compute the pre-completion in Equation 15.\n$y_t = exp(-max(0, W_y \\delta_t + b_y))$ (15)\n$x_t' = y_t x_{t-1} + (1 - y_t) \\overline{x}$ (16)\nwhere $x_t'$ is the computed pre-completion, $x_{t-1}$ is the last non-missing observation and $\\overline{x}$ is the empirical mean for the f-th variable at time $s_t$. We use Equation (16) to replace the missing values in the raw input while preserving the observed ones. Followed by this, we generate the pre-completed input as shown in Equation (17), as depicted in Figure 2 we feed this pre-completed input into Bayes-CATSI and Partial Bayes-CATSI.\n$x_t'' = m_t x_t + (1-m_t)x_t'$ (17)\nWe update the temporal decay module's parameters $W_y \\in R^{F \\times F}$ and $b_y \\in R^F$ during training. Figure 2 demonstrates the pre-processing workflow, where the temporal decay module converts the raw input into pre-completed input by taking the convex combination of the last observation and the mean with the decay factor as the coefficient, this is done for Bayes-CATSI and Bayes-CATSIpartial models.\nThe imputation model by Yin et al. [9] shown in Figure 2 features three key components including context-aware recurrent imputation, cross-feature imputation, and a fusion layer that integrates the results from the preceding layers to generate the final imputed dataset. Our Bayes-CATSI and partial Bayes-CATSI models include modifications to the internal layers of these key components, with further details available in the subsequent sections."}, {"title": "3.3.3. Context-Aware Recurrent Imputation", "content": "Figures 3 and 4 provide a detailed diagram of the architectures of Bayes-CATSI and Partial Bayes-CATSI where the \"Context Aware Recurrent Imputation\" component includes a \"context vector\" represented as $r \\in R^C$ in its imputation process. We optimise (train) this context vector r to extract the global temporal dynamics to represent the corresponding sample data's global characteristics for e.g. a patient's health state where C is the dimension of the context vector which is a pre-specified hyperparameter.\nYin et al. [9] utilised a bidirectional RNN model for imputation by feeding the time series into the forward RNN in original order and into the backward RNN in reverse. The essence of using a bidirectional RNN is that at time step t the hidden states before and after st are computed using forward and backward RNN. This is followed by the recurrent imputation which combines the two hidden states by applying a linear transformation as shown in Equation (18)\n$\\hat{x_t} = W_x[\\overrightarrow{h_{t-1}}; \\overleftarrow{h_{T-t}}] + b_x$ (18)\nwhere h is the hidden state, and [;] indicates concatenation of two hidden states. $\\overrightarrow{h}$ with a rightward arrow atop denotes the parameters of the forward RNN while $\\overleftarrow{h}$ with a leftward arrow atop denotes the parameters of the backward RNN.\nThe modification that we make to the original CATSI model is that both the Bayes-CATSI and partial Bayes-CATSI models (Figures 3 and 4) utilise a Bayesian linear layer to combine the forward and backward hidden states, making the weights Wx and biases bx in Equation 18 probabilistic, rather than deterministic (as in original CATSI).\nYin et al. [9] in CATSI employed the LSTM model to extract hidden states h from the input data and the context vector. The input data alone would neglect the patient's health states, hence CATSI used a context vector as described earlier which helps incorporate patients' health states or global temporal dynamics of the corresponding patient. At each time step, the hidden states are produced with reference to the global dynamics of the input time series by feeding the context vector to the LSTM model together with the time series input. In Bayes-CATSI, we use a probabilistic (Bayesian) LSTM layer while in partial Bayes-CATSI, we use a conventional LSTM model with deterministic weights due to the computationally intensive nature of the Bayesian LSTM layer.\n$h_0 = W_h r + b_h, \\overrightarrow{c_0} = tanh(h_0)$ (19)\n$\\overleftarrow{h_0} = W_h r + b_h, \\overleftarrow{c_0} = tanh(h_0)$ (20)\nThe weights $W_h$ and biases $b_h$ in Equation 19 and $W_h$ and biases $\\overleftarrow{b_h}$ in Equation 20 are probabilistic in case of the Bayes-CATSI model, while they are deterministic in case of the partial Bayes-CATSI model. The context vector r is used to initialise the hidden state h0 and cell state c0 of the subsequent LSTM model, this is beneficial for imputing missing values at the first several time steps according to the original CATSI model [9]. We use the Bayesian LSTM model as mentioned earlier in Bayes-CATSI while the LSTM model is used in partial Bayes-CATSI.\n$\\overrightarrow{h_{t-1}}, \\overrightarrow{c_{t-1}} = LSTM([x_{t-1}; r], \\overrightarrow{h_{t-2}}, \\overrightarrow{c_{t-2}})$ (21)\n$\\overleftarrow{h_{T-t}}, \\overleftarrow{c_{T-t}} = LSTM([x_{t+1};r], \\overleftarrow{h_{T-t-1}}, \\overleftarrow{c_{T-t-1}})$ (22)\nEquations 21 and 22 correspond to the partial Bayes-CATSI model and Equations 23 24 are used for Bayes-CATSI.\n$\\overrightarrow{h_{t-1}}, \\overrightarrow{c_{t-1}} = Bayesian-LSTM([x_{t-1}; r], \\overrightarrow{h_{t-2}}, \\overrightarrow{c_{t-2}})$ (23)\n$\\overleftarrow{h_{T-t}}, \\overleftarrow{c_{T-t}} = Bayesian-LSTM([x_{t+1};r], \\overleftarrow{h_{T-t-1}}, \\overleftarrow{c_{T-t-1}})$ (24)\nYin et al.[9] in CATSI proposed two approaches to train the context vector that included basic statistics and RNN-based model.The first approach captures the basic statistics using an MLP layer while the second approach uses an RNN based model to capture the complex temporal dynamics.\nIn summarising the basic statistics, CATSI captures the overall health state of patients by calculation of the basic descriptive"}, {"title": "3.3.4. Cross-Feature Imputation", "content": "statistics like the mean $\\overline{x}$, standard deviation $\\sigma$, missing rate p and length of the time series $T_p$, for the given patient.\n$r_{MLP} = f(\\overline{x}, \\sigma, p, T_p)$ (25)\nA multilayer perception (MLP) is then used to approximate the function f to summarise the patient's overall health state. As shown in Figure 3, we use two Bayesian linear layers separated by a ReLU activation layer in the MLP-based context model in Bayes-CATSI for summarising the basic statistics. As described in Figure 4 we use only one Bayesian linear layer in the partial Bayes-CATSI model to reduce the computational complexity. The MLP model however summarises only the basic characteristics of a patient's health data and may not be able to capture the complex temporal dynamics.\nWe use the RNN-based modelto capture complex temporal dynamics in CATSI where the hidden states encode the temporal dynamics of the data. We use the Gated Recurrent Unit (GRU) [67] model, which is a simple implementation of the LSTM in partial Bayes-CATSI and Bayes-CATSI.\nThe output from the MLP-based context model and the GRU-based context model is concatenated to output the context vector r as shown in Equation 26.\n$r = [r_{MLP}; r_{GRU}]$ (26)\nThe context-aware recurrent imputation described in the previous section focuses on the temporal dynamics of the corresponding analytes/features. It does not focus on the dynamic correlations that exist between different features/analytes. The cross-feature imputation enables feature correlation, thus the value of one variable can be estimated based on other variables, simultaneously. Equation 27 computes a linear transformation of the raw input with the diagonal parameters of $W_v$ set as zeroes to avoid x from predicting itself.\n$v = W_v x + b$ (27)\n$\\hat{x_t^f} = BayesianMLP(v)$ (28)\nwhere $\\hat{x_t^f}$ is the cross-feature imputation of the f-th feature at t-th time step. The linear (pre-processing) layer in Figures 3 and 4 implements Equation 27. Following this, a probabilistic (Bayesian) MLP is used to generate the cross-feature imputations, that explore the complex feature correlations. One Bayesian layer is used for the partial Bayes-CATSI model while both linear layers are replaced by their Bayesian counterparts in the Bayes-CATSI model."}, {"title": "3.3.5. Fusion Layer", "content": "After obtaining the results from the recurrent and cross-feature imputation, both results are fused using the $\\beta_t \\in R^F$ which is calculated using the missing patterns in the data using missing masks $m_t$ and observation time gaps $y_t$, as shown in Equation 29.\n$\\beta_t = sigmoid(W_{\\beta} [y_t; m_t] + b_{\\beta})$ (29)\nEquation 30 presents the calculation of the final output\n$Y_t = \\beta_t \\odot \\hat{x_t} + (1 - \\beta_t) \\odot \\hat{x_t^f}$ (30)\nwhere $\\odot$ indicates an element-wise matrix multiplication, $\\hat{x_t^f}$ indicates the cross-feature imputation and $\\hat{x_t}$ indicates the recurrent imputation and $y_t$ is the final imputation."}, {"title": "3.3.6. Bayes-CATSI: variational inference for imputation", "content": "We next present the details of implementing variational inferences for training Bayes-CATSI. Algorithm 1 presents the training of the Bayes-CATSI and partial Bayes-CATSI models using variational inference. First, we define the Bayesian neural network $y = f(x, \\theta)$, as shown in Figure 3 for Bayes-CATSI and Figure 4 for partial Bayes-CATSI. We then initialise the variational inference parameters $\\delta = (\\mu, \\rho)$ and set the hyperparameters, which include the number of epochs $N_{epoch}$, the number of samples for Monte Carlo sampling $N_{sample}$ (similar to training epochs in backpropagation algorithm), and the standard deviation of the scaled mixed prior $\\tau = (\\sigma_1, \\sigma_2)$. In this case, $\\sigma_1$ and $\\sigma_2$ correspond to the standard deviations of the two Gaussian densities considered for the scaled mixed prior, as described by Blundell et al.[29].\nNext, we perform masking on the output matrices (Figure 1), retaining only the values that need to be imputed [9]. This is done using the observation mask, which contains details of all the values that are originally missing (described as 'ground-truth' in Table 1), along with the artificially added missing values. Additionally, the evaluation mask contains details of only the artificially added missing values and is used to evaluate the trained model.\nThis masking procedure is followed by the preprocessing of the data into input-output matrices (Figure 1), which is then followed by training the model. During training, we add sample noise from a standard normal distribution and compute the model parameters corresponding to each particular sample in the epoch. The standard deviation parameter $\\sigma = log(1 + exp(\\rho))$ uses a softplus function $f(x) = log(1 + exp(\\rho))$, which ensures that the value of $\\sigma$ is always positive. After extracting the model parameters for all $N_{sample}$, we calculate the variational loss, which is further used in the backpropagation process to update the $\\mu$ and $\\rho$ values."}, {"title": "3.3.7. Loss function", "content": "We compute the simplified form of the loss function as described in Equation (11) using the likelihood cost and the complexity cost. The complexity cost is calculated using the log of prior and the variational density. The variational density is calculated by summing log probability over the Gaussian distributions parameterised by the mean and variance values given in the weight matrix. While the previous sections provide details of the loss function as a whole, this section elaborates on the likelihood cost used in the paper. We use the mean squared"}, {"title": "3.4. Accuracy metrics", "content": "deviation (MSD) of the observed entries as the likelihood cost.\nYin et al.[9", "as": "n$L(Y) = \\frac{||M \\odot (X - Y)||^2"}, {"9": "we accumulate the loss for the recurrent imputations X", "36": "n$l = L(Y) + L(X) + L(Z)$ (35)\n$= \\frac{||M \\odot (Y - X)|||^2}{||"}]}