{"title": "Permutative redundancy and uncertainty of the objective in deep learning", "authors": ["Vacslav Glukhov"], "abstract": "Implications of uncertain objective functions and permutative symmetry\nof traditional deep learning architectures are discussed. It is shown that tra-\nditional architectures are polluted by an astronomical number of equivalent\nglobal and local optima. Uncertainty of the objective makes local optima\nunattainable, and, as the size of the network grows, the global optimization\nlandscape likely becomes a tangled web of valleys and ridges. Some remedies\nwhich reduce or eliminate ghost optima are discussed including forced pre-\npruning, re-ordering, ortho-polynomial activations, and modular bio-inspired\narchitectures.", "sections": [{"title": "Introduction", "content": "The success of practical applications of deep networks trained with stochastic gradi-\nent descent and its variants is undisputed. Despite occasional progress, theoretical\nresearch on the foundations of deep networks is lagging behind their practical suc-\ncesses. Deep learning theories often rely on assumptions that are far from realistic,\ne.g., availability of infinite training data and access to statistical expectations, reg-\nularity - determinism, continuity, smoothness \u2013 of objective functions, and infinite\nproduction run times. Aleatoric uncertainty is often treated as a mere inconvenience\nwhich can be averaged away. Ignorance of epistemic uncertainty is common.\nThis is unfortunate. Better theoretical foundations help guide practitioners to\nsystematically reason about their models and consciously architect cognitive ma-\nchines rather than stumble upon solutions through trial and error.\nIt is evident that with sheer computing power, a deep enough and wide enough\napproximator can model almost any complex real-world phenomenon. The latest\nSoTAs often appear as competitions in tackling complex problems with brutal force\nrather than ingenuity and creativity. In all this excitement, have we forgotten that\nthe point is not to compute more, but to create better - efficient, robust, safe, and\ntransparent - solutions for real-world applications?\nLet us talk about efficiency. The brain is a super-efficient cognitive machine. It\nconsumes only about 20 watts of energy, give or take a few watts. It does not pass a\ncontinuous flow of information through its internal pathways when these pathways\nare not needed. It does not learn by error backpropagation, nor does it adjust its\nconnections using gradients. The brain does not optimize any global objectives in\nthe sense we use the term optimization in deep learning. Nor is it a feed-forward\nmachine.\nDeep learning [LBH15], for which feedforward monolithic architectures, learning\nby gradient optimization, and backpropagation are key, is a breakthrough in tech-\nnology that opened many doors. At the same time, the regrettable side effect of this\nsuccess is that it diverts talent and technical resources on a massive scale away\nfrom other research and development directions in computational cognition.\nWe are reaching the point of exponentially diminishing returns when using brute\ncomputing of deep networks to solve complex real-world problems.\nI would like to draw attention to these architectures' fundamental limitations.\nHere, based on well-known but almost neglected theoretical and experimental re-\nsults, I discuss the implications of gradient optimization of uncertain objectives and\npermutative redundancies of deep networks.\nI also discuss some remedies."}, {"title": "Uncertain objectives in an uncertain world", "content": "Gradient methods are commonly used to optimize deep networks. Architectures\nare chosen to efficiently evaluate the objective and its gradients in the parameter\nspace via a forward pass of data inputs through the depth of the network. For some\nproblems, the network's objective is to approximate dependent variables in the data\nor their distribution, given the set of independent or state variables. For other\nproblems, e.g. in the reinforcement learning settings, the network approximates a\nstrategy that links the state variables with actions, and the objective minimizes\ncumulative loss (or maximizes cumulative reward) incurred during the execution of\nsaid strategy. In almost all cases, excluding perhaps clearly degenerate ones, the\nvalues of the objective and its gradients are uncertain.\nTo handle aleatoric or statistical uncertainty, the known unknowns, data-centric\nmethods rely on the availability of massive amounts of data to handle: machine\nlearning as a branch of applied statistics works only when there is plenty of data.\nNot every situation provides enough data to alleviate statistical uncertainties. More-\nover, data are tenuous and statistical uncertainty stubbornly persists in the most\ninteresting situations and problems.\nAn abundance of data as a requirement places natural limits on all data-centric\nmethods.\nConsider a real-world agent trained to produce an optimal strategy with a vari-\nant of reinforcement learning. The agent must adapt to uncertain and changing\nenvironmental conditions. Three issues arise here.\nFirst When the rate of change exceeds the rate with which the agent updates\nits knowledge, learning an optimal strategy might be too expensive, and the\nagent's best attainable strategy inevitably remains suboptimal.\nSecond When the agent learns about changing conditions only or mostly by exam-\nining the statistical properties of its rewards, it must always continue explo-\nration to avoid being crushed by the evolving environment. This, again, leads\nto the local suboptimality of the agent's performance."}, {"title": "Representation of uncertain objective functions", "content": "Given the data sets representing the inputs xi \u2208 X and outputs yi \u2208 Y the network\napproximates Y = G(0, X). Optimization is looking for an extremum (a minimum\nfor definitiveness) of a loss function:\n$$0* = argmin L(Y, G(0, X))$$\n\u03b8\n(1)\nFor a regression or classification problem, the loss function may look like\n$$L(Y, G(0, X)) = ||Y \u2013 G(0, X)||$$\n(2)\nwhere the norm ||\u00b7 || is suitably chosen for each particular case. A quadratic norm\nis almost always a default."}, {"title": "Note on the structure of the optimization objective", "content": "The choice of the norm in Eq 2, although routine, is by no means self-evident or\noften even compelling. Remember that Lis assumed to represent an approximation\nof the real-world losses associated with imperfect modelling and risk. In practice,\nlosses often are much less convenient than a quadratic norm is capable of represent-\ning: they have a non-trivial structure, are often asymmetric, statistical estimates in\nEq 2 can misbehave and have no well-defined or reasonably attainable limit when\nnormalized by the number of data points. Researchers adjacent to machine learning\nareas of applied statistics have long realized that a realistic structure of losses due to\nan imperfect model and uncertainty is crucially important for mission-critical appli-\ncations (see for example [Vio17] and [NV24] and references therein). I will discuss\nthese issues elsewhere in more detail."}, {"title": "Quadractic structure", "content": "For our purposes, it makes sense to simplify the notation and adopt the objective\nfunction of data and parameters:\n$$0*(D) = argmin J(0, D)$$\n\u03b8\nwhere D denotes data. The notation is general enough to describe a sample of I,\nas well as its statistical averages. The explicit dependency of the optimal set of\nparameters 0*(D) on data D is important.\nThe idealized representation of the \"true\" objective function of parameters often\nused in studying the mathematical foundations of deep learning is\n$$J(0, D) = (0 \u2013 0*(D))'H(D)(0 \u2013 0*(D))$$\n(3)\nwhere H is the Hessian matrix. The assumption of the existence of a regular J(0)\nthat does not depend on data in the limit of an infinite data set is often made in\nthe literature.\n$$T(0) = lim J(0, D)$$\n||D||\u2192\u221e\nIt is a strong assumption: the rest immediately falls in place once you accept a\nsmooth, twice differentiable structure of J(0). It is also clearly a flawed assumption:\ndata are always limited.\nThere are many ways to represent uncertainty in the objective function. For\nexample, in [GKX19] the following representation is adopted:\n$$J(0, D) = (0 \u2013 0*(D) + \u0454)'H(D)(0 \u2013 0*(D) + \u0454)$$\n(4)\nThis form is also compatible with that used in [YYVS16] where the authors studied\nthe effects of sampling noise. An implicit assumption here is that the magnitude of\nuncertainty is a function of the Hessian.\nI am interested in making as few implicit assumptions as possible. To better\ncapture the real-world properties of the objective function I adopt the following\nrepresentation which differs slightly from Eq 4:\n$$J(0, D) = (0 \u2013 0*(D))'H(D)(0 \u2013 0*(D)) + \u00ea(\u03b8, D)$$\n(5)"}, {"title": "Gradient convergence near a presumed local\nquadratic optimum under an uncertain objec-tive", "content": "Sampling noise is an obvious source of uncertainty of the objective. Convergence of\ngradient methods for non-deterministic objective functions with sampling noise has\nbeen studied in [YYVS16] and references therein. Another general study of stochas-\ntic gradient convergence with noisy gradients is in [DDB18], see their references as\nwell.\nFor this work, the main result of the above studies is that contrary to the case of\na deterministic objective function, where gradient methods converge exponentially\nfast, convergence toward a noisy objective does not necessarily hit an optimum.\nInstead, the gradient iterates eventually reach a stationary vicinity in the parame-\nter space around the optimum. The shape and the size of the equilibrium vicinity\ndepend on the learning rate, the extent of the noise, and the structure of the de-\nterministic part of the objective function near the optimum. Authors of [YYVS16]\nand [DDB18] approach the problem differently but arrive at similar conclusions.\nPractitioners mostly ignore this distinction between deterministic (on average)\nand inherently stochastic systems, and assume that some clever reduction of the\nlearning rate, the key hyper-parameter in all variants of gradient methods, should\neventually drag the gradient iterates toward the optimum. What we know is that\nconvergence to an optimum is slow, and, to my knowledge, there is neither general\nformal proof, nor guarantee of convergence. In the case of a purely quadratic ob-\njective function in the vicinity of the optimum, the only result that is reasonably\nwell proven is that the expectation of the gradient iterates does approach the true\noptimum but only eventually, in the long run. Expectations, however, are not\npractical because training always deals with a limited data set and production runs\nrequire definite values of the model parameters. Even if we resort to using aver-\nages in place of expectations, it is unclear how to practically create a deep network\nconfiguration corresponding to an average set of parameters. We don't know and\ncannot presume that a model with \"average\" values of parameters will even work:\n$$T(0) \u2260 I(0)$$\n(11)\nQuadratic approximation might be a convenient theoretical tool, but, as we will see\nshortly, the true shape of the objective function in the vicinity of an optimum is far\nfrom a smooth quadratic form in Eqs 3, 4 or 5.\nFollowing [GKX19] and [SBL17] and [DDB18], I derive the expression for the vari-\nance of the model parameters in the case of an uncertain objective function J(0, D)\nwhere 0 are the parameters, and D is the input data.\nIn practice, we use a portion of the training data set to estimate the gradient\niterates. Let B be a batch of data. Without loss of generality in the local vicinity\nof the optimal 0*, as in Eq 5:\n$$J.(0, \u0392) ~ (0 \u2013 0*(Dr))' H(0 \u2013 0*(Dr)) + \u00ea(\u03b8, \u0392),$$\n2\nwhere 0*(DT) is a hypothetical optimal value of parameters given the training data\nset and \u20ac(0, B) is the uncertainty, or noise, of the objective function. As I already\ndiscussed, it is not unreasonable to assume that, evaluated on the same batch B,\nthe actual sample in the close vicinity of \u03b8 is close to the sample at 0\n$$\u00ea(\u03b8 + \u03b4\u03b8, \u0392) = \u00ea(\u03b8, \u0392) + \u03b5(\u03b8, \u0392) \u00b7 \u03b4\u03b8$$\n(12)\nAccess to the uncertainty of the objective function's gradient is almost always\navailable in practical deep learning applications. Gradient iterates are:\n$$0t+1 = \u03b8\u03c4 - \u03b7\u00b7 \u0397 \u00b7 (0\u2081 \u2212 0*) + \u03b7\u00b7 \u03b5(\u03b8, \u0392+)$$\n(13)"}, {"title": "Boltzmann's equilibrium around the presumed optimum", "content": "The dynamics of gradient iterates in the presence of stochasticity is essentially a\ndiscrete version of the stochastic dynamics described by the overdamped Langevin\nequation, or, in a more general case, by a Fokker-Plank equation:\n$$\u2202P(\u03b8,t)/\u2202t =  {\\frac{\\eta^2 \\cdot ||e||^2}{2} \\cdot \u2207^2 -\u03b7 \\cdot \u2207 \\cdot J(0)}P(\u03b8,t)$$\n(19)\nIn Eq 19 both differential operators act on everything to their right following the\nstandard chain rule.\nIt is a well known fact that under the assumption of a finite variance of the\nnoise term, the equilibrium distribution of the continuous version of the overdamped\nLangevin equation reproduces a Boltzmann distribution (see the excellent treatment\nof the topic in [SK00]).\n$$P(0,t \u2192 \u221e) ~ exp(-\u03b2\u00b7J(0))$$\n(20)\nwhere the temperature governing the extent of the distribution in the parameter\nspace is\n$$\u03b2^{-1} ~ \u03b7||\u03b5||^2$$\n(21)\nTo summarize, relaxation to the equilibrium distribution occurs exponentially\nfast first, governed by the deterministic part of the gradient, and then slowly, gov-\nerned more by the stochastic term and the anisotropy of the Hessian. A clever choice\nof the learning rates (equivalently, freezing schedule) reflecting the anisotropy of the\nHessian helps improve the convergence rate in situations of low uncertainty. See, for\nexample, the description of Adam in [KB17] and the classic text [KSH12]. Similar\nmethods can indeed be applied in the situation of high uncertainty of the objective,\nbut the optimum might prove unreachable within a reasonable computing time and\nwith a reasonable amount of data."}, {"title": "Effects of the anisotropy of the Hessian", "content": "In the preceding, I reproduce a well-known but often overlooked result is that,\nfor a noisy objective function, the stochastic gradient method and its derivatives\nwith a finite learning rate converge not to the optimum set of parameters but to\nan equilibrium distribution around the optimal point. The noise variance and the\nobjective function's curvature near the optimum define the equilibrium distribution\nof the model parameters.\nThen, from Eqs 20 and 21 the ball of a radius of at least\n$$R~ \\sqrt{\u03b7||\u03b5||^2 \\over  \u03bb\u03b9}$$\n(22)\nin the parameter space surrounds the optimum 0*. The following property is going\nto be used shortly: the volume of the parameter space occupied by the gradient\niterates is\n$$V*~ {\u03c0^{n/2}\\over \u0393(n/2 + 1)}  (\u03b7||\u03b5||^2)^{n/2} \u03a0\u03bb\u03b9^{-1/2}$$\n(23)\nLastly, the magnitude of the stochastic term in the equation for the gradient\niterates increments 15 depends on the number of effective batches participating in\nthe averaging along the trajectory of the gradient iterates. The effective number of\nbatches in the direction of i is defined by\n$$\u03c4i ~ \\frac{1}{log(1 \u2013 \u03b7\u03bb\u03af)}~ \\frac{1}{\u03b7 \u03bb\u03af}$$\nThis number is small for directions i in the parameter space corresponding to\nthe largest eigenvalues of the Hessian. The objective function uncertainty is well"}, {"title": "Implications of uncertain objectives and per-mutative redundancy of deep networks", "content": "Modern deep learning architectures rely on computational structures that consist of\nmultiple layers or blocks of relatively simple interchangeable and permutable input-\noutput elements. The ability of these structures to approximate the complexity of\nrelationships found in the real world is the key aspect of the deep learning paradigm.\nRedundancies of deep learning architectures of various types have been docu-\nmented in the literature\u00b3. Unfortunately, they are still poorly understood. It is\nimpossible to fill this gap in one short note, therefore here I focus on one particu-\nlar redundancy of deep networks due to the permutability of their elements in an\nattempt to draw attention to the issue."}, {"title": "Redundancy of layered structures of permutable ele-ments", "content": "The output of a simple two-layer fully connected trained network is invariant with\nrespect to a permutation of nodes within one layer, as illustrated in Fig 1 for a toy\ncase of a two-node permutation in a two-layer network."}, {"title": "Equilibrium stochastic dynamics over multiple redun-dant optimua", "content": "In section 4 I demonstrate that the convergence rate and the convergence outcome\ncritically depend on the shape of the objective in the parameter space. The Hessian\nis a representation of the objective's local structure around an optimum. To properly\nassess the implications of the existence of the enormous number of global optima in\nthe optimization space it helps to refer to experimental studies of Hessian eigenvalues\nin [SBL17] and [GKX19].\nAuthors of [SBL17] show for toy networks that the bulk of the Hessian spectrum\nconcentrates near zero. Authors argue that discrete positive eigenvalues indicate\ndependence on the data and the training objective, while the near-zero bulk is a\nfunction of the size of the network. Ghorbani et al in [GKX19] further develop these\nideas and extend their analysis to larger networks.\nUncertain objective function alone makes the convergence of gradient descent\nmethods problematic. The situation is even more complicated when there are many\nequivalent global optima in the neighbourhood of each global optimum."}, {"title": "Remedies", "content": ""}, {"title": "Sorensen reordering heuristic", "content": "The absence of inverse stability, that is, similarity in functionality does not imply\nsimilarity in the internal structure, of standard deep network architectures based on\nlayers of permutationally equivalent elements (see the already mentioned [EBG19])\nis a very inconvenient property.\nFor mission-critical applications, deep network stability in the course of adaptive\nre-training is often an important desideratum: not only do stakeholders demand the\nevidence of the functional continuity of a newly re-trained variant, but the stability\nand continuity of parameters are required, too, as the evidence of the model self-\nconsistency. That is, if 0*(D) is a parameterization of a deep network subject to\ndata D, F(0*(D)) is a symbolic representation of its functionality, then it is desired\nthat for small disturbances in the data set, both the deep network functionality and\nits parameterization must also be small\n$$\u2200D' : ||D' \u2013 D||D \u226a ||D||D$$,\n$$\u2203LF : ||F(0*(D')) \u2013 F(0*(D))||F < LF\u00b7 ||D' \u2013 D||D/||D||D$$\n$$\u2203Lo: ||0*(D') \u2013 0*(D)||\u04e9 < Le \u00b7 ||D' \u2013 D||D/||D||D$$\n(34)\nThe substance of || . ||D, ||\u00b7 ||F, and || . ||e is a function of an application.\nData themselves are easy || || D might mean cardinality, a set of statistical prop-\\perties of D, Kullback-Leibler divergence of respective distributions, etc.\nThe substance of || . ||F could be defined via a vector of numeric results of pre-\ndefined regression tests, which fits a typical production deployment requirement, in\nwhich case || ||F emphasizes the stability of aggregate functionality, etc.\nThe substance of || . ||e is much more stubborn. Considering the permutative in-\nvariance of deep networks and lack of \"inverse stability\", it is not obvious. Aggregate\nmetrics such as the spectral analysis suggested in [MPM21] help assess the statisti-\ncal properties of the network, but do not provide an assessment of the closeness of\nnetwork parameters or their stability from one re-training to another.\nIn the rest of this section, I explain how the combination of\n\u2022 reordering of individual layers and\n\u2022 the Frobenius normalized metrics\nprovides an approximate assessment of the closeness, in some sense, of two networks\nof the same architecture. The heuristic has been extensively tested for practical\ndeep learning settings.\nThe purpose of a reordering algorithm is to order each layer's nodes while pre-\nserving the nodes'connectivity. The goal of reordering is to impose a structure\nonto the network parameterization so that two networks of the same architecture\ntrained with the same data to the same level of optimality have the same or very\nclose internal structures.\nLet @ be a network configuration and \u03b8' is a disturbance in the coefficients of its\nmatrices. The disturbance of a matrix is small if some aggregate of the elementwise\ndifference between the disturbed matrix and the original matrix is small. I found a\nvariant of the Frobenius norm works pretty well for practical purposes:\n$$\u03a6(W, W')) = {\u03a3ij (Wij \u2013 Wij)^2\\over \u03a3\u03ad\u03b6 (Wij + Wij)^2}$$\n(35)\nIf W and W' are random matrices of the same structure, \u03a6(W,W')) \u2248 1. If\nthey are the same, \u03a6(W, W\u2032)) = 0. If W' is a randomly permuted version of W,\ntheir Frobenius norm is close to 1. The smaller the norm, the greater the correlation\nbetween the elements of the two random matrices. Empirically, the above procedure"}, {"title": "Pre-break the symmetries", "content": "The permutability of interchangeable elements is the source of deep networks' ex-\npressive power. Said interchangeability also leads to structural redundancies. Can\nwe preserve deep networks' expressivity by changing their structural elements to\nreduce or eliminate redundancies?"}, {"title": "Non-permutatiuve pre-pruning", "content": "In-training and post-training pruning have already been suggested and used as a\nregularization and efficiency-increasing method. I suggest using pre-training pruning\nwith setting some parameters of the network to zero permanently to break the\npermutative symmetry of network layers and reduce the number of equivalent global\noptima.\nPermutable connectivity between two consecutive layers k \u2212 1 and k is expressed\nvia a matrix W(k) of weights nk\u22121 \u00d7 nk (see Eq 28).\nIntroduce a binary matrix B(k) with the following property:\n$$\u2200j, j' : B.,j = bj \u2260 bj' = B.,j'$$\n(37)\nIn other words, no two columns of W(k) are the same. This can easily be achieved\nwith a high probability by populating B(k) with random Os and 1s with a rate p\nof zeros. The probability of a collision remains small for nk\u22121,nk \u226b 1 and can be\nneglected for practical purposes.\nThe network layer k which is defined by the pre-pruned matrix of parameters\n$$W(k) = W(k)\u3002B(k)$$\n(38)\nwhere AB is the Hadamard product, is non-permutative with a high probability:\neach connection to the previous layer is almost uniquely encoded by the binary\nmatrix B(k).\nRemoval of the permutativity of layers reduces the astronomical number of equiv-\nalent optima but does not guarantee their absence e.g. in some degenerate cases\nof substantially overdetermined networks.\nLastly, pre-pruning with the rate p can reduce the representation power of the\nnetwork. To avoid it, just to keep the representative power constant, it is reasonable\nto suggest increasing the number of nodes in the layer by (1-p)\u00af\u00b9 before pre-pruning."}, {"title": "Ortho-polynomial activations", "content": "The first working example of a deep network was reported by Ivakhnenko and his\ncollaborators who were working in the 1960s and 1970s in what is modern Ukraine (\n[Iva70] [Iva71] and references therein). The authors built a decision-making network\nbased on layers consisting of Kolmogorov-Gabor polynomial activation functions.\nIn [AP21] the authors revive Ivakhnenko's approach and offer an alternative to\ndeep networks based, again, on families of orthogonal polynomials. The authors find\nthe method particularly suitable for financial applications.\nAn orthogonal polynomial basis is an effective nearly universal approximator.\nThe elements of the basis are\n$$\u03a6 = {(x)}, i = 1 ...\u221e$$\nCX2\n$$< f(x), g(x) > = \\int_{X1} f(x).g(x)\u00b7 \u03a8(x)dx$$\n(39)\nwhere < .,. > denotes the inner product and \u03a8(x) and the integration interval\n(X1,X2) are specific for each orthogonal polynomial type.\nBy Parceval's theorem for a good enough function f(x)\n$$|| f(x) \u2013 \u2211 Wi\u00b7 \u03a6\u2081(x)||^2 = || \u2211 Wi\u00b7 \u03a6\u2081(x)||^2 < \u221e$$\ni=0\ni=N+1\n(40)\nwhere ||g(x)||2 =< g(x), g(x) >. Clearly, a polynomial approximator of a degree N\ncannot represent polynomials of higher degrees, but can be as representative as we\nneed for large enough N.\nBasis functions \u03c6\u2081(x) represent features of finer scales as i \u2192 \u221e. If the scale of\nfeatures of f(x) is limited from below by its nature or by a particular application\n(which is almost always the case), a finite number of basis functions is sufficient to\napproximate any reasonable function:\n$$|| f(x) \u2013 Wi\u00b7 \u03a6i(x)||^2 < En||f(x)||^2$$\n(41)\nMoreover, an analogue of the Nyquist-Shannon theorem is valid for the orthogo-\nnal polynomial approximation: under reasonable assumptions about the behaviour\nof f(x) and its derivatives, and the scale of features necessary for a particular ap-\nplication, the number of sampling points needed to determine w\u2081 in Eq 41 cannot\nbe significantly greater than O(N).\nA one-layer Kolmogorov-Gabor-Ivakhnenko polynomial perception-\"polytron\" of\nwidth N is a universal approximation for all functions f(x) for which en in Eq 41\nis small for practical purposes.\nWith the polynomial universal approximation the scale of features it captures\nis easily controlled, various regularizations are interpretable, and the relationship\nbetween the data set size necessary to train it seems also easily inferrable from its\ninternal structure.\nThe gradient iterates corresponding to an output node in Fig 2 and a quadratic\nobjective function based on the inner product in Eq 39 are:\n$$7wkJ (w,x) ~ (f(x) - \u2211 Wii(x)) \u00b7 \u03a8(x)\u00b7 \u03a6k(x)$$\n(42)"}, {"title": "Kolmogorov-Arnold Networks (KANs)", "content": "In [LWV+24] and [PP24] the authors explore the possibility of using arbitrary ac-\ntivation functions parameterized by splines at the nodes of a multilayer network.\nNetwork output permits automated differentiation and is as efficient as the deep\nnetwork consisting of linear permutable elements. It is a promising approach. I see\ntwo non-substantial drawbacks of current KAN architectures which can be easily\nremedied.\na) Splines are less efficient than regular orthogonal polynomials as the basis for\nrepresentation. Nothing prevents practitioners from replacing the spline-based\nunits with ortho-polynomial units,\nb) KAN elements, as envisioned by the authors, are still mutually permutable.\nTherefore, KANs should suffer from the guaranteed existence of multiple equiv-\nalent global and local optima which, in turn, guarantees to make the optimiza-\ntion landscape in the case of an uncertain objective a tangled network of ridges\nand valleys. This limitation does not seem critical: individual nodes, be they\nsplines or polynomial representations, can be easily uniquely constrained and\nmanaged via binary pre-pruning."}, {"title": "Bio-inspired alternatives to monolithic feedforward ar-chitectures and the backpropagation training", "content": "Undeterred by a lack of attention and scarce resources, a few enthusiasts are working\non bio-inspired alternatives. I mention just a few.\nAuthors of [SGCCB22] demonstrate a remarkable result that a bio-inspired visual\nobject recognition system is capable of representing objects in the standard datasets\nMNIST, F-MNIST, ORL, and CIFAR10 with only 200 neurons, 15 spikes per neuron\n(a measure of temporal response), and a reasonable set of spatial frequency filters.\nThe author's own research [Glu22] confirms this finding from a slightly differ-\nent perspective. Only 20-30 arbitrary multi-scale filters successfully sort standard\nEMNIST objects into non-overlapping areas in the feature space (Fig 3). Once this\nis achieved, learning a new object is very efficient and incremental, i.e. does not\nrequire re-learning from scratch. The recognition system built on these principles\nturns out to be robust to distortion and noise even when learning does not include\ndistorted and noisy samples.\nState-of-the-art deep learning architectures tackling the same set of problems\nrequire hundreds of thousands if not millions of trainable elements. The comparison\nin efficiency is stunning.\nAuthors of [GMZ+23] compare bio-inspired architectures with the standard back-\npropagation one. They find that bio-inspired architectures learn faster, can use a\nfraction of data, and outperform traditional backpropagation architectures in accu-\nracy. The so-called direct feedback alignment, one of the approaches used by the\nauthors, is evidently inspired by the existence of multiple feedback pathways in the\nbrains of higher animals. Feedback mechanisms play a crucial role in the processing\nof visual information in mammals: the non-retinal inputs (i.e. coming not from the\neye's retina, but elsewhere in the brain, including the downstream visual apparatus)\nprovide an overwhelming majority of inputs to the lateral geniculate nucleus (LGN)\nresponsible for the early stages of processing of visual stimuli (see [BCP16], [Wey16]\nand references therein).\nIt is not a secret that a significant part of monolithic deep learning architectures\nseems to optimize the extraction of basic features from the training dataset. The\nfact that early layers of trained convolutional networks are essentially spatial filters\ntuned to the training data set has long been well known. In Krizhevsky's seminal\nwork on image recognition and generation [Kri09] with Restricted Boltzmann Ma-\nchines, the author has initially achieved very reasonable success with a modular and\nhierarchical approach resembling the functioning of the mammalian visual system\nbut has ultimately chosen a massive monolithic RBM, a large portion of which was\nfound to be busy learning and using spatial filters. The modular approach was\ntherefore not unpromising. It could have led to more robust, more adaptive, and\nless expensive visual recognition machines had this line of inquiry been pursued a\nbit further.\nAnother example is large language models. The basic functionality of language\nmodels' transformers, at least in some layers, can be understood as simply cap\u0440-\nturing N-gram rules of the training data set, which can be construed as the most\nfundamental statistical features of language (see [Ngu24] and [SC24] and references\ntherein).\nDeep learning architectures are designed to be fine-tuned to a particular environ-"}, {"title": "Thanks", "content": "The author thanks Vladimir Markov for his helpful comments."}]}