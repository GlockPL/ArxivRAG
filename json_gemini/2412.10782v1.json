{"title": "ANAGRAM: A NATURAL GRADIENT RELATIVE TO ADAPTED MODEL FOR EFFICIENT PINNS LEARNING", "authors": ["Nilo Schwencke", "Cyril Furtlehner"], "abstract": "In the recent years, Physics Informed Neural Networks (PINNs) have received strong interest as a method to solve PDE driven systems, in particular for data assimilation purpose. This method is still in its infancy, with many shortcomings and failures that remain not properly understood. In this paper we propose a natural gradient approach to PINNs which contributes to speed-up and improve the accuracy of the training. Based on an in depth analysis of the differential geometric structures of the problem, we come up with two distinct contributions: (i) a new natural gradient algorithm that scales as min(P2S, S2P), where P is the number of parameters, and S the batch size; (ii) a mathematically principled reformulation of the PINNs problem that allows the extension of natural gradient to it, with proved connections to Green's function theory.", "sections": [{"title": "Introduction", "content": "Following the spectacular success of neural networks for over a decade [LeCun et al., 2015], intensive work has been carried out to apply these methods to numerical analysis [Cuomo et al., 2022].In particular, following the pioneering work of Dissanayake and Phan-Thien [1994] and Lagaris et al. [1998], Raissi et al. [2019a] have introduced Physics Informed Neural Networks (PINNs), a method designed to approximate solutions of partial differential equations (PDEs), using deep neural networks. Theoretically based on the universal approximation theorem of neural networks [Leshno et al., 1993], and put into practice by automatic differentiation [Baydin et al., 2018] for the computation of differential operators, this method has enjoyed a number of successes in fields as diverse as fluid mechanics [Raissi et al., 2019b,c, Sun et al., 2020, Raissi et al., 2020, Jin et al., 2021, de Wolff et al., 2021], bio-engineering [Sahli Costabal et al., 2020, Kissas et al., 2020] or free boundary problems [Wang and Perdikaris, 2021]. Nevertheless, many limitations have been pointed out, notably the inability of these methods in their current formulation to obtain high-precision approximations when no additional data is provided [Krishnapriyan et al., 2021, Wang et al., 2021, Karnakov et al., 2022, Zeng et al., 2022]. Recent work by M\u00fcller and Zeinhofer [2023], however, has substantially altered this state of affairs, proposing an algorithm similar to natural gradient methods in case of linear operator (cf. Appendix E), that achieves accuracies several orders of magnitude above previous methods.\nM\u00fcller and Zeinhofer [2024] argue for the need to take function-space geometry into account in order\nto further understand and perfect scientific machine-learning methods. With this paper, we intend to support and extend\ntheir approach by making several contributions:\n\u2022 (i) We highlight a principled mathematical framework that restates natural gradient in an equivalent, yet simpler\nway, leading us to propose ANaGRAM, a general-purpose natural gradient algorithm of reduced complexity\nO(min(P2S, S2P)) compared to O(P3), where P = #parameters and S = #batch samples.\n\u2022 (ii) We reinterpret the PINNs framework from a functional analysis perspective in order to extend ANAGRAM\nto the PINN's context in a straightforward manner.\n\u2022 (iii) We establish a direct correspondence between ANaGRAM for PINNs and the Green's function of the\noperator on the tangent space.\nThe rest of this article is organized as follows: in Section 2, after introducing neural networks and parametric models in\nSection 2.1 from a functional analysis perspective, we review two concepts crucial to our work: PINNs framework in\nSection 2.2, and natural gradient in Section 2.3. In Section 3, we introduce the notions of empirical tangent space and\nan expression for the corresponding notion of empirical natural gradient leading to ANaGRAM 1. In Section 4, after\nreinterpreting PINNs as a regression problem from the right functional perspective in Section 4.1, yielding ANaGRAM\nalgorithm 2 for PINNs, we state in Section 4.2 that natural gradient matches the Green's function of the operator on the\ntangent space and analyse the consequence of this on the interpretation of PINNs training process under ANaGRAM.\nFinally, in Section 5, we show empirical evidences of the performance of ANaGRAM on a selected benchmark of\nPDEs."}, {"title": "Position of the problem", "content": "Our starting point is the following functional definition of parametric models, of which neural networks are a non-linear\nspecial case:"}, {"title": "Neural Networks and parametric model", "content": "Our starting point is the following functional definition of parametric models, of which neural networks are a non-linear\nspecial case:\nDefinition 1 (Parametric model). Given a domain \\(\\Omega\\) of \\(R^n\\), \\(K\\in \\{\\mathbb{R}, \\mathbb{C}\\}\\) and a Hilbert space \\(\\mathcal{H}\\) compound of functions\n\\(\\Omega \\rightarrow K^m\\), a parametric model is a differentiable functional:\n\\[ u:\\begin{cases}\\mathbb{R}^P \\rightarrow \\mathcal{H} \\\\ \\theta \\mapsto (x\\in \\Omega \\mapsto u(x; \\theta)) \\end{cases} \\quad \\qquad (1)\\]\nTo prevent confusion, we will write \\(u_\\theta(x)\\) instead of \\(u(\\theta)(x)\\), for all \\(x \\in \\Omega\\)\nSince a parametric model is differentiable by definition, we can define its differential:\nDefinition 2 (Differential of a parametric model). Let \\(u : \\mathbb{R}^P \\rightarrow \\mathcal{H}\\) be a parametric model and \\(\\theta \\in \\mathbb{R}^P\\). Then the\ndifferential of the parametric model \\(u\\) in the parameter \\(\\theta\\) is:\n\\[ du_\\theta:\\begin{cases}\\mathbb{R}^P \\rightarrow \\mathcal{H} \\\\ h \\mapsto  \\sum_{p=1}^P h_p \\frac{\\partial u}{\\partial \\theta_p} \\end{cases} \\quad \\qquad (2)\\]\nTo simplify notations, we will write for all \\(1 \\leq p \\leq P\\) and for all \\(\\theta \\in \\mathbb{R}^P\\), \\(\\partial_p u\\vert_\\theta\\), instead of \\(\\frac{\\partial u}{\\partial \\theta_p} \\vert_\\theta\\).\nGiven a parametric model \\(u\\), we can define the following two objects of interest:\nThe image set of \\(u\\) : this is the set of functions reached by \\(u\\), i.e. :\n\\[ \\mathcal{M} := \\operatorname{Im} u := \\{u_\\theta: \\theta\\in\\mathbb{R}^P\\} \\quad \\qquad (3)\\]\nAlthough not strictly rigorous\u00b9 \\(\\mathcal{M}\\) is often considered in deep-learning as a differential submanifold of \\(\\mathcal{H}\\), so\nwe will keep this analogy in mind for pedagogical purposes.\nThe tangent space of \\(u\\) at \\(\\theta\\) : this is the image set of the differential of \\(u\\) at \\(\\theta\\), i.e. the linear subspace of \\(\\mathcal{H}\\) compound\nof functions reached by \\(du_\\theta\\), i.e. :\n\\[ T_\\theta \\mathcal{M} := \\operatorname{Im} du_\\theta = \\operatorname{Span}\\left(\\partial_p u\\vert_\\theta : 1 \\leq p \\leq P\\right) \\quad \\qquad (4)\\]\nOnce again, this definition is made with reference to differential geometry.\nWe give several examples of Parametric models in Appendix B. We now introduce PINNs."}, {"title": "Physics Informed Neural Networks (PINNS)", "content": "As in Definition 1, let us consider a domain \\(\\Omega\\) of \\(\\mathbb{R}^n\\) endowed with a probability measure \\(\\mu\\), \\(K\\in \\{\\mathbb{R}, \\mathbb{C}\\}\\), \\(\\partial \\Omega\\) its boundary\nendowed with a probability measure \\(\\sigma\\), and \\(\\mathcal{H}\\) a Hilbert space compound of functions \\(\\Omega \\rightarrow K^m\\). Then let us consider\ntwo functional operators:\n\\[ \\mathcal{D}: \\begin{cases} \\mathcal{H} \\rightarrow L^2(\\Omega \\rightarrow \\mathbb{R}, \\mu) \\\\ u \\mapsto \\mathcal{D}[u] \\end{cases} , \\quad \\mathcal{B}: \\begin{cases} \\mathcal{H} \\rightarrow L^2(\\partial\\Omega \\rightarrow \\mathbb{R}, \\sigma) \\\\ u \\mapsto \\mathcal{B}[u] \\end{cases} \\quad \\qquad (5)\\]\nthat we will assume to be differentiable\u00b2 . We can then consider the PDE:\n\\[\\begin{cases} \\mathcal{D}(u) = f \\in L^2(\\Omega \\rightarrow \\mathbb{R}, \\mu) \\text{ in } \\Omega \\\\ \\mathcal{B}(u) = g \\in L^2(\\partial\\Omega \\rightarrow \\mathbb{R}, \\sigma) \\text{ on } \\partial \\Omega \\end{cases} \\quad \\qquad (6)\\]\nThe PINNs framework, as introduced by Raissi et al. [2019a] consists then in approximating a solution to the PDE by\nmaking the ansatz \\(u = u_\\theta\\), with \\(u_\\theta\\) a neural network, sampling points \\((\\mathbf{x}_i^\\Omega)\\_{1 \\leq i \\leq S_D}\\) in \\(\\Omega\\) according to \\(\\mu\\), \\((\\mathbf{x}_i^{\\partial\\Omega})\\_{1 \\leq i \\leq S_B}\\)\nin \\(\\partial \\Omega\\) according to \\(\\sigma\\) and then to optimize the loss:\n\\[ \\ell(\\theta) := \\frac{1}{2S_D} \\sum_{i=1}^{S_D} \\left(\\mathcal{D}[u_\\theta](\\mathbf{x}_i^\\Omega) - f(\\mathbf{x}_i^\\Omega)\\right)^2 + \\frac{1}{2S_B} \\sum_{i=1}^{S_B} \\left(\\mathcal{B}[u_\\theta](\\mathbf{x}_i^{\\partial\\Omega}) - g(\\mathbf{x}_i^{\\partial\\Omega})\\right)^2 \\quad \\qquad (7)\\]\nby classical gradient descent techniques, used in the context of deep learning, such as Adam [Kingma and Ba, 2014], or\nL-BFGS [Liu and Nocedal, 1989]. One of the cornerstones of Raissi et al. [2019a] is also to use automatic differentiation\n[Baydin et al., 2018] to calculate the operators D and B, thus obtaining quasi-exact calculations, whereas most classic\ntechniques require either approximating operators as for Finite Differences, or carrying out the calculations manually as\nfor Finite Elements.\nAlthough appealing due to its simplicity and relative ease of implementation, this approach suffers from several well-\ndocumented empirical pathologies [Krishnapriyan et al., 2021, Wang et al., 2021, Grossmann et al., 2024], which can be\nunderstood as an ill conditioned problem [De Ryck et al., 2024, Liu et al., 2024] and for which several ad hoc procedures\nhas been proposed [Karnakov et al., 2022, Zeng et al., 2022, McClenny and Braga-Neto, 2022]. Following M\u00fcller and\nZeinhofer [2024], we argue in this work that the key point is rather to theoretically understand the geometry of the\nproblem and adapt PINNs training accordingly."}, {"title": "Natural Gradient", "content": "Natural gradient has been introduced, in the context of Information Geometry by Amari and Douglas [1998]. Given a\nloss: \\(\\ell : \\theta \\rightarrow \\mathbb{R}^+\\), the gradient descent:\n\\[ \\theta_{t+1} \\leftarrow \\theta_t - \\eta \\nabla \\ell_\\theta, \\quad \\qquad (8)\\]\nis replaced by the update:\n\\[ \\theta_{t+1} \\leftarrow \\theta_t - \\eta F_\\theta^{\\dagger} \\nabla \\ell_\\theta, \\quad \\qquad (8)\\]\nwith \\(F_\\theta\\) being the Gram-Matrix associated to a Fisher-Rao information metric [Amari, 2016] or equivalently, the\nHessian of some Kullback-Leibler divergence [Kullback and Leibler, 1951], and \\(\\dagger\\) the Moore-Penrose pseudo-inverse.\nThis notion has been later further extended to the more abstract setting of Riemannian metrics in the context of\nneural-networks by Ollivier [2015]. In this case, given a Riemannian-(pseudo) metric \\(G_\\theta\\), the gradient-descent update is\nreplaced by:\n\\[ \\theta_{t+1} \\leftarrow \\theta_t - G_\\theta \\nabla \\ell_\\theta, \\quad \\qquad (9)\\]\nwhere \\(G_{\\theta_{p,q}} := G_\\theta(\\partial_p u\\vert_\\theta, \\partial_q u\\vert_\\theta)\\) is the Gram matrix of partial derivatives relative to \\(G_\\theta\\). Despite its mathematically\nprincipled advantage, natural gradient suffers from its computational cost, which makes it prohibitive, if not untractable\nfor real world applications. Indeed:\n\u2022 Computation of the Gram matrix \\(G_\\theta\\), is quadratic in the number of parameters.\n\u2022 Inversion of \\(G_\\theta\\) is cubic in the number of parameters.\nDifferent approaches have been proposed to circumvent this limitations. The most prominent one is K-FAC introduced\nby Heskes [2000] and further extended by Martens and Grosse [2015], Grosse and Martens [2016], which approximates\nthe Gram matrix by block-diagonal matrices. This approximation can be understood as making the ansatz that the partial\nderivatives of weights belonging to different layers are orthogonal. A refinement of this method has been proposed by\nGeorge et al. [2018], in which the eigen-structure of the block-diagonal matrices are carefully taken into account in\norder to provide a better approximation of the diagonal rescaling induced by the inversion of the Gram matrix. In a\ncompletely different vein, Ollivier [2017] has proposed a statistical approach that has been proved to converge to the\nnatural gradient update in the 0 learning rate limit.\nTo conclude this section, let us give a more geometric interpretation of natural gradient. To this end, let us consider the\nclassical quadratic regression problem :\n\\[ \\ell(\\theta) := \\frac{1}{2S} \\sum_{i=1}^{S} (u_\\theta(\\mathbf{x}_i) - f(\\mathbf{x}_i))^2, \\quad \\qquad (10)\\]\nwith \\(u_\\theta\\) a parametric model, for instance a neural-network, \\((\\mathbf{x}_i)\\) sampled from some probability measure \\(\\mu\\) on some\ndomain \\(\\Omega\\) of \\(\\mathbb{R}^N\\). In the limit \\(S \\rightarrow \\infty\\) (population limit), this loss can be reinterpreted as the evaluation at \\(u_\\theta\\) of the\nfunctional loss:\n\\[ \\mathcal{L}: v \\in L^2(\\Omega, \\mu) \\rightarrow \\frac{1}{2} \\Vert v - f \\Vert_{L^2(\\Omega,\\mu)}. \\quad \\qquad (11)\\]\nTaking the Fr\u00e9chet derivative, one gets: for all \\(v, h \\in L^2(\\Omega, \\mu\\)\n\\[ d\\mathcal{L}\\vert_v(h) = <v - f, h>_{L^2(\\Omega,\\mu)}, \\quad \\qquad \\]\ni.e. the functional gradient of \\(\\mathcal{L}\\) is \\(\\nabla \\mathcal{L}\\vert_v := v - f\\). As noted for instance in Verbockhaven et al. [2024], Natural\ngradient has then to be interpreted from the functional point of view as the projection of \\(\\nabla \\mathcal{L}\\vert_{u\\vert_\\theta}\\) onto the tangent space\n\\(T_\\theta \\mathcal{M}\\) from Equation (4) with respect to the \\(L^2(\\Omega, \\mu)\\) metric. However, this functional update must be converted into a\nparameter space update. Since the parameter space \\(\\mathbb{R}^P\\) is somehow identified with \\(T_\\theta\\mathcal{M}\\) via the differential application\n\\(du_\\theta\\), it would be sufficient to take the inverse of this application to obtain the parametric update. In general \\(du_\\theta\\) is not\ninvertible but at least it admits a pseudo-inverse \\(du_\\theta^{\\dagger}\\). Moreover, since \\(T_\\theta \\mathcal{M} = \\operatorname{Im} du_\\theta\\) by definition, \\(du_\\theta^{\\dagger}\\) is defined\non all \\(T_\\theta\\mathcal{M}\\). Thus, we have that the natural gradient in the population limits corresponds to the update:\n\\[ \\theta_{t+1} \\leftarrow \\theta_t - \\eta du_\\theta^{\\dagger}\\left(\\Pi_{T_\\theta \\mathcal{M}}\\left(\\nabla \\mathcal{L}\\vert_{u\\vert_\\theta}\\right)\\right). \\quad \\qquad (12)\\]\nNote that the use of the pseudo-inverse implies that the update in the parameter space happens in the subspace\n\\((\\operatorname{Ker} du_\\theta)^{\\perp} \\subset \\mathbb{R}^P\\)."}, {"title": "Empirical Natural Gradient and ANaGRAM", "content": "In practice, one cannot reach the population limit and thus Equation (12) is only an asymptotic update. Nevertheless,\nwe can derive a more accurate update, when we can rely only on a finite set of points \\((\\mathbf{x}_i)\\_{i=1}^S\\) that is usually called a\nbatch. Following Jacot et al. [2018], we know that quadratic classical gradient descent update with respect to a batch in\nthe vanishing learning rate limit \\(\\eta \\rightarrow 0\\), rewrites in the functional space as:\n\\[ \\frac{du_\\theta}{dt}(\\mathbf{x}) = \\sum_{i=1}^{S} \\operatorname{NTK}\\_{\\theta_t}(\\mathbf{x}, \\mathbf{x}_i) (u\\vert_{\\theta_t}(\\mathbf{x}_i) - y_i), \\qquad \\operatorname{NTK}\\_{\\theta}(\\mathbf{x}, \\mathbf{y}) := \\sum_{p=1}^P (\\partial_p u\\vert_\\theta(\\mathbf{x})) (\\partial_p u\\vert_\\theta(\\mathbf{y}))^\\dagger. \\quad \\qquad (13)\\]\nFurthermore, Rudner et al. [2019], Bai et al. [2022] show that under natural gradient descent, the Neural Tangent\nKernel \\(\\operatorname{NTK}\\_{\\theta}\\) should be replaced in Equation (13) by the Natural NTK:\n\\[ \\operatorname{NNTK}\\_{\\theta}(\\mathbf{x}, \\mathbf{y}) := \\sum\\_{1 \\leq p,q \\leq P} (\\partial_p u\\vert_\\theta(\\mathbf{x})) G_{\\theta}^{pq} (\\partial_p u\\vert_\\theta(\\mathbf{y}))^\\dagger, \\qquad G_{\\theta}^{p,q} := <\\partial_p u\\vert_\\theta, \\partial_q u\\vert_\\theta>. \\quad \\qquad (14)\\]\nAs a consequence, one may see that the update under natural gradient descent with respect to a batch \\((\\mathbf{x}_i)\\_{i=1}^S\\) happens\nin a subspace of the tangent space, namely the empirical Tangent Space:\n\\[ T^{NNTK}\\_{\\theta, (\\mathbf{x}_i)}\\mathcal{M} := \\operatorname{Span}(\\operatorname{NNTK}\\_{\\theta}(\\cdot, \\mathbf{x}_i) : (\\mathbf{x}_i)\\_{1 \\leq i \\leq S}) \\subset T_\\theta\\mathcal{M}. \\quad \\qquad (15)\\]\nSubsequently, Equation (12) can then be adapted to define the empirical Natural Gradient update:\n\\[ \\theta_{t+1} \\leftarrow \\theta_t - du_\\theta^{\\dagger}\\left(\\Pi\\_{T^{NNTK}\\_{\\theta, (\\mathbf{x}\\_i)}}\\mathcal{M}}\\left(\\nabla \\mathcal{L}\\vert\\_{u\\vert\\_\\theta}\\right)\\right). \\quad \\qquad (16)\\]\nNote that this update can be understood from the functional perspective as the standard Nystr\u00f6m method [Sun et al.,\n2015], bridging the gap between our work and the many methods developed in this field. Nevertheless, the \\(\\operatorname{NNTK}\\_{\\theta}\\)\nkernel cannot be computed explicitly in our case, since it requires a priori inverting the Gram matrix, which adds further\nchallenge. With this in mind, we present a first result, encapsulated in the following theorem, which is one of our main\ncontributions:"}, {"title": "ANaGRAM", "content": "Theorem 1 (ANaGRAM). Let us be for all \\(1 \\leq i \\leq S\\) and for all \\(1 \\leq p \\leq P\\):\n\\[ \\Phi_{\\theta ip} := \\partial_p u\\vert_\\theta(\\mathbf{x}_i); \\qquad \\Lambda_{\\theta i} := u\\vert_\\theta (\\mathbf{x}_i) - f(\\mathbf{x}_i). \\quad \\qquad \\]\nThen:\n\\[ \\Pi\\_{T^{NNTK}\\_{\\theta, (\\mathbf{x}\\_i)}}\\mathcal{M}}\\left(\\nabla \\mathcal{L}\\vert\\_{u\\vert\\_\\theta}\\right) = (\\Phi + E\\_{\\text{metric}})^{-1} (\\Lambda\\_{\\theta} + E_{\\theta}), \\quad \\qquad (17)\\]\nwhere \\(E\\_{\\text{metric}}\\) and \\(E\\_{\\theta}\\) are correction terms specified in Equations (49) and (50) in Appendix C.3, respectively accounting\nfor the metric's impact on empirical tangent space defintion, and the substraction of the evaluation of the orthogonal\npart of the functionnal gradient.\nA proof of this theorem, as well as a more comprehensive introduction to empirical natural gradient, encompassing a\nd\u00e9tour through RKHS theory, can be found in Appendix C.\nRemark 1. In some important cases the correction terms \\(E\\_{\\text{metric}}\\) and \\(E\\_{\\theta}^\\perp\\) vanishes. This happens for instance for \\(E^\\perp_\\theta\\)\nwhen solving \\(\\mathcal{D}[u] = 0\\) with \\(\\mathcal{D}\\) linear and \\(u\\) an MLP (see Appendix B.2). We refer to Proposition 2 and Remark 7 at\nthe end of Appendix C.3. \\(E\\_{\\text{metric}}\\) cancels out in the following case:\nProposition 1. There exist \\(P\\) points \\((\\mathbf{x}_i)\\) such that \\(T^{NNTK}\\_{\\theta, (\\mathbf{x}\\_i)}\\mathcal{M} = T_\\theta \\mathcal{M}\\). Then notably \\(E\\_{\\text{metric}} = 0\\).\nAs a first approximation, we can neglect those two terms, yielding the following vanilla algorithm:\nNote that algorithm 1 is equivalent to Gauss-Newton algorithm applied to the empirical loss in Equation (10) also\nconsidered recently in Jnini et al. [2024] with a different setting. Nevertheless, our work aims at a more general\napproach, giving rise to different algorithms depending on the approximations of \\(E\\_{\\text{metric}}\\) and \\(E\\_{\\theta}^\\perp\\). One of the pleasant\nbyproducts of the ANaGRAM framework is also that it leads to a straightforward criterion to choose points in the batch,\nnamely:\n\\[ (\\mathbf{x}^*) := \\underset{(\\mathbf{x}\\_i) \\in \\Omega}{\\arg \\min} \\Vert \\Pi\\_{\\operatorname{Span}(\\operatorname{NNTK}\\_{\\theta}(\\mathbf{x}\\_i,\\cdot): 1\\leq i \\leq S)} (\\nabla \\mathcal{L}) - \\nabla \\mathcal{L} \\Vert\\_\\mathcal{H}, \\quad \\qquad (18)\\]\nwhich is amenable to various approximations, subject to further investigations. Taking the best advantage of this\ncriterion should eventually allow us to use natural gradient in a stochastic setting while staying close to the convergence\nrate of the full batch natural gradient as characterized in Xu et al. [2024]. We will now show how ANaGRAM can be\napplied to the PINNs framework."}, {"title": "ANGRAM for PINNS", "content": "Generalizing ANaGRAM to PINNs only requires to change the problem perspective."}, {"title": "PINNs as a least-square regression problem", "content": "The only difference between the losses of Equation (7) and Equation (10) is the use of the differential operator \\(\\mathcal{D}\\)\nand the boundary operator B in Equation (7). More precisely, PINNs and classical quadratic regression problems are\nessentially similar, except that in the case of PINNs we use the compound model (D, B) ou instead of u directly, where,\nusing the definitions of Equation (5):\n\\[ (\\mathcal{D}, \\mathcal{B}) \\circ u : \\begin{cases}\\mathbb{R}^P \\rightarrow \\mathcal{H} \\rightarrow \\mathcal{L}^2(\\Omega, \\Omega) := L^2(\\Omega \\rightarrow \\mathbb{R}, \\mu) \\times L^2(\\partial \\Omega \\rightarrow \\mathbb{R}, \\sigma) \\\\ \\theta \\mapsto  (\\mathcal{D}[u\\_\\theta], \\mathcal{B}[u\\_\\theta]) \\end{cases} \\quad \\qquad (19)\\]\nThe derivation of vanilla ANaGRAM in PINNS context is then straightforward:"}, {"title": "PINNs Natural Gradient is a Green's Function", "content": "Knowing the Green's function of a linear operator is one of the most optimal ways of solving the associated PDE,\nsince it then suffices to estimate an integral to approximate a solution [Duffy, 2015]. However, this requires prior\nknowledge of the Green's function, which is not always possible. Here, we show that using the natural gradient for\nPINNs implicitly uses the operator's Green's function. In Appendix D, we briefly recall the main definitions required to\nstate and prove the following theorem:\nTheorem 2. Let \\(\\mathcal{D} : \\mathcal{H} \\rightarrow L^2(\\Omega \\rightarrow \\mathbb{R}, \\mu)\\) be a linear differential operator and \\(u : \\mathbb{R}^P \\rightarrow \\mathcal{H}\\) a parametric model. Then\nfor all \\(\\theta \\in \\mathbb{R}^P\\), the generalized Green's function of \\(\\mathcal{D}\\) on \\(T_\\theta \\mathcal{M} = \\operatorname{Im} du_\\theta\\) is given by: for all \\(x, y \\in \\Omega\\)\n\\[ \\partial_{T_\\theta \\mathcal{M}}(x, y) := \\sum\\_{1 \\leq p,q \\leq P} \\partial_p u\\vert_\\theta(x) G^{pq} \\partial_q \\mathcal{D}[u\\_\\theta](y), \\quad \\qquad (20)\\]\nwith: for all \\(1 \\leq p,q \\leq P\\)\n\\[ G_{pq} := <\\partial_p \\mathcal{D}[u\\_\\theta], \\partial_q \\mathcal{D}[u\\_\\theta]>_{L^2(\\Omega\\rightarrow\\mathbb{R},\\mu)}. \\quad \\qquad (21)\\]\nIn particular, the natural gradient of PINNs defined at the end of Section 4.1 can be rewritten:\n\\[ \\theta_{t+1} \\leftarrow \\theta_t - \\eta du\\_\\theta^{\\dagger}\\left(x \\in \\Omega \\mapsto \\int\\_{\\Omega} \\partial_{T_\\theta \\mathcal{M}}(x,y) \\nabla \\mathcal{L}\\vert\\_{u\\vert\\_\\theta}(y) \\mu(dy)\\right). \\quad \\qquad (22)\\]\nA few comments should be made about Equation (22). First, if \\(\\eta = 1\\), then the natural gradient can be understood as\nthe least-square's solution of \\(\\mathcal{D}[u] = f\\) at order 1, i.e. in the affine space \\(u\\vert\\_{\\theta\\_t} + T\\_{\\theta\\_t}\\mathcal{M}\\). However, it does not hold a\npriori that:\n\u2022 \\(\\mathcal{D}[u\\vert_{\\theta\\_t} + T\\_{\\theta\\_t}\\mathcal{M}]\\) correctly approximates \\(f \\in L^2(\\Omega \\rightarrow \\mathbb{R}, \\mu)\\).\n\u2022 \\(u\\vert_{\\theta\\_t} + T\\_{\\theta\\_t}\\mathcal{M}\\) correctly approximates the image space \\(\\mathcal{M} = \\{u\\vert\\_\\theta : \\theta \\in \\mathbb{R}^P\\}\\).\nMultiplying by a learning rate \\(\\eta \\ll 1\\) is then essential. In this way, natural gradient can be understood as moving in the\ndirection of the solution of \\(\\mathcal{D}[u] = f\\) in the affine space \\(u\\vert_{\\theta\\_t} + T\\_{\\theta\\_t} \\mathcal{M}\\), and thus getting closer to the solution, while\nexpecting that the change induced by this update will improve the approximation space \\(u\\vert_{\\theta\\_{t+1}} + T\\_{\\theta\\_{t+1}} \\mathcal{M}\\). On the\nother hand, when we approach the end of the optimization, i.e. when the space \\(\\mathcal{D}[u\\vert_{\\theta\\_t} + T\\_{\\theta\\_t}\\mathcal{M}]\\) approximates \\(f\\) \u201cwell\nenough\u201d, while \\(du\\_\\theta\\) approximates \u201cwell enough\u201d \\(\\mathcal{M}\\), then it is in our best interest to solve the equation completely,\ni.e. to take learning rates \\(\\eta\\) close to 1. This is why the use of line search in ANaGRAM (cf. line 6 in Algorithm 2) is\nessential. We should then conclude that the quality of the solution found by the parametric model \\(u\\) depends only on:\n\u2022 How well \\(\\Gamma = \\{\\mathcal{D}[u\\vert\\_\\theta] : \\theta\\in\\mathbb{R}^P\\}\\) can approximate the source \\(f \\in L^2(\\Omega \\rightarrow \\mathbb{R}, \\mu)\\).\n\u2022 The curvature of \\(\\Gamma\\). More precisely, if its non-linear structure induces convergence to a \\(\\mathcal{D}[u\\vert\\_\\theta]\\) such that\n\\(f - \\mathcal{D}[u\\vert\\_\\theta]\\) is non-negligible, while being orthogonal to the tangent space \\(\\mathcal{D}[T\\_\\theta\\mathcal{M}]\\).\nIf we assume now that D is also nonlinear, then all the above analysis also holds for the linear operators \\(d\\mathcal{D}\\_{\\vert u\\vert\\_\\theta\\_t}\\), the\ndifference being that the operator changes at each step. This means that in the case of non-linear operators, we have to\ndeal with both the non-linearity of D and u, but that does not change the overall dynamic.\nFinally, assuming that both D and u are linear (this is for instance the case when we assume u to be a linear combination\nof basis functions, like in Finite Elements, or Fourier Series). Then \u201clearning\u201d \\(u\\vert\\_\\theta\\) with natural gradient (and learning\nrate 1) corresponds to solve the equation in the least-squares sense with a generalized Green's function."}, {"title": "Experiments", "content": "We test ANAGRAM on four problems: 2D Laplace equation; 1+1 D heat equation; 5D Laplace equation; and 1+1 D\nAllen-Cahn equation. The first three problems comes from M\u00fcller and Zeinhofer [2023], while the last one is proposed\nin Lu et al. [2021].\nFor training, we use multilayer perceptrons with varying layer sizes and tanh activations, along with fixed batches of\npoints: a batch of size SD to discretize \\(\\Omega\\) and a batch of size SB to discretize \\(\\partial\\Omega\\). The layer size specifications, cutoff\nfactor \\(\\epsilon\\), values of SD and SB, and discretization procedures are specified separately for each problem. Currently, the\ncutoff factor is chosen manually and warrants further investigation."}, {"title": "Conclusion and Perspectives", "content": "We introduce empirical Natural Gradient", "2023": "for linear PDEs at a fraction of the computational cost", "limitations": "one concerns the chosing procedure of the batch\npoints, which is so far limited to simple heuristics; the second is the hyperparameter tunning, more specifically the\ncutoff factor, which is so far chosen by hand, while it may probably be automatically chosen based on the spectrum of\nthe \\(\\Phi_\\theta\\).\nImportant perspectives include"}]}