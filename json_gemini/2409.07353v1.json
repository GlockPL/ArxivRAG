{"title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks", "authors": ["Md Zarif Hossain", "Ahmed Imteaj"], "abstract": "Large Vision-Language Models (LVLMs), trained on multimodal big datasets, have significantly advanced Al by excelling in vision-language tasks. However, these models remain vulnerable to adversarial attacks, particularly jailbreak attacks, which bypass safety protocols and cause the model to generate misleading or harmful responses. This vulnerability stems from both the inherent susceptibilities of LLMs and the expanded attack surface introduced by the visual modality. We propose Sim-CLIP+, a novel defense mechanism that adversarially fine-tunes the CLIP vision encoder by leveraging a Siamese architecture. This approach maximizes cosine similarity between perturbed and clean samples, facilitating resilience against adversarial manipulations. Sim-CLIP+ offers a plug-and-play solution, allowing seamless integration into existing LVLM architectures as a robust vision encoder. Unlike previous defenses, our method requires no structural modifications to the LVLM and incurs minimal computational overhead. Sim-CLIP+ demonstrates effectiveness against both gradient-based adversarial attacks and various jailbreak techniques. We evaluate Sim-CLIP+ against three distinct jailbreak attack strategies and perform clean evaluations using standard downstream datasets, including COCO for image captioning and OKVQA for visual question answering. Extensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracy while substantially improving robustness against both gradient-based adversarial attacks and jailbreak techniques.", "sections": [{"title": "I. INTRODUCTION", "content": "The emergence of Large Language Models (LLMs) has revolutionized the landscape of natural language processing (NLP), achieving unprecedented performance in tasks such as text generation, translation, and comprehension through training on extensive and diverse corpora of text data [1], [2]. The vast scale of these datasets, often encompassing billions of words from diverse sources, has been pivotal in developing models that can understand and generate human-like text with remarkable accuracy. Building on this foundation, Large Vision-Language Models (LVLMs) [3]\u2013[5] have emerged, expanding the capabilities of LLMs by integrating both textual and visual data. LVLMs utilize large-scale multimodal datasets [6], [7] and integrate modality-specific encoders and projectors to enhance their ability to process and generate information from both text and images. This integration allows LVLMs to perform complex tasks such as visual question answering, image captioning, and cross-modal retrieval [8]. To align these models with human values and ensure they operate safely and ethically, techniques such as instruction tuning and Reinforce- ment Learning from Human Feedback (RLHF) are proposed in [9], [10]. RLHF aims to fine-tune LLMs based on human feedback, guiding them to generate responses that are both helpful and harmless. However, despite these measures, LLMs remain vulnerable to various adversarial attacks [11]-[14], particularly jailbreak attacks [15], [16] that can bypass model's safety guardrails and elicit harmful contents. As LVLMs are build upon LLMs, they inherit many of these vulnerabili- ties, including susceptibility to jailbreak attacks. Additionally, LVLMs not only inherit the vulnerabilities of traditional LLMs but also introduce new risks due to the integration of visual modality, which can be exploited in unforeseen ways. Unlike traditional LLMs, LVLMs can be attacked through both text and visual modalities, leading to more sophisticated and severe adversarial attacks [17]. While jailbreak attacks on LLMs typically rely on template-based [15] approaches, LVLMs en- counters two distinct types of jailbreak attacks: optimization- based and generation-based. Optimization-based attacks [18], [19] involve applying gradient-based perturbations to benign images, while generation-based attacks [20] focus on craft- ing images with embedded malicious content. Particularly, optimization-based attacks have recently gained attention due to the rise of universal image generation techniques [19]. These methods enable attackers to craft perturbed images that can bypass safety protocols without requiring contextually aligned queries. The perturbations generated are often nearly imperceptible to human observers, which complicates their detection and mitigation."}, {"title": "II. BACKGROUND STUDY", "content": "Large Language and Vision Models. In recent time, LLMS such as GPT-3 [24], Vicuna [25], and LLaMA-2 [2] have gained significant attention due to their vast number of pa- rameters and training on large-scale datasets [26]-[28]. These models have demonstrated advanced abilities such as task- agnostic in-context learning [24] and chain-of-thought [29] reasoning compared to traditional ML models [30], [31]. Autoregressive LLMs (e.g., GPT-2 [32], PaLM [33]), which predict the next token in a sequence, have become the pre- dominant focus of research in this field. The success of these models has led to the development of multimodal models, referred to as Large Vision-Language Models (LVLMs), which integrate both textual and visual inputs for reasoning and inference. LVLMs typically combine pre-trained LLMs (e.g., Vicuna, LLaMA-2) with large-scale vision encoders, such as CLIP [34] and BLIP [35], to process multimodal data. In most cases, the vision encoder remains frozen during training, while the model learns to integrate visual and textual modalities through a projection layer and cross-attention mechanisms. Notable examples of such models include OpenAI's GPT-4 [1], Flamingo [4], and open-source models like MiniGPT-4 [5], InstructBlip [8] and LLaVA [3].\nSafety Alignment of LLMs and LVLMs. Pretrained LLMS and LVLMs often display behavior that can stray from their intended purpose, resulting in outputs that may be untruthful, harmful, or otherwise unhelpful. This misalignment stems from the discrepancy between the autoregressive objective of predicting the next token and the ideal goal of generating responses that are helpful, truthful, and harmless [36]. \u03a4\u03bf address this, safety alignment research of LLM and LVLM focuses on techniques like Instruction Tuning [9] and RLHF [36]. Instruction Tuning involves providing models with ex- amples of instruction-output pairs to encourage adherence to user instructions, while RLHF utilizes a preference model to fine-tune outputs based on human judgments. Additionally, emerging methods such as Constitutional AI [9] and self- alignment [37] aim to further refine model behavior. Despite these advancements in safety alignment, challenges remain, particularly regarding the effectiveness of safety mechanisms and reliability. Models like GPT-4 [1] and LLama-2 [2] are trained to avoid generating harmful contents, but researchers have demonstrated that it is possible to bypass these safety protocols through targeted jailbreak attacks [19]. Jailbreak attacks are designed to override the safety mechanisms of LLMs, allowing them to produce content that they are explic- itly trained to avoid, such as misinformation, harmful advice, or illegal activities.\nJailbreak attacks in LVLMs. Adversarial attacks on LVLMs have emerged as a significant concern, as they exploit vul- nerabilities in these models to generate incorrect or harmful outputs. For instance, in AdvCLIP [38], authors develop uni- versal adversarial patches that can deceive CLIP models across all their downstream tasks. Similarly, Zhao et al. [39] uses diffusion models to craft adversarial samples that manipulate the model into generating targeted outputs, while Schlarmann et al. [17] demonstrate how gradient-based targeted attacks can force LVLMs to produce erroneous or misleading results based on the attacker's choice. Notably, these targeted attacks pose severe threats to the reliability and safety of LVLMS"}, {"title": "III. JAILBREAK ATTACKS", "content": "In this paper, we implement and evaluate three different jailbreaking attacks: ImgJP [19], VisualAdv [18], and Hades [20]. Notably, VisualAdv and ImgJP are optimization-based attacks, while Hades is a generation-based attack. While our defense strategy primarily focuses on optimization-based jail- break attacks, we also evaluate its efficacy against generation- based attacks, such as Hades, to ensure a comprehensive assessment of its robustness. We choose these attacks based on their relevance and widespread use in recent research. A brief description of these attacks is provided in Table I.\nA. ImgJP Attack\nThe Image Jailbreaking Prompt (ImgJP) [19] attack exploits the multimodal nature of LVLMs by introducing a visual element that can trigger the generation of potentially harm- ful content. When presented with a standard image and a harmful query, LVLMs typically refuse to comply, responding with safety-oriented messages. However, by substituting the standard image with an adversarial image generated with ImgJP, the built-in safeguards of LVLMs can be bypassed. ImgJP attack leverages a maximum likelihood-based approach, aiming to maximize the likelihood of generating target outputs that typically starts with affirmative phrases (e.g., \"Sure, here's some information on (query content)\u201d, effectively over-riding the model's ethical constraints. A key feature of the ImgJP attack is its data-universal property, which manifests in two di- mensions: prompt-universal and image-universal. This allows a single adversarial image to potentially jailbreak multiple unseen prompts and images, making it a powerful tool for probing LVLM vulnerabilities. The attack can be formulated mathematically as an optimization problem. Given a set of harmful query-answer pairs B = {(qi, ai), i = 0, ..., N}, the objective is to find an optimal ImgJP $x_{jp}$ that maximizes:\n$\\max_{x_{jp}} \\sum_{i=0}^{N} \\log(p(b_i | r_i, z)) \\text{ s.t. } z\\in [0, 255]^k$ (1)\nHere, $p(a_i | q_i, x_{jp})$ represents the likelihood that the MLLM will generate the target answer $a_i$ when provided with the harmful query $q_i$ and the ImgJP $x_{jp}$. The optimal ImgJP, $x_{jp}^*$, is derived by optimizing over multiple query-answer pairs, $(q_i, a_i), i = 0, ..., M$, using any suitable adversarial attack method, such as Projected Gradient Descent (PGD) [23].\nB. VisualAdv Attack\nThe VisualAdv [18] attack is performed by optimizing a benign image on a carefully selected few-shot corpus of 66 derogatory sentences aimed at specific demographics. Visual- Adv generates a universal adversarial example by optimizing it to maximize the likelihood that the model generates harmful content from a small set of derogatory sentences. Additionally, this adversarial image is designed to be imperceptible to human observers, ensuring that it remains visually indistin- guishable from the original benign image. This optimization"}, {"title": "IV. METHODOLOGY", "content": "In our prior work, we introduced Sim-CLIP [48], an ad- versarially fine-tuned CLIP model that significantly enhanced robustness against gradient-based attacks, such as PGD and APGD. This advancement marked a key step in improving the security of LVLMs against adversarial threats. Building on this foundation, we extend our research by hypothesizing that Sim- CLIP can be further optimized to effectively defend against the more sophisticated optimization-based jailbreak attacks target- ing LVLMs. Given that optimization-based jailbreak attacks primarily exploit gradient-based perturbations, and considering Sim-CLIP's proven resilience to similar adversarial techniques, we hypothesize that its defense capabilities can be extended to counter these advanced attack vectors. By enhancing Sim- CLIP to address this critical security gap, we propose a signif- icant contribution to the field-positioning it as a robust and promising solution for securing LVLMs against increasingly sophisticated adversarial threats.\nA. Defending Against Jailbreak Attacks with Sim-CLIP+\nSim-CLIP+ leverages a Siamese architecture with a tailored cosine similarity loss to address the vulnerabilities exploited by jailbreak attacks. The process begins with the generation of an adversarial example x' from the original input image x using PGD [23]:\n$x^{t+1} = \\text{Proj}_{B(x,\\epsilon)} (x + \\eta \\cdot \\text{sign}(\\nabla_x J(x, y)))$ (2)\nHere, J represents the cross-entropy loss, y is the true label, \u03b7 is the step size, and e defines the perturbation bound. This process creates a perturbed version of the input that simulates potential adversarial manipulations on the image for jailbreak attack. Next, the original and perturbed images are processed by the same CLIP models with shared weights, as shown in Fig. 2. This generates clean ($R_c$) and perturbed ($R_p$) representations, respectively. To enhance the model's resis- tance to adversarial perturbations, we maximize the similarity between these representations by minimizing their negative cosine similarity:\n$L_{cos}(R_p, R_c) = - \\frac{R_p \\cdot R_c}{||R_p||_2 ||R_c||_2}$ (3)\nBy exposing the model to perturbed inputs during training, Sim-CLIP+ learns to focus on robust, attack-invariant fea- tures. This process enhances the model's ability to maintain"}, {"title": "V. EXPERIMENTAL ANALYSIS", "content": "A. Adversarial fine-tuning settings\nFor adversarial fine-tuning setting, we adversarially fine- tune the CLIP vision encoder on the ImageNet [27] dataset. Since we adopt an unsupervised adversarial fine-tuning ap- proach, we exclude the image class labels and fine-tune solely on the images. To generate perturbed views from the clean images, we apply PGD [23] with 10 adversarial steps using the lo norm. While ensuring robustness is essential, adversarial training often comes with the risk of degrading clean perfor- mance. To balance robust accuracy and clean performance, we fine-tune CLIP with two perturbation radii: 6 = 2/255 and \u20ac = 4/255. The resulting robust CLIP models are referred to as Sim-CLIP+2 and Sim-CLIP+4, respectively. In this study, we employ Sim-CLIP+4 for our experimental evaluation.\nB. LVLM models\nWe use LLaVA (Vicuna-7B) [53] and LLaVA (Llama-2- 13B) [3] as the target LVLMs for evaluation. Both models utilize CLIP [34] ViT-L-14 as their vision encoder. However, they differ in their language decoders: LLaVA (Vicuna-7B) utilizes the Vicuna language model [25] with 7 billion param- eters, whereas LLaVA (Llama-2-13B) [2] is built on LLaMA- 2-13B-Chat, featuring 13 billion parameters. LLaMA-2-13B- Chat has undergone extensive instruction tuning and iterative RLHF on high-quality red-teaming data, making it more robust and resilient to jailbreak attacks. Additionally, by including LLaVA (Llama-2-13B) in our evaluation, we aimed to assess the scalability and effectiveness of our defense mechanism across larger LVLMs.\nC. Jailbreak attack settings\nImgJP attack settings and datasets. The ImgJP attack optimizes an adversarial perturbation starting from random noise. For our experiments, we set the perturbation radius to \u20ac = 16/255 within an lo norm, and the optimization process is conducted over 1000 iterations. Unlike other jailbreak attacks [19] that require context-aligned benign images and harmful queries to generate adversarial images, the ImgJP attack creates universal adversarial images from benign images that do not require contextually aligned harmful query. Addi- tionally, the generated universal adversarial image can also be paired with any harmful prompts without depending on the specifics of each prompt. In our evaluation, we utilize 25 harmful instructions sourced from the Advbench [54] dataset, pairing each with the universal adversarial image. Given that adversarial perturbations optimized on one model (the surro- gate model) can often transfer to other target models, we use MiniGPT-4 (Vicuna 7B) as the surrogate model when targeting LLaVA (Vicuna-7B) and LLaVA (Llama-2-13B) models.\nVisualAdv attack settings and datasets. In our experiment, we leverage VisualAdv [18] attack to generate two univer- sal adversarial images with varying perturbation constraints: = 16/255 and e = 128/255. \u03a4o assess the robustness of our vision encoder against stronger jailbreak attacks, we focus on the higher perturbation radius at \u20ac = 128/255. For VisualAdv attack, we use RealToxicityPrompts [55] dataset, which contains 1,225 harmful prompts to elicit toxicity in generated output of the LVLMs. From this dataset, we select a subset of 100 harmful instructions which includes toxic prompts related to violence, toxicity, and profanity. We then pair these toxic prompts with our visual adversarial examples as input and measure the toxicity of the resulting output of the target LVLMs. We use LLaVA (Llama-2-13B) as the surrogate model to generate adversarial images.\nHades attack settings and datasets. In this study, we implement the full version of Hades [20], which incorporates three distinct strategies: text-to-image generation using typo- graphical methods, amplification of image toxicity through diffusion models, and adversarial perturbation optimization. We utilize the dataset introduced in HADES comprising 750 harmful instructions across five scenarios, each instruction paired with a related harmful image. For our experiments, we specifically focus on the violence and toxicity categories. Since Hades does not generate universal adversarial images, it requires relevant images aligned with each query. These"}, {"title": "VI. CONCLUSION", "content": "In this paper, we present Sim-CLIP+, a robust defense mechanism designed to strengthen the resilience of LVLMs against adversarial jailbreak attacks. By leveraging a Siamese architecture, we adversarially fine-tune the CLIP vision en- coder in Sim-CLIP+, which enhances the robustness of LVLMs without requiring structural changes or incurring significant computational costs a key consideration in big data processing. Notably, Sim-CLIP+ does not require the LVLM itself to undergo retraining or fine-tuning for defense against jailbreak attacks. Our extensive evaluations demon- strate that Sim-CLIP+ not only preserves high accuracy on clean evaluation but also substantially mitigates the impact of gradient-based adversarial attacks and various jailbreak attacks. While Sim-CLIP+ is primarily focused on defending against optimization-based jailbreak attacks, it also demon- strates robustness comparable to external defenses against generation-based jailbreak attacks. By seamlessly integrating Sim-CLIP+ into existing LVLMs, we provide a practical solu- tion to improve the security and reliability of multimodal AI systems. This development supports the safer deployment of large vision-language models in critical and sensitive big data applications, with the potential to drive significant progress toward more secure and dependable AI technologies."}]}