{"title": "Vision-based Manipulation of Transparent Plastic Bags in Industrial Setups", "authors": ["F. Adetunji", "A. Karukayil", "P. Samant", "S. Shabana", "F. Varghese", "U. Upadhyay", "R. A. Yadav", "A. Partridge", "E. Pendleton", "R. Plant", "Y. Petillot", "M. Koskinopoulou"], "abstract": "This paper addresses the challenges of vision-based manipulation for autonomous cutting and unpacking of transparent plastic bags in industrial setups, aligning with the Industry 4.0 paradigm. Industry 4.0, driven by data, connectivity, analytics, and robotics, promises enhanced accessibility and sustainability throughout the value chain. The integration of autonomous systems, including collaborative robots (cobots), into industrial processes is pivotal for efficiency and safety. The proposed solution employs advanced Machine Learning algorithms, particularly Convolutional Neural Networks (CNNs), to identify transparent plastic bags under varying lighting and background conditions. Tracking algorithms and depth sensing technologies are utilized for 3D spatial awareness during pick and placement. The system addresses challenges in grasping and manipulation, considering optimal points, compliance control with vacuum gripping technology, and real-time automation for safe interaction in dynamic environments. The system's successful testing and validation in the lab with the FRANKA robot arm, showcases its potential for widespread industrial applications, while demonstrating effectiveness in automating the unpacking and cutting of transparent plastic bags for an 8-stack bulk-loader based on specific requirements and rigorous testing.", "sections": [{"title": "I. INTRODUCTION", "content": "Industry 4.0\u2014also called the Fourth Industrial Revolution or 4IR\u2014is the next phase in the digitization of the manufacturing sector, driven by disruptive trends including the rise of data and connectivity, analytics, human-machine interaction, and improvements in robotics [1], [2]. This could make products and services more easily accessible and transmissible for businesses, consumers, and stakeholders all along the value chain [3]. Preliminary data indicate that successfully scaling 4IR technology makes supply chains more efficient and sustainable [4], creates a safer and more productive environment for the employees, reduces occupational accidents and factory waste, and has countless other benefits.\nAutonomous manipulation of plastic packages in industrial setups typically involves the use of robotic systems and automation technologies [5]. These systems are designed to handle, move, and manipulate plastic packages in a variety of industrial processes, such as packaging, recycling and sorting, food processing, and quality control [6], [7].\nCollaborative robots, or cobots, are widely used in various industrial applications, working alongside humans without needing extensive safety barriers, cages, or other restrictive measures [8]. These robots use different sensors to identify their environment, recognise objects and are programmed for better accessibility, flexibility and repeatability. Example cases can be found in the textile industry as described in [9], where the authors proposed a dual arm collaborative system for textile material identification. By imitating human behavior, in this work the robots use actions such as pulling and twisting to identify and learn more about textile properties. In recent years, the recycling and waste management industry has begun to use vision-based robotic systems for the classification and accurate sorting of waste materials [10]. Indicative examples can be found in different recycling industries for the management of construction waste [11], [12], recyclable materials [13], [14] or electronic parts [15], [16].\nThe vision-based manipulation and autonomous cutting of transparent plastic bags presents a set of intricate challenges and a compelling need for innovative AI solutions [17], [18]. The inherent transparency of the bags poses difficulties in accurate detection due to the reflection and refraction of light, demanding sophisticated computer vision algorithms for reliable identification [19]. The deformable nature of plastic bags adds complexity to the grasping and manipulation process, necessitating advanced robotic control strategies to handle their variability [20].\nAdditionally, autonomous cutting requires well-considered mechanical design and precise vision-guided tools to discern optimal cutting points while avoiding unintended damage. Ensuring the safety and efficiency of these systems in real-time, dynamic environments further amplifies the challenge. The pressing need for such technologies arises from the increasing demand for automated waste management, recycling, and packaging processes, where vision-based systems can enhance efficiency, reduce human intervention, and contribute to sustainable practices by facilitating the effective processing of transparent plastic bags [21].\nIn this work, through the use of advanced Machine Learning algorithms, based on Convolutional Neural Networks (CNNs), the system can identify transparent plastic bags within its visual field, taking into account variations in lighting and background. Once the bags are detected, the system utilizes tracking algorithms to follow the pick and placement of the bags, and, integrate depth sensing technologies for 3D spatial awareness. The next steps involve developing algorithms for robotic grasping and manipulation, accounting for the chal-"}, {"title": "II. MECHANICAL DESIGN", "content": "The mechanical design encompasses three key components:\ni Feeding: This involves the precise picking and placing of eight packaged plastic-container stacks from an adjacent tote into eight individual enclosures aligned with the bulk loader's feeding system.\nii Cutting: This entails the opening, removal, and disposal of the packaging surrounding the plastic-container stacks. This operation is performed while the plastic-container stacks are securely held within the eight individual enclosures.\niii Delivery: This stage involves opening the enclosures containing the plastic-containers and strategically placing the unpackaged stacks into the bulk loader, facilitating the seamless integration of the plastic-containers into the larger industrial process.\nThis section details each subsystem within the design of the prototype.\nFeeding. In the feeding system, a collaborative robot arm (the 7-axis Franka Emika Panda, equipped with an Intel RealSense D350 camera and a custom suction cup gripper) has been employed for manipulation and vision tasks (Figure 1b). The gripper comprises a Schmalz PSPF 33 SI-30/55 G1/8-AG suction cup, an SBP 15 G02 SDA vacuum generator, and a VS VP8 SA M8-4 pressure sensor enclosed in a custom 3D printed housing (Figure la: CAD model of the custom gripper and b:real printed griping system). The feeding process initiates with the camera capturing a top-down image of the tote, identifying the tops of the plastic stacks, and assigning a value to establish the picking order. The robot arm, guided by the established order, uses suction to pick and place individual stacks into 1 of 8 custom enclosures made of aluminum extrusion and acrylic (Figure 2)a. Stack placement is verified using HC-SR04 ultrasonic sensors on the enclosure's back wall. To facilitate picking, the tote containing the stacks is inclined at an angle ($12^{\\circ}$) to prevent the stacks from toppling. The tote is labeled with a QR code for arm position estimation between picks. Solenoids control suction activation, and the pressure sensor provides feedback to confirm successful suction. This comprehensive setup ensures effective and reliable feeding for the subsequent stages of the automated unpacking and cutting process.\nCutting. The cutting system consists of two interconnected components: a gripping mechanism and a cutting mechanism. The gripping mechanism employs eight vacuum-driven suction cup grippers to securely hold the bottom of each stacks packaging. A vacuum line, directed through two compressors and solenoid valves, divides the vacuum between the cobot arm's end effector gripper and the eight suction cup grippers. Each of the eight individual air conduits is equipped with a dedicated vacuum generator, generating ample vacuum for secure suction, along with pressure sensors to ascertain suction power. Suction cups, mounted on 3D printed arms and connected to motors, swing into contact with the stack bases (Figure 2d). The cutting mechanism features a scalpel blade housed within a custom 3D printed mount, affixed to a linear belt drive (Figure 2e). The cutting process commences by opening the solenoid valve, supplying pressure to the vacuum generators. The swinging rod engages each suction cup gripper, making contact with the bottom of all eight packages. As suction secures the packages, the sensors on each gripper gauge the required suction power. Once optimal conditions are reached, the rod rotates, creating tension in the packaging. Simultaneously, the cutting mechanism traverses the linear rail, slicing through the packaging. Upon completion, the valves close, releasing the cut plastic into a container beneath the aluminum frame. After opening all eight stacks, the cobot arm reverses the pick-and-place task, using the suction cup gripper to grasp the top of each remaining packaging item. These are then placed into a designated disposal bin. This comprehensive cutting system ensures precise and efficient packaging removal, complementing the overall automated process for unpacking and cutting transparent plastic bags in industrial setups.\nDelivery. The delivery system incorporates nine WL-22040921 linear actuators, with eight arranged in parallel through a two-channel relay module to create the pushing mechanism against the back walls of the enclosures. The remaining actuator, also connected to a two-channel relay module, is dedicated to the custom door mechanism. In the pushing mechanism (Figure 2c), each linear actuator, positioned against the back walls, executes forward move- ments, serving as pushers, and integrates ultrasonic sensors for precision. A custom plate at the base catches the stacks,"}, {"title": "III. DEEP-LEARNING-BASED OBJECT DETECTION AND MANIPULATION", "content": "The vision system's comprehensive workflow for real-time detection and tracking of transparent bags is presented in the following. The camera framework operates on ROS Noetic, leveraging the ROS Wrapper for Intel RealSense Devices pro- vided by Intel. By initiating the RealSense camera package, the camera commences the publication of depth and vision (RGB) data, readily accessible for subscription and utilization as needed. These captured data are transformed into monochrome and disseminated to subsequent detection stages. QR codes are employed for zone categorization, aiding in depth data estimation. The detection process is executed using YOLOv5, integrated into ROS through the ROS wrapper. YOLOv5 is renowned for its efficiency and performs real-time detection of plastic-container stacks, providing precise picking locations to the robotic system.\n1) QR Code Based Depth Estimation and Tracking: Depth estimation is required for the conversion of the camera coordi- nates to Cartesian coordinates. However, this is non-trivial as depth estimation using the current camera is not always robust due to inconsistencies caused by the transparency of the bags and plastic-containers. To overcome this, QR codes have been placed at the side of the tote as shown in Figure 3a. The QR codes provide clear points of reference and an accurate depth reading at the beginning of the task. Four QR codes have been used in total, each of which corresponds to one of the four rows of stacks in the tote. The distance between the stack and the camera is then calculated based on the distance between the camera and the respective QR code. This also allows the bags to be grouped into different zones as shown in Figure 3b. Picking of the stacks presupposes optimal tracking, such that the robot can return to the next stack in the sequence after loading the feeding system. In order to achieve this, the detection of the bags is performed by zone rather than by tote. In Figure 3b, the detection of the stacks of zone 1 is shown, by identifying 4 bags in red and their corresponding confidence level. The green dot within the right-hand red box indicates the target stack that the robot is going to pick next. Whereas, Figure 3c illustrates the process sequence after the first zone stacks are successfully picked and placed by the robot and the detection of the 6 stacks of zone 2. By finding the coordinates of the leftmost stack, the robot can detect and pick the stacks one by one from left to right. Once the picking of the stacks of zone 1 has been completed, it can then proceed to zones 2, 3 and finally 4.\n2) Data Acquisition: A custom data-set was created using the same Intel D435 camera used for object detection by capturing the plastic-container stacks and labelling them to be used as training data for the algorithm. A sample image from the raw images captured is shown in Figure 4 (Original). The images were grouped into training, validation and test sets of 150, 40 and 24 images respectively. The data were collected"}, {"title": "3) Robot control for pick-n-place of detected plastic bags:", "content": "The robotic manipulation process begins with a fixed Home Pose, where the robot is positioned at a predefined location that provides a complete view of all the packaged stacks within the tote. This home position serves as the starting point for subsequent operations. Next, the robot initiates the detection of packaged stacks which marks commencement of the third workflow step, by invoking the packaged stack detection code through the ROS architecture using a ROS service. The outcome of this service query is the identification of the next single packaged stack to be picked by the robot within the camera's visual frame. Figure 5 presents the overall automation logic of the robot control for this pick-n-place task.\nUpon successfully detecting the packaged stack, the three- dimensional coordinates of the detected stack in the cam- era's optical frame are transformed into the robot's reference frame using MoveIt hand-eye calibration package [22]. This calibration process generates a calibration file specific to the robotic setup which calculates the cobot configuration from the camera's three-dimensional coordinates.\nThe cobot then initiates the fourth workflow step which involves trajectory planning and execution. The robot uses the integrated motion planner, MoveIt Pilz Industrial Motion Planner (LIN), to plan and execute a linear path to reach the identified packaged stack. LIN utilises the Panda Franka Emika's cartesian constraints to create a trapezoidal velocity profile in Cartesian space for the cobot's movement. This pro- file ensures the cobot accelerates, maintains a constant speed, and then decelerates during its movements. This approach proves highly effective, especially when handling packaged stacks as they can deform after collisions with objects in the workspace if the speed profiles are not controlled.\nTo further refine control, scaling factors for Cartesian ve- locity and acceleration are integrated in the system, which imposes a maximum speed limit on the trajectories generated by the planner. The speed and planning parameters used for the robots pick-n-place testing are tabulated in Table II."}, {"title": "IV. AUTONOMOUS CUTTING & CONTROL", "content": "The whole system is integrated with the aid of robotic control and electronic automation. The robotic control is developed in the Robot Operating System (ROS), through customisation of libraries such as motion-planner and move- it. A custom made automation solution oversees the whole operation through feed-back from various sensors and the electronic actuation of motors. The automation logic is imple- mented using a Raspberry-Pi module which acts as the master controller and commands the Arduino module, which in turn handles the actuation and feed-back based on the commands sent by the master module.\nThe automation system uses a combination of Arduino Mega, sensors (including ultrasonic and pressure sensors), linear actuators, solenoids, and a Raspberry Pi 4 for automa- tion. The sensors and actuators are linked to the Arduino Mega. The Arduino communicates with the Raspberry Pi 4 via serial communication. The Raspberry Pi 4 reads sensor data transmitted by the Arduino and sends commands to activate solenoids, motors, and linear actuators.\nThe Raspberry Pi is coded using ROS python which is the automation controller in this implementation. It coordinates actions with the cobot by publishing and subscribing to the relevant topics at the appropriate time.\nThe operation starts with the Raspberry Pi controller pub- lishing system ready to send commands to the cobot, which then initializes the cobot operation. The cobot then moves to a position over the tote, so that it can detect the stacks, and gives the ready for picking command to the controller. The Raspberry Pi then switches on the solenoid valve connected with the cobot suction gripper, which aids the picking of the bags from the tote. The cobot then approaches the bag for picking, meanwhile the controller monitors the pressure sensor value from the cobot gripper to check whether the pick has been successful. If the picking fails the cobot moves to the reset position and restarts the picking process. If the pick is successful, the cobot moves to the dropping position over the enclosure. The cobot then gives the drop command to the pi module, indicating its position over the dropping zone. The controller then activates the bottom suction solenoid valves and rotates the motor to position the suction cups below the stacks. The bottom suction will remain in this state for the rest of the cycle.\nFeedback from the ultrasonic sensor is checked to ensure the stack's position within the enclosure. If the stack is identified, feedback is given to the cobot, if not, the cobot will move back to drop position again and reattempt the placement of the stack. The pressure is checked during this time using the pressure sensors, and if it drops below a cut off value and the ultrasonic sensor still detects a stack, feedback is given which triggers the cobot to start the next feeding operation to the adjacent enclosure position. If the bottom suction pressure drop is not detected within the scheduled time, a failed state is fed back to the cobot, which moves back to the drop position to repeat the dropping again. This cycle is continued until all eight stack positions in the enclosure are filled with the bags. Once all 8 positions are filled, a finished cycle message is received from the cobot controller which then triggers the bottom suction motor to rotate to create tension on the bag. Cutting is done using the motor control interface separately. The automatic control of the system is paused until these operations are completed.\nOnce cutting is complete, the Raspberry controller publishes to start the removal of the bags, along with the turning"}, {"title": "V. EXPERIMENTAL RESULTS", "content": "To evaluate the performance of our proposed vision- based manipulation system for the autonomous cutting and unpacking of transparent plastic bags, we conducted a series of experiments consisting of 10 iterations of the complete cycle. Each cycle involved five distinct phases as shown in Fig.7: real-time bag detection and tracking, robot feeding, autonomous cutting, robotic unfolding, and autonomous delivery to the bulk loader. The following sections detail the outcomes and observations for each phase.\ni. Real-Time Bag Detection and Tracking: Vision Detection Performance. The first phase involved the detection and tracking of transparent plastic bags using Convolutional Neural Networks (CNNs). The system demonstrated high accuracy in identifying the bags under varying lighting and background conditions. Across the 10 iterations, the average detection accuracy was 96.8%, with a standard deviation of 1.2%. The tracking algorithm maintained a robust performance, ensuring continuous monitoring of the bags' positions with an average tracking error of 1.5 mm.\nTo assess the performance of the trained network we followed the standard evaluation procedure considering three metrics, namely (i) precision, (ii) recall, and (iii) F1-score, which are calculated as follows:\nprecision = $\\frac{tp}{tp+fp}$ (1)\nrecall = $\\frac{tp}{tp+fn}$ (2)\nF1 score = 2*$\\frac{precision * recall}{precision + recall}$ (3)\nIn these equations tp, fp and fn denote respectively the true positive, false positive and false negative identifications of the plastic bags. true positives were considered for the cases when the predicted and real bounding box pair has an IoU score that exceeds the imposed threshold IoU=0.5. Table III summarises the results of the bag detection performance on both the validation and test set. As the model has been trained on a targeted dataset acquired from the same environment, the inference results very high scoring on average 100% accuracy to all the experiments conducted.\nii. Robot Feeding. In the second phase, the FRANKA robot arm picked the bags one by one from the box and placed them into each enclosure of the feeding system until all eight enclosures were filled. Table IV provides numerical counts for the number of stacks detected, picked, and placed, with the maximum result for each being eight. The average success rates for picking and placing were 86.25% and 82.5%, respectively, with an overall average of seven out of eight bags successfully handled. The average time taken for the robot to complete this task was 8.3 minutes per iteration, with a standard deviation of 0.5 minutes. Challenges in this phase included the suction system's grip failures and workspace constraints, leading to collisions and placement errors. Overall, as tabulated in Table IV, the average numbers of successful picked and placed is 7 out of 10."}, {"title": "iii. Autonomous Cutting.", "content": "During the third phase, the autonomous cutting mechanism was activated to cut all eight bags. The cutting process was highly efficient, with an average completion time of 15.7 seconds per iteration and no cutting errors observed across all 10 iterations. The precision of the cuts was within an acceptable margin, with an average deviation of 0.2 mm from the intended cut lines. This phase did not experience the interdependent issues observed in detection, picking, and placing, thus maintaining consistent performance."}, {"title": "iv. Robotic Unfolding.", "content": "The fourth phase involved the robotic unfolding of each of the eight bags. The FRANKA robot arm demonstrated high dexterity and control, successfully unfolding all bags in each iteration. The average time taken for unfolding all eight bags was 38.9 seconds, with a standard deviation of 2.8 seconds. The system's compliance control with vacuum gripping technology ensured minimal damage to the bags and plastic containers during the unfolding process."}, {"title": "v. Autonomous Delivery to Bulk Loader.", "content": "In the final phase, the unfolded bags were autonomously delivered to the bulk loader by activating the pushers. The system successfully delivered all eight bags in each iteration, with an average delivery time of 18.4 seconds and a standard deviation of 1.9 seconds. The placement accuracy was consistently high, with an average deviation of 0.1mm from the target position."}, {"title": "Overall System Performance", "content": "The integrated system's per- formance across all 10 iterations was evaluated based on the cumulative time taken to complete all five phases, the accuracy of each task, and the overall reliability. The average total cycle time per iteration was recorded as 8.3 minutes, which includes the detection, picking, placing, cutting, unfolding, and delivery processes. The system demonstrated a high level of reliability, with no critical failures observed throughout the experiments.\nThe results from Table IV emphasize the interdependence of the robot's actions: a failure in detection directly impacts the picking and placing activities. For instance, in test 7, despite achieving complete success in picking and placing, full cycle success could not be attained due to detection failures. This underlines the importance of reliable detection to ensure overall process success. A supplementary video with the whole experimental procedure can be found at this link: https://youtu.be/MXxTeyBVJWg.\nThe successful testing and validation of the proposed solution in the laboratory environment with the FRANKA robot arm showcases its potential for widespread industrial applications. The system effectively automated the unpacking and cutting of transparent plastic bags for an 8-stack bulk- loader, meeting the specific requirements and demonstrating robustness under rigorous testing conditions. These results highlight the system's capability to enhance efficiency and safety in industrial processes, aligning with the Industry 4.0 paradigm."}, {"title": "VI. DISCUSSION & CONCLUSIONS", "content": "In this paper, we have presented a comprehensive and innovative system for the autonomous cutting and unpacking of transparent plastic bags in industrial setups, aligned with the principles of Industry 4.0. Leveraging advanced Machine Learning algorithms, particularly CNNs, our system success- fully identifies and manipulates transparent plastic bags using a collaborative robot arm equipped with a custom suction cup gripper and depth sensing technologies. The cutting process is facilitated by a combination of vacuum-driven suction cup grippers and a precise linear belt-driven scalpel blade. The delivery system, employing linear actuators and custom door mechanisms, ensures the smooth transition of unpackaged stacks into the bulk loader.\nDespite the achievements of our system, there are avenues for further exploration and improvement. Future work could involve enhancing the system's robustness in handling varia- tions in lighting and background conditions, refining the ac- curacy of detection algorithms, and extending the capabilities of the cutting mechanism to accommodate different types of packaging materials. Integration with more sophisticated artificial intelligence techniques and adaptive control strategies may contribute to further autonomy and efficiency in the unpacking and cutting process. Additionally, exploring the scalability of the system for diverse industrial applications and evaluating its performance in real-world scenarios will be crucial for its widespread adoption. Continuous refinement and adaptation to evolving technologies will be essential to maximize the system's potential in the dynamic landscape of industrial automation."}]}