{"title": "IMPROVED DEEP LEARNING OF CHAOTIC DYNAMICAL SYSTEMS WITH MULTISTEP PENALTY LOSSES", "authors": ["Dibyajyoti Chakraborty", "Seung Whan Chung", "Ashesh Chattopadhyay", "Romit Maulik"], "abstract": "Predicting the long-term behavior of chaotic systems remains a formidable challenge due to their extreme sensitivity to initial conditions and the inherent limitations of traditional data-driven modeling approaches. This paper introduces a novel framework that addresses these challenges by leveraging the recently proposed multi-step penalty (MP) optimization technique. Our approach extends the applicability of MP optimization to a wide range of deep learning architectures, including Fourier Neural Operators and UNETs. By introducing penalized local discontinuities in the forecast trajectory, we effectively handle the non-convexity of loss landscapes commonly encountered in training neural networks for chaotic systems. We demonstrate the effectiveness of our method through its application to two challenging use-cases: the prediction of flow velocity evolution in two-dimensional turbulence and ocean dynamics using reanalysis data. Our results highlight the potential of this approach for accurate and stable long-term prediction of chaotic dynamics, paving the way for new advancements in data-driven modeling of complex natural phenomena.", "sections": [{"title": "1 Introduction", "content": "Chaotic systems are ubiquitous in nature, encompassing fields as diverse as meteorology, fluid dynamics, and chemical reactions. They exhibit complex multi-scale dynamics, without any scale separation, making their forecasts extremely challenging. They are also characterized by their extreme sensitivity to initial conditions, meaning a small perturbation in the initial condition leads to completely diverging trajectories over time. A deterministic long-term prediction for chaotic systems is irrelevant due to their very nature. Therefore, several current works on data driven long term prediction of chaotic systems focus on preserving the invariant statistics of the system[Linot et al., 2023, Schiff et al., 2024, Li et al., 2021, Guan et al., 2024]. However, minimizing the deviations from ground truth in one autoregressive time step of prediction using a mean-squared error, typically used to optimize ML models, is not effective for long term dynamics. Recent developments have tried to tackle this limitation by several techniques like using multiple timesteps for accumulating errors before gradient computation [Keisler, 2022], including structures from governing differential equations[Linot et al., 2023] and implementing physical laws in optimization [Raissi et al., 2019].\nIn this work, we focus on the challenges data-driven ML models trained using multiple timesteps (rollouts) face for predicting chaotic systems. Gradient-based optimization used in neural networks, aiming to minimize the difference between predictions and ground truth, proves particularly difficult for such systems. The extreme sensitivity to perturbations leads to exploding gradients during optimization for any long term objective with underlying chaotic"}, {"title": "2 Methodology", "content": "Let us consider an operator S that advances the state q of a dynamical system in time. It can be considered as the theoretical solution of any underlying governing differential equation that controls the dynamics of a system. So, the state evolution can be given as,\n$q_t = S(q_{t-1}) = S(S(S(...S(q_0)))) = S^t(q_0)$\nwhere $q_t$ is the state at time t. S can be approximated by a neural network architecture $F_\\theta(q)$, depending on learnable parameters $\\theta$. The parameters are obtained by minimizing the mismatch from ground truth data (with discrete index i) given by a 1 step loss function,\n$L_1 = E_i [|| F_\\theta(q_i) - S(q_i)||]$\nThe popular multi-rollout loss function, $L_M$ used to train several state-of-the-art models is defined as\n$L_M = E_i \\sum_{t=1}^{t=n} ||\\lambda(t) (F(q_i) \u2013 S^t(q_i)) ||$\nwhere n is the number of rollouts that the training sees and $\\lambda(t)$ is a hyper-parameter that gives lower weights to mismatch in trajectories that are farther in time. Furthermore, the 'Pushforward Trick' introduced in Brandstetter et al. [2022] can be used to reduce the computational cost and induce stability by breaking the computational graph between intermediate rollouts. However, these techniques are themselves insufficient to capture the invariant metric of the underlying dynamical system for prediction of chaotic systems (Schiff et al. [2024]). The MP method introduces learnable intermediate discontinuities in the long trajectory and adds a penalty term to the rollout loss defined in Equation 3 penalizing the magnitude of the discontinuities. Therefore, the problem of extreme gradients can be overcome and the trajectory learned is stable without explicitly specifying invariant properties in the loss function as in Schiff et al. [2024] which are either unknown or computationally expensive to calculate.\nAs shown in Figure 1, we augment the ground truth loss($L_{GT}$) with a penalty loss($L_{p}$) as,\n$L_{MP} = E_i [\\sum_{t=1}^{t=sr} ||\\lambda(t)(F_\\theta(q_i) \u2013 S^t(q_i)) || + \\mu \\sum_{k=1}^{s} ||\\delta_k||]$\nwhere r is the number of rollouts before introducing a discontinuity, s is the number of splits (discontinuities) and $\\delta_s$ are the introduced discontinuities (learnable parameters). This is a modification from the previous implementations of"}, {"title": "3 Results", "content": "This section evaluates the performance of our proposed framework on two-dimensional homogeneous isotropic turbulence driven by Kolmogorov forcing, governed by the incompressible Navier-Stokes equations. These experiments aim to assess MP optimization's capabilities for improving performance of the Fourier Neural Operator. Forced two-dimensional turbulence, a classic example of chaotic dynamics, has become a standard benchmark for ML methods used in dynamical system prediction (Stachenfeld et al. [2021], Brandstetter et al. [2022], Schiff et al. [2024]). The Reynolds number $Re = 10^5$ chosen for this study. The initial condition is a randomly generated divergence-free velocity field Kochkov et al. [2021]. For more details on dataset construction, refer to the work by Shankar et al. [2023]. The trajectories are temporally sub-sampled empirically after flow reaches the chaotic regime to guarantee sufficient separation between snapshots. It can be observed in Figure 2 that the the predictions match the ground truth closely in the starting timesteps and then diverge as a property of chaotic system. However, both FNO and MP-FNO shows no sign of instability even after a high number of autoregressive rollout. The MP optimization clearly improves upon the"}, {"title": "3.2 Ocean Reanalysis Data", "content": "In this experiment we implement the MP algorithm to predict the sea-surface height (SSH), longitudinal (SSU), and meridional (SSV) velocities of the northwest Atlantic Ocean's western boundary extending from 92\u00b0W into the Atlantic 75\u00b0W in the Gulf of Mexico (GoM). For this, we have used the GLORYS version 4 [Garcia and Brown, 2021] reanalysis dataset, which is an eddy-permitting dataset at 1/12\u00b0 (8 Km). The training data (available daily) is temporally sub-sampled by a factor of 3 to keep sufficient distinction between the snapshots. We implement the MP algorithm with a UNET [Ronneberger et al., 2015] architecture and compare the predictions for a test (unseen) time period of a major GoM Loop Current Eddies (LCE) shedding event: Eddy Sverdrup (Jul 2019-Jan 2020).\nFigure 4 shows that both UNET and MP-UNET captures the dynamics of the data accurately for the time-period of the eddy event. We also found out that the vanilla UNET shows signs of instability after longer periods of time whereas MP optimization makes it stable for over 270 days while testing. However, to delve deeper into the results we compare"}, {"title": "4 Conclusion", "content": "This paper focuses on the challenges posed by the long-term prediction of chaotic systems. Our proposed method provides a modified extension of the multi-step penalty(MP) optimization framework to a broader class of deep learning models such as Fourier Neural Operators and UNETs. We demonstrate its advantage by forecasting challenging"}]}