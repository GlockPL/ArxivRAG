{"title": "Attention when you need", "authors": ["Lokesh Boominathan", "Yizhou Chen", "Matthew McGinley", "Xaq Pitkow"], "abstract": "Being attentive to task-relevant features can improve task performance, but paying attention comes with its own metabolic cost. Therefore, strategic allocation of attention is crucial in performing the task efficiently. This work aims to understand this strategy. Recently, de Gee et al. [1] conducted experiments involving mice performing an auditory sustained attention-value task. This task required the mice to exert attention to identify whether a high-order acoustic feature was present amid the noise. By varying the trial duration and reward magnitude, the task allows us to investigate how an agent should strategically deploy their attention to maximize their benefits and minimize their costs. In our work, we develop a reinforcement learning-based normative model of the mice to understand how it balances attention cost against its benefits. The model is such that at each moment the mice can choose between two levels of attention and decide when to take costly actions that could obtain rewards. Our model suggests that efficient use of attentional resources involves alternating blocks of high attention with blocks of low attention. In the extreme case where the agent disregards sensory input during low attention states, we see that high attention is used rhythmically. Our model provides evidence about how one should deploy attention as a function of task utility, signal statistics, and how attention affects sensory evidence.", "sections": [{"title": "Introduction", "content": "It is well known that paying attention to relevant features in a task aids in achieving better task performance [2-4]. However, attention, like other brain computations, may come at its own costs. For example, studies demonstrate that humans find tasks requiring sustained attention over long periods"}, {"title": "Related Work", "content": "Studies on attention. There are several dimensions of attention studied in the neuroscience community [15, 16]. In our work, we focus on one such class of attention known as sustained attention, also known as vigilance, which is the subject's preparedness to detect infrequent and unpredictable signals over extended periods [17]. However, despite the term \"sustained\" in the terminology, there is evidence in literature suggesting that attention fluctuates between optimal and sub-optimal states over time, due to factors such as arousal, mind wandering, and competing behavioral demands.[18, 19, 12]. Our work explores one such factor, frugal use of cognitively demanding attentional resource, especially how it shapes the deployment of heightened attention within trials, during a session with varying utility of attention.\nRecently, de Gee et al. [1] conducted a study on a large cohort of mice performing an auditory feature-based sustained-attention task. The study investigated how the mice modulated its pupil-linked arousal for different values of task utility. They found the pre-trial pupil sizes for high task utility trials to be close to the optimal mid-level size. Their analysis also includes fitting the data using drift-diffusion models to understand performance changes with task utility. In our work, we study the same task structure but we take an alternate route of building a normative model to understand how attention might be modulated during the trial.\nModeling approaches. Balkenius et al. [10] posited that attention should be viewed as an action rather than solely a sensory process. Several works have adopted this perspective and built sophisticated optimal control models with attention as an action choice, especially using RL, to determine where [20-22] and how often [23] to attend. The most relevant among them is [24], which examined the trade-off between the benefits of high-quality observations in decision-making and the attentional costs involved. They developed RL agents to solve an abstract model of a sustained attention task with a fixed trial duration and stochastic signal duration. In their study, they investigated how the agents' strategies evolved with changes in task parameters, focusing on the average trends across trials for agents' beliefs and attention choices. In our work, we address a different task setup where the signal duration is fixed, but the trial duration is variable and potentially long. Our analysis focuses on how attention is distributed within each trial, leading to new insights as shown in Section 5."}, {"title": "Background", "content": "Recently, de Gee et al. [1] conducted a study on a large cohort of mice (88 mice, 1983 sessions) performing an auditory feature-based sustained-attention task with intermittently shifting reward magnitude. To succeed in the sustained-attention task, the mice need to pay attention to identify whether there is a signal amid noise. However, analyzing a sound source for long periods of time"}, {"title": "Experimental approach", "content": "The study was done with 88 mice performing a total of 1983 sessions. As shown in Fig 1A, each experimental session includes multiples blocks, and each block includes multiple trials. The details of a single trial, block, and session are as follows.\nSingle trial. Each trial follows a structured sequence. Initially, a variable duration of noise, depicted by a random cloud of tones, is introduced. This duration is randomly selected from an exponential distribution with a mean of 5 seconds. If the mouse licks during this noise phase, the trial promptly concludes, imposing a 14-second time-out penalty on the mouse. However, should the mouse abstain from licking throughout this noise period, a distinct signal phase ensues. This signal phase persists for a fixed duration of 3 seconds, presenting coherent time-frequency motion (pitch sweeps) embedded within the random tonal array. Should the mouse lick during this signal phase, it is rewarded with sugar water. The trial concludes either when the mouse licks at any phase or after the signal phase elapses before the mouse licks. A trial is counted as a miss if the mouse does not lick during the trial. If the mouse licks in the noise/signal phase, then it is considered as a false alarm (FA)/hit trial.\nA single block is a set of 60 trials with inter-trial intervals featuring pink noise, which is highly discernible from the intra-trial sound stimuli. The quantity of sugar water administered as reward remains fixed across the 60 trials within each block. Each single session comprises seven blocks with rewards alternating between low (2 \u00b5L) and high (12 \u00b5L) quantities of 10% sucrose solution (sugar water), beginning with low. As in [1], we focus our analysis on the last six blocks as the mice spent the first block in each session gradually engaging in the task."}, {"title": "Mice's behavioral trends", "content": "The authors hypothesized that high sugar water reward blocks are interpreted by the animal as having a higher value for expending attention than low sugar water reward blocks. In addition, even when blocks have the same objective sugar water reward, the subjective value of food reward diminishes due to fatigue or satiety as the session progresses. According to these notions, the subjective rewards"}, {"title": "Method", "content": "Our goal is to create an agent model of a mouse that emulates the behavioral trends described in Section 3.2. We then use this model to investigate how attention is economically deployed. We make the following approximations of the actual experiment to keep the problem tractable. We approximate the continuous-time experiment with discrete time steps with a temporal resolution of 120 milliseconds. This resolution is assumed to be fine enough to make qualitative predictions for physiological/neural attention correlates. We assume the mouse's decision-making is based on optimizing its objective for a single trial as compared to entire block/session. For simplicity, we assume that at each time step, our agent decides whether to lick and whether to pay low or high attention for the next time step. The level of attention chosen determines the quality of observation in the following time step. That is, the observation at a time step is sampled stochastically depending on if the trial is in the signal or noise phase, with the degree of stochasticity being influenced by the selected level of attention. Higher attention is considered computationally expensive, but generates less stochastic observations. The observations are then used to inform its decision at the next time step, and so on until the end of the trial. The overall objective of the agent is to succeed in the task while minimizing the cost of attention allocation. Based on these assumptions, we formulate the experiment as an agent solving a partially observable Markov decision process (POMDP; Fig 2A) [25]. Below, we formally describe the elements of the PODMP.\nStates. The latent world dynamics follow a finite state machine in discrete time (Fig 2B), with one probabilistic transition but mostly deterministic transitions. Our baseline model includes a single start state, one noise state (indexed as state-0), 25 signal states (indexed from state-1 to state-25 that serve as a clock), and one terminal state. A trial begins at the fully observable start state, which then transitions deterministically to the partially observable noise state in the next time step. The fully observable start state informs the agent that the trial is beginning, analogous to how the end of the easily recognizable pink noise during the inter-trial interval informs the mice in the experiment that a new trial is starting. Once in the noise state, if the agent chose to lick, then the agent transitions to the terminal state signifying the premature end of trial. However, if the agent chose not to lick, it either remains in the noise state with a probability of 0.976 or transitions to the signal state-1 with a probability of 0.024 at the next time step. This setup approximates the signal onset time, which in the actual experiment is drawn from an exponential distribution with a mean of 5 seconds. In the experiment, the signal then persists for exactly 3 seconds, which we approximate in discrete time as a sequence of deterministic transitions. Once in a signal state, if the agent does not lick, it transitions deterministically to the next signal state until reaching the terminal state. However, if the agent licks during any signal state, it transitions deterministically to the terminal state."}, {"title": "Actions", "content": "At each time step, the agent has two actions: to lick or not, and to pay low or high attention in the next time step. The decision to lick determines the agent's next state, while the attention choice affects the quality of observation it receives in the next time step."}, {"title": "Observations", "content": "We assume the agent knows the start state and terminal state with absolute certainty. For other states, we assume that observations are binary numbers generated from a Bernoulli distribution, with probability determined by the attention level chosen in the previous step and the current latent state. Let $P_{low}$ and $P_{high}$ denote the probabilities of correctly observing the latent state for low and high attention, respectively, and $P_{high} > P_{low} \u2265 0.5$. In the noise state, observations are drawn from either Bernoulli($1 \u2013 P_{low}$) or Bernoulli($1 \u2013 P_{high}$), depending on the prior attention choice. In the signal state, observations are drawn from Bernoulli($plow$) or Bernoulli($Phigh$) similarly."}, {"title": "Rewards", "content": "If the agent licks while in the noise state, it incurs a penalty known as a false alarm (FA) cost, reflecting the 14-second time-out in the experiment. Each instance the agent chooses to allocate high attention comes with an associated attention cost. If the agent licks during any signal state, its reward is calculated as the food reward divided by the time taken until the lick. The food reward here is the subjective value of sugar water dispensed upon successful completion of the trial. Normalizing the food reward by the time taken to lick promotes faster reaction times, mirroring observations from the actual experiment. This normalization conceptually aligns with the mouse's natural inclination to acquire sugar water more swiftly. Consequently, the total reward accumulated in a trial comprises the cumulative attention cost and FA cost, or the normalized food reward, contingent on when the agent licks."}, {"title": "Beliefs", "content": "The agent's decision at any time point depends on what it believes about the current world state. For instance, if the agent strongly believes that the signal phase has started, it should lick soon to avoid missing the reward. Conversely, if the agent strongly believes it is in the noise phase, to avoid the false alarm cost it should wait to lick until it more likely has transitioned out of this phase. Since the actual states during the noise and signal phases are hidden from the agent, it bases its choices on a probability distribution over the states, which represents the likelihood of being in a particular state given all the observations up to that point. This distribution is called the agent's belief. Mathematically, this would be a vector with a dimension equaling the total number of states, each element in the vector holding the probability of being in the corresponding state. As we are dealing with a POMDP, the belief at any time point can be calculated exactly based on the current observation, the belief at the previous time step, and the actions chosen at the previous time [26]. Fig 3 shows an example of how the beliefs evolve with observations and chosen actions in a trial.\nThe agent's behavior is determined by how it chooses actions based on its beliefs. This mapping from agent's beliefs to its actions is known as the policy. Mathematically, the policy is the distribution from which the agent samples its actions, conditioned on its belief. Solving the POMDP essentially involves learning the policy that maximizes the average reward acquired over trials. We achieve this by framing the POMDP as a Markov Decision Process (MDP) in the space of beliefs and then optimizing the policy using the Proximal Policy Optimization (PPO) algorithm [27], as implemented in Stable-Baselines3 [28].The reinforcement learning training and simulation analysis were conducted on a Linux server with 28 CPU cores. The code will soon be available on GitHub. In the following section, we discuss the results obtained from our trained agents."}, {"title": "Results", "content": "We analyze our agent to gain insights into the animals' attention strategies during a trial and how these strategies vary with task utility. First, we present a single trial simulation to illustrate the agent's behavior. We then demonstrate that the trained agent's behavior aligns with observed experimental trends. Next, we examine how the agent's action choices change based on its belief in being in the signal phase and the task utility. Finally, using our model, we make empirical predictions for when the next high attention instance would be deployed, conditioned on the agent's current belief in being in the signal phase."}, {"title": "Agent's performance changes in the same direction as mice when food reward increases", "content": "Figure 3A illustrates how the agent interacts with the simulated trial. Throughout the trial, the agent maintains an internal belief representing the likelihood of being in different states. At the start of the"}, {"title": "Attention and lick propensities increase with signal belief and food reward", "content": "Now that we have an agent that conforms to the trends observed in the experimental data, we can use our model to explore how attention operates during a trial in greater detail. As illustrated in Fig 4C, the average number of high-attention time instances per trial increases as the food reward rises. This trend occurs because higher food rewards justify the expenditure of additional attention resources to enhance signal discriminability, thereby aiding in the attainment of those greater rewards.\nAt each time step, the agent selects its actions based on a policy distribution influenced by its current belief. To understand the agent's behavior, we can examine how this action distribution changes with varying belief it is in the signal phase. This belief is quantified as the sum of probabilities in the belief vector for signal nodes 1 to 25, referred to as the agent's signal belief. Fig 3B is an example demonstration how the signal belief and the different action probabilities change during a trial based on the agent's observations.\nFor this analysis, we conduct multiple trial simulations, logging the agent's beliefs and the corre-sponding probabilities of licking and paying attention as dictated by its policy. We group beliefs with similar signal beliefs together and average the attention and lick probabilities within each group to determine the average probabilities for each signal belief bin. When plotting these averages for a fixed food reward value, we observe that as the signal belief increases, the average attention probability also rises gradually (Fig 4D). This indicates that the agent is less inclined to expend attention on checking"}, {"title": "Since attention is costly, only use it when necessary", "content": "When attention is costly, we see that a temporal structure emerges in how the agent allocates its attention. For pedagogical purpose, we first analyze the simpler scenario where low attention does not yield informative observations. We then extend our discussion to the more general case where both low and high attention provide informative observations, but with different levels of information.\nInitially, we examine the scenario where the agent switches between disengaging ($P_{low}$ = 0.5) and focusing ($P_{high}$ > 0.5) during a trial. In Figs 4G and 4H, we display the attention time points across multiple trials for various food rewards and signal durations, along with their corresponding autocorrelation plots. For fixed food reward and signal duration, we observe that the initial high attention time point remains consistent across all trials. This consistency arises because $P_{low}$ = 0.5 yields observations that carry no information about the underlying latent state, thus not contributing to belief updates. However, each high attention instance provides an observation used to update the belief, influencing subsequent action choices. Even during high attention instances, the observation is randomly drawn (0.5 < $P_{high}$ \u2264 1), leading to two possible belief updates even if the latent state remains unchanged. This randomness, along with stimulus onset randomness, compounds over time, resulting in significant variability in trial dynamics after the first high attention instance."}, {"title": "Discussion", "content": "Temporal structure in correlates of attention. Our theory provides predictions on how neural and physiological correlates of attention could change with food reward. We predict the change in the overall number of high attention instances in a trial, a clear rhythmic pattern of attention instances when disengagement is an option, and, for the more general case, a distribution of attention as a function of the animal's signal belief (probed via its neural correlates).\nInterestingly, several studies found similar rhythmic structure in the way attention is deployed. For example, in visual attention tasks, VanRullen et al. [11] developed models based on psychometric curves to show attention is deployed periodically even when attending to a single target. Busch and VanRullen [12] showed electrophysiological evidence for attention facilitating perception in a rhythmic manner. Helfrich et al. [13] showed neural evidence in support of functional architecture of top-down attention being intrinsically rhythmic. Using electroencephalography measurements, Merholz et al. [14] showed that the frequency of periodic attention increased with increasing attentional demand of the task. In the auditory front, Zalta et al. [29] examined trials of individuals tasked with discerning whether a pure tone stimulus occurred on-beat or off-beat. They found the performance to be the highest for a specific beat frequency, characterizing it to be the natural occurring sampling rate of periodic attention. Shen and Alain [30] showed that temporal attention can be allocated at a particular time for short-term consolidation of auditory information. We hypothesize that the observed attention patterns could be a consequence of attention cost constraints, and could benefit from our approach in understanding the underlying principle.\nRamping Vs Jumping. Neural signals in the parietal cortex reflect the integration of sensory evidence [31]. Mean activity ramps up over time, with a slope that depends on the strength of the sensory evidence. This lead to the expectation that individual neurons encode the current estimates about decision variables, though activity on single trials is noisy making this computation hard to see. Later studies argued instead that neurons were not ramping on individual trials, but jumping from a low to high firing state [32], and that only on average did the rate ramp up. Our modeling efforts suggest a third, intermediate possibility: neural activity could ramp up in bursts, corresponding to the fluctuating strength of evidence as attention fluctuates (Fig 3B). These fluctuations should also exhibit correlations between other proxies for attention, such as pupil dilation [1] or other gain-related signals in the brain. We plan to test this prediction in future analyses.\nLimitations and future work. Our observation model and controlled attention unrealistically both allow only binary values. A continuous model for observation and attention would provide a closer approximation of the animal's sensory processing and computations. This might also expand or contract the task parameter space where rhythmic attention patterns arise. Our model also allows for fast attentional changes, but a switching cost for transitions between low and high attention states could better reflect the actual cognitive costs experienced by the animals [24]. Our analysis used the signal belief as a summary statistic to gain a broad understanding of the animal's decision-making process. Future work could delve deeper into the full belief, including uncertainty about timing. This detailed analysis could address questions such as whether animals increase their attention when they think they might be approaching the end of the trial.\nBroader impacts. Understanding the neural foundations of thought would have major impacts on human life, through neurology, brain-computer interfaces, artificial intelligence, and social communi-"}, {"title": "Conclusion", "content": "We proposed a normative attention model that maximizes task performance when there is a cost for paying attention. We developed this model for a task inspired by a recent study on an attention-based auditory foraging task, solved it using reinforcement learning, and analyzed the properties of the resultant solution. Interestingly, our model suggests that to use attention economically, blocks of low attention must be interspersed between blocks of high attention. This aligns with the rhythmic patterns of attention inferred in visual attention literature and provides a normative explanation for apparent signal neglect (attention lapses) in de Gee et al. Our results suggest principled, quantitative predictions and encourage further investigation into neural and behavioral correlates of rhythmic attention."}, {"title": "Appendix", "content": "Model performance without FA cost: We see that in the absence of a false alarm (FA) cost, with increasing food reward, the chances of a FA decreases, chances of a hit increases, and probability of misses is almost negligible (Fig 6). This is because in the absence of FA cost, there is a strong incentive to lick at some point in every trial, leading to a negligible probability for missing a trial. The decrease in FA and increase in hit with increasing food reward can be attributed to a higher permissible budget for paying attention. However, this is not the trend we observe in the experimental data. Therefore, inclusion of a sufficiently large FA cost is imperative for the model."}]}