{"title": "VidSketch: Hand-drawn Sketch-Driven Video Generation with Diffusion Control", "authors": ["Lifan Jiang", "Shuang Chen", "Boxi Wu", "Xiaotong Guan", "Jiahui Zhang"], "abstract": "With the advancement of generative artificial intelligence, previous studies have achieved the task of generating aesthetic images from hand-drawn sketches, fulfilling the public's needs for drawing. However, these methods are limited to static images and lack the ability to control video animation generation using hand-drawn sketches. To address this gap, we propose VidSketch, the first method capable of generating high-quality video animations directly from any number of hand-drawn sketches and simple text prompts, bridging the divide between ordinary users and professional artists. Specifically, our method introduces a Level-Based Sketch Control Strategy to automatically adjust the guidance strength of sketches during the generation process, accommodating users with varying drawing skills. Furthermore, a TempSpatial Attention mechanism is designed to enhance the spatiotemporal consistency of generated video animations, significantly improving the coherence across frames. You can find more detailed cases on our official website.", "sections": [{"title": "1. Introduction", "content": "With the development of generative artificial intelligence, some studies (Koley et al., 2024; Zhang et al., 2024a) have successfully automated image generation from sketches, significantly lowering the barrier for non-experts to produce high-quality drawings. However, existing methods primarily focus on static images, leaving a gap in automatically creating video animations from hand-drawn sketches.\nTo meet the needs of ordinary people for video animation creation, we propose VidSketch, a novel approach that, unlike traditional video editing methods, is based on Video Diffusion Models (VDMs) (Shi et al., 2024; Qing et al., 2024; Liu et al., 2024; Feng et al., 2024; Menapace et al., 2024) to produce high-quality video animations directly from any number of hand-drawn sketches and simple prompts. This approach removes barriers, enabling non-experts to create high-quality animations and meet diverse aesthetic needs.\nVariations in users' drawing skills make it impractical to use a single sketch control strength for VDMs during inference. Abstract sketches need lower control strength to avoid subject distortion and poor text alignment. To address this, we propose the Level-Based Sketch Control Strategy, which dynamically evaluates levels of abstraction in a sketch sequence and adjusts guidance strength during video generation, ensuring VidSketch's generalizability.\nUnlike static images, video animations require strong inter-frame consistency to prevent issues such as discontinuity. To solve this, we introduce a TempSpatial Attention mechanism, which improves the spatiotemporal consistency and fluidity of the generated video animations.\nWe evaluated the fine-tuned VidSketch model, and results show it aligns well with hand-drawn sketches while ensuring high video quality, aesthetic appeal, style diversity, and spatiotemporal consistency. Moreover, it removes professional barriers, meeting ordinary users' needs for video anima-"}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Video Diffusion Models", "content": "Video Diffusion Models (Shi et al., 2024; Qing et al., 2024; Yuan et al., 2024; Chen et al., 2023; Guo et al., 2023) have emerged as a powerful method for video generation, gaining significant attention in recent years. Building on the success of diffusion models in Text-to-Image tasks (Li et al., 2024b; Liang et al., 2024; H\u00f6llein et al., 2024; Ding et al., 2024; Zhang et al., 2024b), VDMs extend the denoising process into the temporal domain to tackle video generation complexities. Recent studies (Xing et al., 2024; Wu et al., 2023; Li et al., 2024a) have introduced innovative attention mechanisms to ensure smooth transitions and spatiotemporal consistency, further advancing VDMs development. However, generating high-quality video animations directly from hand-drawn sketch sequences and simple text prompt remains unexplored, limiting ordinary users' artistic potential. To address this, our work integrates motion priors and leverages hand-drawn sketch sequences and simple prompts to guide video animations generation, expanding VDMs applications and laying groundwork for future advancements."}, {"title": "2.2. Sketch-Guided Generation Method", "content": "Sketch-guided generation methods have gained significant attention in generative models. Early studies (Chen et al., 2023; Ye et al., 2023) mainly used sketches from concrete or original images to control the generation process but struggled with the varying abstraction levels of hand-drawn sketches. With technological advancements, some works have succeeded in generating static images from hand-drawn sketches. Studies (Voynov et al., 2023; Koley et al., 2024) have shown that hand-drawn sketches effectively serve as semantic cues to create detailed, context-rich images. Recently, automated sketch animation generation has progressed notably. For example, (Gal et al., 2024;"}, {"title": "3. Method", "content": "In this section, we first introduce the detailed implementation of our proposed Hand-drawn Sketch-Driven Video Generation method in Section 3.1. Next, we describe how to dynamically assess the abstraction level of the input sketches and control the entire video animation generation process in Section 3.2. Finally, we provide an intuitive and comprehensive explanation of the functioning of the TempSpatial Attention mechanism in Section 3.3."}, {"title": "3.1. Hand-drawn Sketch-Driven Video Generation", "content": "As shown in Figure 3 (a), our training approach adheres to the traditional VDMs framework. First, we conducted an extensive search across the internet to collect high-quality training videos for each action category, with 8\u201312 videos. Subsequently, we fine-tuned the TempSpatial Attention and Temporal Attention modules separately for each action category. This strategy effectively mitigates the challenge of limited high-quality video data, enhancing the spatiotemporal consistency and quality of the generated videos. Studies such as (Yan et al., 2023; Ku et al., 2024; Feng et al., 2024; Wu et al., 2023) highlight that the first frame of a video contains rich and detailed content information. Based on this, we modified the forward diffusion process by excluding noise for the first frame $x_{1,0}$ and applying it only to subsequent frames $x_{f,t}$ (f > 1), thereby preserving the first frame's information and ensuring spatiotemporal consistency. The selective noise addition is defined as:\n$q(x_{f,t} | x_{f,0}) = \\begin{cases} \\delta(x_{1,t} - x_{1,0}) & \\text{if } f = 1, \\\\ \\mathcal{N} (x_{f,t}; \\sqrt{\\alpha_t} x_{f,0}, (1 - \\alpha_t)I) & \\text{if } f > 1, \\end{cases}$        (1)\nwhere $\\delta$ represents the Dirac delta function, which ensures that the first frame remains completely unchanged throughout all timesteps of the diffusion process.\nMoreover, during training, our loss function focuses on"}, {"title": "noise prediction errors for frames f > 1, excluding the first frame, guiding VDMs to learn spatiotemporal dependencies for video animations generation. The loss function is:", "content": "$L = E_{f=2,..., F, t, x_0,\\epsilon} [||\\epsilon - \\epsilon_{\\theta}(x_t, f, t)||^2]$, (2)\nwhere $\\epsilon$ is the actual noise added during diffusion, and $\\epsilon_{\\theta}(x_t, f, t)$ is the noise predicted by the model.\nAs shown in Figure 3 (b), during the inference stage, the user only needs to provide a prompt and hand-drawn sketch sequences to guide the video animation generation process. The user-supplied first-frame sketch is first processed through existing techniques, for which we recommend using A(\u00b7), representing (Mou et al., 2024), to generate the initial frame, serving as a critical reference for video animation generation. During reverse diffusion, the first-frame image $x_{1,0}$ is fixed as a condition at all time steps t to ensure consistency. Subsequent frames (f > 1) are progressively generated based on the following conditional probability:\n$p_{\\theta}(x_{f,t-1} | x_{f,t}, z) = \\begin{cases} \\delta(x_{1,t-1} - x_{1,0}) & \\text{if } f = 1, \\\\ \\mathcal{N} (x_{f,t-1}; \\mu_{\\theta}(x_{f,t}, t, z), \\Sigma_{\\theta}(x_{f,t}, t, z)) & \\text{if } f > 1, \\end{cases}$        (3)\nwhere $\\mu_{\\theta}$ and $\\Sigma_{\\theta}$ represent the mean and covariance predicted by the model, respectively, and the condition z incorporates the user's prompt and sketch information.\nIn practical applications, users only need to provide any number of hand-drawn sketches to control video generation, accommodating the diverse preferences and drawing skills of different users. To meet this requirement, when users input any number of sketches, we generate intermediate frames using linear interpolation to ensure smooth transitions between video animation frames. The formula for generating interpolated frames is as follows:\n$F_t = F_i + (F_{i+1} - F_i) \\times \\frac{\\tau - \\sigma_i}{\\sigma_{i+1} - \\sigma_i}$        (4)\nWhere $F_t$ represents the t-th frame to be generated in the video animation, $F_i$ refers to the i-th frame in the input sketch sequence, and $\\sigma_i$ indicates the position of the i-th frame from the input sketch sequence in the video animation to be generated. Then, all interpolated frames are concatenated with the user-provided hand-drawn sketch frames to form the final sketch frame sequence, represented as:\n$F_{final} = [F_1, F_2, ..., F_N]$.      (5)\nNext, we map the sketch's abstraction score $S_c$ to the adapter's adjustment scale s and threshold $\\tau$, with $S_c$, s and"}, {"title": "T computed as detailed in Section 3.2.4. The adjustment scale s is applied to the adapter output (AInflated(Ffinal)), which extends the pre-trained components from (Mou et al., 2024) to the temporal dimension, controlling its influence during inference and producing the residual features R:", "content": "$R = s \\cdot A_{Inflated}(F_{final})$.    (6)\nFinally, R injects sketch-driven guidance into the current hidden states H to iteratively refine the latent representation. At each time step t, the process is defined as follows:\n$H \\leftarrow \\begin{cases} H + R & \\text{if } t \\ge \\text{threshold}, \\\\ H & \\text{if } t < \\text{threshold}, \\end{cases}$      (7)\nwhere threshold = (1 - $\\tau$) $\\cdot$ T, and T is the total number of sampling steps. Integrating R into H after $\\tau$threshold enables the model to utilize sketch information in later diffusion stages, controlling the video animation generation process and ensuring stronger sketch-to-video consistency."}, {"title": "3.2. Level-Based Sketch Control Strategy", "content": "To accommodate the significant variations in users' drawing skills, we conduct a detailed quantitative analysis of continuity, connectivity, and texture detail in sketch sequences to comprehensively evaluate the abstraction level of sketch sequences. This enables us to dynamically adjust the control strength during the video generation process. The effectiveness of the Level-Based Sketch Control Strategy can be intuitively demonstrated in Figure 4."}, {"title": "3.2.1. CONTINUITY ANALYSIS", "content": "Continuity in sketches measures the extent of filled regions and the completeness of boundaries. Higher continuity indicates a lower abstraction level. To extract continuity features, we adopt a method combining Connected Component Analysis (CCA) (Haralick & Shapiro, 1991) and contour extraction, where contour extraction utilizes the efficient Suzuki algorithm (Suzuki et al., 1985), capable of identifying hierarchical contour structures in images. To avoid noise interference during calculation, contours with an area smaller than 5 pixels and a perimeter shorter than 10 pixels are ignored. For the remaining valid contours, the perimeter is computed by approximating the contour as a pixel poly-line and summing the Euclidean distances between adjacent points. The area is calculated via the Shoelace Theorem (Lee & Lim, 2017), an efficient algebraic method for determining closed polygon areas from vertex coordinates. Therefore, the abstract continuity score Ac is computed as:\n$A_c = \\frac{S \\cdot P_{max}}{S_{max} \\cdot P}$        (8)\nwhere S is the contour area, P its perimeter, and Smax, Pmax the image's maximum area and perimeter, normalizing the score to [0, 1], with lower scores indicating stronger continuity and less abstraction."}, {"title": "3.2.2. CONNECTIVITY ANALYSIS", "content": "The connectivity of sketch sequences is measured by the number of connected components in the image. More connected regions indicate a lower level of abstraction, as they suggest a more detailed structure. In this study, we use 8-connectivity model and Depth-First Search (DFS) algorithm to traverse and label the connected regions. Starting from an unvisited pixel, DFS recursively explores its neighbors until the entire connected region is labeled. Each traversal identifies one independent connected component, determining the total number of connected regions L in the image. The abstraction score AL is calculated as:\n$A_L = 1 - \\frac{L}{L_{max}}$        (9)\nwhere Lmax is the maximum assumed number of connected components, normalizing AL to [0, 1], with lower scores indicating higher connectivity and less abstraction."}, {"title": "3.2.3. TEXTURE DETAIL ANALYSIS", "content": "The texture detail of sketch sequences provides important information about its level of abstraction. Complex and detailed textures often indicate a lower level of abstraction. To quantify texture details, this study employs the Gray Level Co-occurrence Matrix (Haralick et al., 1973) to capture the spatial relationships of pixel gray levels, providing a statistical representation of texture distribution to analyze the level of abstraction. For each pixel, its co-occurrence with neighboring pixels at specified directions (0\u00b0, 45\u00b0, 90\u00b0, and 135\u00b0) and a fixed distance is recorded to construct the co-occurrence matrix. Then, the matrix is normalized to eliminate the influence of image size on the statistical results. Subsequently, texture features like contrast (C), dissimilarity (D), and homogeneity (H) are extracted, scaled to [0, 1], and averaged to compute the texture abstraction score:\n$A_T = \\frac{(1 - C_{scaled}) + (1 - D_{scaled}) + H_{scaled}}{3}$,       (10)\nwhere AT represents the texture details score, and Cscaled, Dscaled, and Hscaled denote the scaled values of contrast, dissimilarity, and homogeneity, respectively, with lower scores indicating more texture details and less abstraction."}, {"title": "3.2.4. ABSTRACTION-LEVEL SCORE", "content": "All in all, the abstraction score Sc is calculated as a weighted sum of continuity, connectivity, and texture detail. Stronger continuity, stronger connectivity, and more details result in a lower level of abstraction, defined as:\n$S_c = w_c \\cdot A_c + w_L \\cdot A_L + w_T \\cdot A_T$,        (11)\nwhere WC, WL, and WT are the weights for continuity, connectivity, and texture detail, and Ac, AL, and AT are computed using Equations (8) to (10). Sc maps to the adapter's adjustment scale s and threshold $\\tau$ (Appendix A.4), applied in Equations (6) and (7), dynamically adjusting the sketch's"}, {"title": "3.3. TempSpatial Attention Mechanism", "content": "The primary distinction between video animation generation and image generation tasks lies in the requirement to maintain spatiotemporal consistency across video frames. To address the inherent challenges of video animation generation, as illustrated in Figure 5, we propose a TempSpatial Attention mechanism. In this mechanism, for each frame i in the video sequence, the query (Q) representation of the current frame i is used to compute the attention mechanism with the key/value (K/V) representations extracted from the first frame, the second frame, and the preceding frame (i - 1). The attention computation is formalized as:\n$\\text{Attention}(Q_i, K, V) = \\text{Softmax} (\\frac{Q_i K^{\\text{concat} T}}{\\sqrt{d_k}}) V_{\\text{concat}}$,  (12)\nwhere Kconcat and Vconcat denote the concatenated key and value matrices, which are specifically defined as:\n$K_{\\text{concat}} = [K_1; K_2; K_{i-1}] \\quad V_{\\text{concat}} = [V_1; V_2; V_{i-1}]$. (13)\nAs depicted in Figure 9 (d) (f), this mechanism effectively maintains the inter-frame consistency under identical conditions, significantly enhancing the quality of the generated video animations and better satisfying the demand for high-quality video animation production."}, {"title": "4. Experiment", "content": ""}, {"title": "4.1. Implementation Detail", "content": "It is worth noting that our training was conducted on a single RTX4090 GPU. We fine-tuned TempSpatial Attention and Temporal Attention modules using pre-trained weights from (Rombach et al., 2022), with a learning rate of 3e-5, a batch size of 1, and 15,000 training iterations. To quantitatively analyze, we used traditional metrics PickScore, MSE, and the VBench (Huang et al., 2024) tool, benchmarking against ControlNet (Zhang et al., 2023), IP-Adapter (Ye et al., 2023), and T2I-Adapter (Mou et al., 2024). To demonstrate superiority, we visually compared video quality across methods under the same prompts and conducted a user study addressing the limitations of inherent evaluation metrics. Furthermore, an ablation study validated the importance of modules in Section 3.2 and Section 3.3, reinforcing our method's effectiveness."}, {"title": "4.2. Comparisons with Baseline", "content": "Quantitative Result. In this experiment, we generated 20 video animations for each baseline to comprehensively evaluate performance. We employed traditional evaluation metrics such as PickScore and MSE, alongside metrics derived from the VBench (Huang et al., 2024) tool, including subject consistency, background consistency, motion smoothness, aesthetic quality, and imaging quality, to assess the performance of VidSketch in comparison with IP-Adapter, ControlNet, and T2I-Adapter. As shown in Table 1, VidSketch outperforms all baselines across the majority of metrics evaluated by the VBench tool, demonstrating the high quality of video animations generated by our method. Additionally, our method also performs well on traditional"}, {"title": "5. Conclusion", "content": "In this study, we introduce VidSketch, a novel and efficient method for generating high-quality video animations that break professional barriers for ordinary users in animation creation, using only hand-drawn sketches and a simple text prompt. To accommodate variations in users' drawing abilities, we introduce the Level-Based Sketch Control Strategy, which automatically determines the abstraction level of hand-drawn sketches and dynamically adjusts the control strength during the inference stage. Additionally, to address the unique inter-frame consistency requirements of video animations, we propose the TempSpatial Attention mechanism, which significantly enhances both spatiotemporal consistency and the overall quality of generated animations, as evidenced by experimental results."}, {"title": "A. Details of the Level-Based Sketch Control Strategy", "content": "To quantify the abstraction levels of sketches, this study analyzes three aspects: continuity, connectivity, and texture details. By employing mathematical models and statistical feature extraction, the geometric and textural properties of sketches are systematically characterized, and a comprehensive scoring method is constructed. Specifically, the abstraction levels are described using continuity score Ac, connectivity score AL, and texture detail score AT. These scores are normalized to the range [0, 1], ensuring consistent scales and comparability, thus providing a reliable mathematical foundation for the comprehensive evaluation of sketch abstraction levels. The following derivations and formulas supplement the detailed calculations presented in Section 3.2."}, {"title": "A.1. Continuity Calculation", "content": "Continuity reflects the integrity of filled regions and the coherence of boundaries in a sketch. To quantify continuity, connected component analysis (CCA) and contour extraction techniques are used to compute the area and perimeter of valid contours, normalized to eliminate the influence of image size.\nThe contour area S is computed using the Shoelace theorem:\n$S = \\frac{1}{2} | \\sum_{i=1}^{n-1} (x_i y_{i+1} - x_{i+1} y_i) + (x_n y_1 - x_1 y_n) |$,\nwhere $(x_i, y_i)$ represents the coordinates of the i-th contour point, and n is the total number of points.\nThe contour perimeter P is calculated by summing the Euclidean distances between adjacent points:\n$P = \\sum_{i=1}^{n-1} \\sqrt{(x_{i+1} - x_i)^2 + (y_{i+1} - y_i)^2} + \\sqrt{(x_1 - x_n)^2 + (y_1 - y_n)^2}$,\nTo normalize the results, the maximum possible area Smax and perimeter Pmax of the image are defined as:\n$S_{max} = \\text{Width} \\times \\text{Height}, \\quad P_{max} = 2 \\cdot (\\text{Width} + \\text{Height})$,\nwhere Width and Height denote the dimensions of the image.\nThe continuity abstraction score Ac is then defined as:\n$A_c = \\frac{S \\cdot P_{max}}{S_{max} \\cdot P}$,\nwhere $A_c$ represents the continuity abstraction score, S and P are the area and perimeter of the contours, and Smax and Pmax are the image's maximum area and perimeter. Lower values of Ac indicate stronger continuity and a lower level of abstraction."}, {"title": "A.2. Connectivity Calculation", "content": "Connectivity is measured by the number of connected components L in the image, where a higher number of connected regions implies a higher level of abstraction. This study adopts the 8-connectivity model and Depth-First Search (DFS) algorithm to identify connected components.\nThe image is represented as an undirected graph G = (V, E), where V is the set of pixels and E denotes the edges representing 8-connectivity between pixels. For each unvisited pixel v \u2208 V, the recursive DFS function is defined as:\n$\\begin{aligned} \\text{DFS}(v) = \\begin{cases} \\text{Terminate:} & \\text{if } v \\in \\mathcal{M}, \\text{ return;} \\\\ \\text{Mark:} & \\mathcal{M} \\leftarrow \\mathcal{M} \\cup \\{v\\}; \\\\ \\text{Recurse:} & \\forall u \\in \\mathcal{N}(v), \\text{ DFS}(u). \\end{cases} \\end{aligned}$\nwhere $\\mathcal{M}$ is the set of visited pixels, and $\\mathcal{N}(v)$ is the neighborhood of v."}, {"title": "Each DFS traversal identifies one connected component, and the total number L is computed as:", "content": "$L = \\sum_{v \\in V} \\delta(v)$,\nwhere:\n$\\delta(v) = \\begin{cases} 1, & \\text{if v starts a new DFS traversal; } \\\\ 0, & \\text{otherwise.} \\end{cases}$\nThe connectivity abstraction score AL is defined as:\n$A_L = 1 - \\frac{L}{L_{max}}$,\nwhere AL represents the connectivity abstraction score, L is the actual number of connected components, and Lmax is the maximum possible number of components. Higher values of AL indicate higher abstraction levels, corresponding to a greater number of connected regions."}, {"title": "A.3. Texture Detail Calculation", "content": "Texture details are quantified using the Gray Level Co-occurrence Matrix (GLCM) (Haralick et al., 1973), which captures spatial relationships between pixel intensities. For specified directions (0\u00b0, 45\u00b0, 90\u00b0, and 135\u00b0) and a fixed distance d, the co-occurrence matrix M(i, j) is defined as:\n$M_{norm}(i, j) = \\frac{M(i, j)}{\\sum_{i, j} M(i, j)}$,\nwhere Mnorm(i, j) is the normalized probability of intensity pair (i, j).\nTexture features are extracted as follows:\n$C = \\sum_{i, j} (i - j)^2 M_{norm}(i, j) \\quad D = \\sum_{i, j} |i - j| M_{norm}(i, j) \\quad H = \\sum_{i, j} \\frac{M_{norm}(i, j)}{1 + |i - j|}$\nwhere C, D, and H represent contrast, dissimilarity, and homogeneity, respectively.\nAfter normalization, the texture abstraction score AT is computed as:\n$A_T = \\frac{(1 - C_{scaled}) + (1 - D_{scaled}) + H_{scaled}}{3}$,\nwhere AT represents the texture abstraction score, and Cscaled, Dscaled, Hscaled are the normalized texture features. AT is defined such that:\n\\textendash (1 - Cscaled): Higher contrast (Cscaled) indicates more texture details; thus, 1 - Cscaled decreases with increasing texture details.\n\\textendash (1 - Dscaled): Higher dissimilarity (Dscaled) also indicates more texture details; thus, 1 - Dscaled decreases with increasing texture details.\n\\textendash Hscaled: Higher homogeneity (Hscaled) indicates less texture detail; thus, it directly increases with abstraction.\nTherefore, lower values of AT indicate richer texture details and lower abstraction levels, while higher values indicate smoother textures with higher abstraction levels."}, {"title": "A.4. Mapping", "content": "The sketch abstraction level Sc is quantitatively assessed and mapped to the adapter's adjustment scale s and threshold \u0442. The mapping is defined as follows, where different ranges of Sc correspond to specific values of s and \u0442, ensuring dynamic adjustment of the adapter's guidance strength:\n$(s, \\tau) = \\begin{cases} (0.55, 0.4) & \\text{if } S_c \\leq 0.5, \\\\ (0.65, 0.5) & \\text{if } 0.5 < S_c < 1.0, \\\\ (0.85, 0.6) & \\text{otherwise.} \\end{cases}$"}, {"title": "B. Details of User Study", "content": ""}, {"title": "B.1. Objective", "content": "To evaluate the performance of the proposed VidSketch method, we conduct this user stduy and compare the generated results with three classic open-source baselines: IP-ADAPTER, CONTRONET, T2I-ADAPTER. The evaluation focuses on aesthetics, spatiotemporal consistency, Sketch-to-Video consistency, Smoothness, Stability and Detail Richness."}, {"title": "B.2. Methodology", "content": "Video Generation. We select a set of 12 prompts to generate videos, including dynamic content featuring playing guitar, fish swimming, flag waving, candle burning and so on, covering a total of 10 types. For each prompt, videos are produced using four methods. This results resulted in a total of 48 videos.\nStudy Procedure. Each participant is asked to evaluate a series of videos. For each videos, participants are instructed to provide six separate scores (each ranging from 1 to 5 scale, with 1 being the lowest and 5 being the highest) based on the following criteria:\n\\textendash Aesthetics\n1 point: The video lacks visual appeal; colors are dull, and overall presentation is unattractive.\n3 points: The video has acceptable visual quality with some aesthetic elements, but lacks refinement or consistency.\n5 points: The video is highly visually appealing, with vibrant colors, smooth transitions, and a polished presentation.\n\\textendash Spatiotemporal Consistency\n1 point: The video exhibits significant inconsistencies across frames, with noticeable glitches or abrupt changes.\n3 points: The video maintains general consistency, but some minor temporal or spatial artifacts are present.\n5 points: The video demonstrates excellent spatiotemporal consistency, with smooth transitions and uniform visual elements throughout.\n\\textendash Sketch-To-Video Consistency\n1 point: The video does not follow the input sketches; the content diverges significantly from the provided sketches.\n3 points: The video partially follows the input sketches, but some elements are misaligned or missing.\n5 points: The video accurately follows the input sketches, reflecting all key elements and maintaining fidelity to the sketches.\n\\textendash Smoothness\n1 point: The video has noticeable jerky motions and abrupt transitions between frames, making it difficult to follow.\n3 points: The video has moderate smoothness, with occasional abrupt transitions or noticeable discontinuities between frames.\n5 points: The video is extremely smooth, with seamless transitions between frames and fluid motion throughout."}, {"title": "\u2022 Stability", "content": "1 point: The video exhibits frequent instability, with noticeable artifacts such as flickering, shaking, or significant changes in the visual elements.\n3 points: The video is generally stable, but there may be occasional fluctuations or minor artifacts that reduce the overall consistency.\n5 points: The video is highly stable, with no noticeable artifacts or fluctuations, maintaining a consistent quality throughout.\n\u2022 Detail Richness\n1 point: The video lacks fine details, with blurry or overly simplified visual elements that reduce the depth and complexity.\n3 points: The video contains acceptable detail, but some important visual elements may be underrepresented or blurry.\n5 points: The video is rich in detail, with clear and well-defined visual elements that provide a deep sense of texture and complexity."}, {"title": "B.3. Results Interpretation", "content": "The results are shown in Table 2. Due to file size limitations for submissions, we are now temporarily unable to publicly share the specific video examples and their scoring details involved in the user study."}, {"title": "C. Different styles of presentation", "content": "In this chapter, we extensively demonstrate the powerful generalization and generation capabilities of VidSketch across various visual styles, including pixel art, realism, fantasy, and magical styles. For each style, we present generation results for multiple action categories, with the corresponding hand-drawn sketches displayed on the left side. This allows readers to more easily evaluate the sketch-to-video consistency of our method. Based on Figures 10 to 13, it is visually evident that the videos generated by our method exhibit high aesthetic quality, strong spatiotemporal consistency, and robust sketch-to-video alignment capabilities. We encourage readers to visit our provided supplementary materials to explore more impressive examples."}, {"title": "D. Comparison with baseline methods", "content": "In this chapter, we extensively showcase the significant improvements of VidSketch compared to other baselines in terms of aesthetic quality, spatiotemporal consistency, sketch-to-video alignment, smoothness, stability, detail richness, and other aspects, as illustrated in Figures 14 and 15."}]}