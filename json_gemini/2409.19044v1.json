{"title": "On the Inductive Bias of Stacking Towards Improving Reasoning", "authors": ["Nikunj Saunshi", "Sobhan Miryoosefi", "Stefani Karp", "Shankar Krishnan", "Sashank J. Reddi", "Sanjiv Kumar"], "abstract": "Given the increasing scale of model sizes, novel training strategies like gradual\nstacking [Gong et al., 2019, Reddi et al., 2023] have garnered interest. Stacking\nenables efficient training by gradually growing the depth of a model in stages\nand using layers from a smaller model in an earlier stage to initialize the next\nstage. Although efficient for training, the model biases induced by such growing\napproaches are largely unexplored. In this work, we examine this fundamental\naspect of gradual stacking, going beyond its efficiency benefits. We propose a\nvariant of gradual stacking called MIDAS that can speed up language model train-\ning by up to 40%. Furthermore we discover an intriguing phenomenon: MIDAS\nis not only training-efficient but surprisingly also has an inductive bias towards\nimproving downstream tasks, especially tasks that require reasoning abilities like\nreading comprehension and math problems, despite having similar or slightly worse\nperplexity compared to baseline training. To further analyze this inductive bias, we\nconstruct reasoning primitives \u2013 simple synthetic tasks that are building blocks for\nreasoning \u2013 and find that a model pretrained with stacking is significantly better\nthan standard pretraining on these primitives, with and without fine-tuning. This\nprovides stronger and more robust evidence for this inductive bias towards reason-\ning. These findings of training efficiency and inductive bias towards reasoning are\nverified at 1B, 2B and 8B parameter language models. Finally, we conjecture the\nunderlying reason for this inductive bias by exploring the connection of stacking to\nlooped models and provide strong supporting empirical analysis.", "sections": [{"title": "1 Introduction", "content": "With the advent of very large deep learning models, efficient training to reduce the compute and\ntime requirements is becoming increasingly important. Along with efficient optimization procedures,\nthere has been a surge in interest to design efficient training strategies. One practical approach is\nto use smaller models to initialize larger models. Usually, this results in much faster convergence\ncompared to vanilla training [Chen et al., 2022, 2016, Gong et al., 2019, Reddi et al., 2023, Wang\net al., 2023, Li et al., 2023, Kim et al., 2023, Yao et al., 2024, Wang et al., 2024]. Stacking and\ngrowing based approaches have particularly gained traction recently. For instance, gradual stacking\n[Reddi et al., 2023] is a prominent approach where in each stage the last few layers of the model"}, {"title": "2 Problem Setup", "content": "In this section, we first present the problem setup and background material needed for this paper.\nBefore we discuss the problem setting, we set up the following notation for the rest of the paper.\nNotation. For a deep network f, we use fi and #(f) to denote the ith layer and the number of layers\nof the network, respectively. With slight abuse of notation, we use fi,b (where i, b \u2208 Z+) to denote\nthe layers between (i - 1) \u00b7 b to i \u00b7 b of a deep network f. In other words, fi,t denotes the ith block of\nb layers in a deep network f. a1:k is used to denote a sequence of k scalars {a1, ...,ak}.\nOur goal is to learn a function f : X \u2192 Y which minimizes the loss E(x,y)~D l(f(x), y), for some\nloss function l : Y \u00d7 Y \u2192 R+ \u222a {0} and data distribution D on X \u00d7 Y. We are interested in functions\nof the form f = fL \u00b0 fL\u22121 \u00b0\u2022\u25cb f\u2081 where \u25cb and L represent function composition and depth of the\nnetwork, respectively. We use F\u2081 to denote the function class consisting of functions of this form.\nGiven samples from the distribution D, we typically use an iterative stochastic optimizer (e.g., SGD)\nto learn a function that minimizes the loss. We note that the optimization procedure is inconsequential\nto the arguments in the paper. For standard training, each iteration is of the form:\nft = ft-1 + A(ft\u22121, Bt, Nt), (Standard Training)\nwhere Bt is a mini-batch from distribution D and A(ft\u22121, Bt, nt) represents the iterative optimizer\nupdate at ft-1 on Bt and learning rate nt. The computation cost and memory requirement for training\ntypically increases linearly with the depth, making even simple algorithms, like SGD, slow for very\nlarge models. Throughout this paper, we use T to denote the total number of training iterations.\n2.1 k-stage training\nSince we primarily focus on stagewise training approaches, it is useful to formally define a stagewise\ntraining procedure. In contrast to standard training, k-stage training involves dividing the training\nprocess into k stages, and at each stage, using the the model from the previous stage to initialize the\nmodel in the current stage. For simplicity, we assume L is divisible by k. The following are the key\ningredients:\n1. Function class across stages. At stage i, we use function class Fd(i) where d(i) denotes the depth\nof the network at that stage. When d(i) \u226a L, training is more efficient.\n2. Training schedules across stages. As training is divided into k stages, we use T\u2081,.., Tk steps\nacross stages such that \u2211=1 T\u2081 = \u03a4."}, {"title": "2.2 Progressive & Gradual Stacking", "content": "Progressive and gradual stacking are two special instantiations of the aforementioned framework. We\nprovide a brief description of these approaches since they are important for our discussion.\nProgressive Stacking [Gong et al., 2019]. This is simple instance of k-stage training setup where\nmodel in the previous stage is stacked onto itself to initialize the model in the next stage. In particular,\n(1) depth d(i) = 21-1d(0) grows exponentially, (2) schedule T\u2081 is typically T/k or proportional to\nd(i), and (3) the growth function M\u00bf(f) = f \u00b0 f.\nGradual Stacking [Reddi et al., 2023]. In contrast to progressive stacking, gradual stacking incre-\nmentally increases the model size where only the last L/k layers of model in the previous stage are\nstacked to initialize the model in the next stage, as follows.\n1. The depth d(i) = L\u00b7i/k grows linearly with the stage.\n2. Ti is typically either T/k or allocated proportional or exponential to depth.\n3. Mi(fd(i-1) 0\u2026\u2026\u20260 f1) = fd(i\u22121)\u2026\u2026\u25cb fd(i\u22121)\u2212(L/k)\u22121 \u00b0 fd(i\u22121) \u00b7\u00b7\u00b7 f1. This corresponding\nto stacking the last L/k layers onto the network to initialize the next stage model.\nIn the next section, we study a novel variant of gradual stacking that enables faster training and\nexhibits interesting inductive bias, which we examine carefully."}, {"title": "3 Algorithm: MIDAS", "content": "We present the MIDAS algorithm in this section. We first discuss the motivation behind this variant\nof gradual stacking and then formally define the algorithm."}, {"title": "3.1 Motivation", "content": "The motivation for MIDAS touches upon two crucial aspects: (a) the role of different layers in a\ndeep network and (b) a connection to looped models. Before delving into more technical details, it is\nimportant to illustrate these points. We present the case for MIDAS based on three observations.\nObservation 1: gradual stacking breaks the natural role of layers. Recall that gradual stacking\ninitializes a larger model model by duplicating and stacking the last block of b from the smaller model.\nThus in the newly initialized model, the second-last block of b layers will be the same as the last\nb layers of the smaller model. Intuitively, this is undesirable since the last few layers\nhave been shown to play a different role compared to other layers for Transformer models [Belrose\net al., 2023]. We further validate this in Figure 6. Thus, duplicating the last few layers can break\nthe natural role of layers at the initialization, making it a suboptimal choice. However, it is plausible\nthat the similarity structure across layers is broken after continued training and the initialization\nis inconsequential. The next observation shows that this is not true, and establishes a connection\nto looped models \u2013 networks with shared parameters between layers.\nObservation 2: gradual stacking leads to models resembling looped models. To check the\neffect of the initialization, we measure the cosine similarity between weights of layers for a model\npretrained with gradual stacking. In Figure 2b, we observe that indeed the layers continue to have\nvery high cosine similarity at the end of training, thus establishing a connection between stacking and\nlooped models like ALBert [Lan et al., 2020] and Universal Transformers [Dehghani et al., 2018].\nUnsurprisingly, the similarity structure for gradual stacking is lopsided towards the end of the model,\nwhich raises the question: Is this similarity structure natural for looped models?\nObservation 3: looped models exhibit similarity in the middle. In order to study this, we train a\nprototypical looped model, ALBert, where all layers share the same parameters. Surprisingly, despite\nparameters being shared, a natural similarity structure emerges between layers: yet again the first\nand last layers tend to be functionally dissimilar to other layers, whereas the functional similarity\nbetween layers is the highest in the middle (see Figure 2a).\nThe above observations provides a strong motivation for stacking in the middle rather than at the end,\nthus inspiring our MIDAS algorithm."}, {"title": "3.2 MIDAS algorithm", "content": "First we define the following mapping operator that is useful for stage initialization in MIDAS.\nM(f,b) = f1,6 \u00b0\uff65\uff65\uff65 \u25cb \u0192[n/2],b \u00b0 f[n/2],b 0\uff65\uff65\uff65 \u25cb fn,b, (1)\nReplication\nwhere n = #(f)/b is the number of blocks of b layers in deep network f. Note that operator M(f, b)\nexpands the size of the network by size b. Based on this operator, MIDAS can again be described\nas a simple instantiation of the k-stage training framework, as seen below. For completeness, the\npseudocode for the MIDAS in listed in Algorithm 1."}, {"title": "3.3 Experiments: UL2 Pretraining", "content": "In this section, we evaluate MIDAS for standard language model pretraining. We train a 24L decoder-\nonly model with 1.5B parameters using the UL2 objective [Tay et al., 2022] on a mixture of C4,\nWikipedia, Arxiv and Github. The observations also hold for GPT-style autoregressive language\nmodeling. To enable fair comparison, we cached the pretraining dataset and so all methods are\ntrained for the same number 500B tokens in the same order, using the same batch size (refer to\nAppendix A.1 for more details on the training setup). We pretrain models with three methods:\n(a) standard training (Baseline), (b) gradual stacking (GRADSTACK) and (c) our proposed method\nMIDAS. The goal is to compare them with respect to validation loss and downstream performance on\nseveral diverse benchmarks. Motivated by the proportional schedules from prior work, we try the\nfollowing generalized proportional schedules for gradual stacking and MIDAS.\nDefinition 3.1 (PROP-a schedule). For a total training budget of T steps, the schedule PROP-a\nspends time Ti in each stage such that T; \u221d i\u03b1 for all stages i \u2208 [k]. Thus T\u2081 = T\u22c5i\u03b1/\u2211k\u03b1i=1\nPROP-1 schedule has been found to work very well for BERT pretraining [Reddi et al., 2023]. Since\nUL2 pretraining is a harder task, we also explore less aggressive schedules like PROP-1 and PROP-2\nthat spend more time on larger models.\nEfficiency and perplexity findings. We summarize the main results in Table 1, for various stacking\nmethods and schedules. Firstly, we note that for all schedules, MIDAS has significantly better\nvalidation log perplexity than GRADSTACK at the same speedup level. This suggests that stacking\nin the middle is a lot more effective for optimization than stacking at the end of the model. With the\nPROP-2 schedule, MIDAS is 24% faster and nearly matches baseline's log perplexity. Additionally,\nwe observe that the findings are robust to the choice of block size for stacking.\nDownstream benchmark evaluations. While perplexity can serve as a decent proxy for model\nquality, there is growing evidence that it is not the best measure [Liang et al., 2023]. Downstream\nbenchmark evaluations serve as a more holistic measure for quality and are out-of-distribution\nevaluations of skills. To this effect, we evaluate MIDAS on many standard benchmarks and these"}, {"title": "4 Inductive bias of stacking", "content": "Results in Table 1 demonstrate that MIDAS not only yields training speedups, but also improves\ndownstream evaluations when trained on the same number of tokens as standard training. This\nsuggests that stacking can extract more skills out of the same data. Here, we take a closer look at\nthis improvements in downstream evaluations through the lens of an inductive bias of stacking.\n4.1 Downstream performance vs log perplexity\nA reasonable expectation from pretraining is that improvements in the pretraining objective would\ncorrelate with improvements in model quality and downstream performance. This notion of transfer\nhas even been theoretically formalized for language modeling in Saunshi et al. [2020], Arora and\nGoyal [2023]. Thus, based on this, a natural explanation for the downstream improvements of\nstacking would be that it generalizes better on the pretraining objective. However, as we see in"}, {"title": "4.2 Reasoning vs memorization for QA", "content": "For a clearer display of the inductive bias, we measure the improvements due to MIDAS on closed\nbook vs open book QA tasks. It is reasonable to assume that closed book QA tasks require strong\nmemorization abilities whereas open book QA tasks requires some reasoning abilities to infer answers\nfrom the context that is provided. On average, we see much larger improvements on open book QA\ntasks compared to closed book QA tasks, as already evident in Figure 1 and Table 1.\nMIDAS is significantly better on Open book QA. To make a direct comparison, we consider\nTydiQA-GoldP and TydiQA-NoContext tasks \u2013 the datasets are identical and the only difference is\nwhether or not additional context is provided (the answer for the contextual version is guaranteed to\nbe inferred from the given context). In Figure 3, we see that the improvements by various MIDAS\nbased models on the contextual version of TydiQA are much higher than those on the non-contextual\nversion. This provides a direct evidence of the bias of MIDAS towards improving tasks that require\nreasoning. Furthermore, we find that the memorization performance of stacking improves as the\nschedule spends more time on larger model."}, {"title": "4.3 Reasoning in math tasks", "content": "To test reasoning abilities, we evaluate the language models on various math word problem datasets\nlike SVAMP [Patel et al., 2021], ASDiv [Miao et al., 2020], AQuA dataset for algebraic word\nproblems, the MAWPS benchmark [Koncel-Kedziorski et al., 2016]. We report 5-shot evaluation for\nthe pretrained model on these tasks. Following Wei et al. [2022], we use an external calculator to\ndo the arithmetic and evaluate the models on their ability to compute the correct expression for the\nanswer. This is because small models have bad arithmetic accuracy. The choice of using calculator or\nnot does not significantly affect the trends of the results. For stacking, we use MIDAS PROP-2 model\nbecause it achieves nearly the same perplexity as the baseline model (while being 24% faster), thus,\nleading to a fair comparison based on the previous notion of inductive bias.\nMIDAS is significantly better on Math/Reasoning tasks. Detailed results can be found in Table 5.\nFor most math tasks, we observe that MIDAS based pretrained model is significantly better than the\nbaseline model, especially for the MAWPs benchmark. This provides further evidence of better math\nand reasoning capabilities of MIDAS.\nGSM8K fine-tuning. We also evaluate the 2B and 8B models on harder math problems from the\nGSM8k dataset [Cobbe et al., 2021] through few-shot prompting and fine-tuning. Full results are\npresented in Table 2. For MIDAS we use the PROP-2 model that has very similar perplexity as the\nbaseline model. We find that MIDAS has much higher accuracy after fine-tuning, thus suggesting\nthat the benefit of the inductive bias continue after fine-tuning and are not just restricted to few-shot\nevaluations. In particular, on the test set, the accuracy metric increased from 5.3% (for baseline\nmodel) to 10.4% (for MIDAS) for the 2B model (these numbers were produced by computing the\naverage score over three runs with different random seeds). Similarly the GSM8k accuracy of the 8B\nmodel improves from 12.3% to 15.2%. This suggests that MIDAS not only improves the performance\non harder math tasks, but also that the gains remain or improve after fine-tuning.\nEffect of calculator. For LLMs with less than 20B parameters, Wei et al. [2022] found that the\nmodels often solve the problem correctly but make arithmetic errors. This leads to low accuracy on\nmath word problems. Wei et al. [2022] remedied this by computing all arithmetic expressions using a\nPython program as an external calculator. In Table 2 we find that this improves the accuracy for our\nmodels too. Interestingly, we find that the gap between MIDAS and baseline gets even larger with\nthe use of calculators in almost all comparisons. We believe this is because arithmetic abilities is\ncloser to memorization for smaller models [Razeghi et al., 2022] and the use of calculator makes the\nproblem closer to reasoning, since now the model only has to infer the right expression. We believe\nthis interplay between reasoning and memorization for math problems deserves further investigation."}, {"title": "4.4 Connection to looped models", "content": "Given the nature of the growth operator in each stage, we hypothesize that stacking based models are\nclose to looped models. The layer duplication that happens at every stage ensures that blocks of layers\nstart from a common initialization. We measure the similarity between different blocks of layers by\nmeasuring cosine similarities between the parameter vectors (see Figure 2). Since looped models\nhave been conjectured to solve algorithmic problems [Giannou et al., 2023] by finding iterative"}, {"title": "5 Deep dive into reasoning improvements", "content": "To further investigate the nature of this inductive bias, we construct various simple synthetic tasks\nto help tease apart the model's capabilities. We conjecture that these simple tasks capture core\nbasic capabilities needed for contextual reasoning, and we therefore call these tasks \u201ccontextual\nreasoning primitives\". They are: induction copying, variable assignment, and pre-school math\n(PSM), discussed further below. Overall, across various few-shot evaluations and fine-tuning, we\nsee significant performance gaps between MIDAS and baseline training, suggesting that we have\nsuccessfully isolated some of the basic capabilities at which MIDAS excels relative to baseline\ntraining. We refer the reader to Appendix B for more results and the exact input format.\nPrimitive 1: Induction copying. The \u201cinduction copying\" primitive presents a sequence of words,\nfollowed by a subsequence selected randomly from within this original sequence, and asks the model\nto output the next word in the sequence. A simplified example is: \"pum nyj gdq ocu rzk jbw\nmlz eny kyx uni rzk jbw mlz eny kyx\", and the expected output is \"uni\". This primitive is\ninspired by the \"induction head\" mechanism introduced in Olsson et al. [2022], which is posited to be\nthe basic mechanism for in-context learning more generally. In Figure 5, task \"Copying\u201d, we present\nresults for 3-letter words of random letters, separated by spaces, with a sequence length of 10 and a\nsubsequence length of 5.\nPrimitive 2: Variable assignment. The \"variable assignment\u201d primitive tests the model's ability to\nassociate a value with a variable name and apply this ability compositionally, which we test by varying\nthe \"depth\" of the task. We conjecture that this ability is a core function in contextual reasoning,\nparticularly in math. An example of the depth-0 variant is \u201cu=1; t=0; v=13; y=4; f=22; y=\",\nand the expected output is 4. An example of the depth-2 variant is \"y=7; f=0; z=3; b=9; x=8;\nq=y; 1=f; m=z; h=x; a=b; n=h; j=m; t=a; i=1; g=q; n=\", and the expected output is 8.\nRefer to Appendix B for more details.\nPrimitive 3: Pre-school math (PSM). This tests the model's ability to solve a very simple \"pre-\nschool math\" problem by correctly associating multiple values and variables simultaneously and\napplying this association to a particular task. An example is \u201cz=6; b=5; i=-z+b; i=\", and the\nexpected answer (with chain-of-thought) is \"-6+5=-1\".\n5-shot evaluation results. Figure 5 presents the results for representative tasks, with more results in\nAppendix B. Overall, we see that MIDAS outperforms baseline training across all tasks. In particular,\nwe see that MIDAS is significantly stronger than baseline at Depth 0, Copying, PSM-calc, and Depth\n1, in decreasing order of magnitude of the performance gap. Depth-2 is much harder and is at random\nguessing (20%) for both models.\nFine-tuning results. Due to the difficulty of the variable assignment task at Depths 1 and 2, we\ninvestigate fine-tuning on these tasks as well. We fine-tune on a mixture of 32 depth-1 examples and\n32 depth-2 examples (i.e., only 64 examples total), using full-batch gradient descent. Figure 5 reports"}, {"title": "6 Conclusions and future work", "content": "In this work we propose a novel stacking method that outperforms previous stacking methods and\nspeeds up language model pretraining by 25-40%. In the process, we uncover a very intriguing\ninductive bias of stacking \u2013 its ability to improve downstream reasoning tasks. Through extensive\nempirical analysis, the paper makes a strong case for the presence and significance of this inductive\nbias. We believe this deserves further attention and exploration since understanding this inductive\nbias could unlock new approaches to improving model quality, reasoning in particular. The reasoning\nprimitives start to provide more insights by isolating the reasoning improvements and we hope that the\ndataset is useful for future reasoning on improving reasoning. Finally understanding the dichotomy\nbetween memorization and reasoning, and how this affects the performance on various tasks is an\ninteresting direction to pursue."}, {"title": "B.1 Exact input format", "content": "Expanding on Section 5, here we provide the format of the inputs and target outputs. The only caveat\nis that, for simplicity of presentation, we present the inputs in 0-shot form here vs. their 5-shot form.\nIn 5-shot form, which is how we conduct the 5-shot evaluations, each example is separated by two\nconsecutive newline characters.\nFor each dataset below, the inputs are separated from the targets by the \"l\" character (this is not a\ntoken in the input), and the targets are colored in red.\nFigure 5 uses the following evaluation datasets, in the following order:\n1. Copying (random-letter words)\n2. Variable assignment depth 0 (code)\n3. Variable assignment depth 1 (code)\n4. Variable assignment depth 1 (code)\n5. Variable assignment depth 2 (code)\n6. Variable assignment depth 2 (code)\n7. Pre-school math (PSM)\nCopying (random-letter words):\nFill in blank:\npum nyj gdq ocu rzk jbw mlz eny kyx uni rzk jbw mlz eny kyx\nCopying (real words):\nFill in blank:\neat fit ban sea vet zit pea cat van tea sea vet zit pea cat\nVariable assignment depth 0 (basic):\nFill in blank:\no=14\ns=4\nu=8\nm=10\nq=12"}, {"title": "Variable assignment depth 1 (basic):", "content": "Fill in blank:\ng=21\nb=24\nv=3\ns=23\nh=20\nk=b\na=s\nn=v\nf=g\nd=h\na=___\nVariable assignment depth 2 (basic):\nFill in blank:\nw=24\n1=12\nd=16\ne=5\nj=9\ng=j\ny=e\nr=1\nk=d\nh=w\nv=g\ni=r\nc=h\nt=k\np=y\nc=___\nVariable assignment depth 0 (math):\nThe following is a set of simple mathematical equations.\nn=22\nr=16\nw=13\nv=6\nk=10\nWhat is the numerical value of n?"}, {"title": "Variable assignment depth 1 (math):", "content": "The following is a set of simple mathematical equations.\nh=20\nw=9\nc=22\nj=11\nv=5\ng=c\nk=w\na=j\ns=h\no=v\nWhat is the numerical value of s?\nVariable assignment depth 2 (math):\nThe following is a set of simple mathematical equations.\ng=9\nv=24\nk=15\np=6\nc=10\nt=p\ns=g\na=c\ny=v\nn=k\nl=s\nw=n\nj=t\nm=y\ni=a\nWhat is the numerical value of j?"}, {"title": "Variable assignment depth 0 (code):", "content": "The following is a very short Python program. Use the program to resolve\nthe value of the variable in the question.\nProgram:\nq=12\nk=17\nl=1\ny=3\na=6\nQuestion:\nWhat is the value of k?\nVariable assignment depth 1 (code):\nThe following is a very short Python program. Use the program to resolve\nthe value of the variable in the question.\nProgram:\nk=11\nf=21\ne=10\nl=7\nc=13\ny=f\no=c\nr=e\nu=k\nn=l\nQuestion:\nWhat is the value of o?\nVariable assignment depth 2 (code):\nThe following is a very short Python program. Use the program to resolve\nthe value of the variable in the question.\nProgram:\nt=13\nj=14\nv=4\ns=17\ny=21\nq=j\nl=s\ne=y\nh=t\nx=v\nb=x\nf=e\nn=q\na=h"}, {"title": "Pre-school math (PSM):", "content": "Fill in blank:\nk=1\nj=8\nl=-k+j\nl=___. ->|-1+8=7\nArithmetic:\n-3+2=-1\n-6+1=-5\n+9-7=2\n-6-4=-10\n-6-1=-7\n+1+9=10"}, {"title": "B.2 Fine-tuning details", "content": "For fine-tuning, we use the \"code\" variant of the variable assignment task, depths 1 and 2, in 0-shot\nform (i.e., no in-context examples). Due to the randomness of the data generation process and\nthe rather small size of each dataset (64 examples), we randomly generate 3 different 64-example\nfine-tuning datasets (consisting of 32 depth-1 examples and 32 depth-2 examples), fine tune on each,\nand report our results as an average across the 3 runs. Table 7 reports the standard deviations as well.\nRegarding hyperparameters, we continue to use AdaFactor [Shazeer and Stern, 2018] with the same\nhyperparameters as in the pretraining phase, with the exception of learning rate and batch size. We use\na constant learning rate of 0.001, which was chosen to match the final learning rate of the pretraining\nphase. We use full-batch training with our 64-example datasets. We then evaluate performance\nseparately on depth 1 and depth 2.\nFor every step i\u2208 {200,..., 300}, chosen to be significantly after training has converged to 100%\naccuracy (we do not observe overfitting in this range as training continues), we evaluate performance"}, {"title": "B.3 Full 5-shot and fine-tuning results", "content": "5-shot. Table 6 includes 5-shot evaluation results for all contextual reasoning primitives. Rows 1, 9,\n10, 11, and 14 are the rows which appear in Figure 5.\nWhen performance is better than random guessing, MIDAS consistently outperforms the baseline in\nrows 1-11.\nFor pre-school math (rows 12-14), the value we report in Figure 5 is \"with calculator\". This is because\nthe pre-school math task actually combines two capabilities: reasoning and arithmetic. Arithmetic\ncan be thought of as a memorization task. We evaluate arithmetic for MIDAS and baseline training,\nand we see that arithmetic is quite poor for both models (7.8% and 9.6%, respectively, in Table 6).\nHowever, by evaluating PSM with chain-of-thought and only assessing the accuracy of the reasoning\nchain itself, i.e., \u201c-6+5\u201d vs. \u201c-1\u201d, we can successfully disentangle reasoning and memorization in our\nevaluation. This is equivalent to having access to a calculator, so we call it \u201cPSM with calculator\" or\n\"PSM-calc\" in Figure 5.\nFine tuning. Table 7 presents the fine-tuning results from Figure 5 along with corresponding\nstandard deviations (across the 3 trials)."}]}