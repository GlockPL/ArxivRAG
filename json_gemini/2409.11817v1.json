{"title": "EFCM: Efficient Fine-tuning on Compressed Models for deployment of large models in medical image analysis", "authors": ["Shaojie Li", "Zhaoshuo Diao"], "abstract": "The recent development of deep learning large models in medicine shows remarkable performance in medical image analysis and diagnosis, but their large number of parameters causes memory and inference latency challenges. Knowledge distillation offers a solution, but the slide-level gradients cannot be backpropagated for student model updates due to high-resolution pathological images and slide-level labels. This study presents an Efficient Fine-tuning on Compressed Models (EFCM) framework with two stages: unsupervised feature distillation and fine-tuning. In the distillation stage, Feature Projection Distillation (FPD) is proposed with a TransScan module for adaptive receptive field adjustment to enhance the knowledge absorption capability of the student model. In the slide-level fine-tuning stage, three strategies (Reuse CLAM, Retrain CLAM, and End2end Train CLAM (ETC)) are compared. Experiments are conducted on 11 downstream datasets related to three large medical models: RETFound for retina, MRM for chest X-ray, and BROW for histopathology. The experimental results demonstrate that the EFCM framework significantly improves accuracy and efficiency in handling slide-level pathological image problems, effectively addressing the challenges of deploying large medical models. Specifically, it achieves a 4.33% increase in ACC and a 5.2% increase in AUC compared to the large model BROW on the TCGA-NSCLC and TCGA-BRCA datasets. The analysis of model inference efficiency highlights the high efficiency of the distillation fine-tuning method.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, deep learning models have emerged as potent tools in medicine. They have shown outstanding performance in medical image analysis [1], disease diagnosis, and treatment planning. The emergence of large models has further promoted the application of deep learning in the medical field. In medical image processing, large models achieve more accurate feature extraction and analysis, more accurate disease diagnosis and classification, as well as better understanding and processing capabilities for complex pathological images, thus providing a more reliable basis for medical diagnosis and treatment, so large models have great application value and development potential in the medical field. The large model Virchow proposed by Vorontsov et al. [2], has 632 million parameters and surpasses state-of-the-art methods across multiple computational pathology tasks.\nHowever, despite the remarkable achievements and potential of large models in medicine, the huge number of model parameters makes it challenging to deploy these models online or on mobile devices in terms of memory cost and inference latency [3].\nIn recent years, knowledge distillation has emerged as a promising approach for training lightweight deep neural network models in computer vision tasks [4]. The core idea behind knowledge distillation is to train a compact student model to mimic the outputs, or soft labels, of a pretrained cumbersome teacher model. This method is initially introduced by Hinton et al. [5]. However, existing distillation methods have limitations when dealing with slide-level pathology images. Pathology images usually have huge resolution and are only available with slide-level label [6]. To deal with this situation, it is usually necessary to segment the whole slide image (WSI) into small instances and use a Multiple Instance Learning (MIL) [7] approach to synthesize a series of instances as a bag of samples for decision-making. However, end-to-end training on the MIL classification problem is very difficult due to the computational limitations, as the slide-level gradients cannot be backpropagated in parallel to a feature encoder with more than 10k instances of a bag [8].\nAlso, typically many large models use transformer architectures, and we need the student model to be small enough. If the student model is also a transformer architecture, although they can align features in the same feature space, it is very challenging to transfer extensive knowledge from a large model with hundreds of millions of parameters to a tiny model with millions of parameters by distillation [9]. Thus, how to improve the knowledge absorption of the student model has become an urgent problem.\nIn the domain of unsupervised domain adaptation, Liang et al. [10] proposed a distill and fine-tune two-step adaptive framework, which has been demonstrated to be effective. To address the first problem, we propose a distillation followed by fine-tuning approach as the framework of Efficient Fine-tuning on Compressed Models (EFCM). First, a compact student model is trained on feature dimensions using the unsupervised feature distillation technique in knowledge distillation. Then, we further optimize the distilled student model using an end-to-end fine-tuning strategy.\nIn order to enhance the knowledge absorption of the student model, inspired by the proposal of [11] and [12], we propose"}, {"title": "II. RELATED WORK", "content": "In this section, we present a concise review of the existing literature, focusing on three key areas: medical large models, knowledge distillation and fine-tuning."}, {"title": "A. Medical Large Models", "content": "In recent years, the field of large medical models has been booming. Large models have demonstrated great adaptability and versatility. Zhang et al. [13] introduce BiomedGPT, which can perform a variety of tasks in the biomedical domain across multiple modalities (e.g., radiographs, digital images, and text). Wu et al. [14] introduce the Radiological Fundamental Model (RadFM), which effectively fuses medical scans with natural language, demonstrating the advantages of RadFM in visual and textual information synthesis. Chen et al. [15] propose UNI, a large-scale pathology model based on self-supervised learning that outperforms previous techniques in various computational pathology tasks. However, deploying large models remains challenging due to the black-box nature of many models (accessible via APIs) and their high computational cost. Hence, alternative solutions are needed to harness the capabilities of large models for knowledge-intensive inference tasks."}, {"title": "B. Knowledge Distillation", "content": "Knowledge distillation is an effective approach for compressing models, leveraging the output logits of a pre-trained teacher model as guidance to train lightweight student models. This concept is initially proposed by Bucilu\u0103 et al. [16] and further refined by Hinton et al. [5]. Subsequent work further improves logits-based knowledge distillation through structural information, model ensembling, or contrastive learning. Recently, Huang et al. [17] introduce a distillation approach that relaxes the KL divergence loss to accommodate significant capacity disparities between teacher and student. Apart from logits, some knowledge distillation methods utilize intermediate features as hints. Yim et al. [18] employ flow-based process matrices generated from features as hint knowledge. Additionally, there are numerous other feature distillation methods utilizing various hint designs [19].\nDespite the significant performance improvement achieved by existing feature-based distillation methods, most of them use feature hints as an auxiliary to guide output prediction. However, when faced with slide-level pathological image classification, the slide-level gradients cannot be backpropagated to a feature encoder in parallel due to computational limitations."}, {"title": "C. Fine-Tuning", "content": "When fine-tuning the entire network for downstream tasks, the exponential growth of model parameters poses computational challenges, and full-parameter fine-tuning may result in a decrease in the out-of-distribution (OOD) performance of pre-trained models [20]. Consequently, some researchers explore parameter-efficient fine-tuning methods to train subsets of the model or add modules with fewer parameters while achieving comparable or even superior performance. Methods like Adapter [21] insert trainable modules (e.g., Multilayer Perceptrons (MLPs) with activation functions and residual structures) into the network to facilitate transfer learning. LORA [22] leverages low-rank updates to large-scale frozen models and introduces bypass paths to mimic fine-tuning of the entire model parameters. Despite some success achieved by LoRA and Adapter methods, there may be limitations in applicability and performance on specific tasks or datasets. There is a trade-off between compression and performance preservation in these methods. To further improve model compression, we propose to use feature distillation techniques to compress pre-trained models into smaller models while maintaining performance during the fine-tuning process."}, {"title": "III. METHOD", "content": "This section presents a novel EFCM framework that addresses the limitation that pathology image MIL classification cannot effectively backpropagate the sliding gradient to update the feature encoder parameters during end-to-end training through a two-step process of distillation and fine-tuning."}, {"title": "B. Feature Projection Distillation (FPD)", "content": "In this section, we provide a detailed exposition of the design of the student model, the TransScan module, and the distillation loss in the FPD method. Specifically, the student model in the FPD method comprises two components: the feature extractor and the projection, with the TransScan module playing a crucial role within the projection component. The distillation loss serves as a supervisory mechanism to facilitate the alignment of predicted features generated by the student model with those of the teacher model within the feature space.\n1) Design of Student Model: The original intention of FPD design is to obtain a student model with strong knowledge absorption ability in the distillation framework. We start with conventional feature distillation and improve the design of the student model. First, we retain only the shallow CNN in Vanilla Feature Distillation (VFD) as the feature extractor. Then, we construct a projection head, mainly consisting of the TransScan module. We compare the VFD and FPD methods, as shown in Fig. 2.\nThe VFD uses the soft goals generated by the teacher model to guide the training of the student model. As shown in Fig. 2(a), the training image x passes through the teacher and student models, producing the corresponding teacher feature map $F_t$ and student feature map $F_s$. Typically, differences in size and dimension between the student and teacher feature maps require the use of a projector module, usually a convolutional layer, to align them before distillation. The student model in VFD extracts features from the \"layer3\" of a pre-trained ResNet50 network and aligns them directly with those generated by the teacher model through a projector function $\\Phi_s(\\cdot)$. The projector function $\\Phi_s(\\cdot)$ is usually a fully connected layer. The aligned student feature map $F_s^\\prime$ and teacher feature map $F_t$ are represented as follows:\n\\begin{equation}\nVFD\\begin{cases}\nF_t = F_{\\text{teacher}}(x) \\\\\nF_s^\\prime = \\Phi_s(\\text{ResNet50}_{\\text{layer3}}(x))\n\\end{cases}\n\\end{equation}\nThe FPD method aims to improve the characterization of the student model. As shown in Fig. 2(b), the FPD method mainly includes the following elements. The aligned student feature map $F_s^\\prime$ and teacher feature map $F_t$ are represented as follows:\n\\begin{equation}\nFPD\\begin{cases}\nF_t = F_{\\text{teacher}}(x) \\\\\nF_s^\\prime = P \\left( F_n \\left( F_{\\text{2D}} \\left( \\text{ResNet50}_{\\text{layer1}}(x) \\right) \\right) \\right)\n\\end{cases}\n\\end{equation}\nInitially, we employ the shallow layer of a pre-trained ResNet50 network [24] trained on the ImageNet dataset [25] as a feature extractor. Following this, we append a projection comprised of a 2D convolutional layer $F_{2D}(\\cdot)$, multiple TransScans $F_n(\\cdot)$ and a fully connected layer $P(\\cdot)$. This design aims to ensure that the student model can extract shallow feature representations from raw input data and project these features into a higher-dimensional representation space to predict features more accurately.\nSpecifically, the feature extraction part is \u201clayer1\" of a pre-trained ResNet50 network. The projection part starts with a 2D convolutional layer kernel size of 4, a stride of 4, and the input feature dimensions converted from 256 to \u201cdim\u201d, which is typically set to 384. The cascading TransScan module is then set to a depth of 3. Finally, the generated features are aligned to the teacher model features using a fully connected layer $P(\\cdot)$.\n2) TransScan Module: The TransScan module comprises two key components: the transformer and the SCAN structure. The detailed structural configuration of this module is illustrated in the rightmost part of Fig. 1.\nFor a given input image with dimensions of $3\\times H \\times W$, after feature extraction using a shallow ResNet50, the output feature map has a size of $256\\times \\frac{H}{4} \\times \\frac{W}{4}$. Subsequently, the feature map undergoes 2D convolutional layer processing, resulting in a feature map X with dimensions of \u201cdim\u201d $\\times \\frac{H}{16} \\times \\frac{W}{16}$. For ease of subsequent presentation, we label the dimensional size of the feature map X as $C \\times H^{\\prime} \\times W^{\\prime}$. This feature map serves as the input to the TransScan module.\nFor the given feature map X with dimensions $C \\times H^{\\prime} \\times W^{\\prime}$, we first apply two transformations: $F_1: X \\rightarrow \\tilde{U}$ and $F_2: X \\rightarrow \\hat{U}$. Specifically, both $F_1$ and $F_2$ use convolution operations with a kernel size of 3, and the group number G is often set to 32. However, they differ in their padding and dilation settings, which have values of 1 and 2, respectively."}, {"title": "C. Fine-tuning on Distilled Model", "content": "In the fine-tuning process, we classify the fine-tuning into slide-level and patch-level based on the differences between histopathology images and other images. As shown in Fig. 1, the whole process of slide-level image data processing and distillation fine-tuning is given. The patch-level fine-tuning is relatively straightforward. The process involves adding a basic fully connected classification head to facilitate end-to-end fine-tuning and ultimately achieve the desired classification results.\nFor slide-level pathology images, it is typically necessary to use MIL to synthesize a series of instances into a bag sample for classification. Due to computational limitations, the slide-level gradients cannot be backpropagated in parallel to a feature encoder with more than 10k instances of a bag. Therefore, during the fine-tuning process of slide-level pathology images, it is necessary to sample the instances for each WSI, corresponding to Stage 3 in the EFCM framework as depicted in Fig. 1. After the instance sampling is completed, a small number of instance samples are used to perform end-to-end fine-tuning of the distilled student model [23].\nWe employs a progressive approach to compare three different strategies for evaluating the performance of distillation models. These methods comprise Reuse CLAM, Retrain CLAM, and End2end Train CLAM. CLAM is a weakly supervised learning technique that utilizes an attention mechanism. It collectively identifies a sequence of instances as bag samples to achieve accurate slide classification using MIL.\nIn the Reuse CLAM strategy, the student model acquired through distillation employs the CLAM classification head of the teacher model. In the Retrain CLAM strategy, the distilled student model needs to be frozen, and the CLAM"}, {"title": "IV. EXPERIMENTS", "content": "The purpose of this experiment is to validate the significant performance improvement and efficiency gains of the EFCM framework for slide-level classification of pathology images. We compare the FPD method with the traditional feature distillation method to validate the effectiveness of the TransScan module in distilled fine-tuning. In addition, we apply the EFCM framework to generalize verification in patch-level tasks. Finally, the generalization of the TransScan module to pre-training and parameter-efficient fine-tuning also proves to bring some improvement."}, {"title": "A. Experimental Details", "content": "Our experimental subjects comprise three large models in the medical domain: RETFound for retina [30], MRM for chest X-ray [31], and BROW for histopathology [32]. These models address crucial tasks across various medical domains.\n1) Dataset Details: Our experiment consists of a total of 11 datasets, all of which are downstream task datasets for large models and are not present in the training data of the large model. As shown in Table I, this table summarizes the dataset information in different medical fields such as retina, chest X-ray and histopathology. It includes details on classes, disease categories, and the distribution of training, validation, and test data. All downstream datasets are publicly accessible and available online.\nTo enhance the diversity of the retinal images, a set of augmentation procedures is executed, with detailed parameter configurations delineated in Table II. These augmentation processes contribute to a broader and enriched dataset of images.\n2) Distillation Training Details: In distillation training, we follow the standard practices for data augmentation by resizing input images to 224 \u00d7 224 and normalizing them through practical mean channel subtraction. We choose the AdamW optimizer [44] to adjust model parameters using the following settings: a learning rate of 1e-4, beta values of (0.9, 0.999), weight decay of le-2, and epsilon of 1e-8. For learning rate adjustment, we utilize the CosineAnnealingLR [45] as a learning rate scheduler with a warm-up step of 200, causing the learning rate to increase linearly from 0 to the initial setting of le-4 during the warm-up phase, followed by cosine annealing to adjust the learning rate smoothly throughout training. In addition, we use a batch size of 64 for parallel data processing to optimize computational efficiency. The AdamW optimizer updates model parameters based on the specified learning rate, weight decay, and other parameter values.\n3) Fine-tuning Implementation: For fine-tuning on distilled model, we initially initialize the model with the weights trained through distillation. In the fine-tuning process of the student model of FPD, the shallow ResNet50 parameters remain frozen, while other parameters are fine-tuned at a lower learning rate, typically set to 1e-5. In the fine-tuning of VFD, the learning rate is also set to le-5. The learning rate of the classifier head is usually set to 5e-3 when the classification task is performed.\nDuring the fine-tuning stage, the images are randomly cropped to 224 \u00d7 224, as well as random horizontal flipping and standardization. The training process employs a batch size of 16, and we adopt the AdamW optimizer to adjust the model's parameters, set an appropriate learning rate, and apply weight decay. To mitigate overfitting, we incorporate label smoothing to soften the true labels of the training data and adjust the output distribution. Following each epoch, the model is evaluated on the validation set, and the weights of the model with the highest AUC on the validation set are saved as checkpoints for both internal and external evaluations.\nIn the case of full-parameter fine-tuning, no parameters need to be frozen. However, during parameter-efficient fine-tuning, specific parameters need to be adjusted. In contrast to distillation fine-tuning model, both full-parameter fine-tuning and parameter-efficient fine-tuning, when combined with the classification head, utilize the same learning rate, typically set at 5e-3. Other settings can be referenced from the parameters used in the distillation fine-tuning process."}, {"title": "C. Ablation Study", "content": "In this section, our primary goal is to evaluate the impact of different loss functions and model architectures on the performance of distillation fine-tuning. We also aim to explore what hyperparameters can constitute a good TransScan module. To ensure the generalizability and reliability of our findings, we conduct all experiments on the IDRiD dataset. Our research employs a systematic approach to experimentation, replacing components or parameters of the model step-by-step to observe the effect on overall performance.\nWe experimentally explore the effect of different distillation losses on distillation fine-tuning performance, and the results"}, {"title": "D. Analysis of Model Efficiency", "content": "Optimizing model efficiency is critical in the rapidly evolving field of artificial intelligence, especially under resource constraints or stringent inference speed requirements. Researchers aim to achieve an optimal balance between model performance and resource consumption by employing a variety of techniques and strategies. This section provides a thorough discussion of the benefits and limitations of several approaches, offering readers valuable insights into large model optimization."}, {"title": "V. APPLICATIONS STUDY OF TRANSSCAN MODULE", "content": "We further deeply explore the generalization application ability of the TransScan module. The TransScan module is respectively applied to model pre-training and parameter-efficient fine-tuning to explore whether the TransScan module can also bring about performance improvement."}, {"title": "A. TransScan for Pre-training", "content": "TransScan Module can be used in pre-training by introducing it into existing transformer architecture models and replacing the original transformer of the model. For our experiments"}, {"title": "B. TransScan for Parameter-efficient Fine-tuning", "content": "We introduce the TransScan module in parameter-efficient fine-tuning and call this new method AdaptScan, which aims to use the SCAN structure to quickly adapt the large model to new tasks. We apply this method to five datasets of retinal images."}, {"title": "VI. CONCLUSION", "content": "In this study, we construct a novel framework of EFCM. The framework is initially applied to slide-level pathology image classification tasks to address the limitations of traditional knowledge distillation, resulting in significant improvements in model efficiency and performance. Subsequently, we apply the method of distillation followed by fine-tuning to patch-level image tasks, successfully obtaining small models that perform comparably to large models.\nIn the EFCM framework, our proposed FPD method plays a crucial role, with the TransScan module being instrumental. The TransScan module enhances the model's ability to handle visual tasks by adaptively adjusting receptive fields using SCAN. Additionally, when comparing the FPD method with the VFD method, we find that the distilled models obtained through FPD preserve more knowledge from the teacher model while maximizing compression. The performance and generalization ability of these compressed models exceed those obtained through the VFD method, demonstrating the potential of our approach in distillation.\nWe perform slide-level and patch-level distillation fine-tuning experiments on three large models in the medical domain. The results indicate that the FPD ETC method is the most effective slide-level distillation fine-tuning approach, achieving a 4.33% increase in ACC and a 5.2% improvement in AUC compared to the larger model in the TCGA-NSCLC and TCGA-BRCA datasets. Patch-level distillation fine-tuning enhances generalization, maintains performance, and reduces model parameters, thus enhancing its suitability for real-world deployment.\nFinally, we provide a comprehensive analysis of different model fine-tuning techniques based on various metrics such as the number of parameters, MAC, GFLOPS, and FPS, which provide valuable insights for model optimization. Further research is needed to improve model efficiency and generalization and to explore the potential of TransScan in other areas.\nOverall, our proposed distillation fine-tuning method shows promise in improving model efficiency and accuracy in various medical imaging tasks, and particularly excels in slide-level pathology image tasks."}]}