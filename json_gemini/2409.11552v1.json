{"title": "Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images", "authors": ["Armand Collin", "Arthur Boschet", "Mathieu Boudreau", "Julien Cohen-Adad"], "abstract": "Quantifying axon and myelin properties (e.g., axon diameter, myelin thickness, g-ratio) in histology images can provide useful information about microstructural changes caused by neurodegenerative diseases. Automatic tissue segmentation is an important tool for these datasets, as a single stained section can contain up to thousands of axons. Advances in deep learning have made this task quick and reliable with minimal overhead, but a deep learning model trained by one research group will hardly ever be usable by other groups due to differences in their histology training data. This is partly due to subject diversity (different body parts, species, genetics, pathologies) and also to the range of modern microscopy imaging techniques resulting in a wide variability of image features (i.e., contrast, resolution). There is a pressing need to make AI accessible to neuroscience researchers to facilitate and accelerate their workflow, but publicly available models are scarce and poorly maintained. Our approach is to aggregate data from multiple imaging modalities (bright field, electron microscopy, Raman spectroscopy) and species (mouse, rat, rabbit, human), to create an open-source, durable tool for axon and myelin segmentation. Our generalist model makes it easier for researchers to process their data and can be fine-tuned for better performance on specific domains. We study the benefits of different aggregation schemes. This multi-domain segmentation model performs better than single-modality dedicated learners (p=0.03077), generalizes better on out-of-distribution data and is easier to use and maintain. Importantly, we package the segmentation tool into a well-maintained open-source software ecosystem 3.", "sections": [{"title": "1 Introduction", "content": "Neurological disorders constitute the most prevalent cause of physical and cognitive disability and the second highest cause of death [11]. They are also a major financial burden to society, given the associated medical costs and the reduced"}, {"title": "2 Methods", "content": null}, {"title": "2.1 Data", "content": null}, {"title": "2.2 Models", "content": "Architecture and Training Details Two main criteria were considered to help decide the backbone for our experiments: an overall competitive performance and a durable implementation, to ensure support in the medium to long term. The latter is difficult to achieve, notably in the open-source community where project involvement and funding is often volatile. The nn-UNet framework [14] was selected for its consistency and popularity in the field. This project has been maintained for some years and was recently integrated into the MONAI project ecosystem [3]. As such, it seemed like the most durable option. It leverages a typical encoder-decoder U-Net architecture, which is a well-known standard for biomedical image segmentation tasks. Other alternatives were considered, including transformer-based methods [16,19], but preliminary results were not convincing and it was unclear if their implementation would still be actively"}, {"title": "Resolution-Ignorance", "content": "An important design decision was to ignore the native resolution of input images. Typically, the input images fed to the network at train and test time are resampled to a common resolution, such that the model effectively works at a fixed resolution. When training on a single domain, this is not problematic because the resampling operation required to resize the train and test images is known. However, applying this model to an arbitrary image implies an appropriate resampling to the fixed internal resolution of the network. The end user needs to apply this transformation himself, or it can be done automatically based on the acquired image resolution and model target resolution. In both cases, this operation will either downsize the image, which causes information loss, or upsize it, which is computationally inefficient. Furthermore, for aggregation purposes, resampling is a liability because our data comes from a wide range of acquired resolution (spanning 2 orders of magnitude) and converting everything to the same resolution would inevitably cause catastrophic degradation in training label quality. Our proposed model is thus resolution-ignorant, as opposed to having a fixed resolution (see [13]), but we claim its capacity is more than sufficient to efficiently generalize across scales."}, {"title": "2.3 Experiments", "content": "Two types of models are compared: dedicated learners, exclusively trained on data from a specific domain, and generalist learners, trained on aggregated data. For both experiments, we select a collection of datasets, then train a dedicated model per dataset and a generalist model on the whole collection. A visual description of our experiments is included in the appendix (see Figure A)."}, {"title": "Intra-Modality Aggregation", "content": "To study the importance of intra-modality variability on model training, the intra-modality aggregation experiment uses 3 bright-field microscopy datasets (BF1, BF2, BF3). Despite a similar visual appearance and resolution, each dataset comes from a different species (rat, rabbit, human) and the data was acquired from multiple body parts (peripheral nervous system, brain, muscle). Additional variability comes from pathologies. Dedicated learners were trained on each dataset separately and a generalist model was trained on the concatenation of all three: BF_AGG."}, {"title": "Inter-Modality Aggregation", "content": "The second and most important experiment targets the impact of inter-modality variability on model performance. As such,"}, {"title": "3 Results and Discussion", "content": "We report Dice scores for all experiments in heatmaps, where every row represents a target dataset and every column a model trained on the specified source dataset. All Dice values presented are obtained by ensembling the 5 folds of the cross-validation scheme. Results for both axon and myelin classes are presented. In 3.2, the generalist model is applied to unseen data."}, {"title": "3.1 Intra- and Inter-Modality Aggregation Results", "content": null}, {"title": "3.2 Out-of-Distribution Generalization", "content": null}, {"title": "4 Conclusion", "content": "Our proposed generalist model produces better segmentations than single modality learners on in-distribution and out-of-distribution images. Our work shows that although intra-modality aggregation is useful, inter-modality data aggregation is the most beneficial. Our strategy is more sustainable than maintaining multiple dedicated systems, and leads to a single easy-to-use model. Models trained on aggregations BF_AGG and FULL_AGG are publicly available. We hope this project facilitates both the workflow of neuroscience researchers and the medium- to long-term maintenance of the method."}]}