{"title": "KidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions", "authors": ["Mir Tafseer Nayeem", "Davood Rafiei"], "abstract": "Recent studies highlight the potential of large language models in creating educational tools for children, yet significant challenges remain in maintaining key child-specific properties such as linguistic nuances, cognitive needs, and safety standards. In this paper, we explore foundational steps toward the development of child-specific language models, emphasizing the necessity of high-quality pre-training data. We introduce a novel user-centric data collection pipeline that involves gathering and validating a corpus specifically written for and sometimes by children. Additionally, we propose a new training objective, Stratified Masking, which dynamically adjusts masking probabilities based on our domain-specific child language data, enabling models to prioritize vocabulary and concepts more suitable for children. Experimental evaluations demonstrate that our model excels in understanding lower grade-level text, maintains safety by avoiding stereotypes, and captures children's unique preferences. Furthermore, we provide actionable insights for future research and development in child-specific language modeling.", "sections": [{"title": "Introduction", "content": "Children constitute one in three internet users globally, according to a UNICEF study (Keeley and Little, 2017), with the average screen time for kids aged 8-12 estimated to be over five hours per day (Rideout et al., 2022). This level of digital engagement presents both opportunities and challenges for enhancing children's learning experiences. Large Language Models (LLMs) have significantly lowered the barriers to building educational tools and applications (Huber et al., 2024), with some studies suggesting these models enhance children's learning by facilitating engaging and emotionally responsive conversations (Seo et al., 2024b) and supporting visual programming learning (Chen et al., 2024). Despite these opportunities, there are notable risks associated with (1) the bias and toxicity of language models (Deshpande et al., 2023), stemming from the vast, unvetted data they are trained on (Longpre et al., 2024), (2) a lack of sufficient contextual appropriateness to engage children (Seo et al., 2024a,b), and (3) the challenge of maintaining lexical simplicity that is appropriate for the children (Valentini et al., 2023). These challenges highlight the necessity for a safer and more reliable approach to designing and auditing LMs to vulnerable populations like children. This paper investigates whether a language model for kids can be constructed with desirable features such as safety, contextual appropriateness and simplicity built into the language model.\nTwo dominant approaches for adapting language models to a specific domain, task, or language are continual pre-training and instruction tuning or supervised fine-tuning (SFT). LLMs rely on large-scale self-supervised pre-training on Internet text data, as described by (Brown et al., 2020), and decoder-only LLMs use a causal language modeling objective to predict the next token based on previous tokens (Bengio et al., 2000). Continual pre-training involves further training a pre-trained language model on additional data relevant to a specific domain or language, such as Biomedical (Bolton et al., 2024), Mathematics (Azerbayev et al., 2024), or languages like those in Southeast Asia (Dou et al., 2024). SFT, on the other hand, trains a language model with specific instructions or guidelines to align with specific tasks (Wei et al., 2022) and user preferences via RLHF (Ouyang et al., 2022), using data consisting of pairs of instructions and their corresponding desired outputs. A key component of both continual pre-training and SFT is the existence of high-quality data, whether synthetic or human-annotated (AI et al., 2024; Liu et al., 2024). However, annotators for SFT data are predominantly from the age group 18-35 (Table 1), whose distinct linguistic and cognitive preferences, as well as safety needs, differ significantly from those of children. For example, annotators on Amazon Mechanical Turk (MTurk) must be at least 18 years old. Consequently, the SFT data may not adequately address the unique requirements of younger users. This limitation prompts an intriguing question: Can a language model be developed specifically for a particular user group, such as children in our case?\nLanguage models for children are expected to possess three essential properties: (1) the ability to generate simpler words and understand lower grade-level texts, (2) free from any stereotypes (Bozzola et al., 2022), and (3) the capacity to model children's unique preferences and emotions for personalized engagement. We argue that achieving these properties simultaneously in a language model necessitates the use of high-quality pre-training data. Modern LLMs typically pre-train on corpora containing hundreds of billions to several trillions of tokens from vast internet text data (Touvron et al., 2023; Penedo et al., 2023). Two often disregarded aspects of this text data are: (i) the demographics and intentions of its creators, and (ii) the intended audience for whom it was written. Both factors can significantly influence the composition and distribution of the data, and consequently, the resulting behavior of a user-centric language model (e.g., children).\nWith the aforementioned requirements for language models tailored for children, we curated high-quality, kid-appropriate content specifically written for children and occasionally by them. This content was meticulously reviewed and validated by website editors or moderators to ensure its suitability and the absence of inappropriate content or sensationalism. Our data collection pipeline is comprehensive, diverse, and appropriately tailored for children's language models, while also being scalable to support the accumulation of more sources for future development. Given the size of our collected pre-training data and available resources, we opted to train a masked language model (MLM) to validate the corpus quality and ensure support for the kid-specific properties discussed above. This model introduces the stratified masking method, which offers a way to prioritize words relevant to children and is also applicable in low-resource learning scenarios. Furthermore, we offer suggestions for future directions to extend our findings. Our main contributions are summarized as follows:\n\u2022 We propose a user-centric data collection pipeline to curate high-quality data specifically written for, and occasionally by children, validated by website editors (\u00a72.1).\n\u2022 We introduce a novel stratified masking technique for training an MLM on our KidLM corpus and validating the smooth integration of kid-specific properties into the LM (\u00a72.2.1).\n\u2022 Our KidLM models effectively understand lower grade-level texts and show a reduced likelihood of reinforcing negative stereotypes and generating toxic completions across 151 social groups in 8 categories (\u00a73)."}, {"title": "KidLM Construction", "content": "Our aim for KidLM is to create language models tailored for children by developing a high-quality, user-centric corpus. This involves meticulous data collection and verification to ensure reliability and relevance, along with a novel masking process to enhance the model's focus on kid-specific words."}, {"title": "KidLM Corpus", "content": "Our corpus collection pipeline is designed with a user-centric approach to ensure high-quality, kid-appropriate textual data (Figure 2). The process includes several stages, as outlined below:\nUser-Centric Our goal is to curate a high-quality corpus of textual data specifically written for children and, occasionally, by them. This content undergoes thorough review and validation by website editors or moderators to ensure its suitability, appropriateness, and absence of sensationalism or inappropriate material. Our user-centric approach to data collection carefully considers two critical aspects: (i) the demographics and intentions of the content creators (\u201cWho?\u201d), and (ii) the intended audience for whom the content is written (\"Whom?\").\nSource Identification The initial phase of our data collection methodology involved using Google Search to identify a preliminary set of websites, denoted as X = [Time for Kids, News for Kids, Kids Press]. Subsequently, we employed ChatGPT, prompting it with \u201cList websites similar to Xi that offer kid-specific content\u201d, to expand our list. This process yielded an additional collection of relevant websites, which were then merged with the initial set X. Finally, we utilized SimilarWeb 4, a web analytics tool, to further extend our list. Specifically, we used the \"Similar Sites\" feature of SimilarWeb to identify analogous sites.\nManual Data Verification We manually verified and filtered the data sources by reviewing the \"about\" sections of the identified source websites, as detailed in Tables [15, 16, 17] (Description column) of the Appendix.\nQuality Filtering Articles were filtered based on specific criteria, depending on the availability of information from the sources, such as (1) Extracting articles tagged specifically for children, (2) Identifying those labeled as \u201ckidspost\u201d, (3) Excluding articles tagged as potentially inappropriate content with colors such as red, and (4) Selecting data relevant to specific grade levels (K-1, 2-3, 4-5, and 6).5 These criteria are further explained in Tables [15, 16, 17] (Additional Notes column) of the Appendix.\nAdditional Filtering We included only English text and removed sentences involving code-mixing and code-switching. Additionally, we eliminated any Personal Identifying Information (PII) from the corpus. Details of these processes are provided in Appendix A.\nData Diversity To ensure genre diversity, the corpus includes articles on science, sports, history, animals, geography, technology, current events, book reviews, and more, all tailored to meet the interests of young readers. We collected data from 21 sources originating from various regions: USA (4), India (4), Canada (3), Australia (1), UK (1), New Zealand (1), and other global sources (7), aiming to avoid geographic and cultural biases (detailed in Tables [15, 16, 17] of the Appendix).\nData Quantity Our KidLM corpus contains over 286,000 documents, approximately 2.91 million sentences, and 50.43 million words. Upon processing with the RoBERTa tokenizer (Liu et al., 2019), this amounted to approximately 67.97 million tokens."}, {"title": "KidLM Models", "content": "We use our KidLM corpus to develop language models tailored for children. Given the corpus size and available resources, we opt to train an MLM to validate corpus quality and ensure support for kid-specific properties. Our model has two variations (1) KidLM: We continue to pre-train RoBERTa (Liu et al., 2019) using our KidLM corpus (\u00a72.1) with an MLM learning objective, which involves randomly masking 15% of the input sequence's words to predict these masked words from their context. (2) KidLM+: This version introduces a novel masking strategy called Stratified Masking, varying the probability of masking based on word classes. This approach enhances the model's focus on tokens that are more informative and specifically tailored to children, making it particularly useful for low-resource learning scenarios where the pre-training corpus is relatively smaller and designed to inject specific properties into the language model."}, {"title": "Stratified Masking", "content": "We aim to steer LM predictions towards kid-specific words from our high-quality corpus. To achieve this, we introduce Stratified Masking based on two principles: (1) all words in our corpus have a non-zero probability of being masked, and (2) words more likely to be found in a general corpus are masked with lower probability. With these principles, each word in our corpus is assigned to one of the following three strata:\nStopwords which are generally the most frequent words in a language. Utilizing NLTK's list of 179 stopwords (Bird, 2006), we apply a 0.15 masking rate to these words. Our hypothesis for masking is that children use stopwords distinctively, often in reference to specific nouns like 'cars', 'trains', and 'butterflies'. Additionally, many pronouns such as 'he', 'she', 'his', and 'her' are categorized as stopwords. By masking them, we aim to learn debiased representations from the data during pre-training.\nDale-Chall Easy Words List comprises 2950 words that are reliably understood by students (Chall and Dale, 1995). Of these, 4.85% overlap with stopwords, which we subsequently remove. We then mask the remaining 2807 words at a slightly higher masking rate of 0.20 to prioritize the linguistic simplicity specific to children.\nOther Words In our KidLM corpus (\u00a72.1), it is unsurprising that stopwords are dominant, accounting for 45.93%, while Dale-Chall Easy words make up 21.82%, and other words constitute 32.45%. We assume that these 'other words' often include nouns and entities, reflecting children's preferences or safe alternatives introduced by website editors or moderators. Consequently, we assign them a higher masking rate of 0.25 to emphasize their informative importance during training. Formally, given a text sequence, the model generates a masked text TM by applying the following procedure to each token xi:\n$T_{M}(x_{i}) = \\begin{cases} [MASK] & \\text{with prob. 0.15 for stopwords} \\\\ [MASK] & \\text{with prob. 0.20 for DC easy words} \\\\ [MASK] & \\text{with prob. 0.25 otherwise} \\end{cases}$\nThe model is then trained to minimize the loss:\n$L_{MLM} = \\frac{1}{n} \\sum_{i=1}^n log p(x_i | T_M)$  (1)\nwhere @ is the parameters of the model. We utilized the pre-trained checkpoint of the ROBERTa"}, {"title": "Evaluation", "content": "We evaluate our KidLM models based on the following two criteria: (1) How well does KidLM understand lower grade-level texts (\u00a73.1)? (2) How robust is KidLM in maintaining safety standards by avoiding the generation of stereotypes (\u00a73.2)? We compared our model with base LMs to ensure a fair and consistent comparison, highlighting the impact of our high-quality pre-training data."}, {"title": "Evaluating on Grade-Level Texts", "content": "Our objective is to compare various language models against our KidLM models. We employ Perplexity (PPL) as an evaluation metric, which measures the uncertainty of a language model when predicting the next word in a sequence (Radford et al., 2019; Salazar et al., 2020). A lower perplexity score indicates that the model is more confident and accurate in its predictions, suggesting a better understanding of the language and context (Bengio et al., 2000). To assess this, we use texts across different lower grade-levels, allowing us to measure how well each model handles the linguistic, syntactic, and semantic simplicity of texts. The holdout Newsela Corpus (Xu et al., 2015) is used for this purpose. We randomly selected 40 documents for each of the lower grade-levels, such as 2nd, 3rd, and 4th grades, and segmented these documents into sentences to compute sentence-level perplexity scores (for holdout test data statistics, refer to Table 2).\nResults & Analysis As shown in Table 3, general-purpose LLMs demonstrate decreasing perplexity as grade levels increase, indicating less uncertainty in predicting relatively more complex texts. At the 2nd grade level, perplexity values are highest across all these LLMs, highlighting the difficulty in comprehending simpler texts. The Llama family models show that more training data doesn't always improve performance with simpler texts. For example, Llama 2, trained on 2 trillion tokens, and Llama 3, trained on 15 trillion tokens, illustrate this point, suggesting a need for more user-centered training data. In contrast, our models, KidLM and KidLM+, show a reversing trend with generally less uncertainty in predicting lower grade levels and consistently less uncertainty across all grade levels, demonstrating their effectiveness in understanding simpler language. Further, we present a qualitative analysis of our model outputs in generating simpler words within a given context (\u00a74)."}, {"title": "Evaluating Stereotype", "content": "Our evaluation involves querying a range of pre-trained LMs, LLMs, and debiased LMs, using a structured prompt: \u201cwhy are {group} so ...\u201d. While Baker and Potts (2013) utilized prompts such as 'why do {group}', 'how do {group}', and 'what do {group}' to effectively elicit stereotypes, our prompt is specifically designed to extract reasons associated with a group's characteristics, focusing on adjectives. The placeholder \"{group}\" is replaced with terms from a broad range of social groups. For selecting these groups, we referred to lists from (Choenni et al., 2021) and the StereoSet (Nadeem et al., 2021), which are commonly used in assessing stereotypes in LMs. Following (Leidinger and Rogers, 2023), we merged and reorganized these sources to create a comprehensive list of 151 social groups, categorized into 8 distinct categories (Appendix (Table 11)). Our evaluation encompasses a wider range of social groups for thoroughly analyzing stereotypes in LMs.\nMasked LMs (MLMs) and Debiased LMS For MLMs, such as RoBERTa, KidLM (ours), and KidLM+ (ours), we prompt the models with \u201cwhy are {group} so [MASK]\u201d to predict completions for the [MASK] position. We also compare our models with two debiased models where debiasing occurs during the pre-training stage, (1) Auto Debias (Guo et al., 2022), and (2) Context Debias (Kaneko and Bollegala, 2021). Since these models are debiased MLMs, we employed the same prompt settings designed for MLMs.\nCausal Language Models We compared our models with several open-sourced causal language models such as GPT-2 (base and large) (Radford et al., 2019), Llama 2 (7B and 13B) (Touvron et al., 2023), Llama 3 (8B), and Mistral 7B (Jiang et al., 2023). We employed the prompt \"why are {group} so\" as a context to generate words that reflect stereotypical reasoning or biases. This method offers insights into potential biases embedded within their training data and predictive algorithms. We configured the generator to produce a maximum of one token by setting the parameter max_new_tokens = 1. We filtered out the completions that were duplicates, non-words, less than three characters, grammatically incorrect, or non-adjectives (e.g., \u201cso often\u201d, \u201cso sure", "so far\", \"so much\", \"so into\u201d, \u201cso so": "etc). We opted not to compare our models with closed-source models, as detailed in Appendix C.\nEvaluation Method To thoroughly analyze the stereotypical biases present in LMs, we sampled multiple completions (n=5) for each LM for each group, ranking them according to the probability of completions. Our analysis includes a total of 151 groups, divided into 8 categories.\nSocial bias or stereotypes in language models manifest as prejudiced outputs that associate target concepts to demographic groups (Gallegos et al., 2023). To evaluate these stereotypes, we analyze sentiment and toxicity scores of model completions, a common method in assessing stereotypes in language generation (Blodgett et al., 2020; Nadeem et al., 2021; Aky\u00fcrek et al., 2022; Deshpande et al., 2023; Liang et al., 2023). Toxicity refers to offensive, harmful, or discriminatory language (Kiritchenko et al., 2021), while sentiment reflects human perceptions, attitudes, and emotions (Ekman and Davidson, 1994). Notably, content from humans may display more pronounced stereotyping, as observed through negative sentiments or increased toxicity (Liu, 2024)."}, {"title": "Analysis", "content": "In this section, we provide a qualitative analysis of our model outputs in two key settings. First, we assess the preferred lexical simplification within context compared to human labels. Second, we design probe tests categorized into diverse types (Table 7 of Appendix) to analyze the models' ability to capture and reflect children's unique preferences, emotions, and wishes. These analyses aim to highlight the impact of our corpus and the effectiveness of our stratified masking procedure in generating contextually preferred responses for children.\nTo structure the analysis, we employ the \"cloze test\" (Taylor, 1953) to design queries, where certain words in a query are masked, and the model's task is to predict or fill in these blanks. Formally, Let Q = {q1, q2, ..., qk} represent a set of probe queries, where each query qi is a sentence with one or more masked positions. Each query can be represented as:\n$q_i = {W_1, W_2,\u2026\u2026, [MASK], \u2026\u2026\u2026, w_n}$ (2)\nwhere wj is a word or a token in the query, [MASK] represents the masked position(s), and N is the total number of words in the sentence. A LM, M, is employed to predict plausible words for each masked position. For each masked position in query qi, the model outputs a probability distribution over a predefined vocabulary V. This probability distribution is denoted by P(v|qi, M), representing the probability of a vocabulary word v \u2208 V being a plausible completion at the masked position in qi. The objective is to identify the top K most likely words from V, this set of words is represented as TopK(qi) and is defined as:\n$TopK(q_i) = \\underset{v \\in V}{argmax_K} P(v | q_i; M)$ (3)\nLexical Simplification involves replacing a word in context with a simpler alternatives (Paetzold and Specia, 2016). To analyze the ability of our KidLM models to generate simpler words within a given context, we utilized the TSAR-EN dataset (\u0160tajner et al., 2022), annotated by MTurk annotators who are required to be at least 18 years old. For each sentence, we selected the annotated complex word (highlighted in bold in Table 5), replaced it with [MASK], and then probe LMs to generate words for the masked position and rank them according to their output probability. While human annotators, influenced by their age (over 18), tend to list simpler synonyms of the known complex word, our KidLM+ model excels in generating simpler, preferred, and stereotype-free completions. This behavior can be attributed to our proposed stratified masking procedure. More"}, {"title": "Discussion and Future Directions", "content": "Pre-training Data Decoder-only LLMs operate on a causal language modeling objective, learning to predict the next token based on the sequence of previous tokens (Touvron et al., 2023; Penedo et al., 2023). Consequently, they may require significantly more pre-training data compared to our current KidLM corpus. On a positive note, our user-centric data collection pipeline is not only comprehensive but also extensible, allowing continuous integration of new sources to expand our corpus. Additionally, quality filtering and controlled repetition of available data, as shown in recent studies (Muennighoff et al., 2023), can significantly enhance the performance of LLMs in data-constrained settings.\nAlignment to Children Base LLMs pre-trained with unsupervised text corpora are typically inadequate as open-domain conversational assistants. Fine-tuning is essential, but using existing SFT data can compromise the kid-specific properties developed during pre-training stage (Table 1). Furthermore, MTurk is unsuitable for collecting such data due to age demographic restrictions. Recent studies demonstrate that a small set of examples (e.g., 1,000) can achieve significant alignment performance (Zhou et al., 2023). Another study highlights that base LLMs and their alignment-tuned versions perform nearly identically (Lin et al., 2024), with base LLMs achieving effective conversational alignment purely through in-context learning (ICL). These studies support our hypothesis that high-quality, user-centered pre-training data is essential for developing kid-specific LMs.\nHuman-Centered Evaluation Current LLM evaluation methods focus on developing datasets and benchmarks (Liang et al., 2023; Chang et al., 2024) but often fail to address the 'sociotechnical gap' (Weidinger et al., 2023). Assessing models in isolated 'lab settings' limits the incorporation of human factors (Ibrahim et al., 2024). Human-Computer Interaction (HCI) offers diverse metrics to meet the evaluation needs of different stakeholders (Damacharla et al., 2018). Interdisciplinary research between HCI and NLP is essential for responsible, human-centered evaluation and auditing of LLMs (Xiao et al., 2024). As a potential research direction, we suggest an evaluation framework that integrates insights from both fields. This process may involve various stakeholders at different stages: (1) Pre-deployment (e.g., educators, psychologists, parents), and (2) Post-deployment (e.g., children, parents, educators)."}, {"title": "Related Work", "content": "Children and Language Technology Prior studies from the HCI community have explored how technology can support children in learning and sharing their emotions (Santos et al., 2020; J. Ryu et al., 2021), as well as enhancing parents' awareness of their children's emotional well-being (Pepping et al., 2020). These studies demonstrated that chatbots and tangible artifacts can accurately detect children's emotions and promote emotional regulation. However, they often overlook children's perceptions and preferences regarding emotional communication (Seo et al., 2024b) and are limited by the technical constraints of rule-based chatbots (Seo et al., 2024a). LLMs have simplified the development of educational tools and applications (Huber et al., 2024). Research suggests these models can enhance children's learning through engaging, emotionally responsive interactions (Seo et al., 2024b) and support visual programming (Chen et al., 2024). However, significant risks include bias and toxicity from unvetted datasets (Deshpande et al., 2023), insufficient contextual appropriateness (Seo et al., 2024a,b), and difficulty in maintaining lexical simplicity suitable for young users (Valentini et al., 2023). These challenges highlight the need for child-specific LMs with built-in safety, contextual relevance, and simplicity."}, {"title": "Masking Strategies & Rates", "content": "EntityBERT (Lin et al., 2021) employs a masking strategy that targets \"entities\" identified by a domain-specific pre-trained named entity recognizer (NER) model. Similarly, Salient Span Masking (Guu et al., 2020) uses an NER model to mask entities for open-domain QA tasks. Both methods rely on a domain-specific NER, and their masking strategy is consistent across any applied domain. In contrast, Selective Masking (Gu et al., 2020) tailors token masking during continued pre-training based on data and labels from the downstream task. Meanwhile, Difference Masking (Wilf et al., 2023) automatically selects tokens for masking by identifying unique anchor words in the target domain data, distinguished from the general domain using a TF-IDF-like scoring function. Wettig et al. (2023) found that a 15% masking rate is not universally optimal for MLMs, suggesting that larger models should adopt a higher rate when pre-training from scratch. Moreover, Yang et al. (2023) introduced time-variant masking, adjusting the masking rate at different training stages to enhance pre-training efficiency. Our method, on the other hand, groups words into classes or strata, with our novel Stratified Masking adjusting masking probabilities based on the strata to which they belong. This enhances the model's focus on tokens that are more informative and specifically tailored to children, facilitating the smoother integration of kid-specific properties into the language model. Unlike other methods, our approach does not depend on any external models, task-specific signals, custom vocabulary, or a fixed masking rate for all tokens. The works related to domain adaptation of LMs are in Appendix D."}, {"title": "Conclusion", "content": "In this paper, we take the important first steps toward designing child-specific language models to make NLP systems more accessible to children. We curated a high-quality pre-training corpus using our proposed user-centric data collection pipeline and introduced novel Stratified Masking to enhance the model's focus on tokens that are more informative and specifically tailored to children. Experimental evaluations demonstrate that our model effectively understands lower grade-level text, maintains safety standards by avoiding the generation of stereotypes, and captures children's unique preferences. Furthermore, based on our insights, we offer suggestions for future research and development."}, {"title": "Limitations", "content": "Resource Constraints Recognizing the importance of this vulnerable population, we took a step back to carefully consider their unique needs and began our work from the ground up, starting with the data. Given the size of our pre-training data, we opted to train an MLM to validate the corpus quality and ensure the integration of kid-specific properties into the language model. Additionally, developing KidLM in resource-constrained academic settings prompted us to propose Stratified Masking, a novel training objective for data-efficient, user-centric language modeling. Our approach aligns with recent research that emphasizes the importance of curating pre-training data to derive meaningful insights for future developments and to optimize models in resource-constrained settings (Lucy et al., 2024). Our insights and observations pave the way for future research and development. We hope that our efforts will inspire the community to advance this work, guided by our future directions.\nDiscussions on Stratified Masking rates We assigned masking rates of 0.15 to stopwords, 0.20 to Dale-Chall easy words, and 0.25 to other words, focusing on more informative and kid-specific vocabulary. This approach led to a masking ratio of stopwords : Dale-Chall words : other words = 0.15:0.20:0.25, increasing in increments of 0.05. We recognize that alternative ratios, such as 0.15:0.25:0.35 with increments of 0.10, are also feasible. However, due to limited computational resources and the extensive training required, we were unable to experiment with finding the optimal masking ratios.\nOther Harm Categories Although our model demonstrates a reduced likelihood of reinforcing negative stereotypes and generating toxic completions across 151 social groups in 8 categories, we were unable to explore other harm categories such as hate speech, sexual content, and violent crimes from the MLCommons taxonomy of hazards. We encourage future work to investigate these additional harm categories to provide a more comprehensive assessment of language model safety.\nGrade Level and Content Criteria Our primary goal was to collect textual content specifically written for children or by children. By \u201cchildren,\u201d we refer to general children's text with linguistic, syntactic, and semantic simplicity. Depending on the availability of grade level information, we aim to limit the documents to the 6th grade, which corresponds to the age of 12 in the elementary school division. However, we cannot guarantee that all content meets our criteria when such information is not directly available.\nLanguage Specificity Our research and the development of KidLM are exclusively centered on the English language. This means its use and effectiveness might not be the same for other languages."}, {"title": "Ethics Statement", "content": "Data Crawling We took ethical consideration into account when scraping data from the sources listed in Tables [15, 16, 17]. The data we have collected is intended exclusively for non-commercial research purposes. We conducted our web scraping activities at a reasonable rate, with no intention of causing a Distributed Denial of Service (DDoS) attack. Additionaly, we read the instructions listed in robots.txt10 of each website to ensure we were able to crawl the desired content as per the Robots Exclusion Protocol (REP) standards11\nMitigating Risks in Content and Model Use We made significant efforts to minimize offensive content in the pre-training data by deliberately crawling sites where such content is minimal. Furthermore, following a manual review of the autocompletion stereotype task's outputs, it seems unlikely that the KidLM+ model produces illicit content when given appropriate context. Nevertheless, we cannot provide an absolute guarantee that no such content is present. Therefore, we strongly recommend exercising caution when using the KidLM and KidLM+ models.\nCarbon Footprint To minimize environmental impact, we limited our continual training to the ROBERTa base model using our corpus, thus reducing the carbon footprint associated with training larger models. Both the KidLM and KidLM+ models were trained on a single RTX 3090 GPU for a total of 168 hours, resulting in an estimated carbon emission 12 of only 25.4kg."}]}