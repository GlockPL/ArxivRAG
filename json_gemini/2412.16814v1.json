{"title": "An Exploration of Pattern Mining with ChatGPT", "authors": ["Michael Weiss"], "abstract": "This paper takes an exploratory approach to examine the use of ChatGPT for pattern mining. It proposes an eight-step collaborative process that combines human insight with AI capabilities to extract patterns from known uses. The paper offers a practical demonstration of this process by creating a pattern language for integrating Large Language Models (LLMs) with data sources and tools. LLMs, such as ChatGPT, are a new class of AI models that have been trained on large amounts of text, and can create new content, including text, images, or video. The paper also argues for adding affordances of the underlying components as a new element of pattern descriptions. The primary audience of the paper includes pattern writers interested in pattern mining using LLMs.", "sections": [{"title": "1 Introduction", "content": "The problem examined in this paper is whether ChatGPT (GPT 4) can be used to mine patterns.\u00b9 The paper is exploratory in nature. I am interested in learning how the pattern mining process can benefit from ChatGPT, and whether the results of this process are useful in practice. Large Language Models, (LLMs) such as ChatGPT, are a new class of AI models that have been trained on large amounts of text, and can generate new content, including text, images, or video [2].\nThe paper makes three contributions. First, it shows how ChatGPT can be used to mine patterns from known uses. Second, it demonstrates the practical application of the process. Third, it argues for the addition of affordances as a new element of pattern descriptions. The primary audience of the paper includes pattern writers interested in pattern mining using LLMs."}, {"title": "2 Literature Review", "content": "This section reviews existing approaches to pattern mining, then discusses examples of human-AI collaboration with ChatGPT, and finally summarizes existing work on patterns for LLMs. Note that the goal of this section is to provide a quick overview of related work, not a thorough review of the literature."}, {"title": "2.1 Pattern Mining", "content": "Iba & Isaku [6] describe a holistic pattern mining approach that includes element mining, visual clustering, and the discovery of pattern \"seeds\". Hanmer [5] documents patterns that facilitate the pattern writing and review process. Iba & Isaku [7] present a detailed pattern language for mining Iba-style patterns. Weiss [13] describes an empirical approach to discovering patterns from design documentation using word clouds to help identify pattern elements. Linden & Cybulski [8] show how the hermeneutic method can be used to induce patterns from interviews with designers. These existing approaches do not use AI."}, {"title": "2.2 Human-AI Collaboration with ChatGPT", "content": "De Paoli [4] describes an exploratory approach to conducting thematic analysis using LLMs. This paper is interesting for two reasons. It is an example of a hybrid approach that involves the collaboration between human researchers and an AI. Pattern mining and thematic analysis also involve similar inductive analysis steps. Schmidt [10] share their experience using ChatGPT to discuss and refine a pattern description."}, {"title": "2.3 Patterns for LLMS", "content": "White [14] document a catalog of patterns for prompt engineering, which the authors see as a way to program an LLM through prompts. Schukart [11] describes patterns for the effective use of generative AI in a work environment. However, patterns for integrating LLMs with data and tools have yet not been documented. I felt it would be a suitable test case for my approach."}, {"title": "3 Process for Mining Patterns with ChatGPT", "content": "In the proposed process, pattern mining involves a collaboration between domain expert and ChatGPT, in which human skills are augmented by those of the AI. Identifying the steps of the process and the particular prompts to be used at each stage required some experimentation. Table 1 shows the steps of the process and their objectives. These steps were conceptualized by the author and iteratively revised during the experiment. However, we used ChatGPT to help produce the initial descriptions of the steps by asking it to summarize the process from the conversation log.\nThe process begins with the selection and documentation of application scenarios to be used as the initial examples, or known uses, from which the patterns will be mined. These scenarios should be provided by a domain expert, and must contain sufficient detail about the design goal and the key steps of the application design.\nThere should be some overlap as well as some variation between these examples, so that the examples cover the space of design choices that we want the patterns to capture. If we conceptualize a solution as a combination of capabilities offered by the underlying components (in the case of an LLM, a component, an example of a capability would be its ability to synthesize content), then each example should show a slightly different combination of capabilities.\nThe purpose of the next two steps (extracting common solutions, and defining problems) is to infer the solutions and their corresponding problems inductively in collaboration with ChatGPT. These problem-solution pairs can then be distilled into patterns. For each pattern, we extract the context and forces, and capture the known uses of the solution from the examples. The output of this step is short forms of the patterns. These can later be elaborated into more detailed versions of the patterns in the refinement step.\nThe goal of the next step is to extract the affordances (i.e., key functionalities or capabilities) of the application components. This will give us a deeper understanding of the components and how they interact in the examples and the patterns. In this context, \"affordances\" refers to the inherent features, capabilities, or functionalities of components that can be used in the design of a system. Affordances determine which capabilities can be selected when integrating these components into solutions or applications.\nFollowing affordance actualization theory [12], a theory for how to design with affordances, each solution can be thought of as a configuration of affordances. For an analogy, think of cooking a meal. This is the equivalent of the solution. To prepare the meal, you need ingredients. Those are the components of the solution. Each ingredient has properties such as flavor or nutritional value. Those are the affordances.\nRelating these affordances to patterns allows us to understand which patterns use which affordances and how. We can make the affordances used by a pattern explicit by including them in its description. A good place to do this is in the body of the pattern's solution. Affordances, thus, provide a common language for describing the solutions of patterns within a pattern language. Traditionally,"}, {"title": "4 Application of the Pattern Mining Process", "content": "The following sections describe an application of the proposed pattern mining process. It is organized around the steps from Table 1, and provides examples of the prompts used to implement the steps. Prompts are visually highlighted by putting them inside a box."}, {"title": "4.1 Identify Initial Examples", "content": "The examples of applications from which the patterns were mined are shown in Table 2. For this experiment, the examples were applications implemented by the author as illustrative use cases of applying LLMs. The examples were chosen because they share certain elements, but also exemplify unique design challenges. Here we chose three examples, but more examples may be required.\nTo extract the common solutions, I provided the examples and then instructed ChatGPT to identify the recurring solutions common to those examples. The prompt begins with:\nIn the following, you will help me to mine patterns for the integration of Large Language Models (LLMs), data sources, and external tools through a structured, iterative dialog. I will start by providing examples of how LLMs, data sources, and external tools can be integrated in real-world applications. In each case, the integration is achieved by an application that defines the overall logic of how the LLM, data sources, and external tools are combined and calls on them as needed.\nThis is followed by the text of the application scenarios from Table 2. The prompt ends with:\nWhat are the recurring solutions across these examples?\nThe result is a list of solutions and their descriptions (for brevity, just the names of the solutions are shown below):\nData Preprocessing\nData Structuring and Enhancement\nIntegration with External Tools\nSemantic Understanding and Synthesis\nIterative Refinement and Feedback\nCustom Application Logic\nAdaptive Response Generation"}, {"title": "4.2 Define Problems", "content": "With the recurring solutions identified, I then asked ChatGPT to identify the common problems underlying the solutions:\nFor each solution, what is the common problem underlying this solution?\nThe problem corresponding to the solution above was:\nCustom Application Logic: This solution addresses the problem of the diverse and specific requirements of different applications. A one-size-fits-all approach is often insufficient for the wide range of tasks and goals that LLMs can be applied to, necessitating custom logic that tailors the integration of data, tools, and LLM capabilities to the specific needs of each application."}, {"title": "4.3 Distill Problem-Solution Pairs into Patterns", "content": "Next, I asked ChatGPT to compile the problem-solution pairs and describe them as patterns. The following prompt defines the expected output format by specifying the sections of a pattern:\nFor each problem-solution pair, describe the solution in the following format:\nContext: in which situation does the problem occur?\nProblem: succinct statement of the problem\nForces: elaboration on what makes the problem challenging\nSolution: succinct statement of the solution\nKnown uses: how do the examples illustrate the solution?\nNote that this template does not include a resulting context section. This section will be added during the refinement step. The resulting context should also include the related patterns that help address the consequences of the pattern. However, we cannot identify the relationships between the patterns until all the patterns have been described. These pattern descriptions created in this step can then be provided to ChatGPT as context.\nHere is the output generated from the problem and solution for the Custom Application Logic pattern:\nCustom Application Logic\nContext: Diverse applications of LLMs, each with unique"}, {"title": "4.4 Identify Affordances", "content": "In this step, I used the following prompt to help me identify the affordances of the components. To guide ChatGPT and as a way of defining what we mean by affordances, the goal of this step is explicitly stated as part of the prompt.\nNext, I want you to help me identify affordances. The goal is to gain a deeper understanding of the patterns and how they work. What affordances of LLMs, databases and external tools are actualized in these patterns?\nThe response to this prompt is a list of the affordances for each component. The result is better visualized as a table, see Table 3. These are affordances that can be actualized or realized by solutions, as well as, applications that use those solutions."}, {"title": "4.5 Relate Patterns to Affordances", "content": "Now, we are ready to determine which patterns use or build on which affordances by asking:\nCross-reference the affordances with the patterns by identifying which affordances are used by which patterns.\nThis output lists the affordances actualized by each pattern. To make it easier to view the output, these affordance can then be collated into a table (Table 4). Each entry of the table represents an affordance and which patterns it is used in. For example, for the Custom Application Logic pattern the output is:\nLLM Affordances:\nContent Generation: LLMs can be programmed with custom logic to generate specific outputs required by the application.\nDatabase Affordances:\nData Organization and Categorization: Databases can be structured according to the custom logic of the application to optimize data retrieval and storage.\nExternal Tool Affordances:"}, {"title": "4.6 Refine Iteratively", "content": "Dependencies between the patterns can be explored with the help of the prompt below. This prompt includes instructions for what we mean by pattern dependencies and how to identify them.\nThere are some possible dependencies between the patterns, where you may want to use another pattern after applying a pattern. For the patterns in the list above, identify the patterns that should be considered next after applying the pattern, if any. Also include a reason why this pattern should be considered in the context of this pattern. The result should be captured in the Resulting Context section of the pattern. Don't repeat the whole patterns, but just give me the Resulting Context sections.\nDependencies will be documented in the resulting context section of a pattern. For example, the generated resulting context for the Custom Application Logic pattern is:\nResulting Context: After establishing custom application logic, Integration with External Tools may be revisited to ensure that all necessary tools are effectively integrated into the workflow. This ensures that the application logic fully leverages the capabilities of each component, from data handling to analysis and response generation."}, {"title": "5 Pattern Language", "content": "This section describes a pattern language for integrating LLMs with data sources and tools, which was created using the process described in the previous section. The motivation for these patterns is that LLMs need to be integrated with external data sources and tools to realize their full potential. Those data sources and tools augment the capabilities of an LLM. Table 5 summarizes the patterns in this pattern language.\nThe patterns are described using the Alexandrian format [1]. It begins with the name of the pattern, and is followed by its context. The problem statement (in bold) is separated from the context by stars. The problem statement is elaborated by the forces. The word \"Therefore\" separates the forces from the solution. The solution consists of a solution statement (in bold) and (optional) solution details. Separated from the solution by stars is the resulting context. It describes the consequences of applying the solution and (optionally) any related patterns."}, {"title": "5.1 Data Preprocessing", "content": "Data sources provide data in a variety of formats.\n***\nRaw data is often unstructured, noisy, and not in a directly usable format.\nThe diversity of data sources and formats, the presence of irrelevant or redundant information, and the need for data to be in a specific structure for analysis or processing make this problem challenging.\nTherefore,\nImplement preprocessing steps to clean, normalize, and structure raw data into a usable format.\nData preprocessing and enhancement, including scraping, cleaning, and formatting raw data, is crucial to making the data usable for analysis by LLMs or storage in databases. The content generation capabilities of an LLM can also be used to preprocess text and normalize or summarize information before it is indexed in a database or further analyzed in an LLM.\n***\nIn the customer support example, FAQs are parsed and structured for indexing. In the research assistant scenario, research abstracts are retrieved and preprocessed for topic modeling.\nAfter data is retrieved and preprocessed, consider applying Data Structuring and Enhancement to organize and enhance the data to make it more suitable for analysis or querying. This is critical because structured, enhanced data can be more effectively indexed, searched, and analyzed, especially when dealing with complex queries or the need for semantic understanding."}, {"title": "5.2 Data Structuring and Enhancement", "content": "Raw or preprocessed data lacks organization, categorization, or semantic richness.\n***\nData is unstructured and lacks semantic organization, making it difficult to analyze or query effectively.\nThe inherent complexity of natural language, the variety of possible interpretations, and the need for semantic understanding to perform specific tasks contribute to the challenge.\nTherefore,\nStructure and enhance data using techniques like semantic indexing, embedding, or categorization to add meaningful organization and context.\nTo organize preprocessed data in an accessible manner, it is essential to structure the data (structured data storage), for example, as a list or in JSON format. Semantic indexing and retrieval can be used to enhance the retrieval process, making it more relevant and"}, {"title": "5.3 Tool Integration", "content": "LLM capabilities need to be augmented with specialized functions for analysis or processing.\n***\nDespite their versatility, LLMs cannot handle all specialized tasks required by specific applications.\nThe specialized nature of certain tasks, the limitations of LLMs in specific analytical domains, and the need for processing efficiency and scalability make this a challenging problem.\nTherefore,\nAugment LLMs by integrating external tools that provide specialized functions or capabilities not inherent to LLMs.\nLLMs can adjust their responses based on output from external tools (adaptive learning). The tools provide specialized analytical capabilities that LLMs do not inherently have.\n***\nThe use of a topic modeling tool in the research assistant scenario and a clustering algorithm in the startup failure analysis example illustrate this integration.\nThe output of the tool may not be directly suitable for analysis, requiring Data Processing to transform the output into a suitable format that can be consumed by further processing steps."}, {"title": "5.4 Semantic Understanding and Synthesis", "content": "An application requires deep understanding of context and generation of coherent, contextually appropriate content.\n***\nSimple models fail to capture the nuances of language and context, leading to superficial or irrelevant responses.\nThe complexity of natural language, the subtlety of context, and the need for nuanced understanding and synthesis in responses add to the challenge.\nTherefore,\nHarness LLMs for their deep semantic understanding and synthesis capabilities to generate coherent and contextually relevant content.\nNatural language understanding and content generation are at the heart of this pattern, enabling LLMs to understand context and generate coherent, contextually relevant content.\n***\nThe generation of topic labels and summaries in the research assistant example and the synthesis of cluster descriptions in the analysis of startup failures demonstrate this capability.\nFollowing semantic understanding and synthesis, consider Adaptive Response to ensure that the system can handle edge cases or queries outside its training range. This is important to maintain user engagement and trust by providing relevant responses even when definitive answers are not available."}, {"title": "5.5 Adaptive Response", "content": "LLMs may encounter user queries that fall outside their training or the scope of available data.\n***\nInevitable encounters with edge cases or limitations in data or LLM capabilities can lead to inadequate responses.\nThe unpredictability of user queries, the inherent limitations of LLMs, and the finite scope of available data all contribute to this challenge.\nTherefore,\nDesign LLMs to generate adaptive responses, either by acknowledging limitations or by providing the best possible alternative response.\nNatural language understanding enables an LLM to recognize when a user query falls outside its training or the scope of the data provided to it as part of the context. LLMs can craft responses that acknowledge limitations or provide the best possible alternative response (content generation).\n***\nThe customer support example, where the LLM generates a response when no suitable FAQ answer is found, demonstrates adaptive response generation.\nAfter adaptive response generation, revisit Semantic Understanding and Synthesis to further refine the system's understanding and output based on user interactions and feedback. This could be achieved, for example, by fine-tuning the LLM or adding in-context examples. This cycle improves the system's ability to generate even more contextually appropriate and helpful responses over time."}, {"title": "5.6 Custom Logic", "content": "Applications of LLMs cover a wide range of domains, user needs, and data sources.\n***\nA one-size-fits-all approach does not adequately address the specific requirements of different applications.\nThe diversity of application domains, the specificity of user needs, and the variability of data sources contribute to the application's complexity.\nTherefore,\nDevelop custom logic that defines the specific integration and interaction of LLMs, data, and tools tailored to the goals of the application.\nLLMs can be prompted to generate application-specific outputs required by the application (content generation). Data sources can be structured according to the custom logic of the application (data"}, {"title": "6 Pattern Story", "content": "The following pattern story shows the patterns exemplified by the Research Assistant application scenario. The draft of the story was generated by using the following prompt:\nHere is a recap of the Research Assistant example. Please identify the relevant patterns.\nThis example is a system designed to act as a research assistant. It facilitates the creation of a literature review by integrating a search engine, a topic modeling tool, and an LLM. The process begins by retrieving abstracts of research articles that match user-supplied search terms from arXiv.org via its API. Next, the topic modeling tool is called on to categorize these abstracts into a predetermined number of topics. For each topic, a list of keywords and a set of abstracts that best represent the topic are returned. The LLM is then prompted to assign labels to these topics and generate descriptive summaries for them. Finally, the LLM is asked to synthesize the content from the most representative abstracts for each topic, crafting them into a cohesive narrative for each stream of the review.\nThis example integrates several patterns in the use of LLMs, databases, and external tools to facilitate the creation of a literature review. Here are the relevant patterns identified for this example:\n(1) Data Preprocessing: The system retrieves article abstracts from arXiv.org using its API. The abstracts need to be extracted from the search results returned by the API.\n(2) Data Structuring and Enhancement: The topic modeling tool groups the retrieved abstracts into topics, structuring the data into a more organized form that enhances its usability for the task at hand. This involves semantic understanding and categorization of the content of the abstracts.\n(3) Tool Integration: The system integrates a search engine (arXiv.org's API) for data retrieval and a topic modeling tool for data categorization. These tools augment the capabilities of the LLM by providing specialized functionality not inherent to the LLM, such as accessing a database of research articles and categorizing abstracts into coherent topics."}, {"title": "7 Discussion", "content": "In this exploratory paper, I proposed an approach to pattern mining that involves a collaboration between a human domain expert and an AI (ChatGPT). In this section, I discuss how well ChatGPT was able to extract patterns from known uses, as well as the necessary changes to the initial pattern descriptions generated by ChatGPT that would be required by a domain expert."}, {"title": "7.1 Ability to Extract Patterns from Examples", "content": "The quality of the examples and the variety of design choices they represent will have a significant impact on the patterns that can be extracted. Also, to a large extent, patterns will only be generated for design choices that are reflected in the examples. Domain experts will be aware of other patterns and design choices, but these may not be directly supported by evidence from the examples.\nChatGPT is also known to occasionally produce generic answers, especially when asked specific questions that go beyond the data on which it was trained. This can also be observed in the patterns it generates. Out of seven patterns in this experiment, ChatGPT suggested one pattern that was quite general (Iterative Refinement and Feedback). While, iteration is obviously a widely used principle, including such a pattern in the pattern language does not strengthen the language. This pattern was also not supported by the pattern stories generated for the examples."}, {"title": "7.2 Necessary Changes to the Pattern Descriptions", "content": "The output produced by ChatGPT should be taken as a starting point for the pattern descriptions. Pattern mining with ChatGPT should not be seen a one-way street where we simply accept the output it produces, but as a co-creation process where the AI augments our human ability.\u00b3 Thus, instead of asking ChatGPT to combine the various pattern elements it produced (initial description, resulting context, and affordances) into a complete pattern description, I chose to combine them manually.\nThis gave me a chance to digest what ChatGPT had produced, and \"humanize\" the pattern descriptions, which meant reflecting them through the lens of a domain expert, correcting errors and"}, {"title": "8 Conclusion", "content": "This paper describes the results of an experiment using ChatGPT (GPT 4) for pattern mining. The initial results are promising, however, in its current form the proposed process has limitations that should be addressed by future work."}, {"title": "8.1 Limitations", "content": "There are three potential limitations of the current process: in how broadly the process is applicable, in how known uses are made available to the LLM for processing, and in the how the outcomes depend on the quality of the prompts. I will discuss each in turn.\nApplicability of the process. Since this paper describes only a single case study of the use of this process, it is difficult to generalize how widely applicable it is. Instead, I will offer tentative answers to the following questions: Is the process better suited to certain types of patterns or domains than for others? What are the characteristics of a domain that suggest that the process can or cannot be applied?\nMy sense is that the process is more applicable to contexts where the known uses are described in a narrative format, rather than in (raw) code or design diagrams. Such a narrative format would be expected to include a rationale for the various design decisions illustrated by the application scenarios. Code or design diagrams, on the other hand, often do not include an explanation of why the solutions they embody work, although, they may, of course do so (in particular, if the code is properly commented).\nThus, the process should be more appropriate for domains that involve architectural design decisions, processes (e.g., in software development), or organizational design choices. Initial experiments with an earlier version of the process, where I tried to rediscover patterns for agile practices from a description of their known uses, suggest that the process is also applicable to such contexts. However, more work (as discussed in the Future Work section) is needed to draw more solid conclusions.\nMaking known uses available. Currently, the full description of the known uses is included in the context of the prompt. However, LLMs have limits on how much context can be provided as part of a prompt. Technically, the number of words or tokens that can be used to describe the context is limited. While larger contexts can be accommodated as LLMs evolve, there are also known issues with how much attention an LLM pays to information in the context. This can lead to an uneven processing of the information. Thus, not all known uses may be given the same weight during processing.\nA solution to this issue could come from the recent progress in conducting thematic analysis using LLMs [4]. In this approach, known uses could be annotated with codes (e.g., marking a sentence as related to a problem or a solution), and the coded sections could then be fed into the LLM for further processing instead of the full, unprocessed known uses. Such an approach would allow the proposed process to scale to larger numbers of known uses.\nQuality of the prompts. The prompts identified may not be optimal or as reliable as alternative prompts. The prompts could also incorporate more explicit knowledge about patterns and writing patterns, rather than relying on the definitions available to Chat-GPT."}, {"title": "8.2 Future Work", "content": "As an exploratory paper, this paper does not pretend to have all the answers. There is much room for improvement and expansion of the work described. Some of these opportunities are identified next in the form of future experiments to be conducted."}]}