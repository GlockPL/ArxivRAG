{"title": "Leveraging Contrastive Learning and Self-Training for Multimodal Emotion Recognition with Limited Labeled Samples", "authors": ["Qi Fan", "Yutong Li", "Yi Xin", "Xinyu Cheng", "Guanglai Gao", "Miao Ma"], "abstract": "The Multimodal Emotion Recognition challenge MER2024 focuses on recognizing emotions using audio, language, and visual signals. In this paper, we present our submission solutions for the Semi-Supervised Learning Sub-Challenge (MER2024-SEMI), which tackles the issue of limited annotated data in emotion recognition. Firstly, to address the class imbalance, we adopt an oversampling strategy. Secondly, we propose a modality representation combinatorial contrastive learning (MR-CCL) framework on the trimodal input data to establish robust initial models. Thirdly, we explore a self-training approach to expand the training set. Finally, we enhance prediction robustness through a multi-classifier weighted soft voting strategy. Our proposed method is validated to be effective on the MER2024-SEMI Challenge, achieving a weighted average F-score of 88.25% and ranking 6th on the leaderboard. Our project is available at https://github.com/WooyoohL/MER2024-SEMI.", "sections": [{"title": "Introduction", "content": "Multimodal Emotion Recognition (MER) has increasingly garnered interest due to its potential in various applications including human-computer interaction (HCI) (Moin et al. 2023; Li et al. 2024; Song et al. 2023), mental health monitoring (Xu, Wu, and Liu 2022; Shen et al. 2024), and social media analysis (Chandrasekaran, Nguyen, and Hemanth D 2021; Yu et al. 2024). Research in MER primarily concentrates on three modalities: text, audio, and visual. There has been substantial progress in this field with the adoption of advanced techniques such as deep learning and multimodal fusion. Recent studies, for instance, Li et al. (Li et al. 2021) have introduced architectures like Deep Convolutional Neural Networks (DCNN) and Deep Separable Convolutional Neural Networks (DSCNN) for speech and face recognition. MFN (Wu et al. 2019) explores how attention mechanisms can be successfully applied to model emotion recognition from rich narrative videos. DEFR (Zhao and Liu 2021) incorporates a novel transformer approach that combines convolutional spatial transformers with temporal transformers for enhanced extraction of facial features over time and space. Despite these advancements, challenges remain, particularly in the time-consuming and costly process of sentiment data annotation. The reliance on extensively annotated datasets severely restricts the practical applicability of most existing multimodal sentiment recognition methods.\nIn response to the above problems, the ACM MM unveiled the MER2024-SEMI challenge. This initiative requires participants to predict discrete emotions with a limited set of labeled data, complemented by a significant volume of unlabeled data for training. To enhance model generalization, the challenge encourages the exploration of large-scale unlabeled data utilization. One effective strategy is semi-supervised learning (SSL), particularly self-training methods that have shown promise in various domains (Liu et al. 2024a; Xin et al. 2023; Wang et al. 2023b). These methods generate pseudo-labels from unlabeled data, integrating these with labeled data during the training process. Another promising strategy involves self-supervision (Chen et al. 2020a,b; Liu et al. 2024b), which entails pretraining models on large amounts of unlabeled data and then finetuning them for specific tasks (Xin et al. 2024a,b). In the context of the MER2024-SEMI challenge, participants must contend with a discrete set of six emotions: neutral, anger, happiness, sadness, worry, and surprise. Importantly, the distribution of data across these classes varies (as shown in Figure 1), posing additional challenges related to class imbalance that participants must address.\nTo tackle the MER2024-SEMI challenge, we propose a straightforward yet robust semi-supervised multimodal emotion recognition method. To address the class imbalance, we implement a crucial oversampling strategy, focusing on expanding classes with limited data. Next, we propose a modality representation combinatorial contrastive learning framework for effectively utilizing the provided unlabeled data in the trimodal input. Additionally, we explore semi-supervised techniques, particularly finding the self-training strategy valuable. This approach involves generating pseudo-labels for unlabeled data and integrating them into the training process. Furthermore, to enhance the robustness of the prediction results, we employ ensemble learning via aggregating the confidence scores of multi-classifiers. Our approach achieves an 88.25% weighted average F-score on MER2024-SEMI."}, {"title": "Method", "content": ""}, {"title": "Problem Formulation", "content": "MER2024-SEMI challenge provides a small amount of labeled data \\(D_l = \\{(x_i, y_i)\\}_{i=1}^{N_l}\\) and and a large amount of unlabeled data \\(\\tilde{D}_u = \\{(x_i)\\}_{i=1}^{N_u}\\), where \\(y_i\\) is the discrete label of the sample \\(x_i\\) and \\(y_i \\in \\{1,2, ..., C\\}\\). Here, \\(N_l\\), \\(N_u\\), and \\(C\\) respectively denote the number of labeled samples, unlabeled samples, and emotion classes. For each \\(x_i\\) (or \\(\\tilde{x}_i\\)), we extract visual features \\(F_v\\), acoustic features \\(F_a\\) and text features \\(F_t\\) from the visual, audio, and text modalities, where \\(F_v \\in \\mathbb{R}^{d_v}\\), \\(F_a \\in \\mathbb{R}^{d_a}\\), \\(F_t \\in \\mathbb{R}^{d_t}\\), and \\(\\{d_m\\}_{m\\in\\{v,a,t\\}}\\) represent the feature dimensions for each modality. For simplicity, we omit the subscript \\(i\\) in the following when there is no ambiguity."}, {"title": "Feature Extraction", "content": "For the whole dataset, we extract the utterance level feature according to the MER2024 baseline (Lian et al. 2023, 2024).\nVisual feature: Since the initial samples in MER2024 are in video format, we first crop and align the facial regions of each frame using the OpenFace toolkit (Baltru\u0161aitis, Robinson, and Morency 2016). Subsequently, we leverage the pretrained CLIP-Large model (Radford et al. 2021) to extract frame-level features for each face image. These frame-level visual features are then aggregated using average pooling to generate video-level embeddings.\nAcoustic feature: First, we utilize the FFmpeg toolkit to separate the audio from the video with a sampling rate of 16kHz. Following this, we employ the Chinese-HuBert-Large acoustic encoder (Hsu et al. 2021) to extract features, which performs well on Chinese sentiment corpus. The averaged hidden representations from the last 4 layers of the model are used as the final acoustic representations due to their high sensitivity to the semantic information.\nText feature: We first convert audio files into transcripts by employing WeNet (Yao et al. 2021), an open-source automatic speech recognition toolkit. Then, we choose the Baichuan2-13B-Base model (Baichuan 2023) that has been pre-trained on a large-scale corpus as a feature extractor and further input transcripts into the language model to obtain a feature vector as the representation of the text modality."}, {"title": "Data preprocessing", "content": ""}, {"title": "Imbalanced Data Processing", "content": "The distribution of labeled data provided by MER2024-Train&Val is shown in Figure 1. Among them, the labels \"neutral\", \"angry\u201d, and \u201chappy\u201d are dominant, \u201cworried\" and \"sad\" are secondary, while \u201csurprise\u201d accounts for only 3.8%. This dataset exhibits imbalance, where minority class samples risk being overwhelmed by majority samples during training, potentially impacting model performance. To address this issue, we rebalance the training data distribution by employing random oversampling. Through this process, each emotion category achieves a predetermined sample count, resulting in a more balanced dataset."}, {"title": "Noise embedding enhancement strategy", "content": "Various data augmentation techniques can be applied to the raw data across different modalities to enhance the generalization of the model. For instance, image flipping and random cropping can be utilized for visual data, while spectrogram transformation can augment audio data. These augmentation strategies help the model generalize better and perform more reliably in diverse scenarios.\nHowever, integrating these diverse data enhancement methods with existing models can be challenging. To address this, inspired by the work of Wang et al. (Wang et al. 2023a; Hazarika et al. 2022; Fan et al. 2023; Ho, Jain, and Abbeel 2020), we introduce a noise embedding enhancement (NEE) strategy to intervene in the representation of each modality. It constructs noise embeddings \\(\\{N_m\\}_{m\\in\\{v,a,t\\}}\\) for each extracted feature \\(\\{F_m\\}_{m\\in\\{v,a,t\\}}\\) using Gaussian noise. The noise schedule is built across time steps, and the process is described by the following formula:\n\\[N_m = \\sqrt{\\alpha_\\tau}F_m + \\sqrt{1 - \\alpha_\\tau}\\epsilon\\qquad(1)\\]\n\\[\\alpha_\\tau = \\prod_{t=1}^{T} (1 - \\beta_t)\\qquad(2)\\]\nwhere \\(F_m\\) denotes the embedding at time step 0, and \\(\\epsilon\\) is a noise vector randomly sampled from a Gaussian distribution, \\(\\epsilon \\sim N(0, 1)\\). Timestep parameter \\(T\\) is used to control the degree of noise injected into the data and simulate the process of gradual destruction of the data by random noise. The parameter \\(\\beta_t\\) is the t-th value of the schedule parameter sequence \\(\\{\\beta_1, \\beta_2,\\dots,\\beta_t, \\dots, \\beta_T\\}\\). Here, the total number of steps \\(T\\) affects the gradual transition of the original embedding to the noise-dominated state and is set to 100. In our experiments, \\(\\beta_1\\) and \\(\\beta_T\\) are set to 0.001 and 0.1, respectively. By embedding random noise at the feature vector level through the aforementioned NEE strategy, the model is encouraged to adapt to variations in sample data to enhance its generalization ability."}, {"title": "Model Architecture", "content": ""}, {"title": "Modality Representation Combinatorial Contrastive Learning", "content": "We propose Modality Representation Combinatorial Contrastive Learning (MR-CCL) framework to leverage the given unlabeled data more effectively. The overall architecture is illustrated in Figure 2.\nIn the realm of multimodal emotion recognition tasks, a proven strategy involves extracting features specific to each modality as well as invariant features that capture relationships between different modalities, which facilitates the fusion and interaction of diverse modal inputs (Zuo et al. 2023; Liu et al. 2024b). To implement our framework, we begin by pre-training three specificity encoders and one invariant encoder using all available unlabeled data. This process is designed to enable the model to discern and encode the intrinsic characteristics and structural nuances of multimodal data. Each specificity encoder is composed of multiple Transformer layers, while the invariant encoder is structured with linear layers.\nThe input embedding \\(\\{F_m\\}_{m\\in \\{v,a,t\\}}\\) for each modality along with their respective noise embeddings \\(\\{N_m\\}_{m\\in\\{v,a,t\\}}\\) are fed into the specificity encoders to obtaining modality-specific embeddings \\(\\tilde{F}_m\\}_{m\\in\\{v,a,t\\}}\\) and \\(\\tilde{N}_m\\}_{m\\in\\{v,a,t\\}}\\. Subsequently, the invariant encoder transforms the \\(\\tilde{F}_m\\}_{m\\in\\{v,a,t\\}}\\) embeddings into a unified vector space, extracting modality-invariant emotion features \\(\\{H_m\\}_{m\\in\\{v,a,t\\}}\\), a technique validated for its effectiveness in multimodal emotion recognition (Liu et al. 2024b). Following this, we proceed the combinatorial contrastive learning method based on the features obtained from the encoders above.\n(1) Intra-modality Contrastive Learning with Noisy Embedding (IMCL-NE). We employ contrastive learning techniques to learn a consistent representation between the modality-specific representation \\(\\tilde{F}_m\\}_{m\\in\\{v,a,t\\}}\\) and the noisy modality-specific representation \\(\\tilde{N}_m\\}_{m\\in\\{v,a,t\\}}\\) to enhanced feature robustness and model generalization performance. Noise contrastive estimation (Oord, Li, and Vinyals 2018) is utilized to calculate intra-modality contrastive loss of each modality. Taking a pair of visual embeddings \\(\\tilde{F}_v\\) and \\(\\tilde{N}_v\\) as an example, it can be implemented through the following equation:\n\\[L_{v\\_fn} = - \\frac{1}{B} \\sum_{k=1}^{B} \\log(\\frac{\\exp((\\tilde{F}_{v}^{k})^T(\\tilde{N}_{v}^{k})/T_v)}{\\sum_{l=1}^{B} \\exp((\\tilde{F}_{v}^{k})^T(\\tilde{N}_{v}^{l})/T_v)}) -  \\log(\\frac{\\exp((\\tilde{F}_{v}^{k})^T(\\tilde{F}_{v}^{k})/T_v)}{\\sum_{l=1}^{B} \\exp((\\tilde{F}_{v}^{k})^T(\\tilde{N}_{v}^{l})/T_v) + \\exp((\\tilde{F}_{v}^{k})^T(\\tilde{F}_{v}^{l})/T_v)})\\qquad(3)\\]\nwhere \\(B\\) is the batch size, \\(T_v\\) is the temperature, \\((\\cdot)^T\\) represents the vector transpose operation, and \\(L_{a\\_fn}\\) and \\(L_{t\\_fn}\\) are calculated similarly. Thus, the total intra-modality contrastive loss \\(L_{fn\\_intra}\\) is given by: \\(L_{fn\\_intra} = (L_{v\\_fn} + L_{a\\_fn} + L_{t\\_fn})/3\\).\n(2) Inter-Modality Contrastive Learning with Aligned Embeddings (IMCL-AE). Inspired by Shvetsova et al. (Shvetsova et al. 2022), we aim to calculate the inter-modality contrastive loss between all possible modality combinations, mapping the inputs from single or multiple modalities into a joint embedding space where semantically similar inputs are closely positioned. First, the contrastive losses between single modalities are calculated using three pairs of embeddings: \\((H_v, H_v)\\), \\((H_a, H_a)\\), and \\((H_t, H_t)\\), from which the losses \\(L_{av}\\), \\(L_{at}\\), and \\(L_{vt}\\) are respectively calculated according to Equation (3). Subsequently, we learn the close representation between one modality and the other two modalities. For instance, considering the embedding \\(H_v\\) and concatenated vector \\(H_{at} = Concat(H_a, H_t)\\), this relationship can be quantified through the following equation:\n\\[L_{v\\_at} = - \\frac{1}{B} \\sum_{k=1}^{B} \\log(\\frac{\\exp((H_{v}^{k})^T(H_{at}^{k}) /T_{at})}{\\sum_{l=1}^{B} \\exp((H_{v}^{k})^T(H_{at}^{l})/T_{at})}) -  \\log(\\frac{\\exp((H_{v}^{k})^T(H_{at}^{k})/T_{at})}{\\sum_{l=1}^{B} \\exp((H_{v}^{k})^T(H_{at}^{l})/T_{at}) + \\exp((H_{v}^{k})^T(H_{at}^{l})/T_{at})})\\qquad(4)\\]\nwhere \\(T_{at}\\) is the temperature, \\(B\\) is batch size, \\(L_{a\\_vt}\\) and \\(L_{t\\_av}\\) are calculated similarly. The total inter-modality contrastive loss \\(L_{ime}\\) is given by: \\(L_{ime} = (L_{av} + L_{vt} + L_{at} + L_{v\\_at} + L_{a\\_vt} + L_{t\\_av})/6\\).\nSelf-Training Strategy\nPseudo-labeling plays a significant role in utilizing unlabeled data. According to previous works (Li, Gao, and Li 2023), different models have different preferences for classification."}, {"title": "Experiments and Results", "content": ""}, {"title": "Datasets and Evaluation Metric", "content": "MER2024 (Lian et al. 2023) is a Chinese emotion dataset designed for emotion recognition challenge, comprising four subsets: Train&Val, MER-SEMI, MER-NOISE, and MER-OV. The Train&Val subset consists of 5030 labeled single-speaker video segments used for training and validation purposes in the MER2024 challenge. The MER-SEMI subset contains a total of 115,595 unlabeled data, with 1169 of these serving as the true test set for evaluating performance in the Track1 \u2018semi-supervised learning challenge'. Participants in this track are tasked with predicting discrete emotions through a large number of unlabeled samples. They are encouraged to employ semi-supervised learning techniques to improve emotion recognition performance. Here, discrete emotion labels include 6 classes, i.e., worried, happy, neutral, angry, surprised, and sad. Similar to the baseline (Lian et al. 2024), we adopt weighted average F-score (WAF) (Lian, Liu, and Tao 2021) to evaluate the overall recognition performance due to the inherent class imbalance."}, {"title": "Implementation Details", "content": "We conduct all experiments on an NVIDIA A100 80GB GPU. For feature extraction, the embedding dimensions \\(d_v\\), \\(d_a\\), and \\(d_t\\) for video, audio, and text are 768, 1024, and 5120, respectively.\nIn the process of pseudo label selection, we apply a threshold of 0.99 for the categories \u201chappy\u201d, \u201cneutral\u201d, \u201cangry\u201d, and \"sad\u201d to determine pseudo labels. Given the inherent imbalance in the categories \u201cworried\u201d and \u201csurprise\", we use a lower threshold of 0.85. Table 2 presents the specific counts of pseudo labels assigned to each class. The weight of classifiers A, V, T, and F are set to [0.7, 0.5, 0.4, 0.7]. During training, we employ the Adam optimizer for MR-CCL, setting the initial learning rate to le-4 and utilizing a batch size of 512. We train the model with a maximum of 40 epochs and employ an early stopping strategy with a patience of 5 epochs.\nIn the final training process, we incorporate both pseudo-label and oversample strategies. In the first step, we use the oversampling strategy to increase the three minority classes to 850 samples; in the second step, we add the data with false labels to the training set, increase the number of over-sampling to 1000, and reduce the learning rate from 1e-4 to 5e-5. Each step contains 20 epochs.\""}, {"title": "Main Results and Analysis", "content": "Based on the MER-SEMI track theme, we employ two main ways to utilize unlabeled data: 1) The contrastive learning method based on unlabeled data, and 2) Expanding the training set by labeling unlabeled data with pseudo labels. Table 1 summarizes the results of our model in comparison to the baseline, as well as the outcomes from ablation experiments. The results indicate that using unlabeled data significantly enhances multimodal emotion recognition performance.\nWe present the results of expanding the training set with only the pseudo-labeling strategy or the oversample strategy in Table 2 and Table 3. We can observe that utilizing the pseudo-label strategy gets a higher WAF on the validating set (and is more closer to the testing set) than using the original training set and the oversample strategy. This phenomenon proves that the pseudo-labeled data provides more diverse training data, improves the generalization ability of the model, and may make the data distribution of the training set closer to the data distribution of the test set, thus reducing the impact of distribution differences on the model performance. In addition, with the addition of pseudo-labeled data, the overfitting phenomenon of the model has been alleviated, and the generalization ability and performance of the model can be improved. We can also observe that the result of the pseudo-label strategy on the test set is similar to the oversample strategy. This may be because the distribution of the testing set still has differences from the validating and training sets, or because the model is overconfident in its predictions and generates false pseudo labels. Therefore, there is still room for improvement in the pseudo-labeling strategy. Table 4 also shows that the decrease in learning rate helps stabilize the performance of the model, which provides support for the above analysis."}, {"title": "CONCLUSION", "content": "This paper describes our proposed semi-supervised multimodal emotion recognition method for the MER2024-SEMI challenge. Our method comprises three main steps: modality representation combinatorial contrastive learning, self-training, and multi-classifier voting. Additionally, we perform oversampling to address the class imbalance problem. The effectiveness of the proposed method is validated, achieving a weighted average F-score of 88.25% on the test set of the MER2024-SEMI challenge. We intend to consider the stronger semi-supervised training strategy and fusion module in future work."}]}