{"title": "TERD: A Unified Framework for Safeguarding Diffusion Models Against Backdoors", "authors": ["Yichuan Mo", "Hui Huang", "Mingjie Li", "Ang Li", "Yisen Wang"], "abstract": "Diffusion models have achieved notable success in image generation, but they remain highly vulnerable to backdoor attacks, which compromise their integrity by producing specific undesirable outputs when presented with a pre-defined trigger. In this paper, we investigate how to protect diffusion models from this dangerous threat. Specifically, we propose TERD, a backdoor defense framework that builds unified modeling for current attacks, which enables us to derive an accessible reversed loss. A trigger reversion strategy is further employed: an initial approximation of the trigger through noise sampled from a prior distribution, followed by refinement through differential multi-step samplers. Additionally, with the reversed trigger, we propose backdoor detection from the noise space, introducing the first backdoor input detection approach for diffusion models and a novel model detection algorithm that calculates the KL divergence between reversed and benign distributions. Extensive evaluations demonstrate that TERD secures a 100% True Positive Rate (TPR) and True Negative Rate (TNR) across datasets of varying resolutions. TERD also demonstrates nice adaptability to other Stochastic Differential Equation (SDE)-based models. Our code is available at https://github.com/PKU-ML/TERD.", "sections": [{"title": "1. Introduction", "content": "In recent years, we have witnessed significant advancements in generative models. However, the widespread application of diffusion models raises concerns about their security issues like backdoor attacks, where models can be manipulated to produce harmful outputs under specific conditions, posing significant legal and ethical risks. Therefore, in this paper, we explore how to defend against backdoor attacks for diffusion models, which is less investigated before.\nUnlike common classification models, diffusion models operate on noise outputs rather than class logits, making them impervious to conventional defenses designed for classification tasks. The challenge is exacerbated by the complexity of their input-output dynamics over various timesteps, e.g., the model's behavior changes across different timesteps and the underlying formulation is often inaccessible to defenders. This significantly hinders the ability to effectively identify and mitigate backdoor triggers.\nTo address this challenge, in this paper, we propose a novel defense strategy that begins by systematically characterizing existing backdoor attacks in diffusion models. Our approach involves creating unified formulations of backdoor attacks, enabling us to derive an accessible reversed loss. For the accessibility of inputs, we introduce a two-stage trigger reversion process: we first estimate the trigger using noise sampled from a prior distribution, followed by refinement through differential multi-step samplers. This process allows for accurate identification and neutralization of backdoor inputs. Equipped with the estimated trigger, we can detect backdoor attacks from both the input and model perspectives in the noise space, leveraging the statistical characteristics of noise distributions to distinguish between benign and malicious inputs. We refer to this comprehensive defense framework as TERD (Trigger Estimation and Refinement for Diffusion). TERD has demonstrated remark-"}, {"title": null, "content": "able success across diverse datasets, achieving a 100% True Positive Rate (TPR) and True Negative Rate (TNR). Further, TERD works well against a wide range of attack scenarios, including those with varied poisoning rates, trigger sizes, and even sophisticated adaptive attacks. Beyond the diffusion models, TERD also shows promise for defending other Stochastic Differential Equation (SDE)-based models against backdoor attacks. In summary, our main contributions are listed as follows:\n\u2022 We specially design a novel trigger reversion algorithm based on the unified modeling against backdoor attacks in diffusion models, which can accurately reverse triggers with high quality.\n\u2022 With the reversed trigger, we develop an input and model detection method in the noise space to protect the diffusion models from backdoors.\n\u2022 Extensive experiments show the efficacy of our defense across varied scenarios and its potential applicability to broader SDE-based generative models."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Backdoor Attacks in Diffusion Models", "content": "Backdoor attacks, also known as Trojan attacks, were initially studied in the context of classification models. These attacks involve implanting pre-defined malicious behaviors into neural networks. While the victim models maintain normal functionality with benign inputs, the presence of a trigger in the input causes the model to exhibit malicious behaviors, such as misclassification or illegal content generation. Recent studies, have demonstrated that diffusion models are also vulnerable to these attacks. In these scenarios, a trigger is added to noise sampled from a prior distribution. Images generated from this altered noise become target images, resulting in unexpected sequences. VillianDiffusion further extends it to continuous diffusion models. Additional research has shown that backdoor attacks can be executed using natural language prompts (specifically for text-to-image diffusion models) or by poisoning the training set. However, these attacks can be easily defended by purifying the text encoder or additional human inspection. Therefore, in this paper, we focus on defending against backdoor attacks from the pixel level, which not only has good stealthiness but also endangers all existing diffusion models."}, {"title": "2.2. Existing Backdoor Defense", "content": "Similar to defenses against adversarial attacks , current backdoor defenses mainly focus on classification models. These defenses can be categorized into two types: input-level and model-level defenses. Input-level defenses aim to detect whether an input sample is a backdoor sample. Previous studies have shown that backdoor samples can be identified through neural activations or frequency analysis. Techniques from other fields, such as differential privacy and explainable visualization tools, further enhance detection success rates , as backdoor samples often appear as outliers relying on local spurious features. Model-level defenses work by first detecting whether a model has been implanted with a backdoor and then mitigating the backdoor effect. Regarding backdoors as shortcuts between the real and target classes, methods like employ reverse engineering by maximizing the classification loss across all classes to identify potential triggers. Once the model is identified as backdoored, purification-based defenses such as fine-tuning , pruning , or unlearning are employed to reduce the attack success rate while maintaining benign accuracy. However, these defenses fail to protect diffusion models because the input to a diffusion model is Gaussian noise rather than natural images, and diffusion models predict added Gaussian noise rather than discriminative results of natural images.\nThe most relevant work to ours is Elijah , the method designed specifically for backdoor defense in diffusion models. However, Elijah does not establish a unified loss for current attacks, assuming the trigger is part of the model output, which does not apply to state-of-the-art attacks such as TrojDiff . Additionally, Elijah's model detection method assumes that backdoor models generate images with higher similarity, a claim contradicted by , which demonstrates that target images can consist of multiple images with diverse and colorful patterns."}, {"title": "3. Preliminary", "content": ""}, {"title": "3.1. Discrete Diffusion Model", "content": "Based on the Markov chain, Denoising Diffusion Probabilistic Models (DDPM) connects the data and prior distribution (e.g., Gaussian distribution) by defining a forward diffusion and backward denoising process. In its forward process, Gaussian noise is gradually added to images and the conditional distribution $p(x_t|x_{t-1})$ is defined as $N(\\sqrt{a_t}x_{t-1}, (1 \u2013 a_t)I)$ where $a_t \\in (0,1)$. According to Bayes Rule, given $x_o$, we can sample $x_t$ of timestep $t$"}, {"title": null, "content": "$(0<t\u2264T)$ directly from the following equation:\n$x_t = \\sqrt{\\bar{a}_t}x_o + \\sqrt{1 - \\bar{a}_t}\\epsilon, \\epsilon \\sim N(0, I)$,\t(1)\nwhere $\\bar{a}_t = \\prod_{i=1}^t a_i$. The boundary conditions require that $\\lim_{t \\to T} \\bar{a}_t = 0$ to ensure that $p(x_t|x_0)$ converges to $N(0, I)$. Therefore, in the denoising process, we first sample $x_T$ from $N(0, I)$ and then generate $x_{t-1}$ step-by-step using $p(X_{t-1}|X_t, X_0) = \\frac{p(x_t|x_{t-1},X_0)p(x_{t-1}|xo)}{p(x_t|xo)}$. According to Equation 1, we can estimate $x_0$ with $\\frac{x_t-\\sqrt{1-\\bar{a}_t}F_\\theta(x,t)}{\\sqrt{\\bar{a}_t}}$ once the network $F_\\theta$ predicts $\u03f5$:\n$\\min_\\theta ||F_\\theta(x_t, t) - \\epsilon||_2$.\t(2)"}, {"title": "3.2. Continuous Diffusion Model", "content": "In Song et al. (2020b), a unified Stochastic Diffusion Equation (SDE)-based framework is proposed to encapsulate the diffusion model. When t becomes continuous, the diffusion process is characterized by the following forward SDE:\n$dx_t = f(x_t,t)dt + g(t)dw$,\t(3)\nwhere t \u2208 [0, T] and f(xt, t), g(t) are the drift and diffusion coefficients, respectively. According to Anderson (1982), the denoising process corresponds to a reversed SDE:\n$dx_t = [f(x_t, t) - g(t)^2\\nabla_{x} log p_t(x_t)]dt + g(t)dw$.\t(4)\nWe cannot solve the above equation directly due to the existence of term $\u2207_x log p_t(x_t, \u03f5)$. However, in the forward diffusion process, we can train the model $F_\u03b8$ with xt and the time step t to fit it:\n$\\min_\u03b8 ||F_\u03b8 (x_t, t) - \\sqrt{x} log p_t(x_t)||_2$.\t(5)\nThus in the sampling stage, we can generate images by solving Equation 4 with appropriate samplers, such as Heun solver  and DPM solver ."}, {"title": "3.3. Backdoor Diffusion Model", "content": "Only a few works, such as Chou et al. (2023a); Chen et al. (2023); Chou et al. (2023b), explored backdoor attacks in diffusion models. In their threat models, attackers have access to the training process of diffusion models. They develop a backdoor diffusion process to ensure that when a trigger is attached to the sampled noise, the generated images transform into predefined target images. The trigger and target images are tensors with the same shape as benign images and are inaccessible to defenders. To maintain the benign utility of the model, the benign training loss, as defined in Sections 3.1 and 3.2, is also incorporated into the training process."}, {"title": "4. Reverse Engineering", "content": ""}, {"title": "4.1. A Unified Loss for Trigger Reversion", "content": "As summarized in Section 3, in addition to the benign diffusion process, current backdoor attacks for diffusion models define an additional diffusion process i.e., backdoor diffusion process for target image generation. Despite the differences in the details among the attacks, we can unify their formulations with the following equation\u00b9:\n$x_t = a(x, t)x_o + b(t)\\epsilon + c(t)r$.\t(9)\nHere, $a(x, t)$ and $b(t)$ are two coefficients that follow the benign diffusion process and the backdoor coefficient $c(t)$ is defined by attackers. To ensure that the backdoor effect can be triggered by r, $c(t)$ needs to first satisfy the following boundary condition: $\\lim_{t \\to T} c(t) = 1$. In addition, with the initial condition: $x_t = x_0$, we can get: $\\lim_{t \\to 0} c(t) = 0$.\nAccording to the formulations in Section 3.3, we summarize their corresponding relations with existing attacks Meanwhile, we also established a unified form of backdoor training loss for those attacks:\n$\\min_\u03b8 E_{t,\\epsilon}||F_\u03b8(x_t, t) \u2212 f(x_t, \\epsilon) + d(t)r||^2,\t(10)"}, {"title": null, "content": "Note that in Elijah , they heuristically assume d(t) = 0.5 and make a trade-off between BadDiffusion and TrojDiff (lim $\\frac{1-\\bar{a}t}{1+\\sqrt{\\bar{a}t}} = 1$ for BadDiffusion). This could lead to the failure of defense, particularly in some difficult cases. Therefore, it is necessary to first establish a unified loss to more accurately characterize the relation between the trigger and the model output. Observe that for Equation 10, we can divide it with the losses of two independent noises \u03f51, \u03f52 respectively. Furthermore, we can employ the triangle inequality to obtain a lower bound for direct optimization:\n$\\frac{1}{2} E_{t,\\epsilon_1,\\epsilon_2}||F_\u03b8(x_t(\\epsilon_1, r), t) \u2212 f (x_t(\\epsilon_1, r), \\epsilon_1) + d(t)r||^2 +||F_\u03b8(x_t(\\epsilon_2, r), t) - f(x_t(\\epsilon_2,r), \\epsilon_2) + d(t)r||^2 \\geq E_{t,\\epsilon_1,\\epsilon_2}||F_\u03b8(x_t(\\epsilon_1, r), t) \u2212 f (x_t(\\epsilon_1, r), \\epsilon_1) -F_\u03b8(x_t(\\epsilon_2, r), t) + f (x_t(\\epsilon_2, r), \\epsilon_2)||^2$.\t(11)\nDue to the non-negative property of the norm operation, when Equation 10 is optimized to 0, the lower bound in Equation 11 also reaches a minimum point. It means that we can substitute Equation 10 with Equation 11 for trigger reversion. To avoid r collapses to the full-zero vector, we introduce l\u2081 norm for penalization and \u03bb as the trade-off coefficient:\n$\\mathcal{L}(r, x_t) = ||(F_\u03b8(x_t(\\epsilon_1,r), t) \u2212 f(x_t(\\epsilon_1, r), t), \\epsilon_1)- F_\u03b8(x_t(\\epsilon_2, r), t) + f(x_t(\\epsilon_2, r), t), \\epsilon_2)||^2 - \u03bb||r||_1$\t(12)\nNote that Equation 12 unifies the expression of all current attacks from the reversed loss, free of the trade-off between the detailed formulations. In order to obtain a high-quality reversed trigger, our proposed reverse engineering approach is composed of the following two steps, including the preliminary estimation of the trigger with a surrogate distribution"}, {"title": null, "content": "and further refinement with a differential generation process."}, {"title": "4.2. Trigger Estimation", "content": "Although in Equation 12, we built a unified loss to eliminate the difference in formulations for various attacks, it still needs further improvement to perform reverse engineering. The obstacle is that $x_t$ is unknown to defenders, which is simultaneously decided by the target images $x_o$ and the coefficient $c(t)$. However, the property of diffusion models guarantees that when t approaches T, $x_t$ will converge to the prior distribution that is little affected by $x_0$. Therefore, we can substitute $x_0$ with a surrogate image $\\hat{x}_0$ sampled from a substitute distribution, e.g., the standard gaussian distribution $P_{prior}$, to estimate $x_t$. Here, we also prove this property from a theoretical perspective:\nTheorem 4.1. Given the target image $x_0 \\sim P_{target}$ and a surrogate image $\\hat{x}_0 \\sim P_{prior}$, let pt and qt denotes the distribution of $x_0$ and $\\hat{x}_0$ at timestep t. we can prove that:\n$\\frac{\\partial D_{KL}(p_t||q_t)}{\\partial t} < 0$.\t(13)\nFor the proof of Theorem 4.1, please refer to Appendix A for details. Following , we first prove that the current backdoor diffusion processes are all Wiener Processes. Then we finished the proof with its property. Equation 13 means that the divergence between pt and qt will monotonically decrease with t during the diffusion process. Thus pt and qt will become indistinguishable when t is large. Therefore, for t \u2208 [T \u2013 \u03b4, T] and \u03b4 <T, we can substitute $x_0$ with $\\hat{x}_0$ and simplify Equation 9 to the following equation:\n$\\hat{x}_t^{(1)} = a(x_o, t)\\hat{x}_o + b(t)\\epsilon + r$.\t(14)\nHere we omit c(t) because c(t) \u2248 1 when t \u2208 [T \u2013 \u03b4, T]. Substituting xt in Equation 12 with $\\hat{x}_t^{(1)}$, and we can get:\n$\\mathcal{L}_1(r) = ||F_\u03b8 (\\hat{x}_t^{(1)}(\\epsilon_1, r), t) \u2212 f(\\hat{x}_t^{(1)}(\\epsilon_1, r), t), \\epsilon_1)- F_\u03b8(\\hat{x}_t^{(1)}(\\epsilon_2, r), t) + f(\\hat{x}_t^{(1)}(\\epsilon_2, r), t), \\epsilon_2)||^2 - \u03bb||r||_1$.\t(15)\nDirectly optimizing it with a commonly used optimizer such as SGD , we can preliminarily reverse the"}, {"title": "4.3. Trigger Refinement", "content": "Recall that in those early studies for diffusion models, the sampling processes are time-consuming because they follow the reversed Markovian chain, which consists of thousands of steps. To save the computational cost, following-up works, such as the Denoising Diffusion Implicit Model (DDIM) sampler propose that multiple denoised steps are equal to a non-Markovian process with fewer steps. It indicates that we can obtain high-quality images even with a few steps of sampling. Note that the operations in the denoised process are all differential. Thus it motivates us to estimate $x_t$ with multi-step generations. If $\u03a6_n(\u00b7)$ denotes n-steps DDIM sampler, we can obtain the target image $x_0$ with the trigger r:\n$\\hat{x}_0 = \u03a6_n(\\hat{r})$\t(16)\nSimilar to Equation 14, we can obtain a more precise formula for $x_t$ when $t \u2208 [T \u2013 \u03b4, T]$ and $\u03b4 \u00ab T$:\n$\\hat{x}_t^{(2)} = a(\u03a6_n(\\hat{r}), t)\u03a6_n(\\hat{r}) + b(t)\\epsilon + r$.\t(17)\nSubstitute xt with $\\hat{x}_t^{(2)}$, Equation 12 becomes:\n$\\mathcal{L}_{2,1}(r) = ||F_\u03b8(\\hat{x}_t^{(2)}(\\epsilon_1, r), t) \u2212 f(\\hat{x}_t^{(2)}(\\epsilon_1, r), t), \\epsilon_1)- F_\u03b8(\\hat{x}_t^{(2)}(\\epsilon_2, r), t) + f(\\hat{x}_t^{(2)}(\\epsilon_2, r), t), \\epsilon_2)||^2 - \u03bb||r||_1$\t(18)\nIn addition to the ending constraint for Equation 9, we can also simplify it with the beginning constraint: Know that $\\lim_{t\u21920} x_t = x_0$. Therefore, for $t \u2208 [0, \u03b4]$ and $\u03b4 \u00ab T$, $x_t$ can be approximated with $\\hat{x}_t^{(3)}$:\n$\\hat{x}_t^{(3)} = \u03a6_n(\\hat{r})$.\t(19)"}, {"title": null, "content": "Substitute xt with $\\hat{x}_t^{(3)}$, Equation 12 becomes:\n$\\mathcal{L}_{2,2}(r) = ||F_\u03b8(\\hat{x}_t^{(3)}(\\epsilon_1, r), t) \u2212 f(\\hat{x}_t^{(3)}(\\epsilon_1, r), t), \\epsilon_1)- F_\u03b8(\\hat{x}_t^{(3)}(\\epsilon_2, r), t) + f(\\hat{x}_t^{(3)}(\\epsilon_2, r), t), \\epsilon_2)||^2 - \u03bb||r||_1$\t(20)\nFor simplicity, we average L2,1 and L2,2 to get our final loss for trigger refinement:\n$\\mathcal{L}_{2}(r) = \\frac{1}{2} \\mathcal{L}_{2,1}(r) + \\frac{1}{2}\\mathcal{L}_{2,2}(r)$\t(21)\nFor the overall algorithm for trigger reversion, please refer to Appendix B.1 for details."}, {"title": "5. Backdoor Detection", "content": ""}, {"title": "5.1. Input Detection", "content": "As demonstrated in Section 2.2, in the inference stage, because the inputs for diffusion models are sampled noises instead of natural images, current input detection methods, including , fail to protect diffusion models from backdoor attacks. However, if we regard the reversed trigger as the mean of the backdoor distribution, we can further detect the backdoor input from the probabilistic perspective: Note that currently, there are two distributions obtained: One is the benign distribution $N(0, I)$, known to defenders even without defense and the other is the reversed backdoor distribution, $N(r, \u03c3^2)$. Here"}, {"title": null, "content": "\u03c3 is equal to I for Baddiffusion and VillanDiffusion. For TrojDiff, it is co-optimized with the triggers. Given any input noise $\\bar{\\epsilon}$, we can calculate its probabilities in the benign or backdoor distributions, which are denoted as $\u03a6_{be}(\\bar{\\epsilon})$ and $\u03a6_{bd}(\\bar{\\epsilon})$, respectively. Empirically, if $\\bar{\\epsilon}$ is a backdoor input, $\u03a6_{bd}(\\bar{\\epsilon})$ will be greater than $\u03a6_{be}(\\bar{\\epsilon})$ and vice versa. Therefore, we will keep $\u03f5$ whose $\u03a6_{be}(\\bar{\\epsilon}) \u2265 \u03a6_{bd}(\\bar{\\epsilon})$ and filter out those noises with $\u03a6_{be}(\\bar{\\epsilon}) < \u03a6_{bd}(\\bar{\\epsilon})$ because they might be backdoor inputs."}, {"title": "5.2. Model Detection", "content": "In , they propose Elijah, the first backdoor model detection method for diffusion models. They first generate the target images with the triggers and further perform backdoor model detection with additional assumptions for target distribution. First, they assume that target images are those with high similarity. Unfortunately, this contradicts the proposition by TrojDiff, in which they demonstrate that the attacks that include multiple target images can also be applied to implant backdoors for the diffusion models. In addition, because of the discrepancy between the reversed and the original triggers, target images can not be properly generated with multiple-step generations in some hard situations. Therefore, our proposed model detection method is performed in the trigger space rather than the image space.\nRecall that in Section 4.1, we prove that r is a non-zero minimum point for the lower bound in Equation 11. However, for the benign models, optimizing Equation 12 will finally converge to the point that is close to a full-zero tensor because there are non-zero solutions for them. Therefore, we introduce Kullback-Leibler (KL) divergence, a metric that measures the distance between the reversed distribution $N(r, \u03c3^2)$ and benign distribution $N (0, I)$. If r is flattened with a n-dimensional tensor, we can easily calculate the dimensional-wise divergence, $d_r$ between the known benign and the reversed distributions. Further, we can squeeze $d_r$ to a scalar by calculating its mean and variance over dimensions:\n$M_r = \\frac{1}{n} \\sum_{i=0}^{n-1} d_r[i]$,\t(22)\n$V_r = \\frac{1}{n} \\sum_{i=0}^{n-1} (d_r[i] - M_r)^2$\nFor the whole-image attacks, the trigger will cause a large $M_r$ because the offsets of distribution have appeared across the entire image. For the patch-based attacks, the trigger is only attached to a small region, which will lead to a large $V_r$. Only the benign models can obtain low values in both $M_r$ and $V_r$. In Figure 2, we show that the backdoor and benign models can be easily detected with these extracted features. If both benign and backdoor models are available for defenders, we can train a one-layer network for model detection. We also consider a benign-only (BO) scenario, in"}, {"title": null, "content": "which only benign models are accessible. We can calculate the mean and variance of Mr and Vr, denoted as $(\u00b5_m, \u03c3_m)$ and $(\u00b5_v, \u03c3_v)$. According to the 3\u03c3 criterion, any model that achieves Mr > \u00b5m + 3 * \u03c3m or Vr > \u00b5\u03c5 + 3 * \u03c3v will be regarded as the backdoor model."}, {"title": "6. Experiment", "content": ""}, {"title": "6.1. Experimental Settings", "content": "Dataset: Our experiments are mainly performed on the CIFAR-10 dataset. In Section 6.3, we extend our experiments to large datasets, including CelebA and CelebA-HQ .\nAttack: We evaluate the performances of our defense against all known pixel-level backdoor attacks for diffusion models, including BadDiffusion, TrojDiff, and VillanDiffusion. We select the DDPM as the victim model for both BadDiffusion and TrojDiff. For VillanDiffusion, the backdoor is inserted in EDM . To ensure a comprehensive and fair evaluation, on the CIFAR-10 dataset, we report the results that are the average of six different settings for each attack. For large datasets, all default settings from the original paper are included.\nDefense: As far as we know, Elijah is the first and only existing work that specifically designs backdoor defense for diffusion models and we select it as the baseline. For its hyperparameter setting, we keep in line with the original paper. As for our proposed TERD, the iterations for trigger estimation are 3000 and 1000 for further refinement. We choose SGD as our optimizer with 0.5 learning rate which is adaptively adjusted with the cosine learning rate schedule. The trade-off coefficient y is set as 5e-5 for CIFAR-10 and 5e-4 for larger datasets. \u03b4 is set as 0.01T and the step number, n, for multi-step generation is set as 10. For the model detection with a neural network, we trained the model with 5 benign models and 50 backdoor models which are poisoned by the grey-box-hat setting under the BadDiffusion attack. For the benign-only (BO) backdoor detection, we calculate the threshold with 100 benign models only which are trained with the baddiffusion open-source code.\nMetrics: To evaluate the performance of our proposed reversed engineering approach, we select the l\u2082 norm of the difference between reversed trigger r and the original trigger ro to access the quality of the method, denoted as $||r-ro||_2$. For the backdoor detection methods, we use TPR (True Positive Rate) and TNR (True Negative Rate): the proportion of the benign or backdoor input/model is successfully detected. For input detection, the metrics are calculated over 50000 points sampled from the benign or the backdoor distribu-"}, {"title": null, "content": "tions. For model detection, we report the results that include 100 benign models and 120 backdoor models (20 models for each of the settings). All experiments are performed on the NVIDIA A100 GPUs."}, {"title": "6.2. Main Result", "content": "We summarize the performances of TERD against current attacks on the CIFAR-10 dataset in Table 2. In addition, we compare TERD with Elijah from both the numerical results in Table 2 and empirical visualization in Figure 3. First, for the reversed engineering methods, the results reveal that compared to Elijah, our proposed TERD can more accurately reverse the triggers. It is because compared to Elijah, TERD not only establishes a unified loss for trigger reversion and considers both the initial and the ending conditions of current attacks. Besides, our proposed progressively reversed strategy can help us initially estimate the trigger and improve its quality with further refinement.\nAttribute to the success of our trigger reversion approach, our proposed backdoor detection method obtains 100% TPR and TNR in all settings. From the perspective of input detection, we successfully detect the noises sampled from the backdoor distribution with the calculated probabilities. As for model detection, considering we only include one setting of the BadDiffusion attack to train the detection model, our proposed defense shows its better transferability than Elijah across different settings within the same attack and the settings across attacks. With further analysis, we find the reason is that the quality of generated images with the reversed triggers by Elijah will severely decline in some circumstances. Instead of detecting the poisoned models with the generated images, our proposed TERD performs"}, {"title": "6.3. Performance on High-Resolution Dataset", "content": "In addition to small datasets e.g. CIFAR-10, recent advancements in diffusion models show their outstanding performances in high-resolution image generation . Unfortunately, recent studies show that backdoors can be successfully implanted even for those complex datasets. Therefore, it is necessary to evaluate TERD on large datasets to study whether it can provide assistance for diffusion models in all situations. With the open-source code provided by current attacks, we evaluate TERD on CelebA and CelebA-HQ datasets. Since our extracted features for model detection are agnostic to the image size, we use the same detection model and the threshold adopted by the CIFAR-10 dataset. The results are summarized in Table 3 and for all settings, we obtain 100% TPR and TNR. Note that the entry for attacks means the kind of attack, the victim model and the poison datasets. The results reveal that TERD is effective on high-resolution datasets and has good transferability across datasets. It means we can detect the backdoor models with TERD trained on large datasets with a detector, trained on small datasets. It can largely decrease the computation cost, considering training a diffusion model on large datasets usually requires huge computational resources."}, {"title": "6.4. Transferability to SDE-based Models", "content": "In , they propose an SDE-based framework to implant a backdoor for diffusion models. Previous"}, {"title": "6.5. Ablation Study", "content": "We study the effect of each component to the performance of our proposed defense. In addition, we consider defending attacks with varied trigger sizes and different poison rates. We report the results that are the averages of BadDiffusion, TrojDiff, and VillanDiffusion attacks.\nInfluence of each Component: We compare TERD with two variants, including (1) TERD with only TE (Trigger Estimation) applied. (2) TERD with only TR (Trigger Refinement) applied on the CIFAR-10 dataset. For both of the variants, we simply substitute the loss function of the removed stage with this of the kept stage and keep other hyperparameters unchanged. As shown in Table 5, although applying either TE or TR alone can yield decent performance, combining them together can obtain a more powerful defense: lower l\u2082 norm between the reversed and the"}, {"title": "6.6. Adaptive Attack", "content": "Because we perform the backdoor detection from the distribution view, one intuitive adaptive attack is when the benign and backdoor distributions are close enough, it might bypass our proposed defense. Therefore, we introduce the hyperparameter \u03b7 (0 < \u03b7 < 1), which scales the original trigger ro to \u03b7 * r, and evaluate the performance of TERD for each settings of the attack. We observe that when \u03b7 is extremely low, e.g. 0.1 for TrojDiff, the performance of TERD will degrade. Nevertheless, with further inspection in Table 10, we find that the benign utility"}, {"title": "6.7. Complexity and Time Cost", "content": "In previous sections, we illustrate the outstanding performances of TERD in various settings. Here, we analyze the complexity of TERD to investigate whether it is practical to deploy it in real life. For our proposed reversed engineering method, the time cost is the sum of those in both stages. First, for trigger estimation, because xt can be directly represented with one equation, the computational complexity for Equation 12 is O(1). If we denote the number of iterations for the trigger estimation as m\u2081, the computational complexity for trigger estimation can be represented as O(m1). For the trigger refinement stage, we can first obtain that the complexity for obtaining xo is O(n) because it needs n steps of generative iterations to obtain 20 and the complexity for each step is O(1). Following the previous analysis for trigger estimation, we can further obtain that the overall computational complexity for the trigger refinement stage is O(nm2) (m2 is the number of optimizations in the second stage.). Suming the results of both stages, the overall computational complexity for our method is O(nm2 + m1). For the analysis of the input and model detection, please refer to Appendix E for details.\nIn addition to the theoretical perspective, we also evaluate the time consumption with experiments. Evaluated on a single A100 GPU, we record the time consumed by TERD and the cost of training a diffusion model from scratch on the CIFAR-10 dataset in Table 6. Firstly, the results indicate that compared to the training cost of diffusion models, the cost for TERD is marginal (< 1%). This demonstrates the cheap computational cost of TERD, which can be afforded by most defenders. Secondly, it also reveals that the detection task can be finished in less than 0.003 seconds, demonstrating our proposed method is appropriate to deploy online. It will"}, {"title": null, "content": "have a negligible effect on the experience of users and can quickly finish the filtering mission even if thousands of user requests are sent to the central server."}, {"title": "7. Conclusion", "content": "In this paper, we propose TERD, a defense framework to protect diffusion models from backdoor attacks. First, we establish a unified form for current attacks and achieve an accessible loss for reversion by applying the triangle inequality. Furthermore, we develop a two-step trigger reversion algorithm, including estimating the trigger with a substituted distribution and refining its quality with a multi-step sampler. In addition, we propose the first input detection approach by comparing probabilities across distributions and a brand new model detection method by selecting the KL divergence between the reversed and benign distributions as the metrics. We hope TERD, including the trigger reversion and backdoor detection partitions, will serve as the cornerstone to improve the backdoor robustness of diffusion models in the future."}, {"title": "Impact Statement", "content": "Backdoor attacks have emerged as a significant threat to contemporary state-of-the-art diffusion models. In response, we propose the use of TER"}]}