{"title": "Adjusting Pretrained Backbones for Performativity", "authors": ["Berker Demirel", "Lingjing Kong", "Kun Zhang", "Theofanis Karaletsos", "Celestine Mendler-D\u00fcnner", "Francesco Locatello"], "abstract": "With the widespread deployment of deep learning models, they influence their environment in various ways. The induced distribution shifts can lead to unexpected performance degradation in deployed models. Existing methods to anticipate performativity typically incorporate information about the deployed model into the feature vector when predicting future outcomes. While enjoying appealing theoretical properties, modifying the input dimension of the prediction task is often not practical. To address this, we propose a novel technique to adjust pretrained backbones for performativity in a modular way, achieving better sample efficiency and enabling the reuse of existing deep learning assets. Focusing on performative label shift, the key idea is to train a shallow adapter module to perform a Bayes-optimal label shift correction to the backbone's logits given a sufficient statistic of the model to be deployed. As such, our framework decouples the construction of input-specific feature embeddings from the mechanism governing performativity. Motivated by dynamic benchmarking as a use-case, we evaluate our approach under adversarial sampling, for vision and language tasks. We show how it leads to smaller loss along the retraining trajectory and enables us to effectively select among candidate models to anticipate performance degradations. More broadly, our work provides a first baseline for addressing performativity in deep learning.", "sections": [{"title": "Introduction", "content": "Machine learning models have been experiencing a growing adoption for automated decision-making. High-stake applications necessitate models to generalize beyond the training distribution and perform robustly over distribution shifts. A prevalent but often neglected cause of distribution shift is the model deployment itself. When informing down-stream decisions, and actions, the predictions of machine learning models can change future data. Such patterns are ubiquitous in social settings, where algorithmic predictions impact individual expectations, steer consumer choices, or inform policy decisions. Similarly, standard community practices can lead to future data depending on the deployment of past models; this can be through data feedback-loops [78], active learning pipelines [71], and dynamic benchmarks [61]. Performative prediction [63] articulates how this causal link between predictions and future data surfaces as distribution shift in machine learning pipelines.\nIt is inevitable that repeatedly ad-hoc trained models become suboptimal after deployment under performativity [45, 66, 79]. Thus, a natural question to ask is- -can we learn to foresee these shifts? Of course, in full generality performative shifts can be arbitrarily complex. However, given a low-dimensional sufficient statistic for the shift, Mendler-D\u00fcnner et al. [56] show that a data-driven approach can be successful in anticipating performativity. Once the relevant mechanism mapping the model statistic to the induced data is learnt, shifts can be anticipated and the performative prediction problem can be solved offline [38]. While this approach is appealing theoretically, a demonstration of the practical feasibility in the regime of deep learning was still missing.\nIn particular, existing approaches [e.g., 56, 38] learn predictive models from scratch, assuming access to performativity-augmented datasets that contain statistics about the deployed model, in addition to feature-label pairs. In large-scale deep learning, this approach to anticipating performativity has two fundamental practical limitations. First, large-scale models are extremely data-hungry when trained from scratch, and performativity-augmented datasets are hard to gather and they are not yet widely available. Second, existing pre-trained models only process raw features and they are not compatible with this paradigm, preventing the utilization of valuable data resources and existing open source models. In this work we provide the first practical approach to building performativity-aware deep learning models around pre-trained backbones."}, {"title": "Our work", "content": "We propose a pipeline to adjust deep learning predictions for performative label shift. This refers to a setting where the deployment of a model changes the class proportions in future rounds. This setting is particularly relevant for vision and language tasks, where pretrained backbones prevail. For instance, adaptive data collection [73, 61] and performance-dependent participation [49, 18] are instances of this problem which has been under ongoing investigation [48, 50, 22].\nTo anticipate performative label shift, we propose a modular framework to equip existing pre-trained models with a learnable adaptation module. The adaptation module takes the sufficient statistic for the shift, and the pretrained model's intermediate representations as input and outputs adjusted predictions. In the concrete instantiation of performative label shift, the adaptation module learns to predict the label marginals and corrects for performativity post-hoc with a Bayes-optimal correction to the model's logits. More generally, our framework decouples the task of modeling the underlying concept from modeling performative effects. This has the crucial advantage that existing pre-trained models can be used for the former, and only the parameters of the latter are learned from performativity-augmented data, making it more practical and data-efficient.\nTo evaluate our approach, we draw upon connections between performativity and dynamic benchmarks [61] and simulate performative shifts over vision and language tasks through adversarial sampling. Our main empirical findings can be summarized as follows:\n\u2022 We demonstrate that our proposed adaptation module can learn the performative mechanism effectively from a few performativity-augmented datasets collected along a natural retraining trajectory.\n\u2022 The module enables us to adapt the predictor to future data before deployment, significantly reducing performance degradation due to performative distribution shifts, compared to state-of-the art fine-tuning techniques.\n\u2022 The readily trained adjustment module is flexible in that it can be combined with various pre-trained backbones allowing zero-shot transfer during model updates, e.g., when more performant backbones become available.\n\u2022 We show that our trained adaptation module can anticipate a model's brittleness to performative shifts before deployment, enabling more informed model selection.\nIn a nutshell, we offer the first baseline to effectively adapt state-of-the art deep learning models to performative distribution shifts. Along the way we highlight connections between performative prediction, state-of-the art fine-tuning techniques and their application in dynamic benchmarking, as well as several interesting opportunities for future work."}, {"title": "Background and related work", "content": "Perdomo et al. [63] introduce the framework of performative prediction to study performativity in machine learning. We refer to [26] for a comprehensive overview on related literature. The key conceptual component of the framework is to allow the data distribution to depend on the predictive model. A natural approach to deal with distribution shifts of all kind is to perform naive retraining. Interestingly, such heuristics can converge to equilibria under performativity [63, 56, 46, 17]. However, it is known that retraining can lead to suboptimal solutions even after convergence [63, 59]. Thus, a more ambitious goal is to anticipate performative shifts, instead of solely responding to them [59, 33]. In particular, Mendler-D\u00fcnner et al. [56] suggest treating predictions as features in a machine learning model, assuming that performativity is mediated by predictions. Kim and Perdomo [38] formalize requirements under which such a model allows for optimizing any downstream loss under performativity, also referred to as an omnipredictor [25]. Both of these approaches require a dataset containing information about the deployed model large enough to train a performativity-aware predictor from scratch. Unfortunately, such data is rarely available in practice, and the paradigm prevents the use of existing pre-trained models and benchmark datasets as they lack such information. To the best of our knowledge, we are the first to offer a solution that allows to build on existing pretrained-backbones towards this goal.\nWe primarily focus on label shift in this work. Label shift refers to the shift of the marginal distribution P(Y), while the class conditionals P(X|Y) remain fixed [55, 75, 84, 48]. In contrast to prior work on model-induced shifts, focusing predominantly on covariate shift [e.g., 27] and concept shift [56, 38], our focus on deep learning applications puts forth this novel and important dimension of label shift. While concept shift would mean, e.g., a change in the image labeling function, label shift means a change in the sampling procedure, making it much more ubiquitous. Performance degradation due to label shift has been a long-standing problem for computer vision tasks [57, 36, 53, 14, 72, 20, 86], with a plethora of principled approaches to correcting the label shift through unlabeled test data [2, 23, 21]. However, existing efforts do not consider the dynamic interplay between model deployments and induced shifts. Our work aims to address this issue and provide initial empirical baselines.\nA practical setting where performative label shift surfaces are adaptive data collection settings. Here performativity is a response to the predictive performance of the model. For example in active learning [71] data samples are collected to obtain information in high uncertainty regimes of the current model. Dynamic benchmarks [61] suggest designing datasets adaptively to challenge prior models. Approaches to mitigating fairness issues [1, 24] suggest collecting data for groups on which"}, {"title": "Anticipating performativity", "content": "Performative distribution shifts are caused by model deployment. Thus, having access to the right statistic about the model is in principle, sufficient to foresee performative shifts. This is the core idea making it possible to anticipate performativity, in contrast to arbitrary distribution shifts. Making this more practical is the challenge we tackle in this work. Figure 1 illustrates our proposal.\nProblem setup. We consider discrete time steps, indicating the deployment of model updates. In each step t\u2265 0, first, a dataset of feature label pairs (X, Y) is collected. We consider a classification setting with X \u2208 Rd and Y taking on K discrete values. We use Pt to denote the distribution over data points at time step t. Then, a new model ft is trained to predict Y from X. The model ft is deployed and t is incremented. The new distribution Pt+1 is fully characterized by a sufficient statistic St, which is a function of ft and Pt. This corresponds to a stateful extension of the framework by [63], using a Markovian assumption similar to [9]:\n$P_{t+1}(X,Y) = P(X,Y|S = S_t)$ with $S_t = Stat(f_t, P_t)$  (1)\nAn example of a sufficient statistic could be the model predictions over the previous data [56, 38], or model accuracy across subgroups [61]. Such statistics are typically significantly lower-dimensional than the raw parameters of Pt and ft (and avoid explicit parametric assumptions for Pt). In the following, we assume that, through expert and domain knowledge, we can specify such a statistic. This means we assume the model developer knows, for example, that predictions are causing the shift, rather than the specifics of the model parameters themselves. We leave for future work the possibility of identifying such statistics from data in settings where such knowledge can not be assumed. Following the notion of independent causal mechanisms [70, 64], we assume that the mechanism underlying the distribution shift is fixed and shifts only manifest through instantiations of S.\nPractical challenges. Given a statistic S, anticipating performativity means to predict Y from X taking the instantiation of S into account. This corresponds to learning a performativity-aware predictor of the form\n$f_{perf}: (X, S) \\rightarrow Y$.\nToward this goal, we highlight two important practical challenges:"}, {"title": "A modular adaptation architecture", "content": "Our goal is to develop an architecture to model fperf that uses fpre as a building block. That is, we consider functions of the form\n$f_{perf} (X, S) = F({f_{pre}^{(k)}} (X)}_{k\\geq0}, S)$, (2)\nwhere fpre denotes the pre-trained model's representation at layer k. The adapter module F acts on top of the pre-trained backbone fpre, potentially accesses its intermediate layer representations, and incorporates the statistic S to adjust the model's outputs for performativity.\nThe adapter module reduces to a scalar function if it operates only on top of the pretrained model's predictions, such as the case for self-negating and self-fulfilling prophecies [8, 4], or reflection effects [54]. At the same time, the mechanism mapping X to Y could be arbitrarily complex, and X be high dimensional, such as for image or text. Thus, decoupling the feature extraction step from the performative mechanism can come with a significant reduction in complexity for learning the latter, using fpre as a building block, instead of learning both jointly. Typically, the adapter is given access to more layers of the backbone for the sake of expressivity. At the extreme it get access to the backbone's input, allowing it to learning the performativity-aware predictor from scratch. With access to more information, the complexity, as well as data requirements for learning the adapter module will naturally increase, offering a useful lever to strategically trade-off assumptions and evidence, and to adapt the module to the availability of performativity-augmented data."}, {"title": "Anticipating performative label shifts", "content": "Label shift focuses on the effect of deploying ft on the marginal distribution Pt+1(Y|St) [84, 48]. For a discrete classification task, the marginal over the outcome can be concisely represented with a probability vector A\u2208RK where K denotes the number of classes, and each entry of A specifies the corresponding class probability. Thus, anticipating performativity is equivalent to anticipating changes to \u039b.\nAt the core of the adapter module is a neural network T : S \u2192 \u00c2 that predicts the label marginals A from the sufficient statistic S. These estimates can be used to anticipate the deployment of future models and adapt the predictions by accessing the pretrained-model's logits. More formally, we implement the following adjustment:\n$f_{perf} (X, S_t; T) = \\arg \\max_i \\delta_i(S) [f_{pre}(X)]_i \\text{ with } \\delta_i(S) := \\frac{[T(S)]_i}{A_{pre}},$ (3)\nwhere Apre is denotes the label marginals over the training data of fpre. This expression fully decouples the mechanism underlying the shift from the feature-extraction part on the input. The next result shows that for a well trained T and a good pretrained model, such an adjustment can indeed be optimal under label shift."}, {"title": "Learning adapter module along the retraining trajectory", "content": "Algorithm 1 illustrates a muti-step protocol for training the neural network T to predict the next round's label marginals. In each round fresh data under the deployment of a new model is collected and used to update T. More specifically, in each round, we collect the statistic St-1 of the deployed model ft-1, together with the induced label marginals At over Pt and store it in a memory buffer to learn the predictor T in a supervised manner. Algorithm 1 aggregates data along a natural retraining trajectory, where the previous round's adjusted predictor is deployed repeatedly. This is reflected by S = Stat(ft-1, Pt-1) defining the next distribution.\nEnd product. Our algorithm outputs the trained module T that serves to construct a performativity-aware predictor and to anticipate the performative label shift of future deployments. Once T is known, the consequences of a model deployment can be anticipated before actually putting it out in the wild, simply by feeding the model's statistic into the adjustment module to predict the consequences. While we focus on predictive accuracy as a metric in this work, the same procedure could be used to directly measure class imbalances after deployment, and account for the desire to reflect different groups equally well in the data [82], or other societal desiderata [15, 5]."}, {"title": "Experiments", "content": "We empirically investigate the performance of our adapter module under performative label shift for vision and language classification tasks. For vision, we evaluate our model on CIFAR100 [44], ImageNet100 [12], and TerraIncognita [6]. For language, we use Amazon [60] and AGNews [85]. We evaluate the performances of different baselines in a semi-synthetic setting where we simulate model deployments and performative shifts across multiple rounds of retraining.\nBaselines. We use three different baselines for adjusting a model to performative distribution shifts: Oracle Fine-tuning, Oracle Distribution, and No Adaptation. All of them start with the deployment of a pretrained model and then tackle performative shifts in their own way.\n\u2022 Oracle Fine-tuning adapts the pretrained model by training it with complete information about current round's (x, y) pairs for 25 epochs after observing the shift. While this approach ensures convergence on the available data and allows the model to continuously learn from an expanding number of samples across rounds, it may be computationally costly and potentially overfits to the current distribution, which increases its sensitivity to distribution shifts. In other words, Oracle Fine-tuning updates ft in each step to fit the current distribution Pt(X,Y).\n\u2022 Oracle Distribution uses the true label marginals to adjust the pretrained model's predictions instead of the estimates from the adapter module. It serves as an upper bound.\n\u2022 No Adaptation uses a fixed pretrained model without making any adjustments for the performative distribution shifts over rounds. This baseline provides a reference point for evaluating the value of adaptation strategies in handling performative shifts."}, {"title": "Performative label shift", "content": "We simulate performative label shift caused by a model's predictive accuracy in previous rounds, as observed in the context of dynamic benchmarks [73], adversarial sampling [61] and self-selection [29], see discussion in Section 3.2. Thus, we use the model's class-wise accuracy as a sufficient statistic for the shift, i.e.,\n$S_t := [Acc_t[0], Acc_t[1], ..., Acc_t[K - 1]],$ (5)\nwhere Acct[i] represents the accuracy of class i after model deployment at time step t.\nTo simulate the performative effect, we pass the current rounds class accuracy through a Softmax to obtain the proportion of each class in the next round. Specifically, for any class i the class proportion in round t + 1 is chosen as\n$P_{t+1}(Y = i|S) = \\exp(S_i/\\tau) {\\sum_{j=1}^K \\exp(S_j/\\tau)}^{-1},$ (6)\nwhere \u03c4 \u2260 0 parameterizes the shift and Acc[i] represents the accuracy of class i. The equation in (6) is a modeling choice for simulating the effect which is hidden to the algorithm. For \u03c4 < 0 it emulates the adversarial setting where classes that achieve high accuracy in the past round will diminish in the next round, and vice versa. In contrast, for \u03c4 > 0 classes with higher accuracy would be stronger represented in the next round, accumulating mass in small regions of the input space, making the task trivial.\nStrength of performativity. We use the parameter \u03c4 to simulate different strengths of performativity. We selected three different values for \u03c4, chosen to induce absolute accuracy drops of 2%, 5%, and 10% after observing a balanced distribution for each model and freeze \u03c4 thereof. We refer to these as the low, moderate, and high shift scenarios, respectively.\nEvaluation metric. We simulate each model's retraining trajectory for 200 steps and we repeatedly evaluate the accuracy after deployment, denoted as Acct = Acc(ft, Pt+1). Note that this implies that all models encounter different distributions after the first round. For reference we evaluate all models on the initial balanced distribution at t = 0. For each model, we compare the performance on the trajectory of distributions induced by the respective model to avoid bias toward any particular approach. In addition, we also analyze the utility of the reusable adapter module resulting from the PaP procedure."}, {"title": "Empirical findings", "content": "We conduct experiments with performative label shift, as instantiated above, with varying parameters, applied to data of different modalities.\nRetraining trajectory. Figure 2 shows the experiments conducted on ImageNet100 and CIFAR100 datasets. We observe that our adapter module (PaP) demonstrates comparable performance to Oracle Fine-tuning even in the low shift scenario, yet without the more resource-intensive demands in terms of time and compute. High shift scenarios reveal the sensitivity of fine-tuning strategy even when it has complete information about the samples collected throughout rounds. Adopting fine-tuning makes the model lean heavily towards the previous round's class marginals, making the model vulnerable to the upcoming distribution shift. Instead, PaP leverages the causal relationship between performance and subsequent distribution, relying solely on the label marginals from previous rounds to model this relationship. In Figure 5 we show the average accuracy improvement of different approaches over No Adaptation. We see significant average accuracy improvements of 3.31% and 4.25% for PaP on CIFAR100 and ImageNet100, respectively. While these improvements fall short of the Bayes-optimal update's enhancements of 9.4% and 6.88% on the same datasets, they underscore the effectiveness of our adapter module in approximating the causal mechanism. Additionally, its"}, {"title": "Modularity and zero-shot model updates", "content": "We demonstrate the modularity of our approach in Figure 4 under high label shift. We first trained the adapter module compined with a ResNet18 backbone at high shift on ImageNet100. Then, using the same pretrained frozen adapter, we simulated 200 rounds starting with ResNet18. Over the rounds, we switched the deployed backbone from ResNet18 to ResNet34, and then from ResNet34 to ResNet50. Since the predictor captures the inherent relationship between the sufficient statistic (i.e., class level accuracies) and the label marginals, it is not coupled with the specific model it attaches to and continues to improve with its corrections. Consequently, practitioners can update the current model if a more suitable one becomes available at any time during deployment cycles, without the need to retrain the predictor.\nAnticipating performativity. We demonstrate that the learnt adapter module PaP can effectively anticipate the future performance of a model before its deployment, providing valuable information for model selection. Our experiment involves training various models with different random initializations. For each model, we evaluate its performance on a balanced dataset (pre deployment), then deploy it and measure performance again on the induced data (post deployment). In parallel, we use our learnt adapter model to predict post deployment performance, given only the sufficient statistic, and sample access to the current distribution. In Table 1, we compare our anticipation with the actual performance. We can observe that our approach provides a much better estimate than the initial performance. Importantly, the ranking based on our estimates exactly matches the true shift performance ranking, which cannot be inferred from first round performances alone. For example, Model 1 initially outperforms Models 2 and 3. However, the nature of the performative shift affects"}, {"title": "Beyond label shift", "content": "As a more general setting, we combine label shift with domain shift. For illustration purposes, we simulated an almost extreme scenario. Specifically, we randomly selected two domains and sampled data points exclusively from these domains. Using the TerraIncognita dataset [6] and employing the same experimental setting over 200 rounds, we evaluate the average accuracies across rounds with their standard errors in this scenario. The Performativity-aware Predictor achieves an average accuracy of 78.25 \u00b1 3.24, outperforming the No Adaptation case with 75.39 \u00b13.32 and the No Adaptation (only label shift) case with 75.89 \u00b1 1.31. The high standard error shows that the presence of simulated domain shift results in higher fluctuation in performance, reflecting the extremity of our simulation. However, assessing the average accuracy performance reveals that the effect of additional domain shift on performance is not highly significant. Furthermore, one can see the effectiveness of using the adapter module compared to No Adaptation when both types of shifts are present. PaP outperforms no adaptation cases by almost 3%, both in the presence of label shift alone and when both shifts are combined. This demonstrates that our adapter module designed to correct for performative label shift remains effective even in the presence of additional sources of shifts on the input distribution."}, {"title": "Conclusion", "content": "This work investigates performative prediction in deep learning. We design the first practical algorithm to adjust pre-trained models for performativity that is compatible with existing deep learning assets. We motivate the use of modular architectures to increase data efficiency and evaluate our approach under performative distribution shifts arising in typical dynamic benchmark settings. On multiple vision and language datasets with different types of shifts, we observe consistent performance gains along the retraining trajectory compared to standard baselines for the same adjustment module applied to different backbones. Finally, we illustrate how the adapter can be used for model selection under performativity to enable more informed model deployments and anticipate unwanted consequences."}, {"title": "Limitations and extensions", "content": "Overall, our work is the first tackling performativity in deep learning. Thus, there are countless possible extensions of our method. We demonstrated the feasibility of designing a modular architecture in the context of performative label shift by accessing the logits of the pre-trained models. An interesting and natural direction could be to capture and adjust representations, closer to the input level. This would allow to account for more complex shifts, and offers a natural lever to trade off expressivity of the adapter and sample requirements. Further, our approach critically assumes known statistics, which are easily encoded in the label shift setting we consider, and easy to reconstruct with minimal knowledge about the paradigm. An interesting extension is to learn such statistics, perhaps leveraging causal representation learning tools [70]. Besides being more general, it could also serve open vocabulary tasks [65], where even labels shifts would be challenging to characterize with a finite dimensional vector, or even generative modeling. Another unanswered question is to derive theoretical guarantees for learning the underlying performative mechanism such as causal identification guarantee, similar to [56], as well as sample complexities. These results can potentially guide more data-efficient algorithms, or more effective strategies to select the sequence of models to deploy during the training phase of the adapter module, akin to [33]."}, {"title": "Appendix", "content": "Here we report implementation details omitted from the body of the paper due to space limitations. We first give details about the datasets used, then explain training details of our approach.\nDataset Details\n\u2022 ImageNet100: The ImageNet100 dataset [12] is a subset from the ImageNet Large Scale Visual Recognition Challenge 2012. It contains random 100 classes, each having 1350 samples with resolution 3 x 224 \u00d7 224.\n\u2022 CIFAR100: The CIFAR100 [44] dataset has 60,000 images with 100 different classes and resolution 3 x 24 x 24.\n\u2022 TerraIncognita: The TerraIncognita dataset [6] consists of wild animal photographs with 4 domains based on the location where the images were captured. It contains 24, 788 images with a resolution of 3 x 224 \u00d7 224 and 10 classes.\n\u2022 Amazon: The Amazon review dataset [60] is a text classificaiton dataset containing reviews for products together with the scores from the users. It has 4,002,170 reviews with 5 classes.\n\u2022 AGNews: The AGNews dataset [85] consists of a collection of collection 127,600 news articles with 4 classes.\nTraining Details. We use a train-test-split with ratios 0.4, 0.3 and 0.3 respectively. Each dataset is treated as a data pool for sampling. To compute the initial performance of the pretrained model and generate the first statistic (class-level accuracies) we sample instances using a Dirichlet distribution. Choice of parameter \u03b1 for the distribution guides the skewness of the initial distribution for the initial model. We set \u03b1 = 100 to evaluate the initial model on a fairly balanced dataset. For each round, we sample 1,000 train and validation samples and 2,000 test samples from the data pools to simulate the round. Each iteration of the loop (rounds) follows: (1) evaluation of the existing model on the current distribution, (2) updating the model using the current distribution, (3) computing the statistics using the updated model on the current distribution. The computed statistics in the final step determine the next distribution and these steps are repeated over 200 rounds. Baselines differ based on their approach to step (2). Oracle Fine-tuning uses train and validation set to fit to the current distribution. No Adaptation skips that step and Performativity-aware Predictor adds previous round statistic, current label marginal pair to its memory buffer and make a pass over it to update the label marginal predictor. This memory buffer simulates epoch-like training for the label marginal predictor. Since it passes over the first pair it has added to the memory buffer many times, we apply a scaling to balance sample importance exponentially with a decay factor 0.995. For the vision experiments we use a cosine annealing learning rate"}, {"title": "Additional Experimental Results", "content": "Label shift on TerraIncognita dataset. Figure 6 shows a similar pattern as of the previous experiments under the label shift setting in the high shift setting. However, for the moderate and low shift settings, it can be seen Oracle Fine-tuning continues to improve its performance over rounds varying training epochs on the current distribution for Oracle Fine-tuning controls distribution bias incorporation. Figure 7 illustrates the effect of different training epochs on the current distribution. In our main experiments, we trained until convergence using the current distribution dataset. Setting the number of epochs to 0 reduces Oracle Fine-tuning to No Adaptation. Our ablation study compares training for 5 and 40 epochs. As expected, in high performative shift scenarios, partial fitting to the current distribution (5 epochs) outperforms full fitting (40 epochs) due to significant differences between consecutive distributions.\nPaP is a lightweight adaptation module. Table 3 compares the number of trainable parameters and training FLOPs across baselines. PaP offers negligible computational cost while providing (i) adaptation for models under performative label shift, and (ii) evaluation of multiple potential models for deployment selection.\nPaP can be used for pre-deployment performance evaluation. Table 4 expands on Table 1, demonstrating that PaP rankings closely align with true shift performance. Moreover, PaP provides more accurate estimates of post-shift performance compared to initial model evaluations on the first distribution."}]}