{"title": "Decoding Game: On Minimax Optimality of Heuristic Text Generation Strategies", "authors": ["Sijin Chen", "Omar Hagrass", "Jason M. Klusowski"], "abstract": "Decoding strategies play a pivotal role in text generation for modern language models, yet a puzzling gap divides theory and practice. Surprisingly, strategies that should intuitively be optimal, such as Maximum a Posteriori (MAP), often perform poorly in practice. Meanwhile, popular heuristic approaches like Top-k and Nucleus sampling, which employ truncation and normalization of the conditional next-token probabilities, have achieved great empirical success but lack theoretical justifications. In this paper, we propose Decoding Game, a comprehensive theoretical framework which reimagines text generation as a two-player zero-sum game between Strategist, who seeks to produce text credible in the true distribution, and Nature, who distorts the true distribution adversarially. After discussing the decomposibility of multi-step generation, we derive the optimal strategy in closed form for one-step Decoding Game. It is shown that the adversarial Nature imposes an implicit regularization on likelihood maximization, and truncation-normalization methods are first-order approximations to the optimal strategy under this regularization. Additionally, by generalizing the objective and parameters of Decoding Game, near-optimal strategies encompass diverse methods such as greedy search, temperature scaling, and hybrids thereof. Numerical experiments are conducted to complement our theoretical analysis.", "sections": [{"title": "Introduction", "content": "Decoding strategies underpin the mechanism of generating a text sequence from a given language model, and there-fore become an essential component of modern Large Language Models (OpenAI, 2024). Specifically, given an autore-gressive language model P which encodes the conditional next-token probability P(Xt|X<t), one aims to generate a high-quality sequence (X1, ..., X\u2081) by some strategy based on P. Perhaps one of the most straightforward strategies is Maximum a Posteriori (MAP), looking for the most probable sequence, i.e., the one with the maximum predicted like-lihood P(X1, . . ., \u0425\u0442). Considering the computation cost of an exact MAP, one would naturally turn to some heuristic surrogates such as greedy search and beam search (Graves, 2012; Sutskever et al., 2014).\nCounter-intuitively, numerous empirical studies have reported that such likelihood maximization strategies usually lead to low-quality, degenerate texts, even with heavily trained state-of-the-art language models (Hashimoto et al., 2019; Holtzman et al., 2020). Instead, sampling methods that randomly select the next token are observed to yield better outputs. Popular strategies include Top-k sampling (Fan et al., 2018), Nucleus (Top-p) sampling (Holtzman et al., 2020), \u03b7 sampling (Hewitt et al., 2022), and Mirostat sampling (Basu et al., 2021), among others. They follow a truncation-normalization design, sampling the next token from a truncated distribution by removing the tail probabilities and rescaling the remaining probabilities by a normalizing constant. Some other methods (Finlayson et al., 2024; Meister et al., 2023) also rely on truncation but may discard high-probability tokens besides the tail; see Section 2 for details. Formally, if (p1,..., pa) is the vector of the predicted probability of all candidate next-tokens {1, ...,d}, these methods decide an index set S and sample a token i with probability\nqi \u221d pil(i\u2208S)."}, {"title": "Motivation and our framework", "content": "First thought. At first sight, a statistician may naturally relate these truncation methods with the concept of sparsity and regularization, and further attempt to handcraft a constrained or penalized optimization problem where they are optimal strategies. This is easier than it may sound: for example, we can design a distance metric so that Top-k sampling is the best sparse approximation to the original distribution according to this metric, such as lo distance that directly controls sparsity. The major flaw of such approaches is that their objectives and constraints mostly come from non-principled, reverse engineering and lack statistical motivations. Therefore, they may not be able to provide theoretical insight into questions like why sparse solutions are favored, and why we should adopt a specific regularization term and distance metric.\nSecond thought. Let us restart from the most significant observation that likelihood-maximization approaches fail in practice. What does this imply? If P is the true distribution of natural language, it is reasonable to expect that the trained language model P is away from P. This makes P-likelihood an unreliable criterion of a generated text, and hence leads to the failure of likelihood maximization. On the other hand, the appropriate criterion a strategy would like to maximize is the P-likelihood of a generated sequence.\nHowever, note that for generality, we restrict ourselves from assuming too many structures on the true distribution IP, except for its bounded deviation from P. This \u201cmodel-free\u201d setup brings an adversarial nature to text generation: in the worst case, IP can try its best to degrade the quality of our generated text within its distance budget.\nDecoding Game. These ideas lead to our proposal, Decoding Game, a two-player zero-sum game between Strategist (S) and Nature (N). In this game, player S chooses a (randomized) decoding strategy to generate a text sequence that, in expectation, achieves good log-likelihood in the true distribution. On the other hand, player N is always able to shift the true distribution adversarially to reduce the text quality. Knowing the decoding strategy beforehand, player N chooses the worst-case true distribution P. Formally, a T-step Decoding Game is represented by\n$\\max_{Q} \\min_{P \\in \\mathcal{N}(P)} E_{Q} \\log P(X_1, ..., X_T|X_0),$ where, conditioned on a given prompt X0, Q is the probability measure on (X1, . . ., XT) induced by the decoding strategy of player S, and N(P) refers to a neighborhood of P. The objective of the game is the true log-likelihood of a length-T sequence generated from strategy Q, in expectation.\nIf there is no adversary (P = P always holds), then the game reduces to P-likelihood maximization, and naive MAP is exactly the solution. However, when an adversary is present, MAP becomes sub-optimal and the game invites more interesting consequences."}, {"title": "Contribution", "content": "In the rest of this paper, we conduct in-depth investigations into Decoding Game. The main results include:\n\u2022 Decomposibility of multi-step Decoding Game. In Section 3, we identify a recursive structure of the multi-step Decoding Game, and argue the computational intractability of obtaining a global solution in modern LLMs. Instead, we construct an inductive decision process that involves solving an one-step Decoding Game at each timestep.\n\u2022 Optimality of heuristic strategies under implicit regularization. Under total variation (TV) distance, we provide closed-form solutions to the one-step Decoding Game for both players. In Section 4.1, we show that the optimal strategy of player N imposes an l\u221e-type regularization on the log-likelihood. As a result, truncation-normalization sampling strategies emerge as first-order approximations to the optimal strategy of player S; see Section 4.2."}, {"title": "Related works", "content": "Theoretical explanations for decoding methods have been very sparse and existing works are relatively limited. Known perspectives presented in literature include (1) overestimation of token probabilities, (2) surprisal and perplexity of gen-erated text, and (3) implicit regularization on MAP.\nFollowing the long-held intuition (Holtzman et al., 2020) that language models tend to assign excessive probability to the unreliable tail, Finlayson et al. (2024) explained truncation as a remedy for this problem, showing that it can correctly discard the tokens out of the support of the true distribution when overestimation is upper-bounded. They further credited overestimation to the Softmax Bottleneck (Yang et al., 2018) brought by the language model architecture, motivating a new truncation mechanism that may remove high-probability tokens besides the tail. However, this structural assumption may not well account for the broadness of the source of overestimation. Additionally, it is unclear why rescaling all the remaining probabilities by the same constant is considered the best approach after truncation.\nA similar idea of identifying the correct support has also driven prior works such as Hewitt et al. (2022) and Meister et al. (2023). Hewitt et al. (2022) modeled the predictions as a mixture of true distribution and uniform-like smoothing distribution, and viewed truncation as a way to desmooth the output. Meister et al. (2023) proposed to compute the support that better aligns with the information-theoretic metrics of human text measured by token surprisal and entropy, under assumptions on the behavior of human speakers. In this method, high-probabilty tokens may be discarded as well.\nBasu et al. (2021) theoretically derived the perplexity of various sampling methods under the statistical assumption that next-token probabilities follow a Zipf distribution, comparing how the hyperpamameter of each method influences the order of perplexity. This particular Zipfian assumption may not fully capture the complicated probability distributions encoded by modern language models.\nAn earlier work (Meister et al., 2020) attempted to explain heuristic methods such as beam search as an implicit reg-ularization imposed on MAP. However, the design of regularization term seems to lack statistical motivations. They also suggest a qualitative relationship between the regularization term and the uniformity of surprisal of generated sequences, but the detailed mechanism is not well understood mathematically.\nOverall, each of them motivates the design of heuristic decoding methods to a certain extent, but to the best of our knowledge, we are not aware of any comprehensive theoretical framework that establishes optimality results."}, {"title": "Text generation as decision making", "content": "Another line of work, though not directly working on the theory of heuristic decoding schemes, views the problem of text generation as optimizing the policy of a decision-making agent working in an environment with or without adversary. This perspective resonates with our rationale behind the Decoding Game. For example, Jacob et al. (2024) proposed a game between a generation strategy and a text quality discriminator, and empirically demonstrated the advantage of the decoding strategy at the Nash equilibrium of this game in multiple tasks. Other recent works such as Kim et al. (2023); Mudgal et al. (2024); Snell et al. (2023) modeled next-token generation as (token-level) Markov decision processes, and applied reinforcement learning techniques for controlled decoding."}, {"title": "Robust optimization and regularization", "content": "Our framework also draws a connection to robust optimization (Ben-Tal et al., 2009), which aims to find solutions with sta-ble performance under data uncertainty or perturbations. Given the adversarial nature of uncertainty, robust optimization is typically formulated as a minimax problem that seek the best response to the worst-case data realizations. Interestingly, while the sparsity brought by truncation sampling can be seen as regularization, it is known that regularization and robust-ness are strongly correlated in various machine learning problems (Bertsimas et al., 2011; Derman et al., 2021; Shaham et al., 2018), as solving an optimization problem with regularization is equivalent to solving its non-regularized robust counterpart. The theory developed in this paper also confirms such an equivalence between robustness and regularization."}, {"title": "Formulation", "content": "Throughout, we use boldface letters to represent vectors and vector-valued mappings, and use blackboard bold letters to represent probability measures and expectations. For a sequence (x0,x1,...,xT), we define x<t = (x0,..., Xt\u22121). For a vector a = (a1,..., ad), a1:i = (a1,..., ai) is the vector extracting its first i components. The lp norm (p \u2265 1) of a is defined as ||a||p = (\u2211di=1 |ai|p)1/p, with l\u221e norm ||a||\u221e = maxi\u2264d|ai|. The total variation (TV) distance between two probability vectors p, q is defined as dTV (p,q) = \u00bd ||p \u2212 q||1. For a function f : R \u2192 R, f(a) represents the elementwise application of f to the vector a, and a/b represents the elementwise division of a by b. For a finite set V, \u2206(V) denotes the probability simplex of dimension |V|, and Vt denotes the Cartesian product V \u00d7 \u22c5\u22c5\u22c5 \u00d7 V (t times). We always assume that optimization variables have to satisfy probability constraints (e.g. lying in the probability simplex, or the space of probability measures), and will not specify them explicitly for conciseness."}, {"title": "Probability structure of text generation", "content": "Suppose V = {1, 2, . . ., d} is the vocabulary of all d tokens, and we are interested in generating T tokens beginning with the prescribed context X0 = x0 = (prompt, (BOS)). For a random variable sequence (X1,..., XT) taking values in V, its joint law under a measure Q can be decomposed into a collection of vector-valued mappings based on conditional probability rules. Specifically, for any t < T, let qt : Vt\u22121 \u2192 \u2206(V) that maps a length-(t \u2212 1) sequence x<t to the vector qt (x<t) describing next-token probabilities, which is defined elementwise by\nqt(x<t)xt = Q(Xt = Xt|X<t = X<t), \u2200x<t \u2208 Vt\u22121.\nNote that the subscript xt refers to the xt-th coordinate of the vector, and specially x<1 = x0. Then, the vector-valued mappings q1,..., q\u012b constitute an exact representation of Q, since for any (x1,...,xT) \u2208 VT,\nQ(X\u2081 = x1,..., X\u0442 = x\u0442|X0 = xo) = q1(X<1)x1q2(x<2)x2\u2022\u2022\u2022qT(X<T)x\u2533.\nSimilarly, for t < T, q1, . . ., qt represents the marginal law Q(X1,..., XtXo).\nDetermining such a mapping qt : Vt\u22121 \u2192 \u2206(V) is equivalent to assigning a probability vector to each qt(x<t) for each x<t \u2208 Vt-1. From a graph-theoretic perspective, by designating all the mappings, we are assigning probability weights to all the edges of a uniform d-aray tree of depth T.\nFrom now on, we use the notation Q and (q1, ..., qT) interchangeably, and likewise for other probability measures such as P \u2194 (p1, ..., pt) and P \u2194 (P1,...,\u0440\u0442)."}, {"title": "Multi-step Decoding Game", "content": "We describe the T-step Decoding Game in full detail. Suppose the natural language follows a true distribution P. Upon training, we obtain a language model P that approximates the true P. Beginning with a prescribed context Xo = (prompt, (BOS)), we generate a sequence (X1,..., XT) with a possibly randomized decoding strategy, which is rep-resented by another measure Q such that next tokens are selected with probability Q(Xt|X<t). We can then evaluate the quality of the generated sequence by testing whether the true distribution P is also likely to yield such a sequence. Specifically, for a typical sequence generated from Q, we use its log-likelihood in P-measure as the criterion\n$\\mathcal{L}_T (Q, P) = E_Q \\log P(X_1, ..., X_T|X_0),$ which is also the negative cross-entropy between Q and P when viewed as distributions on VT.\nThe T-step Decoding Game is a two-player zero-sum game on this criterion between Strategist (S) and Nature (N), where player S chooses Q (by choosing a decoding strategy) to maximize the criterion of the generation, while player N chooses P within a neighborhood of P to minimize it. This gives rise to the following formulation:\n$\\max_{Q} \\min_{P \\in \\mathcal{N}(P)} \\mathcal{L}_T (Q, P) = \\max_{Q} \\min_{P \\in \\mathcal{N}(P)} E_{Q} \\log P(X_1, ..., X_T|X_0).$ We define N (P) to be P {P : d(P, P) < \u20ac}, the e-ball around P in the sense of the following TV-sup norm:\u00b9\nd(P, P) = ||P \u2013 P||TV,\u221e := max{dTV (pt(x<t), pt(x<t)): x<t \u2208 Vt\u22121,t < T}.\nLikewise, we can define the distance between two mappings\nd(pt, Pt) = ||Pt - Pt||TV,\u221e := max{dTV (pt(x<t), Pt(x<t)) : x<t \u2208 Vt\u22121}.\nThen immediately, N(P) is a direct product of e-balls around Pt, namely\n$\\mathcal{N}(P) = \\mathcal{N}(p_1) \\times \\mathcal{N}(p_2) \\times \\dots \\times \\mathcal{N}(p_T), \\mathcal{N}(p_t) = \\{p_t : d(p_t, \\hat{p}_t) < \\epsilon\\}.$\nBased on the vector-valued mapping notations, we can recast the original game (MDG) as\n$\\max_Q \\min_{P \\in \\mathcal{N}(P)} E \\log P(X_1, ..., X_T|X_0) = \\max_Q \\min_{P \\in \\mathcal{N}(P)} \\sum_{t=1}^T E_Q \\log P(X_t|X_{<t})$\n$= \\max_{Q} \\min_{P \\in \\mathcal{N}(P)} \\sum_{t=1}^T E_{Q} [E_Q[\\log P(X_t|X_{<t})|X_{<t}]]$\n$= \\max_{q_1, \\dots, q_T} \\min_{p_1 \\in \\mathcal{N}(p_1), \\dots, p_T \\in \\mathcal{N}(p_1)} \\sum_{t=1}^T E_{q_1, \\dots, q_{t-1}} \\sum_{x_{<t}} q_t(X_{<t}) \\log p_t(X_{<t})$\n$= \\max_{q_1, \\dots, q_T}  \\sum_{t=1}^T E_{q_1, \\dots, q_{t-1}} \\min_{p_t \\in \\mathcal{N}(p_t)} q_t(X_{<t}) \\log p_t(X_{<t}).$\nHere, (2) is due to the neighborhood definition in (1), and (3) uses the fact that the minimization problem is inherently separable in each pt."}, {"title": "Reduction to one-step Decoding Game", "content": "Directly solving (MDG) or (3) is computationally intractable. Theoretically, one can exploit the recursive structure in (3) and apply dynamic programming, but the scale of the problem grows as (dT), and such an approach would take poly(dT) time. Considering the computational cost of working on a modern LLM, in this paper we turn to local solutions that do not probe into the global structure of the problem.\nTechnically, the hardness of (3) lies in the fact that the problem is non-separable in qt: the expectation intertwines q1,..., qt-1 with qt. To avoid this problem, instead of globally solving it beforehand, we can consider a sequential decision process that builds up qt step by step. Such a decision process (91,\u2026, qr) is inductively given by a locally optimal mechanism\n$\\hat{q}_t = argmax_{q_t} \\sum_{s=1}^{t} \\mathbb{E}_{q_1, ..., q_{s-1}} \\min_{p_s \\in \\mathcal{N}(p_s)} q_s(X_{<s}) \\log p_s(X_{<s}),$ which leads to\n$\\hat{q}_t(x_{<t}) = argmax_{q_t(x_{<t})} \\min_{p_t \\in \\mathcal{N}(p_t)} q_t(x_{<t}) \\log p_t(x_{<t}) \\forall x_{<t} \\in \\mathcal{V}^{t-1},$ as q1,..., qt-1 have been fixed. It turns out that such a mechanism already provides the optimal worst-case performance over all decision processes that do not exploit future information of P."}, {"title": "Theoretical analysis", "content": "In this section, we study the optimal strategies for both p and q in (ODG). We will show that the optimal q imposes an l\u221e-type regularization to likelihood maximization, and the optimal p solving the regularized maximization yields truncation-normalization sampling methods. Finally, we extend our analysis from log-likelihood to a general type of objectives, which recovers other heuristic methods and also highlights an exclusive advantage of log-likelihood. For (ODG), we make the following assumptions.\nAssumption 4.1. The probabilities are strictly positive and, without loss of generality, sorted in decreasing order, i.e., p1 \u2265 p2 \u2265 \u00b7\u00b7\u00b7 \u2265 pd > 0.\nAssumption 4.2. The distance budget e satisfies pd < \u20ac < p1."}, {"title": "p-strategy: implicit regularization", "content": "For a given q, we have the following characterization for the optimal strategy of p.\nTheorem 4.3. Under Assumption 4.1 and 4.2, let \u00ee = max{i : pi > e}. Define w \u2208 R elementwise by W\u1d62 = \\log p\u1d62/(p\u1d62-e)\nThen, for a given q, the optimal p for (ODG) satisfies\n$\\min_{p : d_{TV}(p,\\hat{p})<\\epsilon} q\\log p = \\begin{cases} max_q q\\log p - \\epsilon ||q_{1:\\hat{i}} / \\mathcal{W}||_{\\infty}, & \\text{if } q_i = 0, \\forall i > \\hat{i}; \\\\\\ -\\infty, & \\text{otherwise}. \\end{cases}$\nWe give two remarks on Theorem 4.3. First, due to the fact that limx10 log(x) = \u2212\u221e, any q possessing a non-zero tail in the index set {i : p; < \u20ac} will lead to a -\u221e objective value, because setting the corresponding pi to zero is within the reach of the adversary. This implies a hard truncation constraint on q, namely the optimal q must come without such a tail.\nSecond, with optimal p, the remaining part of the game is a regularized log-likelihood maximization\n$\\max_{q} q \\log p - \\epsilon ||q_{1:\\hat{i}} / \\mathcal{W}||_{\\infty}$ s.t. qi = 0, \u2200i > \u00ee with an lx-type regularization term ||q1:\u00ee/W||. Note that we have w \u2248 P1:\u00ee by applying first-order approximation to the function log(1 + x). If no adversary is present (\u20ac = 0), there is no regularization effect and trivially, greedy sampling solves the one-step log-likelihood maximization. This establishes an equivalence between regularization and robustness against an adversary, which has been observed in the robust optimization literature; see Section 2.3."}, {"title": "q-strategy: heuristic sampling methods", "content": "Now we present the optimal solution to the regularized maximization problem.\nTheorem 4.4. Under Assumption 4.1 and 4.2, let \u00ee = max{i : pi > e}, and w\u1d62 = \\log(pi/(pi-e)) for all i \u2264 \u00ee. Define the threshold\nI* = max \\{I : \\sum_{i=1}^{I-1} \\mathcal{W}_i \\log (p_i/p_I) \\le \\epsilon, p_I > \\epsilon \\\\}.\nThen, the optimal \u011f for (ODG) is given elementwise by\n$\\hat{q}_i \\propto \\mathcal{W}_i \\mathbb{1}_{\\{1 \\le i \\le I^*\\}} .$\nCorollary 4.5. By first-order approximation, w\u2081 \u2248 pi, and hence q; \u221d pil(1\u2264i\u2264I*), where\n$\\begin{aligned}I^* & \\coloneqq max \\{I : \\sum_{i=1}^{I-1} p_i \\log (p_i/p_I) \\le \\epsilon, p_I > \\epsilon \\\\}.\\end{aligned}$ Thus, up to first-order approximation, a truncation-normalization sampling strategy is optimal.\nClearly, Theorem 4.4 describes a sampling strategy that truncates tail probabilities and keeps only the subset {1, . . ., I* } as the support. Note that I* < \u00ee always holds. The new distribution on the support 91:1* is not a simple rescaling of the original weights P1:1* by a normalization constant, which is different from past heuristic designs. However, according to Corollary 4.5, rescaling emerges as a first-order approximation to the optimal strategy.\nUnder first-order approximation, the truncation threshold of q has an information-theoretic interpretation. Note that if I belongs to the support {1, ..., I*}, then \u2211i=1 Pi log(pi/Pr) \u2264 6, which is equivalent to\n$\\log (1/p_I) \\le H(\\mathcal{P}_{1:I-1}) + C_{I-1},$ where H (P1:1-1) is the entropy of P1:1\u22121, and CI\u22121 := log(\u03a3=1P) + \u20ac/(\u03a3=1P) \u2265 \u20ac. Thus, when constructing the support, we grow the existing support {1, . . ., I - 1} by adding I if its self-information (surprisal) log(1/pr) is small compared to the existing entropy. This token selection mechanism modulates the total number of surprisals."}, {"title": "Generalization from log-likelihood", "content": "Beyond the log-likelihood, our analysis can be extended to games with more general objectives, taking the form\n$\\min_{p : d_{TV}(p,\\hat{p})<\\epsilon} q f(p),$\nwhere f: IR \u2192 R is applied elementwise on p. We assume the following for (f-ODG).\nAssumption 4.6. f is non-decreasing and concave. Moreover, (e, f) satisfies either of the following conditions:\n(i) Pa \u2264 \u20ac <P\u2081, and limx10 f(x) = \u2212\u221e;\n(ii) 0 < \u0454 < pa, and \u2211d-1 - > 1.\nClearly, our previous log-likelihood game (ODG) satisfies this assumption. The following result establishes the solu-tion to the general game (f-ODG), and encompasses Theorem 4.4 as a special case.\nTheorem 4.7. Under Assumption 4.1 and 4.6, let\nS\u2081 =\\sum_{i=1}^{I-1} \\frac{f(p_I) - f(p_i)}{f(p_i) - f (p_i - \\epsilon)},\nand define the threshold I* = max {I : S\u2081 < 1, p\u2081 > \u0454}. Then, the optimal q for (f-ODG) is given elementwise by\n$\\hat{q}_i \\propto \\frac{f'(P_i) - f(P_i - \\epsilon)}{f'(x)} 1 (1\\leq i\\leq I^*).$"}, {"title": "Experiments", "content": "Building on the truncation and normalization mechanism given in the general theory, we propose Game sampling as outlined in Algorithm 1, and empirically evaluate its performance in text generation. Regarding the algorithm design, the objectives to our concern are f(x) = -1 and f(x) = log x, as a special case of r = 1. For better practical results, we relax the restrictions on the value of e in Assumption 4.6."}, {"title": "Conclusion", "content": "In this paper, we propose Decoding Game, a two-player zero-sum game where a Strategist aims to maximize the log-likelihood of the generated text under the true probability measure, while an adversarial Nature seeks to distort the true measure within an e-error budget to degrade the text. After discussing the decomposibility of multi-step generation, we study the optimal strategies for both players of the typical one-step Decoding Game. We prove that, as Nature enforces its optimal strategy, it imposes an l\u221e-type regularization on the log-likelihood maximization problem. By solving this regularized maximization in closed form, we identify truncation-normalization sampling as a first-order approximation to the optimal strategy.\nWe also generalize our theoretical studies from log-likelihood to a broader class of objectives. Deriving the general solution, we observe that log-likelihood is the only objective that makes it optimal to rescale the remaining probabilities by a normalizing constant. Selecting other types of objectives leads to different ways of treating the remaining probabilities, including temperature sampling. Moreover, we empirically evaluate the performance of Game sampling, a sampling strategy built upon the general theory, in open-ended text generation with GPT-2 models.\nWe believe that Decoding Game provides a comprehensive theoretical understanding of the heuristic design of popular sampling strategies, by rigorously establishing regularization effect and optimality results. The statistically meaningful motivations and minimal assumptions behind Decoding Game open up its potential for future research, both theoretical and practical, on text generation strategies. For example, it can be interesting to study the game where the probability measures come from a structured space, such as the space expressed by transformer-based models, and understand its implications on the derived strategy. Also, our formulation of multi-step Decoding Game shares similarities with token-level Markov decision processes, and efficiently tackling multi-step strategy via reinforcement learning can be another direction."}, {"title": "Additional experiments", "content": "In Table 2, we present additional experimental results obtained using various choices of e and 7 in Game sampling algorithm. These experiments provide further insights into the performance and sensitivity of the model under different parameter settings. We also explored different values of e \u2208 {0.1, 0.3, 0.5, 0.8, 0.9} alongside different 7 values. However, since the best performance was consistently achieved with e = 0.95 or \u20ac = 0.99, we report only those values here to highlight the effect of changing \u03c4.\nAs part of this evaluation, we also analyzed the point at which probabilities are truncated and renormalized in Game sampling and Nucleus sampling for a randomly selected article from the WebText test set, using the GPT-2 XL model. The GPT-2 model has a total vocabulary size of 50,000 tokens, so truncating the probability distribution can significantly reduce the set of candidate words for the next token. Figures la and 1b illustrate how these sampling strategies truncate the probability distribution. Figure la shows the distribution for the next word when using only 1 token as context, along with the index where probabilities are truncated and set to zero. In contrast, Figure 1b presents the distribution for the next word when using the first 35 tokens as context, providing more information for the model to generate the next word. With more context, the model is expected to be more certain about the next word, and the figure highlights the corresponding truncation points. Notably, Game sampling truncates a substantial portion of the 50,000-token distribution and dynamically adjusts the cutoff point based on the shape of the distribution (see Algorithm 1)."}]}