{"title": "Large Language Model Benchmarks in Medical Tasks", "authors": ["Lawrence K.Q. Yan", "Ming Li", "Yichao Zhang", "Caitlyn Heqi Yin", "Cheng Fei", "Benji Peng", "Ziqian Bi", "Pohsun Feng", "Keyu Chen", "Junyu Liu", "Qian Niu"], "abstract": "With the increasing application of large language models (LLMs) in the medical domain, evaluating these models' performance using benchmark datasets has become crucial. This paper presents a comprehensive survey of various benchmark datasets employed in medical LLM tasks. These datasets span multiple modalities including text, image, and multimodal benchmarks, focusing on different aspects of medical knowledge such as electronic health records (EHRs), doctor-patient dialogues, medical question-answering, and medical image captioning. The survey categorizes the datasets by modality, discussing their significance, data structure, and impact on the development of LLMs for clinical tasks such as diagnosis, report generation, and predictive decision support. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, and CheXpert, which have facilitated advancements in tasks like medical report generation, clinical summarization, and synthetic data generation. The paper summarizes the challenges and opportunities in leveraging these benchmarks for advancing multimodal medical intelligence, emphasizing the need for datasets with a greater degree of language diversity, structured omics data, and innovative approaches to synthesis. This work also provides a foundation for future research in the application of LLMs in medicine, contributing to the evolving field of medical artificial intelligence.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) are advanced machine learning systems built on deep learning algorithms, primarily utilizing transformer architectures. By employing attention mechanisms, these models can process extensive amounts of textual data while simultaneously focusing on relevant segments of the input. The remarkable success of ChatGPT by OpenAI has rapidly drawn attention to LLMs, catalyzing a revolution across various industries, including education, customer service, marketing, and notably, healthcare.\nIn the healthcare sector, the advent of LLMs has led to a surge of innovative applications, ranging from medical education and drug development to clinical trials and disease diagnosis [1], [2], [3]. These models are now capable of generating reports and facilitating patient-doctor conversations. As LLMs continue to evolve, they can manage increasingly complex data types not just text and images, but also audio, video, 3D structures, and other modalities. This versatility empowers them to enhance diagnostic accuracy, streamline patient interactions, and support clinical decision-making by integrating diverse information sources.\nThe growing integration of LLMs in medicine underscores the urgent need for effective evaluation methods to assess their performance across a variety of tasks using structured and diverse datasets. Such datasets are vital for developing LLMS that can tackle complex clinical challenges-like diagnosis and predictive decision support\u2014while simultaneously improving healthcare delivery through enhanced communication between patients and providers.\nThis manuscript offers a comprehensive overview of benchmark datasets specifically tailored for medical applications of LLMs, which are essential for training and validating these models. The paper categorizes various datasets based on their modalities-text, image, and multimodal and underscores their significance in areas such as electronic health records (EHRs), doctor-patient interactions, medical question-answering, and medical image analysis. Furthermore, it discusses their applications in various discriminative and generative tasks.\nAs the healthcare landscape continues to evolve with technological advancements, understanding how to effectively leverage these datasets will be crucial for maximizing the potential benefits of LLMs in enhancing patient care and optimizing clinical workflows."}, {"title": "II. OVERVIEWS OF BENCHMARK DATASETS", "content": "Benchmark datasets are meticulously curated collections utilized to evaluate and compare the performance of large language models (LLMs). These datasets are typically focused on specific tasks, ensuring they accurately represent the relevant feature space. They are designed to be open, discoverable, and accessible to the research community, facilitating objective comparisons of model performance across various algorithms and techniques. In the biomedical field, a variety of datasets with different modalities and formats are available for training models across multiple applications. These include text datasets in various formats as well as image-caption datasets featuring medical images from modalities such as X-rays and MRIs. The following sections will classify benchmark datasets sourced from diverse origins according to their data formats and summarize them in tables, highlighting key datasets in the accompanying text."}, {"title": "A. Text benchmark datasets", "content": "Text is the most prevalent type of dataset across all modalities, particularly due to the extensive availability of textual data in the biomedical domain. This abundance is attributed to the ease of collection and annotation, as well as the direct relevance of text in language modeling. Biomedical textual data encompasses various structured formats, including electronic health records (EHRs), doctor-patient dialogues, open-access literature and abstracts, question-answer pairs, medical guidelines, instructions, and open access literature and abstracts. This wealth of text data serves as a valuable resource for training biomedical large language models (LLMs). Table I provides a summary of several popular textual corpora utilized in the training of medical large language models (LLMs)."}, {"title": "1) Electronic Health Records(EHRs):", "content": "Electronic health records (EHRs) are digital collections of patient information that provide a comprehensive, real-time view of health data. EHRs include a variety of data types, such as medical history, diagnoses, medications, immunization status, allergies, and laboratory test results.\nThese datasets are typically curated by extracting information from local hospitals, de-identifying it in accordance with regulatory standards like HIPAA, and standardizing it into a common schema. The data is then annotated to create labeled datasets for specific tasks.\nAs electronic systems have become widely adopted, EHRs from different countries and recorded in various languages are increasingly available. MIMIC-III[4] and its successor MIMIC-IV[5] are freely accessible English-language critical care databases containing de-identified health data from over 40,000 patients. This initiative is a collaboration between Beth Israel Deaconess Medical Center (BIDMC) and the Massachusetts Institute of Technology (MIT), featuring clinical notes, discharge summaries, and other structured data. AmsterdamUMCdb[6] is a Dutch-medium intensive care database endorsed by the European Society of Intensive Care Medicine (ESICM). It contains deidentified health data from over 23,000 ICU and HDU admissions to an academic medical center in Netherlands from 2003-2016.\nThe Pediatric Intensive Care (PIC) [7] database is is a Chinese-language resource designed to support research in pediatric critical care; it mirrors the structure of the MIMIC-III database but is tailored for local Chinese data. The database is sourced from The Children's Hospital at Zhejiang University School of Medicine with about 13,000 patients and 14,000 ICU stays admitted from 2010\u20132018.\nOther notable datasets include the German-language HiRID (High-Resolution ICU Dataset) [8] that contains data from 34,000 patient admissions to the Department of Intensive Care Medicine at the Bern University Hospital in Switzerland, and the English-language eICU Collaborative Research Database (eICU-CRD)[9] that includes data from over 200,000 ICU admissions across multiple hospitals in United States."}, {"title": "2) Doctor-patient dialogues:", "content": "Doctor-patient dialogues refer to the interactions and exchanges of information between healthcare providers and patients during medical consultations. These dialogues are essential for effective healthcare delivery, as they facilitate understanding, diagnosis, treatment planning, and patient education.\nMany available datasets of doctor-patient dialogues are sourced from real conversations collected on online medical consultation platforms. For instance, he iCliniq dataset[10] consists of 10,000 authentic conversations between patients and doctors from the iCliniq online consultation platform. A similar dataset, HealthCareMagic-100k[10], consists of 100,000 real conversations from the HealthCareMagic platform. Additionally, datasets are available in languages other than English. The Meddialog dataset [11] contains 1.1 million Chinese consultations sourced from haodf.com, covering nearly all medical specialties and including services like medical consultations and appointment scheduling. The IMCS-21[12] dataset is another popular Chinese-medium doctor-patient dialogue dataset contains over 60,000 medical dialogue sessions with data collected from from Muzhi (http://muzhi.baidu.com), a Chinese online health community offering professional medical consulting service for patients. As these datasets sourced data from real online consultation platform, they require deidentification of both patients and doctors to protect privacy and have strict data regulation according to the HIPAA.\nOther datasets are created using large language models (LLMs) like ChatGPT to generate synthetic dialogues based on medical documents such as clinical notes. This approach allows for the creation of realistic dialogues without compromising patient confidentiality and facilitates the generation of multi-turn dialogue datasets crucial for modeling real-world healthcare conversations. The NoteChat dataset [13] isan example of a synthetic dialogue dataset conditioned on 167k case reports created using a cooperative multi-agent framework that leverages LLMs to generate patient-physician dialogues based on 167,000 case reports from the PubMed Central repository. [14]. SynDial[15] is another synthetic patient-physician dialogues dataset generated through iterative refinement of LLM outputs using electronic health records from the MIMIC-IV dataset."}, {"title": "3) Medical question answering:", "content": "Medical question answering datasets consist of collections of questions and answers pertinent to the medical and healthcare fields. These datasets are curated to evaluate the capabilities of language models and question-answering (QA) systems in understanding and responding to a wide range of medical inquiries, thereby facilitating the research and development of advanced medical QA technologies.\nThese datasets vary in format, typically encompassing semantic, yes/no, multiple-choice, and open-ended question answering. Data is often sourced from literature, clinical exam questions, online health forums, or synthesized from existing medical texts or abstracts found in research papers and textbooks using natural language processing techniques."}, {"title": "a) Semantic Question Answering (SQA):", "content": "Semantic question answering (SQA) focuses on understanding the meaning of questions and their context to provide accurate answers, rather than merely matching keywords. SQA datasets usually datasets contain a wider variety of question types and include ground-truth answers that are verified against a knowledge base. This verification allows for precise measurement of SQA system performance by comparing outputs to these established correct answers.\nA popular SQA datasets in biomedical applications is the BioASQ-QA[16] dataset, which is a manually curated question answering corpus derived from the BioASQ challenges that have been focused on biomedical semantic indexing and question answering since 2013. The dataset contains factoid, list, yes/no, and summary questions in English, along with golden standard exact answers and ideal answers in effect summaries which are referenced to documents indexed for MEDLINE."}, {"title": "b) Multi-Choice Question Answering (MCQA):", "content": "MCQA datasets present the questions in a multiple-choice format, often with four or five answer options. This structure not only tests factual recall but also evaluates the ability to apply knowledge in various contexts, assessing critical thinking and clinical reasoning skills.\nDue to the prevalence of MCQA in examination settings, many datasets are derived from established medical exams. The MedQA dataset[37] sources questions from the United States Medical Licensing Examination (USMLE), covering general medical knowledge. It includes 11,450 questions in the development set and 1,273 questions in the test set, each with four or five answer choices. Another example is the MedMCQA[38] which draws from the Indian medical entrance exams (AIIMS/NEET), and covers 2,400 healthcare topics across 21 medical subjects. It has over 187,000 questions in the development set and 6,100 questions in the test set, with 4 answer choices per question.\nA less common variant of MCQA can be found in the PubMedQA dataset, which adopt a yes/no/maybe format for answers. This biomedical QA dataset is derived from PubMed abstracts, where each question in the dataset is paired with an abstract from a PubMed article excluding its conclusion, which serves as the long answer. To answer the questions in the PubMedQA[34] dataset, models must effectively parse and reason through the abstract to arrive at the correct answer, highlighting the importance of understanding context rather than just retrieving facts. The binary classification approach also emphasizes the model's ability to discern conclusions based on abstract information rather than simply selecting from multiple options."}, {"title": "c) Open Question Answering (OpenQA):", "content": "Datasets for Open-domain Question Answering (OpenQA) usually do not include text explicitly labeled as answers; instead, they are associated with a large knowledge base, such as Wikipedia or other unstructured text sources. Models designed for OpenQA generally follow a retrieve-then-read paradigm, where they first retrieve relevant documents or data from a vast corpus and then perform machine comprehension to generate answers based on the retrieved information. These datasets often adopt a Question-Answer-Context triplet format, which aids the model in understanding the relationships between the different components.\nAs a result, some of the aforementioned datasets are also utilized for training models in the OpenQA task. For example, the MedQA[37] dataset has a document collection constructed from 18 english textbooks and 33 simplified Chinese textbook, the BioASQ-QA dataset[16] has materials indexed for MEDLINE; and PubMedQA has relevant paper abstract, associated with the question and answer pairs."}, {"title": "4) Literatures, abstracts and derivatives:", "content": "Biomedical literature provides a vast repository of specialized knowledge, covering diverse topics such as diseases, treatments, drug interactions, and clinical trials. With their key findings and methodologies summarized in abstracts, both the literature and the abstracts are valuable resources for training large language models (LLMs) in the biomedical domain.\nThese datasets enable LLMs to learn the specific language, terminology, and context relevant to the biomedical field, enhancing their ability to generate accurate and contextually appropriate outputs. They also improve the models' capacity to identify entities such as genes, proteins, diseases, and medications within texts and to understand the complex relationships among them. Training LLMs with both the literature and their corresponding abstract also allows LLMs to develop skills in summarization and information extraction and improve the performance of models in tasks such as information retrieval and question-answering.\nPubMed and its subset PubMed Central (PMC) [14] are both popular sources of literature and abstracts. While PubMed is a comprehensive database that includes references and abstracts from various biomedical literature, including more than 37 million articles from journals that may not be freely accessible; PMC is a free digital repository that specifically archives over 5 million full-text articles in the biomedical and life sciences. S2ORC[48] is another popular dataset consisting of open access academic papers from the Semantic Scholar literature corpus with domain not only on the biomedical field. The dataset consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open-access papers."}, {"title": "5) Datasets curated for a specific NLP task:", "content": "Apart from textual datasets curated as a collection of documents in a specific format, datasets curated for specific natural language processing (NLP) tasks are also available. These datasets are curated to enhance the performance of models in specialized areas, such as named entity recognition (NER), relation extraction, and clinical text generation. Tailoring datasets for specific tasks can significantly improve model performance metrics such as F1 scores, precision, and recall.\nDatasets originated from the MEDIQA[69] initiative are great examples. The MEDIQA refers to a series of shared tasks and challenges focused on evaluating the capabilities of large language models (LLMs) in the biomedical domain, particularly in clinical contexts.The MEDIQA NLI, MEDIQA QA and MEDIQA RQE are datasets sourced from the initiative that focuses on Natural Language Inference (NLI), Question Answering (QA) and Recognizing Question Entailment (RQE), repectively."}, {"title": "6) Large composite corpus:", "content": "Instead of training LLMs on a single dataset for a specific task, researchers usually combine multiple datasets, with data from different sources, modalities and formats, into a larger corpus for the training of the models. By integrating multiple datasets from diverse sources, researchers can enhance model performance through Multi-Task Learning (MTL), which allows for simultaneous training on different related tasks. Notable examples of large composite biomedical benchmark datasets include the Biomedical Language Understanding Evaluation (BLUE)[84], BLURB (Biomedical Language Understanding and Reasoning Benchmark)[85], BoX[86], The pile[92] and BigBIO[83] datasets, which comprise increasing numbers of datasets, task categories, and languages."}, {"title": "7) The evolution of textual benchmarks:", "content": "Early advancements in large language models (LLMs), particularly those based on the GPT and BERT architectures, primarily utilized general-purpose datasets. While these models demonstrated impressive capabilities in generating and understanding human language, they lacked the specificity required for medical applications. As researchers began to explore the potential of LLMs in healthcare, it became clear that general datasets could not adequately address the complexities inherent in medical language and context.\nThe need arose for models capable of understanding clinical terminology, interpreting unstructured clinical notes, and assisting in clinical decision-making. This realization led to the development of medical-specific datasets. Electronic health records (EHRs) emerged as a critical resource, offering vast amounts of unstructured clinical data that could be leveraged to train LLMs tailored for healthcare applications. For instance, datasets like NYU Notes, which encompass millions of clinical notes, exemplify this shift by providing rich, domain-specific content for model training.\nThe introduction of clinician-generated datasets, such as MedAlign, marked a significant advancement in this area. These datasets comprise natural language instructions curated by healthcare professionals to enhance the relevance and applicability of LLMs in real-world clinical settings. As LLM capabilities evolved to handle modalities beyond text and embrace multimodalities, the integration of multi-modal datasets became increasingly important. This trend is illustrated by datasets like MedPix 2.0, which links clinical reports with corresponding imaging data, thereby enhancing the ability of LLMs to perform complex analyses pertinent to patient care. Additionally, Quilt combines image-text pairs to improve LLM training by utilizing diverse data sources.\nIn summary, the shift from general NLP datasets to medical-specific datasets signifies an increasing recognition of the unique demands of healthcare applications, paving the way for more effective and relevant large language models (LLMs) in clinical settings. The following section will discuss benchmark datasets that consist of both images and accompanying caption text."}, {"title": "B. Captioned imagery benchmarks", "content": "The development of large language models (LLMs) has significantly advanced traditional methods for processing text benchmark datasets. Recent improvements in multimodal LLMs have expanded their capabilities, allowing them to effectively manage complex datasets, especially in the medical field. These advancements are crucial for integrating various data types such as clinical text and medical images which are essential for thorough medical analysis and decision-making. Imagery datasets are particularly important, as medical diagnostics often rely on both visual and textual information. By using captioned imagery datasets, LLMs can combine visual data with textual descriptions, enhancing their understanding of complex clinical scenarios and improving diagnostic accuracy.\nIn recent years, captioned imagery benchmarks have emerged as critical tools for evaluating the performance of LLMs in multimodal tasks, especially in vision-language understanding. These benchmarks assess models' abilities to generate coherent and contextually appropriate descriptions of images, which is foundational for various downstream applications such as automated image annotation, accessibility tools for visually impaired users, and improved human-computer interaction.\nIn medical applications, Medical Image Captioning (MIC) combines the power of multimodal LLMs with specialized medical knowledge to produce accurate descriptions of medical images. This task is challenging due to the complexity of medical imagery; models must understand intricate anatomical structures, identify pathological conditions, and communicate findings using professional terminology. Table II summarizes several popular imagery corpora used in training medical LLMs."}, {"title": "1) Examples of Medical Imagery Benchmarks:", "content": "Several notable benchmarks and datasets have been developed to advance research in medical image captioning:\n1. IU X-Ray Dataset: This dataset, derived from the Indiana University Chest X-Ray Collection, contains over 7,000 chest X-ray images paired with radiological reports. It serves as a primary benchmark for developing and evaluating models capable of generating textual descriptions for chest radiographs.\n2. MIMIC-CXR Dataset: Part of the larger MIMIC (Medical Information Mart for Intensive Care) database, MIMIC-CXR provides over 377,000 chest X-rays associated with 227,835 imaging studies. This large-scale dataset is crucial for training robust models that can handle diverse pathological conditions and varying image qualities.\n3. Radiology Objects in Context (ROCO) Dataset: This dataset consists of over 81,000 radiology images from various modalities (e.g., CT, MRI, X-ray) paired with captions extracted from scientific papers. ROCO is particularly useful for training models to generate captions that align with scientific literature standards.\n4. VQA-RAD Dataset: While primarily designed for visual question answering, this dataset of 315 medical images and 3,515 question-answer pairs can also be adapted for image captioning tasks, especially for generating focused descriptions based on specific aspects of medical images.\n5. CheXpert Dataset: Containing 224,316 chest radiographs of 65,240 patients, CheXpert is a large public dataset for chest radiograph interpretation. While not specifically designed for captioning, it can be leveraged to enhance the performance of medical image captioning models, especially in identifying and describing multiple coexisting conditions."}, {"title": "2) Domain-Specific Image-Caption Benchmarks:", "content": "As image-captioning expands, the need for domain-specific benchmarks is growing. In medical applications, large multimodal datasets are frequently used to train models that generate captions from complex medical images like X-rays, CT scans, and MRIs. These captions play a vital role in diagnostics, providing detailed and accurate descriptions that help healthcare professionals make informed decisions. One notable benchmark is the MedICaT dataset, which evaluates models' abilities to describe medical images by combining textual reports with visual data [161]."}, {"title": "3) Image-Caption Benchmarks in Multimodal Tasks:", "content": "With the emergence of multimodal models like GPT-4V, BLIP, and CLIP, there is an increasing demand for benchmark datasets to evaluate how effectively these models can process complex multimodal data. This includes context-aware captions and reasoning across various domains.\nFor example, the Mementos benchmark [162] focuses on image sequences instead of single static images, enabling the assessment of models' abilities in temporal reasoning-tracking changes and interpreting behaviors throughout a series of images. Similarly, the CODIS benchmark evaluates how well models can maintain context across multiple images, which is essential for tasks that require a coherent understanding of visual scenes over time.[161], [162].\nThis shift towards dynamic image-captioning is particularly relevant in real-world applications, such as medical imaging. Techniques like Positron Emission Tomography (PET) and Single Photon Emission Computed Tomography (SPECT) capture sequential images over time, highlighting the need for models to understand and describe non-static visual inputs.\nDynamic understanding and captioning are crucial in fields like medical imaging and robotics. In these domains, models must account for changing visual contexts and generate descriptions that accurately reflect ongoing events, interactions, and outcomes. For example, in robotics, a model may need to detail not only the objects present but also how a robotic arm interacts with them over time, making temporal comprehension vital for effective task execution.[162]."}, {"title": "4) The Evolution of Image-Caption Benchmarks:", "content": "Traditionally, image-caption benchmarks have focused on static datasets consisting of single images paired with human-annotated captions. Early benchmarks like MSCOCO and Pascal VOC aimed to generate descriptive captions that captured key objects, actions, and relationships within a single image. The success of these benchmarks spurred the development of more sophisticated models capable of understanding both visual and textual inputs.\nWith advancements in large language models (LLMs) and multimodal LLMs (MLLMs), the ability to process and generate language from visual inputs has expanded beyond isolated tasks. These models are now evaluated not only on their grammatical accuracy but also on their capability to understand complex scenes, identify relationships between objects, and perform high-level reasoning about images. This evolution necessitates new benchmarks that address a broader range of visual reasoning challenges, including temporal dynamics, object behaviors, and domain-specific imagery."}, {"title": "C. Medical benchmarks in other modalities", "content": "Multimodal large language models (MLLMs) can simultaneously process various data types, enabling a more comprehensive understanding of complex medical scenarios. For example, they can analyze clinical notes, diagnostic images, and patient audio, leading to more accurate diagnoses compared to unimodal systems that rely solely on text or images.\nTo effectively train an MLLM, diverse datasets beyond just text and images are essential. These include audio, video, ECG data, and various omics datasets[163]. Such multi-modal datasets enhance the model's ability to understand intricate medical scenarios and improve predictive accuracy. The integration of these different modalities fosters richer insights and better contextual understanding in healthcare applications. Table III summarizes several other-modality benchmark datasets used in training medical MLLMs."}, {"title": "1) Video:", "content": "Videos are sequences of images (frames) that show motion and changes over time. These temporal information can be challenging to work with because it is hard to caption and align, and models must effectively understand the dynamics of movement. Additionally, videos may have redundant visual information, making it difficult to extract important features. In medical applications, video datasets can be generally classified into 4 categories, including surgical,patient interaction, video-based medical imaging and Human Posture datasets."}, {"title": "a) Surgical videos:", "content": "These datasets capture various surgical procedures, annotated with key steps and outcomes. These can be used to train models for surgical training simulations or to analyze techniques[164]. For example, OphNet[165] is a large-scale video benchmark specifically designed for ophthalmic surgical workflow understanding. It contains more than 2000 videos across 66 types of surgeries, with detailed annotations covering 102 surgical phases and 150 operations. Similarly, Cholec80[166] contains 80 laparoscopic cholecystectomy surgery videos performed by 13 surgeons. These datasets are valuable resource for studying surgical workflows and techniques and improving action understanding and model validation."}, {"title": "b) Patient interaction videos:", "content": "These datasets contain videos documenting patient consultations or examinations can help train MLLMs to interpret both verbal and non-verbal cues in a clinical setting. A notable example is the Bristol Archive Project dataset[167]. This dataset features 327 video-recorded primary care consultations collected in and around the Bristol between 2014 - 2015, along with coded transcripts."}, {"title": "c) Video-based medical imaging:", "content": "These datasets include video sequences of imaging techniques, such as ultrasound or endoscopy, annotated with diagnostic information. Notable examples include the C3VD and the S.U.N. Colonoscopy datasets. The C3VD dataset[168] comprises 22 registered videos with paired ground truth depth, surface normals, optical flow, occlusion, six degree-of-freedom pose, coverage maps, and 3D models. This allows comprehensive analysis and training of algorithms in 3D reconstruction and depth estimation tasks. The S.U.N. Colonoscopy Datasets [169] includes 49,136 annotated frames from 100 different polyps, providing detailed visual data critical for research in automated polyp detection and classification.The S.U.N. datasets serve as a benchmark for developing and evaluating machine learning models aimed at improving polyp detection rates during colonoscopies. By providing high-quality annotated data, the dataset supports the training of deep learning models to enhance diagnostic accuracy and reduce missed detections in clinical settings."}, {"title": "d) Human Posture:", "content": "Human pose estimation is essential for intelligent systems that require an understanding of human activities[170], [171]. Several benchmarks have been developed to advance this field, each addressing unique challenges. The MVOR dataset[172] is the first to offer multi-view RGB-D data from real clinical environments, captured during surgical procedures. It includes synchronized frames from multiple cameras, with annotations for 2D/3D poses and human bounding boxes. Challenges such as occlusions and privacy blurring are present, impacting baseline performance, thus encouraging the development of robust models suited for complex, real-world scenarios. The MPII Human Pose dataset[173] is known for its diversity, covering over 800 human activities. It provides extensive labels, including joint positions, 3D orientations, and occlusion information. The inclusion of adjacent video frames allows for the use of temporal information, enhancing pose estimation. This benchmark sets a standard for evaluating models across a wide variety of real-world activities. The PoseTrack benchmark[174] focuses on video-based multi-person pose estimation and tracking. It introduces tasks for single-frame and video-based pose estimation, as well as articulated tracking, offering a large dataset with labeled person tracks. A centralized evaluation platform enables objective comparison of methods, driving progress in consistent tracking across video sequences. These benchmarks collectively address the complexities of human posture/pose estimation, from handling occlusions to maintaining temporal consistency, fostering more robust and context-aware solutions."}, {"title": "2) Audio:", "content": "There are primarily two types of audio datasets used in medical large language models (MLLMs). The first type consists of recordings of medical conversations between doctors and patients, while the second type includes audio recordings for medical classifications, such as breath sounds[175] and heart sounds[176]. The former provides rich contextual data that enhances the model's understanding of medical terminology and patient interactions, improving the adaptability of speech recognition systems across various accents and dialects, which is crucial in diverse healthcare settings. In contrast, the latter offers alternative inputs for disease classification, thereby enhancing diagnostic accuracy."}, {"title": "a) Audio medical conversation:", "content": "While there are numerous commercially available audio datasets for medical conversations, only a few open-access benchmark datasets exist. One notable example is the synthetic patient-physician interviews created by Fareez et al. (2022), structured in the format of Objective Structured Clinical Examinations (OSCE) and specifically targeting respiratory cases. [177] This dataset contains 272 audio files in MP3 format, along with their transcriptions categorized by disease type. Another example is the Kaggle dataset on medical speech[178], which includes 8.5 hours of audio utterances paired with text for common medical symptoms."}, {"title": "b) Medical classification:", "content": "Audio recordings can also be instrumental in symptom recognition, particularly for breath and heart sounds. For instance, the HF_Lung_V1 database [175] is curated for detecting breath phases and adventitious sounds, which consists of 9,765 audio files of lung sounds, each lasting 15 seconds. It features detailed labeling for breath phases (inhalation and exhalation) and various adventitious sounds, such as wheezes, stridor, rhonchi, and crackles. Another relevant dataset comes from the PASCAL Heart Sounds Challenge[176], which provides several hundred real heart sounds collected from an iPhone app and a digital stethoscope in noisy environments. These sounds are classified into specific categories, enhancing models' ability to detect cardiac pathologies."}, {"title": "3) Electrocardiogram(ECG):", "content": "An electrocardiogram (ECG or EKG) is a diagnostic test that measures the electrical activity of the heart over time, capturing voltage changes associated with the heart's rhythmic contractions. This quick, painless, and noninvasive procedure involves placing electrodes on the chest, arms, and legs to record the heart's electrical signals. ECGs are essential for diagnosing various cardiac conditions, including myocardial infarction, arrhythmias, and unstable angina pectoris. Standard ECG leads are labeled as I, II, III, aVF, aVR, aVL, V1, V2, V3, V4, V5, and V6 and are routinely obtained upon patient admission to emergency departments or hospital floors. They may be repeated for patients exhibiting cardiac symptoms such as chest pain or abnormal rhythms and can be performed daily following acute cardiovascular events like myocardial infarction to monitor the heart's condition.\nIntegrating ECG signals into the training of large language models (LLMs) can lead to the development of models that enhance diagnostic accuracy, improve workflow efficiency, support personalized patient care, and foster better communication between patients and healthcare providers. This integration allows for more nuanced interpretations of ECG data and facilitates the identification of subtle patterns that may indicate underlying health issues. [179]\nThe PTB-XL and MIMIC-IV-ECG datasets are key resources for ECG analysis in machine learning. The PTB-XL dataset[180] consists of 21,837 12-lead ECG recordings from 18,869 patients, each lasting 10 seconds. It includes annotations from up to two cardiologists, resulting in a multi-label dataset with 71 ECG statements that cover various diagnostic categories. This dataset is publicly available and designed to facilitate the training and evaluation of automatic ECG interpretation algorithms. The MIMIC-IV-ECG dataset[181] is part of the broader MIMIC-IV critical care database, containing over 1 million ECG recordings from more than 60,000 patients. It features a wide range of clinical conditions and includes both waveform data and clinical notes, making it valuable for research in ECG signal processing and machine learning applications. Both datasets are instrumental for developing robust machine learning models in the field of cardiology."}, {"title": "4) Omics:", "content": "Omics refers to a comprehensive approach in biological research that examines the complete set of biological molecules within specific categories, such as genomes, transcriptomes, proteomes, and metabolomes. This approach leverages high-throughput technologies to measure these molecules simultaneously, enhancing our understanding of biological functions and interactions. In bioinformatics, various machine learning models (MLLMs) utilizing omics datasets have been proposed[182] for applications like predicting genome-wide variant effects, identifying DNA-protein interactions, analyzing RNA sequences for splicing predictions, and assessing protein sequences for structural properties and functionalities.\nIntegrating omics data into medical MLLMs enables the analysis of complex datasets to uncover novel biomarkers, predict patient responses to treatments, identify genetic markers linked to specific diseases, and facilitate personalized medicine. For example, in oncology, MLLMs can analyze genetic mutations to inform treatment strategies, while in cardiology, they help identify genetic risk factors for heart disease. The Cancer Genome Atlas (TCGA) dataset exemplifies this approach. Launched in 2006 as a collaboration between the National Cancer Institute (NCI) and the National Human Genome Research Institute (NHGRI), TCGA includes over 20,000 tumor and normal samples from more than 11,000 patients across 33 cancer types. This dataset provides valuable insights into cancer progression, treatment responses, and genomic alterations while supporting advanced bioinformatics workflows through standardized clinical data. Another notable resource is the Human Protein Atlas (HPA) dataset, which catalogs protein and RNA expression profiles across various human tissues and conditions using high-throughput techniques like antibody-based assays and transcriptomics. The HPA encompasses data on over 17,000 unique proteins, detailing their spatial distributions, interactions, and profiles in diseases and immune cells.\nUntil now, we have provided a comprehensive overview of benchmark datasets across various data types, formats, and medical modalities. In the upcoming sections, we will explore how some of these datasets can be utilized for training models in specific tasks related to large language models (LLMs), categorized into discriminative and generative tasks."}, {"title": "III. DISCRIMINATIVE BIOMEDICAL TASKS & BENCHMARK", "content": "Discriminative biomedical tasks and benchmarks are integral components of biomedical natural language processing (NLP), focusing on developing methods to identify and classify specific biomedical entities and their relationships. These tasks encompass various domains, such as Named Entity Recognition (NER), Relation Extraction (RE), Text Classification, Natural Language Inference, Semantic Textual Similarity, Information Retrieval, and Entity Linking. Benchmarks in this field are carefully curated datasets used to evaluate and advance the performance of machine learning models on these discriminative tasks. Each benchmark offers unique challenges that reflect real-world complexities, including identifying diseases, chemicals, proteins, adverse drug events, and gene-"}]}