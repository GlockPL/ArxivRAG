{"title": "TemporalStory: Enhancing Consistency in Story Visualization using Spatial-Temporal Attention", "authors": ["Sixiao Zheng", "Yanwei Fu"], "abstract": "Story visualization presents a challenging task in text-to-image generation, requiring not only the rendering of visual details from text prompt but also ensuring consistency across images. Recently, most approaches address inconsistency problem using an auto-regressive manner conditioned on previous image-sentence pairs. However, they overlook the fact that story context is dispersed across all sentences. The auto-regressive approach fails to encode information from susequent image-sentence pairs, thus unable to capture the entirety of the story context. To address this, we introduce TemporalStory, leveraging Spatial-Temporal attention to model complex spatial and temporal dependencies in images, enabling the generation of coherent images based on a given storyline. In order to better understand the storyline context, we introduce a text adapter capable of integrating information from other sentences into the embedding of the current sentence. Additionally, to utilize scene changes between story images as guidance for the model, we propose the StoryFlow Adapter to measure the degree of change between images. Through extensive experiments on two popular benchmarks, PororoSV and FlintstonesSV, our TemporalStory outperforms the previous state-of-the-art in both story visualization and story continuation tasks.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in diffusion model (DM) like Stable Diffusion [13], DALL-E 3 [5], and PixArt-a [11] have demonstrated unprecedented text-to-image generation capabilities, producing high-quality images that semantically align with text prompt. These achievements have significantly advanced text-to-video generation, with models such as SVD [6] and Sora [9] focusing on creating temporally coherent videos based on text prompt. Current text-to-video methods typically generate videos depicting scenes within a limited range or simple motion changes. However, real-world applications often require generating a sequence of coherent images from a given storyline, such as creating comics for storybooks. This task is formalized as story visualization, aims to generate a visually coherent sequence of images from a storyline comprising multiple sentences. Similarly, story continuation seeks to generate a coherent sequence of images given a storyline and the first image, as illustrated in Figure 1.\nStory visualization and story continuation are challenging tasks that require generated images to align with sentences and maintain consistency in characters and scenes. Due to the limited information content of individual sentences, the key to generating a coherent sequence of images lies in providing the model with as much context information as possible. Recent methods [35, 46, 50] have captured temporal dependencies auto-regressively based on previous image-sentence pairs. Among them, AR-LDM [46] introduces a latent diffusion model (LDM) [54] conditioned auto-regressively on historical image-caption pairs. Story-LDM [50] incorporates a visual memory module to capture context across generated images, while StoryGen"}, {"title": "2. Related Works", "content": "Story visualization and continuation. Early methods [33, 34, 40, 41, 62] for story visualization primarily relied on Generative Adversarial Networks (GANs) [18]. Story-GAN utilized full stories and individual sentences as input, employing image and story discriminators to generate contextual images. Subsequent studies improve this model: CP-CSV [62] maintained character and background consistency with a figure-ground discriminator, DuCo-StoryGAN [41] introduced a dual learning framework, VLC-StoryGAN [40] incorporates commonsense knowledge graphs, and Word-Level SV [33] proposed a new sentence representation. Later research adopted Transformer models. VP-CSV [10] used Transformer and VQ-VAE [65] to preserve characters. StoryDALL-E [42] extends the story visualization task to story continuation with a given initial image and uses the pre-trained DALL-E [51] to generate coherent images. CMOTA [1] utilizes a memory architecture in Transformer with online text augmentation. Recently, DMs [26, 60] have shown success in image generation. Some works [14, 35, 46, 50, 58, 61, 64] introduce diffusion models into story visualization. AR-LDM [46], Story-LDM [50], and StoryGen [35] all leverage an autoregressive manner to achieve better consistency, but they are both memory-intensive due to the need to save all previous context. StoryGPT-V [58] combines the image generation capability of LDM with the reasoning ability of LLM to ensure semantic consistency. TaleCrafter [17], Animate-A-Story [23], and AutoStory [68] focus on designing system pipelines for story visualization, all employing LLM to generate storylines. In contrast, our TemporalStory addresses consistency by leveraging spatial-temporal attention to learn complex dependencies in images, departing from auto-regressive methods.\nText-to-image generation. Recently, significant progress [48, 52, 54, 56] has been achieved in text-to-image generation, primarily due to advancements in diffusion models [26]. Methods like LDM apply DM in latent space with a powerful pre-trained autoencoder, allowing efficient training with limited resources while maintaining image quality and diversity. Large-scale models such as DALL-E 2 [52], Imagen [56], and GLIDE [45] produce photorealistic images. Another line of work [12, 15, 25, 29, 31, 36, 44, 55, 72, 77] focuses on flexible and controllable image generation"}, {"title": "3. Method", "content": "3.1. Overview\nPreliminaries. Diffusion models [26, 60] are generative models that approximate the underlying data distribution by iteratively denoising a Gaussian distribution using a reverse process of a fixed Markov Chain of length T. Given a training sample xo ~ q(x0), diffusion models first obtain the noised input through a forward diffusion process q(X1:T|X0) and gradually add Gaussian noise \u0454 ~ \u039d(0,1) to the input\n\nq(xt|xo) = N(xt; \u221a\u0101txo, (1 \u2013 \u0101t)I),\n(1)\n\nwhere \u0101t := \u03a0s=1(1\u2212 \u03b2s). \u03b21,..., \u03b2r is a variance schedule that controls the amount of noise added at each time step t. The diffusion model is then trained to approximate the backward process po(xt-1|xt) using a denoising network \u20ac0(xt, t), whose parameters @ can be learned by minimizing the the mean squared error (MSE) between the predicted and target noise\n\nLDM := Ex,e~N(0,1),t [||\u20ac \u2013 60(x, t)||2] . (2)\n\nIn order to extend the utility of diffusion models to extremely high-dimensional data, such as high-resolution images, LDM [54] first compress the original image into a lower-dimensional space, called latent space, using perceptual image compression. Specifically, LDM first utilize a pre-trained encoder \u025b to map the input image x \u2208 RH\u00d7W\u00d73 from the pixel space into a latent representation z = E(x), where z \u2208 Rh\u00d7w\u00d7c. Then, LDM executes the forward processes q(Z1:T|Zo) and backward processes po(Zt-1|Zt) on the latent representation z. The parameters of the denoising network \u20ac (zt, t, c) can be learned by minimizing the following objective function\n\nLLDM := E\u025b(x),c,e~N(0,1),t [||\u20ac - Eo(Zt, t, c) ||2], (3)\n\nwhere c denotes the conditional signal that diffusion models can be conditioned on. For story visualization task, c represents storyline S. The generated image can be obtained from the denoised latent zo using a pre-trained decoder x = D(z).\nTask overview. The goal of story visualization is to generate a sequence of images that align with a multi-sentence storyline, ensuring semantic consistency with the text. Characters and scenes should remain consistent to cohesively visualize the storyline. In story continuation, the first image is provided as additional input. The first image offers initial storyline information, guiding the model in generating subsequent images by extracting character and scenes, eliminating the need to generate them from scratch.\nTask formulation. Given a storyline S = {S1,...SN} composed of N sentences, the objective of story visualization is to generate a sequence of images \u0128 = {\u00ce\u00b9, \u2026\u2026\u2026\u00ceN} consisting of corresponding N images. During training, ground truth images are denoted as I = {I\u00b9, . . . IN}. For the story continuation task, the goal is to generate subsequent images \u0128 = {\u00ce\u00b2,\u2026\u2026\u2026\u00ceN } given storyline S and the first image I\u00b9.\n3.2. Model Architecture\nSpatial-Temporal attention. UNet consists of a series of spatial downsampling and upsampling blocks with skip connections, each containing spatial convolution and spatial attention layers to capture the spatial dependencies. However, temporal dependencies is also crucial for enhancing the consistency of generated images in the task of story visualization. Inspired by [7, 53, 66, 67, 76], we introduce temporal convolution and temporal attention layers"}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nDatasets. We employ two popular benchmark datasets, PororoSV [34] (Li et al., 2019) and the FlintstonesSV [21], to evaluate the performance of our model in both story visualization and story continuation tasks. PororoSV contains 10,191, 2,334, and 2,208 stories within the train, validation, and test splits, respectively, including 9 main characters. On the other hand, FlintstonesSV contains 20,132, 2,071, and 2,309 stories within the train, validation, and test splits, respectively, featuring 7 main characters and 323 backgrounds. Each story within these datasets comprises 5 consecutive story images.\nAutomatic metrics. To evaluate the quality of generated images, we employ the following three evaluation metrics the same as previous works [14, 40, 42, 46] in story visualization: (1) Frechet Inception Distance (FID) [24]: Measures the distance between feature vectors of ground truth and generated images. (2) Frame accuracy (Frm. Acc.): Utilizes a fine-tuned Inception-v3 model to evaluate the character match to the ground-truth. (3) Character F1-score (Char. F1): Measures the quality of generated characters using the same model as Frame accuracy."}, {"title": "4.2. Quantitative Results", "content": "Story Visualization. Table 2 shows quantitative results for story visualization on the PororoSV dataset. We compare the performance of our TemporalStory to several state-of-the-art methods, including StoryGAN, CP-CSV, DUCO-STORYGAN, VLC-STORYGAN, VP-CSV, Word-Level SV, CMOTA-HR, CMOTA, AR-LDM, Causal-Story and StoryImager. Table 3 shows quantitative results for story visualization on the FlintStonesSV dataset. We compare the performance of our TemporalStory to several state-of-the-art methods, including StoryGAN, DUCO-STORYGAN, VLC-STORYGAN, CMOTA-HR, CMOTA and StoryImager. The quantitative results in Table 2 and Table 3 clearly demonstrate that our TemporalStory significantly outperforms existing methods across all metrics on both datasets, particularly in Char. F1 and Frm. Acc. This superior performance is primarily due to Spatial-Temporal attention, Text adapter, and StoryFlow adapter, which effectively utilize contextual information to generate coherent visual stories aligned with a given storyline.\nStory Continuation. Table 1 presents the quantitative results for story continuation on both the PororoSV and FlintstonesSV datasets. We evaluate the effectiveness of our TemporalStory model against several state-of-the-art methods, such as StoryGANC, StoryDALL-E, MEGA-StoryDALL-E, AR-LDM, CMOTA, Causal-Story and StoryImager. The quantitative results in Table 1 demonstrate that our TemporalStory outperforms existing methods by a large margin in all metrics for the story continuation task on both datasets. This indicates that our model can better utilize contextual information to generate a coherent visual story based on the given storyline and the first image. More results are provided in the supplementary material.\n4.3. Qualitative Results\nStory Visualization. Figure 4 demonstrates qualitative comparison of story visualization on PororoSV and Flint-StonesSV datasets. Stable Diffusion generates images independently based on the given sentences. Although Stable Diffusion can generate high-quality images, the inability to obtain contextual information results in inconsistent appearance of characters and occurrences of character duplication."}, {"title": "5. Human Evaluation", "content": "Due to the limitations of metrics such as FID, Char. F1, and Frm. Acc. in accurately reflecting the quality of generated story images, we conducted human evaluations for the story visualization task on the PororoSV dataset, focusing on Visual Quality, Semantic Relevance, and Temporal Consistency. We randomly selected 100 pairs of story image sequences generated by ARLDM [46] and our TemporalStory on the PororoSV dataset. Annotators were tasked to select better sequence for three attributes: Visual Quality, Semantic Relevance, and Temporal Consistency. Each pair of generated story image sequences was evaluated by three annotators. As shown in Table 4, the evaluation results indicate that our TemporalStory outperforms ARLDM significantly across all three attributes.\n5.1. Ablation Studies\nAblation study of the proposed components. In order to investigate the benefit of each proposed component, we conduct an ablation study on the proposed StoryFlow"}, {"title": "6. Conclusion", "content": "In this paper, we propose TemporalStory, a novel approach for story visualization and story continuation by incorporating Spatial-Temporal Attention to capture complex spatial and temporal dependencies in story images. This method generates coherent images from a given storyline, significantly enhancing consistency. The Text Adapter improves storyline context understanding, while the StoryFlow Adapter uses scene changes between images to guide the model. Extensive experiments on PororoSV and FlintstonesSV show that TemporalStory outperforms previous state-of-the-art methods in both tasks. These results highlight the effectiveness of TemporalStory in generating visually consistent and contextually coherent story images, suggesting promising directions for future research in story visualization and story continuation."}]}