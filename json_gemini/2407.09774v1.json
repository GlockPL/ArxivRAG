{"title": "TemporalStory: Enhancing Consistency in Story Visualization using Spatial-Temporal Attention", "authors": ["Sixiao Zheng", "Yanwei Fu"], "abstract": "Story visualization presents a challenging task in text-\nto-image generation, requiring not only the rendering of vi-\nsual details from text prompt but also ensuring consistency\nacross images. Recently, most approaches address inconsis-\ntency problem using an auto-regressive manner conditioned\non previous image-sentence pairs. However, they overlook\nthe fact that story context is dispersed across all sentences.\nThe auto-regressive approach fails to encode information\nfrom susequent image-sentence pairs, thus unable to cap-\nture the entirety of the story context. To address this, we\nintroduce TemporalStory, leveraging Spatial-Temporal at-\ntention to model complex spatial and temporal dependen-\ncies in images, enabling the generation of coherent images\nbased on a given storyline. In order to better understand the\nstoryline context, we introduce a text adapter capable of in-\ntegrating information from other sentences into the embed-\nding of the current sentence. Additionally, to utilize scene\nchanges between story images as guidance for the model,\nwe propose the StoryFlow Adapter to measure the degree of\nchange between images. Through extensive experiments on\ntwo popular benchmarks, PororoSV and FlintstonesSV, our\nTemporalStory outperforms the previous state-of-the-art in\nboth story visualization and story continuation tasks.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in diffusion model (DM) like Stable\nDiffusion [13], DALL-E 3 [5], and PixArt-a [11] have\ndemonstrated unprecedented text-to-image generation ca-\npabilities, producing high-quality images that semantically\nalign with text prompt. These achievements have signifi-\ncantly advanced text-to-video generation, with models such\nas SVD [6] and Sora [9] focusing on creating temporally\ncoherent videos based on text prompt. Current text-to-video\nmethods typically generate videos depicting scenes within\na limited range or simple motion changes. However, real-\nworld applications often require generating a sequence of\ncoherent images from a given storyline, such as creating\ncomics for storybooks. This task is formalized as story vi-"}, {"title": "2. Related Works", "content": "Story visualization and continuation. Early methods\n[33, 34, 40, 41, 62] for story visualization primarily relied\non Generative Adversarial Networks (GANs) [18]. Story-\nGAN utilized full stories and individual sentences as in-\nput, employing image and story discriminators to gener-\nate contextual images. Subsequent studies improve this\nmodel: CP-CSV [62] maintained character and background\nconsistency with a figure-ground discriminator, DuCo-\nStoryGAN [41] introduced a dual learning framework,\nVLC-StoryGAN [40] incorporates commonsense knowl-\nedge graphs, and Word-Level SV [33] proposed a new sen-\ntence representation. Later research adopted Transformer\nmodels. VP-CSV [10] used Transformer and VQ-VAE [65]\nto preserve characters. StoryDALL-E [42] extends the story\nvisualization task to story continuation with a given initial\nimage and uses the pre-trained DALL-E [51] to generate\ncoherent images. CMOTA [1] utilizes a memory architec-\nture in Transformer with online text augmentation. Re-\ncently, DMs [26, 60] have shown success in image gener-\nation. Some works [14, 35, 46, 50, 58, 61, 64] introduce\ndiffusion models into story visualization. AR-LDM [46],\nStory-LDM [50], and StoryGen [35] all leverage an auto-\nregressive manner to achieve better consistency, but they\nare both memory-intensive due to the need to save all pre-\nvious context. StoryGPT-V [58] combines the image gener-\nation capability of LDM with the reasoning ability of LLM\nto ensure semantic consistency. TaleCrafter [17], Animate-\nA-Story [23], and AutoStory [68] focus on designing sys-\ntem pipelines for story visualization, all employing LLM\nto generate storylines. In contrast, our TemporalStory ad-\ndresses consistency by leveraging spatial-temporal attention\nto learn complex dependencies in images, departing from\nauto-regressive methods.\nText-to-image generation. Recently, significant progress\n[48, 52, 54, 56] has been achieved in text-to-image gener-\nation, primarily due to advancements in diffusion models\n[26]. Methods like LDM apply DM in latent space with a\npowerful pre-trained autoencoder, allowing efficient train-\ning with limited resources while maintaining image quality\nand diversity. Large-scale models such as DALL-E 2 [52],\nImagen [56], and GLIDE [45] produce photorealistic im-\nages. Another line of work [12, 15, 25, 29, 31, 36, 44, 55,\n72, 77] focuses on flexible and controllable image genera-"}, {"title": "3. Method", "content": "3.1. Overview\nPreliminaries. Diffusion models [26, 60] are generative\nmodels that approximate the underlying data distribution\nby iteratively denoising a Gaussian distribution using a re-\nverse process of a fixed Markov Chain of length T. Given\na training sample xo ~ q(x0), diffusion models first ob-\ntain the noised input through a forward diffusion process\nq(X1:T X0) and gradually add Gaussian noise \u0454 ~ \u039d(0,1)\nto the input\nq(xt|xo) = N(xt; \u221a\u0101txo, (1 \u2013 \u0101t)I), (1)\nwhere \u0101t := \u03a0=1(1\u2212 \u03b2s). \u03b21,..., \u03b2r is a variance sched-\nule that controls the amount of noise added at each time step\nt. The diffusion model is then trained to approximate the\nbackward process po(xt-1|xt) using a denoising network\n\u20ac0(xt, t), whose parameters @ can be learned by minimiz-\ning the the mean squared error (MSE) between the predicted\nand target noise\nLDM := Ex,e~N(0,1),t [||\u20ac \u2013 60(x, t)||2] . (2)\nIn order to extend the utility of diffusion models to ex-\ntremely high-dimensional data, such as high-resolution im-\nages, LDM [54] first compress the original image into a\nlower-dimensional space, called latent space, using percep-\ntual image compression. Specifically, LDM first utilize a\npre-trained encoder \u025b to map the input image x \u2208 RH\u00d7W\u00d73\nfrom the pixel space into a latent representation z = E(x),\nwhere z \u2208 Rh\u00d7w\u00d7c. Then, LDM executes the forward pro-\ncesses q(Z1:T Zo) and backward processes po(Zt-1|Zt) on\nthe latent representation z. The parameters of the denois-\ning network \u20ac (zt, t, c) can be learned by minimizing the\nfollowing objective function\nLLDM := E\u025b(x),c,e~N(0,1),t [||\u20ac - Eo(Zt, t, c) ||2], (3)\nwhere c denotes the conditional signal that diffusion mod-\nels can be conditioned on. For story visualization task, c\nrepresents storyline S. The generated image can be ob-\ntained from the denoised latent zo using a pre-trained de-\ncoder x = D(z).\nTask overview. The goal of story visualization is to\ngenerate a sequence of images that align with a multi-\nsentence storyline, ensuring semantic consistency with the\ntext. Characters and scenes should remain consistent to co-\nhesively visualize the storyline. In story continuation, the\nfirst image is provided as additional input. The first im-\nage offers initial storyline information, guiding the model\nin generating subsequent images by extracting character and\nscenes, eliminating the need to generate them from scratch.\nTask formulation. Given a storyline S = {S1,...SN}\ncomposed of N sentences, the objective of story visualiza-\ntion is to generate a sequence of images \u0128 = {\u00ce\u00b9, \u2026\u2026\u2026\u00ceN}\nconsisting of corresponding N images. During training,\nground truth images are denoted as I = {I\u00b9, . . . IN}. For\nthe story continuation task, the goal is to generate subse-\nquent images \u0128 = {\u00ce\u00b2,\u2026\u2026\u2026\u00ceN } given storyline S and the\nfirst image I\u00b9.\n3.2. Model Architecture\nSpatial-Temporal attention. UNet consists of a series\nof spatial downsampling and upsampling blocks with skip\nconnections, each containing spatial convolution and spatial\nattention layers to capture the spatial dependencies. How-\never, temporal dependencies is also crucial for enhancing\nthe consistency of generated images in the task of story\nvisualization. Inspired by [7, 53, 66, 67, 76], we intro-\nduce temporal convolution and temporal attention layers"}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nDatasets. We employ two popular benchmark datasets,\nPororoSV [34] (Li et al., 2019) and the FlintstonesSV [21],\nto evaluate the performance of our model in both story visu-\nalization and story continuation tasks. PororoSV contains\n10,191, 2,334, and 2,208 stories within the train, valida-\ntion, and test splits, respectively, including 9 main charac-\nters. On the other hand, FlintstonesSV contains 20,132,\n2,071, and 2,309 stories within the train, validation, and\ntest splits, respectively, featuring 7 main characters and 323\nbackgrounds. Each story within these datasets comprises 5\nconsecutive story images.\nAutomatic metrics. To evaluate the quality of generated\nimages, we employ the following three evaluation metrics\nthe same as previous works [14, 40, 42, 46] in story visu-\nalization: (1) Frechet Inception Distance (FID) [24]: Mea-\nsures the distance between feature vectors of ground truth\nand generated images. (2) Frame accuracy (Frm. Acc.):\nUtilizes a fine-tuned Inception-v3 model to evaluate the\ncharacter match to the ground-truth. (3) Character F1-score\n(Char. F1): Measures the quality of generated characters\nusing the same model as Frame accuracy."}, {"title": "4.2. Quantitative Results", "content": "Story Visualization. Table 2 shows quantitative results for\nstory visualization on the PororoSV dataset. We compare\nthe performance of our TemporalStory to several state-of-\nthe-art methods, including StoryGAN, CP-CSV, DUCO-\nSTORYGAN, VLC-STORYGAN, VP-CSV, Word-Level\nSV, CMOTA-HR, CMOTA, AR-LDM, Causal-Story and\nStoryImager. Table 3 shows quantitative results for story vi-\nsualization on the FlintStonesSV dataset. We compare the\nperformance of our TemporalStory to several state-of-the-\nart methods, including StoryGAN, DUCO-STORYGAN,\nVLC-STORYGAN, CMOTA-HR, CMOTA and StoryIm-"}, {"title": "4.3. Qualitative Results", "content": "Story Visualization. Figure 4 demonstrates qualitative\ncomparison of story visualization on PororoSV and Flint-"}, {"title": "5. Human Evaluation", "content": "Due to the limitations of metrics such as FID, Char. F1,\nand Frm. Acc. in accurately reflecting the quality of gen-\nerated story images, we conducted human evaluations for\nthe story visualization task on the PororoSV dataset, focus-\ning on Visual Quality, Semantic Relevance, and Temporal\nConsistency. We randomly selected 100 pairs of story im-\nage sequences generated by ARLDM [46] and our Tempo-\nralStory on the PororoSV dataset. Annotators were tasked\nto select better sequence for three attributes: Visual Qual-\nity, Semantic Relevance, and Temporal Consistency. Each\npair of generated story image sequences was evaluated by\nthree annotators. As shown in Table 4, the evaluation re-\nsults indicate that our TemporalStory outperforms ARLDM\nsignificantly across all three attributes."}, {"title": "5.1. Ablation Studies", "content": "Ablation study of the proposed components. In or-\nder to investigate the benefit of each proposed component,\nwe conduct an ablation study on the proposed StoryFlow"}, {"title": "6. Conclusion", "content": "In this paper, we propose TemporalStory, a novel ap-\nproach for story visualization and story continuation by\nincorporating Spatial-Temporal Attention to capture com-\nplex spatial and temporal dependencies in story images.\nThis method generates coherent images from a given story-\nline, significantly enhancing consistency. The Text Adapter\nimproves storyline context understanding, while the Sto-\nryFlow Adapter uses scene changes between images to\nguide the model. Extensive experiments on PororoSV and\nFlintstonesSV show that TemporalStory outperforms pre-\nvious state-of-the-art methods in both tasks. These results\nhighlight the effectiveness of TemporalStory in generating\nvisually consistent and contextually coherent story images,\nsuggesting promising directions for future research in story\nvisualization and story continuation."}]}