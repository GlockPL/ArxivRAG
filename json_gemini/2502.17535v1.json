{"title": "THE LOTTERY LLM HYPOTHESIS, RETHINKING WHAT\nABILITIES SHOULD LLM COMPRESSION PRESERVE?", "authors": ["Zhenheng Tang", "Xiang Liu", "Qian Wang", "Peijie Dong", "Bingsheng He", "Xiaowen Chu", "Bo Li"], "abstract": "Motivated by reducing the computational and storage costs of LLMs, model com-\npression and KV cache compression have attracted much attention from researchers.\nHowever, current methods predominantly emphasize maintaining the performance\nof compressed LLMs, as measured by perplexity or simple accuracy on tasks\nof common sense knowledge QA and basic arithmetic reasoning. In this blog,\nwe present a brief review of recent advancements in LLMs related to retrieval-\naugmented generation, multi-step reasoning, external tools, and computational\nexpressivity, all of which substantially enhance LLM performance. Then, we\npropose a lottery LLM hypothesis suggesting that for a given LLM and task, there\nexists a smaller lottery LLM capable of producing the same performance as the\noriginal LLM with the assistance of multi-step reasoning and external tools. Based\non the review of current progress in LLMs, we discuss and summarize the essential\ncapabilities that the lottery LLM and KV cache compression must possess, which\nare currently overlooked in existing methods.", "sections": [{"title": "CURRENT EFFORTS ON COMPRESSING LLMS AND KV CACHE", "content": "LLMs have demonstrated remarkable proficiency in natural language processing, enabling sophisti-cated interactions and understanding of human language (OpenAI, 2023). To learn the tremendousknowledge in the training datasets, the current advanced LLMs like GPT4 (OpenAI, 2023) andLlama3 (Touvron et al., 2023) have enormous parameters like 7 ~ 750 billion. Training such an LLMrequires extensive computational resources, often measured in enormous GPU days using advancedNVIDIA GPUs (Touvron et al., 2023). This results in substantial electricity consumption, impactingboth economic and energy costs (Samsi et al., 2023; Tang et al., 2019), and raising concerns regardingsustainable computing (Wilkins et al., 2024). Furthermore, providing inference services for LLMsnecessitates numerous GPUs and incurs additional energy costs (Samsi et al., 2023; Tang et al., 2019),making it a significant challenge for widespread deployment (Patel et al., 2024).\nCompression methods. To this end, both academic researchers and industrial engineers are trying tocompress model parameters and reduce the model into a smaller one while keeping its performanceunchanged. The typical compression algorithm includes the pruning (Sun et al., 2024b; Frantar &Alistarh, 2023; Dong et al.) and quantization (Yao et al., 2022; Dettmers & Zettlemoyer, 2022; Donget al., 2024) of LLM parameters, and KV cache compression (Zhang et al., 2023b; Xiao et al., 2024).\nHowever, most of the current methods that compress LLMs and KV cache only show guaranteedperformance of the perplexity on some basic language tasks like Wikitext2 (Merity et al., 2016) andPTB (Marcus et al., 1993), common sense knowledge QA tasks (Hendrycks et al., 2021; Talmor et al.,2019) and the basic arithmetic reasoning tasks (Cobbe et al., 2021) in small-scale evaluation but notin the real-world industrial scenarios.\nMissed aspects. Some recent studies show that the LLMs may lose their advanced crucial abilitiesunder the compressions like the long-context retrieval, long-context generation and long-document"}, {"title": "TACKLING REDUNDANT AND UNREAL KNOWLEDGE OF LLMS WITH\nKNOWLEDGE RETRIEVAL", "content": "Redundant Knowledge. In contemporary applications, many individuals utilize LLMs as encyclo-pedic resources or to verify news and academic research, akin to an Internet search engine. Recentstudies indicate that LLMs exhibit varying performance in knowledge retrieval, contingent uponthe popularity of the information (Mallen et al., 2023a). Specifically, a small subset of real-worldquestion-answer (QA) pairs constitutes the majority of interactions, while a limited number ofQAs receive frequent attention, demonstrating a long-tail distribution in their popularity (Mallenet al., 2023a). LLMs tend to perform better on high-popularity QAs compared to those with lowerpopularity.\nHallucinated Knowledge. LLMs often generate unreal outputs rather than factual knowledge, whichis a phenomenon known as hallucination (Huang et al., 2023). This issue has garnered significantattention from researchers (Huang et al., 2023). There is ongoing debate regarding the feasibility ofcompletely eliminating hallucinations (Farquhar et al., 2024). Some studies suggest that hallucinationsare inevitable, as they are a byproduct of the model's reasoning and generalization abilities (Banerjeeet al., 2024; Xu et al., 2024b).\nRetrieval Augmented Generation (RAG). Large Language Models (LLMs) exhibit robust in-contextlearning capabilities, enabling them to respond to queries using prompts rather than relying solelyon their internal knowledge encoded within model parameters. Consequently, external knowledgesources such as scholarly articles, web pages, books, and other documents can be integrated intoprompts to facilitate the retrieval of additional factual information (Yao et al.), thereby mitigating theoccurrence of hallucinations (Yao et al.). This approach raises significant research questions:\nIs it necessary to store all knowledge within LLM parameters if RAG can accurately retrieve factualinformation from external knowledge bases? If not, which knowledge should be stored and whichshould not?\nConsidering two extreme scenarios:\n\u2022 Storing all knowledge in model parameters: If all knowledge is stored within model parameters,LLMs function as oracle machines, obviating the need for RAG. However, training such an LLM isnearly impossible because not all knowledge can be collected and never outdated (Xu et al., 2024b;Banerjee et al., 2024). Moreover, deploying such a large model is inefficient.\n\u2022 Storing all knowledge in external knowledge bases: If all knowledge is stored externally, LLM pa-rameters could potentially be reduced significantly, allowing for the retrieval of factual informationduring inference.\nNevertheless, LLMs require foundational common knowledge to perform tasks such as reasoningand accurate retrieval. This issue will be further explored in subsequent sections. Thus, compressingall knowledge into external knowledge bases is not feasible. Investigating the nature of learnedknowledge and identifying which knowledge triggers the grokking phenomenon in LLMs remains anopen research question (Nanda et al., 2023).\nTrade-off between model size and knowledge base. Some studies indicate that adaptive knowledgeretrieval is a promising direction to enhance the performance of LLMs and may help to find anoptimal trade-off between the knowledge base and model size (Jeong et al., 2024b). The adaptive"}, {"title": "EXTERNAL TOOLS", "content": "Advanced Large Language Models (LLMs) demonstrate remarkable capabilities in function calling,which involves invoking external tools to address specific tasks. These external tools may includeInternet search engines (Qin et al., 2023), arithmetic calculation functions (Schick et al., 2023),system operations (Ge et al., 2023; Mei et al., 2024), game interfaces, and more. These are formulatedinto programming function calls (Abdelaziz et al., 2024) and conveyed to LLMs via prompts.Based on the function descriptions, LLMs determine which function to call to resolve the givenproblems (Abdelaziz et al., 2024).\nArithmetic Function Calls. To solve arithmetic problems, LLMs are trained on arithmeticdatasets (Cobbe et al., 2021). However, simple errors often occur during the arithmetic reason-ing process, such as LLMs erroneously determining that 9.11 is greater than 9.9 (Choi et al., 2024).\nTo mitigate this, some studies propose enabling LLMs to generate programs that include arithmeticoperations and utilize an external Python interpreter to solve these problems (Gao et al., 2023a).\nAdditionally, some research suggests leveraging arithmetic function calls to solve arithmetic prob-lems (He-Yueya et al.). Experimental results indicate that arithmetic function calling can significantlyenhance the performance of LLMs on arithmetic tasks (Gao et al., 2023a; Yang et al., 2023).\nInternet Search Engine. To augment LLM knowledge with online and dynamically updated externalinformation, the Internet search engine is employed as an external tool (Yao et al.; Vu et al., 2023).\nExperimental results demonstrate that interacting with an Internet search engine, such as a simpleWikipedia API, can significantly improve LLM performance on knowledge retrieval tasks (Yao et al.).\nLLM Operating System (OS). By conceptualizing LLM calls as system calls akin to traditionaloperating systems, recent studies propose developing a new LLM-as-OS framework (Ge et al., 2023),which allows LLMs to invoke external tools like applications in an OS. Recent studies also propose theAIOS framework (Mei et al., 2024) to decouple LLM calls from system calls and implement variousmanagers to enhance AIOS efficiency. The optimized agent framework from the OS perspective\nsignificantly improves both the efficiency and performance of LLM calls."}, {"title": "COMPUTATIONAL EXPRESSIVITY OF LLMS", "content": "Basic Transformer Architecture. Basic transformers, devoid of intermediate decoding steps, exhibitlimited computational expressivity (Merrill & Sabharwal, 2023; Chiang et al., 2023), aligning with therelatively small circuit complexity class $TC0$ (Merrill & Sabharwal, 2023). These basic transformersfall short of Turing completeness, as they are incapable of solving problems that are complete forclasses larger than $TC0$, such as simulating automata, which is $NC1$-complete.\nDecoding-based Transformers. Decoding-based transformers generate output sequentially, word byword, rather than producing a single answer. This approach enhances their computational expressivitycompared to basic transformers, with expressivity increasing in tandem with the length of thedecoding steps (Merrill & Sabharwal). This phenomenon elucidates why the Chain-of-Thought (CoT)reasoning process (Wei et al., 2022) augments the computational expressivity of LLMs (Feng et al.,2023). Some studies demonstrate that with linear steps, transformers equipped with projected-normcan theoretically simulate a Turing automaton (Merrill & Sabharwal). Recent research indicatesthat autoregressive decoding, which facilitates the processing of arbitrarily long input strings, cansimulate a universal Turing machine (Schuurmans et al., 2024).\nDecoding with External Memory. Research suggests that external memory can enhance the compu-tational expressivity of LLMs (Deletang et al., 2023), potentially endowing them with approximateTuring completeness (Perez et al., 2021). Recent advancements have introduced the Stack-Attentionmechanism to further augment the reasoning capabilities of LLMs (Li et al., 2024a). With theintegration of external memory and simple regular expression parsers, transformers can simulate theexecution of a universal Turing machine, specifically $U_{15,2}$ (Schuurmans, 2023)."}, {"title": "MULTI-STEP REASONING", "content": "The Chain-of-Thought (CoT) reasoning paradigm demonstrates that engaging in detailed, step-by-stepreasoning can significantly enhance the performance of Large Language Models (LLMs) comparedto single-step reasoning (Wei et al., 2022). This improvement arises because single-step reasoningmay overlook crucial intermediate steps that are instrumental in problem-solving (Wei et al., 2022).\nThe multi-step reasoning process, inspired by human cognitive processes, can substantially elevatethe performance of LLMs (Wei et al., 2022).\nSingle LLM Call. CoT exemplifies a single LLM call, utilizing the model once. Beyond explicitprompting to initiate detailed reasoning, recent studies propose enabling LLMs to execute advancedsearch algorithms during the decoding process, such as Monte-Carlo Tree Search (MCTS) (Leblondet al., 2021) or Q-star search (Chakraborty et al., 2024). Additionally, some research suggests em-ploying backtracking algorithms to allow LLMs to reconsider previous decisions, thereby enhancingfinal performance (Fu et al.).\nMultiple LLM Calls. Some approaches advocate for multiple LLM calls, which operate indepen-dently of each other, potentially yielding correct answers across these calls (Brown et al., 2024).\nBeyond the single CoT call, CoT-SC proposes multiple CoT-based LLM calls, selecting the optimalanswer to improve final outcomes (Wang et al., b). However, these answers exhibit direct dependen-cies. To optimize scheduling and decomposition of the reasoning process, Tree-of-Thought (ToT)reasoning (Yao et al., 2024) and Graph-of-Thought (GoT) reasoning (Besta et al., 2024) have beenintroduced, structuring reasoning steps in tree-like or graph-like configurations. Some studies alsosuggest integrating knowledge graphs, enabling LLMs to reason within graph structures to enhance"}, {"title": "LOTTERY LLM HYPOTHESIS", "content": "Consider an original language model $f_{\\theta}$ parameterized by the $\\theta \\in \\mathbb{R}^{k_{\\theta}}$, capable of processing input oftoken length n, and an input problem $q \\in \\mathbb{R}^{m \\times h}$ with token length $m < n$ and ground truth $\\mu \\in \\mathbb{R}^{l \\times h}$.The problem q is a question consisting of a sequence of words. And the \u00b5 is also a sequence ofwords representing the answer to the question q. h is the dimension of the word embedding. Theperformance of the model is evaluated using a performance measure $P(\\cdot)$, expressed as $P(f_{\\theta}(q), \\mu)$which map its inputs as a scalar value. We hypothesize the existence of a smaller language model$g_{\\phi}$ with parameters $\\phi \\in \\mathbb{R}^{k_{\\phi}}$ ($k_{\\phi} < k_{\\theta}$) and the same input length n, which can solve the problem qwith performance comparable to $f_{\\theta}$, such that:\n$P(f_{\\theta}(q), \\mu) \\leq P(A_{g_{\\phi},D,R,C,M}(q), \\mu),$  \\tag{1}\nwhere A represents a reasoning algorithm thatmay involve one or multiple invocations of $g_{\\phi}$with various inputs, including the original prob-lem q, documents $d \\in D$ retrieved from theexternal knowledge base D, or function calls$c \\in C$ retrieved from external tools C usingthe retriever R. Each document $d \\in \\mathbb{R}^{n \\times h}$is a vector of words. While the function callsc : $\\mathbb{R}^{n \\times h} \\rightarrow \\mathbb{R}^{n \\times h}$ is a provided function.The knowledge base D is a vector database stor-ing vector-documents as key-value pairs, and Mdenotes the external memory that stores inter-mediate results. All D, C, and M are sets. Anditems in D and C are key-value pairs dependson the specific tasks, like vector database (Panet al., 2024). The retriever R is a function that re-trieves the required documents or function callsfrom the D or C based on the request. Andits specific implementation can be various (Gaoet al., 2023b).\nThe reasoning algorithm A is described as Al-gorithm 1 in Figure 1 and Figure 2, employing adivide-and-conquer strategy to solve the originalproblem q. This dynamic divide-and-conquermethodology is versatile and applicable to nu-merous contemporary reasoning algorithms.\nRecursive and Dynamic Scheduling. Algo-"}, {"title": "CONCLUSION", "content": "This blog aims to elucidate the potential of the lottery LLM and to summarize the essential capabilitiesthe lottery LLM should possess, which are currently lacking in existing methods of LLM andKV cache compression. The discussion on redundant knowledge within LLMs also highlights thetrade-off between knowledge storage and reasoning capabilities. With the development of the lotteryLLM, alongside external tools, knowledge bases, and a robust algorithm A, there is potential forthe lottery LLM to function as a meta-agent akin to human cognition. Its external memory couldserve as long-term memory, the prompt as short-term memory, and the LLM inference process $g_{\\phi}$as the fundamental cognitive process. External tools and knowledge bases can be considered assupplementary tools commonly used in daily life. Deploying the lottery LLM could significantlyreduce energy and resource consumption in large-scale LLM-driven applications. Future research onLLM compression, KV cache compression, and other efficient LLM methodologies should addressboth efficiency and the essential capabilities of LLMs."}]}