{"title": "Scaling 3D Reasoning with LMMs to Large Robot\nMission Environments Using Datagraphs", "authors": ["W.J. Meijer", "A.C. Kemmeren", "E.H.J. Riemens", "J.E. Fransman", "M. van Bekkum", "G.J. Burghouts", "J.D. van Mil"], "abstract": "This paper addresses the challenge of scaling Large\nMultimodal Models (LMMs) to expansive 3D environments. Solv-\ning this open problem is especially relevant for robot deployment\nin many first-responder scenarios, such as search-and-rescue\nmissions that cover vast spaces. The use of LMMs in these settings\nis currently hampered by the strict context windows that limit\nthe LMM's input size. We therefore introduce a novel approach\nthat utilizes a datagraph structure, which allows the LMM to\niteratively query smaller sections of a large environment. Using\nthe datagraph in conjunction with graph traversal algorithms, we\ncan prioritize the most relevant locations to the query, thereby\nimproving the scalability of 3D scene language tasks. We illustrate\nthe datagraph using 3D scenes, but these can be easily substituted\nby other dense modalities that represent the environment, such\nas pointclouds or Gaussian splats. We demonstrate the potential\nto use the datagraph for two 3D scene language task use cases,\nin a search-and-rescue mission example.", "sections": [{"title": "I. INTRODUCTION", "content": "This paper addresses the challenge of scaling the use of\nLarge Multi-modal Models (LMMs) to large 3D environments.\nCurrent models can perform a wide range of 3D scene lan-\nguage tasks such as spatial and semantic reasoning (Figure 2,\nHong et al. [1]), but the impressive capabilities of these models\ndo not yet transfer to large environments that are generally\nencountered in search-and-rescue settings.\nIn these large environments, current 3D LMMs seem to\nface a similar limitation as many Vision Language Models\n(VLMs), namely the resolution curse problem\u00b9. The VLMs\ncannot properly encode the information in high-resolution\ninput images due to the fixed and limited input size of the\nmodel [2]. Similarly, the context window size in transformer-\nbased models limits the maximum number of tokens that can\nbe used [3, 4]. This limits current 3D LMMs to be able to\nprocess only small areas. For example, current models have\nnot yet been used on areas larger than 10 rooms [1, 5].\nRecent developments tackle this issue by enabling\ntransformer-based models to process contexts of infinite length\nflexibly [6]. However, it will take substantial effort to develop\nthe models that are capable of performing all 3D scene tasks\nin Figure 2 in large 3D environments. These developments\nare slowed by a lack of large 3D scene datasets [7] and the\nvast hardware requirements for training new LMMs [6]. In\nthe meantime, the approach in this paper will enable us to use\nexisting 3D LMMs in large environments instead.\nOur approach is inspired by how the resolution curse is\noften solved for 2D tasks, using tiling. In tiling, the input\nimage is subsampled into several tiles of lower resolution.\nIn more advanced works, strategies to dynamically choose\nthese tiles are proposed, by internal dialogue with a Large\nLanguage Model and visual memory [8] or by exploiting the\nattention of the transformer [9]. Similarly, we divide large 3D\nenvironments into smaller areas by exploiting spatial structure\nand then choose the subset of areas that is potentially relevant\nto the task (Figure 1). This, for example, enables us to only\nconsider the part of the environment that is near and reachable\nto a robot.\nIn this paper, we consider a scene representation that retains\nfull 3D information, because we believe that having access to\ndetailed information is crucial to search-and-rescue missions.\nFor example, there is an important distinction between a\nperson lying on the floor of the hallway and a person lying in\nbed. This is often not captured when using 3D scene graphs as\nrepresentation, as context information is lost during the com-\npression steps in the mapping procedure [10, 11]. Subsequent\nreasoning tasks then cannot access important contextual details\nthat were initially captured by the sensors [12].\nWe propose to leverage a spatial graph structure to subdivide\nvast 3D scenes into smaller areas (Figure 1). With existing\ngraph traversal algorithms, smaller chunks of 3D information\ncan be fed iteratively into the LMMs. We demonstrate the\npotential of this approach using two algorithms. Algorithm 1"}, {"title": "II. APPROACH", "content": "This work considers robot deployments for time-critical\nfirst-responder scenarios, such as search-and-rescue missions.\nFirst responders operate in expansive and unknown envi-\nronments, where it is important to quickly build situational\nawareness (SA) [18], and reason about complex questions such\nas the safety of an environment. For this, we use currently ex-\nisting LMMs and combine them with a datagraph to bypass the\nLMM's limited context size. The following sections provide\nmore details on the datagraph and how existing graph traversal\nalgorithms then allow for iterative prompting of the LMM.\n\nA. The datagraph\nThe datagraph is formalized as $G = (V, E)$ with node set\n$V$ and edge set $E$. Each node $v \\in V$ has a pose $x$ and a\ndata snapshot $s$, which makes the tuple $v = (x_v, s_v)$. An edge\n$e_{ij} = (v_i, v_j)$ connects two nodes for neighboring areas. One\ncould consider additional requirements that an edge is only\nadded if there is a traversable route between the two areas [19].\nThis would make $G$ compatible with navigation or exploration\ngraphs commonly used in autonomy stacks [20, 19, 21]. The\n$s$ in the node can be data of any modality that can be used\nto query an LMM, including 3D scenes, Gaussian splats, or\nimages.\n\nB. Graph traversal for iterative LMM prompting\nDepending on the task that the LMM will execute, different\nparts of the environment could be interesting to prompt.\nThe challenge for graph traversal thus is to prompt areas\nwith highest priority first. We exemplify this with two\ngraph traversal algorithms. The first algorithm (Algorithm 1)\nprioritizes the areas that are closest to the robot. The second\nalgorithm (Algorithm 2) retrieves information along a path\nthrough the environment to enable, for example, safer\nnavigation.\n\n1) 3D scene language tasks prioritized by robot proximity:\nVarious situations may require a robot to first recall areas\nclosest to its current situation and execute 3D scene tasks in\nthese areas. A clear example in search-and-rescue missions is\nwhen searching for objects in the environment. A robot may\nneed to go through a locked door and start to search in its"}, {"title": "III. DISCUSSION AND CONCLUSION", "content": "In this paper, we have introduced a novel datagraph structure\nthat significantly expands the spatial context over which Large\nMultimodal Models (LMMs) can reason. The method was\nillustrated using 3D scenes, but in principle it is agnostic to the\ndata type stored in the graph. The graph allows us to prioritize\ndata from locations that are most relevant to the query, such\nas those closest to the agent. This prioritization improves the\nscalability of 3D scene language tasks, enabling their use in\nexpansive robotic mission environments.\nLooking ahead, there are several challenges and oppor-\ntunities for future work. One such challenge arises at the\nboundaries between scenes. Relevant information might end up\nappearing in both scenes, for example, leading to duplicate de-\ntections, or scattered over distinct scenes, negatively affecting\nperformance. Moreover, we see a need for experimental results\nto support our proposed approach. For instance, stitching\nthe rooms of the Scene Verse dataset together could form\nlarge environments that would provide a robust testbed for\nour approach. Additionally, investigating methods of caching\nidentical queries along a path for Algorithm 2 could further\nenhance the efficiency and effectiveness of our approach.\nIn conclusion, our work presents a relevant step forward in\nthe application of LMMs in large environments. By leveraging\nthe datagraph structure and prioritizing relevant locations, we\nhave demonstrated a scalable solution for 3D scene language\ntasks in large robotic mission environments. As we continue\nto refine our approach and address the challenges identified,\nwe are optimistic about the potential impact of our work on\nthe future of robotic missions."}]}