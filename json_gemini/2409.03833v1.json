{"title": "AI forecasting of higher-order wave modes of spinning\nbinary black hole mergers", "authors": ["Victoria Tikiab", "Kiet Pham", "E. A. Huertaabd"], "abstract": "We present a physics-inspired transformer model that predicts the non-linear\ndynamics of higher-order wave modes emitted by quasi-circular, spinning,\nnon-precessing binary black hole mergers. The model forecasts the waveform\nevolution from the pre-merger phase through to the ringdown, starting with\nan input time-series spanning $t \\in [-5000M, -100M)$. The merger event,\ndefined as the peak amplitude of waveforms that include the $l = |m| = 2$\nmodes, occurs at $t = 0M$. The transformer then generates predictions over\nthe time range $t \\in [\u2212100M, 130M]$. Training, validation, and test sets were\ngenerated using the NRHybSur3dq8 model, considering a signal manifold de-\nfined by mass ratios $q \\in [1, 8]$; spin components $\\varsigma_{z_{1,2}} \\in [-0.8, 0.8]$; modes up\nto $l \\le 4$, including the (5, 5) mode but excluding the (4, 0) and (4, 1) modes;\nand inclination angles $\\theta \\in [0, \u03c0]$. We trained the model on 14,440,761 wave-\nforms, completing the training in 15 hours using 16 NVIDIA A100 GPUs in\nthe Delta supercomputer. To evaluate the model, we utilized 4 H100 GPUs in\nthe DeltaAI supercomputer to compute, within 7 hours, the overlap between\nground truth and predicted waveforms on a test set of 840,000 waveforms.\nThe mean and median overlaps achieved were 0.996 and 0.997, respectively.\nAdditionally, we conducted interpretability studies to elucidate the waveform\nfeatures that our transformer model utilizes to produce accurate predictions.\nThe scientific software used for this work is released with this manuscript.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence (AI) is being explored and applied in earnest to ad-\ndress computational grand challenges in gravitational wave astrophysics [1-\n3]. Contemporary AI applications cover the entire spectrum of gravitational\nwave data science, ranging from data quality [4-8], detection [9-17], param-\neter estimation [18-24], denoising [6, 25-27], detection [28\u201332] and forecast-\ning [33-35] of sources with potential electromagnetic counterparts, among\nmany others. Ref. [36] provides a comprehensive compilation of recent AI\nadvances in gravitational wave astrophysics.\nIn this article we study whether AI can learn and predict the physics of\nhigher-order wave modes emitted by the merger of quasi-circular, spinning,\nnon-precessing binary black hole mergers. This work is inspired by previous\nstudies demonstrating that AI can forecast the time-series signals of the $l =$\n$|m| = 2$ mode of quasi-circular, non-spinning binary black hole mergers [37],\nand of quasi-circular, spinning, non-precessing binary black hole mergers [38].\nIn practice, these two studies [37, 38] demonstrated that AI can learn the\nkey features that define the strongly non-linear, highly dynamical behaviour\nof binary black hole mergers, and then predict the time-series evolution of\nthe pre-merger, merger and ringdown phases. This paper explores the use of\ntransformers to learn the non-linear properties of higher-order wave modes\nand discusses methods for training these models on supercomputers with\nmillions of waveforms.\nTo gain a basic understanding of the type of physics that physics-inspired\nAI models need to master in the context of higher-order wave modes, we\npresent in Figure 1 a comparison between former studies and this work. The\nleft column of Figure 1 displays samples of waveforms used in previous work\n[37, 38], which have a very smooth evolution throughout inspiral, merger and\nringdown. Although learning the physics of these waveforms poses challenges,\nthese signals are relatively simple compared to the significantly more complex\nwaveforms analyzed in this work, as illustrated in the right panel of Figure 1.\nIn this case, the amplitude evolution is no longer monotonic up until merger,\nat $t = 0M$, followed by a smooth decay. Rather, these waveforms show a\ncomplex structure due to the inclusion of higher-order wave modes, which"}, {"title": "2. Methods", "content": "We produced three independent datasets of modeled waveforms using the\nnumerical relativity (NR) surrogate model NRHybSur3dq8 [40]. These wave-"}, {"title": "2.1. Data", "content": "forms describe the gravitational emission from quasi-circular, spinning, non-\nprecessing binary black hole mergers throughout the inspiral, merger, and\nringdown phases. The datasets are constrained within the valid parameter\nspace for this NR surrogate model, i.e., mass-ratios $q \\le 8$ and individual\nspins $(|s_{7}| \\le 0.8$ for $i = {1, 2})$. The gravitational wave strain time-series, $h$,\ncan be expressed as a sum of spin-weighted spherical harmonic modes, $h_{lm}$,\non the 2-sphere [41]\n$h(t, \\theta, \\phi) = \\sum_{l=2}^{\\infty} \\sum_{m=-l}^{m=l} h_{lm}(t) {}_{-2}Y_{lm} (\\theta, \\phi),$ (1)\nwhere ${}_{-2}Y_{lm}$ are the spin-weight-2 spherical harmonics, $\\theta$ is the inclination\nangle between the orbital angular momentum of the binary and line of sight\nto the detector, and $\\phi$ is the initial binary phase, which we set to zero. We\ninclude the higher-order wave modes $l < 4$ and $(l, m) = (5, 5)$, except for\n(4, 0) and (4, 1). The waveforms cover the timespan $t = [-5000M, 130M]$.\nTraining dataset. The training dataset consists of 14,440,761 waveforms, gen-\nerated by sampling the mass ratio $q \\in [1, 8]$ in steps of $\\delta q = 0.1$; individual"}, {"title": "Test and validation datasets", "content": "spins $s_i \\in [-0.8, 0.8]$ in steps of $\\delta s = 0.1$; and inclination angle $\\theta \\in [0, \u03c0]$ in\nsteps of $\\delta \\theta = \u03c0/29$.\nTest and validation datasets. Each of these sets consists of 840,000 wave-\nforms generated by sampling values spaced evenly between the training set\nvalues. Figure 2 illustrates the sampling strategy for generating the training,\nvalidation, and test sets, demonstrating that these datasets are independent\nand do not overlap."}, {"title": "Sampling and encoder-decoder split", "content": "Sampling and encoder-decoder split. We reduce the temporal resolution of\nthe data by subsampling the original input sequences, selecting every other\ntimestep. This approach reduces computational complexity. The modeled\nwaveform is then split up such that the transformer's encoder module receives"}, {"title": "2.2. Architecture", "content": "the early inspiral sequence consisting of time steps $[-5000M, -100M]$. The\ndecoder module receives the inspiral, merger and ringdown segments made\nup of time steps $[-101M, 129M]$. The target waveform then consists of the\ngravitational wave segment $[-100M, 130M]$, i.e. the decoder target is one-\ntime step ahead of the decoder input, as shown in Figure 3.\nOur transformer model is based on the original architecture introduced in\nRef. [42], which leverages attention mechanisms to weigh different segments\nof the input sequence and improve performance in sequence prediction tasks.\nWe have modified the model to incorporate specific properties of gravitational\nwaves. Figure 4 schematically illustrates these modifications. Below, we de-\nscribe the functionality of each component of the transformer as it processes\nthe gravitational wave sequences. We represent the amplitudes of the input\nsequences for the encoder and decoder modules by $x_p$, where $p$ indicates the\nposition within the sequence of length $n$. Note that the sequence length $n$\nvaries for encoder and decoder inputs, denoted as $n_{enc}$ and $n_{dec}$, respectively."}, {"title": "2.2.1. Embedding", "content": "The transformer architecture is not intrinsically sensitive to the order of\ntokens within the input sequence. However, for sequence prediction, infor-\nmation about token order is critical. This issue is addressed by introducing\nnon-trainable positional encoding which embeds the input sequence into a\nhigher-dimensional space. Through positional encoding, the input sequence\n$x_p$ is transformed into a sequence of $d_{embed}$ dimensional vectors each taking\nthe form $X_p = (x_p, PE(p, 1), PE(p, 2), ..., PE(p, d_{embed}\u22121))$. This transfor-\nmation carries information about the original token's position. We represent\nthis transformed set of vectors as a matrix $X \\in \\mathbb{R}^{n \\times d_{embed}}$. The embedding\nfunction $PE$, where $p$ is the position of any token within the original se-\nquence and $i$ is the dimension within the positional encoding vector, takes\nthe form:\n$PE(p, 2i) = \\sin \\left(\\frac{p}{10000^{2i/d_{embed}}}\\right),$ (2)\n$PE(p, 2i + 1) = \\cos \\left(\\frac{p}{10000^{2i/d_{embed}}}\\right),$ (3)\nAs demonstrated in Ref. [42], this type of sinusoidal positional encod-\ning enables the model to interpolate and extrapolate positional encodings"}, {"title": "2.2.2. Encoder", "content": "The multi-head self-attention mechanism is designed to allow each posi-\ntion in the input sequence to attend to all positions within the same sequence.\nFor each of the $h (= 10)$ attention heads, the computation reads\n$Attention(Q^i, K^i, V^i) = softmax \\left(\\frac{Q^i(K^i)^T}{\\sqrt{d_k}}\\right) V^i.$ (4)\nHere, $Q^i \\in \\mathbb{R}^{n_{enc} \\times d_q}, K^i \\in \\mathbb{R}^{n_{enc} \\times d_k}$, and $V^i \\in \\mathbb{R}^{n_{enc} \\times d_v}$ denote the query,\nkey, and value matrices, respectively, defined as linear transformations of the\ninput matrix $X$:\n$Q^i = XW_Q^i, K^i = XW_K^i, V^i = XW_V^i,$ (5)\nwith weight matrices $W_Q^i \\in \\mathbb{R}^{d_{embed} \\times d_q}, W_K^i \\in \\mathbb{R}^{d_{embed} \\times d_k}$, and $W_V^i \\in\n\\mathbb{R}^{d_{embed} \\times d_v}$. In this work, $d_k = d_q = d_v = d_{embed}/h$.\n$Q K^T$ functions as a measure of similarity between keys and queries. The\nsoftmax activation operates on the rows of $Q K^T$ and returns weights corre-\nsponding to values $V$. The output of each attention head is concatenated\nalong the last dimension and then linearly transformed:\n$Attention(Q, K, V) = concat (Attention(Q^1, K^1, V^1), ...,$\n$Attention(Q^h, K^h, V^h)) W^O,$ (6)\nwith $W^O \\in \\mathbb{R}^{d_{embed} \\times d_{embed}}$, enabling the model to integrate information\nacross different representational subspaces. The resulting attention matrix\nhas the same shape as the input $X$. After the attention mechanism, each"}, {"title": "2.2.3. Decoder", "content": "position's output undergoes processing by a position-wise feed-forward net-\nwork, which applies two linear transformations with ReLU activations. Each\nsublayer (self-attention and feed-forward network) is enclosed by a residual\nconnection, followed by layer normalization. This configuration aids in sta-\nbilizing the training process of deep networks.\nThe decoder receives both the output from the encoder as well as the de-\ncoder input, which has been embedded in the same way as the encoder input.\nFirst, a masked multihead self-attention mechanism is applied to the decoder\ninput only. This mechanism prevents each position in the decoder input from\nattending to subsequent positions in the sequence, preserving causality and\nenabling autoregressive predictions during inference. The operation is de-\nfined as:\n$MaskedAttention(Q^i, K^i, V^i) = softmax \\left(\\frac{Q^i (K^i)^T + M}{\\sqrt{d_k}}\\right) V^i,$ (7)\n$M$ is a mask matrix that applies negative infinity to positions not to be\nattended to, ensuring that the softmax operation assigns them zero weight.\nAs before, $Q^i$, $K^i$, and $V^i$ are the query, key, and value matrices derived\nfrom linear transformations on the decoder's input. The dimensionalities of\nthese matrices are the same as those of the encoder self-attention matrices\nwith the exception of the sequence length $n$, which varies for encoder inputs\nand decoder inputs. Outputs from each attention head are again concate-\nnated and linearly transformed. These transformations leave the shape of\nthe decoder input unchanged.\nFollowing the self-attention layer, the cross-attention layer allows the decoder\nto focus on relevant positions in the encoder output sequence. The queries\n$Q$ stem from the previous decoder layer (denoted as $Y \\in \\mathbb{R}^{n_{dec} \\times d_{embed}}$), while\nthe keys $K$ and values $V$ are derived from the encoder output $X$:\n$CrossAttention(Q^i, K^i, V^i) = softmax \\left(\\frac{Q^i (K^i)^T}{\\sqrt{d_k}}\\right) V^i,$ (8)\nwith\n$Q^i = YW_Q^i, K^i = XW_K^i, V^i = XW_V^i,$ (9)"}, {"title": "2.2.4. hx mask", "content": "This operation preserves the shape of the queries, in this case the decoder\ninput, ensuring the sequence length of the decoder output matches that of\nthe decoder input. Again, $Q K^T$ serves as a measure of similarity between\nkeys and queries. The array returned by the softmax activation function\nis of shape $\\mathbb{R}^{n_{dec}\\times n_{enc}}$ and serves as an array of weights for the value array\n$V$. Figure 8 presents a visualization of these weights for a choice of input\nwaveform and attention head. The subsequent processing after application of\nthe attention mechanisms mirrors that of the encoder module: each position's\noutput is fed to a feed-forward network, residual connections and layer norms\nare applied after both attention layers as well as the feed forward network\nto stabilize training. Finally, a 1D convolution is applied to the decoder\noutput to produce predictions for the two time-series $(h_+, h_x)$ polarizations,\nrespectively.\nEmpirically, we find that separating waveforms with null cross-polarization\nleads to slightly higher reconstruction accuracy. To facilitate this, we apply\na mask to identify and filter waveforms with inclination angle $\\theta = \u03c0/2$, as\nthese exhibit null cross-polarization. This mask allows us to modify the final\nconvolutional layer and feed-forward layers, which are specifically separated\nfor these waveforms to better capture their unique characteristics. The re-\nmaining transformations in the model are consistent across all waveforms,\nensuring uniform processing while accommodating the distinct features of\nthe null cross-polarization cases."}, {"title": "3. Training and inference", "content": "For our transformer model, we used an embedding dimension of 160 (this\nincludes both the contributions from the real and the imaginary part), a feed-\nforward dimension of 80, and 10 attention heads. This model was trained\non the NCSA Delta system, using 4 quad NVIDIA A100 GPU nodes. We\nemployed a batch size of 16 and trained the model on 14,440,761 waveforms\nusing the Adam optimizer with mean squared error (MSE) loss metric. The\ninitial learning rate was set to 0.001, with decay applied during training.\nTraining was distributed across all GPUs using PyTorch's DistributedData-\nParallel (DDP) framework. The model was trained for 30 epochs and reached\nconvergence in around 15 hours.\nInference on 840,000 test waveforms was completed within 7 hours using\n1 node on the DeltaAI system, i.e., 4 NVIDIA H100 GPUs. During training,\nthe model processes entire sequences at once. Inference, in contrast, involves\nsequential autoregressive generation, where each output is used to predict\nthe next timestep. The functional differences between training and inference\nare illustrated in Figure 5."}, {"title": "4. Results and Discussion", "content": "We evaluated the performance of the transformer model on a test set of\n840,000 waveforms that covers the signal manifold under consideration. We\nused the overlap, O, as the figure of merit to quantify the accuracy of fore-\ncasted waveforms. This metric is equivalent to the cosine similarity between\nthe predicted and target waveforms. Specifically:\n$O(h_{true}, h_{pred}) = max_{t_c} \\frac{\\langle h_{true}, h_{pred}[t_c] \\rangle}{\\sqrt{\\langle h_{true}, h_{true} \\rangle \\langle h_{pred}[t_c], h_{pred}[t_c] \\rangle}},$ (10)\nwhere $\\langle h_{true}, h_{pred} \\rangle$ denotes the inner product between the complex-valued\nwaveforms $h_{true}$ and $h_{pred}$, and $h_{pred}[t_c]$ indicates that the waveform has been\ntime-shifted. A selection of waveform comparisons is depicted in Figure 6,\ndemonstrating the agreement between predicted and target waveforms for\nvarious parameter values. The figure includes two randomly selected wave-\nforms, an example of a low overlap waveform, and a waveform with $\\theta = \u03c0/2$,\nwhere the cross polarization $h_x = 0$.\nTo provide a more detailed assessment of the model's performance, the\ntop panel in Figure 7 presents a histogram of the overlap values across the\ntest set. Notably, none of the overlap values fall below 0.85, only 0.003%\nof test set waveforms fall below 0.9, while 7.15% fall below 0.99, indicating\nthat the model achieves high accuracy in most cases. The mean and median\noverlaps are 0.996 and 0.997, respectively.\nThe bottom panels in Figure 7 display overlaps as a function of merger\nparameters, specifically mass ratio $q$, inclination angle $\\theta$, and effective spin\n$\\varsigma_{eff}^z$, defined as [43]:\n$\\varsigma_{eff}^z = \\frac{q \\varsigma_1^z + \\varsigma_2^z}{1 + q}.$ (11)\nAreas of comparatively poor reconstruction accuracy are those at the edge\nof the signal manifold, i.e., both low mass ratio, $q$, and low inclination angle,\n$\\theta$, as well as those of both high mass ratio and inclination angle around\n$\u03c0/2$, as observed in the bottom left panel. Additionally, we observe that\nlow spin values of the heavier object, $\\varsigma_1^z$, generally result in slightly lower\nreconstruction accuracy, with these regions visible as distinctive green bands\nin the bottom right panel."}, {"title": "5. Summary and conclusions", "content": "We have introduced a physics-inspired transformer model capable of learn-\ning and forecasting the nonlinear dynamics of higher-order wave modes that\ndescribe the physics of quasi-circular, spinning, non-precessing binary black\nhole mergers.\nWe developed algorithms to train AI models using over 14 million wave-\nforms with distributed training, harnessing 16 NVIDIA A100 GPUs in the\nDelta supercomputer. This approach reduced time-to-solution to 15 hours.\nFurthermore, we used distributed inference to quantify the performance of\nour transformer model using 4 NVIDIA H100 GPUs in the DeltaAI super-\ncomputer, finding that over a test set of 840,000 waveforms, our transformer\nmodel predicts waveforms whose mean and median overlaps with ground\ntruth signals are 0.996 and 0.997, respectively. These results indicate that\nthe transformer model learned the physics of these sources to a high level of\naccuracy. There are some regions, however, in which the AI model may be\nimproved in future work, namely: low mass-ratios with anti-aligned spins,\nand high mass-ratio binaries that describe head-on mergers, i.e., with incli-\nnation angles $\\theta = \u03c0/2$. Nonetheless, our results indicate that these samples\nrepresent a small set, since only 7.15% of test cases have overlaps below 0.99.\nThe work presented here, in terms of AI models, distributed training,\nand inference, paves the way for future studies that may consider the signal\nmanifold of higher-order wave modes of spinning, precessing binary black hole\nmergers. This high dimensional signal manifold serves as a sandbox to stress\ntest the capabilities of physics-inspired AI models and exascale computing to\nlearn complex, non-linear dynamics of compact binary sources. This study\nshould be pursued in future work."}]}