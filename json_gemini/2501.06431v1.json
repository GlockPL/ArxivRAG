{"title": "Aug3D: Augmenting large scale outdoor datasets for Generalizable Novel View Synthesis", "authors": ["Aditya Rauniyar", "Omar Alama", "Silong Yong", "Katia Sycara", "Sebastian Scherer"], "abstract": "Recent photorealistic Novel View Synthesis (NVS) advances have increasingly gained attention. However, these approaches remain constrained to small indoor scenes. While optimization-based NVS models have made attempts to address this, generalizable feed-forward methods-offering significant advantages-remain underexplored. In this work, we train PixelNeRF, a feed-forward NVS model, on the large-scale UrbanScene3D dataset. We propose four training strategies to cluster and train on this dataset, highlighting that performance is hindered by limited view overlap. To address this, we introduce Aug3D, an augmentation technique that leverages reconstructed scenes using traditional Structure-from-Motion (SfM). Aug3D generates well-conditioned novel views through grid and semantic sampling to enhance feed-forward NVS model learning. Our experiments reveal that reducing the number of views per cluster from 20 to 10 improves PSNR by 10%, but the performance remains suboptimal. Aug3D further addresses this by combining the newly generated novel views with the original dataset, demonstrating its effectiveness in improving the model's ability to predict novel views.", "sections": [{"title": "I. INTRODUCTION", "content": "Photorealistic Novel View Synthesis (NVS) plays a vital role in applications requiring immersive experiences, such as AR/VR. As these methods gain popularity, there is a growing need to extend their capabilities to outdoor environments. In this study, we introduce Aug3D, a reconstruction-based augmentation technique designed to adapt existing outdoor datasets for NVS applications.\nGeneralizable models, exemplified by works like Pixel-NeRF [38] and Splatter-Image [24], render photorealistic novel views applicable to a wider range of inputs. These models are typically trained on smaller, object-centric scenes or indoor environments. In this work, we extend the ap-plication of NVS to large outdoor environments, aiming to broaden the scope of these methods for novel view synthesis.\nAlternatively, we take inspiration from scene-specific NeRF approaches in the research community, such as MegaNeRF [30] and VastGaussian [15], which fine-tune the NeRF model for NVS on specific scenes. These provide insights into selecting large outdoor scenes for training generalizable models to synthesize novel views.\nChallenges: Utilizing large outdoor scenes for general-izing NVS models presents several hurdles. The first arises from how these scenes are typically captured using drones, often employing constant-altitude grid scans over regions of interest [21], [30]. This results in captures that vary predominantly in a translated direction, introducing novel features to the scene between consecutive shots and posing difficulties for NVS methods to operate effectively. Addition-ally, most existing NVS work focuses on object-centric scene captures, for objects or indoor/outdoor environments. Such captures are vital, as the models rely on correlated features across input images to render novel views. Furthermore, generalizable NVS models typically train on datasets with minimal variation across input images (e.g., DTU dataset [11] ), where input images are placed in an object-centric way and exhibit controlled changes in elevation and azimuth."}, {"title": "II. RELATED WORK", "content": "As a result, novel views are interpolated rather than extrap-olated. Therefore, large outdoor scene environments used for scene-specific NVS models must (1) align with existing generalizable NVS model training setups, introducing fewer new elements across input images, and (2) feature input images that are closely spaced with controlled variations in view poses (e.g., DTU [11], Shapenet [4] dataset).\nAug3D: To address the challenges, we introduce Aug3D 1, an augmentation camera sampling strategy to adapt large outdoor scene datasets such as UrbanScene3D [16] and Mill-19 [30] for training generalizable novel view synthesis models. To mitigate sensitivity to input image poses, we cluster them into N views, maximizing shared points through Structure from Motion (SfM). However, sparse data collec-tion via drone flight requires further measures to enhance feature correlation among input images. To accommodate poses beyond original locations and ensure scale invariance, we sample camera poses by geometric reconstruction of large scenes. While reconstruction quality impacts these views, advances in photorealistic scene-specific NVS models such as Mega-NeRF [30], Block-NeRF [25], and VastGaussians [15] suggest sufficient development within the research com-munity for our proposed method.\nContributions: Our work addresses the question: \u201cHow can we effectively train existing Generalizable NVS models for large-scale outdoor datasets?\" Here are our key contribu-tions:\nGeneralizable NVS. Generalizable, image-based, or feed-forward NVS refers to models that can predict novel views at test time without having to re-optimize any learnable parameters. This is done by conditioning the architecture on sets of input views and describing different scenes while training. In contrast to the optimization-based single-scene networks, feed-forward models can learn semantic priors that make them superior in sparse input NVS.\nWorks like PixelNeRF [38] conditions NeRF on pixel aligned features recovered by projecting a query point onto feature maps of the input views. IBRNet [32] uses a similar approach but uses transformers. MVSNeRF[6] uses 3D con-volutions on top of a plane sweep of input images to get per voxel image features and uses that to condition NeRF per query point. MuRF[36] constructs a frustum volume aligned with the target view allowing them to utilize 3D convolutions to predict the volume. Similar recent works [24], [5], [8] have worked on generalizing 3D Gaussian splatting through input image conditioning.\nAll mentioned works focus on small to medium scale scenes with very limited target view ranges mainly due to the absence of city scale datasets amenable to feed-forward NVS. Our objective is to offer a training and data augmentation strategy to allow such works to learn large-scale urban scene priors efficiently.\nLarge Scale Scene Reconstruction: Large city-scale reconstruction has been a long-standing field of research. Many works attempt to reconstruct large scenes using tra-ditional methods such as Lidar point clouds [14], meshes [31], or signed distance functions [20]. However, there is an increased interest in using neural volumetric representations for their high-fidelity reconstructions. [30], [25], [40] recog-nize NeRF's capacity limitations and propose forms of spatial decomposition and train many NeRF's to represent different parts of the large scene. Mega-NeRF[30] and BirdNeRF[40] focus on bird view reconstruction, while Block-NeRF[25] focuses on street view. BungeeNeRF [35] takes a different approach focusing on satellite view reconstruction, recog-nizes the need for multi-scale reconstruction, and progres-sively trains from big to small scales while increasing network capacity. Urban Radiance Fields [21] presents a multi-modal approach of combining lidar information with RGB signals to address exposure differences in outdoor scenes. VastGaussian [15] introduces spatial decomposition approaches to 3D Gaussian splatting for large-scale bird view scene reconstruction.\nHowever, the aforementioned works develop optimization-based models that need extensive training and are unsuitable for online reconstruction during navigation or data acquisi-tion. We explore the capabilities of feed-forward approaches"}, {"title": "III. APPROACH", "content": "to reconstruct large-scale urban scenes, allowing on the fly reconstruction times.\nAugmentation for scene understanding: Data aug-mentation is a proven technique for improving ML model generalizability. Numerous augmentation methods have been developed in the 2D vision space. We take inspiration from CutOut [9] and CutMix [39] that cut 2D images out and mix cuts respectively. These methods however cannot be directly applied on input images for 3D NVS as they com-promise cross-view consistency. Recently, 3D augmentation techniques have been developed. Notably, Mix3D [18] mixes elements/meshes from different synthetic indoor scenes to compose new scenes that are not necessarily semantically reasonable to improve generalizability following the effective techniques of domain randomization[27], [28]. Their work however is done in a limited indoor setting for 3D semantic segmentation. There exists very few works [3], [7] that tackle augmentation for feed-forward NVS, they only augment in 2D image space, severely limiting the variations introduced.\nA. Data curation for Generalizable NVS\nLarge-scale urban scene data are not readily amenable for generalizable NVS as the data covers a huge baseline. For example, urbanscene3d [16] real datasets can cover more than 1km\u00b2 areas spanning multiple high rise and low rise buildings. Hence, an image in the scan may not necessarily contribute meaningfully to the reconstruction of another view; clustering images meaningfully is critical. We test different algorithms as shown in Fig. 2 to achieve that targeting the following criteria: First, it is pivotal to cluster images in the scan that are related to each other (i.e. looking more or less at the same structures in the scene). Second, the selection of the group size is crucial as too small of a group size will give very little information to the model whilst a very big group size would give confusing and unrelated information to the model. Third, the group size should be constant to allow efficient batching when training. We show a qualitative output of clustering images in Fig. 4.\n1) Capture Sequence grouping: Using the capture se-quence-defined as the order in which images are captured along the camera's trajectory over time\u2014to cluster images is a straightforward but naive approach. Abrupt changes in the camera's trajectory can result in images within the same cluster capturing entirely different parts of the scene, as illustrated in Fig.2.a. Additionally, this method overlooks valuable images from later in the sequence that capture the same scene region but are excluded due to their temporal position.\n2) Grid-Based Grouping: In this approach, a grid is overlaid on the ground plane, and cameras are clustered based on proximity to the centers of grid cells, with each cluster containing the K nearest neighbors to a grid cell center. While straightforward, this method has limitations: cameras that are close in Euclidean space may have vastly different viewing frustums, leading to poor clustering results, as shown in Fig.2.b. To address this, we added an angular constraint to ensure that cameras within a cluster are not only spatially close but also oriented in roughly the same direction. Despite this refinement, the approach still struggles to group images that capture the same scene area from different angles.\n3) Ray intersection with ground plane: To capture both the Euclidean distance and the viewing distance, we pro-jected the center pixel of each image into the world frame so that it intersects with the ground plane. We then use the distances between the intersection points as our clustering metric. A drawback of this approach is that you need to estimate the distance from the ground plane to each camera. To achieve this, we use Metashape to run SfM on small hand-picked images and calculate the height of the cameras relative to the ground plane. We then use this height to calculate all other camera distances to the ground plane, assuming the ground is flat. This approach improved clustering perfor-mance but still failed in many cases near high-rise buildings, as cameras could be looking at different areas even though their rays intersect close to each other at the ground plane level.\n4) SfM shared points: To ensure that images within a cluster view the same structures, we perform a full Structure-from-Motion (SfM) process for each scene and use the number of shared points among different camera views as the metric for clustering. This approach consistently produced the best results, as illustrated in Fig. 4, while effectively avoiding edge cases seen in previous methods. The core idea is that cameras observing the same scene exhibit high correspondence, which we capture by computing a similarity matrix for all images in the scene using SfM. Based on this matrix, we uniformly select cluster centers across the scene and determine the top K views for each cluster according to their similarity scores. Like all clustering methods, this approach requires careful tuning of cluster size to achieve optimal performance.\nB. Augmentation\nRecognizing the challenges of training feed-forward NVS models directly on the real data with unconstrained capture trajectories, we further propose to augment such scenes with constrained sampling methods. First we reconstruct the scene using traditional structure from motion and multi-view stereo approaches, then sampling novel views in an object-centric manner to augment the training of the feed-forward model. We discuss various approaches to sampling in what follows below.\nA Background on scene sentric dome sampling: A common approach for sampling images from a reconstructed scene uses an Archimedean spiral or dome above the mesh, as shown in Fig.3, commonly applied in models like Pixel-NeRF [38]. While effective for standard setups, it struggles with large scenes, often resulting in flat, disproportionate reconstructions and reliance on simple homography transfor-mations. To address this, we propose two improved camera sampling strategies for larger scenes."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "1)Multiscale Grid Sampling: A straightforward approach involves dividing the scene into cells at varying grid scales, as shown in Fig.3a. Using multiple scales helps prevent the model from overfitting to a single scale. Virtual domes are then placed over each cell, and cameras are uniformly sampled within a limited azimuth and elevation range. To avoid manually fine-tuning grid scales for each scene, we dynamically adjust them based on the scene's height-to-width/length ratios. This ensures finer grids for large scenes and coarser grids for smaller ones, as illustrated in Fig.3a.\n2) Semantic Building Sampling: This approach focuses on underrepresented areas, such as urban regions, which are often overshadowed by forest-dominated samples. Unlike the multiscale grid method that uniformly samples the scene, this method uses semantic camera sampling to identify urban areas as regions of interest and concentrates camera samples around them. As illustrated in Fig.3b, this strategy reduces forest overrepresentation and improves dataset diversity by prioritizing urban scenes.\nPlane fitting: We simplify perform building detection us-ing a geometric approach: fitting a plane to the Kth percentile of points (sorted by Z height) in the scene point cloud via least-squares. This plane slices the point cloud, rendering a top-down orthographic view, which is converted into binary masks and then bounding boxes. These bounding boxes initialize dome placements for targeted camera sampling.\nTo enable multiscale novel view synthesis (NVS), we extend this by combining bounding boxes. For each detected box, we merge it with 1 to M nearest boxes, creating clusters that represent individual buildings and multi-building regions, ensuring comprehensive and scalable scene coverage as showing in Fig.3b.\nDataset: For our experimental analysis, we focus ex-clusively on the Campus scene from the UrbanScene3D [16] dataset. This scene spans an area of 1.3 \u00d7 106m\u00b2 and includes 178 objects, providing diverse urban structures for evaluation.\nMetric: In evaluating our model, we will apply a com-bination of quantitative metrics and qualitative assessments. Quantitatively, we will utilize the Peak Signal-to-Noise Ratio (PSNR) to measure the fidelity of the reconstructions against the corrupting noise. Our approach will include visual in-spections to assess the realistic rendering of the scenes.\nComparison: Our analysis involves comparing the perfor-mance of PixelNeRF on the real dataset with its performance on an augmented dataset that combines real and synthetic data. To achieve this, we first evaluate PixelNeRF's perfor-mance on the real dataset alone, ensuring that the data is"}, {"title": "V. RESULTS", "content": "Evaluating Data Curation Methods: Experiments on the Campus scene from UrbanScene3D [21] demonstrate that SfM shared grouping out of the methods mentioned in Sec-tion III-A achieves the best performance for Generalizable Novel View Synthesis using PixelNeRF [38]. Using input images set to 3, a cluster size of 20, and Peak Signal-to-Noise Ratio (PSNR, higher the better) as the evaluation metric, SfM shared grouping attains the highest PSNR of 20.03 and an average PSNR of 14.6, outperforming Camera sequence grouping and Grid-based grouping, with PSNR values of 9.7 and 12.2, respectively as shown in Table I. Qualitative results in Fig. 4 confirm that SfM shared grouping provides better visual correspondence and hence leads to stable train-ing performance. Reducing the cluster size from 20 to 10 further improves PSNR to 22.94, additionally highlighting the importance of high overlap within input clusters for reconstruction fidelity, also shown with qualitative results in Fig. 5.\nBaseline Performance: Table II details the results for the real and augmented datasets. For the real dataset, using a cluster size of 20 images, we observe a slight decline in PSNR as the number of input views increases. Specifically, the PSNR decreases from 20.03 for 3 input views to 19.59 for 9 input views. This trend suggests that while additional views provide more information, they may also introduce noise or redundancy that hinders GNVS performance.\nAug3D + Real vs Real dataset: The synthetic dataset, reconstructed using Grid Sampling and Semantic Plane Fit-ting, achieves PSNR values of 29.12 and 28.79, respectively, with 3 input images, a cluster size of 20. Augmenting the real dataset with these under the same parameters yields the best PSNR of 21.80 for the Semantic approach, slightly surpassing Grid Sampling at 21.67. These results validate the effectiveness of the Aug3D dataset in enhancing GNVS performance."}, {"title": "VI. DISCUSSION", "content": "curated effectively. We identify the best-performing approach using the proposed four preprocessing methods described in Section III-A. Once this baseline is established, we integrate augmentations generated through Aug3D, employing the two augmentation strategies detailed in Section III-B, and compare the results to assess the impact of augmentation.\nCompute Setup: PixelNeRF [38] is run with 256 hidden layers and fixed encoder weights, adhering to its default configuration to meet low computational requirements. We utilize two 32GB Tesla V100 GPUs to evaluate the real Campus dataset. Grid-based augmentation, combined with the real dataset, is processed on a single 24GB NVIDIA RTX 3090 Ti. All other experiments are conducted using a 10GB NVIDIA RTX 3080, ensuring consistency across setups where applicable.\nThis work demonstrates the potential of feed-forward Generalizable Novel View Synthesis (GNVS) models like PixelNeRF for large-scale outdoor scenes, exemplified by the UrbanScene3D dataset. To address the need for a dataset curation pipeline, we proposed four clustering strategies, identifying SfM shared grouping as the most effective. Re-ducing the cluster size further improved performance, high-lighting the critical role of high-view overlap. Additionally, our Aug3D augmentation method, which generates synthetic views through Grid Sampling and Semantic Plane Fitting, boosted GNVS performance when integrated with real data. Despite these advances, challenges remain, including mitigat-ing noise from additional input views and ensuring scalability to diverse datasets and models, pointing to future directions in adaptive clustering and semantic-driven 3D augmentation."}, {"title": "APPENDIX", "content": "A. OTHER SEMANTIC SAMPLING METHOD\nIn addition to the geometric plane fitting approach, we experimented with a second semantic sampling method using the Segment Anything Model (SAM) to detect buildings from a top-down view. While SAM showed promise, it was found to be highly sensitive to shadows, resulting in in-consistencies in detecting building structures. Comparatively, the geometric plane fitting method yielded more reliable and accurate results, further emphasizing its suitability for generating semantically meaningful views in diverse lighting conditions.\nB. ADDITIONAL QUALITATIVE RESULTS\nTo further illustrate the effectiveness of the proposed Aug3D augmentation strategies, we provide qualitative com-parisons of the reconstructed scenes using Grid Sampling and Semantic Plane Fitting.\nFigure 6 showcases the qualitative results on the Urban-Scene3D Campus scene with 3 input images, comparing models trained exclusively on synthetic datasets generated via grid sampling versus semantic sampling methods. No-tably, the semantic sampling approach demonstrates im-"}, {"title": "Semantic Sampling", "content": "proved reconstruction fidelity, with sharper edges and more accurate structural details, particularly in regions with com-plex geometries.\nIn Figure 7, we extend this analysis to the Residence scene, evaluating the same augmentation techniques. Similar trends are observed, with semantic sampling outperforming grid sampling in preserving finer scene details and mitigating artifacts. The results underline the potential of semantic-driven augmentation to enhance the diversity and quality of synthetic datasets, thereby benefiting GNVS training.\nThese qualitative evaluations, along with our experiments, reinforce the quantitative findings presented in Section V, validating the advantages of integrating semantic-driven syn-thetic views into the GNVS pipeline. Future work can explore enhancing SAM's robustness to lighting variations or combining its capabilities with geometric methods for more versatile augmentation strategies."}]}