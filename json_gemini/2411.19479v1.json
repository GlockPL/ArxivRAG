{"title": "FLARE: Towards Universal Dataset Purification against Backdoor Attacks", "authors": ["Linshan Hou", "Wei Luo", "Zhongyun Hua", "Songhua Chen", "Leo Yu Zhang", "Yiming Li"], "abstract": "Deep neural networks (DNNs) are susceptible to backdoor attacks, where adversaries poison datasets with adversary-specified triggers to implant hidden backdoors, enabling malicious manipulation of model predictions. Dataset purification serves as a proactive defense by removing malicious training samples to prevent backdoor injection at its source. We first reveal that the current advanced purification methods rely on a latent assumption that the backdoor connections between triggers and target labels in backdoor attacks are simpler to learn than the benign features. We demonstrate that this assumption, however, does not always hold, especially in all-to-all (A2A) and untargeted (UT) attacks. As a result, purification methods that analyze the separation between the poisoned and benign samples in the input-output space or the final hidden layer space are less effective. We observe that this separability is not confined to a single layer but varies across different hidden layers. Motivated by this understanding, we propose FLARE, a universal purification method to counter various backdoor attacks. FLARE aggregates abnormal activations from all hidden layers to construct representations for clustering. To enhance separation, FLARE develops an adaptive subspace selection algorithm to isolate the optimal space for dividing an entire dataset into two clusters. FLARE assesses the stability of each cluster and identifies the cluster with higher stability as poisoned. Extensive evaluations on benchmark datasets demonstrate the effectiveness of FLARE against 22 representative backdoor attacks, including all-to-one (A2O), all-to-all (A2A), and untargeted (UT) attacks, and its robustness to adaptive attacks.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks (DNNs) are widely deployed in mission-critical applications, including autonomous driving [9], [17], and face recognition [25], [26]. Currently, due to the complicated structure and large parameter scale of modern DNNs, training these models usually relies on large-scale datasets, typically from external sources (e.g., data markets and crowd-sourcing platforms).\nHowever, recent studies reveal that using such external datasets introduces security risks, especially (poison-only) backdoor attacks [4], [8], [10], [22], [46]. Specifically, the backdoor adversaries poison a small subset of training data by embedding a predefined trigger (e.g., a subtle white square) and re-assigning their labels as the adversary-specified target label(s). As a result, all DNNs trained on these poisoned samples will learn a hidden backdoor, i.e., the latent and malicious connection between the triggers and the target labels. During the inference process of the backdoored model, the adversaries can activate its backdoor by implanting trigger patterns to maliciously change the predictions of any testing samples to the target labels. This attack is highly stealthy since the attacked model behaves normally on benign testing samples so that users cannot easily detect it simply based on the results of their local validation samples.\nCurrently, researchers have investigated five representative defensive strategies against backdoor attacks. These strategies include: (1) dataset purification [14], [38], [51], (2) poison suppression [7], [16], [42], (3) model-level backdoor detection [44], [45], [48], (4) input-level backdoor detection [11], [14], [36], and (5) backdoor mitigation [27], [49], [52]. Among these strategies, poison suppression modifies the training process to reduce the impact of poisoned samples; model-level backdoor detection and mitigation identify or remove embedded backdoors in post-deployment; and input-level detection prevents backdoor activation by identifying malicious inputs during inference. In contrast, dataset purification acts as a proactive defense, identifying and removing poisoned samples from the dataset before training begins. This paper focuses on dataset purification, aiming to prevent backdoor creation at its source and precisely trace malicious samples to their origins.\nIn this paper, we first revisit existing advanced dataset purification methods. We reveal that their effectiveness relies on an implicit assumption: the connections between triggers and target labels in backdoor attacks are inherently simpler to learn than the benign features. This assumption enables these methods to achieve promising results by focusing primarily on input-output relationships. Specifically, (1) one line of work [19], [53] suggests that DNNs converge significantly faster on poisoned samples, indicating that the backdoor connections are easier to learn; (2) studies such as [5], [15] observe that poisoned samples often exhibit highly localized and small saliency regions. This suggests that backdoor triggers function as shortcut features, allowing the model to bypass learning complex, distributed features in benign data and thus simplifying the backdoor establishment; (3) research from [11], [14] leverages the scaled prediction consistency of poisoned samples, implying that DNNs overfit on distinctive, artificially crafted triggers, thereby simplifying backdoor formation. This assumption generally holds for typical all-to-one (A2O) backdoor attacks, as backdoor triggers are often simple and easy to learn. However, this assumption does not always hold for modern complicated backdoor attacks, especially for all-to-all (A2A) and untargeted (UT) backdoor attacks (as shown in Figure 1). As such, an intriguing question arises:\nShall we design a universal dataset purification method that is effective against various types of backdoor attacks?\nThe answer is in the positive! Inspired by the finding that DNN memorization is distributed across neurons in multiple hidden layers [31], we explore the distinctions between poisoned and benign samples throughout the model instead of simply the input-output relationships. Our analysis of A2A and UT backdoor attacks reveals a critical observation: poisoned and benign samples do not consistently separate within specific layers; instead, distinctions emerge across different hidden layers, varying with the attack types. Building on this insight, we propose a universal method for dataset purification, termed Full-spectrum Learning Analysis for Removing Embedded poisoned samples (FLARE). FLARE comprises two main stages: latent representation extraction and poisoned sample detection. In the first stage, FLARE constructs a comprehensive latent representation for each training sample by consolidating the abnormal values from all hidden layers' feature maps. Specifically, FLARE first aligns all feature maps to a uniform scale (e.g., [0,1]) by leveraging the statistics of Batch Normalization (BN) layers. FLARE then extracts an abnormally large or small value from each feature map and consolidates these values across all hidden layers to construct the latent representation. In the second stage, FLARE detects poisoned samples through cluster analysis. In general, FLARE splits the entire dataset into two distinct clusters and identifies the cluster with higher cluster stability as poisoned. Specifically, FLARE first applies dimensionality reduction to reduce the computation consumption and improve clustering efficiency. FLARE then selects a stable subspace by adaptively excluding category-specific features from the last few hidden layers, isolating an optimal subspace where benign samples from various classes are still close together. FLARE evaluates cluster stability as the 'density' difference between the density level at which the cluster first appears and that where the cluster divides into smaller sub-clusters. FLARE finally identifies the cluster exhibiting higher stability as poisoned since poisoned samples tend to form compact clusters due to the sharing of the same trigger-related features.\nIn conclusion, our main contributions are three-fold. (1) We reveal that the underlying assumption of existing advanced dataset purification methods, i.e., the backdoor connections are easier to learn than benign ones, does not always hold, particularly under all-to-all and untargeted attacks. We also demonstrate that poisoned and benign samples do not consistently separate within particular layers across various types of attacks. (2) Based on our intriguing findings, we develop a universal dataset purification method (dubbed FLARE). It separates poisoned and benign samples throughout the model instead of simply the input-output relationship. (3) We conduct extensive experiments on benchmark datasets, verifying the effectiveness of our FLARE against 22 representative backdoor attacks (including A2O, A2A, and UT ones) and its resistance to potential adaptive attacks."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Poison-only Backdoor Attacks", "content": "Targeted Attacks. Gu et al. [10] proposed the first poison-only backdoor attack, BadNets, where an adversary embeds an adversary-specified trigger into a few training samples and modifies their labels. This attack supports two primary modes: (1) all-to-one (A2O), where all poisoned samples are assigned a single target label, and (2) all-to-all (A2A), where poisoned samples from a class i are relabeled as a different class label, typically the next consecutive class (i.e., i + 1). Models trained on the poisoned dataset establish a backdoor connection between the trigger and target labels. Subsequent research aimed to make backdoor attacks more stealthy by developing invisible triggers [35] and employing label-consistent poisoning [43], where only samples from the target class are poisoned, thus avoiding label modification and bypassing manual inspection. Follow-up research further introduced more sophisticated trigger designs, including sample-specific triggers [24], sparse triggers [8], horizontal triggers [29], and asymmetric triggers [37], all specifically crafted to evade defenses.\nUntargeted Attacks. Targeted attacks assign poisoned samples to adversary-specific labels, ensuring that the backdoored model consistently misclassifies the poisoned samples into particular classes. Recently, some pioneering work has discussed untargeted attacks [21], [50], where the goal is to make the predictions deviate from the true labels instead of approaching particular ones (i.e., target labels). For example, poisoned samples are reassigned with random incorrect labels [21], causing the backdoored model to misclassify the poisoned samples into incorrect classes. As a result, the predictions for all poisoned samples approximate a uniform distribution, making the attack more difficult to learn and detect."}, {"title": "B. Backdoor Defenses", "content": "Based on the stage at which they occur, existing defenses can be divided into five main categories: (1) dataset purification [14], [38], [51], which focuses on detecting and removing poisoned samples from a given suspicious dataset before model training, (2) poison suppression [7], [16], [42], which modifies the training process to limit the impact of poisoned samples, (3) model-level backdoor detection [44], [45], [48], which assesses whether a suspicious model contains hidden backdoors; (4) input-level backdoor detection [11], [14], [36], which identifies malicious inputs at inference; (5) backdoor mitigation [27], [49], [52], which directly removes backdoors after model development. This paper primarily focuses on dataset purification since it reduces backdoor threats from the source and can be easily used to mitigate model backdoors. We hereby provide an overview of the defences related to dataset purification and backdoor mitigation.\nDataset Purification. Existing strategies can be divided into four types: (1) purification via latent separability, (2) purification via early convergence, (3) purification via dominant trigger effects, (4) purification via perturbation consistency.\nSpecifically, latent separability-based defenses exploited the detectable traces left by poisoned samples in the feature space. For example, Chen et al. [3] observed that, in the feature space of the final hidden layer, samples from the target class form two distinct clusters, with the smaller cluster identified as poisoned. Ma et al. [30] utilized high-order statistics (i.\u0435., Gram matrix) to analyze the differences between poisoned and benign samples; Early convergence-based defenses relied on the observation that DNNs converge on poisoned samples more rapidly than on benign ones. During the early stages of training, the losses for poisoned samples quickly drop to near zero, while those for benign samples remain relatively high. For example, researchers in [19], [53] traped samples whose losses decreased more rapidly by using the local gradient ascent technique during the initial five training epochs. To mitigate class imbalance issues during this process, Gao et al. [7] refined the approach by selecting the lowest-loss samples within each class, rather than across the entire dataset; Dominant trigger-based defenses assume that backdoor triggers play a dominant role in DNN predictions. A backdoored model tends to learn an excessively strong signal for the backdoor trigger, such that even small, localized triggers can overpower other semantic features and dictate the model's prediction. For example, Chou et al. [5] utilized model interpretability techniques (e.g., Grad-CAM [40]) to visualize salient regions of an input image, identifying highly localized and small regions as potential trigger areas. Similarly, Huang et al. [15] distilled minimal patterns from input images that influenced the model's predictions and identified images with abnormally small patterns as poisoned; Perturbation consistency-based defenses assumed that poisoned samples are resistant to perturbations. For instance, Guo et al. [11] observed that poisoned samples exhibited prediction consistency under pixel-level amplification and proposed analyzing this consistency to distinguish poisoned samples. To address constraints in pixel values and insensitivity to amplification, Pal et al. [36] optimized a mask for selective pixel amplification; Qi et al. [38] analyzed the prediction consistency by unlearning the benign connections of the backdoored models; and Hou et al. [14] examined prediction consistency under unbounded weight-level amplification.\nHowever, in this paper, we find that all existing methods suffer from poor performance in some cases. Specifically, the first type of purification method generally only utilizes information at a specific layer (e.g., the last hidden layer) and can be easily bypassed by advanced attacks [37]. We will also show that the last three types of methods, however, relied on an underlying assumption that does not always hold, especially under A2A and UT attacks. How to design an effective dataset purification method is still an important open question.\nBackdoor Mitigation. This defense occurs in the post-development phase, aiming to remove implanted backdoors from attacked models. For instance, Li [27] pruned dormant neurons with benign inputs; Wu [47] pruned neurons that are sensitive to adversarial perturbations; Chai [2] applied weight-level pruning, which is more precise than neuron-level pruning, thereby preserving benign task performance; Li [20] adopted knowledge distillation to guide the fine-tuning of backdoored models. Instead of direct removal, researchers in [44], [49], [52] reverse-engineered suspected triggers and decouple them from target labels through unlearning and fine-tuning.\nExisting backdoor mitigation defenses have shown effectiveness against different attacks; however, Zhu et al. [54] demonstrated that the injected backdoors can persist and reactivate during inference, even after backdoor mitigation. This underscores the urgent need for a dataset purification method capable of defending against a broader range of attacks, including A2O, A2A, and UT attacks, to prevent backdoor creation at its source."}, {"title": "III. REVISITING EXISTING DATASET PURIFICATION METHODS", "content": ""}, {"title": "A. Preliminaries", "content": "The Main Pipeline of Poison-only Backdoor Attack. Let $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$ denotes a training set composed of $N$ i.i.d. samples. Each sample $(x, y)$ is characterized by $x \\in \\mathcal{X} = [0,1]^{d_c \\times d_w \\times d_H}$ and $y \\in \\mathcal{Y} = \\{1, 2, ..., K\\}$. An adversary can generate a poisoned dataset $\\mathcal{D}'$ by modifying a subset of benign samples (i.e., $\\mathcal{D}_s$), i.e., $\\mathcal{D}' = \\mathcal{D}_p \\cup \\mathcal{D}_b$, where $\\mathcal{D}_b = \\mathcal{D} - \\mathcal{D} \\cap \\mathcal{D}_s$, $\\mathcal{D}_p = \\{(x', \\hat{y}) | x' = G_x(x)), \\hat{y} = G_y(y), (x,y) \\in \\mathcal{D}_s\\}$, and $p = |\\mathcal{D}_s|/|\\mathcal{D}|$ is the poisoning rate. $G_x : \\mathcal{X} \\rightarrow \\mathcal{X}, G_y : \\mathcal{Y} \\rightarrow \\mathcal{Y}$ are adversary-specified poisoned image generator and target label generator, respectively. For instance, in the BadNets attack [10], $G_x(x) = (1 - m) \\odot x + m \\odot t$, where $m \\in \\{0,1\\}^{d_c \\times d_w \\times d_H}, t \\in \\mathcal{X}$ is the malicious trigger. As for the $G_y(y)$, there are two primary attack paradigms: (1) targeted attacks and (2) untargeted attacks. Specifically, for the targeted attacks, poisoned images are re-labeled to the designated target labels: $G_y(y) = Y_t, Y_t \\in \\mathcal{Y}$ in A2O attacks, and $G_y(y) = (y+1) \\ (mod \\ K)$ in A2A attacks. For the untargeted attacks, each poisoned image is re-labeled to a random label uniformly sampled from $\\mathcal{Y}$ (i.e., $G_y(y) \\sim \\mathcal{U}(1, K)$)."}, {"title": "B. Revisiting Strategy 1: Early Convergence", "content": "In general, the purification methods in [16], [19], [53] are designed based on the observation that DNNs converge faster on poisoned samples during the early stages of training, suggesting that the backdoor connections are easier to learn.\nSettings. To validate the effectiveness of this purification strategy across different attack types, we conduct two representative backdoor attacks, i.e., BadNets [10] and WaNet [35], on the CIFAR-10 dataset using ResNet-18. They are the representatives of sample-agnostic and sample-specific attacks. Each attack includes three variants: A2O, A2A, and UT. For each attack, we calculate the benign accuracy (BA) and the attack success rate (ASR) during the initial ten epochs.\nResults. As shown in Figure 2, the ASR for the BadNets (A2O) and WaNet (A2O) attacks rapidly approaches 100%. However, the ASRs for the other attacks remain lower than the BAs, with UT attacks even nearing 0%. These results suggest that in A2A and UT attacks, DNNs do not converge quickly on poisoned samples, indicating that the assumption that backdoor connections are simpler to learn than the benign ones does not hold for A2A and UT attacks."}, {"title": "C. Revisiting Strategy 2: Dominant Trigger Effects", "content": "Researchers in [5], [15] observed that poisoned samples often exhibit highly localized and small saliency regions, suggesting that trigger-related features can be regarded as 'short-cut' that are more easily learned by DNNs compared to benign features."}, {"title": "D. Revisiting Strategy 3: Perturbation Consistency", "content": "Researchers also observed that poisoned samples exhibit greater prediction consistency than those of benign samples under pixel-level amplification [11], [36] or weight-level alterations [14], [38], [51]. This implies that DNNs tend to overfit on triggers instead of learning semantic features.\nSettings. We hereby also use BadNets and WaNet on CIFAR-10 for our discussions. To evaluate the prediction consistency, we calculate the difference in prediction confidence between the original and perturbed predictions on the initially predicted label for both benign and poisoned samples. Other settings are the same as those used in Section III-C.\nResults. As shown in Figure 4, for BadNets (A2O), the differences are consistently close to zero under the pixel-level and weight-level perturbations. It indicates that the predictions of these poisoned samples are unaffected by such perturbations. In contrast, under A2A and UT attacks, the confidence differences of poisoned samples approach 1, indicating that the perturbations significantly change the predictions of poisoned samples. The sensitivity of poisoned samples to perturbations, similar to that of benign samples, indicates that DNNs struggle to overfit to poisoned samples in A2A and UT attacks. Thus, the assumption that backdoor connections are easier to learn than benign ones does not hold for A2A and UT attacks."}, {"title": "E. Revisiting Latent Separability on a Particular Layer", "content": "Previous studies [3], [30] explored the latent separability between benign and poisoned samples in the feature space. In particular, these works primarily focused on the final hidden layer as a representative intermediate layer. We argue that their success also partly relied on the assumption that backdoor can be easily learned. In this section, we verify it.\nSettings. We also conduct experiments using BadNets and WaNet on the CIFAR-10 dataset to facilitate our analysis. To assess latent separability, we employ t-SNE visualization to examine the latent representations of both benign and poisoned samples across various hidden layers, with particular focus on shallow, middle, and deep layers. All other experimental settings remain consistent with those outlined in Section III-C.\nResults. As shown in Figure 5, poisoned and benign samples do not consistently display separability in specific layers. For example, under the BadNets (A2A) and BadNets (UT) attacks, separability is evident in shallow and middle layers, such as Layer 6 or Layer 10, but diminishes in deeper layers for BadNets (A2A). In contrast, under WaNet (A2A), separability is not evident across most layers. These observations challenge the implicit assumption in existing latent-separability-based purification methods that backdoor triggers are sparsely embedded and primarily affect deeper feature representations, highlighting the necessity of evaluating the broader impact of poisoned samples across multiple layers throughout the model rather than limiting the analysis to a single layer."}, {"title": "IV. THE PROPOSED METHOD", "content": ""}, {"title": "A. Overview", "content": "Motivated by previous findings, we introduce FLARE, a dataset purification method that leverages hidden features across all hidden layers. As illustrated in Figure 6, FLARE consists of two main stages: (1) Latent Representation Extraction: For each sample, FLARE constructs a comprehensive latent representation by consolidating the abnormal values from all hidden layers' feature maps. Specifically, FLARE first aligns the values of all feature maps to a uniform scale using the statistics of Batch Normalization (BN) layer. Then, for"}, {"title": "B. Latent Representation Extraction", "content": "For each training sample, FLARE forms a comprehensive latent representation by leveraging all hidden features of DNNs. This task encounters two main challenges: (C1) hidden features capture diverse characteristics, leading to significant variability in their values; (C2) hidden features are often high-dimensional and noisy. To tackle these challenges, FLARE employs a two-step process: feature alignment and extracting abnormal features. In the alignment step, FLARE normalizes the feature maps to a uniform scale, based on the statistics of BN layers. Subsequently, FLARE extracts an abnormal value from each feature map as its representative and aggregates them across all hidden layers to form the final latent representation. Their technical details are as follows.\n1) Feature Alignment: In this work, we adopt batch normalization (BN) to align all feature maps to a uniform scale. It is mostly because BN transformation was initially proposed to mitigate internal covariate shifts, providing a principled way to stabilize feature distributions. Specifically, for a backdoored DNN model $F$ consisting of $L$ hidden layers, i.e.,\n$F = f_C \\circ f^{(L)} \\circ f^{(L-1)} \\circ ... f^{(l)} ... \\circ f^{(2)} \\circ f^{(1)},$ (1)\nwhere $f^{(l)}$ denotes the $l$-th hidden layer, consisting of a convolutional layer, a BN layer, and an activation function. Let $a$ represent the output of a convolutional layer, i.e., $a \\in \\mathbb{R}^{d_c \\times d_h \\times d_w}$, where $d_c$ is the number of feature maps, and $d_h$ and $d_w$ are the height and width of each feature map, respectively. Utilizing the mean $\\mu$ and variance $\\sigma^2$ of the following BN layer, we define a transformation $\\mathcal{P}(\\cdot; \\mu, \\sigma^2)$ to transform all values in $a$ to obtain the aligned output $\\tilde{a}$ via:\n$\\tilde{a} = \\mathcal{P}(a; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi(\\sigma^2)}}exp(-\\frac{(a-\\mu)^2}{2\\sigma^2}),$ (2)\nIn this way, all values in $\\tilde{a}$ are from a uniform scale [0, 1].\n2) Extracting Abnormal Features: As backdoor-related features usually function as dominant features, they tend to induce abnormally large or small activations in the feature map, as partly supported by findings in [1]. Building on this understanding, we propose to focus on identifying outliers within each feature map, specifically targeting the abnormally small or large values. In particular, all feature maps are normalized and their values adhere to a uniform distribution defined by the statistics of the BN layers after the previous alignment step. At this time, both abnormally large and small values fall outside the central distribution, corresponding to regions with low occurrence probability. This allows us to consistently extract the minimum values from each aligned feature map as indicators of backdoor-related features, without needing to separately select the largest or smallest values. Specifically,"}, {"title": "C. Poisoned Sample Detection", "content": "After obtaining the final representations of all training samples, FLARE applies dimensionality reduction to reduce the computation consumption and improve clustering efficiency. FLARE then conducts cluster analysis to determine which cluster may contain poisoned samples. Arguably, the most straightforward method is to perform clustering on the entire representation directly. However, benign samples from different classes tend to form multiple clusters due to category-specific features, leading to misidentification. To refine this partitioning, FLARE employs a stable subspace selection algorithm to adaptively exclude representations from the last few hidden layers. This approach allows FLARE to isolate an optimal subspace where benign samples are close together rather than dispersing into multiple small clusters.\n1) Cluster Splitting: Let $\\mathbb{R} \\in \\mathbb{R}^{N \\times d}$ represent the latent representations of all $N$ training samples, with each sample in a $d$-dimensional latent space. To enhance clustering efficiency and reduce computational demands, FLARE first performs a dimensionality reduction transformation to obtain:\n$\\hat{\\mathbb{R}} = T(\\mathbb{R}), \\hat{\\mathbb{R}} \\in \\mathbb{R}^{n \\times d'}, d' \\ll d.$ (6)\nHere, $T : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d'}$ denotes the dimensionality reduction function. In this paper, we choose the uniform manifold approximation and projection (UMAP) algorithm [33] as $T$ for its ability to reduce high-dimensional data while preserving the topological structure of the original data manifold. FLARE then performs clustering on the reduced representation $\\hat{\\mathbb{R}}$: $C = H(\\hat{\\mathbb{R}})$, where $C = \\{C_1, C_2, ..., C_k\\}$ represents the set of clusters identified by a clustering algorithm $H$. In this paper, we use the hierarchical density-based spatial clustering of applications with noise (HDBSCAN) algorithm [32] as $H$ for its ability to handle clusters of varying shapes and densities. HDBSCAN constructs a hierarchy of clusters represented by a condensed tree$\\,T$, where clusters are partitioned across varying density levels $\\lambda = 1/d_{core}$, where $d_{core}$ denotes the distance of an object to its 'minPts-nearest' neighbor.\n2) Stable Subspace Selection: Given that poisoned samples typically share the same trigger-related features, they tend to aggregate into a more stable cluster. In contrast, benign samples, which originate from a natural and diverse distribution, generally form clusters with lower stability. Therefore, cluster stability forms an effective indicator for identifying poisoned samples. Traditionally, in HDBSCAN, cluster stability is calculated by aggregating the density level $\\lambda$ of each sample within a cluster. However, in poisoned sample detection, where the number of poisoned samples is much smaller than that of benign samples, this approach introduces bias toward the majority class (benign samples). To alleviate this problem, we redefine cluster stability to focus on the persistence of a cluster across varying density levels $(\\lambda)$, as follows.\nDefinition IV.1 (Cluster Stability in Detecting Poisoned Samples). For a cluster $C$, let $x = \\min_{x \\in C} x$ be the density level where $C$ first appears, and $X = \\max_{x \\in C} x$ be the density level before $C$ divides into sub-clusters. The stability $\\kappa_C$ of cluster $C$ is defined as $\\kappa_C = X - x$.\nWhile benign samples generally form a single cluster, they often tend to form multiple stable clusters, as illustrated in Figure 6. These small but stable benign clusters can significantly interfere with detection performance. To address this issue, FLARE develops a subspace selection strategy to isolate a stable space in which poisoned and benign samples form two distinct clusters. This approach is based on the understanding that models tend to capture semantic information in shallow layers and focus on distinguishing features in deeper layers. Accordingly, FLARE excludes features from the last few hidden layers, obtaining $\\mathbb{R}'$ as the final representation that includes only the earlier layers: $\\mathbb{R}' = [r^{(1)}, r^{(2)}, ...,r^{(L-k)}]$.\nTo determine the optimal number of layers $k$ to exclude, FLARE designs an adaptive algorithm that dynamically selects a suitable $k$. The algorithm begins with a model configured with all $L$ hidden layers and progressively removes the last hidden layer of the modified model at each step. At each iteration, FLARE splits the condensed tree $T$ at the root node to form two primary clusters and assesses the stability $\\kappa_{C_{large}}$ of the larger cluster $C_{large}$. If the stability $\\kappa_{C_{large}}$ surpasses a predefined threshold $\\xi$, it suggests that the benign samples are cohesively grouped within the current subspace. To further ensure the cluster stability against potential anomalies, FALRE generates a condensed tree at each step and traverses from the root node of $C_{large}$ down to depth $d$. If the stability at any depth exceeds $\\xi$, the current subspace is considered stable.\n3) Stability-based Detection: Within this obtained subspace, FLARE splits the condensed tree $T$ of the entire poisoned dataset at the root node to form two primary clusters, $C_1$ and $C_2$. FLARE then evaluates the cluster stability $\\kappa$ of each cluster and identifies the cluster with the highest stability as poisoned. The set of poisoned samples $S_p$ is:\n$S_p = \\{x \\ | \\ x \\in C_p\\},$ (7)\nwhere\n$C_p = arg \\max_{\\{C_1, C_2\\}} (\\kappa_{C_1}, \\kappa_{C_2}).$ (8)"}, {"title": "D. Post-Detection Strategy 1: Secure Training from Scratch", "content": "After completing the above detection process, we can remove the detected poisoned samples, denoted as $\\mathcal{D}_p$, from the training dataset. The remaining samples form a purified dataset, $\\mathcal{D}_b = \\mathcal{D} - \\mathcal{D}_p$, which is assumed to contain only benign samples. Defenders can then train a backdoor-free model $M(\\cdot, \\theta')$ on $\\mathcal{D}_b$ using standard training procedures, i.e.,\n$\\min_{\\theta'} \\sum_{(x,y) \\in \\mathcal{D}_b} L(M(x; \\theta'), y),$ (9)\nwhere $L(\\cdot)$ is the cross-entropy loss function."}, {"title": "E. Post-Detection Strategy 2: Backdoor Removal", "content": "We can utilize the detected poisoned samples to directly remove backdoors from the model. This process is achieved through a two-step method of unlearning and relearning, ensuring the mitigation of backdoors while preserving the model's performance on benign tasks.\nStep 1: Unlearning. This step aims to eliminate the effect of the trigger by unlearning the identified poisoned samples. Specifically, we aim to maximize the cross-entropy loss for these poisoned samples with respect to their malicious labels. To achieve this, we minimize the negative loss of the backdoored model $F(\\cdot;\\theta)$ over the detected poisoned samples $\\mathcal{D}_p$:\n$\\min_\\theta \\frac{1}{|\\mathcal{D}_p|} \\sum_{(x,y) \\in \\mathcal{D}_p} -L(F(x; \\theta), y).$ (10)\nStep 2: Relearning. The unlearning step can effectively mitigate backdoor effects, but it also may result in degraded performance on benign samples. To alleviate this, we perform a relearning step that utilizes the remaining benign samples $\\mathcal{D}_b = \\mathcal{D} - \\mathcal{D}_p$. This step fine-tunes the model as follows:\n$\\min_\\theta \\frac{1}{|\\mathcal{D}_b|} \\sum_{(x,y) \\in \\mathcal{D}_b} L(F(x; \\theta), y),$ (11)\nBoth unlearning and relearning steps are performed in each epoch to optimize the model's performance on benign data without reintroducing susceptibility to backdoor triggers."}, {"title": "V. EXPERIMENTS", "content": ""}, {"title": "A. Main Settings", "content": "Datasets and Models. We conduct experiments on two classical image classification benchmark datasets", "18": "and Tiny-ImageNet [6", "13": "as the default architecture. For ablation studies", "41": "and MobileNetV2 [39", "follows": 1, "attacks": "Bad-Net [10", "4": "Trojan [28"}, {"attacks": "IAD [34", "35": "ISSBA [24", "attack": "LC [43"}, {"attack": "SIBA [8", "formats": "BadNets (A2O), BadNets (A2A), and BadNets (UT). These attacks are implemented using the open-source BackdoorBox toolkit [23", "3": "SCALE-UP [11", "36": "IBD-PSC [14", "38": ".", "FLARE (P)": "with these five methods. For backdoor mitigation, we compare our backdoor removal defense via FLARE, dubbed \"FLARE (R)\", with six SOTA methods, including FP [27", "20": "AMW [2", "19": "SEAM [55", "49": "."}]}