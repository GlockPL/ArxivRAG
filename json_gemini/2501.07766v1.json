{"title": "Large Language Models for Knowledge Graph\nEmbedding Techniques, Methods, and Challenges:\nA Survey", "authors": ["Bingchen Liu", "Xin Li"], "abstract": "Large Language Models (LLMs) have attracted a lot\nof attention in various fields due to their superior performance,\naiming to train hundreds of millions or more parameters on\nlarge amounts of text data to understand and generate natural\nlanguage. As the superior performance of LLMs becomes ap-\nparent, they are increasingly being applied to knowledge graph\nembedding (KGE) related tasks to improve the processing results.\nAs a deep learning model in the field of Natural Language\nProcessing (NLP), it learns a large amount of textual data to\npredict the next word or generate content related to a given text.\nHowever, LLMs have recently been invoked to varying degrees\nin different types of KGE related scenarios such as multi-modal\nKGE and open KGE according to their task characteristics. In\nthis paper, we investigate a wide range of approaches for per-\nforming LLMs-related tasks in different types of KGE scenarios.\nTo better compare the various approaches, we summarize each\nKGE scenario in a classification. In addition to the categorization\nmethods, we provide a tabular overview of the methods and\ntheir source code links for a more direct comparison. In the\narticle we also discuss the applications in which the methods are\nmainly used and suggest several forward-looking directions for\nthe development of this new research area.", "sections": [{"title": "I. INTRODUCTION", "content": "With the development of society and the advancement of\nscience and technology, knowledge graphs (KGs) have been\nmore and more widely used in applications containing rich\nsemantic information in different fields. At present, KGs have\nbeen widely used in various fields such as social networks [1]\n[2], bioinformatics networks [3] [4], traffic flow prediction [5]\n[6] and have been proved to have superior performance. Tra-\nditional approaches to the above tasks often require learning\nbased on knowledge graph embedding (KGE) representations\nfirst, and then performing related tasks based on the accurate\nKGE representations. However, deep learning models in the\ntraditional approach often suffer from performance limitations\ndue to their training scale.\nLarge Language Models (LLMs) are increasingly being\napplied to KGE related tasks to enhance their processing\nbecause of their superior performance. LLMs can be based on\ndatasets several times larger than those of traditional models,\nand combined with the assistance of fine-tuning, calling cue\nwords, etc., they can obtain even better performance compared\nto traditional predictive models for KGE tasks [7] [8] [10] [9]\n[11]. The aim of this paper is to provide a comprehensive\noverview and structural categorization of the various task\napproaches that have been applied to LLMs in different KGE\nscenarios.\nThis survey aims to provide a comprehensive overview\nof work on various types of KGE scenarios in the context\nof LLMs, which cover various aspects of KGE application\nscenarios. This survey provides an overview of the work\nof various types of KGE scenarios from the perspective of\ndifferent ways of calling LLMs, covering all aspects of KGE\nuse cases. The previous review [117] tried to classify LLMs\nand KGE from the perspective of enhancement methods,\nor some reviews [118]classified the different stages of KG\nenhancement and pre-trained language model enhancement\nbased on LLMs. With the continuous progress of research,\nvarious emerging application scenarios of KGE are emerging.\nHowever, there is still a lack of LLMs classification overview\nof KGE representation learning in various application sce-\nnarios. There have also been a number of surveys about\nKGE or LLM before. Some articles have been investigated\nonly for KGE aspects. For example, the review written by\nLiu et al. [12] discusses in detail the current application\nstatus, technical methods, practical applications, challenges,\nand future prospects of KGEs in industrial control system\nsecurity situational awareness and decision-making. A review\nby Zhu et al. [13] summarized the KG entity alignment\nmethod based on representation learning, proposed a new\nframework, and compared the performance of different models\non diverse datasets in detail, looking forward to future research\npaths. Some other articles have been investigated only for"}, {"title": "II. PRELIMINARIES", "content": "To facilitate the reader's understanding, we introduce rel-\nevant basic concepts in this chapter. We summarize all\nacronyms used throughout the manuscript in Table VII.\nLLMs can be defined as a type of artificial intelligence\ntechnology based on deep learning, trained on massive text\ncorpora to generate and understand natural language. Its' basic\nprocess involves extracting language patterns from large-scale\ndata, modeling them with neural networks, and generating\nhigh-quality language outputs based on user inputs. Its' ap-\nplications span various fields, including text generation [16],\nmachine translation [17], question answering [18], sentiment\nanalysis [19], and content summarization [20]. Compared to\ntraditional methods, LLMs offer superior contextual under-\nstanding, higher output quality, and enhanced transfer learning\ncapabilities. LLMs can be categorized into the following main\ntypes:\n1) Text Generation Models [21]: Such as the GPT se-\nries, these models specialize in generating coherent natural\nlanguage text and are commonly used in chatbots, article\ncontinuation, and content creation.\n2) Pre-trained Language Models [22]: Such as BERT,\nthese models utilize bidirectional encoders to understand text\ncontext and are widely applied in sentiment analysis, question\nanswering tasks, and text classification.\nHIN can be defined as a network structure that contains\nmultiple types of nodes and edges, which is capable of\nmodeling rich data types and complex relations in the real\nworld. Compared with traditional homogeneous networks,\nHINs are able to capture the semantic information and struc-\ntural features of data in a more comprehensive way [26],\nwhich provides new perspectives and methods for data mining\nand knowledge discovery. The relations between these nodes\nand edges are not just simple connections, but contain rich\nsemantics, such as purchase, collection, favorite, and so on.\nSemantic mining can be performed in networks based on\ndifferent methods to discover deeper patterns and knowledge,\nwhich are mainly categorized into four types: edge sampling-\nbased representation strategies [27], path-based representation\nstrategies [28], subgraph-based representation strategies [29],\nand deep learning-based representation strategies [30].\nResearch on HINs still faces many challenges and oppor-\ntunities. Future research directions include how to construct\nand analyze more complex HINs more effectively [31], how\nto better integrate HINs with multimodal data and KGs [32],\nas well as how to develop more powerful tools and algorithms\nfor analyzing HINs. In addition, the practical applications of\nHINs in the fields of business [33], cybersecurity [34], and\nmedicine [35]are also important directions for future research."}, {"title": "III. KGE-RELATED TASKS", "content": "With the development of society and the progress of science\nand technology, graph data has been more and more widely\nused in various fields, such as social networks [36] [37],\nacademic networks [38], biological networks [39], etc. Graph\ndata in various domains often consists of multiple types of\nedges and nodes, which contains rich semantic information,\nand puts high demands on accurate graph data information\nrepresentation in various types of downstream applications\nin different domains. Knowledge Graph is a graph-based\nstructured semantic knowledge base that stores graph data of\nvarious domains in the form of (entity, relation, entity) ternary\nstructure. KG can be regarded as a special kind of HIN in\nwhich entities and relations have clear semantic definitions.\nMeanwhile, the research methods and techniques of HIN can\nalso be applied in the construction and analysis of KG.\nLink prediction is a core task in KG completion, aimed at\npredicting missing entity relation pairs (i.e., missing edges) in\na KG [48]. Specifically, given a KG G=(V, E), where V is the\nset of entities and E is the set of relations. The goal is to predict\npotential new entity relation pairs based on existing entities\nand relations. In the link prediction task, the formulas involved\ncan be divided into three categories: the scoring function, loss\nfunction, and evaluation metrics of the model.\n1) Scoring function: The scoring function is used to mea-\nsure the effectiveness of a triplet (head entity, relation, tail\nentity).\nTransE [49]:\n$\\text{score (h,r, t) = } \\text{Ih+r-t\u2161}$\nConsider the relation r as an embedding representing the\ntranslation between the head entity h and the tail entity t.\nFunction: If h + r \u2248 t, then the score of the triplet (h, r,\nt) is low, indicating that the triplet is likely correct. On the\ncontrary, a higher score indicates that the triplet is unlikely to\nbe correct.\nRotatE [50]:\n$\\text{score (h,r, t) = } \\text{Ihor t\u2161}$\nConsider the relation r as a rotation operation in the complex\nplane, and the head entity h should approach the tail entity\nt after rotating through the relation r. It can handle many\ntypes of relations (e.g., symmetric, antisymmetric, inverse, and\nso on) and is suitable for KGs that handle complex relation\npatterns.\nDistMult [116]:\n$\\text{score (h,r, t) = } \\text{hRrt}$\nAssuming that the relation matrix R is a diagonal matrix\nsimplifies the calculation of the model. Capturing the inter-\naction between entities and relations through matrix multipli-\ncation is suitable for handling symmetric relations (such as\n\"friend\" relations).\nConsider the relation r as a rotation operation in the complex\nplane, and the head entity h should approach the tail entity\nt after rotating through the relation r. It is also suitable for\nhandling complex relation patterns.\n$\\text{L=}\\text{ }\\text{ }\\sum_{\\text{(h,r,t) }\\text{EG}}\\text{ }\\sum_{\\text{(h',r',t')EG-}}\\text{ }\\text{max(0, \u03b3+s(h, r, t) \u2013 s(h',r',t'))}$\n$\\text{MR = }\\text{ }\\frac{\\text{1}}{\\text{|T|}}\\text{ }\\sum_{\\text{(h,r,t) ET}}\\text{rank(h,r,t)}$\n$\\text{AUC = }\\text{ }\\int\\text{TPR(FPR) dFPR}$\nEntity Alignment is a key task in KG fusion. Its' goal is to\nfind nodes (entities) representing the same real-world entities\nin multiple KGs and align them. [54] For example, the \"Barack\nObama\" entity in one KG should correspond to the \"Barack\nObama\" entity in another KG. Entity alignment is crucial for\nthe integration of KGs, data fusion, and cross graph inference.\nTranslation-based Embedding(TransE):\n$\\text{h+r\u2248t}$\nIn the embedding space, the vector addition of the head\nentity h and the relation r should be close to the vector of\nthe tail entity t. Its function is to embed entities and relations\ninto a low dimensional space, facilitating subsequent similarity\ncalculations.\nRotatE Rotation-based Embedding:\n$\\text{h\u00b0r\u2248t}$\nThe relation r is regarded as a rotation operation in the\ncomplex plane, and the head entity h should approach the\ntail entity t after rotating through the relation r. The function\nis to handle various types of relations such as symmetric,\nantisymmetric, and inverse relations.\nComplex Embedding(Complex):\n$\\text{Re(hrt)}$\nUse complex embedding to calculate the matching score\nbetween entities and relations. The function is applicable for\nhandling asymmetric relations. These embedded models map\nentities and relations in KGs to vector spaces, providing the\nfoundation for entity alignment.\nCalculate the cosine value of the angle between two vectors\nu and v, the larger the value, the higher the similarity. Used\nto compare the similarity of embedding vectors between two\nentities.\nCalculate the distance between two vectors u and v in\nspace, with smaller values indicating higher similarity. Used\nto measure the distance between entity embeddings.\nCalculate the absolute distance between two vectors u and\nv in space, with smaller values indicating higher similarity.\nUsed to measure the distance between entity embeddings."}, {"title": "IV. TECHNIQUES OF KGS WITH LLMS", "content": "With the emergence of real-world applications, there are\nmany emerging application scenarios for the current KGE\napplication tasks. We depict the differences between the above\ncategories of KGs and traditional KGs in Figure 1. The\nspecific characteristics of the KGE are different in different\nKG scenarios.\nClassic knowledge graph is a structured semantic knowledge\nbase based on graphs, which stores graph data in the form of\na triplet structure. At present, CKG has been widely used in\nvarious task categories in various fields. CKG can be formally\ndefined as\n$\\text{G = (E,R, U)}$\nwhere E is the set of entities, R is the set of relations and\n$U\\subset E\\times R \\times E$ represents the set of relation quaternions.\nWith the deepening of research and the emergence of new\nrequirements in the application, a number of KG scenarios\nthat are different from CKG have emerged, which can be\nintroduced one by one in the following sections. Here, in order\nto distinguish it from KGs in other scenarios, we refer to it\nas the classic knowledge graph (CKG) in the following text.\nBased on the different degrees of use of such methods for\nLLMs, we can roughly divide them into four categories: (1)\nmethods that use LLMs' prompts; (2) methods for fine-tuning\nLLMs; (3) methods of pre-trained LLMs; (4) methods to use\nLLMs as agent.\nIn the LLMs method based on prompts, the data in the\nTKG is generally processed first, so that it can be effectively\nunderstood and processed by the LLM, especially for the\nprocessing of time information. The prompts are usually\nconstructed according to the specific time characteristics and\nother effective characteristics of the time series task."}, {"title": "V. PROSPECTIVE DIRECTIONS", "content": "At present, in various KGE application scenarios, LLMs\nhave already provided effective solutions to specific problems\nthrough different degrees of invocation. There are still many\nprospective research directions for the application of LLMs in\nvarious KGE scenarios in the future, which we will introduce\nin this chapter for researchers' reference. We also hope that\nwith these proposed potential directions, we can cultivate\nresearch interests for beginners in this field. In the process\nof exploration, we have also found some examples in specific\nprocesses, which we will incorporate in the discussion of this\nchapter to facilitate the readers' understanding."}, {"title": "VI. DATASETS AND CODE RESOURCES", "content": "In this section, we have compiled some classic datasets in\ndifferent KGE scenarios for readers, and sorted out the code\nresources of some of the articles referenced in this article."}, {"title": "VII. CONCLUSION", "content": "LLMs have been widely used in tasks in multiple scenarios\nof KGE [111] [112] [113] [114] [115] Currently this as"}]}