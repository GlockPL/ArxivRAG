{"title": "On A Scale From 1 to 5: Quantifying Hallucination in Faithfulness Evaluation", "authors": ["Xiaonan Jing", "Srinivas Billa", "Danny Godbout"], "abstract": "Hallucination has been a popular topic in natural language generation (NLG). In real-world applications, unfaithful content can result in bad data quality or loss of trust from end users. Thus, it is crucial to fact-check before adopting NLG for production usage, which can be expensive if done manually. In this paper, we investigate automated faithfulness evaluation in guided NLG. We developed a rubrics template and use large language models (LLMs) to score the generation into quantifiable scales. We compared popular LLMs as well as the widely adopted natural language inference (NLI) models in scoring quality and sensitivity. In addition, we developed methods to generation synthetic unfaithful data, as well as a heuristics to quantify the percentage of hallucination. Our results on 4 travel-domain industry dataset show that GPT-4 (Achiam et al., 2023) can provide accurate judgement and explanation on whether a source and a generation are factually consistent. Furthermore, we found that tuning NLI models on synthetic data can improve performance. Lastly, we present insights on latency and cost for deploying such system.", "sections": [{"title": "1 Introduction", "content": "The recent advancement of large language models (LLM) has made it easier than ever to build NLP applications. With LLM's powerful natural language generation (NLG) ability, one can easily leverage prompt engineering to generate texts. However, pretrained transformer decoders are prone to hallucinations. Previously it was shown that the performance of LLM's decoding strategies (e.g. beam search) have a dependency on the type of the generation task (Wiher et al., 2022). As a result, LLMs can sometimes generate completed \"made-up\" sequences, in open-ended generation like story telling and close-ended generation like summarization (Maynez et al., 2020). Many recent methods have been proposed to mitigate hallucination, including retrieval-augmented generation (Lewis et al., 2020), chain-of-thoughts (Wei et al., 2022), tree-of-thoughts (Yao et al., 2024), self-consistency (Wang et al., 2023b), self-reflection (Ji et al., 2023b), controlled decoding (Mudgal et al., 2023), instruction tuning (Liu et al., 2023a). Nevertheless, the current state-of-the-art does not guarantee that hallucination can be 100% prevented. To apply NLG in industry applications, oftentimes having a \"faithful enough\" model is not sufficient to productionize an LLM-driven experience. With the aim of adding another layer of security to assure the accuracy of LLM generated content, we investigate an automated approach to effectively score degrees of faithfulness in guided text generation.\nFigure 1 illustrates a simple example of guided generation scenarios. Given guidelines (the prompt template) and grounding data (the source facts), an LLM model can hallucinate and generate either intrinsic hallucinating content in which the facts contradict that of the source or extrinsic hallucinating content in which new facts are added that cannot be verified from the source (Ji et al., 2023a). It should be noted that extrinsic hallucination does not necessarily contain incorrect information with respect to world knowledge. It is possible that the LLM interpreted the additional facts based on its internal knowledge. On the other hand, faithfulness is considered an antonym to \"hallucination\", which is defined as staying factually consistent with the provided source (Ji et al., 2023a). A higher faithfulness score thus indicates lower hallucinations. We define faithfulness of a generated text with respect to its source as the following:\n\u2022 reference: a text containing the source or ground-truth information, often longer\n\u2022 hypothesis: a text to check against the reference, which is derived by a model\nThe hypothesis is factually consistent with the reference if all factual elements in the hypothesis can be traced back to the reference. A faithfulness score measures: 1) to what extent the hypothesis can be verified by the reference; and 2) to what extent the LLM hallucinates when generating the hypothesis from the reference.\nWe investigated two approaches, namely using LLMs with a grading rubrics and natural language inference (NLI) models, to derive a faithfulness score for a given reference and hypothesis pair. We experimented with 5 LLMs and 3 variants of NLI models on 4 travel-domain dataset. We studied the score variation by comparing various types of hypothesis: gold hypothesis, sentence-level gold hypothesis, intrinsic hypothesis, and extrinsic hypothesis. In addition, we explored the model sensitivity in picking up different levels of hallucination. Our main contributions are: First, we showed that LLMs can accurately distinguish faithfulness apart from hallucination, and that reasoning during grading help LLMs gain sensitivity towards hallucination. Second, we presented methods to generate synthetic hallucination by type. Third, we demonstrated tuning with synthetic data can improve NLI model's performance. Lastly, we illustrated the progression of scores based on the percentage of hallucinating content."}, {"title": "2 Related Works", "content": "NLI for NLG evaluation NLI based models has been used widely in factual consistency evaluation. Derived originally from the text entailment (Dagan et al., 2005), given a premise, a model should classify the premise to be an \"entailment\", \"neutral\", or \"contradiction\" to a hypothesis (Bowman et al., 2015). Laban et al. (2022) proposed SummaC (Laban et al., 2022) which demonstrated that a fact check with NLI-based models on summarization performed better when using sentence-level granularity in both reference and hypothesis. Honovich et al. (2022) introduced TRUE benchmark for factual consistency evaluation, which showed that the NLI-based binary classifier could retain state-of-the-art performance over summarization, dialogue, paraphrasing, and fact checking tasks. Falsesum (Utama et al., 2022) is a data generation pipeline that utilized OpenIE and a fine-tuned T5-base (Raffel et al., 2020) model to create synthetic factually inconsistent data for improving NLI-based classifier. The \"neutral\" and \"contradiction\" labels were together treated as \"non-entailment\".\nLLM for NLG evaluation. Recently, many studies have also proven that an LLM could be used as a judge for evaluating NLP tasks (Zheng et al., 2023). Wang et al. (2023a) showed that ChatGPT can perform as well as human judges in NLG evaluation, in which a broader generation quality was graded between the reference and the hypothesis. Liu et al. (2023b) proposed G-Eval which used a chain-of-thought framework to evaluate NLG by granular categories. The faithfulness component was drafted as a binary classification for factual inconsistency identification. Chiang and Lee (2023) expanded on G-Eval and demonstrated that grading with explanation can improve the LLMs evaluation accuracy in summarization. Although the factual consistency was scored on a 1-5 scale, LLM's internal knowledge was used as criteria. Chang et al. (2024) showed that LLM can detect hallucinations as a binary classification problem in RAG systems in a zero-shot setting, and that training on synthetic data can further improve the detection accuracy. Gekhman et al. (2023) presented TrueTeacher which improved both LLM- and NLI-based factual consistency model accuracy by training on LLM-generated synthetic datasets.\nPrevious works on NLI models focus on classification and lack of explainability, while LLM-based evaluation is still emerging which has not arrived formalization. As far as we know, few studies have tried to quantify the degree of faithfulness. Furthermore, little documentation about how NLI models compare to LLM evaluation is available for domain specific real-world use cases. Finally, comparison in terms of what percentage of hallucinations affect the final scores has not been explored extensively."}, {"title": "3 System", "content": "We evaluated faithfulness scoring models on 4 travel-domain industry datasets. It should be noted that our dataset are taken from production and only contains gold data. We discuss means to create synthetic hallucination dataset in Section 3.1. Statistics of the dataset can be found in Table 1."}, {"title": "3.1 Dataset", "content": "ConvoAS is an abstractive summarization dataset. Our source data consists of customer support transcripts between 2 participants, a traveler and an agent. GPT-40 (OpenAI, 2024b) was used to generate a summary for the entire transcript. Information in the summary such as date, price, and location needs to be traceable from the source.\nConvoTS is a topic-specific summarization dataset. Similar to ConvoAS, this dataset also contains customer support transcripts, except 3 topic-specific summaries were generated for each transcript. It should be noted that many redundant turns exist because the chat happened between a traveler and a virtual agent through a guided service. GPT-40 (OpenAI, 2024b) was used for this use case.\nReviewTS is a topic-specific summarization dataset from reviews. Each reference consists of multiple review snippets related to a topic of interest (e.g. pool). A fine-tuned Mistral 7B (Jiang et al., 2023) model was used to extract and summarize the snippets. The summary is restricted to less than 100 characters. Because of the concise nature of this data, full-length and sentence-level hypothesis variants contain large overlaps. However, we kept the same setup for consistency.\nJsonTG contains key-valued pairs represented in JSON format for stylized text generation. This task aims at generating property headlines and descriptions given the grounding information such as amenities. The LLM, Claude3 Haiku (Anthropic, 2024a), was allowed to come up with expressive verbs and adjectives, but the grounding information must be traceable from the source JSON data.\nSynthetic Hallucination Data. Due to insufficient hallucination examples, we drafted guidelines for GPT-40 to generate a synthetic counterpart from the gold dataset to study model performance. Since the hallucination could take the form of either intrinsic, for contradictory facts, or extrinsic for extra knowledge, we mimicked both scenarios. For the former, we prompted GPT-40 to modify a piece of fact in the original sentence; and for the latter, we instructed the model to incorporate a piece of new world knowledge while the original sentence remains unchanged. Each dataset thus resulted in 4 varying hypothesis versions, namely gold full-length, gold sentence-level, intrinsic sentence-level, and extrinsic sentence-level. Furthermore, we simulated a numeric percentage at the sentence level to measure how much effect the portion of unfaithful content has on the evaluation scores. We took a ConvoAS subset with a 5-sentence hypothesis (51 samples). For every gold hypothesis, we incrementally swapped out 1 to 5 random sentences by its counterpart. The percentage of sentences that contain unfaithful information can thus be treated to mimic the \"hallucination percentage\" with respect to the overall hypothesis. It should be noted that it is inherently difficult to justify \"the number of facts\" contained in downstream tasks like abstractive summarization because the granularity of the content is subjective to the use case and the boundary between different \"pieces of facts\" can be fuzzy. While the proposed heuristic approach is far from perfection, it is sufficient to serve our goal of learning the scoring trend and model sensitivity."}, {"title": "3.2 Entailment-based Evaluation", "content": "We adopted Vectara's open-source HHEMv1.01 model (Bao et al., 2023). HHEM is a 184.4 million parameter lightweight Deberta model (He et al., 2021) fine-tuned on various NLI dataset, including TRUE (Honovich et al., 2022) and SummaC (Laban et al., 2022). Given a pair of text (reference, hypothesis), HHEM scores the level of entailment between the pair to a value in the range [0, 1], with 1 being entailment and 0 being contradictory.\nOne limitation is that HHEM model has a max input size of 512 tokens, which is prone to long text pairs. By default, truncation is applied to overflowing tokens starting at the end of the texts. In this case, the hypothesis will be truncated first, which will lead to information loss and worse case, totally wrong evaluation. To minimize information loss for maximized accuracy, we implemented a sliding window segmentation to the reference. Overlapping tokens were taken into consideration to include the previous context. The size of the current segment is computed dynamically based on tokens in the hypothesis as shown below:\n[CLS] overlap + current segment [SEP] hypothesis [SEP] [PAD]\nThe hypothesis is then evaluated against every reference segment and a final score for each pair is max aggregated across all segments. The reason behind segmenting the reference rather than the hypothesis is that the reference is often longer and contains redundancy. It is less sensible to segment the hypothesis, in which the information is typically already compressed, as it will result in more data loss. For the size of the overlapping tokens, we intuitively set it to 32 tokens which corresponds to around 25 words. This length is slightly longer than the length of an average sentence, which is said to be around 15-20 words.\nIn addition to testing the capability of HHEM out-of-the-box, we also fine-tuned 2 versions of the HHEM model with an extension of JsonTG. The preliminary accuracy on JsonTG was only promising when the JSON was decomposed into plain texts. While it is expected that encoder models perform poorly on unseen data structures; and that pursuing a JSON decomposition heuristic is feasible on a case-by-case basis, we were curious whether fine-tuning on stringified JSON can generalize the model to this data structure while preserving the same level of performance on other data."}, {"title": "3.3 Rubrics-based LLM Evaluation", "content": "Following the accuracy metric defined in HELM (Liang et al., 2022), we crafted a rubrics-based approach to use LLM as a judge to score the faithfulness of the generated contents.\nGiven a (reference, hypothesis) pair, the model was instructed with a system prompt to evaluate four aspects: 1) factual consistency which checks if all facts, e.g. numeric values and proper nouns, in the hypothesis can be traced back to the reference; 2) adjective regularity which examines if the adjectives used in the hypothesis are synonymous with their corresponding counterparts in the reference; 3) knowledge congruence which evaluates whether extrinsic information is injected to the hypothesis;"}, {"title": "3.4 Baseline", "content": "We integrated a composite score from ROUGE-L (Lin, 2004), BLEU (Papineni et al., 2002), and BERTScore (Zhang et al., 2019) as our baseline."}, {"title": "4 Result & Discussion", "content": "In this section, we discuss faithfulness scoring on the reference and hypothesis pairs for two sets of experiments. The score ranges from \"1-very unfaithful\" to \"5-very faithful\", with discrete intervals \"2-very faithful\", \"3-slightly unfaithful\", and \"4-very unfaithful\". It should be noted that HHEM models return a continuous probabilistic value in the range [0, 1]. It was later scaled to [1, 5] for ease of interpretation.\nOur first experiment aims to evaluate the models' scoring quality on gold and hallucinating contents. The experiments (Figure 2) outline model comparison across all datasets over the gold hypothesis, gold hypothesis on a sentence level, intrinsic hypothesis on a sentence level, and extrinsic hypothesis on a sentence level. On the other hand, the second set of experiments aims to study the score progression when the percentage of hallucinating content varies in the hypothesis. The comparison (Figure 3) involves scoring on both intrinsic and extrinsic hypotheses with 0% to 100% hallucinating contents in 20% steps. Along with the score progression, we also tested the significance of \"reasoning\" to LLM's scoring capability. Models tested are as follows: for rubrics-based scoring with LLMs: GPT-4 (Achiam et al., 2023), GPT-4o-mini (OpenAI, 2024a), Llama3.1-8B (Meta, 2024),"}, {"title": "4.1 Scoring Quality", "content": "Figures 2a - 2e elaborates on the residuals of the scores. Overall, GPT-4 with zero-shot rubrics performed the best among all models, and Llama3.1-70B followed closely after. The tuned HHEM-extrinsic model also shows high accuracy with a slight flaw in scoring sentence-level JsonTG dataset. A slight performance drop can be observed transitioning from the gold hypothesis (2a) to its sentence level (2b). GPT-4 scored 6.8% out of 1393 gold sentences to \"1-highly unfaithful\", which was mainly due to incomplete information in individual sentences. As a result, several unfaithful occurrences were assigned reason 1 in Table 3. Furthermore, JsonTG reference seems to be consistently challenging to all models across all tasks. After inspecting the LLM provided reasoning, we found that this is because our adjective verification rubrics to capture \"adjectives without any basis\" contradicts the JsonTG generation task which allows LLMs to come up with expressive adjectives. For posterity, it is best to adapt the rubrics to a per-use case basis for better accuracy.\nLLMs with fewer parameters such as Llama3.1-8B and Claude3-Haiku seem to struggle to understand the rubrics. Llama3.1-8B tends to be overly strict in that it scored all tasks to \"3-slightly unfaithful\" and below, indicating the hypothesis is unfaithful. This seems to be a result of Llama3.1-8B's following the instructions too literally. In addition, Llama3.1-8B seems to have confused \"factual consistency\" with \"granularity\". The example LLM reasoning can be found on row 2 and 3 in Table 3. In contrast, Haiku tends to be extremely lenient. It failed consistently on both hallucination tasks while scoring almost perfectly on the gold hypothesis. It appears that Haiku was able to capture the key information in the hypothesis, but could not make the association to the contradictory part in the reference. Row 4 in Table 3 illustrated an example where Haiku incorrectly scored an intrinsic hypothesis to \"5-highly faithful\".\nIt is interesting that the above observation is not reflected in the overall residual plot in Figure 2e as the scores got smoothed out by half of the tasks with good performance. However, it should be noted that although the models' strictness and leniency are consistent across different tasks, in reality, it would be difficult to integrate them into the production pipeline due to the unpredictability brought by their scoring bias.\nUnlike LLMs, HHEM models do not experience significant performance degradation from full length to sentence-level hypothesis. This is likely because the base Deberta model was originally trained for NLI, which uses a slightly fuzzy definition for \"textual entailment\" that a premise entails a hypothesis given a very probable inference can be made from the premise (Dagan et al., 2005). Thus, a piece of a sentence from the hypothesis would receive a higher score during the prediction as long as the encoded information can be inferred from the source. It should be noted that HHEM models are naturally prone to input token size, evaluating on sentence level could improve efficiency by inducing fewer segments on the reference."}, {"title": "4.2 Scoring Progression", "content": "Figures 3 illustrates the score progression trends with respect to the percentage of the hallucinating information. To recap, the percentage-based hallucination dataset was created by randomly replacing the N-numbers of gold sentences with hallucinating ones. Each hallucinating sentence contains a subtle change as demonstrated in Table 2. Consequently, the hallucination percentage is in fact the percentage of sentences containing unfaithful information. In this iteration, we also tested LLM's scoring capability when the \"reasoning\" component was removed from its response. The score-only model's performance is suffixed with \"SO\".\nAs shown in Figures 3a - 3b for both intrinsic and extrinsic hallucination, the faithfulness score decreases accordingly as the percentage of hallucination increases from 0% to 100%. This trend is consistent even for lower-performing LLMs such as Llama3-8B and Claude3-Haiku. As mentioned earlier, all models tend to perform better on intrinsic hallucination over extrinsic ones. We can potentially explain this observation further by comparing the changes in score going from lower to higher hallucination percentage. Taking GPT-4 as an example, in Figure 3a, when the intrinsic hallucination percentage increases from 0% -> 20% -> 40%, the score decreases from 4.90 -> 3.23 -> 2.26; whereas for extrinsic hallucination, the score decreases in a more gradual manner, namely 4.90 -> 4.13 -> 3.15, with every step taken. GPT-4's sensitivity to identify the initial 20% of intrinsic hallucination is roughly mapped to detecting 40% of extrinsic hallucination. Tracing back to our rubrics, although \"avoiding speculative or creative content generation\" was included in the guidelines, the score assignment was lenient towards extrinsic hallucination. We designed the \"3-slightly unfaithful\" to allow unverifiable information to be present without negatively impacting the reader's experience. Tightening the rubrics could help increase the sensitivity in detecting extrinsic hallucinations. However, one should proceed with caution and support the rubric's development by using similar experiments to ensure that the LLMs maintain a good tolerance between unfaithful and creative content.\nWe also quantified the scores without reasoning by simply removing the instruction to justify the reason for grading. Overall, in Figures 3a - 3b, all LLMs demonstrated various levels of performance degradation. For both intrinsic and extrinsic hallucinations, scoring without reasoning made the LLMs more lenient towards the unfaithful content. As the percentage of hallucination increases, the decreases in score converge much slower. As a result, none of the LLMs scored a 100% hallucinating content to \"1-highly unfaithful\". Another interesting observation is that the strictness of Llama3.1-8B and the leniency of Claude3-Haiku have both relaxed slightly without reasoning, which caused a performance \"improvement\". We argue that this is because the reasoning is the key to keeping the model \"sharp\", the \"improvement\" is really just a form of unresponsiveness when the \"thinking\" step is removed from the process."}, {"title": "4.3 Latency & Usage", "content": "Table 4 illustrates the latency and price for each model. All LLMs were tested through HTTP endpoints hosted through the same proxy service. The average time in seconds, number of output tokens, and prices were calculated in a sequential API call setting. By nature, smaller encoder models are inherently faster than decoder-based LLMs, thus we only computed a batch inference latency for HHEM models ran on an AWS G5 instance. The pricing referred to OpenAI 2 and AWS Bedrock 3 standards. While GPT-4 yields the best accuracy, it is also the most costly model on both latency and price. On the other hand, although Llama3.1-70B seems to have balanced out the accuracy vs. latency, Llama3.1-70B provided reasoning can be barely useful to a human auditor due to little granularity. It should be noted that one could customize the prompt instruction to adjust the level of granularity to achieve the desired balance. For real-world usage, we recommend combining a fast model with acceptable performance as a first pass evaluation; then apply more powerful LLMs with detailed explanation to assist human audits."}, {"title": "5 Conclusion", "content": "We proposed a rubrics-based template to use LLMs to score faithfulness in NLG. We evaluated 5 popular LLMs on 4 travel-domain indutry dataset. The results indicate that in a zero-shot setting LLMs, e.g. GPT-4, outperforms NLI models, e.g. HHEM, on capturing both intrinsic and extrinsic hallucinations. HHEM model with fine-tuning on synthetic data can outperform LLMs in domain specific evaluation given carefully crafted training data. Lastly, we introduced a heuristic to quantify faithfulness score by hallucination percentage, which helps visualize the sensitivity when developing faithfulness evaluation models. Our future work include exploring the score association between different LLMs from the same family and tuning the rubrics to adjust the scoring sensitivity."}, {"title": "6 Limitations", "content": "1. Due to resource limitations, we focused on testing only travel-domain dataset, but did not include popular open source faithfulness dataset. We justify that real-world dataset are often more challenging than research domain dataset, as there exist more noise in real-world data. We believe our approach can be generalizable to other domains.\n2. Due to legal and privacy constraints, we do not plan to release our dataset as some of it contains sensitive customer information.\n3. Due to security concerns, the proxy service used for LLM testing contains enterprise guardrails which moderate incoming and outgoing data. Thus, the latency reported in Table 4 would be higher than directly calling OpenAI and Bedrock APIs.\n4. Due to resource limitations, we were not able to evaluate a larger model from the Anthropics family, such as the Claude3.5-Sonnet (Anthropic, 2024b). It was not our intention to make an unfair comparison using only a smaller Claude model."}, {"title": "A Appendix: Additional Data and Results", "content": "All prompts in this paper was experimented with temperature set to 0 for consistency.\nTable 5 contains the prompt used for LLM faithfulness grading. For score only grading, we simply removed the \"reasoning\" field from the output format. During our experiments, without enforcing controlled decoding to ensure consistent JSON output, we noticed that only GPT-4 and GPT-4o were able to always respond with a stringified JSON. Both Llama3.1-8B and Claude-Haiku tends to add heading and tailing texts and sometimes include unhashed double quotes which resulted in a JSON decoding challenge. For open source models which can be served by VLLM (Kwon et al., 2023), we recommend using VLLM's controlled decoding strategy to formulate the structured output.\nTable 6 illustrates prompts used for generating the synthetic intrinsic and extrinsic hallucination data. Each prompt follows the \"Template\" format as noted on row 1. Depending on the use case and the nature of the dataset, we customize the case-by-case instruction for the best results as denoted on row 2-7. It should be noted that a general prompt for generating synthetic data could work, however, the data generated might not be tricky enough so that the changes are subtle. For example, when our source data contains the following ground-truth: \"3-bedroom\" and \"2-bathroom\". There is a chance that a LLM confuses the numeric values and writes \"2-bedroom and 2-bathroom\" in the generation. By including explicit details in the instructions, we were able to mimic similar cases."}, {"title": "A.2 Additional Analysis on Synthetic Data", "content": "Table 7 demonstrates several tricky intrinsic examples in the synthetic dataset. Prior studies have shown that language models tend to struggle with following the instructions when negations present in prompts or generations (Truong et al., 2023; Varshney et al., 2024). While a large portion of negations similar to row 1 were scored correctly, we did see that GPT-4 consistently fail on cases like row 2. In this case, the gold summary was in fact extracted word by word from the source conversation: \"If your original card charge is still processing, it will be dropped automatically.\" The word \"not\" was obviously ignored every time when evaluating the intrinsic sentence against the source. Row 3 and 4 are good examples of near realistic synthetic data. Especially row 4 when the information is almost correct but not 100% accurate.\nTable 8 showcases a few extrinsic hallucination examples generated by GPT-40. We found some of the synthetic sentences extremely humorous, especially when the knowledge is added in an analogy form which potentially triggers a punchline. While these might be too \"easy\" to capture when served as extrinsic hallucinations, we thought it is worth pointing out this discovery, as it may be useful to humor research with LLMs."}, {"title": "A.3 Additional Results", "content": "We also experimented on Qwen-7B & 72B (Yang et al., 2024), but did not include the results in the main paper because Qwen-7B occasionally (about 10% of the times) responded with Chinese reasoning, which caused difficulties in the comparison and analysis. Furthermore, we quickly explored fine-tuning HHEM with intrinsic and extrinsic combined synthetic data, however, the initial results indicate that the HHEM-combined was a little too strict in scoring gold data. This was likely due to the hallucination samples being double in size than the gold samples after combining the datasets. Due to resource limitation, we did not further adjust the training data to achieve desired results."}]}