{"title": "TVR-Ranking: A Dataset for Ranked Video Moment Retrieval with Imprecise Queries", "authors": ["Renjie Liang", "Li Li", "Chongzhi Zhang", "Jing Wang", "Xizhou Zhu", "Aixin Sun"], "abstract": "In this paper, we propose the task of Ranked Video Moment Retrieval (RVMR) to locate a ranked list of matching moments from a collection of videos, through queries in natural language. Although a few related tasks have been proposed and studied by CV, NLP, and IR communities, RVMR is the task that best reflects the practical setting of moment search. To facilitate research in RVMR, we develop the TVR-Ranking dataset, based on the raw videos and existing moment annotations provided in the TVR dataset. Our key contribution is the manual annotation of relevance levels for 94,442 query-moment pairs. We then develop the NDCG@K,IoU > \u00b5evaluation metric for this new task and conduct experiments to evaluate three baseline models. Our experiments show that the new RVMR task brings new challenges to existing models and we believe this new dataset contributes to the research on multi-modality search. The dataset is available at https://github.com/Ranking-VMR/TVR-Ranking", "sections": [{"title": "1 Introduction", "content": "Given a query expressed in natural language, to retrieve or to locate a temporal moment from video(s) that semantically matches the query has many applications. A temporal moment refers to a segment within a source video with identified start and end timestamps. Examples of such applications include searching for a specific scene in security surveillance videos Yuan et al. [2023], locating a medical procedure within an educational tutorial Gupta et al. [2023], or identifying desired scenes for video editing purposes, among others.\n\nA few tasks with different names have been studied for addressing similar objectives, including video retrieval (VR), video moment retrieval (VMR), natural language video localization (NLVL), temporal sentence grounding in video (TSGV), and video corpus moment retrieval (VCMR) Liu et al. [2023], Zhang et al. [2023]. Among them, VR involves retrieving a video from a collection based on visual content, akin to video search on platforms like YouTube, but with the search criteria grounded in the visual content of videos. NLVL and TSGV, more commonly used in CV and NLP communities, refer to the same task as video moment retrieval (VMR) in the IR community. VMR aims to locate a moment within a given video that semantically matches the text query. The VCMR task is a direct extension of VMR, focusing on retrieving a moment from a collection of videos Victor et al. [2019]. As depicted in Figure 1, existing tasks VR VMR and VCMR, all aim to find one answer, being either a video or a moment, for a given query.\n\nThe reason for expecting one exact answer to a query lies primarily in the annotation of benchmark datasets. During annotation, annotators watch a video, then provide textual descriptions of meaningful video moments in this video. Subsequently, each description serves as the query to retrieve the corresponding moment from this source video. Given that a query typically describes a specific moment precisely, a model trained on these datasets can assume the existence of the moment to be searched for, and all queries are from users who possess a good understanding of the source video."}, {"title": "2 Related work", "content": "Current VMR and VCMR datasets fail to simulate real-world moment search scenarios due to two key unrealistic assumptions: users have a deep understanding of the source video and there is only one \"perfect match\" moment for each query.\n\nThe first assumption stems from traditional annotation processes where annotators are required to watch the entire video and then describe meaningful moments therein. Most datasets listed in Table 1 are annotated in this way, including DiDeMo Anne Hendricks et al. [2017], TACOS Regneri et al. [2013], TVR Lei et al. [2020], ActivityNet Caption Krishna et al. [2017], Ego4D(NLQ) Grauman et al. [2022], and UCA Yuan et al. [2023]. Besides, the Charades-STA dataset Gao et al. [2017] extends the Charades dataset Sigurdsson et al. [2016] by segmenting video descriptions into sentences and linking them to specific video timestamps via keywords. In contrast, queries in our dataset may or may not provide precise descriptions of moments, thus embracing users with different levels of understanding of the corpus.\n\nTypically, standard VMR datasets generally link a query to a single relevant moment. For instance, the health-related queries in the MedVidQA dataset Gupta et al. [2022], sourced from WikiHow's 'Health' category, are well-represented of real-world scenarios. Nonetheless, this dataset confines each query to just the most relevant moment. In contrast, real-life situations frequently encompass multiple moments that can be similarly described. The QVHighlight dataset Lei et al. [2021] was pioneering in allowing queries to match multiple moments within a single video. However, it still restricts searches to single videos and focuses only on perfect matches. Our dataset aims to retrieve a ranked list of moments from a video corpus based on imprecise queries, functioning more like a search engine and accommodating both closely and loosely relevant matches. This paradigm not only broadens the utility of the results but also aligns more closely with practical search needs."}, {"title": "3 The TVR-Ranking Dataset Annotation", "content": "In the ideal setting, a dataset shall well reflect the context of real-world applications, e.g., the data source and the information needs from users Yu and Sun [2023]. In the RVMR task setting, we assume there exists a collection of videos, and users search for relevant moments through textual descriptions as queries. However, such kinds of queries can only be collected from logs of video search services, which are not publicly accessible. Without access to such resources, we choose to derive user queries from existing data annotations, i.e., the datasets listed in Table 1.\n\nOur immediate task is to choose which existing dataset to use as the raw data for annotation. To this end, we compare the existing datasets based on the following perspectives: accessibility to the raw videos, number of videos, variants of different activities/scenes, and number of moments annotated. Because existing annotations are mostly descriptions of scenes and/or actions, the number"}, {"title": "3.1 Imprecise Queries", "content": "In TVR dataset, many moment descriptions contain character names and even their dressing details, making them precise descriptions of the moments. To derive imprecise queries, we substitute these words with more general words. In particular, we replace all character names with pronouns. This re-placement is essential for our annotation because our annotators (also users) may not have knowledge about these characters. Table 2 lists three example descriptions before and after substitution.\n\nThe character name substitution is through carefully designed prompts to ChatGPT (see Appendix A), with quality checks. The output of ChatGPT is a quality substitution if it successfully passes two validation checks. The first check is for semantic consistency, to ensure no significant change in terms of semantic meaning after substitution. The SimCSE Gao et al. [2021] similarity of moment descriptions before and after the substitution is expected to be above a threshold (0.4 in our implementation). The second check is to ensure no person names appear in the substituted version. We detect person names in the substituted moment description using Flair Akbik et al. [2019]. If a substitution fails to pass both checks, the moment description undergoes human review and is fed to ChatGPT again for another substitution with a different temperature parameter setting, till it passes the two checks.\n\nThe above procedure replaces 160,701 words across 72,842 moment descriptions, averaging 2.21 words per description. Table 3 presents the top 12 most frequently replaced words, which shows a diversity of personal pronouns in the descriptions.\n\nTo distinguish the moment descriptions before and after the substitution, we call the substituted version moment caption. Observe in Table 2, the three moment descriptions become the same"}, {"title": "3.2 Relevance Annotation and Quality Control", "content": "From the 72,842 moment captions, we randomly select 500 and 2781 moment captions as queries in validation and test sets respectively, for manual annotation. The remaining moment captions will be used to construct a pseudo training set, to be detailed in Section 3.3. As a search task, all queries share the same large pool of source videos.\n\nNext, we manually annotate ground truth moments, along with their degree of relevance, for both the 500 validation and 2781 test queries. Manually annotating all matching moments from such a large video corpus for a given query is infeasible. Therefore, we utilize the moment annotations available in the original TVR dataset during the annotation process. These original annotations serve two purposes. First, we fully rely on the temporal boundaries of all moments in the original TVR annotations. This approach allows us to view the video corpus as a vast collection of moments each accompanied by a moment caption, during the data annotation process. Second, the moment caption provides a reasonably good description of a moment. Consequently, the semantic similarity between a query and a moment caption acts as a proxy for an initial estimation of their relevance.\n\nLet $m.c$ and $m.v$ represent the caption and the visual content of moment $m$, respectively. To annotate the ground truth moments for a query $q$, we initially retrieve the top-K moment candidates based on their similarity to $q$ by using SimCSE Gao et al. [2021], denoted by using $sim(q, m.c)$. In our annotation process, we set $K = 20$ for the first batch. This batch of 20 query-moment pairs is then presented to two annotators. Each annotator independently labels the degree of relevance of every query-moment pair $(q, m.v)$ purely based on the moment's visual content, assigning a score from 0 for irrelevant, to 4 for a perfect match.\n\nIf the difference between the two relevance scores assigned by two annotators is either 1 or 0, then we considered the two annotators to have reached a consensus. The average relevance score was then rounded up to the nearest whole number as the final score for this query-moment pair. If the two annotators fail to reach a consensus, the same query-moment pair is assigned to another two annotators. Then we have a total of 4 scores. Among the 4, we remove one highest score and one lowest relevance score. If the difference between the remaining two scores is either 1 or 0, we consider a consensus is reached; the average of the remaining two scores is rounded up to the nearest whole number as the final score. Otherwise, the pair is discarded.\n\nAfter completing the annotation of all 20 moments for a query in the first batch, the lead annotator (the first author) checks the relevance score distribution of these 20 candidate movements. Recall that the 20 candidate movements are ranked by $sim(q, m.c)$. Movements ranked in the top few positions are likely to be more relevant than those ranked lower. However, if the last 5 candidates among the 20 remain very relevant, then it is a strong indication that the annotation so far has not fully covered all matching moments. The next batch of 20 candidate moments will be retrieved by $sim(q, m.c)$ for annotation. We observe that we can cover all relevant movements for nearly every query after annotating the second batch, totaling 40 candidate moments. Hence, at most, two batches or 40 candidate moments are annotated for a query.\n\nAt the completion of the annotation process, we obtained a total of 9,272 valid annotations for the 500 validation queries and 18,146 annotations for the 2,781 test queries. This resulted in a total annotation cost of approximately 13, 000 USD for around 1,200 working hours contributed by 23 annotators, excluding the lead annotator's effort. All annotators underwent a tutorial and qualifying exam before participating in the annotation task, as detailed in Appendix B.2."}, {"title": "3.3 Pseudo Training Set Generation", "content": "Due to the high annotation cost, we do not manually annotate training data. Instead, we rely on the query-caption similarity, i.e., $sim(q, m.c)$, as a proxy to generate pseudo annotations as the training set. Specifically, given a query, we collect the top-N moments based on $sim(q, m.c)$ as the training set. In our dataset, we include pseudo training sets with $N = 1, N = 20$, and $N = 40$. Datasets with other values of N can be easily generated as well.\n\nAs shown in Table 2, after the substitution, two moment descriptions may become identical. To ensure all queries in the validation and tests do not appear in training, we remove from the pseudo training set the queries that appear either in validation or test, a total of 244 queries. As a result, the pseudo training set contains a total of 69,317 queries."}, {"title": "3.4 TVR-Ranking: Statistics", "content": "Table 4 provides an overview of the annotated TVR dataset, with query length in number of words, moment duration in seconds, and the source video duration in seconds. The average ratio of moment length to its source video length is about one-tenth. In the table, we also list the average number of relevant moments (with a relevance score of 1 to 4) annotated per query is around 27. Recall that, moments are annotated in batches to a query, with each batch containing 20 candidate moments. Specifically, in the validation set, 264 queries (52.80%) were annotated with 20 moments (i.e., one batch) and 236 queries (47.20%) with 40 moments (i.e., two batches). The test set follows a similar distribution, with 1,473 queries (52.97%) annotated with 20 moments and 1,308 queries (47.03%) with 40 moments. The justification of annotating at most 40 moments for a query is detailed in Appendix B.3.\n\nFollowing the consensus verification process, 14,382 (97.79%) annotations in the validation set reached consensus with either two or four annotators, while 325 (2.21%) were found to be in disagreement and subsequently discarded. Each annotation here is a query-moment pair. Again, the test set shows a similar distribution; 80,060 (97.90%) annotations achieved consensus and 1,716 (2.10%) annotations were discarded. With the annotations in consensus, on average each query comes with 27.1 relevant moments in the validation set, and 27.0 in the test set, by counting the moments with relevance scores from 1 to 4."}, {"title": "4 Evaluation Metric for RVMR", "content": "For RVMR, we aim to retrieve a ranked list of relevant moments for a given text query from a video collection. The quality of retrieval can be evaluated from at least two aspects: (i) the quality of moment localization, i.e., to what extent the model correctly identifies the temporal boundaries of a moment, and (ii) the quality of ranking, i.e., to what extent the model correctly ranks the retrieved moments from most to least relevance to the query. Note that, even if a moment is correctly located with perfect start/end timestamps, the moment may not be the more relevant to the query.\n\nIntersection over Union (IoU), denoted by \u03bc, is a common metric widely used in moment retrieval tasks. Given a moment prediction with start and end timestamps, evaluated against the ground truth start and end timestamps, IoU measures the intersection along the timeline against the union along the timeline, illustrated in Figure 2(a), where $g_0$ and $p_0$ denote the ground truth and predicted moments respectively. If there is no overlap between the two moments, then \u03bc = 0. IoU is commonly used as pre-selection criteria for qualifying moments before other measures are computed. For example, a model can be measured by the ability to locate moments with \u03bc \u2265 0.3.\n\nNormalized Discounted Cumulative Gain (NDCG) is delicately designed for evaluating ranking results with different relevance levels Manning et al. [2008]. Specifically, the Discounted Cumulative Gain (DCG) of the top K ranked results is defined in Equation 1, where i is the ranking position with 1 being the top ranked position, $rel_i$ is the level of relevance. For example, the left part of Figure 2(b) shows four ground truth moments with $g_1$ at rank 1 position and $g_4$ at rank 4 position. To their left are the relevance levels with $rel_1$ = 4 for $g_1$ and $rel_2$ = 2 for $g_2$.\n\n$DCG@K = \\sum_{k=1}^{K} \\frac{rel_i}{log_2(i+1)}$\n\nNDCG@K is then defined as the normalized DCG against the DCG@K value of a perfect ranking e.g., all items with a relevance level of 4 are ranked before all items with a relevance level of 3, and so on, till the K cut.\n\nWe process the matching of predicted moments following their ranking returned by a model. If a predicted moment fails to find a matching ground truth with an $IoU \u2265 \u03bc$, it is assigned a relevance score of 0. When multiple ground truths meet the IoU > \u03bc criterion, we select the one with the highest IoU and remove it from the ground truth moment listing, to prevent duplicate matches. The NDCG@K is computed by the relevance scores of the predicted moments at cut K, and the perfect ranking of the top K ground truth moments, regardless these K moments are matched by any predicted moment or not."}, {"title": "5 Baseline Performance", "content": "Illustrated in Figure 1, the closest task setting to RVMR is VCMR. In particular, if a VCMR model can compute a form of confidence for its retrieval result, then a ranking of the predicted moments"}, {"title": "6 Conclusion and Limitations", "content": "In this paper, we study the task of ranked moment retrieval to better reflect the practical setting of moment retrieval from video collection, by queries in natural language. To facilitate the research in this new task, we develop the TVR-Ranking based on the raw videos and moment annotations of the TVR dataset. Our data annotation process considers query rewriting to best simulate the queries from users who may not have watched all videos in the search collection. The main effort is the manual annotation of relevance levels for a large number of candidate moments for validation and test queries. We then develop the evaluation metric by considering measures used in both ranking tasks i.e., NDCG, and in moment retrieval, i.e., IoU. Through experiments, we show that models that perform well on VCMR may not necessarily outperform others on this new RVMR task, indicating the lack of ranking capability of existing models.\n\nOur work has three limitations. First, the queries used in the dataset might not perfectly mirror the real-world needs of users, potentially limiting their practical applicability. As the source videos are from six TV series, diversity is also a concern. Nonetheless, for the purpose of benchmarking and evaluating model capabilities, the dataset is adequate. Second, the annotation process employs a combination of query and caption as a proxy, identifying up to 40 relevant moments. This method may overlook some genuinely relevant moments, but make the annotation feasible for a reasonable coverage. Third, the pseudo training set is generated through a proxy $sim(q, m.c)$, which may lead to a gap compared to the real annotations in the validation and test sets.\n\nOur work provides relevance annotations on top of an existing open-source dataset. We do not anticipate any potential negative societal impacts."}, {"title": "B.1 Annotation Guideline", "content": "Based on annotated samples, we observed that annotators mainly consider three factors in judging the degree of relevances between candidate moments and queries: match of action(s), visual completeness, and temporal completeness. Action match indicates the alignment between query semantics and the action(s) demonstrated in the candidate moment. Visual completeness considers that the entire action is completed within the video frame. Temporal completeness evaluates the proportion of the complete action accounting for the moment. The relevance is low when too many unrelated segments appear at a moment, or when an action is incomplete or interrupted. Based on the observations, we design a guideline and use it for pre-annotation. We then refined the guideline based on the feedback received from the pre-annotation exercise. The formal annotation follows the guideline below to determine the degree of relevance from level 4 (perfect match) to level 0 (unrelated).\n\nLevel 4 A moment perfectly matches the query if the actions expressed in the moment accurately align with the query's semantics. The action occurs at a prominent spot within the frames, and the timespan of the moment fully covers the action without redundancy.\n\nLevel 3 The moment could match the query well, except for a little point mismatched. The action generally aligns with the semantics of the query, but some details are missed.\n\nLevel 2 The moment precisely matches the search query, though there are a few noticeable mis-matches. The action within the moment only conveys a part of the query information. There is a large gap in visual and temporal completeness.\n\nLevel 1 The moment presents relevance to the query on a few points. Though a few points of action are mentioned in the query, the main action is mismatched with the query.\n\nLevel 0 The moment is entirely unrelated to the query.\n\nWe provide examples to illustrate the annotation guidelines. For instance, if the query is \"A person stands up and walks towards the board\", and the moment shows one or two people standing up and walking towards the board, we consider it a perfect match and assign a relevance level of 4. Another moment might also reflect the query's semantics but include additional unrelated visual content; this moment will be assigned a relevance level of 3. If a moment only shows \"a man walks towards a board\" without the action \"stand up,\" it will be assigned a relevance level of 2. An example of a level 1 moment might show \"a man\" and \"a board\" but no interaction between them. Note that, due to the nature of the video source as TV series, many moments will contain a \"person\". If none of their actions are mentioned in the query, such moments are considered unrelated. The \"person\" here is similar to stop words in web search."}, {"title": "B.2 Annotation Setup and Annotators", "content": "The original TVR dataset consists of 21,829 videos, each at a resolution of 480p and with a frame rate of 3 fps. These videos were segmented into 97,410 moments based on their timestamps and converted into GIF files, which were then stored on AWS S3 storage. For the annotation process, we selected Label Studio as our platform, which integrates well with our workflow. For each query, 20 candidate moments are initially identified through query-caption similarity. If more moments are needed, the next 20 candidate moments are identified and annotated. Consequently, the maximum number of moments that could be labeled per query is 40.\n\nWe received the number of 227,808 raw annotations. After cleaning and merging, we obtained the number of 94,442 annotated moments for 3,281 queries with annotation consensus.\n\nOur annotation team consists of 23 diverse students from China, Singapore, and India, all of whom have undergraduate degrees or higher. The team maintains gender balance. The majority of our team members are native English speakers, and the rest possess strong English proficiency. This ensures that our annotators can accurately understand the queries in English.\n\nAll annotators underwent training including an explanation of the RVMR task, the annotation guidelines, and a tutorial of the annotation tool. This training ensures that all annotators share the same understanding of the task and follow the same annotation standards. After the training,"}, {"title": "B.3 Annotation Analysis: Relevance Level vs Moment-Caption Similarity", "content": "In the TVR-Ranking dataset, our goal is to find and label all the relevant moments for a given query. However, in the annotation process, locating all such moments is time-consuming and unnecessary. In the original TVR dataset, workers watched the source videos and wrote moment descriptions. Therefore, we use these moments as candidate moments for our annotation and consider the original moment descriptions to be a good proxy for visual relevance. To distinguish the moment descriptions before and after character name substitution, we refer to the substituted version as the moment caption. We use $m.c$ to denote the moment caption and $m.v$ for the moment's visual content. All our annotations are based on moment captions.\n\nGiven query-moment pairs, we study the relationship between the query-caption similarity $sim(q, m.c)$ and our annotated relevance between the query and the moment's visual content $rel(q, m.v)$. This study is based on annotations for 10 randomly sampled queries. Instead of annotating 40 candidate moments for each query, we have annotated 120 candidate moments for each of these 10 queries. Specifically, for each sampled query, we annotate the 60 candidate moments with the highest $sim(q, m.c)$ scores and then randomly sample another 60 moments from the remaining moments in the TVR dataset."}, {"title": "B.4 Annotation Analysis: Annotation Consensus and Distribution", "content": "To elaborate on the level of consensus among the annotators, we show the distribution of the raw scores they assigned. Recall that each query-moment pair is annotated by either two annotators (if they reach a consensus) or four annotators (if a consensus is not reached initially). Thus, for each query-moment pair, we have either 2 or 4 raw scores. These scores range from a minimum of 0 (non-relevant) to a maximum of 4 (perfect match). We group query-moment pairs according to their minimum and maximum scores among the raw scores assigned by the annotators. Figure 5a shows the distribution of all query-moment pairs. For instance, the cell at the bottom left corner represents the percentage of query-moment pairs with a minimum score of 0 and a maximum score of 4, which is extremely rare at 0.01%, or one in ten thousand, likely due to human errors during the annotation process. The majority of query-moment pairs have either identical scores assigned by all annotators or a small difference of 1 between the minimum and maximum scores, indicating a consensus.\n\nIn Figure 5b, we present the distribution of final relevance scores for all query-moment pairs. The final score is calculated as the mean score from annotators, rounded up to the nearest whole number, or as the trimmed mean if four annotators were involved. The distribution roughly follows a normal distribution, with fewer pairs exhibiting relevance scores of 0 or 4, and the majority having a relevance score of 2. Note that for each query, we annotate at most 40 candidate moments. A relevance score of 0 indicates confirmed non-relevance, which differs from moments that are not annotated for the query, although the latter are also likely to be irrelevant."}, {"title": "B.5 Annotation Analysis: Case Study of Example Queries", "content": "Figure 6 provides two example queries, each with 5 candidate moments ranked by $sim(q, m.c)$ in descending order. The relevance scores assigned by annotators (in bar chart) show a reasonable correlation with $sim(q, m.c)$ (in line chart), where larger $sim(q, m.c)$ also suggests high relevance scores. This is expected based on the earlier analysis in Figure 4a. Yet, the moments with lower $sim(q, m.c)'s$ can be annotated with higher relevance scores, and $sim(q, m.c) = 1.0$ does not guarantee a perfect match. The second example is a good illustration of this point. One possible reason for the discrepancy is the annotation methodology. The original TVR dataset required annotators to watch the full video before selecting and describing specific moments, providing them with full context in the video. In our annotation, only moments are presented to annotators, without the full video. Our annotators make judgments solely based on the provided moment. For query \"A"}, {"title": "C Experiments and Case Study on Evaluation", "content": "In our experiments, we adapt three VCMR models to RVMR and evaluate them on the TRV-Ranking: XML Lei et al. [2020], CONQUER Hou et al. [2021], and ReLoCLNet Zhang et al. [2021].\n\nThe main change in the adaptations is the introduction of weight to the training loss based on query-moment similarity, i.e., $sim(q, m.c)$, recognizing that moments vary in relevance to a query. There is no such loss in the VCMR task setting because there is exactly one ground truth in VCMR. This weight aims to diminish the influence of less relevant moments on model training by adjusting the loss. In terms of features, we follow the original implementation and use both subtitle and video features extracted from the source video for all three models. The query features in our implementation are extracted using BERT Devlin et al. [2018].\n\nNote that, because VCMR was directly extended from VMR, there is a train/test split in the source videos. That is, there is a set of queries and source videos for training, and another set for testing. However, the VCMR task is similar to web search, except the result documents are video moments."}, {"title": "C.1 Baselines and Implementation Details", "content": "The XML model was proposed alongside the TVR dataset. The motivation behind XML is to consider both video and subtitle information when retrieving moments, as some queries in TVR are based on subtitles. The model integrates video and subtitle features as context information and conducts retrieval on this context to achieve more accurate recall. During the retrieval stage, the authors use matrix multiplication to compute the confidence score, enhancing retrieval efficiency. Additionally, they designed another"}]}