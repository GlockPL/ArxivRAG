{"title": "Human Action CLIPS: Detecting Al-generated Human Motion", "authors": ["Matyas Bohacek", "Hany Farid"], "abstract": "Full-blown Al-generated video generation continues its journey through the uncanny valley to produce content that is perceptually indistinguishable from reality. Intermixed with many exciting and creative applications are malicious applications that harm individuals, organizations, and democracies. We describe an effective and robust technique for distinguishing real from AI-generated human motion. This technique leverages a multi-modal semantic embedding, making it robust to the types of laundering that typically confound more low- to mid-level approaches. This method is evaluated against a custom-built dataset of video clips with human actions generated by seven text-to-video Al models and matching real footage.", "sections": [{"title": "Introduction", "content": "Generating human motion in computer-graphics animation is notoriously difficult because of the complexity of human dynamics and kinematics [19, 20, 30, 32] and because of the sensitivity of the human visual system to biological motion [31]. While motion capture has significantly improved the realism of complex human motion, gaps have remained.\nGenerative AI, unlike earlier model-based animation approaches, has emerged as an intriguing new genre for computer animation. While earlier versions of Al-generated video were things nightmares are made of (see, for example, \u201cWill Smith eating spaghetti\"\u00b9), recent advances have shown significant improvements in photorealism and temporal consistency.\nA recent study found that AI-generated faces are nearly indistinguishable from real faces [33], and AI-generated voices are a close second in terms of naturalness and identity [6]. There is reason to believe, therefore, that Al-generated videos may pass through the uncanny valley.\nWe have already started to see AI-generated images weaponized in the form of child sexual abuse material, non-consensual sexual imagery, fraud, and as an accelerant to disinformation campaigns [21]. There is no reason, therefore, to believe that AI-generated video will not follow suit.\nVideo deepfakes fall into two broad categories: impersonation and text-to-video. Although there are several different incarnations of impersonation deepfakes, two of the most popular are lip-sync and face-swap deepfakes. In a face-swap deepfake, a person's face in an original video is replaced with another [34], and in a lip-sync deepfake, a person's mouth region is modified to be consistent with a new voice track [45].\nBy contrast, text-to-video deepfakes are generated entirely from scratch to match a user-specified text prompt. They represent a natural evolution of text-to-image models (e.g., DALL-E, Firefly, Midjourney, etc.). Our focus is on detecting these text-to-video deepfakes, particularly those depicting human motion.\nWe describe a technique to detect AI-generated videos containing human motion. Our initial focus is on humans because these videos are of most concern when it comes to the harms enumerated above. Despite this focus, we will discuss why our approach is likely to generalize to other types of video content. We evaluate the efficacy of our approach on a diverse dataset of our creation consisting of real and matching (in terms of the human actions depicted) AI-generated videos from seven different generative-AI models. We demonstrate the robustness of our detection in the face of standard laundering attacks like resizing and transcoding, and evaluate its generalizability to previously unseen models.\""}, {"title": "Related Work", "content": "Identifying manipulated content (image, audio, video) can be partitioned into two broad categories: (1) active and (2) reactive. Active approaches involve inserting metadata or imperceptible watermarks at the time of synthesis to facilitate downstream detection [14]. These approaches are appealing for their simplicity but are vulnerable to counter-attack in which the inserted credentials can be removed [48] (although the extraction and centralized storage of a distinct digital signature - perceptual hash- can be used to reattach credentials).\nReactive techniques - operating in the absence of credentials - fall into two basic approaches: (2a) learning-based, in which features that distinguish real from fake content are learned by a range of different machine-learning techniques, and (2b) artifact-based, in which a range of low-level (pixel-based) to high-level (semantic-based) features are explicitly extracted to distinguish between real and fake content [21].\nThere is a rich literature of techniques for detecting AI-generated images [21] and a more nascent literature for detecting AI-generated voices [4, 8, 37]. The literature for detecting AI-generated or AI-manipulated videos has primarily focused on face-swap deepfakes [1, 23, 35], and lip-sync deepfakes [9, 10, 17].\nBecause text-to-video AI generation has only recently emerged as perceptually compelling, the literature on detecting these videos is more sparse. A recent example [46] leverages low-level features, but these tend to be vulnerable to compression artifacts, which in video - unlike standard image JPEG compression are highly spatially and temporally variable.\nAnother recent example [24] explores the potential of multimodal large language models (LLMs) for detecting AI-generated faces. In this approach, the authors prompt ChatGPT with an image and prompt like \"Tell me if this is an AI-generated image.\" This approach achieves an average accuracy of 75% (as measured by area under the curve, AUC). Although not particularly accurate, what is intriguing about this approach is that it points to a potentially semantic-level reasoning.\nRecent studies took a more direct semantic approach by leveraging a contrastive language-image pretraining (CLIP) representation [38]. In [16], the authors extract a CLIP embedding from real and AI-generated images and with only a linear SVM achieve detection accuracy ranging from 85% to 90% depending on the amount of image post-processing (cropping, resize, recompression).\nIn [25], the authors also exploit CLIP embeddings along with a range of transfer learning strategies to achieve detection accuracy between 95% and 98%; their classifiers show good but not perfect generalizability, achieving an accuracy between 86% and 89%."}, {"title": "Contribution", "content": "Motivated by, and building upon, earlier work, we describe new techniques tailored to detect AI-generated video containing human motion. Our work contributes to this nascent literature with the:\n(1) development of a task-specific CLIP embedding that outperforms generic CLIP embeddings (FT-CLIP);\n(2) development of a new unsupervised forensic technique that requires no explicit training (frame-to-prompt);\n(3) improvement in the generalizability of detection to previously unseen content and synthesis models;\n(4) extension from image- to video-based analysis; and\n(5) construction and release of a new dataset of real and AI-generated human motion."}, {"title": "Methods", "content": "We begin by describing the collection and creation of real and AI-generated video clips consisting of varying types of human motion, followed by a description of four CLIP-like, multi-modal embeddings and the underlying classification schemes used to distinguish the real from the fake."}, {"title": "Dataset", "content": "We generated 3,100 video clips from seven text-to-video Al models: BD AnimateDiff [28], CogVideoX-5B [49], Lumiere\u00b2 [3], RunwayML Gen3 [22], Stable Diffusion Txt2Img+Img2Vid [7], Veo\u00b3 [47], and VideoPoet [26]. The default generation parameters were used for each model. These AI-generated videos are 297 minutes in length constituting 254,632 video frames. See Figure 2 for representative examples.\nThese video clips depict 100 distinct human actions and vary in length from 2 to 10.7 seconds, in resolution from 512 \u00d7 512 to 2048 x 1152 pixels, and in orientation (landscape and portrait)."}, {"title": "Embeddings", "content": "Multi-modal embedding models map an image and its corresponding descriptive caption into a shared vector space, allowing comparisons between these modalities. These embeddings have been found to be effective across many computer vision and natural language processing tasks [41, 43].\nEach video in our dataset is represented as a sequence of video-frame embeddings, extracted using three off-the-shelf embedding models-CLIP [38], SigLIP [51], and JinaCLIP [27]-as well as a custom fine-tuned CLIP model.\n(1) The CLIP embedding model was trained on LAION-2B [39]. We used the smallest 512-D baseline model ViT-B/324.\n(2) The SigLIP embedding model was trained on WebLI [13], which is over 32% larger than LAION-2B. Unlike CLIP's softmax-based contrastive loss, SigLIP uses a sigmoid loss. We used the 768-D patch16-224 model variants.\n(3) The JinaCLIP embedding model was trained on LAION-400M [40] along with an additional set of 40 text-pair datasets. While CLIP's training optimized for text-image representation alignment, JinaCLIP was trained to jointly optimize text-image and text-text representation alignment. We used the base 768-D v1 model6.\n(4) We created a custom fine-tuned CLIP (FT-CLIP) embedding model from the baseline CLIP model described above."}, {"title": "Classification", "content": "We deploy three different classification strategies that, leveraging the embeddings enumerated in the previous section, make a prediction of a video frame being real or fake. The first two supervised classifiers are based on a support vector machine (SVM), and the third unsupervised classifier is based on a simple cosine similarity between text and frame embedding. In each case, a video, represented as a sequence of frame embeddings, is classified as fake if a majority of the frames are classified as fake.\nWe intentionally take a simple approach here instead of leveraging heavier-weight classifiers so as to place emphasis on the power of the multi-modal embeddings.\n(1) A two-class SVM [15], implemented in scikit-learn7, is used to classify video frames as real or fake, in which all seven text-to-video models are bundled into a single 'fake' class.\n(2) A multi-class SVM [15], also implemented in scikit-learn, classifies the source of each video frame across eight classes. One class represents real videos, and seven classes represent each of seven different text-to-video Al models.\n(3) The previous classifiers follow a typical supervised learning approach in which the SVMs are trained on a subset of the video frames and evaluated on the remaining frames. In this third frame-to-prompt approach, each video-frame embedding is compared \u2013 through a simple cosine similarity - to an embedding of one of two prompts (e.g., \"a real image\" and \"a fake image\"). A frame is classified by selecting the class (real/fake) with the largest cosine similarity."}, {"title": "Results", "content": "We now describe a pair-wise embedding-classifier performance for discriminating between real and AI-generated videos, followed by an evaluation of the robustness in the face of standard laundering attacks, and generalizability to synthesis models not seen during classifier training.\nFor the CLIP, SigLIP, and JinaCLIP embeddings, our dataset is randomly split into an 80/20 train/test partition. For the fine-tuned CLIP, the dataset is randomly split into a 40/40/20 partition, where the first 40% is used for CLIP fine-tuning, the next 40% is used for model training, and the remaining 20% is used for testing.\nIn each case, we report the mean frame- and video-level accuracy on the test set, averaged over five random train/test repetitions. Because our dataset is imbalanced, with significantly more fake than real videos, we under-sample the fake videos to create a balanced dataset. Throughout, we report accuracy as a macro-average by evenly weighting the class accuracies.\nTwo-class. Shown in the left portion of Table 1 is the frame- and video-level accuracy for the two-class SVMs with three different kernels: linear, radial basis function (RBF), and polynomial.\nAt the frame level, macro-accuracy ranges from a low of 79.0% to a high of 92.6%. For the CLIP embeddings (top three rows), the linear kernel is surprisingly more effective than the non-linear kernels. For the fine-tuned CLIP, the RBF kernel is the highest performer. Although performance is somewhat comparable across different embeddings, the fine-tuned CLIP offers the best performance. At the video level, accuracy across embeddings and classifiers is comparable, ranging from a low of 95.3% to a high of 99.2%.\nFor all embeddings and all classifiers, there is a slight fake bias in which fake content is correctly classified at a higher rate by, on average, 5.2 percentage points. For example, for the best performing model (FT-CLIP with a poly kernel), the video-level accuracy for fake videos is 99.9% as compared to 98.5% for the real videos.\nShown in Figure 3 is a visualization of seven pairwise, real-fake finetuned-CLIP embeddings, linearly reduced to a 2D subspace using PCA. Even in this reduced space, we see a good separation between the classes (the seven Al models are considered separately only for ease of visualization).\nMulti-class. Shown in the right portion of Table 1 is the frame- and video-level accuracy for the multi-class SVMs. At the frame level, accuracy slightly outperforms the two-class classifier, ranging from a low of 82.8% to a high of 91.5%. Generally speaking, the non-linear kernels (RBF and polynomial) outperform the linear kernel, and unlike the two-class, the SigLIP embedding now outperforms the other embeddings across all kernel functions.\nThere is a slight bias across all seven Al models with the Veo model consistently misclassified. For example, for the best-performing model (SigLIP with an RBF kernel), the difference between the best (VideoPoet) and worst (Veo) performing inter-class accuracy is 100% and 93.7%.\nAt the video level, accuracy across all embeddings and classifiers are comparable ranging from a low of 97.6% to a high of 98.5%.\nShown in Figure 4 is a PCA-based visualization of the pairwise fine-tuned CLIP embeddings between one text-to-video model"}, {"title": "Robustness", "content": "Whether intentional or not, videos subjected to a forensic analysis often undergo laundering in the form of changes in resolution and compression. Techniques leveraging low-level features are often highly vulnerable to this type of laundering because even these simple modifications obliterate distinguishing characteristics. Because the multi-modal embeddings are designed to extract semantic-level meaning, we expect these features to be more resilient to laundering.\nShown in the left portion of Table 3 are the frame- and video-level accuracies for the CLIP embedding and a two-class linear SVM for videos with progressively lower resolution (we quantify the change in resolution as a percentage because the original videos were of varying resolution, Section 2.1). At the video level, accuracy remains relatively high for videos at 50% or higher of their original resolution. This corresponds to an average resolution of 892 \u00d7 355."}, {"title": "Generalizability", "content": "Our frame-to-prompt classifier requires no training, which makes generalizability to new synthesis models more likely. Our two- and multi-class SVMs, however, do require training, and these types of supervised-learning models often struggle to generalize. To evaluate the generalizability of our SVMs, we performed a leave-one-out analysis in which two-class linear SVMs are trained on CLIP embeddings of six of the seven text-to-video models."}, {"title": "Non-Human Motion", "content": "Having been trained on only videos containing human motion, we wondered if our two-class SVM would generalize to arbitrary text-to-video models. We evaluated our model on 100 videos generated by Sora [44] and RunwayML [22] that did not contain any humans or human-motion. With the CLIP embeddings, the model correctly classified 97.1% of these videos as AI generated showing (a somewhat surprising) generalization to non-human motion and to one text-to-video model not seen in training (Sora).\nFor the frame-to-prompt model, the highest video-level accuracy obtained was for the fine-tuned CLIP embedding and P3 prompt at an accuracy of 97.5%, on par with the human-motion (see Table 2). This result suggests that our model has not learned something specific to Al-generated human motion but instead has learned something distinct about AI-generated content. From a forensic perspective, this is highly desirable.\nWe do not yet fully understand what specific properties of the multi-modal embeddings are distinct from AI-generated content. We speculate, however, that because generative-AI models rely on multi-modal embeddings to convert text prompts into images and videos, the extracted embeddings are distinct from those of real content. Another possibility is that generative-AI models may be prompted with distinct properties like level of descriptiveness, yielding more compact content as compared to real videos."}, {"title": "Talking Heads", "content": "We next wondered if our two-class SVM would generalize to a more constrained type of human motion in the form of face-swap and lip-sync deepfakes [21]. We evaluated our model on 100 real videos and 100 deepfake videos from the DeepSpeak Dataset [5]. These videos depict people sitting in front of their webcam responding to questions and prompts from which face-swap and lip-sync deepfakes were created. With the CLIP embeddings, the macro-average model accuracy is 55.8% heavily biased to classifying content as fake at a rate of 93.1% as compared to real at a rate of 15.2%. This"}, {"title": "CGI", "content": "Lastly, we wondered if our two-class SVM would generalize to non-AI-generated video in the form of CGI. We evaluated the trained two-class SVM on videos from the GTA-Human dataset [11]. We evaluated our model on 100 GTA-Human videos (cropped around the human movement at a resolution of 640 \u00d7 480 pixels). With the CLIP embeddings, the model correctly classified only 39.1% of these videos. Our model does not generalize to CGI.\nResult for the frame-to-prompt model were mixed. Averaged across all prompts (P*), accuracy for the CLIP embedding was 76.1%, as compared to 15.0% for JinaCLIP, and 48.9% for FT-CLIP.\nPerformance for SigLIP across all prompts was perfect at 100%. That is, the SigLIP embedding for these video frames is more similar to the \"AI-generated\u201d, \u201cunrealistic\u201d, or \u201cmanipulated\" prompts than the \"authentic\" or \"realistic\" prompts. We don't fully understand why there is such a large discrepancy here across embedding, but it may be possible that SigLIP - trained on WebLI was in fact exposed to CGI content."}, {"title": "Comparison to Related Work", "content": "Due to the lack of standardized benchmarks, comparing methods remains imperfect. Nevertheless, we provide a comparison to related work by comparing baseline accuracy as well as generalizability to unseen generative models. The latter is particularly important because true efficacy in the wild will be limited by the ability of detection models to generalize.\nWhen trained on videos from all four text-to-video Al models, the method described in [46] attains an AUC in the range 98.5 to 99.3. When one model is left out, the AUC drops to between 67.1 and 77.3, revealing relatively poor generalizability.\nWhen trained on videos from all models, our two-class linear SVM with CLIP embeddings attains an accuracy of 96.9%. When one model is left out, the accuracy drops to between 91.9% and 96.7%. Although base-level accuracy is comparable, our method generalizes to videos from unseen models better than earlier approaches.\nAlthough previous approaches have only focused on images, our frame- and video-level accuracy and generalizability are comparable or better than previous approaches [16, 24, 25] (Section 3.6). Our frame-to-prompt is particularly attractive because it requires no explicit learning, making deployment relatively straightforward."}, {"title": "Discussion", "content": "When we first started to think about the problem of distinguishing real from Al-generated human motion, we thought to take a physics-based approach leveraging recent advances in 3D human-body modeling [29]. We hypothesized that by extracting 3D models of the human body, we could expose implausible dynamics and kinematics in Al-generated motion. We, however, quickly ran into obstacles and found it difficult to consistently and reliably extract 3D models in a wide range of human poses and levels of occlusion.\nThis led us to take a more learning-based approach. Wanting to avoid exploiting low-level features [46] vulnerable to laundering, we looked towards more semantic-level features, which, as we have shown, are more resilient to laundering.\nAs with any authentication scheme, we must consider vulnerability to counter-attack by an adversary. Having already shown resilience to standard laundering, we will eventually need to consider a more sophisticated adversarial attack [12].\nIt has long been the goal - and challenge of media forensics to extract semantic-level features that can distinguish real from fake or manipulated content. While we cannot say for sure that CLIP embeddings of the form explored here capture truly semantic properties, their resilience to resolution and quality and their generalizability suggest semantic-like representations. This is particularly the case for the surprisingly high performance achieved by our frame-to-prompt approach in which a video-frame embedding is simply compared to a pair of text embeddings of the form \u201cauthentic image\u201d and \u201cAI-generated image.\u201d\nAs text-to-video models advance in photorealism and computational efficiency [50], complementary models for video-to-video generation and text-based video editing are also improving [2]. We should expect that the binary classification of real vs. fake will soon be complicated by hybrid videos that are partially AI-generated or AI-adjusted. Whether semantic methods, such as ours, can capture such subtleties remains to be seen."}]}