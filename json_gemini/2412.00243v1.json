{"title": "Realistic Corner Case Generation for Autonomous Vehicles with Multimodal\nLarge Language Model", "authors": ["Qiujing Lu", "Meng Ma", "Ximiao Dai", "Xuanhan Wang", "Shuo Feng"], "abstract": "To guarantee the safety and reliability of autonomous vehi-\ncle (AV) systems, corner cases play a crucial role in explor-\ning the system's behavior under rare and challenging con-\nditions within simulation environments. However, current\napproaches often fall short in meeting diverse testing needs\nand struggle to generalize to novel, high-risk scenarios that\nclosely mirror real-world conditions. To tackle this chal-\nlenge, we present AutoScenario, a multimodal Large Lan-\nguage Model (LLM)-based framework for realistic corner\ncase generation. It converts safety-critical real-world data\nfrom multiple sources into textual representations, enabling\nthe generalization of key risk factors while leveraging the\nextensive world knowledge and advanced reasoning capa-\nbilities of LLMs. Furthermore, it integrates tools from the\nSimulation of Urban Mobility (SUMO) and CARLA simu-\nlators to simplify and execute the code generated by LLMs.\nOur experiments demonstrate that AutoScenario can gener-\nate realistic and challenging test scenarios, precisely tai-\nlored to specific testing requirements or textual descrip-\ntions. Additionally, we validated its ability to produce di-\nverse and novel scenarios derived from multimodal real-\nworld data involving risky situations, harnessing the pow-\nerful generalization capabilities of LLMs to effectively sim-\nulate a wide range of corner cases.", "sections": [{"title": "1. Introduction", "content": "Currently, the safety of autonomous vehicles (AVs) remains\na critical barrier to their widespread deployment on pub-\nlic roads. Discovering and testing corner cases in advance\nhelps secure AV safety and accelerating development cy-\ncles. However, as AV performance improves, further ad-\nvancements become increasingly difficult. This is due to\ncorner cases emerging less frequently and exhibiting greater\ndiversity [28]. As a result, defining and identifying the most\nrelevant corner cases has become increasingly critical for\nachieving further performance gains.\nLanguage descriptions provides a natural solution to bridge\nthis gap, making scenario-based testing more practical and\naccelerating performance evaluation of AVs.\nHowever, building such text-conditioned generation\nmechanism is challenging as it requires modeling every-\nthing from static environment elements to agent behav-\niors while mapping narrative language to detailed config-\nurations. The rise of LLMs and Vision-Language Models\n(VLMs), trained on vast internet data, offers a promising\napproach, as it has been shown exceptional capabilities in\nlearning, reasoning, and complex problem-solving. Their\napplications span fields like medicine, education, finance,\nand engineering [7, 19, 22, 27], showcasing significant ad-\nvancements.\nDriven by these advancements and the need for realis-\ntic and diverse safety- critical scenarios, we developed Au-\ntoScenario, a fully automated pipeline with high controlla-\nbility as shown in Fig. 1. It generates realistic and diverse\nscenarios containing main components that closely mimic\nreal-world environments through prompt engineering and\nthe integration of tools from SUMO, an open-source traf-\nfic simulation package [29], CARLA[13], an open-source\nsimulator powered by Unreal Engine [14] with high-fidelity\ndigital assets, and data-driven deep learning models.\nOur contribution can be concluded as:\n\u2022 We propose AutoScenario, a framework that automates\nthe safety-critical scenario generation pipeline while pro-\nviding a high degree of controllability.\n\u2022 Multimodal real-world corner cases are efficiently lever-\naged to enhance the realism of generated scenarios while\npreserving key risk factors.\n\u2022 We utilize large language model to generalize scenarios\nthrough reasoning and open-world knowledge and em-\nploy simulation tools to increase the stability and realism\nof generated scenarios."}, {"title": "2. Related works", "content": ""}, {"title": "2.1. Safety-Critical Driving Scenario Generation", "content": "The widespread deployment of autonomous vehicles is pri-\nmarily hindered by safety concerns. Significant efforts have\nbeen devoted to identifying and mitigating unsafe compo-\nnents through testing [15, 49, 50]. Scenario-based test-\ning has demonstrated its potential for efficiently evaluat-\ning autonomous vehicles under corner case conditions [31].\nNevertheless, the generation of realistic and plausible cor-\nner cases remains a substantial challenge, primarily due to\nthe inherent complexity of physical environments and traf-\nfic conditions encountered in the real world. An approach\nto address the realism challenge involves replaying driv-\ning data collected from real-world scenarios; however, this\nmethod falls short in creating realistic interaction with av\nbeing tested.\nSignificant efforts have been made in creating chal-\nlenging corner cases, broadly classified into two main\napproaches: data-driven generation and knowledge-based\ngeneration. Data-driven models leverage information from\ncollected datasets [25, 38, 47]. For example, NeuralNDE\n[46] employs a Transformer-based network with safety\nmapping to generate realistic agent behaviors, achiev-\ning distribution-level similarity to real-world distributions.\nSTRVE [35] learns a graph-based conditional VAE as traf-\nfic prior, optimizing each agent's behavior to provoke colli-\nsions with a rule-based AV planner. RealGen [12] uses an\nencoder-decoder architecture and retrieval-based in-context\nlearning to synthesize realistic traffic scenarios. However,\nthese methods are limited to generating scenarios derived\nfrom existing datasets, lacking the capability to produce\ncontrollable, specialized scenarios tailored to specific test-\ning objectives.\nKnowledge-based generation is another approach that\nintegrates external knowledge into the scenario generation\nprocess, reflecting a growing trend in the machine learn-\ning field. Klischat and Althoff [20] utilized an evolutionary\nmethod to minimize the drivable area by manipulating sur-\nrounding vehicles. Shiroshita et al. [37] emphasized the im-\nportance of diversity and high driving skills within scenar-\nios, proposing a distinct policy set selector within their re-\ninforcement learning method to balance these two aspects.\nDing et al. [11] explicitly incorporated domain knowledge\nby representing it as first-order logic in a tree structure to\nachieve Semantically Adversarial Generation (SAG). How-\never, these knowledge-based methods generally suffer from\nlimited realisim."}, {"title": "2.2. Scenario Generation with LLMS", "content": "Multimodal LLMs have seen extensive application in au-\ntonomous driving systems since their inception [8, 9, 41,\n45]. Specifically, LLMs are increasingly employed for gen-\nerating diverse and realistic scenarios [6, 24, 26], which are\ncrucial for testing and evaluating autonomous vehicles. For\ninstance, ChatScene [48] uses LLMs to generate scenarios\nfrom a pre-existing Scenic library [16], while ChatSim [44]\nproduces photo-realistic 3D driving simulations using exter-\nnal digital assets. However, both methods face limitations\nin scene diversity: ChatScene is constrained by its fixed li-\nbrary, and ChatSim can only modify existing scenes, unable\nto create new ones from scratch.\nLarge language models (LLMs) are also being utilized\nfor generating corner cases. CRITICAL [40] uses LLMS\nto refine critical cases by updating scenario configurations\nfor autonomous vehicle training. However, its evaluation\nis limited to freeway scenarios [21], restricting the gener-\nalizability across different traffic conditions and road lay-\nouts. LEADE [39] incorporates an LLM-enhanced adap-\ntive genetic algorithm to search for safety-critical scenar-"}, {"title": "3. Method", "content": "In this section, we delineate the key method of AutoSce-\nnario, the multimodal LLM-driven tool for converting mul-\ntimodal input to corner case generation. We begin by in-\ntroducing concise notation for key components. Then we\nwill introduce the overall pipeline for the whole system, fol-\nlowed by an explanation for the key design of the general-\nized interpreter. Lastly, we introduce how the corner cases\nare generated by grouping these components together."}, {"title": "3.1. Notation", "content": "We model the entire process as an encoding-decoding\nframework, where the input-regardless of modality is\nfirst encoded into a universal, interpretable language space.\nIt is then decoded using multiple codes that direct simula-\ntion tools to precisely reconstruct the specific scenes.\nLet $S_E$ represent a real-world scenario that encom-"}, {"title": "3.2. Pipeline", "content": "The generation framework is illustrated in Fig. 2. The\npipeline accepts multimodal inputs, which includes but not\nlimited to: user request expressed in the text, image taken\nfrom random viewpoints, videos from a driving vehicle's\nperspective. They are sent through specially designed inter-\npreter powered by LLM to generate standard scenario de-\nscriptions, which extracts key components from the input\nwhile adding diverse details. See more details in Section\n4.2.\nMultimodal inputs are preprocessed with tailored atten-\ntion mechanisms to generate consistent descriptions from\nthe provided real-world information. For short user re-\nquests, we expand them to produce more detailed de-\nscriptions. For longer texts with a fixed narrative style,\nsuch as crash reports, we restructure the information into\nfour perspectives:$E_{road}$, $E_{static}$, $E_{agents}$, $E_{weather}$. For\nimages, we use Chain-of-Thought (CoT) [43] to extract lay-\nered, risk-related information. For videos, we downsample\nthe footage and utilize memory-based processing to recon-\nstruct the entire network and motion continuity from the in-\nput scenario. See detailed prompts in the Appendix.\nRoad structure plays a crucial role in scenario genera-\ntion and in identifying risk factors critical to corner case\ngeneration. To achieve this, two approaches are employed.\nIn the first approach, $E_L$ generated by interpreters are used\nto invoke the net generator, which produces the network\nin XML format. Alternatively, real-world road geometry\nis retrieved from OpenStreetMap [3] using GPS input and\nconverted into the net.XML format. The agent generator\ncreates a set of agents based on $E_{agents}$, which includes a\ndiverse range of road users, such as pedestrians, cyclists,\nand various types of vehicles including trucks and passen-\nger cars. LLMs are used to place agents within the scenario\nand assign appropriate speeds. Then, closed-loop simula-\ntions are conducted with a data-driven agent model to repli-\ncate human-like behaviors and interactions under the given"}, {"title": "3.3. Tools in the chain", "content": "As illustrated in 3, SUMO, CARLA, and data-driven mod-\nels are seamlessly integrated with the help of LLM to gen-\nerate the final scenario.\nNet Generator Road geometry is a well structured ob-\nject that can be represented by graph with nodes and edges.\nTo reduce the error rate for pure generation, we prompt\nLLM to generate SUMO compatible Node and Edge file\ndefined in XML format, then convert to full graph with do-\nmain knowledges, i.e. rules with SUMO tools. See Fig. 3\nfor an illustration and more detailed examples in Section 4.\nThis design is not confined to SUMO or its XML formats.\nSince road network naturally represents a graph structure,\nit can be represented by other structured languages [1] and\nprocessed by graph tools [18], compatible to other simula-\ntors such as MATSim [2].\nScenarioGenerator We use LLM to directly gener-\nate Python code that controls 3D scenario agents via the\nCARLA Python API. Blueprints from CARLA's library are\nautomatically utilized to construct diverse road users and\nsimulate varying weather conditions. Digital Twin Tool is\nalso employed to render the scenario realistically, with all\ncritical risky factors generated automatically through the\nAPI.\nAgentBehavior We use a trained data-driven behavior\nmodel to replicate human-like actions in the given scenario,\nonce the state at a critical moment is generated based on the\nuniversal scene description $E_\u03b9$."}, {"title": "4. Experiments", "content": "In this section, we evaluate the performance of our Au-\ntoScenario framework both quantitatively and qualitatively.\nFirst, we examine its ability to generate realistic, diverse,\nand controllable scenarios. Next, we demonstrate its appli-\ncation in creating safety-critical scenarios from multimodal\ninputs for AV testing, and additionally, we quantitatively\nassess the challenges posed by these scenarios through the\nperformance of LLM-based AVs. Finally, we highlight\nthe key components of the framework through an ablation\nstudy."}, {"title": "4.1. Realism, Diversity and Controllability of Gen-\neration", "content": "In our experiments, we observed that with proper prompt-\ning, the whole system displays high level of controllabil-\nity and diversity in generated scenarios. Fig. 5 listed two\nsamples with image interpreter. The interpreter effectively\nidentified critical scene elements, including road network\nconfigurations, vehicle count and color, and obstacles such\nas construction cones. This information is then seamlessly\ntranslated into the generated 3D scene by the components\ngenerator and the scenario generator.\nTo further systematically evaluate the diversity and fi-\ndelity of generation, we define a set of metrics to capture the\nquality of the two main components inside the generation\nprocess, both the interpreter and the generator: 1) Confor-\nmity metrics, which measure the alignment between the text\ndescription and the generated scenario, including network\nstructure, objects, agents, and intermediate codes. Success\nrates assess the likelihood that these generated codes are\ncorrectly recognized by SUMO and CARLA (2) Diversity\nmetrics, which evaluate the diversity of scenarios attributes\nacross generated scenarios. Additional experimental details\nand metric descriptions are provided in the appendix.\nAs shown in Table 1, AutoScenario exhibits high suc-\ncess rates in producing meaningful scenarios. Additionally,\nthe system achieves high accuracy in generating the spec-\nified number and color of vehicles, as well as the type of\nroad obstacles, according to descriptions generated by the\ninterpreter in most cases. The main causes of failures in-\nclude incorrect formatting of keywords (e.g., extra \"#\" char-\nacters) in network generation and blueprint name reusing\nerrors (e.g., \"vehicle.omni.vehicle.omni.bus\"). Given the\ncomplexity of the prompt, these success rates and accuracy\nlevels represent quite favorable results.\nAnother feature of our system is the diversity and com-\nplexity of generated scenarios. To evaluate road network\ncomplexity, we calculate the mean and standard deviation\nof the total number of lanes, edges, and route lengths in\nthe generated road networks. As shown in Table 2, these\nmetrics span a wide range across each scenario set, demon-\nstrating the consistent diversity of the generated outputs."}, {"title": "4.2. Generation of corner cases using diverse input\nsources", "content": "Leveraging multimodal interpreters, AutoScenario gener-\nates safety-critical scenarios from diverse input sources, in-\ncluding text, images, and videos. The output from AutoSce-\nnario is also diversified by highlighting the key components\nof a conflict scenario while generalizing the rest.\nOne application of the text interpreter is to reconstruct\ncritical moments leading up to a crash based on crash re-\nports. Two examples of this are shown in Fig 4, highlighting\nsafety-critical interactions between vehicles and between\nvehicles and vulnerable road users. Additionally, AutoSce-\nnario supports user testing requests that describe scenarios\nat an abstract level, as demonstrated in Fig in 7.\nA properly prompted VLM is used as the image inter-\npreter to convert the input image into a scenario description.\nIt carefully considers the four key perspectives of the sce-\nnario. Next, tools in the chain 3.3 are employed to convert\nthe scenario description into a simulation scenario. During\nour experiment, we found that the performance of the inter-\npreter was limited in complex scenes-specifically, those\nwith numerous buildings and vehicles-resulting in signif-\nicant deviations in the extraction of road network features.\nTo address this, we introduced an enhanced prompt that en-\nabled the model to analyze the road network more effec-\ntively by leveraging the surrounding buildings and parked\nvehicles to infer the geometric structure of the road network.\nThis modification led to an improvement in the model's per-\nformance in complex scenes. The process of scenario gen-\neration is depicted in Fig 6. More details are presented in\nthe appendix.\nAdditionally, when GPS data is provided, we can gen-\nerate testing scenarios based on the text description while\nincorporating real-world road structures. This approach en-\nables effective testing for deployment in this region, reflect-\ning realistic traffic conditions, as demonstrated in Fig 8.\nA VLM-based video interpreter is developed to extract\nroad information and environmental features from the input"}, {"title": "4.3. Ablation Study", "content": "The ablation study of AutoScenario is presented in Table 3.\nIn this study, we examine the key design choices across the\nthree main steps: the Interpreter, the Components Genera-\ntor, and the Scenario Generator, all in relation to the gener-\nation process.\nAfter removing the interpreter, which generates detailed\nnarrative descriptions for each scenario, AutoScenario is"}, {"title": "5. Conclusion and Future work", "content": "We present a scenario generation framework that integrates\nLLMS, VLMs, and data-driven models. This is the first\nsystem to seamlessly translate multimodal real-world data\ninto simulated scenarios, offering a highly controllable and\nflexible simulation tool. Transferring and generalizing risky\nscenarios from the real world to simulators like CARLA, in\nterms of both objects and behaviors, is a foundational step.\nIn the future, we aim to enhance photorealism using 3DGS\nor diffusion models."}, {"title": "A. Supplementary Experiments", "content": ""}, {"title": "A.1. Diversity in generated scenarios", "content": "We select three representative scenario types for demon-\nstration: General Scenarios, Intersections, and Construction\nZones, as these are areas where corner cases are more likely\nto occur. For each scenario type, five input images are pro-\ncessed through the AutoScenario pipelines. Each input im-\nage is then diversified into 10 distinct testing scenarios.\nTo evaluate the diversity of the generated scenarios, we\nassessed network diversity using metrics such as the number\nof road users (including vehicles, pedestrians, and cyclist,\netc.), the number of static objects (like construction cones\nor warning signs), the shortest distance between agents in\neach scenario, and the yaw angles of vehicles generated by\nAutoScenario, ranging from -180\u00b0 to 180\u00b0. We calculated\nthe mean and standard deviation of each metric to provide a\ncomprehensive measure of diversity within the scenarios.\nAs shown in Table 4, the standard deviation values in-\ndicate a wide distribution range for each metric, reflecting\nsubstantial diversity. Compared to the General and Con-\nstruction Zone scenarios, the Intersection area contains the\nhighest number of agents, which is reasonable since inter-\nsections typically have vehicles approaching from multiple\ndirections. The Construction Zone scenarios have the most\nstatic objects, consistent with their nature. To simulate cor-\nner cases, the shortest distances between agents in all three\ntypes of scenarios tend to be around 4 to 5 meters. In the\nGeneral scenarios, most selected scenarios involve straight\nroads without intersections, resulting in a mean vehicle yaw\nangle close to 0\u00b0. In contrast, the other two scenarios in-\nvolve more complex situations such as lane changes, turns,\nand merges, leading to an average vehicle yaw angle of ap-\nproximately 15\u00b0. Additionally, the standard deviation of ve-\nhicle yaw angles tends to be around 90\u00b0, which is expected\nbecause most road intersections in reality are at 90\u00b0 angles."}, {"title": "A.2. Challenging scenarios created by AutoSce-\nnario", "content": "We compare scenarios generated by AutoScenario with\nthose created by randomly placing traffic vehicles within\nthe same scenario using RandomTrip from SUMO tools.\nBoth types of scenarios are executed on road networks de-\nrived from real-world images. This comparison enables us\nto evaluate the effectiveness and realism of AutoScenario in\nreplicating real-world conditions more accurately. See Fig\n11 for the comparison pipeline.\nThe performance of an AV serves as an indicator of the\ndifficulty level of the generated scenarios. Lower perfor-\nmance suggests that the scenarios are more challenging. To\nevaluate the AV's effectiveness, we utilize widely adopted\nperformance metrics that account for driving sophistication\nand task completion levels. The driving score is calculated\nas a weighted combination of ride comfort, driving effi-\nciency, and driving safety. The route completion value is de-\nfined as the ratio of the distance traveled by the driver agent\nto the total length of the predefined route. The total score\nis calculated by multiplying the driving score by the route\ncompletion. For more details, see Limism++. In our exper-\niments, we utilized five different road networks, with each\nnetwork generating five distinct initial vehicle positions us-\ning both our proposed method and RandomTrip. Beyond\nthe evaluation metrics mentioned earlier, we also recorded\nthe number of collisions across these 25 experiments to de-\ntermine the collision rate metric. As shown in Table 5, Au-"}, {"title": "A.3. Similarity between generated scenario and\noriginal input", "content": "To validate the similarity between generated scenario and\noriginal input image, a pipeline is designed to automated\ncompare both in the text space generated by the VLM in-\nterpreter. See Fig 13 for a detailed example. In addition\nto comparing general scene descriptions, we further break\nthem down into main component descriptions and conduct\na detailed comparison for each. Moreover, leveraging the\nability to manipulate scene views in CARLA, we selected\ntwo distinct perspectives for the generated simulated scene:\nthe bird's-eye view (BEV) and the ego vehicle view. These\nperspectives were used for cross-checking the similarity be-\ntween the generated scene and the input image.\nAs shown in Table 6, the generated scenarios exhibit a"}, {"title": "A.4. Online learning for AV with corner case", "content": "For the risk scenarios identified through experiments in sec-\ntion A.2, we extract the prompt inputs for collision scenar-\nios and the corresponding decision outputs from the LLM.\nThese are then used as examples to refine the prompts for\nthe LLM-based AV. Following each collision example, we\nincorporate human suggestions, such as decelerating earlier\nor switching to a safer lane, to improve decision-making\nand safety. We then retest the AV in exactly the same 25\nscenarios as those in section A.2. See table 7, by empha-\nsizing the corner cases, the performance gets improved in"}, {"title": "B. Prompt Examples", "content": "We carefully designed prompts for each component in Au-\ntoScenario to fully leverage the capabilities of multimodal\nLLMs. Overall, each prompt comprises several compo-\nnents: a system prompt summarizing the task, detailed steps\nto guide the generation process, including constraints and\nexamples (narrative descriptions with code snippets), and\nthe specified format for the desired output. Fig 14 and Fig\n15 demonstrates the prompt snippets used in the VLM in-\nterpreter and video interpreter. For the video interpreter, in\naddition to prompts similar to those used in the VLM Inter-\npreter, it incorporates code snippets to assist in tracking the\nego vehicle's movement. Fig 16 illustrates the prompt used\nfor agents and objects generator in AutoScenario, while Fig\n17 demonstrate the prompt designed for scenario generator.\nBased on our experiments and quantitative evaluations\nconducted in the ablation study, prompts are shown to sig-\nnificantly enhance the reasoning and generation capabilities\nof LLMs."}, {"title": "C. Experiment details", "content": "All our experiments are conducted on one NVIDIA RTX\n3090, leveraging the online version of GPT-4 as the mul-\ntimodal LLM alongside a pretrained data-driven model for\nsimulating agent behavior."}, {"title": "D. More examples", "content": "Here, we present two additional results derived from crash\nreports involving conflicts between vehicles, shown in Fig\n18."}]}