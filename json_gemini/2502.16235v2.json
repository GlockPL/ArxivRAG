{"title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning", "authors": ["Yifu Ding", "Wentao Jiang", "Shunyu Liu", "Yongcheng Jing", "Jinyang Guo", "Yingjie Wang", "Jing Zhang", "Zengmao Wang", "Ziwei Liu", "Bo Du", "Xianglong Liu", "Dacheng Tao"], "abstract": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by structuring problem-solving as a spanning tree. However, recent methods focus on search accuracy while overlooking computational efficiency. The challenges of accelerating the ToT lie in the frequent switching of reasoning focus, and the redundant exploration of suboptimal solutions. To alleviate this dilemma, we propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework that aims to dynamically optimize the reasoning path in inference. It includes the Parallelism Streamline in the generation phase to build up a flexible and adaptive parallelism with arbitrary paths by fine-grained cache management and alignment. Meanwhile, the Search and Transition Mechanism filters potential candidates to dynamically maintain the reasoning focus on more possible solutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with Math500 and GSM8K datasets show that DPTS significantly improves efficiency by 2-4\u00d7 on average while maintaining or even surpassing existing reasoning algorithms in accuracy, making ToT-based reasoning more scalable and computationally efficient.", "sections": [{"title": "1 Introduction", "content": "The advent of OpenAI-01 (Jaech et al., 2024), a reasoning large language model, has sparked significant interest in the academic community. A key factor behind its success is the Chain-of-Thought (CoT)-based reasoning technique (Wei et al., 2022; Chu et al., 2023), which improves model's reasoning ability by breaking complex problems into explicit intermediate steps. Building upon this, the Tree of Thoughts (ToT) (Yao et al., 2023) framework has been introduced to further elevate LLMs' reasoning capacities, and be widely used in multi-step reasoning tasks (Plaat et al., 2024). ToT restructures reasoning as a tree search process and employs search algorithms, such as Monte Carlo Tree Search (MCTS) (Chaslot et al., 2008; Xie et al., 2024), to construct a tree-like structure that explores various reasoning pathways, leading to more refined and accurate responses (Sprueill et al., 2023). However, current ToT approaches predominantly focus on improving search accuracy while overlooking computational efficiency (Xie et al., 2024; Cheng et al., 2024; Zhao et al., 2024). We conclude two significant challenges that complicate the acceleration of ToT.\nThe first challenge arises from the frequent switching of reasoning focus in traditional sequential tree search, making it difficult to effectively parallelize (Snell et al., 2024). Unlike conventional deep learning inference that is compatible with end-to-end parallelism, tree search has irregular computational trajectories. As shown in Figure la Challenge 1, it frequently switches among paths, including retrospect and recursion behaviors which complicates the parallelism (green trajectory). For example, if parallelizing nodes 6 and 2 in one generation, the different context lengths and KV cache require additional processing. Moreover, frequent switching also tends to yield shallow exploration (Wang et al., 2025). Due to the limited time and memory budgets, more explored paths mean less exploitation in deep paths.\nThe second challenge stems from the redundant explorations on suboptimal solutions (Li et al., 2024b; Besta et al., 2024). Previous tree search methods fail in identifying the less potential path and terminating it timely (Wan et al., 2024a). Methods like MCTS attempt to balance the exploitation and exploration paths during node selection, but the selected nodes continue to roll out till the termination conditions (time or token limits) even with small prior confidence (Xie et al., 2024). For example, dark nodes in Figure la Challenge 2 have higher confidence. Node 3 with a lower confidence than node 5 will be explored earlier (pink trajectory). However, we observe that paths with low prior confidence have less probability of reaching the optimal solution, as illustrated in Section 3.2.\nTo address these challenges, we propose a novel and efficient tree search framework, DPTS (Dynamic Parallel Tree Search). This framework implements parallelized tree search and optimizes it by dynamically adjusting reasoning focus during the tree growing, thereby improving computational efficiency. It consists of two key components for both the generation and selection phases. (1) DPTS implements a Parallelism Streamline tailored for LLM reasoning in the generation phase. It facilitates the rollout for arbitrary paths in parallel, allowing the expanded nodes to be rearranged at each step. Additionally, we carefully engineer cached data collection and context alignment, paving the way for parallelized inference with varying path length and node selection. (2) Building on this, to prevent excessive exploitation and focus the reasoning on more potential paths, we introduce a Search and Transition Mechanism in the selection phase. It dynamically balances the exploitation-exploration paths by the bidirectional transition, i.e., $Early Stop (Exploitation \\rightarrow Exploration)$, Deep Seek (Exploration \\rightarrow Exploitation), allowing the model to focus on the most promising solutions and mitigate inefficient exploitation on suboptimal solutions.\nOur work provides valuable insights into accelerating the ToT for LLM reasoning, paving the way for future work to solve real-world challenges. Our contributions can be concluded as follows:"}, {"title": "2 Related Work", "content": "Reasoning with LLMs. LLMs have evolved from System 1 tasks (e.g., translation) (Brown et al., 2020) to System 2 reasoning (e.g., math, logic) (Kojima et al., 2022). CoT (Wei et al., 2022) enhances multi-step reasoning, with variants like Self-Consistent CoT (Wang et al., 2022), but its exploration scope remains constrained, limiting its effectiveness. (Chu et al., 2023). Furthermore, ToT (Yao et al., 2023) enables multi-path exploration, leveraging MCTS (Chaslot et al., 2008) for backtracking and heuristic rollouts (Wan et al., 2024b; Wang et al., 2024). However, MCTS remains computationally expensive, with limited work on acceleration methods.\nLLM Inference Acceleration. While LLM inference has been optimized for linear decoding (Lin et al., 2024), tree-structured reasoning remains underexplored (Li et al., 2024a). Approaches like Deft (Yao et al., 2024) optimize prefix sharing, while others use self-consistency for early stopping (Li et al., 2024b). Efficient tree search for LLM reasoning remains an open challenge.\nDue to page limit, we have included a more detailed discussion of related work in Appendix D."}, {"title": "3 Method Rationale", "content": "In this section, we present empirical findings that highlight the key challenges of tree search in LLM and provide the rationale behind our proposed DPTS. First, the frequent switch between paths complicates parallel execution and causes shallow thinking, disrupting the model's ability to engage in efficient deep reasoning (Sec. 3.1). Second, excessive exploitation of low-confidence paths results in redundant rollouts and wastes effort on fewer possible candidates (Sec. 3.2)."}, {"title": "3.1 Frequent Switching", "content": "Tree search inherently exhibits retrospective and recursive behaviors, making efficient parallel execution difficult. Even if each node is constrained to generate the same number of tokens, the focus switching between different reasoning trajectories and the diverse path lengths makes it incompatible with the end-to-end parallelism on GPUs. The detailed illustrations for this phenomenon can be found in Appendix A.2.\nThe focus switching between paths also makes the tree search fail in focused reasoning trajectory (Wang et al., 2025), which prevents deep thinking and leads to a tendency of shallow exploitation. We quantify the switch times of the reasoning focus on each sample in the Math500 dataset. Figure 2 counts the total switch, which is about 35 on average. As well as the switch from the best path to a suboptimal or incorrect one, which is up to 3 times for a single sample. It demonstrates the instability of the tree search algorithm in maintaining a focused reasoning trajectory."}, {"title": "3.2 Redundant Exploration", "content": "The lack of early termination in existing tree search algorithms leads to excessive exploitation and redundant searching. Observations in Figure 3 show that low-confidence nodes rarely contribute to the best solutions, either terminated with suboptimal results (yellow) or failing to be the first to reach the best path (orange). The average probability of the suboptimal results brought by low confidence is 91.3%, while the probability of those nodes being the earliest best path is only 6.2%. It suggests that low-confidence nodes have little potential to reach the best solution, it is even hard to be the first one. It means that most low-confidence nodes have less contribution to the final results but waste computational resources.\nThese findings emphasize the importance of maintaining the focus on deep reasoning and pruning low-confidence paths for efficient inference."}, {"title": "4 Proposed Method", "content": "To address the aforementioned challenges, we propose an innovative framework that allows for efficient reasoning, termed Dynamic Parallel Tree Search (DPTS). In the generation phase, the Parallelism Streamline in Sec. 4.1 supports fine-grained and flexible paralleled expansion for arbitrary paths. In the selection phase, the Search and Transition Mechanism in Sec. 4.2 enables less redundant exploration by identifying the highly potential solutions to focus reasoning."}, {"title": "4.1 Parallelism Streamline", "content": "As illustrated in Figure 4, We fully parallelize the tree search process in our framework with three main components: Tree Structure Building, KV Cache Handling, and Adaptive Parallel Generation. Each component is designed to optimize memory usage and parallel execution during the reasoning process."}, {"title": "4.1.1 Tree Structure Building", "content": "The tree search framework relies on a tree structure where each node represents a reasoning state. Specifically, the node data structure includes the following elements:\n\u2022 Node ID: A unique identifier for each node.\n\u2022 Parent Node: A reference to the parent node, establishing the hierarchical structure of the tree.\n\u2022 Prior Confidence: The confidence of the node, based on prior knowledge and model predictions.\n\u2022 Key-Value Cache (KVn): The key-value cache specific to this node, storing intermediate results during the reasoning process.\n\u2022 Token Sequence ($Seq^{1\\sim n}$): The complete token sequence from the root node to the current node, representing the reasoning path taken so far.\nThe key challenge lies in managing the KV cache (Floridi and Chiriatti, 2020). Instead of storing the entire sequences, each node only retains its own KV cache. This significantly reduces memory usage, particularly when dealing with a large number of nodes in the tree. By keeping each node's cache isolated, we avoid redundant memory usage while ensuring that each node has necessary information to continue reasoning process."}, {"title": "4.1.2 KV Cache Handling", "content": "The KV cache for each node is stored separately, and during parallel execution, these caches need to be collected and concatenated for efficient parallelism. The key challenge is that tree search paths have varying lengths, which means that both the KV caches and the input sequences for different nodes will vary in size and be hard to parallel.\nTo address this, we use a simple but straightforward padding technique to ensure that all sequences have consistent lengths before being processed. Specifically, for nodes with shorter KV caches, we apply left padding with zeros. Similarly, input sequences are padded with a predefined padding token to match the longest sequence in the batch. This padding ensures that all nodes are processed in parallel with consistent sequence lengths and corresponding KV cache, allowing for efficient batch processing across the tree search. The details of padding and concatenating are given in Appendix Eq. (3) and (4)."}, {"title": "4.1.3 Adaptive Parallel Generation", "content": "To further utilize the computational resources, we introduce an adaptive parallelism queue, which dynamically adjusts the number of parallel paths based on the available GPU memory. The parallelism queue size, denoted |P|, is used to restrict the number of exploitation and exploration paths in Sec. 4.2.1. It is calculated by the available and the peak memory usage during previous generations:\n$|P| = \\frac{O_{max} - O_{init}}{O_{peak} - O_{init}} \\qquad(1)$\nwhere $O_{max}$ is the total memory budget, $O_{peak}$ represents the peak memory usage from the previous generation, and $O_{init}$ is the memory consumption during model initialization. As the tree grows, the memory occupation of intermediate results continues to increase even with KV cache cleaning. Since memory overflow is one of the termination conditions, it is important to adaptively adjust the parallel number, preventing excessive memory allocation and early termination.\nAfter the generation phase, the newly generated sequences and KV caches are stored based on the tree width. The sequences for each node are completely stored, while the KV caches are stored partially with only the new tokens generated at this step (details are provided in Appendix 5). The new nodes are then added to the candidate node pool N, where they will be available for subsequent selection processes in the tree search.\nIn summary, our Parallelism Streamline is a well-structured streamline to optimize both memory usage and parallel execution. The overall process is showcased in Algorithm 1. More details can be found in Appendix B.1 due to the limited length."}, {"title": "4.2 Search and Transition Mechanism", "content": "In this section, we introduce the Search and Transition Mechanism in DPTS, which is a hybrid search algorithm that balances exploitation and exploration through separate management and dynamic conversion."}, {"title": "4.2.1 Search", "content": "The Search Mechanism aims to balance exploration and exploitation by dynamically partitioning the nodes in parallel queue P into two categories: explore nodes and exploit nodes.\nAs illustrated in Figure 4 (left), these nodes are selected from the candidate node pool N. The partition ratio p can be manually adjusted according to the task and memory budget. At initialization, the top p|P| highest-scoring nodes are assigned as exploitation nodes, while the remaining (1 \u2013 p)|P| nodes are assigned as exploration nodes. While during searching progress, the proportion of the two types of nodes dynamically fluctuates based on the transition mechanism in Sec. 4.2.2. The primary distinction between these two nodes lies in their origins and roles during the search process.\nExploitation Nodes The exploitation nodes are primarily inherited from parent exploitation nodes, focusing on refining the most promising paths in the search space. When a child node's confidence exceeds a predefined threshold, it inherits the status of its parent exploitation node and continues that path. This inheritance ensures that the most promising paths are deepened and further refined. Additionally, when the number of exploitation nodes falls below a predefined threshold, new high-confidence candidate nodes from the pool N are selected to fill the gap, ensuring that the number of exploitation nodes remains adequate for the search process. This strategy enables the exploitation of high-potential paths while maintaining the focus on areas with high confidence.\nExploration Nodes In contrast to exploitation nodes, the exploration nodes are not inherited from previous nodes but are dynamically selected from the candidate nodes pool. These nodes are responsible for discovering new paths that may have high potential but low current confidence in the search space. At each reasoning step, the exploration nodes are reselected from the candidate pool N, choosing the highest-confidence nodes that are not already assigned as exploitation nodes. The dynamic re-selection of exploration nodes allows the search process to adapt to changing circumstances and uncover new regions of the search space that may lead to better solutions."}, {"title": "4.2.2 Transition", "content": "While the Search Mechanism ensures a balance between exploration and exploitation, the redundant issue is not entirely mitigated. One example is that the initial exploitation nodes are not guaranteed to be the optimal solution. However, they only stop exploiting till reach the termination condition. Another issue occurs when high-confidence nodes are assigned as exploration nodes, but they will wait for the computation resources and do not roll out till the previous paths terminate.\nTo address these issues, we introduce the Transition Mechanism, which consists of two main strategies: Early Stop (Exploitation \u2192 Exploration) and Deep Seek (Exploration \u2192 Exploitation). As illustrated in Figure 4 (middle), these strategies allow an evolving search space with node transits between the two statuses. It helps the tree maintain focused reasoning, ensuring the efficient allocation and utilization of limited computational resources throughout the whole search process."}, {"title": "5 Experiments", "content": "We conduct extensive experiments to evaluate the efficiency of the DPTS framework. We benchmark its performance against various search algorithms across multiple models and datasets to ensure a comprehensive analysis."}, {"title": "5.1 Settings", "content": "Models. We include Qwen-2.5-1.5B-Instruct, Qwen-2.5-7B-Instruct (Yang et al., 2024), LLaMA-3.1-8B-Instruct, and LLaMA-3.2-3B-Instruct (Touvron et al., 2023) to cover various model sizes.\nDatasets. The evaluation datasets include Math500 (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021), both are widely used for reasoning and mathematical problem-solving tasks. We implement the evaluation by referring the approaches used in Qwen-2.5-Math (Yang et al., 2024), supplemented in our submission.\nComparison Methods. We compare DPTS against three widely used search algorithms: Monte Carlo Tree Search (MCTS) (Sprueill et al., 2023), Best-of-N (Cobbe et al., 2021), Beam Search (Yao et al., 2023). Since efficient tree search algorithms have recently regained attention after the emergence of LLM reasoning, the strong baselines are limited. As a result, we primarily compare DPTS against these typical and well-established search algorithms to demonstrate the effectiveness of our method. An introduction of the comparison methods and other details about experimental settings are provided in Appendix C."}, {"title": "5.2 Comparisons on Search Algorithms", "content": "We conduct a comprehensive comparison across different search algorithms on various models and sizes. We emphasize the search efficiency of our method while maintaining accuracy.\nFor efficiency, results in Table 1 show that DPTS significantly reduces inference time compared to other search methods across various models and datasets, demonstrating superior efficiency. On Math500, DPTS achieves the lowest inference time across all models. Particularly, in Qwen-2.5, DPTS reduces the search time from 117.37s (MCTS) to 45.10s in the 1.5B model, achieving nearly a 2.6\u00d7 speedup, and reduces from 121.46s (MCTS) to 53.50s in 7B model, accelerating nearly 2.2\u00d7. The impact is even more pronounced on the GSM8K, where DPTS achieves a 3.9\u00d7 speedup from 79.68s to 19.95s in Qwen-2.5-7B. And DPTS even only requires 17.82s for each sample using Llama-3-8B, also 3.9x. It forcefully suggests that DPTS effectively mitigates redundant rollouts and optimizes search efficiency. We highlight that, especially on the more challenging tasks, the Early Stop plays a crucial role. Without it, trees often run till timeout on Math500, significantly increasing inference time. In contrast, our approach allows the search tree to terminate earlier within a limited number of expansions, effectively reducing computation time.\nFor accuracy, DPTS maintains the searching quality and even outperforms the existing algorithms with half or even less of the reasoning time. On the Math500 dataset, DPTS achieves the highest accuracy in all experiment cases, surpassing MCTS, Best-of-N, and Beam search. Notably, for Qwen-2.5-1.5B, DPTS improves accuracy from 56.6% (MCTS) to 59.2%. A similar trend is observed on GSM8K, where DPTS either matches or slightly improves accuracy over MCTS, and surpasses Best-of-N and Beam Search by a wide margin. On Llama-3-3B, DPTS has 67.8% accuracy, outperforming the previous best MCTS by 3.8% with only 48.5% time consumption. These results highlight that DPTS maintains or even enhances solution quality while significantly improving inference speed, making it a more robust and efficient search algorithm for complex reasoning tasks."}, {"title": "5.3 Ablation Study", "content": "To analyze the contribution of each component within the DPTS framework, we conduct an ablation study on Qwen-2.5-1.5B with Math500 in Table 2. In this study, we use the classical MCTS as the baseline and incrementally integrate our proposed techniques to evaluate their impact.\nWe begin with the original MCTS (non-parallel, |P| = 1) as the baseline. It spends the most time per sample and has the largest best index 10.45. We then apply Parallelism Streamline with Adaptive Parallel Generation (AP), and accuracy improves. It shows that the trees are able to grow faster and larger to include a better solution with parallelism.\nNext, we assign the node status as exploit or explore nodes for each expansion with Search Mechanism, denoted as \"+S | AP\" in Table 2. The search process becomes significantly more structured and targeted, leading to a boost in efficiency. The time of each sample saves by 31.24s (28.9%\u2193). It finds the best path within an average of 4.66 terminated paths, much fewer than Baseline AP.\nMoreover, when only applying the Early Stop strategy in the Transition Mechanism (denoted as \"+T | AP), we obtain fast inference with much less time and paths. However, since we only use the exploitation nodes without exploring the possible branches, the accuracy is relatively low. Therefore, we claim that the Search and Transition Mechanism should be used as a whole: the Search mechanism provides different node statuses for exploitation and exploration, while the Transition mechanism makes them flexibly change and update.\nFinally, we combine our Search and Transition Mechanism (denoted as \"+S+T | AP\"), enabling Early Stop and Deep Seek. It shows the best search results in accuracy and efficiency. It demonstrates that DPTS is efficient in quickly identifying optimal solutions and conducting deep reasoning.\nResults show that each component of DPTS contributes significantly to improving inference speed and reasoning accuracy, making it a robust and scalable framework for parallel tree search."}, {"title": "5.4 Hyperparameter Analysis", "content": "We conduct a hyperparameter study in Table 3 on the thresholds $\\theta_{es}$ and $\\theta_{ds}$ in the Transition mechanism. When t < t*, the threshold $\\theta$ follows the mean-based strategy determined by A. When t > t*, it turns to a max-based one. Empirically, we set t* = 5 to balance the flexibility and efficiency.\nExperimental results demonstrate that our method is robust to A. We try different A and report the average ES/DS ratios per sample. We highlight that $\\theta_{es}$ and $\\theta_{ds}$ can be set differently based on the specific task. But DPTS consistently works well when $\\lambda \\in [0.6, 0.8]$. It demonstrates that the Transition mechanism is effective in mitigating the redundancy issue during search progress. However, it should be noticed that, if \u03bb is large, the ES transition may be aggressive, which leads to unsatisfactory results (e.g. \u03bb = 1.0). Meanwhile, if \u03bb is too small, it degrades to all exploitation nodes, resulting in low efficiency as well."}, {"title": "5.5 Visualizations", "content": "To provide an intuitive understanding of the effectiveness of our proposed method, we present visualizations of searching trajectories.\nFirst, we analyze the dynamic changes in the number of exploitation and exploration nodes throughout the search process in Figure 5. The Deep Seek transition temporarily increases the proportion of exploit paths, allowing promising nodes to receive deeper reasoning. However, as the threshold $\\theta_{es}$ increases, exploit nodes are more likely to reach the threshold and stop. As a result, the number of exploit nodes naturally decreases, reinforcing a balance between exploitation and exploration. This dynamic adaptation ensures that DPTS stretches on the most promising branches under the constraint of computational resources.\nSecond, we show the trees generated by DPTS and analyze the search behavior in Figure 6. It does not continue exploitation on low-confidence nodes, effectively pruning unpromising branches after shallow exploration. Additionally, the trees are capable of stable reasoning focus, with deep exploitation on promising paths. Therefore, the generated trees exhibit a relatively narrow width, as DPTS primarily expands nodes that are more relevant to the optimal path and spend less time on unnecessary regions. It demonstrates that DPTS ensures high-potential paths receive deeper thinking within a limited time and memory budget."}, {"title": "6 Conclusion", "content": "In this paper, we propose DPTS (Dynamic Parallel Tree Search) that effectively enhances the computational efficiency of LLM reasoning. DPTS introduces Parallelism Streamline, allowing efficient inference with arbitrary path lengths and nodes. The Dynamic Search and Transition Mechanism mitigates redundant rollouts and focuses on promising solutions. Experiments show that DPTS achieves the highest reasoning accuracy while delivering 2-4\u00d7 speedup. Our work offers valuable insights into optimizing computation for efficient LLM reasoning, strengthening problem-solving capabilities to tackle real-world challenges."}, {"title": "Limitations", "content": "Our DPTS framework focuses on selecting and refining the search paths, but does not involve hardware design. Therefore, it is orthogonal to low-level methods. For example, we can integrate DEFT (Yao et al., 2024) to reduce the data transportation of the shared prefixes, leading to further acceleration. Also, our method is validated only on math reasoning tasks, and has not been tested on other domains, such as coding or scientific problems. However, we believe its generalizable capabilities make it applicable across a wide range of fields. Additionally, we envision that this method can also be applied to online training by improving generation quality. We leave these attempts to our future work."}, {"title": "A More Observations of Motivation", "content": ""}, {"title": "A.1 Wasted Tokens and Expansions", "content": "To better understand the inefficiencies caused by frequent node switching, we conduct a statistical analysis on Qwen-2.5-1.5B with the Math500 dataset and evaluate the redundancy in token generation and node expansion.\nToken redundancy analysis: In Figure 7(left), we compare the total number of tokens generated for each sample (blue line) against the number of tokens required for the best path (yellow line). The samples are sorted in descending order primarily by total token count and secondarily by best-path token count. Our analysis shows that the total token count does not exhibit a strict multiplicative relationship with the best-path token count, but in general, the number of tokens required for the best path is significantly lower than the total token count. This suggests that traditional tree search algorithms generate a large number of unnecessary tokens during exploration.\nExpansion redundancy analysis: We also examined the number of node expansions during tree growth (Figure 7(right)). The blue line represents the total number of expansions for each sample, while the green line represents the number of expansions on suboptimal paths (i.e., nodes that do not contain any part of the optimal solution). While there is no strict multiplicative correlation between these two metrics, the green line closely follows the blue line, indicating that a significant proportion of expansions occur on suboptimal paths. This further supports the observation that traditional tree search algorithms frequently explore unnecessary areas before finding the best solution."}, {"title": "A.2 Examples of DFS and BFS Trees", "content": "As illustrated in Figure 8, which shows two typical depth-first search (DFS) trees, we visualize the node expansion process in layers based on tree depth. The darker reddish-brown nodes represent high-confidence nodes, while the lighter nodes indicate lower-confidence ones. The arrows denote parent-child relationships, where dark blue arrows indicate later-generated nodes and light blue arrows represent earlier-generated nodes.\nFrom the figure, we can clearly observe the reasoning trajectory of tree search: starting from the root node, the search prioritizes the child node with the highest confidence, then recursively expands deeper by selecting the most promising child node at each level. This continues until a termination condition is met, at which point the search backtracks and explores alternative paths from the root node. Due to the nature of this process, different paths vary significantly in their depth and termination points. Moreover, the next explored path does not follow a strict spatial or hierarchical pattern within the tree.\nWe also observe redundant exploration issues in the right two branches. At tree depths 4/5, the confidence scores of the expanded nodes are noticeably lower compared to previously explored nodes. However, due to the inherent mechanics of depth-first search (DFS), the algorithm continues expanding these nodes until the termination condition is met, even if the intermediate confidence scores remain consistently low. As a result, considerable computation is wasted on redundant expansions and token generations, with little contribution to improving the final output quality.\nAs illustrated in Figure 9, BFS results in a flatter, more uniform, top-down expansion structure compared to the trees observed in Figure 8. This behavior creates two key inefficiencies: (1) Incomplete reasoning before termination: In our experiments on the Math500 dataset, a pure BFS approach resulted in 178 (about 35.6%) of reasoning paths terminating without generating an answer (e.g., Tree 1 in Figure 9). The algorithm explores many different areas of the tree but often fails to pursue any one path deeply enough to reach a valid conclusion. (2) Excessive expansions and token redundancy: Even when BFS eventually finds a correct answer, it tends to consume significantly more expansions and tokens than necessary (e.g., Tree 2 in Figure 9). The best path (highlighted in yellow arrows) has a depth of only 4, yet before discovering this optimal solution, BFS explores a large number of additional nodes (light blue arrows), many of which do not contain any part of the optimal path."}, {"title": "B Formulas and Algorithms", "content": ""}, {"title": "B.1 Formulas in Parallelism Streamline", "content": "Data Collection and Preparation. Before executing parallel inference, we should collect the input data that is stored in separate memory locations. Based on the Node Data Structure (refer to Appendix B.1), which is [id, parent, conf., KVn, $Seq^{1\\sim n}$], we need to concatenate the past KV caches and input sequences of different nodes into single large batch matrices. However, as discussed in Sec. 3, tree search paths exhibit varying path lengths, meaning that both past KV caches and context sequences have different sizes. To handle the length disparity and support arbitrary node parallelism, we apply padding for shorter past KV and context sequence:\n$KV^{1\\sim n} = concat (KV_r,\u2026\u2026\u2026, KV_{a1}, KV_{a2}, KV_n)$,\n$padding'' = 0_{max(\\forall n \\in P |KV^{1\\sim n}|) - |KV^{1\\sim n}|}$,\n$KV^{pad} = concat (padding'', KV^{1\\sim n})$,\n$KV_{all} = stack ([\\forall n \\in p KV^{pad}]),\\qquad(3)$\nwhere r and $a_1, a_2, ...$ are the root node and $1^{st}, 2^{nd}$ ancestors of n, respectively. $0_{len}$ is a matrix of zeros with length len. Similar to above, the input sequences are also padded and stacked:\n$padding'' = padding\\_token_{max(\\forall n \\in P |Seq^{1\\sim n}|)}$,\n$Seq^{pad} = concat (Seq^{1\\sim n}, padding'')$,\n$Seq_{all} = stack (\\forall n \\in p Seq^{pad}),\\qquad(4)$\nwhere $padding\\_token_{len}$ is a vector of the predefined padding token id with length len. In this way, the data is fed into the LLM for parallel generation. Additionally, techniques like DEFT (Yao et al., 2024) are orthogonal to our DPTS and can be integrated to identify and merge shared prefixes, further optimizing inference efficiency.\nGeneration and Updating. After the generation phase, we obtain new output sequences and past KV caches. These are then partitioned based on the tree width. Specifically, we segment past KV caches and only store those corresponding to new tokens. Output sequences are completely stored rather than fragmented, due to the negligible memory overhead. For clearer demonstration, the output of $i^{th}$ node in P can be written as"}, {"title": "B.2 Algorithms for Searching and Transition Mechanism", "content": "In the main text, due to paper length constraints, we only present the overall process in Algorithm 1, which connects the entire framework's algorithms and formulations, including Parallelism Streamline and the Search and Transition Mechanism.\nIn the above section, we provided the mathematical formulation of the Parallelism Streamline. In the following, we further supplement the algorithmic details of Search, Transition, and Reward, offering readers a clear and intuitive representation of the algorithmic process.\nFirstly, Algorithm 2 demonstrates the searching mechanism. Specifically, during initialization or when the number of nodes in the parallel queue P is less than the maximum parallelism TP, the algorithm selects the TP-|P| highest-confidence nodes from the candidate node pool N as supplementary nodes (Lines 8-9).\nThen, the highest-confidence nodes are designated as exploit nodes (Line 12) until the proportion of exploit nodes reaches the ratio p. The remaining selected nodes are assigned as explore nodes. Finally, these newly selected nodes are merged into P to prepare for the next cycle of parallel expansion.\nNext is the Transition Algorithm 3. After each expansion, we iterate through all nodes in the parallel queue P and identify the best child node $n^{*}$ for each node.\nThen, based on the category of node n, we compare $n^{*}$ with the corresponding threshold $\\theta$. If the early stop condition is not met or the deep seek condition is met, we add $n^{*}$ as a new exploit node into P. Otherwise, the node will no longer be expanded and will be evicted from P.\nAfter completing an expansion, we check whether each node's path meets the termination conditions, such as reaching the maximum token limit or generating an end token. If a path satisfies the termination condition, exploration of that path stops, and a reward is computed as the final path score. We demonstrate this process in Algorithm 4, which is identical to previous tree search algorithms and is not discussed in detail. However, for the sake of algorithmic completeness, we explicitly include it here."}, {"title": "C Additional Details about Experiment", "content": ""}, {"title": "C.1 Comparison Methods", "content": "We compare DPTS against three widely used search algorithms: (1) Monte Carlo Tree Search (MCTS) (Sprueill et al., 2023) balances exploration and exploitation when sampling, while the selected paths rollout till termination, (2) Best-of-N (Cobbe et al., 2021) performs multiple independent rollouts and selects the highest-scoring output, (3) Beam Search (Yao et al., 2023) expands multiple hypotheses in parallel, pruning low-scoring candidates at each step to maintain a fixed-width search (Snell et al., 2024). Since efficient tree search algorithms have recently regained attention after the emergence of LLM reasoning, the strong baselines are limited. As a result, we primarily compare DPTS against these typical and well-established search algorithms to demonstrate the effectiveness of our method."}, {"title": "C.2 Experimental Settings", "content": "The experimental settings are as follows: we set the tree width to 4, tree depth to 16, mini step to 100, and the maximum token limit to 2048"}]}