{"title": "COLUMBUS: Evaluating COgnitive Lateral Understanding through Multiple-choice reBUSes", "authors": ["Koen Kraaijveld", "Yifan Jiang", "Kaixin Ma", "Filip Ilievski"], "abstract": "While visual question-answering (VQA) benchmarks have catalyzed the development of reasoning techniques, they have focused on vertical thinking. Effective problem-solving also necessitates lateral thinking, which remains understudied in AI and has not been used to test visual perception systems. To bridge this gap, we formulate visual lateral thinking as a multiple-choice question-answering task and describe a three-step taxonomy-driven methodology for instantiating task examples. Then, we develop COLUMBUS, a synthetic benchmark that applies the task pipeline to create QA sets with text and icon rebus puzzles based on publicly available collections of compounds and common phrases. COLUMBUS comprises over 1,000 puzzles, each with four answer candidates. While the SotA vision-language models (VLMs) achieve decent performance, our evaluation demonstrates a substantial gap between humans and models. VLMs benefit from human-curated descriptions but struggle to self-generate such representations at the right level of abstraction.", "sections": [{"title": "1 Introduction", "content": "Human problem-solving seamlessly combines vertical and lateral thinking (De Bono 2016). Vertical thinking is an analytical search process that rewards logic, rules, and rationality. It optimizes correctness by narrowing down on quality solutions and rejecting suboptimal ones (Hernandez and Varkey 2008). For example, resolving the question mark in Figure 1 (left) requires systematically identifying that all examples adhere to the formula: (left - top) \u00d7 right + bottom = 77. Meanwhile, lateral thinking (De Bono 1971) is explorative, divergent, and creative (Hernandez and Varkey 2008). It expands the solution space by diverging into novel directions. As illustrated in the right part of Figure 1, visual, spatial, verbal, and numerical cues must be interpreted unconventionally (defying common sense (Jiang, Ilievski, and Ma 2024)), a process that lends itself to lateral thinking. In this example of a rebus puzzle, the numbers \"1111\" phonetically represent the word \u201cONCE\u201d, while the visual-spatial relationship between the blue letters \"MO\u201d and \u201cON\u201d spell \"BLUE MOON\u201d. As \u201cONCE\u201d is placed inside \u201cBLUE MOON\" this leads to the solution B) Once in a blue moon. Existing benchmarks for visual question answering (VQA) (Agrawal et al. 2016) have been instrumental in exploring and enhancing the vertical thinking skills of vision-language models (VLMs). Popular subtasks are visual reasoning (Johnson et al. 2017; Li and S\u00f8gaard 2022; Li et al. 2023b), abstract visual reasoning (AVR) (Chollet 2019; Malinowski and Fritz 2015; Ma\u0142ki\u0144ski and Ma\u0144dziuk 2023; Zhang et al. 2019), and visual commonsense reasoning (VCR) (Bitton-Guetta et al. 2023), all requiring both processing of visual as well as linguistic information. Meanwhile, lateral thinking benchmarks have recently been proposed as word and sentence puzzles but are limited only to the textual modality (Jiang et al. 2023b; Huang et al. 2024). Hence, there is a lack of lateral thinking benchmarks for multimodal settings combining text and vision.\nTo this end, we study how well VLMs exhibit multimodal lateral thinking. Our contributions are as follows:\n1. A taxonomy-driven three-step methodology for creating lateral thinking tasks in a multiple-choice VQA format. Our taxonomy definition yields 18 rules that manipulate the visual attributes and relationships of the puzzle's elements (text or icons). The puzzle rendering step leverages this taxonomy to create a graph representation for a puzzle answer and generate an image for the graph. The distractor sampling step is based on a weighted average of orthographic and semantic similarity between a puzzle's correct answer and its visible elements.\n2. A synthetic benchmark called COLUMBUS that applies the lateral thinking methodology to create QA sets with rebus puzzles based on public collections of"}, {"title": "2 Related Work", "content": "Rebus Puzzles. In psychology, rebus puzzles have been known to demand lateral thinking (Salvi et al. 2016; Threadgold, Marsh, and Ball 2018; MacGregor and Cunningham 2008). Prior work (Salvi et al. 2016; Threadgold, Marsh, and Ball 2018) reports human accuracies of 74.5% and 53.31%, respectively, and compares the impact of vertical and lateral thinking, concluding that using lateral thinking led to a significant improvement in the number of puzzles solved.\nTo our knowledge, the only existing benchmark of rebus puzzles that assesses VLMs is REBUS (Gritsevskiy et al. 2024). This benchmark contains 333 human-annotated puzzles separated into 13 categories with three difficulty levels. Half of the models tested in this work achieve less than 5% accuracy. The authors ascribe this difficulty to the benchmark's reliance on world knowledge (e.g., cities, towns, public transport stations) and vertical thinking skills like string manipulation. Instead, we devise a methodology for automatic and scalable generation of rebus puzzles based on publicly available resources. The sources to create COLUMBUS (phrases and compounds) are deliberately selected to focus on lateral thinking only and minimize the need for world knowledge and vertical thinking.\nVertical Thinking in VQA. AVR puzzles, illustrated in Figure 1 (left), are commonly used to assess multimodal reasoning. Discriminative tasks such as Raven's Progressive Matrices (Raven 1941; Barrett et al. 2018; Zhang et al. 2019) and Visual Analogy Problems (Webb et al. 2020) involve completing sequences of panels with abstract shapes selected from a predefined set of options. Bongard problems (Bongard 1968; Nie et al. 2020) require discovering the rules that separate and govern shapes across two sets of panels, though these rules must be described in natural language. MARVEL (Jiang et al. 2024) encompasses these benchmarks with a more comprehensive set of patterns, input shapes, and configurations, along with rigorous checks to assess that model answers are grounded in perception and reasoning. Alternatively, generative approaches, like the Abstraction and Reasoning Corpus (Chollet 2019), test the ability to recreate missing panels without choosing from predefined options. A comprehensive review of AVR puzzles is provided by Ma\u0142ki\u0144ski and Ma\u0144dziuk (2023). Rather than using puzzles, CLEVR (Johnson et al. 2017), QLEVR (Li and"}, {"title": "3 Methodology for Visual Lateral Tasks", "content": "To ensure a straightforward automatic evaluation and minimize answer ambiguity, we frame each puzzle as a multiple-choice VQA pair. A puzzle \\(p = (I, (q, O, c))\\) consists of a rebus image \\(I \\subset I\\) and question \\(q \\in S\\) with correct answer \\(c \\in S\\) chosen from options \\(O = \\{o_1, o_2, o_3, c\\}\\); \\(O \\subseteq S\\). \\(I\\) and \\(S\\) denote the space of images and strings, respectively. Each \\(I\\) can be decomposed into a set of elements \\(E \\subset I \\cup S\\), where \\(d_e \\in E\\) (\\(e \\in I \\oplus e \\in S\\)). The latent rules that govern the appearance and visual-spatial relationships of each \\(e \\in E\\) are determined by \\(R: S \\rightarrow I\\). The goal in solving \\(p\\) is to select a responser \\(r \\in O\\) such that \\(R(r) = R(c)\\).\nFigure 2 depicts our method. As rebus puzzles are typically built around idiomatic expressions, compound words,"}, {"title": "3.1 Taxonomy of Latent Rules", "content": "We derive a novel taxonomy of latent rules to support the development of lateral thinking puzzles. The taxonomy consolidates online guides and databases of rebus puzzles and a rebus categorization scheme outlined by Salvi et al. (2016). We manually select and organize the categories in these sources such that each rule uniquely manipulates an element through visual, spatial, verbal, and numerical properties. We ensure that each rule can be automatically operationalized and mixed with others in the same puzzle.\nThe resulting taxonomy (Figure 3) consists of 18 rules, grouped into three categories according to how they manipulate elements in a puzzle: 1. Individual rules define the unary characteristics of an element in a rebus. Example rules include reversing character order (direction), the text color (style), and adding arrows before the element (highlight). 2. Relational rules define the positioning between a pair of elements. We define four relational rules, placing an element beside/inside/above/outside another. 3. Modifier rules are designed to be mutually inclusive with other individual rules. Examples include repeating an element multiple times or substituting it with a phonetically similar element."}, {"title": "3.2 Puzzle Rendering", "content": "Rebus puzzles include elements (i.e., text or icons) whose appearance and position are determined by latent rules triggered by specific keywords in the puzzle's answer. This is illustrated in Figure 1 (right), where the words \u201cONCE\u201d, \u201cIN\u201d, \u201cBLUE\u201d, and \u201cMOON\u201d determine the puzzle's elements and their arrangement. We expect that SotA generative models cannot be reliably applied to generate rebus puzzles, a hypothesis that we validate in Section 6.5. Instead, we render a puzzle by a taxonomy-driven transformation of its input elements, which first produces a graph and subsequently an image (green part in Figure 2).\nGraph Generation Algorithm. We generate a directed, attributed graph whose nodes are elements that will be rendered into a puzzle image. The node attributes specify the rendering of that element, i.e., the individual or modifier rules that will apply to it. The edges between two nodes are annotated with an attribute that specifies their relational rule. We parse a puzzle answer into a graph by following a separate procedure for compounds and phrases. Figure 4 shows the rebus graphs for two puzzle images based on a compound and a phrase, respectively.\nFor compounds (Figure 4 top), we create a graph with a single node using the following steps. First, we split a"}, {"title": "3.3 Distractor Sampling", "content": "Distractor sampling (blue part in Figure 2) selects the three most similar compounds or phrases to the input semi-automatically. We opt for sampling rather than data augmentation approaches like rephrasing because compounds and proverbial phrases are challenging to generate automatically. To select a distractor for a puzzle, we obtain its visible elements (text and icons) from the graph representation and compute similarity to all other phrases/compounds. The similarity uses a A-weighted combination of Jaccard word overlap (Leskovec, Rajaraman, and Ullman 2014) and cosine similarity using Sentence-BERT embeddings (Reimers and Gurevych 2019). We expect that distractors with word overlaps make the task more challenging because the test-taker works with the visible words. Since word overlap may fail to select relevant distractors when the visible words only occur once or too many times across the entire dataset of phrases and compounds, we also leverage semantic cosine similarity to include distractors that contain synonyms of the words in the original input."}, {"title": "4 The COLUMBUS Benchmark", "content": "We apply our proposed pipeline to instantiate the first visual lateral thinking benchmark, COLUMBUS.\nPuzzle Answer Collection. We start by scraping common English phrases from publicly available sources, namely Wiktionary and www.theidioms.com, yielding 9,745 instances. We use the Large Database of English Compounds (LaDEC) (Gagn\u00e9, Spalding, and Schmidtke 2019) for compound words. This dataset has been feature-engineered and curated by humans, consisting of 8,957 compounds. We fill rules that appear less than ten times across the benchmark by semi-automatically adding compounds and phrases that trigger them, with the assistance of prompting the OpenAI's ChatGPT-3.5 model (Brown et al. 2020). All combined, we collect 18,836 candidate answers from which to generate puzzles. Additionally, we collected homophones (25 samples) and icons (480 samples). Homophones were added manually by recognizing common ones found in rebus puzzle databases. The icon collection combines icons scraped from an online source and manually added ones.\nQuality Control. The graph parsing for all phrases and compounds includes a preprocessing step to remove stopwords that do not belong to the set of rule-triggering keywords. Multiple elements with individual rules can still be present in the same puzzle, and more than one modifier rule can be applied to an element. However, we apply at most one individual rule to a single element. In cases where multiple individual rule can be applied to a single element, we generate these individually for each rule as separate puzzles. To further improve readability and limit the risk of overlapping elements, we restrict the image's rendered elements using heuristics based on the number of elements and their rules (see Appendix A.5). We take the top 1,000 puzzles from the remaining puzzles with the most edges and rules per node to ensure the benchmark is challenging (see more details in Appendix A.1). To provide a fairer comparison between textual and icon puzzles, each puzzle containing an icon is duplicated, and all its icons are converted to their textual counterparts. Finally, we filter out low-quality puzzles with overlapping or overflowing text from this remaining set.\nBenchmark Composition. We split the benchmark into two partitions: COLUMBUS-TEXT that only contain text and COLUMBUS-ICON that contain at least one icon. Between these two partitions, COLUMBUS features an overlap subset of 339 puzzle pairs. Each pair consists of two versions of the same puzzle: one version uses icons, and the other uses text instead of those icons. Table 1 shows key statistics about COLUMBUS. While non-icon puzzles are more numerous, icon puzzles feature more elements. This can be attributed to"}, {"title": "5 Experimental Design", "content": "Model Families. We include open- and closed-source instruction-tuned and non-instruction-tuned VLMs. We also test closed-source models enriched with forward and backward chaining. We evaluate all models in a zero-shot setting using standard hyperparameter values.\nFor non-instruction-tuned models, we test 1) BLIP-2 (Li et al. 2023a) with the OPT-2.7b and the OPT-6.7b LLMs (Zhang et al. 2022); 2) Fuyu-8b (Bavishi et al. 2023), a multimodal text and image transformer that achieves competitive performance on VQA tasks. We also evaluate CLIP (Radford et al. 2021), a seminal VLM and a foundation for many other models used in our experiments. As CLIP is not a VQA model, we switch its task to image classification, which must match the image of a puzzle to the correct answer from the four available choices.\nAs instruction-tuned models, we include 1) BLIP-2 coupled with Flan-T5-11b (Chung et al. 2022), which achieves SotA performance on zero-shot VQA tasks; 2) InstructBLIP (Dai et al. 2023), an instruction-tuned version of BLIP-2 model that uses Vicuna-7b (Zheng et al. 2023); 3) QwenVL (Bai et al. 2023b), a 7 billion visual multimodal version of the Qwen LLM (Bai et al. 2023a) from which we use the chat variant; 4) CogVLM (Wang et al. 2023), a 17 billion parameter VLM that achieves SotA performance on several VQA benchmarks; 5) Llava (Liu et al. 2023a), a large VLM that achieves SotA performance on several vision benchmarks despite its lack of billion-scale data. For Llava, we use the 13b (v1.5) and 34b (v1.6) variants, in which the 34b model is quantized to 4-bit; 6) Mistral-7b (v2) (Jiang et al. 2023a) to use in text-only, question-answering (QA) auxiliary experiments.\nFor closed-source models, we selected four models from two representative families based on their promising performance in public visual reasoning benchmarks (Lu et al. 2023; Liu et al. 2023b): 1) GPT-40 and GPT-40-mini (OpenAI et al. 2024) and 2) Gemini 1.5 (Pro) and Gemini 1.5 (Flash) (Anil et al. 2023).\nWe experiment with two structural variants of closed-source models: forward and backward chaining (Jurafsky and Martin 2009). In forward chaining (FC), the model constructs evidence from an image and connects this evidence with the optimal candidate answer (Wang et al. 2024). The forward chaining approach first prompts models to generate JSON files with attributes (e.g., name, relation, description) for each visible puzzle element, which can later be used as a reference in the final prompting. As a representative of backward chaining (BC) from question and image towards the answer, we test belief graphs (Kassner et al. 2023) where a model recursively derives and evaluates explanations for each candidate answer. Belief graphs excavate additional information by recursively evaluating the truth assignments of premises generated for each answer candidate. The truth assignments are then optimized with a SAT solver, yielding the most probable answer. This approach is evaluated on a random subset of 50 puzzles (see Appendix B).\nHuman Evaluation. To estimate human performance on COLUMBUS, we ask two participants to answer a subset of 105 randomly selected puzzles (39 text and 40 icon puzzles, and 13 overlap puzzles with both a textual and icon variant). For more details about the setup, see Appendix D.\nModel Inputs. We explore four human-curated input levels, each providing the model with increasing information about the puzzle, i.e., its description and details on the nodes or edges of a puzzle's graph. Specifically: 1. no description of the nature of the puzzle, nor the graph; 2. only a description of the nature of the puzzle; 3. description of the nature of the puzzle and the graph nodes; 4. description of the nature of the puzzle and the full graph (both nodes and edges). See Table 6 in Appendix C.1 for the exact prompts used.\nEvaluation Protocol. Following other multiple-choice benchmarks (Jiang et al. 2023b; Zhu et al. 2016; de Faria et al. 2023), we use accuracy as the evaluation metric, defined as the percentage of puzzles solved correctly. To extract answers from a model's output, we use regex to check for choice symbols (e.g., \u201cA\u201d) if they are present and then perform exact string matching to the correct answer/symbol. For the larger, more flexible models that produce long explanations for their answers, we use GPT-40 to extract their answers automatically. For model outputs that answer a given puzzle with multiple options, we pick one of them randomly. We also compute the most frequent option for each model to check for imbalanced answer distributions."}, {"title": "6 Results", "content": "We investigate five questions: 1) How well can VLMs solve rebus puzzles that require lateral thinking? 2) Can forward and backward chaining enhance lateral thinking performance? 3) Do VLMs benefit from prompts that supply more information about the puzzle? 4) How does the performance of VLMs vary across different puzzle rules? 5) Can VLMs generate task puzzles directly?"}, {"title": "6.1 Overall Performance", "content": "Table 2 shows the performance of each model on COLUMBUS-TEXT and COLUMBUS-ICON. The closed-source and larger open-source models perform best on both partitions, while the small, non-instruction-tuned models perform near-randomly and have strong biases to certain answers (typically A or D). Comparing the average model performance with text and icons, we see no significant difference. Namely, the performance is slightly higher on COLUMBUS-ICON, whereas on the overlapping set of 339 puzzles, we observe a slightly higher performance on the textual puzzles. The best model for each partition is consistently GPT-40, which is the expected result. Yet, none of the models surpass human accuracy, with average gaps of 45.15% on COLUMBUS-TEXT and 35.75% on COLUMBUS-ICON."}, {"title": "6.2 Impact of Structural Reasoning", "content": "The two structured variants show opposing results (Table 2). Forward chaining, leading the model to generate graph descriptions in JSON format, yields marginal improvement on GPT4 while boosting Gemini performance greatly (6% to 17%). The structured JSON data allows Gemini to reason about rebuses with fine-grained detail, enabling the model to focus first on visual components and then handle broader reasoning. Both models achieve around 80% with forward chaining, leaving a gap against human performance. Here, we note that the forward chaining models benefit from including rule-related details in the metadata (e.g., a 'reverse' label for words written backward), which motivates further work toward steering models to generate puzzle-specific abstractions. On the contrary, backward chaining yields an 18.3% and 25.15% drop in accuracy for GPT-40 and GPT-40-mini, averaged across the two partitions. We ascribe this to the models lacking a global overview of the image, as each evaluated premise focuses on local parts of a puzzle without cohesively pointing towards a candidate answer."}, {"title": "6.3 Model Sensitivity to Input Information", "content": "Can models benefit from a ground-truth structured description of the puzzle provided in their input? Figure 5 shows that adding information about the nature of the puzzle (prompt 2) has little effect (+2.56% and -0.44% for textual and icon puzzles, respectively). Adding a description of the graph nodes (prompt 3) increases the model performance by 12.25% and 14.8% for non-icon and icon puzzles, respectively, reaching over 90% for GPT-40. However, adding information on the relational rules only increases performance 4.12% and 2.73% for COLUMBUS-TEXT and -ICON, respectively. Interestingly, the forward chaining variant, inspired by prompt 3, also benefits from including information; however, the improvement is much higher when using ground-truth data, indicating that the models struggle to generate a graph description at the right level of abstraction. Considering the example in Figure 1, the models extract the text as is (e.g., extract \u201cMO1111ON\") and cannot make the lateral connection that words/letters need rearranging. Even GPT-40 struggles consistently with this, such as with certain direction rules (see Section 6.4). For the results from the other models, see Table 8 in Appendix E.\""}, {"title": "6.4 Rule-based Analysis", "content": "Figure 6 shows results for how often a puzzle containing a specific rule is solved correctly by the best-performing model (GPT-40). On COLUMBUS-TEXT, GPT-40 performs the best on the relational rules and the worst on individual rules (difference of 17.98%). When individual rules appear together with modifier rules, the model performance is slightly higher (by 3.02%). We see a similar trend for COLUMBUS-ICON, with a gap from individual to relational and modifier rules being 10.96% and 8.21%, respectively. We note that, while the GPT-40's performance is similar on the two partitions, specific rules are more difficult for this model when represented as text (e.g., repetition rules). In contrast, others are more challenging when presented as icons (e.g., size). Such biases align with recent work that shows the perceptual sensitivity of VLMs to object visual attributes (Zhang et al. 2024). As for relational rules, the model performance on text and icon puzzles is on par."}, {"title": "6.5 VLM Generation of Puzzles", "content": "Given the strong generative abilities of VLMs, a natural question arises: can they generate puzzles without the methodology we define in Section 3? To investigate whether the taxonomy-based generation pipeline is necessary, we sample 100 puzzle answers and use DALLE-3 (Betker et al. 2023) to generate corresponding puzzles with the prompt \u201cTry to generate an image for a rebus puzzle on {answer}\u201d. Three human annotators are asked first to label whether the puzzles contain sufficient visible elements to support solving the puzzle and then select the better one between the two puzzles (the one generated by our method and the one by DALLE-3). Our results show that 98% of the rebuses generated by our pipeline contain a sound and complete list of elements, compared to only 44% for DALLE-3. Additionally, the puzzles from our pipeline were preferred over those from DALLE-3 84% of the time, with an 11% tie rate. DALLE-3 struggles as a rebus generator for two main reasons Figure 7 (Betker et al. 2023): 1) Noisy details: unlike the concise rebuses our pipeline produces, DALLE-3 often creates very complex puzzles that include irrelevant information (e.g., the click icon in the right of Figure 7). 2) Abstract representation: DALLE-3 struggles to represent abstract ideas, such as \u201cafter\u201d, whereas our carefully designed rule-based taxonomy can handle these concepts precisely."}, {"title": "7 Conclusions", "content": "This paper defined a novel visual lateral thinking task, associated with a construction methodology and a resulting synthetic, multiple-choice benchmark COLUMBUS consisting of 1008 rebus puzzles with text and icons. Our experiments on COLUMBUS indicated a substantial gap between human and model performance, which was lessened by adding the graph description to the model inputs. The improvement when expanding a model's access to include relational rules implies they rely primarily on the elements in a puzzle alone rather than the spatial relationships between them. The analysis showed that models struggle to think laterally about rules that rearrange text, as these require flexible, puzzle-specific abstractions.\nThe current benchmark's scale is limited in size. Moreover, its relative coverage of icon and text puzzles and various rules is imbalanced. Future work should apply and extend the methodology provided in this paper to develop more extensive and balanced versions of COLUMBUS, possibly including other multimodal formats beyond rebus puzzles and reducing the variability between puzzle categories to better understand performance gaps. Special attention should be devoted to puzzles that require abstraction and creative thinking and cannot be solved easily by word matching a text/icon. While our methodology includes measures to address this (e.g., homophones and questions with multiple distractors containing the visible words of a puzzle), it can be extended by including synonyms or related words."}, {"title": "A.1 Data Filtering", "content": "Of 18,836 puzzles, 4386 remain after automatic filtering (see Appendix A.5). From this subset, we take the top 1000 puzzles according to those that have the most rules per node, plus the most edges (excluding the relational next to rule). We expect these puzzles to be more challenging, as they require one to decode more about an element to understand its use in a puzzle. We manually remove unreadable puzzles that were broken from the generation process, such as too-large elements, overlapping text/icons, and overflowing text/icons. Then, we automatically remove puzzles in which the only rule used is replacing text with icons. As these puzzles are more similar to emoji puzzles than a rebus, we do not consider them to require a sufficient degree of lateral thinking.\nWe perform minimal data filtering on the generated distractors. For the set of distractors belonging to each question, we only replace a distractor if it features another distractor dissimilar by at least one stopword that does not trigger any rules. In such a case, we replace it with the next most similar distractor (if it is also not too similar for the same reason). For example, the idioms \"Fresh start\" and \"A fresh start\" differ by the stopword \u201cA\u201d, which does not trigger any rules, so one of these idioms can be replaced. If two words are the singular and plural forms of each other, we do not replace either, as plurality is used to trigger the repetition rules."}, {"title": "A.2 Impact of Keywords", "content": "Figure 8 shows three examples of rebus puzzles that highlight the relationship between the words in the answer of a rebus puzzle and what kind of rules are being triggered in the image of that rebus puzzle. For example, the rebus puzzle for the phrase Big Deal (Figure 8a), focuses on size as a rule, which is triggered by the word Big. Other keywords that tend to be involved in a rebus puzzle that focuses on size are shown in Figure 8d."}, {"title": "A.3 Ignored Words", "content": "The list of ignored words is: \"the\", \"a\", \"of\", \"is\", \"let\", \"and\", \"at\"."}, {"title": "A.4 Rule Keywords", "content": "Tables 3 and 4 show the keywords that trigger the rules belonging to the individual and relational categories."}, {"title": "A.5 Image Generation", "content": "Heuristics. Our image generation will not render a puzzle in the following four cases:\n1. Graphs with nodes connected with the next to relational rule will not be rendered if they have more than three nodes.\n2. Graphs that contain an above relational rule will not be rendered if either the top and bottom portions exceed two nodes.\n3. Graphs that contain an inside relational rule will not be rendered if either the inside portion has more than one node or the outside portion has more than two nodes.\n4. Graphs that contain an outside relational rule will not be rendered if either the inside or outside portions have more than one node.\nTemplates. Each template contains x, y coordinates and a multiplier to control the font size. Three of the four templates are points (up to three) spaced roughly equally along the x-axis in the middle of the image. Each element in a graph (from left to right) corresponds to a node in the template (the number of nodes in a graph must equal the number of points in a template). The remaining template is used for graphs involving the above relational rule. During image generation, we sequentially fill the points in these templates in with text/icons represented by the graph nodes. We change the appearance of these elements according to the attributes of that element's respective node."}, {"title": "A.6 Rule Frequency", "content": "Table 5 shows the frequency of each rule across all three categories, on both COLUMBUS-TEXT and COLUMBUS-ICON."}, {"title": "B Implementation and Computation Details", "content": "For image generation, each rebus puzzle is a 400 \u00d7 400 pixel image created in Matplotlib. We use Consolas's monospaced font for text, as it is convenient for spatial calculations. Icons are rendered in the Segoe UI Emoji font.\nFor distractor generation, we use  A = 0.8 to prioritize word overlap similarity over semantic similarity in the weighted average calculation. The Sentence-BERT model used to calculate semantic similarity is all-MiniLM-L6-v2, adapted from (Wang et al. 2020). This is the most downloaded sentence similarity model available on Huggingface.\nDuring inference, we do not alter the hyperparameters provided in the documentation of their Huggingface pages outside of the generated output token length where necessary. The models with altered output token lengths are Fuyu-8b (200 tokens), Mistral (100 tokens), and the two Llava models (200 tokens). All models were evaluated on an in-house Rocky Linux operating system cluster. Almost all"}, {"title": "D Human Evaluation", "content": "Figures 9 and 10 show the launch and puzzle pages, respectively, that a human participant sees while answering puzzles in the benchmark. This setup is implemented with Google Sheets."}, {"title": "E Additional Results", "content": "E.1 Model Results per Prompt\nTable 8 shows the extended results across all models for each of the four prompts."}, {"title": "F Code Appendix", "content": "The code for generating a rebus graph from a compound word or phrase (see Section 3.2) is shown in Listings 1 and 2, respectively."}]}