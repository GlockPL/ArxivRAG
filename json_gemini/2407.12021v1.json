{"title": "Adaptive Draft-Verification for Efficient Large Language Model Decoding", "authors": ["Xukun Liu", "Bowen Li", "Ruqi Zhang", "Dongkuan Xu"], "abstract": "Large language model (LLM) decoding involves generating a sequence of tokens based on a given context, where each token is predicted one at a time using the model's learned probabilities. The typical autoregressive decoding method requires a separate forward pass through the model for each token generated, which is computationally inefficient and poses challenges for deploying LLMs in latency-sensitive scenarios. The main limitations of current decoding methods stem from their inefficiencies and resource demands. Existing approaches either necessitate fine-tuning smaller models, which is resource-intensive, or rely on fixed retrieval schemes to construct drafts for the next tokens, which lack adaptability and fail to generalize across different models and contexts. To address these issues, we introduce a novel methodology called ADED\u00b9, which accelerates LLM decoding without requiring fine-tuning. Our approach involves an adaptive draft-verification process that evolves over time to improve efficiency. We utilize a tri-gram matrix-based LLM representation to dynamically approximate the output distribution of the LLM, allowing the model to adjust to changing token probabilities during the decoding process. Additionally, we implement a draft construction mechanism that effectively balances exploration and exploitation, ensuring that the drafts generated are both diverse and close to the true output distribution of the LLM. The importance of this design lies in its ability to optimize the draft distribution adaptively, leading to faster and more accurate decoding. Through extensive experiments on various benchmark datasets and LLM architectures, we demonstrate that ADED significantly accelerates the decoding process while maintaining high accuracy, making it suitable for deployment in a wide range of practical applications.", "sections": [{"title": "1 Introduction", "content": "Large language model (LLM) decoding involves generating a sequence of tokens based on a given context, where each token is predicted one at a time using the model's learned probabilities [Brown et al., 2020, Zhang et al., 2022, Touvron et al., 2023a,b]. The core mechanism is autoregressive, where each new token is generated conditioned on the previously generated tokens and the given context. This process is crucial for applications like text generation [Li et al., 2024a, Peng et al., 2023, Chang et al., 2023], machine translation [Zhang et al., 2023, Moslem et al., 2023, Hendy et al., 2023], and conversational AI [Shanahan, 2024, Wu et al., 2023, Saka et al., 2023]. However, each decoding step involves a forward pass through the model, making the process inherently sequential and computationally expensive. The inefficiencies arise due to the need to reload the model for each token prediction, leading to high computational costs and memory bandwidth usage. This serial nature of decoding is a significant bottleneck, especially for real-time applications [Liu et al., 2023a, Mandvikar, 2023, Antoniol et al., 1994] where latency is critical. Thus, optimizing the decoding speed of LLMs is essential for practical deployment in various real-world scenarios."}, {"title": "2 Methodology", "content": "We propose a new fast fine-tuning-free draft-verification LLM decoding method by introducing adaptability into the decoding and learning from LLM. Existing accelerated decoding algorithms either require additional fine-tuning or lack adaptability to LLM's output distributions, resulting in significant additional cost or insufficient acceleration. To address these issues, we design an adaptive LLM representation based on a tri-gram matrix to adaptively approximate the output distribution of the LLM; develop an MCTS-based draft maker that balances exploration and exploitation for self-evolution towards high-quality drafts; and verify the drafts using tree attention."}, {"title": "2.1 Preliminary: Speculative Decoding & Monte Carlo Tree Search", "content": "Speculative decoding is a method to accelerate language model inference by using a smaller auxiliary model to generate a draft sequence, reducing the computational load on the larger model Leviathan et al. [2023]. Retrieval-based speculative decoding extends this by incorporating a retrieval system instead of the smaller model, leveraging pre-stored corpus segments for relevant text generation. Monte Carlo Tree Search (MCTS) [Coulom, 2007, Browne et al., 2012, James et al., 2017, \u015awiechowski et al., 2023] is an AI algorithm that optimizes decision-making by balancing exploration and exploitation of future states. It selects nodes for further exploration using a combination of node visit counts and estimated values, aiming to maximize overall outcomes. For a comprehensive discussion of these methods, please refer to Appendix C."}, {"title": "2.2 Adaptive LLM Representative", "content": "In order to approximate the output token distribution of the LLM without fine-tuning the small model, we distill linguistic knowledge from a small corpus and construct a tri-gram matrix as an initial representation of the LLM, which allows us to leverage the statistical regularities of language at a granular level. Specifically, we summarize and count the three tokens that appear in the corpus and compute the probability of the third token appearing conditional on the first two tokens. The formula is as defined in Eq. (1):\n\n$P(w_i | w_{i-2}, w_{i-1}) = \\frac{C(w_{i-2}, w_{i-1}, w_i)}{C(w_{i-2}, w_{i-1})},$\n\nwhere $P(w_i | w_{i-2}, w_{i-1})$ is the conditional probability of a word $w_i$ given the two preceding words $w_{i-2}$ and $w_{i-1}$, $C(w_{i-2}, w_{i-1}, w_i)$ is the count of the tri-gram occurrence in the corpus, and $C(w_{i-2}, w_{i-1})$ is the count of the preceding bi-gram Mori et al. [1998].\nIn this way, we can obtain a good initial LLM representative at a much lower cost, which can generate an approximate distribution of the next token based on the previous tokens. This LLM representative will collaborate with our draft maker to generate drafts and get feedback to update the tri-gram matrix for adaptability and self-evolution. Please see Section 2.3 for more details."}, {"title": "2.3 Draft Maker and Self-Evolution", "content": "With the help of the LLM representative, we further propose a draft maker that balances exploration and exploitation while searching for candidate drafts that are closer to the LLM output. On the one hand, our draft maker leverages the conditional probabilities from the LLM representative, which include current knowledge of the LLM output. On the other hand, our draft maker is encouraged to search more in the unexplored or less explored draft space to find better draft candidates. Then, with feedback from the LLM output, the LLM representative can update its understanding of the LLM output, improve the draft maker's search, and achieve self-evolution. Details are provided below.\nDraft Search Score: Given the initial tokens, we exploit Monte Carlo Tree Search (MCTS) Coulom [2007] to guide the search process of the drafts of the next tokens, where we prioritize candidate tokens according to the conditional probability from the tri-gram matrix-based LLM representative and the node visitation counts during the tree search. Our scores play a key role in balancing exploration and utilization during Monte Carlo tree search and is defined as Eq. (2). This is a kind of PUTC Score [Rosin, 2011, Silver et al., 2017]. More specifically, $Q(s, a)$ assesses the quality of taking action $a$ in state $s$, while $P(s, a)$ represents the prior probability of selecting action $a$ in state $s$. The term $N(s, a)$ denotes the number of times the action $a$ has been taken from state $s$, and $N (s, b)$ sums the counts for all actions from state $s$. The constants $C_1$ and $C_2$ adjust the balance between exploration and exploitation, improving the decision-making process in draft construction. This formula ensures that our draft choices are contextually appropriate and optimizes the robustness and coherence of text generation.\n\n$\\max Q(s, a) + P(s,a) \\cdot \\sqrt{\\frac{\\Sigma_l N(s,b)}{1+N(s,a)} }\\cdot (c_1+\\log(\\frac{\\Sigma_b N(s,b) }{ C_2}+ C_2 + 1)).$\n\nSelf-Evolution Strategy Transfer: Based on the final search score obtained during the Monte Carlo tree search, we can construct draft candidates and verify them to get the final decoding output (please see Section 2.4) and feed it back for self-evolution. This final output decoding represents LLM's output distribution, which would be a good learning material for the LLM representative. Therefore, we feed this knowledge into the LLM representative in order to obtain updated conditional"}, {"title": "2.4 Draft Construction and Verification", "content": "To validate the draft sequences, it is noted that many have common starting segments that can cause redundant recalculations in the Transformer layers if not managed correctly. To address the issue, a pseudo-sequence that guarantees that each draft is a sub-sequence and that any common prefix appears only once is created He et al. [2023]. We also use a specific attention mask for each attention layer, called tree attention [Miao et al., 2023, Cai et al., 2024]. This mask aligns the computations for each token with its dependencies according to the original draft sequence, preserving the draft's contextual integrity and preventing unnecessary computations. The approval of drafts relies on a comparison with the conditional distribution from the LLM. At each position, new tokens are sampled and compared to the draft tokens. If a sampled token corresponds to the draft token, it is approved; otherwise, the draft is discarded from that point. This selective approval ensures that the output sequence aligns with what would be produced by a typical autoregressive process, thus upholding the authenticity of the generated text."}, {"title": "3 Theoretical Insight: Why ADED uses MCTS", "content": "In this section, we explore the theoretical parallels between the Monte Carlo Tree Search (MCTS) algorithm used in our ADED framework and the inference mechanisms of large language models (LLMs) to demonstrate the use of MCTS and the self-evolution of ADED. We show that draft search in ADED using MCTS can be viewed as a form of policy optimization, and that the inference mechanism of LLM can be viewed as a similar form of penalty optimization.\nMCTS in ADED: The token selection procedure in ADED decoding can be viewed as an action selection process. The MCTS algorithm optimizes its policy by iteratively building a search tree and updating visit counts for each node (state-action pair) based on the search paths. The visit count distribution $\\pi(a | x)$ is defined as:\n\n$\\pi(a | x) = \\frac{n(x,a)}{1+\\Sigma_l n(x, b)},$\n\nwhere $n(x, a)$ represents the visit count for action $a$ in state $x$. Then, the action selection in MCTS can be written as selecting the action $a*$: \n\n$a^*(x) = arg \\max[Q(x, a) + \\lambda_v \\cdot \\frac{\\pi_p(a|x)}{\\pi(a|x)}].$\n\nFollowing [Grill et al., 2020], we use $q \\in R^{|A|}$ to denote the vector of Q-function $Q(x, a)$. With proper choice of hyper-parameters, the MCTS algorithm can be viewed as searching for the optimum solution to a policy optimization problem [Grill et al., 2020] as below:\n\n$arg \\max [q^T \\pi_{\\theta} - \\lambda_{KL}[\\pi_{\\theta}, \\pi]].$\n\nwhere S is the |A|-dimensional simplex, $\\lambda_y$ is a regularization parameter that depends on hyperparameters and balances exploration and exploitation, and KL is the KL-divergence.\nLLM Inference Mechanism: Large language models, particularly those based on the Transformer architecture, generate text by predicting the probability distribution of the next token given the previous tokens. During inference, the model maximizes the log-likelihood of the observed data, which is equivalent to minimizing the cross-entropy loss:\n\n$\\mathcal{L}(\\theta) = - \\sum_{t=1}^{T} log P(W_t | W_{1:t-1}; \\theta),$\n\nwhere P denotes the conditional probability of LLM, w denotes the tokens, and @ denotes the model parameters. Regularization techniques, such as KL divergence, are often incorporated to prevent overfitting and ensure generalization:\n\n$\\mathcal{L}(\\theta) = - \\sum_{t=1}^{T} log P(w_t | W_{1:t-1};\\theta) + \\alpha KL(P_{model}, P_{data}).$\n\nComparison between MCTS & LLM Inference: As shown in Eq (5) and Eq. (7), both MCTS and LLM Inference can be viewed as regularized optimization problems for selecting the distribution of the next tokens. On the one hand, the Q-function in MCTS for ADED can be viewed as an approximation to the log-likelihood of LLM\n\n$Q(x, a) = - \\sum_{t=1}^{T} log P(w_t | W_{t-1}, W_{t-2}; \\theta) \\approx P(w_t | W_{t-1}, W_{t-2}; \\theta) \\approx - \\sum_{t=1}^{T} log P(w_t | W_{1:t-1};\\theta)$\n\nwhere P and P are the conditional probability distribution from tri-gram-matrix-based LLM representative and LLM, respectively. On the other hand, both MCTS and LLM Inference improve the optimization procedure by employing regularization techniques. Through a comparative analysis, we show the similarities between MCTS and LLM Inference in terms of optimization and regularization, and highlight our rationale for choosing MCTS for the ADED framework."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Setup", "content": "Models and Datasets To evaluate the efficacy of ADED for the inference of large-language models, we execute a series of experiments employing five distinct models in four datasets. We test our algorithm on three Vicuna models Chiang et al. [2023] (7B, 13B, 33B) and two LLaMA2-chat models Touvron et al. [2023b] (7B, 13B) to evaluate its acceleration capabilities across different sizes and types of models. Our assessment incorporated the HumanEval Chen et al. [2021], MT-Bench Zheng et al. [2023] and Alpaca Taori et al. [2023] dataset to ascertain general natural language understanding and generation competencies. These datasets were meticulously chosen to guarantee a comprehensive analysis of our acceleration techniques across various tasks.\nCorpus We constructe two corpus. The first one is built using a portion of the Python pre-training code from The Stack Kocetkov et al. [2022], comprising approximately 2.7M Python code samples with a resulting size of 1007MB. The second corpus is constructed using data derived from Ultra-Chat Ding et al. [2023], consisting of around 774K ChatGPT conversations, which produces a corpus with a size of 574MB.\nMetrics To assess the performance of our acceleration algorithms on large language models, we utilize two main metrics: speedup ratio and average acceptance length. The speedup ratio, calculated as the ratio of the time required by the baseline models to complete inference tasks without acceleration to the time required by our ADED, effectively measures the efficiency gains introduced by our algorithm. The second metric, average acceptance length, measures the average number of tokens accepted per forward pass by the target large language models, excluding any overhead of retrieving and constructing draft tokens, indicating the maximum possible acceleration.\nBaselines In this study, we investigate various foundational approaches to improve the decoding speed of large language models. We examine Lookahead Decoding Fu et al. [2024a], an innovative, precise, parallel decoding algorithm that significantly cuts down latency without relying on draft models. We also assess REST He et al. [2023] (Retrieval-Based Speculative Decoding), which adopts a retrieval-based strategy to create draft tokens, in contrast to conventional speculative decoding methods that rely on a draft model. Collectively, these baseline methods provide a solid framework for evaluating the efficiency of our proposed acceleration techniques in the LLM decoding process. All experiments are conducted on an NVIDIA A6000, except for the 33B model, which utilizes an NVIDIA A100. Where not mentioned, the experiments default to Greedy sampling."}, {"title": "4.2 Main Experimental Results", "content": "In our experiment, we investigate the efficacy of diverse methodologies applied to multiple models, utilizing three distinct datasets: MT-Bench, Human-Eval, and Alpaca. We focus on metrics such as Accepted Length, Latency, and Speed Up to evaluate the efficiency and responsiveness."}, {"title": "4.3 Stability of ADED", "content": "In this section, we analyze the stability of our algorithm, ADED, across different categories of tasks. The categories considered include writing, roleplay, reasoning, math, coding, extraction, STEM, and humanities. The experimental results, as shown in Table 2, indicate that ADED maintains consistent performance across all categories. The average accept length remains stable, demonstrating that ADED can effectively handle a diverse range of tasks without significant variations in performance.\nTo further evaluate the robustness of ADED, we examined the effects of varying the top-p and temperature parameters on its performance. Figures 5a and 5b illustrate the impact of these parameters on the average accept length.\nFigure 5a shows that changes in the top-p parameter do not significantly affect the performance of ADED. The average accept length remains relatively stable across different values of top-p, indicating that ADED is not overly sensitive to this parameter.\nSimilarly, Figure 5b demonstrates that variations in the temperature parameter have minimal impact on the performance of ADED. The consistency in the average accept length across different temperature values further supports the robustness of our algorithm.\nThese results confirm that ADED exhibits robust performance across a variety of tasks and maintains stability despite changes in key parameters, making it a versatile and reliable choice for diverse applications."}, {"title": "5 Ablation Study", "content": "To gain a deeper understanding of our method, we conduct a series of ablation studies and analyses focused on each individual component. Please see full ablation studies in the Appendix.\nEffect of the adaptive strategy. Figure 6 illustrates the performance impact of our adaptive strategy on two models, Vicuna-7B and Vicuna-13B, with a comparative analysis of average accepted lengths over varying token counts. The graphs show that the adaptive strategy consistently maintains higher average accepted lengths across the input range for both models, compared to the non-adaptive. The adaptive strategy's success can be attributed to its dynamic adjustment of the model's probability distributions based on the tri-gram frequencies from prior outputs. This allows the model to better manage longer contexts and maintain relevance, enhancing stability and coherence in longer interactions. The marked performance improvement, particularly in managing larger token counts, highlights the adaptive strategy's efficacy in sustaining effective and coherent outputs in extended sequences.\nEffect of the corpus size."}, {"title": "6 Related Work", "content": "Approach Without Draft Models. A significant portion of recent advances in language model decoding strategies has focused on improving efficiency without relying on draft models. Two notable approaches in this realm are Lookahead decoding Fu et al. [2024a] and Retrieval-Based Speculative Decoding He et al. [2023]. Lookahead decoding is an approach that enhances the efficiency of the decoding process through the prediction of subsequent tokens via Jacobi Iteration Sleijpen and Van der Vorst [2000]. It employs a heuristic to estimate the future cost of a sequence without the need to explicitly create a draft. This technique not only accelerates the decoding process by minimizing the number of tokens to be processed but also seeks to preserve or improve the quality of the output text by taking into account potential future scenarios in the decision-making process. Retrieval-Based Speculative Decoding(REST) He et al. [2023] introduces a retrieval-enhanced generation model that speculatively decodes sequences without the need for producing preliminary drafts. It instead searches and prioritizes possible continuations from an already established sequence database. This approach utilizes the inherent redundancy in natural language to anticipate probable continuations, thereby eliminating the requirement for draft sequences and greatly reducing computational costs. Both Lookahead decoding and REST demonstrate the capabilities of decoding methods that avoid intermediate drafts, providing a more straightforward and computationally efficient route to generating high-quality text.\nApproach With Draft Models. Draft models are also used to improve the decoding efficiency. Techniques such as Speculative Decoding [Leviathan et al., 2023, Spector and Re, 2023, Chen et al., 2023, Stern et al., 2018], Madusa Cai et al. [2024], Eagle Li et al. [2024b], various other approaches requiring draft models [Zhang et al., 2024, Liu et al., 2023b, Kim et al., 2024, Fu et al., 2024b] fall into this category, utilizing models to generate drafts. Although these methods aim to speed up response times and reduce computational load during initial text generation, their adoption comes with significant drawbacks. The primary issue is the necessity for additional training specific to the draft models, which can be resource-intensive. Moreover, these techniques generally depend on GPU resources [Kwon et al., 2023, Sheng et al., 2023, Park et al., 2024] for inference, potentially limiting their application in environments where such hardware is unavailable or when operating under strict resource constraints. This dependence not only increases operational costs but also restricts flexibility in deployment scenarios."}, {"title": "7 Conclusion", "content": "ADED improves the LLM decoding process by introducing adaptability and efficiency, significantly reducing latency and computational demands. This method achieves up to a 2.5X speedup in decoding and a 20% improvement in acceptance rates, outperforming traditional techniques. Unlike existing approaches, ADED dynamically adjusts the draft distribution using a tri-gram matrix and enhances draft quality through MCTS, eliminating the need for fine-tuning. The continuous feedback loop ensures ongoing improvements in draft generation. While ADED demonstrates robust performance across various benchmarks, future work will focus on further optimizing the adaptability mechanisms and exploring its application in more diverse real-world scenarios. Additionally, addressing potential limitations in extremely large-scale deployments will be a priority."}, {"title": "Appendix", "content": null}, {"title": "A Advantages on Computation Effiency", "content": "Our technique provides substantial benefits when implemented on edge devices like laptops and smartphones, which often face limitations in GPU capabilities and memory. In contrast to traditional decoding methods that depend heavily on GPU power or large memory sizes, our strategy is crafted for high efficiency with low resource demands.\nReduced GPU Requirements Our approach, which does not require fine-tuning and utilizes a lightweight probabilistic model, primarily operates on the CPU, eliminating the need for substantial GPU resources. This feature is especially advantageous for edge devices with limited GPU access. By minimizing GPU dependency, our technique can be applied more widely, enhancing LLM decoding across a broader array of devices.\nLow Memory Usage Our method avoids the need for bulky initial models or intricate neural network architectures, considerably lowering the memory usage typically needed for LLM decoding. This aspect is particularly suitable for devices with limited memory, such as budget laptops and mobile phones. The decrease in memory usage not only leads to quicker processing times but also reduces power consumption, which is vital for devices running on batteries. Compared to REST, which also requires a corpus, our method significantly reduces memory usage; for instance, both using the Stack dataset, our method requires only less than 1GB while REST needs 27GB.\nUltimately, our decoding method is exceptionally apt for practical use in edge systems, where there is often a scarcity of computational resources. It offers a viable and effective option for improving LLM decoding without sacrificing speed or precision, thus bringing sophisticated language processing to less powerful devices."}, {"title": "B Broader Impacts", "content": "The advancements presented in this paper, specifically the accelerated LLM decoding via Monte Carlo Tree Search (MCTS) and self-evolving speculation, have several broader impacts worth discussing. These impacts span multiple domains including technology, society, and ethics."}, {"title": "Technological Impact", "content": "Our method significantly enhances the efficiency and speed of autoregressive LLM decoding. This improvement can benefit numerous applications that rely on real-time language processing, such as interactive chatbots, automated customer service, and real-time translation systems. By reducing the computational load and memory requirements, our approach also makes it feasible to deploy advanced LLMs on edge devices like smartphones and IoT devices, broadening their accessibility and usability."}, {"title": "Societal Impact", "content": "The ability to perform faster and more efficient language model decoding can have a profound impact on society. For instance, it can improve the responsiveness and accuracy of assistive technologies for individuals with disabilities, such as voice-controlled assistants and text-to-speech systems.\nAdditionally, educational tools that rely on real-time feedback and interactive learning can benefit from quicker and more reliable LLM responses, enhancing the learning experience for students."}, {"title": "Ethical Considerations", "content": "While our advancements offer significant benefits, they also raise important ethical considerations. The increased efficiency of LLMs could lead to more widespread use of automated systems, which might replace human jobs in certain sectors. It is crucial to address the potential displacement of workers by fostering skills development and creating new job opportunities that leverage human-LLM collaboration.\nMoreover, the deployment of more powerful LLMs on a wider scale necessitates robust measures to mitigate misuse. Enhanced LLM capabilities could be exploited for malicious purposes, such as generating misleading information or deepfake content. Therefore, it is essential to implement strong ethical guidelines and monitoring mechanisms to prevent abuse and ensure that the technology is used responsibly."}, {"title": "Environmental Impact", "content": "Improving the efficiency of LLM decoding can also contribute to environmental sustainability. By reducing the computational resources required for LLM operations, our method decreases the energy consumption associated with running these models. This reduction is particularly important given the growing concerns about the environmental footprint of large-scale AI systems. Our approach aligns with the broader goal of developing greener AI technologies that minimize their impact on the planet.\nIn summary, the proposed method for accelerating LLM decoding has far-reaching implications across various domains. While it offers substantial benefits, it is essential to address the accompanying ethical, societal, and environmental challenges to ensure that the technology is developed and deployed in a responsible and beneficial manner."}, {"title": "C Preliminary", "content": "Retrieval-Based Speculative Decoding: Decoding in large language models describes the procedure of text generation by sequentially predicting tokens. Given a context sequence s = (X1, ..., Xt\u22121, Xt), conventional autoregressive decoding techniques produce the subsequent token at position t + 1 using conditional probability:\n\n$X_{t+1} ~ P(x|x_1,..., X_t; \\Theta_{large}),$\n\nwhere p denotes the conditional probability distribution calculated by the LLM with parameters @large. To reduce these computational burdens during inference, speculative decoding is proposed Leviathan et al. [2023]. It reduces the frequency of forward passes using large by incorporating an auxiliary language model with fewer parameters @small.\nThe speculative decoding process is implemented iteratively as follows: Using the smaller model Osmall, a draft sequence of the next m tokens is generated autoregressively:\n\n$X_{t+i} ~ p(x/S, X_{t+1}, ..., X_{t+i-1}; \\Theta_{small}),$\n\nwhere i = 1, ..., m. Despite the sequential nature of this generation, the reduced model complexity of small leads to a lower computational overhead compared to large. Retrieval-Based Speculative Decoding extends the basic speculative decoding framework by replacing the smaller language model with a retrieval system. This method uses:\n\n$X_{t+i} ~ p(x|s, X_{t+1}, ..., X_{t+i-1}; \\Theta_{Corpus}),$\n\nwhere retrieve(x1,...,xt) fetches contextually relevant text segments from a pre-stored corpus, reducing reliance on frequent recalculations with large.\nMonte Carlo Tree Search: Monte Carlo Tree Search (MCTS) Coulom [2007] is a popular algorithmic in artificial intelligence, which explores potential future states from a current decision point by building a tree of possibilities, balancing exploration of new paths and exploitation of known beneficial paths."}, {"title": "D Configuration of ADED", "content": "For our experiments, we use the following hyperparameters to optimize the performance of ADED: We set t, the threshold for the elimination of tri-gram probability, at 12 to focus only on the most prevalent trigrams. The number of iterations for Monte Carlo Tree Search (MCTS), denoted s, is fixed at 150. The parameters in the MCTS PUTC Score function, c\u2081 and C2, are 2.414 and 8.0, respectively, to effectively balance exploration and exploitation. The search depth and the length of each retrieved continuation candidate, represented by l, is 4; and the number of continuation candidates, n, is set at 24."}, {"title": "E Configuration of Baselines", "content": "REST\nThe REST baseline uses the default settings with the following specific configurations:\n\u2022 Number of threads: 6\n\u2022 Draft choice: 64\n\u2022 Datasets: UltraChat and The Stack\n\u2022 Token spans: [16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2]\nLookahead\nThe Lookahead baseline uses the default settings with the following specific configurations:\n\u2022 LEVEL: 5\n\u2022 WIN: 15\n\u2022 GUESS: 15\n\u2022 FLASH: 0"}, {"title": "F Implementation Details", "content": "In this section, we describe the implementation details of our proposed Monte Carlo Tree Search (MCTS) algorithm for text generation and the dynamic adjustment of tri-gram probabilities."}, {"title": "F.1 Monte Carlo Tree Search for Text Generation", "content": "Algorithm 1 illustrates the Monte Carlo Tree Search (MCTS) procedure for text generation. The procedure, RunMCTS, takes the number of iterations as input. The algorithm proceeds through the following phases:"}, {"title": "F.2 Dynamic Adjustment of tri-gram Probabilities", "content": "Algorithm 2 details the procedure for dynamically adjusting tri-gram probabilities. The procedure, Adjust3Gram, takes a sequence of tokens, the maximum length of n-grams to consider, an increment value, and the maximum probability.\nFor each trigram in the recent portion of the token sequence, the probability is increased by the increment value, up to the specified maximum probability. If the trigram is not already in the tri-gram matrix, it is added with the increment value as its initial probability."}, {"title": "G Additional Experimental Results", "content": "In this section, we present additional experimental results to further illustrate the effectiveness of our proposed methods. We compare the performance of Vicuna-7B and Vicuna-13B models with and without the adaptive strategy and analyze the impact of varying Monte Carlo Tree Search (MCTS) search counts on performance.\nFigure 7 shows the performance comparison on the MTBench dataset for Vicuna-7B and Vicuna-13B models with and without the adaptive strategy. The results demonstrate the advantage of using the adaptive approach. For both models, the adaptive strategy significantly reduces the average exact length of the generated sequences as the number of tokens increases, indicating more efficient and accurate text generation.\nFigure 8 presents the results for Vicuna-7B and Vicuna-13B models on the MTBench dataset, showing the impact of varying MCTS search counts on performance. For both models, increasing the number of MCTS search counts leads to improved performance, with the optimal counts varying by model size. The average exact length and latency are plotted against the number of MCTS search counts, illustrating the trade-off between performance and computational cost. As shown in the plots, there is a notable improvement in the average exact length as the search counts increase, while latency also increases, indicating a balance between the depth of search and the time taken for generation."}]}