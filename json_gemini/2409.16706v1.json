{"title": "Pix2Next: Leveraging Vision Foundation Models for RGB to NIR Image Translation", "authors": ["Youngwan Jin", "Incheol Park", "Hanbin Song", "Hyeongjin Ju", "Yagiz Nalcakan", "Shiho Kim"], "abstract": "This paper proposes Pix2Next, a novel image-to-image translation framework designed to address the challenge of generating high-quality Near-Infrared (NIR) images from RGB inputs. Our approach leverages a state-of-the-art Vision Foundation Model (VFM) within an encoder-decoder architecture, incorporating cross-attention mechanisms to enhance feature integration. This design captures detailed global representations and preserves essential spectral characteristics, treating RGB-to-NIR translation as more than a simple domain transfer problem. A multi-scale PatchGAN discriminator ensures realistic image generation at various detail levels, while carefully designed loss functions couple global context understanding with local feature preservation. We performed experiments on the RANUS dataset to demonstrate Pix2Next's advantages in quantitative metrics and visual quality, improving the FID score by 34.81% compared to existing methods. Furthermore, we demonstrate the practical utility of Pix2Next by showing improved performance on a downstream object detection task using generated NIR data to augment limited real NIR datasets. The proposed approach enables the scaling up of NIR datasets without additional data acquisition or annotation efforts, potentially accelerating advancements in NIR-based computer vision applications.", "sections": [{"title": "1 Introduction", "content": "Visible range cameras (e.g., RGB cameras), which capture images within the spectrum of light detectable by the human eye, often have limitations in challenging conditions such as low light, adverse weather, or situations where the object of interest lacks sufficient contrast against the background. To address these challenges, one potential solution is utilizing imaging technologies that extend beyond the visible spectrum (Bijelic et al. (2018)). In particular, this study focuses on the Near-Infrared (NIR) spectrum. NIR cameras operating beyond the visible range demonstrate significant advantages, such as capturing reflections from materials and surfaces in a manner that enhances detection and contrast. For example, NIR cameras can penetrate fog, smoke, or even certain materials, making them valuable in applications such as surveillance, autonomous vehicles, and medical imaging where"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Image-to-Image Translation", "content": "Image-to-image (I2I) translation is a critical task in computer vision that involves converting images from one domain to another while retaining the underlying structure and content. This field has wide-ranging applications, including style transfer, image super-resolution, and domain adaptation. The advent of deep learning, particularly Generative Adversarial Networks (GANs) (Goodfellow et al. (2014)), has significantly advanced the capabilities of I2I translation.\nOne of the earliest and most influential models in I2I translation is Pix2pix (Isola et al. (2017)), which operates using paired datasets to learn the mapping between input and output domains, employs a conditional GAN framework where the generator is trained to produce images that the discriminator classifies as real, thereby learning to generate high-quality and realistic outputs.\nBuilding upon Pix2pix, Pix2pixHD (\u0422.-\u0421. Wang et al. (2018)) was developed to handle the challenges associated with generating high-resolution images. It introduced several improvements over the original Pix2pix, including a multi-scale discriminator and a coarse-to-fine generator architecture, which together enable the production of more detailed and realistic images."}, {"title": "2.2 NIR/IR Range Imaging", "content": "Near-Infrared (NIR) and Infrared (IR) imaging are crucial in various applications that require capturing information beyond the visible spectrum, such as night-time surveillance, automotive safety, and medical diagnostics (Kumar et al. (2021), S. Liu et al. (2020), and Luo et al. (2010)). NIR imaging, which operates within the 700 to 1000 nanometer(nm) wavelength range (Figure 5), is particularly valuable in low-light conditions and for highlighting features that are not visible in standard RGB images."}, {"title": "3 Method", "content": "The Pix2pixHD model uses coarse-to-fine generator architectures to transfer the global and local details of the input image to the generated image. With Pix2Next, we extended this framework by employing residual blocks within an encoder-decoder architecture instead of using separate global and local generators. Residual blocks are integral to our design, as they allow the network to maintain critical feature details by facilitating identity mappings through shortcut connections. These connections help to address the vanishing gradient problem, ensuring stable training and enabling the network to learn more complex transformations essential for high-quality image generation.\nTo further improve the preservation of fine details and overall image context, we integrate a vision foundation model (VFM) into our architecture, which serves as a feature extractor. This model captures broad global features that work together with the local features learned by the encoder-decoder structure. These features are combined throughout the network using cross-attention mechanisms, which help align and merge the global and local features during the image generation process. This approach is key to accurately capturing the specific characteristics and subtle details of the NIR domain, leading to better quality and more reliable translated images."}, {"title": "3.1 Network Architecture", "content": "The Pix2Next architecture is composed of three key modules. The extractor module is responsible for extracting detailed features from input RGB images, which are then fed into the generator module's encoder, bottleneck, and decoder layers via cross-attention. The generator module, designed with an encoder-bottleneck-decoder framework, focuses on generating NIR images and incorporates U-Net-inspired skip connections to facilitate information flow between the encoder and decoder layers. Finally, the discriminator module is implemented as a patch-based GAN, comprising three consecutive discriminators at different resolutions, thereby optimizing the image generation process in a coarse-to-fine manner.\nIn the following sections, we will delve into the specifics of each module. First, we will examine the Feature Extractor (section 3.1.1), which leverages state-of-the-art VFMs to capture rich and contextual image representations. We will then explore the structure and innovations of our Generator (section 3.1.2), which synthesizes high-quality images by adopting an Encoder-Bottleneck-Decoder structure with novel mechanisms for feature integration and attention. Lastly, we will discuss the details of the Discriminator architecture (section 3.1.3) and its role in enhancing the generation of high-quality, realistic NIR images."}, {"title": "3.1.1 Feature Extractor", "content": "Our proposed model employs a state-of-the-art VFM as our feature extractor to capture detailed global representations from input images. Specifically, we utilize the Internimage (W. Wang et al. (2023)) architecture due to its exceptional performance in capturing long-range dependencies and adaptive spatial aggregation. The primary role of the feature extractor in our model architecture is to generate a comprehensive global representation of the input image, which is then used to guide the image translation process in the generator. This approach allows our model to maintain the global context and structural integrity of the RGB image during the NIR translation. We implement the feature extractor as follows:\n\u2022 Input Processing: The RGB input image (256x256x3) is fed into the InternImage model.\n\u2022 Feature Extraction: The InternImage model processes the input through its hierarchical structure of deformable convolutions and attention mechanisms.\n\u2022 Global Representation: The output of the final layer of InternImage serves as our global feature representation. This global representation is then used in the cross-attention mechanisms throughout our generator's encoder, bottleneck, and decoder stages.\nThe selection of InternImage as our feature extractor is motivated by its ability to capture both fine-grained local details and broader contextual information. The deformable convolutions in InternImage allow for adaptive receptive fields, enabling the model to focus on the most relevant parts of the image for our translation task. To validate the effectiveness of our chosen feature extractor, we conducted ablation studies comparing InternImage with other architectures such as ResNet (He et al. (2016)), ViT (Dosovitskiy et al. (2021)), and Swin Transformer (Z. Liu et al. (2021)). Our experiments demonstrated that InternImage outperformed other models in our RGB to NIR translation task, providing a more informative global representation that led to improved translation quality (see Table 2 for detailed results).\nBy leveraging this powerful feature extractor, our model can better understand the global structure and context of the input RGB image, leading to more accurate and contextually consistent NIR translations. This global representation serves as a guiding framework for our generator, ensuring that local modifications during the translation process remain coherent with the overall image structure and content."}, {"title": "3.1.2 Generator", "content": "The Generator in our proposed model adopts an Encoder-Bottleneck-Decoder architecture designed to process 256x256 RGB images. The key components of our generator are as follows:\n\u2022 Encoder: Seven blocks progressively increase channel depth from 128 to 512, utilizing Residual and Downsample layers with an Attention layer in the final block.\n\u2022 Bottleneck: Three blocks maintain 512 channels, combining Residual and Attention layers for complex feature interactions.\n\u2022 Decoder: Seven blocks gradually reduce channel depth from 512 to 128, using Upsample layers alongside Residual and Attention layers.\n\u2022 Normalization: Group Normalization with 32 groups is applied throughout the network.\nOur approach significantly diverges from the conventional Pix2pixHD architecture, incorporating several key innovations. Unlike Pix2pixHD's separate global and local generators, we implement a single, deeper Encoder-Bottleneck-Decoder structure. This design is enhanced with skip connections inspired by the U-Net architecture (Ronneberger et al. (2015)), which concatenates features from the encoder with those in the decoder. These connections facilitate the fusion of multi-scale feature representations to enhance the accuracy of the generated output and effectively preserve intricate details throughout the image synthesis process. Additionally, we introduce a cross-attention mechanism that utilizes features extracted by the VFM Feature Extractor. This mechanism is applied at each stage of the Generator's Encoder, Bottleneck, and Decoder, allowing for effective integration of global contextual information with local features.\nThe cross-attention operation can be formulated as:\n$\\Attention(Q, K, V) = softmax(\\frac{QKT}{\\sqrt{dk}})V$\nwhere $Q\u2208 R^{nxda}$ is the query matrix derived from the current layer features, $K\u2208 R^{mxdk}$ and $V\u2208 R^{mxdv}$ are the key and value matrices derived from the Feature Extractor output, n is the number of query elements, m is the number of key/value elements, and $d_k$ is the dimension of the keys.\nThis architectural design enables our model to capture and process multi-scale features more effectively, balancing global and local information to generate high-quality images. The combination of these elements achieves a balance between high-quality image generation, computational efficiency, generalization capability, and preservation of fine details. As a result, our model demonstrates significant improvements over previous approaches in image-to-image translation by producing detailed and contextually coherent translations from RGB to NIR domains."}, {"title": "3.1.3 Discriminator", "content": "We adopt the multi-scale PatchGAN architecture from Pix2pixHD for our study as the discriminator. This design utilizes three discriminators (D1, D2, D3) operating on different image scales: the original resolution and two down-sampled versions (by factors of 2 and 4 respectively). Each discriminator uses a PatchGAN structure, divides the input image into overlapping patches and classifies each as real or fake. The network consists of 4 convolutional layers (kernel size 4, stride 2), followed by leaky ReLU activations and instance normalization. The final layer produces a 1-dimensional output for each patch. The varying scales result in different receptive fields: D1 focuses on fine details, while D3 captures more global structures.\nUtilizing three varying resolution-focused discriminators enables more realistic image generation at various levels of detail, balanced local and global consistency, stable and reliable feedback to the generator, and computational efficiency compared to full-image discriminators. We maintained this discriminator architecture from Pix2pixHD due to its proven effectiveness in similar image-to-image translation tasks and its compatibility with our enhanced generator."}, {"title": "3.2 Loss Function", "content": "In this study, we enhanced the model's performance by incorporating additional loss components into the standard loss function of Generative Adversarial Networks (GANs) (Goodfellow et al. (2014)). Specifically, we added the Structural Similarity Index Measure (SSIM) (Z. Wang et al. (2004)) loss and the Feature Matching loss (T.-C. Wang"}, {"title": "3.2.1 GAN Loss", "content": "The standard loss function of GANs is defined through adversarial learning between the Generator and the Discriminator. The Generator aims to produce samples that closely resemble the real data distribution, while the Discriminator attempts to distinguish between real and generated samples. This process can be defined by the following equation:\n$\\min_G max_D LGAN (G, D) = E_{x~pdata (x)} [log D(x)] + E_{zpz(z)} [log(1 \u2013 D(G(z)))]$"}, {"title": "3.2.2 SSIM Loss", "content": "The SSIM loss was introduced to optimize the structural similarity between the generated and target images directly. SSIM measures the structural similarity between two images, modeling how the human visual system perceives structural information in images by considering luminance, contrast, and structure (Z. Wang et al. (2004)). The SSIM loss is defined as follows:\n$\\SSIM(x, y) = \\frac{(2\u03bcx\u03bcy + C1) (2\u03c3xy + C2)}{(\u03bcx^2 + \u03bcy^2 + C1) (\u03c3x^2 + \u03c3y^2 + C2)}$\n$LSSIM = 1 - SSIM(x, G(z))$\nWhere \u03bc\u03b1, \u03bcy are the mean luminance values of the images, \u03c3\u03b5 and \u03c3y are the standard deviations, \u03c3xy represents the covariance, and c\u2081 and C2 are small constants added for stability. As the SSIM value ranges from -1 to 1, LSSIM takes values between 0 and 2, where values closer to 0 indicate greater structural similarity between the two images.\nBy incorporating SSIM in our loss function, we ensure that our model is optimized to preserve important structural information in the image translation process. This leads generated images to be numerically similar and perceptually close to the target images."}, {"title": "3.2.3 Feature Matching Loss", "content": "Since RGB and NIR are different domains, the preservation of the details has higher importance. In order to penalize low-quality representations and stabilize the training of Pix2Next, we employ a feature matching loss. This loss encourages the generator to produce images that match the representations in real images at multiple feature levels of the discriminator. The feature matching loss is defined as:\n$LFM(G, D) = E_{x~pdata(x)} \\sum_{i=1}^{T} \\frac{1}{Ni} ||D^{(i)} (x) \u2013 D^{(i)} (G(z)) ||$\nwhere D) denotes the i-th layer feature extractor of discriminator Dk, T is the total number of layers, and Ni is the number of elements in each layer.\nThis loss computes the L1 distance between the feature representations of real and synthesized image pairs. By minimizing this difference across multiple layers of the discriminator, the generator learns to produce images that are statistically similar to real images at various levels of abstraction."}, {"title": "3.2.4 Combined Loss", "content": "To optimize the generation process effectively, we combine the previously explained loss functions into a comprehensive total loss (Ltotal). This combined loss leverages the strengths of each individual component to guide the model toward producing high-quality NIR images. The total loss function is formulated as follows:\n$\\Ltotal = min_G max_{D1, D2, D3} (\\sum_{k=1,2,3} LGAN (G, Dk)) + 11 \\sum_{k=1,2,3} LFM(G, Dk)] + A2LSSIM$\n$Ltotal = LGAN + 11LFM + 12LSSIM$\nwhere 1 and 2 are hyperparameters that control the relative importance of the SSIM and Feature Matching loss terms, respectively. In our final model, we set both \u2081 and 12 to 10, based on empirical experiments that showed optimal performance with these values. This combined loss function enables the model to preserve the high-quality image generation capability characteristic of GANs while simultaneously enhancing structural consistency through SSIM and Feature Matching."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets", "content": "We conducted our experiments using the RANUS dataset, which is a large-scale urban scene dataset that has spatially aligned RGB-NIR images. The RANUS dataset is particularly well-suited for our research on domain translation between RGB and NIR images. The RANUS dataset consists of images with a resolution of 512x512 pixels and includes a total of paired 4,519 RGB-NIR images. The dataset is collected in 50 different sessions and routes, covering a diverse range of scenes and objects. We randomly selected 40 out of the 50 image sequences, representing 80% of the dataset, for training our model, while the remaining 10 image sequences were reserved for testing to evaluate our model's performance on unseen categories and environments. In other words, this split strategy allowed us to assess Pix2Next's ability to generalize to new scenes that were not encountered during the training phase.\nTo enhance data quality, we conducted additional preprocessing steps, including a manual review to identify and remove mismatched frames that were not correctly aligned in time between the RGB and NIR image pairs. The final dataset"}, {"title": "4.2 Training Strategy", "content": "The experiments in this study were conducted on a system equipped with four NVIDIA GeForce RTX 4090 Ti GPUs. During the training process, all images were resized to 256 x 256 to ensure efficient use of GPU memory. This choice was made to optimize performance given the hardware constraints. All models were trained around 1000 epochs, ensuring sufficient convergence. Additionally, a cosine scheduler with warmup was applied to adjust the learning rate dynamically. This scheduler gradually increases the learning rate during the warmup phase and then decreases it following a cosine function. The initial learning rate was set to 1e-4 for all model training."}, {"title": "4.2.1 Evaluation Metrics", "content": "To evaluate the quality of the translated images, we employ widely used six standard measures, including the Structural Similarity Index (SSIM) and Peak Signal-Noise Ratio (PSNR) (Z. Wang et al. (2004)), Fr\u00e9chet Inception Distance (FID) (Heusel et al. (2017)), Root Mean Square Error (RMSE), Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al. (2018)), Deep Image Structure and Texture Similarity (DISTS) (Ding et al. (2020)). Each of these metrics possesses unique characteristics in assessing image quality and measuring the performance of translation models, enabling a comprehensive analysis of the model's effectiveness.\n\u2022 FID (Fr\u00e9chet Inception Distance)\nFID is a widely used metric for measuring the statistical similarity between generated images and real images, particularly in evaluating the performance of generated images and real images, particularly in evaluating the performance of generative models. This metric calculates the Fr\u00e9chet distance between the multivariate Gaussian distributions of features extracted from a pre-trained Inception-v3 network. A lower FID score indicates that the quality and diversity of the generated images are closer to those of the real images. The FID is computed as follows:"}, {"title": "4.3 Quantitative Evaluations", "content": "We evaluate the performance of our proposed method, Pix2Next, against several state-of-the-art image-to-image translation models on the Ranus dataset. The quantitative results are presented in Table 1.\nAs shown in Table 1, Pix2Next consistently outperformed the competing methods across all metrics and achieves state-of-the-art (SOTA) results on image generation performance on the RANUS dataset. Specifically, our model achieved a PSNR of 20.83, which is a 1.76% improvement over the best-performing baseline, Pix2pixHD. In terms of SSIM, Pix2Next recorded a value of 0.8031, representing an 8.39% increase over the next best model. The FID score, a critical measure of the generated images' fidelity, was significantly reduced to 28.01, marking a 34.81% improvement over the closest model, CycleGAN, the best-performing"}, {"title": "4.4 Qualitative Evaluations", "content": "Figure 7 showcases the qualitative performance of Pix2Next compared to other image translation methods, including Pix2pix, Pix2pixHD, Cycle-GAN, and BBDM, alongside the ground truth (GT). The results clearly demonstrate Pix2Next's superior ability to preserve image details and produce realistic outputs.\nIn comparison to other methods, Pix2Next delivers images with sharper details and fewer artifacts such as spatial distortion and under-styling. For example, in the first row of Figure 7, Pix2Next effectively maintains the structural integrity of the building and surrounding vegetation, whereas Pix2pix and Pix2pixHD suffer from significant distortions and loss of detail. Similarly, CycleGAN and BBDM generate outputs with visible artifacts and less accurate texture representation, particularly in the foliage and architectural elements. In contrast, Pix2Next closely matches the ground"}, {"title": "4.5 Ablation Study", "content": ""}, {"title": "4.5.1 Effectiveness of Extractor", "content": "To evaluate the effectiveness of the feature extractor in our proposed method, we conducted an ablation study by comparing the performance of the model without a feature extractor (W/O Extractor) to versions using different vision foundation models as feature extractors. As shown in Table 2, the model without a feature extractor yields an FID of 31.26, LPIPS of 0.1116, and DISTS of 0.132. These results indicate that the absence of a feature extractor leads to suboptimal performance. On the other hand, using advanced models like the Vision Transformer (ViT) and SwinV2 shows clear improvements over the absence of an extractor. The ViT-based extractor achieves a FID of 29.05, LPIPS of 0.1185, and DISTS of 0.1338, while using the SwinV2-based extractor results in a FID of 30.24, LPIPS of 0.1117, and DISTS of 0.1299, both outperforming the model without an extractor.\nThe best results are achieved with the Internimage-based feature extractor, which significantly enhances the model's performance, achieving the lowest FID of 28.01, LPIPS of 0.107, and DISTS of 0.1252. This indicates that the choice of feature extractor is crucial for optimizing model"}, {"title": "4.5.2 Effectiveness of Attention Position", "content": "To determine the optimal position for applying attention mechanisms within our network, we conducted an ablation study comparing two configurations on Pix2Next(SwinV2): applying attention solely at the \"B\"ottleneck layer (B-attention) versus applying attention across all key stages of the network, means the \"E\"ncoder, \"B\"ottleneck, and \"D\"ecoder (EBD-attention). The results of this study are presented in Table 3. When attention is distributed across the encoder, bottleneck, and decoder stages, the model shows notable improvements across all metrics. Specifically, the SSIM increases to 0.8063, and the FID decreases significantly to 30.24, indicating better alignment with the ground truth images. Additionally, LPIPS is reduced to 0.1117 and DISTS to 0.1299, suggesting that applying attention throughout the network leads to better feature representation and more accurate image translation. These findings suggest that distributing attention across multiple stages of the network -rather than concentrating it solely on the bottleneck- leads to superior performance in image translation tasks. The application of attention throughout the encoder, bottleneck, and decoder allows the model to effectively capture and refine features at various levels of abstraction."}, {"title": "4.5.3 Effectiveness of Generator", "content": "To assess the effectiveness of the generator design in our proposed method, we conducted an ablation study comparing the performance of the baseline Pix2pixHD model, a modified version of Pix2pixHD where residual blocks were replaced with our extractor (Internimage-based) blocks, and our full model that integrates both the Internimage-based feature extractor and our encoder-decoder based generator. The results are summarized in Table 4. The baseline Pix2pixHD model, which uses traditional residual blocks, achieves a PSNR of 20.474, SSIM of 0.7409, FID of 53.38, and RMSE of 8.53. These metrics serve as the foundation for evaluating the enhancements brought by the modifications. By replacing the residual blocks with Internimage blocks, the Pix2pixHD+Internimage model shows improvements in most of the metrics. Specifically, there is a slight increase in PSNR to 20.87 and a reduction in FID to 45.14, indicating better image quality and closer alignment with the ground truth distribution. However, the SSIM decreases to 0.7327. These results suggest that while the integration of Internimage blocks improves certain aspects of image quality, it may not universally enhance all performance metrics. Our full model, which incorporates both the Internimage-based feature extractor and encoder-decoder based generator, delivers the best performance across all metrics. The substantial improvement in SSIM and FID highlights the effectiveness of our encoder-decoder based generator architecture."}, {"title": "4.6 Effectiveness of Generated NIR Data", "content": "To assess the effectiveness of the NIR data generated by our model, we performed an ablation study on a downstream object detection task. To do this, we employed the Co-DETR model (Zong et al. (2023)), which is currently the state-of-the-art object detection model. We followed two different methods while finetuning the Co-DETR model:\nIn the first method, we used the object annotations in the RANUS dataset and finetuned the model using the training split of the RANUS dataset (Finetune w/ Ranus). In the second method, in order to evaluate the generalizability of our proposed translation model to unseen data, we generated 10,000 NIR images from RGB images of the BDD100k dataset (results are given in Figure 10). These images were used to scale up the RANUS train set, and the newly scaled-up dataset was employed to finetune the Co-DETR model (Finetune w/ Ranus + Gen NIR). Additionally, to establish a baseline for comparison, we also reported the object detection performance of the"}, {"title": "4.7 LWIR translation", "content": "To explore the translation capabilities of our model in different wavelengths, we conducted further experiments on LWIR translation using the aligned FLIR dataset (FLIR (2024)). This dataset comprises 4,113 aligned RGB-LWIR image pairs for training and 1,029 pairs for testing. Specifically, we trained our Pix2Next (SwinV2) model on the dataset's training set and reported the evaluation results on the same test set, comparing them with other methods from the literature (Table 6).\nOur model achieved state-of-the-art performance compared to existing methods as reported in the literature (Chen et al. (2024)). These results validate the effectiveness of Pix2Next in the LWIR domain and also suggest promising avenues for expanding the translation capabilities to other wavelength images in future work."}, {"title": "5 Discussion and Failure Cases", "content": "Unlike traditional methods, our model leverages a vision foundation model to extract global features and employs cross-attention mechanisms to effectively integrate these features into the generator. This approach enables our model to preserve both the overall structure and fine details of the RGB domain, resulting in generated images that are closer to the ground truth (GT) compared to existing methods. As a result, it achieves state-of-the-art (SOTA) image generation performance on the RANUS dataset.\nWhile the proposed translation model demonstrates robust performance in generating NIR images from RGB inputs, there is still room for improvement, especially in instances where it fails to accurately reproduce certain material properties, as illustrated in Figure 12. Specifically, the model encounters challenges in replicating the unique reflectance characteristics of particular materials, notably cloth, and vehicle lights. This shortcoming may be attributed to an underrepresentation of paired images exhibiting these specific characteristics within our training dataset.\nTo overcome these challenges, we plan to continuously refine the model architecture. A promising direction is the integration of diffusion-based models, which have demonstrated potential in capturing fine-grained details and enhancing the robustness of image generation across diverse scenarios."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we proposed a novel image translation model, Pix2Next, designed to address the challenges of generating Near-Infrared (NIR) images from RGB inputs. Our model leverages the strengths of state-of-the-art vision foundation models, combined with an encoder-decoder architecture that incorporates cross-attention mechanisms, to produce high-quality NIR images from RGB images.\nOur extensive experiments, including quantitative and qualitative evaluations as well as ablation studies, demonstrated that Pix2Next outperforms existing image translation models across various metrics. The model showed significant improvements in image quality, structural consistency, and perceptual realism, as evidenced by superior performance in PSNR, SSIM, FID, and other evaluation metrics. Furthermore, our zero-shot experiment on the BDD100k dataset confirmed the model's robust generalization capabilities to unseen data.\nWe validated the utility of Pix2Next by demonstrating performance improvements in an object detection downstream task, achieved by scaling up limited NIR data using our generated images.\nIn future work, we aim to extend the application of this architecture to other multispectral domains, such as RGB to extended infrared (XIR) translation, further broadening the scope of our model's applicability."}]}