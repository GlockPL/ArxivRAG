{"title": "DO MUSIC GENERATION MODELS ENCODE MUSIC THEORY?", "authors": ["Megan Wei", "Michael Freeman", "Chris Donahue", "Chen Sun"], "abstract": "Music foundation models possess impressive music generation capabilities. When people compose music, they may infuse their understanding of music into their work, by using notes and intervals to craft melodies, chords to build progressions, and tempo to create a rhythmic feel. To what extent is this true of music generation models? More specifically, are fundamental Western music theory concepts observable within the \"inner workings\" of these models? Recent work proposed leveraging latent audio representations from music generation models towards music information retrieval tasks (e.g. genre classification, emotion recognition), which suggests that high-level musical characteristics are encoded within these models. However, probing individual music theory concepts (e.g. tempo, pitch class, chord quality) remains under-explored. Thus, we introduce SynTheory, a synthetic MIDI and audio music theory dataset, consisting of tempos, time signatures, notes, intervals, scales, chords, and chord progressions concepts. We then propose a framework to probe for these music theory concepts in music foundation models (Jukebox and MusicGen) and assess how strongly they encode these concepts within their internal representations. Our findings suggest that music theory concepts are discernible within foundation models and that the degree to which they are detectable varies by model size and layer.", "sections": [{"title": "1. INTRODUCTION", "content": "State-of-the-art text-to-music generative models [1-3] exhibit impressive generative capabilities. Past work suggests that internal representations of audio extracted from music generative models encode information relating to high-level concepts (e.g. genre, instruments, or emotion) [4-7]. However, it remains unclear if they also capture underlying symbolic music concepts (e.g. tempo or chord progressions) [8].\nWe aim to investigate whether and to what extent state-of-the-art music generation models encode music theory concepts in their internal representations. Confirming this could enable the creative alteration of these concepts, providing artists with new methods towards more detailed and lower-level control [9] (e.g. changing the key of a song or editing a particular chord in a chord progression). Furthermore, by benchmarking these foundation models, we identify potential avenues for improvement towards stronger concept encoding. Our approach builds upon work in probing and editing concepts in language models, which have shown promise in identifying emergent representations in autoregressive models and editing factual knowledge [9-12]. While previous work probed music generation models for high-level concepts, such as emotion, genre, and tagging [4-7], we focus on uncovering representations of more granular music theory concepts. Although existing datasets such as HookTheory [13] contain rich music theory annotations, their association with copyrighted music potentially complicates their use.\nOur first contribution is a framework that generates diagnostic datasets for probing music theory concepts in music generation models, allowing programmatic control over concept selection while mitigating the presence of potential distractors. Our synthetic music theory dataset, SynTheory, consists of seven music concepts based on Western music theory: tempo, time signatures, notes, intervals, scales, chords, and chord progressions. SynTheory serves as a customizable, copyright-free, and scalable approach towards generating diagnostic music clips for probing music generative models trained on real-world data.\nOur second contribution is the analysis of two state-of-the-art music generative models Jukebox [3] and MusicGen [1] with our SynTheory benchmark. We extract representations for the concepts defined in SynTheory from MusicGen and Jukebox and assess whether these models encode meaningful representations of these concepts. To analyze the internal representations of these models, we train probing classifiers [14] based on ground truth music theory concept labels. A higher classification accuracy implies that these models learn internal representations that \"understand\" music theory concepts, which can be decoded by a multi-layer perceptron (MLP) or a linear model.\nOur results show that music foundation models encode meaningful representations of music theory concepts. These representations vary across different sections of the model (audio codecs, decoder LMs), different layers within the decoder LMs, and different model sizes. Furthermore, the nature of the concepts, from time-varying (e.g. chord progressions) to stationary (e.g. notes, chords), influence the performance of these models across these tasks. We hope our insights on probing music foundation models, along with the synthetic music data generation framework, encourage and facilitate future endeavors on symbolic controllability in music generative models.\nFor reproducibility, we upload our SynTheory dataset to Hugging Face\u00b9, release the code for dataset generation, embedding extraction, probing, and evaluation on GitHub \u00b2, and showcase audio samples on our website \u00b3."}, {"title": "2. RELATED WORK", "content": "The success of large language models (LLMs) [15-18] has sparked new research on probing their internal representations to measure their understanding of linguistic concepts [19, 20] and world knowledge [11, 12, 21] as well as editing the encoded knowledge to make LLMs more faithful to factual knowledge [9, 10]. Studies have shown that LLMs can encode grounded representations on color [22], direction [23], and auditory representations [24]. Thus, we investigate if music generative models, which often share similar model architectures and training objectives as LLMs, are able to encode abstract concepts from high-level music tags (e.g. genre, emotion) to fine-grained music theory knowledge (e.g. tempo, chords).\nRecent work has shown promise towards uncovering conceptual representations from probing audio and music generative models and leveraging different music foundation model architectures towards music understanding tasks. Castellon and Donahue et al. [4] propose using representations from language models trained on codified audio towards downstream MIR tasks as a better alternative to conventional tagging models. The authors train probing classifiers on Jukebox representations on music tagging, genre identification, key identification, and emotion recognition tasks. These results demonstrate the effectiveness of using internal model representations for downstream MIR tasks. Koo et al. [7] focus primarily on probing MusicGen's attention heads in instrument recognition tasks, benchmarking against the tasks highlighted in [4], and propose leveraging these representations for inference-time control. Other works [5,6] assess the impact of model architectures and self-supervised approaches towards music understanding tasks.\nHowever, prior work primarily uses real-world data, which is often concept-entangled and potentially subject to copyright concerns. For example, some of these works use Giantsteps-MTG and Giantsteps [25], which are datasets of primarily electronic dance music with tempo and key annotations, obtained from Beatport. Won et al. [5] use HookTheory for chord recognition, where they focus on major and minor chord identification for each pitch class. The authors also use Harmonix Set [26] and GTZAN [27] for beat and downbeat detection. In the language modality, the authors of ChatMusician [28] produce a multi-choice question answering dataset, MusicTheoryBench, with expert annotation from a professional college music teacher. MusicTheoryBench aims to assess the music understanding capabilities of LLMs through natural language alone. To the best of our knowledge, there is a lack of music theory probing benchmarks in the audio domain that are labeled in detail, copyright-free, and scalable, prior to our proposed SynTheory."}, {"title": "3. SYNTHEORY: SYNTHETIC DATASET OF MUSIC THEORY CONCEPTS", "content": "We design seven datasets to capture isolated music theory concepts - similar to synthetic audio for ear training. Musicians may \"train their ear\" to recognize music concepts like intervals or chord quality in an isolated setting before advancing to the harder, more entangled case that arises in non-pedagogical music. Assessing concept recognition through isolated concepts mitigates the possibility that one intuits or guesses the answer from its context. Literature on instrument-specific absolute pitch in humans corroborates the notion that timbral information may be exploited in identifying a different concept like pitch class [29]. As such, our dataset is designed to remove or reduce features that may correlate with a concept, but are not strictly necessary for identifying it. Our intent is a more pointed assessment towards theoretical concepts as abstract ideas rather than as acoustically realized audio. A more practical motivation for this work is that extracting such low-level, isolated concepts from existing datasets may require non-trivial engineering or domain expert labor. It may even be impossible to disentangle all overlapping concepts. Music stem isolation and concept isolation are distinct; an isolated instrument in a multi-track recording may still exhibit several, intricately intertwined theory concepts. It is not clear how to \"unmix\u201d such concepts once they are blended.\nInstead of attempting to disentangle several concepts from existing audio, SynTheory implements this \u201cear training\" quiz setting by explicitly producing individual concepts. Each of the seven datasets ablates a single musical feature while fixing all others, thereby isolating it to a degree not typically found in recorded music. These ablated concepts consist of tempo, time signatures, notes, intervals, scales, chords, and chord progressions. We adopt isolation as a design choice to mitigate context that may be exploited in deep learning models as \"shortcuts\", i.e. heuristics that correlate with concepts most of the time but do not truly encode the concept.\nUsing this music theory concept-driven synthesis design, we construct label-balanced and copyright-free data. The synthetic approach avoids annotation errors present in other contemporary MIR datasets. For example, the HookTheory data processing step for SheetSage [13] required ad-hoc time-alignment of the expert annotations. In the released SheetSage dataset, 17, 980/26, 175 (68.7%) samples required more precise time alignment. While our synthetic data is no substitute for real music data, to our knowledge, no other dataset so strictly isolates each concept to this level of granularity.\nSynTheory contains two categories: tonal and rhythmic. We make this distinction for stronger concept isolation; we wish to keep the rhythm samples tonally consistent and the tonal samples rhythmically consistent. For each tonal dataset, we voice the same MIDI data through 92 distinct instruments. The selection of instrument voices is fixed, making the distribution of timbres sufficiently diverse but also class-balanced. Each instrument corresponds to one of the 128 canonical MIDI program codes and is voiced through the TimGM6mb.sf2 [30] soundfont. A MIDI \"program\" is a specific instrument preset. The canonical program set includes many instruments, e.g. \"Acoustic Grand Piano\u201d, \u201cFlute\u201d, etc. We exclude programs that are polyphonic, sound effects (e.g. \"Bird Tweet\", \"Gun Shot\"), and highly articulate. A highly articulate program has some unchangeable characteristic (e.g. pitch bending) that destabilizes its pitch. For each rhythmic dataset, we define five metronome-like timbral settings. Each setting uses one of the distinct instruments: \"Woodblock Light\", \u201cWoodblock Dark\u201d, \u201cTaiko\u201d, \u201cSynth Drum\u201d, and the MIDI drum-kit, following the voicing done in Sheetsage [13]. Each setting produces a distinct sound on the upbeat and the downbeats, which defines the time signature concept.\nOne can extend or alter these configurations using the SynTheory codebase. We provide a framework that enables declarative and programmatic MIDI construction in musical semantics, audio export in any soundfont, and dataset construction for use in our framework."}, {"title": "3.1 SynTheory-Rhythmic", "content": ""}, {"title": "3.1.1 Tempo", "content": "We voice integer tempi from 50 to 210 BPM (beats per minute) inclusive in 4 time. To ensure diverse start times, we produce five random offset times per sample. There are (5 CLICK SETTING 161 TEMPO 5 OFFSET) = 4,025 samples in total."}, {"title": "3.1.2 Time Signature", "content": "We voice the following time signatures: 1, 4, 4, 3, 4, 8, 8, and at a fixed tempo of 120 BPM. To add acoustic variation, we add three levels of reverb from completely dry to spacious. We find empirically that this acoustic perturbation increases the difficulty of the probing task. Like the Tempo dataset, we produce ten random offset times per sample. There are (8 TIME SIGNATURE 3 REVERB LEVEL 5 CLICK SETTING.10 OFFSET ) = 1, 200 samples."}, {"title": "3.2 SynTheory-Tonal", "content": ""}, {"title": "3.2.1 Notes", "content": "We voice all twelve Western temperament pitch classes, in nine octaves, using 92 instruments. The note is played in quarter notes at a tempo of 120 BPM, with no distinction between the upbeat or downbeat. There are (12 PITCH CLASS 9 OCTAVE 92 INSTRUMENT) = 9,936 configurations. However, there are only 9,848 distinct samples because 88 configurations at extreme registers are unvoiceable in our soundfont. These silent samples are listed for completeness in our Hugging Face dataset."}, {"title": "3.2.2 Intervals", "content": "We vary the root note, number of half-steps, instrument, and play style (unison, up, and down). To retain consistent rhythm, the up and down styles repeat four times throughout the sample while the unison play style repeats eight times. There are (12 PITCH CLASS\u00b712 HALF-STEP 92 INSTRUMENT\u00b73 PLAY STYLE ) = 39, 744 samples."}, {"title": "3.2.3 Scales", "content": "We voice seven Western music modes (Ionian, Dorian, Phrygian, Lydian, Mixolydian, Aeolian, and Locrian) in all root notes, in 92 instruments, and in two play styles (ascending or descending). The register is constant; we select root notes close to middle C. There are (7 MODE 12 ROOT NOTE 92 INSTRUMENT 2 PLAY STYLE) = 15, 456 samples."}, {"title": "3.2.4 Chords", "content": "We voice triads of all twelve root notes, four chord qualities (major, minor, augmented, and diminished), 92 instruments, and three inversions (root position, first inversion, and second inversion). The chord is struck at each quarter note at 120 BPM. Like the Scales dataset, we fix the register close to middle C. There are (12 ROOT NOTE 4 CHORD QUALITY92 INSTRUMENT 3 INVERSION ) = 13, 248 samples."}, {"title": "3.2.5 Chord Progressions", "content": "We select 19 four-chord progressions, with ten in the major mode and nine in the natural minor mode:\n\u2022 Major: (I-IV-V-I), (I-IV-vi-V), (I-V-vi-IV), (I-vi-IV-V), (ii-V-I-vi), (IV-I-V-vi), (IV-V-iii-vi), (V-IV-I-V), (V-vi\u2013IV\u2013I), (vi-IV-I-V)\n\u2022 Natural Minor: (i-ii\u00b0-v-i), (i-III-iv-i), (i-iv-v-i), (i-VI-III-VII), (i-VI-VII-i), (i-VI-VII-III), (i-VII-VI-IV), (iv-VII-i-i), (VII-vi-VII-i)\nWe vary the root note of the key and instrument. Each chord is played at each quarter notes at 120 BPM. There are (19 PROGRESSION 12 KEY ROOT 92 INSTRUMENT) = 20,976 samples."}, {"title": "4. EXPERIMENTS", "content": "We describe our approach to assess how well the internal representations of music generative models (MusicGen and Jukebox) and handcrafted audio features (mel spectrograms, MFCC, and chroma) encode music theory concepts."}, {"title": "4.1 Evaluation", "content": "A \"probe\" is a simple classifier, often a linear model, trained on the activations of a neural network [14]. Accurate performance of such classifiers suggests that information relevant to the class exists in the latent representations within the network. As such, probes may be used as a proxy for measuring a model's \u201cunderstanding\" or encoding of abstract concepts. Motivated by the use of probes to discover linguistic structure and semantics in NLP [31] and more recently in MIR [4], we use probes to assess whether music theory concepts are discernable in the internal representations of foundation models.\nWe adopt the same probing paradigm as [4] and frame concept understanding as multiclass classification for discrete concepts (notes, intervals, scales, chords, chord progressions, and time signatures) and regression for continuous concepts (tempo). We train linear and two-layer MLP probes on the embeddings of the internal representations of Jukebox and MusicGen and the handcrafted features.\nEach probe is trained independently for its corresponding concept task. That is, the probe trained to identify notes from Jukebox embeddings will not be used to identify intervals, for example.\nFor the classification tasks, we measure the accuracy of our trained probes on the following SynTheory tasks:\n\u2022 Notes (12): C, C#, D, D#, E, F, F#, G, G#, A, A#, and B\n\u2022 Intervals (12): minor 2nd, Major 2nd, minor 3rd, Major 3rd, Perfect 4th, Tritone, Perfect 5th, minor 6th, Major 6th, minor 7th, Major 7th, and Perfect octave\n\u2022 Scales (7): Ionian, Dorian, Phrygian, Lydian, Mixolydian, Aeolian, and Locrian\n\u2022 Chords (4): Major, Minor, Diminished, and Augmented\n\u2022 Chord Progressions (19): (I\u2013IV\u2013V\u2013I), (I\u2013IV-vi\u2013V), (I-V-vi-IV), (I-vi-IV-V), (ii-V-I-vi), (IV-I-V-vi), (IV-V-iii-vi), (V\u2013IV\u2013I\u2013V), (V-vi-IV\u2013I), (vi\u2212IV\u2013I\u2013V), (i-ii-v-i), (i-III-iv-i), (i-iv-v-i), (i-VI-III-VII), (i-VI-VII-i), (i-VI-VII-III), (i-VII-VI-IV), (iv-VII-i-i), and (VII-vi-VII-i)\n\u2022 Time Signatures (8): 3, 4, 4, 3, 4, 8, 8, and\nThese tasks are trained on a 70% train, 15% test, and 15% validation split, using the Adam optimizer and Cross Entropy loss.\nFor the Tempo dataset, we train a regression probe, over the 161 tempo values. To increase complexity in the probing task and test generalization to unseen BPMs, the training set consists of the middle 70% of the BPMs. The test and validation sets consist of the top 15% BPMs and the bottom 15% BPMs, randomly shuffled and split in half. We use MSE loss and report the R\u00b2 score.\nTo select the best performing probe for each concept, we perform a grid search across various hyperparameters for the MusicGen audio codec, mel spectrogram, MFCC, chroma, and aggregate handcrafted features, following [4]:\n\u2022 Data Normalization: {True, False}\n\u2022 Model Type: {Linear, two-layer MLP with 512 hidden units and ReLU activation}\n\u2022 Batch Size: {64, 256}\n\u2022 Learning Rate: {$10^{-5}$, $10^{-4}$, $10^{-3}$}\n\u2022 Dropout: {0.25, 0.5, 0.75}\n\u2022 L2 Weight Decay: {off, $10^{-4}$, $10^{-3}$}\nFor the decoder language models (MusicGen small, medium, and large and Jukebox), we use a fixed set of hyperparameters and select the probe with the best performing layer for each concept, in the interest of computational efficiency:"}, {"title": "4.2 Model representations", "content": "We extract representations from two text-to-music foundation models, Jukebox [3] and MusicGen [1] and benchmark them against three handcrafted, spectral features following [4]: mel spectrograms, mel-frequency cepstral coefficients (MFCC), and constant-Q chromagrams (chroma). These handcrafted features are common in traditional methods of MIR and are a more interpretable baseline against the embeddings of the pre-trained music generative models.\nJukebox consists of a VQ-VAE model that codifies audio waveforms into discrete codes at a lower sample rate and a language model that generates codified audio with a transformer decoder. We trim each audio sample to four seconds and convert it to mono. Using Jukemirlib [4], we pass the audio through the frozen audio encoder and through the decoder language model. Due to resource constraints, we downsample the activation to a target rate of half that in [4], using the Librosa FFT algorithm [32]. Then, we meanpool the representations across time to reduce the dimensionality of the embeddings, resulting a dimension of (72, 4800) per sample, where 72 is the number of layers and 4800 is the dimension of the activations. To further reduce the dimensions, we adopt a layer selection process similar to [4]. We train the probe classifiers with fixed hyperparameters on the music concept tasks as described in Section 4.1. For each concept, we select the layer that results in the highest probing score. This results in a final dimension of 4800 for each Jukebox representation.\nMusicGen consists of a pretrained convolutional auto-encoder (EnCodec) [33], a pretrained T5 text encoder, and an acoustic transformer decoder. We resample the audio to 32 kHz (the sampling rate used in the EnCodec model), trim to four seconds, convert to mono, and pass the audio through the frozen EnCodec audio codec. We do not pass text through the text encoder, as we focus on audio representations. We then extract representations from several regions of the model: the final layer of the audio codec before residual vector quantization and the hidden states of the decoder language model. The number of decoder hidden states vary based on the model size: small (24 layers), medium (48 layers), and large (48 layers).\nFor our four second audio clips, the audio codec representations are of dimension (128,200), where 128 is the dimension of the activation after the final layer of the audio codec and 200 is the sequence length. We meanpool the values of the representations across time, resulting in a final dimension of 128 for the MusicGen audio codec.\nThe decoder hidden states for the small, medium, and large MusicGen models have dimensions (24, 200, 1024), (48, 200, 1536), and (48,200,2048) respectively, where the first axis corresponds to the number of layers, second corresponds to sequence length, and third corresponds to hidden size. To reduce the dimensions, we meanpool across time, resulting in dimensions of (24, 1024) for MusicGen small decoder, (48, 1536) for MusicGen medium decoder, and (48, 2048) for MusicGen large decoder. Similar to our approach with Jukebox, we select the most optimal layer for each decoder model size based on the probing scores. After selecting the best performing layer per concept and model size, the dimensions of the representations are 1024 for MusicGen small, 1536 for MusicGen medium, and 2048 for MusicGen large.\nWe extract the handcrafted features (mel spectrograms, mel-frequency cepstral coefficients, and constant-Q chromagrams) with librosa [32]. Similar to [4], we concatenate the mean and standard deviation across time of these features along with their first- and second-order discrete differences. To obtain an aggregate representation of the handcrafted features, we concatenate the mel spectrogram, chroma, and MFCC features and obtain their mean and standard deviation across time and their first- and second-order differences."}, {"title": "5. RESULTS AND DISCUSSION", "content": "Overall, we observe that music generation models do encode music theory concepts in their internal representations. Jukebox performs consistently well across all SynTheory tasks. MusicGen Decoder LMs also exhibit competitive performance across our benchmark. Interestingly, we observe that MusicGen Decoder (Small) outperforms its larger counterparts. This finding contradicts traditional discussions on scaling laws and the claims in [1] that larger MusicGen models better \"understand\" text prompts and produce better quantitative and subjective scores. Figure 2 further highlights this finding, showing that MusicGen Decoder (Small) maintains consistently high probing scores across all layers compared to that of its larger counterparts. Meanwhile, larger MusicGen models display a steep performance drop in the initial layers, followed by a gradual increase and then a slight decline in the final layers. This pattern suggests that the smaller model may have developed a more efficient encoding of music theory concepts within its representations.\nWhile these models perform well on other concepts, MusicGen slightly underperforms on the notes dataset. We hypothesize this may be due to isolated notes being less prominent as intervals, scales, and chords in real-world music. This reveals the challenge in distinguishing the most fundamental building blocks of music.\nPretrained music decoder LMs generally outperform MusicGen Audio Codec representations and individual handcrafted features. MusicGen Audio Codec exhibits poorer overall performance, since these codecs were trained to reconstruct fine-grained, low-level details localized by time.\nChroma features, which encode pitch class information, perform comparably well on tonal tasks, but slightly underperform on rhythmic tasks. Notably, chroma features outperform MusicGen Decoder LMs on stationary harmonic tasks (notes, scales, and chords) but lag behind on dynamic harmonic tasks (chord progressions and intervals).\nThe aggregate handcrafted features perform comparably to MusicGen Decoder LMs. This suggests that harder music concept understanding benchmarks should address concepts latent in foundation models but not easily encoded in handcrafted features. These harder benchmarks may include entangled concepts, such as probing for both chord progression type and tempo in a chord progression samples of varying tempos. Probing for compositional tasks could reveal differences in concept encoding between model representations and handcrafted features.\nWhile these models have shown that they encode representations for music theory in audio, some state-of-the-art music foundation models primarily rely on text controls. In a future study, we plan to probe for language representations in music generation models to improve text-controllable methods."}, {"title": "6. CONCLUSION", "content": "We introduce SynTheory, a synthetic dataset of music theory concepts, that is concept-isolated, copyright-free, and scalable. We use SynTheory to evaluate the degree to which music theory concepts are encoded in state-of-the-art music generative models. Our experiments suggest that music theory concepts are indeed discernible within the latent representations of these models. We believe this will allow us to understand how to isolate and manipulate such concepts, which advances towards controllable generation and music theory benchmarks. We encourage the community to build more challenging probing datasets with our framework to further understand the relationship between symbolic and audio-based music generation."}, {"title": "7. ETHICS STATEMENT", "content": "Our work aims to understand if music generation models encode music theory concepts in their internal representations. Our dataset may be used to assess music generation models and may be applied towards fine-grained, music-theory based controllable generation.\nOur custom dataset, SynTheory, is based on elementary Western music theory concepts and is generated programmatically. The data does not infringe copyright of musical writers or performers."}]}