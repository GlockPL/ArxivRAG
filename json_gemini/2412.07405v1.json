{"title": "MODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning", "authors": ["Yufei Ma", "Zihan Liang", "Huangyu Dai", "Ben Chen", "Dehong Gao", "Zhuoran Ran", "Zihan Wang", "Linbo Jin", "Wen Jiang", "Guannan Zhang", "Xiaoyan Cai", "Libin Yang"], "abstract": "The growing demand for larger-scale models in the development of Large Language Models (LLMs) poses challenges for efficient training within limited computational resources. Traditional fine-tuning methods often exhibit instability in multi-task learning and rely heavily on extensive training resources. Here, we propose MODULA (Mixture of Domain-Specific and Universal LoRA), a novel Parameter Efficient Fine-Tuning (PEFT) Mixture-of-Expert (MoE) paradigm for improved fine-tuning and parameter efficiency in multi-task learning. The paradigm effectively improves the multi-task capability of the model by training universal experts, domain-specific experts, and routers separately. MODULA-Res is a new method within the MoDULA paradigm, which maintains the model's general capability by connecting universal and task-specific experts through residual connections. The experimental results demonstrate that the overall performance of the MODULA-Flan and MODULA-Res methods surpasses that of existing fine-tuning methods on various LLMs. Notably, MODULA-Res achieves more significant performance improvements in multiple tasks while reducing training costs by over 80% without losing general capability. Moreover, MODULA displays flexible pluggability, allowing for the efficient addition of new tasks without retraining existing experts from scratch. This progressive training paradigm circumvents data balancing issues, enhancing training efficiency and model stability. Overall, MODULA provides a scalable, cost-effective solution for fine-tuning LLMs with enhanced parameter efficiency and generalization capability.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in open-source Large Language Models (LLMs), such as LLaMA (Touvron et al., 2023a), Qwen (Bai et al., 2023), and Yi (Young et al., 2024), have achieved notable successes in natural language processing. However, the increasing complexity and growing size of these models make efficient training within limited computational resources challenging. Researchers tried to address this with Parameter Efficient Fine-Tuning (PEFT), such as LoRA (Hu et al., 2021), Prefix Tuning (Liu et al., 2023), and (IA)\u00b3 (Liu et al., 2022). LoRA has gained prominence for its high performance using low-rank matrices, but it often encounters instability when trained on large, mixed datasets. To mitigate this issue, MoLoRA (Zadouri et al., 2024) has been introduced by extending LoRA and integrating the Mixture-of-Expert (MoE) architecture as shown in Figure 1(a). This approach trains multiple LoRA-adapters concurrently, each serving as an expert, to enhance the base LLMs' generalization ability across diverse tasks. The integration of MoE into LoRA aims to improve training efficiency and stability, facilitating more effective fine-tuning of large-scale language models for a wide range of natural language processing applications.\nDespite its advantages, MoLoRA has some limitations. One limitation is the absence of domain-specific LoRA adapters, as the same experts are employed universally across all tasks. This uniformity may limit the performance ceiling, especially for significantly distinct tasks like math and code, where the inclusion of domain-specific experts could potentially enhance performance (Zeng et al., 2021). Another challenge is the limited pluggability of MoLORA; adding new task capabilities necessitates retraining all parameters from all experts, which can be inefficient and time-consuming.\nTo address the challenges, we propose a three-stage training paradigm called MoDULA, where different domain-specific experts can be trained separately. Moreover, we introduce a more advanced method MoDULA-Res (Mixture of Domain-"}, {"title": "2 Related Works", "content": "Recently, the field of natural language processing has witnessed a paradigm shift with the advent of LLMs (Anil et al., 2023b; Almazrouei et al., 2023; Xu et al., 2023; Scao et al., 2022; Brown et al., 2020; Achiam et al., 2023; Zhang et al., 2023; Du et al., 2022). These state-of-the-art models have departed from traditional approaches that"}, {"title": "2.1 Large Language Model", "content": "Recently, the field of natural language processing has witnessed a paradigm shift with the advent of LLMs (Anil et al., 2023b; Almazrouei et al., 2023; Xu et al., 2023; Scao et al., 2022; Brown et al., 2020; Achiam et al., 2023; Zhang et al., 2023; Du et al., 2022). These state-of-the-art models have departed from traditional approaches that relied on convolutional or recurrent architectures for feature extraction, instead embracing novel techniques such as BERT (Devlin et al., 2019), which leverages the power of Transformers trained on extensive datasets, yielding bidirectional encoder representations. Similarly, Generative Pre-trained Transformer (GPT) (Brown et al., 2020) employs decoder layers from Transformer architecture (Vaswani et al., 2017) as feature extractors and utilizes autoregressive training on vast texts.\nGuided by the principles of scaling laws (Kaplan et al., 2020), the development of LLMs has led to the emergence of colossal models boasting over 100 billion parameters, with prominent examples including GPT-4 (Achiam et al., 2023) and Gemini (Anil et al., 2023a). Interestingly, open-source models such as OPT (Zhang et al., 2022), Falcon (Almazrouei et al., 2023), and Gemma (Mesnard et al., 2024) have demonstrated competitive performance compared to their closed-source counterparts, despite possessing a more modest parameter count. The training process of LLMs typically involves leveraging immense amounts of textual data to enable the prediction of subsequent tokens, empowering these models to generate coherent and comprehensible responses to a wide range of prompts. This training method has proven to be highly effective in capturing the intricacies of language and paved the way for LLMs to achieve SOTA performance across various NLP tasks."}, {"title": "2.2 MoE for PEFT", "content": "Our research closely aligns with the work done by MOLORA (Zadouri et al., 2024), LoraHub (Huang et al., 2023a), MoELoRA (Liu et al., 2024), SiRA (Zhu et al., 2023), and C-Poly (Wang et al., 2023), which explore the intersection of PEFT and MOE. MOLORA employs a full soft MoE on top of LoRA, utilizing a learned gating mechanism to average all experts, and trains the experts in a single stage. LoraHub investigates LoRA composability for cross-task generalization and introduces a simple framework for the purposive assembly of LORA modules trained on diverse given tasks, aiming to achieve adaptable performance on unseen tasks. It can fluidly combine multiple LoRA modules with just a few examples from a new task, without requiring additional model parameters or human expertise. MoELORA devises multiple experts as the trainable parameters and proposes a task-motivated gate function for all MOELORA layers to regulate the contributions of each expert and generate distinct parameters for various tasks. SiRA proposes a sparse mixture of low rank adaptation that enforces the top k experts' routing with a capacity limit. It uses expert dropout to reduce over-fitting. C-Poly combines task-common skills and task-specific skills and jointly learns a skill assignment matrix.\nWhile these methods have significantly contributed to the field, they face particular challenges and limitations. Training experts on mixed datasets as in MoLoRA may lead to performance degradation due to data inconsistency and interference (Dong et al., 2024). LoraHub relies on few-shot examples in inference stage, and MoELORA requires task-id to determine which experts should be activated, which weaken the flexibility of both methods. Sparse routing, as used by SiRA, requires careful tuning of the top-k and capacity hyperparameters for each dataset. C-Poly's joint learning of task-common and task-specific skills can make balancing general and specialized abilities difficult. Additionally, incorporating new experts or skills in these methods may require retraining or modifying existing components, potentially impacting system stability and training complexity. Training new experts often demands substantial data, resulting in high training costs and sub-optimal performance in specific domains. Maintaining optimal performance on domain-specific benchmarks after adding new capabilities can be challenging, and newly added modules may not consistently achieve top performance in their respective benchmarks. These factors can affect the adaptability and efficiency of MoLoRA, SiRA, and C-Poly in meeting expanding task demands.\nIn contrast, MODULA method trains universal and domain-specific experts separately, mitigating performance degradation from mixed datasets. Designed with \"pluggability\" in mind, the MODULA method allows new experts to be added without changing existing ones, ensuring system stability and low training costs. After adding a new expert, only the router requires retraining to maintain near-optimal performance. This staged training balances general and domain-specific capabilities, making our method adaptable and efficient for growing task requirements."}, {"title": "3 Method", "content": "In this section, we present MoDULA for LLM fine-tuning. Within this paradigm, we propose two methods: MODULA-Flan and MODULA-Res. MODULA-Flan consists of a universal expert and an array of domain-specific experts, while MODULA-Res further incorporates residual connections between the universal and domain-specific experts to enhance performance and stability. Figure 1 illustrates the differences between MoLORA, our proposed MoDULA-Flan and MODULA-Res. In all of these, the base LLMs retain a frozen weight configuration, denoted as Wo, corresponding to the fixed linear layers within the architecture.\nMoLoRA. The MoLoRA method serves as the foundation of our MoDULA. As shown in Figure 1(a), the MoLoRA consists of a router and a set of LoRA experts E\u2081, E\u2082,..., E\u2099. Each expert E\u1d62 includes two key components: B\u1d39\u1d62 and A\u1d39\u1d62. The dynamics of the MoLoRA method can be summarized by the following equations:\ns\u1d39\u1d62 = \u03c3(x\u2098)\u1d62 = softmax(W\u1d2ex\u2098)\u1d62\n(1)\ny\u2098 = E\u1d39(x\u2098) + W\u2080x\u2098\n(2)\nE\u1d39(x\u2098) = \u2211\u1d62\u208c\u2081\u207fs\u1d39\u1d62 B\u1d39\u1d62A\u1d39\u1d62x\u2098\n(3)\nIn these equations, x\u2098 represents the hidden vector of the m-th token in the input sequence, s\u1d39\u1d62 denotes the routing coefficient for expert E\u1d62, W\u1d2e is the weight matrix of the router, and E\u1d39(\u22c5) expresses the collective function of the experts in the MOLORA module.\nMODULA. Based on MoLoRA, we propose a three-stage training paradigm called MoDULA, as illustrated in Figure 2. In the first stage, only the universal expert is trained, while the domain-specific experts and router are deactivated. In the second stage, the domain-specific experts are trained individually for each corresponding task, while the parameters of the universal expert are kept frozen. In the third stage, all the experts' parameters are fixed, and only the router is trained. With the MoDULA paradigm, we propose two methods: MODULA-Flan and MODULA-Res.\nMODULA-Flan. MODULA-Flan maintains the same architecture as MoLoRA, as illustrated in Figure 1(b). However, it implements the MODULA paradigm to separate the experts in MOLORA into universal expert and domain-specific experts. The specific training details are as follows. In the first stage, the universal expert E\u1da0\u02e1\u1d43\u207f is trained on universal datasets. In the second stage, the domain-specific experts E\u1da0\u02e1\u1d43\u207f\u2081, E\u1da0\u02e1\u1d43\u207f\u2082,..., E\u1da0\u02e1\u1d43\u207f\u2099 are trained on their respective domain-specific datasets. The forward process in this stage is formally articulated through Equations (4) and (8).\ny\u1da0\u02e1\u1d43\u207f = E\u1da0\u02e1\u1d43\u207f(x\u2098) + W\u2080x\u2098\n(4)\nwhere i \u2208 {1, 2, . . ., n}. In the third stage, the parameters of all experts are kept frozen, and only the router O\u1da0\u02e1\u1d43\u207f is trained. The calculation involved in this routing determination is formally illuminated through the following equations:\ns\u1da0\u02e1\u1d43\u207f\u1d62 = O\u1da0\u02e1\u1d43\u207f(x\u2098)\u1d62 = softmax(W\u1d3fx\u2098)\u1d62\n(5)\ny\u1da0\u02e1\u1d43\u207f = E\u1da0\u02e1\u1d43\u207f(x\u2098) + W\u2080x\u2098\n(6)\nE\u1da0\u02e1\u1d43\u207f(x\u2098) = \u2211\u1d62s\u1da0\u02e1\u1d43\u207f\u1d62E\u1da0\u02e1\u1d43\u207f\u1d62(x\u2098)\n(7)\nE\u1da0\u02e1\u1d43\u207f\u1d62(x\u2098) = B\u1da0\u02e1\u1d43\u207f\u1d62A\u1da0\u02e1\u1d43\u207f\u1d62x\u2098\n(8)"}, {"title": "4 Experiments", "content": "A detailed comparison is conducted among the standard LORA (Hu et al., 2021), MoLoRA (Zadouri et al., 2024), and our newly proposed MoDULA-Flan and MODULA-Res. The base models selected for this study include LLaMA-2 (Touvron et al., 2023b), Qwen (Bai et al., 2023), and Yi (Young et al., 2024). In the training of MoDULA, a batch size of 128 is utilized, encompassing 1 epoch with a learning rate of 2e-4. The maximum input sequence length is defined as 4096 tokens for both LLaMA-2 and Yi. In contrast, Qwen series has 8192 tokens due to variations in maximum positional embeddings among different model zoos. The intrinsic rank is configured to 16 for universal and 8 for domain-specific experts. For the multi-task results, the checkpoint selection is based on the average metrics across all tasks. To enhance fine-tuning efficiency, we leverage libraries like HuggingFace's Transformers (Wolf et al., 2020) and PEFT (Mangrulkar et al., 2022), based on which we design MoDULA."}, {"title": "4.1 Expert Configurations", "content": "A detailed comparison is conducted among the standard LORA (Hu et al., 2021), MoLoRA (Zadouri et al., 2024), and our newly proposed MoDULA-Flan and MODULA-Res. The base models selected for this study include LLaMA-2 (Touvron et al., 2023b), Qwen (Bai et al., 2023), and Yi (Young et al., 2024). In the training of MoDULA, a batch size of 128 is utilized, encompassing 1 epoch with a learning rate of 2e-4. The maximum input sequence length is defined as 4096 tokens for both LLaMA-2 and Yi. In contrast, Qwen series has 8192 tokens due to variations in maximum positional embeddings among different model zoos. The intrinsic rank is configured to 16 for universal and 8 for domain-specific experts. For the multi-task results, the checkpoint selection is based on the average metrics across all tasks. To enhance fine-tuning efficiency, we leverage libraries like HuggingFace's Transformers (Wolf et al., 2020) and PEFT (Mangrulkar et al., 2022), based on which we design MoDULA."}, {"title": "4.2 Training Datasets", "content": "To equip our MoDULA-Flan and MoDULARes with comprehensive capabilities across universal, mathematical, coding, and medical domains, the datasets airoboros-3.2 1, orca-mathword-problems-200k 2, CodeAlpaca-20k 3, and MedQA (Jin et al., 2019) are integrated."}, {"title": "4.3 Evaluation Benchmarks and Metrics", "content": "To comprehensively assess the performance of various methods, we conduct evaluations across a diverse set of benchmarks. Domain-specific performance is evaluated by testing mathematical abilities on GSM8K (Cobbe et al., 2021), Arithmetic (Brown et al., 2020), and MathQA (Amini et al., 2019), coding skills on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), and medical knowledge on MedQA (Jin et al., 2020)"}, {"title": "4.4 Main Experimental Results", "content": "Our experimental results yield several significant observations that demonstrate the robustness and effectiveness of the proposed approach, providing valuable insights into its performance across various benchmarks and real-world applications.\nSuperior Advancement over Baselines: Table 1 highlights the significant performance improvements achieved by our proposed paradigm across Qwen, LLaMA-2, and Yi. Models that are fine-tuned with our paradigm outperform the base models by an average of 16.6% and surpass the performance of MoLoRA by 6.3% on average. Notably, Yi demonstrates the most substantial improvement, with an impressive average increase of 10.9% over MoLORA.\nFurther analysis reveals that performance advancements are more pronounced in smaller-scale models than in their larger counterparts, e.g., 4.9% for Qwen-7B while 2.9% for Qwen-14B. This indicates that small-scale models with fewer parameters and inadequate training are more prone to losing general capability when learning multiple tasks, while residual connections can effectively mitigate this problem.\nMoreover, MoDULA-Flan does not consistently outperform MoLoRA, suggesting that it has the issue of decreased general capabilities (for example, the arithmetic benchmark of LLaMA-2-13B dropped sharply due to the decline in text understanding ability). In contrast, MoDULA-Res addresses this issue by introducing residual connections for general and expert modules, leading to more stable performance and significant improvements over MoLoRA and MODULA-Flan.\nDespite MoDULA-Res demonstrates overall strong performance, it faces challenges with GSM8K and MedQA tasks, likely due to the mismatch between pre-training data and task-specific requirements. We recognize these limitations and leave them for further research.\nExcellent Robustness on Comprehensive Benchmarks: In order to determine whether the general capability of MoDULA-Res trained on multiple tasks will decline, we conduct experiments using the base, MoLoRA, and the MoDULA-Res model on the comprehensive benchmarks MMLU and C-Eval.\nThe results in Table 3 indicate that the average performance of MoDULA-Res across multiple models is about 1% higher than that of MoLORA and the base model, suggesting that the model's general capability is maintained and even partially improved through residual connection.\nFlexible Pluggability over Baselines: To showcase MoDULA-Res's pluggability, we introduce the finance domain (FinGPT-headline) in addition to the initial domains of mathematics, coding, and medical care. Then, we retrained MoLORA, MODULA-Flan, and MODULA-Res, respectively. MoLORA is trained from scratch on the combined dataset, while MoDULA-Flan and MODULA-Res only require training a new financial expert and the router. This results in MODULA-Flan and MODULA-Res using only 19.8% and 37.3% of the training parameters and data compared to MOLORA, respectively.\nThe results in Table 4 indicate that MODULA-Res achieves the best average multi-task performance among the three models, with an average improvement of 8.0% in the financial task. Notably, the overall improvement of Yi-6B is more significant, exceeding 11.0%, due to the fewer parameters and relatively balanced pre-training data. MoLORA encounters issues with data balance, requiring numerous experiments to adjust the data ratio for each task to achieve the best overall performance when new domain-specific tasks are introduced, which is time-consuming and labor-intensive.\nOutstanding Performance in E-Commerce: To assess MODULA's practical applicability in ecommerce, we introduce title optimization and keyword recommendation tasks, which involve refining titles and generating keywords using high-exposure queries to enhance readability and include more key points. We employ GPT-4 to evaluate the optimized titles and keywords across five dimensions: helpfulness, relevance, accuracy, readability, and fluency. Each dimension is scored 0, 1, or 2, with a maximum total score of 10.\nTable 5 demonstrates that MoDULA-Res significantly improves performance on title optimization and keyword recommendation benchmarks, with gains of 44.7% and 24.3% over MoLoRA, respectively. Moreover, MoDULA-Res maintains superior performance on the original multi-task benchmarks. These results highlight MoDULA-Res's potential for e-commerce applications and adaptability to new tasks under resource constraints."}, {"title": "4.5 Analysis on Domain-specific Experts Allocation", "content": "To further analyze MoDULA-Res, router distributions for domain-specific experts based on Yi-6B and Qwen-14B are visualized in Figure 3. Models in Table 1 are reused, and we select layer 0-10-20-30 and 10-20-30-40 for Yi-6B and Qwen-14B, respectively.\nThe results indicate that for both the Yi and Qwen models, the router within the MODULA paradigm allows various experts to concentrate on their own domain. However, the interpretation of expert assignments varies across different layers in different models due to the model's training data and method. For instance, Yi's deeper layers focus more on separating experts, while Qwen in the shallower layers."}, {"title": "5 Conclusion", "content": "In this paper, we introduce MoDULA, a novel multi-stage training PEFT MoE paradigm that enhances efficiency and domain-specific adaptation for LLMs. By integrating universal and domain-specific experts through a three-stage training methodology, MoDULA optimizes both generalization and specialized performance. Experiments on various open-source LLMs, such as LLaMA-2, Qwen, and Yi, demonstrate that MoDULA outperforms existing methods, achieving over 80% reduction in training costs and a 5% performance improvement. These results highlight MoDULA's potential as a scalable and efficient solution for fine-tuning LLMs, paving the way for future advancements in NLP.\nWhile our proposed MoDULA paradigm shows significant advancements in parameter efficiency and multi-task adaptability for LLMs, there are still some limitations that need to be addressed. Despite the overall strong performance of MoDULA-Res, it shows sub-optimal results on certain benchmarks like GSM8K and MedQA. This may be due to discrepancies between the model's pre-training data and the specific task datasets, requiring further investigation to identify the root causes and develop targeted solutions. Our experiments also focus on a limited set of language models (LLaMA-2, Qwen, Yi) and domain-specific tasks (mathematics, coding, medical, finance, e-commerce). To establish stronger generalizability, it would be valuable to extend our evaluations to a broader range of base models and diverse task domains. Furthermore, the current study primarily emphasizes the pluggability and training efficiency of MoDULA when incorporating new domain experts. However, the scalability and robustness of this approach when integrating a larger number of experts require further exploration and stress testing.\nFuture research directions include investigating techniques to mitigate performance degradation on specific benchmarks, conducting comprehensive evaluations on a wider range of models and tasks, exploring the scalability limits of expert integration, streamlining the multi-stage training process, and enhancing the interpretability of the router's decision-making. By acknowledging these limitations and outlining potential avenues for future work, we aim to provide a balanced perspective on the current state of our research and highlight opportunities for further advancements in PEFT for LLMs."}, {"title": "A Analysis on the Residual Connection", "content": "The results in Table 6 validate the importance of the residual connection in the MODULA-Res method. Comparing MoDULA-Res with its non-residual counterpart reveals the residual connection's role in enhancing domain-specific tasks while preserving general language understanding.\nThe residual connection's impact varies among models. For instance, Qwen-7B and Yi-6B models show significant score improvements of 1.71 and 3.01 points, respectively, whereas LLaMA-2-7B shows a smaller gain of 1.77 points. This suggests that the benefits may be model-specific, meriting further investigation.\nIn domain-specific tasks, MoDULA-Res excels, particularly in mathematics and medical fields. For example, in Arithmetic and Medical datasets, MODULA-Res exceeds its non-residual variant by over 5 points, signifying the residual connection's role in effective knowledge transfer.\nHowever, in some tasks like MBPP and MedQA, the non-residual model slightly outperforms MoDULA-Res. This nuance suggests a need to further analyze the residual connection's mechanism across various tasks to improve the model's robustness.\nIn conclusion, the findings affirm the MoDULARes method's efficacy. Residual connections significantly enhance overall performance on domain-specific tasks, offering a promising avenue for future enhancements in the PEFT paradigm. Continued exploration of residual connections in multitask learning is expected to yield more powerful and versatile language models."}, {"title": "BGPT-4 Judge Prompt for E-commerce Tasks", "content": "We would like to request your evaluation of the product title optimization performed by the AI assistant. Please provide a score reflecting the effectiveness of the rewritten product title in terms of search engine visibility and user engagement.\nRate the optimized product title on the following criteria:\nHelpfulness: Does the title clearly showcase the product's key features for potential buyers?\nRelevance: Is the title relevant to the product and its unique selling points?\nAccuracy: Does the title accurately represent the product, including the brand and manufacturer?\nReadability: Is the title easy to read and understand for the average consumer?\nFluency: Does the title flow naturally and avoid awkward phrasing or keyword stuffing?\nPlease first output a single line containing one value indicating the overall score for the assistant's performance. You must rate the response on a scale of 1 to 10, with a higher score indicating a better performance.\nUse the following format for the rating: \"Rating: [[rating]]\", for example: \"Rating: [[8]]\".\nIn the subsequent lines, please provide a thorough explanation for your given score, addressing each individual criterion (Helpfulness, Relevance, Accuracy, Readability, and Fluency) where possible. Please ensure your feedback is clear and adheres to the instructions provided."}, {"title": null, "content": "We invite you to evaluate the product keywords recommended by the AI assistant. Kindly provide a score that reflects the effectiveness of the new keywords in terms of search engine visibility and user engagement.\nAssess the new keywords based on the following criteria:\nHelpfulness: Do the keywords clearly showcase the product's key features for potential buyers?\nRelevance: Are the keywords relevant to the product and its unique selling points?\nAccuracy: Are the number of keywords consistent with the selling points?\nReadability: Are the keywords easy to read and understand for the average consumer?\nFluency: Do the keywords flow naturally and avoid awkward phrasing or keyword stuffing?\nPlease begin by outputting a single line containing one value indicating the overall score for the assistant's performance. You must rate the response on a scale of 1 to 10, with a higher score indicating better performance.\nUse the following format for the rating: \"Rating: [[rating]]\", for example: \"Rating: [[8]]\".\nIn the following lines, provide a detailed explanation for your score, addressing each criterion (Helpfulness, Relevance, Accuracy, Readability, and Fluency) where possible. Ensure your feedback is clear and follows the provided instructions."}, {"title": "MODULA-Res", "content": "In order to further improve the general ability of the model, we propose MODULA-Res, a more advanced method that leverages the strengths of both universal and domain-specific experts. The architecture of MODULA-Res is shown in Figure 1(c). MoDULA-Res integrates both the universal expert E^{res} and the domain-specific experts E^{res}_1, E^{res}_2,..., E^{res}_n, tuned in a balanced way to cater to both general and domain-specific tasks. MODULA-Res introduces a residual connection that allows the model to incorporate the output of universal expert directly into the final result, ensuring that critical information is preserved and enhancing model robustness.\nThe forward process in MoDULA-Res module involves two stages. Initially, a hidden vector h_m is computed using the universal expert:\nh_m = B^{res}A^{res} x_m\nwhere x_m is the hidden vector for the m-th token, and B^{res} and A^{res} correspond to the universal expert matrices. Subsequently, the hidden vector h_m is refined by the domain-specific experts with residual connection to produce the final output y_{res}.\ny_{res} = E^{res}(h_m) + W_0 x_m + h_m\nwhere the function E^{res}(\u22c5) represents the operation of the domain-specific experts:\nE^{res}(h_m) = \\sum_{i=1}^n s^{res}_i B^{res} LeakyReLU(A_i^{res}h_m)\ns^{res}_i is the weight for each expert, computed as:\ns^{res}_i = O(x_m); = softmax(W_R x_m);\nThis integration of a three-stage training paradigm and residual connection ensures that the MODULA-Res module effectively generalizes and specializes simultaneously, thereby enhancing performance across both broad and focused applications."}]}