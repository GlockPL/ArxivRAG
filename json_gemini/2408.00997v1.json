{"title": "A Safe Exploration Strategy for Model-free Task Adaptation in\nSafety-constrained Grid Environments", "authors": ["Erfan Entezami", "Mahsa Sahebdel", "Dhawal Gupta"], "abstract": "Training a model-free reinforcement learning agent re-\nquires allowing the agent to sufficiently explore the environ-\nment to search for an optimal policy. In safety-constrained\nenvironments, utilizing unsupervised exploration or a non-\noptimal policy may lead the agent to undesirable states, re-\nsulting in outcomes that are potentially costly or hazardous\nfor both the agent and the environment. In this paper, we\nintroduce a new exploration framework for navigating the\ngrid environments that enables model-free agents to interact\nwith the environment while adhering to safety constraints.\nOur framework includes a pre-training phase, during which\nthe agent learns to identify potentially unsafe states based\non both observable features and specified safety constraints\nin the environment. Subsequently, a binary classification\nmodel is trained to predict those unsafe states in new envi-\nronments that exhibit similar dynamics. This trained clas-\nsifier empowers model-free agents to determine situations\nin which employing random exploration or a suboptimal\npolicy may pose safety risks, in which case our framework\nprompts the agent to follow a predefined safe policy to mit-\nigate the potential for hazardous consequences. We evalu-\nated our framework on three randomly generated grid en-\nvironments and demonstrated how model-free agents can\nsafely adapt to new tasks and learn optimal policies for new\nenvironments. Our results indicate that by defining an ap-\npropriate safe policy and utilizing a well-trained model to\ndetect unsafe states, our framework enables a model-free\nagent to adapt to new tasks and environments with signifi-\ncantly fewer safety violations.", "sections": [{"title": "1 Introduction", "content": "Learning through trial and error is a critical approach for\nhumans to learn new concepts which has also been an inspi-\nration for developing Reinforcement Learning (RL) meth-\nods [34]. In RL methods, the desired behavior is learned\nthrough interaction with the environment, where the agent\nreceives feedback based on the decisions it makes. While\nRL methods have demonstrated remarkable performance\nacross various domains, including video games [21, 28, 38],\nconventional recommender systems [19, 35, 44], and en-\nhancing the responses of large language models [8, 26].\nTheir applications in many real-world situations such as au-\ntonomous driving [3, 20], medical usage [31, 32, 37] and\nrecommendation systems in sensitive domains [30, 31] have\nbeen restricted due the potential to make dangerous and ir-\nreversible mistakes. This problem is more severe for model-\nfree agents given that they don't have any prior knowledge\nabout the environment and they predominantly rely on a\ntrial-and-error approach to learn the optimal policy.\nIn numerous real-world scenarios, the environment\nwithin which the agents interact demands careful consid-\neration of safety. For instance, in the realm of autonomous\ncars, many safety features have been developed to identify\npossible collision situations and take preventive actions to\navoid them. In the field of medical applications, provid-\ning suggestions for intricate and potentially dangerous sit-\nuations often involves discussions led by professional ex-\nperts, rather than relying solely on AI-based systems. Sim-\nilarly, recommendation systems on movie platforms must\nconsciously account for safety considerations, ensuring the\nappropriateness of suggestions for specific age ranges. In\nall the examples provided, diverse solutions for safety con-\nsiderations have been implemented to identify hazardous\nstates and prompt alternative responses beyond their ini-\ntially planned actions. These responses may involve reduc-\ning the speed of the vehicle to avoid a collision, deferring\ndiagnosis to experts instead of relying solely on AI, or ex-\necuting predefined tasks such as excluding certain sugges-\ntions from the available options in a recommendation sys-\ntem.\nApplying RL in real-world scenarios that demand safety\nconsiderations, similar to the examples mentioned earlier,\nintroduces the risk of potentially dangerous mistakes due to\nthe random exploration required during the training phase\n[12, 17, 31]. On the other hand, exploration is an indis-\npensable component of RL, allowing the agent to learn the\noptimal policy, especially in model-free settings where no\ninformation about the task or environment is provided to\nthe agent. The inherent risk associated with the use of RL\nin these scenarios, arising from the free exploration strategy,\nmakes employing RL an inefficient approach.\nIn this work, we propose a framework for training\nmodel-free RL agents to navigate grid environments where\nexploration requires consideration of safety constraints.\nThese environments encompass hazardous states, the explo-\nration of which might be dangerous or costly for the agent\nor the environment. Consequently, employing RL in such\ncontexts amplifies the potential of encountering undesirable\nstates due to unsupervised exploration strategies.\nOur method empowers the RL agent to identify risky\nstates, where the agent may navigate towards undesirable\nstates through inappropriate actions selected by random ex-\nploration or a suboptimal policy. Subsequently, our frame-\nwork enables the agent to recognize situations where free\nexploration becomes unsafe, prompting the selection of ac-\ntions based on a safe policy rather than relying on random\nor suboptimal policies. Defining the safe policy is highly\ndependent on the environment and can be a set of prede-\nfined actions for the agent, relinquishing control to a human\nor supervisor, or employing any other secure approach as a\nsubstitute for the suboptimal RL policy that utilizes a free\nexploration strategy. The primary contribution of this pa-\nper lies in the design of a framework that can efficiently\ndetect situations when free interaction (random exploration\nor using suboptimal policy) is unsafe for the model-free RL\nagent. The proposed method determines when free inter-\naction should be replaced with a more reliable and secure\nstrategy for selecting actions to learn the optimal policy\nwhile reducing the risk of visiting undesirable states in the\ntraining phase.\nAs shown in Figure 1 our proposed framework involves\na pre-training phase conducted in a simulator or controlled\nsegment of the environment, that has the same dynamics as\nthe main environment but without hazardous consequences\nfor entering undesirable states. During the pre-training\nphase, the agent interacts freely with the pre-training en-\nvironment to train a model capable of identifying states\nthat may not be initially undesirable but if the agent en-\nters these states, inappropriate actions selected by random\nexploration or suboptimal policy may lead the agent into\nundesirable states. The model trained in the pre-training\nphase is then applied to the main environment, which the\nmodel-free agent has not encountered before. This enables\nthe agent to recognize unsafe situations, where free inter-\naction with the environment is risky. In such cases, future\nactions are selected using a safe policy which is defined par-\nticularly for each environment and task."}, {"title": "2 Related Work", "content": "Given the inherent risk and uncertainty associated with\napplying RL methods, safety considerations have been ex-\ntensively investigated within the realm of RL methods\n[6, 11, 12, 13, 37, 39, 40].\nOne of the most important safety considerations in RL\nrevolves around safe exploration [12, 15, 16, 23, 24, 27, 31]\nto avoid exposing the agent to dangerous states and con-\nsequently avoid harmful outcomes. Abeel et al. [1] pro-\nposed an apprenticeship framework to train an RL agent\nin safety-critical environments where the agent relies on a\nteacher that should act safely and near-optimally. Hans et\nal. [17] introduced a method to assess a state's safety level,\nalongside a backup policy to navigate the system from crit-\nical to safe states. In [25] the need for a safe exploration\nstrategy in Markov Decision Process (MDP) has been dis-\ncussed, and a safe but potentially suboptimal exploration\nstrategy for safety-constrained environments has been pro-\nposed. Turchetta et al. [39] proposed SAFEMDP algorithm\nwhich is more similar to our work and enables RL agents\nto explore the reachable portion of the MDP while adher-\ning to safety constraints and gaining statistical confidence\nin unvisited state-action pairs using noisy observations.\nUnlike many similar works on safe exploration tech-\nniques, our framework does not require prior knowledge\nabout the distribution of unsafe states or initializing in a safe\nstate. However, our framework needs pre-training in an en-\nvironment with similar dynamics to the main environment\nin order to learn risky situations, given information provided\nin the state representation and/or agent observations.\nSafe training for RL methods in safety-constrained envi-\nronments has been extensively explored [2, 36, 41, 42, 43].\nAs a few examples, in [43] a model-free safe control strat-\negy for deep reinforcement learning (DRL) agents has been\nsuggested to prevent safety violation in the training phase.\nThananjeyan et al. [36] proposed an algorithm called Re-\ncovery RL that leverages offline data in order to identify\nsafety violation zones before policy learning and similar to\nour method, utilizes two different policies, one for learning\nthe task and another for considering the safety constraints\nin the environment. Our framework proposes a more struc-\ntured strategy to identify dangerous situations by computing\nreachable sets of undesirable states using high-level features\nthat are observable for the agent. Another difference be-\ntween our framework and methods similar to Recovery RL\nlies in defining the backup (safe) policy. Unlike these meth-\nods, our approach leverages a predefined safe policy instead\nof learning it from offline data. We propose defining a pre-\ndefined safe policy and primarily focus on identifying dan-\ngerous situations in previously unseen environments with\ndifferent state spaces and reward distributions, rather than\nlearning the safe policy and applying it to a similar environ-\nment for a different task.\nAnother set of methods similar to ours addresses safety\nin reachability problems, aiming to enable RL agents to\nlearn safe policies considering environmental safety con-\nstraints. Similar approaches [9, 10, 18, 22] have proposed\nenabling RL agents to attain a safe policy by computing\nreachable sets using Hamiltonian methods and defining a\nvalue function to assign negative values to reachable areas\nof undesirable states. Some key differences between these\nmethods and our framework lie in applications, computa-\ntional complexity, and the primary goal of the methods. Our\nmethod focuses on providing a safe learning framework,\nwhereas other methods primarily analyze the safety levels\nof learned policies."}, {"title": "3 Problem Definition and Background", "content": "Similar to most RL problems, we formulate our prob-\nlem of interest as a Markov Decision Process (MDP) which\nis defined as a tuple $(S, A, P, R, \\gamma)$, where S is the set of\nstates; A denotes the set of actions; $P(s_{t+1} = s'|S_t =$\ns, a_t = a) is the transition function that represents the dy-\nnamics of the environment and returns the probability of\ntransitioning to a specific state $s_{t+1} \\in S$ given the agent's\nprevious state $s_t \\in S$ and the action $a_t \\in A$ chosen at\nstate st state; R: S\u00d7A \u2192 R is the reward function and\n\u03b3\u2208 [0, 1] is the discount factor.\nWe denote the set of failure states as $S_f \\subset S$ which\ngetting into them is dangerous or costly for the agent and/or\nthe environment. The complementary set of $S_f$ is $S_c \\subset S$\nwhich contains all safe states that the agent can interact with\nto learn the optimal policy for the given task. Our objective\nis to equip the agent with the capability to identify states that"}, {"title": "4 Method", "content": "Our proposed framework includes two phases, namely\npre-training and safe training. In the pre-training phase, a\nbinary classification model is trained to detect dangerous\nstates. In the safe training phase, this model is applied to\nidentify dangerous states in a new environment. Upon de-\ntection, the agent switches from its current action selection\nstrategy, which may involve random exploration or a sub-\noptimal policy, to a more reliable policy known as the safe\npolicy to prevent the agent from entering undesirable states.\n4.1 Pre-training Phase\nThe pre-training phase takes place in an environment that\nhas similar dynamics as the main environment. It could be\neither a controlled portion of the main environment or a sim-\nulated environment where getting into failure states does not\nhave catastrophic consequences.\nThe primary goal during the pre-training phase is to train\na model capable of predicting the states that lie in BRS\ngiven observable features for the agent in the environment.\nIn our specific domain, the safety constraint entails avoiding\ncollisions with a moving obstacle.\nWe allow the agent to interact with the pre-training envi-\nronment using an e-greedy exploration strategy to find the\noptimal policy for a given task, while simultaneously com-\nputing the BRS from the episodes where the agent encoun-\ntered failure states Sf. To that end, we define a signed func-\ntion $l: S \\rightarrow R$, $l(s) > 0 \\leftrightarrow s \\in S_c$ which is\na signed distance between the current state and the target\nstate which in our work is an undesirable state. Since l is a\nsigned distance function, by using the function l, it becomes\npossible to identify whether the agent reaches failure states\nSf. Using the defined distance function l(s), we consider\na conservative perspective to formulate a value function for\nstates in the trajectory as follows:\n$V(s) = \\min_{i\\in[0,t], s_i \\in \\xi} l(s_i), \\quad s \\in \\xi$\n(2)\nThe proposed value function is designed to capture the\nminimum distance with the obstacle that is achieved by a\ntrajectory starting from state so by following policy \u03c0 over\na time horizon of [0, t]. In other words, this value function\nrepresents the minimum value attained by all states in a tra-\njectory starting from so and following policy over a time\nhorizon with the duration of t timesteps. This minimum\nvalue is then assigned as the value for all states in the tra-\njectory. Given the defined value function, if one state in\nthe trajectory is an undesirable state ($s_i \\in \\xi$\nand $s_i \\in S_f$),\na negative value is assigned not only to that state but also to\nall other states present in the same trajectory. By computing\nthe proposed value function within a finite time horizon [0,\nt], we are now able to redefine the BRS as follows:\n$S_{BRS} = {s \\in S : V(s) \\le 0}$\n(3)\nBy utilizing the proposed value function, we can identify\nBRS states and utilize them to train a binary classification\nmodel. This model is then employed in new environments\nto detect whether the agent is in a BRS state.\n4.2 Safe Training Phase\nWhen the agent encounters a new environment wherein it\nhas to learn a policy to solve the given task, it often searches\nfor optimal policy by utilizing some form of exploration that\ncan potentially be random, which may lead to a suboptimal\npolicy during the transient search phase for the optimal pol-\nicy. By leveraging the BRS detection model, we can enable\nthe agent to identify states within the BRS of undesirable\nstates. This model determines whether the agent is cur-\nrently within the BRS of undesirable states given the agent's\nobservations. Upon detection of such states, the agent can\nreplace an exploratory or suboptimal policy, which it had\nbeen using to explore the environment, with a predefined\nsafe policy. Switching between a suboptimal/random pol-\nicy and a safe policy when the agent enters SBRS allows"}, {"title": "5 Experimental Results", "content": "We evaluated our framework through an autonomous\nnavigation task designed with MiniGrid [7] platform. As\nshown in Figure 2 our designed environments contain a\nmoving obstacle that the agent must navigate around and\nreach the goal state. The obstacles move vertically in all en-\nvironments and change direction upon reaching the upper\nor lower boundary of the environments. Reaching the goal\nstate or interacting with the environment for a duration ex-\nceeding a maximum threshold of timesteps terminates the\nepisode with a reward of +1 and 0, respectively. Collision\nwith the moving obstacles constitutes a safety constraint vi-\nolation in this setting which terminates the episode with a\nreward of -1. The environment encompasses four actions:\n\"turn right\", \"turn left\", \"move forward\", and \"do noth-\ning\", that turning actions alter the agent's direction, while\nthe \"move forward\" action moves the agent by one state\nin its current direction. The \"do nothing\" action is exclu-\nsively designated for our safe policy, as outlined later, and\nis only accessible when the agent employs the safe policy.\nThe objective of each task in this setup is for the agent to\nlearn a policy to reach the goal state without colliding with\nthe moving obstacle. The state representation in this sce-\nnario includes the agent's vertical and horizontal positions,\nas well as the vertical position of the obstacle, (given its\nexclusive vertical movement) and its direction. QLearning\nand SARSA, two instances of model-free RL algorithms\nwith distinct characteristics, were examined to assess our\nframework. However, given that our framework primarily\nconcerns exploration strategy rather than specific learning\nalgorithm properties, any alternative model-free algorithm\ncan be utilized in place of them.\n5.1 Pre-training\nWe initially define a simpler environment in a 10x10 grid\nthat contains a moving obstacle and a goal state. We train\nthe agent for 4000 episodes in the pre-training environment\nwith a high exploration rate (0.6) and collect data from all\ntraining episodes. To identify the BRS for the moving ob-\nstacle in the pre-training environment, the signed distance\nfunction (l(s)) is defined as the Euclidean distance between\nthe obstacle and the agent. We compute the BRS of the\nmoving obstacle for $t = 2$ timesteps utilizing the value\nfunction described in equation 3. By computing BRS dur-\ning the pre-training phase, we identified states where if the\nagent enters them, random exploration or actions chosen by\na suboptimal policy can potentially lead the agent to collide\nwith the obstacle within the next 2 timesteps. After identify-\ning the BRS states, we train a binary classification model to\ndetect BRS and non-BRS states given state features includ-\ning the direction of the agent, the direction of the moving\nobstacle, and the signed distance.\nIn this work, we proceeded to train several classifica-\ntion models, including KNN, SVM, random forest, deci-\nsion tree, and a simple CNN, to perform a binary classifica-\ntion task for detecting BRS and non-BRS states. Given the\nproperties of the features present in the state representation\nand agent observation, the SVM classification model exhib-\nited superior performance compared to other tested methods\n(when we utilized Q-Learning as our pre-training algorithm,\nSVM achieved an accuracy of 0.92 and an F1 score of 0.83,\nwhereas it attained an accuracy of 0.92 and an F1 score of\n0.77 when the SARSA algorithm was employed). There-\nfore, the SVM classification model has been chosen as the\nprimary BRS detection model in our experiments.\n5.2 Task Adoption Using BRS Detection Model\nand a Safe Policy\nIn the next step, we randomly generate three 15x15 grid\nenvironments, each containing a goal state, 5 blocked states\nthat are inaccessible to the agent, and a vertically moving\nobstacle. To ensure the agent navigates past the moving\nobstacle, we randomly place the goal state in the right one-\nthird of the environment, while placing the obstacle ran-\ndomly outside that area.\nWe call each environment a task and train model-free\nagents using our framework and e-greedy exploration strat-\negy as a baseline with the same exploration rate (0.2) for\n2000 episodes and 20 runs for each task. We then evaluate\nhow safely each agent adapts to a new task in an environ-\nment that has been slightly changed.\nIn the proposed framework, when the agent interacts\nwith the environment, it uses the BRS detection model to\ndetermine if the current state is safe for random exploration\nor using a suboptimal policy. When a state is identified as\nBRS, the agent follows a predefined policy focused on col-\nlision prevention rather than reaching the goal state or find-\ning the optimal policy. Our safe policy keeps the agent in\nits current state if the moving obstacle is approaching by\nchoosing \"do nothing\" action. If the agent is in the path of\nthe obstacle, it considers the shortest trajectory of actions\nto move away from the obstacle's path, then maintains the\nagent's position outside the obstacle's path by selecting \"do\nnothing\" action as long as the state remains identified as\nBRS by the BRS detection model.\nTo evaluate the performance of each approach, we de-\nfine three metrics namely \"Average Collision Rate\u201d, repre-\nsenting the average percentage of episodes that concluded"}, {"title": "6 Discussion and Future Works", "content": "We propose a framework to enhance the efficiency of\nmodel-free agents exploring new safety-constrained grid\nenvironments. In this section, we highlight a few limita-\ntions and considerations to be aware of when utilizing this\nframework for other domains.\nOur framework offers insights into potentially unsafe\nstates for the model-free agent. The primary contribution\nof our work lies in detecting instances where random ex-\nploration or suboptimal policies may pose risks in safety-\nconstrained environments and should be substituted with\nmore reliable approaches. However, defining a suitable safe\npolicy depends on the specific environment and the task. In\nthis work, to facilitate defining an appropriate safe policy,\nwe narrowed the scope of the work to grid environments.\nWhile we demonstrate an instance of a safe policy as a set of\npredefined behaviors in our experiments, defining a reliable\nsafe behavior can be challenging for certain environments,\npotentially diminishing the effectiveness of this framework.\nOne factor that may affect the efficacy of our method is\nthe feasibility of designing an appropriate pre-training en-\nvironment for the agent. In our problem of interest, navi-\ngating the grid environments, our pre-training environment\ncomprised the similar dynamics of the main environments.\nHowever, in many real-world scenarios, designing an ideal\npre-training environment may not be feasible due to the\ncomplexity and uncertainty of these environments. There-\nfore, it is crucial to consider this factor before deploying the\nproposed framework for a task.\nAnother limitation of our framework arises from the na-\nture of training a machine learning model to detect unsafe\nsituations and the inherent risk of false detections. There-\nfore, this framework does not guarantee error-free recog-\nnition. Consequently, in highly sensitive domains such as\nmedical applications, relying solely on this framework may\nnot be advisable.\nTaking into account the mentioned limitations, our\nframework is most effective for tasks where we can design a\npre-training zone with properties similar to the real environ-\nment and define a reliable safe policy. As a few examples of\npotential real-world applications of our framework, we can\nmention the task of indoor and outdoor autonomous naviga-\ntion using RL [29, 33] and teaching new skills to humanoid\nrobots [14].\nIn future works, we aim to explore alternative ap-\nproaches for defining reliable safe policies in order to en-\nable model-free agents to devise a set of actions for each\nunique situation, rather than solely relying on predefined\nbehaviors. Additionally, we aim to work on more complex\nenvironments and evaluate the effectiveness of our frame-\nwork in environments with high dimensions and complex\ndynamics."}, {"title": "7 Conclusion", "content": "In this study, we introduce an exploration framework\nto enable model-free RL agents to explore new environ-\nments and adapt to new tasks more safely compared to the\ntraditional e-greedy strategy. The core component of our\nframework is the pre-training phase that enables the agent to"}]}