{"title": "Leveraging LLMs for Predictive Insights in Food Policy and Behavioral Interventions", "authors": ["Micha Kaiser", "Paul Lohmann", "Peter Ochieng", "Billy Shit", "Cass R. Sunstein", "Lucia A. Reisch"], "abstract": "Food consumption and production contribute significantly to global greenhouse gas emissions, making them crucial entry points for mitigating climate change and maintaining a liveable planet. Over the past two decades, food policy initiatives have explored interventions to reshape production and consumption patterns, focusing on reducing food waste and curbing ruminant meat consumption. While the evidence of \"what works\" improves, evaluating which policies are appropriate and effective in specific contexts remains difficult due to external validity challenges. This paper demonstrates that a fine-tuned large language model (LLM) can accurately predict the direction of outcomes in approximately 80% of empirical studies measuring dietary-based impacts (e.g. food choices, sales, waste) resulting from behavioral interventions and policies. Approximately 75 prompts were required to achieve optimal results, with performance showing signs of catastrophic loss beyond this point. Our findings indicate that greater input detail enhances predictive accuracy, although the model still faces challenges with unseen studies, underscoring the importance of a representative training sample. As LLMs continue to improve and diversify, they hold promise for advancing data-driven, evidence-based policymaking.", "sections": [{"title": "Main", "content": "Food consumption and production, accounting for one-quarter to one-third of global greenhouse gas emissions, are key entry points for mitigating climate change, reducing mortality and morbidity, and keeping the world within liveable planetary boundaries (Crippa et al., 2021). Over the past decade, food policy has been at the forefront of sustainability efforts, exploring various interventions to reshape production and consumption patterns and stimulate dietary shifts. These initiatives, including those targeting greenhouse gas emissions in the food system, have significantly advanced our understanding of which interventions are most effective in shifting individual behavior (Lohmann et al., 2024b). Today, climate scientists concur that the most effective approach is to focus on two critical behavioral strategies: reducing food waste and loss (FWL) (Gatto & Chepeliev, 2024; Zhu et al., 2023) and curbing ruminant meat consumption (Li et al., 2024). Just focusing on these two would lead to substantial strides in mitigating food-related emissions (Geyik et al., 2023; Crippa et al., 2021; Xu et al., 2021; Yang et al., 2024).\nThe challenge for policymakers aiming to mitigate climate change is to achieve significant and lasting behavior change. Various policy instruments are available to help drive such changes, from mandates and bans to financial incentives and disincentives, from information and education to behaviorally informed interventions using choice architecture (Ammann et al., 2023). Evidence reviews on the effectiveness of such interventions guide policymakers towards the most effective yet acceptable policies (Lohmann et al., 2024b; Reisch et al., 2021). Behaviorally informed food policies seem well-suited to be included in policy bundles since they can be effective, easy to implement, and often more acceptable to the public than hard policies like taxes and bans (Reisch, 2021). Moreover, they can be applied to a multitude of food system actors at individual and system levels (ibid.). With all these options at hand, how can policymakers evaluate which policies are the most appropriate and effective to tackle a specific policy problem (such as reducing food waste) in a particular setting (such as a public canteen run by a municipality in the UK) for a specific target group (such as vulnerable young consumers)? There are notable practical challenges as the effectiveness of intervention will vary depending on intervention levels, countries, and settings.\nWhile the scientific literature and evidence synthesis networks increasingly produce helpful, systematic re-views and meta-regressions of hundreds or even thousands of studies, traditional approaches are resource-intensive, time-consuming, and hence quickly outdated in today's era of rapid data analysis (Egger et al., 2003). Systematic maps and reviews of case studies can inspire by showing the scope of possible policies. Still, they can hardly be generalized due to the heterogeneity of target populations and the power of context. Indeed, socio-economic, social, and cultural contexts and societal conditions, such as high or low trust in government, heavily influence the success of interventions; sometimes, the outcomes are not even in the predicted direction. In a nutshell, predicting policy outcomes is difficult, and external validity remains a serious challenge (Findley et al., 2021).\nWhile the idea of conducting mega studies seems to address some of these challenges, particularly concerning external validity (Milkman et al., 2021; Duckworth & Milkman, 2022), the primary practical issues\u2014namely, cost and time to conduct experimental studies remain unresolved. Given the complexity and numerous factors influencing the success of interventions, there is a growing need to identify more efficient methods for generating reliable predictions and supporting informed decision-making for researchers and policymakers. Artificial intelligence, particularly large language models (LLMs), offers new possibilities due to their ability to process vast amounts of data and identify patterns with much lower resource and time costs. Originally designed for text, LLMs now excel in various tasks, including coding (Coello et al., 2024), symbolic mathematics (Ahn et al., 2024), and scientific reasoning (AI4Science & Quantum, 2023).\nThere is increasing evidence that LLMs can be utilized for predictive tasks beyond their original design, such as stock market predictions (Lopez-Lira & Tang, 2023) and even more general financial tasks (Wu et al., 2023; Xie et al., 2023; Deng et al., 2023). Additionally, LLMs have been employed to simulate complex daily behaviors and interactions\u2014such as cooking or commuting\u2014 and may offer a realistic approach to agent-based modelling architectures (Park et al., 2023). Moreover, LLMs have shown promising results in simulating classical economic, psycholinguistic, and social-psychological experiments, such as the Ulti-matum Game or the Wisdom of Crowds Aher et al. (2023). Horton (Horton, 2023) further demonstrates how LLMs, functioning as computational representations of human decision-making (or \"homo silicus\"), can"}, {"title": "Results", "content": "We report the ability of a fine-tuned GPT-3.5 turbo model to predict outcomes of empirical studies that measure dietary-based outcomes (e.g. food choices, sales, waste, etc.) due to behavioral interventions and policies. We compute the fine-tuned LLM model's fidelity in predicting multiple dependent outcomes simultaneously, each influenced by the same set of independent features. The outcomes to be predicted include the effect direction (positive or negative), the correlation coefficient (Pearson correlation, r) and the standardized effect size (Cohen's d). We report the statistical performance of LLMs across a collection of user studies in the dataset, specifically held out during the training process. We compare these predicted parameters with the empirical values reported in the source research paper and hence report accuracy in\nLLMs' predictions in terms of average 1 absolute error, eu and variance in absolute errors, \u03c32, for both r\nand d.\nThere is mixed evidence regarding LLMs' capabilities in zero-shot prompting-i.e., prompting without any explicit fine-tuning. While some studies show that models like GPT-3 have demonstrated an emergent ability to find zero-shot solutions to a broad range of analogy problems Webb et al. (2023), others Chang et al.(2024); Liu et al. (2023) indicate that LLMs often fail to generate meaningful responses. This suggests that LLMs are not consistently effective zero-shot reasoners for predicting human behavioral policy outcomes, such as those based on food interventions.\nLLM models fine-tuned on certain prompt formulations struggled to predict all or specific parts of the experi-mental results. In particular, we observed that these models found it more challenging to predict quantitative outcomes (e.g. r and d values) than qualitative ones. Therefore, for the four fine-tuned models (MP1, MP2,MP3, MP4), we report the probability of the model successfully making quantitative predictions, specifically the r and d values. The performance of these models is detailed in Table 1, and the prompts used to fine-tune them are laid out in Figure 1.\nThe results show that models MP\u2081 and MP2, trained with verbose prompts, were more conservative in their predictions. Model MP\u2081 could not predict any of ther and d values, but it predicted the direction of the nudge 94.5% of the time, with an accuracy of 36.7%. Model MP2 predicted the direction 68.2% of the time with an accuracy of 23.1%, and it predicted r and d values with a probability of 19%. In contrast, the modelsMP3 and MP4 with less verbose prompts predicted the direction, r, and d values 100% of the time. This suggests that concise prompts elicit accurate numerical predictions from fine-tuned models more effectively. In particular, removing excessive context (e.g., the step-by-step instructions in P2) can help the LLM focus more on making numerical predictions as requested by the end of the prompt. We hypothesize that overly detailed prompts may lead the LLM to overthink and stray from aligning its responses with the training labels. Both MP3 and MP4 achieved 79% accuracy in predicting the effect direction, and the improvement compared to MP2 is likely due to adding guided completion (\"Please predict ...\") at the end of the prompt.\nRegarding the prediction of r and d values, model MP3 estimated r with an average absolute error of -0.058 and d with an error of -0.151, with variances of 0.142 and 0.441, respectively. Model MP4 performed slightly better, predicting r with an average absolute error of -0.009 and d with an error of -0.051, with variancesof 0.127 and 0.385, respectively. These results show that with effective prompt formulation, LLMs are few-shot learners Brown et al. (2020) for predicting human behavioral policy outcomes based on food policyinterventions."}, {"title": "LLM's Prediction Accuracy Affected by the Number of Training Prompts", "content": "To evaluate the sensitivity of an LLM's predictions to few-shot prompt fine-tuning Mosbach et al. (2023), we fine-tune it on varying numbers of prompts (N). We begin by fine-tuning the model with 10 prompts (the minimum number OpenAI allows) and progressively increase the training data size to 144 prompts,"}, {"title": "Prediction Accuracy Affected by Prompt Features", "content": "We report the importance of various prompt features in the fine-tuning of an LLM by treating our prompt formulation as filling in a standard template with key parameters from empirical studies, such as the pop-ulation type, geographical location, year of the experiment and sample size. This analysis aims to identify any uninformative features that might add noise and reduce the model's predictive accuracy, and hence we report the average predictive errors of feature-removed models bench-marked against the one with all features included (model MP4 as reported in Table 1). As shown in Figure 3, results show the only model that outperforms MP4 is the one fine-tuned without the title of the research article. This suggests that the titles introduce noise into the fine-tuning process as they sometimes carry information orthogonal to the causality between experimental interventions and measured outcomes; including this in the training prompt"}, {"title": "Prediction Accuracy for Unseen Studies", "content": "To further evaluate the predictive accuracy on experiments that were, with certainty, not used during the GPT-3.5 Turbo model training process, we extracted data from 12 experiments unpublished at the time of our model fine-tuning Pizzo et al. (2024); Lohmann et al. (2024a). The main objective is to assess whether our fine-tuned models can generalize the knowledge acquired during fine-tuning to accurately predict future, unseen outcomes of specific policy interventions. When applied to this test set, our best-performing model (fine-tuned with 130 prompts) predicts the effect direction, the r-coefficient, and Cohen's d with 100% coverage. On average, it correctly predicts the effect direction in approximately 6.6 out of 12 experiments, corresponding to an accuracy of 55%. This is slightly below the 79% accuracy observed in the baseline dataset but still higher than a naive estimator, which predicts the most common outcome (\"negative\") and achieves 50% accuracy for this test sample. The model shows an average error of -0.088 in predicting the"}, {"title": "Discussion", "content": "The results strongly suggest that LLMs hold significant promise as tools for policymakers, aiding in an exceptionally difficult task: selecting and evaluating behavioral policies and interventions. Specifically, theMP3 and MP4 fine-tuned models showed strong performance, achieving 79% accuracy in predicting effectdirection. Both models also produced highly accurate results with low variance in predicting correlation coefficients (r) and effect sizes (Cohen's d), indicating consistent performance. At the same time, we offercautionary notes. A level of 21% inaccuracy in predicting effect direction is a serious concern; it is importantto find ways to meet that concern. In addition, the findings suggest that prompt formulation plays a criticalrole in fine-tuning LLMs. We find that including a high level of details in prompts can lead to diminishingreturns, as the model may struggle to filter relevant information effectively. This insight can help refinebest practices for prompt engineering (Ekin, 2023), particularly for tasks that require precise quantitativepredictions. Our methodology also suggests the importance of iterative prompt tuning in achieving accurateLLM predictions.\nOur analysis also indicates that more training data in the fine-tuning process does not necessarily lead tobetter model performance. In our case, the models fine-tuned with 75 and 130 prompts achieved the highestaccuracy, outperforming those trained with either fewer or greater numbers of prompts. This suggests thateven in presence of abundant data, there exists an optimal number (or a few optimal numbers) of trainingdata examples to be included in model fine-tuning. When additional prompts are included, the model'sperformance initially decreases, likely due to over-adjustment and loss of previously acquired knowledge.\nThe performance may improve again (in our case, with 130 prompts), but including further training data beyond this point again leads to a decline in accuracy, possibly due to over-fitting or parameter saturation.\nWe also observed that the effectiveness of fine-tuning is highly sensitive to the features included in the prompt. Omitting crucial features significantly reduces the model's predictive power, while overloading the prompt with unnecessary information introduces noise, further diminishing accuracy. In this study, key features such as the year of the experiment, location, target participants, sample size, treatment group size, and control group size were essential for optimizing fine-tuning. However, including less relevant details, such as the research paper's title, added unnecessary noise and negatively impacted the fine-tuning process. Thus, careful consideration of feature selection in prompts is essential for achieving optimal model performance.\nIn addition to this, our analysis of the model's performance on previously unpublished experiments under-scores the importance of a balanced and especially representative training dataset, particularly for enhancing predictive accuracy on unseen cases and further strengthening its utility for policy-relevant applications. As our findings indicate, carefully compiling a diverse range of interventions is essential for the model to capture all possible interaction effects that may emerge.\nOur attempt to develop and test PREDICT as proof-of-concept offers significant promise for the future. These modest first steps seem to support the observation of an \"unreasonable effectiveness of algorithms\" in addressing public policy problems Ludwig et al. (2024), a high potential upside. It is reasonable to speculate that our findings in the context of food policy might find parallels in other domains of behaviorally informed policy. As potential examples, consider efforts to improve road safety, to reduce smoking, to combat domestic violence, to promote energy conservation, to reduce unlawful immigration, to improve resilience against extreme heat, and to combat opioid addiction. LLMs might be enlisted to give policymakers and others greater clarity on what works and what does not. Despite initial effectiveness, the use of proposed models has some limitations. First, we have not tested these models in real-world scenarios at scale, and its effectiveness in practical applications remains uncertain. We expect significant variability in how different users might interact with a tool powered by these fine-tuned LLM models, and we have yet to identify which specific design features will be most supportive\u2014or potentially problematic-in various contexts. Second, while the dataset used in this study includes 74 published papers, its coverage is not exhaustive: for example, we have not covered all geographical locations and target groups. Consequently, the dataset may not fully represent diverse populations, which could skew predictions and limit the generalizability of the model across different demographics and regions. The incomplete nature of the historical datasets could introduce biases in fine-tuning the LLM, potentially leading to suboptimal policy recommendations. As we present this promising methodology, we also take this opportunity to raise important questions on how LLM-assisted tools should (or should not) be used in policymaking settings, recognizing the convenience it brings by synthesizing historic experiments and the limitation and potential bias it might bring along.\nMoving forward, the development of LLM-based research assistants like PREDICT for policymaking willneed to address several critical questions:\nBuilding trust and confidence: What criteria are necessary to establish trust and confidence in using such a tool? We hypothesize that a robust, scientifically independent database (e.g. empirical studiesthat have undergone rigorous validity and quality assessments) combined with an interdisciplinary team ofauthors and developers will be essential. The next step would involve real-world testing with policymakersto identify which features enhance or diminish trust in PREDICT. Co-design exercises and user interfacecopy-testing will be crucial to adapting the tool to meet the needs of policymakers in real-world politicalprocesses, thereby increasing its credibility and attractiveness.\nAssessing and mitigating risks: What are the potential risks of using PREDICT as a decision-supporttool, and how can these real and perceived risks be mitigated? Potential risks include the risk of error,technological challenges inherent in the use of LLMs, administrative and procedural barriers (such asthe acceptability of AI-based tools in different policy contexts or regulatory frameworks), and ethicalconcerns. Addressing these risks will require tailored solutions, including transparency, education, and clearcommunication about the capabilities and limitations of such a research assistant. Trust in government andtechnology will also play a significant role in mitigating these risks.\nInterpreting LLM decisions: Although fine-tuned LLMs can predict the effectiveness of policy inter-ventions, they might be seen to do so in a 'black box' manner, offering little insight into the rationalebehind their predictions. This lack of interpretability may challenge policymakers who require transparentjustifications for the recommendations. A key goal of this research is to develop techniques that makethe decision-making process of LLMs more transparent and understandable, thereby increasing trust andusability.\nEvaluating long-term impact: What are the opportunities for sustainable food policy beyond this initialproof-of-concept? How accurate are the predictions, and how much better can they be? Research has shownthat government decision-makers often misjudge the benefits and costs of policy interventions. Ludwig etal. (Ludwig et al., 2024) argue that algorithms can improve the rank-ordering of policy options by marginalbenefit, leading to better social outcomes. Future studies should compare PREDICT-based policies withtraditional human-judgement-based approaches to determine the extent of improvement. This will requireexperimentation and long-term systematic monitoring of results, ideally in collaboration with policy unitsresponsible for food policy.\nExploring generalizability across domains: While the fine-tuned LLMs demonstrate effectiveness inpredicting outcomes related to behavioral interventions in food policy, it remains to be determined whetherthese models would perform similarly in other domains, such as transport, occupational safety, education,immigration, and environmental policy. Our findings with respect to a model trained solely on food policydata may or may not have parallels in these and other domains. Further research would be needed to assessthe model's ability to generalize across policy areas, ensuring that its predictive power remains robust inother contexts."}, {"title": "Methods", "content": "Given a collection of empirical studies extracted from published research papers that report dietary-basedoutcomes influenced by food behavior policies and interventions, our objective is to extract a set of experimentdescriptions X and their associated outcomes Y. We represent the features of an experiment as x \u2208 X andassociate them with their respective outcomes y \u2208 V. The dataset, comprising all described experiments andtheir outcomes from the research papers, is defined as D = (x1,Y1),..., (XN,YN). Our goal is to fine-tune alarge language model using a subset Dtraining CD, thereby establishing a function f : X \u2192 Y such that f(x)accurately approximates y over a distribution of inputs x. Once the fine-tuned LLM f has been established,Another data subset Dtest CD is used to evaluate how well it estimates the distribution of Y given X.\nConcretely, the methodology seeks to evaluate the ability of LLMs to predict an outcome of food behavioralpolicies and interventions accurately. We assess this by first fine-tuning an LLMs using a training datasetcontaining a description of existing behavioral policies and interventions with already known outcomes.The fine-tuned LLM is then evaluated on test data. We design an algorithm that takes as its input anex experiment's important details (features) and uses these details to construct a prompt that queries an LLM.The prompt consists of typical information that describes food human behavioral experiments such as theinterventions, the year the experiment was conducted, the location of the experiment, targeted participants,sample size, treatment group size and controlled group size. Based on the query prompt, the LLM predictsa probability distribution over the space of all possible numerical values that evaluate an intervention'seffect on food consumption patterns."}, {"title": "Prompt generation", "content": "LLMs are autoregressive models that generate predictions sequentially by estimating the probability dis-tribution of the next token in a sequence, i.e., p(ti|t1, t2,\u2026\u2026,ti\u22121) for any token ti given previous tokens\nt1,..., ti-1 Aher et al. (2023). To elicit responses from LLMs, prompts are carefully designed to obtain thebest generative response. Prompts can generally be categorized into two main types: cloze prompts Petroniet al. (2019) Cui et al. (2021), which fill in blanks within a textual input, and prefix prompts Li & Liang(2021) Lester et al. (2021), which complete a string prefix. The choice of prompt type depends on the specifictask and the LLM model being used. In our case, we adopt the prefix prompt.\nTo construct the prompt p, we developed a prompt constructor module, fprompt (t, x), which takes a manuallycrafted prompt template t (with some slots left blank) and a textual description x of human behavioralexperiments on food. This module automatically extracts key details from the description x and fills in themissing slots in the template, thereby generating a complete prompt p, i.e., p = fprompt(t, x). Designing aneffective prompt template is essential for both fine-tuning and querying the LLM Sclar et al. (2023). TheLLM's performance is particularly sensitive to the wording, feature selection, and format of the prompt Sclaret al. (2023). To address this, we experimented with various versions of the prompt template to identify theone that produces the best results on the test dataset.\nIn our case, the features included in the prompt were fixed based on experimental descriptions extractedfrom empirical studies on human food behavior. We ensured that all prompt versions contained the samefeatures to avoid any spurious bias from omitting key information. Since we used the OpenAI GPT-3.5turbo LLM\u00b2 via the chat completion API, the format of the prompt was also constrained. OpenAI's LLMSare trained to accept input formatted as a conversation, where the messages parameter contains an array ofmessage objects organized by role (see an example in Figure 1). We adhered to this chat completion format.\nOnce a prompt p has been established, it serves as a seed for generating additional candidate prompts.Starting with the seed prompt P\u2081, we generated three more candidate prompts, P2, P3, and P4, throughiterative paraphrasing. Each subsequent version was derived from the previous one; for example, P3 wasdeveloped by adding a completion guide sentence at the end of P2. We ensured that each new candidateprompt was less verbose compared to its parent prompt, which we found to improve to model performance."}, {"title": "Dataset", "content": "The dataset was initially compiled using systematic search procedures Lohmann et al. (2024b). It included74 published research papers that evaluated food policy interventions to shift individuals towards moresustainable food consumption or reduce food waste. The search was conducted across several scientificbibliographic databases: Web of Science Core Collections Citation Indexes, Scopus, MEDLINE and GoogleScholar (first 20 pages of the output).\nThe final sample was limited to quasi-experimental and experimental studies that measured the actual (orincentivized) behavior of individuals or households and provided valid counterfactuals to quantify interven-tion effects. Any type of intervention (e.g. monetary, information provision, or behavioral nudges) wasconsidered. Details on the exact inclusion and exclusion criteria are provided in Lohmann et al. (2024b).\nFrom each research paper, we manually extracted key details of the experiments, including the title of theexperiment, a brief description of its goal, the location or region where the study was conducted, the targetedpopulation, the total sample size, the sizes of both the treatment and control groups, and the results of theexperiment. Specifically, we captured the direction (positive or negative) and magnitude of the effect inducedby the intervention. These effect sizes were subsequently converted into standardized effect size measures,including the r-coefficient and Cohen's d using the standard formulae described in Ringquist (2013). In some instances, research papers contained multiple experiments, outcomes or interventions. In total, the 74 papers yielded 208 individual effect sizes, which were used to generate 208 distinct prompts. These prompts were then split into 144 for training, 23 for validation, and 41 for testing."}, {"title": "Fine-tuning", "content": "Traditionally, leveraging pre-trained models involved gradient-based fine-tuning on downstream tasks, usingpre-trained parameters as an initialization step Ding et al. (2023). However, as LLMs have scaled to hundredsof billions of parameters, they have demonstrated properties that facilitate few-shot learning, making themadaptable to specific tasks with minimal examples Brown et al. (2020). This process, known as few-shotfine-tuning, involves training the LLM on a small, task-specific dataset to adjust its parameters for enhancedperformance on a particular task VM et al. (2024).\nFor this study, we fine-tuned the LLM using a dataset of 144 training prompts and 23 validation prompts.Each prompt included a comprehensive description of an experiment's features, such as the experiment'stitle, the nudge applied to the treatment group, the location or region where the experiment was conducted,the targeted population, the total sample size, and the sizes of both the treatment and control groups.Additionally, each prompt included the results of the experiment, detailing the direction of the effect (positiveor negative) induced by the nudge, along with labelled statistical metrics like the r-coefficient and Cohen'sd.\nThese prompts were used during the fine-tuning process to adjust the LLM's internal parameters, therebyimproving its ability to predict the impact of various nudges on food behavior. By incorporating bothtraining and validation prompts, we aimed to ensure that the model could generalize well to unseen datawhile maintaining high performance on the specific task of understanding and predicting behavioral outcomesbased on experimental details. The hyper-parameters (e.g. learning rate) are chosen automatically by theOpenAI completions API.\nEach version of the prompt formulations (P1, P2, P3, and P4) was used to generate a corresponding fine-tunedmodel (MP1, MP2, MP3, and MP4)."}, {"title": "Inference", "content": "To evaluate the accuracy of the predictions generated by the fine-tuned LLM models, we assess their abilityto predict the correct effect direction, Pearson correlation coefficient (r), and Cohen's d values inducedby the nudge. The prediction accuracy of the LLM model is determined by first calculating the absoluteerror between the LLM's predicted values and the corresponding reported values from the literature. Forexample, if the model predicts r = -a and the actual reported value is r = q, the absolute error is calculatedas |-a|-|q|. A positive absolute error indicates that the model overestimates the effect, while a negative errorsuggests underestimation. Then, we compute the mean across all 41 test data examples, before re-runningthe LLM inference 10 times and report a distribution of the average absolute errors."}]}