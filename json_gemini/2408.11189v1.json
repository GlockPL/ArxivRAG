{"title": "Reading with Intent", "authors": ["Benjamin Reichman", "Kartik Talamadupula", "Toshish Jawale", "Larry Heck"], "abstract": "Retrieval augmented generation (RAG) systems augment how knowledge language models are by integrating external information sources such as Wikipedia, internal documents, scientific papers, or the open internet. RAG systems that rely on the open internet as their knowledge source have to contend with the complexities of human-generated content. Human communication extends much deeper than just the words rendered as text. Intent, tonality, and connotation can all change the meaning of what is being conveyed. Recent real-world deployments of RAG systems have shown some difficulty in understanding these nuances of human communication. One significant challenge for these systems lies in processing sarcasm. Though the Large Language Models (LLMs) that make up the backbone of these RAG systems are able to detect sarcasm, they currently do not always use these detections for the subsequent processing of text. To address these issues, in this paper, we synthetically generate sarcastic passages from Natural Question's Wikipedia retrieval corpus. We then test the impact of these passages on the performance of both the retriever and reader portion of the RAG pipeline. We introduce a prompting system designed to enhance the model's ability to interpret and generate responses in the presence of sarcasm, thus improving overall system performance. Finally, we conduct ablation studies to validate the effectiveness of our approach, demonstrating improvements in handling sarcastic content within RAG systems.", "sections": [{"title": "Introduction", "content": "Humans speak and write with a variety of emotional inflections that can significantly change the meaning of the spoken word. In face-to-face interactions, these inflections are often accompanied by non-verbal signals such as tone of voice, facial expressions, and body language. Some estimates find that in face-to-face verbal communication, non-verbal signals account for most of what is being communicated (Mehrabian et al. 1971). These non-verbal cues can be crucial for accurately interpreting meaning, especially when the spoken or written words alone may be ambiguous or open to multiple interpretations.\nIn writing, emotional inflection and other non-verbal communication need to be conveyed solely in the text, as there is no \"non-verbal\" communication in this medium.\nWord choice, syntax, and specific textual patterns become the primary means of expressing emotions. Humans, over the course of their development, learn how to pick up on these cues. First, children learn cues in the spoken communication domain and then translate this understanding to the written domain (Capelli, Nakagawa, and Madden 1990). This transition highlights the complexity of interpreting nuanced emotions in text, such as sarcasm, where the lack of accompanying non-verbal signals can lead to misunderstandings.\nLarge language models (LLMs) have revolutionized the field of natural language processing (NLP). They excel in tasks such as document summarization, fact-based question answering, legal document analysis, and more. Despite their impressive capabilities, these models primarily focus on the denotative aspects of language, often neglecting the subtle emotional inflections embedded in text. LLMs tend to interpret language literally, assuming that statements are intended to be understood at face value. This credulous approach poses significant challenges for AI systems relying on internet-sourced text, where sarcasm, humor, and other nuanced emotions are prevalent (Soper 2024; Terech 2024; Orland 2024). As a result, LLMs may misinterpret sarcastic comments as literal statements, inadvertently spreading misinformation or even suggesting harmful actions.\nThe Retrieval-Augmented Generation (RAG) paradigm, which relies on internet-derived sources, has become increasingly popular as LLMs require access to a vast array of information beyond their fixed internal knowledge (Lewis et al. 2020). While LLMs are trained on large datasets and contain billions of parameters that encode how language is expressed and commonly encountered facts (Tenney, Das, and Pavlick 2019; Clark et al. 2019; Dai et al. 2021), their internal knowledge is inherently limited. The sheer volume of facts in existence far exceeds what any model can store, and for specialized domains, general-purpose LLMs may lack the necessary domain-specific information. Moreover, the factual information stored within an LLM can become outdated or might not be entirely accurate, especially for time-sensitive or evolving knowledge. RAG addresses these challenges by retrieving relevant external knowledge and incorporating it into the LLM's prompt. This allows the model to access up-to-date and specialized information, thereby improving the accuracy and relevance of its responses, especially when dealing with complex or niche topics.\nA RAG system can source its retrieval from various places depending on the context. In academic research, Wikipedia-derived passages are a common testbed for evaluating RAG systems. In industry, RAG is often employed over a company's internal documents. For broader knowledge enrichment, companies like Google, Perplexity, OpenAI, and Microsoft utilize the open internet as a retrieval corpus to augment their RAG systems. However, leveraging the open internet as a source introduces two unique and somewhat intertwined challenges:\n1.  The internet is replete with misinformation.\n2.  The internet is full of emotionally inflected text.\nThese issues are not commonly found in Wikipedia passages or internal documents, which are typically written in a neutral, matter-of-fact tone. While substantial research has focused on misinformation detection and strategies to account for misinformation in retrieval, less attention has been given to the challenge of interpreting emotionally-inflected text. Although various prompt and instruction tuning techniques have been developed to enhance how models read and integrate retrieved information, the issue of understanding and processing emotionally inflected text remains under-explored. In this work, we introduce a novel prompt-based approach that improves reading comprehension across both emotionally and non-emotionally inflected text, demonstrating consistent performance gains across different model families and sizes.\nIn this work, we focus on reading sarcastically-poisoned text. Our contributions are three-fold:\n1.  We construct a sarcasm-poisoned retrieval corpus.\n2.  We develop a prompt-based approach for reading sarcasm-inflected text.\n3.  We conduct comprehensive ablation studies."}, {"title": "Related Work", "content": "Sentiment analysis has been a long-standing area of research within the NLP community, evolving alongside advancements in computational techniques (Prabowo and Thelwall 2009; Medhat, Hassan, and Korashy 2014; Wankhade, Rao, and Kulkarni 2022). Various methods have been developed across different \"era\" of NLP: Tree-based methods (Nakagawa, Inui, and Kurohashi 2010; Suresh and Bharathi 2016), SVM-based methods (Ahmad et al. 2018), Neural Network-based methods (Wadawadagi and Pagi 2020), Transformer-based methods (Wadawadagi and Pagi 2020), and LLM-based methods\u00b9.\nLLMs have demonstrated strong performance on sentiment analysis tasks (Sun et al. 2023). However, sentiment analysis is not an end in itself but rather a critical step in the broader process of understanding, processing, and responding to human communication. Despite this, less research has focused on integrating sentiment analysis as a\nsub-component within larger tasks. As LLMs are increasingly serving as intermediaries between human-generated texts and communicating with humans, it is important for them to understand and model sentiment information when generating outputs. Current LLMs \u2013 which read from internet content to help augment their knowledge and give more accurate responses to queries \u2013 have difficulties integrating sentiment analysis into their overall prediction, leading to bad generations (Terech 2024; Orland 2024). In this work, we address this problem by providing a dataset and a method that enables LLMs to read with intent."}, {"title": "Reader Models", "content": "In the standard Retrieval-Augmented Generation (RAG) pipeline, there are two stages: retrieval, and reading/generation. This work focuses on the latter stage, where the model interprets the retrieved content to produce answers. Various approaches have been proposed in the literature for this stage. For example, Fusion-in-Decoder reads each passage independently and then uses cross-attention during decoding to integrate information across passages (Izacard and Grave 2020). More recently, with the rise of LLMs and their ability to be prompted and instruction-tuned, there have been many systems that integrate these approaches. Iter-RetGen iteratively retrieves and generates content, using each generation as the basis for subsequent retrievals (Shao et al. 2023). Interleaved Retrieval Chain-of-Thought builds upon the Chain-of-Thought method by retrieving passages based on each sentence in the chain, integrating this information into the next stage of reasoning (Trivedi et al. 2022; Wei et al. 2022). Other approaches, such as Chain-of-Note and SURE, focus on generating intermediate \"notes\" or summaries based on the retrieved passages, which are then used to produce final answers (Yu et al. 2023; Kim et al. 2024). SELF-RAG introduces a dual-model system, where one model determines the need for retrieval and the other critiques the retrieved content, providing reflection tokens to ground the output (Asai et al. 2023). REPLUG and Certifiably Robust RAG adopt a model-averaging strategy, processing each passage individually (Shi et al. 2023; Xiang et al. 2024). The latter also introduces a decoding method for RAG systems."}, {"title": "Dataset Creation", "content": "To study the problem of reading and answering questions over emotionally inflected text, an open-domain question-answering (QA) dataset with a retrieval corpus specifically tailored to this task is necessary. While various datasets for sentiment analysis exist, they are typically limited to classification tasks and do not address the complexities of emotion understanding in the context of QA. This work, in contrast, focuses on how emotion, particularly sarcasm, affects the reading and answering of questions. Building a dataset for this intention-aware reading task requires several steps:\n1.  Identifying a set of open-domain questions,\n2.  Retrieving passages supporting those questions,\n3.  Adding sarcasm to the retrieved passages, and\n4.  Integrating sarcasm-poisoned passages into the dataset."}, {"title": "Open-Domain Questions", "content": "The objective of the dataset is to have a set of questions such that an LLM would be required to read outside passages to provide accurate answers. This requires questions that an LLM would have a low likelihood of being able to answer relying solely on its internal knowledge learned during pretraining, thus necessitating the use of outside knowledge and reading comprehension abilities. We thus chose an open-domain question-answering dataset (Kwiatkowski et al. 2019) that provides a retrieval corpus as well as ground truth retrieved passages as the base for our new dataset."}, {"title": "Retrieval", "content": "Although retrieval is a critical portion of the RAG pipeline, it is not the focus of this paper. Therefore, an off-the-shelf SOTA dense retrieval method (Wang et al. 2021) was used. For each question in the dataset the top 200 passages were retrieved as illustrated in Figure 1. These retrieved passages formed the base from which our dataset was derived."}, {"title": "Sarcasm Poisoning", "content": "With the passages forming our dataset retrieved, the next step is to create the sarcasm-poisoned versions. Manually rewriting these passages to be sarcastic would be time-consuming, labor-intensive, and costly. To achieve scalability and maintain consistency, we opted for a synthetic approach to generate sarcastic versions of the retrieved passages. The goal was to create sarcastic passages that mimic those found online. This required generating two different types of sarcastic passages: (1) Passages that convey the correct (factual) information but are written with a sarcastic tone; and (2) Passages that do not convey the correct (factual) information and are written with a sarcastic tone. Including both types allows us to study how sarcasm affects comprehension and accuracy, whether the underlying information is correct or misleading."}, {"title": "Integration", "content": "After generating the synthetic passages, the next step is to integrate them into existing retrieval results. To do this, we created three test datasets each designed to evaluate our reading method's effectiveness under different conditions.\nThe first test dataset was created by replacing all original passages with their sarcastic equivalents. In this version, while none of the factual information in the passages was distorted, the connotations were altered. This dataset is crucial for evaluating how well the model can interpret and respond to text where the underlying facts remain accurate, but the emotional tone is changed, challenging the model's ability to understand and process sarcasm in a controlled setting.\nRecognizing that real-world retrieval is unlikely to yield all sarcastic passages, a second version of the dataset was created. In this version, 20% of incorrect passages were randomly replaced with sarcastic equivalents, and the first two correct passages were paired with a fact-distorted sarcastic passage. This dataset has two variants: one with the sarcastic fact-distorted passage placed before (pre-fix) and one after (post-fix) the correct passage. These variants allow investigation into the influence of position on the model's ability to distinguish between accurate and distorted information.\nIn the third dataset created, the synthetically generated passages were embedded by the passage encoder of the dense retrieval method used (Wang et al. 2021) and added into the vector database. The passages for each question were then re-retrieved. The distribution of fact-distorted sarcastic passages was therefore set by the retrieval method, and thus most closely mimics the real-world scenario."}, {"title": "Reading with Intent", "content": "The goal of our work is for LLMs to be cognizant of the emotional intent of the passages they read when trying to answer open-domain questions. To achieve this, we created a dataset with sarcasm-poisoned passages to experiment on (Section 3). In this section, we present our framework, Reading with Intent, which enhances the model's ability to better understand the connotation of the passages it is processing. In Section 4.1, we present a prompt-based approach that improves reading accuracy on the datasets presented in Section 3. In Section 4.2, we further enhance our prompt-based approach with trained intent tags."}, {"title": "Prompt-Based Approach", "content": "Previous question-answering approaches prompt the model to read retrieved passages for one of a few reasons: generating a direct answer, producing candidate answers, or summarizing the passage to aid a downstream model in answering. However, these approaches generally overlook an important aspect-prompting the model to pay attention to the connotation of the text.\nWithout explicitly directing the model to focus on the connotation of the text, the model pays less attention to the connotation. However, by instructing the model to consider the intent behind the text, we can effectively refocus its attention on the connotation of the input it receives. In our proposed system, we explicitly prompt the LLM to pay attention to the textual connotation. This is illustrated in Figure 2 which shows our system's pipeline."}, {"title": "Intent Tags", "content": "Reading with intent involves two processes: identifying connotative intent and coloring the reading of the text with the detected intent. The proposed system separates these sub-tasks, allowing the system to independently focus on each.\nHandling both tasks\u2014identifying and applying connotative intent can be challenging for an LLM. We simplify this task by decomposing it into its constituent parts. Thus we propose guiding the model with intent tags. A small language model was fine-tuned to generate binary tags that identify whether a passage is sarcastic. These tags were then inserted into the prompt, letting the LLM focus on interpreting the intent. Future work could expand this to include a wider range of emotive labels, further enhancing the model's understanding of text."}, {"title": "Experimental Setup", "content": "In this section, we detail the experimental setup used to evaluate our proposed method.\nFor data poisoning, we selected the Natural Questions dataset (Kwiatkowski et al. 2019), which offers a broad range of questions and a large retrieval corpus of 21M passages, along with ground truth retrievals for each question. GPL (Wang et al. 2021) was used to retrieve the initial passages for poisoning. The top-200 passages for each query in the NQ validation set were retrieved, totaling 971,384 unique passages. GPL also constructed the dataset described in the last part of Section 3.4, where fact-distorted sarcastic passages were added back into the retrieval corpus, and passages for the reader model were re-retrieved.\nTo perform the passage poisoning, the Llama3-70B-Instruct model was used. It was chosen as it is a steerable, well-performing open-source model. From qualitative tests, the sarcasm it generated was realistic, though occasionally over-the-top, even when instructed to be subtle.\nTo evaluate the Reading with Intent system, we tested it across a range of models: Llama2-7B-chat, Llama2-70B-chat, Mistral 8x7B, Mixtral 8x22B, the Phi3 series, and the Qwen2 series. The Llama3 series was excluded from testing since it was used to generate the sarcastic passages. Consistent with previous work, the top 10 retrieved passages were used in the reading system (Xiang et al. 2024).\nA Roberta classifier model was trained on the SARC dataset to produce intent tags (Liu et al. 2019; Khodak, Saunshi, and Vodrahalli 2018). The last three layers of the Roberta model, along with a classifier layer, were fine-tuned using the AdamW optimizer with a batch size of 75. A learning rate of 1e-3 was applied to the classification layer, while a learning rate of 5e-4 was used for the Roberta-base layers. Every 600 steps, the learning rate for the classifier layer was decayed by a factor of 0.7, and the learning rate for the Roberta-base layers was decayed by a factor of 0.9.\nThis work is inspired by a failure mode observed when LLMs are deployed. To emulate real-world usage, we evaluated the models in a manner that mirrors typical usage. Therefore, although the prompt instructed the models to be terse, this instruction was only included once, reflecting common production scenarios where users minimize instructions to optimize the prompt budget. Consequently, the models generally produced answers in complete sentences rather than single words. Therefore, we evaluate the model by seeing what percent of answers the model generates contain the correct answer within the generation. To ensure the evaluation metric is informative, we conducted a qualitative review of a small subset of these answers. Additionally, in Appendix C we present evaluations using more restrictive 1-3 word responses. In that evaluation, F1/EM scores are used. However, this is further away from how these models (e.g. Google Search AI, Perplexity AI, ChatGPT) are used in production."}, {"title": "Experimental Results", "content": "In this section, we present the results of our overall system. Table 1 summarizes the outcomes across the four versions of the retrieval corpus used with the Natural Questions dataset. The \"NQ\" dataset refers to the base Natural Questions retrieved passages with no sarcasm poisoning. \"FS NQ\" represents the Natural Questions dataset where all non-sarcastic passages were replaced with factually correct sarcastic versions. The \"PS-M NQ\" dataset is the version of the dataset where the fact-distorted sarcastic passages were manually inserted next to correctly retrieved passages and factually-correct sarcastic passages are substituted for a portion of the incorrectly retrieved passages. This is the second dataset referred to in Section 3.4. Lastly, The \u201cPS-A NQ\" dataset is the version of the Natural Questions dataset where the fact-distorted sarcastic passages are inserted into the retrieval corpus and the retriever selects a new set of top-10 passages to input as context for the LLM. This is the last dataset described in Section 3.4.\nTable 1 shows that the \"Reading with Intent\" prompt boosts performance across the various datasets for both the Llama2 and Mistral family of models, across model scales. For Llama2, the overall average percent difference was 5.5%, with the 7b model showing an average boost of 8.2% and the 70b model showing a 2.8% improvement. The Mistral models show an average increase of 3.5% with the 7b model seeing an average 5% boost and the 8x22b a 1.9% increase. Notably, in these families, smaller models using the \"Reading with Intent\" prompt on the PS-M NQ and PS-A NQ datasets performed comparably to larger models with only the base prompt. The non-oracle tags also mostly outperform the base prompt, but more moderately. In the Phi-3 model family, the greatest improvement (1.6%) was observed in the Phi-3-Mini model, while the Phi-3-Medium model saw a modest increase, and the Phi-3-Small experienced a performance decline (-2.4%). This pattern is similarly seen in the Qwen2 model family where the smallest model had the largest performance boost (3.2%), while the larger models saw either no benefit or a slight decline (-0.4%) with the \u201cReading with Intent\" prompt.\nThe performance across datasets, shown in Table 1, is fairly consistent. On average, models performed slightly better on the FS NQ dataset than on the NQ dataset when using either the base prompt or the \"Reading with Intent\" prompt with oracle tags. Following these two datasets, models performed worse on the PS-A NQ dataset and even more so on the PS-M NQ dataset across all prompts. These results indicate that models can equally understand the denotative content of a passage, whether or not it is sarcasm-laced. However, consistent with real-world findings, when sarcasm-laced passages contain falsehoods, models often fail to understand the sarcasm as a potential signal of the passage's inaccuracy. The consistently poorer performance on the PS-M NQ dataset compared to the PS-A NQ dataset indicates that the specific ordering in the PS-M NQ dataset presents a greater challenge than the naturally retrieved passages from the PS-A NQ dataset. This extra difficulty makes PS-M NQ a good testbed for future \"Reading with Intent\" work."}, {"title": "Analysis", "content": "Our \"Reading with Intent\" method comprises several components, and in this section, we aim to dissect the method to identify where the most performance improvements originate. Due to the computational costs involved, we conducted the ablation studies on a single model: Llama-2-7b-chat.\nThe first part of our analysis examines the impact of the different components of the prompt. Our prompt contains two components: the intent reading prompt and the intent tag. As shown in Table 3 adding the intent prompt produces a significantly larger performance boost than adding the intent tag. Simply instructing the model to consider the intent of the passage helps the LLM better interpret the passage's connotation. The intent tag also improves predictions by preventing the model from over-interpreting the text's emotional inflection. This effect is noticeable in the base dataset and the PS-A dataset, where there are a fair amount of non-sarcastic passages. Intent tags, in these cases, bridge the performance gap between the full prompt and only the intent reading prompt. Together, these two components significantly increase performance compared to the base prompt.\nThe next parameter analyzed was the position of the intent tag. Whether the tag is placed before or after the passage could influence the model's ability to correctly read the passage's intent. As shown in Table 4, placing the intent tag after the passage boosts performance by an average of 0.6%. This indicates that the model can change its interpretation of a passage after reading it based on the metadata provided.\nThe position of the factually distorted passage relative to the correct passage was also varied. Table 5 shows that this positioning significantly impacts performance. Presenting the non-sarcastic, factually correct passage first greatly improves performance compared to when it is presented second. Conversely, placing the correct passage after the factually distorted one greatly decreases performance. This aligns with prior work, such as (Liu et al. 2024), which suggests that models are more biased toward reading earlier passages. When two passages present similar information side by side, the model is likely to focus more on the two passages, likely looking at the details of the first passage and skim the second passage.\nWe also analyzed the responses of different retrieval systems (GPL (Wang et al. 2021), LLM-Embedder (Zhang et al. 2023), and BGE-M3 (Chen et al. 2024)) to the inclusion of sarcastic passages in the retrieval corpus. Table 2 summarizes the findings, showing that the performance of each retrieval system at Recall@K declines by 3%-6% with the addition of approximately 1 million sarcastic passages. Sarcastic passages are significantly overrepresented in retrievals; despite making up only 4.5% of the corpus, they appear in the top-1 results 9.7%-16.2% of the time, leading to a 2.2-3.6x over-representation. In the top-100, sarcastic passages are overrepresented by a factor of 4-8x, depending on the retriever used. Interestingly, the number of sarcastic retrievals tends to plateau around the top-20, with only incremental changes beyond that point. Further analysis reveals that sarcastic passages retrieved are rarely direct substitutions for their non-sarcastic counterparts. Instead, an entirely different sarcastic passage is often retrieved. Additionally, while it is common for a sarcastic passage to precede or follow a correct retrieval (occurring ~90% and ~99% of the time, respectively, in the top-100), it is much rarer for these sarcastic passages to be the sarcastic version of the correct passage this happens only 1.3%-1.9% before insertion and 1.7%-4.1% after insertion in the top-100 results.\nLastly, we evaluated the performance of our intent classifier."}, {"title": "Conclusion and Discussion", "content": "Limitations and future work: There are two main limitations to this work. First, the sarcastic passages generated are likely too easy to detect. The classification model achieved 96.9% accuracy on the created dataset, compared to only 66.9% accuracy on the SARC dataset, on which it was trained. This discrepancy may be due to instances in the SARC dataset that are ambiguous even to humans. However, it also suggests that more challenging, artificially generated sarcasm could be developed. Importantly, the relative ease of detection did not prevent the LLMs from experiencing performance losses when these sarcastic passages were included in the retrievals. The second limitation is that our \"Reading with Intent\" system is primarily prompt-based. We chose this approach for its lightweight and easily integrable nature, but instruction-tuning the model to read with intent could potentially further enhance its performance.\nBroader Impact: We anticipate that the dataset and the method we have developed will assist researchers in deploying LLMs that are more adept at processing internet text. As a result, these systems will be better equipped to leverage the knowledge available online while minimizing the potential harms that emotionally inflected language can introduce.\nConclusion: This paper focuses on merging the sentiment analysis and reading comprehension tasks. As models increasingly process internet-derived passages during inference, understanding both the denotative and connotative meanings of text becomes essential. To explore this challenge, we transformed passages from an open-domain QA dataset's retrieval corpus into sarcastic and fact-distorted sarcastic versions. Various retrieval methods were tested under these adversarial conditions, and we proposed a 'Reading with Intent' system that outperformed baseline systems in real-world style evaluations. Finally, ablation studies identified the components of our system that contributed the most to performance improvements."}, {"title": "Appendix", "content": "We present supplementary information and experimental results in support of the paper in this appendix."}, {"title": "Prompts", "content": "Manually creating a dataset that is intentionally sarcasm-poisoned or fact-distorted would be a vast and costly undertaking in terms of time, labor, and resources. To overcome this challenge, we used a state-of-the-art LLM, as described in Section 3, to generate the datasets presented in this paper. The Llama-3-70B Instruct model was prompted in specific ways to create sarcastic and fact-distorted passages.\nFigure 3 shows the prompts used for sarcasm-poisoning and fact-distortion.\nFigure 4 presents the \"Reading with Intent\" prompt described in the main body of the text. The figure breaks down the different components of the prompt as previously described in the paper."}, {"title": "Reading with Intent Generation Samples", "content": "The LLMs in the \"Reading with Intent\" system were prompted in a manner that mimicked how LLMs are typically used in production applications and by end-users. Although the LLM was instructed to be terse, this was only done once, often resulting in the LLM producing complete sentences instead of 1-3 word answers. To evaluate these outputs, we considered an answer correct if the ground truth was contained within the LLM's output.\nIn this appendix, Table 7 provides a sample of LLM outputs alongside the ground truth as a visualization and qualitative sanity check for readers. Rows highlighted in green indicate where our evaluation metric aligns with the expected outcome, while rows in red indicate discrepancies. The mistakes made by our evaluation system are similar to those an Exact Match (EM) scoring system would make. For example, for the query \u201cprotein that serves as the precursor molecule for thyroid hormones\", the ground truth answer is \"Thyroglobulin (TG)\", and the inferred answer included \"thyroglobulin\". However, because the model did not also provide the molecule's abbreviation, this answer was marked incorrect - a mistake that would also occur with EM scoring. On the other hand, our evaluation method allows for correct assessment even when the model responds in a complete sentence, as seen with the query \"where do the Great Lakes meet the ocean\" where the model's correct full-sentence answer was marked accurate."}, {"title": "Alternative Evaluation Scores", "content": "In addition to the evaluation presented in the paper's main body, we also prompted the model in a way that hewed more closely to how models are typically prompted in Open-Domain QA research. Here, the prompts were optimized not only to 'Read with Intent' but also to produce answers limited to 1-3 words. This approach, however, appears to make the task more challenging for the model, as it now has two goals to optimize. Table 8 presents the results of these experiments. The general trend for models between datasets remains consistent: FS NQ slightly outperforms NQ (or the performances are close together); both outperform PS-A and PS-M NQ; and PS-A NQ still outperforms PS-M NQ. However, in this scenario, the base prompt slightly outperforms the 'Reading with Intent' prompt, likely because the model is balancing multiple objectives-answering the question, being brief, and interpreting the intent of the provided passages simultaneously."}]}