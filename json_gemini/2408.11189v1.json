{"title": "Reading with Intent", "authors": ["Benjamin Reichman", "Kartik Talamadupula", "Toshish Jawale", "Larry Heck"], "abstract": "Retrieval augmented generation (RAG) systems augment how knowledge language models are by integrating external information sources such as Wikipedia, internal documents, scientific papers, or the open internet. RAG systems that rely on the open internet as their knowledge source have to contend with the complexities of human-generated content. Human communication extends much deeper than just the words rendered as text. Intent, tonality, and connotation can all change the meaning of what is being conveyed. Recent real-world deployments of RAG systems have shown some difficulty in understanding these nuances of human communication. One significant challenge for these systems lies in processing sarcasm. Though the Large Language Models (LLMs) that make up the backbone of these RAG systems are able to detect sarcasm, they currently do not always use these detections for the subsequent processing of text. To address these issues, in this paper, we synthetically generate sarcastic passages from Natural Question's Wikipedia retrieval corpus. We then test the impact of these passages on the performance of both the retriever and reader portion of the RAG pipeline. We introduce a prompting system designed to enhance the model's ability to interpret and generate responses in the presence of sarcasm, thus improving overall system performance. Finally, we conduct ablation studies to validate the effectiveness of our approach, demonstrating improvements in handling sarcastic content within RAG systems.", "sections": [{"title": "Introduction", "content": "Humans speak and write with a variety of emotional inflections that can significantly change the meaning of the spoken word. In face-to-face interactions, these inflections are often accompanied by non-verbal signals such as tone of voice, facial expressions, and body language. Some estimates find that in face-to-face verbal communication, non-verbal signals account for most of what is being communicated (Mehrabian et al. 1971). These non-verbal cues can be crucial for accurately interpreting meaning, especially when the spoken or written words alone may be ambiguous or open to multiple interpretations.\nIn writing, emotional inflection and other non-verbal communication need to be conveyed solely in the text, as there is no \"non-verbal\" communication in this medium.\nWord choice, syntax, and specific textual patterns become the primary means of expressing emotions. Humans, over the course of their development, learn how to pick up on these cues. First, children learn cues in the spoken communication domain and then translate this understanding to the written domain (Capelli, Nakagawa, and Madden 1990). This transition highlights the complexity of interpreting nuanced emotions in text, such as sarcasm, where the lack of accompanying non-verbal signals can lead to misunderstandings.\nLarge language models (LLMs) have revolutionized the field of natural language processing (NLP). They excel in tasks such as document summarization, fact-based question answering, legal document analysis, and more. Despite their impressive capabilities, these models primarily focus on the denotative aspects of language, often neglecting the subtle emotional inflections embedded in text. LLMs tend to interpret language literally, assuming that statements are intended to be understood at face value. This credulous approach poses significant challenges for AI systems relying on internet-sourced text, where sarcasm, humor, and other nuanced emotions are prevalent (Soper 2024; Terech 2024; Orland 2024). As a result, LLMs may misinterpret sarcastic comments as literal statements, inadvertently spreading misinformation or even suggesting harmful actions.\nThe Retrieval-Augmented Generation (RAG) paradigm, which relies on internet-derived sources, has become increasingly popular as LLMs require access to a vast array of information beyond their fixed internal knowledge (Lewis et al. 2020). While LLMs are trained on large datasets and contain billions of parameters that encode how language is expressed and commonly encountered facts (Tenney, Das, and Pavlick 2019; Clark et al. 2019; Dai et al. 2021), their internal knowledge is inherently limited. The sheer volume of facts in existence far exceeds what any model can store, and for specialized domains, general-purpose LLMs may lack the necessary domain-specific information. Moreover, the factual information stored within an LLM can become outdated or might not be entirely accurate, especially for time-sensitive or evolving knowledge. RAG addresses these challenges by retrieving relevant external knowledge and incorporating it into the LLM's prompt. This allows the model to access up-to-date and specialized information, thereby improving the accuracy and relevance of its responses, espe-"}, {"title": "2 Related Work", "content": "2.1 Sentiment Analysis\nSentiment analysis has been a long-standing area of research within the NLP community, evolving alongside advancements in computational techniques (Prabowo and Thelwall 2009; Medhat, Hassan, and Korashy 2014; Wankhade, Rao, and Kulkarni 2022). Various methods have been developed across different \"era\" of NLP: Tree-based methods (Nakagawa, Inui, and Kurohashi 2010; Suresh and Bharathi 2016), SVM-based methods (Ahmad et al. 2018), Neural Network-based methods (Wadawadagi and Pagi 2020), Transformer-based methods (Wadawadagi and Pagi 2020), and LLM-based methods\u00b9.\nLLMs have demonstrated strong performance on sentiment analysis tasks (Sun et al. 2023). However, sentiment analysis is not an end in itself but rather a critical step in the broader process of understanding, processing, and responding to human communication. Despite this, less research has focused on integrating sentiment analysis as a\nsub-component within larger tasks. As LLMs are increasingly serving as intermediaries between human-generated texts and communicating with humans, it is important for them to understand and model sentiment information when generating outputs. Current LLMs \u2013 which read from internet content to help augment their knowledge and give more accurate responses to queries \u2013 have difficulties integrating sentiment analysis into their overall prediction, leading to bad generations (Terech 2024; Orland 2024). In this work, we address this problem by providing a dataset and a method that enables LLMs to read with intent.\n2.2 Reader Models\nIn the standard Retrieval-Augmented Generation (RAG) pipeline, there are two stages: retrieval, and reading/generation. This work focuses on the latter stage, where the model interprets the retrieved content to produce answers. Various approaches have been proposed in the literature for this stage. For example, Fusion-in-Decoder reads each passage independently and then uses cross-attention during decoding to integrate information across passages (Izacard and Grave 2020). More recently, with the rise of LLMs and their ability to be prompted and instruction-tuned, there have been many systems that integrate these approaches. Iter-RetGen iteratively retrieves and generates content, using each generation as the basis for subsequent retrievals (Shao et al. 2023). Interleaved Retrieval Chain-of-Thought builds upon the Chain-of-Thought method by retrieving passages based on each sentence in the chain, integrating this information into the next stage of reasoning (Trivedi et al. 2022; Wei et al. 2022). Other approaches, such as Chain-of-Note and SURE, focus on generating intermediate \"notes\" or summaries based on the retrieved passages, which are then used to produce final answers (Yu et al. 2023; Kim et al. 2024). SELF-RAG introduces a dual-model system, where one model determines the need for retrieval and the other critiques the retrieved content, providing reflection tokens to ground the output (Asai et al. 2023). REPLUG and Certifiably Robust RAG adopt a model-averaging strategy, processing each passage individually (Shi et al. 2023; Xiang et al. 2024). The latter also introduces a decoding method for RAG systems."}, {"title": "3 Dataset Creation", "content": "To study the problem of reading and answering questions over emotionally inflected text, an open-domain question-answering (QA) dataset with a retrieval corpus specifically tailored to this task is necessary. While various datasets for sentiment analysis exist, they are typically limited to classification tasks and do not address the complexities of emotion understanding in the context of QA. This work, in contrast, focuses on how emotion, particularly sarcasm, affects the reading and answering of questions. Building a dataset for this intention-aware reading task requires several steps:\n1. Identifying a set of open-domain questions,\n2. Retrieving passages supporting those questions,\n3. Adding sarcasm to the retrieved passages, and\n4. Integrating sarcasm-poisoned passages into the dataset."}, {"title": "3.1 Open-Domain Questions", "content": "The objective of the dataset is to have a set of questions such that an LLM would be required to read outside passages to provide accurate answers. This requires questions that an LLM would have a low likelihood of being able to answer relying solely on its internal knowledge learned during pretraining, thus necessitating the use of outside knowledge and reading comprehension abilities. We thus chose an open-domain question-answering dataset (Kwiatkowski et al. 2019) that provides a retrieval corpus as well as ground truth retrieved passages as the base for our new dataset."}, {"title": "3.2 Retrieval", "content": "Although retrieval is a critical portion of the RAG pipeline, it is not the focus of this paper. Therefore, an off-the-shelf SOTA dense retrieval method (Wang et al. 2021) was used. For each question in the dataset the top 200 passages were retrieved as illustrated in Figure 1. These retrieved passages formed the base from which our dataset was derived."}, {"title": "3.3 Sarcasm Poisoning", "content": "With the passages forming our dataset retrieved, the next step is to create the sarcasm-poisoned versions. Manually rewriting these passages to be sarcastic would be time-consuming, labor-intensive, and costly. To achieve scalability and maintain consistency, we opted for a synthetic approach to generate sarcastic versions of the retrieved passages. The goal was to create sarcastic passages that mimic those found online. This required generating two different types of sarcastic passages: (1) Passages that convey the correct (factual) information but are written with a sarcastic tone; and (2) Passages that do not convey the correct (factual) information and are written with a sarcastic tone. Including both types allows us to study how sarcasm affects comprehension and accuracy, whether the underlying information is correct or misleading."}, {"title": "3.4 Integration", "content": "After generating the synthetic passages, the next step is to integrate them into existing retrieval results. To do this, we created three test datasets each designed to evaluate our reading method's effectiveness under different conditions.\nThe first test dataset was created by replacing all original passages with their sarcastic equivalents. In this version, while none of the factual information in the passages was distorted, the connotations were altered. This dataset is crucial for evaluating how well the model can interpret and respond to text where the underlying facts remain accurate, but the emotional tone is changed, challenging the model's abil-"}, {"title": "4 Reading with Intent", "content": "The goal of our work is for LLMs to be cognizant of the emotional intent of the passages they read when trying to answer open-domain questions. To achieve this, we created a dataset with sarcasm-poisoned passages to experiment on (Section 3). In this section, we present our framework, Reading with Intent, which enhances the model's ability to better understand the connotation of the passages it is processing. In Section 4.1, we present a prompt-based approach that improves reading accuracy on the datasets presented in Section 3. In Section 4.2, we further enhance our prompt-based approach with trained intent tags."}, {"title": "4.1 Prompt-Based Approach", "content": "Previous question-answering approaches prompt the model to read retrieved passages for one of a few reasons: generating a direct answer, producing candidate answers, or summarizing the passage to aid a downstream model in answering. However, these approaches generally overlook an important aspect-prompting the model to pay attention to the connotation of the text.\nWithout explicitly directing the model to focus on the connotation of the text, the model pays less attention to the connotation. However, by instructing the model to consider the intent behind the text, we can effectively refocus its attention on the connotation of the input it receives. In our proposed system, we explicitly prompt the LLM to pay attention to the textual connotation. This is illustrated in Figure 2 which shows our system's pipeline."}, {"title": "4.2 Intent Tags", "content": "Reading with intent involves two processes: identifying connotative intent and coloring the reading of the text with the detected intent. The proposed system separates these sub-tasks, allowing the system to independently focus on each.\nHandling both tasks\u2014identifying and applying connotative intent can be challenging for an LLM. We simplify this task by decomposing it into its constituent parts. Thus we propose guiding the model with intent tags. A small language model was fine-tuned to generate binary tags that identify whether a passage is sarcastic. These tags were then inserted into the prompt, letting the LLM focus on interpreting the intent. Future work could expand this to include a wider range of emotive labels, further enhancing the model's understanding of text."}, {"title": "5 Experimental Setup", "content": "In this section, we detail the experimental setup used to evaluate our proposed method.\nFor data poisoning, we selected the Natural Questions dataset (Kwiatkowski et al. 2019), which offers a broad range of questions and a large retrieval corpus of 21M passages, along with ground truth retrievals for each question. GPL (Wang et al. 2021) was used to retrieve the initial passages for poisoning. The top-200 passages for each query in the NQ validation set were retrieved, totaling 971,384 unique passages. GPL also constructed the dataset described"}, {"title": "6 Experimental Results", "content": "6.1 Main Results\nIn this section, we present the results of our overall system. Table 1 summarizes the outcomes across the four versions of the retrieval corpus used with the Natural Questions dataset. The \"NQ\" dataset refers to the base Natural Questions retrieved passages with no sarcasm poisoning. \"FS NQ\" represents the Natural Questions dataset where all non-sarcastic passages were replaced with factually correct sarcastic versions. The \"PS-M NQ\" dataset is the version of the dataset where the fact-distorted sarcastic passages were manually inserted next to correctly retrieved passages and factually-correct sarcastic passages are substituted for a portion of the"}, {"title": "6.2 Analysis", "content": "Our \"Reading with Intent\" method comprises several components, and in this section, we aim to dissect the method to identify where the most performance improvements originate. Due to the computational costs involved, we conducted the ablation studies on a single model: Llama-2-7b-chat.\nThe first part of our analysis examines the impact of the different components of the prompt. Our prompt contains two components: the intent reading prompt and the intent tag. As shown in Table 3 adding the intent prompt produces a significantly larger performance boost than adding the intent tag. Simply instructing the model to consider the intent of the passage helps the LLM better interpret the passage's connotation. The intent tag also improves predictions by preventing the model from over-interpreting the text's emotional inflection. This effect is noticeable in the base dataset and the PS-A dataset, where there are a fair amount of non-sarcastic passages. Intent tags, in these cases, bridge the performance gap between the full prompt and only the intent reading prompt. Together, these two components significantly increase performance compared to the base prompt.\nThe next parameter analyzed was the position of the intent tag. Whether the tag is placed before or after the passage could influence the model's ability to correctly read the passage's intent. As shown in Table 4, placing the intent tag after the passage boosts performance by an average of 0.6%. This indicates that the model can change its interpretation of a passage after reading it based on the metadata provided."}, {"title": "7 Conclusion and Discussion", "content": "Limitations and future work: There are two main limitations to this work. First, the sarcastic passages generated are likely too easy to detect. The classification model achieved 96.9% accuracy on the created dataset, compared to only 66.9% accuracy on the SARC dataset, on which it was trained. This discrepancy may be due to instances in the SARC dataset that are ambiguous even to humans. However, it also suggests that more challenging, artificially generated sarcasm could be developed. Importantly, the relative ease of detection did not prevent the LLMs from experiencing performance losses when these sarcastic passages were included in the retrievals. The second limitation is that our \"Reading with Intent\" system is primarily prompt-based. We chose this approach for its lightweight and easily integrable nature, but instruction-tuning the model to read with intent could potentially further enhance its performance.\nBroader Impact: We anticipate that the dataset and the method we have developed will assist researchers in deploying LLMs that are more adept at processing internet text. As a result, these systems will be better equipped to leverage the knowledge available online while minimizing the potential harms that emotionally inflected language can introduce.\nConclusion: This paper focuses on merging the sentiment analysis and reading comprehension tasks. As models increasingly process internet-derived passages during inference, understanding both the denotative and connotative meanings of text becomes essential. To explore this challenge, we transformed passages from an open-domain QA dataset's retrieval corpus into sarcastic and fact-distorted sarcastic versions. Various retrieval methods were tested under these adversarial conditions, and we proposed a 'Reading with Intent' system that outperformed baseline systems in real-world style evaluations. Finally, ablation studies identified the components of our system that contributed the most to performance improvements."}, {"title": "A Prompts", "content": "Manually creating a dataset that is intentionally sarcasm-poisoned or fact-distorted would be a vast and costly undertaking in terms of time, labor, and resources. To overcome this challenge, we used a state-of-the-art LLM, as described in Section 3, to generate the datasets presented in this paper. The Llama-3-70B Instruct model was prompted in specific ways to create sarcastic and fact-distorted passages.\nFigure 3 shows the prompts used for sarcasm-poisoning and fact-distortion.\nFigure 4 presents the \"Reading with Intent\" prompt described in the main body of the text. The figure breaks down the different components of the prompt as previously described in the paper."}, {"title": "B Reading with Intent Generation Samples", "content": "The LLMs in the \"Reading with Intent\" system were prompted in a manner that mimicked how LLMs are typically used in production applications and by end-users. Although the LLM was instructed to be terse, this was only done once, often resulting in the LLM producing complete sentences instead of 1-3 word answers. To evaluate these outputs, we considered an answer correct if the ground truth was contained within the LLM's output. In this appendix, Table 7 provides a sample of LLM outputs alongside the ground truth as a visualization and qualitative sanity check for readers. Rows highlighted in green indicate where our evaluation metric aligns with the expected outcome, while rows in red indicate discrepancies. The mistakes made by our evaluation system are similar to those an Exact Match (EM) scoring system would make. For example, for the query \u201cprotein that serves as the precursor molecule for thyroid hormones\", the ground truth answer is \"Thyroglobulin (TG)\", and the inferred answer included \"thyroglobulin\". However, because the model did not also provide the molecule's abbreviation, this answer was marked incorrect - a mistake that would also occur with EM scoring. On the other hand, our evaluation method allows for correct assessment even when the model responds in a complete sentence, as seen with the query \"where do the Great Lakes meet the ocean\" where the model's correct full-sentence answer was marked accurate."}, {"title": "C Alternative Evaluation Scores", "content": "In addition to the evaluation presented in the paper's main body, we also prompted the model in a way that hewed more closely to how models are typically prompted in Open-Domain QA research. Here, the prompts were optimized not only to 'Read with Intent' but also to produce answers limited to 1-3 words. This approach, however, appears to make the task more challenging for the model, as it now has two goals to optimize. Table 8 presents the results of these experiments. The general trend for models between datasets remains consistent: FS NQ slightly outperforms NQ (or the performances are close together); both outperform PS-A and PS-M NQ; and PS-A NQ still outperforms PS-M NQ. However, in this scenario, the base prompt slightly outperforms the 'Reading with Intent' prompt, likely because the model is balancing multiple objectives-answering the question, being brief, and interpreting the intent of the provided passages simultaneously."}]}