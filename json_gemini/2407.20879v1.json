{"title": "A SCALABLE TOOL FOR ANALYZING GENOMIC VARIANTS OF HUMANS USING KNOWLEDGE GRAPHS AND MACHINE LEARNING", "authors": ["Shivika Prasanna", "Ajay Kumar", "Deepthi Rao", "Eduardo Simoes", "Praveen Rao"], "abstract": "The integration of knowledge graphs and graph machine learning (GML) in genomic data analysis\noffers several opportunities for understanding complex genetic relationships, especially at the RNA\nlevel. We present a comprehensive approach for leveraging these technologies to analyze genomic\nvariants, specifically in the context of RNA sequencing (RNA-seq) data from COVID-19 patient\nsamples. The proposed method involves extracting variant-level genetic information, annotating\nthe data with additional metadata using SnpEff, and converting the enriched Variant Call Format\n(VCF) files into Resource Description Framework (RDF) triples. The resulting knowledge graph\nis further enhanced with patient metadata and stored in a graph database, facilitating efficient\nquerying and indexing. We utilize the Deep Graph Library (DGL) to perform graph machine learning\ntasks, including node classification with GraphSAGE and Graph Convolutional Networks (GCNs).\nOur approach demonstrates significant utility using our proposed tool, VariantKG, in three key\nscenarios: enriching graphs with new VCF data, creating subgraphs based on user-defined features,\nand conducting graph machine learning for node classification.", "sections": [{"title": "1 Introduction", "content": "Olson [1993] aimed to sequence the entire human genome, resulting in an official gene map. The gene map has\noffered crucial insights into the human blueprint, accelerating the study of human biology and advancements in medical\npractices. This information has been represented in Variant Calling Format (VCF) files that store small-scale information\nor genetic variation data.\nVariants are genetic differences between healthy and diseased tissues or between individuals of a population. Analyzing\nvariants can tremendously help prevent, diagnose, and even treat diseases. The process of analyzing these genetic\ndifferences or variations in DNA sequences and categorizing their functional significance is called variant analysis.\nRNA sequencing is similar to DNA sequencing but differs in its extraction. RNA is extracted from a sample and\nthen reverse transcribed to produce what is known as copy or complementary DNA called cDNA. This cDNA is then\nfragmented and run through a next-gen sequencing system. Examining DNA provides a static picture of what a cell or\nan organism might do, but measuring RNA tells us precisely what the cell or organism is doing. Another advantage of\nRNA sequencing is that molecular features sometimes can only be observed at the RNA level.\nVariant calling pipeline is the process of identifying variants from sequence data. To measure the deleteriousness of\na variant, the Combined Annotation Dependent Depletion (CADD) Rentzsch et al. [2019, 2021] scores tool is used.\nCADD evaluates or ranks the deleteriousness of a single nucleotide, insertion, and deletion variants in the human\ngenome. The COVID-19 genetic data discussed in this paper was collected from the European Nucleotide Archive\n(ENA) Rao et al. [2021].\nKnowledge graphs such as YAGO Mahdisoltani et al. [2013], Wikidata Vrande\u010di\u0107 and Kr\u00f6tzsch [2014], DBPedia\nLehmann et al. [2015], and Schema.org Guha et al. [2016] are crucial for structuring and linking vast amounts of diverse\ndata, to enable efficient information retrieval, enhance data interoperability, and provide a foundation for advanced\napplications in domains not limited to semantic search, natural language processing, and data integration. One such\nexample is the work of Dong [2018] which focuses on constructing a comprehensive knowledge graph (KG) called\nProductKG from Amazon's large-scale and diverse product catalog. ProductKG captures product attributes, categories,\nand relationships using graph mining and embedding techniques. This structured representation aims to improve\nunderstanding and retrieval of product information, thereby enhancing user experience and supporting various AI\napplications within Amazon's ecosystem. Another such example of KGs is FoodKG, introduced by Gharibi et al. [2020]\nthat demonstrates the importance of knowledge graphs in the Food domain through their tool, FoodKG, by integrating\ndiverse datasets, using NLP to extract meaningful entities and state-of-the-art model to enhance and enrich graphs based\non Food, Energy and Water (FEW) used by the tool. Their contribution highlights the significant role of knowledge\ngraphs in managing and utilizing large-scale, heterogeneous data.\nRepresenting genomic data as knowledge graphs allows vast and diverse information from various sources to be\nintegrated. These specialized graph structures, which model entities as nodes and relationships as edges, provide an\nideal framework for integrating and organizing diverse biological information from multiple sources. Furthermore, it\nallows for efficient querying and indexing and supports inference and new knowledge discovery.\nThe key contributions of this work are:\n\u2022 We introduce a data collection pipeline to extract variant-level genetic information from RNA-seq IDs using\nthe ENA browser. We also discuss adding additional information to the VCF files using the SnpEff tool.\n\u2022 We elaborate on the construction and enrichment of our knowledge graph. We utilize the vcf2rdf tool from\nSparqling-genomics for the VCF files, in addition to our explicitly-defined ontologies. We translate the\nRDF-triples into an efficient and easily parsable format (NQuads).\n\u2022 We demonstrate the use of our tool, VariantKG and present three scenarios which are Graph Enrichment to\nconsume new VCF information or use the existing data from our knowledge base, Graph Creation to create\nsubgraphs with user-defined features and Graph Machine Learning and Inference for node classification tasks\nusing GraphSAGE and Graph Convolutional Network supported by Deep Graph Library.\nThe remainder of the paper is organized as follows: 2 explores the previous works that integrate knowledge graphs\nwith the vast genomic data; 3 discusses the data collection pipeline, construction of our knowledge graph (KG) using\nontology and enrichment of the KG with patient metadata; 4 introduces the graph storage and database used in this\nresearch work and tool; 5 discusses the use of Deep Graph Library (DGL) for node classification tasks and introduces\nour tool, VariantKG in which we demonstrate 3 scenarios, namely how a user can use data from our existing knowledge\nbase, add new VCFs to enrich the dataset, and use a combination of the added files and the existing knowledge base to\nperform node classification tasks using two Graph Neural Networks - GraphSAGE and Graph Convolutional Networks\n(GCNs), using our knowledge graph database for graph machine learning tasks; 6 concludes our work."}, {"title": "2 Related Work", "content": "Knowledge graphs are widely used to integrate and analyze diverse genomic data, providing a comprehensive and\ncontextual representation of genomic information. For instance, [Feng et al., 2024] extended the development of\nGenomicKB by creating a graph database that integrates human genome, epigenome, transcriptome, and 4D nucleome\ndata. This extensive database, annotated from over 30 consortia and portals, includes 347 million entities, 1.36 billion\nrelations, and 3.9 billion properties, covering comprehensive data on pancreases and diabetes' GWAS, disease ontology,\nand eQTL data. Another notable work, [Feng et al., 2023], presented a knowledge graph, GenomicKB, that consolidates\nvarious sources of genomic information, including data from genomic databases, experimental studies, literature, and\npublic repositories, into a single, unified framework. This integration facilitates efficient data analysis and knowledge\ndiscovery through a user-friendly web portal."}, {"title": "3 Knowledge Graph Construction", "content": "In this section, we explain how to construct knowledge graphs. First, we describe how the data was collected and the\nworkflow processes. We also incorporate the patient metadata used to enhance the knowledge base. Then, we highlight\nthe process of annotating the data, followed by the ontology description. Lastly, we discuss the representation of VCF\nfiles and CADD Scores in a knowledge graph."}, {"title": "3.1 Data Collection", "content": "COVID-19 RNA sequence IDs were first collected from the European Nucleotide Archive. A total of 3,716 VCF files\nwere collected till March 2023. The workflow has been shown in Figure 1.\n\u2022 FASTQ files (part A): These IDs were utilized to download the RNA sequences, which were in FASTQ Li\net al. [2008], Li and Durbin [2009] format. The FASTQ file format consists of a series of records, each of\nwhich contains four lines of text: the first line starting with '@' contains a sequence identifier, the second line\ncontains the actual nucleotide sequence, the third line starts with '+' and may optionally contain additional\ninformation about the sequence, and the fourth line contains quality scores encoded as ASCII 10 characters.\nThe quality scores indicate the confidence in the accuracy of each base call and are typically represented as\nPhred scores.\n\u2022 uBAM files (part B): The FASTQ files were then converted to unmapped BAM (uBAM) files for storing\naligned sequencing data. A uBAM file contains unmapped reads, meaning reads that could not be confidently\naligned to a reference genome. These reads can be used for downstream analysis, such as de novo assembly,\nquality control, and identification of novel sequences.\n\u2022 GATK workflow (part C): The uBAM files were passed through the Genomic Analysis Toolkit (GATK)\nworkflow McKenna et al. [2010] that converts the files into Variant Calling Format (VCF) Danecek et al.\n[2011] files. It is a comprehensive toolkit developed by the Broad Institute that includes various tools\nand algorithms for processing genomic data, such as read mapping, local realignment, base quality score\nrecalibration, variant calling, and variant filtering.\n\u2022 The unannotated VCF files that were obtained as the result of the workflow have been shown in part D. For\neach VCF file, there is also a corresponding CADD Scores file that was obtained using GrCh37 through the\nworkflow, as shown in part E."}, {"title": "3.2 Data Annotations", "content": "Once the RNA sequencing was completed, two main files were generated for each RNA-seq ID \u2013 a VCF file and a\nCADD Scores file.\nFor further annotations, SnpEff Cingolani et al. [2012], a command-line, variant annotation, and effect prediction tool,\nwas utilized. This tool annotates and predicts the effects of genetic variants. SnpEff takes the predicted variants (SNPs,\ninsertions, deletion, and MNPs) as input and produces a file with annotations of the variants and the effects they produce\non known genes.\nSnpEff classifies variants as single nucleotide polymorphisms (SNPs), insertions, deletions, multiple-nucleotide\npolymorphisms, or an InDel. While the original VCF file contains the INFO field, SnpEff adds additional annotations\nto this field to describe each variant further. In the process, it also updates the header fields. This field is tagged by\n'ANN', which is pipe symbol separated and provides a summary of the predicted effects of a genetic variant on each\naffected transcript. Figure 2 shows the ANN field highlighted in bold.\nA variant may have one or more annotations and multiple annotations are comma-separated. There are several fields\nwithin the ANN tag, mainly:\n\u2022 Allele (ALT): Information on alternate alleles that are predicted to cause a functional impact on a gene or\nprotein\n\u2022 Annotation (effect): Type of effect caused by the variant on the transcript\n\u2022 Putative impact: Qualitative measure of the impact of the variant on the transcript\n\u2022 Gene name: Name of the affected gene\n\u2022 Gene ID: Unique identifier of the affected gene"}, {"title": "3.3 Metadata", "content": "The metadata was downloaded from the SRA \u00b9 web tool of NCBI. The metadata runs for each patient are stored in the\nStudy section. We leveraged the SRA web tool over entrez to extract a wider range of metadata information such as\ndisease, age, fatality. The obtained metadata was then converted to NQ triples to facilitate the use of named graph to\nlink this information to the variant information obtained, as shown in Section 3.1. An example of the metadata obtained\nfor the accessionID SRR12570493 is as follows:\nSRR12570493,65 (Age),,RNA-Seq,119,1473875214, PRJNA661032, SAMN15967295,582422941,,GEO,\nSevere COVID, Alive, public, \"fastq,sra,run.zq\",\"gs,ncbi,s3\",\"s3.us-east-1,gs.US, ncbi.\npublic\", SRX9058173, NextSeq 500, GSM4762164,, PAIRED, CDNA, Homo sapiens, TRANSCRIPTOMIC,\nILLUMINA, 2021-01-27T00:00:00Z,,2020-09-02T12:06:00Z,1,GSM4762164, male, 6, Patient 14\nblood, SRP279746, Patient 14, Blood,,,,,,,,,,,,,\nThe NQ triple for the age attribute for the same accessionID run is as follows:"}, {"title": "3.4 Ontology", "content": "A knowledge graph is represented using an ontology, which can be represented using a formal language such as RDF\n(Resource Description Framework), OWL (Web Ontology Language), or another domain-specific language. The\nontology in this work has been represented using RDF. Each node-edge-node is represented as a triple by RDF. In\na triple, the subject defines the first node, and the object defines the second node. The predicate defines the edge or\nrelation joining the two nodes. A triple always ends with a dot.\n1(https://www.ncbi.nlm.nih.gov/sra)"}, {"title": "3.5 Conversion of VCF files to Knowledge Graphs", "content": "To transform the data in VCF, SPARQLing Genomics [Di Bartolomeo et al., 2018] was utilized. SPARQLing Genomics\nis an open-source platform for querying and analyzing genomic data using the Semantic Web and Linked Data\ntechnologies. The platform provides an easy-to-use interface as well, that has been built to support SPARQL queries\nand various SPARQL query features, including sub-queries, filters, and aggregates. SPARQLing Genomics provides\nseveral in-built, ready-to-use tools, one of which is vcf2rdf that converts VCF data into RDF triples.\nThe triples generated by the tool consist of uniquely identifiable names with symbolic and literal values like numbers or\ntext."}, {"title": "3.6 Conversion of CADD Score files to Knowledge Graphs", "content": "The SnpEff and vcf2rdf tools were useful for converting VCF files to triples. However, the CADD Scores obtained\nthrough the pipeline were in tab-separated (TSV) format. To enrich the knowledge graphs, the CADD Scores had to be\ntranslated to RDF triples as well. Therefore, the ontology for CADD Scores was explicitly defined."}, {"title": "4 Graph Storage And Database", "content": "Each variant file represented a single knowledge graph, so to unify several knowledge graphs into one single large\ngraph, BlazeGraph bla [2024] has been leveraged. This large knowledge graph was then queried to create a dataset for a\ncase study. It is important to note that the edges are homogeneous in nature.\nBlazeGraph is a high-performance, horizontally scalable, and open-source graph database that can be used to store and\nmanage large-scale graph data. It has been designed to provide efficient graph querying and supports RDF data model\nthat allows it to store and process both structured and semi-structured data. BlazeGraph uses a distributed architecture\nthat can be easily integrated with other big data tools, such as Hadoop and Spark, to perform complex analytics on\nlarge-scale graph data.\nBlazeGraph has been leveraged for efficiently querying the knowledge graphs to generate the dataset for Graph Neural\nNetwork downstream tasks. Other tools such as RIQ Katib et al. [2017, 2016], Slavov et al. [2015] can be used to index\nand query RDF-named graphs.\nThe total number of triples in the knowledge graph, after aggregating only 511 VCF files on a single machine, is as\nlarge as 3.1 Billion."}, {"title": "5 KG Inference: Case Study", "content": "In this section, we will discuss the usage of the knowledge graph created and demonstrate the use of VariantKG. We\naim to use the KG for a classification task using graph machine learning. For the classification task, we are leveraging\nthe open-source graph-based library called Deep Graph Library Wang et al. [2019].\nWe packaged the several components discussed above in a user-friendly, robust and efficient tool called VariantKG, to\nempower users to train their own GNN models using our data or a combination of our data and new VCF files. We will\nfurther expand on the workings of our tool in Section 5.4."}, {"title": "5.1 Deep Graph Library", "content": "To implement the task, Deep Graph Library (DGL), an open-source library supporting graph-based deep learning was\nutilized. DGL provides a set of high-level APIs for building scalable and efficient graph neural network models. With\nDGL, we can create, manipulate, and learn from large-scale graphs with billions of nodes and edges.\nThere are three main tasks supported by DGL:\n\u2022 Node Classification: Predict the class or label of a node in a graph based on its features.\n\u2022 Link Prediction: Predict if there is a connection or an edge between two nodes.\n\u2022 Graph Classification: Classify an entire graph into one or more classes or categories.\nDGL represents a graph as a DGLGraph object, which is a framework-specific graph object. It requires the number\nof nodes and a list of source and destination nodes, where nodes and edges must have consecutive IDs starting from\n0. Since DGL only accepts numeric input, all strings such as URI were mapped to integers. In this case study, node\nclassification was used to classify variants into CADD Score categories based on their features."}, {"title": "5.2 Node Classification Task", "content": "For this task, Graph Convolutional Network (GCN) Zhang et al. [2019] and GraphSAGE Hamilton et al. [2017] have\nbeen used. For both models, each node is associated with a feature vector.\nGCNs use node embeddings and adjacency matrices to compute new embeddings while training. Similar to CNN,\nthe model weights and biases are first initialized to 1, and then a section of the graph is passed through the model.\nA non-linear activation function is used to compute predicted node embeddings for each node. Cross entropy loss is\ncalculated to quantify the difference between the predicted node embeddings and the ground truth. Loss gradients are\nthen computed to update the model using the ADAM optimizerKingma and Ba [2014] for this task. These steps are\nrepeated until convergence.\nGraphSAGE uses SAGEConv layers where for every iteration, the output of the model involves finding new node\nrepresentation for every node in the graph. Mean is used as the aggregation function, and the ReLU activation function\nhas been utilized. ADAM optimizer was used for this model as well. One of the most noted properties of GraphSAGE\nis its ability to aggregate neighbor node embeddings for a given target node. Through the experiments conducted, this\nproperty was observed. GraphSAGE also generalizes better to unseen nodes because of its ability to perform inductive\nlearning on graphs."}, {"title": "5.3 Experiments & Results", "content": ""}, {"title": "5.3.1 Experiment Setup", "content": "The experiments were run on CloudLab Ricci et al. [2014], a testbed for cloud computing research and new applications.\nCloudLab cluster at Clemson University was built in partnership with Dell. Clemson machines were used to carry out\nthe standalone training experiments. They were carried out on nodes with 16 cores per node (2 CPUs), 12x4 TB disk\ndrives in each node, plus 8x1 TB disks in each node. The nodes are configured with 256 GB of memory and 73TB of\nRAM. The operating system installed across all of them was Ubuntu 18.04.\nWisconsin machines were also used to carry out the standalone training experiments described in section 6.2. The\nCloudLab cluster at the University of Wisconsin was built in partnership with Cisco, Seagate, and HP. The cluster has\n523 servers with a total of 10,060 cores and 1,396 TB of storage, including SSDs on every node.\nThe nodes were chosen upon availability for the standalone node classification tasks."}, {"title": "5.4 VariantKG as a tool", "content": "The architecture of our tool, shown in Figure 7 has been designed to facilitate the extraction, processing, and analysis of\nvariant-level genomic data using GNNs. The workflow integrates custom data inputs, feature selection, storage in a\ngraph database, graph creation, and model training for inferencing genomic information. Given the rapidly expanding\nrepository of genomic data, VariantKG offers a robust platform for researchers or users to extract relevant information\nand train Graph Neural Networks (GNNs) such as GraphSAGE for inferential purposes. Users have the flexibility to\nupload one or more VCF files, which may include associated CADD Scores for specific variants. Alternatively, users\ncan select from pre-existing datasets that have been structured into a large-scale, comprehensive knowledge graph."}, {"title": "5.4.1 Scenario 1: Graph Enrichment", "content": "The first part of the VariantKG tool focuses on KG enrichment. A user can upload one or more VCF files containing\nvariant-level information and CADD Scores in a TSV format as shown in Figures 8 or the user can select patients from\nthe existing KG. If the user uploads VCF files, the information in the files is first run through the SnpEFF tool. SnpEFF\nadds new annotated information to each variant in the VCF file and also updates the headers of the file to reflect the\nannotations. These annotations are functional information that is added to the 'INFO' field using the 'ANN' tag. This\nANN field, in turn, consists of several bits of information, such as allele, annotation using Sequence and Ontology\nterms, putative impact, gene name, gene ID, feature type, feature ID, transcript biotype, exon or intron rank, cDNA\nposition, protein position and several types of distances to the feature.\nOnce the files have been annotated, we then utilize the vcf2rdf tool provided by SPARQLing-Genomics to efficiently\ntranslate the information on each variant into several triples in an RDF-suitable N3 format. These N3 triples were\nconverted to NQ format for several reasons. Converting to NQ format allows for the use of a named graph, which\nprovides a robust way to group triples into distinct sets, considerably enhancing data organization and management.\nAnother advantage of using named graph is the support it provides to easily attach new metadata to specific data subsets.\nAdditionally, named graphs also enable more precise and efficient SPARQL queries, improving data extraction quality\nand speed.\nThe triples are stored in Blazegraph, a high-performance graph database, which also stores the larger knowledge graph.\nTo prepare the data from ML tasks, users can select patients using age groups as shown in Figure 9, or using the\naccession IDs, as shown in Figure 10, which are internally fetched using another SPARQL query that is only executed\nwhen the user wishes to prepare data for downstream ML tasks for efficiency.\nThe user can then select the features from a list consisting of the original headers from the VCF files and the annotated\nfeatures by SnpEff. Once the user hits the \u2018Fetch from KG' button, another SPARQL query is then executed in the\nbackend. If the user is selected from an age group, the SPARQL query consists of a filter that fetches all the accession\nIDs, which is passed as a list to the final query. If the user selects the accession IDs, it is passed as a list, similar to the\nprevious query and the final query, shown below, fetches all the features selected for those patients or accession IDs."}, {"title": "5.4.2 Scenario 2: Graph Creation", "content": "Once the data has been fetched in the backend, the user is redirected to 'Graph Creation' where the user can select node\nfeatures in the graph and select the class label from a subset of the features that are meaningful to be classified as a\npart of the classification task. The user is then given complete control of the graph construction where the user can\nselect the edge type which can be using Gene Name (default) or fully connected. The weight of the edges can be 1\n(default), the number of incoming edges to a node, or a user-defined value. The user can then select if the edges should\nbe bidirectional and the train:val splits. The default is 80:10 with the remaining 10% calculated in the backend to reduce\nuser clicks. Using this information, the graph is created for the node classification task. This is shown in Figure 11"}, {"title": "5.4.3 Scenario 3: Graph Machine Learning & Inference", "content": "The graph for the node classification task needs to be in a DGL-specific input format. The data is thus converted to a\n'DGLGraph' object that consists of only integers that are mapped to the actual string values. The user can view the\nsummary of the graph once created. This summary consists of all the selected features, the class label, the number of\nclasses, graph properties, and the number of nodes and edges. The user is given a choice to download the DGL graph or\ncontinue with graph machine learning. For the ML task, the user can select between GraphSAGE and GCN models for\nthe classification task and set the hyperparameters to train the model as shown in Figure 12. This includes the number\nof layers other than the input and output layers that constitute the primary architecture of the model, the number of\nhidden layers, the dropout rate, the learning rate, and the number of epochs for training.\nOnce the training begins, the user can view the training and validation loss plots, the validation accuracy plot, and the\nCPU memory usage as shown in Figure 13.\nIf the user wants to infer the ML task, they will be navigated to the 'Inference' tab shown in Figure 14, which displays\nthe evaluation metrics and confusion matrix. The model is evaluated on Accuracy, macro-, and weighted- Precision,\nRecall, F1 score, and support, which is the number of samples for the given class."}, {"title": "6 Conclusion", "content": "This work shows that representing genomic data as knowledge graphs allows vast and diverse information to be\nintegrated from various sources. Modeling entities as nodes and relationships as edges provides an ideal framework for\nintegrating and organizing diverse information. We first described the data collection pipeline, followed by the usage of\nthe SnpEff tool to obtain additional annotations and the SPARQLing Genomics tool to convert the annotations to an\nRDF format. An ontology to collate information gathered from different sources is presented. Using this ontology, we\ndescribed how the knowledge graph is created. This knowledge graph contains RNA sequencing information at the\nvariant level from COVID-19 patients from different regions such as lung, blood, etc. Lastly, we demonstrate the usage\nof the created knowledge graph for a node classification task using the Deep Graph Library through our tool, VariantKG.\nAs part of this, we described various scenarios that can be performed by a user. As part of our future work, we aim to\nexpand the knowledge graph and explore more avenues to use the same to aid researchers working in this domain."}]}