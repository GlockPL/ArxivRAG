{"title": "A SCALABLE TOOL FOR ANALYZING GENOMIC VARIANTS OF HUMANS USING KNOWLEDGE GRAPHS AND MACHINE LEARNING", "authors": ["Shivika Prasanna", "Ajay Kumar", "Deepthi Rao", "Eduardo Simoes", "Praveen Rao"], "abstract": "The integration of knowledge graphs and graph machine learning (GML) in genomic data analysis offers several opportunities for understanding complex genetic relationships, especially at the RNA level. We present a comprehensive approach for leveraging these technologies to analyze genomic variants, specifically in the context of RNA sequencing (RNA-seq) data from COVID-19 patient samples. The proposed method involves extracting variant-level genetic information, annotating the data with additional metadata using SnpEff, and converting the enriched Variant Call Format (VCF) files into Resource Description Framework (RDF) triples. The resulting knowledge graph is further enhanced with patient metadata and stored in a graph database, facilitating efficient querying and indexing. We utilize the Deep Graph Library (DGL) to perform graph machine learning tasks, including node classification with GraphSAGE and Graph Convolutional Networks (GCNs). Our approach demonstrates significant utility using our proposed tool, VariantKG, in three key scenarios: enriching graphs with new VCF data, creating subgraphs based on user-defined features, and conducting graph machine learning for node classification.", "sections": [{"title": "1 Introduction", "content": "Olson [1993] aimed to sequence the entire human genome, resulting in an official gene map. The gene map has offered crucial insights into the human blueprint, accelerating the study of human biology and advancements in medical practices. This information has been represented in Variant Calling Format (VCF) files that store small-scale information or genetic variation data.\nVariants are genetic differences between healthy and diseased tissues or between individuals of a population. Analyzing variants can tremendously help prevent, diagnose, and even treat diseases. The process of analyzing these genetic differences or variations in DNA sequences and categorizing their functional significance is called variant analysis.\nRNA sequencing is similar to DNA sequencing but differs in its extraction. RNA is extracted from a sample and then reverse transcribed to produce what is known as copy or complementary DNA called cDNA. This cDNA is then fragmented and run through a next-gen sequencing system. Examining DNA provides a static picture of what a cell or an organism might do, but measuring RNA tells us precisely what the cell or organism is doing. Another advantage of RNA sequencing is that molecular features sometimes can only be observed at the RNA level.\nVariant calling pipeline is the process of identifying variants from sequence data. To measure the deleteriousness of a variant, the Combined Annotation Dependent Depletion (CADD) Rentzsch et al. [2019, 2021] scores tool is used. CADD evaluates or ranks the deleteriousness of a single nucleotide, insertion, and deletion variants in the human genome. The COVID-19 genetic data discussed in this paper was collected from the European Nucleotide Archive (ENA) Rao et al. [2021].\nKnowledge graphs such as YAGO Mahdisoltani et al. [2013], Wikidata Vrande\u010di\u0107 and Kr\u00f6tzsch [2014], DBPedia Lehmann et al. [2015], and Schema.org Guha et al. [2016] are crucial for structuring and linking vast amounts of diverse data, to enable efficient information retrieval, enhance data interoperability, and provide a foundation for advanced applications in domains not limited to semantic search, natural language processing, and data integration. One such example is the work of Dong [2018] which focuses on constructing a comprehensive knowledge graph (KG) called ProductKG from Amazon's large-scale and diverse product catalog. ProductKG captures product attributes, categories, and relationships using graph mining and embedding techniques. This structured representation aims to improve understanding and retrieval of product information, thereby enhancing user experience and supporting various AI applications within Amazon's ecosystem. Another such example of KGs is FoodKG, introduced by Gharibi et al. [2020] that demonstrates the importance of knowledge graphs in the Food domain through their tool, FoodKG, by integrating diverse datasets, using NLP to extract meaningful entities and state-of-the-art model to enhance and enrich graphs based on Food, Energy and Water (FEW) used by the tool. Their contribution highlights the significant role of knowledge graphs in managing and utilizing large-scale, heterogeneous data.\nRepresenting genomic data as knowledge graphs allows vast and diverse information from various sources to be integrated. These specialized graph structures, which model entities as nodes and relationships as edges, provide an ideal framework for integrating and organizing diverse biological information from multiple sources. Furthermore, it allows for efficient querying and indexing and supports inference and new knowledge discovery.\nThe key contributions of this work are:\n\u2022 We introduce a data collection pipeline to extract variant-level genetic information from RNA-seq IDs using the ENA browser. We also discuss adding additional information to the VCF files using the SnpEff tool.\n\u2022 We elaborate on the construction and enrichment of our knowledge graph. We utilize the vcf2rdf tool from Sparqling-genomics for the VCF files, in addition to our explicitly-defined ontologies. We translate the RDF-triples into an efficient and easily parsable format (NQuads).\n\u2022 We demonstrate the use of our tool, VariantKG and present three scenarios which are Graph Enrichment to consume new VCF information or use the existing data from our knowledge base, Graph Creation to create subgraphs with user-defined features and Graph Machine Learning and Inference for node classification tasks using GraphSAGE and Graph Convolutional Network supported by Deep Graph Library.\nThe remainder of the paper is organized as follows: 2 explores the previous works that integrate knowledge graphs with the vast genomic data; 3 discusses the data collection pipeline, construction of our knowledge graph (KG) using ontology and enrichment of the KG with patient metadata; 4 introduces the graph storage and database used in this research work and tool; 5 discusses the use of Deep Graph Library (DGL) for node classification tasks and introduces our tool, VariantKG in which we demonstrate 3 scenarios, namely how a user can use data from our existing knowledge base, add new VCFs to enrich the dataset, and use a combination of the added files and the existing knowledge base to perform node classification tasks using two Graph Neural Networks - GraphSAGE and Graph Convolutional Networks (GCNs), using our knowledge graph database for graph machine learning tasks; 6 concludes our work."}, {"title": "2 Related Work", "content": "Knowledge graphs are widely used to integrate and analyze diverse genomic data, providing a comprehensive and contextual representation of genomic information. For instance, [Feng et al., 2024] extended the development of GenomicKB by creating a graph database that integrates human genome, epigenome, transcriptome, and 4D nucleome data. This extensive database, annotated from over 30 consortia and portals, includes 347 million entities, 1.36 billion relations, and 3.9 billion properties, covering comprehensive data on pancreases and diabetes' GWAS, disease ontology, and eQTL data. Another notable work, [Feng et al., 2023], presented a knowledge graph, GenomicKB, that consolidates various sources of genomic information, including data from genomic databases, experimental studies, literature, and public repositories, into a single, unified framework. This integration facilitates efficient data analysis and knowledge discovery through a user-friendly web portal.\nKnowledge graphs have been instrumental in understanding the COVID-19 virus and its treatment. For example, [Sakor et al., 2023] developed a framework that integrates diverse COVID-19 drug resources to discover drug-drug interactions among COVID-19 treatments, utilizing RDF mapping language and Natural Language Processing (NLP) to extract relevant entities and relationships. Similarly, [Reese et al., 2021] proposed KG-COVID-19, a knowledge graph framework that integrates heterogeneous data on SARS-CoV-2 and related viruses, supporting downstream tasks such as machine learning, hypothesis-based querying, and user interface exploration. [Chen et al., 2021] used semantic web technology RDF to integrate COVID-19 data extracted from iTextMine, PubTator, and SemRep biological databases into a standardized Knowledge Graph (KG). This COVID-19 KG supports federated queries on the semantic web and is accessible through browsing and searching web interfaces, with a RESTful API for programmatic access and RDF file downloads.\nDeep learning has significantly influenced a wide range of domains, with genomic studies being particularly impacted. For instance, [Liu et al., 2020] introduced DeepCDR, a method using deep learning to predict cancer cells' response to different drugs, facilitating effective cancer treatment. Another innovative approach was proposed by [Lanchantin and Qi, 2019], who developed ChromeGCN for predicting epigenetic states using sequences and 3D genome data. ChromeGCN leverages graph convolutional networks (GCNs) to predict the epigenetic states of genomic regions, representing genomes as graphs where nodes are genomic regions and edges represent relationships between them. This method's predictive power enables the identification of functional genomic elements and regulatory regions, providing insights into gene regulation and cellular function.\nAdditionally, [Harnoune et al., 2021] proposed constructing knowledge graphs from clinical data using the BERT (Bidirectional Encoder Representations from Transformers) model. This approach focused on creating biomedical knowledge graphs by leveraging BERT's contextual understanding capabilities to process biomedical text data, including clinical records and scientific literature, extracting meaningful and contextually rich information. [Domingo-Fern\u00e1ndez et al., 2021] developed a multi-modal cause-and-effect COVID-19 knowledge model using Biological Expression Language (BEL) as a triple (i.e., source node-relation-target node) with metadata about nodes. This model utilized GraphML, NDEx, and SIF representations for network visualization and was made accessible through a web platform to enhance its visibility and utility.\nLastly, [Al-Obeidat et al., 2020] focused on extracting and utilizing knowledge from COVID-19-related news articles, providing a platform for researchers, data analysts, and data scientists to investigate and recommend strategies to address global challenges. [Sun et al., 2018] proposed KGBSVM (Kernelized Generalized Bayesian Rule Mining with Support Vector Machines), a method to analyze high-dimensional genome data, aiming to improve classification accuracy on general tasks, whether binary or multi-class. This method enhances the accuracy and efficiency of genomic data classification, contributing to better data analysis and interpretation in the field.\nThese diverse efforts highlight the significant potential of integrating knowledge graphs with genomic data and deep learning, facilitating comprehensive data integration, efficient analysis, and innovative solutions to complex problems in genomics and beyond."}, {"title": "3 Knowledge Graph Construction", "content": "In this section, we explain how to construct knowledge graphs. First, we describe how the data was collected and the workflow processes. We also incorporate the patient metadata used to enhance the knowledge base. Then, we highlight the process of annotating the data, followed by the ontology description. Lastly, we discuss the representation of VCF files and CADD Scores in a knowledge graph."}, {"title": "3.1 Data Collection", "content": "COVID-19 RNA sequence IDs were first collected from the European Nucleotide Archive. A total of 3,716 VCF files were collected till March 2023. The workflow has been shown in Figure 1.\n\u2022 FASTQ files (part A): These IDs were utilized to download the RNA sequences, which were in FASTQ Li et al. [2008], Li and Durbin [2009] format. The FASTQ file format consists of a series of records, each of which contains four lines of text: the first line starting with '@' contains a sequence identifier, the second line contains the actual nucleotide sequence, the third line starts with '+' and may optionally contain additional information about the sequence, and the fourth line contains quality scores encoded as ASCII 10 characters. The quality scores indicate the confidence in the accuracy of each base call and are typically represented as Phred scores."}, {"title": "3.2 Data Annotations", "content": "Once the RNA sequencing was completed, two main files were generated for each RNA-seq ID \u2013 a VCF file and a CADD Scores file.\nFor further annotations, SnpEff Cingolani et al. [2012], a command-line, variant annotation, and effect prediction tool, was utilized. This tool annotates and predicts the effects of genetic variants. SnpEff takes the predicted variants (SNPs, insertions, deletion, and MNPs) as input and produces a file with annotations of the variants and the effects they produce on known genes.\nSnpEff classifies variants as single nucleotide polymorphisms (SNPs), insertions, deletions, multiple-nucleotide polymorphisms, or an InDel. While the original VCF file contains the INFO field, SnpEff adds additional annotations to this field to describe each variant further. In the process, it also updates the header fields. This field is tagged by 'ANN', which is pipe symbol separated and provides a summary of the predicted effects of a genetic variant on each affected transcript. Figure 2 shows the ANN field highlighted in bold.\nA variant may have one or more annotations and multiple annotations are comma-separated. There are several fields within the ANN tag, mainly:\n\u2022 Allele (ALT): Information on alternate alleles that are predicted to cause a functional impact on a gene or protein\n\u2022 Annotation (effect): Type of effect caused by the variant on the transcript\n\u2022 Putative impact: Qualitative measure of the impact of the variant on the transcript\n\u2022 Gene name: Name of the affected gene\n\u2022 Gene ID: Unique identifier of the affected gene"}, {"title": "3.3 Metadata", "content": "The metadata was downloaded from the SRA \u00b9 web tool of NCBI. The metadata runs for each patient are stored in the Study section. We leveraged the SRA web tool over entrez to extract a wider range of metadata information such as disease, age, fatality. The obtained metadata was then converted to NQ triples to facilitate the use of named graph to link this information to the variant information obtained, as shown in Section 3.1. An example of the metadata obtained for the accessionID SRR12570493 is as follows:\nSRR12570493,65 (Age),,RNA-Seq,119,1473875214, PRJNA661032, SAMN15967295,582422941,,GEO, Severe COVID, Alive, public, \"fastq,sra,run.zq\",\"gs,ncbi,s3\",\"s3.us-east-1,gs.US, ncbi. public\", SRX9058173, NextSeq 500, GSM4762164,, PAIRED, CDNA, Homo sapiens, TRANSCRIPTOMIC, ILLUMINA, 2021-01-27T00:00:00Z,,2020-09-02T12:06:00Z,1,GSM4762164, male, 6, Patient 14 blood, SRP279746, Patient 14, Blood,,,,,,,,,,,,,\nThe NQ triple for the age attribute for the same accessionID run is as follows:\n<https://www.ncbi.nlm.nih.gov/sra/?term=SRR12570589> <https://www.wikidata.org/wiki/Q11904283> \"61.0\"^^<<\\protect\\vrule width0pt\\protect\\href{http://www.w3.org/2001/XMLSchema#float}{http://www.w3.org/2001/XMLSchema#float}> <sg://SRR12570589>"}, {"title": "3.4 Ontology", "content": "A knowledge graph is represented using an ontology, which can be represented using a formal language such as RDF (Resource Description Framework), OWL (Web Ontology Language), or another domain-specific language. The ontology in this work has been represented using RDF. Each node-edge-node is represented as a triple by RDF. In a triple, the subject defines the first node, and the object defines the second node. The predicate defines the edge or relation joining the two nodes. A triple always ends with a dot."}, {"title": "3.5 Conversion of VCF files to Knowledge Graphs", "content": "To transform the data in VCF, SPARQLing Genomics [Di Bartolomeo et al., 2018] was utilized. SPARQLing Genomics is an open-source platform for querying and analyzing genomic data using the Semantic Web and Linked Data technologies. The platform provides an easy-to-use interface as well, that has been built to support SPARQL queries and various SPARQL query features, including sub-queries, filters, and aggregates. SPARQLing Genomics provides several in-built, ready-to-use tools, one of which is vcf2rdf that converts VCF data into RDF triples.\nThe triples generated by the tool consist of uniquely identifiable names with symbolic and literal values like numbers or text."}, {"title": "3.6 Conversion of CADD Score files to Knowledge Graphs", "content": "The SnpEff and vcf2rdf tools were useful for converting VCF files to triples. However, the CADD Scores obtained through the pipeline were in tab-separated (TSV) format. To enrich the knowledge graphs, the CADD Scores had to be translated to RDF triples as well. Therefore, the ontology for CADD Scores was explicitly defined."}, {"title": "4 Graph Storage And Database", "content": "Each variant file represented a single knowledge graph, so to unify several knowledge graphs into one single large graph, BlazeGraph bla [2024] has been leveraged. This large knowledge graph was then queried to create a dataset for a case study. It is important to note that the edges are homogeneous in nature.\nBlazeGraph is a high-performance, horizontally scalable, and open-source graph database that can be used to store and manage large-scale graph data. It has been designed to provide efficient graph querying and supports RDF data model that allows it to store and process both structured and semi-structured data. BlazeGraph uses a distributed architecture that can be easily integrated with other big data tools, such as Hadoop and Spark, to perform complex analytics on large-scale graph data.\nBlazeGraph has been leveraged for efficiently querying the knowledge graphs to generate the dataset for Graph Neural Network downstream tasks. Other tools such as RIQ Katib et al. [2017, 2016], Slavov et al. [2015] can be used to index and query RDF-named graphs.\nThe total number of triples in the knowledge graph, after aggregating only 511 VCF files on a single machine, is as large as 3.1 Billion."}, {"title": "5 KG Inference: Case Study", "content": "In this section, we will discuss the usage of the knowledge graph created and demonstrate the use of VariantKG. We aim to use the KG for a classification task using graph machine learning. For the classification task, we are leveraging the open-source graph-based library called Deep Graph Library Wang et al. [2019].\nWe packaged the several components discussed above in a user-friendly, robust and efficient tool called VariantKG, to empower users to train their own GNN models using our data or a combination of our data and new VCF files. We will further expand on the workings of our tool in Section 5.4."}, {"title": "5.1 Deep Graph Library", "content": "To implement the task, Deep Graph Library (DGL), an open-source library supporting graph-based deep learning was utilized. DGL provides a set of high-level APIs for building scalable and efficient graph neural network models. With DGL, we can create, manipulate, and learn from large-scale graphs with billions of nodes and edges.\nThere are three main tasks supported by DGL:\n\u2022 Node Classification: Predict the class or label of a node in a graph based on its features.\n\u2022 Link Prediction: Predict if there is a connection or an edge between two nodes.\n\u2022 Graph Classification: Classify an entire graph into one or more classes or categories.\nDGL represents a graph as a DGLGraph object, which is a framework-specific graph object. It requires the number of nodes and a list of source and destination nodes, where nodes and edges must have consecutive IDs starting from 0. Since DGL only accepts numeric input, all strings such as URI were mapped to integers. In this case study, node classification was used to classify variants into CADD Score categories based on their features."}, {"title": "5.2 Node Classification Task", "content": "For this task, Graph Convolutional Network (GCN) Zhang et al. [2019] and GraphSAGE Hamilton et al. [2017] have been used. For both models, each node is associated with a feature vector.\nGCNs use node embeddings and adjacency matrices to compute new embeddings while training. Similar to CNN, the model weights and biases are first initialized to 1, and then a section of the graph is passed through the model. A non-linear activation function is used to compute predicted node embeddings for each node. Cross entropy loss is calculated to quantify the difference between the predicted node embeddings and the ground truth. Loss gradients are then computed to update the model using the ADAM optimizerKingma and Ba [2014] for this task. These steps are repeated until convergence.\nGraphSAGE uses SAGEConv layers where for every iteration, the output of the model involves finding new node representation for every node in the graph. Mean is used as the aggregation function, and the ReLU activation function has been utilized. ADAM optimizer was used for this model as well. One of the most noted properties of GraphSAGE is its ability to aggregate neighbor node embeddings for a given target node. Through the experiments conducted, this property was observed. GraphSAGE also generalizes better to unseen nodes because of its ability to perform inductive learning on graphs."}, {"title": "5.3 Experiments & Results", "content": null}, {"title": "5.3.1 Experiment Setup", "content": "The experiments were run on CloudLab Ricci et al. [2014], a testbed for cloud computing research and new applications. CloudLab cluster at Clemson University was built in partnership with Dell. Clemson machines were used to carry out the standalone training experiments. They were carried out on nodes with 16 cores per node (2 CPUs), 12x4 TB disk drives in each node, plus 8x1 TB disks in each node. The nodes are configured with 256 GB of memory and 73TB of RAM. The operating system installed across all of them was Ubuntu 18.04.\nWisconsin machines were also used to carry out the standalone training experiments described in section 6.2. The CloudLab cluster at the University of Wisconsin was built in partnership with Cisco, Seagate, and HP. The cluster has 523 servers with a total of 10,060 cores and 1,396 TB of storage, including SSDs on every node.\nThe nodes were chosen upon availability for the standalone node classification tasks."}, {"title": "5.4 VariantKG as a tool", "content": "The architecture of our tool, shown in Figure 7 has been designed to facilitate the extraction, processing, and analysis of variant-level genomic data using GNNs. The workflow integrates custom data inputs, feature selection, storage in a graph database, graph creation, and model training for inferencing genomic information. Given the rapidly expanding repository of genomic data, VariantKG offers a robust platform for researchers or users to extract relevant information and train Graph Neural Networks (GNNs) such as GraphSAGE for inferential purposes. Users have the flexibility to upload one or more VCF files, which may include associated CADD Scores for specific variants. Alternatively, users can select from pre-existing datasets that have been structured into a large-scale, comprehensive knowledge graph."}, {"title": "5.4.1 Scenario 1: Graph Enrichment", "content": "The first part of the VariantKG tool focuses on KG enrichment. A user can upload one or more VCF files containing variant-level information and CADD Scores in a TSV format as shown in Figures 8 or the user can select patients from the existing KG. If the user uploads VCF files, the information in the files is first run through the SnpEFF tool. SnpEFF adds new annotated information to each variant in the VCF file and also updates the headers of the file to reflect the annotations. These annotations are functional information that is added to the 'INFO' field using the 'ANN' tag. This ANN field, in turn, consists of several bits of information, such as allele, annotation using Sequence and Ontology terms, putative impact, gene name, gene ID, feature type, feature ID, transcript biotype, exon or intron rank, cDNA position, protein position and several types of distances to the feature.\nOnce the files have been annotated, we then utilize the vcf2rdf tool provided by SPARQLing-Genomics to efficiently translate the information on each variant into several triples in an RDF-suitable N3 format. These N3 triples were converted to NQ format for several reasons. Converting to NQ format allows for the use of a named graph, which provides a robust way to group triples into distinct sets, considerably enhancing data organization and management. Another advantage of using named graph is the support it provides to easily attach new metadata to specific data subsets. Additionally, named graphs also enable more precise and efficient SPARQL queries, improving data extraction quality and speed.\nThe triples are stored in Blazegraph, a high-performance graph database, which also stores the larger knowledge graph. To prepare the data from ML tasks, users can select patients using age groups as shown in Figure 9, or using the accession IDs, as shown in Figure 10, which are internally fetched using another SPARQL query that is only executed when the user wishes to prepare data for downstream ML tasks for efficiency.\nThe user can then select the features from a list consisting of the original headers from the VCF files and the annotated features by SnpEff. Once the user hits the \u2018Fetch from KG' button, another SPARQL query is then executed in the backend. If the user is selected from an age group, the SPARQL query consists of a filter that fetches all the accession IDs, which is passed as a list to the final query. If the user selects the accession IDs, it is passed as a list, similar to the previous query and the final query, shown below, fetches all the features selected for those patients or accession IDs."}, {"title": "5.4.2 Scenario 2: Graph Creation", "content": "Once the data has been fetched in the backend, the user is redirected to 'Graph Creation' where the user can select node features in the graph and select the class label from a subset of the features that are meaningful to be classified as a part of the classification task. The user is then given complete control of the graph construction where the user can select the edge type which can be using Gene Name (default) or fully connected. The weight of the edges can be 1 (default), the number of incoming edges to a node, or a user-defined value. The user can then select if the edges should be bidirectional and the train:val splits. The default is 80:10 with the remaining 10% calculated in the backend to reduce user clicks. Using this information, the graph is created for the node classification task. This is shown in Figure 11"}, {"title": "5.4.3 Scenario 3: Graph Machine Learning & Inference", "content": "The graph for the node classification task needs to be in a DGL-specific input format. The data is thus converted to a 'DGLGraph' object that consists of only integers that are mapped to the actual string values. The user can view the summary of the graph once created. This summary consists of all the selected features, the class label, the number of classes, graph properties, and the number of nodes and edges. The user is given a choice to download the DGL graph or continue with graph machine learning. For the ML task, the user can select between GraphSAGE and GCN models for the classification task and set the hyperparameters to train the model as shown in Figure 12. This includes the number of layers other than the input and output layers that constitute the primary architecture of the model, the number of hidden layers, the dropout rate, the learning rate, and the number of epochs for training.\nOnce the training begins, the user can view the training and validation loss plots, the validation accuracy plot, and the CPU memory usage as shown in Figure 13.\nIf the user wants to infer the ML task, they will be navigated to the 'Inference' tab shown in Figure 14, which displays the evaluation metrics and confusion matrix. The model is evaluated on Accuracy, macro-, and weighted- Precision, Recall, F1 score, and support, which is the number of samples for the given class."}, {"title": "6 Conclusion", "content": "This work shows that representing genomic data as knowledge graphs allows vast and diverse information to be integrated from various sources. Modeling entities as nodes and relationships as edges provides an ideal framework for integrating and organizing diverse information. We first described the data collection pipeline, followed by the usage of the SnpEff tool to obtain additional annotations and the SPARQLing Genomics tool to convert the annotations to an RDF format. An ontology to collate information gathered from different sources is presented. Using this ontology, we described how the knowledge graph is created. This knowledge graph contains RNA sequencing information at the variant level from COVID-19 patients from different regions such as lung, blood, etc. Lastly, we demonstrate the usage of the created knowledge graph for a node classification task using the Deep Graph Library through our tool, VariantKG. As part of this, we described various scenarios that can be performed by a user. As part of our future work, we aim to expand the knowledge graph and explore more avenues to use the same to aid researchers working in this domain."}]}