{"title": "Audio Captioning via Generative Pair-to-Pair Retrieval with Refined Knowledge Base", "authors": ["Choi Chang In", "Lim Sung Jun", "Rhee Won Jong"], "abstract": "Recent advances in audio understanding tasks leverage the reasoning capabilities of LLMs. However, adapting LLMs to learn audio concepts requires massive training data and substantial computational resources. To address these challenges, Retrieval-Augmented Generation (RAG) retrieves audio-text pairs from a knowledge base (KB) and augments them with query audio to generate accurate textual responses. In RAG, the relevance of the retrieved information plays a crucial role in effectively processing the input. In this paper, we analyze how different retrieval methods and knowledge bases impact the relevance of audio-text pairs and the performance of audio captioning with RAG. We propose generative pair-to-pair retrieval, which uses the generated caption as a text query to accurately find relevant audio-text pairs to the query audio, thereby improving the relevance and accuracy of retrieved information. Additionally, we refine the large-scale knowledge base to retain only audio-text pairs that align with the contextualized intents. Our approach achieves state-of-the-art results on benchmarks including AudioCaps, Clotho, and Auto-ACD, with detailed ablation studies validating the effectiveness of our retrieval and KB construction methods.", "sections": [{"title": "1 Introduction", "content": "Recent advances in captioning have been based on large language models (LLM). Increasing model size and training with more data has demonstrated human-level captioning performance. However, continually expanding model size and training data is a burden that increases training cost. Retraining the model with each new dataset significantly increases computational demands, escalating time and cost constraints.\nAn alternative approach involves retrieving information as additional input to the model instead of directly injecting new knowledge into the model parameters. This retrieval-augmented generation (RAG) approach has been adopted for its ability to reduce training costs in terms of time and data while improving performance. This method allows LLMs to generate responses using external knowledge bases, focusing on information from retrieved data rather than relying solely on memorized knowledge.\nIn image captioning, the RAG approach has been widely adopted to take advantage of external knowledge and improve performance (Ramos et al. 2023; Li et al. 2024; Ramos, Elliott, and Martins 2023). Although recent studies have begun to apply RAG to audio captioning (Ghosh et al. 2024; Kong et al. 2024), there has been limited exploration of how to effectively adapt this approach to the specific challenges of the audio captioning. One of the key challenges when applying RAG to audio captioning is the inherent ambiguity of audio signals (Martin Morato and Mesaros 2021; Lipping, Drossos, and Virtanen 2019; Drossos, Lipping, and Virtanen 2020), where the same audio can be interpreted in multiple ways depending on the context and the listener's perception. This ambiguity complicates the task of retrieving relevant information that can enhance caption generation, as the retrieved data may not always align with the contextualized intent of the audio query. For example, the audio caption \"a person is talking over an intercom as vehicles and a train pass by in the background\" could be described as \"background noise is present\" or \"faint talking is at an event with music in the background.\" Apparently, the same audio can be described differently as its intent may vary depending on the context and situation in which it is used. If the caption of a retrieved audio is described as \"background noise is present,\" it may not provide the necessary context for accurate generation. The fact that audio can be described differently can reduce the consistency of audio captioning. Moreover, this issue can lead to performance degradation in real-world scenarios where the contextualized intent differs drastically, such as distinguishing between a construction site and a hospital. Previous methods have typically relied solely on unimodal audio queries, using simple retrieval strategies and a basic knowledge base.\nHowever, these approaches, as mentioned earlier, do not take into account the diverse interpretations that can arise from the same audio input. Therefore, it is necessary to explore how the RAG approach can be adapted to address the ambiguity of audio data, ensuring that the retrieved information considers not only the audio modality but also the text modality for generating captions that accurately reflect the contextualized intent of the audio query.\nTo address these challenges, we propose a generative pair-to-pair retrieval method combined with a refined knowledge base for audio captioning. Unlike traditional unimodal"}, {"title": "2 Related works", "content": "Automated Audio Captioning: Audio captioning traditionally follows an encoder-decoder architecture, where an audio encoder and a language model are trained end-to-end to generate captions. Various audio encoders extract features from the spectrogram employing techniques such as frame-wise processing and attention mechanisms to maintain temporal coherence and capture relevant features over different time scales using CNN or Transformer architectures (Chen et al. 2020a; Mei et al. 2021). Recent advances in audio captioning have taken advantage of pretrained models such as HTS-AT (Chen et al. 2022) to encode audio features from spectrograms. Building upon these robust audio encoder architectures, methods such as CLAP (Kim et al. 2024; Wu et al. 2022; Huang et al. 2022) employ contrastive learning to map audio and language features into a unified embedding space. Additionally, AAC-Prefix (Kim, Kim, and Oh 2023) proposed an adapter that maps audio features as prefix tokens into the language model. Furthermore, ACT (Mei et al. 2021), WavCaps (Mei et al. 2023), CoNeTTE (Labb\u00e9, Pellegrini, and Pinquier 2023), and EnCLAP (Kim et al. 2024) have explored various encoder and decoder combinations, while other approaches, such as BART-Tags (Gontier, Serizel, and Cerisara 2021) and AutoCAP (Haji-Ali et al. 2024) use additional conditioning signals, such as predicted AudioSet tags and audio metadata to improve captioning speed and reduce overfitting.\nAudio Foundation Models: Recently, with the rapid advancement of LLMs, research has shifted towards leveraging pre-trained LLMs for audio tasks. In this approach, audio embeddings produced by the audio encoder are passed through a connector, transforming them into audio tokens that the LLM can process. The LLM auto-regressively generates text from the given instruction and query audio by understanding audio tokens. Training these audio language models involves instruction tuning on multi-tasks, which are computationally intensive but achieve advanced performance in audio captioning tasks. SALMONN (Tang et al. 2024) employs a dual encoder architecture, concatenating the outputs of the dual encoders using the Q-former of the sliding window level for effective cross-modal processing. Qwen-Audio (Chu et al. 2023) shows robust performance with a convolution audio encoder with extensive data on pre-training and instruction tuning. AudioPALM (Rubenstein et al. 2023) enhances performance by combining audio and speech understanding. Our model also adopts an audio language model architecture for training, focusing on effective RAG for audio captioning.\nRetrieval-augmented audio captioning: Retrieval-augmented audio captioning leverages pre-trained large-scale language models and retrieval to improve performance, addressing the challenge of limited training data. Despite its potential, there are relatively few methods that explore this approach. The existing work has primarily concentrated on optimizing how to fuse the retrieved information with lightweight training parameters, without thoroughly investigating the retrieval methods or the knowledge bases used. For example, RECAP (Ghosh et al. 2024) explored a lightweight retrieval-augmented model with a pre-trained audio encoder to improve captioning following approaches in SmallCaps (Ramos et al. 2023). Furthermore, Audio Flamingo (Kong et al. 2024) excels in audio understanding, few-shot learning, and multi-turn dialogue by leveraging in-context learning and retrieval.\nQuery Transformation Query transformation optimizes retrieval by rewriting or abstracting user queries. RRR (Ma et al. 2023) enhances queries by incorporating feedback from the generative model, making retrieval more effective. HyDE (Gao et al. 2022) generates hypothetical documents based on the original query and retrieves information by focusing on the similarity between these answers. Step-back Prompting (Zheng et al. 2023) abstracts the query into a high-level question, expanding the search space and improving the quality of retrieved results by combining both the original and step-back queries. Our approach leverages query transformation by combining the audio input with its generated caption in a multimodal pair-to-pair retrieval framework, demonstrating improved performance in audio captioning tasks."}, {"title": "3 Method", "content": "In this section, we propose an effective RAG-based approach for audio captioning. We describe the model architecture and training methodology, and introduce two key methods: generative pair-to-pair retrieval and a refined knowledge base, both of which utilize audio-text query to use pair-to-pair retrieval."}, {"title": "3.1 Model Architecture", "content": "The model architecture consists of three main components: a pre-trained audio encoder, a language model for text genera- tion, and a projection network to map audio features into the language model's embedding space. When an audio query is received, the Laion-CLAP (Wu et al. 2023) encoder extracts features and maps them into the language model embedding space via an MLP projection layer as a connector.\nThe audio encoder processes variable-length audio input, converts them into mel-spectrogram features with a 1024 window length, and projects the audio features to the language model as audio tokens. To reduce computational complexity, we decrease the number of audio tokens by applying average pooling with a stride, balancing important audio details and efficiency. For audio inputs longer than 30 seconds, a special length token is added before passing the input to the Vicuna (Chiang et al. 2023) language model, which then generates the caption. The model also retrieves relevant audio-text pairs to provide contextual information, improving the accuracy of the caption. Details on the optimal number of audio tokens are provided in the Appendix A."}, {"title": "3.2 Training Methods", "content": "We adopt two-stage training methods following recent approaches (Li et al. 2022, 2023; Liu et al. 2023). In the first stage, we focus on aligning the audio modality with the language model. This involves training audio connector to map audio features into the language model embedding space, enabling the language model to interpret and generate accurate captions.\nThe second stage involves RAFT (Retrieval Augmented Fine Tuning), we train IA3 (Liu et al. 2022) as a parameter-efficient fine-tuning (PEFT) method. We retrieve relevant audio-text pairs and train model using interleaved audio-text data. We structure the second stage of the training process into two phases, following a curriculum learning approach (Bengio et al. 2009; Hacohen and Weinshall 2019). In the first phase, we fine-tune the parameters using only the audio-captioning dataset, without incorporating the retrieved interleaved audio-text pairs. In the second phase, we fine-tune the parameters again, this time including the retrieved interleaved audio-text pairs, using the same dataset.\nWe train audio language model using audio captioning datasets. We use auto-regressive text generation objective on dataset containing audio-caption pairs. We also obtain audio-text pairs from the audio-caption datasets using dense retrieval based on audio encoder. We compute loss to predict each caption with query audio and retrieved audio. We vary a number of interleaved samples from 1 to K so that the model can understand and retrieve audio-caption pairs to predict the last query audio caption. $a_{<k} = \\{a_1, a_2,..., a_k\\}$ denotes the sequence of audio inputs up to the k-th audio sample including the audio query. $t_{<k} = \\{t_1, t_2,...,t_{k-1}\\}$ denotes the sequence before the last text sequence to be predicted. $N_k$ is the length of the text sequence $t_k$.\n$L = - \\sum_{k=1}^{K} \\sum_{i=1}^{N_k} log P(t_{k,i} | t_{k,<i}, a_{\\leq k},t_{<k})$"}, {"title": "3.3 Pair-to-Pair Retrieval", "content": "Effectively configuring the retrieval method is crucial for RAG. In multimodal retrieval tasks, information is retrieved in relevant pairs corresponding to input modality, such as audio-text pairs. To accurately process query, these candidate pairs should contain detailed and relevant descriptions that closely align with the query input.\nWe propose a pair-to-pair search method that uses a multimodal query, combining both an audio query and the corresponding text query, to find relevant pairs. This approach allows for the retrieval of pairs more accurately similar to the audio query by effectively utilizing both modalities. The corresponding text query contains contextualized intent related to the task, defining what should be extracted from the audio query. For example, in audio captioning, the text query describes key sound events in the audio.\nFirst, let $Enc_T$ be the text encoder and $Enc_A$ be the audio encoder. Given an audio query $A_q$ and its text query $T_q$, we aim to retrieve relevant audio-text pairs $(A_k, T_k)$ from the knowledge base.\n* $S_A$ is the similarity score between the audio query $A_q$ and the retrieved audio $A_k$.\n* $S_T$ is the similarity score between the text query $T_q$ and the retrieved caption $T_k$.\n* $W$ is a weighting parameter that controls the balance between the audio score and the text score.\n$S_A(A_q, A_k) = \\langle Enc_A(A_q), Enc_A(A_k) \\rangle$\n$S_T(T_q, T_k) = \\langle Enc_T(T_q), Enc_T(T_k) \\rangle$\n$S = W \\cdot S_A + (1 - W) \\cdot S_T$\nThis approach ensures that both audio and text information is effectively leveraged to enhance the relevance and accuracy of the retrieved pairs, leading to improved performance in audio captioning tasks.\nHowever, in audio captioning, where the input is audio and the text is the target of generation, utilizing the text query is not straightforward because only an audio is provided as the input. To address this problem, we propose a generative pair-to-pair approach that incorporates generated caption in the retrieval process, enabling the use of a multimodal query combining both audio and text. Additionally, we leverage pair-to-pair retrieval to construct a refined knowledge base by eliminating irrelevant candidates and extracting only those that are relevant."}, {"title": "3.4 Generative Pair-to-Pair Retrieval", "content": "To accurately retrieve relevant audio-text pairs for a given audio query, we propose Generative Pair-to-Pair retrieval for audio-text pairs considering both modalities. In this approach, the generated caption serves as an text query $T_q$, transforming the audio query into a text query which describes query audio content. For a given query audio $A_q$ and the generated caption $T_q$, we retrieve relevant pairs $(A_k, T_k)$ from the knowledge base using combined similarity scores of both modalities, as defined in Equation (4).\nWe hypothesize that generated caption can partially con- tain description of query audio. Our retrieval method aims to identify audio-text pairs with descriptions that closely align with the contextualized intent of the query audio. We formalize the generative query transformation process where an audio language model $f_\u03b8$ generates a text query $T_q$ from the given audio query $A_q$.\n$T_q = f_\u03b8(A_q)$\nOnce $T_q$ is generated, it is used in the pair-to-pair retrieval framework to compute the similarity score as follows:\n$S_T(T_q, T_k) = \\langle Enc_T(f_\u03b8(A_q)), Enc_T(T_k) \\rangle$"}, {"title": "3.5 Refined Knowledge Base", "content": "Our analysis reveals that simply increasing the size of the knowledge base does not necessarily lead to better performance. The key factor is the inclusion of relevant candidates that positively contribute to the generation process. Adding irrelevant candidates can actually degrade performance. Therefore, instead of expanding the knowledge base indiscriminately, it is crucial to include relevant and contextual information. To avoid confusion in generation and retrieve only helpful pairs, we propose a refined knowledge base designed to contain only consistent audio-captions pairs that contribute to accurate generation, effectively reducing noise and the overall size of the knowledge base. The process involves the following steps:\nEmbedding Generation: For each audio-caption pair $(A_i, T_i)$ in the knowledge base, we generate embeddings using pre-trained encoders $Enc_A(\\cdot)$ and $Enc_T(\\cdot)$:\n$z_{A_i} = Enc_A(A_i), z_{T_i} = Enc_T(T_i),$\nwhere $z_{A_i} \\in \\mathbb{R}^{d_A}$ and $z_{T_i} \\in \\mathbb{R}^{d_T}$.\nEmbedding Concatenation: The audio and text embeddings are then concatenated to form a single embedding vector:\n$z_i = [z_{A_i}; z_{T_i}] \\in \\mathbb{R}^{d},$\nwhere $d = d_A + d_T$, and $[\\; ; \\;]$ denotes the concatenation operation.\nPair-to-Pair Retrieval: For a given training pair, its corresponding embedding $z_j = [Enc_A(A_j); Enc_T(T_j)]$ is used as the query vector. The retrieval process finds the top-k closest pairs in the knowledge base based on cosine similarity following Equation 4:\n$\\langle (A_i, T_i) \\rangle = Sim \\left( z_j, z_i \\right)_{i=1}^{N},$\nwhere N is the total number of pairs in the knowledge base.\nFiltered Knowledge Base: The retrieved pairs are then used to construct the refined knowledge base M, ensuring that the audio-text pairs are aligned with the trainset. This filtering process enhances the relevance of the retrieval results.\nWe propose a pair-to-pair retrieval method that utilizes a multimodal query, considering both modalities, in contrast to traditional approaches that rely solely on unimodal queries. This method introduces generative pair-to-pair retrieval and a refined knowledge base. generative pair-to-pair retrieval aids in retrieving relevant pairs using generated captions as a text query, while the refined knowledge base extracts useful pairs to form a more accurate knowledge base. Both approaches effectively retrieve pairs that enhance audio captioning performance."}, {"title": "4 Experiments", "content": "4.1 Datasets and Metrics\nDatasets: We train our model on large-scale audio caption- ing datasets, including WavCaps (Mei et al. 2023), Audio- Caps (Kim et al. 2019), and Clotho (Drossos, Lipping, and Virtanen 2020), which cover a diverse range of environmen- tal sounds with corresponding textual descriptions. We con- sidered the duration of the audio, excluding audio over 40 seconds, and used 290k audio-text pairs in total. For the re- trieval KB, we additionally use the AutoACD (Sun et al. 2023) dataset, a large-scale audio captioning dataset that contains 1.9 million audio-text pairs created using an in- novative automatic audio caption generation pipeline. We blend AudioCaps and Clotho as the same amount when fine-tuned. Audio files that overlap with the AudioCaps and Clotho evaluation sets are excluded from WavCaps.\nRetrieval methods: For comparison, we consider previous retrieval methods that use only the audio query to retrieve relevant pairs, leveraging the unified embedding space of Laion-CLAP, which utilizes contrastive learning to match similarity between pairs.\nAudio-to-Audio: Unimodal retrieval which relies on the similarity score between the query audio $A_q$ and the retrieved audio $A_k$, computed as:\n$S(A_q, A_k) = \\langle Enc_A(A_q), Enc_A(A_k) \\rangle$\nAudio-to-Text: Cross-modal retrieval utilizes the audio query to find related textual descriptions by calculating:\n$S(A_q, T_k) = \\langle Enc_A(A_q), Enc_T(T_k) \\rangle$\nAudio-to-Mixture (Yasunaga et al. 2022): This method inte- grates unimodal and cross-modal retrieval within the CLAP unified embedding space, computing the similarity score by averaging the contributions from both modalities towards audio query:\n$S(A_q, A_k, T_k) = \\frac{\\langle Enc_A(A_q), Enc_A(A_k) \\rangle + \\langle Enc_A(A_q), Enc_T(T_k) \\rangle}{2}$"}, {"title": "4.2 Experiment Results", "content": "In this section, we quantitatively compare our method with other audio captioning models on in-domain datasets. We include models trained on both AudioCaps and Clotho datasets and evaluate their performance on each dataset. To ensure a comprehensive evaluation, we apply our proposed methods outlined in Tables 1 and 2. This includes the best- performing configurations across generative pair-to-pair retrieval and refined knowledge base settings. The results of each are presented in Table 1 and Table 2.\nThe evaluation of AudioCaps, shown in Table 1, demonstrates the effectiveness of our method. Our few-shot RAG results achieve state-of-the-art, indicating significant improvements over previous approaches. Our base model without RAG achieves competitive results in the AudioCaps dataset despite using fewer trainable parameters and a smaller data scale. Incorporating the refined huge knowledge base boosts performance, increasing the SPIDEr score to 0.473, highlighting the effectiveness of using high-quality, contextually relevant audio-text pairs. Finally, generative pair-to-pair retrieval with curriculum learning using best few-shot further enhances performance, achieving a SPIDEr score of 0.507 by leveraging a multimodal query that improves alignment between retrieved pairs and the query audio.\nThe evaluation of Clotho, presented in Table 2, indicates that our method achieves state-of-the-art results compared to the baselines. Our few-shot results achieve the highest scores, with a CIDEr of 0.490, SPICE of 0.137, and SPIDEr of 0.314. By incorporating our pair-to-pair retrieval method and refined knowledge bases, we achieve state-of-the-art results in both datasets, further enhancing our performance and demonstrating the robustness of our approach.\nWe evaluated the effect of retrieval performance on an out-of-domain benchmark using the AutoACD dataset. Table 2 compares several audio foundation models with state-of-the-art zero-shot methods. Our approach achieves the highest scores without retrieval. Incorporating few-shot learning with RAG, our model further improves significantly."}, {"title": "5 Impact Analysis", "content": "In this section, we analyze how similarity scores between audio queries and captions, across both modalities, affect the performance of audio captioning tasks. Using Figure 1 and Table 4, we evaluated how the similarity of the retrieved audio and text pairs influences the quality of generated captions. This analysis examines various retrieval strategies, including our proposed methods, to highlight their impact on captioning performance.\nIn Figure 1, we evaluate the performance of various retrieval methods in different size of knowledge bases. The audio-to-audio retrieval method achieves high audio similarity scores but struggles to capture the variety of details conveyed in the corresponding text. The audio-to-text cross-modal retrieval fails to capture similar audio, underscoring its focus on text over audio content. On the other hand, the audio-to-mixture retrieval method balances audio and text similarities by averaging their embeddings in CLAP's unified projection space. Our proposed pair-to-pair retrieval method uses an audio-text pair as a query and maintains higher text similarity than other retrieval methods while preserving audio similarity. In our ablation study, we found that the text similarity of retrieved audio-text pairs, which more closely aligns with the query audio captions, consistently provides stronger performance than audio similarity in retrieval tasks. This trend highlights the importance of text similarity in selecting audio-text pairs that accurately reflect the audio content, serving as a strong indicator of how well these pairs describe the similar audio.\nIn our evaluation of the AudioCaps and Clotho datasets in Table 4, we analyze how the similarity of retrieval pairs affects performance in captioning tasks. Using audio-to-audio retrieval with huge KB as the baseline which have highest audio similarity pairs, we observe that refining the knowledge base enhances performance by finding a better text description with similar audios. Our generative pair-to-pair retrieval, which identifies higher text similarity audio-text pairs with described audio content using query transformation, shows improved results on both datasets. Combining generative pair-to-pair retrieval with our refined KB yields the best overall performance in AudioCaps dataset. However, the Clotho dataset does not demonstrate this synergy, indicating that combining methods does not always lead to enhanced outcomes. When using both methods together to find audio-text pairs, if we can generate more accurate and detailed captions, combining these methods could lead to better results."}, {"title": "6 Ablation Studies", "content": "In this section, we present ablation studies to evaluate and refine the key methods of our retrieval-augmented audio captioning model. We conduct these ablations in the order of the following subsections, providing insights into how each method contributes to the audio captioning performance in generating accurate and contextually relevant captions."}, {"title": "6.1 Knowledge Base", "content": "We analyzed how to construct the knowledge base that contains the candidate pairs for the retrieval. According to (Gao et al. 2023), retrieval-augmented generation relies on external knowledge to enhance large language models, and both the type of retrieval source and the granularity of the retrieval affect the final generation result.\nIn Table 5, we analyze how refining the knowledge base impacts audio captioning performance by selecting candidate pairs closely aligned with high-quality training datasets. The results demonstrate that refining the knowledge base, even with a smaller version, consistently improves performance across different retrieval methods. For instance, in the AudioCaps dataset, the refined large KB outperforms the large KB in all retrieval scenarios. Interestingly, in the huge KB setting, performance decreased despite the increase in knowledge base size. This can be interpreted as follows: a larger knowledge base increases the number of candidate pairs available for retrieval, raising the possibility of retrieving relevant information for more accurate generation. However, adding noisy candidates can introduce irrelevant context, which may degrade performance. This experiment indicates that expanding the knowledge base size alone does not guarantee improved results."}, {"title": "6.2 Curriculum Learning", "content": "We also evaluated the impact of curriculum learning on RAFT training, comparing the performance of RAFT with and without this approach. Specifically, we used a few-shot training strategy with mixed data (McKinzie et al. 2024) as a solid baseline.\nTable 6 presents the SPIDEr scores for RAFT with and without curriculum learning on the AudioCaps and Clotho datasets. RAFT with curriculum learning consistently outperformed RAFT baseline in both zero-shot and best few-shot settings across both datasets."}, {"title": "6.3 Retreival Strategies", "content": "To effectively select candidate pairs that aid in generation, it is crucial to determine the appropriate retrieval method. Generally, retrieval is performed through either audio-to-audio, audio-to-text, or audio-to-mixed, all of which involve a single audio modality. In our experiments, we adapted the curriculum learning approach to improve performance, as the quality of generative pair-to-pair retrieval benefits significantly from more accurate generated captions.\nExamining Table 7 demonstrates that each audio and text similarity plays a critical role in RAG performance. In terms of audio similarity, a larger knowledge base more effectively retrieves audio-text pairs that closely align with the query audio, generally leading to improved overall performance. The results with a larger knowledge base outperform those with smaller ones. In particular, the text similarity of the retrieved pairs, which closely matches the query audio captions, generally provides stronger performance than audio similarity alone. The pair-to-pair retrieval method, which uses a generated caption as the text query, achieves the highest performance by balancing strong text similarity with good audio similarity, demonstrating its effectiveness on both the AudioCaps and Clotho datasets."}, {"title": "7 Conclusion", "content": "We introduced generative pair-to-pair retrieval and refined knowledge base, combining audio and text queries in retrieval. This approach improves audio captioning by capturing the content of query audio through enhanced text similarity. Evaluations on the AudioCaps, Clotho, and AutoACD datasets demonstrate performance, highlighting the effectiveness of proposed methods in generating accurate captions using audio-text pairs that are highly relevant to both the query audio and their corresponding captions."}]}