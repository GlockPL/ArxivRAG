{"title": "Hierarchical Repository-Level Code Summarization for Business Applications Using Local LLMs", "authors": ["Nilesh Dhulshette", "Sapan Shah", "Vinay Kulkarni"], "abstract": "In large-scale software development, understanding the functionality and intent behind complex codebases is critical for effective development and maintenance. While code summarization has been widely studied, existing methods primarily focus on smaller code units, such as functions, and struggle with larger code artifacts like files and packages. Additionally, current summarization models tend to emphasize low-level implementation details, often overlooking the domain and business context that are crucial for real-world applications. This paper proposes a two-step hierarchical approach for repository-level code summarization, tailored to business applications. First, smaller code units such as functions and variables are identified using syntax analysis and summarized with local LLMs. These summaries are then aggregated to generate higher-level file and package summaries. To ensure the summaries are grounded in business context, we design custom prompts that capture the intended purpose of code artifacts based on the domain and problem context of the business application. We evaluate our approach on a business support system (BSS) for the telecommunications domain, showing that syntax analysis-based hierarchical summarization improves coverage, while business-context grounding enhances the relevance of the generated summaries.", "sections": [{"title": "I. INTRODUCTION", "content": "In large scale software development projects, it is extremely crucial to have accurate source code comprehension capabilities for effectively developing and maintaining complex software systems. As software projects grow in size and complexity, understanding the functionality, structure, and intent behind the code becomes increasingly difficult. At the same time, the expectation of having well-documented and well-commented code is often unrealistic, especially with tight project deadlines. Even when comments are present, they may not be consistently updated with code revisions. It is reported that developers often spend more than 50% of their time in comprehending existing code [1]. Therefore, having an automatic source code summarization capabilities is a critical requirement in large-scale software projects. Code summarization refers to the process of generating concise, human-readable descriptions of various code components such as packages, files, classes, and functions, capturing their purpose and behavior within the business context. High-quality, automatically generated code summaries facilitate tasks such as code understanding, review, maintenance, debugging, and developer onboarding, ultimately leading to improved software development cycles.\nCode summarization has been extensively studied, with early research primarily focusing on rule-based or template-based methods [2], [3]. In recent years, deep learning approaches have gained significant traction [4], [5], framing code summarization as a machine translation or text summarization problem. These methods typically involve training sequence-to-sequence or transformer-based models. Lately, the accuracy of code summarization has improved significantly with the advent of large language model (LLM) based fine-tuning approaches. However, these models often rely on datasets such as CodeSearchNet [6] and CodeXGlue [7], which are primarily sourced from GitHub and contain a large proportion of system-level code (at method level). Fine-tuned models trained from such dataset often focus heavily on low-level implementation details and struggle to adequately capture the business context typically found in enterprise software applications. As a result, they do not transfer well to business applications where understanding the underlying intent is just as critical as the implementation details at method level.\nClosed-source, API-based LLMs, such as OpenAI's GPT [8], have significantly advanced code summarization, achieving remarkable improvements not only at the method level but also at the file and package levels. However, privacy concerns limit their adoption, as organizations are still reluctant to share sensitive proprietary source code. While cloud service providers like Azure offer access to these LLMs with data privacy guarantees, organizations still hesitate to adopt them. Local LLMs provide a solution here by allowing deployment on-premises, keeping data private. Despite this advantage, local LLMs still struggle with repository-level summarization, particularly for large files and packages. For example, in our experiments with Llama3.2 (128K context window) [9] on a Java file containing 124 functions, many functions were omitted from the file-level summary. While local LLMs ensure privacy and security, their accuracy remains limited for large-scale summarization tasks.\nAt our organization, we are developing a generative AI-based framework for accelerated delivery of software products in Brownfield setting, with a goal to reduce time and efforts to create bespoke product offerings, managing feature request, and so on. Automatic source code summarization is a critical module in our framework, providing backbone to"}, {"title": "II. RELATED WORK", "content": "Code summarization has a long history of research, with early works primarily focusing on pattern-based and template-based models [2], [3]. With the advent of deep learning, the field shifted toward neural machine translation models, such as sequence-to-sequence summarization for languages like C# [4] and Java [5]. This progression was soon followed by transformer-based models, including encoder-decoder architectures [11] and BERT-based approaches [12]. More recently, the emergence of code-specific LLMs, such as CodeLlama"}, {"title": "III. REPOSITORY-LEVEL CODE SUMMARIZATION", "content": "Our goal is to develop a repository-level code summarization system that captures both low-level code artifacts (such as variables and functions) and the intent of higher-level artifacts (such as files and packages). While local LLMs are effective for summarizing smaller, function-level code, our observations show that they still struggle with accurate and coherent repository-level summaries.\nTo address this issue, it is essential to break down repository-level artifacts into smaller, manageable units that local LLMs can process effectively. By generating cohesive summaries at the block level, we can then combine them to form a comprehensive understanding at the repository level. Figure 1 illustrates this approach. First, we decompose individual files in the repository using a language parser to extract meaningful file-level segments. Next, we apply custom summarizers, tailored to each segment type, to generate detailed summaries. These segment-level summaries are aggregated into file-level summaries, incorporating domain and business context, which are further combined into package-level summaries. All summarization steps in this process are implemented using LLMs in inference-only mode. The rest of this section describes the key components of our system.\nTo effectively segment a codebase, it is crucial to identify cohesive components that can be interpreted independently. Ideally, one would like each individual source file to be treated as a distinct component, allowing its summary to be easily combined into package and repository level summaries. However, this approach becomes problematic when the size of a single file grows significantly. In such cases, summarizing large source files may result in the omission of important details, such as individual function descriptions, or result in an oversimplified summary that neglects key portions of the original code. Therefore, it is essential to decompose large source files into smaller, coherent, and meaningful units to enable more detailed summarization.\nTo achieve this, we perform syntactic analysis on the source file using a Java parser [28] to generate its Abstract Syntax Tree (AST), which provides an intermediate representation of the code. The AST represents the corresponding source file as a hierarchical tree, with nodes corresponding to various language constructs, such as declarations, expressions, definitions, statements, etc. We then traverse the AST to identify nodes whose type matches one of the following categories: Function, Variable, Constructor, Enum, and Interface. For each such node, we consider the subtree rooted at that node as a logical unit and serialize it into a code segment. Additionally, we retain localization information, including the file name and the start and end line numbers of the segment within the file.\nThe individual segment types discussed in the previous section each play distinct roles in contributing to the overall understanding of a source file. For instance, variables generally serve as value holders, while static variables provide constant values with specific purposes. Similarly, functions typically implement specific features, while certain types of functions, such as constructors, are primarily used for initializing variables. The key to successful summarization lies in understanding the unique roles these segments play in constructing a comprehensive understanding of the source file. To achieve this, we design custom prompts for each segment type, focusing on their role and purpose within the code. The following outlines the key aspects considered for each segment type:\nFor functions, we consider six key aspects: name, input, output, workflow, side effects, and purpose. The function name is extracted along with its input arguments, whose meanings are inferred from surrounding context, such as the function name and argument type. Similarly, the output is inferred based on its type and usage. The workflow captures a comprehensive, line-by-line summary of all statements within the function. Side effects focus on global updates, state changes, and logging information. All of these aspects are then used to derive the function's primary purpose.\nFor constructors, our goal is to identify the key properties that define class creation, such as instance variables and initialization values, helping us understand the initial state of objects and their intended behavior.\nFor variables, we aim to capture their type, scope, and role in the code. Static variables are treated similarly, with a focus on their constant values, which may have a broader impact across the application.\nFor enums, we aim to identify the constant values they represent and the role of these constants in controlling application logic.\nFor interfaces, we examine their role in defining a contract for classes to implement with a focus on the intended purpose of methods.\nOnce the summaries for individual segments are generated, we first combine them to generate file-level summary. Similarly, the file-level summaries are then combined to generate package-level summaries.\nTo obtain a file-level summary, we first order its segments according to their positions using the localization information. We then combine them using a custom prompt that not only uses comprehensive segment-level summaries but also considers the domain and the problem context (as detailed in sec. III-D) of the business application to determine the file's purpose and role within the overall codebase, and its key functionality.\nWe combine the individual file-level summaries to obtain a package-level summary. Since understanding the purpose and role of each individual file provides the necessary context for generating the package summary, we use only the file summary and omit all the details of its constituent segment summaries. The package-level summaries are then similarly combined to obtain the repository-level summary.\nLLMs, trained on vast amounts of data from the web, capture a broad range of knowledge about real-world and abstract entities, their properties, and their relationships [29]. They have also been shown to contain domain-specific knowledge. For instance, knowledge about gene, protein, etc. in Biomedical domain. While LLMs can handle generic tasks out-of-the-box, their performance can be significantly improved when they are grounded in the specific domain and problem context of a business application. We have observed this across various domains, including enterprise digital twins, manufacturing, and chemical synthesis.\nIn the context of code summarization, grounding the LLM to the business domain and problem context is essential for generating accurate, meaningful summaries.\nProviding a succinct description of the domain helps the LLM develop a deeper understanding of the concepts relevant to the business application. This understanding allows the LLM to generate summaries that align more closely with the domain-specific context.\nSimilarly, offering detailed context about the specific problem being addressed enhances the LLM's understanding of the task at hand. This enables the LLM to uncover the intended purpose of various source code artifacts, improving the quality of the summaries."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "Our organization has developed an operations and business support system (OSS/BSS) product for the telecommunications domain. While we have evaluated our approach on the full product codebase, this paper presents our findings based on a smaller, publicly available GitHub repository [10] that mirrors the characteristics of our BSS module. This repository includes code for supporting various business functions such as sales and service management, customer management, and billing. The codebase is written using the Java Spring framework and consists of 122 Java class files, 20 interfaces, 762 functions, 704 variables (including member fields and statics), and 11 enums.\nWe experimented with instruct-tuned general and code-specific LLMs, all in inference-only mode. The following LLMs were used in our experiments:\nAn instruct-tuned variant of Llama-3 (8B) that performs well on both general and code-related tasks [9].\nAn instruct-tuned variant of Starcoder-2 (15B) [30], optimized for code-related tasks.\nA code-specific 22B LLM from Mistral [31], designed for code generation tasks.\nTo obtain ground-truth data for evaluation, we utilized GPT-4 [8] from OpenAI, known for its strong performance in tasks ranging from language generation to code understanding. We randomly selected 10 files from our repository, covering multiple packages, to design prompts for generating file-level summaries. The following outlines the process we followed:\nWe first extracted all relevant code segments such as functions, variables, etc., from the selected files using syntax analysis.\nWe then iteratively optimized segment-specific prompts, using techniques like in-context learning and chain-of-thought prompting, incorporating feedback from subject matter experts (SMEs).\nUsing the segment-level summaries, we generated file-level summaries through an additional round of prompt engineering.\nNext, we used the optimized prompts to generate summaries for all files in the repository. We then selected two packages and iteratively developed prompts for package-level summarization based on the files within the packages. Finally, the optimized package-level prompt was applied to generate summaries for all the remaining 34 packages, resulting in ground-truth data for repository-level summarization. It should be noted that, rather than relying solely on GPT-4 output, the above process was guided by detailed feedback and validation from SMEs, resulting in reliable ground-truth data.\nIn summary, using GPT-4, we obtained ground-truth summaries for the following code elements: functions, constructors, variables, static variables, enums, interfaces, classes (files), and packages."}, {"title": "V. RESULTS AND ANALYSIS", "content": "This section begins with prompts and results for function summarization, followed by results for other segment types. We designed the prompts for function summarization progressively, starting with simple instructions and gradually incorporating more detailed prompts that leverage chain-of-thought reasoning and in-context learning. The following describes these prompts:\nThis prompt provides a general instruction to summarize the code segment, with the expectation to obtain important aspects such as purpose, arguments and return types.\nThis prompt explicitly asks the LLM to generate structured output with clearly defined fields including purpose, inputs, outputs, workflow, side effects, and a final summary. By explicitly specifying these fields, the prompt guides the LLM in focusing on the most relevant aspects of the code, improving the accuracy and clarity of the output.\nThis version is an in-context learning variation of the structured prompt, where the LLM is provided with a single example function and its summary. This one-shot example helps the model understand the expected format and the level of detail required for each field.\nThe baseline generic prompt performs reasonably well across different LLMs. As expected, directing the LLMs to generate structured outputs using the chain-of-thought prompt enhances accuracy compared to the baseline. Additionally, incorporating in-context learning with a one-shot example leads to significant improvements. On average, the structured one-shot prompt outperforms the generic prompt by more than 13% in completeness, 5% in correctness and cohesiveness, and approximately 1% in conciseness. These results suggest that while LLMs tend to produce concise summaries, they may struggle with completeness, correctness, and cohesiveness. Both chain-of-thought prompting and in-context learning contribute substantially in improving summaries in these areas.\nSimilar to function summarization, we applied generic and structured prompts, incorporating chain-of-thought and in-context learning, to optimize summarization for other segment types, including constructors, variables, enums, and interfaces. For each segment, we tailored the prompts to highlight the specific aspects they represent and their role within the broader codebase. For example, static variables often store key state information or configuration values that influence the behavior of a class. In contrast, enums and their constants define fixed sets of values, helping to constrain the potential values of certain variables. Understanding constructor skeletons is crucial, as they reveal mandatory member variables and provide insights into class functionality.\nThe summarization results, as evaluated using the traditional metrics, are reported. The results seem promising across all segment types, with Llama-3 achieving the highest summarization quality. Constructors, which typically exhibit a limited number of variations, achieve particularly high scores. In contrast, variables and enums, due to their diverse initializations, exhibit greater variability, resulting in slightly lower scores. Interfaces, being collections of abstract methods, are not segmented further but instead processed directly at the file level. This approach likely explains their relatively lower scores. However, our manual inspection confirms that the summarization quality remains generally consistent and satisfactory across all segment types.\nTo generate file-level summaries, we first combine the segment-level summaries while considering the sequential order of the segments. We then generate the purpose of the given file, eliciting the role it plays in the overall repository and its key functionality. As mentioned earlier, LLMs, trained on vast amounts of data from the web, already carry a high-level understanding of various domains and perform well out-of-the-box on many NLP tasks, including code summarization. However, providing the right context, both in terms of the domain at hand and the problem context of the business application, helps LLMs in improving the generated summaries, especially for components that require a deeper understanding, such as the intent or purpose of the entire file.\nThe LLMs we used for segment-level summarization encountered limitations when applied to file-level summarization, mainly due to their limited context window. To address this, we focused on the recently proposed Llama-3.2 model, which supports a context length of 128K tokens, to generate the file-level summary. We experimented with two variants: Llama-3.2 where segment summaries were obtained using the Llama-3 model; and Llama-3.2# where segment summaries from the Starchat2 model were used. As shown, grounding the model with both domain and problem context significantly improved domain relevance (DS) by over 7%. Importantly, the conciseness and cohesiveness of the generated summaries remained consistent with the baseline, where no additional business context was provided. Moreover, grounding also led to improvements in the completeness of the summaries. These findings confirm that grounding LLMs in the specific business application context enhances their ability to generate better, more relevant file-level summaries. evaluates the generated summary for a sample Java file from our repository.\nPrompting LLMs directly with large source files often leads to missing information in the generated summaries. For example, all the LLMs we tested struggled to reliably summarize many functions in a Java file containing 124 functions. To test this quantitatively, we provided complete source files as input to the LLMs and evaluated the generated summaries for coverage, focusing on different segment types. For instance, we considered a function to be faithfully represented in the summary only if it included the purpose of that function (even partially). As we couldn't identify consistent patterns to automatically verify this, we manually inspected all the files in the repository. Our tests with Llama-3.2 showed that direct file-level summarization missed approximately 11% of the functions and 24% of the variables. A similar trend was observed with GPT-4, where 9% of functions and 11% of variables were omitted from the summaries. In contrast, our approach avoids this issue, as it is explicitly designed to generate summaries for all segments, ensuring that every relevant component is included in the file-level summaries."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "In this paper, we presented a two-step hierarchical approach for repository-level source code summarization, specifically tailored to business applications. Our approach first decomposes large code artifacts into smaller segments, enabling local LLMs to summarize them with high accuracy. These segment-level summaries are then aggregated to produce coherent file and package-level summaries. To enhance the relevance and quality of the summaries, we incorporate domain and problem descriptions to ground them in the business context. Our evaluation on a publicly available repository for a business support system in the telecommunications domain demonstrates the effectiveness of our approach, yielding concise, coherent, and domain-aware summaries that capture all key constructs in the repository.\nFor future work, we plan to incorporate agentic models with self-reflection capabilities to further enhance the summarization quality. We also intend to extend our approach to a"}]}