{"title": "Uni-AdaFocus: Spatial-temporal Dynamic Computation for Video Recognition", "authors": ["Yulin Wang", "Haoji Zhang", "Yang Yue", "Shiji Song", "Chao Deng", "Junlan Feng", "Gao Huang"], "abstract": "This paper presents a comprehensive exploration of the phenomenon of data redundancy in video understanding, with the aim to improve computational efficiency. Our investigation commences with an examination of spatial redundancy, which refers to the observation that the most informative region in each video frame usually corresponds to a small image patch, whose shape, size and location shift smoothly across frames. Motivated by this phenomenon, we formulate the patch localization problem as a dynamic decision task, and introduce a spatially adaptive video recognition approach, termed AdaFocus. In specific, a lightweight encoder is first employed to quickly process the full video sequence, whose features are then utilized by a policy network to identify the most task-relevant regions. Subsequently, the selected patches are inferred by a high-capacity deep network for the final prediction. The complete model can be trained conveniently in an end-to-end manner. During inference, once the informative patch sequence has been generated, the bulk of computation can be executed in parallel, rendering it efficient on modern GPU devices. Furthermore, we demonstrate that AdaFocus can be easily extended by further considering the temporal and sample-wise redundancies, i.e., allocating the majority of computation to the most task-relevant video frames, and minimizing the computation spent on relatively \"easier\" videos. Our resulting algorithm, Uni-AdaFocus, establishes a comprehensive framework that seamlessly integrates spatial, temporal, and sample-wise dynamic computation, while it preserves the merits of AdaFocus in terms of efficient end-to-end training and hardware friendliness. In addition, Uni-AdaFocus is general and flexible as it is compatible with off-the-shelf backbone models (e.g., TSM and X3D), which can be readily deployed as our feature extractor, yielding a significantly improved computational efficiency. Empirically, extensive experiments based on seven widely-used benchmark datasets (i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2, Jester, and Kinetics-400) and three real-world application scenarios (i.e., fine-grained diving action classification, Alzheimer's and Parkinson's diseases diagnosis with brain magnetic resonance images (MRI), and violence recognition for online videos) substantiate that Uni-AdaFocus is considerably more efficient than the competitive baselines. Code & pre-trained models are available at https://github.com/blackfeather-wang/AdaFocus, https://github.com/LeapLabTHU/AdaFocusV2, and https://github.com/LeapLabTHU/Uni-AdaFocus.", "sections": [{"title": "1 INTRODUCTION", "content": "The proliferation of online videos, exemplified by the platforms such as YouTube and TikTok, has necessitated the development of automated methods for identifying human actions, events, and other elements within them. This is crucial for facilitating the applications such as recommendation [1], [2], [3], surveillance [4], [5], and content-based searching [6]. In recent years, remarkable success in accurate video recognition has been achieved by leveraging deep networks [7], [8], [9], [10], [11], [12]. However, the noteworthy performance of these models usually comes at the price of high computational costs. In real-world scenarios, computation directly translates into power consumption, carbon emission and practical latency, which should be minimized under economic, environmental or safety considerations.\nTo address this issue, a number of recent works propose to reduce the inherent temporal redundancy in video recognition [13], [14], [15], [16], [17], [18], [19]. As illustrated in Figure 1 (b), it is efficient to concentrate on the most task-relevant video frames, and allocate the majority of computation to them rather than all frames. Nevertheless, another important source of redundant computation in image-based data, specifically spatial redundancy, has rarely been explored in the realm of efficient video recognition. In fact, it has been shown in 2D-image classification that deep networks (e.g., ConvNets or vision Transformers) are able to produce correct predictions by examining only a few discriminative regions instead of the entire images [20], [21], [22], [23], [24], [25]. By performing inference on these relatively small regions, one can dramatically reduce the computational cost of visual backbones (e.g., processing a 96x96 patch requires ~18% computation of inferring a 224x224 image).\nIn this paper, we are interested in whether this spatial redundancy can be effectively leveraged to facilitate efficient video recognition. We start by introducing a novel adaptive focus (AdaFocus) approach to dynamically localize and attend to the task-relevant regions of each frame. In specific, our method first takes a quick glance at each frame with a lightweight deep model to acquire cheap and coarse global information. Then we train a policy network on its basis to select the most valuable region for recognition. This procedure leverages the reinforcement learning algorithm due to the non-differentiability of localizing task-relevant regions. Finally, we activate a high-capacity and accurate local encoder to process only the selected regions. Since the proposed regions are usually small patches with a reduced size, considerable computational costs can be saved. An illustration of AdaFocus can be found in Figure 1 (c). Our method allocates computation unevenly across the spatial dimension of video frames according to the contributions to the recognition task, leading to significant improvements in efficiency with a preserved accuracy.\nOn top of the vanilla AdaFocus framework, we delve deep into the optimal design of efficient spatial dynamic computation algorithms, and further improve AdaFocus in several important aspects. Firstly, we simplify the training of AdaFocus by reformulating it as an end-to-end algorithm, eliminating the need for the complicated three-stage training procedure with reinforcement learning. This yields reduced training cost, improved test accuracy, and greater accessibility for practitioners. Secondly, we present a discussion on how to introduce appropriate supervision signals for learning to select task-relevant regions, and propose a deep-feature-based approach for training more effective region selection policies. Lastly, we propose a deformable patch mechanism that enables AdaFocus to adapt flexibly to the task-relevant regions in various scales, shapes, and locations.\nIt is worth noting that the basic formulation of AdaFocus does not account for the temporal-wise and sample-wise redundancies, meaning that the computation is uniformly allocated along the temporal dimension and across different videos. Therefore, our method is compatible with the ideas of temporal-adaptive and sample-adaptive dynamic inference. For instance, it can be extended by concentrating computational resources on the most informative video frames and by decreasing the computation spent on relatively \u201ceasier\u201d samples. In this paper, we demonstrate that these goals can be attained by introducing a dynamic frame sampling algorithm as well as a conditional-exit mechanism.\nIncorporating the aforementioned methodology innovations, we present unified AdaFocus (Uni-AdaFocus, see Figure 1 (d)), a holistic framework that seamlessly integrates spatial, temporal, and sample-wise dynamic computation. Importantly, it is compatible with a wide range of off-the-shelf backbone models (e.g., TSM [26] and X3D [27]), which can be conveniently deployed as the feature extractor in Uni-AdaFocus for improving their computational efficiency. Furthermore, the inference cost of Uni-AdaFocus can be adjusted online without additional training (by modifying the criterion for sample-conditional computation). This adaptability enables it to fully utilize fluctuating computational resources or achieve the desired level of performance flexibly with minimal power consumption, both of which are the practical demands of numerous real-world applications, such as search engines and mobile applications.\nEmpirically, the effectiveness of Uni-AdaFocus is validated based on seven widely-used benchmark datasets and three real-world application scenarios. Extensive experiments demonstrate that Uni-AdaFocus consistently outperforms the competitive baselines by significant margins, attaining a new state-of-the-art performance in terms of both theoretical computational efficiency and practical inference speed.\nThis paper extends previous conference papers that introduced the basic AdaFocus framework [28] and preliminarily discussed its end-to-end training [29]. Moreover, our deep-feature-based approach for training the patch selection policy (Section 4.1.1) is conceptually relevant to, and improved upon [30] (see Table 15 for a detailed comparison). We have improved these earlier works substantially in several important aspects, which are summarized in Appendix A."}, {"title": "2 RELATED WORKS", "content": "Video recognition. Convolutional networks (ConvNets) have made a noteworthy impact on the field of automatic video recognition, demonstrating exceptional accuracy on large-scale benchmarks [31], [32], [33], [34], [35]. The methods employed within this field can be broadly categorized into several distinct approaches. One such method entails the concurrent capture of spatial and temporal information through the use of 3D convolution, as demonstrated by the works such as C3D [11], I3D [10], ResNet3D [12], X3D [27], etc. An alternative technique involves the initial extraction of frame-wise features, followed by temporal-wise aggregation using specialized architectures. This approach can be seen in studies utilizing temporal averaging [36], recurrent networks [37], [38], [39], and temporal channel shift [26], [40], [41]. A third category of work employs two-stream architectures to model short-term and long-term temporal relationships, as seen in [7], [9], [42], [43]. More recently, driven by the success of vision Transformers (ViTs) [44], a considerable number of works focus on facilitating effective video understanding with self-attention-based models [45], [46], [47], [48]. Notwithstanding the accomplishments of the aforementioned studies works, the expensive computational cost of deep networks, particularly 3D-ConvNets, usually constrain their practical application. Recent research efforts have been made towards improving the efficiency of video recognition [27], [49], [50], [51], [52], [53].\nTemporal redundancy. A prominent strategy for facilitating efficient video recognition entails the minimization of the"}, {"title": "UNI-ADAFOCUS: SPATIAL-TEMPORAL DYNAMIC COMPUTATION FOR VIDEO RECOGNITION", "content": "temporal redundancy within videos [14], [15], [16], [17], [18], [19], [54], [55], [56], [57], [58], [59]. Since not all frames are equally important for a given task, the model should ideally allocate fewer computational resources towards less informative frames [23]. Several effective algorithms have been proposed along this direction. For example, LiteEval [13] adaptively selects an LSTM model with appropriate size at each time step in a recurrent recognition procedure. Adaptive resolution network (AR-Net) [18] processes different frames with adaptive resolutions to save unnecessary computation on less important frames. VideoIQ [57] processes video frames using different precision according to their relative importance. FrameExit [55] learns to conclude the inference process after seeing a few sufficiently informative frames. Compared to these approaches, the contributions of this paper lie in that 1) we develop the methodologies of reducing spatial redundancy (AdaFocus), that is, to concentrate major computation on the task-relevant regions of video frames; 2) we demonstrate that AdaFocus is compatible with the spirit of reducing temporal redundancy by proposing a dynamic frame sampling algorithm tailored for our method; 3) we integrate the spatial, temporal, and sample-wise dynamic computation into a Uni-AdaFocus framework, yielding state-of-the-art computational efficiency.\nIn particular, OCSampler [19] proposes a novel and effective framework that learns to select task-relevant frames with reinforcement learning. Our work is related to [19] in the basic paradigm of formulating frame selection as a sequential weighted sampling problem without replacement, where the distribution is dynamically parameterized conditioned on each video utilizing a policy network. However, we develop novel theoretical analyses, which directly consider the expected loss of this problem as an optimization objective, and reveal that it can be decomposed into a differentiable form solved by the Monte Carlo method, yielding an efficient end-to-end trainable algorithm (Section 4.2). Compared to [19], our method does not rely on reinforcement learning or multi-stage training, considerably reduces both the theoretical complexity and the practical training wall-time, yet significantly improves the performance (Table 16).\nSpatial-wise dynamic networks perform computation adaptively on top of different spatial locations of the inputs [23], [60]. The AdaFocus network studied in this paper can be classified into this category as well. Many of the spatially adaptive networks are designed from the lens of inference efficiency [23], [61], [62], [63], [64]. For example, recent investigations have revealed that 2D images can be efficiently processed via attending to the task-relevant or more information-rich image regions [22], [24], [65], [66]. In the realm of video understanding, the exploitation of spatial redundancy as a means to reduce computational cost remains a relatively unexplored area. It has been shown by the attention-based methods [67], [68] that the contributions of different frame regions to the recognition task are not equivalent. Some preliminary studies [28], [29] have begun to underscore the potential benefits of this approach.\nThe spatial transformer networks [60] are trained based on an interpolation-based mechanism, which is similar to the differentiable patch selection technique in AdaFocus. However, they focus on actively transforming the feature maps for learning spatially invariant representations, whereas our objective is to localize and attend to the task-relevant regions of the video frames for improving the computational efficiency. Moreover, we demonstrate that a straightforward implementation of this mechanism fails to yield competitive results in our problem. To address the optimization challenges, our algorithm necessitates the introduction of improved designs, as discussed in this paper."}, {"title": "3 ADAPTIVE FOCUS NETWORK (ADAFOCUS)", "content": "Different from most existing works that facilitate efficient video recognition by leveraging the temporal redundancy, we seek to save the computation spent on the task-irrelevant regions of video frames, and thus improve the efficiency by reducing the spatial redundancy. To attain this goal, we propose an AdaFocus framework to adaptively identify and attend to the most informative regions of each frame, such that the computational cost can be significantly reduced without sacrificing accuracy. In this section, we first introduce the basic formulation of AdaFocus and its network architecture (Section 3.1). Then we show that the straightforward optimization problem derived from this basic formulation can be solved by a reinforcement-learning-based three-stage algorithm (AdaFocusV1, Section 3.2). Built upon these discussions, we further establish the feasibility of reformulating the training of AdaFocus into an end-to-end algorithm, which consistently improves the accuracy with a simpler and more efficient training process (AdaFocusV2, Section 3.3)."}, {"title": "3.1 Network Architecture", "content": "Overview. We start by giving an overview of AdaFocus (Figure 2). Without loss of generality, we consider an online video recognition scenario, where a stream of frames come in sequentially while a prediction may be retrieved after processing any number of frames. At each time step, AdaFocus first takes a quick glance at the full frame with a lightweight deep network $f_G$, obtaining cheap and coarse global features. Then the features are fed into a policy network $\\pi$ to aggregate the information across frames and accordingly determine"}, {"title": "UNI-ADAFOCUS: SPATIAL-TEMPORAL DYNAMIC COMPUTATION FOR VIDEO RECOGNITION", "content": "the location of an image patch to be focused on, under the goal of maximizing its contribution to video recognition. A high-capacity local encoder $f_L$ is then adopted to process the selected patch for more accurate but computationally expensive representations. Finally, a classifier $f_C$ integrates the features of all previous frames to produce a prediction. In the following, we describe the four components of our method in details.\nGlobal encoder $f_G$ and local encoder $f_L$ are both backbone networks that extract deep features from the inputs, but with distinct aims. The former is designed to quickly catch a glimpse of each frame, providing necessary information for determining which region the local encoder $f_L$ should attend to. Therefore, a lightweight network is adopted for $f_G$. On the contrary, $f_L$ is leveraged to take full advantage of the selected image regions for learning discriminative representations, and hence we deploy large and accurate models. Since $f_L$ only needs to process a series of relatively small regions instead of the full images, this stage enjoys high efficiency as well. Importantly, the formulation of $f_G$ and $f_L$ is general and flexible, i.e., most state-of-the-art deep learning models can be conveniently deployed in AdaFocus to improve their computational efficiency for inference. Representative examples are given in Section 5.\nFormally, given video frames $\\{v_1, v_2, . . .\\}$ with size $H \\times W$, $f_G$ directly takes them as inputs and produces the coarse global feature maps $e^G_t$:\n$e^G_t = f_G(v_t), t = 1, 2, ...,$   (1)\nwhere t is the frame index. By contrast, $f_L$ processes $P \\times P$ ($P < H, W$) square image patches $\\{\\tilde{v}_1, \\tilde{v}_2, . . .\\}$, which are cropped from $\\{v_1, v_2, . . .\\}$ respectively, and we have\n$e^L_t = f_L(\\tilde{v}_t), t = 1, 2, . . .,$   (2)\nwhere $e^L_t$ denotes the fine local feature maps. Importantly, the patch $\\tilde{v}_t$ is localized to capture the most informative regions for the given task, and this procedure is fulfilled by the policy network $\\pi$, which is introduced in the following.\nPolicy network $\\pi$ specifies which region the local encoder $f_L$ should attend to for each frame, i.e., the locations of $\\{\\tilde{v}_1, \\tilde{v}_2, ...\\}$. This goal is attained by leveraging the coarse global features $e^G_t$ from the global encoder $f_G$. Note that the features of both previous and current frames can be used, and hence $\\pi$ should be designed as the architecture capable of encoding temporal information (e.g., via incorporating recurrent networks, 3D convolution or self-attention modules). The detailed formulations and training algorithms related to $\\pi$ will be further discussed in Sections 3.2 and 3.3.\nClassifier $f_C$ is a prediction network aiming to aggregate the information from all the frames that have been processed by the model, and output the current recognition result at each time step. To be specific, we perform global average pooling on the feature maps $e^G_t$, $e^L_t$ from the two aforementioned encoders to get feature vectors $\\bar{e}^G_t$, $\\bar{e}^L_t$, and concatenate them as the inputs of $f_C$, namely\n$p_t = f_C([\\bar{e}^G_1,\\bar{e}^L_1], \\ldots, [\\bar{e}^G_t,\\bar{e}^L_t])$,   (3)\nwhere $p_t$ refers to the softmax prediction at $t^{th}$ step. It is noteworthy that $e^G_t$ is leveraged for both localizing the informative patches and recognition, under the goal of facilitating efficient feature reuse. This design is natural since it has been observed that deep networks (e.g., ConvNets and Vision Transformers) excel at learning representations for both recognition and localization simultaneously [69], [70], [71]. Many existing methods also adopt similar reusing mechanisms [13], [15], [17], [18], [72]. In addition, the architecture of $f_C$ may have different choices, such as recurrent networks [73], [74], averaging the frame-wise predictions [18], [26], [41], and accumulated feature pooling [55]."}, {"title": "3.2 AdaFocusV1: Three-stage Reinforcement Learning", "content": "Patch localization as a sequential discrete decision task. As aforementioned, the policy network $\\pi$ localizes the task-relevant patches $\\{\\tilde{v}_1, v_2, . . .\\}$, which are cropped from video frames and processed by the local encoder $f_L$. However, the cropping operation is inherently non-differentiable. To address this issue, a straightforward approach is to formulate $\\pi$ as an agent that makes a series of discrete decisions, such that $\\pi$ can be trained with reinforcement learning.\nAs a basic assumption, we suppose that the location of the patch $\\tilde{v}_t$ is drawn from an action distribution parameterized by the outputs of $\\pi$:\n$\\tilde{v}_t \\sim \\pi(e^G_1,..., e^G_t)$.   (4)\nNote that we do not perform any pooling on the features maps $e^G_t$ since pooling typically corrupts the useful spatial information for localizing $\\tilde{v}_t$. In our implementation, we consider multiple candidates (e.g., 36 or 49) uniformly distributed across the images, and establish a categorical distribution on them. At test time, we simply adopt the candidate with maximum probability as $\\tilde{v}_t$ for a deterministic inference procedure. An illustration is shown in Figure 3.\nThree-stage training. Since the formulation above includes both continuous (i.e., video recognition) and discrete (i.e., patch localization) optimization, the standard end-to-end training paradigm cannot be directly applied. Therefore, we introduce a three-stage training algorithm to solve the continuous and discrete optimization problems alternatively, which can be found in Appendix B, due to spatial limitations."}, {"title": "3.3 AdaFocusV2: Differentiable End-to-end Training", "content": "Limitations of AdaFocusV1. The underlying logic behind the aforementioned three-stage training is straightforward."}, {"title": "UNI-ADAFOCUS: SPATIAL-TEMPORAL DYNAMIC COMPUTATION FOR VIDEO RECOGNITION", "content": "However, this procedure is unfriendly for practitioners. First, effectively deploying the reinforcement learning algorithm is nontrivial. It requires considerable efforts for properly designing the key components (e.g., the action space and the reward function), and implementing specialized optimization techniques (e.g., deep Q-Network [75] or proximal policy optimization [76]). Second, the three-stage alternative algorithm is an indirect formulation for optimizing the recognition objective, which tends to be time-consuming, and may result in sub-optimal solutions. Third, the performance of AdaFocusV1 largely depends on a number of implementation configurations (e.g., performing pre-training, freezing some components in different stages, and stage-wise hyper-parameter searching) that need to be carefully tuned on a per-dataset or per-backbone basis.\nIn the following, we present an end-to-end trainable formulation for AdaFocus to address the issue of inefficient training. The proposed network, AdaFocusV2, can be conveniently implemented to achieve consistently better performance than AdaFocusV1 with reduced training cost. A comparison of AdaFocusV1 and V2 is given in Table 1."}, {"title": "3.3.1 Interpolation-based Patch Selection", "content": "To enable end-to-end training, we propose a differentiable solution to obtain $\\tilde{v}_t$. Suppose that the size of the original frame $v_t$ and the patch $\\tilde{v}_t$ is $H \\times W$ and $P \\times P$ ($P < H,W$), respectively. We assume that $\\pi$ outputs the continuous centre coordinates $(x, y)$ of $\\tilde{v}_t$, namely\n$(x, y) = \\pi(e^G_1,..., e^G_t),  (\\tilde{x}, \\tilde{y}) \\in [\\frac{P}{2}, H - \\frac{P}{2}] \\times [\\frac{P}{2}, W - \\frac{P}{2}]$.   (5)\nNotably, we refer to the coordinates of the top-left corner of the frame as (0,0), and Eq. (5) ensures that it will never go outside of $v_t$. Our aim is to calculate the values of all pixels in $\\tilde{v}_t$, while allowing the gradients to be back-propagated through $(x, y)$.\nFeed-forward. We first introduce the feed-forward process of our method. Formally, the coordinates of a pixel in the patch $\\tilde{v}_t$ can be expressed as the addition of $(x, y)$ and a fixed offset:\n$(x_{ij}, y_{ij}) = (x, y) + o_{ij}, o_{ij} \\in \\{-\\frac{P}{2}, -\\frac{P}{2} + 1,...., \\frac{P}{2} - 1, \\frac{P}{2}\\}$.   (6)\nHerein, $(x_{ij}, y_{ij})$ denotes the horizontal and vertical coordinates in the original frame $v_t$ corresponding to the pixel in the $i^{th}$ row and $j^{th}$ column of $\\tilde{v}_t$, while $o_{ij}$ represents the vector from the patch centre $(x, y)$ to $(x_{ij}, y_{ij})$. Given a fixed patch size, $o_{ij}$ is a constant conditioned only on $i, j$, regardless of $t$ or the inputs of $\\pi$.\nSince the values of $(x, y)$ are continuous, there does not exist a pixel of $v_t$ exactly located at $(x_{ij}, y_{ij})$ to directly get the pixel value. Alternatively, as illustrated in Figure 4, we can always find that the location $(x, y)$ is surrounded by four adjacent pixels of $v_t$, forming a grid. The coordinates are $([x_{ij}], [y_{ij}]), ([x_{ij}]+1, [y_{ij}]), ([x_{ij}], [y_{ij}]+1)$ and $([x_{ij}]+1, [y_{ij}]+1)$, respectively, where $[\\cdot]$ denotes the rounding-down operation. By assuming that the corresponding pixel values of these four pixels are $(m_{ij})_{00}, (m_{ij})_{01}, (m_{ij})_{10},$ and $(m_{ij})_{11}$, the pixel value at $(x_{ij}, y_{ij})$ (referred to as $m_{ij}$) can be obtained via interpolation algorithms. For example, we may simply adopt the differentiable bilinear interpolation:\n$m_{ij} = (m_{ij})_{00}([x_{ij}]-x_{ij}+1)([y_{ij}] - y_{ij}+1)  + (m_{ij})_{01}(x_{ij}-[x_{ij}]) ([y_{ij}]-y_{ij}+1)  + (m_{ij})_{10}([x_{ij}]-x_{ij}+1)(y_{ij} - [y_{ij}])  + (m_{ij})_{11}(x_{ij}-[x_{ij}])(y_{ij} - [y_{ij}])$.   (7)\nConsequently, we can obtain the image patch $\\tilde{v}_t$ by traversing all possible i, j with Eq. (7).\nBack-propagation. Give the training loss L, it is easy to compute the gradient $\\frac{\\partial \\mathcal{L}}{\\partial m_{ij}}$ with standard back-propagation. Then, following on the chain rule, we have\n$\\frac{\\partial \\mathcal{L}}{\\partial x} = \\sum_{ij} \\frac{\\partial m_{ij}}{\\partial x} \\frac{\\partial \\mathcal{L}}{\\partial m_{ij}},  \\frac{\\partial \\mathcal{L}}{\\partial y} = \\sum_{ij} \\frac{\\partial m_{ij}}{\\partial y} \\frac{\\partial \\mathcal{L}}{\\partial m_{ij}}$.   (8)\nCombining Eq. (6) and Eq. (8), we can further derive\n$\\frac{\\partial \\mathcal{L}}{\\partial x} = \\sum_{ij} \\frac{\\partial m_{ij}}{\\partial x_{ij}} \\frac{\\partial x_{ij}}{\\partial x} \\frac{\\partial \\mathcal{L}}{\\partial m_{ij}},  \\frac{\\partial \\mathcal{L}}{\\partial y} = \\sum_{ij} \\frac{\\partial m_{ij}}{\\partial y_{ij}} \\frac{\\partial y_{ij}}{\\partial y} \\frac{\\partial \\mathcal{L}}{\\partial m_{ij}}$.   (9)\nEq. (9) can be solved by leveraging Eq. (7), such that we can obtain the gradients $\\frac{\\partial \\mathcal{L}}{\\partial x}$ and $\\frac{\\partial \\mathcal{L}}{\\partial y}$. Given that $x$ and $y$ are the outputs of the policy network $\\pi$, the regular back-propagation process is able to proceed."}, {"title": "3.3.2 Training Techniques", "content": "Naive implementation. Thus far, we have enabled the gradients to be back-propagated throughout the whole AdaFocus network for updating all trainable parameters"}, {"title": "UNI-ADAFOCUS: SPATIAL-TEMPORAL DYNAMIC COMPUTATION FOR VIDEO RECOGNITION", "content": "simultaneously. Consequently, end-to-end training has been feasible. For example, one can minimize the frame-wise cross-entropy loss $L_{CE}(\\cdot)$ in AdaFocusV1:\n$\\mathop{\\mathrm{minimize}}\\_{f_G, f_L, f_C, \\pi} L = \\mathbb{E}_{\\{v_1,v_2,...\\}} \\sum_{t=1}^T [L_{CE}(p_t, y)],$   (10)\nwhere $T$ and $y$ denote the length and the label of the video $\\{v_1, v_2, . . .\\}$, and $p_t$ is the softmax prediction at $t^{th}$ frame.\nHowever, importantly, such a straightforward implementation leads to the severely degraded performance (see Table 6 for experimental evidence). We attribute this issue to the absence of some appealing optimization properties introduced by the three-stage training procedure, namely the lack of supervision, input diversity and training stability. To solve these problems, we develop simple but effective training techniques, with which end-to-end training can significantly outperform the three-stage counterpart. These techniques do not introduce additional tunable hyper-parameters, while achieving consistent improvements across varying datasets, backbone architectures, and model configurations.\nLack of supervision: auxiliary supervision. The effectiveness of three-stage training is largely ensured by a proper initialization. For example, AdaFocusV1 pre-trains the two encoders (i.e., $f_G$, $f_L$) separately using a direct frame-wise recognition loss (by simply appending a fully-connected layer) [28]. However, when solving problem (10), we do not introduce such pre-training mechanisms, which hurts the overall training efficiency of our method. In other words, $f_G$ and $f_L$ are trained without specialized initialization, while they are only indirectly supervised by the gradients from the classifier $f_C$. To this end, we find that explicitly introducing auxiliary supervision on $f_G$ and $f_L$ effectively facilitates the efficient end-to-end training of AdaFocus. In specific, we attach two linear classifiers, $FC_G(\\cdot)$ and $FC_L(\\cdot)$, to the outputs of $f_G$ and $f_L$, and replace the loss function $L$ in (10) by $L'$:\n$L' = \\mathbb{E}_{\\{v_1,v_2,...\\}} \\mathbb{E}_{\\{P_1, P_2, ...\\}} \\sum_{t=1}^T [L_{CE}(p_t, y)  + L_{CE} (SoftMax(FC_G(e_t^G)), y)  + L_{CE}(SoftMax(FC_L(\\bar{e}^L_t)), y)]$,   (11)\nwhere $e_t^G$ and $\\bar{e}^L_t$ are the feature vectors after performing global average pooling on the feature maps $e_t^G$ and $e^L_t$ output by $f_G$ and $f_L$. Intuitively, minimizing $L'$ enforces the two encoders to learn linearized deep representations, which has been widely verified as an efficient approach for training deep networks [71], [77], [78]. This paradigm benefits the learning of $f_C$ as well, since its inputs are explicitly regularized to be linearly separable.\nLack of input diversity: diversity augmentation. In the stage I for training AdaFocusV1, image patches are randomly cropped, yielding highly diversified inputs for learning well-generalized local encoder $f_L$. However, the patch selection process presented in Section 3.3.1 is deterministic. In Eq. (11), given a video frame, the local encoder $f_L$ only has access to the patch specified by the policy network $\\pi$. This procedure leads to the limited diversity of training data for the inputs of $f_L$. Empirically, we observe that it results in the inferior performance of $f_L$. We address this issue by proposing a straightforward diversity augmentation approach. For each video, we first compute $L'$ by activating $\\pi$ as aforementioned. Then we infer $f_L$ and the classifier $f_C$ for a second time using randomly cropped patches, obtaining an additional loss $L_{Crandom}$, which follows Eq. (11) as well. Our final optimization objective combines $L'$ and $L_{Crandom}$:\n$\\mathop{\\mathrm{minimize}}\\_{f_G, f_L, f_C, \\pi} L = \\frac{1}{2}(L' + L_{Crandom})$.   (12)\nLack of training stability: stop-gradient. In AdaFocusV1, the policy network $\\pi$ is learned on top of the fixed and completely trained global encoder $f_G$. When it comes to end-to-end training, $\\pi$ and $f_G$ are simultaneously updated. In this case, we observe that the gradients back-propagated from $\\pi$ interfere with the learning of $f_G$, leading to an unstable training process with slow convergence speed. We find that this problem can be solved by simply stopping the gradients before the inputs of $\\pi$. In other words, we propose to train $f_G$ using the pure classification objective without any effect from $\\pi$, as done in AdaFocusV1. This design is rational since previous works have revealed that the representations extracted by deep recognition networks can naturally be leveraged for localizing task-relevant regions [69], [70], [71]."}, {"title": "4 UNIFIED ADAFOCUS (UNI-ADAFOCUS)", "content": "In this section", "4.1)": 1}]}