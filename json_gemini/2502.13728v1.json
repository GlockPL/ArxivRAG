{"title": "Secure Federated Data Distillation", "authors": ["Marco Arazzia", "Mert Cihangiroglu", "Serena Nicolazzob*", "Antonino Noceraa"], "abstract": "Dataset Distillation (DD) is a powerful technique for reducing large datasets into compact, representative synthetic datasets, accelerating Machine Learning training. However, traditional DD methods operate in a centralized manner, which poses significant privacy threats and reduces its applicability. To mitigate these risks, we propose a Secure Federated Data Distillation (SFDD) framework to decentralize the distillation process while preserving privacy. Unlike existing Federated Distillation techniques that focus on training global models with distilled knowledge, our approach aims to produce a distilled dataset without exposing local contributions. We leverage the gradient-matching-based distillation method, adapting it for a distributed setting where clients contribute to the distillation process without sharing raw data. The central aggregator iteratively refines a synthetic dataset by integrating client-side updates while ensuring data confidentiality. To make our approach resilient to inference attacks perpetrated by the server that could exploit gradient updates to reconstruct private data, we create an optimized Local Differential Privacy approach, called LDPO-RLD (Label Differential Privacy Obfuscation via Randomized Linear Dispersion). Furthermore, we assess the framework's resilience against malicious clients executing backdoor attacks (such as Doorping) and demonstrate robustness under the assumption of a sufficient number of participating clients. Our experimental results demonstrate the effectiveness of SFDD and that the proposed defense concretely mitigates the identified vulnerabilities, with minimal impact on the performance of the distilled dataset. By addressing the interplay between privacy and federation in dataset distillation, this work advances the field of privacy-preserving Machine Learning making our SFDD framework a viable solution for sensitive data-sharing applications.", "sections": [{"title": "1. Introduction", "content": "Dataset Distillation (DD, hereafter) is defined as a set of approaches designed to generate compact yet highly informa-tive data summaries to capture the essential knowledge of a given dataset. These distilled representations are optimized to function as efficient substitutes for the original dataset, enabling accurate and resource-efficient applications such as model train-ing, inference, and architecture search [1]. DD has attracted much attention from the deep learning community because it addresses the problem of handling unlimited data growth with limited computing power.\nTypically, DD methods operate within a centralized and static framework, where the entire dataset is accessible at a sin-gle location [1, 2, 3]. In particular, a straightforward approach to constructing a synthetic dataset implies that each data owner shares its data with a central server so DD can happen. Despite the numerous benefits, concentrating information in a single point of aggregation may lead to privacy leakages. Indeed, the central server may be honest but curious (or even malicious) and might take advantage of all the shared sensitive informa-tion. One specific application domain, in which this aspect may represent a critical issue is the sharing of medical datasets to establish the cross-hospital flow of medical information and im-proving the quality of medical services (for instance to construct high-accuracy computer-aided diagnosis systems) [4, 5]. In this context, the different hospitals should share the medical informa-tion of all their patients with an external entity that is responsible for distilling the data before starting the specific analysis. In most cases, patients are reluctant to share such highly sensitive information, making DD approaches impractical [6].\nBorrowing some ideas from the new paradigm of Federated Learning (FD, hereafter), which distributes the learning process across multiple entities to enhance privacy, our work proposes a novel framework for Secure and Federated Data Distillation (SFDD). This approach seeks to enable efficient DD while pre-serving data privacy by decentralizing the distillation process across multiple participants. This ensures that raw data remains local while only distilled knowledge is shared. Observe that our proposal takes a different perspective from the existing Feder-ated Distillation (FD) schemes [7, 2, 8]. With the objective of improving FL performance and privacy guarantees, FD distills knowledge from multiple local models independently and trans-fers only compact, distilled information to the server that trains a global model using this distilled data. By contrast, the aim of our approach is to collaboratively distill data to produce a common distilled dataset without sharing local information. To do so, we start considering the method proposed by Zhao et al. [3] for learning a synthetic set such that a deep network trained on it would preserve similar performance to that obtained when trained on the original large dataset. The synthetic data can later be used to train a network from scratch in a small fraction of the original computational load. We adopt this method and we make it distributed among different data owners (or clients). Firstly, a central server (or aggregator), that is in charge of aggregat-ing the different contributions, produces and shares a random synthetic set of data. Then, the clients create local distillation contributions and at each step, they optimize the synthetic set of data and return to the server an enhanced version for aggregation till convergence is obtained.\nHowever, in the pure FL context, some work [9] have demon-strated that it is possible to obtain private training data from publicly shared gradients. Hence an honest but curious server may take advantage of the obtained updates sent by the clients to reconstruct the original batch of data and leak the privacy of data owners. For this reason, we first assess the vulnerability of our approach to this threat. We then enhance its security by implementing an improved Local Differential Privacy (LocalDP) strategy, called LDPO-RLD (LabelDP Obfuscation via Ran-domized Linear Dispersion). Adding this defense to our SFDD framework allows the clients to obfuscate the point-to-point cor-relation between distilled images and real ones. Experiments demonstrate that including LDPO-RLD in SFDD is not only an effective defense against deep leakage attacks but also outper-forms the standard LocalDP in distillation performance.\nAdditionally, we test our solution also in the presence of malicious clients. Indeed, a recent work introduced a new back-door attack method, called Doorping, which attacks during the dataset distillation process rather than after the model training [10]. Our experimental campaign demonstrates that under the assumption of a sufficient number of clients, our framework is robust also to this new type of attack.\nThe results obtained during our experimental campaign demon-strate that our Federated Data Distillation approach is secure against known threats.\nIn summary, the key contributions of our framework are the following.\n\u2022 We propose a strategy that allows diverse data owners to participate in a global and fully distributed Data Distilla-tion process without sharing local data. DD is computed by merging all the contributions of the different clients, thus no raw data or detailed model parameters are ex-changed, and privacy is preserved.\n\u2022 Our framework is also secure against inference attacks. In fact, honest-but-curious servers cannot infer sensitive information about clients data by exploiting the gradient and update dynamics exchanged during the distillation process thanks to the presence of our LabelDP Obfus-cation via Randomized Linear Dispersion (LDPO-RLD) defense strategy.\n\u2022 We also demonstrate that, under the assumption of a suffi-cient number of clients, our framework is robust to client-side attacks, such as the Doorping attack."}, {"title": "2. Related Work", "content": "Dataset Distillation (DD, hereafter) [1] has recently emerged as a novel paradigm to synthesize a significantly smaller dataset from a large dataset, aiming to maintain the same training accu-racy performance as if it was trained on the original large dataset. In this section, we will describe the proposals that combine FL with Distillation. The work presented in [11, 12, 13, 14, 15] leverage Knowledge Distillation (KD, for short) to transfer knowledge from local client models to a centralized FL server model to improve FL performance. In particular, [11] describes FedMD, a new FL framework that allows participants to inde-pendently design their models and maintain them private due to privacy and intellectual property concerns. To collaborate with each client, owning a private dataset and a uniquely designed model, FedMD relies on KD to translate its learned knowledge into a standard format. The authors of [12] try to solve the classical FL issue of data heterogeneity (i.e., data from the real world are usually not independent and identically distributed) by proposing a data-free Knowledge Distillation approach for FL. This framework extracts the knowledge from users without depending on any external data through KD and then it regulates local model updating using this extracted knowledge. Instead, the proposed framework called FD [13] utilizes distillation to reduce FL communication costs. It synchronizes logits per label which are accumulated during the local training. The averaged logits per label (over local steps and clients) are exploited as a distillation regularizer for the next round's local training. More-over, Lin et al. [14] propose ensemble distillation for model fusion with the aim of training the central classifier through unlabeled data on the outputs of the models from the clients. Finally, Afonin and Karimireddy [15] describe a theoretical framework, called Federated Kernel ridge regression, which is a model-agnostic FL scheme allowing each agent to train their model of choice on the combined dataset. All the above works propose KD-based Federated Learning protocols with a purpose that is quite different from ours. Indeed, firstly, they deal with Knowledge Distillation instead of Data Distillation, secondly, we aim to design a new scheme to distribute DD on FL, but we do not aim to improve the FL protocol through Distillation.\nOnly a little research has explored Dataset Distillation ap-proaches in FL. For instance, the strategies proposed in [7, 8, 2] try to reduce the communication cost of FL while achieving comparable performance by exploiting DD. In these protocols, instead of sharing model updates, the clients distill the local datasets independently in just one round, and then they send the synthetic data (e.g. images or sentences) to the central server that aggregates those decentralized distilled datasets. In those works, similarly to ours, DD is used instead of KD. In particular, Zhou et al. [7] propose a method combining DD and distributed one-shot learning. For every local update step, each synthetic data successively updates the network for one gradient descent step. Thus, synthetic data is closely related to one specific net-work weight, and the eavesdropper cannot reproduce the result with only leaked synthetic data.\nAs already said we employ a different perspective. Indeed in the above-cited approaches, DD has been applied to FL to develop a privacy-preserving distributed model training scheme such that multiple clients collaboratively learn a model without sharing their private data. Instead of transmitting model updates as the standard way of FL, which may cause demanding com-munication costs they transmit the locally generated synthetic datasets proved for privacy protection and essence information preservation. Instead, we aim to improve the Dataset Distillation framework by federating it. To the best of our knowledge, our proposed scheme is the first of its kind."}, {"title": "3. Background", "content": "In this section, we illustrate the main concepts that can be useful for a clear understanding of our approach. In particular, we focus on the description of Federated Learning (FL) and Knowledge Distillation (KD) mechanisms.\nFederated Learning is designed to train an ML model in a decentralized manner across different devices holding local data samples. Keeping local data confidential without exchanging them with other participants or a central server allows privacy preservation; whereas sending only model updates reduces com-munication overhead and network traffic.\nAs visible in Figure 1, the participants of this protocol are mainly of two types:\n\u2022 the worker nodes, also called clients, that are C devices executing local training with their private data;\n\u2022 an aggregator node, or central server, which is in charge of the coordination of the whole FL approach and aggre-gates the local updates.\nHence, the main goal of FL is to train a Global Model (GM), say w, by uploading the weights of Local Models (LMs) {w\u00b9\u013ci \u2208 C} to the central server. Equation 1 shows the loss function to be optimized:\n$\\min_W l(w) = \\sum_{i=1}^n \\frac{s_i}{n}L_i(w^i)$  (1)\nwhere $L_i(w^i) = \\sum_{j\\in I_i} l_j(w^i, x_i)$ is the loss function, si is the local data size of the i-th worker, and $I_i$ identifies the set of data indices with |I| = si, and xj is a data point.\nThe basic FL workflow can be divided into three main phases [16]. During the first stage, called Model initialization, the server (i) initializes the necessary parameters for the GM w; and (ii) select the workers for the FL process. The second phase con-sists of the LMs training and uploading. The clients download the current GM and perform local training on their private data during this stage. After that, each client computes the model parameter updates and sends them to the server. The regional training involves more than one iteration of back-propagation, gradient descent, or other optimization methods to improve the LM's performance. In particular, for each iteration, the different clients update the GM with their datasets: $w \\leftarrow w - \\eta \\nabla_{L(w,b)}w$, where \u03b7 specifies the learning rate and b is the local batch. Fi-nally, the GM aggregation and update phase is performed. In this step, the server collects and aggregates the model parameter updates from all the workers, {w\u00b9li \u2208 C}. The aggregator can employ various methods like averaging, weighted averaging, or secure multi-party computation (SMC) to incorporate the received updates from each client.\nAs visible in Figure 2, FL can assume the following three configurations according to the different data partition strategies considered [17]:\n\u2022 Vertical Federated Learning (VFL) in the case in which the datasets share overlapping data samples but differ in the feature space (see Figure 2a). This scheme can be applied if two different organizations (i.e., an Internet service provider and an online TV streaming provider) have data about the same group of people with different features and want to collaboratively train an ML model while keeping their data private.\n\u2022 Horizontal Federated Learning (HFL) that is used for cases in which each device contains a dataset with the same feature space but with different sample instances. For instance, think of two branches of the same insurance company that hold the same type of data about different clients (see Figure 2b).\n\u2022 Federated Transfer Learning (FTL) borrows some char-acteristics from both VFL and HFL and is suitable for scenarios in which there is little overlapping in both data samples and features as visible in Figure 2c. A good ex-ample is the case in which a bank wants to train its ML model by cooperating with an insurance company that shares part of the client and part of the features.\nEven if FL has been designed to achieve data confidentiality it has been demonstrated that it is still prone to possible attacks targeting data privacy that any participants of the scheme can perpetrate [18]. The most common attacks in this context are the following:\n\u2022 Inference attacks that aim at inferring the sensitive infor-mation about individual data points (attribute inference) or participants (membership inference) in the training dataset by analyzing the behavior or outputs of the feder-ated model [19, 20, 21].\n\u2022 Poisoning attacks can be divided into data or model poi-soning attacks. The first category involves adversaries that try to poison the training data in a certain number of devices participating in the learning process to compro-mise the GM accuracy. The adversary can inject poisoned data (i) directly into the targeted device or (ii) through other devices [22, 23]. In a model poisoning attack the adversary tries to poison the LMs instead of the local data to introduce errors in the GM.\n\u2022 Backdoor attacks through which an adversary can mis-label certain tasks without affecting the accuracy of the GM. This kind of attack manipulates a subset of training data by injecting adversarial triggers such that the models trained on the tampered dataset will make arbitrarily (tar-geted) incorrect predictions on the test set with the same trigger embedded [24, 25]."}, {"title": "3.2. Dataset Distillation", "content": "In general, in the context of ML, Distillation (known as Model Distillation) is a methodology to transfer knowledge from a larger, more complex model (called \"teacher\") to a smaller, simpler model (known as \u201cstudent\") to improve model perfor-mance or deploy the model on resource-constrained devices, such as Internet of Things (IoT) devices.\nAn alternative concept proposed by [1] is called Dataset Distillation (DD) and consists of the summarization of real data in a few highly informative and synthetic data points in such a way that models trained on the last dataset achieve comparable generalization performance to those trained on the real data. has been created to address this problem of large data volume [26].\nTo formally define DD, we start with some preliminary defi-nitions, namely a target dataset:\n$T = \\{(x_i, y_i)\\}_{i=1}^m$\nwhere x; \u2208 Rd, d is the dimension of the input data, y; is the i - th label, and (xi, yi), with 1 \u2264 i \u2264 m are independent and identically distributed (i.i.d.) random variables drawn from the data generating distribution D. The goal of DD is to extract the knowledge of T into a small synthetic dataset called:\n$S = \\{(s_j, y_j)\\}_{j=1}^n$"}, {"title": "4. Description of Our Approach", "content": "In this section, we provide a detailed description of our proposal. In particular, in Section 4.1, we give an overview of our approach and its underlying model, formally defining the (i) the involved parties and (ii) the Secure Federated Data Distillation (SFDD) process. After that, in Section 4.2, we describe a possible vulnerability of our scheme to inference attack and the modification we applied to make it robust to this particular kind of attack.\nThis section describes our SFDD architecture for decen-tralized and secure Data Distillation. We start by defining the fundamental components of our strategies, in particular, the involved parties are:\n\u2022 a Central Unit C that acts as an aggregator node and is in charge of initializing the global synthetic set of data S\u00ba, randomly;\n\u2022 a set of Workers or clients W = {W1,..., wz} of size z, which execute the Federated Distillation algorithm.\nIn our configuration, each worker w; in W holds a private data set $T^{w_i} = \\{(x, y)\\}_{i=1}^m$ to be distilled. All the private datasets of the network are independent and identically distributed (IID) and do not overlap. The set composed of all the $T^{w_i}$ for each worker w; can be formally defined as follows:\n$T^W = \\{T^{w_1},...,T^{w_z}\\}; T^{w_i} = \\{(x,y)^{w_i}\\}_{j=1}^{m}$\nwhere m is the number of data points for a set.\nOur adopted technique relies on gradient matching and fol-lows the main step described in [3]. According to this strategy, each worker $w_k \\in W$ holds a private model $M^{W_k}$ used to transfer knowledge from its private set of data $T^{W_k}$ to the global synthetic set S\u00ba. The set of private models can be referred to as:\n$M^W = \\{M^{W_1},..., M^{W_z} \\}$\nIn the following, we detail all the steps of our Secure Feder-ated Data Distillation approach. In brief, the steps are:\n1. Data Initialization;\n2. Local Data Distillation Update;\n3. Global Data Distillation Aggregation.\nThe first step of our framework, called Data Initialization, is performed by the central unit C that starts the process by randomly initializing the synthetic set of data S\u00ba. Specifically, it initializes a given number ipc of data (representing the number of images per class) for each class of the original dataset as follows:\n$S^0 = \\{S^0_1, . . ., S^0_c, ..., S^0_{n_c}\\}; S^0_c = \\{(s_i, y_c)\\}_{i=1}^{ipc}$\nwhere ye is the label assigned to class c and ne is the total number of classes. The synthetic dataset S\u00ba is distributed to all the workers in W.\nOnce each worker receives S\u00ba, the second step of the frame-work of Local Data Distillation Update takes place and the local distillation phase of the process carried out by the workers starts in a parallel and independent way. In particular, each worker wk distills the data contained in its private set TWk using its private model MWk.\nThe distillation process is conducted separately for each class and it generates the gradients of the model MWk using two contributions, namely (i) the synthetic data $V_{M^{W_k}}S^0$ and (ii) a batch of real data $V_{M^{W_k}}T^W_c$ (where $T^{W_k}_c$ is a subset of $T^{W_k}$ in which y = yc).\nThen, the obtained partial gradients are used to compute the matching loss using the matching loss function ML of the considered category.\nThe Local Data Distillation Update step is repeated for all the classes in the dataset and its outcome is a total loss that is back-propagated onto the global synthetic dataset S\u00ba. More formally, this step can be formulated as follows:\n$\\nabla_{M^{W_k}} CE(M^{W_k}(S^0), y_c); \\nabla_{M^{W_k}} CE(M^{W_k}(T^{W_k}), y_c)$ (2)\n$S^{W_k} = S^0 - \\sum_{c=1}^{n_c} ML(\\nabla_{M^{W_k}})$ (3)\nwhere CE() is the cross-entropy loss used to compute the gra-dients on MWk and $S^{W_k}$ is the locally updated version of S obtained by the worker wk. All the $S^{W_k}$ data are then used to train the local model MWk before the next iteration of the Feder-ated Distillation.\nThe last step of the framework is the Global Data Distillation Aggregation, in which the obtained updates on S are sent back to the central unit C to be aggregated using the FedAvg strategy and sending back to the workers the new global S\u00ba.\nThe last two steps of the process are repeated until the model's accuracy on the distilled dataset converges."}, {"title": "4.2. Defence against Data Leakage Attack", "content": "Like traditional Federated Learning strategies, as tested ex-perimentally in Section 5, our approach is vulnerable to data leak attacks perpetrated by an honest but curious server [9]. This attack is carried out to obtain the private training data from the publicly shared gradients. To perform it, a server takes advan-tage of the updates returned by the different workers to reverse the distillation process and obtain the original batch of data TWk used in the current epoch by the worker wk.\nIn particular, in this context, the server can randomly ini-tialize a tensor LB of the same shape as the one used by the workers to distill the knowledge (batch - size X nc). The tensor is defined as follows:\n$LB^{W_k} = \\{LB^{W_k}_1,..., LB^{W_k}_c,..., LB^{W_k}_{n_c}\\}$\nwhere $LB^{W_k}_c$ is the leaked batch used by wk to distill the synthetic data S\u00ba for the corresponding class c. To reverse the process, the server tries to replicate the distillation strategy by freezing the S\u00ba at the previous step and calculating the updates $VS^0$ emulating the workers but using LBWk instead of Twk for all the classes. In this way, the server tries to replicate and match the updates $VS^{W_k}$ on S\u00ba returned by the worker wk. The process can be formalized as follows:\n$\\nabla_{M^{W_k}} CE(S^{M^{W_k}}(S^0), y_c); M^{W_k} \\leftarrow \\nabla_{M^{W_k}} CE(S^{M^{W_k}}(LB^{W_k}), y_c)$ (4)\n$VS_l = \\sum_{c=1}^{n_c} ML(\\nabla_l^{M^{W_k}}, \\nabla_l)$ (5)\n$LB^{W_k} \\leftarrow ||VS_l - VS^{W_k}||_2$ (6)\nal(7)\nwhere the matching between $VS_l$ and $VS^{W_k}$ is performed using the L2 norm $|| \\cdot ||_2$.\nTo guarantee the preservation of the privacy of the workers from an honest but curious server adoping the method above, we include in our framework an enhanced Local Differential Privacy (LocalDP) strategy. Traditional LocalDP can fit the considered scenario by adding Gaussian or Laplacian noise and clipping to the obtained updates of S\u00ba. For our approach to be effective and, at the same time, to preserve the performance of the distilled data we have to estimate the amount of both (i) the noise and the (ii) clipping level. To achieve this, a worker must perform a grid search over various noise and clipping parameters while simultaneously evaluating performance retention and testing the attack effectiveness under these conditions. However, the computational overhead introduced by this exhaustive search could significantly slow down the distillation process.\nTo overcome this limitation, we propose an optimized strat-egy based on a community-oriented Label Differential Privacy (LabelDP) method, inspired by Arazzi et al. [20]. Our approach, called LabelDP Obfuscation via Randomized Linear Dispersion (LDPO-RLD, hereafter), enables workers to obscure the correla-tion between distilled and real images. Instead of using one-hot encoded labels to compute gradient matching, workers redis-tribute an e fraction of the primary label's probability across a set of k randomly selected labels using a linear function, Lin(.). To generate noisy labels, the authors of [20] propose employing Knowledge Distillation through a pre-trained teacher network. However, the use of a teacher network would add unnecessary overhead, particularly in resource-constrained environments. In our scenario, such an additional complexity is not required, as we use the noisy-label approach to just obfuscate the correlation between real and distilled images. Therefore, we can select the k labels at random. This modification reduces the complexity while maintaining the intended obfuscation level. The resulting changes in the loss and gradient calculations are as follows:\n$\\nabla_l S_M \\leftarrow CE(S^{M^{W_k}}(S^0), Lin(y_c, k, \\epsilon));$ (8)\n$\\nabla_l M^{W_k} \\leftarrow CE(S^{M^{W_k}}(LB^{W_k}), Lin(y_c, k, \\epsilon))$ (9)\nWith this new formulation, an attacker attempting to reverse the distillation process to recover the original batch of images cannot accurately reconstruct the worker's scenario, as the spe-cific label distribution used remains unknown.\nIn Section 5, we present experimental results evaluating the modified loss against the reference Data Leakage attack, along with the final performance of the distilled dataset on the test set."}, {"title": "5. Experiments and Results", "content": "In this section, we discuss the experiments carried out to assess the performance of our framework. Specifically, in Sec-tion 5.1, we describe the datasets and the metrics used for our experimental campaign. Section 5.2 is dedicated to evaluating the performance of our solution, and finally, in Section 5.3, we show the performance of our defense strategy (LDPO-RLD) against both server and client-side attacks."}, {"title": "5.1. Experimental Setup", "content": "In our experiments, we utilized five state-of-the-art datasets, selected for their diversity in image content and classification complexity, allowing for a comprehensive evaluation of our SFDD approach. Below, we provide details on each of the five datasets used:\n\u2022 MNIST [27] consists of 70, 000 grayscale images of hand-written digits (0-9), with 60,000 images designated for training and 10,000 for testing. Each image is 28x28 pixels in size.\n\u2022 CIFAR-10 [28] contains 60,000 32x32 color images cat-egorized into 10 classes, with 50,000 images for training and 10,000 images for testing.\n\u2022 SVHN (Street View House Numbers) [29] is a real-world image dataset designed for digit recognition in nat-ural scene images. The dataset is provided by Stanford University and is divided into 73, 257 training and 26, 032 testing images. Additionally, there are 531, 131 extra im-ages, which are considered somewhat less challenging by the dataset provider; however, these additional images were not used in our experiments.\n\u2022 GTSRB (German Traffic Sign Recognition Benchmark) [30] contains 39, 270 images of traffic signs categorized into 43 classes. The dataset is divided into 26, 640 training images and 12, 630 testing images.\n\u2022 CIFAR-100 [28] is similar to CIFAR-10, CIFAR-100 contains 60,000 32x32 color images, but with 100 classes instead of 10. Each class has 600 images, divided into 500 training images and 100 testing images per class.\nThe following metrics are employed to quantify system per-formance:\n\u2022 Accuracy is defined as the proportion of correctly pre-dicted instances out of the total number of instances:\n$Accuracy = \\frac{TP}{TP + FP}$ where TP (True Positives) refers to the number of in-stances where the model correctly predicts the positive class, whereas FP (False Positives) refers to the number of times that the model incorrectly predicts a positive class.\n\u2022 Mean Squared Error (MSE) measures the average squared difference between the estimated values and the true value:\n$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2$\nwhere n is the number of data points, y; is the actual (true) value, \u0177\u00a1 is the predicted (estimated) value, and the squared difference (y; - \u0177\u00a1)\u00b2 measures the error for each prediction.\n\u2022 Attack Success Rate (ASR) refers to the percentage of times an attack achieves its intended goal or outcome. In the context of backdoor attacks like the Doorping attack, the success rate would be the proportion of instances in which the attack successfully manipulates the model or dataset as intended by the attacker."}, {"title": "5.2. Performance Analysis", "content": "In this section, we analyze the results of the experiments designed to evaluate the performance of our approach. As out-lined in Section 3.2, the distillation process requires two models, one for distillation and the second for the performance eval-uation of the distilled images. In our performance analysis, we employ the same CONVNET architecture described in [3] for both models. This architecture features multiple convolu-tional layers, each followed by normalization, activation, and pooling layers. The design of the CONVNET allows for exten-sive customization, including the number of filters (net_width), network depth (net_depth), activation functions (net_act), normalization techniques (net_norm), and pooling strategies (net_pooling). The input images are standardized to a size of 32 x 32 pixels. We maintained consistent network settings for both the distillation and evaluation models across the centralized version and the federated clients.\nIn the first experiment, we compared the performance in terms of the accuracy result of the standard DD solution against our SFDD approach with 5 clients as a baseline. To fulfill the assessment, we evaluated both solutions by changing the setting for the produced Images Per Class (IPC). In particular, for each considered dataset, we collected the results producing 1, 10, and 50 images for each class.\nAs shown in Table 2, our SFDD approach consistently achieved comparable performance across the five different datasets. For the MNIST dataset, our approach slightly surpasses the central-ized approach with 1 and 10 IPC and remains highly competitive with 50 IPC. In the CIFAR10 dataset, SFDD performs slightly better with 1 IPC and maintains very close performance with 10 and 50 IPC. The SVHN dataset results show that SFDD is nearly as effective as the centralized method with 1 IPC and slightly outperforms it with 10 IPC. For the GTSRB dataset, our method exhibits superior performance with 1 IPC and shows strong re-sults with 50 IPC. In the CIFAR100 dataset, the SFDD performs admirably with 1 IPC and is nearly identical in performance with 10 IPC. Overall, our SFDD approach proves to be a robust and effective method, consistently delivering performance on par with the state-of-the-art centralized method.\nThe second experiment aims to assess whether the number of clients participating in SFDD impacts the quality of the gen-erated images. To evaluate this, we conducted a performance analysis using 10 and 15 clients, alongside the default setting of 5 clients. We measured the final mean accuracy and standard error on test sets across multiple randomly initialized networks trained with the distilled images. For consistency, we set the number of images per class to 10 as the baseline for this experiment."}, {"title": "5.3. Security Analysis", "content": "As for the experiments dealing with the security analysis, we employed the same CONVNET architecture from [3", "9": "."}, {"9": "."}]}