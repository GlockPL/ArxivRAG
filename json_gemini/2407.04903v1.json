{"title": "MMSci: A Multimodal Multi-Discipline Dataset for PhD-Level Scientific Comprehension", "authors": ["Zekun Li", "Xianjun Yang", "Kyuri Choi", "Wanrong Zhu", "Ryan Hsieh", "HyeonJung Kim", "Jin Hyuk Lim", "Sungyoung Ji", "Byungju Lee", "Xifeng Yan", "Linda Ruth Petzold", "Stephen D. Wilson", "Woosang Lim", "William Yang Wang"], "abstract": "The rapid advancement of Large Language Models (LLMs) and Large Multimodal Models (LMMs) has heightened the demand for AI-based scientific assistants capable of understanding scientific articles and figures. Despite progress, there remains a significant gap in evaluating models' comprehension of professional, graduate-level, and even PhD-level scientific content. Current datasets and benchmarks primarily focus on relatively simple scientific tasks and figures, lacking comprehensive assessments across diverse advanced scientific disciplines. To bridge this gap, we collected a multimodal, multidisciplinary dataset from open-access scientific articles published in Nature Communications journals. This dataset spans 72 scientific disciplines, ensuring both diversity and quality. We created benchmarks with various tasks and settings to comprehensively evaluate LMMs' capabilities in understanding scientific figures and content. Our evaluation revealed that these tasks are highly challenging: many open-source models struggled significantly, and even GPT-4V and GPT-4o faced difficulties. We also explored using our dataset as training resources by constructing visual instruction-following data, enabling the 7B LLaVA model to achieve performance comparable to GPT-4V/o on our benchmark. Additionally, we investigated the use of our interleaved article texts and figure images for pre-training LMMs, resulting in improvements on the material generation task. The source dataset, including articles, figures, constructed benchmarks, and visual instruction-following data, is open-sourced.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in generative artificial intelligence, including Large Language Models (LLMs) (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023a,b) and Large Multimodal Models (LMMs) (Li et al., 2023; Liu et al., 2024; Zhu et al., 2023; Achiam et al., 2023), have demonstrated remarkable capabilities in solving problems requiring educated knowledge across various domains, including mathematics (Cobbe et al., 2021; Chen et al., 2023; Hendrycks et al., 2021; Lu et al., 2022b), history, computer science, law, and technology (Hendrycks et al., 2020). While these models excel at tasks ranging from elementary to undergraduate-level knowledge, there is an increasing demand for more professional AI scientific assistants that can comprehend and process advanced, graduate-level, and even PhD-level scientific knowledge (noa, 2023; White, 2023; Vert, 2023).\nIn response, researchers have begun exploring the application of these generative models in fields such as biomedicine (Thapa & Adhikari, 2023), health (Tian et al., 2024), chemistry (Zheng et al., 2023; Bran et al., 2023), and material science (Xie et al., 2023; Miret & Krishnan, 2024) for purposes including research automation, education, and assistance (Meyer et al., 2023). A critical aspect of developing effective AI science assistants is their ability to understand academic scientific literature, which often includes complex figures like data visualization plots and charts, schematic diagrams, macroscopic and microscopic photographs, and other specialized content from various fields.\nHowever, there is currently a lack of comprehensive evaluation of models' understanding of professional PhD-level multimodal scientific knowledge, particularly with figures, across diverse scientific disciplines. Existing evaluations of LMMs on scientific problems are typically limited to up to college-level knowledge and a few science disciplines, such as computer science, mathematics, physics, chemistry, and biology (Lu et al., 2022a; Wang et al., 2023; Yue et al., 2023), as shown in Table 1. Furthermore, the evaluation of models' abilities to understand scientific figures has been restricted to simple charts and plots (Chen et al., 2020; Kahou et al., 2017; Siegel et al., 2016), and suffer from relatively narrow scopes and lower quality (Li et al., 2024).\nTo bridge the gap, we collected a multimodal, multi-discipline dataset MMSci from high-quality, open-access articles published in Nature Communications\u00b3, which are freely and permanently available upon publication under a Creative Commons Attribution 4.0 International (CC BY) license. This dataset spans 72 scientific disciplines, primarily within the natural sciences (the top 30 subjects with most articles can be seen in Figure 1). We created a benchmark to evaluate models' understanding of PhD-level multimodal scientific knowledge across various disciplines. The benchmark includes scientific figure captioning and visual question answering (VQA) tasks in various settings, thoroughly assessing LMMs' capabilities in understanding scientific figures and content. Our evaluation revealed significant challenges and deficiencies in current LMMs in interpreting scientific figures and content. Many open-source models struggled considerably with these tasks, demonstrating limited capability. Even GPT-4V and GPT-4o encountered difficulties in producing accurate, relevant captions and matching figures with their descriptions under challenging settings.\nFurthermore, our dataset includes a vast collection of high-quality academic articles and figures, which we explored as training resources to enhance models' understanding of scientific content. To achieve this, we constructed visual instruction-following data with discussions about figure content, structured as single or multi-turn interactions. Additionally, we investigated pre-training LMMs using our interleaved article text and figure images to improve their acquisition of scientific knowledge. Experimental results show that our visual instruction-following data enhanced the 7B LLaVA model, achieving performance comparable to GPT-4V/o on our benchmark. Moreover, experiments on a materials science task demonstrated that pre-training on our interleaved multimodal data could improve the performance on material generation. Overall, our contributions are threefold:\n\u2022 Data scope and quality: Our dataset is unique as it consists of high-quality peer-reviewed academic articles and figures across 72 diverse scientific disciplines.\n\u2022 Challenging benchmark: Our benchmark includes tasks with varying settings for comprehensive assessment. Our evaluation reveals notable deficiencies in current LMMs in effectively interpreting figures in scientific literature."}, {"title": "2 Related Dataset Work", "content": "Scientific Figure Understanding Scientific figures in academic articles convey rich, valuable information, and there has been extensive research on evaluating the understanding of these figures. Early approaches typically focused on data visualization figures. For instance, Chen et al. (2020); Kahou et al. (2017); Kafle et al. (2018) created synthetic datasets comprising various types of plots and charts. To obtain more diverse and complex scientific figures, FigureSeer (Siegel et al., 2016) and SciCap (Yang et al., 2023) gathered computer science (CS) papers from arXiv to extract article figures from PDFs. More recently, ArxivQA/Cap (Li et al., 2024) collected papers from 32 subjects on arXiv. However, their collection still primarily focuses on CS and math, with limited inclusion of rich and diverse natural science subjects. Additionally, since these arXiv papers are not peer-reviewed, their quality is not guaranteed. In contrast, our dataset emphasizes natural science disciplines and collects high-quality, peer-reviewed articles and figures from the prestigious Nature Communications journals. Covering 72 diverse science disciplines, our dataset ensures both diversity and quality.\nMultimodal Science Problems With the advancement of LMMs, many studies have focused on evaluating their capabilities in solving scientific problems in a multimodal context. However, ScienceQA (Lu et al., 2022a) primarily addresses problems ranging from elementary to high school levels (K1-12). SciBench (Wang et al., 2023) focuses solely on three science disciplines: physics, chemistry, and mathematics. MMMU (Yue et al., 2023) includes various subjects such as art, business, history, health, humanities, and technology, but its coverage of science subjects is limited to 25 disciplines according to the categories of the Nature website. In contrast, our dataset evaluates PhD-level scientific knowledge across 72 diverse scientific domains."}, {"title": "3 Data Curation", "content": null}, {"title": "4 Benchmarks", "content": "We developed two benchmark tasks with varying settings to comprehensively test models' comprehension of scientific articles and figures from different aspects, as shown in Figure 3."}, {"title": "5 Training Resources", "content": "Our dataset consists of rich articles and figure data, which we explore as training resources to enhance models' capabilities in comprehending scientific figures and content."}, {"title": "6 Benchmark Evaluation Results", "content": "We benchmarked various prevalent open-source and proprietary LMMs on the market, including: Kosmos-2 (Peng et al., 2023), BLIP-2 (Li et al., 2023), Qwen-VL-Chat (Bai et al., 2023), and the LLaVA series models (Liu et al., 2024, 2023), including LLaVA1.5-7B, LLaVA-Next (LLaVA1.6-Vicuna-7B), LLaVA-Next-Mistral (LLaVA1.6-Mistral-7B), and the proprietary GPT-4V (Achiam et al., 2023) and GPT-4o. The exact model versions are provided in the Appendix. Additionally, we fine-tuned a LLaVA-Next (LLaVA1.6-Vicuna-7B) model using our visual instruction-following data, containing around 1,080k training samples, for one epoch. This resulted in our model called LLaVA-Next-MMSci.\nFor scientific figure captioning, we ran the inference three times and reported the average scores for BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee & Lavie, 2005), and BERTScore (Zhang et al., 2019) by comparing the generated captions to the oracle captions. We also reported reference-free image captioning metrics, CLIPScore and RefCLIPScore (Hessel et al., 2021), which directly compare the generated captions with the images. However, note that these metrics are primarily designed for natural images with relatively short captions and will truncate"}, {"title": "7 A Case Study in Material Sciences", "content": "Material science is the subject with the most articles and figures in our dataset. It is an important and highly interdisciplinary field, requiring knowledge from various subjects. Therefore, we conducted a case study to enhance material science knowledge using our dataset.\nThere has been research on using language models for material science tasks (Walker et al., 2021; Rubungo et al., 2023; Miret & Krishnan, 2024). A recent study (Gruver et al., 2024) achieved promising results by utilizing LLaMA2 (Touvron et al., 2023b) for material generation. In this study, material crystal structures were represented as text strings, and the LLaMA2 model was trained to generate these structure strings. However, LLaMA2 may lack sufficient scientific knowledge to fully comprehend the principles of material generation. Therefore, we explored the continuous pre-training"}, {"title": "8 Conclusion", "content": "In this work, we present MMSci, a multi-discipline multimodal dataset that includes high-quality peer-reviewed articles and figures across 72 science disciplines. Using this dataset, we construct a challenging benchmark to evaluate the capabilities of LMMs in understanding scientific figures and content, revealing significant deficiencies. Additionally, we explore the use of our dataset as training resources to enhance models' scientific comprehension. By constructing visual instruction-following data and interleaved text and image data for pre-training, we achieve improvements on both our benchmark and the material generation task. We anticipate that our dataset will serve as a valuable resource for evaluating and enhancing the scientific comprehension of generative models, thus advancing the development of AI-based scientific assistants."}, {"title": "A.1 Dataset Description", "content": "A.1.1 Dataset Summary\nOur dataset MMSci is a multimodal, multi-discipline dataset containing high-quality, open-access articles published in Nature Communications journals. This dataset encompasses five major subjects and spans 72 diverse science disciplines, primarily within the natural sciences. We have developed a benchmark to evaluate models' comprehension of PhD-level multimodal scientific knowledge across various advanced disciplines. Additionally, we constructed visual instruction-following data for visual instruction tuning and interleaved text and image data for visual pre-training.\nA.1.2 Data and Code Access\nWe provide access to our data, model checkpoints, and code through the following links:\n\u2022 Source dataset, including the collected articles and figures:\n\u2022 Benchmark sets, including the dev and test sets for evaluation and the train set consisting of visual instruction following data:\n\u2022 Pre-training data, including the interleaved article and figure data for pre-training:\n\u2022 Checkpoints, including the LLaVA-Next (LLaVA1.6-Vicuna-7B) model fine-tuned on our visual instruction-following data:\n\u2022 Code: All the code used in our experiments is available at:\nA.1.3 Subjects\nOur dataset spans five major categories and includes 72 distinct scientific disciplines, representing a broad range of scientific knowledge. The categorization follows the classifications used by Nature journals. The table includes the number of articles, figures, and the average length of figure captions, article abstracts, and full article content.\nA.1.4 Image Types\nManual Review Initially, our authors conducted a thorough manual inspection of the figures and sub-figures from 100 randomly sampled articles from the five major categories in MMSci. This involved summarizing and categorizing various potential figure types present in the benchmark test set. From this detailed analysis, we identified and categorized the figures into seven primary types. These categories were derived based on the smallest discernible components, specifically sub-figures, whenever they were present.\nAutomated Classification Using GPT-4o Following this review, we employed GPT-4o to automatically classify the images in the benchmark test set. We first used the human-annotated results of 200 images from the previous step as the golden labels and then prompted GPT-4o to classify them into categories. Cohen's Kappa score was calculated to be 0.72, showing a very high agreement score between humans and GPT-40. So, we utilized GPT-40 to label all the image types.\nA.2 Datasheet\nA.2.1 Motivation\nWith the advancement of large language and multimodal models, there is a growing demand for professional AI scientific assistants capable of comprehending and processing advanced, graduate-level, and even PhD-level scientific knowledge (noa, 2023; White, 2023; Vert, 2023). A crucial aspect of developing effective AI scientific assistants is their ability to understand academic scientific literature, which often includes complex figures such as data visualization plots, charts, schematic diagrams, macroscopic and microscopic photograph, and other specialized content from a variety of scientific fields. However, there is currently a lack of comprehensive evaluation for models' understanding of advanced PhD-level multimodal scientific knowledge, especially in the context of complex figures across diverse scientific disciplines. Existing evaluations tend to focus on simpler charts and plots (Chen et al., 2020; Kahou et al., 2017; Siegel et al., 2016) and suffer from narrow scopes and lower quality (Li et al., 2024).\nOur dataset, MMSci, is designed to address this gap. MMSci is a multimodal, multi-discipline dataset comprising high-quality, peer-reviewed articles and figures from 72 scientific disciplines, predominantly within the natural sciences. We created a benchmark to evaluate models' understanding of PhD-level multimodal scientific knowledge across these disciplines. Additionally, this dataset can serve as a training resource to enhance models' comprehension of multimodal scientific knowledge.\nA.2.2 Intended Use\nThis dataset is used to evaluate and enhance the large multimodal models (LMMs)' understanding of advanced multimodal scientific knowledge.\nA.2.3 Data Collection\nData Source The dataset comprises open-access articles published in Nature Communications. These articles are freely and permanently accessible upon publication under the Creative Commons Attribution 4.0 International (CC BY) License. Detailed information on the open-access policy of Nature Communications is available at\nData Collection Process We collected various types of information for each article from the Nature Communications website. The articles' information includes titles, abstracts, main body content, references, and PDF versions of the articles, all directly accessible from their respective sections on the article's webpage (e.g.,  where \"xxx\" is the article's unique ID). Additionally, figures and their captions were sourced from a dedicated figures section linked from each article's main page (e.g., ). This user-friendly platform facilitates easy acquisition of all necessary data, eliminating the needs for quality control and data filtering.\nAnnotations The dataset does not include explicit annotations. Instead, the authors themselves carried out a small-scale manual review and classification of the image types specifically for analysis. No external annotators or crowdworkers were involved in this process.\nPersonal and Sensitive Information The dataset does not include any personal or sensitive information. All article content is publicly accessible. All author information are also publicly available, and no personal information was explicitly extracted, stored, or used from the authors.\nA.2.4 Social Impact and Ethical Considerations\nBenefits The benefits of our dataset are two-fold: (1) Evaluation Benchmark: This dataset serves as a valuable evaluation benchmark for assessing the understanding of large multimodal models (LMMs) regarding scientific articles and figures. (2) Training Resources: It can be used as a training resource to enhance LMMs' comprehension of scientific articles and figures, improving their performance in various scientific and research-related tasks.\nA.2.5 Limitations\nCurrently, our evaluation benchmark primarily focuses on understanding figures in scientific articles based on the article content or not. We encourage further efforts to expand these evaluations to include a broader range of scientific knowledge using our dataset.\nA.2.6 Author Statement\nThe authors declare full responsibility for any rights violations, including but not limited to intellectual property rights and privacy rights, that may arise from the publication and use of this dataset. We confirm that all data provided is licensed under appropriate licenses, ensuring legal compliance and transparency.\nA.2.7 Hosting, Licensing, and Maintenance Plan\nThe dataset will be hosted on GitHub, offering reliable and secure access. We commit to maintaining the repository with regular updates, security patches, and user support to ensure the data's integrity and usability over time. Licensing terms will be clearly communicated to users, adhering to the appropriate data licenses to promote proper usage and distribution. The data is licensed under the CC BY 4.0 License, which permits sharing and adaptation with proper attribution. The primary codebase for our project is licensed under the Apache 2.0 License.\nA.3 Experimental Setup\nA.3.1 Evaluated Model\nWe evaluated two proprietary models GPT-4V and GPT-4o and six open-source LMMs. Additionally, we tested our fine-tuned model, which is based on LLaVA-Next (LLaVA1.6-Vicuna-7B). For evaluations of open-source models, we utilized checkpoints available on Hugging Face. The specific versions of proprietary models and paths for open-source models are detailed in Table 8. All inferences for the open-source models were executed on a computing cluster equipped with eight NVIDIA A100 GPUs, each with 40GB of memory.\nA.3.2 Evaluation Setup and Results\nAs described in the main paper, we set the temperature to 0.7 for inferences on both the scientific figure captioning and multiple-choice Visual Question Answering (VQA) tasks. For the figure captioning task, we conducted the inference three times, and the averaged results along with their\nA.3.3 Visual Instruction Tuning\nFollowing the visual instruction tuning approach described in (Liu et al., 2024), we continuously fine-tuned the LLaVA-Next model (LLaVA1.6-Vicuna-7B). The original vision encoder, openai/clip-vit-large-patch14-336, was kept unchanged, while the projector and language model components were updated. The hyperparameters used in this process are detailed in Table 10. The fine-tuning was performed on a computing cluster equipped with eight NVIDIA A100 GPUs, each with 40GB of memory. This training process took approximately 24 hours to complete.\nA.3.4 Visual Language Pre-training\nIn our case study experiments on the material generation task, we continuously pre-train a LLaMA2-7B model using our interleaved article and figure data to infuse more material science-relevant knowledge. Specifically, for pre-training on the interleaved text and image data, we follow the methodology outlined in (Lin et al., 2023).\nModel Architecture Following the approach outlined in (Liu et al., 2024; Lin et al., 2023), we extend the LLaMA2-7B model from a text-only model to a multimodal model by augmenting the LLM with a visual encoder to learn visual embeddings and a projector to bridge the embeddings between the text and visual modalities. Specifically, the visual encoder processes the image and outputs visual features. These features are then mapped into the word embedding space by the projector, creating visual tokens. These visual tokens are concatenated with the word tokens and fed into the LLM, allowing the model to integrate both text and visual information for generation. The specific LLM, visual encoder, and projectors used in our experiments are presented in Table 11.\nA.3.5 Materials Generation\nAs a case study to investigate whether scientific knowledge has been effectively infused into the LLM (LLaMA2-7B in our experiments) and whether it can enhance performance on material science-related tasks, we follow the methodology from Gruver et al. (2024) to explore the material generation task. The primary objective is to format material crystal structures into text strings and fine-tuning the LLM to generate stable materials.\nPrompt design We adhere to the prompt design described in (Gruver et al., 2024). There are two types of prompts in the training data: the generation prompt with one or multiple conditions and infilling prompts, where partial crystal structure strings are masked and the model generates the masked parts. The specific prompt templates are shown below, adapted from (Gruver et al., 2024).\nGeneration Prompt\nInfilling Prompt\nBelow is a description of a bulkmaterial. [The chemical formula is Pm2ZnRh]. Generate a description of the lengths and angles of the lattice vectors and then the element type and coordinates for each atom within the lattice:\nBelow is a partial description of a bulk material where one element has been replaced with the string \u201c[MASK]\u201d:\nCrystal string with [MASK]s\n[Crystal string]\nGenerate an element that could replace [MASK] in the bulk material:\nMasked element\nBlue text is the condition for generation. Purple text stands in for string encodings of atoms.\nThe formula condition as shown above is always included, while other conditions are sampled from the following: formation energy per atom, band gap, energy above hull, and space group number.\nEvaluation Our evaluations follows (Xie et al., 2021; Gruver et al., 2024), including four key aspects. We reiterate some details here. Structural validity is assessed by ensuring that the shortest distance between any pair of atoms exceeds 0.5 \u00c5. Compositional validity is evaluated by verifying that the overall charge is neutral, as calculated using SMACT (Davies et al., 2019). Coverage metrics, COV-R (Recall) and COV-P (Precision), measure the similarity between ensembles of generated materials and ground truth materials in the test set. The property distribution metrics quantify the earth mover's distance (EMD) between the property distributions of generated materials and those in the test set, specifically for density ($\\rho$, in g/cm\u00b3) and the number of unique elements ($N_{el}$). Metastability and stability are assessed based on the energy above the convex hull, denoted as $\\rm E_{hull}$. Two approaches are employed to estimate $\\rm E_{hull}$: M3GNet (Chen & Ong, 2022) and Density Functional Theory (DFT) using the VASP code (Hafner, 2008). For M3GNet, each sample undergoes relaxation using force and stress calculations before evaluating the energy of the final structure. For DFT, relaxation is performed using the VASP code, which provides more accurate results but requires significantly more computational resources. A material is considered metastable by M3GNet if the predicted energy above the hull, $E^{M3GNet}_{hull}$, is less than 0.1 eV/atom. Furthermore, if validated by DFT, the material must have $E^{DFT}_{hull}$ < 0.0 eV/atom to be considered stable. The percentages of such materials are reported over the total 10,000 inferences. We use the Materials Project (Jain et al., 2013) dated 2023-02-07.\nTraining Details Following the approach in (Gruver et al., 2024), we utilize 4-bit quantization (Dettmers et al., 2021) and Low-Rank Adapters (LORA) (Hu et al., 2021) for efficient fine-tuning. The model is trained with a batch size of 1 for 1 epoch. We set the LoRA rank to 8 and the LORA alpha to 32. The learning rate is 0.0001, annealed by a cosine scheduler. The training was conducted on a single NVIDIA A100 GPU, took approximately 4 hours to complete."}]}