{"title": "Comprehensive Performance Evaluation of YOLOv10, YOLOv9 and\nYOLOv8 on Detecting and Counting Fruitlet in Complex Orchard\nEnvironments", "authors": ["Ranjan Sapkota", "Zhichao Meng", "Dawood Ahmed", "Martin Churuvija", "Xiaoqiang Du", "Zenghong Ma", "Manoj Karkee"], "abstract": "Object detection, specifically fruitlet detection, is a crucial image processing technique in agricultural automation, enabling the\naccurate identification of fruitlets on orchard trees within images. It is vital for early fruit load management and overall crop\nmanagement, facilitating the effective deployment of automation and robotics to optimize orchard productivity and resource use.\nThis study systematically performed an extensive evaluation of the performances of all configurations of YOLOv8, YOLOv9, and\nYOLOv10 object detection algorithms in terms of precision, recall, mean Average Precision at 50% Intersection over Union\n(mAP@50), and computational speeds including pre-processing, inference, and post-processing times for fruitlet (of fruitlet)\ndetection in commercial orchards. Additionally, this research performed and validated in-field counting of fruitlets using an iPhone\nand machine vision sensors in 5 different apple varieties (Scifresh, Scilate, Honeycrisp, Cosmic crisp & Golden delicious). This\ninvestigation of total 17 different configurations of YOLOv8, YOLOv9 and YOLOv10 (5 for YOLOv8, 6 for YOLOv9 and 6 for\nYOLOv10) revealed that YOLOv9 outperforms YOLOv10 and YOLOv8 in terms of mAP@50, while YOLOv10x outperformed all\n17 configurations tested in terms of precision and recall. Specifically, YOLOv9 Gelan-e achieved the highest mAP@50 of 0.935,\noutperforming YOLOv10n's 0.921 and YOLOv8s's 0.924. In terms of precision, YOLOv10x achieved the highest precision of 0.908,\nindicating superior object identification accuracy compared to other configurations tested (e.g. YOLOv9 Gelan-c with a precision\nof 0.903 and YOLOv8m with 0.897. In terms of recall, YOLOv10s achieved the highest in its series (0.872), while YOLOv9 Gelan-\nm performed the best among YOLOv9 configurations (0.899), and YOLOv8n performed the best among the YOLOv8 configurations\n(0.883). Meanwhile, three configurations of YOLOv10; YOLOv10b, YOLOv10l, and YOLOv10x; outperformed all other\nconfigurations in YOLOv9 and YOLOv8 family of models in terms of post processing speed (only 1.5 ms), while YOLOv9 Gelan-e\n(1.9 ms) and YOLOv8m (2.1 ms). Furthermore, YOLOv8n exhibited the highest inference speed (detection speed) among all\nconfigurations tested, achieving a processing time of 4.1 milliseconds. In comparison, YOLOv9 Gelan-t and YOLOv10n also\ndemonstrated impressive inference speeds of 9.3 ms and 5.5 ms, respectively Additionally, counting validation studies across four\napple varieties (Scifresh, Scilate, Honeycrisp, and Cosmic Crisp) using an iPhone 14, and two varieties (Golden Delicious and\nScifresh) using Microsoft Kinect Azure DK and Intel RealSense D435i sensors, highlighted YOLOv9 Gelan-e as the most accurate\nconfiguration. It achieved an RMSE of 3.11 and an MAE of 4.58, surpassing all other YOLOv8, YOLOv9, and YOLOv10\nconfigurations in accuracy. This performance underscores the YOLOv9 models' superiority in fruitlet detection and counting within\ncomplex orchard settings. Despite some YOLOv10 configurations offering marginally better precision and recall, the YOLOv9\nmodels provide comparable accuracy while maintaining significantly faster inference speeds, making them more suited for\nagricultural applications where computational resources and data availability are often constrained.", "sections": [{"title": "1. Introduction", "content": "Object detection in commercial orchards is the foundation to developing agricultural automation and robotics\nsolutions for labor intensive tasks such as harvesting, thinning, and pruning [1], [2], [3]. One such labor-intensive\noperation in apple orchards is thinning green fruit in their early growth stage (fruitlets), which is crucial due to its role\nin enhancing crop yield and quality. Automating fruitlet thinning process is essential for minimizing the dependence\nof rapidly depleting farm labor, which requires a robust machine vision system for fruitlet detection and localization\nin orchard environments [4].\nMost of the tree fruit crops often set a greater number of fruits per tree than the desired number which causes fruit-\nto-fruit competition for water, sunlight, and nutrients, resulting inadequate exposure of the fruits to the sun, less space\nto grow, and overall reduced fruit quality [4]. Additionally, too many fruits can result in reduced cold hardiness,\nbreakage of tree limbs, and exhaustion of tree reserves [5]. Fruitlet thinning in the commercial production of tree fruit\ncrops such as apples, kiwifruit, pears, peaches and plums has been practiced for thousands of years [6] to address these\nchallenges and ensure optimal fruit size and quality [7] [8].\nApples is the third most consumed fruit in the United States (U.S.). U.S. also produces an average of 4.6 million\ntons of apples yearly, making the country the world's second-largest contributor in apple production [9], [10]. Around\n382 thousand acres of land in the U.S. is used for farming apples commercially, which contributes to exporting 42\nmillion bushels of apples worldwide with an estimated downstream value of $21 billion dollars each year [11]. In the\nU.S., approximately 1.5 million hired workers are employed in agriculture annually, with about three-fourths or 1.1\nmillion working in crop-production activities [12]. One of the most labor-intensive operations in apple production is\nfruitlet thinning, and therefore developing robotic fruitlet thinning solutions is essential for sustainable apple\nproduction in U.S and around the world.\nThe availability of farm labor has continued to decline over the past decade, a situation worsened by the global\npandemic, which led to an estimated loss of $309 million in agricultural production from March 2020 to March 2021\n[13], [14]. On the other hand, global urbanization has been transforming rural areas, causing 68% of the population to\nreside in urban environments by 2050 (United Nations). which would further increase the labor shortage in agriculture.\nOn top of that, labor costs on specialty crop farms in the United States are 3 times higher than the average cost of labor\nin all U.S. farms (USDA, 2018), which further emphasizes the need for automating labor-intensive operations in apple\nand other specialty crop fields.\nIn commercial apple orchards, the demand for labor-intensive fruitlet thinning peaks during the summer months.\nFigure la demonstrates the overcrowding of fruitlet in tree branches as each flower cluster leads to several fruits\nrequiring fruitlet thinning to optimize the number of fruits in each branch based the branch diameter. sunlight, space,\nand water. Figure 1b shows farm workers manually thinning apple clusters, a process that is both time-consuming and\nlaborious, requiring a significant number of seasonal workers.\nAs the U.S. agricultural workforce continues to decline, maintaining competitiveness in the global market\nnecessitates the adoption of labor-saving technologies. Agricultural robots, which can replicate human tasks such as\nfruitlet thinning during the early growing season, offer a promising solution to labor-intensive operations [17]. In\naddition to reducing dependance on manual labor, automated and robotic systems for fruit thinning can also contribute\nto enhancing worker health and safety. Occupational health studies highlight the significant risks associated with\nmanual labor in orchards. For instance, May et al. [18] reveals that farmworkers suffer from a high incidence of\nmusculoskeletal injuries, skin diseases, and other health issues, with the agricultural sector experiencing a fatality rate\nseven times the national average. Further supporting this need, the study by Earle-Richardson et al. [19] documents\nthe detrimental effects of prolonged physical labor on migrant orchard workers, demonstrating significant decreases\nin muscle strength within a single workday.\nThe first crucial step in automating the fruit thinning process is developing a robust vision system capable of\ndetecting green apple fruitlets during the early thinning stage in commercial orchards. Green apple detection is\nfundamentally an object detection challenge, a domain extensively studied within computer vision. Apple detection\nsystems in orchard environments for possible automation have been reported since the late 90s [20]. In the past, many\nstudies have reported apple detection models for supporting robotic harvesting using traditional image processing\ntechniques such as image color segmentation in RGB, HIS (hue, saturation, and intensity), and/or HSL (hue, saturation,\nluminance) color spaces [21], [22], [23], [24]. However, the accuracy of these traditional methods has been relatively\nlow, and the approach is limited to a specific environmental condition such as uniform lighting and background\nconditions created artificially in orchards. It is challenging to implement these image processing algorithms to detect\napples in a natural environment with high level of occlusion, fluctuating illumination, and complex backgrounds[25].\nDuring the last few years, machine learning (ML) has become widely adopted for object detection in agriculture,\nproviding more robust detection capabilities with reduced level of image pre-processing and feature extraction [26].\nAdditionally, deep learning (DL)-based models have become one of the most effective methods for performing\nobject detection in agriculture as it provides end-to-end processing capability without any manual feature extraction.\nTo perform apple detection in orchards, researchers have implemented DL models such as Faster R-CNN models\nbased on AlexNet and based on ResNet101 [29]. More recently, Kang et al. [30] have purposed a deep-learning-based\ndetector called 'LedNet', which is able to perform real-time and accurate apple detection in orchards. The LedNet\narchitecture utilizes the 3-levels feature pyramid network (FPN) and Atrous Spatial Pyramid Pooling (ASPP), a\nsemantic segmentation module used in the feature processing block of the DL module.\nAlthough researchers have been successful in detecting mature apples for robotic harvesting, there are only a few\nreported studies on the detection of apples during their early growing season for robotic thinning applications. During\nthe early growing season of apples in the natural environment, the color of apples and light reflection is similar to that\nof leaves, which poses a great challenge in accurately detecting apple fruitlet. Additionally, factors such as occlusion\nand overlap of fruits with leaves and branches, and uncertain illumination make it difficult to detect fruitlet effectively\n[31], [32].\nXia et al. [33] used improved Hough transform algorithm and SVM to detect green apples in a natural environment\nand reported an F1 score of 90.3%. The author used an iterative threshold segmentation (ITS) algorithm to detect\nregions of interest (ROI) that contained potential apple fruitlet pixels. However, this technique was limited to detecting\nonly non-overlapping apples. Recently, to detect young apples, Tian et. al [34] implemented a YOLOv3 model called\n'Densenet' and reported an F1 score of 83.2%. Additionally, to detect apples, Huang et al. [35] recently used an\nimproved YOLOv3 model based on the CSPDarknet53 DL network. The study reported an inference speed of 8.6 ms\nin detecting immature apples. However, F1 and mAP achieved by the proposed model were only 0.65 and 0.67. Most\nrecently, Wang at al. [36] presented a YOLOv5s-based deep learning technique to detect apple fruitlet before thinning\nactivity in the natural environment. The author of this study implemented transfer learning through a channel pruned\nYOLOv5s object detection model and fine-tuned the model to achieve a higher detection rate. However, the model\nsize implemented in this study was very small (1.4MB) and the author reported a false detection of 4.2%.\nRecent advancements in deep learning models for object detection have significantly enhanced fruitlet detection\nperformance in field environments. The You Only Look Once (YOLO) framework, modified for specific agricultural\nneeds, has shown promising results. For instance, [40] reported that a tailored YOLO model improved the detection\nof green citrus in complex orchard environments with a substantially better accuracy and speed compared to YOLO\nv4 [37]. Similarly, a lightweight version of YOLOv8 was developed for detecting pomegranate fruitlets, achieving\nhigh precision with reduced model complexity, making it suitable for mobile devices [38]. Additionally, a channel-\npruned YOLO V5s model effectively detected apple fruitlets under varying conditions, outperforming several methods\nwhile maintaining a compact size for potential use in mobile fruit thinning terminals[39].\nSimilarly, an adaptation of the YOLOX-m model, optimized for robust feature extraction and enhanced by a feature\nfusion pyramid network with an Atrous Spatial Pyramid Pooling (ASPP) module, has shown improvements in\ndetecting fruitlets. The ASPP module-based network achieved an average precision of 64.3% for apples and 74.7%\nfor persimmons, outperforming several common detection models and providing a solid reference for diverse fruit and\nvegetable detection tasks [40]. Similarly, the YOLO-P model, a modified version of YOLOv5 designed specifically\nfor pear detection in orchards, incorporated advanced architectural changes such as shuffle blocks and a convolutional\nblock attention module (CBAN). These modifications improved feature extraction capabilities, allowing the model to\nperform well under challenging environmental conditions including complex backgrounds and varied lighting. This\nmodel not only supports rapid and precise pear detection but also offers insights for enhancing fruit detection systems\nin similar unstructured environments [41].\nMoreover, a YOLO v4-based method specifically tailored for fig detection in dense foliage has shown\nimprovements over previous models including Faster R-CNN, emphasizing YOLO's capability in challenging\nconditions [42]. YOLOv8 has also been successfully applied to size immature green apples using geometric shape\nfitting techniques, showcasing high precision and robustness against occlusions in orchard environments [4].\nFurthermore, an enhanced YOLOv8 model has proven to be effective for detecting Yunnan Xiaomila peppers,\nintegrating attention mechanisms and deformable convolutions to handle small targets against complex backgrounds\n[43]. Additionally, modifications to YOLOv5 have improved plum recognition, adapting the algorithm to handle fruit\nocclusions and environmental variability [44]. Another study with YOLOv7-based model equipped with attention\nmechanisms and specialized pooling has shown to achieve high precision in detecting plums in natural settings [45].\nIn orchard automation, YOLO object detection models have been pivotal in enhancing the accuracy and efficiency of\nfruit detection [4], [46], [47], flower identification [48], [49], [50], and automated harvesting processes [51], [52],\n[53]. These models adeptly identify and classify fruits at various stages of ripeness, detect flowers with high precision,\nand facilitate efficient harvesting operations. The development of YOLO models has introduced significant\nimprovements specifically tailored to meet the challenges faced in agricultural environments. For instance, the\nintroduction of multi-scale predictions in YOLOv5 has improved the detection of small and clustered objects like\nflowers and young fruits, which are crucial during the early stages of crop yield management [54].\nThe rapid advancement of YOLO-based models has significantly enhanced their precision and processing speeds, vital\nfor implementing robotic solutions such as fruitlet thinning in orchards. Given these swift innovations, continuous\nevaluation of newer models is essential to harness these improvements for agricultural automation.\nThis study provides a comprehensive evaluation of the latest YOLO versions: YOLOv10, YOLOv9, and YOLOv8,\ntargeting fruitlet detection in commercial apple orchards by examining 17 configurations across these models and\nutilizing a commercial orchard's dataset of RGB images from an iPhone 14 across four apple varieties: Scifresh,\nScilate, Honeycrisp, and Cosmic Crisp. Each image was annotated with a complete count of visible and occluded\napples directly observed in the field, providing a comprehensive dataset for validation. The study also validates the\ntop-performing models using machine vision sensors from Microsoft Azure and Intel Realsense, specifically targeting\nthe Golden Delicious and Scifresh varieties."}, {"title": "2. YOLO Background and Overview: Insights into YOLOv8, YOLOv9,\nand YOLOv10 Models", "content": "Figure 2a shows the timeline history of the YOLO from its release to upto date version as YOLOv10. The YOLO\nobject detection algorithm was first introduced by Joseph Redmon et al. [55] in 2015, revolutionized real-time object\ndetection by combining region proposal and classification into a single neural network, significantly reducing\ncomputation time. YOLO's unified architecture divides the image into a grid, predicting bounding boxes and class\nprobabilities directly for each cell, enabling end-to-end learning [55]. YOLO is versatile, and its real-time detection\ncapabilities have revolutionized not only agriculture [56], but also many other field such as medical object detection\n[57], autonomous vehicle industry[58], security and surveillance systems [59], and industrial manufacturing [60]\nwhere accuracy and speed are paramount. The current state-of-the-art iteration of YOLO version is YOLOv10, and\nFigure 2b shows the FPS and mAP of each model starting from YOLOv1 to YOLOv10.\nAfter the release of first YOLO version, YOLOv2, or YOLO9000 [61], [62], expanded on this foundation by\nimproving the resolution at which the system operated and by being capable of detecting over 9000 object categories,\nthus enhancing its versatility and accuracy. YOLOv3 further advanced these capabilities by implementing multi-scale\npredictions and a deeper network architecture, which allowed better detection of smaller objects [63]. The series\ncontinued to evolve with YOLOv4 and YOLOv5, each introducing more refined techniques and optimizations to\nimprove detection performance (i.e., accuracy and speed) even further [64], [65]. YOLOv4 incorporated features like\nCross-Stage Partial (CSP) connections and Mosaic data augmentation, while YOLOv5, developed by Ultralytics,\nbrought significant improvements in terms of ease of use and performance, establishing itself as a popular choice in\nthe computer vision community. Subsequent versions, YOLOv6 through YOLOv10, have continued to build on this\nsuccess, focusing on enhancing model scalability, reducing computational demands, and improving real-time\nperformance metrics . Each iteration of the YOLO series has set new benchmarks for object detection capabilities and\nsignificantly impacted various application areas, from autonomous driving and traffic monitoring to healthcare and\nindustrial automation..\nStarting with YOLOv1, the foundational model introduced significant capabilities with an mAP of 63.4%, although it\nexperienced higher latencies[64] .This was followed by YOLOv2 and YOLOv3, which further improved detection\naccuracy, achieving mAPs of 76.8% and 57.9%, respectively, at the cost of increased latency. YOLOv4 continued this\ntrend, reaching an mAP of 43.5% and serving as a bridge to more refined models. YOLOv5 emerged as a popular\nchoice, balancing performance and efficiency with a competitive mAP of 50.7% and a latency of 140 ms [66]. The\nYOLOv6 series, including variants from YOLOv6-N to YOLOv6-L, offered mAP scores ranging from 37.0% to\n51.8%, with moderate latencies, marking a significant milestone in optimizing detection speed and accuracy [67].\nLastly, YOLOv7, including the YOLOv7-tiny and standard YOLOv7 models, achieved mAPs of 56.4% and 51.2%\nrespectively, but with significantly higher latencies, indicating a shift towards prioritizing accuracy over speed.\nLikewise, YOLOv8 demonstrates commendable performance with mAP scores ranging from 37.3% to 53.9%, and\nlatencies between 6.16 ms to 16.86 ms. While it made significant strides, YOLOv8 falls slightly short in terms of\nefficiency and accuracy when compared to its successors. Progressing to YOLOv9 [68], this iteration includes models\nlike YOLOv9-N, YOLOv9-S, YOLOv9-M, YOLOv9-C, and YOLOv9-X, which achieve mAP scores from 39.5% to\n54.4% [68]. Although YOLOv9 matches YOLOv10 in top mAP scores, its latency figures, particularly for YOLOv9-\nX, are higher, reflecting lesser efficiency than YOLOv10. This discrepancy highlights YOLOv10's advancements in\nbalancing high accuracy with reduced computational demands. The latest in the lineup, YOLOv10 [69], introduces a\nrange of variants: YOLOv10-N, YOLOv10-S, YOLOv10-M, YOLOv10-B, YOLOv10-L, and YOLOv10-X offering\nprecision scores from 38.5% to 54.4% on the MS-COCO dataset. Notably, YOLOv10-N and YOLOv10-S boast the\nlowest latencies at 1.84 ms and 2.49 ms, respectively, optimizing them for real-time applications. YOLOv10-X not\nonly achieves the highest mAP of 54.4% but does so with a remarkably low latency of 10.70 ms, demonstrating a\nsignificant leap in enhancing both accuracy and inference speed over previous models [69].\nYOLOv8 introduced enhancements to its backbone network with an upgraded version of the Darknet architecture,\nwhich improved computational efficiency and feature extraction. This version also added layers and refined the loss\nfunctions to handle class imbalances and stabilize training outcomes, making it suitable for real-time applications with\nstrict latency demands [67].\nYOLOv9 stands out in the realm of real-time object detection with its strategic architectural innovations aimed at\novercoming the challenges of information loss in deep neural networks, a critical factor in maintaining detection\naccuracy and system efficiency. Central to YOLOv9's design is the implementation of the Information Bottleneck\nPrinciple and Programmable Gradient Information (PGI), which ensure that crucial data is preserved throughout the\nlayers of the network. This approach facilitates better model convergence by maintaining reliable gradient generation\nacross the network's depth. Further enhancing YOLOv9's capabilities is the integration of Generalized Efficient Layer\nAggregation Network (GELAN) and reversible functions. GELAN optimizes the model's computational blocks for\nflexible performance across various applications, boosting both parameter utilization and efficiency. Meanwhile,\nreversible functions address the core issue of information degradation in deep learning architectures; these functions\nallow the model to invert transformations without any loss, preserving integral data necessary for accurate object\ndetection. This property is especially beneficial in ensuring that YOLOv9 maintains complete information flow,\ncrucial for precise updates to model parameters and effective learning."}, {"title": "3. Methods", "content": "This study was conducted across commercial orchards in Pullman, Prosser, and Naches, Washington State, USA, over\nthe years 2022 to 2024. It focused on evaluating datasets of green apples, featuring varieties such as Scifresh, Scilate,\nCosmic Crisp, Honeycrisp, and Golden Delicious. RGB images for this study were captured using IntelRealsense\nmachine vision camera and were manually labeled to prepare the constant dataset for training of each configuration\nacross YOLOv8, YOLOv9, and YOLOv10 models. In total, 17 model configurations were examined: 5 for YOLOv8,\n6 for YOLOv9, and 6 for YOLOv10. All model configurations were trained under standardized hyperparameter\nsettings on the same computational system to ensure consistency and comparability. Validation of these models\ninvolved additional RGB images captured from various orchards using an Apple iPhone 14 smartphone.\nThis validation process aimed to assess each model's fruit counting accuracy against manually counted ground truths,\nwhich included scenarios with occluded apples. Furthermore, this validation step was designed to rigorously test the\nhighest-performing models in terms of speed and accuracy using images from different sensors. The details of this\ncomprehensive methodology are illustrated in Figure 4, providing a visual reference for the experimental setup and\ndata collection approach employed in this study.\n3.1 Study Site and Data Acquisition\nThe primary data collection for this study was conducted at Allan Brothers Orchard in Prosser, Washington State,\nUSA, focusing on datasets of immature green apples for training models. RGB images were acquired using an Intel\nRealSense 435i camera during June 2024. For validation, additional datasets were collected from different locations\nand times: Cosmic Crisp apple images from the ROZA experiment station at WSU IAREC in Prosser, Honeycrisp\napple images from an orchard in Naches owned by Allan Brothers Fruit Company, and Golden Delicious apple images\nfrom Pullman, collected on June 14, 2022. Furthermore, validation of apple counts using the Intel RealSense camera\nand Microsoft Azure camera was conducted in June 2023 and June 2022 respectively. All images were captured prior\nto the manual thinning process conducted by orchard workers. The details of each machine vision sensor used in data\ncollection of this study are:\nIntel RealSense D435i: This camera features a 2-megapixel RGB sensor capable of capturing high-quality images.\nOperating at a resolution of 1280\u00d7720 pixels, it provides a comprehensive view with a 69.4\u00b0 horizontal and 42.5\u00b0\nvertical field-of-view, ensuring broad coverage in various environments. The Intel RealSense D435i is designed to be\ncompact and lightweight, making it highly effective for capturing RGB data in diverse settings.\nMicrosoft Azure Kinect DK: Equipped with a 12-megapixel color camera, this sensor delivers exceptional image\nclarity and detail. The RGB camera component of the Azure Kinect DK sensor excels in a range of lighting conditions,\nenhancing its utility in natural orchard environments where lighting can vary. The device's high-resolution imaging\ncapabilities make it ideal for detailed visual data collection in agricultural research.\nApple Iphone14: The Apple iPhone 14 features an advanced 12-megapixel RGB camera that excels in capturing high-\nresolution images with remarkable clarity and color accuracy. Its sophisticated sensor and computational photography\ncapabilities enable detailed and vibrant image capture, making it ideally suited for collecting visual data in various\nlighting conditions, crucial for applications such as agricultural monitoring and research.\n3.2 Data Preparation and Model Training\nA total of 1,147 images were manually annotated with bounding boxes using the online labeling platform provided by\nRoboflow. These images were allocated into training and validation at the ration of 8:2 respectively, facilitated by\nRoboflow's distribution tools. No image preprocessing steps were undertaken in this study, as the objective was not to\nenhance any specific model but rather to compare and evaluate the performance of these models using raw data\ncollected in natural orchard settings.\nThe computational analyses for this study were conducted on a high-performance workstation equipped with an Intel\nXeon(R) W-2155 CPU, featuring a base clock speed of 3.30 GHz across 20 cores. This setup provided substantial\nprocessing power necessary for handling intensive data processing tasks. The workstation was also outfitted with\nNVIDIA Corporation GP102 [TITAN Xp] graphics cards, enhancing its ability to perform complex image processing\nand machine learning tasks efficiently. The system included a substantial storage capacity of 7.0 TB, facilitating\nextensive data management and analysis. It operated under Ubuntu 20.04.6 LTS, a robust and stable 64-bit operating\nsystem, using GNOME version 3.36.8 for its graphical interface and X11 as its windowing system.\nThe analysis of YOLOv8, YOLOv9, and YOLOv10 encompassed a total of 17 configurations (five for YOLOv8, six\nfor YOLOv9, and six for YOLOv10). Each model configuration was rigorously tested for precision, recall, mAP@0.5,\nand image processing speed. For the training of all models, a consistent configuration was rigorously maintained to\nensure uniformity and comparability across experiments. Each model was trained for 700 epochs, reflecting a\nsubstantial duration to adequately learn and adapt to the dataset's complexities. The batch size was set at 8, optimizing\nthe balance between memory usage and processing speed. Images were resized to a uniform resolution of 640x640\npixels to standardize the input data size across all models. The Stochastic Gradient Descent (SGD) optimizer was\nemployed to update model weights effectively, favored for its efficiency in handling large-scale and complex data.\nAdditionally, the configuration threshold for confidence was set at 0.25, and the Intersection over Union (IoU)\nthreshold was maintained at 0.7, criteria chosen to optimize the balance between precision and recall during object\ndetection tasks.\nThe hyperparameter settings outlined in table 1 provide a detailed framework for optimizing the training of YOLO\nmodels in the study. Key parameters such as the initial and final learning rates were set at 0.01, facilitating a controlled\nadjustment of learning throughout the training process. Momentum was maintained at 0.937 to ensure consistent\nupdates across epochs, while a minimal weight decay of 0.0005 helped prevent overfitting. The training initiated with\na warmup phase spanning 3 epochs to stabilize the learning parameters early in the training. Loss adjustments were\nspecifically tuned, with box loss, class loss, and definition loss set at 7.5, 0.5, and 1.5 respectively, to balance the\ncontributions of different components of the loss function. Image augmentation techniques such as hue, saturation,\nand value adjustments were precisely defined to enhance model robustness under varied lighting conditions typically\nfound in orchard environments. Specific settings for geometric transformations like rotation, translation, and scaling\nwere employed to simulate different orientations and sizes of objects, crucial for improving the model's ability to\ngeneralize across diverse scenarios. The table also highlights the use of flipping and mosaic data augmentation to\nfurther enrich the training dataset, ensuring comprehensive exposure to potential real-world variations."}, {"title": "3.3 Performance Evaluation", "content": "To evaluate the detection capabilities of each configuration of YOLOv8, YOLOv9, and YOLOv10, the metrics of\nprecision, recall, and mean Average Precision at Intersection over Union (IoU) threshold of 0.50 (mAP@50) were\nemployed. Precision was calculated as the ratio of true positive detections to the total predicted positives, given by\nEquation 1, where TP denotes true positives and FP denotes false positives. Recall measured the ratio of true positive\ndetections to the actual positives, formulated as Equation 2 where FN represents false negatives. The mAP@50 was\ndetermined by averaging the precision across all recall levels for an IoU > 0.50. Additionally, the image processing\nspeed for each model was analyzed and compared across three categories: preprocessing, inference, and\npostprocessing. These evaluations were systematically conducted across 17 configurations of the YOLO models:\nYOLOv8m, YOLOv8s, YOLOv81, YOLOv8x, YOLOv8c for YOLOv8; YOLOv9 Gelan-e, YOLOv9 Gelan-c,\nYOLOv9 Gelan-s, YOLOv9 Gelan-t, YOLOv9 Gelan-m, and YOLOv9 Gelan for YOLOv9, and YOLOv10m,\nYOLOv10s, YOLOv101, YOLOv10x, YOLOv10c, YOLOv10d for YOLOv10, to evaluate their efficiency in detecting\nfruitlets in commercial orchards.\nPrecision = $\\frac{TP}{TP + FP}$\nEquation 1\nRecall = $\\frac{TP}{TP + FN}$\nEquation 2\nAdditionally, the assessment involved analyzing preprocessing, inference, and postprocessing speeds for each model\nconfiguration, as these metrics are critical for real-time object detection systems. Preprocessing speed determines how\nquickly a model can prepare images for detection, inference speed measures the time taken to identify objects within\nimages, and postprocessing speed reflects how swiftly the model finalizes the outputs after detection. Each of these\nstages is essential for efficient operation in agricultural applications, where timely and accurate detection can\nsignificantly impact decision-making and resource management. The computational efficiency of these models\ndirectly influences their practical utility in automated fruit detection systems, making this evaluation crucial for\nadvancing agricultural technology solutions."}, {"title": "3.4 In-Field Counting Validation", "content": "Upon evaluating the performance metrics of each YOLOv8, YOLOv9, and YOLOv10 configuration for detecting\nfruitlets, the most effective model from each version was selected based on the highest accuracy achieved at\nmAP@0.5. These top-performing models from YOLOv8, YOLOv9, and YOLOv10 were further validated to assess\ntheir detection capabilities in a real commercial orchard setting across five distinct apple varieties in Washington State.\nThis validation process utilized images collected both by smartphone and machine vision sensors. Initially, images of\nScifresh, Scilate, Cosmic Crisp, and Honeycrisp apples were captured using an iPhone 14. A total of 128 images, 32\nper variety, were analyzed to evaluate the models' counting accuracy before the thinning process.\nSubsequently, an additional set of images was used for further validation. This included 32 images of Golden Delicious\napples captured using a Microsoft Azure camera, and another 32 images of Scifresh apples taken with an\nIntelRealsense camera. Notably, while the IntelRealsense camera images of Scifresh and Scilate apples served as the\ntraining dataset, validation focused on varieties like Golden Delicious, Cosmic Crisp, and Honeycrisp, which were\nnot included in the training phase. For each image, a meticulous count of all visible and occluded apples was manually\nperformed directly in the field. This comprehensive manual counting provided a precise ground truth for each image\nsample across the different apple varieties, ensuring robust validation of the models' performance."}, {"title": "4. Results and Discussion", "content": "The evaluation of performance across different YOLO model configurations: YOLOv8"}, {"title": "Comprehensive Performance Evaluation of YOLOv10, YOLOv9 and\nYOLOv8 on Detecting and Counting Fruitlet in Complex Orchard\nEnvironments", "authors": ["Ranjan Sapkota", "Zhichao Meng", "Dawood Ahmed", "Martin Churuvija", "Xiaoqiang Du", "Zenghong Ma", "Manoj Karkee"], "abstract": "Object detection, specifically fruitlet detection, is a crucial image processing technique in agricultural automation, enabling the\naccurate identification of fruitlets on orchard trees within images. It is vital for early fruit load management and overall crop\nmanagement, facilitating the effective deployment of automation and robotics to optimize orchard productivity and resource use.\nThis study systematically performed an extensive evaluation of the performances of all configurations of YOLOv8, YOLOv9, and\nYOLOv10 object detection algorithms in terms of precision, recall, mean Average Precision at 50% Intersection over Union\n(mAP@50), and computational speeds including pre-processing, inference, and post-processing times for fruitlet (of fruitlet)\ndetection in commercial orchards. Additionally, this research performed and validated in-field counting of fruitlets using an iPhone\nand machine vision sensors in 5 different apple varieties (Scifresh, Scilate, Honeycrisp, Cosmic crisp & Golden delicious). This\ninvestigation of total 17 different configurations of YOLOv8, YOLOv9 and YOLOv10 (5 for YOLOv8, 6 for YOLOv9 and 6 for\nYOLOv10) revealed that YOLOv9 outperforms YOLOv10 and YOLOv8 in terms of mAP@50, while YOLOv10x outperformed all\n17 configurations tested in terms of precision and recall. Specifically, YOLOv9 Gelan-e achieved the highest mAP@50 of 0.935,\noutperforming YOLOv10n's 0.921 and YOLOv8s's 0.924. In terms of precision, YOLOv10x achieved the highest precision of 0.908,\nindicating superior object identification accuracy compared to other configurations tested (e.g. YOLOv9 Gelan-c with a precision\nof 0.903 and YOLOv8m with 0.897. In terms of recall, YOLOv10s achieved the highest in its series (0.872), while YOLOv9 Gelan-\nm performed the best among YOLOv9 configurations (0.899), and YOLOv8n performed the best among the YOLOv8 configurations\n(0.883). Meanwhile, three configurations of YOLOv10; YOLOv10b, YOLOv10l, and YOLOv10x; outperformed all other\nconfigurations in YOLOv9 and YOLOv8 family of models in terms of post processing speed (only 1.5 ms), while YOLOv9 Gelan-e\n(1.9 ms) and YOLOv8m (2.1 ms). Furthermore, YOLOv8n exhibited the highest inference speed (detection speed) among all\nconfigurations tested, achieving a processing time of 4.1 milliseconds. In comparison, YOLOv9 Gelan-t and YOLOv10n also\ndemonstrated impressive inference speeds of 9.3 ms and 5.5 ms, respectively Additionally, counting validation studies across four\napple varieties (Scifresh, Scilate, Honeycrisp, and Cosmic Crisp) using an iPhone 14, and two varieties (Golden Delicious and\nScifresh) using Microsoft Kinect Azure DK and Intel RealSense D435i sensors, highlighted YOLOv9 Gelan-e as the most accurate\nconfiguration. It achieved an RMSE of 3.11 and an MAE of 4.58, surpassing all other YOLOv8, YOLOv9, and YOLOv10\nconfigurations in accuracy. This performance underscores the YOLOv9 models' superiority in fruitlet detection and counting within\ncomplex orchard settings. Despite some YOLOv10 configurations offering marginally better precision and recall, the YOLOv9\nmodels provide comparable accuracy while maintaining significantly faster inference speeds, making them more suited for\nagricultural applications where computational resources and data availability are often constrained.", "sections": [{"title": "1. Introduction", "content": "Object detection in commercial orchards is the foundation to developing agricultural automation and robotics\nsolutions for labor intensive tasks such as harvesting, thinning, and pruning [1], [2], [3]. One such labor-intensive\noperation in apple orchards is thinning green fruit in their early growth stage (fruitlets), which is crucial due to its role\nin enhancing crop yield and quality. Automating fruitlet thinning process is essential for minimizing the dependence\nof rapidly depleting farm labor, which requires a robust machine vision system for fruitlet detection and localization\nin orchard environments [4].\nMost of the tree fruit crops often set a greater number of fruits per tree than the desired number which causes fruit-\nto-fruit competition for water, sunlight, and nutrients, resulting inadequate exposure of the fruits to the sun, less space\nto grow, and overall reduced fruit quality [4]. Additionally, too many fruits can result in reduced cold hardiness,\nbreakage of tree limbs, and exhaustion of tree reserves [5]. Fruitlet thinning in the commercial production of tree fruit\ncrops such as apples, kiwifruit, pears, peaches and plums has been practiced for thousands of years [6] to address these\nchallenges and ensure optimal fruit size and quality [7] [8].\nApples is the third most consumed fruit in the United States (U.S.). U.S. also produces an average of 4.6 million\ntons of apples yearly, making the country the world's second-largest contributor in apple production [9], [10]. Around\n382 thousand acres of land in the U.S. is used for farming apples commercially, which contributes to exporting 42\nmillion bushels of apples worldwide with an estimated downstream value of $21 billion dollars each year [11]. In the\nU.S., approximately 1.5 million hired workers are employed in agriculture annually, with about three-fourths or 1.1\nmillion working in crop-production activities [12]. One of the most labor-intensive operations in apple production is\nfruitlet thinning, and therefore developing robotic fruitlet thinning solutions is essential for sustainable apple\nproduction in U.S and around the world.\nThe availability of farm labor has continued to decline over the past decade, a situation worsened by the global\npandemic, which led to an estimated loss of $309 million in agricultural production from March 2020 to March 2021\n[13], [14]. On the other hand, global urbanization has been transforming rural areas, causing 68% of the population to\nreside in urban environments by 2050 (United Nations). which would further increase the labor shortage in agriculture.\nOn top of that, labor costs on specialty crop farms in the United States are 3 times higher than the average cost of labor\nin all U.S. farms (USDA, 2018), which further emphasizes the need for automating labor-intensive operations in apple\nand other specialty crop fields.\nIn commercial apple orchards, the demand for labor-intensive fruitlet thinning peaks during the summer months.\nFigure la demonstrates the overcrowding of fruitlet in tree branches as each flower cluster leads to several fruits\nrequiring fruitlet thinning to optimize the number of fruits in each branch based the branch diameter. sunlight, space,\nand water. Figure 1b shows farm workers manually thinning apple clusters, a process that is both time-consuming and\nlaborious, requiring a significant number of seasonal workers.\nAs the U.S. agricultural workforce continues to decline, maintaining competitiveness in the global market\nnecessitates the adoption of labor-saving technologies. Agricultural robots, which can replicate human tasks such as\nfruitlet thinning during the early growing season, offer a promising solution to labor-intensive operations [17]. In\naddition to reducing dependance on manual labor, automated and robotic systems for fruit thinning can also contribute\nto enhancing worker health and safety. Occupational health studies highlight the significant risks associated with\nmanual labor in orchards. For instance, May et al. [18] reveals that farmworkers suffer from a high incidence of\nmusculoskeletal injuries, skin diseases, and other health issues, with the agricultural sector experiencing a fatality rate\nseven times the national average. Further supporting this need, the study by Earle-Richardson et al. [19] documents\nthe detrimental effects of prolonged physical labor on migrant orchard workers, demonstrating significant decreases\nin muscle strength within a single workday.\nThe first crucial step in automating the fruit thinning process is developing a robust vision system capable of\ndetecting green apple fruitlets during the early thinning stage in commercial orchards. Green apple detection is\nfundamentally an object detection challenge, a domain extensively studied within computer vision. Apple detection\nsystems in orchard environments for possible automation have been reported since the late 90s [20]. In the past, many\nstudies have reported apple detection models for supporting robotic harvesting using traditional image processing\ntechniques such as image color segmentation in RGB, HIS (hue, saturation, and intensity), and/or HSL (hue, saturation,\nluminance) color spaces [21], [22], [23], [24]. However, the accuracy of these traditional methods has been relatively\nlow, and the approach is limited to a specific environmental condition such as uniform lighting and background\nconditions created artificially in orchards. It is challenging to implement these image processing algorithms to detect\napples in a natural environment with high level of occlusion, fluctuating illumination, and complex backgrounds[25].\nDuring the last few years, machine learning (ML) has become widely adopted for object detection in agriculture,\nproviding more robust detection capabilities with reduced level of image pre-processing and feature extraction [26].\nAdditionally, deep learning (DL)-based models have become one of the most effective methods for performing\nobject detection in agriculture as it provides end-to-end processing capability without any manual feature extraction.\nTo perform apple detection in orchards, researchers have implemented DL models such as Faster R-CNN models\nbased on AlexNet and based on ResNet101 [29]. More recently, Kang et al. [30] have purposed a deep-learning-based\ndetector called 'LedNet', which is able to perform real-time and accurate apple detection in orchards. The LedNet\narchitecture utilizes the 3-levels feature pyramid network (FPN) and Atrous Spatial Pyramid Pooling (ASPP), a\nsemantic segmentation module used in the feature processing block of the DL module.\nAlthough researchers have been successful in detecting mature apples for robotic harvesting, there are only a few\nreported studies on the detection of apples during their early growing season for robotic thinning applications. During\nthe early growing season of apples in the natural environment, the color of apples and light reflection is similar to that\nof leaves, which poses a great challenge in accurately detecting apple fruitlet. Additionally, factors such as occlusion\nand overlap of fruits with leaves and branches, and uncertain illumination make it difficult to detect fruitlet effectively\n[31], [32].\nXia et al. [33] used improved Hough transform algorithm and SVM to detect green apples in a natural environment\nand reported an F1 score of 90.3%. The author used an iterative threshold segmentation (ITS) algorithm to detect\nregions of interest (ROI) that contained potential apple fruitlet pixels. However, this technique was limited to detecting\nonly non-overlapping apples. Recently, to detect young apples, Tian et. al [34] implemented a YOLOv3 model called\n'Densenet' and reported an F1 score of 83.2%. Additionally, to detect apples, Huang et al. [35] recently used an\nimproved YOLOv3 model based on the CSPDarknet53 DL network. The study reported an inference speed of 8.6 ms\nin detecting immature apples. However, F1 and mAP achieved by the proposed model were only 0.65 and 0.67. Most\nrecently, Wang at al. [36] presented a YOLOv5s-based deep learning technique to detect apple fruitlet before thinning\nactivity in the natural environment. The author of this study implemented transfer learning through a channel pruned\nYOLOv5s object detection model and fine-tuned the model to achieve a higher detection rate. However, the model\nsize implemented in this study was very small (1.4MB) and the author reported a false detection of 4.2%.\nRecent advancements in deep learning models for object detection have significantly enhanced fruitlet detection\nperformance in field environments. The You Only Look Once (YOLO) framework, modified for specific agricultural\nneeds, has shown promising results. For instance, [40] reported that a tailored YOLO model improved the detection\nof green citrus in complex orchard environments with a substantially better accuracy and speed compared to YOLO\nv4 [37]. Similarly, a lightweight version of YOLOv8 was developed for detecting pomegranate fruitlets, achieving\nhigh precision with reduced model complexity, making it suitable for mobile devices [38]. Additionally, a channel-\npruned YOLO V5s model effectively detected apple fruitlets under varying conditions, outperforming several methods\nwhile maintaining a compact size for potential use in mobile fruit thinning terminals[39].\nSimilarly, an adaptation of the YOLOX-m model, optimized for robust feature extraction and enhanced by a feature\nfusion pyramid network with an Atrous Spatial Pyramid Pooling (ASPP) module, has shown improvements in\ndetecting fruitlets. The ASPP module-based network achieved an average precision of 64.3% for apples and 74.7%\nfor persimmons, outperforming several common detection models and providing a solid reference for diverse fruit and\nvegetable detection tasks [40]. Similarly, the YOLO-P model, a modified version of YOLOv5 designed specifically\nfor pear detection in orchards, incorporated advanced architectural changes such as shuffle blocks and a convolutional\nblock attention module (CBAN). These modifications improved feature extraction capabilities, allowing the model to\nperform well under challenging environmental conditions including complex backgrounds and varied lighting. This\nmodel not only supports rapid and precise pear detection but also offers insights for enhancing fruit detection systems\nin similar unstructured environments [41].\nMoreover, a YOLO v4-based method specifically tailored for fig detection in dense foliage has shown\nimprovements over previous models including Faster R-CNN, emphasizing YOLO's capability in challenging\nconditions [42]. YOLOv8 has also been successfully applied to size immature green apples using geometric shape\nfitting techniques, showcasing high precision and robustness against occlusions in orchard environments [4].\nFurthermore, an enhanced YOLOv8 model has proven to be effective for detecting Yunnan Xiaomila peppers,\nintegrating attention mechanisms and deformable convolutions to handle small targets against complex backgrounds\n[43]. Additionally, modifications to YOLOv5 have improved plum recognition, adapting the algorithm to handle fruit\nocclusions and environmental variability [44]. Another study with YOLOv7-based model equipped with attention\nmechanisms and specialized pooling has shown to achieve high precision in detecting plums in natural settings [45].\nIn orchard automation, YOLO object detection models have been pivotal in enhancing the accuracy and efficiency of\nfruit detection [4], [46], [47], flower identification [48], [49], [50], and automated harvesting processes [51], [52],\n[53]. These models adeptly identify and classify fruits at various stages of ripeness, detect flowers with high precision,\nand facilitate efficient harvesting operations. The development of YOLO models has introduced significant\nimprovements specifically tailored to meet the challenges faced in agricultural environments. For instance, the\nintroduction of multi-scale predictions in YOLOv5 has improved the detection of small and clustered objects like\nflowers and young fruits, which are crucial during the early stages of crop yield management [54].\nThe rapid advancement of YOLO-based models has significantly enhanced their precision and processing speeds, vital\nfor implementing robotic solutions such as fruitlet thinning in orchards. Given these swift innovations, continuous\nevaluation of newer models is essential to harness these improvements for agricultural automation."}, {"title": "2. YOLO Background and Overview: Insights into YOLOv8, YOLOv9,\nand YOLOv10 Models", "content": "Figure 2a shows the timeline history of the YOLO from its release to upto date version as YOLOv10. The YOLO\nobject detection algorithm was first introduced by Joseph Redmon et al. [55] in 2015, revolutionized real-time object\ndetection by combining region proposal and classification into a single neural network, significantly reducing\ncomputation time. YOLO's unified architecture divides the image into a grid, predicting bounding boxes and class\nprobabilities directly for each cell, enabling end-to-end learning [55]. YOLO is versatile, and its real-time detection\ncapabilities have revolutionized not only agriculture [56], but also many other field such as medical object detection\n[57], autonomous vehicle industry[58], security and surveillance systems [59], and industrial manufacturing [60]\nwhere accuracy and speed are paramount. The current state-of-the-art iteration of YOLO version is YOLOv10, and\nFigure 2b shows the FPS and mAP of each model starting from YOLOv1 to YOLOv10.\nAfter the release of first YOLO version, YOLOv2, or YOLO9000 [61], [62], expanded on this foundation by\nimproving the resolution at which the system operated and by being capable of detecting over 9000 object categories,\nthus enhancing its versatility and accuracy. YOLOv3 further advanced these capabilities by implementing multi-scale\npredictions and a deeper network architecture, which allowed better detection of smaller objects [63]. The series\ncontinued to evolve with YOLOv4 and YOLOv5, each introducing more refined techniques and optimizations to\nimprove detection performance (i.e., accuracy and speed) even further [64], [65]. YOLOv4 incorporated features like\nCross-Stage Partial (CSP) connections and Mosaic data augmentation, while YOLOv5, developed by Ultralytics,\nbrought significant improvements in terms of ease of use and performance, establishing itself as a popular choice in\nthe computer vision community. Subsequent versions, YOLOv6 through YOLOv10, have continued to build on this\nsuccess, focusing on enhancing model scalability, reducing computational demands, and improving real-time\nperformance metrics . Each iteration of the YOLO series has set new benchmarks for object detection capabilities and\nsignificantly impacted various application areas, from autonomous driving and traffic monitoring to healthcare and\nindustrial automation..\nStarting with YOLOv1, the foundational model introduced significant capabilities with an mAP of 63.4%, although it\nexperienced higher latencies[64] .This was followed by YOLOv2 and YOLOv3, which further improved detection\naccuracy, achieving mAPs of 76.8% and 57.9%, respectively, at the cost of increased latency. YOLOv4 continued this\ntrend, reaching an mAP of 43.5% and serving as a bridge to more refined models. YOLOv5 emerged as a popular\nchoice, balancing performance and efficiency with a competitive mAP of 50.7% and a latency of 140 ms [66]. The\nYOLOv6 series, including variants from YOLOv6-N to YOLOv6-L, offered mAP scores ranging from 37.0% to\n51.8%, with moderate latencies, marking a significant milestone in optimizing detection speed and accuracy [67].\nLastly, YOLOv7, including the YOLOv7-tiny and standard YOLOv7 models, achieved mAPs of 56.4% and 51.2%\nrespectively, but with significantly higher latencies, indicating a shift towards prioritizing accuracy over speed.\nLikewise, YOLOv8 demonstrates commendable performance with mAP scores ranging from 37.3% to 53.9%, and\nlatencies between 6.16 ms to 16.86 ms. While it made significant strides, YOLOv8 falls slightly short in terms of\nefficiency and accuracy when compared to its successors. Progressing to YOLOv9 [68], this iteration includes models\nlike YOLOv9-N, YOLOv9-S, YOLOv9-M, YOLOv9-C, and YOLOv9-X, which achieve mAP scores from 39.5% to\n54.4% [68]. Although YOLOv9 matches YOLOv10 in top mAP scores, its latency figures, particularly for YOLOv9-\nX, are higher, reflecting lesser efficiency than YOLOv10. This discrepancy highlights YOLOv10's advancements in\nbalancing high accuracy with reduced computational demands. The latest in the lineup, YOLOv10 [69], introduces a\nrange of variants: YOLOv10-N, YOLOv10-S, YOLOv10-M, YOLOv10-B, YOLOv10-L, and YOLOv10-X offering\nprecision scores from 38.5% to 54.4% on the MS-COCO dataset. Notably, YOLOv10-N and YOLOv10-S boast the\nlowest latencies at 1.84 ms and 2.49 ms, respectively, optimizing them for real-time applications. YOLOv10-X not\nonly achieves the highest mAP of 54.4% but does so with a remarkably low latency of 10.70 ms, demonstrating a\nsignificant leap in enhancing both accuracy and inference speed over previous models [69].\nYOLOv8 introduced enhancements to its backbone network with an upgraded version of the Darknet architecture,\nwhich improved computational efficiency and feature extraction. This version also added layers and refined the loss\nfunctions to handle class imbalances and stabilize training outcomes, making it suitable for real-time applications with\nstrict latency demands [67].\nYOLOv9 stands out in the realm of real-time object detection with its strategic architectural innovations aimed at\novercoming the challenges of information loss in deep neural networks, a critical factor in maintaining detection\naccuracy and system efficiency. Central to YOLOv9's design is the implementation of the Information Bottleneck\nPrinciple and Programmable Gradient Information (PGI), which ensure that crucial data is preserved throughout the\nlayers of the network. This approach facilitates better model convergence by maintaining reliable gradient generation\nacross the network's depth. Further enhancing YOLOv9's capabilities is the integration of Generalized Efficient Layer\nAggregation Network (GELAN) and reversible functions. GELAN optimizes the model's computational blocks for\nflexible performance across various applications, boosting both parameter utilization and efficiency. Meanwhile,\nreversible functions address the core issue of information degradation in deep learning architectures; these functions\nallow the model to invert transformations without any loss, preserving integral data necessary for accurate object\ndetection. This property is especially beneficial in ensuring that YOLOv9 maintains complete information flow,\ncrucial for precise updates to model parameters and effective learning."}, {"title": "3. Methods", "content": "This study was conducted across commercial orchards in Pullman, Prosser, and Naches, Washington State, USA, over\nthe years 2022 to 2024. It focused on evaluating datasets of green apples, featuring varieties such as Scifresh, Scilate,\nCosmic Crisp, Honeycrisp, and Golden Delicious. RGB images for this study were captured using IntelRealsense\nmachine vision camera and were manually labeled to prepare the constant dataset for training of each configuration\nacross YOLOv8, YOLOv9, and YOLOv10 models. In total, 17 model configurations were examined: 5 for YOLOv8,\n6 for YOLOv9, and 6 for YOLOv10. All model configurations were trained under standardized hyperparameter\nsettings on the same computational system to ensure consistency and comparability. Validation of these models\ninvolved additional RGB images captured from various orchards using an Apple iPhone 14 smartphone.\nThis validation process aimed to assess each model's fruit counting accuracy against manually counted ground truths,\nwhich included scenarios with occluded apples. Furthermore, this validation step was designed to rigorously test the\nhighest-performing models in terms of speed and accuracy using images from different sensors. The details of this\ncomprehensive methodology are illustrated in Figure 4, providing a visual reference for the experimental setup and\ndata collection approach employed in this study.\n3.1 Study Site and Data Acquisition\nThe primary data collection for this study was conducted at Allan Brothers Orchard in Prosser, Washington State,\nUSA, focusing on datasets of immature green apples for training models. RGB images were acquired using an Intel\nRealSense 435i camera during June 2024. For validation, additional datasets were collected from different locations\nand times: Cosmic Crisp apple images from the ROZA experiment station at WSU IAREC in Prosser, Honeycrisp\napple images from an orchard in Naches owned by Allan Brothers Fruit Company, and Golden Delicious apple images\nfrom Pullman, collected on June 14, 2022. Furthermore, validation of apple counts using the Intel RealSense camera\nand Microsoft Azure camera was conducted in June 2023 and June 2022 respectively. All images were captured prior\nto the manual thinning process conducted by orchard workers. The details of each machine vision sensor used in data\ncollection of this study are:\nIntel RealSense D435i: This camera features a 2-megapixel RGB sensor capable of capturing high-quality images.\nOperating at a resolution of 1280\u00d7720 pixels, it provides a comprehensive view with a 69.4\u00b0 horizontal and 42.5\u00b0\nvertical field-of-view, ensuring broad coverage in various environments. The Intel RealSense D435i is designed to be\ncompact and lightweight, making it highly effective for capturing RGB data in diverse settings.\nMicrosoft Azure Kinect DK: Equipped with a 12-megapixel color camera, this sensor delivers exceptional image\nclarity and detail. The RGB camera component of the Azure Kinect DK sensor excels in a range of lighting conditions,\nenhancing its utility in natural orchard environments where lighting can vary. The device's high-resolution imaging\ncapabilities make it ideal for detailed visual data collection in agricultural research.\nApple Iphone14: The Apple iPhone 14 features an advanced 12-megapixel RGB camera that excels in capturing high-\nresolution images with remarkable clarity and color accuracy. Its sophisticated sensor and computational photography\ncapabilities enable detailed and vibrant image capture, making it ideally suited for collecting visual data in various\nlighting conditions, crucial for applications such as agricultural monitoring and research.\n3.2 Data Preparation and Model Training\nA total of 1,147 images were manually annotated with bounding boxes using the online labeling platform provided by\nRoboflow. These images were allocated into training and validation at the ration of 8:2 respectively, facilitated by\nRoboflow's distribution tools. No image preprocessing steps were undertaken in this study, as the objective was not to\nenhance any specific model but rather to compare and evaluate the performance of these models using raw data\ncollected in natural orchard settings.\nThe computational analyses for this study were conducted on a high-performance workstation equipped with an Intel\nXeon(R) W-2155 CPU, featuring a base clock speed of 3.30 GHz across 20 cores. This setup provided substantial\nprocessing power necessary for handling intensive data processing tasks. The workstation was also outfitted with\nNVIDIA Corporation GP102 [TITAN Xp] graphics cards, enhancing its ability to perform complex image processing\nand machine learning tasks efficiently. The system included a substantial storage capacity of 7.0 TB, facilitating\nextensive data management and analysis. It operated under Ubuntu 20.04.6 LTS, a robust and stable 64-bit operating\nsystem, using GNOME version 3.36.8 for its graphical interface and X11 as its windowing system.\nThe analysis of YOLOv8, YOLOv9, and YOLOv10 encompassed a total of 17 configurations (five for YOLOv8, six\nfor YOLOv9, and six for YOLOv10). Each model configuration was rigorously tested for precision, recall, mAP@0.5,\nand image processing speed. For the training of all models, a consistent configuration was rigorously maintained to\nensure uniformity and comparability across experiments. Each model was trained for 700 epochs, reflecting a\nsubstantial duration to adequately learn and adapt to the dataset's complexities. The batch size was set at 8, optimizing\nthe balance between memory usage and processing speed. Images were resized to a uniform resolution of 640x640\npixels to standardize the input data size across all models. The Stochastic Gradient Descent (SGD) optimizer was\nemployed to update model weights effectively, favored for its efficiency in handling large-scale and complex data.\nAdditionally, the configuration threshold for confidence was set at 0.25, and the Intersection over Union (IoU)\nthreshold was maintained at 0.7, criteria chosen to optimize the balance between precision and recall during object\ndetection tasks.\nThe hyperparameter settings outlined in table 1 provide a detailed framework for optimizing the training of YOLO\nmodels in the study. Key parameters such as the initial and final learning rates were set at 0.01, facilitating a controlled\nadjustment of learning throughout the training process. Momentum was maintained at 0.937 to ensure consistent\nupdates across epochs, while a minimal weight decay of 0.0005 helped prevent overfitting. The training initiated with\na warmup phase spanning 3 epochs to stabilize the learning parameters early in the training. Loss adjustments were\nspecifically tuned, with box loss, class loss, and definition loss set at 7.5, 0.5, and 1.5 respectively, to balance the\ncontributions of different components of the loss function. Image augmentation techniques such as hue, saturation,\nand value adjustments were precisely defined to enhance model robustness under varied lighting conditions typically\nfound in orchard environments. Specific settings for geometric transformations like rotation, translation, and scaling\nwere employed to simulate different orientations and sizes of objects, crucial for improving the model's ability to\ngeneralize across diverse scenarios. The table also highlights the use of flipping and mosaic data augmentation to\nfurther enrich the training dataset, ensuring comprehensive exposure to potential real-world variations."}, {"title": "3.3 Performance Evaluation", "content": "To evaluate the detection capabilities of each configuration of YOLOv8, YOLOv9, and YOLOv10, the metrics of\nprecision, recall, and mean Average Precision at Intersection over Union (IoU) threshold of 0.50 (mAP@50) were\nemployed. Precision was calculated as the ratio of true positive detections to the total predicted positives, given by\nEquation 1, where TP denotes true positives and FP denotes false positives. Recall measured the ratio of true positive\ndetections to the actual positives, formulated as Equation 2 where FN represents false negatives. The mAP@50 was\ndetermined by averaging the precision across all recall levels for an IoU > 0.50. Additionally, the image processing\nspeed for each model was analyzed and compared across three categories: preprocessing, inference, and\npostprocessing. These evaluations were systematically conducted across 17 configurations of the YOLO models:\nYOLOv8m, YOLOv8s, YOLOv81, YOLOv8x, YOLOv8c for YOLOv8; YOLOv9 Gelan-e, YOLOv9 Gelan-c,\nYOLOv9 Gelan-s, YOLOv9 Gelan-t, YOLOv9 Gelan-m, and YOLOv9 Gelan for YOLOv9, and YOLOv10m,\nYOLOv10s, YOLOv101, YOLOv10x, YOLOv10c, YOLOv10d for YOLOv10, to evaluate their efficiency in detecting\nfruitlets in commercial orchards.\nPrecision = $\\frac{TP}{TP + FP}$\nEquation 1\nRecall = $\\frac{TP}{TP + FN}$\nEquation 2\nAdditionally, the assessment involved analyzing preprocessing, inference, and postprocessing speeds for each model\nconfiguration, as these metrics are critical for real-time object detection systems. Preprocessing speed determines how\nquickly a model can prepare images for detection, inference speed measures the time taken to identify objects within\nimages, and postprocessing speed reflects how swiftly the model finalizes the outputs after detection. Each of these\nstages is essential for efficient operation in agricultural applications, where timely and accurate detection can\nsignificantly impact decision-making and resource management. The computational efficiency of these models\ndirectly influences their practical utility in automated fruit detection systems, making this evaluation crucial for\nadvancing agricultural technology solutions."}, {"title": "3.4 In-Field Counting Validation", "content": "Upon evaluating the performance metrics of each YOLOv8, YOLOv9, and YOLOv10 configuration for detecting\nfruitlets, the most effective model from each version was selected based on the highest accuracy achieved at\nmAP@0.5. These top-performing models from YOLOv8, YOLOv9, and YOLOv10 were further validated to assess\ntheir detection capabilities in a real commercial orchard setting across five distinct apple varieties in Washington State.\nThis validation process utilized images collected both by smartphone and machine vision sensors. Initially, images of\nScifresh, Scilate, Cosmic Crisp, and Honeycrisp apples were captured using an iPhone 14. A total of 128 images, 32\nper variety, were analyzed to evaluate the models' counting accuracy before the thinning process.\nSubsequently, an additional set of images was used for further validation. This included 32 images of Golden Delicious\napples captured using a Microsoft Azure camera, and another 32 images of Scifresh apples taken with an\nIntelRealsense camera. Notably, while the IntelRealsense camera images of Scifresh and Scilate apples served as the\ntraining dataset, validation focused on varieties like Golden Delicious, Cosmic Crisp, and Honeycrisp, which were\nnot included in the training phase. For each image, a meticulous count of all visible and occluded apples was manually\nperformed directly in the field. This comprehensive manual counting provided a precise ground truth for each image\nsample across the different apple varieties, ensuring robust validation of the models' performance."}, {"title": "4. Results and Discussion", "content": "The evaluation of performance across different YOLO model configurations: YOLOv8, YOLOv9, and YOLOv10 is\ncomprehensively illustrated in Figures 5, 6, and 7, respectively. Figure 5a presents an original image showcasing green\napples in a complex orchard environment characterized by shadow and partial sunlight, set against an all-green\nbackground. This setting represents a challenging scenario for object detection due to the camouflaging color palette\nand varying light conditions. Figure 5b details the performance of the YOLOv81 model configuration, where a false\ndetection has occurred. Notably, a double bounding box is erroneously placed over a region obscured by trellis wire\nand leaves, mistakenly identified as fruitlets. This misidentification is highlighted by a yellow circle, indicating the\narea of false positive detection. Figure 5c illustrates the results from the YOLOv8m configuration, which also resulted\nin a double detection over a single green apple. This overprediction is marked by a yellow dotted circle, showcasing\nthe model's sensitivity to overlapping features within the detection area.\nFigure 5d captures the detection outcomes from the YOLOv8x model, which successfully identified five apples\naccurately in the described complex conditions. This model's effectiveness in handling the intricacies of the\nenvironment underscores its robustness. Figures 5e and 5f depict the performances of the YOLOv8n and YOLOv8s\nconfigurations, respectively. Both configurations have correctly identified the green apples, demonstrating their\ncapability to accurately detect objects without the interference observed in other configurations."}, {"title": "4.1 Assessment of Detection Accuracy: Precision and Recall Metrics", "content": "The comparative analysis of detection accuracy across different YOLO configurations, as illustrated in Figure 8,\nunderscores the nuanced performance variations in precision and recall metrics. Notably, YOLOv9 configurations\noutperformed others, with YOLOv9 Gelan-c achieving the highest precision (0.903), indicating exceptional accuracy\nin fruitlet detection while effectively minimizing the misidentification of non-fruits. Conversely, YOLOv9 Gelan-e\nexhibited the highest recall (0.891), highlighting its ability to capture a larger proportion of true positive samples"}]}]}