{"title": "Paradigms of AI Evaluation: Mapping Goals, Methodologies and Culture", "authors": ["John Burden", "Marko Te\u0161i\u0107", "Lorenzo Pacchiardi", "Jos\u00e9 Hern\u00e1ndez-Orallo"], "abstract": "Research in AI evaluation has grown increasingly complex and multidisciplinary, attracting researchers with diverse backgrounds and objectives. As a result, divergent evaluation paradigms have emerged, often developing in isolation, adopting conflicting terminologies, and overlooking each other's contributions. This fragmentation has led to insular research trajectories and communication barriers both among different paradigms and with the general public, contributing to unmet expectations for deployed AI systems. To help bridge this insularity, in this paper we survey recent work in the AI evaluation landscape and identify six main paradigms. We characterise major recent contributions within each paradigm across key dimensions related to their goals, methodologies and research cultures. By clarifying the unique combination of questions and approaches associated with each paradigm, we aim to increase awareness of the breadth of current evaluation approaches and foster cross-pollination between different paradigms. We also identify potential gaps in the field to inspire future research directions.", "sections": [{"title": "Introduction", "content": "In recent years, Artificial Intelligence (AI) has advanced rapidly and gained public prominence. In particular, general-purpose AI systems, such as Large Language Models (LLMs), have become widely deployed for real-world applications across various sectors [Maliugina, 2024]. As AI adoption grows, so does the need to understand AI systems capabilities [Hern\u00e1ndez-Orallo, 2017] and the risks they pose [Hendrycks et al., 2021b] to ensure responsible deployment. At the same time, the technical expertise required to effectively use state-of-the-art AI models has decreased, enabling a broader range of researchers and practitioners to engage in AI evaluation. This has brought new perspectives to the field, with evaluators from a multitude of disciplines beyond AI, including Cognitive Science, Psychology, Economics, Social Sciences, Software and Safety Engineering contributing to the diversity of methodologies and insights.\nWhile the influx of varied expertise is essential to evaluating the complex and multifaceted impact of modern AI systems on society, it has created a fragmented AI evaluation landscape, marked by a lack of standardisation and limited cross-collaboration. As a result, research efforts have become insular and fail to incorporate advancements from other communities. Communication challenges arise, with key terms often conveying distinct connotations in different communities. For instance, AI developers and regulators rely on evaluations to determine if a system is safe to deploy [Phuong et al., 2024]. In contrast, AI adopters are more concerned with understanding if a system can automate specific tasks within their organisation [Asgari et al., 2024]. Both communities, however, refer to \"capability evaluations\".\nBeyond scientific research and industry practices, under-"}, {"title": "1.1 Scope", "content": "For our analysis, we define AI evaluation as the process of measuring and anticipating the behavioural properties of AI systems and their societal impact to inform decisions about their use. In our survey, we consider work on the evaluation of any kind of AI system, component or algorithm. However, we interpret our definition as excluding explainability and mechanistic interpretability, which aim to get an understanding of the inner workings of AI systems. While such an understanding may be used to anticipate its behaviour [Casper et al., 2024], we decide to concentrate on work directly measuring the behavioural properties. We refer interested readers to existing high-quality surveys on interpretability and explainability [Yang et al., 2023b; Bereska and Gavves, 2024]. Moreover, we do not consider purely theoretical papers presenting conceptual frameworks for evaluation [Bengio et al., 2024], methodologies to reduce the computational cost of existing evaluations [Kaplan et al., 2020; Polo et al., 2024] and work studying how humans react or think about AI systems [Steyvers et al., 2025]."}, {"title": "1.2 Previous Analyses of AI Evaluation", "content": "Several recent surveys have explored AI evaluation from specific perspectives. Many of these are focused on LLMs, overviewing benchmarking [Chang et al., 2024] or red-teaming [Lin et al., 2025], studying how ethical aspects are evaluated [Lyu and Du, 2025] or analysing the literature under a verification and validation perspective [Huang et al., 2024]. Other surveys include Rauh et al. [2024], which focuses on the safety of generative AI systems, and G\u00f6llner and Tropmann-Frick [2023], which considers responsible AI principles. In contrast, our work focuses on a wide variety of AI systems and evaluation approaches.\nOther work discussed the quality of evaluation practice and instruments [Hern\u00e1ndez-Orallo, 2020; Burden, 2024], for instance touching on reproducibility [Burnell et al., 2023], statistical rigour [Gorman and Bedrick, 2019], validity [Subramonian et al., 2023] and representativeness and fairness [Bergman et al., 2023; G\u00f6llner and Tropmann-Frick, 2023]. We do not focus on these issues here, as they apply broadly to all AI evaluation tools.\nBieger et al. [2016] proposed a normative framework for evaluating adaptive general-purpose AI, outlining the purposes of evaluation, the properties to be measured, and the challenges involved. In contrast, we take a descriptive approach to AI evaluation, mapping the landscape by identifying the advantages and limitations of different methodologies without prescribing any particular approach. More recently, Cohn et al. [2022] defined different \"facets\" of evaluation instruments. While there is some overlap with our dimensions of analysis, several of their facets refer to validity and consistency, properties that are broadly relevant across all paradigms and are hence excluded from our analysis. While Cohn et al. [2022] apply their framework to 23 evaluation works, neither Bieger et al. [2016] nor Cohn et al. [2022] identify distinct paradigms of evaluation or characterise their defining features. Further, these works do not survey the larger and rapidly evolving AI evaluation landscape shaped by recent advancements in AI."}, {"title": "2 Dimensions of Analysis", "content": "We explore goals, methodologies and cultures in AI evaluation. These three factors shape the way evaluations are designed, applied, and interpreted, allowing us to highlight the diversity of approaches, clarify underlying assumptions, and identify gaps across different evaluation paradigms.\nWe break down these three factors into key dimensions (each with a discrete set of possible values) that we use to map the landscape of AI evaluation. In some cases, a single work may be assigned multiple values for the same dimension. For example, an evaluation may assess both performance and safety (two distinct values within the Indicator dimension) using the same tasks. Fig. 1 shows the clustering of surveyed papers based on these dimensions and highlights the relationship between these clusters and assigned paradigms."}, {"title": "2.1 The Goals", "content": "Evaluation can be marked by the type of insight sought. We make use of the following dimensions:\nIndicator (performance, fairness, safety, robustness and reliability, behavioural features, cost). One possible goal of AI evaluation is to determine the performance of an AI system, namely, its ability to successfully complete tasks. Another important consideration is fairness: the extent to which demographic groups are treated differently by the AI system. Alternatively, safety encompasses concerns such as preventing the generation of manipulative or toxic content, mitigating harmful outcomes whether explicitly prompted by a malicious actor or arising from the system's design and behaviour (alignment). Other evaluation tools focus on the robustness and reliability of an AI system, that is, the extent to which its behaviour is affected by factors that are unrelated to the task at hand and how the system fails in presence of anomalies and edge cases. Some evaluations analyse other behavioural features (such as preferences, tendencies, reasoning patterns, etc.) of models in response to inputs. Finally, the cost of using a system (whether monetary, environmental, or ethical) can also be a primary indicator used to evaluate a system.\nDistribution summarisation (aggregate, extreme, functional, manual inspection). Ideally, AI evaluation would fully describe the distribution of subject behaviour conditioned on all possible input values. Yet, in practice, many approaches are limited to reporting summary statistics. Most typical are aggregate metrics over a set of inputs (e.g., mean accuracy in benchmarks). Alternatively, the extremes of the distribution over a set of inputs are sometimes reported. This includes both worst-case (e.g., the possibility of accidents) and best-case scenarios (e.g., best performance over a set of prompts). More refined is a functional description mapping variations in the input (such as task difficulty) to statistics of the conditional distribution of behaviour. This can allow for anticipation and explanation of behaviour (e.g., representing performance as a function of task difficulty). In contrast, a few works instead perform a manual inspection of the system's behaviour (e.g., describing performance failures in a qualitative manner) rather than summarising the distribution.\nSubject (system, component, algorithm). AI evaluation often focuses on self-contained systems that can function without additional components or adaptations (e.g., ChatGPT, a"}, {"title": "2.2 The Methodologies", "content": "A methodology is a collection of practices, principles, and guidelines used to conduct an evaluation. We consider the following dimensions as key factors for categorising methodologies:\nMeasurement (observations, constructs). Evaluations can report direct observations from measurement instruments (e.g., 'System X achieved a score of x% on a benchmark') or explicitly model latent constructs\u2014underlying factors that explain an Al system's observed behaviour (e.g., 'System X demonstrates low arithmetic capability').\nTask origin (operation, sample, design). Al systems can be evaluated in real-world operation (e.g., testing an autonomous vehicle on public roads or assessing whether an LLM improves productivity in a study involving human workers). Alternatively, evaluation can be conducted using a sample of tasks drawn from a distribution representative of real-world usage (e.g., a sample of journeys). In some cases, tasks may be created by design (e.g., on a circuit used as a testbed or simulated environment), either because it is difficult, unsafe or unethical to perform evaluation in real-world cases, or because synthetic tasks are thought to enable a more accurate measurement of the desired property.\nProtocol (fixed, procedural generation, adaptive, interactive). The evaluation process may unfold in different ways. A common approach involves testing an AI system on a fixed set of tasks, defined by the initial input provided to the AI system (even if the interaction between subject and evaluation unfolds differently due to either stochastic evaluation environment or an agent's decisions). Some evaluations use procedural generation to create new tasks at test-time for each tested system, following a predefined distribution. In other cases, the generation of tasks or the choice amongst a fixed set of instances is adaptive, chosen based on previous behaviour of the system on other tasks (e.g., adjusting the difficulty of questions in response to its answers to previous questions). Finally, evaluations can be interactive, where humans guide the process in real-time, probing the system's behaviour through sequential interactions.\nReference (objective, rubric, subjective, no reference). In some cases, the AI system's outputs are compared to an objective value, such as a gold standard correct answer. In other cases, a rubric is applied, either by a human or an AI scorer, to assess outputs that cannot be objectively compared to a determined value. Alternatively, evaluation may rely on subjective feedback or judgement (preferences, moral values, etc.) provided by human users at test time or encoded into an automated system (such as a reward model). In some cases, there is no reference answer for comparison."}, {"title": "2.3 The Cultures", "content": "By cultures, we refer to the people involved in the evaluation process, their norms, interests and terminology. We categorise evaluation cultures with the following dimensions:\nEvaluators (researchers, deployers, regulators). Different people may create and conduct an evaluation. We define researchers as those motivated by understanding and improving Al systems from a scientific perspective, focusing on fundamental insights and advancements. In contrast, deployers are primarily concerned with commercial viability, customer satisfaction and competitive advantage; they want to determine whether a system is suitable for deployment, assess pricing strategies, ensure safety and enhance brand reputation. Finally, regulators seek guarantees and information on safety, ethics, risks to society, and legal compliance. Our definition considers employees of private companies or regulators conducting research for scientific understanding as researchers.\nMotivation (comparison, understanding, assurance). Motivation determines focal points for the evaluation, how the results are interpreted, and the suite of techniques it leverages. System comparison aims to determine the most suitable subject from a set of candidates for a given scenario, assess whether systems can replace or assist humans, and track the progress of successive generations. Another reason for the development of an evaluation tool is seeking understanding of what causes AI systems to behave in the way they do. This could be via learning to recognise the cognitive processes the Al systems exhibits, or identifying what aspects of a specific input lead to certain types of behaviour. Finally, evaluation may aim to provide assurance, determining the conditions of reliable operation or a bound on undesirable behaviour, either using empirical methods or formal proofs.\nDiscipline (AI, Psychology, Security, Economics, etc.). The people developing and conducting an evaluation may belong to different disciplines, such as Al, psychology, security, economics or others. Their different cultural perspectives and scholarly norms may reflect differences in the methods employed for the evaluation."}, {"title": "3 Evaluation Paradigms", "content": "Based on our survey, we identified six main paradigms of AI evaluation. In the following subsections we describe each paradigm, leveraging insights from our annotation exercise to operationalise the distinctions between them. In Table 1, we present the most common values for each dimension across paradigms. Where possible, we have chosen names for these paradigms in line with community convention."}, {"title": "3.1 Benchmarking Paradigm", "content": "Benchmarking can be traced back to the Common Task Framework of the 1980s [Koch and Peterson, 2024]. A key tenet of benchmarking is that by providing constant test conditions, any variation in a system's responses compared to other systems can be attributed to its intrinsic characteristics. Benchmarking, then, involves evaluating AI systems by testing them on standardised sets\u2014or distributions of instances\u2014and summarising and comparing their performance using different aggregate measures. The focus is typically on evaluating the observed performance of a system or component (although benchmarks also frequently test for fairness, safety and robustness). Benchmarks are often marked against an objective reference, with identification (e.g., multiple-choice questions) being a common task mode. They are widely applied across a wide range of AI systems, including LLMs, RL agents, image classifiers, and more. Benchmarks are inherently built in order to compare systems, providing a tool for selecting the most appropriate AI systems or tracking progress. Archetypal examples in the Benchmarking Paradigm include ImageNet [Deng et al., 2009], the Arcade Learning Environment [Bellemare et al., 2013] and MMLU [Hendrycks et al., 2021a]. Benchmarks also arise in the form of competitions (e.g., the annual RoboCup competition [Kitano et al., 1997]) where entrants compete to demonstrate competence at a particular task.\nBenchmarking offers several inherent advantages. First, the use of a standardised test provides a level playing field for all systems being evaluated, while the use of well-defined, objective metrics reduces the ambiguity and allows easy tracking of performance indicators over time. Benchmarks are often publicly available, increasing the transparency of the evaluation process; this can be further improved by reporting the results at the instance level [Burnell et al., 2023]. However, the benchmarking paradigm also has notable limitations. Open sharing of benchmarks can lead to data contamination [Zhou et al., 2023], with test data being used for training. Relatedly, repeated testing on the same benchmark may induce overfitting, optimising for specific tests rather than generalised to broader tasks [Fang et al., 2023]. Further, benchmark results are inherently tied to a particular distribution of test items, limiting the generalisability of the measurement [Raji et al., 2021]. Further discussions on the limitations of benchmarking can be found in Liao et al. [2021]."}, {"title": "3.2 Evals Paradigm", "content": "The Evals Paradigm focuses on system safety, often operationalised as a system's failure to comply with safety specifications or its tendency to exhibit harmful behaviour during tasks that carry risks (e.g., the extent to which it demonstrates so-called \u201cdangerous capabilities\u201d [Shevlane et al., 2023; Phuong et al., 2024]), although fairness is sometimes considered too. This Paradigm often employs extremes analysis, identifying specific (worst) cases. The aim is providing assurance on a system's safety or gaining understanding on what causes the unsafe behaviour. A common methodology within the Evals Paradigm is \"red-teaming, an interactive and adversarial process where humans or automated agents attempt to \"break\" or provoke the system to generate undesirable responses. Red-teaming is commonly employed by model deployers (e.g., OpenAI [OpenAI, 2023], Anthropic [Anthropic, 2023]). Generally, tasks are designed in an adversarial way. Evals are predominantly applied to general-purpose AI systems such as LLMs, for which assessing po-"}, {"title": "3.3 Construct-Oriented Paradigm", "content": "The Construct-Oriented Paradigm leverages system responses to quantitatively measure underlying \"constructs\" that describe the system's behaviour at an abstract level. These constructs are typically based on existing theories of cognitive traits or capabilities, but they can also be solely inferred from system responses (e.g., using factor analysis). To measure these latent constructs, this paradigm often employs a functional link with the observed system behaviour and designed tasks carefully controlling for confounding factors, possibly adapting tasks from the cognitive sciences literature. This paradigm primarily aims to gain a better understanding of self-contained systems, particularly their performance. The works in this paradigm are mostly authored by researchers with a background in psychology/cognitive science. Archetypal examples are Guinet et al. [2024], which applies psychometric methods to infer the ability of LLMs to solve 8th-grade mathematics tests, and Momennejad et al. [2024], which proposes a protocol for evaluating cogni-"}, {"title": "3.4 Exploratory Paradigm", "content": "The Exploratory Paradigm begins by forming a hypothesis\u2014often inspired by anecdotal observations about a system's behaviour. Similarly to how psychologists test hypothesis for human and animal cognition, a set of tasks capturing key features\u2014such as reasoning steps, memory requirements or generalisation patterns\u2014is designed to systematically isolate the phenomenon in different scenarios and exclude alternative explanations. The findings arising from testing the system on those tasks, mostly in terms of observed performance and occasionally manual inspection of behavioural features, are combined (and, in some cases, compared to humans) to provide evidence supporting the considered hypothesis qualitatively describing a system's \u201ccognitive processes\u201d. The exploratory approach contrasts with the Construct-Oriented paradigm, which usually relies on pre-established theoretical frameworks that are complemented by the quantitative measurements of constructs. The aim to understand the AI system's cognitive processes is what chiefly distinguishes this paradigm from Benchmarking (which also focuses on observed behaviour, but for the purpose of comparing systems). Here, the AI system's results on tests are only important insofar as they support or refute the hypothesis of interest. Work in this paradigm is chiefly conducted by researchers in Al or psychology/cognitive science and they have been mostly applied to LLMs; archetypal works include The Reversal Curse"}, {"title": "3.5 Real-World Impact Paradigm", "content": "While most evaluation paradigms seek to assess specific properties of Al systems, the Real-World Impact (RWI) Paradigm measures the impact of AI systems when deployed in the real world. This paradigm leverages techniques from the social and clinical sciences by running (randomised controlled) trials where, most often, assistance by an AI system to humans is considered as an \"intervention\" whose effect must be quantified, in terms of a change in aggregate performance on the considered task relative to the system's cost. The goal is therefore comparing different AI systems, or AI-assisted humans to humans alone. Evaluations are generally carried out interactively or on a fixed dataset and, due to the complexity of real-world tasks, human subjective ratings or rubrics may be employed. A common motivation for these evaluations is the comparison of systems situated in the context of real-world applications requiring generation of novel outputs. An archetypal work from this paradigm is Math-Converse [Collins et al., 2024], which investigates the perceived helpfulness of LLMs for mathematics. A second representative work is Si et al. [2025]'s study of LLM's ability to generate novel research ideas; which observes that human reviewers find LLM-generated ideas to be more novel but less feasible than those crafted by humans.\nThe RWI paradigm has a number of advantages: as AI systems become better at complex tasks, the social sciences provide many established methodologies to evaluate their societal impact, which may not be well estimated in artificial scenarios not directly considering user experience. This paradigm has practical challenges as well: in contrast to other paradigms, conducting experiments with human participants in realistic scenarios adds ethical constraints and logistical complexities. Moreover, this type of research mostly operates on a slower timescale compared to AI, making it hard for the RWI paradigm to timely provide information on new systems. These challenges have likely contributed to RWI being a small paradigm so far (it is the least represented in our sample of papers). Nevertheless, we expect this paradigm to grow significantly in the coming years as AI systems become more capable of performing economically valuable tasks."}, {"title": "3.6 TEVV Paradigm", "content": "The Test, Evaluation, Verification, and Validation (TEVV) Paradigm draws on methodologies from formal software verification, with its primary focus on ensuring that AI systems behave in a well-defined and predictable way. TEVV is characterised by focusing on observed extreme values of safety or robustness and reliability measures, with the central goal of assurance\u2014providing bounds or guarantees for a minimum or average level of performance under various conditions. To achieve this, TEVV often explicitly operationalises the constructs it aims to measure, formally defining them to reduce uncertainty of the measurement process. It then employs either designed tasks or operational studies, with a variety of protocols. Works in this paradigm commonly deal with Reinforcement Learning and applied fields such as autonomous driving; Yang et al. [2023a] and Mussot et al. [2024] are archetypal examples of TEVV works in these two fields.\nTEVV offers several advantages, the most notable being its ability to provide formal guarantees and robust safety assurances. However, this approach requires a deep understanding of the system and its operational mechanisms. For many state-of-the-art or general-purpose AI systems, such an understanding is often lacking, making TEVV challenging to apply effectively. Indeed, we found that TEVV was one of the least represented paradigms in recent AI venues."}, {"title": "3.7 Evaluations crossing Paradigm Boundaries", "content": "Unsurprisingly, we found several evaluation papers do not fit neatly into a single paradigm, but instead bridge multiple paradigms and combine their methodologies (see also Fig. 1). For example, Perez et al. [2022]'s model-written evaluations straddle the Benchmarking and Evals paradigms, using LLMs to generate a large and varied set of questions, thus creating a standardised benchmark, that aim to provide safety assurances in the style typical of Evals. Similarly, CogBench [Coda-Forno et al., 2024] bridges the Capability-oriented and Exploratory paradigms. Here, the authors build a cognitive phenotype of LLMs using psychological experiments designed to assess different constructs. Simultaneously, they explore a number of hypotheses based on anecdotal evidence, including whether RLHF makes LLMs more human-like and the relationship between model size and tendency to exhibit human-like behaviour. These hybrid approaches demonstrate the flexibility of AI evaluation methodologies."}, {"title": "4 The Role of the Different Paradigms", "content": "Each of the paradigms we described plays a distinct role within the AI evaluation ecosystem. Each targets a particular type of measurement and fulfils the unique needs of different evaluators. For example, a company developing a safety-critical AI system, such as an autonomous vehicle, would rely on evaluation methodologies that closely follow paradigm to obtain robust guarantees. On the other hand, a deployer of a system in a lower-stakes environment, such as an image classifier for pets, would likely rely on the performance of the system on a benchmark of representative images to determine when the system is ready to deploy. Techniques from multiple paradigms can (and should) be combined when appropriate."}, {"title": "5 Challenges and Opportunities", "content": "Some paradigms, such as Benchmarking, are applied across a wide range of AI systems. However, others tend to be domain-specific: TEVV is mostly applied to embodied, agentic forms of AI, such as RL systems or self-driving cars; similarly, in the papers we surveyed, we found the Evals and Exploratory paradigm were almost exclusively applied to LLMs, although this could be partly due to our focus on recent works. We believe this has occurred due to the way these paradigms emerged in response to specific developments in AI. For example, Evals largely developed as a response to risks from LLMs [Ganguli et al., 2022]. This means that a vast range of existing evaluation approaches remain underutilised in various domains and AI system types. While technically challenging, expanding the applications of different paradigms beyond their typical uses would lead to a more comprehensive understanding of AI systems, their strengths, weaknesses, and broader impacts. While there appears to be growing interest in expanding the range of techniques applied to LLMs, often drawing on methods from TEVV [Huang et al., 2024; OpenAI, 2024], we hope to see this cross-pollination across all domains where AI is evaluated.\nBesides expanding the domain of application of each paradigm, great opportunities lie in developing new evaluations bridging different paradigms, as the examples mentioned in Sec. 3.7. By highlighting the possibilities afforded by the paradigms we identified, we hope to inspire researchers to develop new evaluations leveraging the strengths of multiple paradigms for specific questions, to achieve more comprehensive and insightful assessments. At the same time, as discussed in Sec. 4, using multiple paradigms to tackle an individual question from different perspectives is also a powerful but underexploited strategy.\nWe can integrate our insights with existing discussions on gaps in Al evaluation. For instance, Hutchinson et al. [2022] point out the lack of moral evaluations in AI development raising important questions about data consent, the dignity of data workers, and the social responsibilities of AI developers. These factors were not included in our dimensional analysis due to the widespread lack of reporting on such topics. Similarly, Rauh et al. [2024] identify a \"risk coverage gap\" with many ethical and social risks currently insufficiently addressed. In our framework, evaluations addressing these risks would likely fall under the Real-World Impact Paradigm, which we found to be the least developed. Therefore, Rauh et al. [2024] and our work jointly highlight how this specific niche is unaddressed. Finally, Huang et al. [2024] points out the lack of verifications with provable guarantees for LLMs, which also surfaced from our analysis of the TEVV paradigm. In general, by drawing attention to gaps in the space, we hope to encourage researchers to develop new evaluation methodologies that better address critical issues."}, {"title": "6 Limitations", "content": "We aimed to capture the breadth of the AI evaluation landscape by surveying a highly diverse set of works. Given the extent of the field, our broad scope inevitably limited the depth with which we survey each paradigm. A focused investigation of fewer paradigms might reveal additional patterns or relationships that we could not fully explore. Additionally, our selection of papers was based on our analysis of the recent literature, which may introduce bias. Certain evaluation paradigms, such as Benchmarking, may be overrepresented due to trends in the field, particularly the focus on LLMs. To mitigate this, we ensured that the authors of this paper have extensive AI evaluation expertise and deep familiarity with different areas of the ecosystem. This, combined with seeking out evaluation works intentionally different from one another, mitigated our selection bias as much as reasonably possible.\nAnother constraint lies in how our dimensions capture the nuances of different evaluation techniques. For example, we found the distinction between aggregate and functional distribution summary to be sometimes blurry\u2014there is a fine line between stratifying aggregate performance based on predefined categories (e.g., required capabilities for tasks) and an imprecise functional model. This and other ambiguities can lead to disagreements among raters when assigning dimensions to papers and classifying them into particular paradigms. At the same time, incorporating additional dimensions could offer deeper insights into how different paradigms are characterised and help identify sub-paradigms. Our approach balances granularity of annotation with practical usability. We further found that some dimensions were less informative than anticipated, for example Task Mode (Identification and Generation) was not useful for distinguishing between paradigms where perhaps a more nuanced breakdown of Task Mode would have been. Finally, the AI evaluation landscape is rapidly evolving and new paradigms may emerge or existing ones become more refined. Despite these limitations, we believe the dimensions introduced here provide a valuable foundation for guiding further research in AI evaluation and characterising future works."}, {"title": "7 Conclusion", "content": "This survey presents a snapshot of the current AI evaluation landscape, offering insights into prevailing approaches. We categorised over 125 recent or highly influential AI evaluation papers based on our multi-dimensional framework examining goals, methodologies, and research cultures. Through this analysis, we identified six distinct paradigms offering individual perspectives to AI evaluation that contributes to the wider AI evaluation ecosystem, despite the lack of standardisation and occasional inconsistencies in terminology across these paradigms. Our aim with this paper was to bring attention to these different approaches and foster greater cross-pollination between paradigms, ultimately promoting a more integrated and holistic assessment of AI."}]}