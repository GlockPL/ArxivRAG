{"title": "FFAA: Multimodal Large Language Model based Explainable Open-World Face Forgery Analysis Assistant", "authors": ["Zhengchao Huang", "Bin Xia", "Zicheng Lin", "Zhun Mou", "Wenming Yang"], "abstract": "The rapid advancement of deepfake technologies has sparked widespread public concern, particularly as face forgery poses a serious threat to public information security. However, the unknown and diverse forgery techniques, varied facial features and complex environmental factors pose significant challenges for face forgery analysis. Existing datasets lack descriptions of these aspects, making it difficult for models to distinguish between real and forged faces using only visual information amid various confounding factors. In addition, existing methods do not yield user-friendly and explainable results, complicating the understanding of the model's decision-making process. To address these challenges, we introduce a novel Open-World Face Forgery Analysis VQA (OW-FFA-VQA) task and the corresponding benchmark. To tackle this task, we first establish a dataset featuring a diverse collection of real and forged face images with essential descriptions and reliable forgery reasoning. Base on this dataset, we introduce FFAA: Face Forgery Analysis Assistant, consisting of a fine-tuned Multimodal Large Language Model (MLLM) and Multi-answer Intelligent Decision System (MIDS). By integrating hypothetical prompts with MIDS, the impact of fuzzy classification boundaries is effectively mitigated, enhancing the model's robustness. Extensive experiments demonstrate that our method not only provides user-friendly explainable results but also significantly boosts accuracy and robustness compared to previous methods.", "sections": [{"title": "Introduction", "content": "The widespread availability of facial data online and access to deepfake technologies (FaceSwapDevs 2019; Kowalski 2018) have facilitated the forgery of identities. Malicious actors employ deepfake technology to manipulate faces for fraudulent purposes, generating false information and inciting public panic, which pose serious threats to both personal and public information security. To combat face forgery attacks, researchers have analyzed a variety of face forgery datasets (Rossler et al. 2019; Li et al. 2020) and developed numerous advanced forgery detection models (Dong et al. 2023; Ba et al. 2024) utilizing deep learning techniques.\nHowever, previous methods exhibit a substantial decline in detection accuracy when applied to open-world scenarios. There are two primary reasons: (1) High-quality forged"}, {"title": "Related Work", "content": "Face Forgery Analysis Datasets and Methods\nFF++ (Rossler et al. 2019) is the first large-scale face forgery video dataset, which includes four types of forgery techniques: DeepFakes (FaceSwapDevs 2019), FaceSwap (Kowalski 2018), Face2Face (Thies et al. 2016) and Neural-Textures (Thies, Zollh\u00f6fer, and Nie\u00dfner 2019). Afterwards, some real-world datasets emerge, such as DFDC (Dolhansky et al. 2020), Deeperforensics (Jiang et al. 2020). Although these datasets providing numerous images or videos, they lack the necessary textual descriptions. Recently, Zhang et al. construct the DD-VQA dataset by manually incorporating reasoning process for certain real and forged faces that could be discerned on common sense. However, numerous face forgeries in open-world scenarios are challenging to identify on common sense and require expert scrutiny.\nDespite the datasets, researchers have previously proposed numerous methods (Chollet 2017; Zhao et al. 2021; Cao et al. 2022) for detecting face forgeries. However, these methods exhibit limited generalization in open-world scenarios. To improve generalization, researchers have made improvements from various perspectives. Dong et al. find that inadvertently learned identity representations in images hinder generalization thus design a specialized network to mitigate this impact. Ba et al. design a deep forgery detection framework grounded in information bottleneck theory to extract more forgery clues. Recently, Khan and Dang-Nguyen leverage the powerful zero-shot capabilities of CLIP (Radford et al. 2021), employing Prompt-Tuning with a learnable context to integrate textual and visual information for deep-fake detection, demonstrating better generalization.\nHowever, these methods still demonstrate limited generalization in complex open-world scenarios. Additionally, these methods typically approach the problem as a binary classification task, yielding solely binary outputs or heatmaps, making it difficult to understand the model's decision-making process. Therefore, it is essential to develop an open-world face forgery analysis model with strong generalization and robustness, while also providing user-friendly and explainable results.\nMultimodal Large Language Models\nRecently, Multimodal Large Language Models (MLLMs) represented by GPT-4V and GPT-40 have garnered significant attention due to their remarkable image understanding and analysis capabilities. Some studies (Jia et al. 2024; Shi et al. 2024) have explored their potential in face forgery analysis by employing prompt engineering. They find that carefully designed prompts are essential for inducing analysis and making judgments, yet the accuracy of discrimination remains suboptimal. In other fields, some studies (Lai et al. 2024; Li et al. 2024) employ instruction tuning to fine-tune pre-trained MLLMs, resulting in domain-specific MLLMs with strong zero-shot capabilities."}, {"title": "Dataset", "content": "Data Collection\nFig. 2a shows the data collection process. Based on some studies (Tolosana et al. 2020; Dang et al. 2020) and real-world scenarios, we classify face forgery types into three categories: identity exchange (i.e., replacing the target face with another face), facial attribute manipulation (i.e., manipulating attributes of the target face, including expressions), and entire face synthesis (i.e., generating non-existent faces). Then, we utilize FF++ (Rossler et al. 2019), Celeb-DF-v2 (Li et al. 2020), DFFD (Dang et al. 2020) and GanDiffFace (Melzi et al. 2023) as our source datasets since they encompass diverse forgery techniques and enriched facial characteristics. Finally, we construct the Multi-attack (MA) dataset, comprising 95K images with diverse facial features and various types of forgeries.\nGPT4-assisted Analysis Generation\nManually writing the image description and forgery reasoning requires specialized knowledge and is quite cumbersome, making it difficult to establish a large-scale and high-quality dataset. Inspired by some studies (Chen et al. 2023; Liu et al. 2024b; Li et al. 2024) using GPT for image captioning, we employ GPT-4o to generate face forgery analysis processes. The pipeline of analysis generation is shown in Fig. 2b, which can be summarized as follows. First, we integrate a carefully designed system prompt, a data source-specific prompt, the target image and available reference images as inputs to GPT-40 to generate responses in a specific format. Subsequently, responses with incorrect formats, erroneous results or low probability are filtered out. Finally, experts scrutinize the analysis process and the qualified responses are organized into a VQA format.\nPrompt Design. Given the challenges of face forgery analysis in open-world scenarios, it is crucial to supply domain-specific auxiliary information and carefully crafted prompts to guide the generation. Motivated by these, our system prompt comprises four sections: role-playing, the introduction to forgery types, the Chain of Thought and conversation requirements. To leverage relevant domain knowledge of GPT-40 and generate responses with domain-specific characteristics, we first introduce its role and the task. Then, the types of face forgeries are defined to avoid ambiguity and assist in the analysis. For the Chain of Thought, we advocate that conducting complex face forgery analysis demands a robust foundation in image understanding, requiring multi-perspective analysis before delivering the final discrimination result. Thus, we design a progressive Face Forgery Analysis Chain of Thought (FFA-CoT) consisting of 'Image description', 'Forgery reasoning' and 'Analysis result' (including classification result, probability and forgery type). Finally, some requirements are claimed to ensure the consistency of the responses. To enhance the reliability of the response, the authenticity and the forgery type of the image are indicated in the data source specific prompt. Additionally, we provide the original face image or forged mask as reference to assist GPT-40 in the analysis.\nQuality Verification. To ensure data quality, responses with incorrect formats, erroneous results or low probability are first filtered out. Subsequently, the images, reference images and responses are provided to experts for scrutinizing the analysis process. Finally, we obtain 20K high-quality face forgery analysis data which are then supplemented with semantically identical inquiry prompts to form the FFA-VQA dataset. In addition, we conduct experiments to verify the quality of FFA-VQA. As depicted in Fig. 3, incorporating high-quality image descriptions and forgery reasoning enables MLLMs to perform face forgery analysis based on image understanding before making final judgments, which effectively reduces overfitting and enhances generalization. See Ablation Study for more experimental details."}, {"title": "Method", "content": "Although fine-tuning MLLMs on the FFA-VQA effectively enhances generalization, the model's robustness remains suboptimal. We also observe that prompts with similar semantics but varied content can yield different authenticity judgments for faces that are challenging to discern. This variability arises from the advancement of forgery techniques and the complexities of open-world scenarios, which have considerably blurred the boundaries between real and forged faces. Motivated by these, we introduce hypothetical prompts to the FFA-VQA dataset that presume the face is either real or fake prior to analysis. By fine-tuning the MLLM on this dataset, we enable the generation of answers based on varying hypotheses. Subsequently, we propose MIDS to select the answer that best matches the image's authenticity, which effectively mitigates the impact of fuzzy classification boundaries between real and forged faces.\nFace Forgery Analysis Assistant\nFig. 4a depicts the architecture of FFAA, which primarily consists of two modules: a fine-tuned MLLM and MIDS. Initially, the MLLM generates the anchor answer $X_a$ without hypotheses, the positive answer $X_p$ and the negative answer $X_n$ based on varying hypotheses, all of which are input into MIDS along with the face image. The model then calculates the match score between the image and each answer based on MIDS's output vectors $(m_a, m_p, m_m)$ and the determination result of each answer as follows:\n$s =\\begin{cases} m_0/(m_0 + m_2), & \\text{if } r(X_a) \\text{ is 'real'}, \\\\ m_3/(m_3 + m_1), & \\text{if } r(X_a) \\text{ is 'fake'}, \\end{cases}$ (1)\nwhere $m_i$ represents the i-th element of m and $r(X_a)$ represents the determination result of $X_a$. Finally, the answer with the highest match score is selected as the best answer.\nFine-tuning MLLM with Hypothetical Prompts\nWe categorize the prompts into two types: non-hypothetical (e.g., The image is a human face image. Is it real or fake? Why?) and hypothetical (e.g., This is a [real/fake] human face. What evidence do you have?). We randomly select one-third of the conversations from FFA-VQA and modify their prompts from non-hypothetical to hypothetical, aligning the hypothesis with the authenticity of the face. Then, we fine-tune an MLLM on this dataset with LoRA (Hu et al. 2021). For the image X, its conversation data is (Xq, Xa), where $X_q = X_q$ for non-hypothetical prompts and $X_q = [X_h, X_q]$ for hypothetical prompts. The fine-tuning objective is consistent with the original auto-regressive training objective of the LLM. For a sequence of length L, the probability of the target answer is computed as:\n$p(X_a | X_v, X_q) = \\prod_{i=1}^L p_\\theta(x_i | X_v, X_q, X_{a,<i})$, (2)\nwhere $\\theta$ is the trainable parameters and $X_{a,<i}$ are answer tokens before current prediction token $x_i$. From the conditionals in Eq. 2, it is evident that for prompts with hypothesis, $X_a$ depends on both $X_h$ and X. For faces that are challenging to discern, where altering $X_h$ may result in varying authenticity judgments, we label them as 'Hard'. For easily distinguishable faces, even if altered, the model is likely to rely on the high-weight image tokens in Xv to generate Xa, maintaining its original judgment. We label these as 'Easy'. Therefore, by simply altering Xh, we can identify faces that are challenging to distinguish and subsequently make a more informed decision among multiple answers.\nMulti-answer Intelligent Decision System\nMIDS consists of three components: Feature Encoding, Image-Answer Cross Fusion Module and Classification Layer, as depicted in Fig. 4b. We first utilize the fine-tuned MLLM to extract answers from unused samples in the Multi-attack dataset for training. Specifically, for a given image Xv, a non-hypothetical prompt is randomly selected to query the MLLM, yielding Xa. Based on the determination in Xa, two hypothetical prompts assuming the same or opposite determination are used to query again, yielding the positive answer Xp and the negative answer Xn respectively. Finally, based on the authenticity $y_v \\in \\{0, 1\\}$ of Xv and the determination result $y_a \\in \\{0, 1\\}$ of Xa, we assign a label y for each (Xv, Xa) as:\n$y = 2y_v + y_a \\in \\{0, 1, 2, 3\\}$, (3)\nFeature Encoding. For each given answer Xa, to prevent the model from relying solely on the final classification result while ignoring the intermediate analysis process, we mask the 'Analysis result' of Xa. Then, we employ the pre-trained T5-Encoder (Raffel et al. 2020) with frozen weights for good linguistic consistency to obtain the text feature Ft. For the image Xv, we employ the pre-trained CLIP-ViT-L/14 (Radford et al. 2021) as the visual feature encoder. To maintain basic image understanding and enhance the perception of local and global features related to face authenticity, the weights of the last two layers are unfrozen. Then, we employ trainable projection matrices $(W_{vg}, W_{vl})$ to project the output features of the last two layers into the same dimensional space as Ft, resulting in Fvl and Fvg.\nImage-Answer Cross Fusion. To extract the corresponding visual features based on the answer and to identify matching textual features from the visual features, we employ dual cross-attention for comprehensive fusion across all dimensions. Specifically, it can be expressed as:\n$F_{v\\rightarrow t} = LayerNorm(softmax(\\frac{QK^T}{\\sqrt{d}})V_t)$, (4)\n$F_{t\\rightarrow v} = LayerNorm(softmax(\\frac{Q_tK^T}{\\sqrt{d}})V_v)$,\nwhere d is the dimension of these features, ${Q_t, K_t, V_t\\} = F_{qkv}(F_t)$, ${Q_v, K_v, V_v\\} = F_{qkv}(F_v)$. We perform the operations of Eq. 4 with $(F_t, F_{vl})$ and $(F_t, F_{vg})$ respectively, to obtain $F_{tvl}, F_{vl\\rightarrow t}, F_{t\\rightarrow vg}$ and $F_{vg\\rightarrow t}$. Subsequently, we employ learnable vectors $f^r$ to obtain the corresponding aggregated feature based on cross-attention:\n$f^r = LayerNorm(softmax(\\frac{Q_fK_f^T}{\\sqrt{d}})V_F)$, (5)\n$Q_f = W_Q f^{r-1}, K_F = W_K F, V_F = W_V F$,\nwhere r denotes the r-th iteration. Through the operations of Eq. 5, we obtain text feature-enhanced visual domain features $(f_{tvl}, f_{t\\rightarrow vg})$ and image feature-enhanced textual domain features $(f_{t\\rightarrow tl}, f_{vtg})$ for each (X, Xa).\nClassification and Loss Function. To select the best answer, the model must possess a certain ability to discern the authenticity of faces and answers. Therefore, we set the classification labels as the four-class categorization shown in Eq. 3. The [CLS] tokens of Fvl and Fvg, containing enriched image category information, are passed through projection matrices $(W_{cls}, W_{cls})$ to obtain $(f_{cls}, f_{cls})$. Then, we concatenate $(f_{vl}, f_{t\\rightarrow vl}, f_{vtg}) and (f_{cls}, f_{vl}, f_{t\\rightarrow ti})$ to obtain $H_f$, which are further enhanced by M iterations of self-attention with residual connections (He et al. 2016), resulting in $H'$. Finally, $H'$ is passed through $W_e$ and a softmax to obtain $m \\in \\mathbb{R}^4$. The loss is computed as follows:\n$\\mathcal{L}_{cls} = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{c=1}^4 Y_{i,c} \\log(m_{i,c})$, (6)\nwhere N is the number of samples, $Y_{i,c}$ is the true label for the i-th sample in the c-th class and $m_{i,c}$ is the predicted probability for the i-th sample in the c-th class."}, {"title": "Experiments", "content": "In this section, we first establish the OW-FFA-Bench to evaluate the model's generalization and robustness. We then compare our method with the state-of-the-art methods on this benchmark and conduct ablation studies. Furthermore, we perform qualitative studies by comparing FFAA with advanced MLLMs and employing several visualizations.\nExperimental Setup\nDatasets. We create the generalization test sets by randomly collecting face images from the public datasets: Deeperforensics (DPF) (Jiang et al. 2020), DeepFakeDetection (DFD) (Google 2019), DFDC (Dolhansky et al. 2020), PGGAN (Karras et al. 2017), Celeb-A (Liu et al. 2018), WhichFaceIsReal (WFIR) (West and Bergstrom 2019) and MultiFFDI (MFFDI) (on the Bund 2024). The images are then divided into six generalization test sets based on their source as shown in Tab. 1, with each containing 1K images and differing in distribution. See Appendix for more details.\nImplementation details. We first fine-tune an MLLM, e.g. LLaVA-v1.6-mistral-7B (Liu et al. 2024a) with LoRA (rank=32, alpha=48) on the FFA-VQA dataset using 2 RTX 3090 GPUs for 3 epochs with a learning rate of 1e-4 and a batch size of 16. Then, we train MIDS using 2 RTX 3090 GPUs for 2 epochs with a learning rate of 1e-4 and a batch size of 48. See Appendix for more training details.\nEvaluation metrics. We utilize the Accuracy (ACC), Area Under Receiver Operating Characteristic Curve (AUC) and Average Precision (AP) to empirically evaluate generalization performance on individual test sets as well as across the entire OW-FFA-Bench (ALL). Additionally, we employ the standard deviation of ACC (SACC) across all test sets to assess the model's robustness.\nComparison with Existing Methods\nIn this section, we compare our method against state-of-the-art face forgery analysis methods on 1K in-domain test set of MA and the OW-FFA-Bench. These methods can be divided into two categories: methods using only visual information (i.e., Xception (Chollet 2017), RECCE (Cao et al. 2022), Implicit (Dong et al. 2023), FedForgery (Liu et al. 2023), Exposing (Ba et al. 2024) and NPR (Tan et al. 2024)), and methods based on CLIP using both textual and visual information (i.e., CLIPping (Khan and Dang-Nguyen 2024)).\nTab. 2 presents the comparison results. Methods in the first category display varying degrees of overfitting and poor generalization, underscoring the challenges in open-world scenarios where various confounding factors complicate the discrimination using solely visual information. Conversely, CLIPping effectively prevent overfitting and demonstrate better generalization. However, the accuracy and robustness of these methods remain suboptimal. In contrast, we frame face forgery analysis as a VQA task, requiring the model to analyze forgery before making a judgment. This approach improves the model's generalization ability and clarifies its decision-making process. Given the fuzzy boundaries between real and forged faces, we employ hypothetical prompts and MIDS to comprehensively compare answers based on varying hypotheses, boosting robustness.\nQualitative Study\nComparison with Advanced MLLMs. Fig. 5 depicts a quality example. We present some responses from advanced MLLMs, including LLaVA-v1.6-34b (Liu et al. 2024a) and GPT-40, and compared them with ours. These advanced MLLMs exhibit strong image understanding capabilities but often struggle to determine face authenticity, sometimes avoiding definitive conclusions. In contrast, our method not only understands the image but also conducts a reasoned analysis of authenticity from various perspectives.\nHeatmap Visualization. To verify whether MIDS can learn more face authenticity representation features guided by enriched textual features, we utilize Grad-CAM (Selvaraju et al. 2020) to visualize the attention heatmap of Fvl and Fvg. As illustrated in Fig. 7, guided by enriched textual features, MIDS considers multiple aspects of the image and focuses on key features related to face authenticity.\nVisualization of Easy and Hard Samples As illustrated in Fig. 8, the fake faces in the 'Easy' samples usually have obvious signs of forgery while real faces have higher image quality. Conversely, 'Hard' samples often exhibit varying image quality and more convincing forgeries, posing significant challenges in determining face authenticity."}, {"title": "Conclusion", "content": "In this paper, we introduce a novel OW-FFA-VQA task aimed at understanding the decision-making process of face forgery analysis models and establish the corresponding benchmark and dataset. To tackle this task, we introduce FFAA: Face Forgery Analysis Assistant, which consists of a fine-tuned MLLM and MIDS. By integrating hypothetical prompts with MIDS, the impact of fuzzy classification boundaries between real and forged faces is effectively mitigated, enhancing the model's robustness. Our experiments demonstrate that FFAA not only yields user-friendly and explainable results but also significantly boosts generalization, accuracy and robustness compared to previous methods."}]}