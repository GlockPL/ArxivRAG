{"title": "FLAME 3 Dataset: Unleashing the Power of Radiometric Thermal UAV Imagery for Wildfire Management", "authors": ["Bryce Hopkins", "Leo O'Neill", "Michael Marinaccio", "Eric Rowell", "Russell Parsons", "Sarah Flanary", "Irtija Nazim", "Carl Seielstad", "Fatemeh Afghah"], "abstract": "The increasing accessibility of radiometric thermal imaging sensors for unmanned aerial vehicles (UAVs) offers significant potential for advancing AI-driven aerial wildfire management. Radiometric imaging provides per-pixel temperature estimates, a valuable improvement over non-radiometric data that requires irradiance measurements to be converted into visible images using RGB color palettes. Despite its benefits, this technology has been underutilized largely due to a lack of available data for researchers. This study addresses this gap by introducing methods for collecting and processing synchronized visual spectrum and radiometric thermal imagery using UAVs at prescribed fires. The included imagery processing pipeline drastically simplifies and partially automates each step from data collection to neural network input. Further, we present the FLAME 3 dataset, the first comprehensive collection of side-by-side visual spectrum and radiometric thermal imagery of wildland fires. Building on our previous FLAME 1 and FLAME 2 datasets, FLAME 3 includes radiometric thermal Tag Image File Format (TIFFs) and nadir thermal plots, providing a new data type and collection method [1], [2]. This dataset aims to spur a new generation of machine learning models utilizing radiometric thermal imagery, potentially trivializing tasks such as aerial wildfire detection, segmentation, and assessment. A single-burn subset of FLAME 3 for computer vision applications is available on Kaggle here, with the full 6 burn set available to readers upon request.", "sections": [{"title": "I. INTRODUCTION", "content": "Monitoring wildfire for both intentional and unintentional ignitions is a crucial data source for not only the characterization of fire behavior and rate of spread, but also for future prediction of fire characteristics, fire effects, firefighter safety, and tactical decision support for Incident Management Teams (IMT). Unmanned Aerial Systems (UASs) are playing an increasing potential role in wildfire fire monitoring, specifically the rapid development of sensors, systems, and machine learning approaches to rapidly produce actionable data. UAS approaches for wildfire monitoring differ from traditional approaches used by the US Forest Service-managed National Infrared Operations (NIROPS), which produces an incident-wide suite of fire intelligence data to produce a broad common picture of a fire or complex. UAS operations are generally used for strategic and local monitoring of wildfire (e.g. division, fire starts, safety, patrolling) due to endurance (~25-45 min flights), restrictions of flight altitude, and airspace deconfliction. The Interagency UAS program\u00b9 is tasked to employ UAS operations for fires by developing standards for data collection, processing, and management in support of incident management teams and resource management objectives. This program has focused on the use of Type 3 and 4 rotor UASs to map fire, enhance situational awareness, and provide aerial ignition operations. Advantages of UAS systems include the ability for rapid deployment, high temporal and spatial resolution, a larger range of flight operation conditions, loitering capabilities, lower operational cost, and reduced risk for personnel. A key new area of interest in UAS operations is the enhancement of image processing pipelines that can rapidly deliver fire intelligence to incident managers. These data streams can include video streams, multiple image mosaics of infrared images, vector interpretations of the heat perimeters, or multi-spectral imaging with simultaneous/coincident infrared imagery. These data provide a lattice of information that has potential importance to improving next-generation fire behavior models, better characterize fuels using structure from motion (SfM), provide temporally rich fire progressions that can be integrated into data driven (machine learning) short-term fire modeling, and machine learning approaches to detect fire spread and ignition from coincident RGB data using infrared labels.\nThe contributions of this work are as follows:\n\u2022 Provides insights on the key data challenges suppressing developments in aerial multi-spectral wildfire assessment.\n\u2022 Presents the FLAME Data Pipeline, including procedures for maximizing prescribed fire aerial imagery collection as well as a machine-learning-oriented software suite to semi-automate data processing.\n\u2022 Publishes the FLAME 3 CV Subset and FLAME 3 Modeling Subset, two openly available datasets of processed imagery targeting computer vision and wildfire modeling tasks.\n\u2022 Provides empirical evaluation for classification task, and comparison of performance for FLAME datasets with varying modality input combinations, highlighting improved performance with the use of radiometric thermal TIFF data"}, {"title": "II. WILDFIRE DATA COLLECTION CHALLENGES", "content": "Wildfire and prescribed fire environments are complicated workspaces for UAS data collection, where data collection and fire intelligence products are secondary to fire environment safety and operations. More profoundly, there are significant hurdles for researchers to access wildfire environments due to the UAS platform, agency specific standards and certifications, and IMT resource ordering requirements. To date, most wildfire data collection with UAS has been done directly by federal and state agency UAS teams or through approved private contractors. Research has generally focused on UAS data collection over prescribed fires as these fires are smaller in scale, provide better access to UAS as they are managed by a spectrum of land ownership (e.g. private, state, and federal), and are generally low fire intensity and complexity. The limitation of research focused on prescribed fires is the confined range of fire behavior and spread that is prevalent in low intensity fires ignited on high fuel moisture margins [3]. These fire characteristics are difficult to compare to the more intense fire behavior and fire characteristics in large wildfire incidents."}, {"title": "A. Wildfire Datasets", "content": "In recent years, several fire image datasets have been released, targeting a variety of domain tasks [23]. These datasets can be categorized based on how they improve upon existing datasets, enhance understanding of conditions leading to wildfires, create 3D fire simulations [24], aid in the development of fluid dynamic models of wildfires [25], [26], and provide high-quality data for deep learning researchers working on classification [21], [22], segmentation [27], [28], and fire spreading pattern models [29]-[31].\nWildfire datasets include images and videos of prescribed burns or actual wildfires, captured via cellular phones or ground cameras [6], [7], [18], satellites [32], or UAVs [21], [22]. In wildfire monitoring, deep learning approaches rely on annotated imagery datasets. The vast majority label wildfire images for classification purposes [7], [10], [14], [22]. Although limited, some datasets also provide pixel-wise annotations, categorizing each pixel as Fire/No Fire or Smoke/No Smoke [6], [18]. Others include bounding box coordinates indicating the locations of fire in an image [15], [16]. To the best of our knowledge, notable wildfire imagery datasets are listed in Table I."}, {"title": "B. Gaps in Existing Datasets and Related Concerns", "content": "While wildfire imagery datasets have improved significantly in recent years, there exist gaps in these datasets that limit research progression for certain tasks. Notably, tasks in detection, monitoring, modeling, post-assessment, and prescribed fire planning. Table I overviews related wildfire imagery datasets, as well as showcases the gaps in and between them. Comparing each individual dataset in Table I with FLAME 3 highlights how FLAME 3 addresses these gaps.\n1) Spatial and Temporal Resolution: A key challenge with aerial imagery datasets is a lack of spatial resolution. That is, image pixels represent large ground areas that are insufficient for characterizing fine-grain features. In the wildfire domain, this inhibits the effectiveness of detection and assessment models and reduces model altitude generalization. Critically, thermal imagery is naturally prone to reduced spatial resolution, as thermal cameras generally posses much lower camera resolutions than visible spectrum cameras. Note that camera resolution refers to the number of image pixels captured by a camera (typically 1920x1080, 3840x2160, or 3000x4000 for visible spectrum cameras). For a given altitude, the resulting spatial resolution (also known as ground resolution) is the ground area that each image pixel measures. For datasets that depend on web-sourced imagery, it can be challenging to ensure consistent high-definition imagery, typically resulting in reduced resolution samples such as in [8]-[10]. Still other datasets that employ targeted data collections rely on low-resolution imaging sensors, reducing the resulting data applicability [4], [6]. Vliet and Hendrik's work [33] demonstrated a novel method to improve spatial resolution in thermal imagery. They collected images from a vibrating IR camera, and used interpolation to improve the spatial resolution.\nOne of the key contributions of FLAME 3 is providing data as well as procedures for collecting data with high spatial resolutions. The FLAME 3 dataset provides high camera resolution (4000x3000 pixels) RGB imagery from DJI M30T and M2EA drones capable of maintaining high spatial resolution at high altitudes (up to the Federal Aviation Administration (FAA) regulated maximum of 122 m AGL) [1]. The gaps introduced by low altitude aerial imagery can be seen in the State Key Laboratory of Fire Science from the University of Science and Technology of China [6], as well as in a large portion of imagery from FLAME 1 [34].\nMany applications benefit from improved temporal resolution of collected data. That is, repeated imagery samples of the same geographic area, with the revisit rate (inverse of the sampling interval) being referred to as the temporal resolution of the data. For tasks such as spread modeling or burn behavior characterization, having a sufficient temporal resolution of collected samples is critical. However, the vast majority of available datasets are web-sourced and thus completely unsuited for such tasks [8]-[10], [13], [17]. For this purpose, the provided FLAME 3 data collection procedures include methods for collecting nadir thermal plots, which provide regular georeferenced snapshots of burn progression over a set area [2]."}, {"title": "III. FLAME 3 DATASET", "content": "The Flame 3 dataset consists of paired visual spectrum and long-wave infrared imagery from various prescribed burns [1]. This dataset includes generalized aerial oblique imagery collected via UAS at six different prescribed fire events, which showcase diverse burn behaviors across pine forests, grasslands, and low sagebrush environments. Additionally, nadir thermal plots were established at three of these six burns, resulting in a focused subset of FLAME 3 that facilitates the characterization of fine-grained temporal and spatial dynamics and supports predictive modeling [2]."}, {"title": "A. Radiometric Thermal TIFF", "content": "1) Pixel-level Fire/No-Fire Labeling and Temperature Ground Truth: A critical step in supervised learning for different machine learning-based wildfire management tasks involves labeling each image for the intended task. Typically, this labeling process is carried out manually, where a human annotates images through text files, ground truth masks, or annotation software. This manual approach is often tedious, time-consuming, and susceptible to human error. In binary wildfire classification, for instance, each image or video must be labeled as either Fire or No Fire, while in wildfire segmentation, individual pixels must be classified according to predefined classes, further increasing the labor involved. Other types of labeling are emerging in recent years, such as multilabeling schemes seen in [48]. Yunchao [49] presents a method of multilabeling without the need for bounding box annotations to begin with.\nThermal TIFFs offer a distinct advantage in automating the annotation process for fire classification tasks. The radiometric data embedded within these files provides accurate temperature values for each pixel, enabling automated detection of heat signatures specific to fires. This capability minimizes reliance on manual labeling, as algorithms can utilize these temperature readings to distinguish fire-related regions from non-fire areas with high precision. To mitigate the costly procedure of labeling data, the FLAME 3 dataset leverages radiometric thermal data for generating preliminary binary Fire or No-Fire labels. By applying thresholding techniques to the radiometric data, it effectively distinguishes between obvious Fire and No-Fire images. Specifically, images with a maximum temperature below approximately 80\u00b0C are highly likely to be classified as No-Fire, while those with temperatures exceeding 200\u00b0C typically depict active fire. This initial screening allows human experts to efficiently review algorithmically labeled data, focusing their attention on removing blurry or inconclusive samples. For images that fall outside of these temperature thresholds, manual labeling or removal is performed as needed. The combination of visible spectrum and radiometric thermal data significantly improves the accuracy of Fire/No-Fire classification, as thermal data often reveals the presence of hotspots that may not be visible in standard imagery.\nThe pixel-wise radiometric thermal data also has the potential to greatly simplify the annotation of fire imagery for segmentation tasks. Through binary thresholding or more sophisticated methods like Otsu or Hysteresis thresholding, the dataset enables rapid algorithmic classification of individual pixels based on temperature values. Moreover, the radiometric thermal data offers pixel-level ground truth for temperature rather than just a Fire/No-Fire classification, making it feasible to train models for pixel-wise temperature prediction using visible spectrum or paired visible spectrum and non-radiometric thermal imagery, with radiometric data serving as the ground truth. However, the success of this pixel-wise labeling approach depends on achieving perfect alignment between the radiometric thermal TIFFs and the corresponding visible spectrum images. As discussed in the following subsection, optimal alignment between RGB and IR imagery remains an ongoing research challenge.\n2) Image Pair Alignment: For data collection, a drone with side-by-side RGB-Thermal camera was used, resulting in paired imagery. An uncommon preprocessing technique that FLAME 3 aims to emphasize is to perform field-of-view (FOV) corrections on the dataset's image pairs to properly align RGB and thermal images side-by-side. Raw visible spectrum and thermal images typically do not perfectly mirror one another due to FOV and resolution differences of RGB and thermal cameras. Examples of unaligned data can be seen in side-by-side datasets such as [6] and [18]. FLAME 3 addresses this by manually cropping the RGB images as close to a 1:1 pixel correspondence as possible, with future works planning algorithmic or automated FOV correction to improve upon the remaining pixel offsets, which can be seen in Figure 8. The eventual goal is to enable visible-thermal data fusion by convolution across visible and thermal layers, preserving spatial relation and inductive bias between data types. It is expected that such a fusion approach would outperform existing late-fusion and early-fusion methods for RGB-Thermal feature extraction.\n3) TIFFs vs JPEGs: Radiometric IR cameras return a thermal signal, and this signal is represented as a 1-band raster."}, {"title": "B. Nadir Thermal Plots", "content": "The FLAME 3 Nadir Thermal Plot datatype provides a persistent overhead recording of a fire's progression across a set georeferenced plot. While both visible spectrum and thermal imagery is typically captured, they are referred to as nadir thermal plots as the radiometric thermal data is typically the most interesting and informative aspect of the data. During collection, operators locate a UAV above the center of a preset georeferenced area, configuring the UAV to take repeat imagery every 3-5 seconds. When collated, orthorectified, and appropriately stacked, the collected data encapsulates burn progression behaviors across the studied plot fuels. With appropriate calibration, the resulting data product can be used to measure centimeter-grade rate of spread as well as evaluate energy release [52]."}, {"title": "IV. METHODOLOGY", "content": "To encourage and enhance UAS-collected wildfire imagery, we provide a set of guidelines for collecting image data at prescribed burns/wildfires. We also provide two python programs to assist with post-collection data processing, focusing specifically on unlocking radiometric thermal data's underutilized capabilities for ground truthing in machine learning applications. Finally, we provide a single-burn computer-vision-oriented subset of example post-processed imagery data collected at a prescribed fire in the pine forests at Sycan Marsh, Oregon in October 2022 [1]. We also provide a single-burn modeling focused subset including nadir thermal plot data from a prescribed fire in the tree stands at Hanna Hammock, Florida in February 2023 [2]."}, {"title": "A. Data Collection Procedures", "content": "With FAA, and USFS restrictions, short weather-based notices, remote burn locations, and limited burn seasons, attending prescribed fires to collect data is challenging. Thus, it is in a researcher's best interest to collect as much data per burn as possible. Data collection itself is complicated by uncertain flight restrictions during the burn that are necessary to avoid interfering with burn management operations. As such, it is important to incorporate flexibility into data collection procedures. The following subsections detail data collection goals for a UAS team at a prescribed fire pre-burn, during-burn, and post-burn. The outlined goals aim to collect imagery for both machine learning and wildfire modeling.\n1) Pre-Burn: The FLAME pre-burn objectives are as follows:\n\u2022 Pre-burn structure-from-motion (SfM) photogrammetry\n\u2022 Generic No-Fire oblique imagery\n\u2022 Place and georeference metal plates for nadir thermal plot(s)\n\u2022 (Supplemental) Setup weather station\nThe first pre-burn objective is to perform a structure from motion mapping of the burn plot. Many drones (including DJI drones) have some built-in functionality for oblique mapping. If not, they often have the ability to import photogrammetry flight routines created in a third party software such as Drone Deploy. Depending on the burn plot area and site availability pre-burn, it may not be possible to fully map the site. If so, prioritize areas with interesting expected burn behavior or with high research importance (such as the location of a nadir thermal plot).\n\n\nThe second pre-burn objective is to collect generic oblique imagery of the surrounding no-fire environment. The collected images and videos serve as negative samples for machine learning applications. For image pairs, the UAV is piloted in an effort to capture a variety of scenes with minimal repeat imagery. Videos are collected with similar motivation, though with added effort to maintain smooth footage. Note that the pixel-wise correlation between RGB and thermal data might vary significantly depending on the UAV used. For some UAVs, this discrepancy can be exacerbated with gimbal motion, UAV lateral motion or yaw, and video length. To mitigate these discrepancies, rapid accelerations (both UAV and gimbal) should be minimized when possible.\n\nThe last non-supplemental pre-burn objective is to set up and georeference reflective plates for nadir thermal plots. The plates act as ground control points for georeferencing all UAV-derived products, a known location and distance measurement, and help the pilot orient the UAV during imagery acquisition.\n2) Active-Burn: The FLAME during-burn objectives are as follows:\n\u2022 Generic oblique imagery\n\u2022 Nadir thermal plots\n\u2022 (Supplemental) Regular weather measurements\nFor optimal collection, a UAV was dedicated to nadir thermal plots while one or more other UAVs patrols the flaming front, capturing generic oblique imagery. For generic oblique imagery, the same collection parameters in Table V that were used for oblique imagery collection during pre-burn are suggested, with the exception that the imagery subject should be the flaming front or interesting burn behavior. For nadir thermal plots, the objective of the nadir plot imagery is to capture the fire behavior in a area representative of entire fire. This sampling approach captures ignition, flaming combustion, and post-frontal consumption for each pixel (area) in the nadir plot as described in [52]. The UAV is positioned 122 m (400 ft) directly over the center of the nadir plot and oriented north. Plots are established prior to ignitions with the four aluminum ground control points. Immediately prior to fire entering the UAV sensor frame, we began sampling images on a 5-second interval (0.2 Hz), pausing occasionally to swap UAV batteries. Ideally, all fire radiative energy released is captured, though post-frontal smoldering can occur for hours and even days while coarse woody debris is consumed. We balanced capturing multiple nadir plots at each fire with measuring post-frontal energy release, sampling plots until radiant energy release had subsided.\n3) Post-Burn: The post-burn portion of the FLAME 3 dataset includes Post-burn structure-from-motion (SfM) photogrammetry. After the burn, the same structure-from-motion oblique mapping flight plan as was completed during the pre-burn stage should be flown. The resulting pre-burn and post-burn point clouds should then be directly comparable to one another, providing insights on fire effects on vegetation and other ecological processes."}, {"title": "B. Data Processing", "content": "This section assumes that data has been collected according to the previously outlined FLAME imagery collection procedures. Example raw data can be found here.\n1) Imagery: There are two imagery data types collected during the burn: frame pairs and video pairs. A general data processing outline can be seen in Figure 4. It is important to recognize that TIFF files, which are effectively temperature arrays, are not human readable by default.\n1) Starting point: Folder of raw thermal JPG images, raw RGB JPG images, raw thermal MP4 videos, and raw RGB MP4 videos.\n2) Use \"FLAME Raw File Sorter.py\" to the raw data folder. The script does the following as visualized in Figures 4-6:\na) Pairs RGB/thermal images and videos based on metadata datetime.\nb) Extracts radiometric thermal date from radiometric JPG metadata. Saves as a TIFF.\nc) Regenerates thermal JPG from temperature values with a known color map. This helps standardize thermal imaging across camera brands/settings.\nd) Scales, crops, and translates RGB images to align with thermal images for each image pair.\ne) Copies all sorted and paired images and videos to output folders. An example of the resulting output can be seen found here.\n3) (optional) Use \u201cFLAME Image Labeling Tool.py\" to iterate through the sorted image pairs, applying a binary Fire/No Fire label as necessary. Basic temperature thresholding options are available for batch labeling. Image pairs are sorted to correspondingly named output folders, an example of which can be seen at FLAME Data Pipeline Tools.\nThe FLAME 3 FOV correction procedures as outlined above were performed iteratively, refining crop, translation, and scaling parameters to minimize the average pixel error between the RGB and thermal image for a given pair. To do this, a simple python script was employed to overlay the detected thermal features onto the RGB images. As thermal cameras are quite prone to noticeable lens error, there is an inconsistent small alignment offset between the images."}, {"title": "V. FLAME CLASSIFICATION COMPARISON", "content": "In order to evaluate the improvement of aerial imagery contained within FLAME 3 compared to aerial imagery from previous FLAME 1 [21] and 2 [22] dataset iterations, testing was performed for a generic Fire / No Fire classification task. The classification one stream and two stream archi-tectures from the FLAME 2 publication [56] were used. The one stream model consists of an initial convolutional layer with batch normalization and ReLU activation and two convolutional blocks consisting of a separable convolution (depthwise and pointwise convolutions), batch normalization, and ReLU activation. A residual layer and global pooling layer are included as well. The two stream model is a dual one stream model with a separate instance for RGB and IR images individually, which are then concatenated at the end before the final output prediction.\nThe one stream model was used for single modality (RGB, Thermal, and TIFF Only) and the two stream model was used for multimodal (RGB-Thermal / RGB-TIFF) combinations. $H_e$ weight initialization was used for convolutional layers and Xavier weight initialization for linear layers. The FLAME 1 classification set was used containing 30,155 Fire RGB images and 17,837 No Fire RGB images. The FLAME 2 set contained 39,751 Fire RGB-T pairs and 13,700 No Fire RGB-T pairs. The FLAME 3 set consisted of imagery from five burns: Willamette, Shoetank, Sycan Marsh, Payson Hundred Rx, and Hanna Hammock. Each evaluation scenario describes the FLAME 3 training and testing set used. Varying combi-nations of the FLAME datasets and each individual FLAME dataset were evaluated. Additionally, Bayesian optimization was used to perform a hyperaparameter search. It is important to recognize that this was not an exhaustive search, but rather used to find valid parameters for each dataset input and showcase FLAME 3 improvement over previous dataset iterations. The optimization sweep setup is defined in Table VI. Each evaluation was trained for 30 epochs with a batch size of 64."}, {"title": "A. Unimodal Evaluation - RGB Only", "content": "In order to evaluate the improvements FLAME 3 made in data collection procedures and quality of imagery, classification testing was performed in an RGB input only scenario. FLAME 1 and 2 image counts were as described previously. The FLAME 3 training set contained imagery from Shoetank, Payson Hundred Rx, and Hanna Hammock, totaling 3,710 Fire and 2,023 No Fire images. The FLAME 3 testing set contained 348 Fire and 348 No Fire images from Shoetank, Sycan Marsh, and Willamette. It was ensured that the testing images from Shoetank were randomly sampled and removed from the training set to avoid data leakage.\nIn Table VII, the highest performing dataset combination was FLAME 1&2&3, with 82.90% testing accuracy, followed closely behind by FLAME 1&3 and FLAME 3 respectively. FLAME 1&2&3 also has the highest precision and specificity, showing the fewest false positives and ability to avoid false positives. This is important as identifying the presence of fire correctly is objectively more important in real world application versus the lack of fire. From this it can be seen that FLAME 3 imagery inclusion provides the best performance overall, highlighting the quality, and variety of FLAME 3. From observed trends during optimization sweep, FLAME 3 showcased the most stable training. FLAME 3 also had the highest F1-score providing the best balance between precision and recall, and the highest sensitivity, showing it correctly identified all fire images. FLAME 1 and 2 perform the worst overall, and show that the FLAME 3 RGB imagery is superior in terms of performance."}, {"title": "B. Radiometric Thermal TIFF Evaluation", "content": "In order to evaluate the effectiveness and value of the radiometric thermal TIFF files, evaluation was performed under multimodal input scenarios, as well as with unimodal thermal or TIFF as input. FLAME 2 image counts were as described previously, and FLAME 1 excluded from this evaluation due to no thermal images designated for classification from that dataset. Additionally, FLAME 2 did not include TIFF files. The FLAME 3 training set contained imagery from Shoetank, Willamette, Payson Hundred Rx, and Hanna Hammock, totaling 3,942 Fire and 2,255 No Fire images. The FLAME 3 testing set contained 116 Fire and 116 No Fire images from Sycan Marsh. FLAME 3 RGB-TIFF and TIFF only combinations had Payson Hundred Rx and Hanna Hammock removed in the respective training sets, and the TIFF raw data was normalized between 0 and 255 before converting to PyTorch tensors.\nThe results from this testing are seen in Table VIII. The highest test accuracy of 91.38% was observed under the FLAME 3 TIFF only input, followed by the second highest test accuracy of 90.95% for FLAME 3 RGB-TIFF. These outperform all other multimodal and unimodal inputs utilizing the thermal imagery from FLAME 2 or 3. Additionally, the TIFF only and RGB-TIFF inputs were trained using a smaller subset of the FLAME 3. This showcases that the FLAME 3 TIFF files contain rich contextual information covering a wide range of wildfire scenarios, allowing a model trained with this dataset to generalize much better than previous FLAME dataset iterations. FLAME 3 inputs utilizing the TIFF files also show the best F1-scores and sensitivity, highlighting the trained models' ability to identify positive instances of fire images, while rejecting false positives.\nFLAME 3 thermal only input also provides comparable testing accuracy in relation to other inputs, showing the value in the imagery included in FLAME 3 itself. Another important point is that the multimodal FLAME 3 inputs perform better than multimodal FLAME 2 inputs, showing the effort put toward improving alignment of paired imagery in FLAME 3."}, {"title": "VI. CONCLUSIONS", "content": "In this study, we have addressed the critical gap in data availability for AI-driven aerial radiometric wildfire management by introducing the FLAME 3 dataset. This dataset represents the first comprehensive collection of synchronized visual spectrum and radiometric thermal imagery of wildland fires, gathered using UAVs at six prescribed fires. By providing radiometric thermal TIFFs and nadir thermal plots, FLAME 3 builds upon our previous FLAME 1 and FLAME 2 datasets, introducing new data types and collection methods that are crucial for advancing research in this field.\nThe per-pixel temperature estimates provided by radiometric imaging offer a significant improvement over traditional non-radiometric data, facilitating more accurate and efficient machine learning models. These models have the potential to simplify and enhance tasks such as aerial wildfire detection, segmentation, and assessment, ultimately contributing to more effective wildfire management strategies. Through the dissemination of the methods and tools utilized for collecting and processing FLAME datasets, our aim is to democratize access to radiometric thermal data for researchers worldwide.\nThe FLAME 3 dataset opens new avenues for research and development in AI-driven wildfire management, encouraging the creation of robust models that leverage the rich information contained in radiometric thermal imagery. We believe that the advancements enabled by this dataset will play a vital role in improving the precision and responsiveness of wildfire detection and monitoring systems, thereby enhancing public safety and environmental protection. Future work will focus on further expanding the dataset and on developing more sophisticated lens distortion removal procedures to better align visible spectrum and thermal image pairs."}]}