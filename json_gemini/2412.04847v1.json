{"title": "MTSpark: Enabling Multi-Task Learning with Spiking Neural Networks for Generalist Agents", "authors": ["Avaneesh Devkota", "Rachmad Vidya Wicaksana Putra", "Muhammad Shafique"], "abstract": "Currently, state-of-the-art RL methods excel in single-task settings, but they still struggle to generalize across multiple tasks due to \u201ccatastrophic forgetting\u201d challenges, where previously learned tasks are forgotten as new tasks are introduced. This multi-task learning capability is significantly important for generalist agents, where adaptation features are highly required (e.g., autonomous robots). On the other hand, Spiking Neural Networks (SNNs) have emerged as alternative energy-efficient neural network algorithms due to their sparse spike-based operations. Toward this, we propose MTSpark, a novel methodology to enable multi-task RL using spiking networks. Specifically, MTSpark develops a Deep Spiking Q-Network (DSQN) with active dendrites and dueling structure by leveraging task-specific context signals. Specifically, each neuron computes task-dependent activations that dynamically modulate inputs, forming specialized sub-networks for each task. Moreover, this bioplausible network model also benefits from SNNs, enhancing energy efficiency and making the model suitable for hardware implementation. Experimental results show that, our MTSpark effectively learns multiple tasks with higher performance compared to the state-of-the-art. Specifically, MTSpark successfully achieves high score in three Atari games (i.e., Pong: -5.4, Breakout: 0.6, and Enduro: 371.2), reaching human-level performance (i.e., Pong: -3, Breakout: 31, and Enduro: 368), where state-of-the-art struggle to achieve. In addition, our MTSpark also shows better accuracy in image classification tasks than the state-of-the-art. These results highlight the potential of our MTSpark methodology to develop generalist agents that can learn multiple tasks by leveraging both RL and SNN concepts.", "sections": [{"title": "1. Introduction", "content": "In recent years, reinforcement learning (RL) has demonstrated tremendous advancements, particularly in training agents to solve complex tasks autonomously through trial and error. The conventional RL-based methods have shown exceptional success in training systems (agents) to perform well in single-task environments, such as mastering individual games like Chess or Go [16, 17, 29]. With the increasing demands of agents that can adapt to dynamic real-world applications, the ability to handle multiple tasks is highly required [30]. However, state-of-the-art RL methods only excel in single-task settings and still struggle to generalize across multiple tasks due to catastrophic forgetting (CF) challenges, where previously learned tasks are forgotten as new tasks are introduced [13, 15, 33]. These limitations are shown by label-\u2460 in Figure 1, where the state-of-the-art RL-based methods in both Deep Neural Network (DNN) and Spiking Neural Network (SNN) domains, cannot effectively learn multiple tasks (i.e., three Atari games). Here, Deep Q-Network (DQN) [16] and Deep Spiking Q-Network (DSQN) [3] can only perform well in certain games. This limitation severely impacts the practicality of RL-based agents in real-world applications, where adaptability and generalization across tasks are critically required (e.g., autonomous robots that can adapt to diverse environments) [30]. Therefore, the targeted research problem is how can we develop an RL-based generalist agent that can"}, {"title": "1.1. State-of-the-Art and Their Limitations", "content": "To enable multi-task learning, several strategies have been explored in DNN domain, like the widely-used replay- and architectural-based methods [15, 33]. Replay-based methods revisit data from previously learned tasks [10, 26], while incurring a substantial storage cost to retain representation. Meanwhile, architectural-based methods show promising performance by dynamically adding new modules while freezing old parameters [9, 27], but they incur high inference latency and lead to uncontrolled growth in network size. Despite some success of conventional neural network (NN) and transformer architectures [25], these models are often data-intensive, requiring millions of samples and significant computational resources [31, 32]. The state-of-the-art work proposes the deep Q-network (DQN) [16], but it only focuses on a single-task learning, thus suffering from low performance for multi-task learning; see \u2460 in Figure 1.\nOn the other hand, biologically-inspired Spiking Neural Networks (SNNs) have emerged as alternate NN algorithms with promising results in many machine learning (ML) tasks, such as image classification [18\u201320], automotive [4, 23], healthcare [11], and robotics [2, 24]. Temporal information in SNNs is essential for efficient data stream processing, and beneficial for enabling multi-task learning. For instance, recent works leverage temporal information among spikes using bio-plausible learning rules to improve performance when learning multiple tasks subsequently [1, 21, 22]. However, these works still consider simple workloads (e.g., MNIST) under unsupervised learning settings. The state-of-the-art work proposes the deep spiking Q-network (i.e., DSQN [3]), but it also only focuses on a single-task learning, thus suffering from low performance for multi-task learning; see \u2460 in Figure 1. Therefore, the impact of SNNs with RL settings for multi-task learning remain unexplored, and hence hindering their deployments on real-world application use-cases."}, {"title": "1.2. Associated Research Challenges", "content": "Existing SNN architectures are not suitable for multi-task learning, as they are typically designed to only learn a single task. However, developing an RL-based SNN architecture that supports multi-task learning is non-trivial, as it poses the following research challenges.\n\u2022 The network model should employ a representative yet simple function to quantify the reward and punishment values for the training process, thereby enabling an efficient RL mechanism.\n\u2022 The network model should leverage context information (i.e., task identity) to enhance the learning quality, as task identity effectively guides the learning process.\n\u2022 The network model should understand the importance of possible actions to take after a given state, to decide the best next action for improving the performance."}, {"title": "1.3. Our Novel Contributions", "content": "To address the research challenges, we propose MTSpark, a novel design methodology to enable Multi-Task learning using Spiking neural networks under RL settings for realizing generalist agents; see an overview in Figure 2. Its high performance across multiple tasks are shown by \u2461 in Figure 1. Moreover, it is also the first work that enables multi-task learning through SNNs with RL settings, marking a notable step toward generalist agents.\nContributions: The main contributions of this work are summarized as the following.\n\u2022 We investigate the limitations of the state-of-the-art RL-based methods for DNN and SNN domains. Specifically, we perform an experimental case study to evaluate the performance of DQN [16] and DSQN [3] when learning multiple tasks subsequently. This study shows that the existing state-of-the-art still struggle to learn multiple tasks.\n\u2022 We propose a novel methodology (see Figure 2) to develop an SNN architecture that can learn multiple tasks subsequently. Its key idea is the inclusion of active dendrites and dueling structure to enhance the specialization"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Deep Q-Network (DQN)", "content": "The development of NN architectures for handling complex and dynamic environments is challenging. Conventional Deep Neural Networks (DNNs) struggle with tasks that require learning from multiple environments (tasks), due to the CF issue where the networks forget previously acquired knowledge from old tasks when learning new tasks [13]. This phenomenon significantly limits the applicability of DNNs in multi-task learning scenarios. To address this, DQN is proposed [16]. DQN is an RL-based algorithm that integrates the benefits of DNNs with Q-learning. Specifically, DQN employs the Q-learning algorithm, which seeks to estimate the optimal action-value (i.e., Q-value) by mapping each state to its expected future reward, guiding the agent towards the best possible actions. It also aims to improve stability in learning by storing sequences of experience in a replay buffer (memory). This approach allows agents to effectively learn optimal policies within complex environments by leveraging the representation capability of DNNs. The state-of-the-art DQN architecture is illustrated in Figure 3, where there are three convolutional (CONV) layers and 2 fully-connected (FC) layers, with Q-value calculation to quantify the action value. The environment provides reward to the network for enhancing the training process, and state information as the next input."}, {"title": "2.2. Continual Learning (CL) Methods", "content": "Continual learning (CL) capabilities are crucial for generalist agents to address CF issues, and hence accommodating multi-task learning in real-world scenarios [12, 14, 15]. Toward this, many methods have been developed [15, 33], and the state-of-the-art include the architectural-, replay-, and regularization-based methods."}, {"title": "2.2.1. Architectural-based Methods", "content": "These methods employ techniques where a dedicated architecture enhancement (e.g., layer) is added for each new task learning, hence reducing interference with previous tasks. However, these architectures face scalability issues as task numbers grow, hence limiting their applicability in real-world settings. To address this scalability issue, recent research explores bio-inspired architectures to enhance learning in dynamic environments, such as by incorporating \u201cactive dendrites\u201d [6]. Active dendrites allow networks to dynamically restrict and route information contextually, thus facilitating efficient multi-task learning while minimizing forgetting. The impact of active dendrites is also studied for SNN domain [18]. Its experiments underscore the potential of using active dendrites to improve CL capabilities, making it relevant to environments where information is presented sequentially."}, {"title": "2.2.2. Replay-based Methods", "content": "These methods produce synthetic samples of previously learned tasks [28] to be presented again during the training process for updating the networks' knowledge, therefore interleaving on new tasks and reintroducing the model to previously learned tasks. However, these methods typically need significant memory and computational overheads, making them suboptimal for deployment at the edge."}, {"title": "2.2.3. Regularization-based Methods", "content": "These methods aim to regularize network parameters so that they can preserve the old yet important knowledge, while learning new information. Here, the elastic weight consolidation (EWC) technique is a widely used. Its key idea is to stabilize important weights, and hence preserving knowledge across multiple tasks [7, 15, 33]. EWC calculates the significance of each weight for previous tasks, then protect these important weights when learning new tasks by adding a regularization term to the loss function, penalizing large changes to important weights, thus helping the model retain knowledge from earlier tasks while adapting to new ones."}, {"title": "3. The MTSpark Methodology", "content": ""}, {"title": "3.1. Spiking Neuron with Active Dendrites", "content": "To enable multi-task learning, we equip our spiking neuron model with active dendrites, which modulate the spiking activity of the integrate-and-fire (IF) neurons based on task-specific context signals; see the illustration in Figure 4. The context signal (c) is provided as input to all the dendrites on each IF neuron. After introducing active dendrites to the IF neurons, the incoming spikes contribute to the membrane potential of the respective neuron, whise behavior can be stated as Equation 1.\n$v(t) = (1 - \\lambda) V(t - \\Delta t) + \\lambda \\left( \\sum_{i} s_{i}(t), \\underset{j}{max}(dc_{j}) \\right)$ (1)\nHere, $d_{j}$ are the dendritic weights associated with the $j^{th}$ dendritic segment, and $f(.)$ is a modulating function. Meanwhile, $V(t)$ is the membrane potential at time t and $s_{i}(t)$ is the incoming spike from the ith presynaptic neuron If the membrane potential $V(t)$ crosses a predefined threshold $V_{th}$, the neuron fires a spike.\n$s(t) =\\begin{cases}1 \\text{ if } V(t) \\geq V_{th}, \\\\0 \\text{ otherwise} \\end{cases}$ (2)"}, {"title": "3.1.1. Dendritic Segment", "content": "Each IF neuron can be equipped with any number of active dendrites in each dendritic segment. In the MTSpark, we utilize the same number of dendrites as the number of tasks to ensure efficient implementation."}, {"title": "3.1.2. Context Signals", "content": "To enable task differentiation, we represent each task (Ti) with a unique one-hot-encoded vector (c), referred to as the context signal. The context signal functions as a task identifier, modulating the neuron activations in order to bias the network toward task-relevant pathways and minimize interference between different tasks."}, {"title": "3.1.3. Modulating Function", "content": "We define the modulation function $f(.)$ for active dendrites as Equation 3. Here, the sum of spikes from all presynaptic neurons are weighted by $\\sigma(\\underset{j}{max}; (dc_{j}))$.\n$f\\left( \\sum_{i} s_{i}(t), \\underset{j}{max}(dc_{j}) \\right) = \\sigma(\\sum_{i} s_{i}(t)) \\sigma(\\underset{j}{max}(dc_{j})) $ (3)"}, {"title": "3.1.4. Task-Specific Spiking Activity", "content": "Each IF neuron with active dendrites can selectively adjust its response to a context signal based on the modulation provided by maximal dendritic activation. This modulation provides the network with a form of \"gating\" that prevents interference between tasks and minimizing the CF effects. Different tasks exhibit different dendritic activations, and as a result, they contribute differently to the membrane potential of each neuron, leading to different spiking patterns and \"sub-networks\" optimized for each task."}, {"title": "3.2. Network Architecture Design", "content": "This step aims to develop an SNN architecture that facilitates multi-task learning. To achieve this, we leverage the DQN architecture from the state-of-the-art shown in Figure 3, and modify it for adapting to spiking domain called the deep spiking Q-network (DSQN); see its network architecture in Figure 5. In the last layer of DSQN, we implement non-spiking neurons to accumulate incoming spikes in the final layer of the network, then the maximum membrane potential from the non-spiking neurons is taken as the representation of the Q-value. This way, the Q-value can be computed despite it is performed in the spiking domain.\nAfterward, we leverage the DSQN architecture to develop a novel MTSpark network that incorporates active dendrites for enhancing its learning quality. In general, MTSpark network features three CONV layers with kernel sizes of 8, 4, and 3 and strides of 4, 2, and 1, respectively, each followed by batch normalization and spiking IF neuron layer. Input observations are processed through these layers, producing feature maps that are flattened and passed to a fully connected layer with 512 units. A spiking neuron layer equipped with active dendrites then incorporates context-signals, followed by a final FC layer producing the Q-value over all possible actions in the environment."}, {"title": "3.3. Training Strategy", "content": "This step aims to define the training scenario, replay buffer size, exploration rate, and target function, which are important for determining the appropriate training strategy.\nTraining Scenario and Replay Buffer: We train the network on each environment for P episodes before switching to the next environment. To facilitate this, we maintain a fixed size replay buffer (N) to efficiently store transitions for each environment."}, {"title": "4. Evaluation Methodology", "content": "We evaluate MTSpark methodology through Python-based implementation, and then run it on the Nvidia RTX A6000 GPU machines. Here, we evaluate both MTSpark_AD and MTSpark_ADD architectures. For comparison partners, we use the state-of-the-art Deep Q-Network (DQN) [16] and Deep Spiking Q-Network (DSQN) [3]. Apart from that, we also implement their variants with dueling technique, i.e., DQN with dueling structure (DQN_D) and DSQN with dueling structure (DSQN_D). The experimental setup parameters are presented in Table 3.\nEnvironments: We consider three Atari games (i.e., Pong, Breakout, and Enduro) as the target environments, and utilize 18 possible actions. This approach maintains consistency in the action space across all environments. We also use observations represented as 210\u00d7160\u00d73 RGB images, identical to those presented to human players.\nTraining Settings: All the models are trained considering the Adam optimizer, a static learning rate of 1e-4, and a batch size of 64 over 4 million frames in each environment. Following the best practices, we apply a discount factor \u03b3 of 0.99 to stabilize learning, use a replay buffer of size 220 for each environment to store transitions for experience replay, and decay exploration from \u20ac = 1.00 to \u20ac = 0.10 over 1 million total frames. We collect information on 25 episodes while training in each environment before switching to the next environment."}, {"title": "5. Results and Discussion", "content": ""}, {"title": "5.1. Evaluation on Atari Games", "content": "Figure 7 and Table 4 present the performance of different network models across three Atari games. We observe that models that do not contain spiking components (i.e., DQN and DQN_D) struggle the most, as they show significant performance degradation after point 1 in Pong and point \u25cf in Enduro. The reason is spiking networks benefit from their temporal information related to action dependencies. Our models, MTSpark_AD and MTSpark_ADD demonstrate significantly more balanced performance across these tasks, with results that match or exceed human-level scores in two of the three environments; see label 3. In Pong, MTSpark_ADD achieves the highest score among the models (-5.4), approaching human performance (-3) and outperforming all other models; see label 36. MTSpark_AD also performs strongly with a score of -9.4. In contrast, DQN, DSQN, DQN_D, and DSQN_D achieve lower scores, ranging from -11.2 (DSQN) to -20.2 (DQN_D), reflecting their struggle to match the learning efficacy of MTSpark in this environment. Similarly, in Enduro, MTSpark_ADD achieves score of 371.2, slightly surpassing human performance (i.e., 368) and demonstrating the highest performance amongst different models. MTSpark_AD also performs competitively with a score of 363.2, while other models do not reach the performance levels of the MTSpark models or humans. The reason for all these improvements is because MTSpark models employ active dendrites and/or dueling structure that help identifying the context and task-specific sub-networks during the learning process.\nBreakout represents a challenging environment across all models, as indicated by label 4. Here, DQN and our MTSpark_AD achieve two highest scores with 3.2 and 2, respectively, but this remains far below human-level performance with score of 31. In general, none of the models are able to learn effectively in Breakout, indicating that the Breakout environment could pose challenges with regards to sparse rewards and long-term action dependencies.\nIn summary, these results highlight the ability of our MTSpark excels in gaining and retaining the task-specific knowledge without incurring significant additional parameters, hence achieving high performance in multi-task learning efficiently. Meanwhile conventional DNNs struggle to maintain consistent performance in multi-task learning."}, {"title": "5.2. Evaluation on Image Classification", "content": "In addition to its performance in RL tasks, we also explore the performance of our MTSpark when trained on three different image classification datasets (i.e., MNIST [5], Fashion MNIST [35], and CIFAR-10 [8]). We also compare this performance against the DNN counterpart. Here, DNN is a compact network designed for image classification. It starts with three CONV layers, having 32, 64, and 64 filters of sizes 8\u00d78, 4x4, and 3\u00d73 respectively followed by batch normalization and ReLU activation. After flattening, a FC layer with 128 units processes the features, followed by a final layer mapping to 10 output classes. MTSpark mirrors this DNN architecture, but replaces the ReLU activations with IF neurons with active dendrites.\nMTSpark demonstrates superior performance in comparison to the DNN when trained on MNIST, FashionMNIST and CIFAR-10, as shown in Figure 8. For MNIST, the DNN achieves only modest learning performance, reaching around 57% accuracy by epoch 50 (see 6), whereas MTSpark shows a steep learning curve, achieving over 92% by epoch 10 and culminating at approximately 98% by the end of training (see 6). Similar trends are observed for the Fashion-MNIST dataset, as the DNN suffer from low learning quality, i.e., achieving only 29% accuracy after 50 epochs (see 6), while MTSpark quickly obtains over 84% accuracy by epoch 10 and reaches around 88% by the end (see ). The CIFAR-10 dataset, which typically requires higher model complexity, further highlights the differences between the models. Here, MTSpark started with an initial accuracy of 37%, steadily improving to around 57% by the final epoch (see 6), while the DNN struggles to exceed 32% (see 6). These results indicate that MTSpark provides a more efficient training process and a greater ability to generalize across varied datasets. The reason is that, active dendrites in MTSpark effectively provides accurate context information on the task identity. These results also further highlight that our MTSpark methodology provides a promising approach for multi-task learning settings."}, {"title": "5.3. Further Discussion", "content": ""}, {"title": "5.3.1. Model Sizes", "content": "DSQN and MTSpark_AD have nearly identical parameter counts, indicating that equipping the IF neurons with active dendrites does not incur significant parameter overhead."}, {"title": "5.3.2. Efficient Multi-Task Adaptation", "content": "MTSpark is designed to handle multiple tasks without expanding its parameter count significantly, thereby achieving efficiency in both memory usage and computational demands. Parameter count of MTSpark also remains comparable to that of the conventional architectures, suggesting that its performance is not derived merely from an increase in model complexity. Unlike conventional multi-task RL frameworks that often rely on separate, task-specific network modules or require extensive off-line data storage, our MTSpark utilizes a unified parameter set across all tasks, relying only on the context signals to differentiate tasks and exhibit varying network behavior. Furthermore, our MTSpark also leverages context signals and dendritic modulation to adapt to multiple tasks simultaneously, unlike the approaches that freezing certain parts of the network, and adding task-specific modules or extensive replay."}, {"title": "5.3.3. Ablation Study on Different Multi-Task Scenarios", "content": "We also conduct an ablation study to investigate the impact of the convetional DNN and MTSpark architectures on different multi-task scenarios. To do this, we perform experimental case studies for two scenarios: (1) the two-task learning considering MNIST and Fashion MNIST datasets; and (2) the three-task learning considering MNIST, Fashion MNIST, and Imagenette datasets. Since we employ 10 output actions (i.e., classes) in the network architectures, we use 10-class subset of ImageNet (i.e., Imagenette).\nIn two-class learning, the DNN can only achieves high accuracy on a single task (i.e., MNIST); see Figure 9(a). The reason is that, this DNN is designed to only learn a specific task, hence it struggles to learn a new task, which is indicated by frequent fluctuations in the corresponding learning curve. Meanwhile, our MTSpark can achieve high accuracy on both tasks with smooth learning curves and at the early training epoch; see Figure 9(b). These indicate that MTSpark can perform learning easily, due to the active dendrites that effectively modulate the spiking activity based on the context signals.\nIn three-class learning, the DNN can only achieves high accuracy on MNIST, moderate accuracy on Fahion MNIST, and very low accuracy on Imagenette; see Figure 10(a). Since DNN is designed to only learn a specific task, adding"}, {"title": "6. Conclusion", "content": "We propose the MTSpark methodology for enabling efficient multi-task learning leveraging both RL and SNN concepts. MTSpark employs active dendrites to enable a dynamic and context-sensitive task differentiation within a unified network structure. It also incorporates dueling structure to further enhance the performance. Experimental results show that our MTSpark outperforms the state-of-the-art methods for multi-task learning in Atari games (i.e., Pong, Breakout, and Enduro), achieving human-level performance. MTSpark also outperforms the state-of-the-art methods in image classification (i.e., MNIST, Fashion MNIST, and CIFAR-10). These show the potential of our MTSpark methodology for developing generalist agents, that can learn multiple tasks with efficient resource cost."}]}