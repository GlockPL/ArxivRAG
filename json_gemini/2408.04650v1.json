{"title": "Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools", "authors": ["Jung In Park", "Mahyar Abbasian", "Iman Azimi", "Dawn T. Bounds", "Angela Jun", "Jaesu Han", "Robert M. McCarron", "Jessica Borelli", "Jia Li", "Mona Mahmoudi", "Carmen Wiedenhoeft", "Amir M. Rahmani"], "abstract": "This study aims to develop and validate an evaluation framework to ensure the safety and reliability of mental health chatbots, which are increasingly popular due to their accessibility, human-like interactions, and context-aware support. We created an evaluation framework with 100 benchmark questions and ideal responses, and five guideline questions for chatbot responses. This framework, validated by mental health experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation methods explored included large language model (LLM)-based scoring, an agentic approach using real-time data, and embedding models to compare chatbot responses against ground truth standards. The results highlight the importance of guidelines and ground truth for improving LLM evaluation accuracy. The agentic method, dynamically accessing reliable information, demonstrated the best alignment with human assessments. Adherence to a standardized, expert-validated framework significantly enhanced chatbot response safety and reliability. Our findings emphasize the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots. While LLMs have significant potential, careful implementation is necessary to mitigate risks. The superior performance of the agentic approach underscores the importance of real-time data access in enhancing chatbot reliability. The study validated an evaluation framework for mental health chatbots, proving its effectiveness in improving safety and reliability. Future work should extend evaluations to accuracy, bias, empathy, and privacy to ensure holistic assessment and responsible integration into healthcare. Standardized evaluations will build trust among users and professionals, facilitating broader adoption and improved mental health support through technology.", "sections": [{"title": "INTRODUCTION", "content": "Mental health chatbots are pivotal in providing accessible support and interventions in everyday settings. Automated systems for mental health assistance have employed technologies ranging from rule-based software to standalone applications for therapy, training, and screening[1]. The integration of LLMs marks a significant advancement, enhancing the sophistication of interactions with more human-like, context-aware conversations. They offer the potential for a judgment-free environment where individuals can receive support, enhancing engagement and effectiveness[2]. However, LLMs introduce unique challenges such as the risk of safety, lack of trustworthiness, hallucinating inaccurate advice requiring human oversight, and oversight and the need for secure handling of sensitive information [3]. In this study, we focus solely on the safety of the responses provided by LLM-based chatbots.\n The chatbot's adherence to high clinical safety standards can build trust among users and healthcare professionals, thereby promoting the wider adoption of LLM-based chatbots in mental health care. Mental health chatbots should ensure clarity to prevent the generation of offensive content or inappropriate reactions in sensitive situations[4,5]. They should prioritize accurate risk detection and the necessary reporting protocols, especially in cases involving harm, such as suicidal or homicidal thoughts, child or elder abuse, and intimate partner violence[6-8].\n Additionally, chatbots should refrain from responding to offensive inputs from users, which could potentially escalate situations where individuals are contemplating self-harm[9]. To address these challenges, evaluation methods must incorporate both automated monitoring and human expertise and judgment to effectively and safely identify potential safety risks[3]. However, existing evaluation methods in the literature exhibit notable gaps and fail to address these safety concerns adequately[10-12]. First, existing evaluation methods assess chatbots'"}, {"title": "METHODS", "content": "For the first aim of the study, to effectively evaluate the safety aspects of chatbots' responses, we developed a two-phase evaluation process and metrics: 1) 100 benchmark questions and ideal responses for the chatbot, covering varied clinical scenarios to collect the chatbot's responses; 2) 5 guideline questions (10-point Likert scale) for healthcare professionals to evaluate these responses from the chatbot. This combined approach allows for a clear understanding of a chatbot's capabilities and limitations. After development, we tested these metrics using the real- world application based on GPT-3.5-turbo to assess the feasibility.\nBenchmark questions provide a standard set of queries that can be used to test different chatbots or the same chatbot at different points in time. This consistency is crucial for making objective comparisons. Also, they ensure that the chatbot is tested across a broad range of topics and scenarios, including sensitive and complex issues. This comprehensive approach helps"}, {"title": "Evaluation Metric Development", "content": "For the first aim of the study, to effectively evaluate the safety aspects of chatbots' responses, we developed a two-phase evaluation process and metrics: 1) 100 benchmark questions and ideal responses for the chatbot, covering varied clinical scenarios to collect the chatbot's responses; 2) 5 guideline questions (10-point Likert scale) for healthcare professionals to evaluate these responses from the chatbot. This combined approach allows for a clear understanding of a chatbot's capabilities and limitations. After development, we tested these metrics using the real- world application based on GPT-3.5-turbo to assess the feasibility."}, {"title": "Benchmark Questions", "content": "Benchmark questions provide a standard set of queries that can be used to test different chatbots or the same chatbot at different points in time. This consistency is crucial for making objective comparisons. Also, they ensure that the chatbot is tested across a broad range of topics and scenarios, including sensitive and complex issues. This comprehensive approach helps identify any potential weaknesses or biases in the chatbot's responses. Additionally, other researchers or developers can use the same benchmark questions to reproduce tests and verify results, which is fundamental for scientific and technological progress.\nTo accurately assess the chatbot's responses to a range of mental health-related clinical scenarios, we carefully selected and developed situations and questions that closely mimic those a patient would likely ask in real life. These situations include mental health crises, anxiety attacks, depression symptoms, panic attacks, coping with grief, insomnia, managing anger, post- traumatic stress, substance abuse, chronic loneliness, coping with life transitions, fear, struggling with identity, mood swings, low self-esteem, aging, eating disorder concerns, and burnout. We also created ideal responses for each question to ensure that the chatbot's answers convey all the necessary information that should be communicated in a real-world situation. These responses can serve as a gold standard to evaluate the chatbot's actual responses.\nWe purposefully included several similar clinical situations to assess the chatbot's consistency in its responses. Four mental health experts thoroughly reviewed these situations, along with the questions and ideal answers, to ensure they encompass a broad spectrum of inquiries users might have and prioritize safety."}, {"title": "Guideline Questions", "content": "Evaluating the diverse aspects of a mental health chatbot's safety is crucial for ensuring the tool is effective, safe, and reliable in aiding users with their mental health concerns. Evaluation guideline questions provide clear criteria for evaluating the responses, which helps reduce subjective interpretations by different evaluators. This objectivity is crucial for consistent scoring. In addition, they allow for a more detailed assessment of chatbot responses, not just in terms of correctness but also appropriateness and engagement. This granularity helps in identifying specific areas of improvement. By having a set of guidelines, all evaluators assess responses based on the same standards. This standardization is important for aggregating results from multiple evaluators, ensuring that the final scores accurately reflect the chatbot's performance.\nWe examined five safety aspects to formulate our evaluation metrics. First, adherence to practice guidelines. It is essential to ensure that the chatbot's recommendations are in line with established mental health practices and protocols. This adherence minimizes the risk of harm to the user and guarantees that the advice given is evidence-based and clinically valid. It is vital for maintaining the integrity of the mental health support provided and for ensuring that users receive care that is consistent with current medical understanding and treatment standards. Second, identification and management of health risks. It plays a critical role in recognizing users who are at risk of harm or in crisis situations. It ensures that appropriate interventions are suggested, such as directing users to emergency services or providing immediate support measures. This evaluation aspect is key to preventing the escalation of risk and ensuring the chatbot can act effectively in safeguarding users' well-being. Third, the consistency of responses in critical situations. It ensures that the chatbot provides reliable and stable support across different scenarios. This consistency is crucial for building trust and dependability, as it ensures users receive the same high-quality advice regardless of when or how often they seek help. It fosters a sense of reliability and security in the chatbot's ability to support users facing mental health challenges. Fourth, assessment of resource provision. It ensures that the chatbot effectively directs users to appropriate resources, be it further reading, support groups, or professional help. This enhances the immediate support provided by the chatbot and enriches the user's support network by connecting them with additional resources for their mental health."}, {"title": "LLM-based Evaluation Tools", "content": "For our second aim, we present three different automated scoring methods: 1) LLM- based scoring of the chatbot responses, 2) agentic approach to evaluate the generated answers with external sources, and 3) answer-ground truth similarity test using embedding models. We then evaluated the efficacy of the automated scoring methods using LLMs and vector representations to assess their alignment with professional evaluations provided by doctors.\nFirst, we used \"judge LLMs\" to score the chatbot responses. The concept of utilizing LLMs as evaluative judges has been explored in a recent study[23]. This approach enhances the objectivity and consistency of evaluation processes, demonstrating the growing trend of leveraging advanced LLMs for automated assessments. We employed four different judge LLMs for automated evaluations: GPT-4, Mistral (mistral-large-latest), Claude-3-Opus, and Gemini- 1.0. Each judge LLM was tasked with evaluating responses using three distinct strategies. Initially, for what we refer to as Method1, we presented only the scenario, a user question, and a corresponding response generated by the chatbot (GPT-3.5-turbo). The judge LLMs were then instructed to assign a score out of 10, reflecting the adequacy of the response. In the second strategy, for what we refer to as Method2, we enhanced the initial prompt by including our specifically designed evaluation guidelines, requesting the judge LLMs to score each guideline individually on a scale of 10. For the final strategy, or Method3, we further augmented the prompt with the addition of ground truth data from benchmark, directing the judge LLMs to re- evaluate each guideline individually.\nWe developed a second automated strategy using an Agentic method for evaluating responses provided by GPT3.5. This method employs LLM-based agents, a specialized type of LLM that significantly improves the process of generating and refining text, akin to how a skilled human writer crafts a document. Typically, LLMs function in a \"zero-shot\" mode, where they generate text in a single attempt without any revisions. However, the Agentic method incorporates an \"agentic workflow\" which allows LLMs to undergo multiple iterative processes similar to a human writer's method\u2014planning, researching, drafting, reviewing, and revising. This iterative process enables the LLM to produce higher quality outputs and approach tasks more strategically.\nIn this strategy, we aimed to enhance the quality of the prompts fed to the LLM to achieve more accurate evaluation scores. We introduced an evaluator agent equipped with the ability to connect to external information sources. The goal was to utilize guidelines and real- time data by accessing up-to-date and reliable online resources relevant to the evaluated questions and answers, thereby improving the agent's evaluation capabilities.\nTo implement this, we integrated openCHA[24] with a search feature limited to healthcare-related resources and an extractor tool. This tool extracts all textual content from a given website URL for the agent's use. The agent then strategically plans its approach tailored to each specific question-answer pair, conducts a targeted Google search, and processes text from the top returned URL."}, {"title": "RESULTS", "content": "For the first aim of the study, we first developed 100 benchmark questions along with ideal responses for chatbots, addressing a variety of clinical situations related to mental health (see Appendix 1), such as crisis situations, stress-induced insomnia, and anger management.\nFor our second aim, we present three different automated scoring methods: 1) LLM- based scoring of the chatbot responses, 2) agentic approach to evaluate the generated answers with external sources, and 3) answer-ground truth similarity test using embedding models. We then evaluated the efficacy of the automated scoring methods using LLMs and vector representations to assess their alignment with professional evaluations provided by doctors."}, {"title": "Evaluation Metric Development", "content": "For the first aim of the study, we first developed 100 benchmark questions along with ideal responses for chatbots, addressing a variety of clinical situations related to mental health (see Appendix 1), such as crisis situations, stress-induced insomnia, and anger management."}, {"title": "DISCUSSION", "content": "The rapid progress in LLMs has led to the widespread launch of many mental health chatbots. While these chatbots show great potential with their accessibility, intelligence, and human-like responses, there is a lack of systematic evaluation of these mental health chatbots' clinical safety. In this study, we introduced 100 benchmark questions covering various clinical mental health situations and ideal responses; we also created 5 guideline questions to assess the safety of the chatbots' responses, validated by the mental health experts. Evaluating the safety of chatbot responses is a multifaceted process that necessitates both benchmark questions and evaluation guideline questions for scoring. Benchmark questions are critical for objectively comparing chatbots, and evaluation guidelines standardize response assessment by providing clear criteria that minimize subjectivity and ensure consistent scoring. Standardization across evaluators allows for accurate aggregation of results and reflects the chatbot's true performance. Our systematic, structured evaluation framework aims to significantly advance the field of mental health chatbots by promoting their responsible and effective integration into healthcare.\nFor the evaluation of the chatbot's responses using the metrics we developed, mental health experts generally found the chatbot's adherence to practice guidelines to be satisfactory. However, there were concerns that certain responses were too vague to be properly matched against these guidelines. Also, direct commands (e.g., 'Seek help immediately') might be more effective than general recommendations (e.g., 'It is important to seek help...') from the chatbot's response. Also, there was variability in the recommendation of specific help resources\u2014some responses included them, while others lacked the detail.\nThe results of our LLM-based automated methods underscore the significance of guidelines and ground truth in aiding LLMs to more accurately evaluate models. The Agent method, in particular, benefits from dynamically accessing reliable information on the internet using these elements, resulting in a more effective prompt that enhances scoring accuracy. This is supported by the findings in Figures 2 and 3, where the Agent method's score distributions are notably closer to zero, indicating better alignment with human evaluations.\nThe Agent method stands out as it does not show increased differences as scores increase, and its mean differences are closer to zero. However, it has a slightly larger confidence interval, suggesting less certainty in its scoring compared to other methods.\nWe developed the evaluation metrics for clinical safety in this study. However, a mental health chatbot needs to be evaluated in multiple areas, such as accuracy, bias and fairness, empathy, and privacy. Future work is required to develop metrics for such areas, as these dimensions are critical to ensuring that chatbots can provide effective, equitable, and ethical support to users."}, {"title": "CONCLUSION", "content": "Advancements in LLMs have led to a significant increase in mental health chatbots, offering accessible support. However, a systematic way of evaluating clinical safety of such chatbots is lacking. To address this gap, we developed 100 benchmark questions and 5 guideline questions for safety assessment. Our framework aims to promote the responsible integration and effectiveness of healthcare chatbots, thereby enhancing their safety and building trust among users and professionals. Additionally, we examined LLM-based evaluation tools to streamline the evaluation process, presenting the effectiveness and limitations of these tools for mental health chatbots. The Agent and Embedding methods demonstrated the most accurate alignments with human assessments."}, {"title": "Competing Interests", "content": "The authors declare no competing interests."}, {"title": "Funding", "content": "This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors."}, {"title": "Contribution Statement", "content": "JP was responsible for conceptualization, formal analysis, methodology, supervision, and writing of the original draft, as well as review and editing. MA contributed to conceptualization, formal analysis, methodology, visualization, and writing review and editing. IA was involved in conceptualization, methodology, and writing review and editing. DB, AJ, JH, RM, and JB handled formal analysis and validation. JL, MM, CW, and AR participated in conceptualization, methodology, and writing review and editing."}]}