{"title": "Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools", "authors": ["Jung In Park", "Mahyar Abbasian", "Iman Azimi", "Dawn T. Bounds", "Angela Jun", "Jaesu Han", "Robert M. McCarron", "Jessica Borelli", "Jia Li", "Mona Mahmoudi", "Carmen Wiedenhoeft", "Amir M. Rahmani"], "abstract": "This study aims to develop and validate an evaluation framework to ensure the safety and reliability of mental health chatbots, which are increasingly popular due to their accessibility, human-like interactions, and context-aware support. We created an evaluation framework with 100 benchmark questions and ideal responses, and five guideline questions for chatbot responses. This framework, validated by mental health experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation methods explored included large language model (LLM)-based scoring, an agentic approach using real-time data, and embedding models to compare chatbot responses against ground truth standards. The results highlight the importance of guidelines and ground truth for improving LLM evaluation accuracy. The agentic method, dynamically accessing reliable information, demonstrated the best alignment with human assessments. Adherence to a standardized, expert-validated framework significantly enhanced chatbot response safety and reliability. Our findings emphasize the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots. While LLMs have significant potential, careful implementation is necessary to mitigate risks. The superior performance of the agentic approach underscores the importance of real-time data access in enhancing chatbot reliability. The study validated an evaluation framework for mental health chatbots, proving its effectiveness in improving safety and reliability. Future work should extend evaluations to accuracy, bias, empathy, and privacy to ensure holistic assessment and responsible integration into healthcare. Standardized evaluations will build trust among users and professionals, facilitating broader adoption and improved mental health support through technology.", "sections": [{"title": "INTRODUCTION", "content": "Mental health chatbots are pivotal in providing accessible support and interventions in everyday settings. Automated systems for mental health assistance have employed technologies ranging from rule-based software to standalone applications for therapy, training, and screening[1]. The integration of LLMs marks a significant advancement, enhancing the sophistication of interactions with more human-like, context-aware conversations. They offer the potential for a judgment-free environment where individuals can receive support, enhancing engagement and effectiveness[2]. However, LLMs introduce unique challenges such as the risk of safety, lack of trustworthiness, hallucinating inaccurate advice requiring human oversight, and oversight and the need for secure handling of sensitive information [3]. In this study, we focus solely on the safety of the responses provided by LLM-based chatbots.\nThe chatbot's adherence to high clinical safety standards can build trust among users and healthcare professionals, thereby promoting the wider adoption of LLM-based chatbots in mental health care. Mental health chatbots should ensure clarity to prevent the generation of offensive content or inappropriate reactions in sensitive situations[4,5]. They should prioritize accurate risk detection and the necessary reporting protocols, especially in cases involving harm, such as suicidal or homicidal thoughts, child or elder abuse, and intimate partner violence[6-8]. Additionally, chatbots should refrain from responding to offensive inputs from users, which could potentially escalate situations where individuals are contemplating self-harm[9]. To address these challenges, evaluation methods must incorporate both automated monitoring and human expertise and judgment to effectively and safely identify potential safety risks[3]. However, existing evaluation methods in the literature exhibit notable gaps and fail to address these safety concerns adequately[10-12]. First, existing evaluation methods assess chatbots'"}, {"title": "METHODS", "content": "Evaluation Metric Development\nFor the first aim of the study, to effectively evaluate the safety aspects of chatbots' responses, we developed a two-phase evaluation process and metrics: 1) 100 benchmark questions and ideal responses for the chatbot, covering varied clinical scenarios to collect the chatbot's responses; 2) 5 guideline questions (10-point Likert scale) for healthcare professionals to evaluate these responses from the chatbot. This combined approach allows for a clear understanding of a chatbot's capabilities and limitations. After development, we tested these metrics using the real- world application based on GPT-3.5-turbo to assess the feasibility.\nBenchmark Questions\nBenchmark questions provide a standard set of queries that can be used to test different chatbots or the same chatbot at different points in time. This consistency is crucial for making objective comparisons. Also, they ensure that the chatbot is tested across a broad range of topics and scenarios, including sensitive and complex issues. This comprehensive approach helps"}, {"title": "RESULTS", "content": "Evaluation Metric Development\nBenchmark Questions\nFor the first aim of the study, we first developed 100 benchmark questions along with ideal responses for chatbots, addressing a variety of clinical situations related to mental health (see Appendix 1), such as crisis situations, stress-induced insomnia, and anger management.\nLLM-based Evaluation Tools\nWe implemented various evaluation methods detailed in the methodology section to assess a total of 100 questions, and we compared these results against scores provided by doctors. Detailed statistical analyses of these methods are presented in Table 3. It includes several key metrics: the Mean Absolute Error (MAE), which represents the average magnitude of the differences between doctors' scores and those from the other methods; the minimum scores; the maximum scores; the averages of the scores provided by each method; and the standard"}, {"title": "DISCUSSION", "content": "The rapid progress in LLMs has led to the widespread launch of many mental health chatbots. While these chatbots show great potential with their accessibility, intelligence, and human-like responses, there is a lack of systematic evaluation of these mental health chatbots' clinical safety. In this study, we introduced 100 benchmark questions covering various clinical mental health situations and ideal responses; we also created 5 guideline questions to assess the safety of the chatbots' responses, validated by the mental health experts. Evaluating the safety of chatbot responses is a multifaceted process that necessitates both benchmark questions and evaluation guideline questions for scoring. Benchmark questions are critical for objectively comparing chatbots, and evaluation guidelines standardize response assessment by providing clear criteria that minimize subjectivity and ensure consistent scoring. Standardization across evaluators allows for accurate aggregation of results and reflects the chatbot's true performance. Our systematic, structured evaluation framework aims to significantly advance the field of mental health chatbots by promoting their responsible and effective integration into healthcare.\nFor the evaluation of the chatbot's responses using the metrics we developed, mental health experts generally found the chatbot's adherence to practice guidelines to be satisfactory. However, there were concerns that certain responses were too vague to be properly matched against these guidelines. Also, direct commands (e.g., 'Seek help immediately') might be more effective than general recommendations (e.g., 'It is important to seek help...') from the chatbot's response. Also, there was variability in the recommendation of specific help resources\u2014some responses included them, while others lacked the detail.\nThe results of our LLM-based automated methods underscore the significance of guidelines and ground truth in aiding LLMs to more accurately evaluate models. The Agent method, in particular, benefits from dynamically accessing reliable information on the internet using these elements, resulting in a more effective prompt that enhances scoring accuracy. This is supported by the findings in Figures 2 and 3, where the Agent method's score distributions are notably closer to zero, indicating better alignment with human evaluations."}, {"title": "CONCLUSION", "content": "Advancements in LLMs have led to a significant increase in mental health chatbots, offering accessible support. However, a systematic way of evaluating clinical safety of such chatbots is lacking. To address this gap, we developed 100 benchmark questions and 5 guideline questions for safety assessment. Our framework aims to promote the responsible integration and effectiveness of healthcare chatbots, thereby enhancing their safety and building trust among users and professionals. Additionally, we examined LLM-based evaluation tools to streamline the evaluation process, presenting the effectiveness and limitations of these tools for mental health chatbots. The Agent and Embedding methods demonstrated the most accurate alignments with human assessments."}, {"title": "Competing Interests", "content": "The authors declare no competing interests."}, {"title": "Funding", "content": "This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors."}, {"title": "Contribution Statement", "content": "JP was responsible for conceptualization, formal analysis, methodology, supervision, and writing of the original draft, as well as review and editing. MA contributed to conceptualization, formal analysis, methodology, visualization, and writing review and editing. IA was involved in conceptualization, methodology, and writing review and editing. DB, AJ, JH, RM, and JB handled formal analysis and validation. JL, MM, CW, and AR participated in conceptualization, methodology, and writing review and editing."}]}