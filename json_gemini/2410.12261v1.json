{"title": "CATCH: CHANNEL-AWARE MULTIVARIATE TIME SERIES ANOMALY DETECTION VIA FREQUENCY PATCHING", "authors": ["Xingjian Wu", "Xiangfei Qiu", "Zhengyu Li", "Yihang Wang", "Jilin Hu", "Chenjuan Guo", "Hui Xiong", "Bin Yang"], "abstract": "Anomaly detection in multivariate time series is challenging as heterogeneous subsequence anomalies may occur. Reconstruction-based methods, which focus on learning nomral patterns in the frequency domain to detect diverse abnormal subsequences, achieve promising resutls, while still falling short on capturing fine-grained frequency characteristics and channel correlations. To contend with the limitations, we introduce CATCH, a framework based on frequency patching. We propose to patchify the frequency domain into frequency bands, which enhances its ability to capture fine-grained frequency characteristics. To perceive appropriate channel correlations, we propose a Channel Fusion Module (CFM), which features a patch-wise mask generator and a masked-attention mechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM is encouraged to iteratively discover appropriate patch-wise channel correlations, and to cluster relevant channels while isolating adverse effects from irrelevant channels. Extensive experiments on 9 real-world datasets and 12 synthetic datasets demonstrate that CATCH achieves state-of-the-art performance. We make our code and datasets available at https://anonymous.4open.science/r/CATCH-E535.", "sections": [{"title": "1 INTRODUCTION", "content": "Modern cyber physical systems are often monitored by multiple sensors, which produce sucessive multivariate time series data. Multivariate Time Series Anomaly Detection (MTSAD) aims to detect abnormal data in multivariate time series. It is applied widely including but not limited to financial fraud detection, medical disease identification, and cybersecurity threat detection.\nTime series anomalies are typically classified into point anomalies and subsequence anomalies. Recent reconstruction-based methods show strong capability of detecting point anomalies, which are characterized by specific values that significantly deviate from the normal range of the probability distribution. However, subsequence anomalies consist of values that fall within the probability distribution, making them much harder to detect. According to the behavior-driven taxonomy , the subsequence anomalies can be further divided into seasonal, shapelet, and trend anomalies. A promising approach is to transform the time series into the frequency domain to better derive the subsequence anomalies.\nWhen transformed into the frequency domain, distinct subsequence anomalies also show prominent differences against the normal series in different frequency bands. In this case, shapelet anomalies mainly affect the third frequency band while seasonal anomalies affect the first two frequency bands. However, the frequency domain features a long-tailed distribution that most information centralizes in the low frequency bands. Coarse-grained reconstruction-based methods may neglect the details in the high frequency bands, thus failing to detect correspond anomalies, which calls for fine-grained modeling in each frequency band to precisely reconstruct the normal patterns, so that heterogeneous subsequence anomalies can be detected. Moreover, considering the relationships among channels also promotes better reconstruction for normal patterns.  shows a multivariate time series with three channels, and we observe the varying channel associations in different frequency bands, where Channel 1 and Channel 2 are similar in the third band but dissimilar to Channel 3, and all channels are similar in the fourth band but show dissimilarity in the fifth. However, the commonly-used Channel-Independent (CI) and Channel-Dependent (CD) strategies exhibit polarization effects, rendering them inadequate for this task. CI uses the same model across different channels and overlooks potential channel correlations, which offers robustness but lacks generalizability and capacity. CD considers all channels simultaneously with larger capacity, but may be susceptible to noise from irrelevant channels, thus lacking robustness. This calls for flexibly adapting the distinct channel interrelationships in different frequency bands.\nInspired by the above observations, we propose CATCH, a Channel-Aware MTSAD framework via frequency patching. Technically, we utilize Fourier Transformation to stretch across time and frequency domains to facilitate the detection of both point and subsequence anomalies, of which the latter can be improved by patching in the frequency domain for fine-grained modeling. To flexibly utilize the channel correlations in frequency bands, we propose a Channel Fusion Module (CFM) that incorporates a channel correlation discovering mechanism and utilizes masked attention through a bi-level multi-objective optimization process. Specifically, we utilize a patch-wise mask generator to adaptively discover channel correlation for each frequency band. The discovered channel correlation is between CI and CD, providing both the capacity and robustness by clustering relevant channels while isolating the adverse effects from irrelevant channels. The contributions are summarized as follows:\n\u2022 We propose a general framework called CATCH, which enables simultaneous detection of heterogeneous point and subsequence anomalies via frequency patch learning. The framework enhances subsequence anomaly detection through frequency-domain patching and integrates fine-grained adaptive channel correlations across frequency bands.\n\u2022 We design the CFM to fully utilize the fine-grained channel correlations. Driven by a bi-level multi-objective optimization algorithm, the CFM is able to iteratively discover appropriate channel correlations and facilitate the isolation of irrelevent channels and the clustering of relevent channels, which provides both the capacity and robustness.\n\u2022 We conduct extensive experiments on 21 multivariate datasets. The results show that CATCH outperforms state-of-the-art baselines. Additionally, all datasets and code are avaliable at https://anonymous.4open.science/r/CATCH-E535."}, {"title": "2 RELATED WORK", "content": "2.1 MULTIVARIATE TIME-SERIES ANOMALY DETECTION (MTSAD)\nTraditional MTSAD methods can be classified into non-learning and machine learning. Recently, deep learning methods have shown exceptional MTSAD performance and have received substantial extensive attention. They can be classified into forecasting-based, reconstruction-based and contrastive-based methods. GDN is a forecasting-based model that uses a graph structure to learn topology and a graph attention network to encode input series, with anomaly detection based on the maximum forecast error among channel variables. Anomaly Transformer is a reconstructive approach that combines series and prior association to make anomalies distinctive. DCdetector uses contrastive learning in anomaly detection to create an embedding space where normal data samples are close together and anomalies are farther apart . We focus on the reconstruction-based methods due to their prominent performance on commonly-used benchmark datasets.\n2.2 CHANNEL STRATEGIES IN MTSAD\nRegardless the above mentioned learning mechanism, a core issue is to model channel correaltions for better MTSAD. There are mainly two existing approaches that consider relationships among channels. Channel-Independent (CI) based methods such as: PatchTST and DLinear impose the constraint of using the same model across different channels. While it offers robustness, it overlooks potential interactions among channels and can be limited in generalizability and capacity for unseen channels. Previous studies have shown that correlation discovery in data is crucial for time series anomaly detection . Channel-Dependent (CD) based method such as: MSCRED uses a convolutional-LSTM network with attention and a loss function to reconstruct correlation matrices among channels in multivariate time series input. iTransformer embeds time points into variate tokens and applies an attention mechanism to capture multivariate correlations. MTAD-GAT treats each univariate time series as a feature and uses two parallel graph attention layers to capture dependencies across both temporal and channel dimensions. The existing methods could not adequately extract interrelationships, they may be susceptible to noise from irrelevant channels, reducing the model's robustness.\n2.3 FREQUENCY DOMAIN ANALYSIS FOR MTSAD\nFrequency-based models for MTSAD have gained significant attention in recent years. The Spectral Residual (SR) technique introduced in SR-CNN leverages a frequency-based approach to create a saliency map for MTSAD. PFT uses partial Fourier transform to achieve substantial speedup without sacrificing accuracy. TFAD leverages frequency domain analysis with augmentation and decomposition. Nevertheless, the existing methods do not offer the point-granularity alignment for the frequency domain, still facing challenges in precisely detecting subsequence anomalies. Dual-TF employs both the time and frequency domains for MTSAD, effectively addressing the granularity discrepancy between them. However, rather than introducing novel architectural changes, it adopts the existing anomaly-transformer model. Our proposed CATCH offers fine-grained insights in the frequency domain and discovers appropriate channel correlations to fully reconstruct the normal patterns, thus strengthening the detection of subsequence anomalies."}, {"title": "3 CATCH", "content": "In the context of time series anomaly detection, the time series $X \\in \\mathbb{R}^{N \\times T}$ from the observed successive systems is represented by a sequence of equally spaced time points < $X_{:,1}, X_{:,2},\\cdots, X_{:,T}$ >, where $X_{:,t} \\in \\mathbb{R}^{N},t = 1,2,\\dots,T$ denotes the values of the observed N features in time step t. The multivariate time series anomaly detection problem is to determine whether $X_{:,t}$ is anomaly or not. For clear delineation, we separate dimensions with commas and use this format throughout this paper. For example, we denote $X_{i,j}$ as the i-th channel at the j-th timestamp, $X_{n,:}\\in \\mathbb{R}^{T}$ as the time series of n-th channel, where $n = 1,2,\\dots, N$.\n3.1 STRUCTURE OVERVIEW\nFigure 2 shows the overall architecture of the CATCH, which consists of three main moudles, the Forward Module, the Channel Fusion Module and the Time-Frequency Reconstruction Module.\nIn the Forward Module, we first apply the Instance Normalization to unify the distribution of training and testing data. To model time series in both time and frequency domains, we then utilize the efficient FFT to transform time series into orthogonal trigonometric signals in the frequency domain, where we keep both the real and imaginary (imag) parts through $X_{R}, X_{I} = FFT(X)$ for maximum information retention, where $X, X_{R}, X_{I} \\in \\mathbb{R}^{N\\times T}$. Additionally, to capture fine-grained details in different frequency bands, we apply the patching operation in the frequency domain, the process is formalized as follows:\n$\\begin{equation} \\{P^{R}_{1}, P^{R}_{2},..., P^{R}_{L}\\} = Patching(X_{R}), \\{P^{I}_{1}, P^{I}_{2}, ..., P^{I}_{L}\\} = Patching(X_{I}), \\tag{1} \\end{equation}$\nwhere $P^{R}_{i}, P^{I}_{i} \\in \\mathbb{R}^{N\\times p}$ denote the i-th patch of $X_{R}$ and $X_{I}$. $L = \\lceil[T \\text{--} p]/s + 1\\rceil$ is the total patch number, where p is the patch size and s is the patch stride. We then concat each pair of $P^{R}_{i}$ and $P^{I}_{i}$ into $P_{i} \\in \\mathbb{R}^{N\\times 2p}$, as the i-th frequency patch. After patching in the frequency domain, the frequency patches are then projected into the high-dimensional hidden space through $P^{\\prime}_{i} = Projection(P_{i})$.\nThen, we utilize the Channel Fusion Module (CFM) to model the patch-wise channel correlations:\n$\\begin{equation} \\{P_{1}, P_{2}, \\cdots, P_{L}\\} = CFM(\\{P^{\\prime}_{1}, P^{\\prime}_{2}, ..., P^{\\prime}_{L}\\}), \\tag{2} \\end{equation}$\nwhere $P_{i}, \\Phi_{i} \\in \\mathbb{R}^{N\\times d}$. d is the hidden dimension in attention blocks. The CFM parallels patch-wise to model the frequency patches simultaneously. We further introduce the details of CFM in Section 3.2 and the corresponding optimization mechanism in Section 3.3, which ensures the capability of CFM to discover appropriate channel correlations for fine-grained frequency bands.\nFinally, we utilize the Time-Frequency Reconstruction Module (TFRM) to flatten the patch-wise representations and reconstruct all frequency spectrums with MLP projections separately for real and imaginary patches:\n$\\begin{equation} X^{\\prime}_{R} = Projection_{R}(FlattenHead(\\{P^{R}_{1}, \\Phi^{R}_{2}, ..., \\Phi^{R}_{L}\\})), \\tag{3} \\end{equation}$\n$\\begin{equation} X^{\\prime}_{I} = Projection_{I}(FlattenHead(\\{P^{I}_{1}, \\Phi^{I}_{2},\\dots, \\Phi^{I}_{L}\\})), \\tag{4} \\end{equation}"}, {"title": "3.2 CHANNEL FUSION MODULE", "content": "To better discriminate the heterogeneous subsequence anomalies with different occupancies on frequency bands, we devise the Channel Fusion Module which dynamically perceives the channel associations in each fine-grained frequency band and generates appropriate channel correlations to enhance the reconstruction ability. Specifically, we propose a patch-wise mask generator and utilize the masked attention mechanism to capture the channel correlations.\nPatch-wise Mask Generator. Inspired by Selective State Space Models such as Mamba , which utilizes Linear projections to flexibly update the hidden states based on the current data for larger capacity, the patch-wise channel associations can also be seen as a changing hidden state strongly associated with the current patch. Therefore, we devise a Linear-based mask generator to perceive the suitable channel associations for each frequency band by generating binary mask matrices to isolate the adverse effects from irrelevant channels. Note that the binary mask is an intermediate state between CI (identity matrix) and CD (all-ones matrix) strategies. Moreover, the mask generator itself works in a CI manner to mitigate the adverse effects from noisy channels. To ensure the rationality of discovered channel correlations and provide both robustness and capacity, we also design specific optimization objectives in Section 3.3. We take the i-th frequency patch as an example:\n$\\begin{equation} D^{i} = \\sigma(Linear(P^{\\prime}_{i})), M^{i} = Resample(D^{i}), \\tag{5} \\end{equation}$\nwhere $D^{i}$ is the probability matrix for i-th patch, with the shape of $\\mathbb{R}^{N \\times N}$. Sigmoid function $\\sigma$ projects the values to probabilities. Since our goal is to filter out the adverse effects of irrelevant channels, we further perform Bernoulli resampling on the probability matrices to obtain binary mask matrix $M^{i}$ with the same shape. Higher probability $D^{i}_{l,m}$ results in $M^{i}_{l,m}$ closer to 1, indicating a relationship between channel l and channel m. And we manually keep the diagonal items to 1. To ensure the propagation of gradients, we use the Gumbel Softmax reparameterization trick during Bernoulli resampling.\nChannel-Masked Transformer Layer. After the patch-wise mask generator outputs the mask matrices for frequency bands, we utilize the transformer layer to further capture the fine-grained channel correlations. The Layer Normalization is applied before each attention block to mitigate the over-focusing phenomenon on frequency components with larger amplitudes:\n$\\begin{equation} \\Phi^{*i} = LayerNorm(\\Phi^{\\prime i}) = (\\Phi^{\\prime i} - Mean^{N}_{l=1}(\\Phi^{\\prime}_{l,:}))/\\sqrt{Var^{N}_{l=1} (\\Phi^{\\prime}_{l,:})}. \\tag{6} \\end{equation}$\nEmpirically, we utilize the masked attention mechanism to further model the fine-grained interrela-tionships among relevent channels and integrate the mask in a calculated way to keep the propagation of gradients:\n$\\begin{equation} Q^{i} = \\Phi^{*i}.W^{Q}, K^{i} = \\Phi^{*i}.W^{K}, V^{i} = \\Phi^{*i}.W^{V}, \\tag{7} \\end{equation}$\n$\\begin{equation} T^{i} = Q^{i} \\cdot (K^{i})^{T}, S^{i} = T^{i} \\odot M^{i} + (1 - M^{i}) \\odot (-\\infty), \\tag{8} \\end{equation}$\n$\\begin{equation} MaskedScores^{i} = \\frac{S^{i}}{\\sqrt{d}}, \\Phi^{i} = Softmax(MaskedScores) \\cdot V^{i}, \\tag{9} \\end{equation}$\nwhere $\\Phi^{*i} \\in \\mathbb{R}^{N\\times d}, P^{i} \\in \\mathbb{R}^{N\\times d}, W^{Q},W^{K},W^{V} \\in \\mathbb{R}^{d\\times d}$. We utilize the same Feed-Forward networks and skip connections as the classical transformers . We also apply multi-head mechanism to jointly attend to information from different representational subspaces, and the Channel-Masked Transformer Layer can be stacked multiple times."}, {"title": "3.3 TIME-FREQUENCY TRAINING", "content": "We design a novel training process to enhance the model's ability to detect both point anomalies and subsequence anomalies. And the Channel Fusion Module (CFM) is also encouraged to discover appropriate channel correlations for different frequency bands. Specifically, we design multiple optimization objectives and a bi-level optimization process to achieve these.\nFine-grained Channel Correlation Discovering. We introduce the forward propagation process of CFM in Section 3.2. From an optimization perspective, it is essential to design appropriate optimization objectives to enhance the effectiveness of generated masks. A direct motivation is to explicitly enhance the attention scores between relevent channels defined by the mask, thus aligning the attention mechanism with the currently discovered optimal channel correlation, which helps isloate the adverse effects from irrelevant channels and provides robustness for the attention mechanism. Then we iteratively optimize the mask generator to refine the channel correlations, tuning the capacity of attention mechanism to fully capture the interrelationships between channels. Intuitively, we devise a compound loss function to guide the mask generator exploring the space of channel correlations (from CI to CD). The proposed compound loss function for patch $\\Phi^{*i}$ is formalized as:\n$\\begin{equation} ClusteringLosS = \\sum^{N}_{k=1}(\\sum^{N}_{m=1}(\\frac{exp(S_{k,m})}{\\tau})), RegularLoss = \\frac{1}{N}||I - M^{i}||_{F}, \\tag{10} \\end{equation}$\nwhere $\\tau$ is the temperature coefficient. The ClusteringLoss is similar to the InfoNCE in form but does not fix the number of \u201cpositive\u201d pairs. In contrast, it changes its \u201cpositive\" pairs for different patches based on the current discovered channel correlation $M^{i}$. As shown in , it sets the Query and Key views of relevent channels in a frequency band ($M_{i,m} = 1$) as the \"positive\" pairs, thus encouraging the $W^{Q}$ and $W^{K}$ to cluster the patch-wise relevent channels in the hidden spaces and lead to higher attention scores. We share the calculating results of $S^{i}$ and T with the attention mechanism in Section 3.2 to save the computational cost. However, only a single ClusteringLoss may cause some adverse effects by urging the mask generator to output constant ones matrix, so that we add a RegularLoss to mitigate this risk by restricting the number of relevent channels. Equipped with the two optimization objectives, the mask generator is encouraged to discover appropriate patch-wise channel correlations between CI and CD, and the attention mechanism is also enchanced by optimizing the $W^{Q}$ and $W^{K}$ to learn fine-grained channel representations in the hidden spaces.\nTime-Frequency Reconstruction. We adopt the reconstruction loss functions both in time and frequency domains to separately enhance the ability of point-to-point and subsequence modeling. The loss functions for input series X is formalized as:\n$\\begin{equation} RecLosstime = ||X - X^{\\prime}||_{2}, RecLossfreq = ||X_{R} - X^{\\prime}_{R}||_{1} + ||X_{I} - X^{\\prime}_{I}||_{1} \\tag{11} \\end{equation}$\nWe utilize 2-norm in the time domain and 1-norm in the frequency domain due to the distinct numerical characteristics of time and frequency domains .\nBi-level optimization. We weightsum the four optimization objectives to a TotalLoss $\\mathcal{L}$ and utilize a bi-level optimization Algorithm 1 to iteratively update the mask generator and other model parameters. Intuitively, the process optimizes model parameters for current channel correlations and then discovers better channel correlations for the optimized model parameters, which facilitates the refinement of channel correlations in a continuous way."}, {"title": "3.4 FREQUENCY-ENHANCED POINT-GRANULARITY SCORING", "content": "When calculating the anomaly score, the convention is to obey the point-to-point manner by assigning an anomaly score for each timestamp, thus mainly reflecting the point anomlies in the time domain. To better quantify subsequence anomalies, existing methods often consider coarse-grained window-granularity scoring, which adds a frequency anomaly score to each point in the whole input window. However, they fail to know the actual boundaries of subsequence anomalies, thus causing misjudgment or omission. As our proposed CATCH is optimized for $RecLossfreq$, it is capable of detecting subsequence anomalies in the frequency domain. During scoring, we perform the patching operation in the input window with the stride length equal to 1. In , the shadow in the Time Series indicates a series of subsequence anomalies. Take the red point as an example, we collect all the patches to which it belongs, then transform these patches into the frequency domain, calculate the reconstructed frequency anomaly scores through $RecLossfreq$ and take the average value as frequency anomaly score of this point, as this gives superior performance compared to the minimum or the maximum . Obviously, the method can reflect the real surroundings of each point by considering all possible subsequence anomalies around this point, thus achieving the point-granularity alignment and showing strong sensitivity. Finally, we weightsum the time and frequency anomaly scores point-to-point. We provide the details of an efficient implementation version in Appendix A.3."}, {"title": "4 EXPERIMENTS", "content": "Datasets We conduct experiments using 9 real-world datasets and 12 synthetic datasets (TODS datasets) to assess the performance of CATCH, more details of the benchmark datasets are included in Appendix A.1. The synthetic datasets are generated using the method reported in . Please refer to Appendix A.5 for specific implementation. We report the results on 9 real-world MTSAD datasets, including MSL, PSM, SMD, CICIDS, CalIt2, NYC, Creditcard, GECCO and Genesis in the main text. We also report the mean results of the 6 types of synthetic anomalies. The complete results can be found in the Appendix C.\nBaselines We comprehensively compare our model against 14 baselines, including the latest state-of-the-art (SOTA) models. These baselines feature the 2024 SOTA iTransformer (iTrans) , ModernTCN (Modern) , and DualTF , along with the 2023 SOTA Anomaly Transformer (ATrans) , DCdetector (DC) , TimesNet (TsNet) , PatchTST (Patch) , DLinear (DLin) , NLinear (NLin) , and AutoEncoder (AE) . Additionally, we include non-learning methods such as One-Class SVM (OCSVM) , Isolation Forest (IF) , Principal Component Analysis (PCA) , and HBOS .\nSetup To keep consistent with previous works, we adopt Label-based metric: Affiliated-F1-score (Aff-F) and Score-based metric: Area under the Receiver Operating Characteristics Curve (ROC) as evaluation metrics. We report the algorithm performance under a total of 16 evaluation metrics in the Appendix C, and the details of the 16 metrics can be found in Appendix A.2. More implementation details are presented in the Appendix A.3.\n4.1 MAIN RESULTS\nWe first evaluate CATCH with 14 competitive baselines on 9 real-world multivariate and 6 types of synthetic multivariate datasets generated by the methods reported in TODS as shown in Table 1. It can be seen that our proposed CATCH achieves SOTA results under the widely used Affiliated-F1-score metric in all benchmark datasets. Besides, CATCH has the highest AUC-ROC values on all datasets. It means that our model performs well in the false-positive and true-positive rates under various pre-selected thresholds, which is important for real-world applications. CATCH effectively handles both point (Contextual, Global) and subsequence (Seasonal, Shapelet, Trend, Mixture) anomalies while showing greater improvement in detecting the subsequence anomalies. In addition, as shown in Table 2, CATCH's performance on other metrics mostly remains at the forefront, further validating the robustness of our algorithm.\n4.2 MODEL ANALYSIS\nAblation study To ascertain the impact of different modules within CATCH, we perform ablation studies focusing on the following components: (1) Substitute the channel correlation discovering mechanism with fixed Channel Strategies. (2) Delete one of the four optimization objectives separately. (3) Remove the patching operation during training process. (4) Replace the Scoring technique with others. (5) Replace the bi-level optimization process with a normal process to optimize the mask generator and model simultaneously. Table 3 illustrates the unique impact of each module. We have the following observations: 1) Compared to the Channel-Independent (CI) Strategy, considering the relationships between variables using Channel Dependent (CD) Strategy or random masking yields better results, with the random masking performing worse than the CD method. Ours outperforms the CD method, further demonstrating the effectiveness of the channel correlation discovering mechanism. 2) Removing any of the four optimization objectives leads to a decline in model performance, with the most significant drop occurring when the frequency loss is removed. This fully demonstrates the rationality and effectiveness of the four optimization objectives. 3) When the patching operation is removed during training and replaced with a window-based approach to model the relationships between variables, the model performance significantly decreases. This indicates that the patching operation captures fine-grained frequency information, which is more conducive to anomaly detection. 4) When replacing the combination of point-granularity temporal anomaly scores and patch-wise point-aligned frequency anomaly scores with the combination of point-granularity temporal anomaly scores and window-granularity frequency anomaly scores, or when using only one of them, the model performance decreases in both cases. This indicates our Scoring technique shows stronger sensitivity in detecting anomalies. 5) When using a normal optimization process, the model performance also decreases consistently, which provides empircal evdience for the bi-level optimization.\nParameter Sensitivity We also study the parameter sensitivity of the CATCH. shows the performance under different input window sizes. As discussed, a single point can not be taken as an instance in time series. Window segmentation is widely used in the analysis, and window size is a significant parameter. For our primary evaluation, the window size is usually set as 96 or 192. Besides, we adopt the score weight in section 3.4 to trade off the temporal score and the frequency score see . We find that score weight is mostly stable and easy to tune in the range of 0.01 to 0.1. and present that our model is stable to the Traing patch size and Testing patch size respectively over extensive datasets. Note that a small patch size indicates a larger memory cost and a larger patch number. Especially, only considering the performance, its relationship to the patch size can be determined by the data pattern. For example, our model performs better when the traing patch size is 8 for the MSL dataset, the testing patch size is 32 for the CICIDS dataset.\nAnomaly criterion visualization We show how CATCH works by visualizing different anomalies in . This figure showcases CATCH's performance across five anomaly categories on the TODS dataset, with temporal and frequency scores displayed in the second row, and the final anomaly scores in the third row. For point anomalies (first and second columns), the temporal scores exhibit sharp increases at the true anomaly locations, dominating the total scores. In contrast, for subsequence anomalies (third, fourth, and fifth columns), frequency scores remain elevated across the entire anomaly interval, show strong sensitivity to the actual boundaries of subsequence anomalies and compensate for the insensitivity of the temporal scores, which provides empircal evidence for the Frequency-Enhanced Point-Granularity Scoring technique in Section 3.4. Consequently, each domain contributes uniquely, allowing the final anomaly scores to accurately capture both point and subsequence anomalies."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose a novel framework, CATCH, capable of simultaneously detecting both point and subsequence anomalies. To sum up, it patchifys the frequency domain for fine-grained insights into frequency bands, flexibly perceives and discovers appropriate channel correlations, optimizes the attention mechanism for both robustness and capacity with a bi-level optimization algorithm. These innovative mechanisms collectively empower CATCH to precisely detect both point and subsequence anomalies. Comprehensive experiments on real-world and synthetic datasets demonstrate that CATCH achieves state-of-the-art performance."}, {"title": "6 REPRODUCIBILITY", "content": "The study meets reproducibility requirements. Specifically, the datasets are available for download in a standardized format from here, and the code can be browsed at https://anonymous.4open.science/r/CATCH-E535. It may take some time to download the datasets, please wait patiently."}, {"title": "A EXPERIMENTAL DETAILS", "content": "A.1 DATASETS\nIn order to comprehensively evaluate the performance of CATCH, we evaluate 9 real-world datasets and 12 synthetic datasets which cover 8 domains. The anomaly ratio vary from 0.17% to 11.07%, the range of feature dimensions varies from 3 to 72, and the sequence length varies from 5,040 to 1,416,825. This substantial diversity of the datasets enables comprehensive studies of MTSAD methods.\nA.2 METRICS\nThe metrics we support can be divided into two categories: Score-based and Label-based. Label-based metrics includes Accuracy (Acc), Precision (P), Recall (R), F1-score (F1), Range-Precision (R-P), Range-Recall (R-R), Range-F1-score (R-F) , Precision@k, Affiliated-Precision (Aff-P), Affiliated-Recall (Aff-R), and Affiliated-F1-score (Aff-F) . Score-based metrics includes the Area Under the Precision-Recall Curve (A-P) , the Area under the Receiver Operating Characteristics Curve (A-R) , the Range Area Under the Precision-Recall Curve (R-A-P), the Range Area under the Receiver Operating Characteristics Curve (R-A-R), the Volume Under the Surface of Precision-Recall (V-PR), and the Volume Under the Surface of Receiver Operating Characteristic (V-ROC) . As noted earlier, CATCH computes all metrics to provide a complete picture of each method. More implementation details are presented in the Appendix A.3.\nA.3 IMPLEMENTATION DETAILS\nThe \"Drop Last\" issue is reported by several researchers . That is, in some previous works evaluating the model on test set with drop-last=True setting may cause additional errors related to test batch size. In our experiment, to ensure fair comparison in the future, we set the drop last to False for all baselines to avoid this issue.\nAll experiments are conducted using PyTorch in Python 3.8 and execute on an NVIDIA Tesla-A800 GPU. The training process is guided by the L1 loss, employing the ADAM optimizer. Initially, the batch size is set to 32, with the option to reduce it by half (to a minimum of 8) in case of an Out-Of-Memory (OOM) situation. We do not use the \u201cDrop Last\" operation during testing. To ensure reproducibility and facilitate experimentation, datasets and code are available at: https://anonymous.4open.science/r/CATCH-E535.\""}, {"title": "A.4 IMPLEMENTATION DETAILS OF SCORING", "content": "We provide an efficient implementation of Frequency-Enhanced Point-Granularity Scoring in Section 3.4. We present the pseudo-code in Algorithm 2. Specifically, we adopt the scatter operation in Pytorch to efficiently parallel the collection of patches to which each point belongs."}, {"title": "\u0410.5 \u0421\u043e\u043cMAND USED FOR GENERATING THE SYNTHETIC DATASETS", "content": "We use the provided source code (Lai et al., 2021) without alterations as demonstrated below, except for adjusting the length parameter to generate a longer time series, to ensure a fair comparison."}, {"title": "B ADDITIONAL CASE STUDIES", "content": "As shown in , we visualize the anomaly scores of various recent SOTAs to obtain an intuitive comparison of detecting accuracy. Our proposed CATCH shows most distinguishable anomaly scores in detecting both point and subsequence anomalies."}]}