{"title": "KL Penalty Control via Perturbation for Direct Preference Optimization", "authors": ["Sangkyu Lee", "Janghoon Han", "Hosung Song", "Stanley Jungkyu Choi", "Honglak Lee", "Youngjae Yu"], "abstract": "Direct Preference Optimization (DPO) demonstrates the advantage of aligning a large language model with human preference using only an offline dataset. However, DPO has the limitation that the KL penalty, which prevents excessive deviation from the reference model, is static throughout the training process. Several methods try to turn this static KL penalty into a dynamic one, but no approach can adaptively assign different KL penalties for each preference pair. In this paper, we propose $\\varepsilon$-Direct Preference Optimization ($\\varepsilon$-DPO), which allows adaptive control of the KL penalty strength $\\beta$ for each preference pair. Specifically, $\\varepsilon$-DPO adaptively controls $\\beta$ for each preference pair based on the monotonicity of logits as a preference model under the perturbation of $\\beta$ during training by simply reusing the logit of the current policy and the reference policy. Experimental results show that $\\varepsilon$-DPO outperforms existing direct alignment algorithms and KL penalty relaxation methods on general chatbot benchmarks, highlighting the significance of adaptive KL penalty relaxation at the instance-level in DPO.", "sections": [{"title": "1 Introduction", "content": "Aligning large language models with human preferences for helpfulness and harmless principles (Askell et al., 2021; Bai et al., 2022; Cui et al., 2023) is a crucial requirement for general chatbot agents. Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2020) is the pioneering approach that regards the alignment of large language models as a reward maximization problem and solves it by reinforcement learning (Schulman et al., 2017). However, the complicated training pipeline of RLHF increases the"}, {"title": "2 Preliminaries", "content": "Reinforcement Learning from Human Feedback\nTo obtain a language model that aligns with human preference, RLHF (Ziegler et al., 2020) introduces reinforcement learning. It is equivalent to approaching preference alignment as a reward maximization problem, where we find a policy $\\pi$ that maximizes a ground-truth reward function $r^*$ representing human reward for a response $y$ obtained from a corresponding policy for a given prompt $x$. However, since the ground-truth reward function cannot be accessed, a reward model trained from the preference dataset is introduced as a proxy reward function. On the other hand, to prevent the policy update from deviating too much from the current policy from the initial policy, the KL divergence from the reference policy $\\pi_{ref}$ serves as a penalty and regards the initial policy as a reference policy. At this time, coefficient $\\beta$ controls the strength of the penalty. The optimal policy that satisfies the maximization of the modified objective function under $\\beta$ has a closed-form solution $\\pi_{\\beta}^*$,\n$\\pi_{\\beta}^* := \\arg \\max_{\\pi} {E_{x,y} [r^*(x,y)] - \\beta D_{KL}(\\pi || \\pi_{ref}) },$\n$\\pi_{\\beta}(y|x) = \\frac{\\pi_{ref}(y|x) exp (\\frac{1}{\\beta} r^*(x,y))}{Z(x)},$\n$Z(x) = \\sum_y \\pi_{ref}(y|x) exp (\\frac{1}{\\beta} r^*(x,y)).$\nDirect Preference Optimization RLHF has a limitation in efficiency due to the additional training step of the reward model. In this respect, DPO (Rafailov et al., 2023) proposes an approach that can perform preference alignment without training the reward model. DPO focuses on the fact that the ground-truth reward function can be implicitly reparameterized by the closed-form solution $\\pi_{\\beta}^*$ and reference policy $\\pi_{ref}$ with an intractable normalizing constant $Z$. If we assume the Bradley-Terry model (Bradley and Terry, 1952) for the ground-truth human preference function, then the human preference can be modeled by the margin between the reward of the chosen response $y^w$ and the rejected response $y^l$ with the sigmoid function $\\sigma$, which can ignore the intractable term $Z$ by cancellation. From this observation, DPO performs preference alignment through preference model optimization using an offline dataset in the sense that obtaining an optimal policy through policy optimization in RLHF can be obtained by training a"}, {"title": "3 $\\varepsilon$-Direct Preference Optimization", "content": "In this section, we describe our proposed method, $\\varepsilon$-Direct Preference Optimization ($\\varepsilon$-DPO), that adaptively controls KL penalty coefficient $\\beta$ at the instance-level based on the logit monotonicity as a preference model according to the perturbation of $\\beta$. Figure 2 illustrates the difference between $\\varepsilon$-DPO and existing KL penalty relaxation methods.\n3.1 Relaxation of KL Penalty in DPO\nThe KL penalty introduced by RLHF can be regarded as an approach to solve the constrained optimization problem in the trust region (Schulman, 2015) defined near the reference policy $\\pi_{ref}$ as an unconstrained optimization by treating $\\beta$ as a Lagrange multiplier (Schulman et al., 2017). From this perspective, even though DPO reformulates the problem of finding an optimal policy under fixed $\\pi_{ref}$ and $\\beta$ as a preference modeling problem, using a single $\\beta$ and a fixed trust region for all instances may lead to suboptimal results. This hypothesis regarding relaxation of KL penalty can be supported by the experimental results of $\\beta$-DPO (Wu et al., 2024) that adaptively control $\\beta$ based on the statistics of implicit reward margin during the training process and TR-DPO (Gorbatovski et al., 2024) that updates $\\pi_{ref}$ during the training process for preventing over-optimization (Rafailov et al., 2024) from the vanishing curvature of the loss landscape.\nHowever, $\\beta$-DPO fails to perform instance-level $\\beta$ control despite claiming that the quality of each preference pair should determine $\\beta$. Instead, it performs batch-level $\\beta$ control using momentum-based estimation of batch-level margin disparities, which is strongly affected by the micro-batch size. In addition, TR-DPO updates the reference model without adaptive criteria, which can lead to inefficient KL divergence trade-off between performance and incur computational costs for updating the reference model. Therefore, instance-level adaptive KL penalty control without requiring additional computational cost that achieves an efficient KL trade-off is still undiscovered for DPO."}, {"title": "3.2 Logit Monotonicity under Perturbation", "content": "Establishing a criterion to adaptively change the KL penalty for each instance of preference dataset is not a trivial problem. As a proxy criterion, we can exploit that the policy obtained via DPO can function as a preference model $P_{\\theta,\\beta}$. Formally, $P_{\\theta,\\beta}$ can be expressed as a binary classifier with logit $z_{\\theta}$ and margin $\\gamma$ for a preference triplet $(x, y^w, y^l) \\in D$,\n$P_{\\theta,\\beta}(\\cdot | \\cdot) = \\sigma (\\beta (z_{\\theta}(\\cdot) - \\gamma(\\cdot))),$\n$z_{\\theta}(x, y^w, y^l) := \\log \\frac{\\pi_{\\theta}(y^w|x)}{\\pi_{\\theta}(y^l|x)},$\n$\\gamma(x, y^w, y^l) := \\log \\frac{\\pi_{ref}(y^w|x)}{\\pi_{ref}(y^l|x)}.$\nThis shows that $\\beta$ serves as an inverse temperature of a binary classifier. For a given $\\beta$, we define $\\beta_{\\varepsilon}$ and $\\beta^+$ with a positive constant $\\varepsilon > 0$. That is, $\\beta_{\\varepsilon}$ and $\\beta^+$ refer to values that have been perturbed to be slightly larger or slightly smaller than the $\\beta$,\n$\\beta_{\\varepsilon} := \\frac{\\beta}{1 + \\varepsilon},$\n$\\beta^+ := \\frac{\\beta}{1 - \\varepsilon}.$\nLet us denote the parameters obtained via DPO as a function of $\\beta$, $\\theta(\\cdot) : R^+ \\rightarrow \\Theta$. Consider the case we observe the strict monotonicity of logits happens according to the perturbation of $\\beta$ on $\\theta(\\cdot)$,\n$z_{\\theta(\\beta_{\\varepsilon})}(\\cdot) > z_{\\theta(\\beta)}(\\cdot) > z_{\\theta(\\beta^+)}(\\cdot),$\n$z_{\\theta(\\beta_{\\varepsilon})}(\\cdot) < z_{\\theta(\\beta)}(\\cdot) < z_{\\theta(\\beta^+)}(\\cdot).$\nIntuitively, this corresponds to observing monotonic changes in preference confidence under the same test-time temperature scaling (Guo et al., 2017). If the logits monotonically decrease with increasing $\\beta$, then raising the training temperature (i.e., lowering $\\beta$) yields a clearer separation of $y^w$ and $y^l$ in the neighborhood of $\\beta$, despite having a softer decision boundary. Conversely, if they monotonically increase, a higher training temperature harms the separation of $y^w$ and $y^l$. From this, we can estimate the benefit of adjusting $\\beta$ for each instance within the neighborhood defined by $\\varepsilon$."}, {"title": "3.3 Estimating KL Penalty Perturbation", "content": "Note that $\\theta(\\cdot)$ is intractable since it is equivalent to having access to models trained on each $\\beta$. However, Liu et al. (2024b) shows that optimal policy under $\\beta_{\\lambda}$ can be expressed by $\\pi_{ref}$ re-weighted with importance ratio using $\\pi_{\\beta}^*$. If we assume the autoregressive prior of optimal policy, then the optimal policy under $\\beta_{\\lambda}$ can be estimated by the optimal policy under $\\beta$ and the reference policy, as we respecify Proposition 1 from Liu et al. (2024b),\nProposition 1 (Liu et al. (2024b)) Under the assumption of optimal autoregressive policy $\\pi^*$ where the prompt x \u2208 X, response vocabulary $Y_i \\in V$, and logit $f : X \\times V^{i-1} \\rightarrow R^{|V|}$, the optimal policy $\\pi_{\\beta_{\\lambda}}^*$ can be approximated by the arithmetic mean of logits between $\\pi_{\\beta}^*$ and reference policy $\\pi_{ref}$,\n$\\pi_{\\beta_{\\lambda}}^* (y_{1:n}|x) = \\prod_{i=1}^{n} \\pi_{\\beta_{\\lambda}}^* (y_i|x, y_{1:i-1})$\n$\\approx \\prod_{i=1}^{n} Softmax(\\lambda f(x, y_{1:i-1}) + (1 - \\lambda) f_{ref}(x, y_{1:i-1}))_{y_i}$\nProof. See Appendix A.\nUsing Proposition 1, we can approximate $\\pi_{\\theta(\\beta_{\\varepsilon})}^*$ and $\\pi_{\\theta(\\beta^+)}^*$ by trained policy and reference policy without accessing $\\theta(\\cdot)$ since they are the approximated policies for $\\pi_{\\beta_{\\varepsilon}}^*$ and $\\pi_{\\beta^+}^*$. To adaptively control $\\beta$ for each preference triplet $(x, y^w, y^l)$ during the training process, we regard the policy $\\pi_{\\theta}$ obtained in the current step as the best approximation of the optimal policy under current $\\beta$ and estimate $\\pi_{\\theta(\\beta_{\\varepsilon})}^*$ and $\\pi_{\\theta(\\beta^+)}^*$ for $z_{\\theta(\\beta_{\\varepsilon})}$ and $z_{\\theta(\\beta^+)}$,\n$\\pi_{\\theta(\\beta_{\\varepsilon})}^* (y_{1:n}|x) = \\prod_{i=1}^{n} \\pi_{\\theta(\\beta_{\\varepsilon})}^* (y_i|x, y_{1:i-1})$\n$\\approx \\prod_{i=1}^{n} Softmax((1 + \\varepsilon) f_{\\theta}(x, y_{1:i-1}) + \\varepsilon f_{ref}(x, y_{1:i-1}))_{y_i},$\n$\\pi_{\\theta(\\beta^+)}^* (y_{1:n}|x) = \\prod_{i=1}^{n} \\pi_{\\theta(\\beta^+)}^* (y_i|x, y_{1:i-1})$\n$\\approx \\prod_{i=1}^{n} Softmax((1 - \\varepsilon) f_{\\theta}(x, y_{1:i-1}) + \\varepsilon f_{ref}(x, y_{1:i-1}))_{y_i}.$\nRecall that we need not only the logit of the current policy $f_{\\theta}$ but also the logit of the reference policy $f_{ref}$ to compute the estimated log-likelihood ratio. However, in order to compute the loss function of DPO, $L_{DPO}$, the log-likelihood from the reference policy must be computed for each training instance, which allows us to simply reuse $f_{ref}$ for estimation without any additional computation cost"}, {"title": "4 Experimental Setup", "content": "In this section, we discuss the experimental setup for validating our proposed method, $\\varepsilon$-DPO. We check the feasibility of $\\varepsilon$-DPO using UltraFeedback (Cui et al., 2023), compared to the diverse direct alignment algorithms (Rafailov et al., 2023; Yuan et al., 2023; Zhao et al., 2023; Azar et al., 2024; Xu et al., 2024; Ethayarajh et al., 2024; Hong et al., 2024; Park et al., 2024; Meng et al., 2024) as a method for general chatbot alignment. We also use Anthropic-HH (Bai et al., 2022) for a detailed comparison with existing methods for KL penalty relaxation of DPO (Wu et al., 2024; Gorbatovski et al., 2024). The implementation details for each experimental setting are in Appendix B.\n4.1 UltraFeedback\nUltraFeedback (Cui et al., 2023) is an AI feedback dataset where GPT-4 (Achiam et al., 2023) rates responses obtained from four different language models. We follow the experimental setting of SimPO (Meng et al., 2024) for comparison with various direct alignment algorithms, including DPO (Rafailov et al., 2023), RRHF (Yuan et al., 2023), SLiC-HF (Zhao et al., 2023), IPO (Azar et al., 2024), CPO (Xu et al., 2024), KTO (Ethayarajh et al., 2024), ORPO (Hong et al., 2024), and R-DPO (Park et al., 2024). Specifically, we use the Instruct setting starting from instruction-tuned language models (Jiang et al., 2023a; Dubey et al., 2024). We evaluate resulting models by AlpacaEval 2 (Dubois et al., 2024), Arena-Hard (Li et al., 2024), and MT-Bench (Jiang et al., 2023b), which are widely used for general chatbot benchmarks.\n4.2 Anthropic-HH\nAnthropic-HH (Bai et al., 2022) is a human preference dialogue dataset containing two subsets based on the helpfulness and harmlessness principle. Here, we use helpful-base and harmless-base splits to validate the criterion using logit monotonicity for instance-level $\\beta$ control used in $\\varepsilon$-DPO and the efficiency in terms of trade-off between performance and KL divergence (Rafailov et al., 2024). We choose gemma-2-2B (Team et al., 2024) to obtain the reference policy through Supervised Fine-tuning with chosen responses. Following DPO (Rafailov et al., 2023), we evaluate the models trained with each method under various $\\beta$ in the single-turn dialogue setting. We regard PairRM (Jiang et al., 2023b) as an evaluator for checking performance by win rate comparing their responses and chosen responses in the test splits."}, {"title": "5 Experimental Results", "content": "Main Results of $\\varepsilon$-DPO In Table 1, we observe that $\\varepsilon$-DPO shows notable performances across AlpacaEval 2 (Dubois et al., 2024), Arena-Hard (Li et al., 2024), and MT-Bench (Jiang et al., 2023b) using UltraFeedback. In particular, we find that the performance of $\\varepsilon$-DPO outperforms most direct alignment algorithms, which generally modify the loss objective, highlighting that the major assumption of fixed KL penalty in DPO is overlooked. In addition, we observe that $\\varepsilon$-DPO performs better than other KL penalty relaxation approaches (Wu et al., 2024; Gorbatovski et al., 2024) from Table 2."}, {"title": "7 Conclusion", "content": "In this paper, we present $\\varepsilon$-Direct Preference Optimization ($\\varepsilon$-DPO), an instance-level adaptive KL penalty control for DPO, adjusting $\\beta$ by observing the monotonicity of the log-likelihood ratio between the chosen response and the rejected response when the $\\beta$ used during training is perturbed. The criterion for instance-level adaptive control of $\\beta$ only requires estimating the policy under the perturbed $\\beta$, which can be efficiently estimated by reusing the current policy and reference policy logits without relying on batch-level statistics and requiring additional computation cost. Resulting models obtained through $\\varepsilon$-DPO perform"}, {"title": "Limitations", "content": "E-DPO requires the reference policy because it has a KL penalty from the reference policy, like DPO in default. It leads to the limitation that it requires additional memory consumption and computation for reference policy compared to other direct alignment algorithms that do not perform regularization through the reference policy (Zhao et al., 2023; Xu et al., 2024; Hong et al., 2024; Meng et al., 2024).\nHowever, theoretically, $\\varepsilon$-DPO can save memory consumption by pre-computing the logits of the responses from the reference policy, similar to DPO. Meanwhile, $\\varepsilon$-DPO is a general purposes approach not specially tailored for safety alignment, so additional safety considerations may be required to control inappropriate responses in real usages."}, {"title": "A Proof of Proposition 1", "content": "Proposition 1 (Liu et al. (2024b)) Under the assumption of optimal autoregressive policy $\\pi^*$ where the prompt x \u2208 X, response vocabulary $Y_i \\in V$, and logit $f : X \\times V^{i-1} \\rightarrow R^{|V|}$, the optimal policy $\\pi_{\\beta_{\\lambda}}^*$ can be approximated by the arithmetic mean of logits between $\\pi_{\\beta}^*$ and reference policy $\\pi_{ref}$,\n$\\pi_{\\beta_{\\lambda}}^* (y_{1:n}|x) = \\prod_{i=1}^{n} \\pi_{\\beta_{\\lambda}}^* (y_i|x, y_{1:i-1})$\n$\\approx \\prod_{i=1}^{n} Softmax(\\lambda f(x, y_{1:i-1}) + (1 - \\lambda) f_{ref}(x, y_{1:i-1}))_{y_i}$\nProof of Proposition 1. Recall that optimal policy $\\pi_{\\beta}^*$ has a closed-form solution and ground-truth reward function $r^*$ can be reparameterized using the normalizing constant $Z_{\\beta}$,\n$\\pi_{\\beta}(y|x) = \\frac{1}{Z(x)} \\pi_{ref}(y|x) exp (\\frac{1}{\\beta} r^*(x, y)),$\n$Z(x) = \\sum_y \\pi_{ref}(y|x) exp (\\frac{1}{\\beta} r^*(x,y)),$\n$r^*(x, y) = \\beta log \\frac{\\pi_{\\beta}(y|x)}{\\pi_{ref}(y|x)} + \\beta log Z(x).$\nHere, we plug the reparameterization of r* to the close-form solution of $\\pi_{\\beta}^*$ and simple algebra yield,\n$\\pi_{\\beta}(y|x) = \\frac{1}{Z(x)} \\pi_{ref}(y|x) exp (\\frac{1}{\\beta} r^*(x,y))$\n$=\\frac{\\pi_{ref}(y|x) exp (\\frac{1}{\\beta} (\\beta log \\frac{\\pi_{\\beta}(y|x)}{\\pi_{ref}(y|x)} + \\beta log Z(x)))}{\\sum_y \\pi_{ref}(y|x) exp (\\frac{1}{\\beta} (\\beta log \\frac{\\pi_{\\beta}(y|x)}{\\pi_{ref}(y|x)} + \\beta log Z(x)))}$\n$=\\frac{\\pi_{ref}(y|x) exp (log (\\frac{\\pi_{\\beta}(y|x)}{\\pi_{ref}(y|x)}) + log Z(x))}{\\sum_y \\pi_{ref}(y|x) exp (log (\\frac{\\pi_{\\beta}(y|x)}{\\pi_{ref}(y|x)}) + log Z(x))}$\n$=\\frac{\\pi_{ref}(y|x) (\\frac{\\pi_{\\beta}(y|x)}{\\pi_{ref}(y|x)}) Z(x)}{\\sum_y \\pi_{ref}(y|x) (\\frac{\\pi_{\\beta}(y|x)}{\\pi_{ref}(y|x)}) Z(x)}$\n$=\\frac{\\pi_{\\beta}(y|x)}{\\sum_y \\pi_{\\beta}(y|x)}$\n$=\\frac{\\pi_{\\beta}(y|x) / Z(x)}{\\sum_y \\pi_{\\beta}(y|x) / Z(x)}$\n$=\\frac{\\pi_{\\beta}(y|x) Z^{-1}(x)}{\\sum_y \\pi_{\\beta}(y|x) Z^{-1}(x)}$\nwhere Z denotes the normalizing constant of reparameterized form of $\\pi_{\\beta}$. Now, we use the assumption of autoregressive policy $\\pi_{\\beta_{\\lambda}}^*$. This assumption gives us to evade intractable normalizing constant Z,\n$\\pi_{\\beta_{\\lambda}}^* (Y_i|x, Y_{1:i-1}) \\approx \\frac{1}{Z(x, Y_{1:i-1})} \\pi_{\\beta_{\\lambda}}^* (Y_i|x, Y_{1:i-1})^{\\lambda} \\pi_{ref} (Y_i|x, Y_{1:i-1})^{1-\\lambda}$\n$=\\frac{ \\pi_{\\beta_{\\lambda}}^* (Y_i|x, Y_{1:i-1})^{\\lambda} \\pi_{ref} (Y_i|x, Y_{1:i-1})^{1-\\lambda}}{\\sum_{v \\in V} \\pi_{\\beta_{\\lambda}}^* (v|x, Y_{1:i-1})^{\\lambda} \\pi_{ref} (v|x, Y_{1:i-1})^{1-\\lambda}}$\n$=\\frac{Softmax (f_{\\beta_{\\lambda}} (x, Y_{1:i-1}))_{Y_i}^{\\lambda} Softmax (f_{ref} (x, Y_{1:i-1}))_{Y_i}^{1-\\lambda}}{\\sum_{v \\in V} Softmax (f_{\\beta_{\\lambda}} (x, Y_{1:i-1}))_{v}^{\\lambda} Softmax (f_{ref} (x, Y_{1:i-1}))_{v}^{1-\\lambda}}$\n$=\\frac{exp (f_{\\beta_{\\lambda}} (x, Y_{1:i-1}))_{Y_i}^{\\lambda} exp (f_{ref} (x, Y_{1:i-1}))_{Y_i}^{1-\\lambda}}{\\sum_{v \\in V} exp (f_{\\beta_{\\lambda}} (x, Y_{1:i-1}))_{v}^{\\lambda} exp (f_{ref} (x, Y_{1:i-1}))_{v}^{1-\\lambda}}$\nwith eliminating $(\\sum_{v \\in V} exp (f_{\\beta_{\\lambda}} (x, Y_{1:i-1})))^{\\lambda} (\\sum_{v \\in V} exp (f_{ref} (x, Y_{1:i-1}))) ^{1-\\lambda}$ from nominator"}]}