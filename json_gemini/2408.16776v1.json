{"title": "Online Behavior Modification for\nExpressive User Control of RL-Trained Robots", "authors": ["Isaac Sheidlower", "Mavis Murdock", "Reuben M. Aronson", "Emma Bethel", "Elaine Schaertl Short"], "abstract": "Reinforcement Learning (RL) is an effective method for robots\nto learn tasks. However, in typical RL, end-users have little to no\ncontrol over how the robot does the task after the robot has been\ndeployed. To address this, we introduce the idea of online behavior\nmodification, a paradigm in which users have control over behavior\nfeatures of a robot in real-time as it autonomously completes a task\nusing an RL-trained policy. To show the value of this user-centered\nformulation for human-robot interaction, we present a behavior-\ndiversity-based algorithm, Adjustable Control Of RL Dynamics\n(ACORD), and demonstrate its applicability to online behavior mod-\nification in simulation and a user study. In the study (n=23), users\nadjust the style of paintings as a robot traces a shape autonomously.\nWe compare ACORD to RL and Shared Autonomy (SA), and show\nACORD affords user-preferred levels of control and expression,\ncomparable to SA, but with the potential for autonomous execu-\ntion and robustness of RL. The code for this paper is available at\nhttps://github.com/AABL-Lab/HRI2024_ACORD", "sections": [{"title": "1 INTRODUCTION", "content": "Real-world robots must complete tasks well and meet the needs\nof users. In many cases, a robot is optimized for only one of these.\nFor instance, an industrial assembly line robot is programmed to\nperform a very specific task in a very specific way, typically in an\nisolated environment, and thus requires relatively little supervision\nfrom a person. Such a robot may have learned to complete the\ntask optimally through Reinforcement Learning (RL). However, this\n\"one policy fits all\" approach is unlikely to work when robots are\nworking closely with humans. There are many cases where users\nmay wish to have a robot that can autonomously perform a task\nwhile allowing for control over some dimensions of the robot's\nbehavior. For example, a user may want a dishwashing robot to\nmove more slowly when cleaning their favorite mug or an assistive\nrobot to use less force when helping with dressing. While an RL-\nbased policy may be successful at completing the task, it may not\nsuit the user's in-the-moment user needs for how that task should\nbe completed.\nIn many situations, there is a need to facilitate interactions that\ngive users this control over the style of task completion without\nburdening the user with potentially-lengthy human-in-the-loop\nteaching [13, 55]. Some existing methods augment a typical RL pol-\nicy, but these methods have not been adapted for or validated with\nreal users. Existing methods include goal-conditioned RL (GCRL)\nfor example, in which a robot's behavior is dictated by the parame-\nterization of a goal state. Similarly, behavior diversity approaches\noften parameterize a robot's policy with a latent variable that en-\ncodes a skill or certain way of completing a task. These approaches\nare robot-centered approaches which do not explicitly allow user\ncontrol over the resulting policies. We believe these approaches can\nbe reformulated in a user-centered way to give users control over a\nrobot's behavior as it completes a task. To enable close user-robot\ncollaboration, we propose and study an approach that gives users\ndirect control over these latent variables to adjust a robot's behavior\nto their liking.\nIn this paper, we present online behavior modification, a for-\nmulation that combines fully autonomous task completion with\nuser-controlled behavior styles. This formulation is compatible\nwith state-of-the-art methods for learning a task offline, such as\nRL, GCRL, and quality-diversity (QD), while making explicit a de-\ngree of online user control. We then present Adjustable Control\nOf RL Dynamics (ACORD), a user-centered, diversity-based algo-\nrithm which serves as a proof of concept for this formulation. We"}, {"title": "2 RELATED WORK", "content": "Users like to have control over robots. Users desire this control via\nteleoperation [14, 24, 25, 41, 54], via dictating which actions the\nrobot should not take [2, 9, 52], or via having a degree of direct\ncontrol over a collaborative algorithm [20, 38, 56, 57]. A theme\nof these works is that allowing users to influence robot behavior\nallows for more expressivity and robustness than a single policy\nmay represent. In fact, RL is also moving towards more expressive\nand capable policies. Goal-conditioned RL (GCRL), for example,\nenables a policy to perform different tasks depending on how a goal\nstate is selected [12, 17, 29, 32]. Skill learning and diversity-based\napproaches [16, 31, 34], such as Quality-Diversity (QD) [15, 18, 43,\n48, 49], allow a robot to autonomously learn meaningfully variations\nin its behavior. Algorithmically similar to our work, Kumar et al.\n[28] and Osa et al. [40] use diversity-based approaches to increase\nan agent's robustness to its environment, while we propose using\nsimilar techniques to give users more control over a robot's behavior.\nWe also highlight that most previous robot-centered approaches\nhave not been validated with users, a critical step to ensure that\nthese methods serve the needs of users.\nWhile pure teleoperation maximizes user control, our work is\napplicable in tasks where direct user control is impractical or im-\npossible. A more analogous method, used as a baseline in this work,\nis Shared Autonomy (SA), which starts with user direct control and\nadds an automated assistance behavior to direct the robot to an\ninferred user goal or skill based on some input [20, 23, 35, 42, 46].\nAlthough RL has also been used to enhance SA [18, 45], it has not\nbeen used to adjust how the robot completes its task given the\ntarget task is known. There is a need for approaches such as ours\nthat do just that.\nHuman-in-the-loop learning has offered approaches to guiding\nrobot behavior via reward shaping [7, 11, 37, 44], ensuring various\nsafety constraints are met [2, 30, 52], or, most closely related to\nhow a robot does a task, via queries about behavior features [6-8]."}, {"title": "3 LEARNING POLICIES FOR ONLINE\nBEHAVIOR MODIFICATION IN RL SETTINGS", "content": "To enable human-centered control over how a robot complete\nits task, we propose three key properties for online behavior modifi-\ncation. First, the robot must always autonomously make \"task\nprogress\" and ensure the task does not fail. In this context, \"progress\u201d\nmay mean \"expected completion in finite time\" or \"always getting\ncloser to a goal\"; formalization depends on the task. Second, there\nmust be a non-empty set of behavior features, each of which has\nan associated behavior oversight parameter, k, that control the robot\nalong the behavior feature axis. In other words, the policy must be\nexplicitly parameterized with one or more observable variables that\ndictate an aspect of the robot's behavior. Finally, for each behavior\nfeature that has a certain k associated with it, the adjustment of that\nk must be interpretable to a user and there must be an accessible\ninterface that facilitates a user to freely adjust each k as the robot\ncompletes its task. These properties describe an interaction that\nensures the user can have a robot that both meets their needs and\ncan be personalized without having to teach the robot the task or\ntheir preferences.\nIn this section, we present Adjustable Control Of RL Dynamics\n(ACORD), a proof-of-concept algorithm for learning a policy for\nonline behavior modification in continuous state and action space\nrobotics tasks. ACORD is a behavior-diversity-inspired algorithm\nwhich explicitly gives users control over a robot's behavior. We\ndescribe how to adapt a standard RL setting to facilitate ACORD\nand demonstrate it in a simulation environment."}, {"title": "3.1 ACORD for Continuous Control RL-tasks", "content": "We assume a task modeled as a Markov decision process (MDP)\nwith states S, actions A, transition function T(s, a) \u2192 s', and dis-\ncount factor \u03b3. To define task failure, we assume some environmen-\ntal reward function Renv. To this system, we introduce behavior\noversight parameters. Assume that S = Rn and define the space of\nbehavior oversight parameters as K = [0,1]m,1 \u2264 m \u2264 n. Con-\nsider the coordinate representation of s = (s1,..., si,..., sn) and\nk = <k1,..., kj,..., km). Each coordinate of k, kj, controls a coor-\ndinate of s, noted si. The set of all si that have a kj mapping to them\ndefine a set of behavior goals for the robot, and the corresponding\ni-axes are behavior feature axes. Any si with no corresponding kj is\na free variable whose value is not explicitly constrained by a setting\nof k. For generality, we assume the range of behavior goals is un-\nknown prior to learning (e.g., the maximum and minimum speeds\nthe robot can move while completing its task are unknown). After\nlearning, a user can directly adjust the values of k, thus changing"}, {"title": "3.2 ACORD Algorithm", "content": "ACORD makes use of three components: a discriminator that\nlearns a continuous mapping from si \u2192 kj to generate a diversity-\ninspired reward; an environment reward to define failure states and\na task progress heuristic h(s, a) to ensure task performance; and\na domain randomization component that ensures that the agent\nlearns and is robust to various different settings of k such that k\nmay be adjusted in real time.\nACORD Discriminator We train a set of discriminators Wj to\npredict kj given si, denoted: Wj (si) \u2208 [0, 1]. We parameterize the\ndiscriminator as a neural network and train it via the novel loss\nfunction:\nL(Wj(si), kj) = MSE(Wj(si), kj)+\n\\frac{1}{|max(Wj,s\u00a1~D(si)) \u2013 min(Wj,si~D($i))|+ \u03b5}\nwhere Wj,si~D refers to the discriminator output of a batch sampled\nfrom a replay buffer Dw, and e is a small number to avoid division\nby zero. This loss function enforces high prediction accuracy (via\nMSE) and that the predictions cover as wide a range as possible. The\nlatter property is explicitly enforced by the denominator, leading to\na faster convergence to the range covered by each kj, resulting in\nmore stable task behavior (see supplementary material for ablation\nstudy).\nRL Task Description and Agent We define the state space of\nthe RL agent to be SUK. This makes k observable to the agent. We\nwill still denote any given state with s. We design a reward function\nsuch that the agent avoids failures, makes progress, and learns to\nenforce behavior goals:\nR(s, a) = {  Renv(s) if s \u2208 F*\n         -c if h(s, a) \u2264 0\n         -\u2211 m  i=1 (\u2212log |Wi(si) \u2212 ki|) else} (2)\nwhere Renu denotes the reward from the environment, F* is the\nset of failure states which lead to a large negative reward, h(s, a)\ndenotes a heuristic for measuring task progress, and c is a positive\nconstant that punishes the agent if it fails to make task progress.\nLast is the reward generated by the discriminator which ensures\nthat, for a given ki, the agent is acting in the part of the state\nspace where the discriminator can easily predict the ki value. Since\nWi(si) - ki \u2208 [0, 1], this reward is always positive and the other\nconditions are always negative. This allows the reward function to\nbe adapted and scaled to different environments with relative ease."}, {"title": "3.2.1 On Using a Heuristic Progress Function", "content": "Online behavior\nmodification as an interaction emphasizes that the robot can au-\ntonomously complete the task by constantly making progress in\nthat task. There are several ways to formalize this constraint, and\nonline behavior modification does not necessarily require a par-\nticular one. For example, in this work we define a task progress\nmeasure h(s, a) and require that \u03c0\u03ba prioritize trajectories that make\nh(s, a) non-negative; this approach is appropriate for many robot-\nics problems where there is a physical destination for the robot's\nmotion (e.g., [32]). Another natural approach might be to use the\nenvironmental reward function Renv (s, a) to measure task progress\nor require that the trajectories following \u03c0\u03ba eventually reach a ter-\nminal success state. The exact specification will depend on the task\nand the formulation of the learning problem.\nA heuristic progress function h can ensure the robot always com-\npletes the task despite a user changing how it does so. This aligns\nwith our goal of giving users the most control possible over a ro-\nbot's behavior while still accomplishing the task. This is in contrast\nto prior approaches that optimally solve for a trade-off between\nenvironmental reward and diversity, as in Quality-Diversity-based\napproaches [24, 25], or use a hyperparameter to dictate how each\nof the two objectives are weighted [26]."}, {"title": "3.3 ACORD in Simulation", "content": "We train ACORD in simulation to show that the learned policy\nhas the desired properties: it aligns pre-specified behavior features"}, {"title": "4 USER STUDY", "content": "To study ACORD and online behavior modification with real\nusers, we designed a robot painting environment wherein users\ncan adjust a robot's painting style as or before it traces a drawing.\nThis domain is an inherently creative activity in which a person has\nstyles and preferences that they wish to express. Online behavior\nmodification captures the idea that task completion itself is not\nalways the only desirable metric of a human-robot interaction: hav-\ning control over how the task is completed can also be an important\nfactor, as is the case with painting and other artistic tasks.\nRobot Painting Task The painting task involved the robot\ntracing a previously generated shape. We specify each shape as an\nordered list of waypoints in the x-y plane, (po, \u00b7\u00b7\u00b7, pr). We formu-\nlate the task as an MDP where the state s is a vector containing the\nrobot's end-effector position, orientation, and velocity; the position\nand orientation of a brush the robot is gripping; and the next way-\npoint that the robot should reach. Actions are relative Cartesian x-y\nvelocities. Reward is given as R(s, a) = -|Pbrush - pil, the negative\ndistance between the current pose of the brush pbrush and the next\nwaypoint pi. Episodes terminate when the robot has reached every\nwaypoint that makes up the shape or with failure when the arm\nleaves the workspace or is in collision.\nExperimental Setup The setup (Figure 3) consisted of a Kinova\nGen3 robot arm on a table with the participant sitting next to it.\nDepending on the condition, users had access to a different interface\nto interact with the robot. On the table was paper with a shape\noutlined in red on which the robot would paint. The participants\nwere told which shape they would paint: heart or house (Figure\n3). These shapes contain various motions and strokes and provide\nscope for participants to paint in their own style.\nPainting Styles We define two different axes for the robot to\nvary its painting style. One is by adjusting the height of the brush\nor end-effector, thus affecting the pressure that the brush applies to\nthe canvas. This can result in thinner or wider strokes. The other\nway is by rotating the robot's wrist or brush. This adjusts the angle\nof the brush, resulting in more varied strokes."}, {"title": "4.1 Conditions", "content": "We assume for all conditions that the robot knows how to per-\nform the task optimally according to the MDP formulation. We fix\nthe painting policy across each baseline to ensure the same amount\nof time is spent on each painting and that the style adjustment was"}, {"title": "4.2 Experimental Procedure", "content": "Recruitment We recruited a total of 24 participants from the\nuniversity and the surrounding area with a variety of different\nbackgrounds. All participants were 18 years or older. Of those par-\nticipants 15 were female and 9 were male. 13 participants were in\nthe age range of 18-24, 9 in the range of 25-35, 1 in the range of\n35-44 and 1 in the range of 55-64. Participants reported their level\nof programming expertise from 0 (none) to 10 (expert). The mean\nlevel of programming experience was 2.9 with a standard deviation\nof 2.3. Furthermore, 11 participants reported having experience\ninteracting with robots, and 3 of those 11 had significant expertise\n(attending robotics conferences and events regularly). The study\nlasted approximately 45 minutes and participants were compen-\nsated $15. Of the 24 participants, the data from one participant was\nexcluded due to non-participation (ignoring the robot's behavior\nand providing only uniform feedback on all surveys). This left data\nfrom n = 23 participants for analysis. The study procedure was\napproved by the Tufts University IRB.\nProcedure Participants provided informed consent then took a\nbackground survey. The experimenter then explained the task and\ncontrol in the conditions, including allowing participants to practice\nwith SA and ACORD. In each condition, participants painted the\nhouse shape and then the heart shape, then filled out a survey\nabout that condition. Conditions were fully counterbalanced within\nsubjects. Finally, participants completed a post-study survey, were\nthanked, and given compensation.\nOutcome Measures The post-condition survey included NASA\nTLX [1] and UTAUT [53] surveys. We adjusted the scale of all ques-\ntions to a 5-item Likert-scale. We also asked two other Likert-scale"}, {"title": "5 RESULTS", "content": "To analyze the data, we use Bayesian statistics following the in-\nterpretation scheme presented in [51]: a Bayes Factor (BF) between\n3 and 10 we interpret as \"moderate evidence\" for the alternative\nhypothesis, between 10 and 30 as \"strong evidence,\" and 30 or above\nas \"very strong evidence.\" To evaluate the post-study survey data,\nwe encoded responses as pairwise comparisons between two of the\nthree conditions. For each comparison, the rank was encoded as 1\nif the \"left\" condition was preferred, -1 if the \"right\" condition was\npreferred, and 0 if the participant ranked the two conditions equally.\nTo analyze this data, we used a Bayesian Wilcoxon Signed Ranked\ntest with a Cauchy prior distribution with r = 1/\u221a2. To analyze the\nLikert scale data, we used a Bayesian Repeated Measures ANOVA.\nWe used a Bayesian Paired Samples T-Test to analyze the coverage\nand consistency metrics.\nUser preferences We find strong evidence that ACORD is pre-\nferred over RL (BF=17.16) and anecdotal evidence that people prefer\nSA over RL (BF=2.11). There is strong evidence that people found\nACORD more fun than RL (BF=79.87) and moderate evidence peo-\nple found SA more fun than RL (BF=5.03). These results provide\nsupport for ACORD being preferred over RL while being no less\npreferred than SA. We also find a trend towards ACORD being\npreferred to a greater extent over RL than SA. Finally, we found\nthat users rated RL as much less mentally demanding than SA and\nACORD (BF=112.87 and BF=45.92 respectively), and much less\nhard work (BF>10000 and BF>10000), although the previous results\nsuggest this was not a significant factor in user preferences. These\nfindings partially support H1 and directly support that ACORD\nprovides at least as much benefit to user experience as SA.\nUser Control and Expression In the post study-survey we\nfind strong evidence that people find ACORD and SA more ex-\npressive than RL (BF=18.40 and BF=13.65) and similarly for the\npost-condition survey measure of expressiveness (BF=23.38 and\nBF=40.31). Users also found a greater sense of control with ACORD\nand SA (BF=6318.61 and BF=40.31). There is anecdotal evidence\nthat users reported more control in ACORD than SA (BF=2) and dif-\nferences between the two were often commented on in open-ended\nresponses. These results support the first part of H2, that users felt\nmore in control in ACORD than in RL, however our results suggest"}, {"title": "Qualitative Results", "content": "That part is omitted, because it contain images."}, {"title": "6 DISCUSSION", "content": "Online behavior modification describes an interaction in which\na user has control over how an otherwise autonomous robot com-\npletes a task. While prior work has offered various algorithmic\navenues to fulfill this type of user control, such as GCRL or Skill\nLearning, they have been formulated in robot-centered ways and\nlack validation in terms of usability and acceptance by actual users.\nIn contrast, online behavior modification is a user-centric formula-\ntion that can leverage the benefits of these approaches to empower\nusers in ways that can be systematically tested and compared.\nIn this paper, we have presented online behavior modification, the\nACORD algorithm, and the user study. As future work, we propose"}, {"title": "limitations", "content": "That part is omitted, because it contain images."}, {"title": "Conclusion", "content": "That part is omitted, because it contain images."}]}