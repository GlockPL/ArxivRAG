{"title": "EVALUATION OF ARTIFICIAL INTELLIGENCE\nMETHODS FOR LEAD TIME PREDICTION IN\nNON-CYCLED AREAS OF AUTOMOTIVE\nPRODUCTION", "authors": ["Cornelius Hake", "Jonas Weigele", "Frederik Reichert", "Christian Friedrich"], "abstract": "The present study examines the effectiveness of applying Artificial Intelligence methods in an automotive production environ-\nment to predict unknown lead times in a non-cycle-controlled production area. Data structures are analyzed to identify contextual\nfeatures and then preprocessed using one-hot encoding. Methods selection focuses on supervised machine learning techniques.\nIn supervised learning methods, regression and classification methods are evaluated. Continuous regression based on target size\ndistribution is not feasible. Classification methods analysis shows that Ensemble Learning and Support Vector Machines are the\nmost suitable. Preliminary study results indicate that gradient boosting algorithms LightGBM, XGBoost, and CatBoost yield the\nbest results. After further testing and extensive hyperparameter optimization, the final method choice is the LightGBM algorithm.\nDepending on feature availability and prediction interval granularity, relative prediction accuracies of up to 90% can be achieved.\nFurther tests highlight the importance of periodic retraining of AI models to accurately represent complex production processes\nusing the database. The research demonstrates that AI methods can be effectively applied to highly variable production data, adding\nbusiness value by providing an additional metric for various control tasks while outperforming current non AI-based systems.", "sections": [{"title": "I. INTRODUCTION AND LITERATURE REVIEW", "content": "Production planning and control (PPC) is an essential component for controlling and operating a production system. In\ncontrast to vehicle assembly, the vehicles in the Test and Finish Centre (TFC) are no longer cycled due to the individual scopes\nof testing and work. As a result, the sequence of vehicles can be arbitrary, which leads to a need to optimize the sequence in\nPPC. The control of a TFC is an extremely complex task that depends on many influencing factors, especially in the production\nof vehicles with a high number of variants and a low number of units. For this reason, the exact lead time of a vehicle through\nthe TFC cannot be determined with certainty at the present time [1].\nIn this context, the term lead time is defined as the interval between the sixth and eighth reporting points, which are\nassigned when a defined change of location of the vehicle occurs within the production chain. The sixth reporting point\nmarks the conclusion of assembly and the transfer of the vehicle to the TFC, while the eighth reporting point signifies the\ncompletion of the vehicle and the transfer of ownership from the production to the sales department [1]. The ability to predict\nthe period between the sixth and eighth reporting points with a high degree of accuracy presents several potential advantages.\nFrom the perspective of the TFC, a further parameter is established, which serves to optimize the decision-making process\nfor the scheduling and retrieval of individual vehicles. Furthermore, customers benefit from the enhanced precision of delivery\ndates. From the standpoint of sales, there is the potential for optimization in the planning of transportation. The explainable\ndecision-making process of the AI model enables a root cause analysis, which provides information on the characteristics that\nare currently decisive in determining whether a vehicle has a longer or shorter lead time through the TFC. This offers the\npotential to optimize the TFC processes in a subsequent step.\nThe results of the literature research show that there are several works that deal with the prediction of lead times in production\nbased on AI. It is important to note, that only the author's company own past research and that of Krzoska et al. specifically\nconsider lead times in automotive production [2]. Nevertheless, all studies pursue the goal on the precise prediction of lead\ntimes in industrial production using AI methods.\nPrevious research by the author's company investigated two methods for predicting lead times in a TFC with AI: regression\nand classification using LightGBM (LGBM). The contextual features analyzed included four configuration variants (order type,\ncolor selection, steering wheel position, and special refinement) and all inspection codes recorded before the sixth reporting"}, {"title": "II. METHODS AND MATERIALS", "content": "Python is used as the programming language for the algorithms, which are computed on a workstation with an Intel Core\ni9-12900KF processor (16 cores, max. 5.20 GHz), 64 GB RAM, and an Nvidia RTX 3060 GPU with 12 GB GDDR6 graphics\nmemory [15], [16]. To perform the analysis, it is necessary to collect, store and access the production data for the AI application,\nas illustrated in Figure 1.\nIt is important to use a unique identification key with a vehicle reference to link data from various sources and assign it to\na specific vehicle. The data can be available in metric, ordinal, nominal and unstructured form in each source, which is why\nstandardized data preparation must be carried out for further processing."}, {"title": "B. Obtaining and Preprocessing the Training Data", "content": "The first step is to select contextual data. A data model is searched for data influencing the vehicle lead time in the TFC.\nA distinction is made between characteristics known at the time of TFC entry and those recorded during the process. Known\ncharacteristics at entry include configuration variants, specific product key (vehicle derivative), and entry time (weekday and\nhour) as illustrated in Figure 2. These define product complexity and shift time, considering shift changes and downtime.\nAdditional characteristics captured during progression include priority, parking locations, and inspection type/location combi-\nnation. Specific inspection codes for each vehicle influence the processing time in the TFC. Table II provides an overview of\nrelevant characteristics, data types, and processing types. Non-series vehicles are excluded from the analysis. The product key\ndistinguishes between internal combustion engine vehicles (ICE) and battery electric vehicles (BEV) data."}, {"title": "C. Selection Process and Methodology for AI Model Evaluation", "content": "Two prediction approaches can be considered [18]: exact continuous value prediction using regression models and discrete\nprediction using classification methods [3], [4]. Regression models with supervised learning require a linear data distribution,\nbut since TFC lead times follow an exponential distribution, linear regression is inappropriate. Non-linear regression models\nare excluded due to their high resource requirements. Classification methods considered include decision trees, ensemble\nlearning, naive Bayes, k-nearest neighbor, SVMs, and neural networks. Neural networks are resource intensive and lack\nexplanatory power, while naive Bayes assumes feature independence, leading to poor generalization. K-nearest neighbor requires\nall data points, resulting in high resource requirements and longer classification times. Ensemble learning methods and SVMs\nare suitable due to good generalization and fast classification times. However, ensemble learning with decision trees loses\nexplainability with more trees. Nevertheless, ensemble learning performs feature selection during training, which improves\nmodel performance and allows feature relevance analysis [3], [17], [19]. A preliminary experiment is necessary to make the\nfinal selection decision. For this purpose, the specific methods of ensemble learning, and SVMs are first selected below [20].\nEnsemble learning methods such as random forest (bagging) and gradient boosting machines (boosting) are particularly\nsuitable. Bagging reduces model variance by aggregating multiple independent models but can introduce bias and reduce\nprediction accuracy [21]. Boosting algorithms, such as LGBM, XGBoost (XGB), and CatBoost (CB), address this by weighting\nobservations. Bent\u00e9jac et al. [14] highlight these algorithms for their generalizability, training performance, and predictive\nprecision. Therefore, the analysis focuses on these specific ML algorithms regarding their suitability for the prediction of\nlead times [22]\u2013[24]. For configuring SVMs, choosing the kernel and strategy for multi-class classifications is crucial. The\nRBF kernel is ideal for complex data requiring non-linear separation. One-vs-All or One-vs-One methods are used, depending\non the SVM library. ThunderSVM (TSVM), which supports OVO for multi-class classification, is suitable for GPU-based\ncomputations with the given setup."}, {"title": "D. Labeling the Data", "content": "In the supervised learning methodology, each data point requires an associated target variable or label that represents the\nexpected output based on its attributes. For discrete labeling of the data points, groups are formed based on the actual lead\ntimes of the vehicles. Using domain-specific knowledge, the groups are defined based on time intervals by analyzing frequency\ndistributions and the general relevance for the company to assign the data points [17]. To predict the lead times between\nthe sixth and eighth reporting points, both the operational benefit and sufficient data distribution within the classes must be\nconsidered. The exact time intervals of the classes are shown in Table IV. Initially, nine test series are formed with an increasing\nnumber of classes. These start with two different classes and extend up to ten different classes. The classification for the lead\ntime prediction starts with two classes: Vehicles that reach the eighth reporting point within 24 h from the sixth reporting\npoint (55.7%), and those that do so after 24 h (44.3%). With increasing class division, the class with the most data points is\nsubdivided more finely. With eight classes, except for the direct runners, no class contains more than 10% of the data. The\nlabels must be assigned to each data set as a target value for the training process."}, {"title": "E. Training Procedure for AI Methods", "content": "This section describes the preparation, training, and validation of gradient boosting machines. First, a preliminary experi-\nment with standard hyperparameters determines achievable target classes for which complex hyperparameter optimization is\nperformed later. The data set is partitioned into 80% training, 10% validation, and 10% test data, ensuring that each class is\nequally represented by stratified splitting and using the random state value 42 for consistency. After data preparation, model-\nspecific default hyperparameters are defined, including multi-class classification via loss functions. Models are initialized with\ndeveloper-defined default hyperparameters. The training process records training time, feature relevance, number of decision\ntrees, and the AI model. To improve the performance, robustness, and generalization of GBDT models, it is recommended to\noptimize model-specific hyperparameters such as learning rate from 0.01 to 0.3, number of leaves from 32 to 1024, max depth\nfrom 2 to 200, leaf est. iterations from 1 to 10, and leaf reg. from 1 to 10 [4]. This involves testing different hyperparameter\ncombinations using grid search and k-fold cross-validation, and then evaluating their impact on training results. Due to the\nhigh resource requirements, it's performed on the LA and UA data sets, with the results applied to the ICE and BEV data sets.\nA fivefold cross-validation grid search is used, with 80% of the data for training and 20% for evaluation. Accuracy is chosen\nas the evaluation metric for the cross-validation results."}, {"title": "III. RESULTS", "content": "Finally, LGBM, XGB, and CB are trained and tested for prediction accuracy. The test data labels are predicted by each model,\nand the actual labels are used to calculate the percentage of correct predictions, resulting in a relative prediction accuracy.\nModels that achieve at least 80% prediction accuracy on the test data are further evaluated. The evaluation includes parameters\nsuch as prediction accuracy, training time, and number of decision trees generated.\nThe analysis shows that within the UA data set, the experimental design goals are only met for class numbers between\ntwo and four with prediction accuracies up to 90%. For finer classifications from nine to ten classes, the prediction accuracy\ndecreases significantly to 32%. The LGBM model slightly outperforms the XGB and CB models in quality by two to four\nclasses, using 460 instead of 999 (XGB) or 997 (CB) decision trees, highlighting its efficiency. In terms of training times, XGB\ntakes 141 s, while CB is the fastest at 9 s, with LGBM also performing well at 16 s. To efficiently meet the test objectives,\ntarget classes must be limited to two to four, highlighting the predictability of wider intervals. Overall, the LGBM model leads\nin prediction accuracy and efficiency.\nThe analysis of the training results of the gradient boosting machines using the LA data set indicates an adjustment of\nthe target value of the relative prediction accuracy to 70% due to the reduced number of features. The results reveal that the"}, {"title": "B. Evaluation of the Prediction Quality", "content": "A comparison is made between the actual, rule-based and target, AI-based systems for 465 vehicles. The planned date of\nthe actual system is created once before it is entered into the TFC and is not continuously updated as in the target system. As\na result, LGBM achieves up to 65% higher prediction accuracy than the actual system for data records without characteristic\nlimits as it is illustrated in Figure 4. The systems are best compared using the feature-limited data sets LA, LI, and LB, as in\nthese cases the systems work with the same data basis. The results show that the LGBM model of the ICE production line\nachieves 32.8% better prediction accuracy than the actual system with a data preparation and classification time of 146 s. In\nthe case of the BEV production line, a prediction accuracy 11.5% better than the actual system can be achieved with a data\npreparation and classification time of 156 s. The lower accuracy value is due to process-related changes implemented during\nthis work. The test results of this comparison illustrate how powerful AI models can be compared to conventional systems. In\nall cases, the use of AI can achieve a significantly higher prediction accuracy of vehicle lead times in the TFC."}, {"title": "IV. CONCLUSION AND FUTURE WORK", "content": "The research results show that AI-based prediction of lead times with GBDT models is possible for vehicles produced in\nlow unit numbers and individual test scopes. The work also highlights the limitations of the system. Due to a process change"}, {"title": "", "content": "in the BEV production line, the training data no longer accurately reflects the real production processes. As a result, the test\nresults of the GBDT model are worse than the development results, which shows the high requirements for a good training\ndatabase. The result illustrates the relevance of periodic retraining of the AI models when they are used in a real production\nenvironment. LGBM, as the GBDT model with the best test results, offers for the first time the ability to predict the lead\ntime of a vehicle in the TFC with up to 90% accuracy. This provides an additional decision-making parameter for production\nplanning and control in the TFC. The test results show the improvement of the AI-based system compared to the actual,\nrule-based system. Using hyperparameter optimization, the average improvement in prediction accuracy was only 2%.\nA subsequent analysis of the relevance of the characteristics identified procedural inefficiencies. Two further research tasks\narise from the research results. The first research task relates to the development of improved algorithms for explainable AI.\nExplainable decision-making enables a detailed analysis of the causes, creates transparency, and thus ensures that AI applications\nare widely accepted. The second research task is to develop an algorithm for the automated optimization of the time periods\nbetween the retraining of the AI models. The retraining ensures constant, high prediction quality. At the same time, training\nAl models is resource intensive and time consuming. Optimizing the number of retraining ensures that the prediction quality\nremains high while operating as economically as possible. This shows the relevance and motivation for further research in the\nfield of AI applications in production."}]}