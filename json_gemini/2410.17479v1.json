{"title": "Composing Diffusion Policies for Few-shot Learning of Movement Trajectories", "authors": ["Omkar Patil", "Anant Sah", "Nakul Gopalan"], "abstract": "Abstract-Humans can perform various combinations of physical skills without having to relearn skills from scratch every single time. For example, we can swing a bat when walking without having to re-learn such a policy from scratch by composing the individual skills of walking and bat swinging. Enabling robots to combine or compose skills is essential so they can learn novel skills and tasks faster with fewer real world samples. To this end, we propose a novel compositional approach called DSE- Diffusion Score Equilibrium that enables few-shot learning for novel skills by utilizing a combination of base policy priors. Our method is based on probabilistically composing diffusion policies to better model the few-shot demonstration data-distribution than any individual policy. Our goal here is to learn robot motions few- shot and not necessarily goal oriented trajectories. Unfortunately we lack a general purpose metric to evaluate the error between a skill or motion and the provided demonstrations. Hence, we propose a probabilistic measure \u2013 Maximum Mean Discrepancy on the Forward Kinematics Kernel (MMD-FK), that is task and action space agnostic. By using our few-shot learning approach DSE, we show that we are able to achieve a reduction of over 30% in MMD-FK across skills and number of demonstrations. Moreover, we show the utility of our approach through real world experiments by teaching novel trajectories to a robot in 5 demonstrations.", "sections": [{"title": "I. INTRODUCTION", "content": "For robots to be deployed in unstructured environments and interact with humans, they should be capable of learning new skills from very few demonstrations. For example, wiggling the end-effector while moving forward to clean a table is a combination of two independent motions. This wiggling motion can be combined with different primitive motions to clean floors, to wash dishes, to fit a bed-sheet, to iron a cloth, etc. These are not goal oriented trajectories, but continuous motions that are sometimes dynamical trajectories in configuration space where a robot follows a sequence of movements. Robots should not be expected to learn these composed motions one at time but rather combine previously learned skills along with utilizing any given demonstrations. However, finding the right skills to combine from a base set and the extent of their contributions in the resulting motion is non-trivial. Existing compositionality methods either directly pick and choose the priors to compose while only learning the ratios of the priors' contribution [1], or do not have a method to utilize residual information in the provided demonstrations [2, 3].\nTo tackle these shortcomings, we propose Diffusion Score Equilibrium(DSE), a compositional method that works over a set of base policies by inferring the extent of their contribution given a few demonstrations. Importantly, our method does not assume the policies to compose for achieving the desired behavior, and scales the contribution of base policies based on the information available in the provided demonstrations. A core element of our approach is inferring the contribution of each base policy in the resulting behavior, which we refer to as compositional weights henceforth. We infer these weights by minimizing the distance between a proposed trajectory and the few-shot demonstration data-distribution.\nUnderlying our approach is the insight that composing diffusion models can result in novel motion generation that interpolates between the individual distributions. We leverage this insight to efficiently learn a novel skill by interpolating between the noisy distribution learned from the few demon- strations of a novel skill and the set of base policy distributions for minimizing the distance to the few-shot demonstration data-distribution.\nHowever, an impediment to this approach is the lack of a general purpose metric that can measure this distance between a set of trajectories. Most metrics in robotics and within compositional works use task specific metrics such as success rate [3, 2] that are specific to their chosen task and actions. This however does not measure the quality of the composed distribution with respect to the reference trajectories or even base policies. To fill this gap and evaluate the compositional weights in our method, we propose Maximum Mean Discrep- ancy on the Forward Kinematics Kernel (MMD-FK) based on Maximum Mean Discrepancy [4] and the Forward Kinematics kernel [5]. Our metric measures the distance between two robot trajectory distributions considering all the physical links in the robot's body and not just the end effector.\nWe show that by inferring the compositional weights by minimizing MMD-FK, our method DSE scales with the number of provided demonstrations and achieves superior performance in both low and high data regimes. DSE results in 30% to 50% lower MMD-FK error in different data regimes than a demonstration fine-tuned policy and is also superior to prior compositional approach using diffusion models. Our contributions in this work are as follows-\n\u2022 We present a novel compositional approach for sample- efficient learning called Diffusion Score Equilibrium (DSE). Our method does not rely on manually choosing which base policies to compose, and scales the perfor- mance with the number of demonstrations provided for the new skill. To the best of our knowledge, our work is also the first to learn compositional weights over a set of diffusion policies from the target demonstrations.\n\u2022 We propose MMD-FK to fill the gap of a task and action space agnostic metric. We use the novel combination of"}, {"title": "II. BACKGROUND", "content": "Large scale robot learning has shown considerable promise in terms of generalization. However, it requires large com- putational overheads and massive data collection, which is challenging for robotics. Efficiency of learning new skills and being able to reuse them will be crucial for robot learning to be successful in unstructured environments. Compositionality is one such approach that aims to maximize the reuse of policies or representations. Compositionality can be induced in various aspects of the policy learning process for sample efficiency gains. Functional [6], representational [7], temporal [8, 9] and policy composition [10, 1, 3] feature prominently in the machine learning and robotics literature. Policy composition enables embodied agents to reuse two or more of their learned policies together for sampling from a novel distribution. Previ- ously, a mixture of experts [11] such as a weighted average or a product of distributions [10] representing sub-policies has been used to model skill, motion or constraint composition. Unlike other forms, in policy composition, the distributions learned for motions or constraints are probabilistically composed and the resulting sample is generated from the new probability landscape.\nIn supervised learning, traditionally energy based models have been utilized to probabilistically compose distributions [12], since the energy functions could be added to obtain a product of corresponding distributions, from which samples could then be generated using Markov Chain Monte Carlo (MCMC). Recently, the number of works leveraging composi- tionality has surged owing to the discovery of the connection"}, {"title": "B. Diffusion Models", "content": "Our aim is to learn the action distribution $a_f$ for a fixed trajectory length $L$ from $D$ demonstrations. Here, we use $a$ to denote action for all the trajectory time-steps for brevity and drop the $L$ notation. Gaussian diffusion models [18] learn the reverse diffusion kernel $p_\\theta(a_{t-1}| a_t)$ for a fixed forward kernel that adds Gaussian noise at each step $q(a_t | a_{t-1}) = \\mathcal{N}(a_t; \\sqrt{\\alpha_t}a_{t-1}, (1 - \\alpha_t)I)$, such that $q(a_T) \\approx \\mathcal{N}(0, I)$. Here $t \\leq T$ represents the diffusion time-step and $\\alpha_t$ the noise schedule. To generate trajectories from the learned data distribution $p_\\theta(a_0)$, we sample at time step $T$ from $\\mathcal{N}(0, I)$ and apply the reverse diffusion kernel $p_\\theta(a_{t-1}| a_t)$ at each time step. For training the model, maximization of the log- likelihood of the data distribution $\\log q(a_0)$ and reparametriza- tion of the forward diffusion kernel yields the following loss used in practise [19]:\n$L_t(\\theta) = E_{q(a_0) \\mathcal{N}(\\epsilon_0;0,I)} [|| \\epsilon - \\epsilon_\\theta(a_t, a_0, t)||^2]$ (1)\nHere $\\Lambda_t$ is a function of $\\alpha_t$, and the network $\\epsilon_\\theta$ is conditioned on observation $o$. We train our model to predict the noise $\\epsilon_0$ added to action $a_0$ for generating the noisy action $a_t$ taken as the input to the network. Tweedie's formula [20] can be used to show that $\\epsilon_0$, and consequently $\\epsilon_\\theta$ are proportional to the score"}, {"title": "C. Policy Composition and Sampling", "content": "To sample from a product of distributions, we need the score of the composition at each noise scale of the ancestral sampling chain. Our product distribution can be expressed as $p_{comp}(a_0) = p^1(a_0) * p^2(a_0)$, where $a_0$ has been specifically written to reflect that the distributions are composed in the data space. Then the score of the composed distribution at diffusion time $t$ can be written as:\n$\\nabla_{a_t} \\log q_{comp}(a_t) = \\nabla_{a_t} \\log (\\int [\\prod_i p^i(a_0)] q(a_t | a_0) da_0)$ (3)\nA long line of works instead add the individual scores of the distributions being composed $\\sum_i (\\nabla_{a_t} \\log [\\int q^i(a_0) q(a_t | a_0) da_0])$, since Equation 3 is not tractable. Du et al. [13] bring this out as the reason for inferior quality of samples from composed image distributions and suggest Annealed MCMC samplers instead of ancestral sampling that does not result in the correct sequence of marginals expected by the reverse diffusion process. However, we utilize this sequence of marginals to interpolate between distributions."}, {"title": "A. Novel Motion Generation by Composing Diffusion Models", "content": "To spatially blend between distributions for generating novel motion, we propose to sample from $q_{comp}(a_0) = \\prod_{i=1}^N (a_0)^{W_i}$, where $\\sum_{i=1}^N W_i = 1$, where we have $N$ base policies. The sum of scores of the composed distribution at each time-step can then be written as:\n$\\nabla_{a_t} \\log q_{comp}(a_t) \\approx \\sum_{i=1}^N W_i (\\nabla_{a_t} \\log [\\int p^i(a_0) q(a_t | a_0) da_0]) = \\sum_{i=1}^N W_i \\nabla_{a_t} (\\int p^i(a_0) \\frac{q(a_t | a_0)}{\\phi (a_t)} \\phi (a_t) da_0) $ (4)\nWhere $\\phi$ is the standard normal distribution. Here, we have split the mean and variance effects of the forward diffusion transition kernel $q(a_t | a_0)$ to suggest that the individual distri- butions being composed are not invariant across time-steps.\nExpressing the $i$th base policy distribution at diffusion time-step $t$ as an EBM $p_{i;t}(a) \\propto exp(-E_{i;t}(a)) / Z_e$, we get its score as $\\nabla \\log p_{i;t}(a) = -\\nabla E_{i;t}(a)$, where $E_{i;t}$ represents the noisy shifted energy function. Conse- quently, samples generating using Equation 4 converge to the $exp(-\\sum_{i=1}^N W_i E_{i;t}(a)) / Z_o$. This can be interpreted a weighted mixture of noisy energy functions that become more accurate as the diffusion time-step tends to 0. Changing the weights will relocate the minimums of the composed distribution, with the $w_i = 1$ falling back on the base policy $i$. The gradient of the energy function $\\nabla E_{i;t}(a)$ is proportional to the output of diffusion models $\\hat{\\epsilon}_{i;0}(a_t, t)$, both of which estimate the score of the data distribution corresponding to the $i$th base policy [13]. Thus a weighted addition of the diffusion model outputs $\\sum_{i=1}^N W_i \\hat{\\epsilon}_{i;0}(a_t, t)$ where $\\sum_{i=1}^N W_i = 1$ is proportional to the gradient of the weighted energy function $(\\nabla \\sum_{i=1}^N (W_i E_{i;t}(a))$ at diffusion time-step $t$. Hence, this enables sampling from regions that are not minimums in any of the individual energy functions or distributions being com- posed, while also lending some control over it's placement. We would like to emphasize that the denoising process in diffusion models is essential for this as it provides gradual gradient estimates to move towards the target minimums.\nTo verify our claims, we train two diffusion models on 2D data samples from Gaussian's with means at (5,5) and (-5,-5) and variance 1. The data distribution has limited overlap. The results on composing them using reverse diffu- sion sampling are shown in figure 2. On adjusting the weight applied to the score of each model, the samples derived from the approximated score of the composition shift towards the respective distributions, implying that we can blend them in relative proportions of their scores."}, {"title": "B. MMD-FK Metric", "content": "Metrics commonly used in the robotics community are task and action space dependent. Wang et al. [3] and Urain et al. [2] use success rate as a metric to evaluate policy composition. However, success rate is not indicative of the quality of the composed trajectory. For evaluating the composition of a set of base policies, we would like to measure the distance between the composed distribution and a reference distribution of trajectories, if available. Several integral probability metrics have been proposed in the image generation literature such a"}, {"title": "C. Diffusion Score Equilibrium", "content": "We present our few-shot learning approach DSE shown in Figure 1 in this section. As elaborated in Section III-A, the weighted addition of scores $\\sum_{i=1}^N W_i \\hat{\\epsilon}_{i;0}(a_t, t)$ tends towards a minimum of the weighted mixture of energy functions. The weighted scores reach an equilibrium at the minimum, or a maximum in terms of the composed probability distribution $\\prod_{i=1}^N q_i(a_t)^{W_i}$. We use this fact to tune the composition weights $w_i$ to reduce the distance between the resulting energy minima and that of the few-shot data distribution.\nAssuming $M$ motion demonstrations $D_j$ where $j = 1..M$, we want to learn the optimal policy, which we evaluate using the MMD-FK distance between the data-distribution and samples from the policy. Given the limited number of demonstrations, the policy trained on the few-shot data learns a very noisy estimate of the score function. Sampling from such a policy often results in incorrect motions as the energy function gradient estimates are not accurate. Our main insight is to use gradient priors from the base set of policies to get a more accurate estimate of actual gradient towards the minimum. Prior compositional approaches combine diffusion policies as the weighted sum of scores of the base policies, where the weights are empirically decided based on sample quality and need not necessarily sum to 1 [13]. A parallel to our approach would be $\\sum_{i=1}^N W_i \\hat{\\epsilon}_{i;0}(a_t, t)$ where $\\sum_{i=1}^N W_i = 1$ for $N$ base policies, which forms our baseline. We use this score estimate as a prior for our policy learned on the few-shot data $w_{comp} \\epsilon_{comp;0}(a_t, t) + W_{fs} \\hat{\\epsilon}_{fs;0}(a_t, t)$ where $W_{comp} + W_{fs} = 1$. This can be reformulated as $\\sum_{i=1}^{N+1} w_i \\hat{\\epsilon}_{i;0}(a_t, t)$ where $\\sum_{i=1}^{N+1} w_i = 1$, where the $(N + 1)$th policy is trained on the few-shot demonstrations $D$. Finally, we estimate $w_i$ by minimizing MMD-FK between the few-shot demonstration data and our composed policy samples.\nEstimating $w_i$ is challenging, but attempts have been made previously to estimate the sampling parameters in differen- tiable samplers for diffusion models [23] with gradient based methods. These gradient based methods are computationally expensive due to multiple backward passes through the model. Instead, we utilize a non-gradient based quadratic optimizer [24] to tune our weights with the objective function of MMD- FK. Our approach is described in Algorithm 1."}, {"title": "IV. EXPERIMENTS", "content": "We use prior motions corresponding to a line, a circle and inverted pendulum along the X, Y and Z axis as base policies for most of our experiments, unless explicitly specified. We generate joint-position demonstration data using damped-least squares based differential inverse kinematics [25] for Franka Research-3 robot in Mujoco [26]. The priors execute these trajectories in task space with random initial end-effector orientations and positions. All our policies are trained on the smallest variant of DiT [27], conditioned on the initial state of the robot in configuration space. The model $\\hat{\\epsilon}_\\theta(a_T, o, t)$ learns to predict the noise that was added to the input $a_t$, conditioned on the diffusion time-step $t$ and the observation $o$ using AdaLN [28]."}, {"title": "B. Baselines", "content": "We utilize two baselines to compare against our approach. The first is the composition of diffusion policies as proposed by Du et al. [13, 12]. We find optimal compositional weights for this method using the optimization procedure similar to ours. The sample size for the optimization procedure is adjusted based on the number of demonstrations in the few- shot dataset. The second is a non-compositional baseline of a diffusion model trained on the demonstration data."}, {"title": "C. Composition and Spatial Blending", "content": "We train multi-modal priors to test compositional approach's ability to sample from regions of high probability in both the distributions as shown in image A and B of Figure 4. We train policy A to reach towards the +X or +Y direction and policy B to reach towards the +X or the -Y direction. We expect the composed policy C with $w_1 = w_2 = 0.5$ to sample from the modes of reaching towards the +X direction as the +X behavior exists in both Policy A and B. We see exactly this behavior as the MMD-FK between Policy A and policy C is 0.58, between Policy B and Policy C is 0.27 and Policy C and a +X direction policy is 0.11. Lower values of MMD- FK indicates lower errors or higher match between the two trajectory distributions. Composing policies to sample from the common regions of high probability was also shown for the reach and obstacle avoidance task by Urain et al. [2]. However, their work used hand crafted potential functions to compose these distributions [2]. We also showcase spatial blending where we compose a policy CircleX and policy LineX to create a spiral, as shown in image C Figure 4. Finally, we showcase the result of composing the multi-modal policy Line+X/ + Y and policy CircleX in image D in Figure 4. The composed policy is more dominant along the +Y direction due to the directional similarity of motions."}, {"title": "D. Few-shot learning", "content": "We present our few-shot learning results in this section. A core element of our approach is the optimization procedure to evaluate the compositional weights. For all the experiments, we run the optimization procedure 4 times, where it is ini- tialized with the normalized MMD-FK values between the prior motion datasets and the novel demonstration dataset, and three random initial values that sum to 1. We found that the optimization was also able to recover the base policies from corresponding demonstration data collected on the real robot. We compare DSE against our baselines for 5 novel trajectories not seen by the robot, three in a simulated setting, and two collected on the real robot. We report MMD-FK values with the reference trajectory distribution wherever available, evaluated over 50 samples. We also show the mean squared error(MSE) between the collected demonstrations on the real robot and the rolled out trajectory from the corre- sponding initial states. DSE also achieved lower MSE with the collected demonstrations than the baselines, confirming the utility of our metric MMD-FK for evaluating compositional weights.\nWe analyze each experiment closely below.\n\u2022 SpiralX: We generate a spiral trajectory along the X-axis as the target policy. In this experiment, along the lines of Section IV-C, we consider only Line+X/+Y and CircleX as our prior policies. The vanilla composition method clearly struggles in this case due to the prior policy being multi- modal, as explained in Section IV-C. DSE performs the best of the three approaches compared, as shown in Figure 5.\n\u2022 Step: We generate a step trajectory in the XZ plane. We observe that DSE policy performs surprisingly well with just 5 demonstrations, largely due to the base policy gradient priors, while the fine-tuned policy does not perform well."}, {"title": "V. DISCUSSION AND LIMITATIONS", "content": "As the number of training demonstrations are increased, the weight assigned by our approach DSE to the fine-tuned model increases. This is expected as if we have more demonstrations our model picks the true data distribution rather than the compositions over the base policies. However, as we observe more data vanilla composition models also perform better as they get a better estimate of the trajectory distribution.\nOur priors are not orthogonal and can be chosen with a lot of freedom. The DSE method also works with multi-modal prior policies, as showcased in Section IV-C. This is unlike policy composition using multiplicative Gaussian policies [1] which cannot handle multi-modality. Moreover, Gaussian Mixture Models face the challenge of exploding number of modes as the number of prior policies increase, further highlighting the efficiency of DSE. Our results can also improve with more priors however this would lead to increased compute time to find optimal weights.\nThe metric MMD-FK is created to evaluate the distance between two trajectory distributions for the whole body of the robot. However, we can re-weight the links to pay more emphasis on the end-effector or any other link as required. Empirically we see that our fine-tuned policy forms an upper- bound for the MMD-FK score obtained for DSE.\nWe do want to acknowledge that these compositions are in the state space of the robot rather than in the raw observation space such as the visual observations of the robot."}, {"title": "VI. CONCLUSION", "content": "We present a novel compositional approach to few-shot learning called Diffusion Score Equilibrium (DSE) based on equilibrium of scores predicted by diffusion models. Our ap- proach composes a policy trained on the target demonstrations with a set of base policy priors and infers the compositional weights by minimizing a measure of distance between the resulting composed distribution and the demonstration data distribution. Empirically, we observed that DSE will perform better than a policy simply trained on the data irrespective of the number of provided demonstrations on average by 30% \u2013 50%, while outperforming it by significant margins in the few-shot regime. We also propose a novel metric MMD- FK to measure the distance between two movement trajectory distributions for the whole body of the robot."}]}