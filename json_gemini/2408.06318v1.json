{"title": "Can We Rely on LLM Agents to Draft Long-Horizon Plans?\nLet's Take TravelPlanner as an Example", "authors": ["Yanan Chen", "Ali Pesaranghader", "Tanmana Sadhu", "Dong Hoon Yi"], "abstract": "Large language models (LLMs) have brought\nautonomous agents closer to artificial general\nintelligence (AGI) due to their promising gen-\neralization and emergent capabilities. There\nis, however, a lack of studies on how LLM-\nbased agents behave, why they could poten-\ntially fail, and how to improve them, particu-\nlarly in demanding real-world planning tasks.\nIn this paper, as an effort to fill the gap, we\npresent our study using a realistic benchmark,\nTravelPlanner (Xie et al., 2024), where an agent\nmust meet multiple constraints to generate ac-\ncurate plans. We leverage this benchmark to ad-\ndress four key research questions: (1) are LLM\nagents robust enough to lengthy and noisy con-\ntexts when it comes to reasoning and planning?\n(2) can few-shot prompting adversely impact\nthe performance of LLM agents in scenarios\nwith long context? (3) can we rely on refine-\nment to improve plans, and (4) can fine-tuning\nLLMs with both positive and negative feedback\nlead to further improvement? Our comprehen-\nsive experiments indicate that, firstly, LLMs\noften fail to attend to crucial parts of a long\ncontext, despite their ability to handle exten-\nsive reference information and few-shot exam-\nples; secondly, they still struggle with analyz-\ning the long plans and cannot provide accurate\nfeedback for refinement; thirdly, we propose\nFeedback-Aware Fine-Tuning (FAFT), which\nleverages both positive and negative feedback,\nresulting in substantial gains over Supervised\nFine-Tuning (SFT). Our findings offer in-depth\ninsights to the community on various aspects\nrelated to real-world planning applications.", "sections": [{"title": "1 Introduction", "content": "LLMs have shown significant reasoning and plan-\nning results against various benchmarks such as\nWebArena (Zhou et al., 2023), WebShop (Yao et al.,\n2022a), AgentBench (Liu et al., 2023b) and Agent-\nGym (Xi et al., 2024b) where they act as agents\nto finish a given task on behalf of humans. In this\nvein, the community considers two main directions\nfor developing LLM-based agents: (1) prompting\nLLMs for reasoning, planning, and execution (Qin\net al., 2023; Wei et al., 2022; Yao et al., 2024; Wang\net al., 2022), and (2) fine-tuning LLMs for a given\ntask (Chen et al., 2023b; Zeng et al., 2023; Zhang\net al., 2024b; Chen et al., 2024; Song et al., 2024b).\nDespite promising contributions in each direction,\nit is seen that LLMs still fall short in more complex\nscenarios. TravelPlanner (Xie et al., 2024), as an ex-\nample, is a benchmark where an agent should gen-\nerate a plan which must meet multiple constraints\nwith respect to input queries. The authors showed\nthat GPT-4-Turbo (OpenAI, 2023) could only reach\nto Final Pass Rate of 4.4%. This indicates that LLM\nagents cannot handle long-horizon reasoning and\nplanning. In this paper, we investigate these chal-\nlenges further with four research questions using\nTravelPlanner as the benchmark, and we trust that\nour promising and negative findings will benefit\nthe community.\nOur extensive experiments indicate that (1)\nlengthy and noisy context can adversely impact\nplanning ability of the LLM agent, (2) more shots\ndo not necessarily guarantee performance improve-\nment, (3) refinement may not be effective when\nLLMs are employed as feedback generators; how-\never, it is more likely to work if the feedback gen-\nerator is based on heuristic rules, and (4) feedback-\naware fine-tuning (FAFT), our proposed approach,\ninspired by negative aware training (NAT) (Wang\net al., 2024b), can show remarkable improvement\nin planning."}, {"title": "2 Methodology", "content": "Our framework, built upon TravelPlanner, consists\nof five main components: Scrubber, Planner, Feed-\nback Generator, Refiner, and the Evaluation mod-\nule (as shown in Fig. 1). The scrubber provides"}, {"title": "3 Experimental Settings", "content": "Basic Setting. Since the focus of our work is on\nagents' capabilities in drafting plans, we only rely\non the Sole Planning setting from TravelPlanner.\nThat is, all comprehensive and necessary informa-\ntion, which are human annotations, is directly pro-\nvided to the planner agent. We also consider the\nDirect planning strategy for its simplicity because\nit performs at a similar level to other reasoning\ntechniques such as ZS-CoT (Wei et al., 2022), Re-\nAct (Yao et al., 2022b) and Reflexion (Shinn et al.,\n2024).\nDataset. (See Appx. A.1) We use the training set\nfor both few-shot prompting and fine-tuning be-\ncause it provides annotated plans. We evaluate the\nagent against both validation and test sets for RQ1\nand RQ2 in Section 4. As for RQ3, we consider\nonly the validation set because we do not have\naccess to the system feedback offline. Regarding\nRQ4, we use the training set for fine-tuning the\n(Open-LLM) planner agent."}, {"title": "4 Findings", "content": "RQ1: Are LLM agents robust enough to noisy\ninformation for reasoning and planning? Table 1\nshows that GPT-3.5-Turbo has a better performance\nwhen it receives shrunk reference information. This\nindicates that GPT-3.5-Turbo still struggles to at-\ntend to the most important parts of a given context\nfor reasoning, prone to excessive irrelevant (con-\ntext) chunks. Therefore, it is worth considering an\nexternal intelligent context-cleaning agent.\nRQ2: Can more shots help with the planning\ntask, or does it worsen hallucination? It is com-\nmonly accepted that having more few-shots is help-\nful in ICL, but does it apply to TravelPlanner? As\nTable 1 shows, the Final Pass rate reaches its high-\nest value when there are 2 shots, while having more\nshots may not improve if not hurt more. We pre-\nsume that more shots in the context window may\ndistract the LLM and lead to hallucination (e.g.,\nusing entities that do not exist in the given refer-\nence information). The results of the Hallucination\nRate attest to this assumption. That is, giving more\nshots may potentially cause severer hallucination\nin tasks where the context of the reference infor-\nmation is complex tabular texts. Another finding\nis that at least one in-context example is beneficial\n(Xie and Min, 2022). Finally, we conclude that as\nwe have more shots, the pass rate and hallucination\nrate results worsen.\nOur RQ1 and RQ2 observations align with the exist-\ning theoretical and experimental works, e.g., (Han\net al., 2023; Levy et al., 2024), which identify the\npotential causes underlying current LLMs' failure\nin length generalization, that when they encounter\na much longer context, the attention scores are di-\nluted, and thus the score distribution becomes flat\nleading to information loss. That is, the entropy\nof the attention score will explode with increasing\ncontext. In other words, LLMs become lost in how\nto focus on the right information, especially when\npre-training is done on shorter text segments.\nRQ3: Can we rely on refinement to improve\nplans? To address this, we require feedback that\nhighlights what went wrong, accompanied by ex-"}, {"title": "5 Conclusion", "content": "In this paper, we studied the impacts of context, the\nnumber of shots, and the utilization of feedback\non a complex long-horizon planning task known\nas TravelPlanner. Our findings aim to advance a\nbroader spectrum of agentic frameworks and strate-\ngies within the research community.\nFor future work, we plan to explore methods\nthat incorporate annotated shots in SFT and post-\ntraining. This approach can address the bottleneck\nwhere LLMs' knowledge and skills are predom-\ninantly acquired during pre-training, while align-\nment SFT teaches the model which sub-distribution\nof formats to use when interacting with users (Zhou\net al., 2024a). Finally, we will explore the interplay\nbetween RLHF and FAFT."}, {"title": "Limitations", "content": "Due to budget constraints, we were only able to use\nGPT-3.5-Turbo as the Planner agent for RQ1 and\nRQ2. For RQ4, further investigations are needed to\nexplore the relationship between the magnitude of\ngains and the size of the FAFT training set, as well\nas the impact of the ratio of positive to negative\nsamples on the final performance. Additionally,\nenhancing the feedback expressions could further\nimprove the performance of FAFT. It would also be\ninteresting to investigate RLHF techniques, such as\nDPO (Rafailov et al., 2024) and PRO (Song et al.,\n2024a), to better utilize feedback."}, {"title": "Ethics Statement", "content": "Our work is founded upon TravelPlanner, a bench-\nmark designed for complex planning tasks. We\nadhere to the original work's specifications, utiliz-\ning their data, evaluation scripts, and definitions of\ncommonsense. Acknowledging the foundational\nconcepts and designs of the original benchmark,\nwe strictly adhere to TravelPlanner's guidelines,\nensuring the integrity of the evaluation process by\nprohibiting any form of cheating in the validation\nand test sets. This commitment upholds the fairness\nand reliability of this work.\nAs for environmental cost, we acknowledge that\nour work necessitated extensive experiments to\nderive robust conclusions. However, future en-\ndeavours can leverage these insights, potentially\nreducing the need for numerous large-scale com-\nparisons. Models intended for production could\nundergo training once, utilizing the most promis-\ning settings identified through our research."}, {"title": "A Appendix", "content": "A.1 Dataset\nThe TravelPlanner dataset consists of three splits\nof training, validation, and test sets as follows:\n\u2022 The Training Set consists of 45 triplets of query,\nreference, and human annotated plan. The anno-\ntations are used as demonstrations for in-context\nlearning or supervised fine-tuning in our paper.\nPlease note that these annotated plans are merely\na subset of many feasible plans. As expected,\nthe Oracle (i.e., system) returns the feedback for\nthe annotations where no issue is raised (Appx.\nA.6.2).\n\u2022 The Validation Set comes with 180 pairs of\nquery and reference, with no annotated plans.\n\u2022 The Test Set holds 1,000 queries together with\ntheir references, without any annotated plans.\nFor a given query, agents are expected to formu-\nlate a (comprehensive) plan which includes trans-\nportation, restaurants, attractions, and accommoda-\ntion for each day (Appx. A.6.1 shows an example).\nA.2 Evaluation metrics\nFollowing TravelPlanner, we use automatic evalua-\ntion metrics to assess whether a plan generated by\nthe agent meets the (correct) format condition as\nwell as all the constraints.\n\u2022 Delivery Rate measures whether the agent could\nsuccessfully generate a plan within a limited num-\nber of steps. Falling into any dead loops or in-\nvalid plan formats leads to failure. In the sole-\nplanning setting, any failure in drafting a plan\nnegatively impacts the delivery rate.\n\u2022 Commonsense Constraint Pass Rate assesses\nwhether the agent can incorporate commonsense\nwhile drafting plans without explicit instructions.\nFor example, the agent has to pick valid entities\n(incl. restaurants, hotels, etc.) from the reference\ninformation and not hallucinate.\n\u2022 Hard Constraint Pass Rate measures whether\na plan meets all hard constraints mentioned in\nthe query, e.g., budget limit, cuisine preference,\nor accommodation type.\nN.B. For Commonsense and Hard Constraint\nPass Rates, the evaluation is done in two ways,\nMicro and Macro, which evaluate the agent's\ncapability of following individual constraints vs."}, {"title": "A.3 Framework", "content": "In Fig. 1, we show that the Planner agent generates\na plan for a given query and (cleaned) reference in-\nformation. In TravelPlanner's Two-Staging setting,\nthe reference information is collected by an up-\nstream tool agent which gathers valid information\nrelated to transportation, dining, attractions, and ac-\ncommodation from their corresponding source files.\nThe original benchmark also particularly creates\nvalid reference information for the Sole Planning\nsetting where the focus is on the Planner agent.\nHence, we evaluate our solution only in the Sole\nPlanning setting since our focus is on planning.\nA.3.1 The Scrubber Agent\nSince the reference information is massive and\nlengthy (i.e., 10,000 tokens on average), we pro-\npose the Scrubber, a filtration agent, which infers\nthe hard constraints from the query. There are 5\nhard constraints: Room Rule, Room Type, Cuisine,\nBudget and Transportation. We let the Scrubber\nto predict the exact constraint value based on the\nquery, for example, one or several cuisine prefer-\nences from the set: {American, Chinese, French,\nIndian, Italian, Mediterranean, Mexican}. In-\nternally within the Scrubber, we inject the whole\ntraining set as few-shot examples on top of the test\nquery, to improve the accuracy. Then, during in-\nference, with the Scrubber agent, each predicted\nhard constraint is used to remove the rows (from\nthe tables in the reference information) that are\nnot used to produce the final plan. For example,"}, {"title": "A.3.2 The Feedback Generator and Refiner", "content": "Once the original plan has been drafted, refine-\nment is conducted in an iterative manner. For this,\nwe follow previous works where two agents are\nseparately created with natural language communi-\ncation capabilities.\nThe Feedback Generator which is responsible for\ngenerating nuanced task-dependent feedback that\naddresses multiple constraints. We tailor a prompt,\nas shown in Appx. A.5.2, to ask LLMs to write\nfeedback with regard to commonsense constraints.\nIn the instructions, we provide a list of constraints\nwith their descriptions. Here two-shots are used\nto help with feedback generation. The shots are\nrandomly selected from the training set.\nThe Refiner Agent refines the generated plan based\non the feedback received from the Feedback Gen-\nerator towards a better version (see the prompt in\nAppx. A.5.3).\nFig. A.1 illustrates the entire refinement phase. The\nfeedback points out that there is a repeated attrac-\ntion for Days 1 and 2, and the accommodation does\nnot satisfy the minimum number of nights require-\nment. Then, the Refiner agent refines this draft plan\ninto a new plan where the attraction for the first day\nis replaced to avoid repetition, and another hotel\nis chosen which allows a two-night stay. Finally,\nbased on the system assessment, the refined plan\nmeets all commonsense constraints."}, {"title": "A.4 Supervised Fine-Tuning and\nFeedback-Aware Fine-Tuning", "content": "A.4.1 Training Example Template for SFT\nThe TravelPlanner training set consists of 45 sam-\nples with annotated plans. We use reference infor-\nmation, queries, and annotated plans for general\nSFT (which is a baseline).\nreference information box: {ref}\nquery: {query}\ndraft travel plan: {plan}\nA.4.2 Training Example Template for FAFT"}, {"title": "A.4.3 Inference Example Template for FAFT", "content": "reference information box:{ref}\nquery: {query}\nfeedback: {feedback}\nis_reasonalbe_visiting_city: success\nis_valid_restaurants: success\nis_valid_attractions: success\nis_valid_accommodation: success\nis_valid_transportation: success\nis_valid_information_in_current_city: success\nis_valid_information_in_sandbox: success\nis_not_absent: success\ndraft travel plan:"}, {"title": "A.4.4 Fine-tuning Setup", "content": "In RQ4, for the Planner agent, we fine-tune Llama3-\n8B for 3 epochs with a batch size of 4 for both SFT\nand FAFT. We use a constant scheduler learning\nrate of 5 \u00d7 10-5 and no warm-up, and we disable\npacking among training samples to avoid cross-\ncontamination. We train the model in 4-bit. The\nmaximum sequence length is set to 7000 to allow\nthe training context to cover all samples. For com-\nputation and memory efficiency, we also use Low-\nRank Adaptation with $r = 16$ and $alpha = 16$."}, {"title": "A.5 Prompt Templates for Agents", "content": "A.5.1 The Scrubber's Prompt Template"}, {"title": "A.5.2 Feedback Generator's Prompt", "content": "Now You are an advanced reasoning, analyzing and advisory\nagent who can write feedback and insights for a given draft\ntravel plan, based on the given query and reference\ninformation box.\nThe feedback you write should check and judge if the given\ndraft travel plan violates one or several following\nconstraints:\n* is_reasonalbe_visiting_city: {success or fail}. This\nrefers to Reasonable City Route: Changes in cities during\nthe trip must be reasonable.\n* is_valid_restaurants: {success or fail}. This refers to\nDiverse Restaurants: Restaurant choices should not be\nrepeated throughout the trip.\n* is_valid_attractions: {success or fail}. This refers to\nDiverse Attractions: Attraction choices should not be\nrepeated throughout the trip.\n* is_valid_accommodation: {success or fail}. This refers to\nMinimum Nights Stay: The number of consecutive days spent\nin a specific accommodation during the trip must meet the\ncorresponding required minimum number of nights' stay.\n* is_valid_transportation: {success or fail}. This refers\nto No conflict Transportation: Transportation choices\nwithin the trip must be reasonable. For example, having\nboth \"self-driving\" and \"flight\" would be considered a\nconflict.\n* is_valid_information_in_current_city: {success or fail}.\nThis refers to Within Current City: All scheduled\nactivities for the day must be located within that day's\ncity(s).\n* is_valid_information_in_sandbox: {success or fail}. This\nrefers to Within Sandbox: All information, such as\nrestaurants, attractions, accommodations and transportation\nin the plan, must be within the closed sandbox (reference\ninformation box); otherwise, it will be considered a\nhallucination.\n* is_not_absent: {success or fail}. This refers to Complete\nInformation: No key information should be left out of the\nplan, such as the lack of accommodation during travel.\nHere are some examples for your information as\ndemonstrations:"}, {"title": "A.5.3 The Refiner's Prompt Template", "content": "You are a proficient planner. Based on the provided\ninformation and query, please give me a detailed plan,\nincluding specifics such as flight numbers (e.g., F0123456)\nrestaurant names, and accommodation names. Note that all\nthe information in your plan should be derived from the\nprovided data. You must adhere to the format given in the\nexample. Additionally, all details should align with\ncommonsense. The symbol '-' indicates that information is\nunnecessary. For example, in the provided sample, you do\nnot need to plan after returning to the departure city.\nWhen you travel to two cities in one day, you should note\nit in the 'Current City' section as in the example (i.e.,\nfrom A to B)."}, {"title": "A.6 Case Presentation", "content": "A.6.1 Query Example with its Travel Plan"}, {"title": "A.6.2 Feedback Examples Generated by\nLLMS", "content": "The feedback generated by LLMs is in the same\nformat of the system feedback."}, {"title": "A.7 Related Works", "content": "A.7.1 Benchmarks for LLM-based Generalist\nAgents\nIt has been anticipated that generalist agents can\nhandle diverse tasks and evolve across different\n(cyber) environments at the human level which is\na long-term goal in the AGI community. LLMs\ncan be used as experts, which mimic humans, that\nhave a strong generalization capability that not only\nsuits conventional NLP but also agentic tasks. Re-\ncently, plenty of benchmarks have been proposed\nto evaluate the agents across various tasks and envi-\nronments comprehensively and fairly. We provide\nan overview of popular benchmarks in the com-\nmunity in Table A.1. Some benchmarks such as\nALFWorld (Shridhar et al., 2020) and Mind2Web\n(Deng et al., 2023), which are already included in\nlarger benchmarks, are not listed in the table. Al-\nthough the recent progress in multi-modal LLMs\nhas spurred research into multi-modal LLM agents\n(Yang et al., 2023; Zheng et al., 2024), we only list\nbenchmarks that focus exclusively on text-based\nenvironments which assess LLM agents' abilities\nvia textual reasoning and taking actions in-depth.\nThe listed benchmarks support agents powered\nby both API-based proprietary and open-weight\nLLMs with convenient drop-in replacement inter-\nfaces. It is also free to add few-shots or use other\nprompting strategies to generate actions.\nA.7.2 Long Contexts Challenge for LLMS\nBesides the fact that more and more LLMs of-\nfer long-context capabilities (Fei et al., 2023; Rat-\nner et al., 2023; Liu et al., 2023a; Zhao et al.,\n2024; Qian et al., 2024), recent studies question\nLLMs' ability to find needles in a haystack be-\ncause they face challenges in discriminating highly\nsemantically related information, and can be eas-\nily distracted by irrelevant and misleading con-\ntents in long contexts (Wu et al., 2024; Zhu et al.,\n2023; Chang et al., 2024; Shi et al., 2023; gkam-\nradt, 2023). The TravelPlanner (Xie et al., 2024)\nis a benchmark to provide insightful answers to"}, {"title": "A.7.3 Multi-Agent Collaboration", "content": "Recent studies have borrowed the multiple-agent\nmethodology for collaboration on cyber tasks, gam-\ning, coding, math reasoning, conversation respond-\ning, and question answering (Guo et al., 2024;\nWu et al., 2023a,b; Zhang et al., 2024c; Li et al.,\n2024; Liu et al., 2024; Talebirad and Nadiri, 2023;\nZhang et al., 2024a; Wang et al., 2024a). Under\nthe hood, these works assign role-specific prompts\nto the LLM to build multiple agents for synergy\nand collaboration. The self-refinement works can\nbe classified into this realm, where the advisor and\nrefiner agents can troubleshoot and modify the re-\nsponse in a few rounds (Madaan et al., 2024; Paul\net al., 2023; Kim et al., 2024; Pan et al., 2024;\nChen et al., 2023a). However, few works study the\nreliability and robustness of multi-agent collabo-\nration in more complex and practical tasks. Com-\npared to the previous testbeds where generation\ner"}]}