{"title": "HEIGHT: HEterogeneous Interaction GrapH Transformer for Robot Navigation in Crowded and Constrained Environments", "authors": ["Shuijing Liu", "Haochen Xia", "Fatemeh Cheraghi Pouria", "Kaiwen Hong", "Neeloy Chakraborty", "Katherine Driggs-Campbell"], "abstract": "We study the problem of robot navigation in dense and interactive crowds with environmental constraints such as corridors and furniture. Previous methods fail to consider all types of interactions among agents and obstacles, leading to unsafe and inefficient robot paths. In this article, we leverage a graph-based representation of crowded and constrained scenarios and propose a structured framework to learn robot navigation policies with deep reinforcement learning. We first split the representations of different components in the environment, and propose a heterogeneous spatio-temporal graph to model distinct interactions among humans, robots, and obstacles. Based on the heterogeneous st-graph, we propose HEIGHT, a novel navigation policy network architecture with different components to capture heterogeneous interactions among entities through space and time. HEIGHT utilizes attention mechanisms to prioritize important interactions and a recurrent network to track changes in the dynamic scene over time, encouraging the robot to avoid collisions adaptively. Through extensive simulation and real-world experiments, we demonstrate that HEIGHT outperforms state-of-the-art baselines in terms of success and efficiency in challenging navigation scenarios. Furthermore, we demonstrate that our pipeline achieves better zero-shot generalization capability than previous works when the densities of humans and obstacles change. More videos are available at https://sites.google.com/view/crowdnav-height/home.", "sections": [{"title": "I. INTRODUCTION", "content": "Robots are increasingly prevalent in human-centric environments. In applications such as last-mile delivery and household robots, the ability to navigate among humans is crucial. For example, Fig. 1 shows a navigation scenario with abundant subtle interactions: Obstacles have a one-way effect on the paths of agents (i.e. humans and the robot), while the influence among agents is mutual. Among agents, humans may react to other humans and robots in different ways. To navigate, a robot directly participates in some interactions in its close proximity, and simultaneously, is indirectly affected by other interactions. These interactions are heterogeneous, dynamic, and difficult to infer, making navigation in such environments challenging.\nRising to these challenges, previous works have explored various approaches for robot crowd navigation [1]-[3]. However, these works typically have one of two limitations: (1)"}, {"title": "II. RELATED WORKS", "content": "In this section, we first review the literature of robot crowd navigation, which is divided into model-based and learning-based approaches. Then, we discuss previous crowd navigation efforts in constrained spaces. Finally, we review graph attention mechanism with a focus on the usage of attention networks in multi-agent interaction modeling."}, {"title": "A. Model-based methods", "content": "Robot navigation in human crowds is particularly challenging and has been studied for decades [1], [10]-[12]. Model-based approaches have explored various mathematical models to optimize robot actions [1], [2], [13]-[16]. As an early example, ROS navigation stack [17] uses a cost map for global planning and dynamic window approach (DWA) for local planning [1]. DWA searches for the optimal velocity that brings the robot to the goal with the maximum clearance from any obstacle while obeying the dynamics of the robot. By treating humans as obstacles, DWA exhibits myopic behaviors such as oscillatory paths in dynamic environments [18]. As a step forward, optimal reciprocal collision avoidance (ORCA) and social force (SF) account for the velocities of agents. ORCA models other agents as velocity obstacles and assumes that agents avoid each other under the reciprocal rule [2], [13]. SF models the interactions between the robot and other agents using attractive and repulsive forces [14]. However, the hyperparameters of the model-based approaches are sensitive to crowd behaviors and thus need to be tuned carefully to ensure good performance [18], [19]. In addition, model-based methods are prone to failures, such as the freezing problem, if the assumptions such as the reciprocal rule are broken [20], [21]. In contrast, while our method also models these interactions, we learn the hyperparameters of the model from trial and error with minimal assumptions on human behaviors posed by model-based methods."}, {"title": "B. Learning-based methods", "content": "Learning-based approaches have been widely used for navigation in dynamic environments to reduce hyperparameter tuning efforts and the number of assumptions. One example is supervised learning from expert demonstrations of desired behaviors, where the expert ranges from model-based policies [22]-[24], human teleoperators in simulators [25], to real pedestrians [26], [27]. Supervised learning does not require explorations of the state and action spaces of the robot, yet the performance of learned policy is limited by the quality of expert demonstrations.\nAnother line of work takes advantage of crowd simulators and learns policies with RL. Through trial and error, RL has the potential to learn robot behaviors that outperform model-based approaches and expert demonstrations [28]. For example, Deep V-Learning first uses supervised learning with ORCA as the expert and then uses RL to learn a value function for path planning [3]-[5], [29], [30]. However, Deep V-Learning assumes that state transitions of all agents are known without uncertainty. In addition, since the networks"}, {"title": "C. Crowd navigation in constrained environments", "content": "Besides dynamic agents, real-world navigation environments usually consist of static constraints, such as walls, furniture, and untraversable terrains. To deal with these constraints and humans, some methods such as DWA [1] and DS-RNN [8] use groups of small circles to represent the contours of obstacles. While groups of circles is a straightforward representation of obstacles, as we will show in Sec. V, they are not scalable as the number of obstacles increases and can lead to overfitting problems for learning-based approaches.\nOther learning-based approaches use raw sensor images or point clouds to represent the whole environment [7], [25], [33], [34]. These end-to-end (e2e) pipelines have made promising progress in simulation. However, generalizing these e2e methods to real-world scenarios is challenging due to domain gaps such as inaccurate human gait simulation [35], [36]. Despite the recent advancements in crowd navigation simulators [37]- [39] and datasets [26], [40], [41], learning a deployable e2e policy that outperforms human teleoperation in dense and interactive crowds remains an open challenge.\nFurthermore, prior works that demonstrate strong real-world performance in dense crowds and obstacles usually leverage processed inputs such as detected human states and processed sensor readings [28], [32], [42]. As such, we develop a structured input representation of humans and obstacles, which splits human and obstacle representations and feeds processed states to the policy network. This input representation leads to strong performance in both simulation and real-world. In contrast to the above works with processed inputs that focus on pedestrian behavior prediction [32], reward design [28], and incorporating risks into map [42], our paper proposes a principled way for network architecture design from the structures of complex navigation scenarios."}, {"title": "D. Graph attention networks for multi-agent interactions", "content": "In recent years, attention mechanisms have demonstrated success in various fields [43]-[45]. Vaswani et al. propose a transformer with self-attention mechanism that achieves state-of-the-art performance in machine translation [43]. Later, graph attention networks show the effectiveness of attention on learning relationships of data with graphical structures such as citation networks [44]. Inspired by these works, researchers in trajectory prediction and crowd navigation have found that attention networks are also well-suited to capture interactions amongst agents and entities, which contain essential information for multi-agent tasks [3], [8], [45]-[47]. For each agent, these works compute attention scores for all neighboring agents. Due to the permutation invariance property, attention scores better capture the importance of pairwise relationships than combining the information of all agents with concatenation or an LSTM encoder [4], [29].\nMore specifically, in crowd navigation of robots and autonomous vehicles, a line of works uses a robot-human attention network to determine the relative importance of each human to the robot [3], [8], [48], [49]. However, interactions among humans, which can also influence the robot, are not explicitly modeled. To this end, other works use a homogenous graph network to include both RH and HH interactions [6], [50]. However, since these works feed RH and HH features to a single attention network, the resulting robot policy has difficulty reasoning the specificity of each type of feature, which limits the robot's ability to adapt to different interactions, as demonstrated in [51] and our experiments in Sec. V. In addition, the works above only deal with open-world social navigation and thus ignore the interactions between agents and obstacles. To this end, Chen et al. treat the humans, the robot, and static objects as different types of nodes in a heterogeneous graph attention network [51].\nThe main difference from our work is that Chen et al. focus on semantic navigation, where a robot must navigate to an object in simulation. Our work focuses on point-goal navigation in crowded and constrained real-world environments. This difference leads to (1) Different representations of obstacles: the object representation in Chen et al. comes from a known semantic map of the environment, which is hard to obtain in the real-world. Thus, in our case, treating all obstacles as a 2D point cloud is more suitable for studying collision avoidance; (2) Chen et al. learn a value function and plan paths assuming simplified dynamics for agents, whereas we learn a model-free RL policy and assume unknown state transition for all agents, which is more suitable for sim2real transfer where the dynamics of pedestrians and robots are uncertain."}, {"title": "III. PRELIMINARIES", "content": "In this section, we formulate constrained crowd navigation as a Markov Decision Process (MDP) and introduce the scene representation and the reward function."}, {"title": "A. MDP formulation", "content": "We model the constrained crowd navigation scenario as a MDP, defined by the tuple $(S, A, P, R, \\gamma, S_o)$. Let $w_t$ be the robot state which consists of the robot's position $(p_x, p_y)$, velocity $(v_x, v_y)$, goal position $(g_x, g_y)$, and heading angle $\\theta$. Let $h^i_t$ be the current state of the i-th human at time t, which consists of the human's position and velocity $(p^i_x, p^i_y, v^i_x, v^i_y)$. Let $o_t$ be the current observation of the static obstacles and walls, which is represented as a 2D point cloud. We define the"}, {"title": "B. State representation", "content": "In our MDP, a state consists of a large and varying number of agents. To aid policy learning with such a complicated state space, we develop a structured representation of states that will be fed into the structured policy network. To reduce sim2real gaps caused by raw sensor readings, our scene representation leverages processed information from perception, maps, and robot localization, which are relatively robust to domain shifts.\nIn Fig. 3, at each timestep t, we split a scene into a human representation $h_1,..., h_n$, and an obstacle representation $o_t$. In human representation (Fig. 3(b)), the position and velocity of each human are detected using off-the-shelf human"}, {"title": "C. Reward function", "content": "Our reward function consists of three parts. The first and main part of the function awards the robot for reaching its goal"}, {"title": "IV. METHODOLOGY", "content": "In this section, we present our approach to decompose the constrained crowd navigation scenario as a heterogeneous st-graph, which leads to the derivation of the HEIGHT architecture in a structured manner."}, {"title": "A. Heterogeneous Spatio-Temporal Graph", "content": "The subtle yet highly dynamic interactions among agents and entities are important factors that makes crowd navigation difficult. To model these interactions in a structured manner, we formulate the navigation scenario as a heterogeneous st-graph. In Fig. 4a, at each timestep t, our heterogeneous st-graph $G_t = (V_t, E_t)$ consists of a set of nodes $V_t$ and a set of edges $E_t$. The nodes include the detected humans $h_1,..., h_n$ and the robot $w_t$. In addition, an obstacle node $o_t$ represents the point cloud of all obstacles as a whole. At each timestep t, the spatial edges that connect different nodes denote the spatial interactions among nodes. Different spatial interactions have different effects on robot decision-making. Specifically, since we have control of the robot but not the humans, RH interactions have direct effects while HH interactions have indirect effects on the robot actions. As an example of indirect effects, if human A aggressively forces human B to turn toward the robot's front, the robot has to respond as a result of the interaction between A and B. Additionally, since the agents are dynamic but the obstacles are static, interactions among agents are mutual while the influence of static obstacles on agents is one-way. Thus, we categorize the spatial edges into three types: HH edges (blue in Fig. 4), obstacle-agent (OA) edges (orange), and RH edges (red). The three types of edges allow us to factorize the spatial interactions into HH, OA, and RH functions. Each function is parameterized by a neural network that has learnable parameters. Compared to previous works that ignore some edges [3], [8], [9], our method allows the robot to reason about all observed spatial interactions that exist in constrained and crowded environments.\nSince the movements of all agents cause the visibility of each human to change dynamically, the set of nodes $V_t$ and edges $E_t$ and the parameters of the interaction functions may change correspondingly. To this end, we integrate the temporal correlations of the graph $G_t$ at different timesteps using another function denoted by the purple box in Fig. 4(a). The temporal function connects the graphs at adjacent timesteps, which overcomes the short-sightedness of reactive policies and enables long-term decision-making of the robot.\nTo reduce the number of parameters, the same type of edges in Fig. 4(a) share the same function parameters. This parameter sharing is important for the scalability of our graph because the number of parameters is kept constant when the number of human changes [55]."}, {"title": "B. HEIGHT Architecture", "content": "In Fig. 4b, we derive our network architecture from the heterogeneous st-graph. We represent the HH and RH functions as feedforward networks with attention mechanisms, referred to as HH attn and RH attn respectively. We represent the OA function as an MLP with concatenation, and the temporal function as a gated recurrent unit (GRU). We use W and f to denote trainable weights and fully connected layers.\n1) Attention among agents: The attention modules assign weights to all edges that connect to a robot or a human node, allowing the node to pay attention to important edges or interactions. The two attention networks are similar to the scaled dot-product attention with a padding mask [43], which computes the attention score using a query Q and a key K, and applies the normalized score to a value V, which results in a weighted value v.\n$v := Attn(Q, K, V, M) = softmax\\left(\\frac{(QK^T + M)}{\\sqrt{d}}\\right)V$ (4)\nwhere d is the dimension of the queries and keys and acts as a scaling factor. The mask M is used to handle the changing number of detected humans at each timestep, as we will expand below.\nHuman-human attention: To learn the importance of each HH edge to the robot decision at time t, we first weigh each observed human w.r.t. other humans using an HH attention network, which is a self-attention among humans. In HH attention, the current states of humans $h_1,..., h_n$ are concatenated and passed through linear layers with weights $W_Q^{HH}, W_K^{HH}, and W_V^{HH}$ to obtain $Q^{HH}, K^{HH}, V^{HH} \\in \\mathbb{R}^{n\\times d_{HH}}$, where $d_{HH}$ is the attention size for the HH attention.\n$Q^{HH} = [h_1,..., h_n]^TW_Q^{HH} = [q_1, ..., q_n]^T$ (5)\n$K^{HH} = [h_1,..., h_n]^TW_K^{HH} = [k_1, ..., k_n]^T$\n$V^{HH} = [h_1,..., h_n]^TW_V^{HH} = [v_1, ..., v_n]^T$\nwhere $q_i \\in \\mathbb{R}^{1\\times d_{HH}}, k_i \\in \\mathbb{R}^{1\\times d_{HH}}$, and $v_i \\in \\mathbb{R}^{1\\times d_{HH}}$ are the query embedding, key embedding, and value embedding of the i-th human, respectively.\nIn addition, following Eq. 4, a mask $M_t \\in \\mathbb{R}^{n\\times n}$ indicates the visibility of each human to the robot at current time t and is obtained from the perception system of the robot. Assume the n-th human is not visible at time t. Then $M_t$ is a matrix filled with zeros, except that every entry in n-th column is $-\\infty$. The numerator of Eq. 4 can be express as\n$Q^{HH} (K^{HH})^T + M_t = \\begin{bmatrix} q_1k_1 & ... & q_1k_{n-1} & q_1k_{n} \\\\ : & & : & : \\\\ q_nk_1 & ... & q_nk_{n-1} & q_nk_{n} \\end{bmatrix} + \\begin{bmatrix} 0 & ... & 0 & -\\infty \\\\ : & & : & : \\\\ 0 & ... & 0 & -\\infty \\end{bmatrix}$ (6)\nLet $V^{HH} = [v_1,..., v_n]^T$, where $v_i \\in \\mathbb{R}^{1\\times d_{HH}}$ is the value embedding of the i-th human. Then, based on Eq. 4, the weighted human embeddings $\\tilde{V}^{HH} \\in \\mathbb{R}^{n\\times d_{HH}}$ is\n$\\tilde{V}^{HH} = softmax\\begin{pmatrix}\\frac{Q^{HH} (K^{HH})^T + M_t}{\\sqrt{d}}\\end{pmatrix}V = \\begin{bmatrix} c_1 & ... & c_n \\end{bmatrix}\\begin{bmatrix} q_1k_1 & ... & q_1k_{n-1} & 0 \\\\ : & & : & : \\\\ q_nk_1 & ... & q_nk_{n-1} & 0 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ : \\\\ v_{n-1} \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} c_1q_1k_1v_1 + ... + c_{n-1}q_1k_{n-1}v_{n-1} \\\\ : \\\\ c_1q_nk_1v_1 + ... + c_{n-1}q_nk_{n-1}v_{n-1} \\end{bmatrix} = \\begin{bmatrix} \\tilde{v}_1 \\\\ : \\\\ \\tilde{v}_{n-1} \\\\ \\tilde{v}_n \\end{bmatrix} = \\tilde{V}^{HH}$ (7)\nwhere $c_1,..., c_n$ are constants that reflect the effect of the scaling factor d and the softmax function. Thus, the value of the n-th missing human $v_n$ is eliminated by the mask $M_t$ and thus does not affect the resulting weighted human embedding $\\tilde{V}^{HH}$. The mask that indicates the visibility of each human prevents attention to undetected humans, which is common in crowd navigation due to the partial observability caused by the limited robot sensor range, occlusions, imperfect human detectors, etc [31]. Additionally, the mask provides unbiased gradients to the networks, which stabilizes and accelerates the training [9], [56].\nRobot-human attention: After the humans are weighted by HH attention, we weigh their embeddings again w.r.t. the robot with another RH attention network to learn the importance of each RH edge. In RH attention, we first embed the robot state $w_t$ with a fully connected layer, which results in the key for RH attention $K^{RH} \\in \\mathbb{R}^{1\\times d_{RH}}$. The query and the value, $Q^{RH}, V^{RH} \\in \\mathbb{R}^{n\\times d_{RH}}$, are the other two linear embeddings of the weighted human features from HH attention $\\tilde{V}^{HH}$.\n$Q^{RH} = \\tilde{V}^{HH}W_Q^{RH}, K^{RH} = \\tilde{v}W_K^{RH}, V^{RH} = \\tilde{V}^{HH}W_V^{RH}$ (8)\nWe compute the attention score from $Q^{RH}, K^{RH}, V^{RH}$, and the mask $M_t$ to obtain the weighted human features for the second time $\\tilde{V}^{RH} \\in \\mathbb{R}^{1\\times d_{RH}}$ as in Eq. 4.\n2) Incoporating obstacle and temporal information: We first feed the point cloud that represents obstacles, $o_t$, into a 1D-CNN followed by a fully connected layer to get an obstacle embedding $v_o$. Then, we embed the robot states $w_t$ with linear layers $f_r$ to obtain a robot embedding $v_R$.\n$v_o = f_{CNN}(o_t), v_R = f_r(w_t)$ (9)\nFinally, the robot and obstacle embeddings are concatenated with the twice weighted human features $\\tilde{V}^{RH}$ and fed into the GRU:\n$h^t = GRU(h^{t-1}, ([\\tilde{V}^{RH}, v_R, v_o]))$ (10)\nwhere $h^t$ is the hidden state of GRU at time t. Finally, the $h^t$ is input to a fully connected layer to obtain the value $V(s^t)$ and the policy $\\pi(a_t|s_t)$."}, {"title": "V. SIMULATION EXPERIMENTS", "content": "In this section, we present our environment, experiment setup, and results in simulation. Our experiments are guided by the following questions: (1) What is the advantage of our split scene representation compared with alternative representations? (2) What is the importance of the graph formulation and why do we differentiate types of edges with a heterogenous st-graph? (3) What is the importance of HH and RH attention in HEIGHT? (4) What are the failure cases of our method?"}, {"title": "A. Simulation environment", "content": "We conduct simulation experiments in random environment in Fig. 5(a) developed with PyBullet [58]. The robot, humans, and static obstacles are in a 12m\u00d712m arena. In each episode, rectangular obstacles are initialized with random shapes and random poses. The width and the length of each static obstacle are sampled from $N(1,0.62)$ and are limited in [0.1, 5] meters. The initial positions of the humans and the robot are also randomized. The starting and goal positions of the robot are randomly sampled inside the arena. The distance between the robot's starting and the goal positions is between 5 m and 6 m. Some humans are dynamic and some are static. The goals of dynamic humans are set on the opposite side of their initial positions to create circle-crossing scenarios. In training, the number of humans varies from 5 to 9 and the number of static rectangular obstacles varies from 8 to 12. Among all humans, 0-2 of them are static and the rest are dynamic. In testing, the number of humans and obstacles are shown in the first column"}, {"title": "B. Experiment setup", "content": "We now introduce the baselines and ablation models, training procedure, and evaluation metrics.\n1) Baselines: We compare the performance of our method with the following baselines:\n\u2022 Dynamic window approach (DWA) [1] searches for the optimal velocity that brings the robot to the goal with the maximum clearance from any obstacle. DWA is a model-based method that only considers the current state. In addition, DWA represents both humans and obstacles as groups of small circles.\n\u2022 A*+ CNN [7] is a hybrid method. With a map of the environment, A* is the global planner and generates 6 waypoints in the beginning of an episode. The inputs to the robot RL policy are a 2D LiDAR point cloud,"}, {"title": "C. Results", "content": "1) Effectiveness of scene representation: To analyze the effects of input scene representations on crowd navigation algorithms, we compare ours and HomoGAT, the two methods that distinguish human and obstacle inputs, with baselines that mix humans and obstacles in input representation: the model-based DWA, the RL-based DS-RNN, and the hybrid planner A*+ CNN. The results are shown in Table I.\nFor DWA, treating humans as obstacles leads to the highest average human collision rates (0.54) and a freezing problem, indicated by the highest average timeout rates (0.21), in all environments. For example, the robot in Fig. 6 stays close to everything and thus fails to avoid the magenta human in time.\nSimilarly, by representing obstacles as groups of circles, DS-RNN performs better in the More crowded environment (Fig 7(d)) than in the More constrained environment (Fig 6(d)). In addition, Table I shows that DS-RNN achieves the highest average collision rate with obstacles (0.13) in all environments. Furthermore, the obstacle collision rate of DSRNN increases in all 4 OOD environments, indicating an overfitting problem. Among the OOD environment, the percentage of obstacle collision increase is higher in environments with higher obstacle-to-human ratios. For example, in Less crowded with the highest percentage of obstacles, the obstacle collision rate (0.21) increases by 133% w.r.t. the obstacle collision rate in training distribution (0.09). On the contrary, in More crowded with the lowest percentage of obstacles, the obstacle collision rate (0.06) drops by 33%. The reason is that DS-RNN has trouble inferring the geometric shapes of obstacles from a large group of circles, and thus fails to avoid collision with them. Thus, treating both humans and obstacles as circles is not an ideal input representation for robot navigation algorithms.\nFor A*+ CNN, the A* global planner does not account for humans and the CNN local planner represents all observations as a 2D point cloud. As a result, from Table I, A*+CNN has the highest average timeout rate (0.10) and the longest average time (18.99) among RL-based methods in all environments, especially the two most challenging ones, for the following 2 reasons. (1) The waypoints can fail to guide the robot to reach the goal because the waypoints lose optimality as agents move."}, {"title": "VI. REAL-WORLD EXPERIMENTS", "content": "In this section, we present our hardware setup and sim2real testing results in everyday environments with pedestrians and static constraints."}, {"title": "A. Experiment setup", "content": "We train the sim2real policies in the simulators of a hallway (Fig. 5(b)) and a lounge (Fig. 5(c)) for 2 \u00d7 108 timesteps with a decaying learning rate 5 \u00d7 10-5. To learn robust policies, we inject noises into the agent positions and robot control. Then, as shown in Fig. 5 (d) and (e), we test the policies in the two corresponding everyday environments in a university building without any additional training. In the hallway environment where the free space is extremely narrow, we test the robot's ability to handle constraints with low density crowds. In the lounge environment, we test the robot's ability to avoid dense crowds and obstacles with more diverse shapes. The distance between the starting and the goal position of the robot ranges from 6m to 11m. The pedestrians were told to react naturally to the robot based on their own preferences. In some testing episodes, other pedestrians who were unaware of our experiment also engaged with the robot.\nThe configuration of hardware testing is shown in Fig. 2(b). We use a TurtleBot 2i with an RPLIDAR-A3 laser scanner. We first use the ROS gmapping package to create a map of the environment. Then, we process the map to combine small obstacles and eliminate noises. To reduce sim2real gap of the LiDAR point clouds, we use artificial point clouds obtained"}, {"title": "B. Results", "content": "In the highly constrained yet less crowded Hallway environment, ROS navigation stack often needs to spin in place to replan, as shown by the higher navigation time in Table III. Some of the spinning recovery attempts fail and result in timeouts. In ROS navigation stack, both global and local planners treat humans as obstacles. As a result, similar to the baselines in Sec. V-C1, the robot has difficulties distinguishing"}, {"title": "VII. DISCUSSIONS, LIMITATIONS, AND FUTURE WORK", "content": "Deep RL is a promising tool to solve robotic problems that are beyond the capabilities of traditional rule-based methods without large-scale real-world datasets. However, preventing the performance degradation of a deep RL policy when inevitable distribution shifts happen, especially in real-world, is challenging. In this section, we reflect on the key components of our framework, discuss the limitations of our approach, and propose directions for future research."}, {"title": "A. Sim2real through Real2sim", "content": "To overcome sim2real gaps, the design of simulation pipelines needs to be guided by the constraints of hardware and environments in the real-world. On one hand, we determine the input representation of HEIGHT based on what could be obtained from sensors and off-the-shelf perception modules and how accurate they are. We find that intermediate features, such as detected human positions and processed point clouds, reduces sim2real gaps. On the other hand, we also ensure the consistency of the simulation and real-world, such as the robot action space, whenever possible. These design choices allow our policy to generalize to different simulation environments and deploy to challenging real-world scenarios.\nHowever, although we have minimized the sim2real gaps through real2sim, certain gaps still exist. In real-world experiments, the difficulty of the task is reduced and the agility of the robot policy is only partially transferred from simulation. To further align the simulation and the real-world, we plan to explore the following directions for future work: (1) developing a more natural pedestrian model to replace the ORCA humans in the simulator, (2) revising our pipeline to enable self-supervised RL fine-tuning in the real-world [60], [61], (3) using a small amount of real-world data to automatically optimize the parameters of our simulator to match the real-world environment [62], [63]."}, {"title": "B. Scene representation", "content": "A good scene representation is tailored to the needs of its downstream task. Besides the above sim2real considerations, our scene representation is split due the different nature of humans and obstacles for robot collision avoidance. The size of humans are small and their shapes are simple. Therefore, to avoid humans, the robot only needs to treat them as circles with a heading direction. In contrast, obstacles have larger and more complicated surfaces. The part of the obstacle contours that faces the robot is more important for collision avoidance. Therefore, point clouds are the most intuitive way to represent such useful information about obstacles. Our experiments in Sec. V show that this split scene representation reduces robot collision avoidance with both dynamic and static obstacles.\nA side effect of our scene abstraction is the loss of detailed information such as gaits of humans and 3D shapes of obstacles. However, we argue that this is a tradeoff to minimize sim2real gaps with limited simulation tools and computation resources. Another side effect is the cascading errors between the perception modules and robot policy, such as inaccurate"}, {"title": "C. Structured neural network", "content": "Model-based approaches require low-fidelity data yet heavily rely on assumptions. In contrast, end-to-end learning approaches need few assumptions yet require high-fidelity data. Our structured learning method combines the best of both worlds: It requires low-fidelity data yet relies on minimal assumptions. By injecting structures into the neural network, we decompose a complex problem into smaller and relatively independent components. Note that our decomposition does not break the gradient flow, which keeps HEIGHT end-to-end trainable. We propose a principled way for network architecture design, increasing the transparency of these black-boxes. Our experiments demonstrate that the structured network outperforms both model-based methods and RL-based methods without structures, which empirically proves the effectiveness of structure learning for interactive tasks with multiple heterogeneous participants."}, {"title": "D. Training method", "content": "Deep RL enables the robot to explore the environment and learn meaningful behaviors through trial and error. Without heuristics or demonstrations, the robot has to collect reward signals to improve its policy. Consequently, simulation development with a randomization scheme and a good reward design are indispensable to the performance of our method. Nevertheless, our method is subjective to the inherent limitations of RL, such as low training data efficiency and difficulties in long-horizon tasks. In future work, combining the RL policy with other traditional planners or imitation learning has the potential to alleviate these problems [36]."}, {"title": "VIII. CONCLUSION", "content": "In this article, we proposed HEIGHT, a novel structured graph network architecture for autonomous robot navigation in dynamic and constrained environments. Our approach takes advantage of the graphical nature and decomposability of the constrained crowd navigation problem, introducing the following two key novelties. First, we split and process the human and obstacle representations separately. This allows the robot to effectively reason about the different geometrics and dynamics of humans and obstacles, improving its ability to navigate complex environments. Second, we propose a heterogeneous st-graph to capture various types of interactions among the robot, humans, and obstacles. The decomposition of the scenario as a heterogeneous st-graph guides the design of the HEIGHT network with attention mechanisms. Attention enables the robot to reason about the relative importance of each pairwise interaction, leading to adaptive and agile robot decision-making during navigation.\nOur simulation experiments show that the HEIGHT model outperforms traditional model-based methods and other learning-based methods in terms of collision avoidance and"}]}