{"title": "TTS-Transducer: End-to-End Speech Synthesis with Neural Transducer", "authors": ["Vladimir Bataev", "Subhankar Ghosh", "Vitaly Lavrukhin", "Jason Li"], "abstract": "This work introduces TTS-Transducer a novel architecture for text-to-speech, leveraging the strengths of audio codec models and neural transducers. Transducers, renowned for their superior quality and robustness in speech recognition, are employed to learn monotonic alignments and allow for avoiding using explicit duration predictors. Neural audio codecs efficiently compress audio into discrete codes, revealing the possibility of applying text modeling approaches to speech generation. How-ever, the complexity of predicting multiple tokens per frame from several codebooks, as necessitated by audio codec models with residual quantizers, poses a significant challenge. The proposed system first uses a transducer architecture to learn monotonic alignments between tokenized text and speech codec tokens for the first codebook. Next, a non-autoregressive Transformer predicts the remaining codes using the alignment extracted from transducer loss. The proposed system is trained end-to-end. We show that TTS-Transducer is a competitive and robust alternative to contemporary TTS systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Neural text-to-speech (TTS) is a sequence-to-sequence task where the model learns to generate a speech sequence condi-tioned on the input text sequence. TTS synthesis is monotonic, preserving the order between input text and output speech. Since speech is produced at the frame level, one phoneme can correspond to multiple frames, and output length varies with the speaker's style, making text-to-speech alignment challenging. Non-autoregressive (NAR) TTS models [1], [2] use explicit phoneme or text token duration predictor. In autoregressive (AR) encoder-decoder [3] TTS models [4], alignment is learned implicitly, while they produce more natural speech, they also suffer from hallucination, skipping or repeating words [5].\nThe transducer architecture (RNNT) [6], widely used in automatic speech recognition (ASR), enforces a monotonic alignment constraint. Thus, it could provide a robust solution for this problem. However, direct application of transducers to TTS is challenging since transducers are designed to predict discrete units, but speech is typically represented in continuous form, e.g., with a mel-spectrogram. Recent development in neural audio codecs [7]-[9] allows to transform the audio prediction task into a discrete units prediction task. This ap-proach significantly simplifies TTS pipeline, and many recent"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Recurrent neural network transducer [6] is a universal ar-chitecture for sequence-to-sequence tasks requiring the mono-tonic alignment between input and output. Transducer consists of three neural modules: (1) encoder, which processes the input sequence (originally audio features) and generates high-level representations, (2) prediction network (predictor) an autoregressive network (originally RNN) that uses previously predicted tokens to generate the next output, starting from the special (SOS) (start-of-sequence) token, and (3) joint net-work, which combines outputs of encoder step i and prediction network step j to produce the distribution of the probabilities $(P_{i,j})$ over the vocabulary augmented with special (blank) $((b))$ symbol. RNNT optimizes the total probability of all pos-sible alignments between input and output sequences, where the target sequence includes inserted (blank) symbols, acting as delimiters between frames.\nThere are several decoding strategies for transducers. Greedy decoding uses nested loop design, where the outer loop iterates over frames of the encoder output, and the inner loop retrieves labels one by one with the maximum probability by combining the encoder output for the current frame and prediction network output using joint network until the (blank) symbol is found. In our experiments, we apply nucleus sampling [17] using predicted probabilities instead of greedy selection."}, {"title": "B. Neural Transducers for Text-to-Speech Synthesis", "content": "Segment-to-Segment Neural Transduction (SSNT) [18] in-troduced transducer-based monotonic restrictions in TTS but factorized joint probability into alignment transition proba-bility and emission probability for acoustic features. Speech-T model [19] also decouples aligning and mel-spectrogram prediction. The authors use a modified RNNT loss to learn the alignment, but the model requires the external forced aligner to construct diagonal constraints in the probability lattice to help the transducer learn the alignment.\nRecently introduced Transduce-and-Speak [20] model has two components trained separately. The first component is a neural transducer generating \"semantic\u201d tokens (one token per frame). Such tokens are obtained from the clustered output of pretrained Wav2Vec 2.0 [21]. The second component is a modified non-autoregressive (NAR) VITS [22] model synthesizing speech from the semantic tokens. A similar two-stage approach with intermediate \"semantic\" tokens was used in [23] with the second component predicting audio codes. More recently, the VALL-T [24] combined transducer with a decoder-only Transformer. The authors combined relative position embeddings with absolute positional embeddings, where a relative position of 0 specifies the current phoneme under synthesis. The\u3008blank) symbol in the output indicates a position shift. During training, the model uses all possible shifts of the positional embeddings, and the output is combined to form the transducer lattice. Transducer loss is used to optimize the model. VALL-T needs a large amount of memory and multiple forward passes during training due to all the relative position shifts."}, {"title": "C. TTS using Audio Codes", "content": "Recently, a new TTS approach has emerged, where TTS is considered a language modeling task that translates text input to discrete audio tokens. Similar to large language models (LLMs), there are two main types of models: (1) decoder-only (AudioLM, VALL-E, UniAudio, Bark, SpeechX [10],"}, {"title": "III. TTS-TRANSDUCER MODEL", "content": "The architecture of the TTS-Transducer, inspired by the VALL-E [10] system, consists of two components trained sep-arately. The first component of VALL-E is an autoregressive Transformer model that predicts codes of the first codebook given the input text and a prompt. The second one is a non-autoregressive Transformer that predicts codes from all other residual codebooks based on the prediction of the first component, input text, and a prompt.\nTTS-Transducer schema is shown in Fig. 1. We use a neural transducer to predict the codes of the first codebook given the text units by learning alignment between text and audio. The second component, residual codebook head (RCH), is a non-autoregressive Transformer. It predicts the remaining audio codes iteratively, given all previously predicted codes along with the aligned encoder output. The encoder of the transducer and the residual codebook head are conditioned on speaker embeddings. After predicting all the codes, the decoder of the audio codec model is used to produce audio.\nPrediction of the first codebook $c_{0,i}$ is learned by a neural transducer. Encoder is a non-autoregressive Trans-former [15] model, which transforms tokenized text $t_{i}$ to the sequence of vectors $e_{i}$. We train models with Byte-Pair Encoding (BPE) [29] tokenization, and also experi-ment with phonemes from International Phonetic Alphabet (IPA). We also add speaker embedding conditioning to the encoder using conditional LayerNorm [30]. The prediction network is an autoregressive Transformer-Decoder, which transforms a sequence of audio codes $C_{0,i}$ with prepended (SOS) symbol to the sequence of vectors $p_{j}$. For each combination of vectors $e_{i}, p_{j}$ the joint network is applied: $j_{i,j} = Softmax(Linear(ReLU(e_{i} + p_{j})))$. The output of the joint network is the probability distribution for the tokens of the first codebook augmented with the (blank) symbol.\nTo predict all residual codebooks from 1 to n, we use a non-autoregressive Transformer-encoder, which predicts i-th codebook codes using previously predicted [0...i - 1] codebooks and the aligned encoder output. The input is the sum of embeddings for previously predicted codes $C_{0:i-1,j}$, concatenated with the corresponding encoder vector $e_{k}$. We use speaker conditioning similar to the first component of our system.\nTo represent speakers, TTS systems typically use fixed embeddings from a speaker verification model [31], but these embeddings do not generalize well beyond seen speakers. So we use Global Style Tokens (GST) [14] to capture the style of the speaker as used in [32]. In our work, we convert target speaker's reference speech to mel-spectrogram and feed it to the speaker representation module. The speaker representation module consists of a convolutional recurrent neural network-based encoder that learns the style tokens. A multi-head attention layer combines the learned style tokens to give the speaker embedding.\nWe train our system end-to-end. On each training step, we first perform a forward pass for the first component of our system. We use WFST-based implementation of the RNNT loss [33] in the k2 framework [34]. This allows us to extract the alignment between audio codes and encoder output (corre-sponding to text units) from the calculated RNNT lattice. We distribute the encoder frames according to the extracted align-ment. We also randomly select i from all residual codebooks [1: n] to predict i+1 codebook codes with the second part of our system. We optimize this component by applying cross-entropy loss (ACE). The total loss is the weighted sum of the losses from the first and second components of our network: $A_{total} = (1 \u2212 a) * A_{RNNT} + \u03b1 * A_{CE}$. We use $a = 0.4$ in our experiments.\nIn decoding, we first evaluate the RNNT component, getting the predictions for the first codebook. Due to the nature of the decoding algorithm as described in Section II-A, getting"}, {"title": "IV. EXPERIMENTS", "content": "We use LibriTTS-R [37] dataset, an improved 24 kHz version of the LibriTTS [38]. The LibriTTS corpus contains a diverse set of speakers reading English audiobooks. We train the model on train-clean-100, train-clean-360, and train-other-500 subsets of the LibriTTS-R. We set aside 1.15 hours of data from the train-clean-100 to test the model on seen speakers. We filter the data by a maximum duration of 15 seconds, which results in 464 hours in the training dataset. To evaluate our system on unseen speakers, we randomly choose 0.35 hours of data for unseen speakers with 39 unseen speakers from the dev-clean subset. Additionally, we evaluate our model in out-of-domain conditions on a subset of 200 utterances (0.2 hours) from VCTK [39] to test the generalization abilities to multiple acoustic conditions. We use reference audios of length between 3 to 5 seconds."}, {"title": "B. Model Details", "content": "We use pretrained neural audio codecs: EnCodec [7] (8 codebooks, 6 kbps), NeMo-Codec [9] with RVQ (8 codebooks, 6.9 kbps), and Descript Audio Codec [8] model (9 codebooks, 8 kbps). Our TTS-Transducer model has 12 Transformer layers in the encoder, 6 layers in the prediction network, and 12 layers in residual codebook head. The encoder and residual codebook head have 2 attention heads and a feed-forward dimension of 1536, with model dimension of 640 and 512, respectively. The prediction network Transformer-decoder uses 4 attention heads, with a model dimension of 512 and a feed-forward dimension of 2048. This results in 199M parameters for EnCodec and NeMo-Codec, and 200M parameters for the DAC model due to a larger embedding table since DAC codec uses 9 codebooks. For the speaker embedding model, we use 1024 640-dimensional GST [14] learnable embeddings.\nAll TTS-Transducer models are trained in NeMo [16] toolkit with a global batch of 2048 for 200 epochs using 32 NVIDIA A100 GPUs. We use AdamW [40] optimizer with cosine annealing scheduler [41] with 2000 warmup steps and a maximum learning rate of 1e-3.\nDuring inference, we use nucleus sampling [17] (p = 0.95) for predicting codes from the first codebook. The second component predicts remaining codebooks greedily."}, {"title": "V. CONCLUSION", "content": "We presented a novel TTS-Transducer system that predicts audio neural codec tokens directly from phonemes based on a neural transducer. It combines the strength of the neural transducer to learn monotonic alignment between text and audio and the effectiveness of neural audio codecs. The neural transducer predicts the first codebook. The remaining residual codes for the same frame are predicted by a separate block based on the learned alignment. Both components are opti-mized jointly. We demonstrated that our model can produce high-quality, reliable speech with popular audio codecs. Our experiments showed that the model achieves results compara-ble to SOTA TTS models in naturalness, and surpasses them in intelligibility on challenging texts without requiring large-scale pretraining."}]}