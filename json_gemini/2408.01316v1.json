{"title": "Synergistic pathways of modulation enable robust task packing within neural dynamics", "authors": ["Giacomo Vedovati", "ShiNung Ching"], "abstract": "Understanding how brain networks learn and manage multiple tasks simultaneously\nis of interest in both neuroscience and artificial intelligence. In this regard, a recent\nresearch thread in theoretical neuroscience has focused on how recurrent neural net-\nwork models and their internal dynamics enact multi-task learning. To manage different\ntasks requires a mechanism to convey information about task identity or context into the\nmodel, which from a biological perspective may involve mechanisms of neuromodula-\ntion. In this study, we use recurrent network models to probe the distinctions between\ntwo forms of contextual modulation of neural dynamics, at the level of neuronal ex-\ncitability and at the level of synaptic strength. We characterize these mechanisms in\nterms of their functional outcomes, focusing on their robustness to context ambiguity\nand, relatedly, their efficiency with respect to packing multiple tasks into finite size net-\nworks. We also demonstrate distinction between these mechanisms at the level of the\nneuronal dynamics they induce. Together, these characterizations indicate complemen-\ntarity and synergy in how these mechanisms act, potentially over multiple time-scales,\ntoward enhancing robustness of multi-task learning.", "sections": [{"title": "Introduction", "content": "The question of how neuronal networks embed multiple and/or context-dependent tasks\nis of importance to both neuroscience and artificial intelligence (AI). From a biological\nand ethological perspective, multitasking is innate part of encountering and mastering\ndynamic environments with multiple or quickly fluctuating requirements [1]. In the AI\ndomain, it remains an open question as to how to most efficiently construct learning\nsystems that can embed many tasks at the same time, and hence biological insights may\nprove valuable in advancing those computational architectures.\nSpurred by these questions, there has been a surge of recent theoretical neuroscience\nresearch aimed at understanding neural mechanisms of multi-task learning. Most ger-\nmane to this paper are approaches to this problem that use recurrent neural networks\n(RNNs) as a normative model [2, 3]. Recurrent neural networks are popular in this\ndomain because they can handle history-dependence (e.g., memory tasks, including se-\nquence prediction, association, etc.), via their internal dynamics [3\u20136], thus enabling\nthe engagement of cognitive-style tasks. Within this paradigm, it becomes possible to\nexamine two related issues: (i) distinctions in the (learned) dynamics of these networks\nas a function of task setup and requirements, and (ii) whether embedding multiple tasks\nmakes the learning of new tasks easier.\nThe formulation of these models requires a mechanism to modulate their processing\nbased on task identity or context. In other words, a task identity signal is projected or\n'gated' onto the model parameters according to a specific hypothesis. Contextual gat-\ning in the brain can leverage neuromodulatory signals that adjust the activity of neural\ncircuits to suit different tasks or environmental contexts [7]. In this study, we exam-\nine two biologically interpretable mechanisms of neuromodulation: (a) excitability of\nneurons [3-5] and (b) scaling of synapses [8, 9]. Excitability modulation occurs at the\nneuron-level and affects how easily and quickly neurons can change their activity in\nresponse to inputs. Scaling of synapses, on the other hand, modulates the strength of\nsynaptic connections. These mechanisms are expected to arise under different biologi-\ncal conditions. Excitability changes, such as those resulting from alterations in intrinsic\nmembrane conductances, allow for rapid and reversible adjustments in network states,\nand may enable quick adaptation to new tasks [10, 11]. Synaptic scaling, often asso-\nciated with long-term potentiation [12] or depression mechanisms [13], can provide a\nstable and persistent, albeit slower, form of adaptation. Both can be connected with\ncontext-dependent processing, though it remains unresolved as to whether or how they\naddress different requirements in multitask learning.\nImportant questions thus arise: Are all modulation mechanisms created equal? What"}, {"title": "Materials and methods", "content": "The backbone of this research is a context-gated recurrent neural network architecture,\nwherein contextual signals impinge at the neuronal and synaptic levels."}, {"title": "Contextual and excitability modulated recurrent neural net-\nworks", "content": "We present first the general model we consider for handling contextual information in\nRNNS:\n$\\tau\\dot{x} = -x + [J + (1 + \\alpha Hdiag(u_c)H^T)]tanhx + Bu_s + \\beta Du_c$ (1)\n$y = Wtanhx.$ (2)\nHere, x represents the state vector of the network, capturing the internal state of individ-\nual neurons. The term tanh x captures the vector of neural firing rates, implementing\na typical saturation-type non-linear activation function. The exogenous input u deliv-\ners task-relevant stimuli via the feedforward gain B. The matrix J embeds \u2018baseline'\nsynaptic connectivity and will interact with two mechanisms of contextual modulation,\ndescribed thus:\nExcitability modulation. The first mechanism is an additive input with gain (i.e.,\n$\\beta Du_c$), which we refer to as excitability modulation. This input affects the baseline\nlevel of excitability of neurons, consistent with well-characterized phenomena such as\nshifts in baseline activation in the visual system occurring at the moment of task tran-\nsition [16, 17]. Of note, this form of modulation has been a common choice to embed\ncontextual information into RNNs, with $u_c$ commonly taking the form of a binary 'hot'\nvector [4, 5, 18, 19]. From a computational perspective, this form of modulation is\nhighly effective as it is compatible with batch training paradigms."}, {"title": "Task setup", "content": "We tested the networks on a spatial working memory paradigm, a relatively standard\nclass of task for RNN studies. Our primary interest lies not in the absolute performance\nof the networks for these tasks, but rather in understanding the differences in how con-"}, {"title": "Contextual ambiguity and input disturbtance", "content": "A key issue in our study pertains to the robustness of the above modulation mecha-\nisms to ambiguity in contextual information. Presumably, the clearer the indication\nof belonging to a specific contextual class, the easier will be the ensuing multi-task\ntraining. To model contextual ambiguity, we represented each context vector $u_c$ as a\nmultivariate Gaussian process with distribution $N(n, \\sigma I)$. Here, n is a standard basis\nvector, i.e., a one-hot vector with a single nonzero entry.\nWe also formulated noise at the level of task stimuli (we use the descriptor 'distur-\nbance' here for specificity). Thus, task stimuli are formulated as:\n$u_s = \\bar{u}_s + \\eta,$ (3)\nwhere $\\eta \\sim N(0, \\epsilon I)$. Here $\\bar{u}_s$ denotes the deterministic task stimulus, i.e. the undis-\nturbed points on the circle."}, {"title": "Computational setup and implementation", "content": "Table 1 summarizes the computational setup of all models, in terms of the which pa-\nrameters are trainable. All networks were coded in Pytorch. Learning/optimization was\nperformed via back-propagation and optimized using Adam [21]. Several additional\ntechnical specifications are provided in the Appendix."}, {"title": "Results", "content": ""}, {"title": "Dual mechanisms provide enhanced robustness to contextual\nambiguity and input disturbance", "content": "The first question we investigated was how well the networks tolerated ambiguity in the\ncontextual modulation. In this regard, our goal is less to determine which mechanism\nis best, but rather to clarify the relative degradation in performance with increasing\nambiguity, and any gains in robustness that arise through combined modulation. We\nproceeded to train all the networks on the same level of contextual ambiguity (set to\n$\\sigma$\n= 1.0), then progressively increased this level during testing. We observe, as ex-\npected, that both modulation mechanisms can train on the baseline level of ambiguity.\nMoreover, and also as expected, both incur increasing response variability with increas-\ning ambiguity. What is most interesting is the profile of the SERNN (i.e., with dual\nmechanisms). This network is much more robust to increasing ambiguity, maintaining\nnear baseline levels of response variability. There is an impression of the whole being\ngreater than the sum of parts in this characterization.\nWe also examined robustness to disturbance in the task stimulus. This characteri-\nzation is distinct from the above, insofar as it examines robustness within context, but\nostensibly enabled by the across context modulation. Here, we again observe synergy in\nthe two modulation mechanisms, such that response variability is stabilized for higher\nlevels of noise. However, this effect is not as significant as in the case of context am-\nbiguity, since the SRNN on its own achieves greater robustness than the ERNN in this"}, {"title": "Dual mechanisms enable efficient task packing", "content": "Based on the robustness of context ambiguity, we surmised that there should be ap-\npreciable differences in the efficiency by which these mechanisms can pack multiple\ntasks into finite-size networks. We expected that synaptic modulation would enjoy\nadvantages in task packability, owing to potential for more significant alterations to\nnetwork-wide and sub-ensemble dynamics [22]. As aforementioned, a special case of\nsuch modulation would be to simply 'disconnect' segments of the network. While it\nis clearly possible to pack many tasks within the ERNN mechanism, relying on a sin-\ngle connectivity matrix J shared across multiple tasks may limit performance in this\ncontext.\nTo test these premises, we varied the number of neurons and the number of tasks the"}, {"title": "Dual mechanisms provide complementarity in transferability", "content": "A significant question with contextually modulated networks pertains to how well they\nenable transfer learning, wherein the dynamics embedded in the network through prior\ntasks can be re-deployed in the service of a new, related task. We adopted this frame-\nwork in the current study. Specifically, we considered two phases of learning: the pre-\ntransfer phase, where the network learns all available parameters on a subset of tasks,\nand the transfer phase, where we assume that only the contextual modulation is updated\n(see Appendix for technical details).\nThe networks were trained on 4 different tasks during the pre-transfer phase. We\nincluded context ambiguity during this phase of learning. During the transfer phase, the\ncontextual ambiguity was reduced to zero, motivated by the premise of a human or an\nanimal finding themselves in a novel but unambiguous single-task learning scenario.\nAs depicted in Figure 4, we compared the performance achieved by the different net-\nworks, pre- and post-transfer learning, by evaluating the response distribution of the net-\nworks during the recall phase of each context. We also analyzed the cost during transfer\ntraining exhibited by each network. We compared this with the cost-minimization pro-\ncess of a vanilla neural network trained on the same single task. In so doing, we can\ncharacterize transferability in terms of speed, quality, and durability of prior solutions\n(i.e., catastrophic forgetting).\nWhile both excitability and synaptic modulation enable transfer, there are notable\ndistinctions in their performance. Both display an initial cost advantage over the (fully\ntrainable) vanilla RNN. The ERNN advantage is larger, and it displays faster, convex\nconvergence (Fig. 4-A) though performance ultimately is limited relative to the vRNN.\nThe SRNN transfers more slowly (Fig. 4-B), though it does achieve marginally better\nperformance for a fixed number of training epochs. Nonetheless, the loss does not\nexceed the reference vRNN and is evidenced through variable performance (Fig. 4-E).\nWhen the two forms of modulation are both available, we see clear synergy in trans-\nferability. Transfer occurs quickly, such that the overall loss outpaces the vRNN for the\nentirety of learning epochs (Fig. 4-C). This results in a much less variable learned per-\nformance (Fig. 4-F). Notably, there are no major gains in the durability of previously\nlearned solutions (Fig. 4-D:F), which survive transfer equally well in all cases (see also\nDiscussion)."}, {"title": "Discussion", "content": "Our results indicate differences in how modulatory mechanisms may enable context\nto be embedded within neural dynamics. The two forms of modulation considered\nat synaptic and excitability levels of action \u2013 do seem to provide distinct outcomes in\nrobustness and learnability within multi-task scenarios. There is also a strong synergy\nbetween these mechanisms, where their combination results in highly robust, efficient\nand durable solutions, indicating the potential importance of multiple modulatory path-\nways in neural circuits.\nThe immediate question that arises is whether there are identifiable differences in"}, {"title": "Vector field geometry associated with learned solutions", "content": "To begin, we performed typical linear dimensionality reduction on network activity, fo-\ncussing on the delay period. For this category of task, it is well validated in prior compu-\ntational studies and indeed experiments that memory representations form a topological\nring in state space [23]. Our analysis is in agreement with these extant results but sug-\ngests important mechanistic distinctions. In the case of excitability modulation, the ring\nrepresentations are laterally offset from each other (Fig. 5-A), suggestive of a nullcline\nmoving through state space, in agreement with reports in [4]. On the other hand, synap-\ntic modulation (Fig. 5-B) produces rotated ring representations, a qualitatively different\narrangement of learned dynamics. This observation indicates that synaptic modulation\nalters the orientation of the learned vector field depending on the context. As would"}, {"title": "Reduced model analysis", "content": "There are several basic properties of these network models\nthat provide some insight as to the observed phenomenology. We note first that clearly,\nthe equilibria x* for excitability modulation satisfy\n$0 = -x^* + J tanh(x^*) + Du_c.$ (4)\nIn R2, this means that all nullclines are of the form\n$0 = -x_1^* + J_1 tanh x_1^* + J_2 tanh x_2^* + du_{c1}.$ (5)\nThe modulation generated by Du can have several effects on the umodulated (i.e.,\n$u_{c1} = 0$) nullcline. Of particular note, this modulation will in general laterally offset\nthe nullcline from the origin (Fig. 6A,B).\nby:\nOn the other hand, in the synaptic modulation case, the equilibria are determined\n$0 = -x^* + J \\odot (1 + Hdiag(u_c)H^T) tanh(x^*),$ (6)\nso that in R2 nullclines are of the form\n$0 = -x_1^* + J_1 u_{c1} tanh x_1^* + J_2 u_{c2} tanh x_2.$ (7)\nThere are some immediate distinctions in the way modulation shapes the nullclines in\nthis setting. Of note, the origin $x^* = 0$ is always an equilibrium of these dynamics,\nand hence all nullclines will pass through the origin. Moreover, all nullclines have the"}, {"title": "Biological considerations", "content": "Biologically, it is of interest to consider the linkage of these mechanisms to specific neu-\nromodulatory pathways in the brain. Furthermore, a potential conceptual conundrum is\nthat a mechanism such as synaptic modulation/scaling could require a separate learning\nmechanism unto itself, and it is unclear how this would occur. On this latter point, we\nnote that the mere existence of modulation may be sufficient for conventional learning\nmechanisms to leverage (i.e., without learning the modulation itself). The former issue\nis not one we can easily answer here due to the abstractness of our models, except to\nobserve that the modulation of synaptic strength and neuronal excitability are rather\ngeneric features of key neuromodulators [24]. One intriguing theory that partially mo-\ntivated our results here is the idea that non-neuronal cells such as astrocytes may be a\nkey intermediary in conveying both synaptic and excitability modulation onto neuronal\nnetworks [25, 26], via neuromodulators that such as norepinephrine that can directly\nalter synaptic strength [27]."}, {"title": "Conclusion", "content": "Our results provide a theoretical account of how different forms of modulation may\nbe able to pack multiple tasks or contexts within network dynamics. Several of our\nobservations are compatible, at least schematically, with neurobiological theory. For\ninstance, our results indicate that excitability modulation may allow for rapid changes\nin network dynamics, facilitating quick adaptation to new tasks. Synaptic modulation,\nwhile potentially slower, can provide a more robust and durable form of adaptation\n[28, 29]. A key point of our paper is that these mechanisms likely act in concert, po-\ntentially over two time-scales, to convey maximum capacity. Looking ahead, a direct\nway to substantiate these theoretical accounts would be to characterize the geometry of\nmemory representations within multitask settings."}, {"title": "Appendix", "content": "During training, we implemented the following loss function:\n$L = \\frac{1}{T}\\sum_{t=0}^{T} m_t||y_t - \\bar{y}_t||^2$\nwhere, yt and \u04ef\u0165 are the network output and the target output respectively. t is the index\nof time and T indicates the duration of the whole trial. A weighted mask m\u2081 was also\nimplemented, to modulate the loss with respect to certain time intervals. During the\nfixate and target presentation phase the m\u2081 = 1, to promote stable fixation on the center\nof the target. During the delay period, the mask was set to mt 0, while during the\nresponse Mt = 5.\nThe optimization algorithm, Adam, was initialized with a learning rate of 10-3, and\na decay rate for the first and second-moment estimates of 0.9 and 0.999, respectively.\nWe used mini-batches of 50 trials.\nWe interleaved the different contexts randomly during the training phase, which was\nterminated when the loss function flattened, usually around 5 \u00d7 103 epochs.\nInitialization The connectivity matrix J was initialized as low rank,\nsparse matrix, where the non-zero elements were drawn from a normal distribution\nN(0, 0.01). The decoding matrix W was initialized as a random matrix, with elements\ndrawn from a normal distribution N(0, \u20a9), where N is the number of neurons in the\nnetwork. All the other parameters were randomly drawn from a normal distribution\nN(0, 1). The time constant 7 was set to 0.1.\nAs previously mentioned, the networks were pre-trained on a sub-\nset of tasks, where all the parameters were learned. During the transfer phase, only the\ncontextual matrices D and H are optimized. Because u is a hot vector, this amounts to\nlearning one row of these matrices."}]}