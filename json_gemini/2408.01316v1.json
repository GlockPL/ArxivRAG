{"title": "Synergistic pathways of modulation enable robust task packing within neural dynamics", "authors": ["Giacomo Vedovati", "ShiNung Ching"], "abstract": "Understanding how brain networks learn and manage multiple tasks simultaneously\nis of interest in both neuroscience and artificial intelligence. In this regard, a recent\nresearch thread in theoretical neuroscience has focused on how recurrent neural net-\nwork models and their internal dynamics enact multi-task learning. To manage different\ntasks requires a mechanism to convey information about task identity or context into the\nmodel, which from a biological perspective may involve mechanisms of neuromodula-\ntion. In this study, we use recurrent network models to probe the distinctions between", "sections": [{"title": "1 Introduction", "content": "The question of how neuronal networks embed multiple and/or context-dependent tasks\nis of importance to both neuroscience and artificial intelligence (AI). From a biological\nand ethological perspective, multitasking is innate part of encountering and mastering\ndynamic environments with multiple or quickly fluctuating requirements [1]. In the AI\ndomain, it remains an open question as to how to most efficiently construct learning\nsystems that can embed many tasks at the same time, and hence biological insights may\nprove valuable in advancing those computational architectures.\nSpurred by these questions, there has been a surge of recent theoretical neuroscience\nresearch aimed at understanding neural mechanisms of multi-task learning. Most ger-\nmane to this paper are approaches to this problem that use recurrent neural networks\n(RNNs) as a normative model [2, 3]. Recurrent neural networks are popular in this\ndomain because they can handle history-dependence (e.g., memory tasks, including se-"}, {"title": "2 Materials and methods", "content": "The backbone of this research is a context-gated recurrent neural network architecture,\nwherein contextual signals impinge at the neuronal and synaptic levels."}, {"title": "2.1 Contextual and excitability modulated recurrent neural networks", "content": "We present first the general model we consider for handling contextual information in\nRNNS:\n\\begin{equation}\n\\tau \\dot{x} = -x + [J + \\alpha \\text{diag}(u_c)H^T] \\tanh x + Bu_s + \\beta Du_c\n\\end{equation}\n\\begin{equation}\ny = W \\tanh x.\n\\end{equation}\nHere, x represents the state vector of the network, capturing the internal state of individ-\nual neurons. The term $\\tanh x$ captures the vector of neural firing rates, implementing\na typical saturation-type non-linear activation function. The exogenous input $u$ deliv-\ners task-relevant stimuli via the feedforward gain B. The matrix J embeds \u2018baseline'\nsynaptic connectivity and will interact with two mechanisms of contextual modulation,\ndescribed thus:\nExcitability modulation. The first mechanism is an additive input with gain (i.e.,\n$\\beta Du_c$), which we refer to as excitability modulation. This input affects the baseline\nlevel of excitability of neurons, consistent with well-characterized phenomena such as\nshifts in baseline activation in the visual system occurring at the moment of task tran-\nsition [16, 17]. Of note, this form of modulation has been a common choice to embed\ncontextual information into RNNs, with $u_c$ commonly taking the form of a binary 'hot'\nvector [4, 5, 18, 19]. From a computational perspective, this form of modulation is\nhighly effective as it is compatible with batch training paradigms."}, {"title": "Synaptic modulation", "content": "We formulate a second form of modulation occurring at the\nlevel of synaptic weights. Specifically, we consider the low-rank modulation matrix\n$H \\text{diag}(u_c)H^T$, formed from the diagonalized bias vector $u_c$ (associated with a specific\ntask context) and the matrix H. Then, through the Hadamard product between J and 1+\n$\\alpha H \\text{diag}(u_c)H^T$ (1 here denotes a matrix of ones), we implement the context-dependent\nscaling of synaptic connections. The above formulation generates, in essence, a low-\nrank multiplicative modification of the baseline synaptic connectivity, wherein H will\ndetermine the spatial reach of $u_c$. This construction preserves the same number of mod-\nulatory parameters as in the excitability case. While abstract, the synaptic modulation\nis schematically compatible with the actions of neuromodulators that act directly at the\nlevel of synaptic strength [20].\nIn the absence of contextual modulation, uc (the vector representing the context or\ntask-specific bias) will be zero, and the entire model reduces to a vanilla RNN. The\npurely synaptic modulated recurrent neural network (SRNN) and the excitability mod-\nulated recurrent neural network (ERNN) can be viewed as special cases of this general\nmodel, corresponding to $\\beta= 0$ and $\\alpha = 0$, respectively. We will refer to the full model\n(both terms nonzero) as an SERNN, through which we will examine the differences and\nsynergies of these mechanisms."}, {"title": "2.2 Task setup", "content": "We tested the networks on a spatial working memory paradigm, a relatively standard\nclass of task for RNN studies. Our primary interest lies not in the absolute performance\nof the networks for these tasks, but rather in understanding the differences in how con-"}, {"title": "2.3 Contextual ambiguity and input disturbtance", "content": "A key issue in our study pertains to the robustness of the above modulation mecha-\nnisms to ambiguity in contextual information. Presumably, the clearer the indication\nof belonging to a specific contextual class, the easier will be the ensuing multi-task\ntraining. To model contextual ambiguity, we represented each context vector $u_c$ as a\nmultivariate Gaussian process with distribution $\\mathcal{N}(n, \\sigma I)$. Here, n is a standard basis\nvector, i.e., a one-hot vector with a single nonzero entry.\nWe also formulated noise at the level of task stimuli (we use the descriptor 'distur-\nbance' here for specificity). Thus, task stimuli are formulated as:\n\\begin{equation}\nu_s = \\bar{\\nu_s} + \\eta,\n\\end{equation}\nwhere $\\eta \\sim \\mathcal{N}(0, \\epsilon I)$. Here $\\bar{\\nu_s}$ denotes the deterministic task stimulus, i.e. the undis-\nturbed points on the circle."}, {"title": "2.4 Computational setup and implementation", "content": "Table 1 summarizes the computational setup of all models, in terms of the which pa-\nrameters are trainable. All networks were coded in Pytorch. Learning/optimization was\nperformed via back-propagation and optimized using Adam [21]. Several additional\ntechnical specifications are provided in the Appendix."}, {"title": "3 Results", "content": ""}, {"title": "3.1 Dual mechanisms provide enhanced robustness to contextual ambiguity and input disturbance", "content": "The first question we investigated was how well the networks tolerated ambiguity in the\ncontextual modulation. In this regard, our goal is less to determine which mechanism\nis best, but rather to clarify the relative degradation in performance with increasing\nambiguity, and any gains in robustness that arise through combined modulation. We\nproceeded to train all the networks on the same level of contextual ambiguity (set to\n$\\sigma = 1.0$), then progressively increased this level during testing. We observe, as ex-\npected, that both modulation mechanisms can train on the baseline level of ambiguity.\nMoreover, and also as expected, both incur increasing response variability with increas-\ning ambiguity. What is most interesting is the profile of the SERNN (i.e., with dual\nmechanisms). This network is much more robust to increasing ambiguity, maintaining"}, {"title": "3.2 Dual mechanisms enable efficient task packing", "content": "Based on the robustness of context ambiguity, we surmised that there should be ap-\npreciable differences in the efficiency by which these mechanisms can pack multiple\ntasks into finite-size networks. We expected that synaptic modulation would enjoy\nadvantages in task packability, owing to potential for more significant alterations to\nnetwork-wide and sub-ensemble dynamics [22]. As aforementioned, a special case of\nsuch modulation would be to simply 'disconnect' segments of the network. While it\nis clearly possible to pack many tasks within the ERNN mechanism, relying on a sin-\ngle connectivity matrix J shared across multiple tasks may limit performance in this\ncontext.\nTo test these premises, we varied the number of neurons and the number of tasks the"}, {"title": "3.3 Dual mechanisms provide complementarity in transferability", "content": "A significant question with contextually modulated networks pertains to how well they\nenable transfer learning, wherein the dynamics embedded in the network through prior\ntasks can be re-deployed in the service of a new, related task. We adopted this frame-\nwork in the current study. Specifically, we considered two phases of learning: the pre-\ntransfer phase, where the network learns all available parameters on a subset of tasks,\nand the transfer phase, where we assume that only the contextual modulation is updated\n(see Appendix for technical details).\nThe networks were trained on 4 different tasks during the pre-transfer phase. We\nincluded context ambiguity during this phase of learning. During the transfer phase, the\ncontextual ambiguity was reduced to zero, motivated by the premise of a human or an\nanimal finding themselves in a novel but unambiguous single-task learning scenario.\nAs depicted in Figure 4, we compared the performance achieved by the different net-\nworks, pre- and post-transfer learning, by evaluating the response distribution of the net-\nworks during the recall phase of each context. We also analyzed the cost during transfer\ntraining exhibited by each network. We compared this with the cost-minimization pro-"}, {"title": "4 Discussion", "content": "Our results indicate differences in how modulatory mechanisms may enable context\nto be embedded within neural dynamics. The two forms of modulation considered\nat synaptic and excitability levels of action \u2013 do seem to provide distinct outcomes in\nrobustness and learnability within multi-task scenarios. There is also a strong synergy\nbetween these mechanisms, where their combination results in highly robust, efficient\nand durable solutions, indicating the potential importance of multiple modulatory path-\nways in neural circuits.\nThe immediate question that arises is whether there are identifiable differences in"}, {"title": "4.1 Vector field geometry associated with learned solutions", "content": "To begin, we performed typical linear dimensionality reduction on network activity, fo-\ncussing on the delay period. For this category of task, it is well validated in prior compu-\ntational studies and indeed experiments that memory representations form a topological\nring in state space [23]. Our analysis is in agreement with these extant results but sug-\ngests important mechanistic distinctions. In the case of excitability modulation, the ring\nrepresentations are laterally offset from each other (Fig. 5-A), suggestive of a nullcline\nmoving through state space, in agreement with reports in [4]. On the other hand, synap-\ntic modulation (Fig. 5-B) produces rotated ring representations, a qualitatively different\narrangement of learned dynamics. This observation indicates that synaptic modulation\nalters the orientation of the learned vector field depending on the context. As would"}, {"title": "4.2 Biological considerations", "content": "Biologically, it is of interest to consider the linkage of these mechanisms to specific neu-\nromodulatory pathways in the brain. Furthermore, a potential conceptual conundrum is\nthat a mechanism such as synaptic modulation/scaling could require a separate learning\nmechanism unto itself, and it is unclear how this would occur. On this latter point, we\nnote that the mere existence of modulation may be sufficient for conventional learning\nmechanisms to leverage (i.e., without learning the modulation itself). The former issue\nis not one we can easily answer here due to the abstractness of our models, except to\nobserve that the modulation of synaptic strength and neuronal excitability are rather\ngeneric features of key neuromodulators [24]. One intriguing theory that partially mo-\ntivated our results here is the idea that non-neuronal cells such as astrocytes may be a\nkey intermediary in conveying both synaptic and excitability modulation onto neuronal\nnetworks [25, 26], via neuromodulators that such as norepinephrine that can directly\nalter synaptic strength [27]."}, {"title": "Conclusion", "content": "Our results provide a theoretical account of how different forms of modulation may\nbe able to pack multiple tasks or contexts within network dynamics. Several of our\nobservations are compatible, at least schematically, with neurobiological theory. For\ninstance, our results indicate that excitability modulation may allow for rapid changes\nin network dynamics, facilitating quick adaptation to new tasks. Synaptic modulation,\nwhile potentially slower, can provide a more robust and durable form of adaptation"}, {"title": "Training Procedure", "content": "During training, we implemented the following loss function:\n\\begin{equation}\nL = \\frac{1}{T} \\sum_{t=0}^{T}m_t||y_t - \\hat{y}_t||^2\n\\end{equation}\nwhere, yt and \u0177t are the network output and the target output respectively. t is the index\nof time and T indicates the duration of the whole trial. A weighted mask m\u2081 was also\nimplemented, to modulate the loss with respect to certain time intervals. During the\nfixate and target presentation phase the mt = 1, to promote stable fixation on the center\nof the target. During the delay period, the mask was set to mt = 0, while during the\nresponse Mt = 5.\nThe optimization algorithm, Adam, was initialized with a learning rate of 10-3, and\na decay rate for the first and second-moment estimates of 0.9 and 0.999, respectively.\nWe used mini-batches of 50 trials."}, {"title": "Hyperparameter initialization", "content": "The connectivity matrix J was initialized as low rank,\nsparse matrix, where the non-zero elements were drawn from a normal distribution\n$\\mathcal{N}(0, 0.01)$. The decoding matrix W was initialized as a random matrix, with elements\ndrawn from a normal distribution $\\mathcal{N}(0, \\frac{1}{N})$, where N is the number of neurons in the\nnetwork. All the other parameters were randomly drawn from a normal distribution\n$\\mathcal{N}(0, 1)$. The time constant $\\tau$ was set to 0.1."}, {"title": "Transfer Learning", "content": "As previously mentioned, the networks were pre-trained on a sub-\nset of tasks, where all the parameters were learned. During the transfer phase, only the\ncontextual matrices D and H are optimized. Because u is a hot vector, this amounts to\nlearning one row of these matrices."}]}