{"title": "Logically Constrained Robotics Transformers for Enhanced Perception-Action Planning", "authors": ["Parv Kapoor", "Sai Vemprala", "Ashish Kapoor"], "abstract": "With the advent of large foundation model based planning, there is a dire need to ensure their output aligns with the stakeholder's intent. When these models are deployed in the real world, the need for alignment is magnified due to the potential cost to life and infrastructure due to unexpected faliures. Temporal Logic specifications have long provided a way to constrain system behaviors and are a natural fit for these use cases. In this work, we propose a novel approach to factor in signal temporal logic specifications while using autoregressive transformer models for trajectory planning. We also provide a trajectory dataset for pretraining and evaluating foundation models. Our proposed technique acheives 74.3 % higher specification satisfaction over the baselines.", "sections": [{"title": "I. INTRODUCTION", "content": "Most autonomous robots deployed in the real world need to operate in a safe and reliable manner. This problem is exacerbated for safety critical applications such as autonomous vehicles where failure to respect safety constraints can lead to catastrophic consequences. A key issue with ensuring safety constraints can be attributed to imprecise specification of desired behaviors. Existing methods of encoding behaviors through objectives, such as cost functions or reward functions, can be exploited by underlying algorithms in a suboptimal manner. [1] This allows them to achieve high scores without fully meeting the intended requirements. This problem is only worsened by the recent advent of using Natural Language (NL) to communicate instructions to robots. Due to NL's inherent ambiguity, it is unclear how to use it for encoding precise behavior or constraints in safety critical applications. [2, 3]\nAn alternative is to use specifications written in Temporal logics (TL) that have rich and precise semantics. Additionally, TL specifications provide a tractable way to check if the system achieves the desired behavior. In line with this, there has been significant recent interest in specifying temporal and logical constraints on system behaviors through a continuous- time real-valued TL called signal temporal logic (STL) [4]. STL specifications can be defined over state action trajectories of robots. Additionally, STL is equipped with a score of satisfaction/violation called Robustness that can be used as feedback for generating desired behavior. There exist a large body of work that uses STL specifications to generate behavior through Reinforcement Learning [5, 6], Mixed Integer Convex"}, {"title": "II. PRELIMINARIES AND PROBLEM STATEMENT", "content": "A. Signal Temporal Logic\nSTL is a logic specification language used to define properties of continuous time real valued signals [4]. A signal s is a functions: T \u2192 Rn that maps a time domain TC R\u2265o to a real valued vector. Then, an STL formula is defined as:\n\u03c6 := \u03bc | \u00ac\u03c6|\u03c6\u2227 \u03c8 | \u03c6\u2228 \u03c8 | \u03c6 U[a,b] \u03c8\nwhere \u03bc is a predicate on the signal s at time t in the form of \u03bc = \u03bc(s(t)) > 0 and [a,b] is the time interval. The until operator U defines that \u03c6 must be true until \u03c8 becomes true within a time interval [a, b].\nGiven a signal st representing a signal starting at time t, the Boolean semantics of satisfaction of st |= \u03c6 are defined inductively as follows:\n$t = \u03bc\u21d4 \u03bc(s(t)) > 0\n$t |= \u00ac\u03c6 \u21d4 \u00ac($t = \u03c6)\n$t |= $1 \u2227 $2 \u21d4 ($t = \u03c61) \u2227 ($t |= \u03c62)\n$t |= F[a,b] (\u03c6) \u21d4 \u2203t' \u2208 [t + a, t + b] s.t. st\u2032 = \u03c6\n$t |= G[a,b] (\u03c6) \u21d4 \u2200t' \u2208 [t + a,t + b] s.t. str = \u03c6\nB. Problem Statement\nConsider a dynamical system with states xt \u2208 Rn and actions at \u2208 Rm at discrete time steps t. The objective is to predict a sequence of state-action pairs {(xt, at)}=0 over a finite horizon T such that the predicted trajectory satisfies a given STL specification \u03c6.\nOur system dynamics are defined by Xt+1 = f(xt,at) where f: Rn \u00d7 Rm \u2192 Rn maps a state action pair (xt \u2208 Rn, at \u2208 Rm) at a given timestep t to the next state Xt+1 \u2208 Rn.\nFor generating trajectories, we utilize a causal transformer model to autoregressively predict the next state-action pair based on the past sequence:\n{(Xt+k, at+k)}=1 = Transformer({(xi,ai)}+6-1,0) (1)\nwhere \u03b8 represents the parameters of the transformer model. Then, An STL specification \u03c6 is defined over the state and action variables encoding the safety and liveness requirement. A trajectory {(xt, at)}=0 satisfies \u03c6 if:\n{(xt, at)}=0 |= \u03c6 (2)\nThe goal is to find the parameters \u03b8 of the causal transformer model such that the predicted trajectory {(xt,at)}=0 maximizes the likelihood of satisfying the STL specification \u03c6.\nMathematically, this can be expressed as:\n\u03b8* = arg max Pr((xt, at)=0 |= \u03c6|\u03b8) s.t. Xt+1 = f(xt, at) \u2200t \u2208 0,1,..., T \u2013 1 XO Xinit, \u03b1\u03bf Ainit (3)"}, {"title": "III. METHODOLOGY", "content": "A. Approach\nThe overall approach is described in Figure 1. We call our model Perception Action Signal TemporaL Transformer (PASTEL). PASTEL takes as input the state, action and specification embeddings and autoregressively predicts the future sequence of states and actions conditioned on the embeddings such that the state trajectory signal satisfies the user specified STL specification. The length of the state-action trajectory is fixed as the horizon of the original specification keeping in line with the original STL semantics."}, {"title": "C. Implementation Details", "content": "1) Training objective: Our training objective is carefully designed to account for the expectation of high precision state action trajectory predictions. We use a combination of Mean Absolute Error ( MAE) and Mean Squared Error (MSE) loss defined over the state as well as action predictions. Formally,\nLstate = MSE(Stobs, \u015ctobs) + MAE(Stobs, Stobs) (4)\nLaction = MSE(at,\u00e2t) + MAE(at, \u00e2t) (5)\nAdditionally, we also add a specification relevance loss that penalizes deviation from the expected impact of the specification on the predicted trajectory. This loss is defined over the specification embeddings that capture the semantic information about the specification and the cross attention outputs that integrates information from all the input embeddings. Specifically, we use a cosine similarity measure to quantify how well the instruction is reflected in the final outputs. Formally,\nLspec = 1 - Cos_sim(Temb, C) (6)\nwhere Temb represents the mean text embeddings and C represents the mean cross-attention output embeddings:\nTemb = 1/n * \u03a3(i=1 to n) ti and C = 1/n * \u03a3(i=1 to n) Ci\nHere, n is the number of embeddings (e.g., batch size), ti are the text embeddings, and ci are the cross-attention output embeddings.\nFinally, our total loss is:\nLtotal = Lstate + Lact + Lspec (7)"}, {"title": "IV. EVALUATION", "content": "A. Experimental Setup\nWe evaluate PASTEL\u00b9 against the baseline PACT implementation as provided in [11] . Both the models were trained on the same dataset comprising a total of 20000 state action trajectories across all the specifications. All experiments were run on a workstation with a NVIDIA GeForce RTX 4070 GPU.\nThe two main research questions we investigate in this paper are:\n1) RQ1: Does PASTEL achieve higher specification satisfaction compared to Vanilla PACT?\n2) RQ2: Does modifying the text specification at test time have an impact on the final trajectory?\nWhile RQ1 can be measure quantitatively through sampling, RQ2 is evaluated qualitatively by modifying the specification string manually. RQ1 investigates improvement in satisfaction while RQ2 investigates if the model is memorizing the state action trajectories without any dependence on the text specification."}, {"title": "B. Results and Discussion", "content": "Table 2 summarizes the benchmarking results for our experimental setup. The % satisfaction is computed out of 100 trajectories generated by the model from 100 randomly sampled states. We observe that for \u03c62 and \u03c63, PASTEL improves satisfaction over baseline by approximately 82.9 and 54.3 percent respectively. For \u03c61, we observe an 85.7 percent performance improvement over baseline. Additionally, all the generated trajectories are smooth and respect the actuation constraints measured separately via examining the generated actions.\nAs can be inferred, PASTEL has medium to high satisfaction for \u03c62 and \u03c63 while the PACT has low to medium satisfaction. We hypothesize this is due to the relative complexity of \u03c62 and \u03c63 involving two nested tasks (reaching and staying in a region and reaching two regions) respectively which is complex for autoregressive models like PACT to infer without additional contextual information. Due to our conditional prediction and cross attention mechanism, the model is able to leverage the specification embeddings to satisfy the task succesfully. However, both PASTEL and PACT's satisfaction percentage drops for \u03c61 which is the most complex specification involving a disjunction operator. This implies that there are multiple possible ways to satisfy the requirement and the underlying dataset reflects the same. Nonetheless, PASTEL still outperforms PACT for \u03c61 relatively. Hence, we definitively answer RQ1. Additionally, the generated trajectories satisfy the actuation constraints which was never explicitly encoded into the model design. This implicit effect is due to the loss function defined over the ground truth actions that respect the actuation constraints due to the optimisation problem setup. This lends additional support to the power of autoregressive trajectory generators in generating safe constraint satisfying trajectories.\nFor RQ2, we modified specification text and visualised attention matrix to observe dependence on text. We underline our two key qualitative insights:\n1) The satisfaction drops when the test specification is different than the training one in terms of the atomic propositions encoding the regions.\n2) The attention matrix visualisation highlights the dependence of state and action tokens on the specification tokens further substantiating our original hypothesis."}, {"title": "V. RELATED WORK", "content": "Constrained trajectory generation for robots has been a focal point in robotics research, particularly in applications requiring high precision and adherence to strict operational constraints. Traditional methods have relied on optimization- based approaches, such as Mixed-Integer Linear Programming (MILP) and Sequential Quadratic Programming (SQP), which are effective but often computationally intensive and challenging to scale for complex tasks. Sampling-based planners like Rapidly-exploring Random Trees (RRT) [17, 18] and Probabilistic Roadmaps (PRM) [19] have been used to address these issues from primarily a collision-avoidance perspective, introducing probabilistic guarantees for constraint satisfaction. Constrained variants of RRT-like methods have also been proposed in the literature [20]. However, these methods can struggle with high-dimensional spaces and intricate constraints, limiting their applicability in real-world scenarios. Another class of techniques involves the usage of control barrier functions: learning safe policies using explicit control barrier functions [21, 22], or constructing CBFs jointly with the policy using neural networks [23, 24].\nData-driven trajectory planning techniques mainly adapt two paradigms: reinforcement learning and imitation learning. While reinforcement learning has been widely used to learn safe policies, the success of these methods often depends on manual reward shaping, which is a laborious and non-trivial effort, as well as the existence of a capable simulator or sandbox that allows for a large number of training episodes. Imitation learning has the potential to reduce sample complexity using a more focused set of demonstrations, for example, in the case of safe trajectory planning, learning only from a set of safe trajectories. Imitation learning can take the form of simple Behavior Cloning [25], which may not generalize to the out-of- distribution scenarios induced by on-policy deployment, and often requires additional training [26]. Other methods such as inverse reinforcement learning [27] can mitigate this challenge but they do not explicitly account for constraints either and rely on implicit extraction of a reward signal apparent in the data.\nIn recent work, reinforcement learning and imitation learning have been posed as sequence modeling problems to leverage the immense efficacy of Transformer models at learning from data. One seminal work was the Decision Transformer [28], which uses a causally masked Transformer conditioned on desired rewards to output optimal trajectories. Similarly, works such as Gato, PACT use Transformer models directly on demonstrations to learn trajectories. Extensions such as ConBAT [29] attempt safe trajectory planning by training on a combination of safe and unsafe demonstrations."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this work, we propose a novel approach to factor in STL specifications in transformer based trajectory planning using specification conditioned prediction and cross attention based mechanism for specification relevance.\nWhile, our initial results are promising our approach currently suffers with specifications with long horizons and complex nested tasks. We plan to remedy this using the STL decomposition techniques proposed in [8] and updating the specification token at each timestep to only focus on the relevant part of the specification. Additionally, we also plan to factor in external feedback in the form of STL robustness similar to SMART[30]. Finally, we aim to evaluate the feasibility of our technique via field testing on navigation robots."}]}