{"title": "Regression and Classification with Single-Qubit Quantum Neural Networks", "authors": ["Leandro C. Souza", "Bruno C. Guingo", "Gilson Giraldi", "Renato Portugal"], "abstract": "Since classical machine learning has become a powerful tool for developing data-driven algo-\nrithms, quantum machine learning is expected to similarly impact the development of quantum\nalgorithms. The literature reflects a mutually beneficial relationship between machine learn-\ning and quantum computing, where progress in one field frequently drives improvements in the\nother. Motivated by the fertile connection between machine learning and quantum computing\nenabled by parameterized quantum circuits, we use a resource-efficient and scalable Single-Qubit\nQuantum Neural Network (SQQNN) for both regression and classification tasks. The SQQNN\nleverages parameterized single-qubit unitary operators and quantum measurements to achieve\nefficient learning. To train the model, we use gradient descent for regression tasks. For classifi-\ncation, we introduce a novel training method inspired by the Taylor series, which can efficiently\nfind a global minimum in a single step. This approach significantly accelerates training compared\nto iterative methods. Evaluated across various applications, the SQQNN exhibits virtually error-\nfree and strong performance in regression and classification tasks, including the MNIST dataset.\nThese results demonstrate the versatility, scalability, and suitability of the SQQNN for deploy-\nment on near-term quantum devices.", "sections": [{"title": "1. Introduction", "content": "Classical machine learning, particularly through artificial neural networks, has transformed\nmany fields by enabling pattern recognition, prediction, and complex decision-making tasks. The\nneuron, a computational unit inspired by biological neurons [1], is at the core of these networks.\nOne of the earliest models, the perceptron [2], functions as a binary classifier, producing an output\nbased on a weighted sum of inputs [3]. This foundational unit, though limited in complexity,\nenables the construction of multilayered architectures that drive advancements in support vector\nmachines and deep learning [4].\nQuantum computing has opened new frontiers in machine learning, leading to the develop-\nment of quantum machine learning models that exploit quantum mechanics to achieve compu-\ntational advantages over classical approaches [5, 6, 7, 8]. However, some authors dispute this\nassertion [9]. Among these models, Quantum Neural Networks (QNNs) have attracted particu-\nlar interest because they use quantum parallelism and entanglement to process high-dimensional\ndata more efficiently than classical neural networks. Some of these architectures employ quantum"}, {"title": "2. Single-Qubit Neuron Model", "content": "A quantum neuron is a quantum-inspired adaptation of the classical neuron, designed for\nsupervised learning tasks. Given a dataset\n$\\mathcal{D} = \\{(x_1,y_1), (x_2,y_2),\\ldots, (x_n, y_n)\\} \\subset \\mathbb{R}^p \\times [-1, 1]$,\nwhere each input $x_i \\in \\mathbb{R}^p$ is a feature vector and each corresponding output $y_i \\in [-1,1]$ is\nthe goal of the quantum neuron. Quantum computational principles are applied to predict the\noutput y for an untested input $x \\in \\mathbb{R}^p$. The weights of the quantum neuron are encoded within\nthe angles of 1-qubit rotation gates\n$R_x(\\theta) = \\begin{bmatrix}\\cos\\frac{\\theta}{2} & -i\\sin\\frac{\\theta}{2} \\\\ -i\\sin\\frac{\\theta}{2} & \\cos\\frac{\\theta}{2} \\end{bmatrix}$,\n$R_y(\\theta) = \\begin{bmatrix}\\cos\\frac{\\theta}{2} & -i\\sin\\frac{\\theta}{2} \\\\ \\sin\\frac{\\theta}{2} & \\cos\\frac{\\theta}{2} \\end{bmatrix}$,\n$R_z(\\theta) = \\begin{bmatrix}e^{-i\\theta/2} & 0 \\\\ 0 & e^{i\\theta/2} \\end{bmatrix}$.\nThose gates are combined to build a general unitary operator that depends on three real parame-\nters. Since the most general 1-qubit unitary operator can be expressed as $e^{i\\alpha} R_z(\\beta) R_y(\\gamma)R_z(\\gamma)$ [42],\nwe employ the most general single-qubit quantum neuron $N$, modulo a global phase, as\n$N = R_z(\\gamma) R_y(\\beta) R_z(\\alpha)$.\nFig. 1 depicts the most general version the single-qubit quantum neuron together with its\ninput and a measurement. The output depends on the angles $\\alpha(x)$, $\\beta(x)$, and $\\gamma(x)$, which are\nfunctions of the data input with a weight vector and are used in the rotation gates $R_y$ and $R_z$.\nThe input to the circuit is the quantum state $|\\psi\\rangle = \\cos(\\theta/2)|0\\rangle + e^{i\\varphi} \\sin(\\theta/2)|1\\rangle$, where $\\theta$ and $\\varphi$\nare two extra parameters."}, {"title": "3. Single-Qubit Quantum Neural Network", "content": "We describe a Single-Qubit Quantum Neural Network (SQQNN) model that uses the intercon-\nnection of single-qubit neurons in a streamlined manner. Consider a multi-neuron QNN composed\nof K neurons $(N_1, N_2, \\cdots, N_K)$, each embodying a single-qubit architecture as depicted in Fig. 3.\nIn this configuration, each neuron $N_k$ is defined by the sequence $N_k = R_z(\\gamma_k) R_y(\\beta_k) R_z(\\alpha_k)$, for\n$k = 1,\\ldots, K$, where $\\alpha_k$, $\\beta_k$, and $\\gamma_k$ are trainable parameters specific to each neuron. The input\nstate $|\\psi\\rangle$ and the observable $O$ each have a single trainable parameter that is not associated with\nindividual neurons. The SQQNN is applicable to both regression and classification tasks.\nThe SQQNN structure is characterized by the product of the matrices representing each of the\nK sequentially connected neurons. Since each neuron facilitates an arbitrary 1-qubit rotation,\nthe cumulative effect of combining K neurons is equivalent to a single, effective quantum neuron,\nas follows:\n$N_{\\text{effective}} = N_K \\cdots N_2 N_1$.\nBy employing multiple neurons, the model gains flexibility, increasing the model's expressive\npower and enhancing its ability to capture complex patterns in the data.\nIn this design, the angles $\\alpha_k$, $\\beta_k$, and $\\gamma_k$ are determined by arbitrary functions applied to\nthe input data. By composing these functions across multiple neurons, we significantly expand\nthe SQQNN's degrees of freedom and enhance its capacity to adapt to the data during training.\nThus, for any function defining the angles in a single-neuron SQQNN, there exists an inverse\nprocess and a corresponding set of functions for defining the angles in a multi-neuron SQQNN.\nThis method provides a structured approach to enhance the flexibility of single-neuron SQQNNs,\nallowing for greater adaptability and improved training outcomes.\nThe circuit of the network is parameterized by the set of angles $\\{\\alpha_k, \\beta_k, \\gamma_k, \\theta_k,w_k\\}$. Let $\\Omega_k$\ndenote one of the model's parameters associated with $N_k$. To implement the learning algorithm,"}, {"title": "4. Training Methods", "content": "In machine learning, training refers to the process through which a model improves its perfor-\nmance on a specific task by identifying patterns within a dataset. This improvement is typically\nachieved by optimizing a set of weights, $(c_0, ...,c_p) \\in \\mathbb{R}^{p+1}$, with the goal of minimizing the\nloss function. This function quantifies the discrepancy between the model's predictions and the\nactual outcomes. Throughout training, the model iteratively adjusts its parameters based on\nfeedback from the loss function, gradually reducing errors and enhancing predictive accuracy.\nThis process is generally carried out through gradient descent, which incrementally refines the\nmodel's parameters by following the gradient of the loss function, or via other optimization al-\ngorithms described later. The ultimate goal of learning is to enable the model to generalize well,\nnot only on the training data but also on new, unseen data, by capturing essential patterns and\nrelationships within the dataset.\nFor regression tasks, we use the Mean Square Error (MSE) loss function\n$\\text{MSE}(\\Omega, \\mathcal{D}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2,$\nand for classification tasks, we use the hinge loss function\n$\\text{Hinge}(\\Omega, \\mathcal{D}) = \\frac{1}{n} \\sum_{i=1}^n \\text{max}\\{0, 1 - y_i \\hat{y}_i\\},$\nwhere $y_i$ is the model's estimated output when the input is the dataset element $x_i$. The estimated\noutput $\\hat{y}_i$ is calculated using Eq. (4). With $\\phi$ and $\\varphi$ set to 0, the model's output is defined as\nfollows\n$\\hat{y} = \\cos\\beta \\cos\\theta \\cos\\omega - \\cos\\alpha \\sin\\beta \\sin\\theta \\cos\\omega - \\sin\\beta \\cos\\gamma \\cos\\theta \\sin\\omega + \\sin\\alpha \\sin\\gamma \\sin\\theta \\sin\\omega - \\cos\\alpha \\cos\\beta \\cos\\gamma \\sin\\theta \\sin\\omega$.\nThe angles $\\alpha, \\beta, \\gamma, \\theta$, and $\\omega$ are estimated based on the weights and input data. The loss\nfunction can be minimized with respect to the weights using standard methods such as the\ngradient descent method."}, {"title": "4.1. Gradient Descent", "content": "The primary concept of gradient descent is to iteratively adjust the model parameters in the\nopposite direction of the gradient of the loss function. Initially, the model parameters are set\nrandomly. In each iteration, the gradient of the loss function with respect to the parameters is\ncomputed. The parameters are then updated by a factor controlled by the learning rate ($\\eta$), as\nshown in the following equation:\n$\\Omega_{t+1} = \\Omega_t - \\eta \\nabla_{\\Omega} \\text{Loss} (\\Omega_t, \\mathcal{D}) .$\nThis process continues until the loss function reaches a sufficiently low value or until the gradient\napproaches zero, indicating that a minimum has been reached. The advantages of using gradient\ndescent include simplicity of the model, efficiency, and adaptability to high-dimensional data.\nHowever, it can become trapped in local minima. It is important to choose a learning rate that\nensures effective convergence."}, {"title": "4.2. Polynomial-based Linear Least Squares", "content": "In this subsection, we introduce a training algorithm specifically designed for binary classi-\nfication tasks using reduced SQQNNs, based on successive powers of the entries of the dataset\nelements. The function mapping data to the angle $\\beta$ is defined as\n$\\beta(x) = \\text{arccos tanh} \\left[ c_0 + \\sum_{k=1}^K \\sum_{j=1}^p c_{kj} x_{ij}^k \\right].$\nSubstituting this function into Eq. (6) yields\n$\\text{arctanh}(\\hat{y_i}) = c_0 + \\sum_{k=1}^K \\sum_{j=1}^p c_{kj} x_{ij}^k$\nwhere $\\hat{y_i} = -1 + \\epsilon$ if $y_i = -1$, $\\hat{y_i} = 1 - \\epsilon$ if $y_i = 1$, and $\\hat{y_i} = y_i$ otherwise. The small parameter\n$\\epsilon > 0$ is introduced because the $\\text{tanh}$ function never reaches -1 or 1, necessitating the adjustment\nof $y_i$ by this parameter. Typically, in applications, we set $\\epsilon = 10^{-16}$.\nThe coefficients $c_0$ and $c_{kj}$ are determined using the least squares linear regression formula\n$S = (X^T X)^+ X^T Y,$\nwhere $S$ is the vector of coefficients\n$S = [c_0 \\quad c_{11} \\quad \\ldots \\quad c_{1p} \\quad \\ldots \\quad c_{1K}]^T,$\n$Y$ is the label vector modified by the $\\text{arctanh}$ function\n$Y = [\\text{arctanh}(y_1) \\quad \\text{arctanh}(y_2) \\quad \\ldots \\quad \\text{arctanh}(y_n)],$\nand $X$ is a matrix constructed from the dataset inputs"}, {"title": "5. Regression Evaluation", "content": "In this section, we evaluate the effectiveness of SQQNN in performing regression tasks. Re-\ngression is a critical application for quantum neural networks as it requires accurately modeling\ncontinuous relationships and predicting real-valued outputs, which were initially rescaled to the\nrange [-1, 1] and subsequently recalibrated to their original range. We apply SQQNN to three\ntypes of regression tasks: logic gates, modeling continuous functions, and analyzing real-world\ndatasets. By assessing its performance in these cases, we aim to demonstrate SQQNN's preci-\nsion and adaptability in diverse regression scenarios, using the gradient descent method. For the\nreal-world datasets, we employed 10-fold cross-validation to ensure robust evaluation."}, {"title": "5.1. Logic Gate Evaluation", "content": "The results of training the SQQNN to evaluate logical gates are shown in Table 1. Here,\nwe present the training error and the number of gradient descent epochs required for each gate,\nincluding AND, OR, XOR, NAND, NOR, and XNOR. The SQQNN was trained with a single\nneuron (K = 1)."}, {"title": "5.2. Sinc function", "content": "The sinc function, defined as\n$\\text{sinc}(x) = \\frac{\\sin(x)}{x},$\nis a standard benchmark for regression tasks due to its oscillatory, non-linear nature and decaying\namplitude, which present challenges for models to accurately capture without overfitting. For\nthis experiment, we generated data based on the sinc function and trained SQQNNs with a single\nneuron to predict previously unseen data points. The dataset consisted of 800 data points for\ntraining, 100 for validation, and 100 for testing."}, {"title": "5.3. Combined Cycle Power Plant Dataset", "content": "The Combined Cycle Power Plant (CCPP) dataset, sourced from the UCI Machine Learn-\ning Repository\u00b9, provides 9,568 samples of data collected from a gas turbine under full load\nconditions. It features four input variables ambient temperature, ambient pressure, relative\nhumidity, and exhaust vacuum and one target variable, the net electrical energy output of the\nplant, measured in megawatts. The dataset is widely used for regression tasks due to its smooth,\nnon-linear relationships and strong correlations between features and the target variable. With\nits clean and comprehensive structure, the CCPP dataset serves as a valuable benchmark for\nmodeling and predicting energy production under varying environmental and operational condi-\ntions."}, {"title": "5.4. Communities and Crime Dataset", "content": "The Communities and Crime dataset, sourced from the UCI Machine Learning Repository\u00b2,\nprovides comprehensive data on crime rates across 1,994 U.S. communities, alongside key socio-\neconomic and demographic factors. The dataset focuses on various types of crimes, including\nviolent offenses such as murder, rape, robbery, and assault, as well as property crimes like\nburglary, theft, and motor vehicle theft. Its target variable is the violent crime rate per capita,\nmaking it particularly suitable for regression analysis. In addition to crime statistics, the dataset\nincludes socio-economic variables such as population size, income levels, education attainment,\npoverty rates, and the proportion of minority populations. With its combination of detailed\ncrime metrics and contextual information, this dataset enables an in-depth exploration of the"}, {"title": "6. Classification Evaluation", "content": "In this section, we present the results of the SQQNN architecture, evaluated on both synthetic\nand real datasets to demonstrate its effectiveness and versatility in various binary classification\ntasks. The training method employed is the polynomial-based linear least squares. For the\nanalysis of real datasets, we employed accuracy, precision, sensitivity, specificity, and F1 score\nto evaluate the classification performance. The output classes used 1 and -1 to represent each\ncategory. Additionally, 10-fold cross-validation was applied to ensure robust evaluation of the\nreal datasets."}, {"title": "6.1. \u03a4\u03c9\u03bf Moons Dataset", "content": "The Two Moons dataset is a synthetic dataset commonly used for testing classification models.\nIt consists of two interlocking half-moon shapes that are challenging to separate linearly, making\nit an ideal benchmark for assessing the capabilities of nonlinear classifiers. For this experiment,\nthe dataset was generated with a noise parameter set to 0.07, the training set comprised 1,000\ndata points and test set with 100 samples.\nThe results in Table 4 highlight the model's high performance. The classification achieved\naccuracy, precision, sensitivity, and F1-score of 1 across the board, indicating that the model\nfully captured the underlying structure of the data. In this application, two neurons are sufficient\nto achieve high accuracy. Fig. 6 illustrates this progression, showing a significant improvement in\nclassification accuracy with additional neurons. The SQQNN quickly adapts to fit all synthetic\ndata points, demonstrating its ability to effectively classify complex, nonlinearly separable data."}, {"title": "6.2. Wisconsin Breast Cancer Dataset", "content": "The Wisconsin Breast Cancer Dataset (WBCD), sourced from the UCI Machine Learning\nRepository\u00b3, is a widely recognized benchmark in medical diagnostics and machine learning re-\nsearch. This dataset comprises 569 samples, each characterized by 30 numeric features derived\nfrom fine needle aspiration (FNA) tests of breast tissue. The features capture various morpholog-\nical properties of cell nuclei, including radius, texture, perimeter, area, and smoothness, among\nothers. These descriptors enable the differentiation between benign and malignant tumors, fa-\ncilitating accurate diagnostic predictions. The WBCD serves as a critical testbed for evaluating\nthe performance of classification algorithms, offering a well-balanced distribution of classes and\na real-world application of machine learning in healthcare.\nThe results presented in Table 5 were produced using the SQQNN architecture, with the\nmodel parameter K increased from 1 to 10. This adjustment was made to enhance the network's\ncapacity and improve classification accuracy. By progressively increasing K, we evaluated the re-\nlationship between model complexity and performance, leveraging the WBCD dataset to validate\nthe effectiveness of SQQNN in reducing classification errors.\nDespite fluctuations in all performance measures, Table 5 highlights a clear trend toward"}, {"title": "6.3. MNIST Dataset", "content": "The MNIST dataset, a standard benchmark in the field of machine learning, consists of 70,000\ngrayscale images of handwritten digits, partitioned into 60,000 training samples and 10,000 test\nsamples. Each image, represented as a 28\u00d728 pixel grid, encapsulates the complexity of real-\nworld digit recognition tasks while maintaining computational simplicity. Despite its relative\nsimplicity, MNIST remains a critical testbed for evaluating novel neural network architectures.\nIn this work, we use MNIST to show the performance and generalization potential of the SQQNN\narchitecture, establishing a baseline for its capabilities in image recognition tasks. To achieve\nthis, we applied the Discrete Cosine Transform (DCT) [47] to the images as a pre-processing\nstep.\nFig. 7 depicts sample digits from 0 to 9 from the MNIST dataset that were incorrectly\npredicted, highlighting the variability in handwriting styles and pixel intensity. The results\npresented in Table 6 were produced using K = 1 and the reduced version of SQQNN, specifically\ndesigned to optimize computational efficiency while maintaining high classification accuracy. The\nreduced model was trained using the polynomial-based linear least squares method, leveraging\nthe MNIST training set for evaluation. Each training iteration took approximately 8 seconds\non a DELL Vostro 3480 notebook with an Intel Core i5-8265U 1.60GHz x8 CPU, 8GB DDR4\nRAM, and Ubuntu 22.04.4 LTS.\nThe results presented in Table 6 demonstrate the remarkable performance of the SQQNN\narchitecture across various MNIST class pairings, achieving near-perfect accuracy, precision,\nsensitivity, specificity, and F1 scores in most cases. For simpler pairings, such as digits 0 and 1,\nthe network attained an accuracy of 0.999 \u00b10.001, reflecting its ability to distinguish visually"}, {"title": "7. Final remarks", "content": "We use a Single-Qubit Quantum Neural Network (SQQNN), a quantum machine learning\nframework that leverages the simplicity and efficiency of single-qubit neurons. By employing\nparameterized single-qubit operations, the SQQNN offers a streamlined yet powerful architecture\nfor both regression and binary classification tasks. This design significantly reduces hardware\nrequirements, simplifies implementation, and is compatible with near-term quantum devices,\nmaking it a practical alternative to more complex multi-qubit approaches.\nWe validated the SQQNN's capabilities through diverse applications. The model achieved\nnear-zero training errors in logic gate evaluations, demonstrating its ability to handle linear and\nnonlinear relationships efficiently. For other regression tasks, the SQQNN successfully modeled\ncontinuous functions and real-world datasets, showcasing its adaptability and generalization. In\nclassification tasks, increasing the number of neurons significantly reduced error, and the model\nperformed strongly on both synthetic and real-world datasets, including MNIST. These results\nhighlight the SQQNN's scalability and flexibility.\nTo optimize the network, we employed two learning methods: gradient descent with iterative\nrefinement for regression tasks and an improved global optimization approach using a matrix-\nbased dataset representation called polynomial-based linear least squares (LLS) for classification\ntasks. The polynomial-based LLS method offers rapid results, and its use with a single neuron\n(K = 1) eliminates the need for deep learning architectures. This demonstrates an efficient and\nstreamlined training approach, making it a valuable contribution to both quantum and classical\nmachine learning. This combination of training methods ensures accurate results across various\ntasks, highlighting the SQQNN's versatility in leveraging different optimization strategies tailored\nto specific problems.\nThe SQQNN provides a promising, efficient, and adaptable framework for practical quantum\nneural networks, bridging the gap between current quantum hardware and machine learning\napplications. Future research could explore extensions to multi-qubit systems, the development\nof novel activation functions, and the application of the model to diverse pattern datasets and\nreal-world scenarios. Employing qudits for multi-ary classification, by capitalizing on the multi-\noutput nature of quantum measurements, presents another intriguing direction."}]}