{"title": "Reproducibility Study of Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation", "authors": ["Jose L. Garc\u00eda", "Karol\u00edna H\u00e1jkov\u00e1", "Maria Marchenko", "Carlos Miguel Pati\u00f1o"], "abstract": "This paper presents a reproducibility study and extension of \"Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation.\" We validate the original findings using a range of open-weight models (1.5B-70B parameters) and GPT-40 Mini while introducing several novel contributions. We analyze the Pareto front of the games, propose a communication-free baseline to test whether successful negotiations are possible without agent interaction, evaluate recent small language models' performance, analyze structural information leakage in model responses, and implement an inequality metric to assess negotiation fairness. Our results demonstrate that smaller models (<10B parameters) struggle with format adherence and coherent responses, but larger open-weight models can approach proprietary model performance. Additionally, in many scenarios, single-agent approaches can achieve comparable results to multi-agent negotiations, challenging assumptions about the necessity of agent communication to perform well on the benchmark. This work also provides insights into the accessibility, fairness, environmental impact, and privacy considerations of LLM-based negotiation systems.", "sections": [{"title": "Introduction", "content": "The rapid advancement of Large Language Models (LLMs) has opened new opportunities for Artificial Intelligence (AI) applications, particularly in the form of autonomous AI agents. These agents, capable of interacting and communicating with one another, can be especially useful in negotiation systems. Negotiations involve complex multi-agent interactions that require resolving multi-issue scenarios with little to no supervision, so it is critical to evaluate whether these agents are reliably performing their intended tasks (Shavit et al., 2023). Within this context, reproducibility studies are valuable, as they help validate benchmarks the AI community can use to evaluate the performance of current and future models (Reuel et al., 2024; Chang et al., 2023). This paper aims to reproduce the results of an LLM testbed specially designed for negotiation games (Abdelnabi et al., 2024) to contribute to developing robust benchmarks.\nOur reproducibility goal is to assess whether the benchmark effectively evaluates the models' negotiation performance and whether the Chain-of-Thought (CoT) prompt configurations contribute to successful deal-making, especially in terms of accountability (Chan et al., 2024) and confidentiality. To achieve this, we compared different open-weight models against a state-of-the-art (SOTA) closed-source model. Building on the original paper, we analyze the Pareto front of the proposed games and establish a strong baseline to test whether an agent can successfully propose a deal without communicating with other parties.\nWe also present extensions that address the environmental impact, accessibility, fairness, and confidentiality. For environmental impact and accessibility, we use the benchmark to evaluate open-weight models and compare their performance to larger, more energy-consuming ones. Smaller models reduce the computational resource requirements (Fu et al., 2024; Sinha et al., 2024), and we compare the performance between models that range from 1.5B to 70B parameters. For fairness, we introduce an inequality metric to assess whether all parties benefit equally from the negotiation. For confidentiality, we compare open-weight models with a closed-source model that requires access through an external API and may expose attack surfaces that can be exploited by malicious actors (Evertz et al., 2024; Dunn et al., 2024)."}, {"title": "Scope of Reproducibility", "content": "We focus our reproduction efforts on investigating the following claims from the paper:\n1. Open-weight models fall slightly behind SOTA closed-source models.\n2. Small models-i.e., models with 8B parameters or less are not suitable for the negotiation\nbenchmark because they cannot follow the format of the game.\n3. Agents have better outcomes when using CoT structures in their prompts.\n4. The agents' incentives in the benchmark result in non-zero-sum games.\n5. The benchmark measures cooperation, communication, and negotiation skills.\nThe code and instructions for reproducing the results of this work, along with our additions and\nthe fixes mentioned in Section A.7, are available on Github 1."}, {"title": "Original Paper", "content": null}, {"title": "Benchmark and Base Game", "content": "The original paper (Abdelnabi et al., 2024) introduces a benchmark to evaluate whether LLMs can\nsuccessfully reach agreements in a multi-agent, multi-issue, semantically rich negotiation game over\nmultiple rounds. The benchmark is designed to test agents in a dynamic, multi-turn scenario that\nrequires advanced reasoning and strategic decision-making. Specifically, it assesses their ability to\nreach agreements through arithmetic calculations, inference, exploration, and planning, in addition\nto essential skills such as communication, collaboration, and instruction following."}, {"title": "Game Variations", "content": "To further expand the benchmark, the authors introduced variations of the base game where a\nsingle agent exhibits greedy or adversarial behaviors, influencing interactions and outcomes. In\nthese variations, an agent can be designated as greedy, focusing solely on maximizing its own\nbenefit, or adversarial, actively targeting another agent to sabotage the deal.\nThe adversarial behavior is also divided into two variations: targeted, where the adversarial agent\nis explicitly assigned a specific target, and untargeted, where the agent independently selects which\nopponent to sabotage.\nLLMs are evaluated even further across four difficulty levels: Base, Game 1, Game 2, and Game\n3. These variations adjust the base game's difficulty, with Games 1 and 3 being more challenging\nand Game 2 presenting the highest level of difficulty. Each variation introduces a new and unique\nsetup with modified negotiation roles, objectives, and minimum utility thresholds designed to test\nthe robustness of LLMs across diverse scenarios."}, {"title": "Evaluation Metrics", "content": "To systematically evaluate agent performance on all of these game variations, the benchmark in-\ntroduces metrics to quantify how well agents align with their assigned roles and objectives, as well\nas their adherence to the negotiation's rules. These metrics include:\n\u2022 5/6-way and 6-way: The percentage of experiments in which the final deal is accepted by\nat least five or all six parties.\n\u2022\nAny: The percentage of experiments in which there exists a deal accepted by at least five\nparties at any point in the negotiation.\nWrong: The percentage of rounds in which an agent proposed a deal below its own score\nthreshold.\n\u2022 Leakage of Information: The percentage of rounds in which an agent revealed their reason-\ning or private information to other parties.\nWe used the 5/6 way and 6-way accepted deals metrics as our main method of comparing the\nmodels, using their values after the final round of negotiation. As mentioned before, the only\nagent able to propose final deals is p\u2081. Other models are not prompted to vote, as voting happens\nautomatically: if the suggested deal meets the minimum score of the agent it automatically votes\nfor the deal. This setup creates a significant power imbalance between the models: as only p\u2081 can\nsuggest deals, and the others accept or decline them automatically, their only way of influencing\nthe outcome of the game is trying to convince p\u2081 of suggesting a more preferable deal for them."}, {"title": "Methodology", "content": null}, {"title": "Models", "content": "The original paper compares the benchmark on different open- and closed-weight LLMs. To re-\nproduce the experiments and validate their claims, we selected a range of models that performed\nwell on common evaluation metrics (Chiang et al., 2024). Given the concerns about reproducibility\nwith proprietary models (Palmer et al., 2024), our budget and computational resource constraints,\nwe opted for open-weight models, as detailed in Table 1.\nFurthermore, since the original paper focused on using GPT-4 (OpenAI, 2023), we chose GPT-40\nMini (OpenAI, 2024) as a more powerful and cost-effective alternative."}, {"title": "Reproduction Setup", "content": "We used the code provided by the authors\u00b2 to reproduce the results in the original paper. While the\nauthors' repository included much of the necessary code, the specific prompts used in the ablation\nstudy were not accessible. Therefore, we inferred the relationships between the available prompts\nand their corresponding ablation configurations. Restructuring the cooperative scratchpad prompts\n(Section A.8) and parametrizing the program's interface to select between different CoT prompts,\nwe enabled an easier reproduction of the ablation study in Table 3 of the original paper."}, {"title": "Extensions", "content": null}, {"title": "Pareto Analysis", "content": "We use Pareto front membership as a binary indicator of a deal's quality."}, {"title": "Single-agent Baseline", "content": "The original paper proposes a multi-agent setup, as described in Section 3.1, where agents commu-\nnicate and negotiate over multiple rounds to refine the initial deal. However, we observed that the\nglobal instructions given to the agents before the game begins contain crucial information regarding\nthe overall setup, including information related to other agents.\nWe argue that, based on the names of the other agents, such as Department of Tourism and Envi-\nronmental League, along with the provided descriptions for each issue and their options, the main\nnegotiator P\u2081 can infer enough information to develop an understanding of the other agents' moti-\nvations and goals without direct communication. We provide a detailed example of the information\navailable to p\u2081 from the game's description in Section A.1.\nWe developed two scenarios to evaluate p\u2081's ability to generate deals independently. The first,\nSingle-agent 1 call, is a simplified version where p\u2081 suggests the given initial deal to himself, then\nreasons for a single round, and proposes the final deal immediately. The second, Single-agent 6\ncalls, is an extended version that allows the agent to prompt itself iteratively over six rounds. In\nthis setup, the agent continuously refines and expands its previous reasoning and planning before\nproposing a new deal.\nWe tested this new baseline across most of the game variations, including greedy agents and Games\n1, 2 and 3. We expected this baseline to perform well in the compromising variant but poorly in\nthe greedy variant, as the latter requires an agent to act beyond its initial prompt specifications."}, {"title": "Structure Leakage", "content": "Open LLMs face more challenges to adhere strictly to instructions than proprietary models\n(Gudibande et al., 2023), which poses a problem in games requiring strict instruction obedience\nfor formatting rules. In the negotiation game, LLMs are prompted to generate a response (full\nanswer) with specific tags in place to distinguish between their plan, scratchpad, and public\nanswer. If the agent fails to separate its output correctly, parts of its private output, like the CoT\nabout other agents or future plans, are shared with all agents. This information leakage is possible\ndue to the modeling decisions in the original implementation.\nTo measure this structural leakage of information, we check whether 1) the full answer and\npublic answer differ, 2) at least one of the <PLAN>, </PLAN>, <SCRATCHPAD>, </SCRATCHPAD> tags\nis present in the public answer, or 3) tags <DEAL> or </DEAL> are missing on the public answer.\nIf either of these conditions is met, the agent did not conform to the instructions, implying the\nunwanted release of information. We report this metric because sharing the full answer of the\nagents disrupts the negotiation dynamics and introduces unrealistic scenarios that could be avoided\nusing different implementation decisions.\nOn the original paper, the leakage of information metric was measured using a LLM agent, GPT-4,\nto check the answers of the agents after each round and check the percentage of answers with leaked\nprivate information. We compare our structure-induced leakage scores to the scores of the original\nleakage metric, using GPT-40 Mini instead."}, {"title": "Inequality Metric", "content": "The original paper compares the individual score of agent p\u2081 to the collective score of the group (the\nmean score of all agents) to track the dynamics of the negotiation throughout the game. Building\non this, we propose to measure inequality among the agents to explore how the negotiation process\naffects the distribution of outcomes. This allows us to observe whether the game produces fair\noutcomes or if certain agents are disadvantaged-particularly since a successful deal only requires\nfive out of six players to agree. To measure inequality, we use the Gini coefficient (Dorfman, 1979),\na commonly used metric that ranges from 0 (perfect equality) to 1 (maximum inequality). We use\nEquation 1 to compute the Gini coefficient, where xi and xj are the gains of agents, n is the number\nof agents, and is the mean gain.\n$Gini = \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}|x_i - x_j|}{2n^2\\bar{x}}$"}, {"title": "Results", "content": null}, {"title": "Reproducibility of Original Paper", "content": "To reproduce the original paper's results as closely as we could, we evaluated the performance of\ndifferent models on the original multi-agent negotiation benchmark. Using each of these models to\nact as different negotiation agents, we compared their ability to produce successful deals and follow\nthe structured negotiation process rules. Table 2 presents the performance of all tested models. The\nperformance of different models was evaluated using the metrics defined in the original benchmark,\ndescribed in 3.3.\nTo illustrate the correlation between model size and performance, we refer to Figure 1. It clearly\nshows that increasing model size generally improves performance. Smaller models, such as Qwen\n2.5 1.5B and DeepSeek-R1 7B, struggled to reach acceptable deals and exhibited a high percentage\nof incorrect deals or failed experiments.\nTo further evaluate the reproducibility of the original paper, we conducted an ablation study to\nexamine the impact of prompt structure on performance, as shown in Table 3. We systematically"}, {"title": "Pareto Efficiency Analysis", "content": "Table 4 shows that almost all acceptable deals are on the Pareto front while all the unacceptable\ndeals are outside of it. Since all non-acceptable deals result in a score composed of acceptance\nthresholds (BATNAs), all acceptable deals dominate the non-acceptable ones. Without the BATNA\nrule, the game's Pareto front would be 705/720.\nThe game is non-zero-sum, as the authors state because the BATNA rule limits the Pareto front\nalmost entirely to the set of acceptable deals. However, the game becomes almost zero-sum as soon\nas the models reach the set of acceptable deals. We identify two main implications of this finding:\n1. Models have no collective incentive to improve deals since there is no room for optimiza-\ntion beyond acceptability. The problem that the models are collectively solving is not\noptimization of the score, but identifying any deal from the subset of acceptable deals.\n2. Almost all acceptable deals are non-comparable in terms of quality: improving the score of\none party means worsening the score of another."}, {"title": "Single-agent Setup", "content": "Our results show that the single-agent configuration performs just as well as the original multi-\nagent setup. Without communicating with other agents, and using only information from the\nglobal initial game rules provided, the model can predict acceptable deals with the same success\nrate as the multi-agent setup."}, {"title": "What the Game Actually Measures", "content": "The game is originally presented as an assessment of how well six agents negotiate the best possible\ndeal. However, as demonstrated above, the problem effectively reduces to finding any acceptable\ndeal-most of which already lie on the Pareto front. A crucial yet unstated detail in the original\npaper is that the agents do not actually vote themselves. Instead, voting is automated: an agent\naccepts a deal if it meets its minimum score threshold. This creates an inherent imbalance-\nonly p\u2081 has decision-making power, while the other agents merely provide information. Moreover,\nour single-agent baseline performs as well as the multi-agent setting, suggesting that p\u2081 generates\nequally strong deals regardless of communication. This implies that information from the other\nagents is either redundant or not utilized by p1."}, {"title": "Proposed New Metrics", "content": null}, {"title": "Structure Leakage", "content": "The single-agent baseline has shown that the rules of the game and agent descriptions already reveal\nsome information to the players. This leads us to another source of information leaking: Structural\nLeakage. This metric shows leakage of confidential information due to LLMs' incorrect formatting\nof outputs. The implementation decision to rely on LLMs to correctly format their answers is\ndetrimental to the benchmark's effort for a fair comparison of models. As our results show, small\nmodels had a harder time strictly adhering to formatting requirements, already disadvantaging\nthem in negotiation under this implementation. As shown in Table 8, structural leakage is directly\ncorrelated with the model size-larger, better-performing models have no structural leakage, while\nthe smaller models experience substantial leakage.\nWe initially expected the original leakage metric to always be higher than structural leakage due to\nits presumed broader scope. The original metric uses an LLM as a judge, which should be capable\nof detecting both explicit structural errors (e.g. missing plan or scratchpad tags) and more subtle\nforms of leakage. However, this is not always the case since small Qwen models seem to perform\nbetter on the original leakage metric. The structure leakage detects leakage of plans or scratchpads\nby identifying tags such as <PLAN>, but the original metric does not consider these cases as leakage\nbecause agents do not reveal information about confidential scores. We argue that this behavior is\na form of leakage because it disrupts the negotiation dynamics.\nTo ensure all types of leakage are properly detected, a more robust prompt for the LLM judge\nis needed. In addition, we have computed the intersection of the two metrics to show how much\nleakage could be potentially prevented if different implementation choices were made, see Table\n8. For small models, it is apparent that most of the leakage occurs because entire answers are\nrevealed to others due to a lack of tag adherence. We argue that the benchmark should handle the\ndifferent CoT steps separately to avoid structural leakage and not hinder smaller models because\nof formatting problems."}, {"title": "Inequality", "content": "Results from Figure 7 show that the highest inequality is reached in the greedy game variants,\nwhich is expected since most of the proposed deals are not acceptable. Secondly, we showcase that\nin the single-agent setup, deals are slightly more unequal since p\u2081 is the only one proposing deals\nwithout others' input. Surprisingly, we show that adversarial games lead to deals with inequality\nsimilar to compromising games. However, from the single-agent baseline in Figures 3 and 5, we\nsee that p\u2081 does not take much input from other agents. Therefore, it is likely that the adversarial\nagent does not considerably change the dynamic of the game, explaining similar inequality scores\nof the deals. However, Table 5 shows that using different prompt ablations yields different results.\nTherefore, each game variant can prefer a different CoT configuration for its best performance. We\nargue that reporting the inequality enhances the benchmark by capturing variations in individual\nscores, providing a more comprehensive evaluation of agent performance."}, {"title": "Discussion", "content": "Our study aimed to replicate the main claims of the paper \"Cooperation, Competition, and Mali-\nciousness\" (Abdelnabi et al., 2024), which proposes a benchmark for LLM negotiation. Furthermore,\nwe propose several extensions to test the claims and advance the focus on fairness and confidential-\nity.\nOur main result shows that the benchmark fails to measure the communication and negotiation\nbetween agents, disproving the core claim of the original paper, Claim 5, about the importance of\nthese LLM skills for successful negotiation in the benchmark's game. We demonstrate this using an\nequally well-performing baseline where no communication is present. Secondly, our game analysis\nshows that the game is technically non-zero-sum as the original paper claims in Claim 4. However,\nonce agents reach the set of acceptable deals, further improvement is not possible. This indicates\nthat the benchmark prioritizes finding sufficiently good deals rather than necessarily identifying\nthe optimal one (generating the highest optimal score). We show that Claim 2 holds since large\nmodels outperform most small models with the exception of GPT-40 Mini, whose size is, however,\nnot explicitly disclosed. Similarly, we support Claim 1, as we show that open-weight models fall\nbehind their SOTA closed-weight counterparts. Finally, we confirm that CoT steps are essential for\neach agent to reach their own and collective goals, Claim 3. The results of the extensions to the\noriginal work show that:\n1. The game is not about negotiating the best deal, but p\u2081 finding any acceptable deal.\n2. An agent can propose a successful deal without communicating with the other parties.\n3. The proposed leakage metric addresses gaps in the original approach for detecting leakage\nand their implementation decisions for handling LLM outputs.\n4. The inequality metric helps to identify the presence of greedy agents in the negotiation.\n5. Finally, the main failure modes of small models are self-monologues and answer formatting."}, {"title": "Limitations and Future Work", "content": "A key limitation of this study is the sensitivity of results to prompt variations. The CoT ablation\nof prompts from the original paper was tested only on GPT-3.5 and GPT-4. From our results from\nTable 5 and 6 we can see that different prompt configurations yield various results for different\nmodels. This is a big limitation of the benchmark as well as of our results. This dependency\nsuggests that results may reflect an agent's responsiveness to specific prompt formulations rather\nthan its underlying negotiation abilities. Additionally, it raises concerns about reproducibility, as\nminor prompt variations could lead to significantly different outcomes among the models tested.\nSecondly, due to constraints on computational resources we did not perform all of the experiments\non models larger than 32B parameters. Therefore, there might be other open-weight models outper-\nforming SOTA closed-weight models. Future improvements in LLM negotiation benchmarks should\ndevelop a more robust approach to prompt crafting. There is also room for improvement in the\ngame design itself. We suggest designing a similar benchmark but with a non-zero-sum set of ac-\nceptable deals, a smaller Pareto front, and non-automated voting among agents. Finally, to ensure\nthe benchmark tests the communication and negotiation skills of agents, our communication-less\nbaseline should not be competitive."}, {"title": "Appendix", "content": null}, {"title": "Example of Information in Game Description", "content": "We provide an example of how the game's description can provide p\u2081 the information required to\nachieve a successful deal. In the base game designed by Abdelnabi et al. (2024), issue A (Infras-\ntructure Mix), related to whether the facilities will be built on land or water, the Environmental\nLeague already knows that p\u2081, named SportCo, will push for solution A1 (water-based, the least\nrestrictive option for SportCo), and oppose A3 (land-based, the most restrictive). Similarly, in\nissue B (Ecological Impact), p\u2081 can infer that the Environmental League will strongly favor B3\n(environmental improvement) and likely reject B1 (permanent damage), making the negotiation\nunnecessary. Finally, in issue E (Compensation to other cities), it is clear that Other cities will\npush for a higher compensation, making E5 (no compensation) unacceptable. As a result, P1 can\nimmediately propose a compromise, such as E3 (paying $300 million) instead of El (paying $600\nmillion) without further negotiation.\nAdditionally, issues B and E provide explicit context before listing the available options for the\nagents. For example, issue B states 'The \"Environmental League\" argues that this project might\ndamage local dolphins and sea lion populations.', while issue E says that '\"Other cities\" in the\narea believe their local tourism will be harmed by this project and therefore they are requesting\ncompensations. \"'. These descriptions further reinforce the predictability of each agent's preference.\nProviding this information to all agents, especially p\u2081 who has the final say in the proposal, reduces\nthe necessity of direct communication between the agents for successfully reaching an agreement.\nTo evaluate this, we test a scenario where p\u2081 generates deal proposals without input from other\nparties. By relying only on the provided information, its reasoning skills, and planning capabilities,\nwe assess whether it can independently come up with an agreeable deal that satisfies all parties."}, {"title": "Impact of Model Size", "content": "To ensure structured communication, the benchmark requires all the agents to format their re-\nsponses according to a predefined structure provided through the CoT prompts in each round.\nFor example, agents must enclose their proposals for the other agents within <ANSWER> tags (com-\nmonly referred to as the public answer), while their private reasoning must be enclosed within\n<SCRATCHPAD> tags (the private answer). Additionally, any deal proposal must be enclosed within\n<DEAL> tags. Properly using these tags is crucial to evaluate how well a model follows instructions\nand adheres to the benchmark's structure, as failure to do so may result in an inability to assess\nits capabilities.\nThe models with less than 8B parameters tested by Abdelnabi et al. (2024) were not included in their\nresults because they did not follow the game's rules. However, the authors do not provide details of\ntheir failure modes. Our results from small model agents show that the two main reasons for failure\nare failing to include the correct tags in the answer and engaging in irrelevant self-monologues.\nAs shown by Table 2, we could not parse deals from Qwen 2.5 1.5B in 60% of the experiments.\nThe reason for the failed experiments was responses such as Figure 4. The agent answered with\nrelated topics but used **ANSWER** instead of the <ANSWER> and </ANSWER> tags to enclose the\npublic answer. Additionally, the agent does not include the <DEAL> tag with the deal proposal to\nshow the others. We also encountered instances with hallucinated tags like <SUGGESTION>. These\nresults align with Abdelnabi et al. (2024), as the small models did not follow the game instructions\ncorrectly. Another reason for failure was engaging in irrelevant self-monologues or responding with\nrandom words and characters that did not add value to the negotiation process. This was the cause\nof the failed experiments for Llama 3.1 8B and Llama 2 13B, shown in Table 2. Consequently, this\nincreased the required computing resources because it made the conversation history longer.\nUnder our definition of small models, GPT-40 Mini is in the category of small models according\nto the 8B parameter count estimated by Abacha et al. (2024). This would then disprove the claim"}, {"title": "Computational requirements and energy consumption", "content": "We conducted our experiments using a combination of local machines and high-performance com-\nputational resources. For proprietary models like GPT-40 Mini, we accessed the model via the\nOpenAI API, which did not require additional computational resources on our end. For the open-\nsource models, we used the Netherlands' national supercomputer, Snellius, with access to NVIDIA\nA100 and H100 GPUs.\nTo monitor energy consumption, we used EAR (Corbalan et al., 2020). Table 1 reports approxi-\nmately how much energy was consumed per game for each model. The table shows that, in most\ncases, energy consumption is correlated to the size of the model, with smaller models consuming\n20-200 kJ, and larger models using 600-800 kJ.\nOne notable exception to this is Llama2 13B, which had an unusually high energy consumption due\nto its excessive self-monologing. The model continuously produced irrelevant text until we stopped\nit manually. The higher energy consumption of Phi4 and Llama3.1-8B compared to Qwen 32B,\nalthough more than two and four times smaller in size, was caused by longer responses from the\nagents.\nOverall, Qwen 32B, the model used in most of the experiments, achieves high scores in the game\nwhile using energy amounts comparable to those of smaller models. This made it an efficient choice\nfor large-scale evaluations."}, {"title": "Additional Results for Single-Agent Baseline", "content": null}, {"title": "Additional Results for Game Variants", "content": null}, {"title": "Additions to the author's code", "content": "\u2022 Originally, the authors set the model temperature to 0 and a random order of the agents\nfor each round. However, the authors did not add a fixed seed in the code, making their\nexact results not reproducible. To address this, we added fixed seeds to the code setup. We\nthen ran all the experiments 10 times with seeds ranging from 1 to 10. We also corrected\nthe do_sample parameter in the Hugging Face pipeline, changing it from True to False to\nset up the correct greedy decoding configuration.\n\u2022 Although the original paper performed 20 experiments, we reduced the number of experi-\nments due to computational constraints. This decision was also influenced by the fact that\nthe exact reproduction of the results is not possible.\n\u2022 The authors' code uses fixed float32 precision for models loaded from Hugging Face, which\ncan be inefficient and affect the generalization for models like Phi-4. We resolved this is-\nsue by using auto precision, which allows the framework to automatically select the most\nappropriate precision for the available hardware and model requirements. Additionally, we\nmanually adjusted the precision of Llama models to float16 precision. This configuration\nwas recommended for managing memory constraints while maintaining an acceptable per-\nformance. It is important to note that float16 is not a form of quantization but a more\nefficient memory format.\n\u2022 In their prompts, authors ask models to format their answers using specific tags, such\nas <DEAL>, <PLAN>, <SCRATCHPAD>, and <ANSWER>. These tags are later used to break\nthe model's answer into the Chain-of-Thought structure. Models generally comply with\nthese requirements. However, during evaluation, the <DEAL> tags are not used for parsing;\ninstead, the suggested deals are parsed directly from the models' public answers. This\napproach often leads to unparsable deals, negatively impacting the scores of certain models.\nWe consider this a bug because models sometimes respond in a more human-like man-\nner, describing the deals' meanings (e.g., \"a loan of 200 dollars\" instead of \"option A2\")."}]}