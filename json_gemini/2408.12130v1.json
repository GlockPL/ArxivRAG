{"title": "S-EPOA: Overcoming the Indivisibility of Annotations with Skill-Driven Preference-Based Reinforcement Learning", "authors": ["Ni Mu", "Yao Luan", "Yiqin Yang", "Qing-shan Jia"], "abstract": "Preference-based reinforcement learning (PbRL) stands out by utilizing human preferences as a direct reward signal, eliminating the need for intricate reward engineering. However, despite its potential, traditional PbRL methods are often constrained by the indivisibility of annotations, which impedes the learning process. In this paper, we introduce a groundbreaking approach, Skill-Enhanced Preference Optimization Algorithm (S-EPOA), which addresses the annotation indivisibility issue by integrating skill mechanisms into the preference learning framework. Specifically, we first conduct the unsupervised pretraining to learn useful skills. Then, we propose a novel query selection mechanism to balance the information gain and discriminability over the learned skill space. Experimental results on a range of tasks, including robotic manipulation and locomotion, demonstrate that S-EPOA significantly outperforms conventional PbRL methods in terms of both robustness and learning efficiency. The results highlight the effectiveness of skill-driven learning in overcoming the challenges posed by annotation indivisibility.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) has made impressive strides across a variety of fields, including gameplay (Mnih et al. 2013; Silver et al. 2016), robotics (Chen et al. 2022), autonomous systems (Bellemare et al. 2020), and plasma control (Degrave et al. 2022). Yet, the success of RL algorithms frequently relies on the careful construction of reward functions, a process that can be both labour-intensive and costly. To solve this issue, Preference-based Reinforcement Learning (PbRL) emerges as a compelling alternative (Christiano et al. 2017; Lee, Smith, and Abbeel 2021). PbRL uses human-provided preferences among various agent behaviours to serve as the reward signal, thereby eliminating the need for hand-crafted reward functions.\nExisting PbRL methods (Lee, Smith, and Abbeel 2021; Park et al. 2022a; Shin, Dragan, and Brown 2023; Kim et al. 2023) focus on enhancing feedback efficiency, striving to maximize the expected return with minimal feedback queries. However, these methods rely on high-quality or even ideal expert feedback, overlooking an important issue in preference annotation: the indivisibility of annotations. For example, asking humans to specify preferences between two similar trajectories can be challenging, as it is difficult for human observers to discern which is superior based on their similarity. Consequently, the resulting preference labels are often incorrect, further degrading the performance of the algorithm (Lee et al. 2021). Therefore, the annotation indivisibility issue significantly hinders the wide applicability of PbRL.\nIn unsupervised reinforcement learning, information-theoretic and state entropy methods have been proven to discover useful and diverse skills without reward (Eysenbach et al. 2019; Hansen et al. 2019). These skills can serve as primitives to tackle complex tasks. In contrast to the unlabeled agent behaviors in PbRL, these discovered skills possess a higher degree of distinguishability, allowing humans to express preferences between skills easily. However, the skill-driven preference learning approach is less appreciated in the PbRL setting, and how to apply the discovered skills to the preference annotation needs to be clarified. This naturally leads to the following question:\nHow can we integrate the skill mechanism with PbRL to overcome the indivisibility of annotations?\nIn this work, we aim to provide an effective solution to the important and practical problem in PbRL: the indivisibility of annotations. Firstly, we conduct skill-based unsupervised pretraining to learn useful skills due to the high discriminability of the skill behavior. Then, we propose a novel query selection mechanism in the learned skill space, which can effectively balance the information gain and discriminability of the query. We name our method as Skill-Enhanced Preference Optimization Algorithm (S-EPOA). In our experiments, we further demonstrate the necessity of the above two techniques. Experimental results also show that S-EPOA significantly outperforms conventional PbRL methods in terms of both robustness and learning efficiency.\nIn summary, our contributions are threefold: First, we propose S-EPOA, a skill-driven reward learning framework for PbRL, designed to solve the annotation indivisibility issue effectively. Second, we theoretically prove the insufficiency of the current mainstream query selection mechanism, such as disagreement. Lastly, we demonstrate that S-EPOA outperforms existing PbRL baselines under the noisy feedback setting across diverse tasks. Extensive experimental results indicate that by introducing the skill mechanism, we can effectively solve the annotation indivisibility issue, thereby broadening the application of PbRL."}, {"title": "2 Related Work", "content": "Preference-based reinforcement learning. PbRL enables humans (or supervisors in other forms, like script teachers) to guide the RL agent toward desired behaviors by providing preference on segment pairs, where the feedback efficiency is a primary concern (Lee, Smith, and Abbeel 2021; Park et al. 2022a). Prior works improve the feedback efficiency from various perspectives. Some works focus on the query selection scheme, trying to improve the information quality of queries (Ibarz et al. 2018; Biyik et al. 2020). Some works integrate unsupervised pretraining to avoid the waste on initial nonsense queries (Lee, Smith, and Abbeel 2021). Some works augment queries from humans to better utilize limited human feedback (Park et al. 2022a). These methods depend on reliable feedback. However, humans could make mistakes, especially when the segment pair for comparison is slightly different, which restricts and even harms the performance in practice (Lee et al. 2021; Cheng et al. 2024).\nUnsupervised pretraining for RL. Unsupervised pre-training has been well studied in RL (Xie et al. 2022), which leverages unlabeled data (i.e., transitions without task reward) to learn a policy or a set of policies that have a strong ability to explore the state space by introducing the intrinsic reward. The method to calculate the intrinsic reward varies in different unsupervised pretraining methods, including uncertainty measures like prediction errors (Pathak et al. 2017; Pathak, Gandhi, and Gupta 2019; Burda et al. 2019), state entropy (Hazan et al. 2019; Liu and Abbeel 2021b), pseudocounts (Bellemare et al. 2016; Ostrovski et al. 2017) and empowerment measures like mutual information (Eysenbach et al. 2019; Sharma et al. 2020; Liu and Abbeel 2021a; Park et al. 2022b; Park, Rybkin, and Levine 2023). The learned policy could serve as a good initialization policy for downstream tasks, which improves the sample efficiency in multitask RL and few-shot RL.\nUnsupervised skill discovery methods. Unsupervised skill discovery methods are a subset of unsupervised pre-training methods, which use empowerment measures as the intrinsic reward, trying to discover a set of distinguishable primitives. Mutual information \\(I(s, z)\\) is a common choice for the empowerment measure, where s is a state, and z is a latent variable indicating the skill. Some studies consider the reverse form \\(I(s, z) = H(z) - H(z|s)\\) (Eysenbach et al. 2019; Park et al. 2022b), which train a parameterized skill discriminator \\(q(z|s)\\) together with the policy. On the other hand, the forward form \\(I(s, z) = H(s) \u2013 H(s|z)\\) (Sharma et al. 2020; Liu and Abbeel 2021a) can be integrated with model-base RL and state entropy-based unsupervised pre-training algorithms. Additionally, some studies design the skill latent space for unique properties by parameterizing the distribution \\(q(z|s)\\) or \\(q(s|z)\\). VISR (Hansen et al. 2019) and APS (Liu and Abbeel 2021a) let the latent z be the successor feature to enable fast task inference. LSD (Park et al. 2022b) and METRA (Park, Rybkin, and Levine 2023) bind the distance in state space and latent space to force a significant travel distance in a trajectory, thereby capturing dynamic skills."}, {"title": "3 Preliminaries", "content": "Reinforcement Learning. A Markov Decision Problem (MDP) could be characterized by the tuple \\((S, A, P, r, \\gamma)\\), where S is the state space, A is the action space, \\(P : S \\times A \\rightarrow \\Delta(S)\\) is the transition function, \\(r : S \\times A \\rightarrow \\mathbb{R}\\) is the reward function, and \\(\\gamma \\in [0,1)\\) is the discount factor balancing instant and future rewards. A policy \\(\\pi\\) interacts with the environment by sampling action a from distribution \\(\\pi(s, a)\\) when observing state s. The goal of RL agent is to learn a policy \\(\\pi : S \\rightarrow \\Delta(A)\\), which maximizes the expectation of a discounted cumulative reward:\n\\[\\mathcal{L}(\\pi) = \\mathbb{E}_{\\mu_{0,\\pi}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\right].\\]\nFor any policy \\(\\pi\\), the corresponding state-action value function is \\(Q^{\\pi}(s, a) = \\mathbb{E}[\\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} | s_t = s, a_t = a, \\pi]\\).\nThe state value function is \\(V^{\\pi}(s) = \\mathbb{E}[\\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} | s_t = s, \\pi]\\). It follows from the Bellman equation that \\(V^{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) Q(s,a)\\).\nPreference-based Reinforcement Learning. In PbRL, the reward function r is replaced by preferences on segment pairs (denoted as \\((\\sigma_0, \\sigma_1)\\) provided by humans. A segment \\(\\sigma\\) is a continuous sequence in a fixed length H of states and actions, i.e. \\(\\{s_k, a_k, ..., s_{k+H-1}, a_{k+H-1}\\} \\). The preference is provided in the form of one-hot label \\(y \\in \\{(0,1), (1,0)\\} \\) indicating the segment human prefers. All preference items \\((\\sigma_0, \\sigma_1, y)\\) are stored in the dataset \\(\\mathcal{D}\\). The algorithm first estimates a reward function \\(r_\\psi : S \\times A \\rightarrow \\mathbb{R}\\) parameterized by \\(\\psi\\) using provided preferences, and then uses the learned \\(r_\\psi\\) as the reward function to train the policy with common RL algorithms. Utilizing the Bradley-Terry model (Bradley and Terry 1952; Christiano et al. 2017), we construct \\(r_\\psi\\) as follows:\n\\[P_{\\psi}[\\sigma_1 \\succ \\sigma_0] = \\frac{\\exp \\sum_t r_{\\psi}(s^1_t, a^1_t)}{\\sum_{\\tau \\in \\{0,1\\}} \\exp \\sum_t r_{\\psi}(s^\\tau_t, a^\\tau_t)},\\]\nwhere \\(\\sigma_1 \\succ \\sigma_0\\) indicates the human prefer \\(\\sigma_1\\) than \\(\\sigma_0\\). \\(r_\\psi\\) can be trained by minimizing the cross-entropy loss:\n\\[\\mathcal{L}_{reward}(\\psi) = \\mathbb{E}_{(\\sigma_0, \\sigma_1, y) \\sim \\mathcal{D}} \\left[ y(0) \\log P_{\\psi}[\\sigma_0 \\succ \\sigma_1] + y(1) \\log P_{\\psi}[\\sigma_1 \\succ \\sigma_0] \\right].\\]\nActive Pretraining with Successor Features (APS). APS (Liu and Abbeel 2021a) is an unsupervised skill discovery algorithm that maximizes the forward form of mutual information \\(I(s, z)\\) between state s and skill latent variable z (i.e., \\(I(s,z) = H(s) \u2013 H(s|z)\\)) to learn a set of distinguishable skills. To make the mutual information objective tractable for optimization, practical algorithms maximize the lower bound derived by variational approximation instead (Barber and Agakov 2004):\n\\[I(s, z) \\geq \\mathbb{E}_{s,z} [\\log q(s|z)] - \\mathbb{E}_{s} [\\log p(s)],\\]\nwhere \\(q(s|z)\\) is a posterior of state given latent z, and \\(p(s)\\) is the state distribution. The latent space in APS is designed to be the successor feature (Barreto et al. 2017), which assumes the reward function r is the inner product of some state feature \\(\\phi(s)\\) and latent variable z indicating the task. Specifically, APS normalizes the latent variable z and state feature \\(\\phi(s)\\) to be the unit length and parameterizes \\(q(s|z)\\) as the Von Mises-Fisher distribution with a scale parameter of 1. The intrinsic reward function in APS is\n\\[r_{int}(s, a, s') = \\phi(s)^T z - \\log p(s'),\\]\nwhere \\(-\\log p(s')\\) is the entropy term. We can estimate \\(-\\log p(s')\\) using particle-based entropy estimation (Singh et al. 2003; Liu and Abbeel 2021b):\n\\[-\\log p(s') = \\log \\left(\\frac{1}{k} \\sum_{h \\in \\mathcal{N}} \\delta - \\|\\delta - h \\|^2 \\right),\\]\nwhere \\(\\delta = \\phi(s')\\) is the feature of the successor state \\(s'\\), \\(h_k\\) denotes the k-th nearest neighbors of \\(\\delta\\) in the replay buffer. The integration with the successor feature enables APS to quickly infer the most similar skill given the task reward at inference using Least Square."}, {"title": "4 Skill-Driven PbRL", "content": "In this section, we start with analyzing the indivisibility issue of annotations in PbRL. To solve this issue, we propose a simple yet efficient method, Skill-Enhanced Preference Optimization Algorithm (S-EPOA), which combines the skill discovery mechanism with the existing PbRL framework. Specifically, S-EPOA introduces two key components:\n\u2022 Skill-based unsupervised pretraining, where the agent explores the environment and learns useful skills without supervision (see Section 4.2).\n\u2022 Skill-based query selection, which can select more distinguishable queries based on the learned skill space (see Section 4.3).\nWe show the overall framework of S-EPOA in Figure 1 and Algorithm 1.\n4.1 Indivisibility of Annotations\nIdeally, we could use real human feedback to evaluate the algorithm's efficacy in the real world. However, in practical applications, humans are often required to label two similar behaviors. The situation annoys the labelers since labelers are prone to making mistakes when labeling similar behaviors. The wrong labeling reduces the precision of the trained reward function, further degrading the algorithm's performance. We name this labeling issue, where compared behaviors are similar and indistinguishable, as the Indivisibility of Annotations. The issue of annotation indivisibility significantly limits the broad application of PbRL, especially in safety control fields (Fulton and Platzer 2018) where the precision of the reward function is strictly required. In this work, we focus on how to generate queries with high discriminability.\n4.2 Skill-based Unsupervised Pretraining\nWhat kind of behavior has high discriminability? In unsupervised RL, some research has been proven to discover valuable and diverse skills without reward (Eysenbach et al. 2019; Hansen et al. 2019). In contrast to the unlabeled agent behaviors in PbRL, these discovered skills have high distinguishability. Therefore, a natural approach is to compare discovered skills rather than agent behaviors.\nInspired by the successor features, we utilize the APS algorithm (Liu and Abbeel 2021a) for unsupervised policy pretraining. As mentioned in Sec. 3, APS learns a continuous skill space and a policy \\(\\pi(\\cdot|s, z)\\) conditioned on the skill by maximizing the intrinsic reward in Eq. 4. The intrinsic reward includes the inner product between state features \\(\\phi(s)\\) and skills z, and state entropy \\(H(\\phi(s'))\\). The state-action value function of APS is decomposed as follows:\n\\[Q^{\\pi}(s, a) = \\mathbb{E}_{s_0=s, a_0=a} \\sum_{i=0}^T \\gamma^i (s_{i+1}, a_{i+1}, s_{i+1})\\]\\[\\equiv \\Psi^{\\pi}(s,a)^T z,\\]\nwhere \\(\\Psi^{\\pi}(s, a) = \\mathbb{E}_{s_0=s, a_0=a} \\sum_{i=0}^T \\gamma^i (s_{i+1}, a_{i+1}, s_{i+1})\\) is the successor features of \\(\\pi\\). Then, we minimize the following critic and actor loss:\n\\[\\mathcal{L}_{critic} = ||\\Psi(s_t, a_t)^T z - r_{int} - \\gamma (s_{t+1}, (s_{t+1}, z))^T z||^2\\]\n\\[\\mathcal{L}_{actor} = -\\Psi(s_t, \\pi(s_t, z))^T z\\]\nBased on the above-optimized objective, we can learn diverse skills while quickly inferring the most similar skill using Least Square given the task reward. The pretraining algorithm is illustrated in Algorithm 2.\n4.3 Skill-based Query Selection\nIn PbRL, it is crucial to ensure that the queries we designed provide effective information to the reward model. These queries are expected to clearly demonstrate the teacher's preferences for various behaviors, effectively distinguishing between positive and negative behaviors, thereby guiding the learning algorithm to align with human intentions.\nExisting methods often focus on selecting informative queries, such as the disagreement mechanism in PEBBLE (Lee, Smith, and Abbeel 2021). Similar segments often disagree because it is hard to distinguish clearly which is better or worse. Therefore, the disagreement query selection method tends to select similar segment pairs, which is proved in the following Proposition 1 (the proof is presented in Appendix A). However, as analyzed in Sec. 4.1, while the scripted teacher can always provide a preference label, human teachers may find distinguishing between the two similar segments difficult.\nProposition 1. Let \\{\\(f_i\\)\\} be an ensemble of i.i.d. reward estimators, and \\((\\sigma_1, \\sigma_2)\\) be a segment pair with ground-truth cumulative discounted reward \\(r_1 \\geq r_2\\). Suppose \\(f^i\\) estimates the cumulative discounted reward of \\(\\sigma_i\\) as \\(r^i \\sim \\mathcal{N}(r_j, c)\\) (c is a constant), and induces preference\n\\[P_i[\\sigma_1 \\succ \\sigma_2] = \\frac{\\exp r^i_1}{\\exp r^i_1 + \\exp r^i_2} = sigmoid(\\frac{r^i_1 - r^i_2}{\\tau}).\\]\nThen the disagreement of induced preference across \\(\\{f_i\\}\\), i.e. Var[P[\u03c31 > \u03c32]], approximately and monotonically increases as the dissimilarity of segment pair \\(\\Delta = r_1 - r_2\\) decreases.\nHow to select queries with high discriminability? In contrast to existing methods, skills have high discriminability and are more explanatory. Therefore, we attempt to have the teacher provide preferences between different skills, thereby identifying which skills are positive and which are negative. Specifically, we first define the trajectory estimator \\(R_\\theta(z)\\) to estimate the expected return of the trajectory generated by skill z:\n\\[R_{est}(z) = \\mathbb{E}_{\\tau_{z}} \\left[ \\frac{1}{T} \\sum_{(s,a) \\in \\tau_{z}} r_{\\psi}(s,a) \\right],\\]\nwhere \\(\\tau_z\\) is the trajectory generated by z, and \\(r_\\psi\\) is the learned reward model. In practice, we normalize the targets of \\(R_{est}(z)\\) to the range of [0, 1] for training stability. Based on the trajectory estimator \\(R_{est}(z)\\), we propose the skill-based selection criteria \\(I(\\sigma_0, \\sigma_1)\\) for query \\((\\sigma_0, \\sigma_1)\\) with underlying skills \\((z_0, z_1)\\):\n\\[I(\\sigma_0, \\sigma_1) = (1 + |R_{\\theta}(z_0) - R_{\\theta}(z_1)|) \\cdot \\frac{1}{(1 + Var(P_{\\psi}[\\sigma_1 \\succ \\sigma_0]))},\\]\nwhere \\(P_{\\psi}\\) is the probability that reward model prefer \\(\\sigma_1\\) than \\(\\sigma_0\\), defined in Eq. 1. The first term is used to assess the difference between the skills. The second term is used to measure the uncertainty of the reward model. For training stability, we normalize these two terms to the [0, 1] range and add 1 to balance the two values.\nFor each query \\((\\sigma_0, \\sigma_1)\\), we calculate the skill-based selection criteria \\(I(\\sigma_0, \\sigma_1)\\), and select queries with the highest \\(I(\\sigma_0, \\sigma_1)\\). Based on Eq. 10, we not only consider the uncertainty of the query, which can maximize the information gain, but also consider the differences between skills, ensuring the segments have distinguishable skill explanations. The specific method is illustrated in Algorithm 3."}, {"title": "4.4 Implementation Details", "content": "In this subsection, we describe the overall process of the S-EPOA algorithm, as shown in Algorithm 1. Firstly, we initialize the policy with skill-based unsupervised pretraining, by minimizing the loss in Eq. 7. Then, for each feedback session, we update the trajectory estimator as in Eq. 9, and select queries based on the skill-based selection criteria in Eq. 10. The PbRL reward model \\(r_\\psi\\) is trained using the selected queries based on Eq. 2. Finally, we minimize the critic and actor loss in Eq .7 by replacing \\(r_{int}\\) with \\(r_\\psi\\).\nTo convert the unsupervised pretraining policy \\(\\pi(a|s, z)\\) in Section 4.2 to PbRL's policy in Section 4.3, we attempt to obtain the skill nearest to the current task, denoted as \\(z_{task}\\). Specifically, we collect a batch of transition (s, a, s', r(s, a)), and calculate the Least Square:\n\\[z_{task} = \\arg \\min_z ||r(s,a) - (s')^T z||^2.\\]\nBased on the Eq. 11, we can find the skill whose inner product with the state feature \\((s')\\) is closest to the given \\(r(s, a)\\). Besides the two key components in Section 4.2 and 4.3, we also adopt the semi-supervised data augmentation technique for reward learning (Park et al. 2022a). To elaborate, we randomly sub-sample several shorter pairs of \\((\\sigma_0, \\sigma_1)\\) from the queried segments \\((\\sigma_0, \\sigma_1,y)\\), and use these \\((\\sigma_0, \\sigma_1, y)\\) to optimize the cross-entropy loss in Eq. 2. Moreover, we sample a batch of unlabeled segments \\((\\sigma_0,\\sigma_1)\\), generate the artificial labels \\(\\hat{y}\\), if \\(P_{\\psi}[\\sigma_0 \\succ \\sigma_1]\\) or \\(P_{\\psi}[\\sigma_1 \\succ \\sigma_0]\\) reaches a predefined confidence threshold. More details on the implementation of S-EPOA are provided in Appendix B.2."}, {"title": "5 Experiments", "content": "We design our experiments to answer the following questions:\n1. How does S-EPOA compare to other state-of-the-art methods under non-ideal teachers?\n2. Can S-EPOA select queries with higher discriminability?\n3. What is the contribution of each of the proposed techniques in S-EPOA?\n5.1 Setup\nDomains. We evaluate S-EPOA on several complex robotic manipulation and locomotion tasks from DMControl (Tassa et al. 2018) and Metaworld (Yu et al. 2020). Specifically, We choose 4 complex tasks in DMControl: Cheetah_run, Walker_run, Quadruped_walk, Quadruped_run, and 3 complex tasks in Metaworld: Door_open, Button_press, Window_open. The details of experimental tasks are shown in Appendix B.1.\nBaselines. We compare S-EPOA with other several state-of-the-art methods including PEBBLE (Lee, Smith, and Abbeel 2021), SURF (Park et al. 2022a) and RUNE (Liang et al. 2022). We also train SAC with ground truth reward, as a performance upper bound, For PEBBLE, SURF and RUNE, we employ the disagreement query selection scheme, which performs the best among all the query selection schemes. More details on the algorithm implementation are provided in Appendix B.2.\nNoisy scripted teacher imitating humans. Similar to prior works (Lee, Smith, and Abbeel 2021; Park et al. 2022a), in order to systemically evaluate the performance, we consider a scripted teacher that provides preferences between two trajectory segments according to the sum of ground-truth rewards for each segment.\nWe design a noisy scripted teacher to mimic human decision-making uncertainty. When the performance of two policies is too close, it is challenging for humans to make a clear distinction. To imitate this, we introduce an error mechanism: if the ground truth returns of two trajectories are marginally different, we randomly assign the query a label of 0 or 1. The essence of this approach is to evaluate policy performance by comparing the overall returns of entire trajectories, which more closely resembles how humans assess policies by considering their overall effectiveness. Specifically, for a query \\((\\sigma_0, \\sigma_1)\\), its underlying trajectories \\((\\tau_0, \\tau_1)\\) and ground truth reward function \\(r_{gt}\\), if\n\\[|\\sum_{(s,a) \\in \\tau_0} r_{gt}(s,a) - \\sum_{(s,a) \\in \\tau_1} r_{gt}(s,a) | < \\epsilon \\cdot R_{avg},\\]\nthen we give it a random label. \\(R_{avg}\\) is the average return of the latest ten trajectories. We refer to \\(\\epsilon \\in (0,1)\\) as the error rate. For fairness, we constrain each segment pair in queries taken from different trajectories. Note that our noisy scripted teacher differs from the \"mistake\" teacher of B-Pref (Lee et al. 2021). B-Pref randomly flips correct preference labels, while our scripted teacher only introduces errors in too-close queries.\n5.2 Main Results\nLocomotion tasks from DMControl. Figure 2 shows the learning curves of S-EPOA and baselines on the four DM-Control tasks with three error rates, \\(\\epsilon \\in \\{0.1,0.2, 0.3\\}\\), respectively. As shown in Figure 2, S-EPOA exceeds baselines by a large margin in almost all environments and is robust in non-ideal conditions, while other PbRL methods are unstable and even fail.\nRobotic manipulation tasks in Metaworld. Figure 3 shows the learning curves of S-EPOA and baselines on the three Metaworld tasks with error rate \\(\\epsilon = 0.2\\). These results provide further evidence that S-EPOA effectively enhances robustness against non-ideal feedback across a diverse range of complex tasks.\n5.3 Ablation Study\nComponent analysis. To evaluate the effect of each technique in S-EPOA individually, we incrementally apply skill-based unsupervised pretraining and skill-based query selection to our backbone algorithm. Figure 4(a) shows the learning curves of S-EPOA on the Quadruped_run task with error rate \\(\\epsilon = 0.3\\). Firstly, we observe that the skill-based unsupervised pretraining significantly improves performance, for both skill-based query selection (the red curve vs. green) and disagreement query selection (orange vs. blue). The reason behind this is the pretrained policy can generate segments with diverse behavioral patterns, which induce a better-shaped reward. Also, we remark that skill-based query selection has a positive impact on final results (red vs. orange), because more distinguishable queries are selected by skill-based query selection, and the agent obtains more correct query labels, resulting in better performance. In summary, the results show that the critical components of S-EPOA are both effective, and their combination is essential to our method's success.\nQuery selected by S-EPOA is more dividable. To visually assess the discriminability of the queries selected by the skill-based method versus the disagreement mechanism, we visualize the segment pairs chosen by both methods, as Figure 5 shows. In Figure 5(a), we observe that the segment pair selected by the disagreement mechanism has similar behaviors, making it possibly difficult to distinguish for humans. In contrast, the segment pair selected by S-EPOA has distinctly different behaviors, and it is easily observed that the behaviors of segment \\(\\sigma_1\\) are more preferred.  Therefore, high discriminability makes our method more robust to non-ideal teachers and enhances learning efficiency.\nEnhanced learning efficiency under the ideal scripted teacher. Under the ideal scripted teacher where the error rate \\(\\epsilon = 0\\), S-EPOA can also significantly enhance learning efficiency. As depicted in Figure 4(b), the learning curves clearly demonstrate this advantage. The rapid convergence and superior final performance of S-EPOA are attributed to the deliberate selection of skills with high discriminability. This result further substantiates the robustness and effectiveness of our approach, highlighting its ability to surpass strong baselines even under ideal conditions.\nEffect of data augmentation in S-EPOA. We conduct an ablation study to investigate the effectiveness of the data augmentation in S-EPOA. Figure 4(c) shows the learning curve of both S-EPOA and baselines, with and without data augmentation of SURF. As Figure 4(c) depicts, without data augmentation, both our method and baselines (PEBBLE as the backbone) would face similar performance deficiencies. This indicates that data augmentation is essential for achieving superior performance, while it's not the exclusive factor of our method's success, and does not undermine the innovation of our approach."}, {"title": "6 Conclusion and Discussion", "content": "In this paper, we present S-EPOA, a robust and efficient algorithm for PbRL, under conditions of non-ideal, noisy teachers. S-EPOA is designed to address the Indivisibility of Annotation issue, where labelers have difficulty distinguishing similar segment pairs in queries, thus hindering the real-world application of PbRL. Specifically, S-EPOA first learns distinguishable skills via unsupervised learning. Then, S-EPOA generates distinguishable queries on the learned skill space. Extensive experiments show that S-EPOA outperforms the state-of-the-art PbRL methods in terms of both robustness and learning efficiency. Ablation studies further demonstrate that the skill-based query selection can select queries with distinguishable behaviors. In the future, we aim to extend S-EPOA to a broader range of applications."}, {"title": "A Proof", "content": "Relationship between the similarity of segment pairs and disagreement.\nProposition 1. Let \\{\\(f_i\\)\\} be an ensemble of i.i.d. reward estimators, and \\((\\sigma_1, \\sigma_2)\\) be a segment pair with ground-truth cumulative discounted reward \\(r_1 \\geq r_2\\). Suppose \\(f^i\\) estimates the cumulative discounted reward of \\(\\sigma_i\\) as \\(r^i \\sim \\mathcal{N}(r_j, c)\\) (c is a constant), and induces preference\n\\[P_i[\\sigma_1 \\succ \\sigma_2] = \\frac{\\exp r^i_1}{\\exp r^i_1 + \\exp r^i_2} = sigmoid(\\frac{r^i_1 - r^i_2}{\\tau}).\\]\nThen the disagreement of induced preference across \\(\\{f_i\\}\\), i.e. Var[P[\u03c31 > \u03c32]], approximately and monotonically increases as the dissimilarity of segment pair \\(\\Delta = r_1 - r_2\\) decreases.\nProof. Since 1 and 2 are independent Gaussian distributed, \\(\\delta = 1 - 2\\) is also Gaussian distributed, i.e. 8 ~ \\(\\mathcal{N}(\\Delta, \\sqrt{2c^2})\\). Substitute d into Eq.8, we find P[\u03c31 > \u03c32] is logit-normal distributed, whose moments have no analytic solution.\nAs in (Huber 2020), we approximate sigmoid function with probit function using input scaling factor \\(\\lambda = \\sqrt{\\pi/8}\\) (Kristiadi, Hein, and Hennig 2020), leading to an approximation of Var[P[\u03c31 > \u03c32]]:\n\\[Var[P[\\sigma_1 > \\sigma_2]] \\approx \\mu_s(1 - \\mu_s) (1-\\frac{1}{t}),\\]\nwhere \u03bc\u03b5 \u2248 sigmoid(\u0394/t) \u2208 [1,1) is the approximation of E[P[01 > 02]] derived in a similar manner, and t = \u221a1 + 2X2c2 is a constant (Huber 2020).\nUsing Eq. 13, it is straightforward to check the monotonicity of Var[P[01 > 02]], which concludes the proof."}, {"title": "B Experimental Details", "content": "B.1 Tasks\nThe locomotion tasks from DMControl (Tassa et al. 2018) and robotic manipulation tasks from Metaworld (Yu et al. 2020) used in our experiments are shown in Figure 6.\nDMControl Tasks:\n1. Cheetah_run: A planar biped is trained to control its body and run on the ground.\n2. Walker_run: A planar walker is trained to control its body and walk on the ground.\n3. Quadruped_walk: A four-legged ant is trained to control its body and limbs", "Quadruped_run": "A four-legged ant is trained to control its body and limbs, and crawl fast on the ground.\nMetaworld"}]}