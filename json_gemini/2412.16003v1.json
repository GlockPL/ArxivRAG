{"title": "CHOOSE YOUR EXPLANATION: A COMPARISON OF SHAP AND GRADCAM IN HUMAN ACTIVITY RECOGNITION", "authors": ["Felix Tempel", "Daniel Groos", "Espen Alexander F. Ihlen", "Lars Adde", "Inga Str\u00fcmke"], "abstract": "Explaining machine learning (ML) models using eXplainable AI (XAI) techniques has become essential to make them more transparent and trustworthy. This is especially important in high-stakes domains like healthcare, where understanding model decisions is critical to ensure ethical, sound, and trustworthy outcome predictions. However, users are often confused about which explanability method to choose for their specific use case. We present a comparative analysis of widely used explainability methods, Shapley Additive Explanations (SHAP) and Gradient-weighted Class Activation Mapping (GradCAM), within the domain of human activity recognition (HAR) utilizing graph convolutional networks (GCNs). By evaluating these methods on skeleton-based data from two real-world datasets, including a healthcare-critical cerebral palsy (CP) case, this study provides vital insights into both approaches' strengths, limitations, and differences, offering a roadmap for selecting the most appropriate explanation method based on specific models and applications. We quantitatively and quantitatively compare these methods, focusing on feature importance ranking, interpretability, and model sensitivity through perturbation experiments. While SHAP provides detailed input feature attribution, GradCAM delivers faster, spatially oriented explanations, making both methods complementary depending on the application's requirements. Given the importance of XAI in enhancing trust and transparency in ML models, particularly in sensitive environments like healthcare, our research demonstrates how SHAP and GradCAM could complement each other to provide more interpretable and actionable model explanations.", "sections": [{"title": "Introduction", "content": "Significant progress has been made in the development of Graph Convolution Networks (GCNs) for Human Action Recognition (HAR) in recent years. A significant proportion of the research is dedicated to improving the performance of GCN architectures on widely used benchmark datasets, including NTU RGB+D 60/120 [1, 2] and Kinetics [3]. However, less attention has been paid to understanding and explaining the internal workings of the developed architectures. Concurrently, these models' increasing complexity and scale have reduced their interpretability, turning them into \"black boxes\". Still, understanding the inner workings of such systems is crucial, particularly when they are used in high-risk and sensitive environments such as healthcare, where the outcomes of model decisions can have significant ethical and practical implications [4, 5, 6, 7]. In such contexts, the explainability of models becomes not only a technical concern but also a matter of trust, accountability, and safety [8].\nThe need for explainability in machine learning (ML) models is pushing the adoption of eXplainable Artificial Intelligence (XAI) techniques to describe the inner workings of GCNs [9]. XAI provides tools for understanding how ML models generate particular predictions to facilitate trust and confidence among end-users and developers. This is done by translating the abstract and high-dimensional concepts underlying these models into a space accessible and comprehensible for humans. XAI is divided into two key categories: interpretability and explainability [10]. Interpretability refers to establishing clear, transparent rules that define how an ML model arrives at its decisions. These insights can then be used for adequate clinical adoption and to reduce the risks emerging from biases in the ML models since physicians can understand the decision process. On the other hand, explainability entails creating a framework that explains the ML model's workings, often by representing its decision-making process in a simplified manner, making it accessible for humans to understand [7].\nIn domains like healthcare, where decisions can directly impact patient outcomes, the interpretability of ML models used in this field is critical [11, 12, 13]. Clinicians require assurance that ML models produce decisions based on relevant, reliable, and ethically sound data patterns [14]. The lack of interpretability of such systems could lead to dangerous scenarios, including the perpetuation of biases or the incorrect categorization of patient conditions, which might have significant clinical consequences [15, 16]. Therefore, providing explainability by \"opening the black box\u201c of GCN-based HAR models is essential for their safe deployment and acceptance in sensitive environments such as healthcare [17].\nDespite the increasing relevance of XAI, its application within the HAR domain remains largely underexplored. To the extent that XAI methods have been studied for HAR, these focus primarily on three methods: Shapley Additive Explanations (SHAP) [18, 19], Class Activation Mapping (CAM) [20] and its rectification Gradient-weighted Class Activation Mapping (GradCAM) [21]. Beyond this, there remains a noticeable gap in the literature regarding the systematic comparison of these methods in the HAR field. Specifically, no work has been done to assess which explainability approach is better suited for different types of HAR models and datasets. Further, there is a limited understanding of how different explainability methods vary in their outputs. This gap presents a significant challenge, as the effectiveness and interpretability of XAI methods can vary significantly and depend on the context and model architecture in which they are used. For example, SHAP provides insights into each feature's importance but may fail to capture the spatial and temporal dynamics in the data. GradCAM, on the other side, can highlight important regions for the model but falls short in explaining the contribution of the individual input features. The lack of a comprehensive comparison of these methods with different datasets and HAR model types poses a challenge in identifying the most appropriate methods for model interpretation. Furthermore, clinicians and researchers should be provided with a quantitative comparison of these two XAI methods to asses which is best suited for their individual HAR model, dataset, and use case. Understanding what to expect from each XAI method regarding explanation quality, transparency, and adaptability is essential to making informed decisions. Hence, a systematic comparison of these two XAI methods is needed. This can pave the way for trustworthy HAR models that can be safely and effectively implemented in real-world applications."}, {"title": "1.  1 Contributions", "content": "\u2022 Quantitative comparison of SHAP and GradCAM, in the context of HAR with GCN, on skeleton data from two real-world datasets.\n\u2022 Comparative analysis of the two explanation methods, focusing on feature ranking and evaluating the influence of these features through a perturbation sensitivity analysis on predictive performance.\n\u2022 A nuanced understanding of the strengths and weaknesses of the two explanation approaches within the HAR domain."}, {"title": "Related Work", "content": "In the domain of skeleton-based HAR, most XAI techniques rely on saliency-based methods such as CAM [20] or GradCAM [21, 22]. Meanwhile, SHAP, which is widely used in a wide range of ML applications, has received less attention in HAR and was only applied in [18, 19], to the best of our knowledge.\nThe first work using XAI within HAR is the work from [20]. The authors use CAM to visualize important body key points on the two action classes, throwing and drinking water from the NTU RGB+D dataset [2]. However, their investigation is restricted to only a qualitative visual representation of the activated body key points for those two action classes. This limits their applicability and quantitative comparability with other techniques. The same authors expand this approach on a refined architecture in [22], where CAM is used again to produce visual explanations. However, these explanations also lack objective evaluation metrics, making comparing their effectiveness with other approaches difficult. Also, a comparison of the activation maps to the earlier work [20] is missing. It can be stated that the CAM approach does not address the contribution of individual input features, an area where SHAP could offer deeper insights. This may be particularly interesting for the architecture used in [22], where the authors use multiple input branches to process features individually before fusing them into a common stream in the model.\nIn [18], SHAP is applied in a medical context for early cerebral palsy (CP) screening. The model combines video data and secondary features like birth weight, sex, and gestational age to detect fidgety movements, indicating an increased risk for neurological dysfunction if they are absent or sporadic [23]. These features are then classified and combined into a normal and a risk probability for CP. However, the SHAP explanation is limited to the secondary input features, missing the opportunity to explain the primary input features (i.e., the video data). SHAP would also have to be applied to the primary features to properly explain the model, although this represents a greater technical challenge.\nAnother work using SHAP within the medical HAR domain is [19]. Here, the authors use SHAP to explain the contributions of the primary input features on the model output utilizing two real-world datasets. While this work is the first to apply SHAP on the primary input features, the direct comparison with the often-used GradCAM approach is lacking. A comparison between SHAP and GradCAM could provide a more complete evaluation of the individual strengths and weaknesses. This opens up research on how methods like SHAP and GradCAM might complement or outperform each other in different HAR scenarios and datasets."}, {"title": "2.  1 \u03a7\u0391\u0399", "content": "In the domain of skeleton-based HAR, most XAI techniques rely on saliency-based methods such as CAM [20] or GradCAM [21, 22]. Meanwhile, SHAP, which is widely used in a wide range of ML applications, has received less attention in HAR and was only applied in [18, 19], to the best of our knowledge.\nThe first work using XAI within HAR is the work from [20]. The authors use CAM to visualize important body key points on the two action classes, throwing and drinking water from the NTU RGB+D dataset [2]. However, their investigation is restricted to only a qualitative visual representation of the activated body key points for those two action classes. This limits their applicability and quantitative comparability with other techniques. The same authors expand this approach on a refined architecture in [22], where CAM is used again to produce visual explanations. However, these explanations also lack objective evaluation metrics, making comparing their effectiveness with other approaches difficult. Also, a comparison of the activation maps to the earlier work [20] is missing. It can be stated that the CAM approach does not address the contribution of individual input features, an area where SHAP could offer deeper insights. This may be particularly interesting for the architecture used in [22], where the authors use multiple input branches to process features individually before fusing them into a common stream in the model.\nIn [18], SHAP is applied in a medical context for early cerebral palsy (CP) screening. The model combines video data and secondary features like birth weight, sex, and gestational age to detect fidgety movements, indicating an increased risk for neurological dysfunction if they are absent or sporadic [23]. These features are then classified and combined into a normal and a risk probability for CP. However, the SHAP explanation is limited to the secondary input features, missing the opportunity to explain the primary input features (i.e., the video data). SHAP would also have to be applied to the primary features to properly explain the model, although this represents a greater technical challenge.\nAnother work using SHAP within the medical HAR domain is [19]. Here, the authors use SHAP to explain the contributions of the primary input features on the model output utilizing two real-world datasets. While this work is the first to apply SHAP on the primary input features, the direct comparison with the often-used GradCAM approach is lacking. A comparison between SHAP and GradCAM could provide a more complete evaluation of the individual strengths and weaknesses. This opens up research on how methods like SHAP and GradCAM might complement or outperform each other in different HAR scenarios and datasets."}, {"title": "Method", "content": "This section outlines the two explainability techniques, SHAP and GradCAM, which are compared in this study. Furthermore, the model architecture used to predict the skeleton data is briefly described, including at which layer we obtain the gradients for GradCAM; see a visualization in Fig. 1. Finally, the perturbation technique used to evaluate the model's sensitivity is explained, including how specific parts of the architecture are perturbed based on the feature ranking from SHAP and GradCAM."}, {"title": "1 GradCAM", "content": "GradCAM generates an explanation using the gradients from the final convolutional layer of an ML model [24]. To generate explanations, the gradients for class c are computed with respect to the activations of the feature maps in the chosen convolutional layer. Let $y^c$ represent the model's activation for class c before the chosen target layer, corresponding to the model's prediction for class c. The gradients of $y^c$ with respect to each feature map activation $A^k$, where $A^k$ denotes the k-th feature map of the final convolutional layer, are then calculated. These gradients indicate how much each spatial location in the feature map contributes to the class prediction. Next, global average pooling is applied to the gradients, yielding the neuron importance weights:\n$\\alpha_k = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial y^c}{\\partial A_{ij}^k}$,\nwhere Z represents the number of spatial nodes in the feature map $A^k$, and (i, j) are the spatial indices of the feature map. These importance weights $\\alpha_k^c$ reflect the contribution of each feature map to the class prediction.\nThe final localization map, which highlights the relevant regions in the skeleton, is then computed as follows:\n$L_{GradCAM}^c = ReLU(\\sum_k \\alpha_k^c A^k)$"}, {"title": "2 SHAP", "content": "SHAP is a framework for explaining ML model predictions by attributing an importance score to each feature [25]. It is based on the Shapley value from cooperative game theory [26], which measures a player's contribution to a team's overall success. In the context of ML, the Shapley value is adapted to evaluate the impact of each input feature on the model's prediction. The SHAP value $\\phi_i$ for feature i is computed as\n$\\phi_i = \\sum_{S \\subseteq F \\{i\\}} \\frac{|S|!(|F| - |S| - 1)!}{|F|!} [f_{S\\cup\\{i\\}}(x_{S\\cup\\{i\\}}) - f_S(x_S)]$,\nwhere S represents all subsets of features excluding i, and $f_S$ denotes the model's prediction with the subset S. Calculating the SHAP value thus requires systematically evaluating the model with and without each feature present. Performing this calculation for all subsets is computationally expensive (O(2n)), so approximation methods like \u201cKernel SHAP\u201d and \u201cDeep SHAP\u201d are used to speed up the process [25]. These methods also provide estimation methods for obtaining the model prediction in the absence of input features the model was trained on, as detailed in [25]."}, {"title": "3 Perturbation", "content": "Perturbation techniques in XAI are used to analyze model sensitivity by altering input data and observing prediction changes [29]. We use the perturbation technique from [19] to test the two explanation methods and perturb specific parts of the model architecture related to skeleton body key points.\nWe use GCNs as the backbone of our architectures, which adapt convolutional neural networks to graph data, making them suitable for skeleton-based HAR. In a GCN, nodes aggregate information from neighboring nodes, capturing spatial relationships relevant to human movements. This involves multiplying the node feature matrix by a normalized adjacency matrix A, identity matrix I, and a learnable edge importance matrix E. A is then split into Aj, such that A + I = \u2211j Aj. The output is computed as follows [30]:\n$f_{out} = \\sum_j W_j f_{in} (A_j+IA_j^{\\top}E_j)$,\nHere, $f_{out}$ and $f_{in}$ are output and input features. $W_j$, $A_j$, and $A_j$ handle convolution and adjacency matrix normalization, while $E$ (learnable diagonal matrix) has the edge importance for the specific body key point.\nAs in [19], we apply the perturbation by modifying edges $e_n$ in $E$ corresponding to the body key points, using the sorted SHAP and GradCAM values as the importance measure to choose which edge has to be perturbed."}, {"title": "Experiments", "content": "This section thoroughly describes the experiments conducted on the NTU RGB+D and CP datasets. First, we explain the two datasets and outline how they are preprocessed for the model. Then, we give implementation details regarding the GCN model, GradCAM, and SHAP within our framework. Next, we compare how the explainability techniques map importance onto body key points in the skeleton sequences and the overall feature ranking of those. Further, we perform the perturbation experiments to investigate which of the most important body key points, as determined by SHAP and GradCAM, respectively, lead to a more significant decrease in model performance. Also, we correlate GradCAM values from different layers. Finally, we compare the computation time of both explainability methods within our experimental setting."}, {"title": "1 Datasets", "content": "The NTU RGB+D 60 dataset is a 3D human activity dataset widely used for action recognition tasks [2]. It includes 56, 880 video samples, categorized into 60 distinct action classes. We use the skeleton mapping of the videos on n = 19 body key points for our experiments [2]. Our experiments focus on the cross-view (X-View) subset as defined in [2]. This subset divides the dataset based on the recorded camera perspectives. Specifically, camera views two and three are used to form the training set, Dtrain, while camera view one serves as the validation set, Dval. For the scope of this study, we explain three action classes performed by a single individual from Dval."}, {"title": "2 Implementation", "content": "We utilize the best-performing model architecture from [27] for the NTU RGB+D 60 X-View dataset and the architecture from [28] for the CP dataset. In order to compute SHAP values, we apply the \"DeepExplainer\u201c from the SHAP library [25]. The \"DeepExplainer\" algorithm requires a reference dataset representing the underlying data distribution. For this, a random reference set of n = 100 instances is sampled, covering all action classes generated for the NTU RGB+D dataset. Similarly, a reference set consisting of n = 100 randomly selected window samples from both classes is generated for the CP dataset. Since our GPU memory limits the number of background samples that can be processed at once, subsamples of the background (n = 20) are used in each experiment, with the resulting SHAP values aggregated afterward. Given the hardware accessibility of typical medical and research institutions, we recommend this as a practical approach. The GradCAM implementation utilizes the PyTorch registerhook functionality at the specified layer to acquire the gradients and feature maps [34]. All experiments are conducted on a single NVIDIA V100 GPU with 32 GB of memory, using the PyTorch framework (version 2.3.1) [34] and shap (version 0.46.0) [25]. The global random seed for the experiments is set to 1234."}, {"title": "3 Qualitative Explanation on Skeleton", "content": "In Fig. 2 and 3, the results of the SHAP and the GradCAM values are shown when mapped on the skeleton sequence to visualize their contribution at a specific time frame.\nFor the NTU RGB+D skeleton, which is taken from class 6 (Pick up), it can be observed that the GradCAM activations for the TCN and attention activation layer differ from each other. While the GradCAM values for the TCN layer have a higher portion of activated joints, those from the attention activation layer keep a smaller amount of activated joints. This can be explained by the functionality of the joint attention layer compressing the feature map obtained from the TCN layer to focus on fewer important body key points. Interestingly, the SHAP and GradCAM importance mappings do not agree with each other. For instance, in window 40, SHAP attributes higher importance to the upper body, particularly the head and shoulder regions. Meanwhile, GradCAM emphasizes the left limbs, giving them a higher GradCAM value. This contrast highlights the differing perspectives of these two XAI techniques on which body regions are most crucial for the model's predictions.\nFor the CP skeleton analysis, a random infant with CP is selected from the validation set Dval, and both GradCAM and SHAP values are mapped onto the respective body key points. Notably, the two methods do not always align in their activation of body key points. This discrepancy is evident in window 40, where each method highlights different body regions. However, there is an agreement between SHAP and GradCAM activations in the CP dataset compared to the NTU RGB+D dataset, e.g., in window 10, where both methods give the knees a higher activation. This is potentially due to the smaller window size, which offers more thorough explanations without excessive averaging."}, {"title": "4 Perturbation", "content": "We apply the same perturbation approach as in [19]. Hence, we can directly compare the influence of perturbing important or unimportant body key points on the CP and NTU RGB+D datasets. Furthermore, we apply the negated GradCAM values on the TCN layer from the model for the NTU RGB+D dataset. This way, we can also obtain contributions that negatively influence the prediction of the respective class. For the NTU RGB+D dataset, we chose the same perturbation threshold of 0.35% as in [19] to be able to compare the GradCAM results against the reported SHAP values from this study. In Fig. 4, we show the influence of perturbing up to 10 key body points on the prediction performance for the NTU RGB+D dataset for three classes.\nWe compare the perturbation results for the NTU RGB+D dataset when taking the TCN and last activation layers' gradients before the classifier against SHAP. It can be seen that both explanation methods perform better than perturbing"}, {"title": "5 Body key points ranking", "content": "We compare both XAI techniques' body key point ranking for the NTU RGB+D dataset, shown in Fig. 6. It can be seen that SHAP and GradCAM disagree on the ordering of the body key points for the NTU RGB+D dataset. This is likely due to their conceptual difference, where GradCAM gives a more spatial-orientated explanation. Another potential reason for this differing sorting is the averaging technique, where the GradCAM and SHAP values are averaged over the whole sequence. At the same time, SHAP attributes a more general feature contribution to the interactions with other features for a specific body key point. While GradCAM values have a more extensive spread among the different body key points, SHAP, especially for Class 11, has a smaller interquartile range from the median.\nWe apply the same body key point ranking for the GradCAM and SHAP values to one infant with CP and one without CP, shown in Fig. 7. Again, it can be observed that the feature rankings do not agree with SHAP and GradCAM values. Interestingly, the GradCAM values from the frame attention and TCN layers do not agree on their body key point ranking. The same reasons as in Sec. 4.4 can be taken to explain this behavior. As with the NTU RGB+D dataset, the SHAP values have a smaller interquartile range from the median, indicating a higher certainty for the ranking. While SHAP gives the highest importance for the infant with CP to the right and left knee with SHAP, the GradCAM values are also the highest in those two body key points for the attention activation layer. For No CP, the most critical features, based on the median value, are the right index finger and right knee, which also align with the TCN GradCAM values."}, {"title": "6 GradCAM in Early Layers", "content": "To demonstrate the shortcomings of GradCAM in attributing individual input features, we conduct a correlation experiment where GradCAM values are computed for different GCN layers, as shown in Figure 1. We assess the correlation of GradCAM values in 1) initialization layer $L_{in,i}$, 2) TCN layer $L_{TCN,i}$, and, 3) attention activation layer $L_{Att.,i}$ of input branches i to reference values in the attention activation layer of the main branch. We also compute correlation for the sum of input branches ($L_{in,,} L_{TCN,,}$ and $L_{att.,}$, respectively) and include in the analysis the TCN layer of the main branch m ($L_{TCN,m}$).\nSimilar to [21], layerwise GradCAM values are normalized into the range [0, 1] before Spearman's rank correlation coefficient \u03c1 is computed. Our results in Table 1 show that, for the NTU RGB+D dataset and the CP dataset, GradCAM"}, {"title": "Runtime", "content": "Table 2 lists the computation time for GradCAM and SHAP within our environment. We show the time until the individual algorithm arrives at its final explanation. The GradCAM method is computationally faster than the SHAP method for both datasets due to its lower computational complexity and fewer required calculations. GradCAM only"}, {"title": "Discussion", "content": "Our comparison of SHAP and GradCAM within HAR demonstrates that both methods fundamentally differ in their methodology and the outcome of the final explanation. While they both have unique strengths and weaknesses, their applicability depends heavily on specific needs, requirements, and use cases, which we will elaborate on.\nUnderstanding the model's decision-making process is crucial in CP diagnosis or treatment planning. SHAP can be used to provide a feature-level explanation, giving clinicians insight into how much each biomechanical feature of each body key point contributes to the model's final decision. This type of information is highly valuable in clinical settings where practitioners need clear, interpretable reasons behind model outputs. For example, knowing that certain features like acceleration or velocity are more influential in a diagnosis might help to guide the focus on these particular features. GradCAM, on the other hand, provides spatial interpretation, which can be helpful in tasks such as reckoning important body key points in the recording at a specific time frame. It highlights regions of high activation in the model's chosen target layers, which correspond to body key points in the skeletal structure. This explanation is valuable when clinicians need to visualize which body regions are emphasized, though it lacks the granularity of individual feature importance that SHAP provides.\nOne significant advantage of SHAP is its ability to depict the importance of opposing features. With SHAP, it is possible to identify which body key points, in combination with the input features (J, V, B, A), are essential for the model's"}, {"title": "Conclusion", "content": "This paper systematically compared SHAP and GradCAM within the context of GCNs and HAR using skeleton data from two real-world datasets. Our findings highlight that these two explainability methods differ in their approach and output, each offering different benefits depending on the specific requirements of the individual use case.\nSHAP provides a detailed, feature-level explanation, making it practical in scenarios where it is crucial to understand each individual body's key points' contribution in combination with the input features. It offers a more profound, finer view of how specific features impact the model's decisions, which is essential for trust and accountability in sensitive domains. This may be required in healthcare applications such as CP prediction, where decisions must be transparent and explainable to clinicians. However, its computational intensity and longer runtime make it less suitable for real-time applications.\nIn contrast, GradCAM provides quicker, spatially oriented explanations, showing which regions of the body are most influential in the model's decision at a specific time frame. Its speed and simplicity make it a better choice for real-time assessments or scenarios where quick and more straightforward visual feedback is necessary. However, its lack of granularity and inability to capture complex feature interactions from earlier layers limits its effectiveness in circumstances demanding deeper insights.\nNevertheless, our study demonstrates that SHAP and GradCAM should be viewed as complementary tools rather than competing methods. SHAP provides the \"why\u201c behind decisions, while GradCAM offers the \"where.\u201c Depending on the specific demands of the task, whether it is the need for quick, spatial understanding or a thorough, feature-level interpretation, these methods can be used in parallel to provide a more complete and interpretable explanation of the model behavior on the dataset. For future work, hybrid approaches that combine the strengths of both SHAP and GradCAM should be explored. We can enhance these systems' safety, transparency, and trustworthiness by tailoring explainability techniques to specific HAR models and datasets, particularly in high-risk domains like healthcare."}]}