{"title": "A Multi-modal Approach to Dysarthria Detection and Severity Assessment Using Speech and Text Information", "authors": ["Anuprabha M", "Krishna Gurugubelli", "Kesavaraj V", "Anil Kumar Vuppala"], "abstract": "Automatic detection and severity assessment of dysarthria are crucial for delivering targeted therapeutic interventions to patients. While most existing research focuses primarily on speech modality, this study introduces a novel approach that leverages both speech and text modalities. By employing cross-attention mechanism, our method learns the acoustic and linguistic similarities between speech and text representations. This approach assesses specifically the pronunciation deviations across different severity levels, thereby enhancing the accuracy of dysarthric detection and severity assessment. All the experiments have been performed using UA-Speech dysarthric database. Improved accuracies of 99.53% and 93.20% in detection, and 98.12% and 51.97% for severity assessment have been achieved when speaker-dependent and speaker-independent, unseen and seen words settings are used. These findings suggest that by integrating text information, which provides a reference linguistic knowledge, a more robust framework has been developed for dysarthric detection and assessment, thereby potentially leading to more effective diagnoses.", "sections": [{"title": "I. INTRODUCTION", "content": "Dysarthria, a motor speech disorder that is connected with abnormalities in respiration, functioning of larynx, direction of air-flow, and articulation, leads to reduced speech quality and clarity. It often occurs in various neurological disorders and is linked to progressive neurological diseases. Speakers with dysarthria, hence, face difficulties in communicating and in maintaining social con-nections [1]. Therefore, identifying and monitoring the progression of dysarthria becomes crucial for delivering appropriate therapies. Generally, dysarthric detection and severity assessment are carried out by Speech Language Pathologists (SLPs) through a series of severity assessment tests [2]. To enhance efficiency and minimize human error in these subjective assessments, it is recommended to use readily available, user-friendly objective detection and severity assessment methods [3].\nIn such objective assessments, it is imperative to extract meaning-ful features from speech. To focus on different aspects of speech such as articulation, perception, speech quality and prosody, fea-tures like MFCC, mel-spectrogram, perceptually enhanced single frequency cepstral coefficients (PE-SFCC), fundamental frequency (f0), formants, jitter, shimmer have been used [4]\u2013[6]. Features from pre-trained Automatic Speech Recognition (ASR) models such as Wav2Vec, Hubert and Whisper have also been shown to provide improved performance as they include information about linguistic contents that pertain to speech intelligibility [7], [8]. On the other hand, Tripathi et al. [9] have utilized posterior probabilities from DeepSpeech-ASR model for severity classification. Deep Neural Network based classifiers including Convolutional Neural Networks (CNN), Gated Recurrent Units (GRU) and Long Short-Term Memory networks, have demonstrated improved performance compared to traditional machine learning classifiers as they serve as both feature extractors and classifiers [10].\nIn the investigations mentioned above, speech is the most com-monly used modality for detection and assessment of dysarthria.\nGiven the challenges involved in collecting speech data from dysarthric speakers, it is essential to leverage additional information from various modalities, if available. Apart from speech features, video-based approach [11], utilizing kinematic features obtained from lip movements have also been reported to perform efficiently in detection of hypokinetic dysarthria. Another study by Xue et al. [12] uses text based phonetic level measures such as accuracy of phoneme detected and phonetic distance, to measure the articulation patterns of dysarthric subjects.\nWith advancements in speech technology, multi-modal systems have become more effective and robust at capturing complex, high-level information even when the data available is limited. Multi-modal approaches are recently being used in Speech Emotion Recognition tasks to predict emotional states by combining both audio and text modalities using cross attention [13]. Studies proposed by Tong et al. [14] and Liu et al. [15] utilize visual features to jointly learn audio and visual information in assessing dysarthric severity.\nDrawing inspiration from the aforementioned studies, this paper proposes a novel multi-modal method to leverage the words uttered by dysarthric speakers, in the form of text, in addition to speech. Incorporating text features in dysarthric assessment provides a lexical reference or a ground truth representation for standard pronunciation. This lexical reference allows the model to learn more distinguishing representations compared to traditional acoustic features, thus leading to improved performance [16]. Moreover, the linguistic information embedded in the text helps the model to capture effectively, the pro-nunciation errors associated with difficult-to-articulate words, thereby improving accuracy in detection and severity assessment. This multi-modal approach thus enables a more comprehensive evaluation of speech intelligibility, leading to more precise diagnoses and targeted therapeutic interventions. To the best of our knowledge, this is the first time, speech and text information have been combined together for dysarthric detection and severity assessment. The main contributions of this work are summarized as follows:\n\u2022 Proposed a novel methodology wherein text is used as a modal-ity in addition to speech, for dysarthric detection and severityassessment.\n\u2022 Extensive systematic investigations have been conducted tocompare models trained solely with speech features and withboth speech and text features.\n\u2022 The models are assessed on UA-Speech dysarthric database [17]in speaker-dependent (SD) and speaker-independent (SID), seenand unseen words settings.\n\u2022 This work analyses the contribution of different word groupsfrom the UA-Speech database and highlights how text featurescontribute to the interpretability of the model's decision, thuss providing insights into how text and speech interact in the context of dysarthria detection and severity assessment.\nThis paper is structured as follows: Section II explains the proposed approach, Section III introduces the experimental setup along with"}, {"title": "II. PROPOSED APPROACH", "content": "In this section, we propose a novel multi-modal framework that integrates both speech (S) and text features (T) to improve accuracies in dysarthric detection and severity assessment. The core idea is to model the distribution $P(C|S,T)$ to predict the severity class (C) using a cross-attention mechanism, thereby capturing the intricate dependencies between speech and text that are crucial for identifying dysarthria. This section outlines the motivation and mathematical for-mulation of the proposed approach, followed by a detailed description of the architecture.\n$\n$\\ Problem Formulation\nThe proposed framework is designed to improve dysarthric detection and assessment by leveraging the complementary strengths of both speech and text modalities. Integrating text information provides a crucial reference for how the ideal utterance should be articulated [16]. This enables the model to detect deviations (pronunciation errors) caused by dysarthria. The decision rule of the detection and assessment tasks is defined as follows:\n$C^* = arg max{P(C | S,T)}$\nwhere $P(C|S,T)$ represents the posterior probability for estimating the severity class given both acoustic and linguistic informations. Using Bayes' theorem,\n$C^* = arg max\\limits_{C}{\\frac{P(S,T | C) \\cdot P(C)}{P(S,T)}}$\n$C^* = arg max{P(S,T | C) \\cdot P(C)}$\nwhere $P(S,T|C)$ represents joint-likelihood distribution and $P(C)$ represents prior probability distribution. From Equation (3) the deci-sion rule can be rewritten as,\n$C^* = arg max{P(S | T, C) \\cdot P(T | C) \\cdot P(C)}$\nThe conditional distributions $P(S|T, C)$ and $P(T|C)$ can be used to find the posterior probability $P(C|S,T)$, for the detection and assessment tasks. These probabilities can capture how the acoustic realization of speech deviates from the expected norm provided the keyword phrase. Since S and T are conditionally dependent for each keyword, modeling the joint-likelihood $P(S,T|C)$ becomes more intricate. This can be addressed using multivariate Gaussian distribu-tions or multi-modal neural networks. In the proposed multi-modal framework, inputs S and T are processed by distinct encoders and their outputs are jointly processed using a cross-attention mechanism. This mechanism explicitly captures the dependencies between the two modalities before computing the class logits, enabling the model to leverage the interactions between S and T for improved classification.\n$\\ Proposed Architecture\nTo model the likelihood distribution $P(S, T|C)$, we propose an archi-tecture as shown in Fig. 1, that consists of three main components: a speech encoder, a text encoder, and a cross-attention layer. Together, these components learn a shared representation that captures the similarity between the acoustic features of speech and the linguistic representations of text.\n$\\ Speech Encoder\nThe speech encoder receives time-frequency representation of speech in the form of mel-spectrogram which essentially captures the spectral patterns associated with the speech characteristics [4]. The CNN layer and bi-directional GRU (Bi-GRU) are employed to capture low-level features and overall temporal variations present in the speech signal. In this way, the speech embeddings obtained from the speech encoder provide information about the subtle variations in how the word is articulated as well as the overall challenges in uttering the word. These speech embeddings are denoted by the conditional distribution $P(S|T, C)$ which allows the speech encoder to learn diverse speech patterns irrespective of the severity.\n$\\ Text Encoder\nThe text encoder receives character level sequences for any given word. To obtain character level tokenization for processing iso-lated words, an embedding layer with a vocabulary size of 26 (corresponding to the number of distinct characters in English) is employed. These character level embeddings are processed by Bi-GRU to obtain a fixed-dimensional representation, regardless of the number of characters present in any keyword. Text embeddings, thus obtained, provide a unique representation for the character sequence present in each keyword. These text embeddings act as a linguistic representation of speech for each keyword against which speech utterances belonging to different severity levels are compared.\n$\\ Multi-modal Classifier\nCross-attention mechanism [18] is employed to obtain a shared representation between speech and text embeddings, and is modeled by the likelihood function $P(S,T|C)$. The final representation thus obtained retains the cues relevant to speech severity ranging from healthy to high level of dysarthric severity, depending on the task. Finally, necessary dense layers and activation functions are employed to carryout detection and severity assessment tasks."}, {"title": "III. EXPERIMENTAL SETUP", "content": "This section describes the database used, details about the different settings and the implementation details related to the model training."}, {"title": "A. Database", "content": "The experiments in this study are conducted using UA-Speech dysarthric database in English [17]. This database contains speech samples from 15 dysarthric speakers with cerebral palsy (CP) and 11 healthy speakers. The database contains isolated utterances of digits (10), radio alphabets (26), computer commands (19), common words (100) from the Brown corpus and uncommon words (300) selected from Project Gutenberg. There are 3 blocks in the database, each block has 100 uncommon words and 155 words from other cat-egories mentioned above. Based on the speech intelligibility ratings provided by native listeners, the severity is categorized into 4 groups; very low, low, medium and high. The speech samples recorded from array 6 sampled at 16 kHz are used for the analysis. In the database, duration of the speech samples of dysarthric speakers are varied from less than 2 seconds to greater than 18 seconds, depending on the severity of the speakers. For this experiment, samples with duration up to 10 seconds are taken after trimming silences from the leading and trailing ends. For this study, mel-spectrograms generated by extracting 80-dimensional mel-filter bank coefficients from speech for every 10 ms using a 25 ms window, are used as speech features with dimensions (N x 80), where N is the number of frames in each speech sample."}, {"title": "B. Comprehensive Details of Different Settings", "content": "To study the effect of incorporating text knowledge, both detec-tion and severity classification tasks are carried out using speaker-dependent and speaker-independent settings, as well as seen and unseen words settings, namely, SD, SID-1 and SID-2. To include different articulation patterns [17] in training the models, randomly sampled unseen words are included in the train set. For each of these settings, the distribution of common and uncommon words in the training and test sets are detailed in TABLE I. For detection task, 11 healthy speakers and 15 dysarthric speakers have been considered. 26 models are trained for each SID setting by following Leave One Speaker Out (LOSO) cross validation approach. For severity assessment task, as the speaker distribution is uneven across the classes, 8 speakers are considered for training and remaining 7 speakers are used for testing. This methodology has been adapted from [19] to ensure equal number of speakers across severity levels during training. Speech-only detection and assessment models are also employed to compare with the performance of the proposed multi-modality detection and assessment models."}, {"title": "C. Implementation Details", "content": "The speech encoder shown in Fig. 1 consists of two 2D CNN layers to process 2 dimensional mel-spectrograms. To prevent over-fitting and to ensure stable training, a dropout layer of 0.2 and batch normalization are applied after each CNN layer. Two Bi-GRUs (dimension of 64) followed by dense layers are applied in both speech and text encoders to obtain fixed dimensional speech and text features, which are then passed on to the cross-attention module. In cross-attention setup, speech embedding acts as both the key and value, while the text embedding acts as the query. The context vectors obtained from cross-attention are processed by GRU (dimension of 64), and dense layers (128 and 32). Finally, a sigmoid activation function is applied to detect dysarthric speech, while a softmax activation function is employed to assess severity levels (very low, low, medium and high) present in the UA-Speech database. To obtain predictions for speech-only model, speech encoder alone is considered. Conversely, for multi-modality model, the predictions are evaluated after applying cross-attention.\nBinary and categorical cross-entropy loss are utilised as training criteria for binary and multi-class classification, respectively, in conjunction with Adam optimizer with initial learning rate of $10^{-4}$. To dynamically adjust the learning rate, ReduceOnPlateau strategy is applied with patience parameter of 5 epochs. In addition, an early stopping mechanism with a patience of 3 epochs is also applied to prevent over-fitting while training. For training, a NVIDIA Ge-Force RTX 2080 Ti GPU has been used."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "This study investigates the robustness of the proposed multi-modality approach for dysarthric detection and severity assessment. Extensive experimental analyses have been conducted across various settings, as detailed in TABLE I. The experiments utilized the UA-Speech dataset, described in Section III, and the results are presented in terms of accuracy."}, {"title": "A. Comparative Analysis of Speech-Only and Speech-Text Modalities", "content": "This study compares speech-only and speech-text modalities in performance assessment of two tasks: dysarthric speech detection and severity assessment, across various experimental settings (as detailed in TABLE I). The results, presented in TABLE II, demonstrate that in SD setting, incorporating text features yields an absolute improvement of 4.15% and 0.62% in accuracy, for detection and assessment tasks, respectively, compared to the speech-only modality. In the SID-1 setting which is preferred by clinicians for diagnosing new patients, the speech-text modality shows an absolute improve-ment of 3.59% and 2.83% in accuracy, for detection and assessment tasks, respectively. This clearly indicates the model's ability to learn dysarthric related information even when it is tested on unknown speakers using seen words. Conversely, when the model is tested using unseen words, in the SID-2 setting, a performance decline of 2.46% in detection accuracy is observed, in the speech-text modality. Interestingly, for severity classification using the same SID-2 setting, an improvement of 2.71% in accuracy is observed which suggests enhanced generalization to both unknown speakers and unseen words."}, {"title": "B. Performance Assessment of Proposed Framework Across Diverse Word Groups", "content": "The performance of the proposed framework may be influenced by the specific input text information. Therefore, we conducted a detailed analysis of the framework across different word groups and the findings are presented in TABLE III. The results highlight that while the performance trends of both the speech-only and speech-text mod-els are consistent across different word groups, the multi-modality approach consistently outperforms the speech-only model in terms of accuracy due to incorporation of text features as discussed earlier. For instance, considering common and uncommon word groups, the absolute improvement in accuracies for the multi-modality model compared to speech-only model are 3.91% and 3.16% for detection, 0.32% and 3.02% for severity assessment, respectively.\nFor a given modality, across the various word groups indicated in TABLE III, a marginal variation in performance has been observed. This can be attributed to the specific characteristics of dysarthria as well as the phonetic properties of the individual word groups them-selves. Detection accuracy is notably higher for common words, digits and computer commands which are likely to be easier for dysarthric speakers to articulate due to their familiarity and reduced articulatory demands. Even words that need less articulatory movements provide sufficient information for the model to effectively distinguish between healthy and dysarthric speakers. This is clearly shown in terms of the absolute improvement of 4.67% in accuracy for common word group compared to uncommon word group, for the multi-modality model.\nUnlike detection, for severity classification, the multi-modality model performs better for uncommon words such as 'advantageous' and 'boulevard' in the UA-Speech database. These words require more complex articulatory movements [20] and are challenging for dysarthric speakers to pronounce due to potential pauses, repeating phonemes, and other complexities. In such a scenario, the model exhibits an absolute improvement of 9.02% in accuracy compared to the common word group. This can be attributed to the fact that the model leverages the linguistic information embedded in the text to better capture and analyze the pronunciation errors associated with these difficult-to-articulate words, thereby improving severity classification accuracy."}, {"title": "C. Comparison with State-of-the-Art Systems", "content": "The performance of the proposed system is benchmarked against various state-of-the-art systems, with results presented in TABLE IV. To ensure a fair comparison, we selected models evaluated under similar training and testing conditions. In severity assessment, when compared with SECNN model trained with spectrograms [19], the proposed multi-modality model shows an absolute improvement of 0.54% and 0.19% in terms of accuracy for both SD and SID. Also in detection, compared to model trained with Wav2Vec features [7], proposed model shows an absolute improvement of 1.72% for SID setting. The comparison clearly demonstrates that the proposed speech-text multi-modality models outperform the speech-only mod-els. This improvement is attributed to the additional linguistic context provided by the text features, which enhances the model's ability to detect dysarthria and assess its severity by analyzing deviations in pronunciation, effectively."}, {"title": "V. CONCLUSION", "content": "This study demonstrates that the proposed multi-modality model, which integrates both speech and text, provides a significant advan-tage over speech-only model in detecting and assessing dysarthria. By leveraging the complementary strengths of both acoustic and linguistic knowledge, this multi-modality model achieves greater accuracy and consistency, particularly when dealing with the complex phonetic properties associated with dysarthric speech. Notably, in speaker-independent settings which are more preferred in clinical environment, this model shows higher accuracies of 93.20% and 51.97% in dysarthric detection and severity assessment, respectively. Additionally, this study provides insights into the influence of articu-latory patterns having different difficulty levels, on the performance of the model. In the future work, the focus will be on employing different strategies to combine various speech and text representations towards developing more robust models for diagnosis of dysarthria."}]}