{"title": "Fortify Your Foundations: Practical Privacy and Security for Foundation Model Deployments In The Cloud", "authors": ["Marcin Chrapek", "Anjo Vahldiek-Oberwagner", "Marcin Spoczynski", "Scott Constable", "Mona Vij", "Torsten Hoefler"], "abstract": "Foundation Models (FMs) display exceptional performance in tasks such as natural language pro-cessing and are being applied across a growing range of disciplines. Although typically trained on large public datasets, FMs are often fine-tuned or integrated into Retrieval-Augmented Genera-tion (RAG) systems, which rely on private data. This access, along with their size and costly train-ing, heightens the risk of intellectual property theft. Moreover, multimodal FMs may expose sensitive information. In this work, we examine the FM threat model and discuss the practicality and comprehensiveness of various approaches for securing against them, such as ML-based meth-ods and trusted execution environments (TEEs). We demonstrate that TEEs offer an effective bal-ance between strong security properties, usability, and performance. Specifically, we present a solu-tion achieving less than 10% overhead versus bare metal for the full Llama2 7B and 13B inference pipelines running inside Intel\u00ae SGX and Intel\u00ae TDX. We also share our configuration files and insights from our implementation. To our knowl-edge, our work is the first to show the practicality of TEEs for securing FMs.", "sections": [{"title": "1. Introduction", "content": "Foundation Models (FMs) are dominating the machine learn-ing (ML) landscape. Exemplified by Large Language Mod-els (LLMs), such as GPT-4 (OpenAI et al.), GPT-3 (Brown et al.), Llama (Touvron et al., a), and Llama2 (Touvron et al., b), they displayed impressive in-context learning abilities. FMs such as LLMs achieve human-like capabilities that have revolutionized many ML tasks (Awais et al.; Zhao et al.) and have been successfully applied to disciplines relying on confidential user data such as healthcare (Sallam), finance (Wu et al., b), sentiment analysis (Araci), legal cases (Cui et al.), and document translation (Kocmi & Federmann). Simultaneously, the ever-increasing size of FMs has changed their deployment strategies. FMs reach billions to trillions of parameters, necessitating state-of-the-art hardware to achieve reasonable performance. However, such hardware is frequently unavailable or too expensive for small developers. This results in models not being deployed locally but offered as a service (Ribeiro et al.) by major cloud service providers (CSPs). However, CSPs have an interest in using the data users provide for model training. Imagine a situation where you upload health-related docu-ments after an accident for an insurance claim. The insurer uses an LLM deployed in the cloud to parse your data, put it into the correct format for a database, and check for ab-normalities. In a couple of weeks, you open a publically available LLM to realize that after inputting a certain se-quence of characters (Carlini et al.; Patil et al.), your name, address, social security number, prior health history, and accident details are visible. Someone stole your data, or the CSP or model owner used it for training. Such a drastic shift in data interest compared to previous ML models (Xue et al., b) introduces new types of adversaries we discuss in Section 2.\nSuch new adversaries create new threats. The right side of Figure 1 shows example attacks that FMs need to be shielded from. Protection against these threats is becoming increasingly important for FMs, which become abundant in companies and everyday life in a growing number of domains with access to multimodal (Wu et al., a) user con-fidential data. This trend reflects a broader shift in focus toward privacy and security among governments, compa-nies, and users (Goldfarb & Tucker; Petrescu & Krishen; Voss). Apart from confidentiality, theft is another threat to FMs. The cost of obtaining the necessary datasets and engineering considerably increases the IP value of FMs. Training and fine-tuning alone can cost tens of millions of dollars (Sharir et al.). Any security breach involving FMs, including the leak of confidential data or IP, is becoming increasingly costly for CSPs (e.g., Azure, AWS, Google Cloud), model providers (e.g., Meta AI, OpenAI), and end-users (e.g., banks, hospitals). As more companies enter the space of personalized AI (e.g., Meta's AI studio or Adobe Creator), we already observe backlashes from users inter-ested in their data (Ng), making such threats tangible and requiring immediate attention.\nThe ML community approached the security and privacy issues associated with third-party evaluated DNNs (Xue et al., b; Knott et al.; Dowlin et al.) using mechanisms such as watermarking or user authentication. A practical alternative to these methods can be trusted execution envi-ronments (TEEs). TEEs promise strong security features at the expense of workload-dependent performance. We discuss these and other approaches in Section 3. We show that TEEs provide strong, measurable security properties for FMs against the threats important to industry companies deploying models in the cloud, in line with the model from Section 2. Furthermore, besides providing security and pri-vacy, TEEs can be leveraged to assure model-related proper-ties, such as accuracy assurances or dataset content verifica-tion. We introduce in Section 4 a specific flow for providing security and privacy to FMs deployed within TEEs.\nWhile TEEs provide strong properties, we investigate whether they matured enough to be a practical solution for ML practitioners interested in protecting their FMs. Past studies (Mo et al.; Akram et al.) have pointed to two issues with using TEEs: programming difficulty and performance overheads. We address both of these in our work. As we show in Section 5, we implement an FM inference pipeline within TEEs leveraging virtual machines (VMs) and library operating systems (OSs) such as Gramine (Tsai et al.). We open-source our setup for others to leverage and share the insights we learned throughout the process. To address the performance issues, we run an entire Llama2 inference pipeline within TEEs and, for the first time to the best of our knowledge, present performance numbers for such setup in Section 6. The left part of Figure 1 displays our example performance results, showing that TEEs incur only 4-7% throughput reduction as compared to up to 100s% reported in the literature (Akram et al.). Finally, we also discuss training, GPU support, and the choice between different types of TEEs in Section 7."}, {"title": "2. FM threat model", "content": "Three main actors of interest are a part of the modern FM ecosystem: CSPs, model providers, and users. CSPs (e.g., Azure, GCP, AWS) usually only offer a service where mod-els can be deployed and rarely work on FMs. Most FMs are developed by model providers with two forms: AI-focused (e.g., OpenAI, Anthropic) or non-AI-focused (e.g., bank, hospital, insurer). The former focuses on building and of-fering users proprietary general models (e.g., GPT4) as a website or API. The latter usually leverages public models (e.g., Llama) to build case-dependent proprietary models offering users specific services or interfaces to use them. This is usually achieved by fine-tuning the public models or creating Retrieval-Augmented Generation (RAG) solu-tions based on the company's proprietary and sensitive files, which are added to the FM query to provide additional con-text. From an economic perspective, it is not worth training the model from scratch for these non-AI-oriented compa-nies. We focus our work predominantly on threats to such private company deployments on the cloud infrastructure as these have access to more confidential data than gen-eral AI-focused companies. The users in such a case can be private individuals (e.g., insured persons or patients) or company employees (e.g., internally deployed HR model). We discuss other deployment possibilities in Section 4.3.\nExamples of deployments that we focus on are a bank run-ning an LLM parsing client statements to provide insights (e.g., how much did I spend on groceries this month), an in-surer checking medical bills for abnormalities, or healthcare-provider parsing documentation (e.g., for personalized med-ication). Such high-value industries need IP protection for their deployed models, as not only do they constitute a competitive advantage, but they also may contain company secrets and user data. Even if the model used has not been fine-tuned, as is a vanilla public model, these industries require the confidentiality of user data. Because of these needs, the aforementioned industries cannot leverage ef-ficient CSP scaling of advances in FMs. Promises from the CSPs that the data will not be used for other training or that the models will not be investigated are frequently insufficient guarantees.\nIn the above setting, we differentiate between three types of threat surfaces and their corresponding adversaries. The first two are connected with malicious actors trying to steal the model, steal user data, or disrupt the service. The third is associated with the dishonest and organized operation of the CSP. From the perspective of the model provider, the only trusted entity is the hardware itself. The end user trusts the model provider. This assumption is reasonable, as end users already do so for institutions such as banks and hospitals or their employers. The operating system, the network, the system administrators, and the CSP are not trusted. We present an overview of these threat surfaces and adversaries in Figure 2. We focus only on inference and discuss training in Section 7.3. Finally, we do not consider a distributed setup with multiple computing hosts (Ben-Nun & Hoefler). This considerably increases the trusted computing base (TCB) and is usually unnecessary for the inference we focus on.\n1 Eavesdropping: The first threat surface represents ad-versaries that try to eavesdrop on either the models or the user's confidential data. These adversaries are usually privi-leged in some way by either being employed by the CSP or having unauthorized access to the cluster. However, they are malicious and do not act in line with the CSP. An example of such attackers would be a rogue system administrator or another tenant within the system. They can obtain the weights or prompts by locating themselves within the net-work or the system. Furthermore, such adversaries could try to extract sensitive user data from the model by prompting it in the background within the cloud. Finally, the adversary might use certain input/output pairs of the model as their private training data. All these thefts offer potentially large gains with relatively low risks, as detecting such actions at the scale is nontrivial. We do not consider whole model extraction attacks (Tram\u00e8r et al.) as a big threat as such attacks scale poorly for large models such as FMs.\n2 Tampering: The second type of threat surface corre-sponds to adversaries not interested in stealing the models but disrupting the service through modifying critical data. Similarly to the first type, they are also malicious and do not act in line with the CSP. Again, examples include system administrators and other malicious software on the system that is trying to disrupt or hinder the program. They might tamper with the model's weights randomly to lower the pre-diction accuracy and change the output or the user context, which usually results in service disruption.\n3 CSP violations: While similar to the second type, we specified the third type of threat surface separately due to the uniqueness of the organized approach of the actors op-erating on it. Unlike the prior two, the third adversary is not malicious. It is the CSP that is interested in violating different contract agreements by, for example, modifying the deployed model or using the data to train other models. The goal of the modifications might be to improve the per-formance of their runtimes and save money (e.g., automatic quantization) or reduce the accuracy to make the users mi-grate to better and newer models. Such modifications with-out the consent of the users happen in practice (Chen et al.). This adversary is new and did not exist in the prior ML models as their data was rarely in a unified format, and their runtimes were considerably smaller and faster, not requiring as much optimization."}, {"title": "3. Methods to protect FMs", "content": "Broadly, three approaches can be applied to protect against security threats in FMs: ML methods, cryptographic meth-ods such as Homomorphic Encryption (HE) and multi-party computation (MPC), and Confidential Computing (CC) (Mulligan et al.).\nAs noted in the literature (Xue et al., b), current ML IP protection methods lack in the space of actively protect-ing against model theft and instead focus predominantly on model verification and passive protections of already stolen models. The task there is to determine whether an output is coming from a given predefined model. One of the approaches is to use signatures embedded in the model to then submit multiple inputs and using outputs verify that the model is the one that was promised (Lao et al.). This partially covers threats two and three from our threat model. Other methods include approaches such as passport (Fan et al.) or backdoor (Xue et al., a) based user authentica-tion, and watermarking (Szyller et al.; Boenisch), in which a watermark is included in the model's output or weights, allowing for ownership verification.\nWhile these protect against certain attacks, the threat model for ML methods does cover the threats we show in Section 2. Most importantly, they do not provide exhaustive and mea-surable security properties, making it risky for companies to rely purely on them, considering the cost of losing confiden-tiality or theft of IP. Additionally, ML methods have other crucial issues. They frequently require expensive FM re-training, change the accuracy of the model, do not secure the confidentiality of user prompts (Xue et al., b), and cannot be combined together (Szyller & Asokan). Cryptographic approaches such as HE and MPC address these issues with strong cryptographic protocols.\nHE allows to conduct mathematical and logical operations on encrypted data without decrypting it (Acar et al.). HE has been explored in the context of DNNs (Dowlin et al.; Lee et al., b; Wood et al.). However, with the exception of a few structured examples (Chrapek et al.; Burkhalter et al.), the current state-of-the-art HE is not practical. HE approaches do not provide integrity protections (threats two and three). HE operations on encrypted data can also have up to 10,000x performance and size overheads, taking min-utes to conduct simple MNIST inference (Dowlin et al.) and making FM inference intangible. MPC is close to HE and has similar practicality issues but involves multiple comput-ing parties (Viand & Shafagh).\nCC offers a more practical and time-tested alternative in the form of TEEs by approaching the problem using strong se-curity primitives implemented in hardened hardware. Com-pared to HE and MPC which rely on obscuring the data and functions, TEEs offer a secure and isolated environment frequently called an enclave. Users can verify enclaves in a secure, hardware enabled process called attestation. TEEs ensure the confidentiality and integrity of a running program and its data, and protect against external and privi-leged attackers such as system administrators. TEEs ensure these adversaries cannot access or modify the contents of the memory of the running programs (Sabt et al.) such as the weights or user confidential data, mitigating risks on the user side and reducing responsibility on the CSP side. TEEs have been implemented by the academic com-munity and industry (Schneider et al.). Only the latter are available widely on CSP platforms with examples such as AMD's Secure Encrypted Virtualization-Secure Nested Pag-ing (SEV-SNP) (Kaplan), Intel's\u00ae SGX (McKeen et al.; Hoekstra et al.; Costan & Devadas) and TDX (Cheng et al., a), ARM's TrustZone (Pinto & Santos) and CCA (Li et al.). All of these are CPU-based, but accelerators are also enter-ing the space with notable example of Nvidia (Nertney). We discuss their current status in Section 7.1.\nAs we show in Section 4, TEEs outperform ML methods by providing real-time, strong, measurable, and active protec-tions against adversaries without any model modifications, ensuring that the model, data, and runtime environment re-main secure and tamper-proof. While HE and MPC provide confidentiality, TEEs provide more than just that. They offer integrity checks, attestation, and runtime protections, all of which are absent in HE and MPC. TEEs are more suitable for scenarios where performance (Section 6), real-time in-ference, and ease of use are essential, such as in healthcare, finance, and cloud-based Al services.\nTEEs have been investigated in the past for protecting ML models (Mo et al.). Yet most of these approaches offload only parts of the models to the TEE, usually providing weaker notions of security and claiming the low TEE per-formance (hundred times slowdown (Akram et al.)) as the reason. For example, Slalom (Tram\u00e8r & Boneh) would offload all linear layers to the GPU with a probabilistic algo-rithm guaranteeing some security. Such an approach does not resolve the accessibility issue of TEEs and makes it even harder to use with ML models. Furthermore, none of these previous works explored FMs and focused on simpler models, such as VGG16 or MobileNet, as the necessary model changes are large.\nWe address these shortcomings and, compared to previous approaches securing only certain model stages, show that offloading whole FMs to modern TEEs is practical. In Section 4, we address TEE accessibility, show the exact control flow of how to implement an entire pipeline for FMs in TEEs, and discuss the provided security. In Section 5 we describe our implementation and the tools we leverage. In Section 6, we show the performance of our approach and the cost of the achieved security, which is similar or lower to the one noted in the literature."}, {"title": "4. Establishing trust in the FM deployment", "content": "We cannot rely on techniques suggested in prior work (Lee et al., a) to establish trust in the deployment since the in-volved parties are more complex and deployment options differ. The majority of CSPs now offer TEE-based instances, sometimes referred to as confidential VMs (CVMs). While deploying a TEE on a CSP infrastructure can be done with a click of a button, the whole purpose of using a TEE is lost unless it is verified to be a true TEE. As mentioned earlier, TEEs support attestation (Birkholz et al.) that allows them to prove to a remote party that they are indeed a TEE with the correct software state using a generated quote. The verifier can check whether the TEE signer is an actual TEE, and the hash of the code running in a TEE. The model providers can use the CSP attestation service or an independent third-party verifier like Intel Tiber. We discuss the security aspect of this choice in the second part of this section.\nWe assume there is a model provider who wants to create a service using an FM that is fine-tuned on sensitive pro-prietary data or handles confidential user data that requires protection against the threats defined in Section 2. Figure 3 shows the overview of the steps the model provider can follow:\n1 The model provider requests a TEE instance using the CSPs interface specifying the software that will be run on the instance (e.g., VM image).\n2 The CSP allocates the TEE instance and provides the model provider with the associated IP needed to con-nect to the instance.\n3 The model provider requests an attestation report from the TEE instance, which includes a public key for any subsequent communication between the model provider and the TEE.\n4 The model provider leverages a verifier service to check that the running instance is a valid TEE.\n5 The model provider can verify whether the TEE is run-ning appropriate software and firmware using expected values generated before starting the TEE in the cloud.\n\u2465 The model provider provisions the encryption keys for the TEE, allowing it to load all software and the encrypted model.\n7 The model provider can then expose the TEE instance running the FM as part of their service.\n8 An end-user can open a direct secure channel with the TEE instance and submit inference prompts leveraging a public certificate offered by the model provider.\n9 An end-user can also establish that the service is run-ning in a TEE by verifying the attestation. This can happen transparently during secure channel opening if the users leverage a protocol such as Remote Attesta-tion TLS (RA-TLS) (Knauth et al.). RA-TLS in such a setup requires granting user access to a verifier service."}, {"title": "4.1. Protections against threats", "content": "The above flow protects against the attack vectors defined in Section 2. We discuss how this is achieved and how TEEs offer an advantage over other methods for different threats.\nType one and two adversaries: Type one and two adver-saries are protected against using the principal integrity and confidentiality properties of TEEs. The model (step 6) and users' data (step 3) are protected from access by privileged and non-privileged users of the data center. TEEs prevent unauthorized adversaries from reading plaintext data out of the TEE through, for example, garbling the output or raising memory faults. TEEs also prevent unauthorized adversaries from modifying code/data within the TEE. Some TEEs will simply crash after their data is modified in this manner, no-tifying the user and closing the connection. Because the user uses TLS for communication and optionally conducts a provably secure attestation, no listener on the communica-tion ports can understand the contents of the sent messages, blocking effective tracking of the input and output.\nType three adversary: TEEs can also assert certain as-surances that other methods struggle with. By conducting the attestation mentioned previously and because of TEE's strong integrity protection, the model provider can ensure they deploy the models they intended without any modifica-tions and usage of their data. They compare the expected secure hash value of the agreed model with the secure hash value coming from the enclave. Such comparison provably ensures the CSP and its employees adhere to their part of the agreement and do not modify the models, no matter how expensive they are to process. TEEs also provide privacy of users' prompts, where the attestation protects against data or model leakage, ensuring that users' data is provably not used for training purposes. Such strong protection can be important for legally imposed standards in industries such as healthcare or finance. This allows for a unique method of ensuring the quality of service and eliminates the third type of adversary.\nCombining with other protection methods: Businesses have varying needs and sometimes have deployments sus-ceptible to other threats. In such cases, TEEs have another distinct advantage over ML methods. Existing ML protec-tion mechanisms have been shown to eliminate each other's benefits when combined together (Szyller & Asokan). TEEs do not modify the model and, thus, are not susceptible to such an issue. TEEs can be joined with at least one other mechanism reinforcing other passive or active ML protections that might defend against a specific threat, such as model extraction. We believe TEEs comprise a secure baseline foundation for any FM deployment that does not interfere with other approaches."}, {"title": "4.2. Deployment and threat considerations", "content": "We discuss how, from a practical perspective, the above flow can be leveraged in deploying other services and how the deployment decisions influence its security.\nRAG: The above flow also applies to retrieval-augmented generation (RAG) (Lewis et al.). In RAG, FMs query a document index that responds to prompts. This directly maps to our above flow with exchanged actors. The model provider would follow the steps 1 to 7 to deploy an additional RAG service. The details of this service, such as the communication certificate, are then shared with the inference service. The inference service would open a direct, secure connection with the RAG service. Leveraging remote attestation during deployment, encrypted storage, and secure communication ensures the provided RAG documents are not maliciously changed or stolen, similarly to how we described for an inference service in Section 4.1.\nLoad balancers and gateways: Frequently, the user would not have direct access to an instance running on the cloud but would be located behind a load balancer or a gateway. Such mechanisms could isolate the user from the ability to attest the TEE if the TLS sessions are terminated at these points.\nChoice of verifier service: CSPs offer attestation services for TEEs running in their deployment. To establish trust in these offerings, they should be run inside a TEE, which is attested by a trusted 3rd party. Furthermore, the attestation service implementation needs to be validated. Such offer-ings allow CSPs to handle attestations for TEE deployments on their infrastructure, with 3rd party required only for the few attestations of the TEEs running CSP attestation ser-vice. To further reduce the reliance on the CSP, one can also completely resort to 3rd party attestation services."}, {"title": "4.3. Future directions", "content": "While all the above discussions are on practical systems that can be deployed in TEEs, most of the following para-graphs constitute exciting research possibilities rather than available solutions.\nMicroservices: A frequent method to deploy a pipeline of operations is microservices. In such a deployment model, a result of one microservice is provided to the next, which provides its results to the next, and so forth. Microservices form a graph of operations applied to some input. For example, one microservice could be fetching some database data, the other parsing input through LLMs, and the final pushing some data to the database. Frameworks such as Marblerun\u00b9 allow for creating proofs that all microservices are attested. However, verifying such proofs would require knowing how the microservices are structured, a competitive advantage that companies are frequently unwilling to share.\nWrappers on proprietary models: Our flow focuses on private company deployments leveraging public models. However, some model providers offer their models as sim-ple wrappers over proprietary FMs. For example, an insur-ance company might prepend some data to a query with additional context and submit that for evaluation to a GPT4 model. CSPs offer such models as deployments on their platforms. While there does not need to be any trust be-tween the FM provider and the CSP like in our deployment flow, there necessarily needs to be trust between the user company and the FM provider. As we discuss in depth in the next paragraph, eliminating such trust would mean that some competitive advantage of the FM provider would need to be eliminated.\nEliminating the need for trust in the model provider: In our flow, users can attest that their data is processed in a TEE if they directly connect to it. This could be manifested, for example, with a special symbol in a browser like we currently do with TLS. Such a possibility is already a great improvement over the current state-of-the-art, which makes privacy-aware users notice companies that work with TEEs.\nHowever, the user needs to trust the model provider that they are running the model they have been promised and not using their data maliciously. While in our deployment strategy, this does not matter, it would matter if the model deployed is a public service such as public LLM chats or the aforementioned wrappers over proprietary models. TEEs resolve this by attesting the running software within them."}, {"title": "5. Lifting FMs into TEEs", "content": "Our flow and protections against a modern FM adversary generalize to all TEEs. To show a practical deployment, we select a subset of available TEEs and implement the data pipeline, show the insights we gained, and release our configuration. We limit ourselves to TEEs widely offered by major CSPs due to the practicality of such a choice. This reduces the available options to two types of TEEs from the largest CPU vendors, Intel\u00ae and AMD. We selected Intel's\u00ae TEEs for two reasons. Firstly, they provide us with the two common ways of implementing TEEs (virtual ma-chine and application-based) while using the same machine, allowing for an apples-to-apples comparison without any performance result scaling. Secondly, they include support of AMX, a specialized, on-chip, AI hardware accelerator introducing CPU native support for formats such as brain floating numbers and 8-bit integers, increasing overall per-formance. We also discuss GPU-based TEEs in Section 7.1. We outline the basic properties of these TEEs and show how they can be practically leveraged."}, {"title": "5.1. Software Guard Extension (SGX)", "content": "SGX is an application-based TEE. In the security model of SGX, the applications can run on bare metal without any virtualization or within a VM. The TCB involves only parts of the applications and the hardware. The SGX program-ming model differentiates between SGX non-protected and SGX-protected parts of the program. The protected part of the program is located within an enclave and is safeguarded by SGX capabilities, while the non-protected part is located outside of SGX. All the data in the enclave is protected by memory encryption and integrity checks. Operations requir-ing leaving the enclave to run the SGX non-protected part of the program (e.g., reading a file) securely save its state for later reuse and clear the caches for security reasons.\nSGX enclaves are frequently deployed on top of library OSs created specifically for TEEs, such as Gramine (Tsai et al.) or Occlum (Shen et al.). These are lightweight layers between the host system and an application intercepting any systemcalls to ensure they are conducted securely. These addressed some inconveniences of the original SGX SDK, which required users to rewrite their applications with secure and insecure sections. This implied using the neces-sary manual instructions for operations such as entering and leaving sections, implementing protections for loading files, and conducting attestation.\nGramine is a well-established, multi-company-backed, open-source project\u00b2. Figure 4 shows our software stack in which we use Gramine to run PyTorch and Intel Extensions for Pytorch (IPEX) within SGX without major code modifi-cations. It simplifies using common features of TEEs by creating security in the background. For example, it would automatically implement instructions for leaving and enter-ing the enclave. While loading files, it would also conduct integrity and encryption protections for the users and would allow for setting up attestation. While executing, Gramine intercepts and emulates application system calls. Depend-ing on the system call, the functionality can be provided efficiently without exiting the SGX enclave. On the other hand, system calls like file accesses require an exit, which incurs additional costs.\nGramine exposes these features via an application-dependent Manifest file. Figure 4 also shows parts of the Manifest we crafted for our FMs and the layers we moved into SGX. A Manifest allows the users to outline the size of the enclave, the number of threads, what should be run as an entry point binary, which files can be trusted (i.e., loaded without any integrity and confidentiality checks), which files should be allowed (i.e., which files can be accessed), where to obtain the cryptographic decryption keys, and how to con-duct attestation. It then uses this information to generate the necessary cryptographic information passed to the enclave. Manifest files are created from templates with examples in the Gramine repository. We release our Manifests to ease adoption of FMs within TEEs."}, {"title": "5.2. Trusted Domain Extensions (TDX)", "content": "TDX is a virtual machine (VM) based TEE that introduces security features using a hardened hardware-enabled hyper-visor. In the TDX security model, the entire VM is protected. This considerably simplifies the development as the user does not need to worry about special functions when leaving or entering the enclave. Users also do not need to find all the files their application uses and can run their programs within standard Linux OS such as Ubuntu. Furthermore, this approach fits well with the CSP virtualization trend. For our implementation, VMs enable Deepspeed within the enclave, opening venues for easier accelerator support and multi-node inference. However, the price for this comfort is considerably higher TCB as the whole VM OS must be trusted. Using VMs implies a virtualization performance tax that we will show in Section 6, which can be similar to the overheads of SGX.\nTo use TDX, one must create an appropriate VM definition file. It specifies details such as what file should be used to boot the VM, how to map the appropriate virtual cores to physical ones, and the available memory. This definition file considerably influences the VM's performance and can have a larger impact on the final performance than enabling or disabling the security features of TDX. We provide our optimized definition file."}, {"title": "5.3. Performance optimizations", "content": "As our workload does not use many IOs on the critical path, we found that the main driver for the performance of the TEES for FMs is twofold:\n1. the ability to use special purpose accelerators inside the CPU like Intel\u00ae Advanced Vector Extension (AVX) or Advanced Matrix Extension (AMX);\n2. efficient use of memory controllers and reaching peak memory bandwidth.\nThe former can be enabled by appropriate configuration and leveraging frameworks such as IPEX. We found that once such accelerators are used, the performance overheads are dominated by the latter, which is optimized by, in general, lowering the memory pressure. Leveraging accelerators such as AMX also allowed us to achieve this goal by moving from float32 to a native hardware support of bfloat16 and int 8.\nFor both TDX, and SGX, we also optimized our initial per-formance results by using TCMalloc (Durner et al.), and an Open MP (Dagum & Menon) version suitable for Intel\u00ae pro-cessors. The former reduces the memory pressure, while the latter makes better use of hardware. Furthermore, we found sub-NUMA clustering to have a large influence on both SGX and TDX. Sub-NUMA clustering in Intel\u00ae CPUs splits a single CPU into multiple NUMA domains and typically improves the performance for ML workloads. Currently, the TEE drivers and the OS do not support sub-NUMA domains, resulting in inefficient memory placement. Instead, a sin-gle NUMA domain per CPU achieves higher performance. As a result, we disabled sub-NUMA clustering during our experiments.\nFor TDX specifically, we used huge pages (Panwar et al.), which reduced the number of necessary translation looka-side buffer (TLB) accesses, decreasing memory access la-tency. We similarly observed higher performance when not exposing a CPU core's second logical thread (hyper thread) to the VM. In its default configuration, PyTorch only executes on the first logical thread of a core, ignoring the hyperthreads, making the second thread introduce noise."}, {"title": "6. Performance of LLMs in TEEs", "content": "We run FMs within TDX and SGX using our best perform-ing and optimized configurations, and show the performance of the Llama2 (Touvron et al., b) model family as it repre-sents modern FMs well. We show how TEEs are a practical solution for providing security to modern models from a performance perspective with considerably lower overheads than previously reported 100s of percent (Akram et al.). This is even though LLMs are large and complex models, creat-ing considerable memory pressure. As TEE overheads stem from either IO operations or the encryption and decryption of the accessed memory, such models maximize the cost of using TEEs. We focus on user-perceived performance: throughput and latency as measured by the number of tokens per second that our pipeline generates and the time to receive a token in the pipeline. While many metrics were created to measure the performance of ML models (Dehghani et al.), our choice focuses on real-world experience."}, {"title": "6.1. Experimental setup", "content": "In our experiments, we used a dual socket Intel\u00ae Xeon\u00ae Platinum GOLD 6530 system with 32 cores and 16x32GiB 4800MHz DDR5 memory. We used Ubuntu 23.10, with Python version 3.10.14, PyTorch version 2.2.0, transform-ers version 4.35.2, Intel\u00ae extension for PyTorch (IPEX) version 2.2.0, and oneCCL PyTorch bindings version 2.2.0. We measured the throughput and latency for four setups. Baseline results represent results coming from running the model on the bare metal instance, Gramine SGX represents the results of Gramine v1.7 backend running on SGX, VM the results of running a raw VM without any security fea-tures, and TDX the results of running on TDX. The machine is configured with disabled sub-NUMA clustering, and the workload is restricted to a single thread per core in the VM definition or via numactl.\nIn all experiments, we used batch size six for throughput and batch size one for latency. A larger batch size means increased latency but higher throughput as less data move"}]}