{"title": "DOLPHIN: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback", "authors": ["Jiakang Yuan", "Xiangchao Yan", "Botian Shi", "Tao Chen", "Wanli Ouyang", "Bo Zhang", "Lei Bai", "Yu Qiao", "Bowen Zhou"], "abstract": "The scientific research paradigm is undergoing a profound transformation owing to the development of Artificial Intelligence (AI). Recent works demonstrate that various Al-assisted research methods can largely improve research efficiency by improving data analysis, accelerating computation, and fostering novel idea generation. To further move towards the ultimate goal (i.e., automatic scientific research), in this paper, we propose DOLPHIN, the first closed-loop open-ended auto-research framework to further build the entire process of human scientific research. DOLPHIN can generate research ideas, perform experiments, and get feedback from experimental results to generate higher-quality ideas. More specifically, DOLPHIN first generates novel ideas based on relevant papers which are ranked by the topic and task attributes. Then, the codes are automatically generated and debugged with the exception-traceback-guided local code structure. Finally, DOLPHIN automatically analyzes the results of each idea and feeds the results back to the next round of idea generation. Experiments are conducted on the benchmark datasets of different topics and results show that DOLPHIN can generate novel ideas continuously and complete the experiment in a loop. We highlight that DOLPHIN can automatically propose methods that are comparable to the state-of-the-art in some tasks such as 3D point classification. Our homepage: https://unimodal4reasoning.github. io/Dolphin-project-page/.", "sections": [{"title": "1. Introduction", "content": "The fast evolution of Artificial Intelligence (AI) [1, 2, 10, 46] has brought about a profound transformation in various fields [16, 24, 48]. In the landscape of scientific research, AI modes are developed and play an important role in accelerating key research processes, including scientific data collection and processing [11], scientific computation [7, 17], and scientific innovation [27]. Under this trend, the research paradigm is shifting from completely human-driven research to AI-assisted research [3]. More recently, the continuous iteration and upgrading of LLMs has promoted the evolution of AI-assisted scientific research to automatic scientific research [23, 34].\nThe evolutionary trajectory from human-driven research to automatic research consists of four stages as shown in Fig. 1. 1) The entirely human-driven stage requires humans to complete all aspects including idea generation and experiments. 2) In the Al-assisted research stage, researchers use LLMs-based tools [3, 37] to improve the research efficiency. For example, GPT-researcher [3] can help us to decompose complex tasks and generate research reports using LLMs. 3) The semi-automatic research stage enables automation in certain aspects of scientific research. Recently, several works [20, 34, 36] use LLMs to generate ideas for different topics based on relevant works automatically. 4) The ultimate stage of AI development in science is the automatic scientific research stage, where AI system can automatically conduct the entire research process from conception to completion. Recently, AI-Scientist [23] proposes an open-ended auto-research framework capable of performing the complete scientific workflow, from idea generation to experimental validation and academic paper writing.\nDespite the encouraging progress made in existing works, several fundamental challenges continue to hinder the advancement of automatic scientific research. First, current studies face significant difficulties in accurately assessing the effectiveness of AI-generated ideas. Most existing works [20, 34, 36] either rely on human studies or directly employ LLMs to evaluate the quality (e.g., novelty) of generated ideas. However, merely focusing on the novelty of an idea itself does not adequately reflect its effectiveness in experimental validation. While works like AI-Scientist [23] consider experimental validation, they evaluate on the self-constructed simple datasets, making it challenging to draw meaningful comparisons with existing methods in the same field. Second, a critical limitation in previous works [20, 34, 36] is the absence of a feedback mechanism between the experimental validation unit and the idea generation unit a process that is fundamental to human research. Human researchers typically iterate their ideas or refine existing approaches based on experimental outcomes, which serves as a crucial pathway for improving the quality of research ideas.\nTo address these challenges and facilitate further progress towards automatic scientific research, in this work, we propose DOLPHIN, the first closed-loop open-ended automatic scientific research framework which is composed of three key steps in the research cycle (i.e., idea generation, experimental verification, and results feedback). In each research loop, DOLPHIN first retrieves the papers related to the given topic and generates novel and independent ideas. Then, the codes can be generated and debugged under the guidance of the experimental plan, which is directly generated by LLMs. Finally, DOLPHIN automatically analyzes the experimental results of successfully executed experiments which will guide the next loop of idea generation. Besides, for the two key steps in the auto-research loop (i.e., idea generation and experiment verification), we claim that the quality of ideas and execution rate of codes are crucial for enhancing the efficiency of auto-research. To improve the quality of ideas, inspired by ResearchAgent [4], we find that the relevance of retrieved papers to the topic is positively correlated with the quality of generated ideas. Therefore, instead of directly using the retrieved papers as references for idea generation, DOLPHIN filters these papers by judging the topic relevance and task attribute relevance between the retrieved papers and the input topic. Further, to improve the execution rate of DOLPHIN, we design an error-traceback-guided debugging process that analyzes the local code structure related to the error-traceback to guide the debugging process.\nTo further evaluate the effectiveness of automatic research, we conduct experiments on the benchmarks such as ModelNet40 [45], CIFAR-100 [19], and SST-2 [35], covering different tasks like 2D image classification, 3D point classification and sentiment classification. Our findings reveal that DOLPHIN generates ideas that boost the performance on these benchmarks compared to selected baselines like PointNet [28], WideResNet [12], and BERT-base [8]. We would like to emphasize that, DOLPHIN can propose methods based on PointNet that achieve performance comparable to human-designed state-of-the-art 3D classification methods. Besides, the quality of generated ideas improves through feedback, validating the effectiveness of the proposed closed-loop design.\nOur contribution can be summarized as follows:\n\u2022 We propose DOLPHIN, the first closed-loop open-ended automatic research framework that covers three key steps in the human research cycle, including generating ideas, performing experiments, and feedback.\n\u2022 To improve the efficiency of automatic research, we propose task-attribute-guided paper ranking and exception-traceback-guided debugging process to improve the quality of generated ideas and the successful rate of code execution, respectively.\n\u2022 Experimental results on benchmark datasets show that DOLPHIN can generate high-quality ideas and perform experiments in the loop. We were supervised to observe that DOLPHIN is capable of achieving performance comparable to state-of-the-art methods in certain areas through automated research."}, {"title": "2. Related Works", "content": "2.1. AI for Automatic Scientific Research\nOpen-ended Scientific Research. Recent studies [13, 20, 23, 26, 27, 42, 47] have demonstrated that Large Language Models (LLMs) have the potential to generate novel research ideas, a finding that has sparked widespread discussion in academia. Yang et al. [47] focuses on the social science domain and develops a framework called MOOSE based on LLM prompting, incorporating diverse feedback mechanisms to enhance the quality of generated ideas. Li et al. [20] introduces an innovative chain structure in a research domain, aiming to enhance the reasoning capabilities of LLMs. Besides, some iterative optimization strategies [13, 42] can also improve the novelty of ideas. While these approaches strive to improve the novelty of scientific ideas, a crucial aspect is the empirical validation of their practical effectiveness. AI-Scientist [23] proposes an end-to-end framework that automates the entire process from idea generation to experimental execution and paper writing. However, the experimental validation of its idea generation remains preliminary and lacks evaluation on real datasets or scenarios. Besides, the framework lacks a feedback mechanism from experimental validation to idea generation, unlike human researchers who iteratively refine their hypotheses based on experimental outcomes.\nScope-limited Scientific Research. Several studies have successfully applied LLMs to specific scientific discovery tasks. For example, AutoML-GPT [52] leverages LLMS for hyperparameter tuning by combining model and data descriptions as prompts to predict training logs. AgentHPO [21] proposes a creator-executor framework that conducts experiments for specific hyperparameters and iteratively optimizes them based on historical trials. Similarly, MLCopilot [51] constructs an experience pool from historical data to enable LLMs-based hyperparameter prediction. EvoPrompting [6] improves the in-context prompting examples of LLMs to achieve effective code-level neural architecture design. In contrast to these scope-limited approaches, our method focuses on more open-ended, autonomous scientific discovery, encompassing the entire process from idea generation to final experimental validation, thereby achieving a truly closed-loop logic across the complete research lifecycle.\n2.2. LLMs-assisted Research Tools\nAs LLMs [1, 2, 5, 10, 38, 46] continue to evolve rapidly, they have emerged as powerful tools for boosting research efficiency. They have been utilized such as report or survey generation [3, 14, 44], code development [15, 37, 50, 53], and data analysis [18, 32]. A notable example is GPT-Researcher [3], an autonomous agent based on LLMs and multi-agent systems to generate comprehensive research reports on the specified topics by effectively leveraging external knowledge bases. In the code development domain, tools such as GitHub Copilot [37] leverage the power of LLMs to provide intelligent, context-aware code suggestions and completions within Integrated Development Environments (IDEs), which significantly streamline the development process and enhance programming efficiency. The integration of LLMs as research assistants [3] opens up promising avenues for automating routine tasks, accelerating the overall research workflow. This technological breakthrough represents a significant step forward in augmenting human research capabilities."}, {"title": "3. Methods", "content": "In this section, we introduce DOLPHIN, a closed-loop open-ended auto-research framework as shown in Fig. 2, which is mainly composed of an idea generation process, an experiments verification process, and a result feedback process. The closed-loop means that the experimental results will be fed back into the idea generation process and the above three processes form a research cycle. In the idea generation process, DOLPHIN first retrieves papers based on the input topic and then filters papers that are not relevant to the topic. Then, the retrieved papers serve as references to guide LLMs to generate ideas, which will be further filtered through novelty and independence check process. Subsequently, DOLPHIN formulates experimental plans and proceeds to generate and debug code using a specifically designed error-traceback-guided debugging process. Finally, the results will be analyzed and utilized as feedback for the next cycle of ideas generation. In the following section, we will detail the ideas generation process, experiments verification process, and results feedback process in Sec. 3.1, Sec. 3.2, and Sec. 3.3 respectively.\n3.1. Ideas Generation Process\n\"A good beginning is half done.\" As the beginning of the research cycle, high-quality ideas are crucial to the entire research. A promising approach to generating high-quality ideas is to imitate the behaviors of human researchers. They typically first conduct literature reviews and then generate ideas based on the literature [31]. To generate novel ideas, DOLPHIN typically divided the idea generation process into two steps including 1) paper retrieval and ranking, and 2) ideas generation and filtering.\nPaper Retrieval and Ranking. To generate high-quality ideas, the first step is to retrieve papers that are relevant to the topic. Given a research topic (e.g., 3D classification), DOLPHIN begins by searching for relevant papers using Semantic Scholar API, obtaining the essential information such as titles and abstracts. However, the initially retrieved papers are not always directly related to the input topic, which can limit their usefulness in generating subsequent ideas. For instance, if the input topic is 3D classification, some retrieved papers might pertain to 3D detection [44]. Although these topics are interconnected, they typically focus on different challenges. As a result, it is necessary to filter out papers that are irrelevant to the specific topic.\nTo this end, we design a paper ranking process that aims to assign a higher score to the paper relevant to the input topic. In detail, DOLPHIN ranks the retrieved papers based on two main criteria: 1) relevance to the input topic, and 2) alignment of task attributes with those of the input topic. The task attributes typically define a task, including model"}, {"title": "3.2. Experimental Verification Process", "content": "\"Jump in and get your feet wet.\" In the research cycle, experimental verification is crucial as it serves as the most effective way to confirm the validity of a proposed idea. Most of the existing automatic scientific discovery works [20, 36] directly evaluate the novelty of AI-generated ideas by LLMs or humans. However, although novel ideas can be obtained in this way, their effectiveness cannot be guaranteed due to the lack of closed-loop experimental verification, which is inconsistent with our scientific research goals. In contrast, DOLPHIN screens out truly effective ideas through an experimental verification process.\nGiven an idea generated by the ideas generation process (Sec. 3.1) and reference codes, DOLPHIN first prompts the LLM to generate detailed experimental plans and then modifies the reference codes according to the idea and the generated plans. After modified codes, the experiment will automatically proceed. However, we find that the execution success rate is relatively low since LLMs encounter significant challenges in modifying code with complicated nested relationships (e.g., between class and function), while ensuring complete error-free execution. This will further reduce the efficiency of verifying ideas and research.\nBased on our observation, we design an exception-traceback-guided debugging process as shown in Fig. 3, aiming at assisting the LLMs in comprehending code logic with local code structure. Specifically, to generate the code structure related to the code errors, DOLPHIN first extracts information in exception traceback, including function name, line, and code, since traceback contains the nested information between functions. Note that DOLPHIN only focuses on custom codes, excluding library function calls. Then, DOLPHIN prompts the LLM to generate the code structure under the guidance of extracted exception traceback information. After that, the LLM analyzes the exception traceback and local code structure to make necessary modifications, enabling automatic code execution after these adjustments. The debugging process will be repeated until either successful execution is achieved or the predetermined maximum number of debugging times is reached. Subsequently, all successfully implemented ideas will undergo comprehensive analyses in the next phase."}, {"title": "3.3. Results Feedback Process", "content": "\"Experience is the best teacher.\" Human researchers often analyze experimental results to further propose new ideas or improve existing ideas, since insights from previous experiments can be leveraged to effectively enhance the quality of subsequent idea generation. However, recent works either implement feedback mechanisms within the isolated idea generation process [20] or lack feedback mechanisms entirely [23]. To address this limitation, DOLPHIN analyzes experimental results and incorporates the findings into the subsequent round of ideas generation process.\nDOLPHIN first divides the experimental results into three categories (i.e., improvement, maintenance, and decline) compared to the performance of the reference codes. Our goal is to discourage the development of ideas that have previously led to stagnant or declining performance, while actively promoting the creation of innovative concepts or iterations based on past ideas that enhance the model performance. In detail, DOLPHIN incorporate the embeddings of summaries from ideas that maintain or improve the performance into B defined in Sec. 3.1. In this way, ideas will be filtered out if they are similar to previous ideas that cannot improve the performance and avoid redundant verification of the ineffective ideas. Besides, DOLPHIN incorporates performance-enhancing ideas into the idea generation prompt for the next loop. Please refer to the supplementary material for detailed prompts."}, {"title": "4. Experiment", "content": "4.1. Experimental Setups\nTasks and Datasets. We conduct auto-research on three topics including image classification, 3D classification, and sentiment classification. For 2D classification task, we evaluate our method on CIFAR-100 [19], which is widely-used in computer vision. For 3D classification task, we use ModelNet40 [45] which is a 3D CAD dataset and consist of 40 categories. For the sentiment classification task, we use Stanford Sentiment Treebank (SST-2) dataset [35]. More details can be found in supplementary materials.\nImplementation Details. For the ideas generation process, we use gpt-40-2024-08-06 [25] as our LLM agent. The total number of retrieved papers is set to 50 and only papers with scores higher than 8 will be treated as references for the ideas generation process. We generate 20 ideas in each loop and the threshold of the independence filtering is set to 0.8. We use sentence-transformer/all- roberta-large-v1 [33] to extract the summary embedding of each idea. For the experimental verification process, we use deepseek-v2.5 [53] deployed by ollama [39] as our code agent. The maximum number of debugging attempts is set to 5 for experimental efficiency. Following AI-Scientist [23], we use self-reflection to first eliminate some syntax errors before executing the program and use aider as the framework to call LLM agents. The same hyper-parameters and models employed in the ideas generation process are utilized in the results feedback.\n4.2. Main Results\nWe evaluate DOLPHIN's capabilities across various tasks covering images, point clouds, and language modalities. In this section, we conduct experiments on each task for two loops (i.e., 40 ideas). For more detailed analyses of the closed-loop process, please refer to Sec. 4.3. Detailed implementations for the different tasks can be found in our supplementary material.\nResults on 2D Image Classification. We first conduct experiments on the image classification task, using WRN-28-10 [49] as our baseline model. This model is trained on CIFAR-100 [19] for 200 epochs, and we report the Top-1 accuracy. As shown in Tab. 1, the average improvement and max improvement are 0.6% and 0.8%, respectively. Notably, the idea generated and performed automatically by DOLPHIN can achieve comparable performance to hand-crafted methods such as ResNeXt [22] (e.g., 82.0% compared to 82.2%). It should be noted that Transformer-based methods [9] such as ViT are not included in our comparison, due to their heavy dependence on pre-training on large-scale datasets, which, at this stage, requires significant resource investment to validate their effectiveness.\nResults on 3D Point Classification. Further, we conduct experiments on 3D classification task using PointNet [28] as our baseline. We train the model on ModelNet40 [45] for 200 epochs, and report both overall accuracy and mean accuracy. As illustrated in Tab. 1, a total of 5 ideas achieve performance gains in two loops, with an average improvement of 1.0% OA, which shows that DOLPHIN can generate effective ideas and verify the idea by performing experiments. Besides, the maximum improvement can achieve 93.9% overall accuracy, which largely improves the performance compared with the human-designed baseline (i.e., 91.0% achieved by PointNet [28]). More excitingly, such results achieved by auto-research achieve comparable performance to the current state-of-the-art method (i.e., 93.8% achieved by GPSFormer [40]). This method is carefully designed by human researchers based on Transformer architecture. We would like to emphasize that for a fair comparison, we compare the 3D methods without pre-training and the voting mechanism.\nResults on Sentiment Classification. To verify the effectiveness of DOLPHIN on different modalities, we also perform experiments on the NLP task (i.e., sentiment classification). We conduct experiments on SST-2 [35] and report the classification accuracy. We fine-tune the pre-trained BERT-base model [8] as our baseline. It can be seen that DOLPHIN can also generate and perform effective ideas (e.g., 1.5% improvement on SST-2) on NLP task, reducing the performance gap between BERT-base (i.e., 91.0%) and BERT-large (i.e., 93.1%)."}, {"title": "4.3. Further Analyses", "content": "Analysis on Ideas Generation Process. To evaluate the effectiveness of the ideas generation process. We conduct further ablation studies by comparing naive generation, generation with retrieved papers, and our proposed method. Naive generation refers to the direct use of LLMs to generate ideas based on the seed idea and reference codes, similar to the approach used by AI-Scientist [23]. Generation with retrieved papers involves directly searching papers based on the topic and filtering them by their relevance to the input topic. As shown in Tab. 2, the naive generation yields the poorest results, with more than half of the ideas being judged as not novel. Furthermore, the quality of generated ideas is significantly improved when using naive retrieved papers, as this approach more closely aligns with the way human researchers generate ideas. However, as mentioned in Sec. 3.1, this approach tends to retrieve irrelevant papers and will mislead LLMs. As indicated in the first line of Tab. 3, some papers primarily focus on 3D detection or point cloud completion, where the design approach of the model is entirely different from that of the 3D point classification. This phenomenon can be well handled by the designed paper ranking process. As illustrated in Tab. 2 and Tab. 3, the number of novel ideas significantly increased from 8/20 to 19/20, while the proportion of papers related to irrelevant topics substantially decreased. This improvement can be attributed to the availability of more relevant reference papers during the ideas generation process. It is worth noting that the higher occurrence of the keyword \"segmentation\" is due to that, many studies concurrently perform both point classification and segmentation tasks.\nIn addition, the average cost per idea is shown in Tab. 2. It can be seen that the cost of each idea is very small. Besides, the cost of generating each idea is relatively higher when retrieving papers. This is mainly due to the retrieval process and the longer prompt required for idea generation (i.e., both the title and abstract are fed into the LLMs as references).\nAnalysis on Experimental Verification Process. The success rate of experiment execution is crucial for improving research efficiency. We further conducted studies on the experimental verification process. As illustrated in Tab. 4, we conduct experiments on three approaches: 1) directly feed the exception traceback to LLM, 2) extract the local code structure based on exception traceback, and then feed the local code structure and traceback to LLM, and 3) extract local code structure according to the information derived from the exception traceback, and then feed the local code structure and traceback to LLM.\nFirstly, we find that the successful execution rate is relatively low (e.g., 4 / 15) when directly feeding the traceback into LLM for debugging since the LLM cannot fully understand the complicated nested relationships in the codes. For example, when a dimension mismatch error occurs between networks and feature dimensions, LLMs can easily locate where the error occurs. However, since the feature might be obtained through multiple nested modules, LLMs fail to correct the network dimension. Then, by adding the local code structure according to exception traceback, the success rate does not significantly improve. This is because the exception traceback contains lots of information about called libraries, which makes LLMs generate code structures irrelevant to our custom codes. Further, by guiding LLMs to generate code structures with information extracted from traceback, the execution rate can be largely improved (i.e., 33.3% 50.0%). This is due to the extracted information containing custom code information related to the exception, enabling LLMs to focus on relevant functions and variables. Note that to improve efficiency, we only allow a maximum of 5 debugging iterations in our experiments.\nAnalysis on Results Feedback Process. To demonstrate the advantages of the closed-loop framework, we further analyze results on different loops. As shown in Tab. 5, we find that the quality of generated ideas without feedback is relatively low. The reasons can be divided into two folds: 1) Repeated ideas may be generated without a feedback process, resulting in redundant verification of the same idea and decreased experimental efficiency. 2) In the absence of feedback, the model cannot learn what kind of ideas are effective for the specific task.\nIn contrast, DOLPHIN effectively solves these challenges through a closed-loop approach, demonstrating progressive improvement in idea quality as the number of iterations increases (e.g., 2/7 improvement rate in Loop 1 \u2192 4/8 improvement rate in Loop 3). Further, these ideas that improve the performance are different between each round, which further improves the research efficiency and shows the effectiveness of DOLPHIN. Besides, the average cost slightly increases as the iterations continue, since the results will be fed back into the next round of ideas generation process."}, {"title": "4.4. Case Studies", "content": "We illustrate our approach with an example drawn from 3D point classification task as shown in Fig. 4. It can be seen that DOLPHIN can generate codes corresponding to the idea and only in this way, the generated ideas can be effectively verified. Tab. 6 presents the comparison between AI-generated approach (i.e., PointNet-CSR obtained by DOLPHIN) and previous human-designed methods. Idea automatically generated and performed by DOLPHIN can outperform most human-designed methods and achieve comparable performance to current SOTA [40]. Furthermore, as shown in Tab. 7, we carefully investigate the existing works on 3D classification task up to the submission date, identifying the work most relevant to PointNet-CSR (AI-generated 3D work), as illustrated in Fig. 4. The detailed comparison of the idea, implementation, and result can be found in Tab. 7, showing that PointNet-CSR can achieve better and faster performance through a more concise architecture. Please refer to our supplementary material for more comparisons between human-designed works and DOLPHIN-generated works."}, {"title": "5. Conclusion and Future Outlook", "content": "DOLPHIN evaluates idea quality through experimental verification, improving it in a closed-loop fashion. Besides, beyond conducting quantitative experiments that demonstrate DOLPHIN's capability to generate solutions and results comparable to human-designed approaches, we also conducted in-depth case studies to evaluate the novelty of the ideas and the implementation efficiency of the codes generated by DOLPHIN. These quantitative and qualitative evaluations we conducted for DOLPHIN are essential for gaining further insight into the potential and value inherent in DOLPHIN.\nIn the future, we envision DOLPHIN further advancing AI-driven automated scientific research. By harnessing its ability to generate novel ideas in a closed-loop system, we also aspire for DOLPHIN to foster the development of groundbreaking ideas inspired by cross-disciplinary knowledge, ultimately providing innovative solutions for complex scientific challenges."}, {"title": "6. More details of Dolphin", "content": "In this section, we provide more details about DOLPHIN including prompts, qualitative results of some processes, algorithms of some processes, and so on. In the following section, we will give more details of the ideas generation process, experimental verification process, and results feedback process, respectively."}, {"title": "6.1. Ideas Generation Process", "content": "We provide prompts used in paper retrieval, paper ranking, and idea generation process in Fig. 5. We partially refer to the prompt design of previous works [23, 34]. As depicted in our manuscript, the quality of retrieved papers is important to idea generation. Here, we give an example to further illustrate the impact. Given the topic '3D classification', naive retrieval will result in lots of papers related to 3D object detection. As a result, we have identified several ideas that are more closely related to detection (e.g., region proposal PointNet), which are significantly influenced by the detection task.\nFurther, we provide the algorithm of independence check in Algorithm 1. To show the effectiveness of the independence check process, we show an example in Fig. 6. It can be seen that although the name and title of the idea are totally different from each other, the technologies used in the two ideas are almost the same. Our idea independence check process can effectively filter the repeated ideas, further improving the auto-research efficiency."}, {"title": "6.2. Experimental Verification Process", "content": "Fig. 7 shows the local code structure generation prompt, which needs first to extract the exception traceback information. Further, to show how this information can guide the LLM in generating the local code structure, we provide an example for better illustration. As shown in Fig. 8, the LLM tend to copy the original reference code which is useless in the following debugging process."}, {"title": "7. More Details and Results of Experiments", "content": "In this section, we provide a comprehensive overview of the implementation details and dataset information used in our main text."}, {"title": "7.1. Details of Selected Tasks", "content": "7.1.1. Image Classification Task\nDataset: CIFAR-100. The CIFAR-100 dataset [19] includes colored natural images with a resolution of 32\u00d732 pixels, categorized into 100 distinct classes. The training and test sets contain 50,000 and 10,000 images, respectively. We adopt a standard data augmentation scheme (i.e., RandomCrop, RandomHorizontalFlip, RandomRotation).\nImplementation Details. We use WRN-28-10 [49] as our baseline. We partially refer to the codebase2. Our training process employs the SGD optimizer with the CosineAnuealing scheduler. The initial learning rate is set to 0.1, and we train the model for 200 epochs with a batch size of 128. We apply a weight decay of 5e-4 and a Nesterov momentum with a coefficient of 0.9."}, {"title": "7.1.2. 3D Classification Task", "content": "Dataset: ModelNet40. ModelNet40 [45] is a synthetic object dataset that contains 12,311 3D CAD models covering 40 categories. The standard training/validation set of ModelNet40 carries 9843/2468 point clouds.\nImplementation Details. We use PointNet [28] as our baseline. Following PointNet [28], we uniformly sample 1024 points on each object. We use the random scale, random dropout, and point shift during training and train the model for 200 epochs. The initial learning rate is set to le-3. We use Adam optimizer (weight decay=1e-4) and step learning rate decay (step size=20, gamma=0.7). Our implementation partially refers to codebase\u00b3."}, {"title": "7.1.3. Sentiment Classification Task", "content": "Dataset: SST-2. Stanford Sentiment Treebank (SST) [35] contains 11,855 one-sentence movie reviews extracted from Rotten Tomatoes. SST contains 215,154 unique manually labeled texts of varying lengths.\nImplementation Details. Our code of sentiment classification tasks refers to the codebase\u2074. We fully fine-tune the BERT-base model for the classification task for 5 epochs with the learning rate 2e-5. The batch size is set to 32. We use the early stop mechanism during training (i.e., stop training if the accuracy of current epoch is lower than that of the previous epoch)."}, {"title": "7.2. More Qualitative Results", "content": "We provide several cases that are automatically generated and evaluated by DOLPHIN as shown in Fig. 9, Fig. 10, and Fig. 11. We show the ideas and modified codes in figures and the performance in the corresponding caption, respectively."}, {"title": "8. More analysis and Further Works", "content": "8.1. The Extensibility for the Research Task\nRecently, lots of works have explored automatic idea generation [20, 34]. One limitation is that the novelty of an idea can only be judged through human scoring or large model scoring. However, in real scientific research, we need ideas that can lead to performance breakthroughs that cannot be judged without experiments. DOLPHIN which includes ideas generation, experimental verification, and results feedback process can serve as an evaluation protocol. In the future, it can be combined with auto-idea generation works to assess the effectiveness of the idea generation."}, {"title": "8.2. Analysis on Future Works", "content": "DOPLINE achieves the first closed-loop automatic research framework, we still hope that DOPLINE will possess stronger auto-research capabilities. For example, our ultimate goal is to utilize the capabilities of large models to integrate multi-disciplinary knowledge which is hard to be achieved by human researchers. To achieve this goal, we still need to make efforts in the following aspects: 1) develop more powerful code models that can understand and modify project-level code, and 2) retrieve multi-disciplinary papers that may be related to the given topic."}]}