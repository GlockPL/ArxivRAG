{"title": "M\u00b3-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery\nVideo Object Segmentation", "authors": ["Zixuan Chen", "Jiaxin Li", "Liming Tan", "Yejie Guo", "Junxuan Liang", "Cewu Lu", "Yonglu Li"], "abstract": "Intelligent robots need to interact with diverse objects\nacross various environments. The appearance and state\nof objects frequently undergo complex transformations de-\npending on the object properties, e.g., phase transitions.\nHowever, in the vision community, segmenting dynamic\nobjects with phase transitions is overlooked. In light of\nthis, we introduce the concept of phase in segmentation,\nwhich categorizes real-world objects based on their vi-\nsual characteristics and potential morphological and ap-\npearance changes. Then, we present a new benchmark,\nMulti-Phase, Multi-Transition, and Multi-Scenery Video\nObject Segmentation (M\u00b3-VOS), to verify the ability of\nmodels to understand object phases, which consists of\n479 high-resolution videos spanning over 10 distinct ev-\neryday scenarios. It provides dense instance mask anno-\ntations that capture both object phases and their transi-\ntions. We evaluate state-of-the-art methods on M\u00b3-VOS,\nyielding several key insights. Notably, current appearance-\nbased approaches show significant room for improvement\nwhen handling objects with phase transitions. The inherent\nchanges in disorder suggest that the predictive performance\nof the forward entropy-increasing process can be improved\nthrough a reverse entropy-reducing process. These findings\nlead us to propose ReVOS, a new plug-and-play model that\nimproves its performance by reversal refinement. Our data\nand code will be publicly available.", "sections": [{"title": "1. Introduction", "content": "Object understanding is crucial, especially for Embodied\nAI. Lately, large-scale datasets and data-driven methods\n[6, 15] have advanced research in object understanding,\nshifting from traditional category recognition [12] to deeper\nlevels of comprehension, such as identifying higher-level\naffordances [9] and attributes [10], which help establishing\nthe concepts of objects in interactions with environment.\nIn the real world, intelligent robots need to interact with\nvarious objects in diverse ways, during which, objects ex-\nhibit a range of morphological and appearance changes in-\nfluenced by their inherent characteristics, the nature of the\nenvironment, and the specific interactions occurring. Partic-\nularly, the phase characteristics of objects describe their in-\ntrinsic morphological features and change dynamics, mak-\ning them an important component of object knowledge.\nObjects with different phase characteristics exhibit dis-\ntinct change features when subjected to the same interac-\ntions. For instance, placing solid coffee beans in a blender\nresults in their transformation into a powdered form. In con-\ntrast, pouring milk into the blender causes it to splash and\nflow continuously during blending, potentially even lead-\ning to separation at the end. In addition to transformations\nwithin phase states, cross-state phase changes are com-\nmon in the real world, endowing objects with properties\nthey originally did not have. For example, dry ice rapidly\nsublimates, producing a lot of mist and water droplets will\nquickly frost at lower temperatures as Fig. 1. Therefore,\na deep understanding of an object's intrinsic phase charac-\nteristics and the properties of phase transition is crucial for\nrobots to perceive the world, manipulate objects, and fulfill\nvarious complex functions according to human needs.\nDespite efforts exploring the visual understanding of ob-\nject change characteristics from various perspectives, such\nas OCL [14] which focuses on changes in object usability,\nVOST [28] and VSCOS [32] which emphasize the segmen-\ntation of objects with appearance variations, most works re-\nmain limited to a single phase. They often overlook the un-\nderstanding of object knowledge across phase transitions.\nThus, to address the absence of object phase transition\nunderstanding, we first categorize different objects based on\ntheir visual features and change characteristics commonly\nobserved in daily life. Furthermore, we introduce M\u00b3-\nVOS, a fine-grained text-visual annotated dataset for object\nknowledge understanding across multiple scenarios, includ-\ning objects that encompass a variety of transformations, in-"}, {"title": "2. Related Work", "content": "Recently, computer vision has seen significant advance-\nments in object understanding, primarily represented by two\nparadigms. The first is the classification paradigm, which\nhas evolved from object recognition [12] to a deeper under-\nstanding of object attributes [10] and affordances [9]. The\nsecond focuses on pixel-level classification, encompassing\ntasks such as image grounding [18, 21] and segmentation of\nchanging objects in videos [4, 24]. The former emphasizes\nan explicit understanding of object knowledge, where the\nmodel is expected to extract relevant information about ob-\njects directly from images [9, 10, 12]. In contrast, the latter\nhighlights an implicit understanding of object knowledge,\nwhere the model determines the location of objects in an\nimage based on given prompts and its implicit recognition\nof object knowledge [4, 18, 21, 24].\nData-driven foundation models [16] have achieved re-\nmarkable performance in both above paradigms and even\ndemonstrate strong open-vocabulary abilities. However,"}, {"title": "2.2. Video Object Segmentation", "content": "Video object segmentation is the process of segmenting tar-\nget foreground objects from the video background at the\npixel level within a video sequence. The target foreground\nobject is typically specified either by providing a mask in\nthe first frame, known as semi-supervised VOS [20, 22, 29].\nThis task requires the effective tracking of the target and a\nclear understanding of its boundaries, which demands that\nmodels have a thorough comprehension of the object's ap-\npearance [20, 29], dynamic properties [28, 32], and motion\ninformation [7] within the video, as well as other physical\ncharacteristics. This is precisely why we chose video object\nsegmentation as the specific task to evaluate the ability of\nvision models to understand object phase transitions.\nTo prompt the development of Video Object Segment,\n[20, 22, 29] constructs a series of early VOS benchmarks.\nRecently, several works have explored testing the open-\nworld capabilities of VOS models from different perspec-\ntives: VOST [28] and VSCOS [32] focus on changes in ob-\nject appearance; MeVis [7] focuses on the movement char-\nacteristics of objects. However, compared to the M\u00b3-VOS\ndataset, these models are all limited to single-phase objects,\ntypically dealing with solid objects. These objects have rel-\natively simple transformation characteristics and highly pre-\ndictable movement patterns.\nThere are a lot of VOS methods achieve success and can\nbe concluded into two classes:\nMemory-based VOS. Semi-supervised Video Object"}, {"title": "3. Preliminary", "content": "In this section, we introduce the concept of phase and use\nit to classify everyday materials. Then, we categorize the\ntransformations of objects into Intra-Phase Transitions\nand Cross-Phase Transitions."}, {"title": "3.1. Phase Categories", "content": "Rather than defining the phases of objects from the micro-\nscopic perspective provided by chemistry or physics, which\nfocuses on molecular spacing, we adopt a macroscopic ap-\nproach to classify objects based on their appearance and\ndynamic characteristics. In our study, phase refers to an\nattribute that describes both visual characteristics and in-\ntrinsic transformation properties. Next, we categorize com-\nmonly encountered objects into three major types: solid,\nliquid, and aerosol/gas. Then we introduce the subdivided\nphase categories for each major class of materials, along\nwith their respective transformation properties (Tab. 2).\nFor solid materials, particle size also influences their\ntransformation characteristics. Therefore, we further sub-\ndivide solid materials into fragmentary or powder-like\nsubstances and non-fragmentary substances. For non-\nfragmentary substances, we categorize them further based\non whether the material can easily undergo arbitrary defor-\nmation, dividing them into rigid and flexible objects."}, {"title": "3.2. Phase Transition", "content": "In everyday life, objects undergo various changes through\ninteractions with the environment or humans. Based on\nwhether these changes cross different phase states, we cate-\ngorize them into intra-phase and cross-phase transitions.\nIntra-Phase Transformations primarily depend on the\ntransformation characteristics inherent to the phase state of\nthe object itself. Different phase states of materials often\nexhibit unique intra-phase transformations. For example,\nflexible solids can be twisted (e.g., shoelaces or knots), vis-\ncous substances can stretch (e.g., melted cheese or syrup),\nand aerosols/gases can diffuse (smoke or steam diffusion).\nCross-Phase Transformations are quite common in ev-\neryday life but are often overlooked. Due to the differences\nin phase states before and after the transformation, these\nobjects exhibit distinctly different visual characteristics and\ntransformation properties. Therefore, understanding such\ntransformations poses a greater challenge.\nIn Supplementary, we provide more detailed definitions\nof these phases and phase transition."}, {"title": "4. Dataset Design and Construction", "content": "In this section, we introduce the data collection and anno-\ntation of M\u00b3-VOS. The key steps include selecting repre-\nsentative videos, annotating them with instance masks and\ninformation text, and defining an evaluation protocol."}, {"title": "4.1. Video Collection", "content": "We chose to source our videos from YouTube and BiliBili\nmainly, where there are lots of videos about intra-phase\ntransition and cross-phase transition in different scenery:"}, {"title": "4.2. Annotation Design", "content": "Each video clip is accompanied by rich annotations for tar-\nget segmented objects, identifying the corresponding phase\ntransitions. This includes a bilingual (Chinese and English)\ndescription of one or more target objects in each clip, the\nphases of each target object in the first and last frames, as\nwell as pixel-level segmentation annotations with 30 fps.\nIn video collection, we instructed the video collectors to\nuse clear and accurate positional terms to describe each tar-\nget object in the first frame. The descriptions are \u201cIn the\nprocess of [specific action], the [specific object] which at\n[specific location] in the initial frame or whose color is\n[specific color].\", providing accurate guidance for the vol-\nunteers responsible for the mask annotations.\nIn the annotation, we instructed volunteers to use the in-\nformation provided by the video collectors about the tar-\nget objects and the semi-auto Annotation Tool (detailed\nin Sec. 4.3.1) to annotate the masks of the target objects\nin each frame of the video. For various uncertain void\nmasks caused by unclear image quality or motion blur, we\ninstructed volunteers to annotate the corresponding void\nmasks, similar to the approach in the VOST [28]. In this\nway, we can disregard the influence of these pixels during\nthe evaluation process. Volunteers were also required to se-\nlect the initial and final phases of the target objects from\nTab. 2 based on their states in the video, and subsequently\nlabel the corresponding phase transformations.\""}, {"title": "4.3. Annotation Collection", "content": "In this section, we introduce our semi-auto Annotation tool\nwhich efficiently annotates the masks of target objects in the\nvideos and our annotation collection pipeline."}, {"title": "4.3.1. Semi-Auto Multi-Level Annotation Tool", "content": "To efficiently annotate the masks of target objects in the\nvideos, we employed a semi-automated multi-level anno-\ntation tool that utilizes a paint-erase tool (pixel-level), pixel\ncolor difference assistance (appearance-level), and neural\nnetwork-based annotation support (object-level).\nOur annotation tool mainly employs a paint-and-erase\napproach. Volunteers can use a brush and eraser tool, along\nwith a magnification feature, to accurately annotate the\nmasks of target objects and the void masks in the video.\nTo achieve high-frame-rate mask annotation, we integrated\nthe fully automated annotation tool provided by Cutie [4]\nbased on RITM [26] for interactive image segmentation and\nCutie for video object segmentation. After volunteers an-\nnotate masks for a specific frame using the paint-and-erase\napproach or point prompt method, they can utilize Cutie to\npropagate masks for that frame. Subsequently, the propa-\ngated pseudo-ground truth can refined using the paint-and-\nerase approach. The neural network based automatic anno-\ntation tool provides good pseudo ground truth references for\nobjects with clear contours. However, the tool often fails to\nprovide satisfactory pseudo-ground truth for target objects\nlike water vapor, thick smoke, and splashes, which have un-\nclear outlines. Thus, our tool integrates a color difference\nmasking, allowing users to select a specific pixel value in\nthe image and set a range to create a color difference tem-\nplate for the video. Given a pixel $p_s$ and any other pixel $p_i$,\nthe position of $p_i$ will be labeled according to\n$$M(p_i) =\n\\begin{cases}\n1 & \\Delta_{H_S}(p_i) < 0.1\\delta, \\\\\n & \\Delta_{S_S}(p_i) < \\delta, \\\\\n & \\Delta_{V_S}(p_i) < \\delta, \\\\\n0 & otherwise.\n\\end{cases}$$\n$$\\Delta_{\\varphi_S}(p_i) = |\\varphi(p_i) \u2013 \\varphi(p_s)|, \\varphi\\in {H,S,V}$$\nWe label the pixel $p_i$ if and only if the color difference\nbetween $p_s$ and $p_i$ is within $0.1\\delta$, and the differences in\nsaturation and brightness are within $\\delta$. Here, $\\delta$ is a user-\nadjustable parameter. Our testing has shown that this func-\ntionality effectively annotates objects with unclear contours\nbut distinct colors, especially for gases and liquids."}, {"title": "4.3.2. Annotation Pipeline and Statistics", "content": "As illustrated in Fig. 2a, in our annotation pipeline, we hired\n12 volunteers and provided them with 7 days of training to\nuse the annotation tools on the videos we collected.\nTo ensure the quality of the data annotations, we em-\nployed 3 experienced reviewers to audit the annotated data.\nEach volunteer's masks were evaluated based on three cri-\nteria: tracking accuracy, completeness of annotation, and\nboundary stability, scored on a scale from 0 to 3 (detailed\nscoring criteria and the review result can be found in the\nSupplement). The final mask review score was the aver-\nage of the two reviewers' ratings. If any score is below 2,\nwe consider the mask Unqualified. In addition to the MOS\nreviewing, we employed a dual-model cross-validation ap-\nproach to verify the annotated masks (detailed in Sec. 4.4).\nIf the validated mask annotations do not meet the quality"}, {"title": "5. Method", "content": "In data collection and annotation, we observed that when\nobjects undergo intra-phase or cross-phase transitions, their"}, {"title": "Reverse Memory.", "content": "In the forward propagation of the\nmask, we maintain a sliding window of length T. Sup-\npose the model needs to predict the t-th frame as the current\ntime. The sliding window includes image information from\nmax(0, t -T) to max(0, t \u2212 1) and the extracted forward\nmemory readout. The reverse memory is similar to working\nmemory in [2]. The difference is that it only stores high-\nresolution features for the reverse process, serving the same\nfunction as working memory in the reverse propagation. It\nis cleared at the beginning of each reverse propagation as we\nwant the reverse memory to only collect information from\nthe current reverse process."}, {"title": "Booster.", "content": "if the model loses information about the mask\nfor certain object parts during the forward process, this loss\nwill be taken into the reverse process and continue to ad-\nversely affect the segmentation results. To address this is-\nsue, we implement a strategy to boost the mask during for-\nward propagation. This enhancement aims to predict as"}, {"title": "5.1. Implementation Details", "content": "We adopt Cutie-base [4] as our matching backbone,\nwhich is trained under \"MEGA\" setting with 5 datasets:\nYoutubeVOS [30], DAVIS [20], BURST [1], OVIS [23],\nand MOSE [8]. In training, we froze all the parameters\nof Cutie and only trained the Readout Fusion Module. We\nfinetune our model with AdamW [17] optimizer. The learn-\ning rate is set to le-5. Finetuning takes about 75k iterations\nand we reduce the learning rate by 10 times after 60K and\n67.5K iterations. The model is trained on 4 A100 GPUs for\n10 hours."}, {"title": "6. Analysis", "content": "In evaluation, we adopt standard metric Jaccard index I,\nnewly proposed metrics $I_{tr}$ [28] and $I_{cc}$ [32]. $I_{tr}$ rep-\nresents the Jaccard index of the last 25% frames and $I_{cc}$\nrepresents the Jaccard index based on each connected com-\nponent and takes an average over all components. For\nYouTubeVOS [30], we use its official metrics: $I$ and $F$\nfor seen and unseen categories. G is the averaged J&F for\nboth seen and unseen classes."}, {"title": "6.1. Main Results", "content": "We compare ReVOS with SOTA approaches on our dataset\nand standard benchmarks: VOST validation [28] DAVIS\n2017 validation [22] and YouTubeVOS validation [29]. For\nour dataset, we create two versions: M\u00b3-VOS full (all cases)\nand M\u00b3-VOS core (highly representative cases). For a\nfair comparison and open-vocabulary purpose, we choose\nonly one checkpoint that each method performs the best on\nthe DAVIS 2017 validation set and use it to test all other\ndatasets. For DeAOT, we excluded a few long video cases\nas we encountered insufficient memory during inference."}, {"title": "6.2. Intra-Phase vs. Cross-Phase Transformations", "content": "We present the average model performance in 4 cate-\ngories: Intra-Phase (solid), Intra-Phase (Liquid), Intra-\nPhase (Aerosol/Gas), and Cross-Phase, which are abbrevi-\nated as IS, IL, IG and CP in Tab. 5. In addition, we also\npresent the performance of our model for the detailed phase\ntransitions in Fig. 5.\nCompared to pure Cutie, Our Cutie-ReVOS improves\nthe performance in all phase transitions. Compared to the\nSOTA, Cutie-ReVOS still demonstrated excellent perfor-\nmance facing these phase transitions. Especially, when fac-\ning the cross-phase transition, Cutie-ReVOS achieves the\nbest performance in all models we have evaluated."}, {"title": "6.3. Performance Differences across Benchmarks", "content": "Compared to existing benchmarks, M\u00b3-VOS significantly\nexpands the phase range of objects, introducing a series of"}, {"title": "6.4. Ablation Study", "content": "We study various designs of our algorithm in the ablation.\nWe report $I$, $I_{tr}$ and $I_{cc}$ and FPS for M\u00b3-VOS mid set."}, {"title": "Hyperparameter Choices.", "content": "Tab. 6 compares our results\nwith different choices of hyperparameters: sliding window\nlength T and reverse interval L. Note that T = 0 or L = 0\nis equivalent to Cutie-base. T is fixed to 30 when we vary\nthe value of L and vice versa. We find that a larger slid-\ning window length means high performance but also im-\nplies a decrease in FPS. Surprisingly, a larger reverse inter-\nval also means higher performance with an increase in FPS."}, {"title": "7. Discussion and Limitations", "content": "Annotation Bias. Annotation bias is inevitably introduced\nby annotators' tendencies, inherent biases in assisted tools,\nand ambiguous regions in videos. As discussed in Sec. 4.4,\ndespite our implementation of multi-round, multi-level re-\nview processes, some discrepancies remain across annota-\ntion instances. Establishing a more standardized and rigor-\nous data annotation review workflow could effectively mit-\nigate annotation bias.\nModel Performance. All models show limitations on\nour new benchmark with challenging object phase transi-\ntions. In future works, models may be advanced by lever-\naging the physical knowledge embedded in MLLMs."}, {"title": "8. Conclusion", "content": "In this work, we explored the ability of the visual models\nto understand object phase transitions through a video seg-\nmentation task, introducing a fine-grained text-visual anno-\ntated open-vocabulary benchmark M\u00b3-VOS. It includes 14\nscenes, encompassing 120+ objects across 6 phase states"}, {"title": "Overview", "content": "We introduce:\n\u2022 More implementation details of our work in Secs. 9\nto 14.\n\u2022 More experiments about the challenge in M\u00b3-VOS in\nSec. 15.\n\u2022 More failure cases in Sec. 16."}, {"title": "9. Details of Annotations", "content": "We list the specific definitions of phase below:\n\u2022 Solid: Volume is relatively fixed, has distinct boundaries,\nand shapes independent of the container.\nParticulate: Composed of several fragmented parts.\nNon-particulate: Composed of single/few larger\nparts.\n* Rigid Body: Exhibiting a relatively fixed shape and\nresistance to deformation.\n* Flexible Body: Has a relatively unstable shape and\ncan undergo deformation easily.\n\u2022 Liquid: Volume is relatively fixed and has distinct\nboundaries, fluidity, or shape dependent on the container.\nViscous Fluid: Has significant viscosity, and can\nstretch.\nNon-viscous Liquid: No significant viscosity, cannot\nstretch.\n\u2022 Aerosol/Gas: Volume not fixed, has no distinct bound-\naries, shape dependent on the container."}, {"title": "9.2. Phase Transition Definition", "content": "In our works, we ensure that the definition of phase transi-\ntions meets a fundamental requirement: the transition from\nan initial state to a final state may correspond to different\nspecific phase transitions depending on the characteristics\nof the transformation. However, for a specific phase transi-\ntion, its initial and final states must be unique.\nWe list all the initial and final states for each phase tran-\nsition as Tab. 8. Besides, in Tab. 9, we give a detailed defi-\nnition of different phase transitions."}, {"title": "10. Connected Component Jaccard Index", "content": "To avoid ignorance of the small part during evaluation, we\nintroduce the connect component Jaccard Index $I_{cc}$. The\ndefinition of $I_{cc}$ is the average Jaccard Index of the max-\nimum bipartite matching corresponding to all connected\nmask components between the ground truth and the pre-\ndicted image.\nWe implemented our $I_{cc}$ using the Hungarian algo-\nrithm, different from the one in the official implementation\nof VSCOS [32] that calculates the $I_{cc}$ using a two-loop\nmatching process, i.e., iteratively finding for each connected\ncomponent in Mask A the one in Mask B that maximizes the\nJaccard Index."}, {"title": "11. Details of Masks SQA", "content": "We design three criteria to evaluate the annotation in M\u00b3-\nVOS, including:\n\u2022 Tracking Accuracy\n0: Target is lost or tracked incorrectly for a long time.\n1: Target is lost or tracked incorrectly for a short con-\ntinuous period.\n2: Target is lost or tracked incorrectly in a few isolated\nframes.\n3: Target is always tracked correctly.\n\u2022 Mask Annotation Completeness\n0: Mask has been completely missing for a long time.\n1: Mask has been partially missing for a long time.\n2: Mask is partially missing in some frames.\n3: Mask is complete and accurate throughout.\n\u2022 Mask Boundary Stability\n0: Mask boundary shows an obvious jitter for a long\ntime.\n1: Mask boundary shows a slight jitter for a long time.\n2: Mask boundary shows a slight jitter for a short time.\n3: Mask boundary shows no visible jitter."}, {"title": "11.2. SQA Analyze", "content": "We select three experienced reviewers to evaluate all of our\nmasks in M\u00b3-VOS in the criteria of Sec. 11.1 using the re-\nviewer UI as Fig. 6. In the process of constructing M\u00b3-VOS,\nwe make sure the scores in any criterion of all of our masks\nare higher than 2. In the final evaluation of M\u00b3-VOS, the\nMOS in these criteria are 2.95 in tracking accuracy, 2.91 in\nmask annotation completeness, and 2.89 in mask boundary\nstability."}, {"title": "12. Details of Avoidance of Model Bias", "content": "In this part, we introduce the details of the dual-model\ncross-validation method. In this process, we validate that"}, {"title": "12.1. IoU Analysis", "content": "In terms of model selection of the dual-model cross-\nvalidation process, we adapt the annotation model to the\nlatest SAM2 [24]. We utilized the open-source base plus\nmodel configuration and checkpoints, as this configuration\nis more effective in fully segmenting our target objects com-\npared to other model setups.\nIn the dual-model cross-validation, we first randomly\nsampled a subset of videos annotated by Cutie at a ratio\nof 5:1. We selected 6 volunteers from a total of 12 to\nre-annotate this subset using both the SAM2-assisted and\nCutie-assisted annotation tools, resulting in masks desig-\nnated as Mask A and Mask B, respectively. To balance an-\nnotation efficiency and validation effectiveness, we set the\nannotation frame rate to 6 fps in the cross-validation. The\nhigh-frame-rate annotated masks obtained for the dataset\nare referred to as Mask O.\nBy calculating the Intersection over Union (IoU) and the\nother two metrics introduced in [28, 32], the results are\nshown in Fig. 7, indicated that $J_{st}(MaskA, MaskB)$ and\n$J_{mean}$ exceeded 85% and were very close to each other.\nSpecifically, although the difference between Mask A and\nMask B is slightly larger than that between Mask B and\nMask O, we have Eq. (4) holds:\n$$J_*(B,O) \u2013 J_*(A,O) < 1 \u2013 J_*(B,O).$$ These results suggest that the annotations SAM2-\nassisted annotation tool produced are comparable to those\nof the Cutie-assisted tool, without significant bias due to\nmodel differences. They also indicate that the bias intro-\nduced by the models can be considered negligible compared\nto other sources of systematic error, such as volunteer anno-\ntation habits and inadvertent jitters during annotation."}, {"title": "12.2. Blind Review: DMOS Evaluation", "content": "In addition to the quantitative analysis of model bias con-\nducted using the dual-model validation, we also designed a\nmechanism for blind comparison by experienced reviewers.\nWe presented 3 reviewers with both Mask A and Mask B\nfrom the cross-validation annotation process, allowing them\nto evaluate the performance of the two masks based on the\nthree criteria mentioned in Sec. 11.1. The reviewers were\ninstructed to select the mask they deemed superior. If they\nconsidered the performances to be equivalent, they could\nchoose Equal. Throughout this process, the order of Mask\nA and Mask B was randomized to ensure that the reviewers\nwere unaware of which mask corresponded to which model.\nThe final subjective evaluation results are shown in\nFig. 8, indicating that the two masks demonstrated a con-\nsiderable degree of consistency across the three subjective\nevaluation metrics, with no significant bias observed."}, {"title": "13. Multi-Level Semi-Auto Annotation Tool", "content": "In Fig. 9, we show our details of the interactive UI of the\nmulti-level semi-auto annotate tool. We implement this tool\nbased on the interactive demo from Cutie [4], including\npixel level, appearance-level, and object level. In particu-\nlar, we implement the object-level function using the SAM2\nmodel and Cutie model. In this way, we could perform the\ndual-model cross-validate analysis in Sec. 12."}, {"title": "14. Details of Core Subset", "content": "We extract a subset of cases that better represent the full\ndataset and refer to it as core subset. For each specific sce-\nnario, we extracted a subset of cases. During selection of\ncore subset in each scenario, we consider a series of fac-\ntors: number of the full set, number of classes included,\nthe difficulty of cases. As is shown in Tab. 10, We choose\nsize of core subset of each scenario to make it closer to the"}, {"title": "15. Challenge Analysis", "content": "In this part, we explore how the size of the object and the\nvelocity of the target object influence the performance of\nCutie-ReVOS."}, {"title": "15.1. Definition of Object Size", "content": "In our experiment, given a target object o in the image I, its\nsize is measured by the ratio between the mask of the object\n$M_o$ and the area of the image $A_I$, according to\n$$R(o) = \\frac{M_o}{A_I},$$ where R(o) measures the relative size of the object com-\npared to the Image. $M_o$ is the size of the ground-truth mask\nof the object O. $A_I$ is the area of Image I."}, {"title": "15.2. Definition of Velocity", "content": "Generally, the velocity of an object in an image is defined\nas the change in the centroid of the bounding box or mask\nper unit time. However, considering that we cannot measure\nthe relationship between the distance in the image and the\nactual size of the object, we normalize the velocity based\non the size of the object. Given a target object o and the fps"}, {"title": "15.3. Relation between Challenge and Performance", "content": "As the curve in Fig. 10a demonstrated, the smaller the ob- ject's area ratio, the more challenging it is for the model to segment. Besides, for small objects, the performance of ReVOS-Cutie decreases compared to the original Cutie. For large objects, the situation is reversed. Similarly, the curve in Fig. 10b indicates that the relative velocity shows a positive correlation with segmentation dif- ficulty. We observe that when the velocity is more extreme, either too slow or too fast, the performance improvement of ReVOS-Cutie becomes more significant."}, {"title": "16. More Failure Cases", "content": "In this part, we show more failure cases of the current mod- els in Figs. 11 and 12.\nIn case 1 (fry dough) of Fig. 11, the boiling oil makes it difficult to separate the boundaries of the dough sticks accurately. Even for some models, the boiling oil causes the tracking loss. However, our method improves the segment accuracy with visual distribution.\nIn case 2 (assemble puzzles) of Fig. 11, the intra-solid"}]}