{"title": "HARMON: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions", "authors": ["Zhenyu Jiang", "Yuqi Xie", "Jinhan Li", "Ye Yuan", "Yifeng Zhu", "Yuke Zhu"], "abstract": "Humanoid robots, with their human-like embodiment, have the potential to integrate seamlessly into human environments. Critical to their coexistence and cooperation with humans is the ability to understand natural language communications and exhibit human-like behaviors. This work focuses on generating diverse whole-body motions for humanoid robots from language descriptions. We leverage human motion priors from extensive human motion datasets to initialize humanoid motions and employ the commonsense reasoning capabilities of Vision Language Models (VLMs) to edit and refine these motions. Our approach demonstrates the capability to produce natural, expressive, and text-aligned humanoid motions, validated through both simulated and real-world experiments.", "sections": [{"title": "1 Introduction", "content": "Humanoid robots have great potential for seamlessly integrating into the human world due to their human-like physique. We envision these robots operating in human-centered environments and coexisting with people in shared physical spaces. To deploy humanoid robots to work and live with us, the robots should possess the capability to understand natural language instructions akin to our everyday conversations. Furthermore, the ability to exhibit human-like behaviors will render these robots more social and predictable, thus enhancing effective and safe collaborations between humans and robots. To this end, this work focuses on developing a method for generating diverse behaviors of humanoid robots from text descriptions. Our approach takes a natural language description of the desired motions as input and produces corresponding whole-body motions.\nTraining a model to generate natural humanoid motions from language descriptions is challenging due to the lack of large-scale datasets of language-motion pairs. Fortunately, the narrow embodiment gaps between humans and humanoid robots unlock vast amounts of human-centered data sources for model training. Particularly, there is an abundance of large-scale human motion data [1-5]. These data, paired with language descriptions, capture a wide range of human motions. These human motions can be mapped to humanoid robots as informed priors for motion generation. Nonetheless, discrepancies between the human model and the humanoid hinder the effectiveness of direct motion retargeting. First, head and finger motions are typically absent from the human motion dataset, which consequently cannot produce humanoid motions for these body parts. Yet, these motions are critical for the expressiveness of the robot's whole-body motion. Further, retargeting cannot precisely replicate human motion on humanoids due to their kinematic constraints, potentially changing the semantic meaning and legibility of the motion.\nIn this work, we introduce HARMON (Humanoid Robot Motion Generation), which generates whole-body humanoid motions from free-form language descriptions by incorporating human motion priors. We employ PhysDiff [6], a diffusion-based generative model trained on large-scale human motion datasets to generate human motions from language descriptions. PhysDiff incorporates physics constraints during the diffusion process and generates physically plausible human motions. It generates human motion as a sequence of SMPL [7] parameters. We then retarget the human motion to a simulated humanoid robot using inverse kinematics. As mentioned earlier, the generated motion from PhysDiff does not involve hand and finger motions, and retargeting errors can lead to the misalignment between the motion and the language description. To address these issues, we leverage the commonsense reasoning capability of Vision Language Models (VLMs) to edit the humanoid motion. Given a rendering of the humanoid motion and its language description, the VLM generates head and finger motions and refines the arm motion. To realize the whole-body motion on the real robot, we separate the upper- and lower-body motions and control locomotion and upper-body motion independently. Benefiting from the human motion prior and VLM-based motion editing, we can generate natural humanoid motions that align with the language description and execute the motions on simulated and real humanoid robots.\nWe use a Fourier GR1 humanoid robot for the simulation and real-world experiments. We curate a test set with texts from the HumanML3D test set and LLM-generated motion descriptions and conduct a human study to evaluate the quality of the generated motion. HARMON demonstrates natural and text-aligned humanoid motions and is favored by human evaluators on 86.7% of test cases. Furthermore, we execute the generated motions on the physical robot and illustrate diverse and expressive humanoid motions in the real world."}, {"title": "2 Related Work", "content": "Humanoid Motion Control. Humanoid motion control has been widely studied in the computer graphics community, where human avatars are simulated to demonstrate diverse and physically realistic behaviors [8-24]. These methods can demonstrate physically realistic motions on the humanoid avatar by imitating human motions with reinforcement learning. However, deploying them on the real robot is difficult since they use an unrealistic humanoid model (SMPL robot with 23 ball joints and no torque limit). Recent studies [25-27] have explored the imitation of human motions on real and simulated humanoid robots. These approaches involve retargeting human motion to humanoid robots and using reinforcement learning (RL) to train the robots in simulation. Some of these studies [26, 27] have successfully deployed the trained policies to real-world robots. Our work focuses on text-conditional humanoid motion generation. Yoshida et al. [28] also address this problem by directly generating humanoid motion using a Large Language Model. However,"}, {"title": "3 Method", "content": "We study the problem of generating humanoid motions from language descriptions. From a text description $X$, we aim to generate a sequence of robot joint configurations $Q = {q_1, ..., q_T}$, $q_i \\in \\mathbb{R}^C$. Here, $T$ is the sequence length, and $c$ is the number of joints of the humanoid robot.\nFig. 2 depicts our proposed method, HARMON. Firstly, we generate human motion based on the language description and retarget this human motion to create the initial humanoid motion (Sec. 3.1). To improve the alignment between the humanoid motion and the language description, we employ a VLM to generate finger and head motions and iteratively adjust the body motion (Sec. 3.2). Finally, we execute the generated motions on the real humanoid robot (Sec. 3.3)."}, {"title": "3.1 Retargeting Text-Conditioned Human Motion", "content": "Training a model to generate humanoid motions from language descriptions directly is challenging due to the absence of a paired dataset. We utilize a human motion generation model to generate human motions from language descriptions, which we then retarget to the humanoid robot.\nGiven the text description $X$, we use PhysDiff [6], a physics-guided motion diffusion model, to generate the corresponding human motion. The output is a sequence of SMPL [7] parameters $P = {(\\theta_1, t_1),..., (\\theta_T, t_T)}$, where $\\theta_i$ is the joint rotations and $t_i$ is the root translation at time step $i$. The SMPL model also includes a body shape parameter $\\beta \\in \\mathbb{R}^{10}$, encoding attributes such as height and size. Given $\\theta_i, t_i, \\beta$, the positions of each human joint $J_i = S(\\theta_i, t_i, \\beta)$ are computed using the SMPL model $S$. Each $J_i \\in \\mathbb{R}^{24 \\times 3}$ contains the positions of 24 human joints at time step $i$.\nBefore retargeting human motion to humanoids, we first minimize the disparity between human body shape and humanoids to ensure the robots can reach human joint positions. Inspired by He et al. [27], we set the SMPL model and humanoid model to the same T pose, select 17 corresponding joint pairs on both models, and minimize the joint position differences. We employ the Adam optimizer [60] to minimize the joint position difference by optimizing $\\beta$. The optimized $\\beta^*$ is subsequently used to compute joint positions from the SMPL parameters."}, {"title": "3.2 VLM-Based Humanoid Motion Editing", "content": "The retargeting process initializes humanoid motion based on the generated human motion. However, due to the differences in the kinematic structures between the SMPL human model and the humanoid, the retargeted motion might not fully align with the intended language description. Specifically, the humanoid can actuate the neck and dexterous hands, whereas the generated human motion lacks head and finger movements. Consequently, the retargeted motion does not fully exploit the humanoid's potential for expressive motion. Additionally, the retargeting process cannot precisely replicate human motion on the humanoid, potentially resulting in a humanoid motion with a different semantic meaning from the original human motion. These factors can cause a misalignment between the humanoid motion and the language description. To address this issue, we render the retargeted motion into videos and utilize GPT-4 to edit the humanoid motion for better alignment."}, {"title": "3.3 Motion Execution on the Real Robot", "content": "Directly executing the whole body joint configuration sequence $Q^*$ on the real humanoid is infeasible because the kinematic motion does not account for the robot's dynamics and balance. Therefore, following Cheng et al. [26], we separate the lower- and upper-body motion in the real robot experiment. We simplify the lower body motion into locomotion commands and utilize a Zero Moment Point (ZMP)-based [62] controller for locomotion. These locomotion commands are extracted from the trajectory of the humanoid robot's pelvis projected onto the ground plane. Simultaneously, we execute the upper-body motion from the generated joint configuration sequence $Q^*$ on the real robot using joint position control. By separating locomotion and upper-body motion, we can successfully execute our generated motion on the real robot."}, {"title": "4 Experiments", "content": "4.1 Evaluation Setup and Baselines\nEvaluating whole-body humanoid motion generation is challenging due to the absence of datasets containing paired language and humanoid motion data. We conduct a human study to assess our generated motions. We curate a test set of approximately 50 language descriptions of motions. The first part of this test set comprises texts randomly sampled from the HumanML3D [1] test set, focusing primarily on body motions without involving head and finger movements. To evaluate whole-body humanoid motions, we use GPT-4 to generate descriptions that include head and finger motions, forming the second part of the test set.\nWe generate humanoid motions from these descriptions using HARMON and compare them against three baselines:\nVLM-based Motion Generation. Inspired by Yoshida et al. [28], this baseline generates humanoid motions directly using a Large Language Model (LLM). In this approach, we exclude the human motion prior from HARMON and initiate VLM-based motion editing from a static SMPL T-pose. Since VLM-based editing only modifies upper-body joints, we incorporate the lower-body motion from HARMON for a fair comparison. This allows us to evaluate the significance of the human motion priors."}, {"title": "B Additional Experiments", "content": "In Table. 1, we conducted a small-scale study using a subset of text descriptions from our previous test set. In this study, we presented the results of the new variations implemented for rebuttal, alongside the original Harmon results, to human participants. The participants were asked to select the results that best aligned with the text descriptions. They were allowed to select multiple results if they felt they all matched the description to a similar extent. We then calculated the average selection rate for each method. We also report the selection rate relative to Harmon, which helps to highlight the performance differences between the variations and Harmon:\n\u2022 Collision Mesh: Feeding the rendering of the collision mesh to the VLM instead of the visual mesh. Note that we still presented the rendering of the visual mesh to human participants for a fair comparison.\n\u2022 Increased Frame Number: Increasing the number of frames from 4 to 9.\n\u2022 Gemini: Switching the VLM from GPT-40 (69.1% on MMMU validation) to Gemini-1.5 Pro (62.2% on MMMU validation).\n\u2022 Flexible Motion Editing Primitives: Utilizing flexible motion editing primitives. The VLM selects an appropriate primitive and outputs the parameters. For example, it can choose the \"move up\" primitive for the left wrist and specify a distance, such as 0.12 m."}]}