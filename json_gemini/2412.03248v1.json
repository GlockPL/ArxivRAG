{"title": "AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning", "authors": ["Yiwu Zhong", "Zhuoming Liu", "Yin Li", "Liwei Wang"], "abstract": "Large language models (LLMs) have enabled the creation of multi-modal LLMs that exhibit strong comprehension of visual data such as images and videos. However, these models usually rely on extensive visual tokens from visual encoders, leading to high computational demands, which limits their applicability in resource-constrained environments and for long-context tasks. In this work, we propose a training-free adaptive inference method for multi-modal LLMs that can accommodate a broad range of efficiency requirements with a minimum performance drop. Our method consists of a) iterative token merging based on embedding similarity before LLMs, and b) progressive token pruning within LLM layers based on multi-modal importance. With a minimalist design, our method can be applied to both video and image LLMs. Extensive experiments on diverse video and image benchmarks demonstrate that, our method substantially reduces computation load (e.g., a 7-fold reduction in FLOPs) while preserving the performance of video and image LLMs. Further, under a similar computational cost, our method outperforms the state-of-the-art methods in long video understanding (e.g., +4.6 on MLVU). Additionally, our in-depth analysis provides insights into token redundancy and LLM layer behaviors, offering guidance for future research in designing efficient multi-modal LLMs. Our code will be available at https://github.com/LaVi-Lab/AIM.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) [6, 52, 63, 80, 89] have been recently adapted for visual understanding, fostering the developments in both image [4, 7, 22, 38, 96] and video LLMs [33, 36, 44, 77, 91]. However, these multi-modal LLMs usually rely on a large number of visual tokens generated by visual encoders [56, 86], especially for video data where token counts can reach thousands per video. Such high token number requires extensive computational resources for both training and inference, restricting their use in real-world applications (e.g., real-time processing on mobile devices). Further, as the number of video frames increases, the total number of tokens also grows, limiting the model's capacity to handle dense video frames. This often results in the loss of crucial temporal information, which is essential for comprehending long videos [60, 88, 94].\nTo bridge the gap, we propose leveraging the inherent redundancy present in visual data to develop adaptive inference for multi-modal LLMs. Adaptive inference dynamically adjusts a model's computational load during inference based on contextual factors [19], e.g., image content, computational constraints, or desired accuracy levels. Since retaining all visual tokens during inference is often unnecessary, our key insight is to strategically select these tokens throughout the inference process. This approach allows for controlling the amount of computation necessary for inference, so as to optimize the model's efficiency while preserving its accuracy."}, {"title": "2. Related Work", "content": "Multi-modal LLMs. Large language models [46, 50, 51] trained on large-scale datasets exhibit strong performance in text understanding and generation tasks, laying the foundation for developing multi-modal LLMs with visual instruction tuning [7, 39, 96]. To further enhance model capabilities, recent work has focused on increasing visual input resolution [38, 40] and creating higher-quality instruction-tuning datasets [30]. Meanwhile, advancements in video-based LLMs have also been achieved by extending visual instruction tuning to video modality. For example, VideoChatGPT [44] pioneers video-based instruction tuning through the creation of video question-answer pairs, while Video-LLaVA [36] improves model performance by employing text-aligned video features [95]. In parallel, several models have extended their capabilities from static images to dynamic video inputs, such as LLaVA-OneVision [30, 92], Qwen2-VL [66], and mPlug-Owl3 [82].\nRecent work has focused on improving video LLMs for long video understanding. Given the challenge posed by the large number of tokens in long videos, recent approaches [10, 29, 49, 68, 70, 73, 88] adopt Q-former [32] or state space models [18] to aggregate the information before feeding visual tokens into the LLMs. Others extend the context window of LLMs [21, 71, 88] or design sequence parallelism from a systems perspective [79] to process the long video sequence. Built on top of the pre-trained image and video LLMs, our adaptive inference method aims to reduce token redundancy during inference and adapt to diverse efficiency requirements while maintaining model performance.\nToken Merging and Pruning. Transformers are widely used in deep learning models, yet they come with high computational demands. Token merging and pruning methods aim to reduce the number of tokens to reduce this load, commonly developed in both NLP models [15, 26, 27, 64, 81] and vision models [8, 28, 41, 57, 62, 72, 84]. These approaches typically require fine-tuning after token reduction. By contrast, some training-free methods have been developed for efficient vision Transformers [3, 11, 65]. Different from these works, we propose a training-free approach tailored for multi-modal LLMs."}, {"title": "3. Method", "content": "Multi-modal LLMs for images and videos usually require processing a high volume of visual tokens, resulting in substantial computational costs, especially for video tasks. Such computational load arises from the redundancy within visual tokens. To tackle this efficiency problem, we propose a training-free adaptive inference method by pruning redundant tokens in two stages, as shown in Figure 2. First, before visual tokens enter the LLM, we merge highly similar visual tokens, significantly reducing their count while preserving performance. Second, within the LLM, we further rank the visual tokens and remove less important ones at each layer, by applying the Page Rank algorithm to the self-attention weights. Together, our approach enables adaptive inference of multi-modal LLMs, reducing computational demands without compromising reasoning performance, and moreover, supports a broad range of computational requirements."}, {"title": "3.1. Multi-modal LLMs", "content": "Given an image, a typical image LLM [38] first transforms the image into visual tokens via a visual encoder, then projects visual tokens via an adapter (e.g., multilayer perceptron), and finally concatenates the projected visual tokens and text tokens to perform LLM reasoning. Video LLMs [30] adopt a similar approach to image LLMs, except that the input visual data become uniformly sampled images (video frames), and adaptive pooling is oftentimes applied before LLMs to reduce the number of visual tokens. Despite this pooling, the visual token count remains high (e.g., thousands of tokens per video), resulting in substantial computational demands for the LLM.\nOur training-free method is designed to reduce the redundancy of visual tokens, by applying it to the input visual tokens of the LLM and to the visual tokens at each LLM layer. We denote the visual tokens right before LLM as $v^0 \\in \\mathbb{R}^{N_0 \\times D}$ where $N_0$ is the initial number of visual tokens. Within LLM, we denote the input visual tokens and the input text tokens at the l-th layer as $v^l \\in \\mathbb{R}^{N_l \\times D}$ and $t^l \\in \\mathbb{R}^{M_l \\times D}$, respectively. $l \\in [1, L]$ denotes the index of LLM layer and $N_l$ ($M_l$) represents the number of visual (text) tokens at layer l."}, {"title": "3.2. Token Merging before LLM", "content": "The input to LLMs often consists of hundreds of visual tokens for an image or thousands for a video, resulting in substantial redundancy. Merging these redundant tokens before they enter the LLM can significantly reduce computational demands. Inspired by [3], we merge the visual tokens with high similarity, where similarity is quantified by the cosine similarity between token embeddings. As illustrated in Figure 2, given an initial set of visual tokens $v^0$ at the first LLM layer, we divide adjacent tokens into sets A and B, calculate pairwise similarity scores between the sets, and identify the closest matching token in set B for each token in set A. We then merge the token pairs with the highest similarity scores by averaging their embeddings. This process reduces the number of tokens by half at most. We repeat this merging process iteratively (e.g., twice) to achieve the desired retention ratio.\nFor videos, we merge the visual tokens within individual frames. Empirical studies suggest that merging tokens within individual frames has minimal impact on final reasoning performance in video tasks, while merging across frames can be harmful. We hypothesize that merging across frames disrupts temporal order of tokens, leading to a loss of crucial temporal information for video understanding."}, {"title": "3.3. Token Pruning within LLM", "content": "After merging similar visual tokens, we concatenate the merged visual tokens $v^l$ with text tokens $t^l$, forming $x^l = [v^l; t^l]$, which is then passed to the LLM. At each LLM layer, we retain important tokens and prune the less important ones, based on the attention weights computed at each Transformer layer. Specifically, we compute the importance score of each token by applying the Page Rank algorithm [53, 65], using the attention weights as the adjacency matrix. The importance score $s_i^l$ of token $x_i^l$ at layer $l$ is computed as follows:\n$s_i^l = \\frac{1}{N^l + M^l} \\sum_{j=1}^{N^l + M^l} A_{ij}^l s_j^l$ (1)\nwhere $s_i^l$ is initialized uniformly over all tokens and $A^l$ represents the softmax-normalized attention weights. Based on these importance scores, we only prune visual tokens and retain the most important visual tokens, leaving text tokens intact. Our rationale is that pruning text tokens substantially degrades performance, likely because multi-modal LLMs rely on text tokens to perform text-centric reasoning. As a result, the number of visual tokens input to the next layer is $N_0 \\times r^l$, where $r^l$ is the retention ratio at layer l.\nFurther, we design a scheduler to control the retention ratio $r^l$ at the $l$-th layer. It determines the number of visual tokens retained at each LLM layer. Specifically, it is designed as a piece-wise function:\n$\\gamma_l = \\begin{cases}\n1-k(l-l_1), & \\text{if } l < l_1 \\\\\n1, & \\text{if } l_1 \\leq l \\leq l_2 \\\\\n0, & \\text{if } l > l_2\n\\end{cases}$ (2)\nwhere $l$ denotes the layer index and $k = \\frac{1}{l_2 - l_1}$ represents the slope. In this function, $l_1$ determines from which layer the token pruning starts, $l_2$ defines the layer where the visual tokens are pruned at all, and the difference of $l_1$ and $l_2$ controls the pruning progress. By adjusting the parameters $l_1$ and $l_2$, our approach allows a flexible balance between reasoning accuracy and computational efficiency.\nThis scheduler design is supported by our empirical findings: pruning visual tokens at the early layers negatively impacts the performance, while in contrast, pruning a large proportion of visual tokens at later layers can still maintain performance. We hypothesize that the LLM prioritizes cross-modal fusion in early layers and shifts focus toward text reasoning in later layers."}, {"title": "3.4. Adaptive Inference", "content": "By combining token merging and token pruning, our method enables adaptive inference that can meet diverse computational demands. Specifically, we can vary the retention ratio of token merging and the scheduler parameters (l1 & l2) of token pruning, to create a broad range of accuracy-efficiency trade-offs. The specific inference configuration can be determined by the computational requirements in real use cases (e.g., FLOPs and prefill time). We demonstrate adaptive inference in our experiments and showcase that our method achieves considerable computation reduction while preserving accuracy."}, {"title": "4. Experiments", "content": "In this section, we first introduce our implementation details and benchmarks, and then present our results, including the results of video benchmarks, image benchmarks, ablation studies, and adaptive inference.\nImplementation Details. Our method is applied during the inference of pre-trained multi-modal LLMs. For video LLMs, we follow the hyperparamers as base model LLaVA-OV-7B [30], such as sampling 32 frames per video unless otherwise noted. It uses Qwen2 [80] as LLM with 28 layers in total. For our method, we set the retention ratio of token merging as 25%, l\u2081 as 14, and 12 as 22. For image LLMs, we follow the hyperparamers as base model LLaVA-1.5-7B [38] which adopts Vicuna [6] as LLM with 32 layers in total. For our method, we set the retention ratio of token merging as 12.5%, l\u2081 as 13, and 12 as 21. We compute FLOPs and prefill time of LLMs using the library from LLM-Viewer [85], and assume 100 (40) text tokens for video LLMs (image LLMs).\nBenchmarks. We evaluate our method on both video and image benchmarks. For video LLMs, we consider the following widely adopted benchmarks. VideoMME [14] is a comprehensive video benchmark with diverse video durations and video domains. MLVU [94] highlights the reasoning for long videos. Egoschema [45] focuses on ego-centric video understanding. MVBench [34] and NextQA [75] focus on temporal action understanding. PercetionTest [55] is a comprehensive video benchmark that evaluates perception and reasoning skills. For image LLMs, we report results on 7 benchmarks. GQA [23] and VQA-v2 [16] are classic VQA datasets that assess visual reasoning ability, with GQA placing a particular emphasis on visual attributes. MME [13] serves as a broad benchmark for evaluating both perception and cognitive abilities. TextVQA [61] specifically measures OCR reasoning skills. SQA-IMG [43] covers questions across topics in natural sciences, language sciences, and social sciences. MMB [42] tests perception and reasoning capabilities, while POPE [35] addresses the problem of object hallucination."}, {"title": "4.1. Video Benchmarks", "content": "Table 1 and Table 2 show the results of our method applied to base video LLM and our method accepting more sampled video frames, respectively.\nSetup. We choose LLaVA-OV-7B [30] as our base model and follow its evaluation protocol. Specifically, during inference of the pre-trained base model, we apply our token merging to the input visual tokens of Qwen2 LLM and perform token pruning at layers of Qwen2. We measure our model performance on diverse video benchmarks and report efficiency metrics (FLOPs and prefill time) and accuracy metrics of each video benchmark.\nBaselines. We choose baselines as FastV [5] which prunes visual tokens at a particular LLM layer, and LLaVA-1.5-Prumerge [59] which leverages the key-query pairs in the visual encoder to prune and merge visual tokens. We run their official implementation with the pre-trained LLaVA-OV-7B model in the training-free setting.\nAccuracy-efficiency balance. As Table 1 shows, our model achieves substantial reductions in computational demands compared to the base model LLaVA-OV-7b (e.g., 14.76 vs. 99.63 FLOPs), decreasing FLOPs by a factor of 6.8 and prefill time by 8.0, with little or no performance drop across diverse benchmarks. Additionally, compared to the baseline models FastV and LLaVA-Prumerge, our model consistently achieves higher performance on most benchmarks, while only requiring 62.4% FLOPs and 63.3% prefill time at most. These results indicate that our method effectively reduces redundancy in visual tokens, providing an optimal balance between accuracy and efficiency.\nMore sampled frames improve long video understat-ing with close efficiency. In Table 2, our model significantly reduces the FLOPs and prefill time of the base pre-trained model, enabling the sampling of more frames in videos (e.g., from 32 to 192 frames). With comparable total computation (i.e., 99.27 vs. 99.63 FLOPs), our model improves performance on long video understanding benchmarks, especially on long video benchmark MLVU, with a notable gain of +4.6. This improvement is attributed to our method's ability to retain essential visual tokens with much less redundancy, while densely sampled frames capture additional information crucial for long video comprehension."}, {"title": "4.2. Image Benchmarks", "content": "The results are reported in Table 3, comparing our model with the base model and baseline method.\nSetup. We choose LLaVA-1.5-7B [38] as our base model following its evaluation protocol. Again, during inference of the pre-trained base model, we merge the input visual tokens of Vicuna LLM and prune visual tokens at Vicuna's layers. We test our model on several image benchmarks and report efficiency metrics (FLOPs and prefill time) and accuracy metrics of each image benchmark.\nBaselines. We again choose the baseline as FastV [5] and LLaVA-1.5-Prumerge [59] which provide training-free results on image benchmarks. We also compare with a variant LLaVA-1.5-Prumerge+ which trades computation for reasoning performance. Both our models and baseline models are evaluated in the training-free setting with the pre-trained LLaVA-1.5-7B as the base model."}, {"title": "4.3. Ablation Study", "content": "In this section, we conduct ablation studies for our method, by adjusting the retention ratio of token merging, the parameters of the token pruning scheduler, and the pruning strategy. Their results are presented in Table 4, Table 5, and Table 6, respectively.\nSetup. We perform experiments on the VideoMME benchmark as it evaluates models on the videos with various durations and diverse domains. Specifically, we vary the retention ratio of token merging (Table 4), scheduler parameters in token pruning (Table 5), and the strategy whether prune text tokens or not (Table 6), respectively. For token merging, we disable token pruning and investigate the effects of altering the retention ratios. For token pruning, we set the retention ratio of token merging as 25% and study the effects of various pruning schedulers or strategies. Following our main experiments, the metrics include the efficiency aspect (FLOPs and prefill time) and the accuracy aspect.\nToken merging substantially reduces redundancy. As shown in Table 4, video reasoning performance remains stable when the retention ratio is set at 25% or higher, while FLOPs and prefill time are significantly reduced (e.g., to only 23% of FLOPs and 19% of prefill time relative to the base model). This suggests that most visual tokens are redundant, with only around one-quarter providing essential information for video understanding tasks. Moving forward, as the retention ratio is further reduced, there is a gradual decline in reasoning performance, accompanied by substantial savings in FLOPs and prefill time (e.g., down to 3% FLOPs and 2% prefill time of the base model). These findings indicate that with retention ratios below 25%, the model will trade accuracy for efficiency, making it suitable for scenarios requiring high efficiency with less accuracy demands, such as mobile devices.\nAccuracy-efficiency trade-offs controlled by token pruning scheduler. As shown in Table 5, by adjusting the pruning scheduler parameters 11 and 12, our method enables a broad range of accuracy-efficiency trade-offs. FLOPs decrease from 22.90 to 6.71, and prefill time drops from 83.94 to 28.18, with performance unaffected until FLOPs and prefill time are reduced to approximately half. Furthermore, in each block, when comparing the first row with subsequent rows, we observe a reduction in FLOPs and prefill time with little to no impact on performance. For example, Exp. 6 achieves a 16% reduction in both FLOPs and prefill time compared to Exp. 5, while maintaining performance. These findings validate our token pruning design, demonstrating its flexibility in controlling LLM computations and achieving optimal accuracy-efficiency trade-offs.\nEarly LLM layers vs. later LLM layers. We find that LLM emphasizes multi-modal fusion at early layers and shifts the focus to text-centric reasoning at later layers. For example, experiments 5, 8, and 10 in Table 5 remove all visual tokens after layer 22, layer 15, and layer 8, respectively. Performance is maintained when visual tokens are removed starting from layer 22 (Exp. 5), but it declines when tokens are removed from layer 15 (Exp. 8) and more significantly from layer 8 (Exp. 10). Notably, removing tokens as early as layer 8 causes a substantial performance drop (e.g., from 58.0 to 41.9). Based on these findings, we adopt a strategy (Exp. 16) that retains visual tokens in early layers, gradually prunes them in middle layers, and completely removes them in later layers.\nText tokens matter in LLM layers. In Table 6, involving text tokens in the pruning process results in a substantial performance drop (e.g., from 58.2 to 45.7). This finding aligns with the understanding that LLMs primarily perform text-based reasoning, making it essential to retain text tokens throughout inference."}, {"title": "4.4. Adaptive Inference", "content": "As shown in Table 7, by combining token merging and token pruning, our method supports a broad range of accuracy-efficiency trade-offs. For instance, compared to the base model (99.63 FLOPs and 58.2 accuracy), our default configuration achieves the same accuracy with significantly reduced FLOPs (14.76), while an efficient configuration reduces FLOPs even further (2.51) with an acceptable performance decrease (50.9 accuracy). Overall, our approach spans a 40-fold reduction in FLOPs with less than a 13% drop in accuracy. This flexibility is enabled by adjusting the key parameters, including the retention ratio in token merging and 11 & 12 in token pruning. These configurations make our adaptive inference method suitable for various devices and efficiency requirements, such as AR glasses, mobile phones, personal computers, and robots."}, {"title": "5. Conclusion", "content": "In this work, we present a training-free approach of adaptive inference for multi-modal LLMs, through token merging based on visual similarity and token pruning based on multi-modal importance. Extensive experiments demonstrate that our method significantly reduces computational demands while maintaining reasoning performance, such as a 6.8/3.7-fold reduction in FLOPs across video/image benchmarks and improved SOTA performance on long video benchmarks. Additionally, our key findings reveal that only a small fraction of visual tokens are necessary for multi-modal understanding, and progressive token pruning across LLM layers further optimizes computational efficiency. We hope that our work will provide a foundation for future advancements in adaptive multi-modal LLMs, in which the accuracy-efficiency tradeoffs can be dynamically adjusted in response to varying computing environments."}]}