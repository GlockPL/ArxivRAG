{"title": "From Image to Video: An Empirical Study of Diffusion Representations", "authors": ["Pedro V\u00e9lez", "Luisa F. Polan\u00eda", "Yi Yang", "Chuhan Zhang", "Rishabh Kabra", "Anurag Arnab", "Mehdi S. M. Sajjadi"], "abstract": "Diffusion models have revolutionized generative modeling, enabling unprecedented realism in image and video synthesis. This success has sparked interest in leveraging their representations for visual understanding tasks. While recent works have explored this potential for image generation, the visual understanding capabilities of video diffusion models remain largely uncharted. To address this gap, we systematically compare the same model architecture trained for video versus image generation, analyzing the performance of their latent representations on various downstream tasks including image classification, action recognition, depth estimation, and tracking. Results show that video diffusion models consistently outperform their image counterparts, though we find a striking range in the extent of this superiority. We further analyze features extracted from different layers and with varying noise levels, as well as the effect of model size and training budget on representation and generation quality. This work marks the first direct comparison of video and image diffusion objectives for visual understanding, offering insights into the role of temporal information in representation learning.", "sections": [{"title": "1. Introduction", "content": "The field of computer vision has made remarkable strides towards imbuing machines with the ability to see and interpret the visual world. A key challenge in realizing this goal lies in learning effective representations that capture both the rich semantic information and dynamic 4D structure (3D & motion) inherent in real-world visual data. Leading approaches in visual representation learning have primarily focused on contrastive learning and reconstruction. Contrastive models can be exemplified by DINO [8] which applies knowledge distillation between augmented views of images, and CLIP [41] which aligns image and text representations, while MAE [23] and I-JEPA [1] are commonly used reconstruction models that predict masked pixels and features, respectively.\nMeanwhile, generative models have demonstrated an unprecedented ability to synthesize novel, photo-realistic imagery [27, 29]. Among generative approaches, diffusion models [25] have emerged as the state-of-the-art for both image and video generation, achieving remarkable results in synthesizing high-quality visual content [19]. Image diffusion models have also demonstrated their capacity to learn powerful representations for downstream tasks such as image classification [28], depth estimation [47] and semantic keypoint matching [24, 50], suggesting that the denoising process enables these models to acquire a deep understanding of visual semantics and structure. However, despite the recent emergence of video diffusion models capable of generating high-fidelity video content, the representational power of these models remains largely unexplored, particularly in the context of dynamic scene understanding. This leaves a crucial question unanswered: how effectively do video diffusion models capture the interplay between motion and spatial scene understanding, and how do they perform compared to image diffusion models?"}, {"title": "2. Related Work", "content": "Diffusion models [25] have revolutionized the fields of image and video synthesis as they offer unprecedented generation quality [27, 29]. Whereas they once relied heavily on U-Nets [43], transformer-based diffusion models have since gained traction [39]. We refer the reader to Zhang et al. [64] for a survey on diffusion-based image generation.\nImage diffusion for visual understanding. Beyond their impressive generative capabilities, image diffusion models have proved remarkably effective in perception tasks. For example, DDPM-Seg [4] utilizes intermediate features of a U-Net-based diffusion model and excels at semantic segmentation. It has further been extended to different architectures and showed that the best-performing features are found in the middle upsampling layers using small noise levels [59]. Pre-trained text-to-image diffusion models have also been applied to panoptic segmentation [60].\nIn terms of correspondence tasks, DIFT [50], a method based on extracting features from diffusion models, showed that features extracted in earlier layers with larger timesteps tend to be more semantically meaningful, whereas lower-level features with smaller timesteps focus more on low-level details. A related work found that merging diffusion representations with DINOV2 [36] features improves performance on semantic correspondence [63]. Luo et al. [30] demonstrated that having an aggregation network that learns weights for all features maps across all layers and timesteps performs better than manually selecting layers and timesteps. Clark and Jaini [14] proposed a mechanism that makes use of the denoising scores for all possible labels, thereby creating a zero-shot ImageNet classifier which is however computationally impractical. Another work compares various types of readout architectures with similar parameter count and has found attentive readout to perform best [34].\nFurther works propose diffusion models as a general foundational model for a suite of image understanding tasks. For example, Zhao et al. [66] showed that diffusion models excel at both semantic segmentation and depth estimation, while Yang and Wang [61], showed state-of-the-art performance on several image classification, semantic segmentation and landmark detection benchmarks using diffusion models. We refer the reader to a recent survey on the connection between diffusion models and image representation learning for an extensive overview [19].\nVideo representation learning. Building on image representation learning, research is naturally progressing to video analysis for enhanced spatial understanding [37]. Contrastive and masked modeling are popular choices here: V-JEPA [5] predicts video embeddings from a set of random crops in a self-supervised student-teacher setup, while VideoMAE [18, 51, 57] pre-trains vision transformers by masking and reconstructing random video patches. Video-Prism [65] combines both: it uses video-language contrastive learning in a first stage to capture semantic content, while a second stage uses masked modeling with global-local distillation and token shuffling.\nRecent works have started to explore the potential of image diffusion models on video understanding [12, 35]. Since high-quality video generation with diffusion models is only a recent development [22, 29], the literature on investigating their features is sparse: video diffusion representations have recently been found to produce temporally consistent video depth estimates [26], and they have been applied to object segmentation Zhu et al. [69], though without a direct comparison with image diffusion models. A concurrent work with ours, [2], evaluates the performance of the video diffusion model SVD [6] on video understanding tasks, and finds that it outperforms image baselines, in particular Stable Diffusion (SD) [42]. While SVD builds upon SD's U-Net architecture by incorporating temporal convolution and attention layers, it nearly doubles the parameter count (865 M to 1.5 B) which weakens the expressiveness of the comparison. In this work, we specifically choose WALT [22] for our investigations due to its hybrid architecture that allows a more apt comparison. Finally, also concurrently to our work, Man et al. [32] evaluate image and video representation models with a focus on 3D scene understanding (semantic and geometric understanding) and vision-language reasoning tasks and find diffusion models to excel at geometric tasks."}, {"title": "3. Method", "content": "We lead this section with a recap of latent diffusion models and the WALT model in Sec. 3.1 and Sec. 3.2, respectively. Section 3.3 describes how WALT operates on images vs. video, followed by a description of the probing framework for quantitative evaluations in Sec. 3.4.\n3.1. Latent Diffusion Models\nDiffusion Models [48] are probabilistic generative models that can learn a distribution by denoising a normally distributed variable. They are based on two stages, a forward diffusion stage and a backward diffusion stage. In the forward diffusion stage, the input data is gradually corrupted by adding noise with a fixed variance schedule until the data distribution degrades into a standard Gaussian distribution. In the backward stage, the model reconstructs the original input data by learning to gradually denoise the signal.\nLatent Diffusion Models (LDMs) [6] apply the diffusion process in the latent space of a Vector Quantized Variational Autoencoder (VQ-VAE) [17, 53] which helps to significantly reduce the computation requirements when compared to using raw pixels. The VQ-VAE is composed of an encoder $\\mathcal{E}(x)$ that encodes a video or an image $x \\in \\mathbb{R}^{T\\times H\\times W\\times 3}$ into a compressed latent representation $z \\in \\mathbb{R}^{t\\times h\\times w\\times c}$ and a decoder $\\mathcal{D}$ that reconstructs the input data from the latent $\\hat{x} = \\mathcal{D}(z)$.\nThe inverse diffusion process is modeled by applying a learnable function $f_\\theta (z_t, t)$ to the noised latents at each step to recover the original input. More formally, $f_\\theta (z_t) \\approx \\nabla \\log p(z_t)$, where $p(z_t)$ is the probability density function of the latent space at step $t$, $z_t = \\sqrt{\\alpha(t)}z_0 + \\sqrt{1 - \\alpha(t)}\\epsilon$ is a noisy version of $z_0$, $\\epsilon \\sim \\mathcal{N}(0, I)$, $t \\in [0, 1]$, and $\\alpha(t)$ is a predefined monotonically decreasing function from 1 to 0. The function $f_\\theta$ is parameterized by a neural network, which is trained with the denoising objective defined as\n$\\mathbb{E}_{z \\sim p_{data}, t \\sim \\mathcal{U}(0, 1), \\epsilon \\sim \\mathcal{N}(0, 1)}[\\left \\|\\epsilon - f_\\theta(z_t; C, t) \\right \\|^2]$,\nwhere $C$ is the condition, e.g., class labels or text prompts. Beginning with a noise sample $z_1 \\sim \\mathcal{N}(0,I)$, an iterative sampling process repeatedly applies the model $f_\\theta (z_t; C, t)$ to progressively refine an estimate of a clean latent sample $z_0$. This refined latent sample is then decoded, transforming it back into the pixel space. In this work, we do not make use of the decoder since we extract features from the main transformer module.\n3.2. Windowed-Attention Latent Transformer\nWindowed-Attention Latent Transformer (WALT) [22], a transformer-based video diffusion model conditioned on text prompts, is selected for this study because the same architecture can be used for both image and video generation, leading to a fair comparison. Although, a study with a single diffusion model is not ideal, the same fair comparison is not feasible with open-sourced video diffusion models given the architecture size disparities with their image counterparts. For example, there is a significant architecture size disparity between Stable Image Diffusion 2.1 (SD 2.1) [42] and its temporal extension, Stable Video Diffusion (SVD) [6]. More precisely, SD 2.1 uses a U-Net architecture with 865 million parameters, while SVD inserts temporal convolution and attention layers after every spatial convolution and attention layer of SD 2.1, which leads to an additional 656 million parameters.\nWALT is used as a frozen backbone to train light-weight readout heads for downstream perception tasks. It leverages a causal 3D CNN encoder of the MAGVIT-v2 tokenizer [62] to jointly compress images and videos into a shared latent space, which allow the model to be trained on massive image-text and video-text datasets.\nThe input to the model is a batch of latent tensors $z \\in \\mathbb{R}^{(1+m)\\times h\\times w\\times c}$, generated by the 3D CNN encoder, which are first passed through an embedding block, referred to as Block 0, to be independently encoded as a sequence of non-overlapping patches along with learned position embeddings [56]. The first frame is encoded independently from the remaining video frames, allowing static images to be treated as videos with a single frame. In particular, the WALT checkpoints used in this paper were shared by the authors and were trained to process sequences of 17 frames. The video frames are tokenized by the 3D CNN encoder with 5 temporal latents, where the first latent represent the initial frame and the remaining $m = 4$ represent the remaining 16 frames. In terms of the spatial compression factor of the latents, it is set to 8 for both width and height.\nThe WALT architecture introduces a design variation for the transformer in order to reduce the high compute and memory cost of processing image and video tokens. The variation consists of computing self-attention in windows instead of using the traditional global self-attention modules. More precisely, the transformer consists of a con-"}, {"title": "3.3. WALT for images and video", "content": "WALT can be used in both video generation, referred to as V-WALT, and image generation modes. When used for image generation, any given latent in the spatio-temporal blocks only attend to itself. For comparison purposes, one drawback of such design is that not only the temporal attention is removed but also the spatial attention. Given that the spatio-temporal blocks in V-WALT perform both spatial and temporal attention, it is fairer to compare with an image counterpart of the WALT model where window-restricted spatio-temporal attention blocks are replaced by window-restricted spatial attention blocks of the same number of parameters. In that way, we can directly measure the impact of adding temporal attention. Such image counterpart was trained for this paper and is referred to as I-WALT. Note that the window-restricted spatial and spatio-temporal blocks only differ in the windows sizes, and therefore, I-WALT and V-WALT share the same architecture.\nFor training V-WALT, an internal dataset composed of images and videos was used. The same dataset was employed for training I-WALT, but instead of using full videos, frames were randomly extracted from each video. The training settings of I-WALT are the same as WALT."}, {"title": "3.4. Probing Framework", "content": "A probing framework is introduced to extract video representations from the WALT model and subsequently apply a task-specific readout head for various video understanding tasks. The process starts by adding noise to the latent representations of the input data at a time step t to simulate the forward diffusion process before passing them through the denoiser model. Only a single forward pass of the input through the diffusion model is necessary to extract its visual representation, as opposed to going through the entire multi-step generative diffusion process. The forward pass uses a null text embedding. Subsequently, activations of the transformer intermediate blocks are extracted to train task-specific readout heads that will interpret the learned representations. A diagram illustrating the probing framework is shown in Fig. 2. In order to determine an adequate timestep t and the most representative activation block l, we run ablations described in Sec. 4.3 and showcased in Fig. 5.\nThe WALT model is evaluated on representative visual perception tasks, ranging from pure semantics to spatiotemporal understanding: image classification, action recognition, monocular depth estimation, relative camera pose prediction to visual correspondence. For the evaluation on visual perception tasks, we largely follow the probing methodology of Carreira et al. [11].\nImage classification. Image classification, characterized by its purely semantic nature, is one of the most fundamental areas in computer vision. Within this area, the tasks of object classification, scene recognition and fine-grained visual classification are selected for downstream evaluation of WALT. An attentive readout is used for this task [5]. The cross-attention layers are trained with a learnable query token, and the output of the cross-attention is added to the query token and then passed to a two-layer MLP with GeLU activation, followed by layer normalization and a linear classification layer. The readout is trained with the softmax cross-entropy loss. ImageNet [44] and Places365 [67] are used for object and scene classification, respectively. For fine-grained visual classification we use iNaturalist 2018 (Inat2018) [54] which contains visually similar plant and animal species. Top-1 classification accuracy is used for all the image classification tasks.\nAction recognition. Understanding actions in videos often requires capturing temporal dependencies between frames, i.e. the model needs to understand how actions unfold over time and how earlier frames relate to later ones to accurately classify the action. As above, we use attentive readout, though over all video frames here. Kinetics-400 and 700 (K400, K700) [9, 10] are used for appearance-focused action recognition. For more motion-sensitive action recognition, Something-Something v2 (SSv2) [20] is used. We report top-1 classification accuracy.\nMonocular depth prediction. Monocular depth estimation, referred hereafter as Depth, is a 3D perception task that aims at predicting the distance of surface elements in the scene from the camera. Unlike traditional geometric correspondence and triangulation techniques, this requires only a single image. However, it can also be calculated from video to leverage temporal dependencies between frames. Monocular depth estimation is a fundamental problem in computer vision as it bridges the gap between 2D images and the 3D world. For the readout, we use the decoder of the"}, {"title": "4. Experiments", "content": "We begin our experiments in Sec. 4.1 with qualitative investigations into the features learned by I-WALT and V-WALT. In Sec. 4.2, we start our range of quantitative evaluations, comparing representations learned via image and video diffusion pre-training objectives with the help of tasks ranging from pure semantics to spatio-temporal visual understanding. Section 4.3 delves further into the design choices involved when extracting features from generative diffusion models. We conclude our experiments with an analysis on the effect of training budget on feature and generation quality in Sec. 4.4 and a comparison with common visual representation models in Sec. 4.5.\n4.1. Qualitative Observations\nVisualizing latent features in deep neural networks is challenging due to their high dimensionality [31]. To simplify this analysis, we extract the principal component with the largest eigenvalue from the features at late layers of the I-WALT and V-WALT models. This allows us to inspect the most salient features in the latent space.\nFigure 3 visualizes the key feature activations from the first frame of various videos in the DAVIS dataset [40],"}, {"title": "4.2. Video versus image diffusion", "content": "Our initial visual inspections of the learned model features pave the way for an objective assessment. We begin with a key comparison that lies at the heart of our quantitative evaluation. Figure 1 presents a comparison between the performance of I-WALT, the model trained for image generation, and the performance of the same model architecture trained for video generation (V-WALT) across a range of readout tasks described in Sec. 3.4. For a meaningful comparison across tasks, we show the relative performance change $(x_v - x_I) / x_I$, where $x_I$ and $x_v$ denote the absolute performance of the image and video models, respectively.\nV-WALT consistently outperforms I-WALT across all tasks, though the figure reveals a striking range in the extent of this superiority. Perhaps unsurprisingly, improvements are small on the purely semantic image classification tasks Places365 (+0.6%) and ImageNet (+1.8%), while we see a substantial increase in performance on Obj. Tracks (+23%), Cam. Pose (+60%) and PointTracks (+68%) \u2013 tasks that benefit greatly from a deeper understanding of space and motion.\nRemarkably, the video training objective improves performance on the image classification task Inat2018 (+11%). On the action classification tasks, it is interesting to see that the delta is considerable on SSv2 (+42%), but much more subtle on Kinetics (+8% on K400, +12% on K700). This follows the general consensus that the Kinetics tasks primarily measure appearance understanding, while SSv2 is significantly more sensitive to motion understanding [5, 46].\nThe substantial improvement on PointTracks (+68%) could be unexpected, given that image diffusion models have been shown to excel at point correspondence [24, 50]. We attribute this to the fact that point tracking is primarily sensitive to accurate localization of spatial points in the scene, while point correspondence primarily measures semantic understanding. The videos presented by Hedlin et al. [24] illustrate this challenge, demonstrating that the model struggles to distinguish between different instances of the same class (e.g., ears of the same person)."}, {"title": "4.3. Feature extraction from generative diffusion", "content": "Trained for denoising, generative diffusion models offer flexibility in feature extraction due to the lack of a dedicated \"feature extraction\" mode. A crucial consideration is determining both the appropriate time step and the best network block for feature extraction. To this end, we evaluate the features extracted using various noise levels and transformer blocks within the model across all tasks in Fig. 5.\nNoise level. As shown in Fig. 5 (left), introducing high levels of noise generally diminishes downstream task performance. However, a small amount of noise (200) leads to the best results for most tasks, with the exception of PointTracks (best at 100) and Obj. Tracks (best at 0). This aligns with the intuition that lower-level tasks like tracking are more susceptible to noise, while higher-level tasks benefit from subtle amounts of it. We find that the image model I-WALT behaves similarly (see the Appendix). Related works in the literature have investigated this question, though exclusively on image diffusion models, and the consensus is that small amounts of noise have been found helpful for downstream performance, e.g. classification [34, 59] or correspondence tasks [30, 50].\nModel block. Next, we investigate where in the model to extract features from. The WALT model consists of a tokenizer followed by $L = 24$ transformer blocks, see Sec. 3.2. As seen in Fig. 5 (right), we find that the best representations are located at a depth of approximately 2/3 within the model for all tasks. A notable deviation from this is the PointTracks task, which performs slightly better at an earlier block. This finding suggests an implicit separation of the model into encoder and decoder components, leading to the best representations being learned at their intersection."}, {"title": "4.4. Pre-training budget and generation quality", "content": "Prior work has investigated the relationship between reconstruction and downstream performance [3] and found that the most informative features are learned towards the end of the training schedule. To explore this relationship for diffusion models, we train a V-WALT model and capture checkpoints at regular intervals throughout the training process. Each checkpoint is then evaluated on two fronts: the effectiveness of its learned features on the downstream tasks, and training progress, as measured by the training loss and Fr\u00e9chet Video Distance (FVD) [52].\nFigure 6 illustrates the per-task performance of models trained using checkpoints from various stages of the pre-training process. Interestingly, even early checkpoints (representing 20% of total pre-training progress) demonstrate a relative performance exceeding 90% on several tasks. As expected, performance on recognition tasks exhibits a consistent upward trend with continued training. Conversely, tracking and depth estimation tasks appear to achieve optimal performance at earlier stages. Notably, camera pose estimation performance shows a decline after the 26% training mark, suggesting potential overfitting or a shift in learned representations that negatively impacts this specific task."}, {"title": "4.5. Comparisons with visual representation models", "content": "To conclude our experiments, we investigate the scaling behavior of the V-WALT model, and compare its performance at sizes 284 M, as in all other experiments, and 1.9 B, with standard visual representation learning models in the same frozen readout setting. We choose representative models from different self-supervised methods: contrastive DINOv2 [8], image-text alignment SigLIP [13], pixel reconstruction MAE[23], and feature reconstruction model JEPA [1]. We also include their video extensions VideoMAE [51] and V-JEPA [5] to further explore the differences between models trained on images vs. videos.\nIn Fig. 7, we use the performance of I-WALT as baseline (100%) and plot the relative performance of other models. Scaling V-WALT to 1.9 B significantly improves the performance on most tasks, except on PointTracks where the smaller model does marginally better. The most obvious boosts are on image and video classification, and classification tasks with a large number of classes (K700 and Inat2018) tend to benefit more from the increased model size compared to those with fewer classes (K400 and ImageNet). While V-WALT is competitive with the other video models on depth and motion understanding, it is dominated by SigLIP and DinoV2 on the more semantic tasks, revealing a core weakness of generative diffusion models in our readout setting."}, {"title": "5. Discussion", "content": "In this work, we systematically compared the same model architecture trained for video versus image generation, analyzing the performance of their latent representations on various downstream tasks. Results show that video diffusion models consistently outperform their image counterparts, especially for tasks that require motion or spatial understanding. We further analyzed features extracted from different layers and with varying noise levels, as well as the effect of model size and training budget on representation and generation quality. This work marks the first direct comparison of video and image diffusion objectives for visual understanding, offering insights into the role of temporal information in representation learning.\nOur study has several avenues for future work. Firstly, we limited our study to a single model architecture (WALT [22]) for a clean comparison that would not be possible for other models that differ considerably between image and video architectures (e.g., SD and SVD), see Sec. 3.2. Potential extensions of this work could explore these comparisons across a wider range of unified model architectures as they become available, providing a more comprehensive understanding of the representational power of video and image diffusion models.\nSecondly, our investigation primarily focused on the performance of these models on visual understanding tasks. Future research could delve deeper into the intersection of generative capabilities and representation learning, potentially exploring how the quality of generated images and videos influences and is influenced by the learned representations. This could lead to new insights and techniques for improving both the generative and representational capabilities of these models.\nWe believe this study contributes to the ongoing exploration of video and image diffusion models, and we hope our findings inspire further research into their potential for visual understanding and beyond."}]}