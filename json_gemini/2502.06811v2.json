{"title": "Aligning Human and Machine Attention for Enhanced Supervised Learning", "authors": ["Avihay Chriqui", "Inbal Yahav", "Dov Te\u02bceni", "Ahmed Abbasi"], "abstract": "Attention, or prioritization of certain information items over others, is a critical element of any learning process, for both humans and machines. Given that humans continue to outperform machines in certain learning tasks, it seems plausible that machine performance could be enriched by aligning machine attention with human attention mechanisms-yet research on this topic is sparse and has achieved only limited success. This paper proposes a new approach to address this gap, called Human-Machine Attention Learning (HuMAL). This approach involves reliance on data annotated by humans to reflect their self-perceived attention during specific tasks. We evaluate several alternative strategies for integrating such human attention data into machine learning (ML) algorithms, using a sentiment analysis task (review data from Yelp) and a personality-type classification task (data from myPersonality). The best-performing HuMAL strategy significantly enhances the task performance of fine-tuned transformer models (BERT, as well as GPT-2 and XLNET), and the benefit is particularly pronounced under challenging conditions of imbalanced or sparse labeled data. This research contributes to deeper understanding of strategies for integrating human attention into ML models and highlights the potential of leveraging human cognition to augment ML in real-world applications.", "sections": [{"title": "I. INTRODUCTION", "content": "FOR many decision-making tasks, a collaboration between humans and machines, when realized effectively, is widely considered superior to either humans or machines working independently. Human-machine synergy is often achieved through various interactive paradigms, such as \"human-in-the-loop\" systems [Bhardwaj et al., 2023, Kim et al., 2023, Petricek et al., 2023] and reinforcement learning strategies [Te'eni et al., 2023]. This paper aims to further explore how human input can enrich machine learning processes, focusing on a crucial mechanism of learning: attention.\nHuman attention involves the cognitive ability to concentrate on specific aspects of information while disregarding others. This selective focus is crucial for humans to manage the vast array of stimuli encountered daily, enabling effective task performance in areas like reading comprehension, text classification, and sentiment analysis. Yet human attention is not without its flaws; it can be biased and inconsistent and is often influenced by factors such as fatigue or cognitive overload [Yoo et al., 2022].\nMachine attention, in turn, derives from computational models that dynamically prioritize the most informative parts of input data, such as words or images [Luong et al., 2015, Vania and Lopez, 2017]. This principle of prioritizing parts of the available information, inspired by human attention strategies, has revolutionized deep learning models, improving both their performance and interpretability across various tasks in Natural Language Processing (NLP) and computer vision [Devlin et al., 2018, He et al., 2019]. In particular, Vaswani et al. [2017] introduced the transformer model, which relies entirely on attention mechanisms for sequence processing without recurrent or convolutional layers. Transformers use self-attention layers that allow each position in the input sequence to attend to all positions in the sequence, including itself. Self-attention allows the model to capture dependencies regardless of their distance in the input sequence, making it highly parallelizable and efficient for processing long sequences. Machine attention is more consistent than human attention and less prone to bias, and it scales effectively across large datasets. However, it is also limited by its dependency on training data and specific model architectures, which may hinder its ability to address novel or complex scenarios that human cognition handles more flexibly [Bhardwaj et al., 2023].\nGiven the suboptimal nature of current machine attention management, our objective is to enhance it by incorporating insights from human attention management. We anticipate that aligning machine attention with human attention will yield significant benefits, particularly in scenarios where the number of available training samples is limited across all or some of the classes, or in the absence of relevant context [Collins et al., 2018, Zagalsky et al., 2021]. To achieve this, we introduce a novel technique called Human-Machine Attention Learning (HuMAL), which leverages the attention patterns exhibited by humans, when performing specific tasks, to improve machine classification performance.\nMore specifically, the main research gaps we attempt to address are: (1) offering a deeper exposition of strategies for incorporating human attention into a transformer-based model; (2) proposing a method for leveraging human attention in machine learning that offers more effective performance; (3) providing an empirical analysis of the differential benefit provided by such a strategy as a function of class imbalance and labeled data availability.\nIn what follows, we provide a brief overview of the distinctions between human and machine attention mechanisms and discuss prior efforts to integrate the two to improve machine learning performance. We then introduce our HuMAL approach, identifying different strategies to incorporate human"}, {"title": "II. MECHANISMS OF HUMAN VS. MACHINE ATTENTION IN LEARNING PROCESSES", "content": "The concept of \"human attention\", as used herein, refers to how humans naturally focus on and give importance to different words in a text when reading or comprehending it. Human attention is based on cognitive processes and on the human brain's ability to process and interpret information, and it is influenced by past experiences and emotions. In the context of this paper, human attention is considered to be binary (in the sense that a human either attends to a particular aspect of the input or ignores it), with varying degrees of attention assigned to each word based on semantic and contextual cues.\nIn contrast, attention in LLMs is continuous and probabilistic, computed based on learned representations and positional embeddings [Brauwers and Frasincar, 2023]. Machine attention mechanisms, like those in BERT and GPT-3, focus on specific data parts by dynamically assigning weights to tokens. These mechanisms are effective for identifying patterns but are limited by training data and algorithms, resulting in a concrete and deterministic approach-unlike human attention [Brauwers and Frasincar, 2023], which can interpret abstract nuances and subtleties beyond explicit information, such as sarcasm or metaphors. Indeed, studies highlight that machine attention tends to be less flexible and more prone to focusing on spurious features, whereas human perception integrates abstract reasoning and context effectively, as seen in visual tasks and medical diagnoses [Lindsay, 2020, Guo et al., 2021, Makino et al., 2022].\nHow similar or different are human attention and machine attention when reading texts? Previous research has examined this question from different perspectives and using different methods. For example, Zou et al. [2023] investigated whether humans and Deep Neural Networks (DNNs) allocate attention in comparable ways when reading a text passage to answer a specific question. They found that the DNN attention distribution quantitatively resembles human attention distribution measured by eye-tracking, and both are modulated by top-down reading goals and lower-level text features.\nThe concept of attention correctness entails using human attention to evaluate machine attention, and has been applied in the context of image caption text [Liu et al., 2017, Brauwers and Frasincar, 2023]. Sen et al. [2020] conducted a large-scale crowdsourcing study on a Yelp review dataset to collect human attention maps that encode the parts of a text that humans focus on when classifying texts. They compared human attention maps with machine attention maps created by DNNs and found that they had some overlap in word selections, but also some differences in distribution over lexical categories and context-dependency of sentiment polarity.\nCui et al. [2021] analyzed how the performance of deep learning models is impacted by multi-head self-attention: a core component of transformer models' attention mechanism, in which the importance of each token is independently calculated by multiple submodules (\u201cattention heads\"), whose scores are subsequently combined [Vaswani et al., 2017, Radford et al., 2019, Touvron et al., 2023]. Using eye-tracking data, the authors compared human and machine reading, highlighting the significant role of passage-to-question and passage-understanding attention. Their results also underscore the importance of certain word types and the influence of fine-tuning on attention distribution.\nZhao et al. [2023] investigated if deep learning models can mimic human reading behavior, mainly focusing on significant words. The study discovers that while these models can indeed concentrate on significant words, they can also incorrectly focus on irrelevant ones due to inadequate learning. Thus, despite embodying some human reading traits, machine attention mechanisms exhibit certain limitations that necessitate more research. Finally, Lei et al. [2022] explored the relationship between human and machine attention mechanisms in different neural network designs for computer vision tasks. The main finding of the paper was that the more closely artificial attention aligns with human attention, the better the model performance, suggesting that improving alignment may boost the performance of neural networks.\nGiven the research on the relative strengths of human attention compared with machine attention, we find little research on aligning machine mechanisms of attention with human"}, {"title": "III. HUMAL STRATEGIES", "content": "The HuMAL architecture is derived from a transformer model (e.g. encoder-decoder) for text classification, with modifications made to the attention layer that entail aligning the machine's attention with human attention.\nThe presentation of human attention data will follow this format: each token i (i = {0,1,..., N}) in a sentence j, represented as TOKj,i, will be associated with a human attention value ah. This value is either binary, in the case of a single attention decision, or continuous in the case of multiple human attention decisions.\nFor example, let us consider the last sentence in Figure 1: \"Will definitely be going back to this place!\". If we tokenize this sentence on a word level, we obtain the following tokens: [\"will\", \"definitely\u201d, \u201cbe\u201d, \u201cgoing\u201d, \u201cback\u201d, \u201cto\u201d, \u201cthis\u201d, \u201cplace\u201d, \u201c!\u201d]. The corresponding attention vector Ah from the first (human) annotator would be [0, 1, 1, 1, 1, 0, 0, 0, 0]. Alternatively, if we tokenize the sentence into sub-words, it might look like this: [\u201cwill\u201d, \u201cdef#\u201d, \u201c#initely\u201d, \u201cbe\u201d, \u201cgo#\u201d, \"#ing\", \"back\u201d, \u201cto\u201d, \u201cthis\u201d, \u201cplace\u201d, \u201c!\u201d]. In this case, when a human assigns attention to a word, we apply the attention decision to all subwords within that word. Thus, for the given example, the corresponding attention vector from the first annotator would be [0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], indicating that attention is assigned to the subwords within the tokenized sentence accordingly.\nIn turn, the machine (e.g., BERT) attention value corresponding to a token TOKj,i is denoted amil, where l represents the transformer layer in the BERT architecture (l = {0,1, ..., 11}). This attention value captures the focus that the model places on TOK\u2081 when processing the special token CLS; that represents the meaning of the entire sentence [Vaswani et al., 2017, Devlin et al., 2018, Clark et al., 2019]. The value amit is computed via the average attention scores across all attention heads h = {0, 1, ..., 11} within the same layer (ami;1,h), given that heads within a layer have similar attention distributions [Clark et al., 2019] (see Equation 1). The machine attention vector for a given sentence jis represented as the vector Am.\nWe explore three different strategies for aligning the machine's attention mechanism to human attention. All three strategies of HuMAL align machine attention to enhance classification performance, but they differ in the process of alignment. These strategies, illustrated in Figure 2 for a BERT model, are:\n\u2022 Attention as loss (HuMAL-AL): The model is trained to align its attention with human attention according to the loss function. This method involves penalizing the model for deviating from human attention, in addition to the loss function with respect to the binary dependent value."}, {"title": "A. Attention as Loss (HuMAL-AL)", "content": "HuMAL-AL incorporates attention losses as a regularization technique for the LLM classification loss. Here, the classification loss, computed as the cross entropy between the sentence label yj and its predicted probability pj, is regularized by the attention loss, which is calculated based on the cosine similarity between the human attention Ah and the machine attention at the last layer A 11. The HuMAL-AL loss function is given in Equation 2."}, {"title": "B. Attention as Normalizer (HuMAL-AN)", "content": "HuMAL-AN applies normalization to the last layer of the model's attention, A11, by using the human attention A. This normalization process influences the values of the last embedding vector of the CLS; token, represented as Ej,11. While this technique will not affect the training of the pre-trained model, adjusting the attention weights will immediately affect the level of importance assigned to the interaction between the tokens and the CLS tags. This, in turn, has implications for the classification task, which heavily relies on the final embedding representation. The HuMAL-AN modified embedding for sentence j is given in Equation 3."}, {"title": "C. Attention as Prior (HuMAL-AP)", "content": "Similarly to HuMAL-AL, HuMAL-AP incorporates attention losses as a regularization technique for the LLM classification loss. However, there are key differences in how the losses are applied. Here, while cross entropy is employed in the last LLM layer, the attention loss is integrated into the first layer. The primary concept behind this alternative approach is to introduce human attention as a prior to the initial learned attention layer in LLM.\nIn the lower layers of LLM, especially BERT, attention heads tend to exhibit broader focus, allocating less than 10% of their attention to individual words, resulting in a sentence representation that resembles a bag of vectors [Clark et al., 2019]. By approximating the first attention layer with human attention, our aim is to enhance the model's likelihood of converging to a global optimum perceived by a human expert. This concept bears similarities to the utilization of manually defined centroids in unsupervised learning approaches.\nThe HuMAL-AP loss function is given in Equation 4."}, {"title": "IV. EXPERIMENT", "content": "To evaluate our model, we conducted experiments using two tasks: sentiment classification and personality type classification. The datasets utilized for these tasks are the Yelp dataset provided by Sen et al. [2020] and the myPersonality dataset from Stillwell and Kosinski [2015], respectively."}, {"title": "A. Sentiment Classification Task", "content": "For the sentiment classification, we used the dataset provided by Sen et al. [2020]. This dataset comprises Yelp restaurant reviews annotated for sentiment analysis, specifically identifying whether the sentiment expressed is positive or negative.\nTo capture human attention, for each review in the dataset, Sen et al. [2020] asked crowd workers (three workers per review, on average) to highlight words they perceived as reflecting the review's sentiment. These highlighted words serve as indicators of human attention directed towards the most significant words related to the overall task. The dataset is balanced and consists of approximately 4,500 instances. All sentiment labels were agreed upon by the annotators.\nIt is worth noting that binary document-level sentiment polarity classification (as in the case of labeling positive versus negative reviews) is generally considered an \u201ceasy\" machine learning task. A BERT-based classification model, trained on a balanced subset of 2,000 instances from the dataset of Sen et al. [2020], achieves a high AUC score of 0.98 (see more details in the Results section)."}, {"title": "B. Personality Type Classification Task", "content": "For the personality type classification task, we used the myPersonality dataset by Stillwell and Kosinski [2015]. MyPersonality is a large-scale research initiative to gather psychological data from Facebook users who consented to share their information. Participants completed personality surveys and provided access to their digital footprints (posts, likes, comments, etc.). The public dataset includes 250 users' posts, in addition to self-reported personality type these users (i.e., introvert vs. extrovert); the full dataset became inaccessible in April 2018 due to the challenges associated with data management and ethical considerations.\nWe sought to annotate and label users' posts according to their personality types, namely, introvert vs. extrovert. To this end, we recruited workers from Prolific with an academic qualification in psychology and English as their native language. Our annotation procedure was similar to that employed by Sen et al. [2020]. Specifically, each instance underwent annotation by at least three annotators, with a preference for the first three annotators when the total number exceeded three. In each post, annotators indicated words they perceived as reflecting the writer's personality type, and then labeled the personality type as either introvert or extrovert. Instances with less than 2% of words annotated were excluded from the analysis.\nNotably, we observed significant discrepancies between labels assigned by Prolific workers and those self-reported by users. In particular, users' self-reported personality types aligned with Prolific workers' assessments (majority vote) in only 124 out of 250 cases, as illustrated in Table I.\nAccordingly, for instances in which worker-assigned labels did not align with the self-reported label, we used two alternative approaches for including the instances' corresponding annotations in our data: (i) including annotations concurring with the self-reported personality type; and (ii) including annotations agreeing with the majority vote. In each case, if there were more than three annotations that met the criteria, we used the first three. Table II shows the number of instances used for the training set in different experiment subsets."}, {"title": "C. Experiment Mechanism", "content": "For each task and its corresponding dataset, we incorporated the attention data of all annotators to construct a continuous human attention vector. For instance, a token, a highlighted by three human annotators, t \u2208 [0,3), receives three times more attention than a word highlighted by only one annotator.\nWe normalized the sum of humans' attention vectors ah to"}, {"title": "V. RESULTS", "content": "A. Main Results: Comparing the Performance of the Three HuMAL Strategies\nTable III summarizes the performance of the HuMAL alternatives, compared to a general-purpose BERT model, fined-tuned to the specific task. Figure 3 further illustrates the difference in performance between a BERT model and our first HuMAL strategy (HuMAL-AL) on the sentiment classification task.\nOverall, HuMAL-AL demonstrated the best performance compared to both the baseline and the other HuMAL approaches. The performance benefit is particularly pronounced in challenging scenarios that contain a high class-imbalance ratio and a small number of training instances. The results were consistent across tasks (sentiment or personality type classification), regardless of the labeling of the minority class"}, {"title": "B. Data Requirements of HuMAL-AL vs. BERT", "content": "We conducted a comprehensive analysis to compare the amount of labeled data required by a BERT model versus HuMAL-AL while controlling for performance (AUC); this analysis provides an assessment of the extent to which Hu-MAL enables users to save on labeling costs. We focused on the sentiment labeling task only, as the personality type dataset did not contain enough instances for the analysis. Specifically, we varied the size of the training set from 50 to 2000 observations, maintaining a constant imbalance ratio. The two extreme imbalance ratios considered were 1% and 5%. Figure 4 presents a visualization of the results of this analysis, where the AUC curves are displayed using a lowess function with frac=0.2 applied to the AUC values."}, {"title": "C. Impact on the Classification Loss", "content": "To understand how incorporating human attention affects the model's training process, we analyzed (for both tasks) the convergence of the HuMAL-AL loss function. We observed that HuMAL required more epochs to converge compared to BERT. However, it eventually reached the same overall loss as BERT without compromising attention or classification accuracy. By decomposing the HuMAL loss function, as shown in Figure 5, we discovered that the classification loss (cross-entropy) of HuMAL behaves similarly to BERT's throughout all the epochs. In parallel, the attention loss of HuMAL converges monotonously without any tradeoff. This observation highlights the effectiveness of HuMAL in enhancing attention modeling while preserving classification performance."}, {"title": "D. Impact of Text Length on HuMAL Performance", "content": "We investigated the performance of the HuMAL across texts of different lengths, taking into account the insights provided by Sen et al. [2020] regarding the decrease in agreement among human annotators as text length increases, while classification decisions (i.e., negative/positive) remained unchanged. This result suggests that human attention data become less informative as the text length increases. We thus anticipated that, for texts of greater length, HuMAL-AL would provide less pronounced improvement compared to BERT.\nFor the sentiment classification task, we divided the texts in the Yelp dataset into three categories based on length, in accordance with Sen et al. [2020]: (1) up to 50 words (denoted Yelp-50), (2) 51-100 words (Yelp-100), and (3) 101-200 words (Yelp-200). As expected, HuMAL's improvement over BERT decreased with text length (see Table IV), reflecting the decreased in agreement among annotators. Nevertheless, HuMAL consistently demonstrated robust performance across all text lengths, outperforming BERT."}, {"title": "E. Impact on BERT's Attention", "content": "We conducted a close analysis of attention allocation by BERT, HuMAL-AL, and humans, aiming to address two pivotal questions: First, how do humans and BERT differ in their attention placement, particularly regarding the beginning versus the end of each sentence? Second, how does HuMAL change BERT's attention allocation, and how does this change relate to human attention allocation?"}, {"title": "F. Alternative Base Model", "content": "We implemented HuMAL-AL with two alternative LLMs-GPT-2 and XLNet-comparing their performance and the effectiveness of the HuMAL approach.\nGPT-2, or Generative Pre-trained Transformer 2, is a prominent language model developed by OpenAI that operates on the transformer architecture. It is a generative model designed to predict the next word or sequence of words in a given context,In comparison to its successor, GPT-4, GPT-2 represents an earlier version with fewer parameters, indicating a subsequent evolution in model size and complexity. Furthermore, GPT-2 (unlike GPT-4) is open-source, allowing researchers and developers to access and build upon the model, fostering collaborative advancements in natural language processing [Radford et al., 2018].\nGPT-2, as its name suggests, is a generative text model. We added a classification head on top (linear layer) to turn it into a classification model. Based on the default provided by the Transformers library [Wolf et al., 2019], it uses the last token to do the classification, as other causal models do. By design, the first token consistently receives the highest attention weight during model inference, yet paradoxically, this attention is found to be less informative [Vig and Belinkov, 2019]. Therefore we artificially add < |BeginningOf Sentence| > tag, as advised in [Radford et al., 2018].\nXLNet integrates features from both encoder and decoder architectures. It adopts a permutation-based training objective, enabling it to capture a bidirectional context akin to BERT. Notably, during autoregressive text generation, XLNet employs a causal (left-to-right) self-attention mechanism, reminiscent of GPT models. This dual functionality positions XLNet as a versatile hybrid, serving as an encoder for pretraining purposes while functioning as a decoder for tasks involving text generation. The model's unique combination of bidirectional context capture and autoregressive capabilities contributes to its efficacy in a wide range of natural language processing applications.\nTable VI shows that GPT-2 and XLNET require fewer instances to train than BERT for the sentiment classification task. Yet, HuMAL significantly and economically improves performance with small datasets."}, {"title": "VI. DISCUSSION AND CONCLUSIONS", "content": "This paper introduced and evaluated HuMAL, a novel approach for enhancing the performance of language models by aligning machine attention mechanisms with human attention mechanisms. Broadly, this approach incorporates human attention when training the machine by considering specific words or phrases that humans have annotated as relevant for their performance of a particular task. We show how this approach is implemented and when it is effective in boosting machine performance..\nWe explored three alternative strategies to incorporate human attention into an LLM's attention mechanism: Attention as Loss (HuMAL-AL), Attention as Normalizer (HuMAL-AN), and Attention as Prior (HuMAL-AP). We evaluated these strategies on two types of tasks: sentiment analysis, using a well-balanced annotated dataset of Yelp restaurant reviews [Sen et al., 2020]; and personality-type classification (introvert vs. extrovert) based on social media posts (the myPersonality dataset [Stillwell and Kosinski, 2015]), which we annotated for human attention. We compared our proposed HuMAL strategies to a baseline BERT model. Through experimentation, we found that regularization on the loss of the last attention layer (HuMAL-AL strategy) yielded the best results, effectively aligning this layer to the human attention without increasing the classification loss. Moreover, HuMAL-AL consistently outperformed BERT, as well as alternative LLMs (GPT-2 and XLNET), particularly in tasks with sparse and/or imbalanced training data.\nNotably, our results seem to diverge from those of previous studies in which incorporating human attention into machine learning\u2014through the use of sentiment lexicons [Zou et al., 2018] or cognitive data, such as eye-tracking and EEG [McGuire and Tomuro, 2021]\u2014did not lead to significant improvements in classification performance. One possible explanation for this disparity lies in the distinction between objective-aware and objective-unaware attention [McCormick, 1997]. Human attention can be characterized as either aware or unaware, referring to the extent to which individuals consciously focus their attention on specific aspects of the text. In normal, unaware reading, readers tend to allocate attention to terms that are more complex, longer, or ambiguous, reflecting their cognitive processing of the text. Conversely, in task-specific attention (e.g., sentiment classification), readers are more likely to search for cues or information directly related to the objective or task at hand. Our work concentrated on task-specific attention. It may be fruitful to investigate aligning machine attention to human unaware attention too.\nWe emphasize that the benefit of HuMAL over a base model is dependent on the task at hand and the availability"}]}