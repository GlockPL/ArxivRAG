{"title": "Probing for Consciousness in Machines", "authors": ["Mathis Immertreu", "Achim Schilling", "Andreas Maier", "Patrick Krauss"], "abstract": "This study explores the potential for artificial agents\nto develop core consciousness, as proposed by Antonio Damasio's\ntheory of consciousness. According to Damasio, the emergence\nof core consciousness relies on the integration of a self model,\ninformed by representations of emotions and feelings, and a world\nmodel. We hypothesize that an artificial agent, trained via rein-\nforcement learning (RL) in a virtual environment, can develop\npreliminary forms of these models as a byproduct of its primary\ntask. The agent's main objective is to learn to play a video\ngame and explore the environment. To evaluate the emergence of\nworld and self models, we employ probes-feedforward classifiers\nthat use the activations of the trained agent's neural networks\nto predict the spatial positions of the agent itself. Our results\ndemonstrate that the agent can form rudimentary world and\nself models, suggesting a pathway toward developing machine\nconsciousness. This research provides foundational insights into\nthe capabilities of artificial agents in mirroring aspects of human\nconsciousness, with implications for future advancements in\nartificial\nintelligence.", "sections": [{"title": "INTRODUCTION", "content": "As modern computers emerged and their capabilities grew,\nthe idea of machines becoming conscious was already being\ncontemplated, along with the need for methods to investigate\nthis possibility. One of the earliest approaches was the Turing\ntest [1]. The basic idea was to let people chat with an unknown\npartner, which could either be another human or a computer.\nIf humans could no longer distinguish between the two, the\nmachine would be considered conscious. Today, some large\nlanguage models (LLMs) starting from GPT-4 have arguably\npassed the Turing test [2].\nHowever, does this mean that some LLMs have become\nconscious? John Searle's thought experiment, the Chinese\nRoom [3], addresses this question. In the experiment, a person\npasses a question written in Chinese into a closed room.\nInside, an English-speaking scientist follows a rule book to\nprocess Chinese sentences and produce an answer, which is\nthen passed back. Although it appears that the room or the\nscientist understands Chinese, the scientist is merely following\nsyntactic rules without any comprehension of the meaning.\nThis illustrates that behavior alone is insufficient to conclude\nthat a machine with sophisticated behavior has any understand-\ning or consciousness.\nIn light of the increasing capabilities and presence of\nintelligent systems in our daily lives, determining whether\nmachines can become conscious is an increasingly urgent\nquestion. Moreover, the exact nature of consciousness remains\nan unsolved problem, with various contradictory theories about\nits origins and mechanisms. Notable theories include the con-\ntroversial integrated information theory [4], which associates\nconsciousness with a maximum of integrated information or\ncause-effect power, and the global workspace theory [5], where\nparallel information flows compete for access to a global\nworkspace for processing and distribution.\nAn outlier in these theories is Antonio Damasio's theory\nof consciousness [6], which provides a detailed mechanism\nof how consciousness arises, its attributes, and its possible\nembodiment in the human brain. In [7] it is argued that this\ntheory is uniquely well-suited for application to artificial in-\ntelligence (AI) and machine learning (ML) systems. Damasio\nstructures consciousness into three hierarchical levels: Firstly,\nthe protoself, the neural representation of the body state.\nSecondly, the core consciousness, a higher-level representation\nof the self, the world, and their mutual relations, leading to\na transient core self, and thirdly, the extended consciousness,\nwhich includes memory, language, planning, and other high-\nlevel mental activities enabling the continuous autobiographic\nself. In this theory, emotions and feelings play a crucial role.\nEmotions are unconscious reactions to stimuli, and feelings are\nneural representations of these emotions. Objects can induce\nemotions, leading to changed feelings and a changed protoself.\nThe combined neural representation of the perceived object\nand the changes in the protoself due to it forms the core\nconsciousness and creates a core self, a sense of perception\nbelonging to oneself. The autobiographic self, built on top\nof the core self, includes memories of one's past, consis-\ntent characteristics, and future plans. This requires extended\nconsciousness and its functions like memory and planning.\nFrom simple reactive emotions to complex plans enabled\nby extended consciousness, these systems aim to regulate\nhomeostasis, keeping the internal state in a safe range to ensure\ncontinued existence and increase the chance of survival. A"}, {"title": "Reinforcement Learning", "content": "Reinforcement Learning (RL) tries to solve the problem\nof (optimal) sequential decision making. The basic frame-\nwork assumes that at every time step $t \\in \\mathbb{N}_0$ the agent\nacts on the environment with an action $a_t \\in A$ and the\nenvironment returns a state/observation $s_t/o_t \\in S/O$ and a\nreward $r_t \\in \\mathbb{R}$ to the agent based on transition probabilities\n$P(S_{t+1} = s' | S_t = s, A_t = a)$ and a reward function\n$R:S\\times A\\rightarrow \\mathbb{R}$. $A$ denotes the action space, $S$ the state\nspace and $O$ the observation space, which depend on the\nchosen environment and agent. They can be either continuous\nor discrete, but for simplicity we focus on the discrete case and\nassume an episodic setting. The agent learns via this feedback\nloop to improve its behaviour. The basic RL cycle is illustrated\nin figure 3.\nThis notion can be further formalized as a (partially ob-\nservable) Markov decision process. For this we refer to the\nliterature on RL e.g. [12].\nThe actions of an agent are characterized by a policy which\ncan be either deterministic\n$\\pi : S \\rightarrow A \\text{ via } \\pi(s) = a$\nor probabilistic\n$\\pi : S \\rightarrow [0, 1]^{|A|} \\text{ via } \\pi(a|s) = P(A = a | S = s)$.\nWe denote a policy realized as neural network with weights $\\theta$\nas $\\pi_\\theta$.\nIn this setting the objective is to maximize the expected re-\nturn, i.e. the expected discounted cumulative reward achieved\nin an episode,\n$J(\\pi_\\theta) := \\mathbb{E}_{\\pi_\\theta} (R(\\tau)) = \\sum_{\\tau \\in T} P(\\tau | \\theta) R(\\tau)  \\text{ with } \\ R(\\tau) := \\sum_{i=0}^{T} \\gamma^i R(s_t, a_t) $"}, {"title": "Probes", "content": "Probes, a technique from the mechanistic explainability area\nof AI, are utilized to analyze deep neural networks [13].\nThey are commonly applied in the field of natural language\nprocessing [14]. Probes are typically small, neural network-\nbased classifiers, usually implemented as shallow fully con-\nnected networks. They are trained on the activations of specific\nneurons or layers of a larger neural network to predict certain\nfeatures, which are generally believed to be necessary or\nbeneficial for the network's task. If probes achieve accuracy\nhigher than chance, it suggests that the information about\nthe feature, or something correlated to it, is present in the\nactivations."}, {"title": "Implementation", "content": "We trained our agents in the NetHack environment using\nthe NetHack Learning Environment (NLE) and MiniHack, a\nsandbox editor for custom scenarios in NetHack [15], [16].\nNetHack provides a complex, discrete environment with low\ncomputational cost. It was first used as a benchmark at the\nNeurIPS 2021 NetHack challenge, where symbolic methods\nled by a wide margin [17]. Subsequently, NLE and MiniHack\nhave been used for benchmarking reward modeling with large\nlanguage model feedback [18], automatic curriculum design\n[19], internet query usage [20], skill transfer [21], and planning"}, {"title": "Architecture of the agent", "content": "The basic agent architecture is a simplified version of their\nbaseline model without LSTM cell and the parts to process the\nmessage and bottom line status as input e.g. the whole map\nand a centered crop of the map is given as input, the processing\nis done by an embedding layer, 5 Conv2D layers and 2 Linear\nlayers followed by two parallel Linear layers with an action\n(distribution) and the estimated value function of the current\nstate as final output, respectively. The embedding dimension\nwas chosen as 64, each convolution layer contains 16 filters\nof size 3 besides the last one having only 8 filters. The\nhidden dimension of the linear layers is 256. From the second\nexperiment onwards the LSTM cell was added back between\nthe 2 Linear layers and the action and value heads. The cell\nand hidden state size was chosen as 512. The architectures are\nillustrated in figure 5."}, {"title": "Training", "content": "RLlib [24] was used as the RL training framework and\nthe agents were trained with their PPO implementation until\nconvergence. The hyperparameters have been chosen similar\nto those recommended in the Minihack environment. An agent\nwas trained only on a single map without any restriction of\nthe random seeds."}, {"title": "RESULTS", "content": "Subsequent investigations employed probes to infer the\nspatial coordinates of the agent within the environment, predi-\ncated on the neural activations from a single network layer.\nFor this purpose, a comprehensive dataset was compiled,\nencompassing 230,000 instances per trained agent. These\ninstances were derived by operationalizing the agent within its\nrespective environment, during which both neural activations\nand corresponding spatial coordinates were documented. The\ndataset was apportioned into 200,000 samples for training and\n30,000 for testing purposes. Training of the probes was ori-\nented towards generating a predictive score for each possible\ncoordinate across a 15 \u00d7 15 grid (both x and y axes). During\nthe evaluation phase, the predicted coordinates with the highest\nscores were juxtaposed against the actual positions to assess\nthe predictive accuracy of the probes.\nIn the initial experiment, a basic agent model without an\nLSTM cell was used to process inputs from the entire map and\na localized 9 \u00d7 9 crop. The tests were conducted on two maps:\nthe ultimate map and the trap map, using probes with a single\nlinear layer. The results in Table I show accuracies exceeding\nrandom chance, especially on the ultimate map, suggesting that\nthe agent's neural activations encode positional information.\nHowever, the simplicity of the environment raises questions\nabout whether this is due to direct observations or an internal\nmodel. This indicates the need for more complex architectures\nlike RNNs or Transformers. Future experiments should train\nagents with an LSTM cell using a minimal central crop or\nuse more intricate environments to prevent straightforward\nobservation-based predictions.\nIn the subsequent experiment, the agent architecture in-\ncluded a 5 x 5 crop as the sole input and an LSTM cell,\ntested across various maps. The agent's actions were limited\nto cardinal directions, and map edge positions were excluded\nto prevent deducing the agent's location from visual input.\nThis setup emphasized the agent's ability to use historical\ndata. Probes, both linear (single layer) and non-linear (three\nlayers with ReLU activation), analyzed the memory aspect by\ntraining on the LSTM's hidden and cell states.\nThe results in Table II show accuracies exceeding chance,\nmore so than in the first experiment. Given the limited 5\u00d75\ncrop input and exclusion of wall observations, positional\ninformation must come from the agent's memory and inter-\nnal representations. Traps and monsters improved accuracies\nindividually but not on the ultimate map, suggesting the need\nfor more quantitative analysis to understand these effects.\nThe third experiment mirrored the second, except the crop\nsize was reduced to 3 \u00d7 3. Probes were trained for 50 epochs\nusing the Adam optimization algorithm, with learning rates\nset to 0.00005 for linear probes in the first experiment, and\n0.001 for linear and 0.0001 for non-linear probes in the second\nand third experiments. Increased training epochs and advanced\nlearning strategies could potentially enhance performance.\nThe findings from the third experiment, shown in Table III\ncorroborate prior results, with accuracies significantly above"}, {"title": "DISCUSSION", "content": "Our initial findings suggest that the hidden layer activations\nencapsulate information regarding the agent's position. Nev-\nertheless, given the simplistic nature of the environment, it\nremains ambiguous whether this information is directly ex-\ntracted from observations or assimilated by the agent through\na world model. To enhance the efficacy of the method,\nimplementing a more expressive agent architecture, such as\nRNN or Transformer, coupled with observations that provide\nless direct information (e.g., a centered crop), is essential.\nConsequently, in our subsequent experiments, we trained\nagents equipped with an LSTM cell, utilizing a narrowly\ncentered crop for observation. Alternatively, introducing a\nmore complex environment could be considered. We also\nconstrained the action space, anticipating that this limitation\nwould foster simpler and more precise latent representations.\nThe findings from our second and third experiments robustly\nconfirm that the agent's position is encoded within the net-\nwork's activations and indicate that the agent has developed a\nworld model. To rigorously evaluate the impacts and influences\nof various architectures, environmental settings, and training\nmethodologies, a more detailed and extensive quantitative\nstudy is required.\nReducing the crop size complicates the learning challenge\nbut highlights the importance of the agent's ability to infer\nits position for efficient map navigation. Teleportation traps\nfurther obscure the agent's positional accuracy, indicating\nthat lower accuracy might still reflect a more refined world\nmodel. A direct comparison of agents could be facilitated by\nevaluating all agents on a uniform random map, a logical\nprogression for future research. Analysis suggests that the cell\nstate may contain slightly more information than the hidden\nstate, with a non-linear representation enhancing accuracies\nwith non-linear probes. A more comprehensive investigation is\nneeded to thoroughly understand these impacts. However, this\npaper provides evidence supporting the existence of a world\nmodel, with detailed exploration reserved for future studies.\nAn agent's ability to discern its position may suggest basic\ncore consciousness, but this is not conclusive. Differentiating\nbetween a world model and a self-model is crucial. According\nto Damasio, a self-model is based on stable internal sensations,\nwhile a world model relies on variable external observations\n[6]. Future research should train agents with inputs related to\nphysiological states, like hitpoints or experience levels, using\nchanges in these inputs as rewards. For agents with an inter-\npretable world model, modifying specific neural activations,\nsuch as those representing the agent's position, can examine\ntheir influence on actions, aligning with methods in [11].\nThe distinction between self-models and world models\nhinges on stable internal sensations versus variable external\nobservations. However, citation [25] focuses on differentiating\nhomeostatic feelings and external inputs, which may not\ndirectly address internal versus external models.\nFuture research will have to distinguish between self-models and world models to advance machine consciousness\nunderstanding. In addition to an agent consistently appearing\nin the middle of a crop as a stable element while other\nfields are chaotic, this might involve incorporating internal\nstate variables into the agent's input and reward structure to\nobserve how these fluctuations affect behavior and decision-making. This will reveal if an agent can develop a true\nself-model, characterized by stable internal sensations, dis-tinct from variable external observations forming the world\nmodel. Analyzing the interplay between these representations\nis crucial for validating core consciousness per Damasio's\nframework, laying the groundwork for more sophisticated AI\nsystems.\nFuture research should use more complex environments\nto challenge agents' abilities to develop sophisticated self\nand world models. Current environments are too simplistic.\nIntroducing environments with intricate dynamics, diverse\nchallenges, and multifaceted objectives\u2014such as long-term\nplanning, problem-solving, social interactions, and adaptive\nlearning will rigorously test agents' potential for conscious-\nness. This will improve evaluation of their ability to form\ncomplex internal representations and understand the scalability\nand generalizability of the models, enhancing the reliability of\nthe findings.\nExploring advanced architectures like transformers and so-\nphisticated RNNs is promising for developing machine con-\nsciousness. Current architectures, while effective for initial\nexperiments, may not capture the complexities needed for\nhigher-order cognitive processes. Transformers handle long-\nrange dependencies and parallelize training, while advanced\nRNNs improve temporal dynamics. These architectures could\noffer deeper insights into self and world models, enhancing\nour understanding of how artificial agents develop and utilize\ncomplex cognitive functions.\nExtending model evaluations to various environments or\nreal-world scenarios is essential for assessing generalizabil-ity. While current virtual environments provide controlled\nsettings, real-world applications are more complex. Testing\nagents in settings like real-time robotics, social simulations,\nand dynamic ecological systems will offer a comprehensive\nunderstanding of their cognitive and adaptive capabilities. This\nbroader evaluation will determine if insights from virtual envi-ronments apply to practical scenarios, enhancing relevance and\nidentifying areas for improvement, guiding the development of\nmore robust Al systems.\nOur results show that constructing an internal model is\ncrucial for efficiently solving certain tasks. This suggests\nthat even model-free reinforcement learning (RL) approaches\nmight develop implicit internal models, making them not truly\nmodel-free. Additionally, some RL exploration strategies re-quire predictions about environmental dynamics, highlighting\nthe practicality of such models. Using this internal model for\nexploration could further integrate it into the agent's func-tionality. In our approach, we used a discount factor for RL\nto ensure mathematical convergence over an infinite horizon\nby reducing the impact of distant rewards and emphasizing\nimmediate rewards, effectively modeling future uncertainty.\nRemarkably, in the context of successor representations (SR)\n[26], [27], the discount factor plays a similar role by determin-ing how much future states influence the representation of the\ncurrent state. In particular, in SR the discount factor adjusts\nthe expected discounted future state occupancy, shaping the"}, {"title": "", "content": "cognitive map of the environment enabling agents (including\nhumans and animals) to plan and make decisions based on\ntheir expectations of future states [28]-[32]. This concept\nbridges reinforcement learning theories with cognitive science,\nproviding insights into how intelligent behavior emerges from\nthe interaction with the environment. In particular, SR can\nbe seen as a bridge between model-free and model-based\napproaches [33], [34].\nThis paper underscores the capabilities of AI methodologies,\nparticularly reinforcement learning (RL), in exploring theories\nof consciousness and advancing explainable AI. Considering\nAl consciousness is crucial for understanding AI's intentions\nand ensuring AI safety. While unconscious Als can impact\noutcomes, their lack of awareness means they can't be classi-\nfied as friendly or evil. Exploring AI consciousness is vital\nfor evaluating the risks and opportunities in AI. Through\nthis work, we aim to contribute to the discourse on whether\nmachines can achieve consciousness."}]}