{"title": "Towards a More Complete Theory of Function Preserving Transforms", "authors": ["Michael Painter"], "abstract": "In this paper, we develop novel techniques that can be used to alter the architecture of a neural network, while maintaining the function it represents. Such operations are known as function preserving transforms and have proven useful in transferring knowledge between networks to evaluate architectures quickly, thus having applications in efficient architectures searches. Our methods allow the integration of residual connections into function preserving transforms, so we call them R2R. We provide a derivation for R2R and show that it yields competitive performance with other function preserving transforms, thereby decreasing the restrictions on deep learning architectures that can be extended through function preserving transforms. We perform a comparative analysis with other function preserving transforms such as Net2Net and Network Morphisms, where we shed light on their differences and individual use cases. Finally, we show the effectiveness of R2R to train models quickly, as well as its ability to learn a more diverse set of filters on image classification tasks compared to Net2Net and Network Morphisms.", "sections": [{"title": "1. Introduction", "content": "Function preserving transforms (FPTs), also known as network morphisms, provide a method for transferring the performance of a teacher network, f, to a student network, g, with a different (typically larger) architecture. This is assured by computing the initial parameters \u03b8s for the student network from the parameters of the teacher \u03b8t, such that they are function preserving: \u2200x. f(x; \u03b8t) = g(x; \u03b8s).\nFPTs allow us to dynamically increase the capacity of a network, during training, without degrading performance. This enables us to leverage an already trained teacher network and perform a fast evaluation of new architectures, without incurring the overhead of training them from scratch. In particular, FPTs have applications in efficient architecture searches (Cai et al., 2018; Jin et al., 2018). Many architectures can be evaluated quickly by repeatedly applying FPTs, thereby amortizing the high cost of training from a random initialization over many architecture evaluations.\nExisting methods for performing FPTs include Net2Net (Chen et al., 2015) and Network Morphing (Wei et al., 2016). Net2Net provides techniques for widening hidden layers in the network and for introducing new hidden layers (deepening). Conversely, Network Morphing proposes alternative techniques for widening and deepening, as well as methods for kernel size morphing and sub-net morphing.\nPrevious FPTs do not handle residual connections (He et al., 2016), however, they are frequently used to train deep networks faster and more reliably, making them commonplace in the deep learning community. Motivated by this, we propose R2R, consisting of two new FPTs R2WiderR and R2DeeperR, which allow neural networks with residual connections to be morphed.\nWe believe that the two largest barriers preventing FPTs from being used beyond architecture searches are the complexity overhead of understanding and implementing the transform, alongside a lack of flexibility in the architectures that the FPTs can be applied to. We aim to address these issues by introducing simple to understand and implement FPTs and by facilitating an increase in the number of architectures that FPTs can be applied to.\nOur contributions are as follows: (1) proposing R2WiderR and R2DeeperR, novel FPTs that are compatible with residual connections; (2) performing a comparative analysis of R2R, Net2Net and Network Morphism; (3) introducing a novel evaluation to show that FPTs can be used to train networks to convergence with greater computational efficiency on the Cifar-10 (Krizhevsky & Hinton, 2009)."}, {"title": "2. Definitions and notation", "content": "Say tensor T has shape (\u03b1 \u00d7 \u03b2 \u00d7 \u03b3) to formally denote that T\u2208 R\u03b1\u00d7\u03b2\u00d7\u03b3. Also consider a convolutional kernel W and a volume x with shapes (Co \u00d7 Ci \u00d7 kh \u00d7 kw) and (Ci \u00d7 h \u00d7 w) respectively. Let Wij * xk be a 2D convolution defined in any standard way, which may include (integer) stride, padding and dilation for example. For notational convenience assume that the spatial dimensions are preserved (i.e. Wij * xk has shape (h \u00d7 w)). The arguments made in later sections are valid without the need for this assumption, albeit with a slightly more complex shape analysis.\nIn convolutional neural networks, a convolution is often considered to operate between 4D and 3D tensors, such that W * x has shape (Co \u00d7 h \u00d7 w). This complex convolution product can also be defined using matrix notation and the 2D convolution as follows:\n$\\begin{aligned}\nW * x\\stackrel{\\text { def }}{=}\\left[\\begin{array}{cccc}\nW_{11} & W_{12} & \\ldots & W_{1 C_{i}} \\\\\nW_{21} & W_{22} & \\ldots & W_{2 C_{i}} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nW_{C_{o} 1} & W_{C_{o} 2} & \\ldots & W_{C_{o} C_{i}}\\end{array}\\right] *\\left[\\begin{array}{c}\nX_{1} \\\\\nX_{2} \\\\\n\\vdots \\\\\nX_{C_{i}}\\end{array}\\right] \\\\\n=\\left[\\begin{array}{c}\n\\sum_{m=1}^{C_{i}} W_{1 m} * X_{m} \\\\\n\\sum_{m=1}^{C_{i}} W_{2 m} * X_{m} \\\\\n\\vdots \\\\\n\\sum_{m=1}^{C_{i}} W_{C_{o} m} * X_{m}\\end{array}\\right] .   \t\t(1)\n\\end{aligned}$\nWij and Xk are 2D matrices, with shapes (kh \u00d7 kw) and (h \u00d7"}, {"title": "3. Related work", "content": "We provide an overview of the only two other function preserving transform methods to date (to the best of our knowledge). Then, we consider a few existing applications of these function preserving transformations."}, {"title": "3.1. Net2Net", "content": "Net2Net introduces two function preserving transforms: Net2WiderNet for adding more hidden units into the network layers and Net2DeeperNet for expanding the network with additional hidden layers. To widen layer the ith dense layer in a network, hi = \u03c8(Wihi\u22121), from n to q hidden units using Net2WiderNet, the weight matrices W(i) and W(i+1) are replaced with Ui and Ui+1 where:\n$U^{(i)}=W^{(i)} \\left[\\frac{1}{g(j), k}\\right]_{j, k}$\n$U^{(i+1)}{h, j}=\\frac{W^{(i+1)}{h, g(j)}}{|\\{x \\mid g(x)=g(j)\\}\n(6)\nand where\n$g(j)=\\left\\{\\begin{array}{ll}j, & \\text { random sample in }\\{1,2, \\ldots n\\}, \\text { for } j \\leq n \\\\\nrandom sample in }\\{1,2, \\ldots n\\}, & \\text { for } n<j \\leq q .\\end{array}\\right.$\nNet2WiderNet duplicates columns from W(i) to widen a hidden layers and obtain U(i) and then adjusts for their replication in U(i+1) to be function preserving.\nNet2DeeperNet can be used to increase the number of hidden layers in a network by replacing a layer h(i) = \u03c8(Wihi\u22121) with two layers h(i) = \u03c8(V(i)(W(i)h(i\u22121))). The weight matrix for the newly added layer V(i) is initialized to the identity matrix. Net2DeeperNet only works with idempotent activation functions \u03c8, where \u03c8(\u0399\u03c8(v)) = \u03c8(v). While, ReLU activations satisfy this property, sigmoid and tanh do not."}, {"title": "3.2. Network morphism", "content": "Network Morphism (Wei et al., 2016) generalizes Net2Net, and considers a set of morphisms between neural networks that can be used to also increase the kernel size in convolutional layers and to introduce more Network In Network style subnetworks (Lin et al., 2013).\n(Wei et al., 2016) derive their morphisms in a principled way, by introducing the network morphism equation, which provides a sufficient condition for new parameters must satisfy in order for the morphism to be function preserving. To widen a network (ignoring non-linearities) their equations reduce down to solving the equation:\n$\\left[W_{i}^{\\prime}\\right] * W_{i-1}=\\left[W_{i}\\right] * U_{i-1} \\\\\n=W_{i} * W_{i-1}+U_{i} * U_{i-1},     \t\t(7)$\nfor new parameters Ui, Ui\u22121. They solve this by setting either Ui or Ui\u22121 to be zero, and we will refer to widening a network in this way as NetMorph. They also generalize the equations to include non-linearities.\n(Wei et al., 2016) also define kernel size morphing, which zero pads a convolutional kernel with zeros in the spatial"}, {"title": "3.3. Architecture search", "content": "Architecture search algorithms can be considered to simultaneously optimize over neural network architectures and their hyper-parameters. This usually involves training many models and comparing their performance, which can be very computationally expensive, requiring hundreds if not thousands of GPU days. To optimize over architectures a number of methods have been considered such as using evolutionary strategies or reinforcement learning (Real et al., 2018; Zoph & Le, 2016). Moreover, to lower the computational costs efficient architecture searches have been developed."}, {"title": "4. R2R", "content": "R2R consists of two FPTs: R2WiderRand R2DeeperR, which provide alternative methods for increasing the size of hidden layers and for adding hidden layers in a neural network."}, {"title": "4.1. R2WiderR", "content": "To increase the capacity of a network, some intermediate hidden volume xi can be widened by increasing its number of channels. We propose R2WiderR, a novel way of adding 2E channels by padding x with xL and xR, of shapes (E \u00d7 hi \u00d7 wi), thus obtaining a new volume zi. To assure that the value of xi+1 does not change, we initialize new parameters such that xL = xR, and that the contributions of xL and xR cancel each other out when computing xi+1. This idea is visualized in figure 1.\nLet U\u00b9 be an arbitrary tensor with shape (E \u00d7 Ci\u22121 \u00d7 khi\u22121 \u00d7 kwi\u22121), let ci be an arbitrary tensor with shape (E) and let Ui+1 be an arbitrary tensor, with shape (Ci+1 \u00d7 E \u00d7 khi \u00d7 kwi). Additionally, let \u03c1 include all the parameters in \u03c1i and any additional parameters required, such as batch norm's affine parameters, as discussed previously. We define the new kernels and biases as follows:\n$\\begin{aligned}\nW_{i}^{\\prime}=\\left[\\begin{array}{c}\nW_{i} \\\\\nU_{i}\\end{array}\\right],   \t\t(8)\\end{aligned}$\n$\\begin{aligned}\nW_{i+1}^{\\prime}=\\left[W_{i+1}  U_{i+1}  -U_{i+1}\\right] .  \t\t(9)\\end{aligned}$\nThe intermediate volume zi can now be computed:\n$x^{i}=\\left[\\begin{array}{c}x_{L} \\\\\nx_{R}\\end{array}\\right]=\\left[\\begin{array}{c}\\sigma^{i}\\left(W^{i} * x^{i-1}+b^{i}\\right) \\\\\n\\left[\\begin{array}{c}\\sigma^{i}\\left(W^{i} * x^{i-1}+b^{i}\\right) \\\\\n\\sigma^{i}\\left(U^{i} * x^{i-1}+c^{i}\\right)\\end{array}\\right] \\end{array}\\right]$\n$=\\left[\\begin{array}{c}\\sigma^{i}\\left(W^{i} * x^{i-1}+b^{i}\\right) \\\\\n\\left[\\begin{array}{c}\\sigma^{i}\\left(W^{i} * x^{i-1}+b^{i}\\right) \\\\\n\\sigma^{i}\\left(U^{i} * x^{i-1}+c^{i}\\right)\\end{array}\\right] \\end{array}\\right]$ \t\t(10)\\t\t(11)\nIn equation (11) it is clear that we have xL = xR. Thus,\n$W^{i+1} * x^{i} = Wi+1 * x^{i} + Ui+1 * x^{i} - Ui+1 * x^{i} = Wi+1 * x^{i}$.\t(12)\nUsing equation (12) to compute the (i+1)th layer, we obtain:\n$x^{i+1}=\\sigma^{i+1}\\left(W^{i+1} * x^{i}+b^{i+1}\\right).\t\t(13)$\nTo conclude, the R2WiderR transformation defines new kernels and biases as in (8) and replaces equations (3), (4) with the equations (10) and (13) in the neural network. We provide a full derivation of R2WiderR, in appendix C."}, {"title": "4.2. Residual connections in R2WiderR", "content": "We outline how to adapt residual connections to account for R2WiderR operations. Figure 2 gives a schematic overview of the idea. We consider a simplified case where we do not handle (spatial) shape matching, or down-sampling, and only show how to adapt a residual connection for a single application of R2WiderR. We provide a more"}, {"title": "4.3. Zero initializations", "content": "Before we define R2DeeperR, we first consider zero initializations, that is, how we can initialize a network such that its output is always zero, while still being able to train"}, {"title": "4.4. R2DeeperR", "content": "We propose a novel method for deepening a network using a residual block, initialized to be an identity function. Let zo be a network that is zero initialized in the way described in section 4.3, and therefore we have for all x that zo(x) = 0. Adding an x to each side then gives\n$z_{o}(x)+x=x.$\t(24)\nLet xe be a new volume we wish to add between the consecutive volumes xi and xi+1. For simplicity assume that xi and xi+1 are computed as described in equations (3) and (4), however, they be generalized to include residual connections. The layerwise operation now becomes:\n$x^{i}=\\sigma^{i}\\left(W^{i} * x^{i-1}+b^{i}\\right),$\t(25)\n$x_{e}=z_{o}\\left(x^{i}\\right)+x^{i},$\t(26)\n$x^{i+1}=\\sigma^{i+1}\\left(W^{i+1} * x_{e}+b^{i+1}\\right) .$\t(27)\nFrom equations (24) and (24) we must have that xe = xi. Substituting xe for xi in equation (4), the (i + 1)th layer, gives equation (27)."}, {"title": "5. A comparison of function preserving transforms", "content": "This section discusse similarities and differences between each of the FPTs R2R, Net2Net and NetMorph, with the aim of providing insight into their use cases. A summary of differences can be seen in table 5."}, {"title": "5.1. Non-linearities", "content": "Each of the FPTs makes assumptions about the form of non-linearities they can be applied with. This restricts what activation functions can be used. Let \u03c8 denote a non-linearity used in the networks that we are applying the FPTs to. R2WiderR, Net2WiderNet and NetMorph each need the assumption that the non-linearity maintains channel dependencies, as described in equation (5). That is, if V is applied to volume x with some i, j such that i \u2260 j and Xi = xj, then we require \u03c8(x)i = \u03c8(x)j.\nR2DeeperR, as stated in section 4.3, requires the last non-linearity in the introduced residual block to have a fixed point at zero \u03c8(0) = 0. Whereas, in Net2DeeperNet every non-linearity needs to be idempotent, \u03c8(\u03c8(\u00b7)) = \u03c8(\u00b7). (Chen et al., 2015) incorporate batch normalization by setting its affine parameters to invert the normalization and produce and identity function (for a specific mini-batch).\u00b9\n(Wei et al., 2016) manage to avoid these problems by introducing P-activations, which use a parameter to interpolate between an identity function (which satisfies all the properties discussed above) and any arbitrary non-linearity during training. This method could be applied to any of the FPTs."}, {"title": "5.2. Residual connections", "content": "R2R not only allows the incorporation of residual connections in FPTs, but necessarily requires residual connections for Net2DeeperNet. Our deepening operation is fundamentally different to those considered by (Chen et al., 2015) and (Wei et al., 2016), who approach the problem as one of matrix/convolution decomposition, to split one network layer into two. The Net2DeeperNet operation makes use of the matrix decomposition A = IA, where I is the identity matrix, whereas (Wei et al., 2016) consider that more complex, and dense, decompositions could be used. In contrast, R2DeeperR introduces a completely new identity function in the middle of a neural network, using the residual connection to provide the identity. R2DeeperR allows for a dense initialization of new parameters, and introduces an entire residual block for each application of R2DeeperR, rather than introducing an extra layer for each application.\nWe note that both Net2WiderNet and NetMorph could be adapted to allow for residual connections, similar to section 4.2. For NetMorph they could be introduced in almost identically as for R2WiderR, however, for Net2WiderNetit is not straightforward how to do so.\nAlthough it has been shown that residual connections are not necessary to train deep networks, although they are still widely used to help train networks stably and quickly (Szegedy et al., 2017)."}, {"title": "5.3. Preservation of parameters", "content": "A useful property for the transformation is whether it preserves already existing parameters, i.e any parameters that existed in the teacher network are not changed in the student network. This property holds for R2R and NetMorph, however, it does not for Net2Net. This property allows the teacher and student networks to co-exist with shared parameters, potentially allowing for efficient training of multiple"}, {"title": "5.4. Visualizing the transforms", "content": "To help demonstrate the differences between R2WiderR Net2WiderNet and NetMorph we visualize the weights in the first layer of a a three layer convolutions network, initially containing 16 filters. We train the network on the Cifar-10 classification task (Krizhevsky & Hinton, 2009) until convergence. Then we apply R2WiderR, Net2WiderNet or NetMorph, to add another 16 filters to the first layer, giving a total of 32. After widening we train again until convergence and compare the weights to when the network was widened. The visualizations can be seen in figure 4.\nBecause in R2WiderR and NetMorph there is freedom to choose how about half of the new parameters are initialized, we tended to see some more randomness in the training of the new filters and the filters they converge to. In contrast, in Net2WiderNet there is no freedom in the parameter initialization, and we found the orientation of the new filters often did not change from they initial set-up. However, the colors being detected typically changed. As an example, if a new filter from Net2WiderNet was initialized as a pink/green vertical edge detector, after further training it may become a blue/orange edge detector.\nFinally the visualizations for R2WiderR indicate that the new 16 filters were able to learn different filter orientations and that their symmetry was broken without having to add more noise (as required in Net2Net). However, we found that the filters learned by R2WiderR were often noisy themselves. With NetMorph we found that the new filters tended to be quite noisy and/or contain relatively small weights, where the latter is depicted in figure 4. We think that this noise in R2WiderR and the small-weights in NetMorph could be a result of a faint training signal, after the teacher network has already learned to classify many examples in the training set correctly.\nFor all three FPTs, the first 16 filters learned tend to still"}, {"title": "6. Experiments", "content": "We consider training ResNet-18 networks on Cifar-10 to compare the performance of the FPTs. In appendix D we define the ResNetCifar-10(r) and ResNetCifar-18(r) architectures in more detail, where the standard ResNet architectures (He et al., 2016) have been adapted for the smaller images size in Cifar-10. r denotes that we use r times as many filters in every convolutional layer, We chose r to ensure that the performance on Cifar-10 is limited in the teacher network, so that the student network has something to improve upon.\nIn all of our experiments we initialize the free parameters with standard deviation equal to the numerical standard deviation of the existing weights in the kernel for widening and the standard deviation of the weights in kernel prior when deepening. Specifically, if we wanted to initialize new weights with numerical variance s\u00b2 then we sampled from the uniform distribution $U(-\\sqrt{3 s},\\sqrt{3 s})$.\nAll training hyper-parameters are detailed in appendix D."}, {"title": "6.1. Network convergence tests", "content": "In line with prior work we perform tests similar to (Chen et al., 2015), and we compare the converged accuracies when using different FPTs. To test the widening operators we compare performance of ResNetCifar-18(2-3) networks on Cifar-10. To begin with, a teacher network, with \u221a2 fewer channels (i.e. a ResNetCifar-18(2-3.5) network) is trained to convergence. This network is then used as a teacher network for each of R2WiderR, Net2WiderNet and NetMorph, and the respective student networks are trained to convergence. Additionally, we also compare with"}, {"title": "6.2. Faster training tests", "content": "We also consider if FPTs can be used to train a network to convergence faster than initializing the network from scratch, while still achieving the same performance.In these experiments we train a ResNetCifar-18(2-3.5) or ResNetCifar-10(2-3) teacher network and then appropriately widening or deepening the network using R2R, Net2Net or Network Morphism in the middle of training. Each network is then trained until it converges.\nIn figure 8 we see that training is slower with respect to the number of training updates. However, when we consider the actual number of floating point operations (FLOPs) we find that using the FPTs was faster. This suggests that FPTs can be used in similar training procedures to trade off between computational and sample complexities of training.\nThese experiments also indicate that we need to be careful when initializing new parameters in R2R. We observed that if the new weights were initialized with large values with respect to existing weights in the network it lead to instability in the training, and a drop in performance immediately after the widen. We illustrate this case in the Appendix E.\nThe results obtained in this section indicate that FPTs can be used to train our ResNetCifar-18 networks with a lower computational cost and similar performance."}, {"title": "7. Conclusion", "content": "In this work we have introduced new FPTs: R2WiderR and R2DeeperR, that incorporate residual connections into the theory of FPTs. We derived how they preserve the function represented by the networks, despite altering the architecture. We then provided an in depth discussion on the differences, similarities and use-cases of R2R, Net2Net and NetMorph. Experiments conducted on Cifar-10 demonstrated that all three FPT schemes have similar performance, thereby allowing a wide range of neural network architectures to have FPTs applied to them. Finally, we also demonstrated that FPTs can be used to trade off between sample complexity and computational complexity of training."}, {"title": "A. Code", "content": "Code implemented in PyTorch (whichever version was current at the start of 2019): https://github.com/MWPainter/Deep-\nNeuroevolution-With-SharedWeights\u2014Ensembling-A-Better-Solution.\nI spent a bit of time after the ICML submission trying to address the concerns around networks achieving closer to state-of-the-art performance and having results with larger datasets than Cifar-10."}, {"title": "C. A full derivation of R2R", "content": "In this section we provide an extended descriptions of the steps we went through to derive R2R so that the padding produces a function preserving transform. In particular we provide a more principled approach by finding equations that must hold for the transformation/padding to be function preserving and then solving them. Solutions can then be found by observation, yielding the methods presented in section 4.\nAdditional note for arxiv version of this paper: I recall having some maths (that I never got around to writing up) that suggested that the symmetry introduced in the zero-initializations should be broken after some number of backups (if I remember correctly it did rely on either the input/output data not being symmetric in some sense that I cannot recall), which can be observed in Figure 4. However, in retrospect, some of the new filters are still quite symmetrical, so adding a small amount of noise to any new parameters may be helpful for symmetry breaking."}, {"title": "C.1. R2WiderR", "content": "Recall equations (3) and (4) which we used to define the operation of a neural network. Suppose that we want to widen xi by padding with XL and XR x, of shapes (E \u00d7 hi \u00d7 wi), to produce a new volume zi. Let Wi, Wh with shape (Ex Ci-1\u00d7 khi-1 x kwi-1), Wi+1, Wi+1 with shape (Ci+1 x Ex khi \u00d7 kwi), and b, bir with shape (E) be the new"}, {"title": "C.2. Residual connections", "content": "In section 4.2 we outlined how residual connections can be adapted for R2WiderR, however, we did not cover how to deal with either widening a volume multiple times or how to handle down-sampling. In this section we generalize the argument to handle both of these cases.\nFor convenience let \u03b8i = {Wi, bi, \u03c1i } and define Foi as a shorthand for equation (28) as follows:\n$x^{i}= F^{i}\\left(x^{i-1}\\right)\\stackrel{\\text { def }}{=}\\sigma^{i}\\left(W^{i} * x^{i-1}+b^{i}\\right) .$\t\t(35)\nConsider if R2WiderR is applied to an intermediate volume xl, producing the two equal volumes 21 and xr. If xl was used as part of a residual connection, then we need to take into account how to handle xf and xk over that residual connection. We can re-write equation (34) as:"}, {"title": "C.3. Zero initializations", "content": "Here we provide a full derivation of our zero initializations that we use in R2DeeperR. Let g01:n be an arbitrary neural network, with parameters (\u03b81:n and let xn = g01:n (x0) be the output of this network. We will add two additional laters on the output, x\u00ba\u00b9 and x2, with shapes (C\u00ba1 \u00d7 h\u00ba1 \u00d7 w\u00ba1) and (C\u00ba2 \u00d7 h\u00ba2 \u00d7 w\u00ba2) respectively.\nAssuming that C01 and C02 are even, let WL\u00b9, W\u00b9 have shape (C101 /2 \u00d7 Cm \u00d7 h\u2033 \u00d7 wn) and let W12, W\u00b2 have shape (C\u00ba2 \u00d7 C\u00ba1/2 \u00d7 h\u2033 \u00d7 wn). Also let b, b have shape (C1/2) and let b\u00ba2 have shape (CO2). Additionally define \u03c1\u00ba1 = {p, p} and \u03c1\u00ba\u00b2 = {p\u00b2, pR }. Now, define new parameters for the network:\n$\\begin{aligned}\nW_{O 1}=\\left[\\begin{array}{c}\nW_{L}^{O 1} \\\\\nW_{R}^{O 1}\\end{array}\\right],    \t\t(41)\\end{aligned}$\n$W^{O 2}=\\left[W_{L}^{O 2} W_{R}^{O 2}\\right],    \t\t(42)$\n$\\begin{aligned}\nb_{L}^{O 1} \\\\\n\\b_{R}^{O 1}\\end{array}\\right],    \t\t(43)\\end{aligned}$\nThe computation for new layers 01 and 02 is then:\n$x^{O 1}=\\sigma^{O 1}\\left(W^{O 1} * x^{n}+b^{O 1}\\right),     \t\t(44)$\n$x^{O 2}=\\sigma^{O 2}\\left(W^{O 2} * x^{O 1}+b^{O 2}\\right).     \t\t(45)$\nExpanding the definitions of the new params and multiplying out we get:"}, {"title": "C.4. R2DeeperR", "content": "Given the derivation of zero initializations we just covered, the discussion in section 4.4 is a sufficient derivation, as given zero initializations R2DeeperR follows quickly. We still keep this section for completeness of appendix C."}, {"title": "D. Architectures and hyper-parameters", "content": "To be as transparent as possible with how we ran our experiments, in tables 3 and 4 we provide descriptions of the ResNet architectures we used, using the same notation as (He et al., 2016) in the original ResNet paper. Moreover, for every test run we provide all training parameters in table 5. Apart from the teacher networks, which were heavily regularize to avoid the problem described in section 6.1, hyper-paramters were chosen using a hyper-parameter search for each network.\nFor all of our tests, inputs to the neural networks were color normalized. All of the training and validation accuracies computed for Cifar-10 were using a single view and the entire 32 \u00d7 32 image as input, with no cropping. We performed no other augmentation.\nIn all tests, a widen operation consists of multiplying the number of filters by a factor of 1.5, therefore, when a widen operation is applied to ResNetCifar() it is transformed to a ResNetCifar() network. We only ever applied a deepen"}, {"title": "E. Weight initialization in R2R", "content": "In figure 10 we compare what happens if we use a more standard He initialization for new parameters in the network, compared to matching the standard deviations of local weights as described in section 6.2."}, {"title": "F. Future work", "content": "Although we have discussed R2R, Net2Net and Network Morphism in depth in this work, there are still some properties that we would have liked to consider. For example, we typically need to change the learning rate and weight decay, and be careful about the initialization scale of new parameters. We would have liked to consider the interplay between the learning rate, weight decay, weight initialization and optimizer, and provided (theoretical) guidance on how to manipulate these hyper-parameters best when performing an FPT.\nAdditionally, we think that the freedom in the initializations could allow more complex iniilization schemes to be considered. For example, a hybrid system between R2WiderR and Net2WiderNet could be considered, where new channels in R2WiderRare initialized as copies of other channels. Alternatively, learning good initializations has been considered in the meta-learning problem (Finn et al., 2017), and so maybe 'good' initializations could be learned in some problems.\nFinally, Network Morphism also considers kernel size morphing and subnet morphing, and we have not consdered these transformations in this work. We note that our zero initialization from section 4.3 could be used as an alternative subnet morphing scheme to that considered by (Wei et al., 2016)."}]}