{"title": "An Empirical Evaluation of Using ChatGPT to Summarize Disputes for Recommending Similar Labor and Employment Cases in Chinese", "authors": ["Po-Hsien Wu", "Chao-Lin Liu", "Wei-Jie Li"], "abstract": "We present a hybrid mechanism for recommending similar cases of labor and employment litigations. The classifier determines the similarity based on the itemized disputes of the two cases, that the courts prepared. We cluster the disputes, compute the cosine similarity between the disputes, and use the results as the features for the classification tasks. Experimental results indicate that this hybrid approach outperformed our previous system, which considered only the information about the clusters of the disputes. We replaced the disputes that were prepared by the courts with the itemized disputes that were generated by GPT-3.5 and GPT-4, and repeated the same experiments. Using the disputes generated by GPT-4 led to better results. Although our classifier did not perform as well when using the disputes that the ChatGPT generated, the results were satisfactory. Hence, we hope that the future large-language models will become practically useful.", "sections": [{"title": "Introduction", "content": "Searching and finding similar previous cases are the basis for various possible applications in the legal domain. Similar previous cases may provide clues about how judges may judge a new case and what lawyers can do in oral arguments and cross examinations [4][7]. The research for searching similar previous cases started many years ago, and has attracted the attention of many researchers in recent years.\nWe will mention some closely related issues now, but we cannot offer a comprehensive survey for the research of recommending similar previous cases. Researchers have explored two possible research directions. Some suggested that the citation-network-based method may be less applicable to the legal domain [5], and we and many others took the text-based approach [11]. In the text-based camp, researchers must determine how they define the similarity between two cases. This includes at least two issues. First, how do we define similarity between two cases numerically? Second, do we compare every statement in the two texts as a whole, or do we compare selected parts partially [11]? With the support from advanced artificial intelligence techniques, it is possible to consider more specific background information about the lawsuits in comparing the cases [2]. Low-resource remains a challenge for similar case recommendation research. CAIL-2019 [18], which represents the national corpus of China, contains slightly more than 8000 labeled pairs of cases [2][3].\nIn previous work, we explored using a clustering-based method for recommending similar cases [10]. Two cases were judged to be similar if they had similar distributions in the clusters of disputes. Hence, we have a narrow but practical standard of similarity. The disputes between the employees and the employers are the main source of the litigations. The scale of our experiments was thus limited by the number of cases that explicitly listed disputes between the litigants. In this paper, we report the results of our classification of whether or not two cases are similar. We built the classifiers with deep-learning approaches, and judged whether the two given cases were similar based on the disputes between the litigants of civil cases, i.e., the labor and employment cases. This can be achieved with our previous dataset.\nWith the availability of ChatGPT, we evaluated the disputes that we asked the ChatGPT to generate from the litigants' claims. If ChatGPT would serve as a reliable source of itemized disputes, we could expand the scale of our experiments [19]. We replaced the GPT-generated disputes with the original disputes in our classification experiments. If the new outcomes are satisfactory, we may consider using ChatGPT to summarize the disputes of the litigants for cases that do not have itemized disputes, thus alleviating the problems of low resources for our approach.\nWe observed encouraging results when using GPT-3.5 and GPT-4. In the remainder of this paper, we offer a formal definition of our research problem in Section 2. We provide more background information about our data in Section 3, and explain how we used ChatGPT to summarize the litigants' claims for us in Section 4. We elaborate on the design of the experiments in Section 5, and report the experimental results in Section 6. We then wrap up with some discussions in Section 7."}, {"title": "Problem Definition", "content": "We evaluate the potential contribution of ChatGPT in comparing the similarity of two labor and employment cases in Chinese.\nIn a previous work [10], we evaluated clustering-based methods for recommending similar labor and employment cases. Since we have labeled some case pairs by their similarity, we could train some classifiers with these annotated cases, and use the trained classifiers to guess whether two future cases are similar.\nThe real challenge is how we and the annotators could have determined whether the two cases were similar. It is not easy to define \"similar\u201d even in everyday life, let alone in judicial cases. In our work, we focused on whether the disputes between the employees and employers in the two cases are similar. This is certainly not the only way to define \"similarity\" between two cases. For instance, one might be more interested in which party won the cases or in the industry and years of the cases.\nBased on the narrowed perspective of similarity, we can define our work in the following way. Assume that we have collected the judgment documents of m previous cases that explicitly included the disputes between the employees and the employers. We denote this collection as $C = \\{C_1, C_2, \u2026, C_m\\}$. Let $K = \\{K_1, K_2, \u2026, K_m\\}$, where $K_i$"}, {"title": "Data Source, Selection and Preprocessing", "content": "We provide information about the data sources, discuss some basic statistics of our data, and introduce an important step in the preprocessing procedure in this section. We have described the major steps for data preparation in previous papers in JURIX [8] and JURISIN [9] and the legalAIIA workshop in ICAIL [10]. We will not repeat all of the details to avoid the concerns of self-plagiarism.", "sub_sections": [{"title": "Data Source: Taiwan Judicial Yuan", "content": "We obtained the judicial documents from an open repository that is maintained by the Judicial Yuan. The Judicial Yuan, the highest governing body for Taiwan's judicial system, oversees the publication of judgment documents from various courts, including local, high, supreme, and special courts. These documents are typically released on the TWJY website three months after the judgment date, with February judgments, for example, becoming available in May. Some documents may not be published due to legal reasons, such as protecting minors or litigants, and their contents are anonymized for privacy.\nAs of July 2023, the TWJY website hosts approximately 18.7 million documents, dating back to January 1996. Initially, only documents from a limited number of special courts were available in the first few years, with broader coverage starting from 2000 onward. The Judicial Yuan updates the website monthly, with a three-month lag, ensuring that users can access and download new judgment documents from various courts in a compressed file. However, the number of available documents may fluctuate due to retractions based on legal reasons. Anonymization of published documents is mandated by law, and the government takes responsibility for safeguarding the privacy of individuals involved in lawsuits."}, {"title": "Data Selection: Cases with Listed Disputes", "content": "Each document in the TWJY is a JSON file and adopts a common top-level structure. The structure consists of seven fields: JID is the long identification number; JYEAR is the year when the case occurred in terms of Taiwan calendar; JCASE is the abbreviated code for the type of the lawsuit; JNO is the short identification number for the lawsuit; JDATE is the date for the current judgment in terms of the Western calendar; JTITLE is the category of the lawsuit, and JFULL is the full text for the judgment document.\nTherefore, using the contents of JCASE and JTITLE fields to find judgment documents of labor and employment litigations, among a myriad of case categories, is a basic step. We focus on the judgments of the local courts, where the judges would consider the factual parts of the lawsuits, and using the codes in the JCASE and the JTITLE fields could help us exclude appeal cases.\nThe filtering of relevant and usable documents needs more steps. Sometimes, even when the JCASE and JTITLE fields seem to qualify a document, we may find clues in the JFULL field that indicate the case does not meet our needs.\nMost importantly, in the current study, we look for cases in which the courts explicitly recorded the disputes between the plaintiffs and the defendants in the judgment document. We look into the JFULL field to ensure the documents meet this requirement. The listed disputes provide important information about the lawsuits, and help the lawsuits to proceed more effectively. Nevertheless, not all of the judgment documents would record the disputes.\nAt this moment, we found 3835 cases from 21 local courts in Taiwan."}, {"title": "Disputes in Labor and Employment Cases", "content": "For each of the judgment documents we selected in Section 3.2, there is a section that itemizes the disputes between the plaintiffs and the defendants. The format looks like the following, although the exact formats may vary, and they are actually in Chinese. (See Appendix A for real examples.)"}]}, {"title": "Summarizing Disputes with ChatGPT", "content": "Large language models and generative AI have shown their impressive applicability in a wide range of tasks in the past year, including tasks that are in the legal domain [16]. ChatGPT of OpenAI is perhaps the most famous leader for this ground-breaking development. One main concern of applying the generative technology was the hallucination problem. There is a case in which a legal practitioner used the ChatGPT to generate a legal document that cited a non-existing document [15]. The research community has been very concerned about this annoying problem very intensively.\nAs users, we can avoid the hallucination problem using prompts with specific constraints. In a typical judgment document, we can find statements of the plaintiffs and the statements of the defendants. A list of disputes between the plaintiffs and the defendants will follow, if the document has the list. We extracted and used the statements of the plaintiffs and the statements of the defendants from the judgment documents in our prompts.\nSince our goal was to ask the ChatGPT to summarize the disputes between the plaintiffs and the defendants, we must try to confine the ChatGPT to finding the disputes only from the statements of the plaintiffs and the defendants. To this end, our prompts consist of three steps. First, we ask ChatGPT to summarize the plaintiffs' statements with a list of items. Second, we ask the ChatGPT to summarize the defendants' statements with a list of the items. Then, we asked ChatGPT to itemize the disputes between the plaintiffs and the defendants based on the two lists it returned in the previous two steps. Upon the request of the reviewers, we provide the segment of our Python code for this three-step operation in Appendix B.\nThe disputes that we received would look like the examples that we explained in Section 3.3, and we could use them in our experiments as if they were the disputes that were listed in the original judgment documents.\nWe have implemented the ideas with GPT-3.5 and GPT-4 (0613). We set the parameters temperature to 0.7 and 0.3 for GPT-3.5 and GPT-4, respectively, to avoid over-generation. Currently, both ChatGPT versions limit the number of tokens for the conversations, so we confine the number of tokens of our prompts to 11500 and 6000 for"}, {"title": "Classification for Similar Case Recommendations", "content": "We reported some results of applying clustering-based methods to recommend similar cases in [10]. In this presentation, we reported the results of using classification-based methods for the recommendation task. We will report the resulting differences in the quality of the recommendations in Section 6.", "sub_sections": [{"title": "Data for Training, Validation, and Test", "content": "Table 1 provides more statistics about the judgment documents used in the current work. We have 12,385 dispute statements in the selected cases from the TWJY. We have annotated 2288 pairs of similar cases: 1360 not-similar pairs and 928 similar pairs [10]. Only 1543 of the 3835 cases were part of the labeled pairs of cases. We did not have 2\u00d73835=7670 labeled cases because some labeled pairs shared a common case.\nSince all of these 1543 cases were not used in finetuning the BERT models (cf. Section 5.2), there would not be data leakage problems for embedding. In addition, we used cosine similarity in the classifiers, so using the same case in different pairs in the training and test would not cause the problem of data leakage.\nDue to the limitations of GPT-3.5, we did not obtain the dispute statements for cases when we used GPT-3.5 (cf. Section 4). Hence, we had fewer labeled pairs and cases when we used GPT-3.5 in our experiments."}, {"title": "Text Embedding and Fine-Tuning the Pretrained BERT models", "content": "We vectorized the statements of disputes only with the TFIDFVectorizer of scikit-learn [10]. Now, we vectorized the statements with the Sentence-BERT [13] with either the Lawformer [17] or with the Chinese ROBERTa [1].\nThe BERT models are both for Chinese, only that the Lawformer is specifically pre-trained with legal documents of China that were recorded with simplified Chinese. Hence, in our experiments, we may choose to fine-tune the BERT models with the legal documents of Taiwan, which were recorded in traditional Chinese. There are some"}, {"title": "Transforming the Text for Convolutional Neural Networks", "content": "Without loss of generality, we will consider the task to determine whether two cases $C_i$ and $C_j$ are similar in the following deliberation. Following the notation that we introduced in Section 2, we compute the cosine similarity between the disputes in $K_i$ and $K_j$. Let $K_i = \\{k_{i,1}, k_{i,2},\u2026, k_{i,\\alpha}, \u2026, k_{i,u}\\}$ and $K_j = \\{k_{j,1}, k_{j,2},\u2026,k_{j,\\beta},\u2026,k_{j,v}\\}$. The cosine similarity $s_{\\alpha,\\beta}^{i,j}$ between a given pair of disputes, $k_{i,\\alpha}$ from $K_i$ and $k_{j,\\beta}$ from $K_j$, is defined in (1), where $v_{i,\\alpha}$ and $v_{j,\\beta}$ denote the Sentence-BERT vectors of $k_{i,\\alpha}$ and $k_{j,\\beta}$, respectively. Theoretically, the range of $s_{\\alpha,\\beta}^{i,j}$ is [-1,1], but the majority were in the range of [0,1], and $s_{\\alpha,\\beta}^{i,j}$ = 1 only when $k_{i,\\alpha}$ = $k_{j,\\beta}$.\n$S_{\\alpha,\\beta}^{i,j} = cosine\\_similarity(v_{i,\\alpha}, v_{j,\\beta})$ (1)\nWe construct a matrix $M_{ij}$ for $C_i$ and $C_j$, and an element $m_{\\alpha,\\beta}^{i,j}$ in $M_{ij}$ is set to $1-s_{\\alpha,\\beta}^{i,j}$. Namely, the element $m_{\\alpha,\\beta}^{i,j}$ in $M_{i,j}$ at the position ($\\alpha,\\beta$) is the similarity between the $\\alpha$th and $\\beta$th disputes of $C_i$ and $C_j$, respectively.\nWe still clustered all of the disputes of the cases in C, as we did and explained in [10], except that we switched to using the HDBSCAN of scikit-learn for clustering. Let $m_{max}$ and $m_{min}$ denote the largest and the smallest values in $M_{i,j}$, respectively. We set the parameter cluster\\_select\\_epison, $\\epsilon$, for HDBSCAN to $m_{min} + 0.8 \\times (m_{max} - m_{min})$ to control the number of clusters that HDBSCAN may produce. Let $\\mathcal{C} = \\{C_1, C_2, \u2026, C_\\gamma\\}$ denote the resulting clusters that contained all of the disputes in $K$. The actual values of $\\gamma$ varied in different experiments and were determined automatically by HDBSCAN. For convenience of communication, we set the elements in C to integers. Namely, $C_1 = 1, C_2 = 2, ..., and C_{\\gamma} = \\gamma$. After the clustering step, each dispute"}]}, {"title": "Empirical Evaluations", "content": "We conducted experiments to answer two questions. The first is whether we can use ChatGPT to summarize and itemize the disputes for the labor and employment litigations. The second is to examine whether or not and how to use the Lawformer in our studies. More specifically, should we use an ordinary Chinese RoBERTa or Lawformer for embedding? What if we fine-tune both BERT models?\nSince random numbers play important roles in the operations of artificial neural networks (deep learning), we repeated each of our experiments 30 times to draw a boxplot for the experiment. The training, validation, and test data were resampled every time. Since the BERT models were fine-tuned with data that were completely different from the data that we listed in Table 1, we did not repeat the finetuning step.\nAfter completing the training process, we asked the classifiers to predict whether or not the pairs of cases in the reserved 20% of data were similar. We only fine-tuned our BERT model once, but re-split the labeled pairs of cases 30 times so that we could make boxplots for the results. We can create boxplots for both the F1 measure and the traditional accuracy."}, {"title": "Concluding Remarks and Further Discussions", "content": "We reported two lines of work in this paper. The first is about the classification procedure that consisted of a clustering component and a convolutional neural network component. This hybrid design outperformed our previous recommender for similar labor and employment cases [10].\nOur second exploration may be more interesting for the JURISIN. We relied on the itemized disputes that the courts prepared for clustering and classification. Not all judgment documents we could download from TWJY contained such itemized disputes. The availability of such itemized disputes confines the scale of our experiments, so we tried to use the ChatGPT to summarize the claims and to itemize the disputes of the litigants. We designed our prompts to avoid possible hallucinations of the ChatGPT. We evaluated the GPT-generated disputes by using them in place of the court-prepared disputes. The results, as depicted in Figures 4 and 5, were quite encouraging. We observed that GPT-4 was better than GPT-3.5 in the final results. As we read the GPT-generated disputes in person, we also observed the superiority of GPT-4. As the technology of large language models has advanced so much recently, we look forward to the days when they become practically useful.\nThe world of LLMs, including ChatGPT, is changing at an extremely high speed. Since ChatGPT was launched in November 2022, GPT-4, GPT-4 Turbo (or GPT-4-1106-preview), and GPT-4-0125-preview followed within 14 months. The experimental results reported in this manuscript were based on GPT-3.5 and GPT-4. We have completed some preliminary evaluations of GPT-4 Turbo and GPT-3.5 Turbo for our tasks, but we have not evaluated GPT-4-0125-preview completely yet. Using GPT-4 Turbo and GPT-3.5 Turbo offered better results than using the previous GPT-4 and GPT-3.5. That was partially due to the increases in allowed tokens in the conversations. The improvements are not surprising and are certainly welcomed, as we have discussed in Section 6. The evaluation of the LLMs is not a trivial task, especially when the LLMs evolve so quickly and so aggressively [12]. We plan to investigate the factors that led to the improvements and analyze the influences of the factors on the effectiveness and efficiency of our clustering and classification components.\nReviewers of this paper recommended us to calculate the ROUGE scores [6] of the disputes that were generated by the LLMs unanimously, using the disputes that were listed in judgment documents as the references. ROUGE scores were used for evaluating the algorithmically generated summaries in the literature. However, there are different ways to the ROUGE scores, ROUGE-L in particular, when the generated summaries and the references have different numbers of sentences (itemized disputes in our work). In addition, we have to choose a segmentation tool for Chinese text. The task of"}]}