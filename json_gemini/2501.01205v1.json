{"title": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects", "authors": ["Abdullah Mushtaq", "Muhammad Rafay Naeem", "Ibrahim Ghaznavi", "Muhammad Imran Taj", "Imran Hashmi", "Junaid Qadir"], "abstract": "Multi-Agent Large Language Models (LLMs) are gaining significant attention for their ability to harness collective intelligence in complex problem-solving, decision-making, and planning tasks. This aligns with the concept of the wisdom of crowds, where diverse agents contribute collectively to generating effective solutions, making it particularly suitable for educational settings. Senior design projects, also known as capstone or final year projects, are pivotal in engineering education as they integrate theoretical knowledge with practical application, fostering critical thinking, teamwork, and real-world problem-solving skills. In this paper, we explore the use of Multi-Agent LLMs in supporting these senior design projects undertaken by engineering students, which often involve multidisciplinary considerations and conflicting objectives, such as optimizing technical performance while addressing ethical, social, and environmental concerns. We propose a framework where distinct LLM agents represent different expert perspectives, such as problem formulation agents, system complexity agents, societal and ethical agents, or project managers, thus facilitating a holistic problem-solving approach. This implementation leverages standard multi-agent system (MAS) concepts such as coordination, cooperation, and negotiation, incorporating prompt engineering to develop diverse personas for each agent. These agents engage in rich, collaborative dialogues to simulate human engineering teams, guided by principles from swarm AI to efficiently balance individual contributions towards a unified solution. We adapt these techniques to create a collaboration structure for LLM agents, encouraging interdisciplinary reasoning and negotiation similar to real-world senior design projects. To assess the efficacy of this framework, we collected six proposals of engineering and computer science of typical senior capstone projects and evaluated the performance of Multi-Agent and single-agent LLMs using both custom-designed metrics developed in consultation with engineering faculty and some widely used NLP-based metrics. These metrics cover technical quality, ethical considerations, social impact, and feasibility, ensuring that our evaluation aligns with the educational objectives of engineering design. Our findings suggest that Multi-Agent LLMs can provide a richer, more inclusive problem-solving environment compared to single-agent systems, offering a promising tool for enhancing the educational experience of engineering and computer science students by simulating the complexity and collaboration of real-world engineering and computer science practice. By supporting senior design projects, this tool not only aids in achieving academic excellence but also prepares students for the multifaceted challenges they will face in their professional engineering careers.", "sections": [{"title": "I. INTRODUCTION", "content": "The senior design project (SDP), also known as the capstone or final year project, is a vital component of engineering and computer science education [1]. It offers students an opportunity to apply their theoretical knowledge in tackling complex, real-world engineering problems, providing an authentic learning experience that integrates the essential competencies required for 21st-century engineers [2], [3]. Accrediting bodies, such as the US Accreditation Board for Engineering and Technology (ABET) emphasize the importance of SDPs as key opportunities for students to engage with the kinds of complex, multidisciplinary challenges they will encounter in professional practice. The problems addressed in SDPs typically involve no straightforward solutions; instead, they require students to balance competing objectives, such as optimizing technical performance while addressing ethical, environmental, and social concerns.\nA hallmark of modern engineering practice is the increasingly globalized context in which engineers operate. Engineering products must be designed to meet the needs of a global audience, often requiring the integration of diverse perspectives. This necessitates cultural intelligence\u2014the ability to navigate different cultural contexts and work effectively with stakeholders from various backgrounds. Generative AI, particularly LLMs, offers an innovative way to simulate these diverse perspectives, providing students with an environment to engage in interdisciplinary problem-solving [4], [5]. These LLM agents represent different expert viewpoints and foster a collaborative approach that reflects real-world engineering practices.\nThe ability to solve complex problems is an essential competence in both education and professional life, as individuals increasingly face challenges arising from globalization and digitalization. A complex problem occurs when a person seeks to achieve a goal for which no clear or straightforward solution is available. Unlike non-complex problems, complex problem-solving (CPS) involves dynamic and opaque barriers, where the initial information is incomplete or subject to change. This definition, as articulated by Fischer, Greiff, and Funke, emphasizes that complex problems require adaptive thinking and flexibility in problem-solving approaches [6], [7]. In the context of engineering, the complexity is further amplified"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "SDPs, also referred to as capstone or final-year projects, are\na crucial component of computing and engineering education.\nAs required by ABET and other accrediting bodies, SDPs\nserve as a culminating experience that integrates the knowledge\nand skills students have acquired over their academic careers.\nThese projects require students to solve complex, real-world\nengineering problems that involve conflicting objectives, trade-\noffs, and ethical considerations. For instance, engineers must\noften balance cost efficiency with environmental sustainability,\nor optimize technical performance while adhering to regulatory\nconstraints.\nIn these settings, teamwork and collaboration across diverse\nskills and perspectives are essential. Given the globalized nature\nof modern engineering practice, students are also required to\nconsider cultural and social factors when designing engineering\nsolutions. This is especially important as engineers increasingly\nwork in global teams and must develop products that meet the\nneeds of an international audience. Therefore, engaging with\nmultiple stakeholders and considering the broader societal and\ncultural contexts of engineering solutions are vital competencies\nthat SDPs aim to cultivate."}, {"title": "A. The Role of SDPs in Engineering Education", "content": "SDPs, also referred to as capstone or final-year projects, are\na crucial component of computing and engineering education.\nAs required by ABET and other accrediting bodies, SDPs\nserve as a culminating experience that integrates the knowledge\nand skills students have acquired over their academic careers.\nThese projects require students to solve complex, real-world\nengineering problems that involve conflicting objectives, trade-\noffs, and ethical considerations. For instance, engineers must\noften balance cost efficiency with environmental sustainability,\nor optimize technical performance while adhering to regulatory\nconstraints."}, {"title": "B. Complex Problem Solving and Diversity Dividend", "content": "Complex engineering problems, as per the definition of ABET\u00b9, have the following attributes, \u201cinvolving wide-ranging or conflicting technical issues, having no obvious solution, addressing problems not encompassed by current standards and codes, involving diverse groups of stakeholders, including many component parts or sub-problems, involving multiple disciplines, or having significant consequences in a range of contexts.\u201d\nSeveral frameworks have been proposed that leverage diversity to foster effective problem-solving. One well-known approach is Edward de Bono's Six Thinking Hats framework, which encourages individuals to look at problems from multiple perspectives-logical, emotional, and creative, among others. This structured, yet flexible, approach is particularly effective when integrated with MAS, as it mirrors the interdisciplinary thinking necessary to solve complex engineering problems. Scott Page's book The Difference [11] emphasizes the value of diversity in problem-solving, particularly in complex systems. Page argues that teams composed of individuals with different skills, experiences, and perspectives are better equipped to solve complex problems than homogenous teams. This aligns with the wisdom of crowds principle, which suggests that collective intelligence can outperform individual expertise, particularly when the group is diverse and composed of independent thinkers. Similarly, Minsky's The Society of Mind [12] underscores the importance of diverse cognitive processes in problem-solving, positing that complex thought emerges from the interaction of simpler, specialized processes."}, {"title": "C. Multi-Agent Systems and Agent-Based Modeling", "content": "MAS is widely applied in fields like robotics, artificial intelli-gence, and complex systems modeling to simulate autonomous agents acting within dynamic environments. While each agent operates independently, their collective behavior can lead to emergent properties\u2014patterns or outcomes that are challenging to predict solely from individual actions. In engineering education, MAS can be used to simulate stakeholder interactions in complex projects, enabling students to experience realistic decision-making, collaboration, and problem-solving scenarios.\nAgent-Based Modeling (ABM) extends MAS by offering a detailed computational framework for simulating interactions between agents and their environments. ABM is especially valuable for examining systems where individual behaviors directly influence collective outcomes, making it ideal for complex social or engineering contexts. In SDPs, ABM can simulate interactions among engineers, project managers, regulators, and other stakeholders, giving students practical insight into the collaborative and often unpredictable nature of professional practice [13], [14]. These simulations provide a flexible and"}, {"title": "D. MAS and LLMs in Educational Contexts", "content": "The integration of MAS and LLMs offers significant promise for enhancing engineering education, especially in tackling complex, real-world problems. By simulating interactions between diverse autonomous agents, MAS allows educators and students to explore the trade-offs, dilemmas, and decision-making processes typical in multidisciplinary projects. These systems mirror real-world engineering dynamics, where professionals must collaborate across domains, negotiate conflicting objectives, and optimize under constraints.\nLLMs further enhance this framework by representing expert personas such as problem formulation agents, project managers, and ethical and societal agents-enabling students to engage in interdisciplinary dialogue. Studies have shown that this approach leads to more innovative and robust solutions by leveraging the \"wisdom of crowds\" effect, as described by Surowiecki [9]. By fostering diverse perspectives, students gain deeper insights into the complexities of engineering challenges [15], [16], [17].\nExisting research highlights the potential of LLM agents in simulating collaborative problem-solving environments, where students engage with virtual experts on issues such as ethical dilemmas and environmental trade-offs. These interactions help students develop critical thinking skills and a holistic approach to problem-solving, essential for SDPs. Additionally, MAS and LLMs support adaptive learning by providing real-time feedback, allowing students to experiment and learn from their decisions in a risk-free environment. This active learning approach enhances their ability to navigate complex, globalized engineering challenges [15], [16]."}, {"title": "E. Existing LLM Multi-Agent Frameworks", "content": "Several LLM-based multi-agent frameworks have emerged, offering advanced capabilities for addressing complex, inter-disciplinary challenges. In educational settings, these frameworks are particularly valuable for their ability to simulate real-world scenarios, fostering critical thinking, collaboration, and problem-solving skills. Below are some examples of such frameworks and their key features:\n1) Camel-AI [15] is a communicative agent framework designed to simulate a \"society\" of LLM agents representing different personas. It excels in fostering interdisciplinary dialogue among agents, making it ideal for tasks that require negotiation and the integration of diverse viewpoints. Camel's capacity to explore various facets of \u201cmind\u201d interactions makes it a strong candidate for projects where multiple perspectives are needed, such as senior design projects.\n2) Crew.AI [18] is another prominent open-source multi-agent orchestration framework. This Python-based platform allows the orchestration of role-playing AI agents, working together as a cohesive assembly or \u201ccrew\u201d to complete tasks. The framework's strength lies in automating multi-agent workflows, making it particularly useful in scenarios requiring the coordination of diverse agent roles for collaborative decision-making processes.\n3) MegaAgent [16] is designed to handle large-scale cooperation in MAS. Its primary strength lies in its scalability, enabling a vast number of agents to work together in a coordinated fashion. This makes it suitable for simulating large, complex systems, providing students with insights into how large teams or systems operate in engineering contexts.\n4) AgentScope [19] focuses on multi-agent simulation at a very large scale, providing a highly detailed simulation environment. Its capacity to handle complex, dynamic interactions between agents makes it particularly useful for simulations that require a high degree of realism, such as real-time problem-solving.\n5) OpenAgents [20] and Agent Lumos [21] are modular frameworks that enable flexible training of LLM agents. OpenAgents provides a platform for building and deploying autonomous language agents in diverse environments, while Agent Lumos unifies and simplifies the training process for these agents. These frameworks are well-suited contexts where the focus is on developing customized agents that can be tuned for specific tasks, such as providing real-time feedback or conducting collaborative discussions.\nTogether, these frameworks demonstrate the transformative potential of LLM-based multi-agent systems in education. They enable the creation of highly interactive, collaborative environments where students can engage with diverse expert perspectives and gain hands-on experience solving complex engineering problems. These systems provide the flexibility, scalability, and adaptability necessary for real-world problem-solving, making them invaluable tools for enhancing engineering education. For our proposed MAS, we selected Camel AI [15] due to its use of role-playing and inception prompting, which facilitate agent collaboration with minimal intervention, effectively simulating interdisciplinary teamwork. Additionally, its capability to tackle complex problems by breaking them down into focused, manageable subtasks for each agent makes it an ideal choice."}, {"title": "III. METHODOLOGY", "content": "This section presents the methodology used to construct and evaluate our Multi-Agent LLM (MAS LLM) framework for supporting complex engineering problem-solving in SDPs. The methodology is divided into two primary subsections:"}, {"title": "A. MAS LLM Framework Construction", "content": "The first subsection outlines the technical details of how the MAS LLM framework was developed. This includes the LLM model used, the integration of the model into a multi-agent system, and the customization of agent roles to simulate diverse expert perspectives such as project managers, breadth and depth agents, and societal and ethical agents. We describe how the system was designed to promote interdisciplinary collaboration and support real-time feedback for students.\nConcretely, we designed a copilot-style LLM-powered MAS for engineering and computing students' senior design projects. Fig. 1 illustrates the system design of our proposed MAS. This system comprises eight LLM agents:\n1) Problem Formulation Agent\n2) Breadth and Depth Agent\n3) Ambiguity and Uncertainty Agent\n4) System Complexity Agent\n5) Technical Innovation and Risk Management Agent\n6) Societal and Ethical Consideration Agent\n7) Methodology and Approach Agent\n8) Comprehensive Evaluation Agent"}, {"title": "B. Evaluation Methodology", "content": "The second subsection explains how the framework was evaluated in the context of SDPs. We detail the metrics used to assess the effectiveness of the MAS LLM framework in enhancing student learning, collaboration, and problem-solving skills."}, {"title": "1) Comparing Faculty and Agent Scores", "content": "We designed a methodology to systematically evaluate the performance of this proposed MAS. In a typical SDP progression cycle, students write the initial draft of their proposal and share it with their supervisor/advisor for feedback. Students then incorporate the suggested changes into the proposal, repeating this process multiple times to ensure the proposal meets the standards set by the faculty at their institution. This process takes a significant amount of time for both students and supervisors to prepare the proposal before moving on to the actual development phase of the project. We advocate for the engineering and computing community to adopt this co-pilot system to enhance workflows and facilitate the development of more advanced and effective co-pilot-style MAS for SDP assistance.\nFor this evaluation, we asked four faculty members from the Engineering and Computer Science departments at different universities to provide feedback, which we used as a reference standard. We recognize that these evaluations are subjective and naturally vary among evaluators, so they are not an absolute ground truth but serve as a comparative benchmark for our system. The variability in these faculty scores is also shown in Figure 6. This method uses three primary evaluation scores to assess the effectiveness of our multi-agent system-powered copilot in guiding engineering and computer science students.\n1) The Faculty Evaluation Score reflects faculty assessments of key engineering and computing aspects within each SDP proposal, providing a baseline measure of project quality.\n2) The Multi-Agent System Score represents scores generated by individual agents themselves, each focusing on specific engineering and computing criteria aligned with their programmed expertise.\n3) The Single Agent Score represents scores generated by a single agent itself using TOT to simulate different experts to evaluate different aspects of the SDP.\nDifferent engineering and computing aspects we selected for this system are Problem Formulation, Breadth and Depth, Ambiguity and Uncertainty, System Complexity, Technical Innovation and Risk Management, Societal and Ethical Considerations, and Methodology and Approach. Together, these scores across each aspect offer a comprehensive view of both the technical quality of student work and the impact of agent-driven feedback in supporting educational outcomes whether it is a MAS or single agent. We collected six SDP proposals from students in the Engineering and Computer Science departments at X University. All students had completed their SDPs (2023-2024). To ensure anonymity, each proposal was renamed with a randomly assigned number (between 1 & 6). In Section IV, we will further discuss the evaluation results and performance of our proposed methodology."}, {"title": "2) NLP-based Evaluation", "content": "To evaluate the performance of both MAS and single-agent systems, we used four NLP-based scoring metrics: Lexical Cohesion, Average Sentence Length, Clause Density, and Flesch-Kincaid Score. These criteria provide insights into thematic consistency, readability, and structural complexity in system responses.\n1) Clause Density [26] captures sentence complexity by counting clauses per sentence, reflecting layered perspectives. Scores range from 1 (simple, single-idea sentences) to 3+ (highly complex, multi-idea sentences).\n2) Lexical Cohesion [27] measures thematic consistency by analyzing word repetition or related terms, indicating how well the content is built on multiple ideas. Scores range from 0 (no cohesion) to 1 (full thematic consistency).\n3) Flesch-Kincaid Score [28] estimates readability, indicating the U.S. grade level needed to understand the text. A higher score suggests advanced content suitable for expert readers, with an ideal range balancing accessibility and sophistication (0-16 scale) for academic purposes.\n4) Average Sentence Length indicates structural complexity and content depth, with typical ranges from 10 to 40 words. Shorter sentences enhance readability, while longer ones may reflect richer, nuanced perspectives but can be harder to follow.\nThese metrics collectively assess the depth and accessibility of each system's response. Using these scores, we can evaluate the performance of these systems from a NLP perspective."}, {"title": "IV. RESULTS", "content": "In this section, a comprehensive analysis of the results is presented, focusing on faculty evaluations, error rates, and NLP-based performance metrics. The evaluation based on faculty scores is detailed in Figure 3. Error rates for each system, reflecting their relative accuracy, are shown in Figure 4. Finally, the performance of each system, assessed using NLP-based metrics, is illustrated in Figure 5."}, {"title": "A. Faculty Guided Evaluation Results", "content": "As outlined in Section III-B, we designed three distinct evaluation scores to assess the performance of our proposed MAS for SDPs with respect to evaluation from faculty members. Scores comparison for each project across faculty evaluations, MAS scores, and single-agent scores. Green bars represent faculty scores, blue bars represent MAS scores and red bars indicate single-agent scores. This section further discusses key insights from the observed scoring patterns and analysis.\nThe detailed results in Figure 3 show the performance of each system across each aspect and SDP proposals. The graph shows that the MAS consistently matches or outperforms the single-agent system across all aspects, except for an isolated project for the Breadth and Depth aspect. This can be seen by the MAS score in each aspect being much closer to the faculty evaluation scores and following the same trend.\nFigure 4 presents the results showcasing the effectiveness of the MAS in evaluating SDPs compared to the single-agent system, with both systems benchmarked against faculty evaluation scores. The MAS demonstrates greater alignment with faculty evaluations, with a Mean Absolute Error (MAE) of 0.205 compared to 0.388 for the single-agent system, an 89.3% accuracy improvement. Lower bars in the figure represent closer alignment with faculty scores. The MAS excels in technical categories such as Technical Innovation and Risk Management (MAE 0.345 vs. 0.855) and System Complexity (MAE 0.272 vs. 0.355). However, in Breadth and Depth, the single-agent system performs better (MAE 0.208 vs. 0.292), suggesting certain holistic aspects may favor a unified approach. The MAS outperforms in Ambiguity and Uncertainty (MAE 0.440 vs. 0.857) and Societal and Ethical Considerations (MAE 0.355 vs. 0.772), demonstrating its broader effectiveness. Both systems perform equally in Methodology and Approach (MAE 0.293).\nThese results demonstrate the effectiveness of the MAS-based approach in evaluating SDPs, with superior accuracy across most assessment criteria due to its use of specialized"}, {"title": "B. NLP-based Evaluation Results", "content": "From an NLP perspective, we designed a mechanism to evaluate the responses of the MAS and single-agent systems. These metrics assess how each system performed in terms of complex ideation, structural coherence, and readability for both students and supervisors.\nAs shown in Figure 5, the clause density of responses generated by the MAS is more closely aligned with the original proposals compared to those produced by the single-agent system. This indicates that the MAS outputs are more detail-rich, conveying a greater amount of information per sentence, which enhances their effectiveness and suitability in this context. Examining the lexical cohesion graph in Figure 5, we see that MAS responses exhibit stronger thematic consistency. Unlike the original proposals written by students and the single-agent responses, MAS outputs offer more collaborative feedback reflecting the interconnectedness of ideas within the text. With different agents covering specific aspects and building upon each other's responses, MAS produces nuanced, detailed outputs that better support both students and supervisors.\nThe Flesch-Kincaid readability score, which indicates the U.S. grade level needed to comprehend the responses on a first read, is a crucial metric for evaluating academic suitability. As grade level increases, writing typically becomes more organized and detail-rich-a pattern common in academic documents. For senior-year students engaged in final-year SDP projects, an ideal readability score lies between 14 and 16. As shown in Figure 5, the MAS responses in the Flesch-Kincaid graph, predominantly fall within this ideal range, suggesting they are well-suited to the academic level of senior students. The Average Sentence Length metric in this figure indicates that sentence lengths in both MAS and SA responses are similar to those in the original proposals in most of the cases. While typical values for this metric range from 0 to 40, responses from both the MAS and the single-agent system are predominantly concentrated in the mid-range, often aligning more closely with the original proposals. This effect is primarily attributed to the training methodology of the LLMs. Given that both the MAS and single-agent systems rely on the same backend LLM, this behavior is unsurprising.\nThe results in both faculty-based and NLP-based evaluations (\u00a7IV-A and \u00a7IV-B) highlight the superior performance characteristics of the MAS compared to the single-agent system in evaluating SDPs, with each system displaying distinct strengths. MAS responses demonstrated broader coverage, not only in technical aspects, clause density, and thematic consistency but also in providing more detailed, cohesive feedback. Readability scores indicate that MAS responses align well with senior-year academic expectations, although slightly longer sentence lengths may impact accessibility. Additionally, the MAS exhibited impressive performance in addressing ethical and societal considerations. The single-agent system, on the other hand, displayed higher error rates across most metrics, except in areas"}, {"title": "V. DISCUSSIONS", "content": "The landscape of LLM agents and MAS is diverse, with various frameworks actively developed to leverage large language models for complex, collaborative tasks. MAS, a longstanding area of research since the 1980s, provides a foundation for decentralized, autonomous problem-solving across domains like artificial intelligence and robotics [10]. MAS frameworks facilitate agent interactions through mechanisms such as cooperation, coordination, and negotiation, often yielding emergent behaviors beyond the capabilities of individual agents.\nWhile powerful, LLMs alone lack the sophistication to fully function as autonomous agents, as they typically need plugins or external tools to interact meaningfully with other systems. In MAS, agents are traditionally defined as systems capable of autonomous action and interaction within an environment to achieve specific goals [10]. MAS frameworks allow agents to coordinate, negotiate, and cooperate, often resulting in emergent behaviors beyond the reach of individual agents.\nIn GenAI, however, an agent is defined as a GenAI system, usually powered by an LLM, that serves a user's goals by engaging with external systems and executing actions outside the LLM itself [29]. For example, ChatGPT's code interpreter, integrated into GPT-4, combines language processing with"}, {"title": "A. Situating our Initial Approach in the Agentic Landscape", "content": "The landscape of LLM agents and MAS is diverse, with various frameworks actively developed to leverage large language models for complex, collaborative tasks. MAS, a longstanding area of research since the 1980s, provides a foundation for decentralized, autonomous problem-solving across domains like artificial intelligence and robotics [10]. MAS frameworks facilitate agent interactions through mechanisms such as cooperation, coordination, and negotiation, often yielding emergent behaviors beyond the capabilities of individual agents.\nWhile powerful, LLMs alone lack the sophistication to fully function as autonomous agents, as they typically need plugins or external tools to interact meaningfully with other systems. In MAS, agents are traditionally defined as systems capable of autonomous action and interaction within an environment to achieve specific goals [10]. MAS frameworks allow agents to coordinate, negotiate, and cooperate, often resulting in emergent behaviors beyond the reach of individual agents.\nIn GenAI, however, an agent is defined as a GenAI system, usually powered by an LLM, that serves a user's goals by engaging with external systems and executing actions outside the LLM itself [29]. For example, ChatGPT's code interpreter, integrated into GPT-4, combines language processing with"}, {"title": "B. Pedagogical Implications", "content": "Our work impacts key educational stakeholders: students, instructors, and administrators. For students, the multi-agent LLM system provides structured guidance through a web-based co-pilot, helping them tackle complex, interdisciplinary projects while fostering critical thinking without requiring full subject expertise. Figure 7 illustrates a snapshot of the user interface for the proposed MAS co-pilot system. Given that students are often engaging with complex problem-solving for the first time, our framework offers a practical tool that helps bridge their knowledge gaps and guides them in areas like ethical considerations, technical complexity, and societal impact. However, as current LLMs still rely heavily on human input and precise prompt engineering, students who lack experience in both prompt design and advanced subject nuances may face challenges. This system addresses some of these gaps by offering structured pathways for problem-solving, thus enhancing their ability to work on large-scale projects. For instructors, the framework improves instructional efficiency by automating guidance in key project areas, allowing them to focus on high-level mentorship rather than technical troubleshooting. For administrators, the system can support accreditation efforts by providing a scalable tool that aligns with goals for multidisciplinary and real-world education.\nTo encourage further development and adaptation, we have shared the code for our SDP complex problem-solving co-pilot as an open-source resource (Link will be released after paper acceptance), allowing educators and researchers to experiment with and extend the system. By making our framework accessible, we aim to contribute to the wider adoption of advanced multi-agent LLM systems in engineering education, fostering a collaborative movement toward practical tools that support complex problem-solving in educational contexts."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we explored the use of Multi-Agent Large Language Models (MAS LLMs) as a novel framework for enhancing complex problem-solving in engineering senior design projects. By leveraging the collaborative capabilities of MAS and the generative power of LLMs, we provided students with a dynamic, interdisciplinary environment that mirrors real-world engineering challenges. This approach enabled students to engage with multiple perspectives, simulate trade-offs, and explore the complexity of decision-making in globalized engineering contexts. We evaluated our MAS LLMs and a single-agent system based on some custom-designed metrics that are more inclined toward faculty members of universities and some widely adopted NLP-based metrics. Our findings suggest that the MAS LLM framework can enhance student learning by providing details-rich responses, ideation of complex ideas, fostering collaboration, improving problem-solving skills, and allowing for real-time feedback through follow-up questions. With MAS achieving an overall accuracy of 89%. Our analysis also reveals that single-agent systems are not usually aligned with faculty-assigned scores in most of the engineering and computing aspects. This is due to the fact that the single-agent system tries to oversimplify the responses, and the response window of LLMs is much smaller for one agent as compared to multiple responses from each agent in MAS. The ability of MAS to simulate expert personas, representing diverse stakeholders and viewpoints, and responding with higher attention to details and complexity of senior design projects, offers students a deeper understanding of the ethical, technical, and social dimensions of their projects. Moreover, the framework's adaptability to various project types and disciplines makes it a promising tool to be used and evaluated by the engineering and computing education community. While our study demonstrates the potential of MAS LLMs in educational settings, future work could explore further customizations of agent behavior and extend the framework to other fields beyond engineering and computer science. Additionally, the integration of more advanced evaluation metrics and longitudinal studies could provide deeper insights into the long-term educational benefits of this approach."}]}