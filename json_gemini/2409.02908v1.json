{"title": "MASKED DIFFUSION MODELS ARE SECRETLY TIME-AGNOSTIC MASKED MODELS AND EXPLOIT INACCURATE CATEGORICAL SAMPLING", "authors": ["Kaiwen Zheng", "Yongxin Chen", "Hanzi Mao", "Ming-Yu Liu", "Jun Zhu", "Qinsheng Zhang"], "abstract": "Masked diffusion models (MDMs) have emerged as a popular research topic for generative modeling of discrete data, thanks to their superior performance over other discrete diffusion models, and are rivaling the auto-regressive models (ARMs) for language modeling tasks. The recent effort in simplifying the masked diffusion framework further leads to alignment with continuous-space diffusion models and more principled training and sampling recipes. In this paper, however, we reveal that both training and sampling of MDMs are theoretically free from the time variable, arguably the key signature of diffusion models, and are instead equivalent to masked models. The connection on the sampling aspect is drawn by our proposed first-hitting sampler (FHS). Specifically, we show that the FHS is theoretically equivalent to MDMs' original generation process while significantly alleviating the time-consuming categorical sampling and achieving a 20x speedup. In addition, our investigation challenges previous claims that MDMs can surpass ARMs in generative perplexity. We identify, for the first time, an underlying numerical issue, even with the 32-bit floating-point precision, which results in inaccurate categorical sampling. We show that the numerical issue lowers the effective temperature both theoretically and empirically, leading to unfair assessments of MDMs' generation results in the previous literature.", "sections": [{"title": "INTRODUCTION", "content": "There are three primary paradigms of generative models. Diffusion models have been the prevalent way for generative modeling of continuous data with both theoretical and empirical success. They are SOTA in image, speech, video synthesis and serve as the cornerstone of large-scale text-to-image and text-to-video generation systems. Auto-regressive models have dominated the generation of discrete data especially languages, due to the scalability and generalizability of the straightforward next-token-prediction mechanism based on transformer architectures. Masked models, such as BERT for masked language modeling and MaskGIT for masked image generation, are trained to reconstruct randomly masked tokens and sampled by order-agnostic decoding. They are an alternative approach to model discrete data while suffering from insufficient theoretical foundations.\nDiffusion models have been extended to discrete data spaces with principled training and sampling. Compared to ARMs, they predict all the tokens simultaneously and offer a favorable trade-off between generation quality and sampling efficiency. Recently, masked diffusion models (MDMs), the leading variant among discrete diffusion formulations, are emerging as a promising contender of ARMs. Recent works have simplified MDMs to align with the design space of the diffusion models via continuous-time forward processes, training objectives, and sampling procedures, resulting in a unified view and empirical improvements.\nHowever, we argue that the current understanding of MDMs is still quite limited. Positioned at the intersection of diffusion models and masked models, MDMs inherit both the theoretical principles from diffusion models and the simple mechanism from masked models. It remains to be explored how the diffusion theory supports masked models and whether well-established training and sampling recipes of diffusion models can be adapted to enhance MDMs.\nIn this paper, we dive deeper into MDMs and their connection to diffusion models and masked models. We highlight our key findings: (1) MDMs, in both training and sampling, are essentially time-agnostic masked models (or order-agnostic auto-regressive models), enjoying 20\u00d7 faster sampling and diverging from the design choices of diffusion models. This also justifies the theoretical foundation of masked models as they are equivalent and simpler formulations of the more principled MDMs. (2) We challenge previous claims that MDMs can surpass ARMs in generative perplexity by identifying a hidden but critical numerical issue that renders previous evaluations unfair. After correcting it, we find that MDMs significantly lag behind ARMs in generative perplexity.\nFor training, we prove that the continuous-time evidence lower bound (ELBO) objective of MDMS can be expressed by the number of masked tokens with an implicitly defined mixture-of-experts model. It provides a discrete ELBO for masked models and coincides with the ELBO previously derived for order-agnostic auto-regressive models.\nFor sampling, by analytically sampling the time when any mask token is first unmasked, we propose a theoretically equivalent first-hitting sampler (FHS) to avoid most of the time-consuming categorical sampling and perform decoding token by token with no approximation errors. It is further extended to enable parallel decoding and incorporate high-order approximations, achieving a 20\u00d7 speedup compared to previous MDM sampling procedures. When the parameterized model is independent of the time variable, we recover the sampling of masked models.\nFor evaluation, we discover that while MDMs exhibit extremely low generative perplexity with numerous sampling steps, the generation quality is compromised by reduced token diversity. By examining the numerical precision during sampling, we identify a previously unrecognized issue with Gumbel-based categorical sampling. Specifically, reducing the floating-point precision from 64-bit to 32-bit significantly truncates the Gumbel variables, which theoretically lowers the temperature and empirically improves the generative perplexity of pretrained models from 126.11 to 31.24, but with a decreased sentence entropy from 5.66 to 5.17."}, {"title": "BACKGROUND: MASKED DIFFUSION MODELS (MDMS)", "content": "Let X = {0,1,..., m \u2013 1} be the discrete data space, with an extra mask token m added to X. Denote \u2206m = {\u03c0\u2208Rm+1| \u03a3\u03bf\u03c0\u03af = 1, \u03c0 > 0} as the standard m-simplex. For any data token or mask token x \u2208 X, denote ex \u2208 Rm+1 as the corresponding one-hot vector. Continuous-time discrete-space masked diffusion models (MDMs) can be defined akin to diffusion models, with a continuous-time forward noising process\n$$q_{t|0}(x_t|x_0) = Cat(\\alpha_t e_{x_0} + (1 - \\alpha_t)e_m)$$\nwhere \u03b1t is the predefined noise schedule function satisfying \u03b10 \u2248 1,\u03b11 \u2248 0, and Cat(\u03c0) denotes the categorical distribution over the class probabilities \u03c0\u2208 \u0394m. The forward process has a time"}, {"title": "REVISITING THE TRAINING OF MDMS", "content": "MDMs are defined and trained by the continuous-time forward process , time-dependent network parameterization and continuous-time ELBO . However, different from continuous-time diffusion models , the evolution of xt is discrete. In the forward process, any token remains the same until it is masked at some time, and then the masked token no longer changes. The evolution trajectories of (xt, t) are like pairs of \u201cphenotype\u201d and \u201cgenotype\u201d, where the continuous changes in time t may not be reflected on the observable traits of xt.\nIn this section, we aim to disentangle the internal time variable t and the external traits of the masked sequence xt at time t in the training of MDMs."}, {"title": "REFORMULATING THE ELBO WITH THE NUMBER OF MASKED TOKENS", "content": "Previous works show the invariance of the ELBO to the noise schedule \u03b1t by performing the time change-of-variable y = log(1 \u2212 \u03b1t) or = log . However, this does not get to the essence as they still rely on an internal continuous time. In the following proposition, we show that the sequence NELBO of MDMs can be expressed as a partition by the number of masked tokens instead of the continuous time.\nProposition 3.1 (ELBO by the Number of Masked Tokens). For xo with sequence length L, denote xn as a sequence with n masked tokens, and q(xn|xo) as the discrete forward process which randomly and uniformly masks n tokens of xo. Suppose the noise schedule \u03b1t satisfies \u03b10 = 1, \u03b11 = 0. The sequence NELBO in Eqn. (6) can be reformulated as\n$$L^{(L)}_{\\infty} =  \\sum_{n=1}^{L} E_{\\tilde{q}_{n|0}(x_n|x_0)} \\Big[ \\sum_{l: x_n^{(l)}=m} -\\mathbb{E}_{x_0^{(l)}|x_n^{(l)}} [e_{x_0^{(l)}} \\log \\mu_{\\theta} (x_n, t) ] \\Big]$$\nwhere\n$$\\log \\mu_{\\theta}(x_n) = E_{a_n \\sim B(L-n+1,n)} [\\log \\mu_{\\theta} (x_n, \\alpha^{-1}(a_n))],$$\na-1 is the inverse function of \u03b1t satisfying \u03b1\u22121(\u03b1t) = t, and B(a,b) denotes the Beta distribution with shape parameters a, b > 0.\nThis expression offers two aspects of theoretical insights:\nMixture of Experts From Eqn. (8), the time-dependent network \u03bc\u03b8(x, t) implicitly parameterizes a time-independent network \u03bc(x) by aggregating the logarithm at the same x but different t, which can be seen as mixture of experts. The time t is sampled un-evenly so that \u03b1t follows a Beta distribution B(L \u2212 n + 1, n). This distribution has the mode (peak) = and variance . With a large sequence length L, the variance is small and the distribution is concentrated around the mode. Moreover, under the best-performing linear schedule \u03b1t = 1 \u2212 tin MDMs , we have t = 1 \u2212 \u03b1t and the mode of t is = , close to the masked ratio \u03b3. Therefore, the time variable t can be seen as a continuous relaxation and smoothing of the masked ratio.\nDiscrete ELBO From Eqn. (7), the sequence NELBO can be expressed discretely with the time-agnostic network p\u03b8(x). Therefore, Eqn. (7) can serve as a NELBO of masked models in a straightforward approach: uniformly choose the number of masked tokens n from {1, ..., L}, uniformly mask n random tokens in xo to obtain xn, and compute the average cross-entropy loss of p\u03b8(x) on these n positions. The weighting in this NELBO resembles the likelihood weighting in diffusion models, facilitating maximum likelihood training of masked models. Note that early works on order-agnostic auto-regressive models already reveal this weighting from a different perspective. While in the context of masked models, there are few discussions on the ELBO. Discussions on related work are placed in Appendix B."}, {"title": "TIME-INDEPENDENT NETWORK PARAMETERIZATION", "content": "When the original network \u03bc\u03b8 is parameterized without the time input, we have \u03bc\u03b8 = \u03bc\u03b8 in Eqn (7). In this case, the training of MDMs is completely free from the time variable and behaves like masked models. The rationality of time-independent network parameterization has been discussed in recent works. Here we restate this conclusion with our simplified notations from the perspective of the optimal model.\nProposition 3.2 (Optimal Masked Diffusion Model). Given unlimited model capacity, the optimal network \u03b8* that minimizes the NELBO in Eqn. (6) satisfies\n$$\\mu_{\\theta^*}^{(l)} (x,t) = E_{\\tilde{q}_{0|N(x_n)} (x_0|x_n)} [e_{x_0^{(l)}}]$$"}, {"title": "PRACTICAL CONSIDERATIONS", "content": "While there are theoretically equivalent variants for training MDMs (continuous-time/discrete ELBO, time-conditioned/time-independent network), these choices may have practical implications due to differences in network inputs and loss variances. We present some training comparisons and our attempts to improve training (e.g., variance reduction, flow matching) in Appendix I.1. Overall, all options yield similar performance, and the low-discrepancy sampler , when applied to time or the number of masked tokens, can significantly reduce the loss variance.\nNote that while several works suggest that MDMs are competitive with ARMs in language modeling (beating GPT-2 when measured by test/zero-shot perplexity), a more fair comparison (retraining ARMs with the same configurations, Appendix I.1) indicates that MDMs are only advantageous in language understanding tasks (surpassing ARMs and BERT on the GLUE metric)."}, {"title": "REVISITING THE SAMPLING OF MDMS", "content": "In the previous section, we demonstrate how the training of MDMs, from both theoretical and empirical perspectives, can be disentangled with the continuous time variable and behave like masked models. In this section, we turn our attention to the sampling of MDMs, which is also performed in continuous time and seems distinct from masked models. We aim to address its current inefficiency problem as well as establish essential insights into its connection with masked models."}, {"title": "INEFFICIENCY OF CURRENT SAMPLING", "content": "MDMs are sampled in an ancestral way following the parameterized reverse-time process in Eqn. (3). Specifically, the sampling step xt \u2192 xs, from time t to s < t can be expressed as\n$$x_s^{(l)} \\sim Cat\\begin{cases} x_t^{(l)}, & x_t^{(l)} \\neq m \\\\ \\frac{(1-\\alpha_s)e_m + (\\alpha_s-\\alpha_t)\\mu_{\\theta}(x_t,t)}{1-\\alpha_t}, & x_t^{(l)} = m \\end{cases} \\text{ for every } l$$\nGiven the number of sampling steps N, the sampling process involves first discretizing the timesteps as 0 \u2264 t1 < ... < tN = 1, and then performing reverse steps tN \u2192 tN\u22121 \u2192 ... \u2192 t0 according to Eqn. (10). Notable characteristics of MDM's sampling include: (1) Any mask token can only be unmasked once with no further changes. (2) Each sampling step requires a forward pass through the network \u03bc\u03b8 and conducting at most L times of |X|-dimensional categorical sampling, where L is the sequence length and X is the vocabulary size. (3) The number of sampling steps N can be significantly larger than L, and a single sampling step may result in no changes to any token in the sequence. (4) As MDMs are trained with the continuous-time ELBO which assumes an infinite number of reverse steps, sampling with finite N theoretically introduces discretization errors. Sampling with a larger N more faithfully reflects the true generation results of MDMs.\nRecent works propose a simple caching strategy to speedup the sampling of MDMs: when the network \u03bc\u03b8 is parameterized without time input, and the sequence"}, {"title": "FIRST-HITTING SAMPLER", "content": "The current sampling methods of MDMs, including the caching strategy, are neither efficient nor insightful into the essence of MDMs. To address this, we reexamine the sampling step in Eqn. (10).\nWhen the number of sampling steps N \u2192 \u221e and the maximum step size max1<i<N |ti \u2212 ti\u22121|\u2192 0, Eqn. (10) tends to an infinitesimal jump. In this case, the reverse sampling process becomes a continuous-time Markov chain (or Markov process), where each mask token is unmasked at some moment according to the network prediction. Our key insight involves three folds: (1) Whether a mask token will transit or not during a time interval [s, t] is independent"}, {"title": "PARALLEL DECODING AND HIGH-ORDER VARIANTS", "content": "The token-by-token decoding process of MDMs can be extended to parallel decoding by unmasking multiple tokens per step, as the network \u03bc\u03b8 predicts tokens at all positions. This enables speed-quality trade-offs similar to diffusion models. As illustrated in Figure 5, parallel decoding essentially reuses the previous network output to reduce the NFE, thus functioning as an approximation method.\nTo reduce the approximation error, we follow the recipes of high-order diffusion solvers to develop high-order samplers of MDMs. We propose two variants: one based on extrapolating previous network outputs,"}, {"title": "ARE MDMS BETTER THAN ARMS? A CRITICAL FAULT IN LOW-PRECISION GUMBEL-BASED CATEGORICAL SAMPLING", "content": "Before we proceed to verify the effectiveness of our proposed first-hitting sampler, we have to point out a critical fault in MDMs' original sampling. As suggested by previous works , MDMs seem to surpass ARMs with a sufficient number of sampling steps when measured by the generative perplexity (Gen PPL), as shown in Figure 7a. However, in this section, we identify for the first time a hidden implementation issue existing in previous codebases that leads to this false conclusion."}, {"title": "LOW TOKEN DIVERSITY UNDER NUMEROUS SAMPLING STEPS", "content": "Theoretically, the performance of MDMs is more faithfully reflected with an increased number of sampling steps. Empirically, we also observe a reduction in generative perplexity with more sampling steps. Notably, an exceptionally low generative perplexity (< 15) is achieved when the number of sampling steps approaches 50k.\nHowever, when we check the generated content, we discover that the quality is compromised by low token diversity (an extreme case is shown in Figure 6). We further quantify this phenomenon by measuring the sen-"}, {"title": "IDENTIFYING THE NUMERICAL PRECISION PROBLEM", "content": "This low generation quality is unexpected, as theory suggests that approximation error should decrease with more sampling steps. Consequently, samples generated over numerous steps should reflect the true performance of MDMs. We therefore consider this an implementation issue and investigate further to identify the root cause."}, {"title": "CATEGORICAL SAMPLING WITH TRUNCATED GUMBEL", "content": "In the previous section, we empirically observe that truncated Gumbel-based categorical sampling reduces token diversity. Surprisingly, such effects can be precisely depicted in closed-form.\nProposition 5.2 (Closed-Form Categorical Sampling with Truncated Gumbel). Suppose the class probabilities are sorted as \u03c01 \u2264 \u22ef \u2264 \u03c0K, and gi \u223c TG(0,1, M) are truncated Gumbel samples with maximum value M. Denote \u03c00 = 0. For 1 \u2264 n \u2264 K, we have P(argmaxi(log \u03c0i + gi) = n) = \u03c0n \u03a3i=1 \u03b2(i), where\n$$\\beta(i) = \\frac{e^{-\\frac{K+1-i-\\sum_{k=i}^K \\pi_k}{\\pi_i}} - e^{-\\frac{K+1-i-\\sum_{k=i}^K \\pi_k}{\\pi_{i-1}}}e^{-M}}{\\sum_{k=i}^{K} \\pi_k}$$\nTo the best of our knowledge, this formulation has not been revealed in previous works. Intuitively, with truncated Gumbel, the original class probabilities \u03c0n are shifted to \u03c0'n = \u03c0n \u03a3i=1 \u03b2(i). This has two main implications: (1) As \u03b2(i) > 0 and \u03c0n are sorted, if \u03c0n1 > \u03c0n2, then the adjusted class probabilities satisfy \u03c0'n1 > \u03c0'n2. This indicates that relatively larger probabilities are further amplified, creating an effect similar to lowering the temperature, which aligns with our observation of reduced diversity. (2) In the sampling step, the probability of unmasking is adjusted based on the network output, resulting in unequal unmasking probabilities at different positions in a sequence."}, {"title": "A FAIR EVALUATION OF MDMS' GENERATION", "content": "In this section, we will fairly evaluate the generation performance of MDMs and examine the impact of our proposed sampler and the temperature. Our experiments are based on the codebase of MDLM which is inherited from SEDD. We fix the categorical sampling to 64-bit floating-point precision so that the numerical truncation is negligible. We directly use pretrained models (AR, SEDD Absorb, MDLM) provided by MDLM, which share the same network architecture and were trained with the same configuration. Additional experiment details are provided in Appendix H."}, {"title": "ORIGINAL SAMPLER V.S. FIRST-HITTING SAMPLER", "content": "Figure 9 compares both the generative perplexity and the entropy of different models. For the baselines, SEDD is sampled by their analytic sampler (Tweedie T-leaping), and MDLM is sampled with and without the caching strategy. For our first-hitting sampler, the parallel decoding is performed by unmasking the same number of tokens per step. High-order variants employ the extrapolation strategy when the number of sampling steps N \u2264 128, and the predictor-corrector strategy otherwise.\nAfter the numerical problem is fixed, the entropy returns to a normal level (5.60~5.70) for all models. Besides, our sampler can be up to 20x faster than previous sampling strategies of MDMs in terms of the wall-clock time. Despite the notable speedup, the true generative perplexity of MDMS is revealed to be around 100, significantly lagging behind that of counterpart ARMs (< 40)."}, {"title": "TRADING OFF GENERATIVE PERPLEXITY AND ENTROPY VIA TEMPERATURE", "content": "The truncation effect of 32-bit floating-point numbers creates a trade-off between generative perplexity and entropy by varying the number of sampling steps (Figure 7). This trade-off arises from a tricky interplay of inaccurate categorical sampling and the approximation error at limited discretization steps. In Figure 10, we demonstrate that this trade-off can be achieved at a lower time cost by using the correct sampling (our 1024-step high-order sampler) and manually adjusting the temperature within the range [0.8, 1.0]. The trade-off curve of our method is slightly better than the original MDM sampling, while still significantly lagging behind ARMs."}, {"title": "CONCLUSION", "content": "In this work, we advance our understanding of masked diffusion models (MDMs) by revealing their theoretical equivalence to masked models and addressing a critical implementation issue that com-"}]}