{"title": "YOLO-RD: INTRODUCING RELEVANT AND COMPACT\nEXPLICIT KNOWLEDGE TO YOLO BY RETRIEVER-\nDICTIONARY", "authors": ["Hao-Tang Tsui", "Chien-Yao Wang", "Hong-Yuan Mark Liao"], "abstract": "Identifying and localizing objects within images is a fundamental challenge, and\nnumerous efforts have been made to enhance model accuracy by experimenting\nwith diverse architectures and refining training strategies. Nevertheless, a preva-\nlent limitation in existing models is overemphasizing the current input while ignor-\ning the information from the entire dataset. We introduce an innovative Retriever-\nDictionary (RD) module to address this issue. This architecture enables YOLO-\nbased models to efficiently retrieve features from a Dictionary that contains the\ninsight of the dataset, which is built by the knowledge from Visual Models (VM),\nLarge Language Models (LLM), or Visual Language Models (VLM). The flexible\nRD enables the model to incorporate such explicit knowledge that enhances the\nability to benefit multiple tasks, specifically, segmentation, detection, and clas-\nsification, from pixel to image level. The experiments show that using the RD\nsignificantly improves model performance, achieving more than a 3% increase\nin mean Average Precision for object detection with less than a 1% increase in\nmodel parameters. Beyond 1-stage object detection models, the RD module im-\nproves the effectiveness of 2-stage models and DETR-based architectures, such as\nFaster R-CNN and Deformable DETR.", "sections": [{"title": "INTRODUCTION", "content": "In the field of computer vision, object detection models play a pivotal role, these models are de-\nsigned to precisely locate objects within images. They are used in applications such as medical\nimage analysis and autonomous driving. Additionally, they can also be used as backbone models\nfor downstream tasks like multi-object tracking (Zhang et al., 2022; Cao et al., 2023; Aharon et al.,\n2022), and crowd counting (Zhang et al., 2016; Song et al., 2021). As the basis for these extended\ntasks, object detection models must combine high accuracy with low latency to allow downstream\ntasks to stand on the shoulders of giants.\nAmong object detection models, the YOLO (Redmon et al., 2016), FasterRCNN (Ren et al., 2015),\nand DETR (Carion et al., 2020) are notably prevalent. The YOLO series primarily utilizes Con-\nvolutional Neural Networks (CNN) (LeCun et al., 1998), providing a balance between inference\nspeed and accuracy. From YOLOv1 through YOLOv10 (Redmon et al., 2016; Redmon & Farhadi,\n2017; 2018; Bochkovskiy et al., 2020; Jocher, 2020; Li et al., 2022; Wang et al., 2023a; Jocher\net al., 2023; Wang et al., 2024b;a), there has been a consistent focus on refining the architecture\nand training methods to reduce the model's parameters while enhancing accuracy. Beyond the main\nYOLO series, variants such as YOLOR (Wang et al., 2023b) incorporate implicit knowledge and\nother techniques to further boost model performance.\nAs shown in Figure 1, while both CNNs and Transformers (Vaswani et al., 2017) concentrate on the\ninput image, CNNs are restricted to local input data, and Transformers, despite considering interac-\ntions among various inputs, are still confined to the given inputs or other model branches. However,\nthe above-mentioned models often overlook a crucial aspect of explicit knowledge-the comprehen-\nsive dataset information. On the other hand, some contrastive methods, e.g. SimCLR (Chen et al.,\n2020), DINO (Caron et al., 2021) have demonstrated that cross-referencing data is beneficial."}, {"title": "RELATED WORK", "content": "Real-time object detection. Real-time object detection is a foundational problem in computer vi-\nsion, with a focus on achieving low latency and high accuracy. Traditional approaches have primarily\nfocused on CNN architectures, with seminal works such as the OverFeat (Sermanet et al., 2014) and\nFaster R-CNN(Girshick, 2015; Ren et al., 2015). Since the introduction of Vision Transformers\n(ViT) (Dosovitskiy et al., 2021), there have been notable follow-up works, including DETR (Carion\net al., 2020) and RT-DETR (Zhao et al., 2023). However, the CNN-based YOLO series models\nhold a crucial position in the field of real-time detection due to their ease of training from scratch,\nlightweight design, and ability to perform high-speed inference.\nEach version of the YOLO model introduces different architectures and training strategies. For\nexample, YOLOv7 (Wang et al., 2023a) employs ELAN and trainable bag-of-freebies techniques to\nenhance performance, while YOLOv9 (Wang et al., 2024b) incorporates Generalized Efficient Layer\nAggregation Networks (G-ELAN) and Programmable Gradient Information for improved efficiency\nand learning capability. YOLOv10 (Wang et al., 2024a) further introduces the compact inverted\nblock to optimize model size and computation. On the theoretical front, works like YOLOR (Wang\net al., 2023b) leverage the shared characteristics across multiple computer vision tasks, allowing\nthe model to learn implicit knowledge and relax the prediction head, thus generalizing the YOLO\narchitecture to various tasks. Despite these architectural and strategic differences, all YOLO models\nshare a conceptual framework comprising three core modules: a Backbone for downsampling, a\nNeck (e.g. FPN) for feature fusion, and a Detection Head for final prediction. In this paper, we also\nutilize the Backbone as an image encoder.\nDictionary learning. Dictionary learning is a fundamental technique in signal processing and\nmachine learning, aimed at learning a set of basis functions (or atoms) that can efficiently represent\nsignals. This technique has been extensively explored in the context of sparse coding, where signals\nare approximated as sparse linear combinations of dictionary atoms. Additionally, it was discovered\nthat natural images can be effectively represented using sparse coding models (Olshausen & Field,\n1997), laying the foundation for further development of dictionary learning algorithms.\nSeveral algorithms have been proposed to optimize dictionaries and sparse coefficients. Among\nthese, K-SVD (Aharon et al., 2006) has become a standard due to its effectiveness in applications\nsuch as image denoising, compression, and inpainting. With the rise of CNNs, dictionary learning\nhas also seen new developments, such as designing convolutional blocks and defining loss func-\ntions to achieve dictionary learning objectives (Garcia-Cardona & Wohlberg, 2018; Zheng et al.,\n2021). Additionally, dictionary learning has been applied to tasks like content-based image retrieval\n(CBIR), as seen in works like \u015eaban \u00d6zt\u00fcrk (2021); Tarawneh et al. (2019). In this paper, we focus\non dictionary learning rather than sparse dictionary learning and emphasize dictionaries that can\nrobustly represent information and retain critical, relevant signals.\nRetrieval-augmented generation (RAG) Retrieval-Augmented Generation (RAG) (Lewis et al.,\n2020) is a technique that first appeared in large language models. It primarily involves three steps:\nIndexing, where the database is split into chunks, encoded into vectors, and stored in a vector\ndatabase; Retrieval, which retrieves relevant information based on similarity to the input; and Gen-\neration, where both the original input and the retrieved information are fed into the model for further\nprocessing. This approach enables LLM to handle unseen information and has been successfully\napplied in T5 (Raffel et al., 2020), or certain versions of ChatGPT(Achiam et al., 2023).\nDue to the time-consuming nature of the retrieval process, and the real-time requirements of most\ncomputer vision tasks, RAG has seen limited application in vision-based models. Recently, some\nwork has been done to mitigate the bottleneck in Wu & Xie (2024); Kim et al. (2024); Liu et al.\n(2023). For example, RALF (Kim et al., 2024) leverages LLM to embed a huge vocabulary set and\nfind similar meaning words to refine features; REACT (Liu et al., 2023) utilizes the World Wide Web\nas the information source to extend model knowledge. However, they still require an undetachable\nhuge dataset or Language model which leads to extremely high training and inference loading. In\nthis work, we provide the vision-based model with not only a light-weight database but also refinable\nfeatures."}, {"title": "METHOD", "content": "In this work, we introduced the Retriever-Dictionary module, as shown in Figure 2, which enables\ncomputer vision models to utilize comprehensive dataset knowledge with minimal extra parameters\nquickly. This plug-in stores encoded information from various models, enhancing the model's ability\nto identify which features of the input data should be emphasized or diminished, thereby improving\noverall performance. The RD-module is composed of two main components: the Retriever and\nthe Dictionary. The Dictionary consists of N elements, each represented as Rf vectors, known as\natoms a. The Retriever generates the coefficient of a for each pixel according to the input. The\nDictionary can be generated using the YOLO backbone or initialized with diverse models like the\nvisual language model CLIP (Radford et al., 2021) and large language models such as GPT (Radford\net al., 2018). This approach allows the model to align visual and linguistic representations, leading\nto a more balanced and valuable distribution of atoms. Furthermore, the incorporation of linguistic\nknowledge helps the module retain crucial information. The main goal is to adjust the distribution\nof the Dictionary and allow the Retriever core to find the best Atoms' weight for each input pixel."}, {"title": "MODULE STRUCTURE", "content": "The Retriever core aims to efficiently generate the coefficients of each a in the Dictionary. In-\nspired by depthwise convolution Chollet (2017), we separate the Retriever into two components:\nthe Coefficient Generator G and the Global Information Exchanger E. The Coefficient Generator,\ndenoted as $G : R^{f\\times W\\times H} \\rightarrow R^{N\\times W\\times H}$, computes coarse coefficients based on the input feature\nmap $X \\in R^{f\\times W\\times H}$, where f is the input feature dimension and W, H represent the width and\nheight of the feature map, respectively. The coarse coefficients are calculated as follows:\n$Y = G(X) = W_{G} \\cdot X_{w,h}$,\nwhere $W_{G} \\in R^{N\\times f}$ is the projection matrix of G, and $X_{w,h} \\in R^{f}$ is the feature vector at spatial\nlocation (w, h).\nThe Global Information Exchanger, denoted as $E : R^{N\\times W\\times H} \\rightarrow R^{N\\times W\\times H}$, refines and exchanges\ninformation across neighboring pixels, and is defined as:\n$E(Y) = W_{E}^{(i)} * Y^{(i)}$,\nwhere $W_{E} \\in R^{N\\times 1\\times k\\times k}$ is the depthwise convolution filter with kernel size k \u00d7 k, and i \u2208 [0, N)\nindexes the channels of Y. The term $Y^{(i)} \\in R^{W\\times H}$ refers to the spatial feature map of the i-th\nchannel.\nThe combined operation of G and E generates the final coefficients vector c for each pixel:\n$c = E(G(X_{w,h}))$."}, {"title": "Dictionary INITIALIZATION", "content": "By pre-initializing the Dictionary, we embed knowledge into the module atoms of the Dictionary.\nMore precisely, we use the selected encoder to map the entire dataset into a high-dimensional\nspace, by incorporating insights from various modality models, as illustrated in Figure 3. This\nhigh-dimensional space shares the same dimension as the original YOLO backbone middle layer.\nObviously, this will have a huge number of vectors and present multiple groups in high-dimensional\nspace, we employ k-means (Macqueen, 1967) to leave representative vectors, which ultimately serve\nas Dictionary atoms. Through these operations, we can map the dataset pairs into a high-dimensional\nspace in a short time (this operation is equivalent to using the model to traverse the entire dataset\nat the speed of inference), and find vectors that can represent most of the dataset. For features that\nare not in the Dictionary, we can use the feature through the residual mechanism in the module, and\nutilize the Retriever Dictionary to bring closer atoms of the same category to the outlier feature.\nNext, we discuss using different modality models as encoders for encoding knowledge:"}, {"title": "Dictionary COMPRESSION", "content": "The objective of designing Retriever Dictionary (RD) is to retain the most critical information from\nthe dataset. However, even after training, the number of atoms in the Dictionary may exceed the\nrequired amount, with some atoms being infrequently used or irrelevant to the specific dataset do-\nmain. Drawing inspiration from knowledge distillation and the Teacher-Student model (Hinton et al.,\n2015), we condense the original Dictionary D into a smaller and more efficient version, denoted as\nd. To align the output features of d with those of D, we employ contrastive learning (Chen et al.,\n2020). In this process, the model backbone and RD are frozen, while the optimization is focused on\nthe smaller Retriever Dictionary module d. The objective function is defined as:\n$L_{i,w,h} = log \\frac{exp(sim(z_{w,h}^{rd}, z_{w',h'}^{RD})/T)}{\\sum_{i'=1}^{BWH} exp(sim(z_{w,h}^{rd}, z_{w',h'}^{RD})/T)}$,\nwhere $z_{w,h}^{rd} \\in R^{f}$ represents the i-th batch output feature of d at position (w, h), B is the batch\nsize, T is the temperature parameter, and sim(\u00b7, \u00b7) denotes the cosine similarity between two vectors."}, {"title": "EXPERIMENT", "content": "Experimental setup. We primarily validated the method on the Microsoft COCO dataset (Lin\net al., 2014), training on the COCO 2017 train set and evaluating on the COCO 2017 validation\nset. For Object Detection and Segmentation, we respectively used mAP and mAP@.5 as evaluation\nmetrics, testing on YOLOv7, YOLOv9, Faster RCNN, and Deformable DETR. For the Classifica-\ntion task, we used the CIFAR-100 dataset with the YOLOv9-classify model, using top-1 and top-5\naccuracy as metrics.\nImplementation details. All experiments were conducted using 8 Nvidia V100 GPUs. In the\nmain series of experiments, we trained a modified YOLOv7 model, which included the addition of a\nRetriever-Dictionary Module, for 300 epochs over 3 days. The YOLOv9-based model was trained\nfor 5 days with 500 epochs. We also trained a modified Faster RCNN, based on the mm-detection\nframework, for 120 epochs over 3 days. For Deformable DETR, we trained for approximately 120\nepochs over 7 days. The classification task on CIFAR-100 (Krizhevsky et al., 2009) took 2 hours on\na single Nvidia 4090 GPU for 100 epochs."}, {"title": "COMPARISION WITH RD", "content": "Apply to state-of-the-art detectors. As demonstrated in Table 1, we evaluate the proposed mod-\nule mainly on YOLOv7 and also integrate it with both fundamental and SOTA real-time object\ndetection models. The Dictionary is initialized separately by three distinct models: Vision Model\n(VM) with the YOLOv7 backbone, Vision-Language Model (VLM) employing CLIP, and Large\nLanguage Model (LLM) based on GPTv2. Among these, CLIP provides the most significant im-\nprovement, likely due to its well-balanced performance across both the vision and language domains."}, {"title": "ABLATION STUDIES", "content": "Fuse coefficient generator and global information exchanger. In Section 3.1, we discussed\nsplitting the Retriever core into pointwise convolution G and depthwise convolution E can sig-"}, {"title": "VISUALIZATION", "content": "In this section, we employ visualization to demonstrate the impact of the RD-module on object\ndetection models and to visualize the Retriever's selection of atoms from the Dictionary.\nVisualization of Dictionary atom coefficients. To gain a deeper understanding of the behavior of\nthe Retriever and Dictionary core, we visualized the atom coefficients and their distribution during\nthe forward pass. In Figure 4b, the X-axis represents the correlation with the current input and each\na, while the Y-axis displays the corresponding coefficient for each atom. The surrounding points\nretrieved from the dataset, relevant to the current input, are marked as \u00d7 on the plot.\nUsing the bounding box area in Figure 4a as the input, we generate a correlation-coefficient map. As\ndepicted in Figure 4b, most high-correlation and high-coefficient pairs (Figures 4c,4d) correspond\nto traffic signs, while the low-correlation and low-coefficient pairs (Figures 4e,4f) do not. This"}, {"title": "CONCLUSION", "content": "The Retriever Dictionary module offers a lightweight and efficient approach to incorporating dataset\nknowledge into YOLO through various modality models. By leveraging pre-stored explicit knowl-\nedge within the Dictionary, the Retriever effectively retrieves relevant information while the Dictio-\nnary learning mechanism enables fine-tuning of the atoms. This module demonstrates its versatility,\nproviding improvements not only in YOLO-based tasks but also across a range of foundational\nobject detection models and broader computer vision tasks. We believe that this work lays the foun-\ndation for further exploration of real-time computer vision models that integrate explicit or external\nknowledge sources."}, {"title": "APPENDIX", "content": "REPRODUCIBILITY AND TRAINING SETUP\nIn Table 5, we observed that some of the mAP values for the original YOLOv7 and YOLOv9 models\ndiffer from those reported in the original papers. As noted in the corresponding GitHub issue, this\ndiscrepancy may be attributed to differences in the number of GPUs used, as well as the reduced\neffectiveness of batch normalization when training with smaller batch sizes. To ensure a more con-\nsistent and fair comparison, we followed the official code instructions and re-trained the models on\nour hardware setup, using the same batch sizes and number of GPUs specified in the original papers.\nDespite adhering to the official training guidelines, our re-run results for YOLOv7 and YOLOv9\nproduced slightly lower mAP values than those reported in the original papers. Nevertheless, our\nimplementation of the Retriever Dictionary (RD) method is still higher than those reported in the\noriginal papers, demonstrating the effectiveness of our approach.\nFor the Deformable DETR and Faster R-CNN models, we used the MMDetection training code. In\nthis case, our results were consistent with the values reported in the original papers, likely due to the\nfact that MMDetection provides a standardized training batch configuration, minimizing the impact\nof hardware differences on model performance."}, {"title": "ARCHITECTURES OF THE MODEL WITH Retriever Dictionary", "content": null}, {"title": "ADDITIONAL EXPERIMENT RESULTS", "content": "Detailed mAP Results for All Experiments. Table 6 provides the detailed mAP values for each\nexperiment discussed in the experiment section. It includes metrics such as AP, AP.5, AP.75, APS,\nsmall object; APM, middle object; and APL, large object."}, {"title": "TRANSFER LEARNING WITH Retriever Dictionary ON VOC DATASET", "content": "In Table 9, we demonstrate the effectiveness of the Retriever Dictionary (RD) on a transfer learning\ntask. Using pre-trained weights from the MSCOCO dataset, we trained the model on the VOC (Ev-\neringham et al., 2010) dataset with three learning rate schedules: 10-epoch fast training, 100-epoch\nfull-tuning, and training from scratch. In the fast training scenario, the model with the RD mod-\nule showed significantly faster convergence compared to the model without the module. In the\nfull-tuning scenario, the RD-enhanced model achieved higher performance. Lastly, in the training\nfrom scratch scenario, our RD module provided the model with better information, yielding superior\nresults even on a smaller dataset."}, {"title": "FURTHER VISUALIZATIONS: ORIGINAL VS. RD-MODULE MODELS", "content": "We present additional examples in Figures 11 and 12, illustrating input images, the outputs from the\noriginal model, and the outputs from the model with the RD module. The results demonstrate that\nthe RD-Model outputs are noticeably clearer. For example, in Figure 11 (ID 1 and 2), the edges of\nobjects are significantly sharpened. Similarly, in Figures 11 and 12 (ID 3, 4, 5, and 6), our model\nexhibits higher accuracy and fewer false positives in the object's bounding boxes, as indicated by\nthe red arrows."}, {"title": "PSEUDO CODE OF FULL TRAINING PROCESS OF Retriever-Dictionary MODEL", "content": "The complete training process, from initialization to final model, follows the pseudo-code pro-\nvided in Algorithm 1. This process includes Dictionary initialization, regular model training, and\nDictionary compression. The overall training time is approximately equivalent to the original train-\ning epochs, with an additional 2 epochs allocated for setup and compression."}, {"title": "DEEPER DISCUSSION OF THE Retriever CORE", "content": "Two Convolutions Without Activation Functions. Consider two consecutive 1 \u00d7 1 convolutional\nlayers without activation functions. The first layer has weights $W\u00b9 \\in R^{M\\times N}$, and the second has\nweights $W\u00b2 \\in R^{N\\times M}$, with an input $X \\in R^{N\\times H\\times W}$.\nFor the first convolutional layer, the output at a spatial location (h, w) is defined as:"}]}