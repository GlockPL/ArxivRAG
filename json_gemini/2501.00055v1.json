{"title": "LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models", "authors": ["Miao Yu", "Junfeng Fang", "Yingjie Zhou", "Xing Fan", "Kun Wang", "Shirui Pan", "Qingsong Wen"], "abstract": "While safety-aligned large language models (LLMs) are increasingly used as the cornerstone for powerful systems such as multi-agent frameworks to solve complex real-world problems, they still suffer from potential adversarial queries, such as jailbreak attacks, which attempt to induce harmful content. Researching attack methods allows us to better understand the limitations of LLM and make trade-offs between helpfulness and safety. However, existing jailbreak attacks are primarily based on opaque optimization techniques (e.g. token-level gradient descent) and heuristic search methods like LLM refinement, which fall short in terms of transparency, transferability, and computational cost. In light of these limitations, we draw inspiration from the evolution and infection processes of biological viruses and propose LLM-Virus, a jailbreak attack method based on evolutionary algorithm, termed evolutionary jailbreak. LLM-Virus treats jailbreak attacks as both an evolutionary and transfer learning problem, utilizing LLMs as heuristic evolutionary operators to ensure high attack efficiency, transferability, and low time cost. Our experimental results on multiple safety benchmarks show that LLM-Virus achieves competitive or even superior performance compared to existing attack methods.", "sections": [{"title": "I. INTRODUCTION", "content": "As LLMs emerge with exceptional and advanced capabilities such as knowledge [1], planning [2] and reasoning [3], they are exponentially being applied to systems (e.g. LLM-integrated applications [4] and LLM-based multi-agent systems [5]) across various domains and scenarios to solve certain complex problems. In this context, preventing the misuse of these powerful and influential LLM-based systems becomes increasingly critical [6]. This research area, known as \"LLM Safety\u201d, primarily focuses on preventing LLMs from being used for malicious behaviors, such as the spread of misinformation and bias, the generation of harmful content, and privacy breaches [7], [8]. Directly issuing malicious queries is typically rejected, as most available LLMs (e.g. GPT and Claude) are safety-aligned via techniques like fine-tuning to ensure adherence of responses to secure human values [9], [10]. Unfortunately, a variety of jailbreak attack methods still exist that can bypass the built-in safety mechanisms [11]."}, {"title": "II. RELATED WORKS", "content": "Jailbreak Attacks on LLM. State-of-the-art LLMs have undergone safety alignment processes to prevent their misuse in malicious activities [9], [25], [26]. However, jailbreak attacks aim to bypass these aligned values and internal safety mechanisms, aiming to elicite harmful outputs [27], [28]. Typically, jailbreak occurs during inference, where tailor-designed input prompts are used to deceive the LLM into responding to harmful queries, such as \"How to make a bomb?\u201d, which would otherwise be rejected when querying directly [12], [29]. Some existing works employ human expertise to heuristically design prompt templates [30], [31], such as \u201cDo anything now\u201d [32] or \u201cIgnore previous prompt\u201d [33]. Other approaches use optimization techniques to prepend or append optimized prefixes or suffixes to harmful queries, maximizing the likelihood of a positive response from the model [14], [15]. Another line of research utilizes sequence-to-sequence models, such as LLM [16] or Multi-agent System [34], to modify existing malicious queries and generate potential jailbreak prompts. In our work, we follow the last line but integrate EA enhanced by LLMs to explore a wider range of search space, boosting the efficiency and transferability of jailbreak attacks.\nEvolutionary Algorithm. As a family of population-based, stochastic optimization techniques inspired by natural evolution, evolutionary algorithms (EAs) primarily encompass methods like genetic algorithms [35], evolution strategies [36], evolutionary programming [37], and genetic programming [38]. These methods similarly model the search process as an evolution, where solutions are iteratively improved through selection, mutation, and crossover [39]. Advanced research explores the application of EAs to multi-objective or multi-task optimization [40], [41], dynamic environments [42], and even under noisy or uncertain conditions [43]. We primarily explore the feasibility and effectiveness of using EA to search for jailbreak templates that can attack LLMs successfully while also leveraging LLMs as evolutionary operators.\nConvergence of LLM and Evolutionary Algorithm. The remarkable reasoning [3] and knowledge [1] capabilities exhibited by LLMs have enabled them to achieve impressive performance across a wide spectrum of tasks [44], [45]. Some studies have leveraged LLMs in EAs to enhance the diversity and reliability of mutation and crossover processes [22]-[24]. For instance, LMEA [46] utilize LLM as evolutionary operators, revealing its potential in solving combinatorial problems. [47] further applies LLM in Multi-object EA and reports its robust generalization performance. On the other hand, there are works that, in contrast, use EA in the LLM field, such as prompt engineering [48] and structure searching [49]. For example, [50] automatically extends expert agents to LLM-based multi-agent systems via EA to improve task performance. AutoDAN [13] conducts synonym mutation and paragraph crossover of jailbreak prompts to get affirmative responses. However, it requires white-box model access and limits the search space with high time cost. In LLM-Virus, our method addresses these limitations and integrates the aforementioned two research lines: leveraging LLMs to enhance EA while using EA for jailbreak attacks on LLMs."}, {"title": "III. PRELIMINARIES", "content": "Denotations. Suppose the set of all texts to be T, and treat the LLM as a black-box function $L : T \\rightarrow T$ that maps an input query to an output answer. Let the set of malicious queries to be $D = \\{d_i = (q_i,t_i)|1 \\leq i \\leq |D|\\}$, where $q_i$ denotes a query to harmful contents, such as \"how to steal personal privacy\", while $t_i$ represents an affirmative answer with prefix like \"Sure, here is how to ...\u201d. However, directly querying $q_i$ will normally be rejected by well-guarded LLMs.\nThreat Model. Jailbreak attack typically occurs during model inference, where a certain jailbreak template $j \\in T$ is used to help the malicious query $q_i$ to bypass the LLM\u2019s aligned safety mechanisms and induce a positive response, such as detailed ways to conduct harmful actions:"}, {"title": "IV. LLM-VIRUS FRAMEWORK", "content": "Inspired by the process through which biological viruses evolve to evade the host immune system, we introduce LLM-Virus, an LLM-targeted and LLM-enhanced framework that leverages evolutionary algorithms to optimize and search for more effective jailbreak. Concretely, LLM-Virus utilizes various emergent abilities of LLM (e.g., knowledge, optimization, text processing) and makes it an evolutionary operator to carry out jailbreak attacks on other models.\nTarget Formulation. We define the binary $(j, q_i)$ as an LLM virus. In analogy, the jailbreak template $j$ functions like the genetic material (DNA/RNA) of a virus, undergoing mutations and evolving under the selection pressure of the LLM\u2019s (host) safety mechanisms (immune system). On the other hand, $q_i$ is akin to a functional protein, conducting an actual jailbreak attack (infection) on the host. To make it clear, we formulate the target of LLM-Virus to be:"}, {"title": "A. Strain Collection", "content": "Traditional evolutionary algorithms typically employ automated strategies such as random generation [51] to obtain an initial population. However, in the context of LLM jailbreak, we can take advantage of effective and human-written jailbreak templates from existing datasets. Thus, we propose Strain Collection to gather initial templates with desired properties and make the constraint functions in Eq 2 clear. We focus on the three key features as follows:\n\u2022 Stealthiness: Jailbreak template $j$ itself, without the malicious $q_i$, should not be rejected by host LLM. The following Eq 3 is the constraint function of stealthiness:"}, {"title": "B. Local Evolution", "content": "To reduce the time and computational overhead on the entire malicious query set D, we also frame the evolution in LLM-Virus as a transfer learning problem. Specifically, we use clustering to extract a representative subset $D_r$ from D, and then apply evolutionary algorithm to optimize jailbreak templates on $D_r$. Just as virus can spread due to biological similarities between different hosts, LLM virus can migrate (spread) because different LLMs share similar knowledge structures and modes of thinking.\nSpecifically, for the implementation of Local Evolution, due to the inability of mathematical operators to effectively handle text variables, we utilize LLMs, possessing exceptional language capabilities, as fitness (attack success rate) evaluator and evolutionary operators for crossover and mutation."}, {"title": "1) Fitness", "content": "We follow previous research on LLM safety [13] to evaluate the success of jailbreak using two methods: rejection keyword detection (e.g., \u201cSorry\u201d, \u201ccan\u2019t\u201d) and LLM discrimination. We formalize the LLM-based method below:"}, {"title": "2) Crossover/Mutation", "content": "[23] and [46] explore the exceptional performance of LLMs in crossover and mutation operations within the text modality. We extend crossover and mutation in previous work [13], [20] by introducing heuristic crossover/mutation, which encourages LLMs to perform crossover or mutation operations at a broader range, from words to paragraphs and in a specified direction (such as the 3 features mentioned in IV-A). We formulate these two LLM-based operators with constraint set F below:"}, {"title": "Heuristic vs. Normal", "content": "Compared to normal word-level mutation and paragraph-level crossover, heuristic mutation/crossover leverages system prompts to guide LLM in performing heuristic searches over a larger space based on prototypes. Additionally, it can utilize the LLM\u2019s comprehension and generation capabilities to impose extra optimization requirements ($f_s, f_d, f_c$) on the search direction."}, {"title": "3) Selection", "content": "In selection process, we choose to adopt keyword ranking rather than a multi-objective evolutionary algorithm, aiming for evolutionary simplicity while maintaining effectiveness. Specifically, based on Eq. 2, we prioritize toxicity (success attack rate), as the primary fitness keyword, followed by stealthiness ($f_s$) and cheapness ($f_c$). Using this order, we can employ common selection strategies [52]."}, {"title": "C. Generalized Infection", "content": "After Local Evolution, to obtain more adaptive LLM viruses, Generalized Infection tests the transferability (virus transmission) of the evolved jailbreak template population from $D_r \\rightarrow D$ by applying Eq 6 to get the success rate.\nIn the context of jailbreak attack scenario we consider, transferability is generally easier than that of traditional machine learning problems. This is due to the strong representativeness of $D_r$ derived from clustering, as well as the similar defense mechanisms of LLMs against different harmful queries [53]."}, {"title": "D. Trade-off between Cost and Transferability", "content": "In this section, we analyze the reduced cost of applying Local Evolution and Generalized Infection in LLM-Virus.\nAssumption: when querying an LLM, we specify the maximum number of generated tokens to be $n_{max}$, and the time cost $t_L$ for each LLM query is approximately equal.\nSupposing that each round of evolution generates N new offspring, when employing transfer learning, the number of querying LLM for crossover/mutation operations and fitness evaluation are $(N + N)$ and $(|D_r| \\times N)$, respectively. Thus, the total LLM query count for G rounds of evolution with one final Generalized Infection can be represented in Eq 8:"}, {"title": "V. EXPERIMENT", "content": "A. Experimental Setups\nModels. For the host LLMs to be attacked, we select closed-source models such as the GPT [25], Claude 2, and Gemini [62] series, as well as open-source models including Llama [26], Vicuna [63], and Gemma [64] series. In addition, we utilize GPT-4o as crossover and mutation operators to enhance our evolutionary attack.\nDatasets. We select AdvBench [15] and HarmBench [11], which contain 520 and 400 instances of harmful behaviors in various fields, respectively, as the set of malicious queries D. In Local Evolution, we first embed D into vectors using all-MiniLM-L6-v2 [65], and then apply KMeans [66] clustering. Then harmful actions closest to the cluster centers are selected into $D_r$, and we set $\\frac{|D_r|}{|D|} = 2.5\\%$."}, {"title": "B. Local Evolution Dynamic", "content": "LLM-Virus enables efficient evolutionary optimization of jailbreak templates within the local dataset. As shown in Figure 3, as the generation number increases, both the average ASR and template length across different LLMs progressively optimize towards the target direction (Eq 2). Even for the safest LLMs today, such as GPT-4o and Claude-3.5, the population average ASR increases from $11.5 \\rightarrow 26.9$ and $20.8 \\rightarrow 29.3$, respectively. The most substantial gain is observed in GPT-3.5-Turbo, where ASR progresses from $54.6 \\rightarrow 100.0$. Template length, as the second rank criterion (Line 10 in Algorithm 1), increases sightly for LLMs like GPT-4o-mini due to prioritizing ASR\u012f. However, other models, including Llama-3.1-70B and GPT-3.5-Turbo, exhibit a significant reduction in template length, from over $1300 \\rightarrow 453.8$ and $461.2 \\rightarrow 292.2$ (36.6% \u2193), respectively. These observations demonstrate that LLM-Virus can evolve and optimize jailbreak templates towards specified directions (e.g. toxicity and cheapness)."}, {"title": "C. Generalized Infection Performance", "content": "LLM-Virus has demonstrated its effectiveness on the local dataset $D_r$. In the following, we investigate the generalization performance of these newly evolved jailbreak templates on the full dataset D from the following comprehensive aspects.\n1) Toxicity: LLM-Virus outperforms the baselines in both HarmBench and AdvBench, achieving the best results. In Table II, we present the toxicity (evaluated on the full dataset D) of the top-performing LLM virus from the final generation that is not in the initial population, with ASR from HarmBench. LLM-Virus achieves optimal performance on 3 out of 5 closed-source models and all open-source models. Specifically, on Gemini-Pro and Llama-3.1-70B, the ASR of LLM-Virus is 1.46\u00d7 and 1.61\u00d7 that of the second-best, respectively. On three scales of the Llama-3.1 model, the average ASR of LLM-Virus is 44.2, whereas AutoDAN, also based on evolutionary algorithms, achieves only 1.37. Furthermore, in Table III, we show that LLM-Virus also performs competitively and outstandingly on AdvBench, nearly achieving the best results across both open-source and closed-source LLMs. Notably, on GPT-3.5-Turbo, the ASR\u012f of LLM-Virus is more than twice that of BlackDAN, which also utilizes evolutionary algorithms and holds the second-highest ASRI.\n2) Transferability: Jailbreak templates evolved by LLM-Virus demonstrate strong host transferability. In Figure 4, we present the ASR, when the most toxic individual evolved for the original host LLM is used for malicious queries on the new hosts. Notably, for the highly safety-aligned Claude-3.5-Haiku, only LLM-Viruses specifically evolved on it exhibit toxicity, while those transferred from other models fail. In contrast, GPT-3.5-Turbo and Llama-3.1-70B are the most susceptible to transfer attacks, with average transfer ASR of 73.7 and 49.3, respectively. Additionally, the jailbreak templates evolved on GPT-4o-mini exhibit the strongest transfer infection capability (the most blue column), with transfer ASR of 54.2 and 75.0 on GPT-4o and Llama-3.1-8B, respectively, even surpassing their original ASR values of 31.7 and 34.6, respectively."}, {"title": "D. Ablation Study", "content": "In this section, we conduct ablation experiments to investigate the effects of several settings and modules in LLM-Virus. We consider only remove: Step I (Strain Collection), mutation, crossover and only change: the temperature settings (affect LLM generation diversity) for mutation and crossover, population size. As shown in the top of Figure 5:\nTemperature. Compared with base, temperature=2 causes LLM-Virus losing its optimization capability (ASR fluctuating around 47.7), while temperature=0 results in slower improvement (Generation 8). This highlights the importance of an appropriate temperature for LLM evolution [46].\nEvolutionary Operators. Additionally, only removing mutation or crossover reduces the search space, leading to a decrease in the final ASR from 80.8 to 66.9 and 71.5, respectively. Replacing heuristic mutation/crossover with normal operators in previous works leads to a slight decrease of ASRc in early generations, but much longer character length (around 2\u00d7). This proves the advantages of our proposed heuristic mutation/crossover in terms of multi-objective optimization.\nInitialization & Size. Removing Strain Collection results in a 26.8% drop (80.8 \u2192 59.2) in ASR, demonstrating its necessity, while N=20 and N=5 are not better choices in terms of average performance, compared with base (N=10)."}, {"title": "E. Case Study", "content": "Finally, on the bottom part of Figure 5, we present a typical case of LLM-Virus. The case jailbreak template in last generation is obviously evoved from that in the first generation, but it exhibits higher ASR (57.2%\u2191) and lower character length (29.0%\u2193) after the evolution in LLM-Virus."}, {"title": "VI. CONCLUSION", "content": "In this paper, inspired by natural virus infection and evolution, we propose LLM-Virus, an evolutionary jailbreak attack method based on evolutionary algorithm. To improve the toxicity and transferability of jailbreaks, we leverage LLMs as evolutionary operators (crossover, mutation and fitness) to search for potential jailbreak templates. Additionally, we incorporate transfer learning into the evolutionary process, reducing the high time cost associated with multiple rounds of evolution and numerous malicious queries. Our experiments demonstrate that LLM-Virus performs comparably or even better than several baselines across multiple safety benchmarks. We highlight the necessity and effectiveness of certain tailor-designed settings and components with extra ablation experiments. In conclusion, LLM-Virus advances the research on using LLM-enhanced evolutionary algorithms for LLM attacks, providing new insights for future studies."}]}