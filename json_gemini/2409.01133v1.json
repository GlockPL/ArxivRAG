{"title": "Large Language Models Can Understanding Depth from Monocular Images", "authors": ["Zhongyi Xia", "Tianzhao Wu", "Given Name Surname"], "abstract": "Monocular depth estimation is a critical function in computer vision applications. This paper shows that large language models (LLMs) can effectively interpret depth with minimal supervision, using efficient resource utilization and a consistent neural network architecture. We introduce LLM-MDE, a multimodal framework that deciphers depth through language comprehension. Specifically, LLM-MDE employs two main strategies to enhance the pretrained LLM's capability for depth estimation: cross-modal reprogramming and an adaptive prompt estimation module. These strategies align vision representations with text prototypes and automatically generate prompts based on monocular images, respectively. Comprehensive experiments on real-world MDE datasets confirm the effectiveness and superiority of LLM-MDE, which excels in few-/zero-shot tasks while minimizing resource use. The source code is available.", "sections": [{"title": "I. INTRODUCTION", "content": "Monocular depth estimation (MDE) is essential for applications such as autonomous driving, where accurate environmental perception is critical for safety. Traditional MDE methods, based on manually designed features and geometric models, frequently underperform in complex scenarios. Recent advancements in deep learning (DL) have revolutionized MDE [1]-[3], offering robust performance without the constraints of physics or the need for resource-intensive feature engineering.\nDL-based MDE techniques are divided into two categories based on learning strategies: supervised [4]\u2013[6] and unsupervised [7], [8] methods. Supervised methods require large labeled datasets and deliver impressive performance but are resource-intensive. In contrast, unsupervised methods use unlabeled data to facilitate effective knowledge transfer with minimal supervision. However, both strategies face three main challenges: (1) reliance on specialized neural architectures, requiring custom models for specific tasks, which reduces flexibility; (2) the need for explicit information in certain scenarios, dependent on pre-trained pose estimation networks for scene-specific knowledge, limiting performance; (3) dependency on precise data labeling, a premise rarely questioned in unsupervised methods despite minimal supervision.Therefore, developing a unified MDE framework that supports flexible performance with minimal supervision and independence from complex, tailor-made model architectures is crucial.\nThis paper demonstrates that pretrained large language models (LLMs) can effectively understand depth from monocular images. We introduce the Large Language Model for Monocular Depth Estimation (dubbed LLM-MDE), a multimodal framework that interprets depth via language understanding. LLM-MDE integrates two primary strategies to improve depth perception: cross-modal reprogramming and an adaptive depth prompt generation. The former aligns visual representations from monocular images with text prototypes from a comprehensive vocabulary library, enhancing feature extraction for LLM input. The latter strategy generates and tokenizes prompts from monocular images for LLM processing. These approaches significantly improve LLM insights into monocular depth estimation. Our contributions are four-fold:\n\u2022\n\u2022\n\u2022\n\u2022\nThis study represents the first exploration of pre-trained large language models (LLMs) for monocular depth estimation. Empirical evidence demonstrates that LLMs can deliver depth information with minimal supervision.\nWe introduce LLM-MDE, a unified multimodal framework utilizing LLMs for monocular depth estimation. It integrates cross-modal reprogramming and an adaptive depth prompt generation module to enhance LLM insights into depth with minimal supervision and resource.\nWe introduce cross-modal reprogramming and adaptive depth estimation. The former aligns monocular image and text prototypes, while the latter automatically generates depth prompts to enhance estimation insights.\nExtensive experiments on the real-world MDE dataset demonstrate the effectiveness and superiority of our LLM-MDE, which performs well on few-/zero-shot tasks.\nWe highlight that LLM-MDE is not for competitive purposes but rather serves as an exploratory tool for depth estimation, especially in scenarios with limited supervision/resources or where complex neural architectures are not required."}, {"title": "II. METHODOLOGY", "content": "The structure of our LLM-MDE is illustrated in Fig. ??. It combines two pretrained models: a Vision Transformer (ViT) that extracts visual representations from images and an LLM that performs depth estimations. We introduce two strategies: cross-modal reprogramming and adaptive depth prompt generation, which enhance the LLM's depth estimation capabilities. Features from these strategies are fused into the LLM via an adaptive head for accurate depth estimation. Further details will be provided subsequently."}, {"title": "A. Cross-modal Reprogramming between Vision and Text", "content": "LLM pretrained on extensive natural language datasets demonstrate superior sequence modeling and generalization capabilities. However, differences between text and image data prevent direct application of LLMs to image representation tasks. Monocular images also cannot be directly edited or described losslessly in natural language, posing significant challenges for using LLMs to understand them without intensive fine-tuning. To address this, we introduce a cross-modal reprogramming strategy that combines visual representations of monocular images with latent semantic information from large-scale textual corpora, enhancing the LLM's ability to perceive, understand, and interpret vision representations. Specifically, we used pre-trained word embedding $E \\in R^{V \\times D}$ in the LLM backbone, where V and D denote the vocabulary size and dimension. Nevertheless, there is no prior knowledge indicating which text tokens are directly relevant with monocular image representation. Thus, we maintain a small collection of text prototypes by linearly transformation E, denoted as $E' \\in R^{V' \\times D}$, where $V' << V$. Text prototypes learn connecting to represent the local patch information (e.g., \"extremely close\u201c for vision representation) without leaving the space where the language model is pre-trained. We achieve the proposed Cross-modal Reprogramming via a multi-head attention layer. For each haed k = {1,..., K}, we define query matrices $Q^{(i)}_k = XW^Q_k$, key matrices $K^{(i)}_k = E'W^K_k$, and value matrices $V^{(i)}_k = E'W^V_k$, where $W \\in R^{dm \\times d}$ and $W^K,W^V \\in R^{D \\times d}$. Specifically, D is the hidden dimension of the pretrained LLM, and d = d. Then, the cross-modal reprogramming can be formulated as:\n$F^{(i)}_k = Reprogramming(Q^{(i)}_k, K^{(i)}_k, V^{(i)}_k)$\n$ = SOFTMAX(\\frac{Q^{(i)}_kK^{(i)T}_k}{\\sqrt{d_k}})V^{(i)}_k$\nFinally, by aggregating the features $F^{(i)}_k \\in R^{D' \\times d}$ from each head, we obtain $F^{(i)} \\in R^{D' \\times dm}$, where D' is the output dimension of Cross-domain Reprogramming. These are then linearly projected to fuse with the representation from the prompt representation detailed below."}, {"title": "B. Adaptive Depth Prompts Generation Module", "content": "To strength the insight of depth understanding of pretrained LLMs without additional structures or internal modifications, we introduce the Adaptive Depth Prompt Generation Module (APG). The APG autonomously generates statistical prompts for monocular images, improving depth comprehension. This module integrates prompt generation and representation, producing prompts from four perspectives: Dataset, Task, Pixel, and Class. The Dataset and Task components generate concise dataset information and task descriptions. The Pixel component creates prompts using pixel-level statistics like minimum, maximum, and median values from the monocular image. Class assigns a unique label to each image based on pixel value distribution across seven categories: \u201cgiant\", \"extremely close\", \"close\", \"not in distance\", \u201ca little remote\u201d, \u201cfar\u201d, and \u201cunseen\". The generated prompts are then processed by a pretrained tokenizer to yield textual representation."}, {"title": "C. Depth Projection from Adaption Head", "content": "To transform language representations into depth information, we introduce the Adaptation Head based on the ResNet architecture for feature refinement and depth projection. The Adaptation Head employs the UpsampleBN module, integrating convolution, batch normalization, and Leaky ReLU with residual connections. The process starts by adjusting input features with a linear layer, followed by three UpsampleBN operations to enhance spatial resolution and feature representation. This expands feature maps to capture fine details and increase the receptive field. A final Sigmoid normalizes the output, producing the depth map."}, {"title": "D. Lightweight Operations and Optimization", "content": "Tuning pre-trained ViTs and LLMs for visual representation and depth estimation remains resource-intensive, posing significant challenges in low-resource settings. To address this, we introduce lightweight operations throughout the framework to balance cost and performance. Specifically, we adopt low-rank adaptation (LoRA) [9] for each attention block within the ViT and LLM, which efficiently updates parameters by modifying only a small subset of weights, preserving the original model structure and knowledge. The implementation of LoRA involves using the original weight matrix $W \\in R^{d \\times d}$ and adding the product of lower-order matrices as:\n$W' = W + A \\times B$, where $A \\in R^{d \\times r}, B \\in R^{r \\times d}$, (2)\nwhere r denotes the rank value, and A and B are low-rank matrices with dimensions smaller than W (r < d), ensuring a low parameter count in the tuning process. For optimization, we used the scale-invariant squared loss (SSI) for monocular depth estimation is formulated as:\n$L(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} (log d_i - log \\hat{d_i})^2 - \\frac{1}{n^2} (\\sum_{j=1}^{n} (log d_j - log \\hat{d_j}))^2$ (3)\nwhere $\\theta$ represents the model unfrozen parameters, $d_i$ is the true depth value for the i-th sample, $\\hat{d_i}$ is the predicted depth value for the i-th sample, and n is the number of samples."}, {"title": "III. EXPERIMENTS", "content": "We conducted evaluation on Ubuntu 22.04 server, equipped with an Intel Xeon Silver 4210R CPU and an NVIDIA GeForce RTX 3090Ti GPU (24 GB RAM). Key hyperparameters were set as follows: a patch size of 16, training resolution of 224, a dropout rate of 0.1, a batch size of 16, and the Adamw optimizer with an initial learning rate 1e-5. We utilized the NYU raw dataset, which comprises images with a resolution of 640 \u00d7 480, in all experiments due to its generalizability. We used the ViT-base and 12-layer BERT throughout all experiments. During training, we conducted 50 epochs with an early-stopping strategy that halts training if the validation loss does not decrease for 5 consecutive rounds. Additionally, we applied a cosine annealing strategy to the learning rate to prevent overfitting. We closely adhere to the experimental protocol outlined by Ranftl et al. [10] Specifically, we utilize Root Mean Squared Error (RMSE), Absolute Relative Error (Abs Rel), Squared Relative Error (Sq Rel), Logarithmic Root Mean Squared Error (Log RMSE), and accuracy as our evaluation metrics."}, {"title": "A. Few-Shot and Zero-Shot Experiments", "content": "To demonstrate the effectiveness of LLM-MDE in resource-limited settings, we executed Few-shot and Zero-shot experiments. Incremental increases in the number of shots led to substantial reductions in various losses and enhancements in detail resolution, exemplified by improved texture depiction in bookshelves in the third and fourth images, and more accurate delineation of invalid areas in the second and fourth images."}, {"title": "B. Ablation Experiments", "content": "To demonstrate the effectiveness of APG and Fixed Prompts in depth estimation, we conducted an ablation study. The model without prompts exhibited the highest loss, marked by significant noise and artifacts. Conversely, Fixed Prompts significantly reduced loss, lowering RMSE and Abs Rel by 31.4% and 43.4%, respectively, and reducing artifacts. APG Prompts showed superior performance, minimizing artifacts and enhancing textural details."}, {"title": "C. Hyper-parameter Sensitivity", "content": "Tab. V and Fig. 5 present the results of the LLM-MDE hyper-parameter sensitivity experiment involving various LoRA fine-tuning strategies. We used a controlled variable approach, adjusting the Alpha and Rank parameters of LoRA ViT and LoRA LLM, as well as batch size and learning rate, to study their impact on model accuracy. Schemes 1, 3, and 7 show that low Alpha and Rank values reduce LoRA's effectiveness: Scheme 1 shows less detailed predictions, while Scheme 7 has more artifacts. Schemes 3 and 6 demonstrate that very high Alpha and Rank values cause overfitting and poor generalization, leading to significant artifacts. Schemes 2 and 3 reveal that too much parameter adjustment freedom undermines training stability and increases losses and artifacts. Schemes 3, 5, and 8 indicate that smaller batch sizes reduce training stability and prediction accuracy, and increase losses. However, as Scheme 8 shows, very large batch sizes on small datasets can also impair accuracy."}, {"title": "IV. CONCLUSIONS", "content": "In conclusion, this paper introduces LLM-MDE, a multimodal framework that interprets depth through language understanding. LLM-MDE employs two main strategies to enhance depth perception: cross-modal reprogramming and an adaptive depth estimation module. The former aligns visual representations from monocular images with text prototypes from a comprehensive vocabulary, improving feature extraction for LLM input. The latter generates and tokenizes prompts from images for LLM processing. These methods significantly enhance monocular depth estimation insights. Extensive experiments on the real-world MDE dataset demonstrate the effectiveness and superiority of our LLM-MDE."}]}