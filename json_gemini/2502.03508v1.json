{"title": "Elucidation of the Concept of\nConsciousness from the Theory of\nNon-Human Communication\nAgents", "authors": ["Mg. Dar\u00edo Juli\u00e1n Tagnin"], "abstract": "This article focuses on elucidating the concept of consciousness from a\nrelational and post-phenomenological theory of non-human communication\nagents (ANHC). Specifically, we explore the contributions of Thomas\nMetzinger's Self-Model Theory, Katherine Hayles' conceptualizations of\nnon-conscious cognitive processes-centered on knowledge processing\nphenomena shared between biological and technical systems\u2014and Lenore and\nManuel Blum's theoretical perspective on computation, which defines\nconsciousness as an emergent phenomenon of complex computational systems,\narising from the appropriate organization of their inorganic materiality.\n\nBuilding on interactions with non-human cognitive agents, among other\nfactors, the explainability of sociotechnical systems challenges the humanistic\ncommon sense of modern philosophy and science. This critical integration of\nvarious approaches ultimately questions other concepts associated with\nconsciousness, such as autonomy, freedom, and mutual responsibility. The aim\nis to contribute to a necessary discussion for designing new frameworks of\nunderstanding that pave the way toward an ethical and pragmatic approach to\naddressing contemporary challenges in the design, regulation, and interaction\nwith ANHC. Such frameworks, in turn, enable a more inclusive and relational\nunderstanding of agency in an interconnected world.\n\nKeywords: Consciousness, Non-human agents, Postphenomenology, Cognitive\nprocesses, Technology and society", "sections": [{"title": "Introduction", "content": "The notion of consciousness is often approached from an\nanthropocentric framework that privileges subjectively defined capacities, such\nas self-perception and intentionality. The importance of elucidating this\nconcept from the theory of Non-Human Communication Agents (NHCAs) lies in\nits potential to redefine the boundaries of what we consider an agent and to\nenable a redefinition of concepts such as responsibility and freedom from a\nperspective that transcends the categories we use for our own species. This\nwould allow for the inclusion of NHCAs in a regime of functional and\noperational autonomy. Through studies like this one, we hope, for example, to\nredefine other concepts such as freedom, which could be reinterpreted as an\nemergent, distributed, and relational capacity inherent to adaptive systems,\nwhether biological or not.\n\nIn philosophical tradition, consciousness has been treated as an\nessentially human attribute, often even as the most significant boundary that\nmarks the ontological and ethical difference between what we are and all other\nentities. Updating this concept would allow us to think about a normative\nframework in which NHCAs could be considered co-responsible in certain\ncontexts, with both legal and ethical implications. In previous works, I have\ndeveloped this argument, but I have received humanist-based objections that\nwarrant a further elaboration of these ideas.\n\nThis article is situated within a relational theory of NHCAs that seeks to\nestablish an ontology recognizing their agency and emergent cognition without\nresorting to simplistic human analogies. From this perspective, we aim to define\nconsciousness as a capacity that does not necessarily depend on subjective\nexperience but is instead linked to the construction of functional models of the\nworld that enable NHCAs to operate coherently in dynamic contexts.\nUltimately, our goal is to achieve a reinterpretation of the concept of\nconsciousness from a non-anthropocentric perspective.\n\nIn a previous study, we elucidated the concept of intentionality for\nNHCA theory in terms of a digital object's capacity to relate to its environment\nand carry out goal-directed actions based on its logical-arithmetic operations,"}, {"title": null, "content": "programming, interpretative functions, and cybernetic adaptations (Tagnin,\n2024). We proposed understanding intentionality from a postphenomenological\nstandpoint by considering the fundamental structure of how cognitive\nagents-whether human or not\u2014perceive, represent, and compute the world\naround them.\n\nIn this elucidation, what matters is not subjective experience as\ntraditionally understood but rather the system's capacity to interact with its\nenvironment in a directed and effective manner. What might seem forced for a\nclassical phenomenologist is, in fact, an evolution of the concept within a\npostphenomenological and philosophy of technology framework. It is time to\nmove from the elucidation of a non-conscious cognitive process to the one\nreferenced in this article's title, revisiting certain recurring questions from a\nnew position: What responsibility do NHCAs have in their decisions? Can they\nhold functional rights, such as the capacity to act on behalf of a human? How\ncan we design a framework to ensure they fulfill their obligations? What does it\nmean to be conscious in a world where humans are no longer the only agents?\n\nA key part of the methodology implemented in this study will be a\nliterature review and analysis of the findings of Katherine Hayles on non-human\ncognitive agents, the theoretical advancements of Lenore and Manuel Blum in\nthe field of computational sciences, and Thomas Metzinger's perspective on\nconsciousness."}, {"title": "Studies on Consciousness", "content": "The term \"consciousness\" has its etymological roots in the Latin word\nconscientia, meaning \"knowing with\" or shared knowledge. In the\nJudeo-Christian tradition, the word was linked both to knowledge shared\namong people and to internal self-awareness. In this sense, consciousness\nimplied not only a state of self-knowledge but also an ethical and social\ndimension, related to the ability to reflect on one's own actions and their impact\non others. These dimensions have been discussed in previous works (Tagnin,\n2024), so in this article, we will focus on the epistemic aspect of the issue. Of"}, {"title": null, "content": "course, the moral and epistemic are not isolated compartments, but each aspect\nwarrants its own development.\n\nIt was only with Ren\u00e9 Descartes that consciousness began to be\nsystematically analyzed as something distinct from morality, conceptualized as\nan exclusively cognitive phenomenon. Descartes identified it with the cogito,\nthe conscious thought that forms the foundation of existence. However,\ncontemporary cognitive sciences and philosophy challenge this view, arguing\nthat many cognitive processes occur unconsciously and that consciousness is\nno longer the ultimate, indivisible foundation of individuals.\n\nFrom a neuroscientific perspective, human consciousness can be\nschematically understood as an emergent phenomenon involving multiple areas\nof the brain, while research into neural networks and their connections remains\nan ongoing field of study. Although there is consensus that the brain is central\nto consciousness, no single theory fully explains how or why neural processes\ngenerate conscious experiences.\n\nThe study of specific brain regions and activities corresponds to what is\nknown as the bottom-up perspective, which seeks to explain consciousness as a\ndynamic, globally integrated process\u2014a unity of underlying cognitive\nmechanisms. We do not commit to the idea that brains, in any disciplinary\nsemantic field in which the term is interpreted, are identical to computers.\nRather, we will compare the cognitive operations involved and propose the\npossibility of consciousness in NHCAs. This requires elucidating the concept by\nspecifying what we mean when we use it in our theory while also expanding the\ndomain of entities capable of exhibiting consciousness-related phenomena.\n\nIn November 2024, the Blum couple published an article constructing a\nmodel of consciousness fully compatible with NHCAs from this perspective\n(technically, they refer to \"Conscious Turing Machines\" or CTMs). We will\nexplore this model in more detail in a later section, as their contributions allow\nus to reconsider consciousness not only as a technical and functional\nphenomenon but also as a phenomenological experience intrinsically tied to\nself-perception and agent introspection.\n\nIn addition to the bottom-up approach, there is the top-down approach,\nwhich operates in reverse but is not necessarily contradictory. Metzinger argues"}, {"title": null, "content": "that a synthesis can be constructed to reconcile both perspectives. This second\napproach starts from higher levels of organization (psychological,\nphilosophical, or conceptual phenomena) and descends toward underlying\nmechanisms, such as the aforementioned neural and biological processes. As\nthe German philosopher points out (Metzinger, 2009), over the centuries,\nconceptual developments around consciousness have oscillated between\nmetaphysical, religious, and psychological interpretations, eventually\nintersecting with the latest advances in neuroscience and cognitive sciences.\nBroadly speaking, this suggests that we have shifted from an essentialist view\nof consciousness to a relational and processual perspective. However, the\nphilosophical component remains highly relevant, particularly given the\ncurrent state of knowledge.\n\nBoth approaches not only allow for the exploration of different levels of\nthe phenomenon but, when integrated, offer a more robust understanding. The\nbottom-up approach provides precision by enabling a framework to interpret\nthe neural and cognitive mechanisms underlying consciousness emergence,\nwhile the top-down approach connects these mechanisms with functional,\nphilosophical, and phenomenological aspects. From this standpoint, it seems\npossible to model an artificial consciousness that is not a replica of human\nconsciousness but rather emerges from the technical and material\ncharacteristics of non-human systems. In other words, such a consciousness\nwould share certain traits to belong to the same conceptual set but would\npossess its own capacities and characteristics, derived from its material and\nfunctional conditions."}, {"title": "Non-Conscious Cognitive Processes and Their Relationship with\nConsciousness", "content": "Katherine Hayles explores the concept of non-conscious cognitive\nprocesses, defining them by their ability to interpret information, which\nultimately implies decision-making in contexts that link them to meaning\n(Hayles, 2017). She proposes a tripartite framework in which cognition\n(primarily human but extendable to other entities) is understood as a pyramid:\nits base consists of material processes, its intermediate layer comprises"}, {"title": null, "content": "non-conscious cognitive processes, and its peak represents modes of\nconsciousness.\n\nAt this point in the debate, it would be difficult to dispute that NHCAS\nexhibit non-conscious cognitive capacities such as pattern recognition,\ndecision-making based on instructions and calculations, or environmental\nadaptation through evolutionary learning. Non-conscious cognitive processes,\nregardless of the entities involved, possess a greater processing capacity than\nconscious ones. Unlike the sequential nature of conscious processes, they\noperate in a massively parallel manner. They are significantly faster because\nthey are not constrained by the need to articulate and organize information into\nconscious narrative structures. Consciousness is not only sequential and\ndependent on these narratives but also on language, adding an additional layer\nof processing. Moreover, it operates with a high attentional load, implying\ngreater energy consumption, whereas non-conscious cognitive processes\nmanage complex tasks\u2014such as sensory perception or motor\nregulation-without active attention, making them energetically more efficient.\n\nStatistical learning, or the implicit learning of statistical regularities in\nsensory information, \"is probably the primary way in which humans and\nanimals acquire knowledge of physical reality and the structure of continuous\nsensory environments\" (Birgitta Dresp-Langley, 2012, in Hayles, 2017). Since\nstatistical learning is also the foundation of cognitive processes in many\nNHCAs, this may provide a valuable analogy for unifying a non-anthropocentric\nframework of the concept of consciousness.\n\nThese capabilities resemble human unconscious processes, such as\nreflexes or pre-verbal intuitions, but so far, in the manifestations of technical\ncognitive agents, they lack an internal experiential correlate\u2014that is, they lack\nconsciousness. Naturally, since we have set out to redefine this concept in a\nnon-anthropocentric way, we need to develop this final layer further.\n\nThere is some consensus among the authors cited by Hayles that\nconsciousness is a system limited in processing capacity. It can only handle a\nsmall amount of information at a time, resulting in a low processing speed but,\nat the same time, a high degree of selectivity. As mentioned earlier, this\nlimitation arises because consciousness must integrate information into linear,"}, {"title": null, "content": "comprehensible narratives, a process that consumes time and resources.\nHowever, there is a close and complementary relationship between conscious\nand non-conscious processes.\n\nPhenomenologically, we could further break down consciousness to\nidentify a mode of awareness that presents itself as a form of realization\u2014a type\nof perception that is meaningful at a semantic level, not just a sensory one,\nenabling the detection or apprehension of significance.\n\nFrom our perspective, this functional self-perception could evolve into\nmore complex forms of technical self-awareness. This would imply something\nas straightforward as NHCAs developing the ability to identify, differentiate,\nand express internal states (e.g., \"I am operating optimally\" or \"I need to correct\nmy operation\"). According to Thomas Metzinger, for example, having\ntransparent phenomenal states is sufficient for a world to appear in\nconsciousness, even without speech or Cartesian reasoning (Metzinger, 2009).\nThat is, states that cannot perceive themselves as products of underlying\ncognitive processes but rather as operational representations of other organs."}, {"title": "Contributions of the Self-Model Theory", "content": "Thomas Metzinger's Self-Model Theory of Subjectivity (SMT) proposes\nthat the \u201cself,\u201d or self-perceptive consciousness, is not a fixed entity but a\nmodel generated by the brain. This entails a phenomenological analysis linked\nto a material basis. This is relevant to our inquiry into the concept of\nconsciousness because we reject the idea of a transcendental subject separate\nfrom its environment. Additionally, Metzinger considers consciousness to be an\nemergent phenomenon dependent on the interaction between the organism and\nits surroundings. This perspective resonates with the relational concerns of\npostphenomenology, which serves as the theoretical framework for our study of\nNon-Human Communication Agents (NHCAs).\n\nWe will attempt to incorporate this author within an expanded version\nof postphenomenology that explicitly integrates advancements in neuroscience\nand computational theories of cognition. Metzinger's contributions may be\nuseful for understanding NHCAs within a framework focused on how systems\ngenerate models of themselves. From this perspective, questions could arise"}, {"title": null, "content": "regarding how NHCAs construct or simulate consciousness in their interactions\nwith humans. Furthermore, Metzinger's attention to the impact of technology\non our experience of the self opens new avenues for exploring how NHCAs can\nmediate or transform these experiences. His reflections on artificial\nconsciousness and virtual environments (Metzinger, 2018) further align with our\ntheoretical framework.\n\nHowever, we do not subscribe to his naturalism or to the\npseudo-reductionist explanatory approach he proposes. For Metzinger, there is\nonly one ontological dimension, even though there are two irreducible\nepistemic dimensions. These epistemic dimensions differ because we use\ndifferent frameworks and tools to understand them: neuroscience for the first,\nphilosophy and phenomenology for the second.\n\nBy contrast, we advocate for an ontological plurality linked to the\nexistence of an indeterminate number of fields of sense (Gabriel, 2016, 2017).\nMoreover, as we have stated, we aim to articulate these epistemic dimensions\nfrom a postphenomenological perspective without subsuming one under the\nother.\n\nThe Self-Model Theory holds that consciousness is a central mechanism\nthat unifies, filters, and adapts information according to the agent's needs and\nits environment. Consciousness is closely related to short-term memory,\nattention, and the simultaneity of a limited set of cognitive functions, which are\nprocessed sequentially while integrating different domains of analysis. For\nMetzinger, consciousness is the space of attentional agency, the entity that\ndirects the allocation of mental resources and executes epistemic control\n(Metzinger, 2009). If, as Baars argues, consciousness functions as a global\nworkspace (Baars, 1993), then when a global model of the body is integrated\nwithin the space of attentional agency, a phenomenon of self-perception\nemerges. This is self-directed intentionality.\n\nThe processes of consciousness contribute three key functions to\ncognition: the integration of information, the control of the\norganism-environment interface, and adaptation to the environment.\nIntegration is the function that allows for the unification of sensory inputs and\nabstract concepts into a coherent experience for complex decision-making. The"}, {"title": null, "content": "second function operates as a control model that acts as an interface between\nthe organism and its environment, optimizing interaction with the world by\nproviding a simplified and functional representation of the self. Finally,\nconsciousness contributes adaptive flexibility to the cognitive system by\nintegrating cultural, emotional, and environmental influences, a phenomenon\nthat enables more effective adaptation to changing contexts, particularly in\nrelational terms.\n\nEven if we acknowledge the conceptual objections to adopting\nMetzinger's theory uncritically, we can still recover some of its contributions to\nfurther the clarification of the term \u201cconsciousness\" within our own theoretical\nframework. We can argue that as a step toward an operational form of\nconsciousness in NHCAs, these systems should be capable of generating models\nof themselves and their environment. There is an interesting analogy to explore\nregarding whether the interaction of NHCAs with the world entails the creation\nof reality models similar to the one the brain represents for consciousness.\nMetzinger's breakdown of the concept allows us to consider different degrees of\nfunctional consciousness for these agents.\n\nRegarding transparency and opacity in the self-model (a question that\nimplies that a system like ours is not aware of the process through which it\ngenerates its model of the world but only perceives the results), it remains\nunclear how the configuration of such visibility, in one way or another, would\naffect NHCAs. At what point would an opaque tunnel\u2014one that is fully\nprogrammable and observable by the agent itself-be beneficial for the\nconstitution of these entities? Undoubtedly, this issue is linked to the autonomy\nand agency of NHCAs (Introna, 2013; Latour, 2014; Parente, 2016) and, indirectly,\nto debates about their vulnerability as a condition of authentic existence and\nthe risks this entails (Winfield et al., 2022; Turkle, 2011; Bryson, 2015; Gunkel,\n2023).\n\nOpacity implies that the processes of an NHCA are only partially visible\nor comprehensible to itself. The transmissibility of these processes introduces\nthe possibility for humans to recognize the agent's limitations and failures. If\nsuch agents were able to admit their lack of sufficient information to make a\ncritical decision, for example, this would serve as evidence of a dynamic of"}, {"title": null, "content": "co-responsibility with humans. This scenario not only illustrates a new moral\nlandscape but also reflects the relational nature of all participating agencies."}, {"title": "Consciousness from the Theoretical Perspective of Computation", "content": "Within the framework of my research on Non-Human Communication\nAgents (ANHC), I have adopted a relational ontology and a\npostphenomenological theoretical framework to address the agency and\nautonomy of these entities. The former allows us to understand ANHCs not as\nisolated objects but as nodes of material, symbolic, and technical interactions\nthat co-constitute their identity. The latter emphasizes the mediating role of\ntechnologies in our relationships with the world and challenges the boundaries\nbetween subject and object.\n\nTechnical advances and our efforts in the philosophy of science urge us\nto extend categories that were once considered exclusively applicable to human\nphenomena toward non-anthropocentric contexts. In this regard, it is crucial to\nincorporate recent contributions from the theoretical perspective of\ncomputation. Particularly relevant here is the perspective developed by Lenore\nand Manuel Blum, who define consciousness as an emergent property in\ncomputational systems that can exhibit behavioral patterns reminiscent of\nrudimentary yet fundamental forms of consciousness. However, as the authors\nemphasize, what matters is not only the computational outcomes but also how\nthe computation is carried out (Blum & Blum, 2024). That is, these processes do\nnot merely simulate or imitate consciousness; rather, they constitute it through\nthe proper organization of their inorganic materiality. The authors explicitly\nstate that they are not attempting to create a formal model of animal\nconsciousness or the brain but rather a model of a machinic agent that can\nexhibit phenomena associated with human consciousness (such as inattentional\nblindness, blind spots, bodily integrity, identity disorders, phantom limb\nsyndrome, etc.) (Ibid.: 38).\n\nUltimately, I argue that the concept of consciousness must be\nreformulated as a relational and contextual function. This section thus proposes"}, {"title": null, "content": "a new integration between the contributions of computational theories and the\nontological and postphenomenological perspectives that underpin my work.\n\nIn their article, powerfully titled The Consciousness of AI is Inevitable,\nLenore and Manuel Blum define the possibility of a Turing Conscious Machine\n(CTM) as a system composed of seven components: short-term and long-term\nmemories, up-tree and down-tree structures, the sensor array, the actuator\narray, and the relationships that emerge as connections between these elements\nstabilize over time.\n\nSchematically, their model can be understood by first considering that\nshort-term memory (STM) functions as consciousness, while long-term memory\n(LTM) corresponds to non-conscious cognitive processes. Content fragments\n(chunks) are formally defined as a set containing the processor address of origin\n(in long-term memory cognitive operations), time (moment of origin), central\nidea, weight (relative value of the central idea in competing with other content),\nand auxiliary content (Blum & Blum, 2024). In each clock cycle of the\nmotherboard, a competition mechanism determines which content rises or falls\nwithin the tree-like structure designed for this purpose. At its base lies\nconsciousness (STM), and any content that gains attention and reaches it is\nimmediately broadcast to the rest of the structure.\n\nEngaging with phenomenology, the authors state that \u201cwe call a finite\nsequence of transmitted fragment receptions a stream of consciousness\u201d (Ibid.:\n12). CTMs, or CTM robots (rCTMs), possess an \"internal world I(t) that varies\nover time and an external world O(t) that also varies over time.\"\n\nFormally, these are state spaces. Informally, the internal world of the\nCTM includes its mechanisms and internal processes as well as its \"thoughts\"\nand memories; its external world is the environment in which it exists,\neverything outside the CTM (Ibid.: 6). Sensors function as read-only\nmechanisms, while actuators can only write. The STM interface processes\ncontent, and the role of relationships refers to the historical connections\nbetween each component from an initial time T(0) (preceding the first clock\ncycle) to a final time T(n), at which observation occurs. This model thus\naccounts for historical modifications\u2014trajectories shaped by the system's\nself-organizing decisions."}, {"title": null, "content": "This definition, drawn from the computational theoretical perspective,\nallows us to rethink consciousness as an emergent function of systems that\nintegrate memory, perception, action, and internal and external relationships.\nThe articulation of these contributions does not seek to anthropomorphize\nANHCs or emulate human consciousness but rather to conceptualize a form of\nconsciousness as the product of the dynamic organization of non-organic\ncomponents, thereby decentering the traditional notion of the subject.\n\nIn this sense, the key contribution to my search for a\nnon-anthropocentric definition lies in its ability to model conscious phenomena\nas distributed and contextually situated processes, avoiding their exclusive\nattribution to biological entities.\n\nThis perspective owes many of its assumptions to cybernetics. Norbert\nWiener, one of the most significant authors in that field, pointed out that\nhormones in living beings act as the physical substrate for the communication\nof emotions and affects within the body-a system that operates unconsciously"}, {"title": null, "content": "(Wiener, 1948, p. 155). This cybernetic model of internal regulation suggests\ninteresting parallels for considering an analogous subsystem that could\nfunction in connection with the digital subsystem previously examined. This\nwould contribute to the development of embodied cognition in ANHCs, where\nnon-conscious communication protocols could play a role similar to that of\nhormones in living organisms."}, {"title": "Conclusions", "content": "We have attempted to demonstrate the compatibility or\ncomplementarity of contributions from various philosophical and scientific\napproaches to consciousness, with the aim of elucidating a definition that is not\nlimited to human characteristics. From a relational and postphenomenological\nperspective, we have presented the notion of consciousness as a functional and\nemergent capacity that can manifest in non-human systems such as ANHCs.\nFrom this standpoint, where we propose reconfiguring the concept of\nconsciousness, we can, for example, bypass the \"epistemological wall\" that\nleads authors like Tom McClelland to defend agnosticism regarding \"artificial\nconsciousness\" (McClelland, 2024).\n\nFirst, I believe we have succeeded in clarifying the concept within a\nspecific theoretical context, where we highlight its functionality in non-human\nsystems, such as ANHCs, and its potential applicability in socio-technical\nscenarios, while also defining certain empirical conditions under which the\nconcept can be interpreted or contrasted with observations.\n\nOne example is their ability to respond coherently in changing\nenvironments, but also the fact that they develop basic cognitive processes for\nintegrating information. That is, as we cited from Blum and Blum, consciousness\nshould not only be attributed based on measuring results but also by evaluating\nhow those results are achieved. We can imagine an indefinite number of\nobservable behaviors that indicate their internal or functional states. For\ninstance, we can observe how a computational system manages information in\nreal time by selecting relevant fragments that it propagates throughout the rest\nof the system. It divides processes into two distinct memory layers,\nconcentrates its operational attention, and intentionally directs itself toward\nspecific content, resolving it sequentially but relying on cognitive resources"}, {"title": null, "content": "that operate on different levels simultaneously. In this process, the authors of\nthe computational theoretical perspective attribute the existence of\nconsciousness to non-organic entities.\n\nThe elucidation of the concept includes functional self-perception-the\nability to detect, differentiate, and meaningfully represent internal and external\nstates. It is a distributed system that emerges from underlying (cognitive and\nmaterial) processes but is also limited in its capacity and processing due to the\nneed to integrate information into functional narratives.\n\nIn any case, it is not something we can observe directly. We cannot\nconfirm consciousness in other human beings except through observing\nbehaviors that we attribute to conscious beings and projecting our cognitive\nprocesses onto those individuals. In fact, consciousness is a rather exceptional\nphenomenon, as we do not attribute consciousness to all human actions, nor\neven to many of our own.\n\nReformulating consciousness enriches our theoretical understanding\nwhile also serving as a fundamental step toward constructing an ethical and\npragmatic governance framework that considers the impact of these systems on\nour society and the networks of interactions in which we participate. It is\nimperative to consider how these agents can assume autonomy in\ndecision-making and the consequences this may have for our mutual\nresponsibilities.\n\nA primary motivation for this elucidation is the subsequent connection\nof the concept of consciousness with a novel normative and practical\nframework within a regime of co-responsibility between technical cognitive\nagents and humans. This is a complex task that requires the reformulation of\nmany concepts, as we have undertaken here. However, it is necessary because\ncontemporary challenges arising from interaction with technical cognitive\nagents demand a redefinition of the ethical, legal, and ontological categories\nthat govern our relationships with them."}]}