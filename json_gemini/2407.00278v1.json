{"title": "PerAct\u00b2: Benchmarking and Learning for\nRobotic Bimanual Manipulation Tasks", "authors": ["Markus Grotz", "Tamim Asfour", "Mohit Shridhar", "Dieter Fox"], "abstract": "Bimanual manipulation is challenging due to precise spatial and tempo-\nral coordination required between two arms. While there exist several real-world\nbimanual systems, there is a lack of simulated benchmarks with a large task diver-\nsity for systematically studying bimanual capabilities across a wide range of table-\ntop tasks. This paper addresses the gap by extending RLBench [1] to bimanual\nmanipulation. We open-source our code and benchmark, which comprises 13 new\ntasks with 23 unique task variations, each requiring a high degree of coordination\nand adaptability. To kickstart the benchmark, we extended several state-of-the-art\nmethods to bimanual manipulation and also present a language-conditioned be-\nhavioral cloning agent \u2013 PerAct2, an extension of the PerAct [2] framework.\nThis method enables the learning and execution of bimanual 6-DoF manipulation\ntasks. Our novel network architecture efficiently integrates language processing\nwith action prediction, allowing robots to understand and perform complex bi-\nmanual tasks in response to user-specified goals. The project website with code is\navailable at: http://bimanual.github.io", "sections": [{"title": "1 Introduction", "content": "Humans seamlessly manipulate and interact\nwith their environment using both hands. With\nboth hands, humans achieve greater efficiency\nthrough enhanced reachability and can solve\nmore sophisticated tasks. Despite the recent ad-\nvances in grasping and manipulation planning\n[3, 4] the investigation of bimanual manipula-\ntion remains an under-explored area, especially\nin terms of learning a manipulation policy. Un-\nlike tasks that require grasping or manipula-\ntion with a single hand, bimanual manipula-\ntion introduces a layer of complexity due to the\nneed for spatial and temporal coordination and\na deep understanding of the task at hand. This\ncomplexity is compounded by the dynamic na-\nture of real-world tasks, where the state of the\nenvironment and the objects within it are constantly changing, demanding continuous adjustment\nand coordination between both arms."}, {"title": "2 Related Work", "content": "Benchmarking Benchmark protocols\nand frameworks for robotic manipulation\nare designed around reproducibility and\nextensibility. For reinforcement learn-\ning, Fan et al. [7] introduce SURREAL to\nfoster reproducibility for learning robotic\nmanipulation tasks. Similarly, robosuite\n[8], bulletarm [9] and ManiSkill2 [10]\nprovide a standardized benchmark and\nlearning-environment for robotic manip-\nulation. While some of them have three\n(robosuite) or two (ManiSkill2) bimanual tasks, those are not sufficient to efficiently evaluate meth-\nods for bimanual manipulation. James et al. [1] present RLBench, a large scale benchmark and\nlearning-environment for robot learning alongside with baseline algorithms. A crucial aspect here\nis the automated waypoint-based dataset generation, removing the need of human demonstrations\nor by another baseline such as in [11]. However, no bimanual manipulation tasks were considered\nin RLBench. An issue that arises is the comparability, especially, in real world scenarios. RB2\n[12] aims to provide rankings for robotic manipulation tasks by pooling data across labs. Mittal\net al. [13] introduce Orbit, a framework with GPU acceleration and photo realistic scenes, to tackle\nreal-to-sim gap. Their work also includes a bimanual manipulation task, but this was not the main\nfocus of the work. Other works have a focus on dexterous bimanual hand-object manipulation, such\nas [14] or [15]. However, these works focus on the hand and neglect the use of a robotic arm. Last\nbut not least, instead of providing a set of standardized benchmarks in simulation, another way is"}, {"title": "3 Method", "content": "To benchmark, we extend RLBench [1] to the complex bimanual case by adding functionality and\ntasks for bimanual manipulation. We choose RLBench for several key advantages over other frame-\nworks, mainly its ability to generate training data with variations as well as the widespread accep-\ntance in the learning community. To kickstart the benchmark, we also present a bimanual behavioral\ncloning agent. Our method, called PerAct2, extends PerAct [2], which learns a single language-\nconditioned policy for unimanual manipulation actions. Fig. 2 illustrates the system architecture,\nwhich has been modified to accommodate the intricate coordination required between two robotic\narms while responding to language instructions."}, {"title": "3.1 RLBench2", "content": "RLBench is a robot learning benchmark suite featuring over more than 100 tasks to facilitate robot\nlearning, which is widely used in the community. Among task diversity, other key properties in-\nclude reproducibility and the ability to adapt to different learning strategies. We extend RLBench\nto bimanual manipulation, while keeping the functionality and its key properties. This allows us to\nquantify the success of our method and compare it with other baselines. Compared to unimanual\nmanipulation, bimanual manipulation is more challenging as it requires different kinds of coordi-\nnation and orchestration of the two arms. Therefore, we also provide task descriptions along with\nmetrics to benchmark performance and outline the key challenges of each task in Appendix A. For\nthe implementation side this makes it much more complex since synchronization is required when\ncontrolling both arms at the same time."}, {"title": "3.2 Task and Challenges", "content": "We introduce 13 bimanual manipulation tasks with different coupling, coordination, language in-\nstructions and manipulation skills. Fig. 3 illustrates these tasks, which range from instructions like\n\"push the box to the target area\" to \"put the bottle into the fridge\". Out of the tasks, eight are\nprehensile manipulation, three are non-prehensile manipulation, and two involve both. These tasks\nalso show different kind of coupling and coordination. For example, the task \u201clift the tray\u201d has to\nbe executed synchronously and both arms must be coordinated. Here, key distinguishing factors are the coupling as well as the\nrequired coordination between the two arms. We extended the classification in that we also distin-\nguish between physical coupling, i.e., if one arm exerts a force that could be measured by the other\narm. The benchmarking tasks differ in terms of complexity and the coordination required between\ntwo arms. Other attributes, such as the number of objects and the variation count, also affect the\ncomplexity of the task. All of these attributes influence the task complexity, and a rich and diverse\nset of tasks is required for both qualitative and quantitative benchmarking."}, {"title": "3.3 PerAct2", "content": "To address the challenges of bimanual tasks, we present a method for predicting bimanual actions,\nfollowing the framework established by [2]. Our model takes as input a 3D voxel grid, propriocep-"}, {"title": "3.4 Expert Demonstrations", "content": "Demonstrations consist of a set of action tuples for each robot arm that are executed simultaneously.\nFollowing previous works [2], we assume a dataset \\(D = \\{\u2081,..., n\\}\\) of n expert demonstra-\ntions, each paired with language goals \\(G = \\{11,12,...,In\\}\\). For the bimanual setup we assume\neach demonstration contains two actions. Thus, each demonstration (i is a sequence of continu-\nous actions \\(A \\{(a^r_1, a^l_1), (\u03b1^r_2, \u03b1^l_2), ...., (ax^r, a^l_x)\\}\\), where the superscripts r and I denote the right\nor the left robot arm. Each action a contains the 6-DoF pose, gripper open state, and whether the\nmotion-planner used collision avoidance to reach an intermediate pose: \\(a = \\{apose, Aopen, Acollide\\}\\)."}, {"title": "3.5 Keyframe Extraction", "content": "During the demonstrations salient keyframes are identified among the recorded visual and propri-\noceptive sensor data, which are used for training. Similar to prior work [2] and [30] for any given\ndemonstration \u2081 of length m we identify keyframes k\u2081 < k2 <..kn. We extend the heuristic to the\nbimanual case and define a keyframe if\n1. The gripper state of one of the robot has changed, or\n2. A robot reached the end of its executed trajectory, i.e., the pose of an end-effector is no\nlonger changing.\nThe discretisation of keyframe actions k facilitates the conceptualization of our BC agent's training\nparadigm as a classification task, specifically focusing on the identification of the 'next best action'."}, {"title": "3.6 Action Inference", "content": "Our goal is to learn bimanual action-centric representations [31] and retrieve a 6-DoF pose from a\nvoxel for each arm. Hence the 6-DoF pose is split into a translation, a rotation and a gripper state.\nTo this end, we utilize a 3-D voxel grid [32, 33] to represent both the observation and action space.\nThe advantage is that such a representation is view-point independent compared to ACT. The voxel\ngrid v is reconstructed from several RGB-D images \u00f5 and fused through triangulation \u00f5 \u21d2 v from\nknown camera extrinsics and intrinsics. To allow for a fine-grained action representation we use\n1003 voxels with size of 0.01 m to cover a workspace area of 1.0 m\u00b3.\nThe translation is identified as the voxel nearest to the gripper fingers' center. Rotation is quantified\ninto discrete 5\u00ba intervals for each rotational axis. The state of the gripper lopen, either open or\nclosed, is represented through a binary value. Similarly, the 'collide' parameter acollide is binary,\nand indicates if the motion-planner should avoid the voxel grid. This binary mechanism is pivotal\nfor enabling tasks that require both contact-based actions, like opening a drawer, and non-contact\nmaneuvers, such as reaching a handle without physical contact."}, {"title": "3.7 Training", "content": "We extend the loss function as in [2] to the bimanual setup and thus the loss function results in\n\\(Ltotal = \\sum_{iEx} C^{right}_i + \\sum_{iEx} E^{left}_i\\)\nwith X \u2208 {trans, rot, open, collide} and L\u2081 = Ey\u2081[logVi], where\n\\(Vtrans = softmax(Qtrans((x, y, z)|v,1))\\)\n\\(Vopen = softmax(Qopen(w|v))\\)\n\\(Vrot = softmax(Qrot((\\psi, \u03b8, \u03c6)|v, 1))\\)\n\\(Vcollide = softmax(Qcollide (K|v,1))\\)\nSimilar to PerAct, we also augment v and k with translation and rotation perturbations for robust-\nness, keeping other parameters, such as the optimizer, the same."}, {"title": "4 Evaluation", "content": "We study the efficacy of robotic bimanual manipulation. To this end, we compare our method\nagainst the following baselines. a.) ACT: A transformer network with action chunking that out-\nputs joint positions from camera inputs. b.) RVT-LF: Two Robotic View Transformer (RVT) as a\nleader-follower architecture. c.) PerAct-LF: Two Perceiver Actor networks as a leader-follower\narchitecture. d.) PerAct2: A single bimanual Perceiver Actor network as described in Section 3."}, {"title": "4.1 Simulation", "content": "We conduct our primary experiments in simulation\nfor reproducibility and benchmarking. The environ-\nment is similar to [2]. RGB-D sensors are positioned\nat the front, left shoulder, right shoulder, and on the\nwrist. All cameras are noiseless and have a resolu-\ntion of 256 \u00d7 256. The increase in image resolution\nensures future comparability with other methods that\nrequire it.\nWe trained all tasks individually, as this allows for a\nmore refined analysis and different coordination types\ncan be distinguished. We also note that while it is pos-\nsible to train multi-task agents, not all methods accommodate this setting. We used 100 demonstra-\ntions for each task. All single tasks were trained on a NVIDIA A40 GPUs for up to 100k iterations.\nFor each method, the batch size was maximized to fit into the GPU memory. Every 10k-th check-\npoint was evaluated with 100 episodes and the best checkpoint was finally evaluated on a separate\ntest set.\nThe difference in the success rate of image-based methods, such as ACT and RVT can be explained\nby the symmetry in the tasks. For example the both robot arms are the exact same model making\nit difficult to distinguish between them. Furthermore, other challenges for ACT include that the\ndemonstrations can have high variations due to the randomization of the spawned objects or the\ngenerated motion planning paths. Both aspects are challenging because ACT predicts joint angles\nand not a 6-DoF pose."}, {"title": "4.2 Discussion of Failure Types", "content": "In the following, we will briefly discuss common failures. During the handover task, a robot may ex-\nperience a collision with another robot due to inadequate spatial awareness or timing errors. Grasp-"}, {"title": "4.3 Real-World", "content": "To also demonstrated that the framework is robot-agnostic, the method has been integrated into the\nhumanoid robot ARMAR-6 [34, 35]. The robot is equipped with an Azure Kinect RGB-D sensor\nwith a resolution of 1920 \u00d7 1080 and thus for the experiments only a single camera is used, i.e.,\n\u00d5real = {Ofront}. The voxel size is set to 50 \u00d7 50 \u00d7 50 to speed up training, but reduces accuracy.\nA Cartesian waypoint controller as used in [36] moves the end-effector to the predicted targets. For\neach task a single demonstration \u2081 is recorded using Kinesthetic teaching instead of VR. Overall,\nfour different tasks have been demonstrated to the robot. Three of the tasks require a synchronous\ncoordination of both arms, such as \"lifting a bowl\" or \"pushing a chair\". The fourth task, \"put\naway the tool\", requires spatial coordination. While it is possible to quantify results in real world,\nreproducing them is challenging due to a lack of hardware as well as to object poses and sensor\nnoise. ROS2 and integration with other robot software architectures will be\navailable on the website."}, {"title": "5 Conclusion", "content": "In this work, we presented a robotic manipulation benchmark specifically designed for bimanual\nrobotic manipulation tasks. We open-source 13 new tasks with 23 unique task variations, each\nrequiring a high degree of coordination and adaptability. We extended two existing methods to\nbimanual manipulation and run them with another method on the benchmark. Additionally, we\npresented PerAct\u00b2\u2013 a perceiver-actor agent for bimanual manipulation. Due to the architecture\ndesign the method can easily be transferred to other robots, such as a humanoid, as the policy\noutputs a 6-DoF pose and control is separated. Our investigation reveals that our method is the most\nsuccessful in 9 out of 13 tasks and performs effectively in real-world settings while also having the\nfastest training time. For the average task success rate, both PerAct-LF and PerAct2 outperformed\nimage-based methods, achieving average task success rates of 17.5% and 16.8%, respectively.\nLimitations and Future Work None of the methods is able to achieve a sufficiently high success\nrate, which can be explained by the complexity of bimanual manipulation. Another limiting factor\nis that methods using discretized actions rely on a sampling-based motion planner for successful ex-\necution. We acknowledge that tasks in the benchmarks are challenging, and we are looking forward\nfor the community to pick up on this challenge. Future work will focus on extending the benchmark\nby adding more state-of-the-art methods and including mobile manipulation."}, {"title": "A Description of Tasks", "content": "To model the complexity of a task we report in  on the average time length of the demon-\nstrations, the number of identified keyframes, number of items and the number of task variations\nKeyframes are discrete frames in a continuous stream of data, and the number of keyframes is a\nmeasure of the number of actions necessary to complete a task."}, {"title": "(a) push box", "content": "Task Description: The robot's task is to push a heavy\nbox using both arms to move it to a designated target area.\nSuccess Metric: The task is considered successfully\ncompleted when the box reaches the targetarea.\nObjects: The task involves a large box and a target area.\nCoordination Challenges: The primary challenge lies in\nthe weight of the box, which is set to 50 kg, making it\nvery difficult for a single arm to push.\nNB: This task cannot be solved with one robot due to the weight of the box.\nLanguage Instructions: Push the box to the red area."}, {"title": "(b) lift a ball", "content": "Task Description: The robot's task is to grasp and lift a\nlarge ball using both arms.\nSuccess Metric: The task is considered successfully\ncompleted when the ball is lifted to a height above 0.95 m.\nObjects: The task involves a large ball.\nCoordination Challenges: The primary challenge in-\nvolves coordinated non-prehensile manipulation, as the\nball cannot be grasped by the gripper. This requires care-\nful coordination during the lifting motion."}, {"title": "(c) push two buttons", "content": "Task Description: The robot's task is to push two out of\nthree buttons in an environment where the colors of the\nbuttons are randomized. The goal is to press two specified\nbuttons at the same time.\nSuccess Metric: The task is considered successfully\ncompleted when both specified buttons are pressed simul-\ntaneously.\nObjects: The task involves three buttons with different\ncolors and a differently colored base.\nCoordination Challenges: The primary challenge is the synchronous button press. The random-\nization of the button colors adds complexity compared to standard tasks.\nNB: This task is impossible to solve with one robot as two buttons need to be pressed simultaneously.\nLanguage Instructions: Push the (color A) and the (color B) button."}, {"title": "(d) pick up a plate", "content": "Task Description: The robot's task is to pick up a plate\nthat is placed on a table. This involves grasping the plate\nand lifting it.\nSuccess Metric: The task is considered successfully\ncompleted when the robot has securely grasped the plate\nand lifted it.\nObjects: The task involves a single plate.\nCoordination Challenges: The main challenges involve\nnon-prehensile manipulation, as well as coordination during the lifting motion. The plate must be\nhandled delicately to avoid slipping or tilting, which requires precise control.\nNB: This task is difficult to solve with one robot because the plate's flat and smooth surface makes\nit hard to grasp securely with a single gripper. The coordination required to lift the plate without\ntilting or dropping it is challenging for one robot arm.\nLanguage Instructions: Pick up the plate."}, {"title": "(e) put item in drawer", "content": "Task Description: The robot's task is to open a specific\ndrawer in a cupboard and place an item into it. The cor-\nrect drawer must be identified and opened before the item\ncan be placed inside.\nSuccess Metric: The task is considered successfully\ncompleted when the item is placed inside the specified\ndrawer.\nObjects: The task involves an item and a cupboard with\nthree drawers."}, {"title": "(f) put bottle in fridge", "content": "Task Description: The robot's task is to put a bottle into\nthe fridge. This requires opening the fridge door, grasping\nthe bottle, and placing it inside the fridge.\nSuccess Metric: The task is considered successfully\ncompleted when the bottle is placed inside the fridge.\nObjects: The task involves a bottle and a fridge.\nCoordination Challenges: The primary challenges in-\nclude: - The fridge needs to be opened first. - The bottle\nis difficult to grasp. - Collision with the fridge needs to be avoided. - Reachability is an issue as\neither the bottle or the fridge door can only be reached by one robot.\nObjects: A bottle and a fridge\nNB: This task requires two robots due to reachability issues.\nLanguage Instructions: Put the bottle into the fridge."}, {"title": "(g) handover an item", "content": "Task Description: The robot's task is to hand over the\ncolor item. This involves identifying the correct item\nbased on its color, grasping it, and lifting it to the required\nheight.\nSuccess Metric: The task is considered successfully\ncompleted when the robot has securely grasped the cor-\nrect item and lifted it to a height of 80 cm, while the other\narm remains idle.\nObjects: The task involves five items with different colors.\nCoordination Challenges: The main challenge lies in correctly identifying the item based on its\ncolor as specified in the task description, and then coordinating the handover process.\nNB: There are variations of this task: one with only three items instead of five, and a simpler task\nthat involves only a block instead of colored cubes.\nLanguage Instructions: Hand over the (red, green, blue, yellow) item."}, {"title": "(h) pick up notebook", "content": "Task Description: The robot's task is to pick up a note-\nbook that is placed on top of a block. This requires\nthe robot to first manipulate the notebook into a position\nwhere it can be grasped.\nSuccess Metric: The task is considered successfully\ncompleted when the robot has securely grasped the note-\nbook and lifted it off the block."}, {"title": "(i) straighten rope", "content": "Task Description: The robot's task is to straighten a rope\nby manipulating it so that both ends are placed into dis-\ntinct target areas.\nSuccess Metric: The task is considered successfully\ncompleted when both ends of the rope are positioned\nwithin their respective target areas.\nObjects: The task involves a single object: a rope.\nCoordination Challenges: The main challenge involves\nhandling a deformable object, which requires the robot to grasp and manipulate the rope simultane-\nously at different points to achieve the desired straightening.\nLanguage Instructions: Straighten the rope."}, {"title": "(j) sweep dust pan", "content": "Task Description: The robot's task is to sweep the dust\ninto the dust pan using a broom. This involves coordinat-\ning the sweeping motion to ensure the dust is effectively\ncollected.\nSuccess Metric: The task is considered successfully\ncompleted when all the dust is inside the dust pan.\nObjects: The task involves several objects: a broom, a\ndust pan, supporting objects, and dust.\nCoordination Challenges: The main challenge lies in executing the sweeping motion accurately to\nensure that the dust is directed into the dust pan.\nLanguage Instructions: Sweep the dust to the pan."}, {"title": "(k) lift tray", "content": "Task Description: The robot's task is to lift a tray that is\nplaced on a holder. An item is on top of the tray and must\nbe balanced while both arms lift the tray.\nSuccess Metric: The task is considered successfully\ncompleted when both the tray and the item on top reach a\nheight above 1.2 m.\nObjects: The task involves a tray, a holder, and an item.\nCoordination Challenges: The primary challenge lies in\ncoordinating the lifting motion with both arms to maintain the balance of the item on the tray. This\ntask cannot be accomplished with only one arm."}, {"title": "(l) handover item (easy)", "content": "Task Description: The robot's task is to hand over a red\nitem. One robotic arm must grasp the red item while the\nother arm remains free and wait for the handover\nSuccess Metric: The task is considered successfully\ncompleted when the robot has securely grasped the cor-\nrect item and lifted it to a height of 80 cm, while the other\narm remains idle and has not grasped anything.\nObjects: The task involves a red block.\nCoordination Challenges: The primary challenge lies in coordinating the handover process.\nNB: There is also a more complex variant of this task that involves handling multiple objects of\ndifferent shapes and sizes. Refer to the \"handover item\u201d task for details.\nLanguage Instructions: Handover the item."}, {"title": "(m) take tray out of oven", "content": "Task Description: The robot's task is to remove a tray\nthat is located inside an oven. This involves opening the\noven door and then grasping the tray.\nSuccess Metric: The task is considered successfully\ncompleted when the tray is lifted above the oven.\nObjects: The task involves a tray inside an oven.\nCoordination Challenges: The primary challenge lies in\nopening the oven door to make the tray graspable.\nNB: This task can be solved with only one arm.\nLanguage Instructions: Take tray out of oven."}]}