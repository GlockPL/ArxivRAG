{"title": "PerAct\u00b2: Benchmarking and Learning for Robotic Bimanual Manipulation Tasks", "authors": ["Markus Grotz", "Tamim Asfour", "Mohit Shridhar", "Dieter Fox"], "abstract": "Bimanual manipulation is challenging due to precise spatial and tempo- ral coordination required between two arms. While there exist several real-world bimanual systems, there is a lack of simulated benchmarks with a large task diver- sity for systematically studying bimanual capabilities across a wide range of table- top tasks. This paper addresses the gap by extending RLBench [1] to bimanual manipulation. We open-source our code and benchmark, which comprises 13 new tasks with 23 unique task variations, each requiring a high degree of coordination and adaptability. To kickstart the benchmark, we extended several state-of-the-art methods to bimanual manipulation and also present a language-conditioned be- havioral cloning agent \u2013 PerAct2, an extension of the PerAct [2] framework. This method enables the learning and execution of bimanual 6-DoF manipulation tasks. Our novel network architecture efficiently integrates language processing with action prediction, allowing robots to understand and perform complex bi- manual tasks in response to user-specified goals. The project website with code is available at: http://bimanual.github.io", "sections": [{"title": "1 Introduction", "content": "Humans seamlessly manipulate and interact with their environment using both hands. With both hands, humans achieve greater efficiency through enhanced reachability and can solve more sophisticated tasks. Despite the recent ad- vances in grasping and manipulation planning [3, 4] the investigation of bimanual manipula- tion remains an under-explored area, especially in terms of learning a manipulation policy. Un- like tasks that require grasping or manipula- tion with a single hand, bimanual manipula- tion introduces a layer of complexity due to the need for spatial and temporal coordination and a deep understanding of the task at hand. This complexity is compounded by the dynamic na- ture of real-world tasks, where the state of the environment and the objects within it are constantly changing, demanding continuous adjustment and coordination between both arms."}, {"title": "2 Related Work", "content": "Benchmarking Benchmark protocols and frameworks for robotic manipulation are designed around reproducibility and extensibility. For reinforcement learn- ing, Fan et al. [7] introduce SURREAL to foster reproducibility for learning robotic manipulation tasks. Similarly, robosuite [8], bulletarm [9] and ManiSkill2 [10] provide a standardized benchmark and learning-environment for robotic manip- ulation. While some of them have three (robosuite) or two (ManiSkill2) bimanual tasks, those are not sufficient to efficiently evaluate meth- ods for bimanual manipulation. James et al. [1] present RLBench, a large scale benchmark and learning-environment for robot learning alongside with baseline algorithms. A crucial aspect here is the automated waypoint-based dataset generation, removing the need of human demonstrations or by another baseline such as in [11]. However, no bimanual manipulation tasks were considered in RLBench. An issue that arises is the comparability, especially, in real world scenarios. RB2 [12] aims to provide rankings for robotic manipulation tasks by pooling data across labs. Mittal et al. [13] introduce Orbit, a framework with GPU acceleration and photo realistic scenes, to tackle real-to-sim gap. Their work also includes a bimanual manipulation task, but this was not the main focus of the work. Other works have a focus on dexterous bimanual hand-object manipulation, such as [14] or [15]. However, these works focus on the hand and neglect the use of a robotic arm. Last but not least, instead of providing a set of standardized benchmarks in simulation, another way is"}, {"title": "3 Method", "content": "To benchmark, we extend RLBench [1] to the complex bimanual case by adding functionality and tasks for bimanual manipulation. We choose RLBench for several key advantages over other frame- works, mainly its ability to generate training data with variations as well as the widespread accep- tance in the learning community. To kickstart the benchmark, we also present a bimanual behavioral cloning agent. Our method, called PerAct2, extends PerAct [2], which learns a single language- conditioned policy for unimanual manipulation actions. Fig. 2 illustrates the system architecture, which has been modified to accommodate the intricate coordination required between two robotic arms while responding to language instructions."}, {"title": "3.1 RLBench2", "content": "RLBench is a robot learning benchmark suite featuring over more than 100 tasks to facilitate robot learning, which is widely used in the community. Among task diversity, other key properties in- clude reproducibility and the ability to adapt to different learning strategies. We extend RLBench to bimanual manipulation, while keeping the functionality and its key properties. This allows us to quantify the success of our method and compare it with other baselines. Compared to unimanual manipulation, bimanual manipulation is more challenging as it requires different kinds of coordi- nation and orchestration of the two arms. Therefore, we also provide task descriptions along with metrics to benchmark performance and outline the key challenges of each task in Appendix A. For the implementation side this makes it much more complex since synchronization is required when controlling both arms at the same time."}, {"title": "3.2 Task and Challenges", "content": "We introduce 13 bimanual manipulation tasks with different coupling, coordination, language in- structions and manipulation skills. Fig. 3 illustrates these tasks, which range from instructions like \"push the box to the target area\" to \"put the bottle into the fridge\". Out of the tasks, eight are prehensile manipulation, three are non-prehensile manipulation, and two involve both. These tasks also show different kind of coupling and coordination. For example, the task \u201clift the tray\u201d has to be executed synchronously and both arms must be coordinated. Tab. 2 classifies the tasks according to the bimanual taxonomy of [29]. Here, key distinguishing factors are the coupling as well as the required coordination between the two arms. We extended the classification in that we also distin- guish between physical coupling, i.e., if one arm exerts a force that could be measured by the other arm. The benchmarking tasks differ in terms of complexity and the coordination required between two arms. Other attributes, such as the number of objects and the variation count, also affect the complexity of the task. All of these attributes influence the task complexity, and a rich and diverse set of tasks is required for both qualitative and quantitative benchmarking."}, {"title": "3.3 PerAct2", "content": "To address the challenges of bimanual tasks, we present a method for predicting bimanual actions, following the framework established by [2]. Our model takes as input a 3D voxel grid, propriocep-"}, {"title": "3.4 Expert Demonstrations", "content": "Demonstrations consist of a set of action tuples for each robot arm that are executed simultaneously. Following previous works [2], we assume a dataset \\(D = \\{1, 2, ..., n\\}\\) of n expert demonstra- tions, each paired with language goals \\(G = \\{l_1, l_2, ..., l_n\\}\\). For the bimanual setup we assume each demonstration contains two actions. Thus, each demonstration \\(\\zeta_i\\) is a sequence of continu- ous actions \\(A = \\{(a_r^1, a_l^1), (a_r^2, a_l^2), ...., (a_r^{k_i}, a_l^{k_i})\\}\\), where the superscripts r and l denote the right or the left robot arm. Each action a contains the 6-DoF pose, gripper open state, and whether the motion-planner used collision avoidance to reach an intermediate pose: \\(a = \\{a_{pose}, a_{open}, a_{collide}\\}\\)."}, {"title": "3.5 Keyframe Extraction", "content": "During the demonstrations salient keyframes are identified among the recorded visual and propri- oceptive sensor data, which are used for training. Similar to prior work [2] and [30] for any given demonstration \\(\\zeta_i\\) of length m we identify keyframes \\(k_1 < k_2 <..k_n\\). We extend the heuristic to the bimanual case and define a keyframe if\n1. The gripper state of one of the robot has changed, or\n2. A robot reached the end of its executed trajectory, i.e., the pose of an end-effector is no longer changing.\nThe discretisation of keyframe actions k facilitates the conceptualization of our BC agent's training paradigm as a classification task, specifically focusing on the identification of the 'next best action'."}, {"title": "3.6 Action Inference", "content": "Our goal is to learn bimanual action-centric representations [31] and retrieve a 6-DoF pose from a voxel for each arm. Hence the 6-DoF pose is split into a translation, a rotation and a gripper state. To this end, we utilize a 3-D voxel grid [32, 33] to represent both the observation and action space. The advantage is that such a representation is view-point independent compared to ACT. The voxel grid v is reconstructed from several RGB-D images \\(\\tilde{o}\\) and fused through triangulation \\(\\tilde{o} \\Rightarrow v\\) from known camera extrinsics and intrinsics. To allow for a fine-grained action representation we use \\(100^3\\) voxels with size of 0.01 m to cover a workspace area of 1.0 m\\(^3\\).\nThe translation is identified as the voxel nearest to the gripper fingers' center. Rotation is quantified into discrete 5\u00ba intervals for each rotational axis. The state of the gripper \\(a_{open}\\), either open or closed, is represented through a binary value. Similarly, the 'collide' parameter \\(a_{collide}\\) is binary, and indicates if the motion-planner should avoid the voxel grid. This binary mechanism is pivotal for enabling tasks that require both contact-based actions, like opening a drawer, and non-contact maneuvers, such as reaching a handle without physical contact."}, {"title": "3.7 Training", "content": "We extend the loss function as in [2] to the bimanual setup and thus the loss function results in\n\\[\\mathcal{L}_{total} = \\sum_{i \\in x} \\mathcal{L}_i^{right} + \\sum_{i \\in x} \\mathcal{L}_i^{left}\\]\nwith X \u2208 {trans, rot, open, collide} and \\(L_i = E_{y_i}[logV_i]\\), where\n\\[V_{trans} = softmax(Q_{trans}((x, y, z)|v, l))\\]\n\\[V_{open} = softmax(Q_{open}(w|v))\\]\n\\[V_{rot} = softmax(Q_{rot}((\\psi, \\theta, \\phi)|v, l))\\]\n\\[V_{collide} = softmax(Q_{collide}(k|v))\\]\nSimilar to PerAct, we also augment v and k with translation and rotation perturbations for robust- ness, keeping other parameters, such as the optimizer, the same."}, {"title": "4 Evaluation", "content": "We study the efficacy of robotic bimanual manipulation. To this end, we compare our method against the following baselines. a.) ACT: A transformer network with action chunking that out- puts joint positions from camera inputs. b.) RVT-LF: Two Robotic View Transformer (RVT) as a leader-follower architecture. c.) PerAct-LF: Two Perceiver Actor networks as a leader-follower architecture. d.) PerAct2: A single bimanual Perceiver Actor network as described in Section 3."}, {"title": "4.1 Simulation", "content": "We conduct our primary experiments in simulation for reproducibility and benchmarking. The environ- ment is similar to [2]. RGB-D sensors are positioned at the front, left shoulder, right shoulder, and on the wrist. All cameras are noiseless and have a resolu- tion of 256 \u00d7 256. The increase in image resolution ensures future comparability with other methods that require it.\nWe trained all tasks individually, as this allows for a more refined analysis and different coordination types can be distinguished. We also note that while it is pos- sible to train multi-task agents, not all methods accommodate this setting. We used 100 demonstra- tions for each task. All single tasks were trained on a NVIDIA A40 GPUs for up to 100k iterations. For each method, the batch size was maximized to fit into the GPU memory. Every 10k-th check- point was evaluated with 100 episodes and the best checkpoint was finally evaluated on a separate test set. Tab. 3 lists the task success rate for a single-task agents for each individual task.\nThe difference in the success rate of image-based methods, such as ACT and RVT can be explained by the symmetry in the tasks. For example the both robot arms are the exact same model making it difficult to distinguish between them. Furthermore, other challenges for ACT include that the demonstrations can have high variations due to the randomization of the spawned objects or the generated motion planning paths. Both aspects are challenging because ACT predicts joint angles and not a 6-DoF pose."}, {"title": "4.2 Discussion of Failure Types", "content": "In the following, we will briefly discuss common failures. During the handover task, a robot may ex- perience a collision with another robot due to inadequate spatial awareness or timing errors. Grasp-"}, {"title": "4.3 Real-World", "content": "To also demonstrated that the framework is robot-agnostic, the method has been integrated into the humanoid robot ARMAR-6 [34, 35]. The robot is equipped with an Azure Kinect RGB-D sensor with a resolution of 1920 \u00d7 1080 and thus for the experiments only a single camera is used, i.e., \\(\\tilde{o}_{real} = \\{o_{front}\\}\\). The voxel size is set to 50 \u00d7 50 \u00d7 50 to speed up training, but reduces accuracy. A Cartesian waypoint controller as used in [36] moves the end-effector to the predicted targets. For each task a single demonstration \\(\\zeta_i\\) is recorded using Kinesthetic teaching instead of VR. Overall, four different tasks have been demonstrated to the robot. Three of the tasks require a synchronous coordination of both arms, such as \"lifting a bowl\" or \"pushing a chair\". The fourth task, \"put away the tool\", requires spatial coordination. While it is possible to quantify results in real world, reproducing them is challenging due to a lack of hardware as well as to object poses and sensor noise. Fig. 4 shows the tasks for the real-world experiments. A video showing the experiments is available on the website. ROS2 and integration with other robot software architectures will be available on the website."}, {"title": "5 Conclusion", "content": "In this work, we presented a robotic manipulation benchmark specifically designed for bimanual robotic manipulation tasks. We open-source 13 new tasks with 23 unique task variations, each requiring a high degree of coordination and adaptability. We extended two existing methods to bimanual manipulation and run them with another method on the benchmark. Additionally, we presented PerAct2\u2013 a perceiver-actor agent for bimanual manipulation. Due to the architecture design the method can easily be transferred to other robots, such as a humanoid, as the policy outputs a 6-DoF pose and control is separated. Our investigation reveals that our method is the most successful in 9 out of 13 tasks and performs effectively in real-world settings while also having the fastest training time. For the average task success rate, both PerAct-LF and PerAct2 outperformed image-based methods, achieving average task success rates of 17.5% and 16.8%, respectively."}, {"title": "Limitations and Future Work", "content": "None of the methods is able to achieve a sufficiently high success rate, which can be explained by the complexity of bimanual manipulation. Another limiting factor is that methods using discretized actions rely on a sampling-based motion planner for successful ex- ecution. We acknowledge that tasks in the benchmarks are challenging, and we are looking forward for the community to pick up on this challenge. Future work will focus on extending the benchmark by adding more state-of-the-art methods and including mobile manipulation."}]}