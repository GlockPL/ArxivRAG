{"title": "WordVIS: A Color Worth A Thousand Words", "authors": ["Umar Khan", "Saifullah", "stefan agne", "andreas dengel", "Sheraz Ahmed"], "abstract": "Document classification is considered a critical element in automated document processing systems. In recent years multi-modal approaches have become increasingly popular for document classification. Despite their improvements, these approaches are underutilized in the industry due to their requirement for a tremendous volume of training data and extensive computational power. In this paper, we attempt to address these issues by embedding textual features directly into the visual space, allowing lightweight image-based classifiers to achieve state-of-the-art results using small-scale datasets in document classification. To evaluate the efficacy of the visual features generated from our approach on limited data, we tested on the standard dataset Tobacco-3482. Our experiments show a tremendous improvement in image-based classifiers, achieving an improvement of 4.64% using ResNet50 with no document pre-training. It also sets a new record for the best accuracy of the Tobacco-3482 dataset with a score of 91.14% using the image-based DocXClassifier with no document pre-training. The simplicity of the approach, its resource requirements, and subsequent results provide a good prospect for its use in industrial use cases.", "sections": [{"title": "1 Introduction", "content": "Documents play an integral role in modern business communication and record keeping. As a result, there is a growing interest today in the development of automated document processing pipelines in business workflows [1, 22, 27, 29]. Document classification is a common starting point for many of these pipelines. Early classification of documents not only facilitates filtering, searching, and retrieval, but also enhances downstream performance [1, 10, 18]. For example, if it is possible to categorize a specific class with a high degree of confidence, an efficient information extraction module may be developed particularly for that class, thereby enhancing the pipeline's overall efficiency. Due to its fundamental importance, document classification has been extensively studied over the past few decades [1,7,11,21,27] and has been widely adopted by the industry."}, {"title": "2 Related Work", "content": "Deep learning has been extensively studied in recent years in the context of document classification, resulting in a variety of approaches both in the image domain and in the multimodal domain. As a result, the task of building a high-performance document classifier is no longer considered arduous. As long as sufficient computing resources and training data are available, there are numerous state-of-the-art approaches [4, 15, 24, 25,27] that can be directly applied to both large and small datasets to produce exceptional results in document classification. Especially interesting are the recent multimodal approaches which use both visual and textual features to perform the classification task and are particularly successful at countering the problem of high inter-class similarity and intra-class variance commonly found in documents [16, 18]. However, there are several challenges involved with these approaches from a deployment perspective. Firstly, many of these approaches involve multiple streams of networks [4,23] or large multimodal transformer networks [24,27], which greatly increase the computational load. Additionally, such models are particularly challenging to train, since they require self-supervised learning across millions of data points [27,28]. This can be especially problematic for small businesses, which have limited computing resources or training data, making it difficult to deploy such approaches in a practical manner. In addition to these issues, most existing multimodal approaches require feeding the textual and layout information directly to the model [4, 27, 28], which may require overhauling an existing document processing system. Finally, such multimodal techniques can also be difficult to extend to new languages and require additional training data for each target language.\nIn this paper, we attempt to counter the aforementioned issues in multimodal approaches and present a lightweight approach for document classification that utilizes both visual and textual features of a document image without the need for any kind of self-supervised pretraining. Contrary to most existing multi-modal approaches, we embed the textual semantic features and context directly into the visual space of a document by assigning an RGB color to each word in accordance with the similarity of different letters. In this manner, our approach not only allows existing image-based classifiers to directly exploit the textual cues of a document but also substantially reduces the performance overhead associated with the processing of multi-modal data. In addition, it requires no additional pretraining to learn the textual embeddings as in the case of typical multi-modal approaches, making it particularly suitable for small datasets. Due to the simplicity of our approach, not only can it easily be integrated into existing CNN-based document classification pipelines but can also be directly extended for any new languages without the need for additional language-specific data, as is the case with most transformer-based approaches. The overall contributions of our paper can be summarized as follows:\nWe present a novel approach for embedding textual semantic and contextual features in the visual space of a document. This enables training high-performing document classifiers in data-scarce settings.\nFor the ablation study, we evaluate our approach with multiple CNN-based architectures on a small-scale document benchmark dataset Tobacco-3482"}, {"title": "2.1 Document Image Classification", "content": "The field of document image classification has evolved considerably over the past few decades. The early works in the field were primarily based on structural similarities between documents [26], feature matching [21], or their combination [9]. In a different direction, classical machine learning-based approaches such as K-Nearest Neighbors (KNN) [6], Decision Trees [7] and Hidden Markov Models (HMM) [14] have also been explored for this task. A detailed overview of these approaches can be found in the survey paper by Chen et al. [8].\nIt was not long after the seminal work by Krizhevsky et al. [20] in which the popular AlexNet architecture was introduced for natural image classification, that a range of deep-learning based document classification systems were introduced. Kang et al. [18] were the first to demonstrate the effectiveness of a neural network for document classification, which was significantly more successful compared to classical approaches. Later, Afzal investigated the use of much deeper CNNs for the classification of documents, as well as the advantages of transfer learning for document classification. Since then, this trend has continued steadily with the use of newer versions and variations of CNNs for the classification of documents [15,25]. As CNN-based approaches have started to reach diminishing returns for this task, there has also been a growing interest in multi-modal approaches that combine the image, layout, and textual information of the document to perform the classification task. These techniques are generally implemented either in a multi-stream fashion which uses separate streams for visual, textual or other layout features [4,23] or based on multimodal transformer architectures [27,28] that are trained in a self-supervised manner on millions of training samples."}, {"title": "2.2 Visual Encoders for Textual Information", "content": "In this section, we review some previous works for encoding textual semantics or contextual information into visual space in document analysis. Anoop et al. [19] proposed Chargrid, a grid-based color coding scheme for document images where the bounding box region of each character is colorized on a separate image mask based on a predetermined encoding scheme. The original image and its corresponding colorized image mask were then used to train a neural network"}, {"title": "3 WordVis: The Proposed Approach", "content": "WordVIS is a novel pre-processing method that generates enhanced document representations using OCR data. It generates enriched visual representation by encoding text to colors using a score lookup table and applies color masks on words using the original document image. The scoring mechanism was inspired by the fundamental concept of string metric calculation from information theory called \"Levenshtein Distance\". We leverage the core concept of how the distance is measured in strings. In this section, we describe the process of generating these document representations using WordVIS."}, {"title": "3.1 Score Assignment", "content": "Considering all the characters Ne in the language characters space, the limit on supported characters is $lim_{1\\rightarrow\\infty} N_c$. However, each character in the character space will be assigned a score Cs representing the weightage of that character. The limit for the weightage of a character can be defined as $lim_{0-255} C_s$, as the maximum score possible for the $\\sum_{=0}^{=} C_s$ is 255 per channel as that's the maximum value possible in the individual channels of the RGB representation of the color. Given the target dataset is Tobacco-3482, all references in this research publication will point to the use of English language characters. However, it is to be noted that the method itself is language agnostic."}, {"title": "Purpose", "content": "The score assignment process is about assigning weightage Cs to individual characters. This allows the users of the system to leverage weights on individual characters depending on their semantic understanding of their dataset and the need for it. This weightage score forms the basis for the RGB number of the word. If the weights are well distributed on the maximum aforementioned range would give us a unique color for all possible words, as the maximum possible combinations using this scheme are 16777216 which is a far greater number than all possible words in the English language."}, {"title": "Distribution Scheme", "content": "The method is flexible to the color score distribution scheme adopted by the users in accordance with the desired output color range. As the number of maximum possible scores per individual character is $N_C = 3$ one each for the R,G and B channels, the coloring scheme can be based on a single character having a score in all three channels, two out of three or a single channel."}, {"title": "Experiment Scores", "content": "For our experiments, there are 26 English language characters and 10 digits taking our Nc = 36. The assignment operation can be both generic and very targeted towards a specific dataset leveraging the semantic understanding of that dataset. However, for a more fair comparison of the standard dataset, we have chosen to assign more generic scores to the characters of the English language without the use of special characters or semantic understanding of the test dataset. Our range of weights for our experiments is $lim_{1\\rightarrow9} C_s$ however, we believe they can be used to further enhance results with little insights into the dataset. Our experimentation scoring scheme is the division of all available characters between Red, Green, and Blue channels with each channel having unique 12 characters. Our score assignment per character is generic in ascending order for all characters and without the use of special characters for avoiding any semantic encoding using the dataset knowledge."}, {"title": "3.2 Multiplying Factor M", "content": "In order to bring a weight on the length of the word that occurs in the text corpus, we introduced a multiplying factor Mf based on the length of the word."}, {"title": "3.3 Calculating RGB Values", "content": "The formula that is used for converting the individual scores to RGB is given below in Fig. 1 and Fig. 2\n$R = \\Sigma_{C=a}^{C=i} C_s * M_f, G = \\Sigma_{C=j}^{C=r} C_s * M_f, B = \\Sigma_{C=s}^{C=z} C_s * M_f$ (1)\n$RGB_{color} = (R,G,B)$ (2)\nWhereas C's is the score of the individual character and Mf is the multiplying factor derived from $Word_{length}$."}, {"title": "Example", "content": "The above algorithm can be more fairly understood from the following calculation example: Given the word \"deep\", the RGB score according to the above example would be\n$Word_{length} = 4 = M$\n$R_{score} = ((d=3) * 4) + ((e=5) * 4) + ((e=5) * 4) = 52$\n$G_{score} = ((p=7) * 4) = 28$\n$B_{score} = ((0) * 4) = 0$\n$RGB_{score} = (52,28,0)$\nThis example highlights a property of WordVIS, where small errors in OCR are limited by the constraints put on the scoring mechanism leading to more consistent colors even in case of errors, to further continue the previous example: If the given word \"deep\" was incorrectly OCR'ed as \"deeq\", the resulting change would be minimal as shown in the calculation below. $Word_{length} = 4 = M$\n$R_{score} = ((d = 3) * 4) + ((e = 5) * 4) + ((e = 5) * 4) = 52$\n$G_{score} = ((p = 8) * 4) = 32$\n$B_{score} = ((0) * 4) = 0$\n$RGB_{color} = (52, 32, 0)$"}, {"title": "3.4 Output and Sample Analysis", "content": "The following samples shows the resulting documents of several classes from the WordVIS visual embedding on Tobacco-3482 documents:\nFig. 2 shows that the images are pre-processed utilizing varying degrees of colors using the scoring mechanism. Through further inspection, we notice that most of the stop words due to them containing the same letters repeating, contain the same colors and non-stop words tend to take more different colors than the stop words. This property can be observed in all classes, from our Fig. 3 in class ADVE it can be observed that words like \"you, are, the, and, for, new, what\" all have green hues, whereas words such as \"satisfying, blended, taste, medium, smooth\" all take sharper and different tones of colors."}, {"title": "4 Experiments and Results", "content": "To evaluate the efficacy of our proposed method we trained several architectures on the standard dataset of Tobacco-3482. The smaller dataset was used in order to test for the performance gain on small limited amounts of datasets. We trained all these architectures on two different versions of the dataset.\nStandard Tobaccoo-3482 dataset\nWordVIS Colorized Tobaccoo-3482 dataset"}, {"title": "4.1 Dataset", "content": "The dataset consists of a total of 3482 documents split across 10 classes. The splits were generated using random 80% - 20% train-test split resulting in 700 test documents. The validation split was 10% of the training data resulting in 2504, 278, 700 samples in Train, Validation, and Test respectively. Moreover, it is to be noted that these experiments were performed multiple times with the given splits to eliminate the standard deviation as the cause for higher accuracy."}, {"title": "4.2 Training Details", "content": "For hyperparameters consistency, we ensure the same hyperparameters for both with and without the use of WordVIS to eliminate any issues that could be caused by differences in hyper parameters. For training the DocXClassifier-B the original hyperparameters used by authors of DocXClassifier Saifullah et al [25] were used. All the remaining architectures of ResNet50, ResNet101, densenet121, Efficient NetV2 were trained with the same hyperparameters. For the optimizer Stochastic Gradient Decent (SGD) was used with a learning rate of: 0.5, weight decay was set to 1.0e-8, and batch size of 64. All the architectures were trained for 300 epochs, however, the convergence occurred for all before the 80 epoch mark. ferrandode"}, {"title": "4.3 Evaluation", "content": "For comparative analysis using a state-of-the-art document classifier on Tobacco-3482 dataset, we took the DocXClassifier-B, the smallest in terms of the number of parameters in the state-of-the-art category, and tested WordVIS. As seen in Table 1, our method WordVIS improves on the base model drastically, setting a"}, {"title": "4.4 Ablation Study", "content": "To further test our method, we also trained several CNN variant architectures. As we can see from Table 2 that the WordVIS method outperformed the no pre-processing on all architectures."}, {"title": "4.5 Qualitative Analysis", "content": "In order to analyze and verify the improvements brought in by WordVIS, we generated activation HeatMaps for test samples. As seen from Fig. 4 we can see that the WordVIS (Top Row) shows a more refined focus toward specific text areas instead of a generalized focus on the entire image space, suggesting that the network learns to follow the text. In the example of Form Fig. 5 we can see that the WordVIS-trained DocXClassifier-B focuses on the specific boxed area and the activation is quite well tightly bounded as opposed to the base-trained"}, {"title": "5 Conclusion and Future Work", "content": "In this research publication, we presented WordVIS, a novel pre-processing approach for utilizing text in Document Classification on smaller datasets. The heatmap analysis conducted in this research proves that our approach successfully embedded textual data in visual space, leading to the elimination of the textual embedding overhead used in multi-modal approaches. The research also successfully proves that image-based networks can be used to learn contextual text data by enhancing the textual data in document images. This approach not only reduces the required training data and compute overhead but also is able to efficiently leverage small amounts of data to outperform larger networks. Furthermore, the heatmap analysis also proves that such textual coloring techniques not only improve the quantitative accuracy but also drastically improve the quantitative results of the document classifiers. Our goal with this research was to enable businesses with limited training data and compute resources to be able to use and leverage document classifiers for their business use cases. We feel this is a step in the right direction, however, more research needs to be conducted to expand our work in order to improve accuracy on relatively simpler Deep Learning networks and reduce the architectural complexity requirements for more tolerable results."}]}