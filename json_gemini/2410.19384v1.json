{"title": "Learning Neural Strategy-Proof Matching Mechanism from Examples", "authors": ["Ryota Maruo", "Koh Takeuchi", "Hisashi Kashima"], "abstract": "Designing effective two-sided matching mechanisms is a major problem in mechanism design, and the goodness of matching cannot always be formulated. The existing work addresses this issue by searching over a parameterized family of mechanisms with certain properties by learning to fit a human-crafted dataset containing examples of preference profiles and matching results. However, this approach does not consider a strategy-proof mechanism, implicitly assumes the number of agents to be a constant, and does not consider the public contextual information of the agents. In this paper, we propose a new parametric family of strategy-proof matching mechanisms by extending the serial dictatorship (SD). We develop a novel attention-based neural network called NeuralSD, which can learn a strategy-proof mechanism from a human-crafted dataset containing public contextual information. NeuralSD is constructed by tensor operations that make SD differentiable and learns a parameterized mechanism by estimating an order of SD from the contextual information. We conducted experiments to learn a strategy-proof matching from matching examples with different numbers of agents. We demonstrated that our method shows the superiority of learning with context-awareness over a baseline in terms of regression performance and other metrics.", "sections": [{"title": "Introduction", "content": "Two-sided matching is a major problem in mechanism design and has been applied to various real-world applications, such as labor markets for medical interns and residents (Roth 1984), assignments in schools (Abdulkadiro\u011flu and S\u00f6nmez 2003), and matchings between firms and banks in loan markets (Chen and Song 2013). For example in school choice, a public center called a mechanism designer develops a matching mechanism to form pairs of students and schools based on their reported preferences for each other.\nRecently, the public contextual information of agents has played a crucial role for better matching outcomes. For example, in affirmative action for minorities, race and income status are used to achieve a fair matching in school choice in Brazil (Ayg\u00fcn and B\u00f3 2021; Dur, Pathak, and S\u00f6nmez 2020). In matching children with daycare centers, family structures and parents' health are employed as context to reduce the daily burden on families in Japan (Sun et al. 2023). Without the contextual information, we equally treat agents and thus cannot obtain better outcomes for social goods.\nMechanism designers are usually assumed to be able to formulate an objective that represent goodness of a matching. Stability is a standard social objective, whereby that no pair of agents prefers being rematching between them (Roth and Sotomayor 1990). Strategy-proofness is another important social objective, meaning that no agent can benefit from misrepresenting their preferences. This objective is crucial in various applications, such as the affirmative action (Ayg\u00fcn and B\u00f3 2021) because neither majorities nor minorities get benefit than their contexts. Serial dictatorship (SD) is a mechanism that satisfies strategy-proofness (Abdulkadiro\u011flu and S\u00f6nmez 1998; Satterthwaite and Sonnenschein 1981), where agents are ordered sequentially and each agent either selects their most preferred unmatched partner or remains single if it is more desirable.\nClearly defining social objectives is not always possible in the real world. For example, typical ethical requirements for matching are too complex to represent as mathematical formulas (Li 2017; S\u00f6nmez and \u00dcnver 2022). In such cases, mechanism designers have managed to obtain matching using implicit and empirical rules to achieve desired objectives given contexts. No matter how good the matching they have achieved by coordinating stakeholders, we cannot reproduce it because no explicit rule exists. Thus, an alternative approach is required that learns an implicit rule from existing matching records and automatically builds a mechanism.\nNarasimhan, Agarwal, and Parkes (2016) proposed a novel framework for learning desirable matching mechanisms from a set of matching records. They considered reported preferences and matching results as inputs and outputs for a learning task, assuming that the mechanism designer's implicit objective is encoded in the records of matching results. Then, they introduced a parameterized family of matching mechanisms that inherently satisfy stability, and proposed a method for finding a mechanism by optimizing the parameters with a given dataset using structured support vector machines (Tsochantaridis et al. 2005). Although novel, there are three problems with this framework. First, there is no parameterized family of strategy-proof matching mechanisms, even for the learned stable mechanism, because no stable matching mechanism is entirely strategy-proof (Roth 1982; Roth and Sotomayor 1990). Second, their framework implicitly assumes the number of agents to be constant and cannot be applied to varying"}, {"title": "Related Work", "content": "Conitzer and Sandholm (2002, 2004) first introduced automated mechanism design (AMD), which focuses on automatically solving mechanism design problems. AMD focuses on optimizing the expectations of functions that explicitly formulate social objectives (Conitzer and Sandholm 2002, 2004; Sandholm 2003), and most subsequent research follow this framework. For instance, in auction settings, one of the primary social objectives is revenue (Conitzer and Sandholm 2004; Curry, Sandholm, and Dickerson 2023; Duan et al. 2023, 2022; Duetting et al. 2019; Feng, Narasimhan, and Parkes 2018; Peri et al. 2021; Rahme et al. 2021; Sandholm and Likhodedov 2015; Shen, Tang, and Zuo 2019). Other social objectives are also considered, such as negative sum of agents' costs (Golowich, Narasimhan, and Parkes 2018) in facility location, social welfare (Wang et al. 2021) in public project settings, and worst-case efficiency loss (Guo et al. 2015) in general design settings. Here, the requirement of such explicit objectives is avoided by adopting the supervised learning formulation proposed by Narasimhan, Agarwal, and Parkes (2016).\nIn differentiable economics (Duetting et al. 2019), researchers designed neural networks to solve AMD problems. Pioneered by Duetting et al. (2019), who proposed Regret-Net, a line of AMD research focused on solving revenue-optimal auction problems using neural networks (Curry, Sandholm, and Dickerson 2023; Feng, Narasimhan, and Parkes 2018; Peri et al. 2021; Rahme et al. 2021; Shen, Tang, and Zuo 2019); however, the current study focuses on two-sided matching problems. Neural networks that consider contextual information in auction settings have been proposed (Duan et al. 2023, 2022), but none of them considered models for strategy-proof two-sided matching mechanisms. Ravindranath et al. (2023) proposed a neural network for two-sided matching problems to explore the trade-offs between strategy-proofness and stability, but their model lacked rigorous strategy-proofness, did not incorporate contextual information, and only worked with a fixed number of agents. Curry et al. (2022) investigated differentiable matchings in neural network-based auction design, but in contrast to our approach, their matchings did not incorporate contextual information."}, {"title": "Background", "content": "We use $[n]$ to denote a set ${1, ..., n}$ for any $n \\in \\mathbb{N}$. We represent a row vector as $x = [x_1,..., x_d]$, a matrix as $X = [X_{i,j}]_{i,j}$, and a three-dimensional tensor as $X = [X_{i,j,k}]_{i,j,k}$.\nMatching Instance. We focus on one-to-one, two-sided matchings with varying numbers of agents. We refer to each side as 'workers' and 'firms' (Ravindranath et al. 2023). We denote a matching problem as an instance represented by a tuple $I := (n, m, W, F, X_W, X_F, \\geq)$. We denote the set of all possible instances by $\\mathbb{I}$. Unless explicitly considered, we omit the subscript 'I' from its components.\nOur definition explicitly acknowledges changes to $n$ and $m$ to align with our interests and seamlessly introduce the problem set-ting in Section 4. In contrast, the standard definition in mechanism design is simpler, as it considers an arbitrarily fixed instance (for"}, {"title": "Proposed Method", "content": "To construct a parameterized family of strategy-proof mechanisms $\\mathbb{F}$, we employ SD (Abdulkadiro\u011flu and S\u00f6nmez 1998; Satterthwaite and Sonnenschein 1981). Our key idea is to make SD learnable by making its steps differentiable with respect to the ranking and parametrically computing the ranking from the contextual information. We first describe SD and its properties, and propose our algorithm for executing SD using tensor operations. We then develop novel neural networks that can learn a matching mechanism from matchings and contexts."}, {"title": "Serial Dictatorship (SD)", "content": "SD is a matching mechanism widely applied for one-sided matching, and Ravindranath et al. (2023) recently extended SD for two-sided matching. We denote a ranking over agents by $r = (r_1,...,r_{n+m})$, where $r_k = a$ indicates that agent $a$ is assigned to rank $k$. Given a ranking $r$ and report $\\geq'$, SD proceeds as follows:\n(1) Initialize the matching $\\mu$ to be empty.\n(2) For each round $k = 1,...,n + m$, if $r_k$ is not yet matched in $\\mu$, then assign $\\mu(r_k)$ to either the most preferred unmatched agent under $\\geq'$ or to the unmatch"}, {"title": "SD as Tensor Operations", "content": "Since SD is a deterministic algorithm, it is a non-differentiable operator and cannot be used as a layer of neural networks. To overcome this limitation, we propose TSD as a differentiable SD algorithm that exactly computes SD via tensor operations and allows back propagations. We briefly outline the algorithm and provide the complete pseudo-code in Appendix B.\nWe consider an instance comprising $n$ workers and $m$ firms that report their preferences as $\\geq'$. To make the SD algorithm on a ranking $r = (r_1,...,r_{n+m})$ differentiable, we introduce three tensors to our TSD algorithm. $P_W \\in \\{0,1\\}^{n\\times(m+1)\\times(m+1)}$ and $P_F \\in \\{0,1\\}^{m\\times(n+1)\\times(n+1)}$ are three dimensional tensors that represents the reports of all workers and firms, respectively. $R \\in \\{0,1\\}^{(n+m)\\times(n+m)}$ is a permutation matrix representing the ranking $r$.\nIn particular, $P_W = [P_{w_1},..., P_{w_n}]$ comprises $n$ matrices. $P_{w_i}$ is a preference matrix of the worker $w_i$ that represents the report of $w_i$. $P_{w_i} = [P_{i,j,k}]_{j,k\\in[m+1]}$ is defined as follows:\n$P_{i,j,k} := \\mathbb{I} \\left[ \\begin{array}{l} \\text{If } j < m \\text{ and } ord(f_j, \\geq_{w_i}) = k, \\text{ or} \\\\ \\text{if } j = m + 1 \\text{ and } ord(\\perp, \\geq_{w_i}) = k \\end{array} \\right],$  (3)\nwhere $\\mathbb{I}$ is the indicator function and $ord(f,\\geq_{w_i}) := \\#\\{f' \\in F \\mid f' \\geq'_{w_i} f\\}$ is the order of $f \\in F$ in $\\geq_{w_i}$. Similarly, we define $P_F$ comprising the preference matrices of $m$ firms. $R = [R_{i,j}]_{i,j\\in[n+m]}$ is a ranking matrix that represents the ranking over agents $r$ in SD and is defined as\n$R_{i,j} := \\mathbb{I} \\left[ \\begin{array}{l} \\text{If } 1 < i < n \\text{ and } r_j = w_i, \\text{ or} \\\\ \\text{if } n + 1 \\leq i \\leq n + m \\text{ and } r_j = f_{i-n} \\end{array} \\right].$ (4)"}, {"title": "NeuralSD", "content": "We propose NeuralSD, which leverages the contextual information to estimate a ranking matrix and then outputs a matching of agents through TSD using reported preferences. Specifically, we develop an attention-based (Vaswani et al. 2017) sub-network to compute the ranking, ensuring both context-awareness and scale-invariance to the number of agents. By unifying this component with TSD, we optimize the parameters through the ranking, thereby constructing a learnable SD that forms a parameterized family of\nstrategy-proof mechanisms. The architecture of NeuralSD is presented in Figure 1.\nThe strategy-proofness of SD may not hold if its ranking is calculated from reports; thus, we determine the ranking before collecting preference reports. Additionally, we assumed that the contexts are available a priori in Section 3. Therefore, our approach can preserve strategy-proofness while introducing an estimation procedure to SD through parametric computations of rankings to reflect agent contextual information.\nNeuralSD comprises two components: the ranking block, which takes the contexts $X_W$ and $X_F$ and outputs a ranking matrix $\\tilde{R} \\in \\mathbb{R}^{(n+m)\\times(n+m)}$, and the matching block, which runs TSD to output a matching matrix $M = TSD(P_W, P_F, \\tilde{R})$. $\\mathbb{R}_e$ denotes the ranking block and is defined as $\\tilde{R} = \\mathbb{R}_e(X_W,X_F) := SoftSort(\\text{TieBreak}(Linear(SelfAttn(X))))$, where $X := [X_W; X_F]$ represents the concatenation of worker contexts $X_W$ and firm contexts $X_F$. The function $SelfAttn(X) := Attention(XW^Q,XW^K,XW^V)$ applies self-attention (Vaswani et al. 2017) by parameters $W^Q,W^K,W^V \\in \\mathbb{R}^{d \\times d_{emb}}$, where $d$ is the dimension of contexts and $d_{emb}$ is the embedding dimension. The linear transformation $Linear(A) := WA + b$ transforms $A$ into a one-dimensional vector by parameters $w \\in \\mathbb{R}^{d_{emb}}$ and $b \\in \\mathbb{R}$. The tie-breaking function $TieBreak(a) := a + rank(a)$ adds the rank of vector $a$, where $rank(a) := [\\#\\{j \\mid a_j < a_i \\text{ or } a_j = a_i \\text{ and } j < i\\}]_i$.\nFinally, $SoftSort(a)$ is defined as $softmax\\left(-\\frac{\\|sort(a)+\\mathbb{1}-a\\|}{\\tau}\\right)$, where $A := [A_{i,j}]_{i,j}$ is the element-wise absolute value; $softmax$ is applied row-wise and $\\mathbb{1}$ denotes the all-one vector with the same dimension as that of $a$. This provides a differentiable approximation of the $argsort$ operator (Prillo and Eisenschlos 2020). If there are ties in $a$, $SoftSort$ cannot detect them, causing the output $\\tilde{R}$ to deviate from a permutation matrix. Therefore, the tie-breaking function is necessary to ensure that the output remains a"}, {"title": "Loss Function", "content": "To ensure differentiability, we assumed $\\tilde{R}$ as not being a strict permutation matrix and $M$ to be a non-matching matrix during training. To train the parameters in the ranking block, we calculated a row-wise softmax on $M$ and computed the average row-wise cross entropy loss between $M$ and the label $M^\\ell$, up to the $n$-th row. The function $\\mathbb{D}$ in Equation (2) can be expressed as follows:\n$\\mathbb{D}(NeuralSD(P_W^\\ell, P_F^\\ell, X_W^\\ell, X_F^\\ell), M^\\ell) :=\\frac{1}{n} \\sum_{i=1}^{n}\\mathbb{L}_{CE}(Softmax(M_i), M_i^\\ell),$ (5)\nwhere $\\mathbb{W}^\\ell$ and $\\mathbb{F}^\\ell$ are the set of workers and firms in $\\mathbb{I}^{\\ell|\\geq^\\ell}$, $NeuralSD(P_W^\\ell, P_F^\\ell, X_W^\\ell, X_F^\\ell)$ is our model of $f_{\\theta}((\\geq^{\\ell}); pub(\\mathbb{I}^{\\ell}))$, $\\mathbb{L}_{CE}$ is the cross entropy loss function and $\\mathbb{A}_i:$"}, {"title": "Experiments", "content": "We conducted experiments for learning strategy-proof matching mechanisms from examples. We provide details of experimental settings and complementary results in Appendix D.\nExperimental Setup. We synthesized three matching labels. The first is the DA (Gale and Shapley 1962) which is stable and widely used in matching problems. The other two are adapted from the existing work to our setting (Narasimhan, Agarwal, and Parkes 2016). One is an equal-weighted Hungarian matching (EH), and the other is a minority-weighted Hungarian matching (MH). These mechanisms first define a reward for matching a worker-firm pair based on reported preferences and then determine the outcome by maximizing the sum of rewards. The reward is the sum of inverted orders ($n + 2$ or $m + 2$ minus ord) they assign each other, and MH gives additional rewards to some workers. The formal definition is provided in Appendix D.\nWe fixed the dimension of the contexts at $d = 10$ and generated the worker and firm contexts by sampling independently from $\\mathcal{N}(\\mu \\mathbb{I}_{10}, \\mathbb{I}_{10})$ and $\\mathcal{N}(-\\mu \\mathbb{I}_{10}, \\mathbb{I}_{10})$, respectively, where $\\mathbb{I}_{10}$ is the 10-dimensional identity matrix. The report was generated based on Euclidean preferences (Bogomolnaia and Laslier 2007), where an agent prefers another agent more if the distance between their contexts is shorter. We generated 1,000 and 750 instances for training and test data. We set $n = m \\in \\{3, 5, 10, 20, 40, 80, 120, 160, 200\\}$ for the number of agents. We trained our model for $n < 40$ and conducted tests for all values of $n$ to assess the robustness against the increasing number of test agents."}, {"title": "Conclusion and Limitations", "content": "In this paper, we proposed a novel neural network that satisfies strict strategy-proofness, scale-invariance, and context-awareness to learn desirable matching mechanisms from examples. Experimental results confirmed that NeuralSD can recover the desired mechanisms. The main limitation is the computational complexity of forward propagation, which amounts to $O(n^4)$, where $n$ is the maximum number of agents on one side. We leave the exploration of further scalability improvements for future work. Our work has potential negative impacts due to the drawbacks of SD itself. Because SD inherently outputs unstable matchings and prioritizes some agents, it can yield unsatisfactory and unfair matchings for agents. A potential solution is to incorporate fairness constraints, which we will consider in future work."}, {"title": "Proof of Proposition 1", "content": "We prove Proposition 1 as follows.\nProof. Consider an instance with $n$ workers and $m$ firms with the truthful preference profile $\\geq = (\\geq_{w_1},..., \\geq_{w_n}, \\geq_{f_1},..., \\geq_{f_m})$. We fix an arbitrary ranking $r = (r_1,...,r_{n+m})$ over the agents and consider a matching $\\mu$ determined by SD under the ranking $r$ and the truthful report $\\geq$.\nSuppose another matching $\\mu'$ Pareto dominates $\\mu$ under $\\geq$. Then, there must exist at least one rank $k$ where the agent $a = r_k$ strongly prefers $\\mu'(r_k)$ to $\\mu(r_k)$ under $\\geq$. Let $k_0$ be the highest rank among such $k$ and $a_0 := r_{k_0}$ be the corresponding agent.\nNote that $k_0 > 1$, because in the first iteration of SD, the agent $r_1$ can select its most preferred option. Moreover, $\\mu'(a_0) \\neq \\perp$ because if $\\mu'(a_0)$ were $\\perp$, then $a_0$ would have selected $\\perp$ at the $k_0$-th iteration in SD.\nBy contrast, $a_0$ strongly prefers $\\mu'(a_0)$ over $\\mu(a_0)$ under $\\geq$, $a_0$ cannot choose $\\mu'(a_0)$ in SD. Therefore, there must exist a $k_1 \\in [1, k_0)$ such that $\\mu(r_{k_1}) = \\mu'(a_0) \\neq \\perp$. Furthermore, $\\mu(r_{k_1}) = \\mu'(r_{k_1})$ by the definition of $k_0$, then $\\mu'(r_{k_1}) = \\mu'(a_0) = \\mu'(r_{k_0}) \\neq \\perp$. This leads to a contradiction because we consider one-to-one matching. Consequently, $\\mu$ is Pareto efficient under $\\geq$.\nNext, we consider the strategy-proofness. Suppose an agent $a = r_k$ at an arbitrary rank $k$ is better off by a report $\\geq_{-a}$. Since $r$ is predefined before collecting reports, the list of options $\\mathbb{L}_a \\subset \\mathbb{W} \\cup \\mathbb{F}$ that $a$ can designate at iteration $k$ is determined irrespective of $\\geq$. Let $b$ be the top preferred option in $\\mathbb{L}_a$ under $\\geq$. The preference orders over agents excluding $\\mathbb{L}_a$ are not altered because this alternation has nothing to do with the eventual match. By contrast, $a$ also cannot be better off by altering orders over $\\mathbb{L}_a$, because this alternation can yield less preferred matching, which leads to a contradiction. Consequently, $\\geq$ must be equal to $\\geq_{-a}$, implying that SD is strategy-proof."}, {"title": "Description of TSD", "content": "This section describes the computation of SD through tensor operations. The main algorithm is presented in Algorithm 3, which utilizes the helper function described in Algorithms 1 and 2. We have introduced additional necessary notations before explaining these three algorithms.\nAdditional Notations. $\\mathbb{O}_d$ and $\\mathbb{1}_d$ denote $d$-dimensional all-zero and all-one row vectors, respectively. $\\mathbb{O}_{p,q}$ denotes a zero matrix with $p$ rows and $q$ columns. If not specified, the subscripts $d$ or $p$ and $q$ are omitted. Moreover, we used indexing notation similar to that of Numpy. For a matrix $X \\in \\mathbb{R}^{p \\times q}$, $X_{:i,j} = [X_{1,j},..., X_{i,j}]^{\\top}$ represents the top $i$ elements of the $j$-th column, and $X_{i:,j} = [X_{i,j},..., X_{q,j}]^{\\top}$ represents elements from the $i$-th row to the end of the $j$-th column. Furthermore, $Y_{:k} = [Y_1,..., Y_k]$ denotes the first $k$ elements of the vector $y$.\nWe use common operations on tensors as typically found in libraries such as Numpy and PyTorch. $[x\\|c] = [x_1,...,x_d,c]$ is a row vector obtained by concatenating a vector $x = [x_1,...,x_d]$ with a scalar $c$. For a"}]}