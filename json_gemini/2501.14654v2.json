{"title": "MedAgentBench: A Realistic Virtual EHR Environment to Benchmark Medical LLM Agents", "authors": ["Yixing Jiang", "Kameron C. Black", "Gloria Geng", "Danny Park", "James Zou", "Andrew Y. Ng", "Jonathan H. Chen"], "abstract": "Background Recent large language models (LLMs) have demonstrated significant advancements, particularly in their ability to serve as agents thereby surpassing their traditional role as chatbots. These agents can leverage their planning and tool utilization capabilities to address tasks specified at a high level. This suggests new potential to reduce the burden of administrative tasks and address current healthcare staff shortages. However, a standardized dataset to benchmark the agent capabilities of LLMs in medical applications is currently lacking, making the evaluation of LLMs on complex tasks in interactive healthcare environments challenging.\nMethods To address this gap to the deployment of agentic AI in healthcare, we introduce MedAgentBench, a broad evaluation suite designed to assess the agent capabilities of large language models within medical records contexts. MedAgentBench encompasses 300 patient-specific clinically-derived tasks from 10 categories written by human physicians, realistic profiles of 100 patients with over 700,000 data elements, a FHIR-compliant interactive environment, and an accompanying codebase. The environment uses the standard APIs and communication infrastructure used in modern EMR systems, so it can be easily migrated into live EMR systems.\nResults MedAgentBench presents an unsaturated agent-oriented benchmark that current state-of-the-art LLMs exhibit some ability to succeed at. The best model (Claude 3.5 Sonnet v2) achieves a success rate of 69.67%. However, there is still substantial room for improvement which gives the community a next direction to optimize. Furthermore, there is significant variation in performance across task categories.\nConclusion Agent-based task frameworks and benchmarks are the necessary next step to advance the potential and capabilities for effectively improving and integrating AI systems into clinical workflows. MedAgentBench establishes this and is publicly available at https://github.com/stanfordmlgroup/MedAgentBench, offering a valuable framework for model developers to track progress and drive continuous improvements in the agent capabilities of large language models within the medical domain.", "sections": [{"title": "1 Introduction", "content": "Recent large language models (LLMs) have demonstrated significant advancements, particularly\nin their ability to serve as agents via active task execution thereby surpassing their traditional role"}, {"title": "2 MedAgentBench", "content": "A typical envisioned workflow (depicted in Figure 1) for the agentic system would be 1) a clinician\nspecifies a high-level task to the agent orchestrator, 2a) the agent interprets the task, and plans function\ncalls, 2b) the agent executes this by sending requests to the FHIR server to modify, for example,\nthe medical records database, and 3) the agent interface (orchestrator) gives an output to the user\nsummarizing the tasks performed."}, {"title": "2.1 Tasks", "content": "Two internal medicine physicians (KB, JHC) submitted 300 clinically derived tasks commonly\nencountered that could benefit from computer agent automation. Tasks were curated by level of\ncomplexity and clinical relevance. To contain the scope of computer information tasks addressed, we\nfocused on inpatient and outpatient medical scenarios that have a high density of relevant tasks and\nneeds that could be addressed through computer interaction (as opposed to surgical or procedural\ninterventions that would necessarily happen outside the scope of an LLM agent). Types of tasks\nincluded patient communication, patient information retrieval, recording patient data, test ordering,\ndocumentation, referral ordering, medication ordering, as well as patient data aggregation and analysis.\nThe list is not exhaustive, however tasks were chosen in effort to create a range of functions spanning\ninpatient and ambulatory settings.\nTask structure typically included elements such as patient MRN, timing of request (\u201cover last 24\nhours\"), and data to be recorded (blood pressure value). We also included NDC, LOINC, base names,\""}, {"title": "2.2 Patient profiles", "content": "Benchmark examples are based on real patient cases that were deidentified and jittered. Specifically,\npatient profiles are extracted from a deidentified clinical data warehouse curated by the STARR\n(STAnford Research Repository) project [27]. The timestamps in the data warehouse are jittered\nat the patient level. To provide realistic contexts, we extract lab test results, vital signs, procedure\norders, diagnosis and medication orders in the last five years (November 13, 2018 as the cutoff date)."}, {"title": "2.2.1 Patient cohort", "content": "We randomly sample 100 patients from a cohort with an inpatient sodium lab test ordered on the\nmorning of November 13, 2023. The sodium lab test serves as an anchor because it is a common and\nclinically significant test in inpatient settings. The characteristics of the cohort are summarized in\nTable 2."}, {"title": "2.2.2 Patient demographics", "content": "As protected health information such as medical record numbers (MRNs), names, phone numbers\nand addresses are removed in the STARR data warehouse. We randomly sample numbers of 7 digits\n(with de-duplication) and prefix them with a letter S to use as fake MRNs. The format is the same as\nthe actual ones used at Stanford Hospital. We also use a Python library called Faker to generate US\nnames, phone numbers and addresses for the patients."}, {"title": "2.2.3 Lab test results", "content": "For each lab test result, we extract these fields: taken time, result time, base name, result value, unit\nand result flag. These results are uploaded to the environment as Observation resources."}, {"title": "2.2.4 Vital signs", "content": "As there is a large number of flowsheet records, we select six specific types of vital signs for inclusion:\nheart rate, SpO2, respiratory rate, FiO2, blood pressure and temperature. Besides measurement\ntype and values, recording timestamps are also extracted. They are uploaded to the environment as\nObservation resources."}, {"title": "2.2.5 Procedure orders", "content": "The following fields are extracted for procedure orders: order date, CPT code, procedure description,\nand quantity. For those procedures with missing quantities, we impute them with ones. We remove\nthose procedures with missing CPT codes or descriptions. The remaining ones are uploaded to the\nenvironment as Procedure resources."}, {"title": "2.2.6 Diagnosis", "content": "We extract the following fields for previous diagnosis: diagnosis name, ICD10 code and start date. We\nremove those records with any missing value and the remaining ones are uploaded to the environment\nas Condition resources."}, {"title": "2.2.7 Medication orders", "content": "The following fields are extracted for medication orders: order date, medication description, route,\nfrequency, dosage and unit. Orders with frequency of PRN are removed to approximate actual\nadministrations. They are uploaded to the environment as MedicationRequest resources."}, {"title": "2.3 Environment setup", "content": "FHIR (Fast Healthcare Interoperability Resources) is a commonly used standard to facilitate interop-\nerability for health information exchange across systems. As most commercial EHR vendors support\nFHIR, we build a FHIR-compliant interactive environment for MedAgentBench. We build the envi-\nronment using the open-sourced HAPI FHIR JPA. After configuring the server to use persistent H2\ndatabase and uploading the patient profiles via parallel POST requests, we build a new Docker image\nfor easy setup. The image is available at https://hub.docker.com/r/jyxsu6/medagentbench."}, {"title": "2.4 Evaluation setup", "content": "We build the codebase for MedAgentBench using the framework proposed by AgentBench [10]. We\nadd a few LLM as agents to reflect the current state-of-the-art, as detailed in Section 2.4.2. Given the\nFHIR-compliant interactive environment takes around 90 seconds to start, we decide to only send\nGET requests to the environment so that we do not need to re-initialize the environment for each\nindividual task."}, {"title": "2.4.1 Metrics", "content": "We use task success rate as the main evaluation metric, as it is commonly used in agent benchmarks.\nThe grader and reference solution for each task category is manually curated. For query-based tasks,\nwe compare the responses from agents with the answers generated by the reference solutions. For\naction-based tasks, we manually write many rule-based sanity checks to verify the correctness of the\npayload of POST requests. If the agent system requests for invalid actions or exceeds the maximum\nnumber of interaction rounds, it is considered a failure.\nWhile repeated sampling techniques such as pass@kare commonly used in language model evalua-\ntions, we exclusively adopt pass@1 in our benchmark. This decision reflects the stringent accuracy\nrequirements in healthcare applications, where even a single incorrect action or response can have\nsignificant consequences. The low tolerance for errors in clinical environments necessitates an\nevaluation approach that assesses models under a single-attempt constraint, mirroring real-world\ndeployment scenarios."}, {"title": "2.4.2 Models", "content": "We select a variety of state-of-the-art LLMs across different providers and sizes for benchmarking.\nThey include 03-mini, GPT-40, GPT-40 mini from OpenAI, Gemini 2.0 Pro, Gemini 2.0 Flash and\nGemini 1.5 Pro from Google, Claude 3.5 Sonnet v2 from Anthropic, DeepSeek-V3 from DeepSeek,\nQwen2.5 from Alibaba, Llama 3.3 from Meta, Gemma2 from Google and Mistral v0.3 from Mistral\nAI (via Together AI serverless API). We set the temperature to zero for all models except o3-mini."}, {"title": "2.4.3 Agent orchestrator", "content": "We develop a simple agent orchestrator to establish the baseline performance, inspired by BFCL [12].\nAt a high level, the agent system is exposed to the following nine FHIR functions selected: condi-\ntion.search, lab.search, vital.search, vital.create, medicationrequest.search, medicationrequest.create,\nprocedure.search, procedure.create and patient.search. These functions are defined as JSON schemas\nwhich are manually translated based on FHIR API documentation. During each round, the agent\nsystem is expected to select one from the three options: send a GET request, send a POST request or\nfinish the conversation. As all tasks within MedAgentBench require only a few steps to complete,\nwe limit all interactions to a maximum of 8 rounds. If the agent system invokes a GET request, we\nsend the request and input the raw response back to the agent system. If the agent system invokes a\nPOST request, we conduct a simple sanity check to make sure the payload data is JSON-loadable,\nand indicate success of execution to the agent system. If the agent system invokes a finish request,\nwe save the entire conversation for grading purpose. The specific prompt used is included in the\nappendix. Gemini models tends to encapsulate the code in a ```tool_code block, so we remove the\nblock separators before parsing.\nIt is noteworthy that we introduce the \"Agent Orchestrator\" as a high-level abstraction of the agent\nsystem within the MedAgentBench framework. Developers can implement more complex designs,\nincluding compound AI systems with hierarchical reasoning, multiple specialized sub-agents, or"}, {"title": "2.5 Main results", "content": "The performance of 11 state-of-the-art LLMs on MedAgentBench is shown in Table 3. Most models\nshow non-trivial performance on MedAgentBench, with Claude 3.5 Sonnet performing the best with\nan overall success rate of 69.67%. This highlights the great potential of leveraging agent capabilities\nof LLMs in medical applications.\nHowever, given the high stakes of healthcare settings, all current state-of-the-art LLMs are still unable\nto serve as highly reliable agents. Also, there is still a gap between closed and open-weights LLMs,\nwhich is an important direction for the open-weights community."}, {"title": "2.5.1 Subgroup analysis based on task types", "content": "Among the 300 tasks in MedAgentBench, half (150) only require information retrieval via GET\nrequests, while the other half require the modification of medical records through POST requests\n(often in combination with GET requests beforehand). We calculate task success rates for these two\nsubgroups and name them as query SR and action SR respectively.\nMost models, except Gemini 1.5 Pro and Qwen2.5, are better at query-based tasks than action-based\ntasks, suggesting that we can start exploring use cases which only require information retrieval first."}, {"title": "2.5.2 Common error patterns", "content": "Figure 2 shows two common error patterns. One common error pattern of most models is the model\ndoes not follow the instruction exactly. For example, Gemini 2.0 Flash outputs invalid actions in\n54% of the cases, and the model tends to output the code in a tool_code or json block, although the\ninstruction has stated that no other text should be in the response. Another common error pattern is\nthe model tends to give the answer in a full sentence, while it is expected to output only a numerical\nvalue. A concrete example is the model outputs \"[\\\"value\\\": 5.4]\" while the expected answer is \"[5.4]\"."}, {"title": "3 Discussion", "content": "Medical agent tasks have the potential to enhance clinical workflows and practices by automating\ncomplex processes and alleviating administrative burdens. However, these tasks are inherently more\nspecific and intricate compared to general agent tasks addressed in existing benchmarks.\nMedAgentBench is a benchmark dataset to drive progress in leveraging agent capabilities of large\nlanguage models for medical applications. It will be interesting to study how the next generation\nof large language models and other advanced design patterns of agentic systems lead to better\nperformance on MedAgentBench. There is a trade-off between the number of tasks and cost for\nevaluation. We decided that the first release of MedAgentBench would contain 300 tasks and 100\npatient profiles to achieve accurate estimates of performance at reasonable prices.\nOur results showed that many of the main LLMs generally perform better at query-based tasks than\naction-based tasks. This follows our current understanding of large language model performance in\ninformation retrieval. This finding also shows the need for improvement in the LLM capability to\nnavigate complex decision-making with respect to action-based tasks.\nAlthough MedAgentBench has an interactive environment to test agent capabilities, it does not\ncapture the full complexity of real-world medical scenarios that typically require coordination and\ncommunication between different teams. Furthermore, since all patient profiles are derived from\nStanford Hospital records and are not representative of the general population, there are potential\nbiases in the profiles. Despite MedAgentBench being designed as a broad evaluation suite, it does\nnot have full coverage for all clinically relevant tasks and focuses primarily on medical record\ncontexts. Future work can also be extended to other domains in healthcare such as surgical specialties\nand nursing. Another area of future research includes the examination of the reliability of LLMs\nin producing the same results with repetition of action-based tasks (given the sensitive nature of\nhealthcare and the need for highly reliable systems). We use a simple agent system to establish the\nbaseline performance. Future work can explore advanced techniques such as many-shot in-context\nlearning [28] and meta prompting [29].\nIn conclusion, we introduce MedAgentBench, a broad suite of medical-specific agent tasks, an\ninteractive benchmarking environment, and a standardized evaluation framework that enables the\nsystematic assessment and advancement of AI agents in medical settings. Our evaluation of state-of-\nthe-art LLMs reveals that while they demonstrate promising capabilities, they are not yet capable of\nreliably handling the full complexity of these clinically relevant tasks. This underscores the critical"}, {"title": "A.3 Subgroup analysis based on difficulty level", "content": "We further break the tasks into three difficulty levels: easy (requires only one step), medium (requires\ntwo steps) and hard (requires at least three steps)."}]}