{"title": "Testing autonomous vehicles and AI: perspectives and challenges from cybersecurity, transparency, robustness and fairness", "authors": ["David Fern\u00e1ndez Llorca", "Ronan Hamon", "Henrik Junklewitz", "Kathrin Grosse", "Lars Kunze", "Patrick Seiniger", "Robert Swaim", "Nick Reed", "Alexandre Alahi", "Emilia G\u00f3mez", "Ignacio S\u00e1nchez", "Akos Kriston"], "abstract": "This study explores the complexities of integrating Artificial Intelligence (AI) into Autonomous Vehicles (AVs), examining the challenges introduced by AI components and the impact on testing procedures, focusing on some of the essential requirements for trustworthy AI. Topics addressed include the role of AI at various operational layers of AVs, the implications of the EU's AI Act on AVs, and the need for new testing methodologies for Advanced Driver Assistance Systems (ADAS) and Automated Driving Systems (ADS). The study also provides a detailed analysis on the importance of cybersecurity audits, the need for explainability in AI decision-making processes and protocols for assessing the robustness and ethical behaviour of predictive systems in AVs. The paper identifies significant challenges and suggests future directions for research and development of AI in AV technology, highlighting the need for multidisciplinary expertise.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) plays a critical role in the advancement of autonomous driving. It is likely the main facilitator of high levels of automation, as there are certain technical issues that only seem to be resolvable through advanced AI systems, particularly those based on machine learning. However, the introduction of AI systems in the realm of driver assistance systems and automated driving systems creates new uncertainties due to specific characteristics of AI that make it a distinct technology from traditional systems developed in the field of motor vehicles. Some of these characteristics include unpredictability, opacity, self and continuous learning and lack of causality [1], among other horizontal features such as autonomy, complexity, overfitting and bias. As an example of the specificity that the introduction of AI systems in vehicles entails, the UNECE's Working Party on Automated/Autonomous and Connected Vehicles (GRVA) has been specifically discussing the impact of AI on vehicle regulations since 2020 [2].\nIn order to maximize the benefits of using AI and reduce potential negative impacts, several frameworks have been developed that establish principles and requirements for the development of trustworthy AI. For instance, the OECD AI value-based principles for responsible stewardship of trustworthy AI [3], or the EU's ethical guidelines for trustworthy AI [4], [5], [6] which encompass multiple ethical principles, requirements, and criteria to ensure that AI systems are designed following a human-centered approach.\nThese frameworks comprise multiple interrelated components that extend beyond safety concerns, such as cybersecurity, robustness, fairness, transparency, privacy, accountability, societal and environmental well-being, among others. Their general applicability to the autonomous driving domain has been previously analyzed in [7], [8], demonstrating that these are complex frameworks which require addressing multiple problems of varying nature. Some of these problems are still at an early stage of scientific and technological maturity, presenting new research and development challenges in various areas. Progress in this context requires multidisciplinary expertise and tailored analyses.\nIn this work, based on the methodology described in Section 1.1, we present a detailed study of the state of the art in relation to some of the most prominent elements of trustworthy AI frameworks, and how they affect the current and future landscape of testing procedures for AVs. Specifically, we focus on cybersecurity, transparency, robustness, and fairness. The analysis is conducted by a multidisciplinary group of experts and includes the identification of the most pressing future challenges.\nAfter describing the methodology and presenting the terminology used, Section 2, is dedicated to presenting the various operational layers involved in vehicle automation, as well as the impact of AI on each of them. It also presents the regulatory context of AI and its sectoral application to the field of AVs. Section 3 presents the"}, {"title": "1.1 Methodology", "content": "To tackle the main focus areas of our analysis (cybersecurity, transparency, robustness, and fairness), we employed an expert opinion-based methodology at two levels. First, we conducted an interdisciplinary workshop involving 21 expert academics from various disciplines relevant to trustworthy AI and AVs. This interactive, collaborative online workshop allowed each expert to introduce a topic related to one of the focus areas, address all related questions, and provide additional resources as needed. The workshop's outcome was encapsulated in a technical report [9]. From this, a smaller group of experts was selected to go deeper into each focus area. This group aimed to provide a state-of-the-art overview for each area within the context of autonomous driving, specifically focusing on testing methodologies, and subsequently identify the most significant challenges in each area.\nDue to the multidisciplinary nature of trustworthy AI for AVs and the diverse group of experts, it was initially agreed to establish a common terminology, identify the main operational layers of AVs, and analyse the impact of AI on each of them. The regulatory context of AI and vehicles was also collectively examined."}, {"title": "1.2 Terminology", "content": "With respect to the terminology, in this work, we follow the proposal presented in [8]. We use advanced driver assistance systems (ADAS) or assisted vehicle/driving for SAE levels 1 and 2 (driver), automated vehicle/driving for SAE level 3 (a backup driver/user is in charge), and autonomous vehicle/driving for SAE levels 4 and 5 (passenger/unoccupied). In some cases, we also use automated driving systems (ADS) to generically refer to SAE levels 3 to 5 (automated and autonomous driving). Finally, when we use the acronym AV, we refer to automated and/or autonomous vehicles indistinctly."}, {"title": "2 AI for AVs", "content": ""}, {"title": "2.1 Operational layers of AVs and the role of AI", "content": "Autonomous vehicles (AVs) are complex systems that interact with highly complex environments with an almost infinite variety of possibilities. High levels of automation are facilitated by at least seven operational layers (see Figure 1):\nLocalisation: determines vehicle pose (position and orientation) relative to a digital map. This includes enhanced digital or high-definition maps that contain static elements (e.g., traffic lights, traffic signs, pedestrian crossings, lanes, street layout, intersections, roundabouts, parking areas, green areas, etc.) and dynamic real-time information (e.g., weather and traffic conditions). Different map-matching techniques are used to accurately establish the vehicle's position and heading within the map. While traditionally localisation was not based on learning, data-driven and machine learning approaches are becoming prevalent [11].\nPerception (and prediction): a.k.a. scene understanding, this process detects and interprets the dynamic content of the scene, including vehicles and Vulnerable Road Users (VRUs) [7]. It also includes segmenting the static elements of the scene from the perspective of the vehicle. Perception also includes prediction of future behaviours and trajectories of other agents, such as vehicles and VRUs, which are crucial for safe and comfortable planning. Prediction, also referred to as social forecasting, extends beyond simple predictions to forecast long-term dependencies and complex collective interactions. It models social etiquette, the unwritten rules of social interactions, predicting social agents' behaviours based on social models and including awareness of agents' own actions on others' movements. Social agent interaction requires joint reasoning, and the most promising approaches are based on complex machine learning methods (e.g., socially-aware AI for AVs, as depicted in Figure 2).\nPlanning: a.k.a. (local) motion or trajectory planning or decision making. Based on vehicle localization, knowledge of traffic rules, state of signals, and predicted trajectories of dynamic agents, local planning is performed, providing a feasible and smooth trajectory and speed references for the control system. We can distinguish between different methods that have traditionally been applied to deal with motion planning, including graph-search, variational or optimization-based, incremental or sample-based, and interpolation-based [12]. End-to-end deep learning approaches are becoming predominant [13].\nControl: it involves both longitudinal (acceleration and breaking) and lateral control (steering) of an AV [14]. Feedback controllers are used to select the most appropriate actuator inputs to perform the planned local trajectory. Many different types of closed loop controllers have been proposed for executing the reference motions provided by the path planning system [12], including path stabilization, trajectory tracking, and more recently, predictive control approaches.\nHuman-Vehicle Interaction (HVI): involves the design of human-vehicle interfaces for effective interaction and communication with in-vehicle users and external road users [7]. Potential modalities to communicate intention of the AV to road users include explicit, such as audio and video signals, and implicit forms, such as vehicle's motion pattern (speed, distance and time gap) [15]. Regarding driver and passengers, common interfaces are audio, tactile, visual, vibro-tactile, and more recently, natural language processing [16]. In addition, this layer also includes in-vehicle perception systems to detect the users' status.\nSystem Management: it supervises the overall ADS, including functions such as fault management, data management and logging, event data recording, etc.\nAlthough with varying levels of intensity, we can state that AI plays a predominant role in the operational layers of localisation, perception, planning, and human-vehicle interaction, somewhat less in the control layer, and circumstantially in the system management layer."}, {"title": "2.2 The AI Act and its sectoral application", "content": "In April 2021, the European Commission proposed the AI Act [17], which aims to establish trustworthy AI practices in the European Union. The AI Act adopts a risk-based approach that mandates specific obligations for providers of AI systems depending on their risk level. High-risk AI system providers (including those with potential risks to fundamental rights, health, and safety of humans) are obligated to meet defined requirements in Title III, Chapter 2 of the legal text (Articles 8 to 15). These requirements refer to risk management, data governance, technical documentation, record-keeping, transparency and provision of information to users, human oversight, accuracy, robustness, and cybersecurity.\nArticle 6 establishes the rules for the classification of high-risk AI systems. The proposal defines a specific set of AI systems that shall be considered high-risk (Annex III). This can be considered the core of the regulation. However, the AI Act also affects sector specific regulations. More specifically, AI systems intended to be used as a safety component of a product, or as a product itself, covered by Union harmonisation legislation (listed in Annex II, including machinery, toys, medical devices, motor vehicles regulations, etc.), as well as those products requiring a third-party conformity assessment before being put on the market or putting into service. The Annex II includes Regulation (EU) 2018/8584 [18] on the approval and market surveillance of motor"}, {"title": "2.3 Safety components with AI in AVs", "content": "In order to be subject to possible future requirements under a Commission delegated act, the AI system supplied or integrated in the AVs should be considered a safety component within the meaning of the AI Act.\nThe concept of \u201csafety component\u201d is defined in the AI Act [17] as \u201ca component of a product or of a system which fulfils a safety function for that product or system or the failure or malfunctioning of which endangers the health and safety of persons or property\". This general definition does not necessarily overlaps with sector-specific definitions of safety components of products covered by Union harmonisation legislation. Particularly relevant is the new Machinery Regulation (EU) 2023/1230 [20], in which the concept of safety component explicitly specifies that the component \"is not necessary in order for that product to function or for which normal components may be substituted in order for that product to function\". Then, an indicative list of safety components is provided in Annex II, including presence detectors of persons, monitoring devices, emergency stop devices, two hand control devices, among others.\nAnother example is Directive 2014/33/EU [21] relating to lifts and safety components for lifts, which also includes a list of safety components well aligned with the idea of"}, {"title": "3 Vehicle regulations and standards", "content": ""}, {"title": "3.1 Ex-ante requirements and testing", "content": "The product development process of any given system, including ADAS and ADS, is typically based on the V-model of product development (e.g., ISO 26262). Over the course of the development of a system, the degree of abstraction of the system properties first decreases (in the left branch for development) and then increases again (in the right branch for verification). At the same time, the objective shifts from questions of system design (left branch) to validation of the intended system characteristics (right branch). Requirements from vehicle technical regulations or standards and guidelines can also be placed in this scheme, giving vehicle safety bodies a chance to directly influence the development process [23].\nExternal requirements can come from various sources, most prominently the type-approval requirements, and independent car assessment programs such as the European New Car Assessment Program (Euro NCAP) [24]. Euro NCAP procures and tests market-available vehicles (compliant with type-approval requirements), comparing performance via standardized procedures. As these procedures are communicated in advance, manufacturers can integrate anticipated test results into their product development cycles.\nAll vehicles registered in Europe adhere to the EU Type Approval Directive 2007/46/EC [25] or the EU Type Approval Regulation (EU) 2018/858 [18], including amendments like the General Safety Regulations (GSR) Regulation (EC) No. 2019/2144 [19]. However, these revised directives and regulations do not specify technical requirements but refer to delegated or implementing acts of the European Commission (e.g., Implementing Regulation (EU) 2022/1426 regarding ADS or fully"}, {"title": "3.2 Post-market monitoring", "content": "Market surveillance mechanisms, such as those defined by EU Regulation 2018/858 [18], ensure that motor vehicles and components on the market comply with laws, regulations, and safety and health requirements. The European Commission participates in these activities, verifying vehicles' emissions and safety compliance, and collaborating with national authorities for information exchange and peer review of technical actors [31]. While the requirements for market surveillance match those of ex-ante type-approval procedures, the testing methodologies may differ considerably.\nAnother interesting tool recently integrated in the Implementing Regulation (EU) 2022/1426 [26] is the \"In-Service Monitoring and Reporting\" (ISMR), which addresses in-service safety of ADS post-market (i.e., operational experience feedback loop). It is primarily based on in-service data collection to assess whether the ADS continues to be safe when operated on the road and identify safety risks [32].\nThe AI Act [17] also outlines several post-market actions, including market surveillance, human oversight and monitoring (as a requirement), and post-market monitoring. Post-market monitoring involves all activities carried out by providers of AI systems to collect and review experience gained from the use of AI systems to identify any need to immediately apply corrective or preventive actions. This idea is well aligned with the ISMR concept."}, {"title": "3.3 Crash investigation", "content": "Despite diligent design, planning, and safety regulations, accidents occur. Investigations into root causes identify and address contributing factors to prevent recurrence and/or mitigate negative outcomes. The investigation depth varies, ranging from counting crashes in a timeframe to detailed examinations of involved parties and vehicle operation assumptions. Collisions typically involve a mix of human, machine or system, and environmental factors. When AVs are involved in an accident, investigators must establish the required investigation depth. The need of police officers often end once they file a report recording scene, damage, interviews, and traffic law violations. These facts might be contributing factors but at this point, they represent basic background information for examining how an automated system may have failed. Collecting facts and assembling a chronology of events leads to a causal chain establishment. However, \"probable cause\" determination for crash investigations is not the same as for criminal prosecutions, which require \"facts beyond a reasonable doubt\".\nThe investigation depth is a balance between available resources and required information. All investigations involve fact collection, analysis of how facts fit together, and proceeding forward with new understanding. The causal chain typically involves a mix of man and machine. Various methods convey information and establish crash causes, including the \"Five M's and E\" [33], the \"Swiss Cheese Model\u201d citeReason2006, and the \"Fault Tree Analysis\" method [34]. Recorded information and facts are often embedded in multiple vehicle devices and other sources. The recorded data may contain a variety of parameters, requiring manufacturer assistance to obtain and specialized software to interpret. From the investigation and fact collection, a chronology of the accident can be established to identify contributing factors and probable cause.\nWith increasing numbers of vehicles with ADAS and ADS, investigators may question how much these automated systems contributed to collisions. Factors complicating this include drivers' lack of understanding of how ADAS/ADS work, blame shifting to the vehicle, proprietary nature of ADAS/ADS designs, increased time required for ADAS/ADS investigations and potential investigator shortage, and lack of ADAS/ADS training for collision investigators.\nWhile ADAS or ADS in ground vehicles is relatively new, aviation automation has existed for 110 years, maturing into digital system forensics. Despite design and standard differences, the basic elements involved in ground vehicles and aviation remain a continuous loop of requirements, outputs, actuators, and feedback. Human involvement extends to each of these, necessitating context investigation about man and machine involvement in each aspect, as well as potential environmental contributions. Aviation has a long history of autopilot use with occasional involvement in accidents. The two main autopilot-related accident causes have been the human interface and the lack of human supervision or misuse. Two examples are the accidents of the Air France A330 flying from Rio de Janeiro to Paris in 2009 [35] (human interface issue), and the American Airlines Flight 965 flying from Miami to Cali in 1995 [36] (human supervision issue). Similarly, numerous accidents involving ADAS and ADS in road vehicles have occurred when drivers have fallen asleep or intentionally tried to let the vehicle do more than the automation level was intended [37, 38]."}, {"title": "4 Cybersecurity audits for AI components", "content": "Cybersecurity standards have been established for motor vehicles [39], with ADS introducing an additional layer of complexity due to AI components. Existing standards may not necessarily apply to AI [40], which has been shown to introduce security vulnerabilities, impacting any system relying on or incorporating AI components. We consider a security vulnerability as the potential for malicious activity that alters the behaviour [41\u201344] or leaks data [45\u201348] from an AI component, designed for a specific task. To prevent attacks on critical infrastructure, like AVs, we propose an AI security audit structure for standardized assessment of system security against specific threats or resistance to information leakage.\nBefore discussing an audit structure, we need a risk assessment of likely threat occurrence. We propose four levels of AI security research concreteness. The first level describes theoretical research independent of the application or ADAS/ADS, and the fourth level describes a vulnerability exploited in practice. From this perspective, we review three relevant security threats associated with AI components in ADS: evasion, poisoning, privacy and IP. Other cybersecurity risks independent of the AI components will not be discussed here. For an overview, we refer to [49]."}, {"title": "4.1 Security threats", "content": "This section primarily focuses on classification, vision and LiDAR tasks, due to readily available works in the area and the direct exposure of perception modules to the outside world, potentially increasing vulnerability. In discussing practicality of studying threats, we propose four levels of concreteness related to ADAS and ADS threats (see Figure 3):\nI A vulnerability demonstrated on an AI algorithm used in an AV on a non-vehicle related task (e.g. face detection).\nII A vulnerability demonstrated on an AI algorithm used in an AV on a relevant task (e.g. image segmentation or traffic sign classification).\nIII A vulnerability demonstrated on an AI algorithm used in an AV on a relevant task, with experiments showing that the vulnerability is presence in a vehicle or similar testbed.\nIV A reported vulnerability affecting an on-road AV.\nWhile concreteness increases towards real-world threats, a vulnerability can directly occur at level III or IV without being observed at lower levels first. Studying a vulnerability at level I is not a prerequisite for its occurrence at concreteness II, III, or IV.\nIt is also relevant to distinguish between adversarial and benign accuracy when considering AI component vulnerability in AVs. Both are based on accuracy, measured on a specific dataset called the test set, while the AI is optimized on a separate dataset, the training data. Adversarial accuracy is the accuracy when an attacker targets the system to decrease performance using a vulnerability by altering training or test data, or both. Benign accuracy is the accuracy when no attacker is present, though performance decreases may still occur, for example, when test and training data differ. This may happen when a car is released for production and operated in real-world traffic as opposed to recorded scenes, where unknown weather conditions may occur, or unanticipated car behaviour may be observed."}, {"title": "4.1.1 Evading AVs", "content": "In evasion attacks, an attacker alters inputs at test time. These modified inputs may appear as one class to a human observer but get misclassified by the model despite otherwise good performance. For example, a traffic sign recognition algorithm might misclassify a manipulated \"stop sign\" image as a \"yield sign\" assignment [50]. Such manipulations can be achieved through classifier-specific graffiti [50] or subtle changes to the image [51] or sign shape [50, 52]. Evasion is a vulnerability that can occur at deployment or during testing stages and exists across all types of classifiers [53] and input data or tasks [50, 52, 54], often referred to as adversarial examples. AV-specific evasion attacks can target visual sensors [50] or other sensors like LiDAR [52]. They can also directly target the vehicle's planner or affect end-to-end driving by adding shadows or lines on the road [55]. Most attacks proposed on AVs focus on one component and do not trigger complex behaviours. We attempt to give a concise overview of suggested attacks in Table 1. Real-world incidents with vision systems [56, 57] are generally assumed to be benign incidents, not attacks by a malicious entity.\nMany AI engineers and practitioners consider evasion or adversarial examples more of a performance issue than a security problem [57]. Traditional security vulnerabilities (i.e., non-AI related) seem to be more prevalent in practice [57]. However, defence"}, {"title": "4.1.2 Poisoning of AVs", "content": "In contrast to the latter backdoor attacks, there are few poisoning attacks on systems used in AVs. The reason for this inherent complexity of poisoning attacks is that, in contrast to for example evasion attacks (that derive a change against a static, deployed AI), poisoning attacks aim to make the AI deviate in a malicious way during training, where the AI is still dynamically adapting to the data [61]. Some works exist on deep learning [61, 62], where [62] tackle traffic sign recognition as used in AVs. However, also the path finding of the vehicle can be poisoned to only find a sub-optimal route [63].\nIn poisoning attacks, an attacker alters training data, which affects test time performance. They can degrade data quality [62] or install a backdoor, causing the model to misclassify inputs when a specific pattern or trigger is present [41]. These attacks are particularly severe as affected models perform well as long as the trigger is not present [41, 64].\nDespite the complexity when compared with evasion, there are several examples. Some works exist on perception [61, 62] and planning [63] operational layers. However, backdoor attacks are easier to implement by controlling part of the training data [61]. They have been tested on vision models [41], image segmentation [65], LiDAR sensors [42, 43], and even on traffic congestion control systems [64]. Most of these attacks"}, {"title": "4.1.3 Attacks on privacy and IP in AVs", "content": "Privacy concerns in AVs generally arise in the context of data leakage from an AI model [46]. Depending on the data used for training (e.g., real world crashes investigations), data leakage might have privacy implications. It is currently unclear whether video sequences can be retrieved from a model in sufficient detail. A more significant concern is when data from different users is combined to train a new AI, e.g, to improve AV usability based on individual cars. This has been shown to be vulnerable to attribute inference [70] or to the complete training inputs [71]. Independent of the data used, the model can become a target of an attack where it is copied without the owner's consent by observing input and outputs [48] or other data flow within hardware"}, {"title": "4.2 Testing and auditing", "content": "Given the diverse vulnerabilities discussed, the need and utility for audits vary. In the case of evasion, an audit is useful as it evaluates the model's benign accuracy. Adding these tests enables easy later extension against evasion attacks, and auditing benign accuracy indirectly tests for poisoning. Any production model should be audited against existing backdoors. As for privacy, the AV must be audited to prevent leakage of potential users' sensitive information. Lastly, audits related to IP leakage can be made optional, depending on the model owner's wishes, and need not be conducted by a third party.\nWe suggest high-level audit criteria for all vulnerabilities, which include:\n\u2022 Validating the design of the model.\n\u2022 Testing the robustness of the model within the system.\n\u2022 Testing the soundness of potential defence measures within the system.\nThe first point refers to the correct usage of data, which steps (or defences) are implemented and why, and which methods are chosen. This is particularly relevant concerning privacy or when methods with formal guarantees are relied on. The audit then needs to assess if the formal criteria match the application and make sense within"}, {"title": "4.2.1 Prerequisite for a cybersecurity audit", "content": "In a nutshell, a cybersecurity audit should assess the model's robustness using an appropriate attack (evasion) or assessment (backdoors, privacy). This raises the question of good attacks and assessments, which generally differ in their threat model and complexity, encompassing access to the model, expected changes by the attacker (norms), and the complexity of the evaluation.\nModel access: whether the attack uses insider knowledge about this model (white-box) or not (black-box). White-box attacks are considered a worst-case scenario and are stronger than black-box attacks. However, they can also give a false sense of security when not properly tailored for the model. In AVs, we can likely assume that the AI components will be proprietary and thus black-box for an attacker (unless there is an IP breach). For audits, white-box access must be given to study a worst-case attacker, which requires the audit to be performed by a trustworthy, independent third party that is provided confidential access to the model.\nNorms: how an attacker may alter a sample reaching the AI component. To measure the change added to a sample, distance or similarity measures are often used. The most basic functions are called $L_p$ norms, where p denotes the type of alteration captured by this specific norm. For instance, the $L_0$-norm results in sparse, graffiti like alterations, whereas the $L_\\infty$-norm creates minor changes in an image. While much of the research focuses on these norms for vision, other norms that mimic human perception [75] can also be utilized. For LiDAR sensors, different norms that focus on adding a consistent object are required [52]. The norm needs to be chosen consistently with the task and robustness requirements. However, there is no scientific consensus on a norm-based threat model for autonomous cars that would help determine which norms or similarities can be used for an evaluation.\nAudit complexity: the average expected run-time of the evaluation. Complete search of vulnerability is possible [60], but not practical. When choosing a method for auditing, a trade-off between run-time and expected success rate to identify a vulnerability or data leak needs careful consideration. Any corresponding choices should be well documented.\nDeveloping new evasion attacks is an active area of research [76]. Complex attacks are often preferred as they are more likely to find a malicious sample. An important distinction when auditing evasion is whether targeted (e.g., image should be classified as 'yield') or untargeted (e.g., output should not be 'stop sign') attacks are audited. Targeted attacks, while providing greater insight into model analysis and posing higher potential harm in autonomous driving, are more challenging to execute due to their requirement for proprietary access. They are also less transferable to unseen models."}, {"title": "4.2.2 Implementing cybersecurity audits", "content": "Implementing audits for a set of attacks or defences necessitates the use of specific libraries and tools, many of which focus on image classification. Table 3 provides an overview of these, including supported frameworks and their number of implemented attacks and defences. Notably, ART, SecML, and Privacy Meter support more diverse areas. SecML and Microsoft's counterfit\u00b9 allow command-line robustness testing, while ART and foolbox offer plug-in capabilities."}, {"title": "4.2.3 A cybersecurity audit for a system composed by several AI components", "content": "While auditing individual AI components provides insights into their vulnerabilities, it does not guarantee an accurate approximation of the system's overall vulnerabilities. There is limited research on auditing a system comprising multiple AI components.\nA possible approach, inspired by the V-model from ISO26262 [85] software testing, involves testing each model in isolation before integrating and testing subsequent systems. Alternatively, testing the entire system end-to-end is suggested [55], with individual components only investigated upon system failure [86]."}, {"title": "4.2.4 Cybersecurity audit reports for AI components in AVs", "content": "The output of AI components' cybersecurity audits in AVs should mirror a detailed defence evaluation [59]. This includes a rationale for chosen attacks/assessments, their parameters, and norms; benign and attack accuracies or assessment performance with associated hyper-parameters; results of basic sanity checks; and the AV components involved in the test. Evasion sanity checks, as in [59], involve increasing attack perturbation leading to a rise in attack success rate, as well as substantial perturbation reducing model accuracy to random guessing levels. However, similar guidelines for backdoor testing or privacy assessments are currently unavailable. Privacy attack tests, though theoretically possible with overfitting leading to higher inference accuracy [45], are often unattainable due to the unavailability of the model owner's training data. Audits should be conducted before AI model deployment and repeated upon significant model updates or advancements in attack techniques. To prevent model providers from tuning models to audit challenges, audit tests should not become public. Attacks can aid in developing robust models if applied during training and evaluation by ML engineers or specialized teams (e.g., AI red team). However, as stated above, to prevent overfitting to a specific attack, the audit must employ tests not used during training."}, {"title": "4.3 Challenges", "content": "Our limited understanding of AI security threats presents challenges in auditing AI components, especially within complex systems like AVs. The discrepancy between theory and practice is evident in the wealth of academic work versus the scarcity of real-world incidents, resulting in a lack of realistic threat models. This hampers our ability to test relevant attacks and understand the effects an attack might have on an individual AI component within a larger system such as an AV. This is amplified by the limited experience in auditing standalone AI systems.\nModels are often optimized to pass audits, leading to 'gaming' the audit rather than ensuring robustness. Given the limited number of attacks available in libraries (Table 3), this is a pressing concern. The re-implementation of attacks from scientific works can introduce potential errors and inaccurate robustness assessments. This problem is amplified for non-vision based AV modules, such as LiDAR-based, with fewer existing implementations.\nWhile evasion attacks are most commonly implemented, followed by poisoning and IP/privacy attacks, our understanding of securing models against all these attacks"}, {"title": "5 Transparency through explainability", "content": "Explainability in AV testing and validation ensures system safety and reliability by providing an understanding of the underlying algorithms and decision-making processes. It enables performance evaluation in various scenarios and promotes transparency and accountability. As AVs become more prevalent, explaining their behavior and decisions to stakeholders and the public is crucial for building trust and confidence."}, {"title": "5.1 Explainability in AVS", "content": "In the following we provide a high-level description of the operational layers described in Section 1 a review of previous work on explainability in each operation. In [7", "layers.\nLocalisation": "it is critical for the safe operation of AVs [87", "89": ".", "90": ".", "91": ".", "92": "explanations concerning localisation can be useful for system developers for debugging AVs because they can facilitate positional error correction. Explanations can also provide other stakeholders with a perception of reliability and/or safety.\nPerception: perception and scene understanding are fundamental requirements for providing intelligible explanations. To this end, several data-driven explanation methods have been applied to the AV perception task. To train, test and validate explanation methods datasets play a critical role.\nDatasets for training machine learning approaches in the context of AVs [93", "95": "vehicle trajectories [96", "98": "or anomaly identification with bounding boxes [95, 99"}, {"92": "datasets have been categorised by the sensors used for the data collection (exteroception and proprioception), and the annotations that are useful for developing explainable AVs. Furthermore, different stakeholders that can potentially benefit from the explanations have been identified.\nVision-based explanation methods represent an important class of (deep learning) methods that have been proposed. Gradient-based or backpropagation methods are generally used for explaining convolutional neural network models [100", "102": "the reader can find extensive surveys on vision-based explanation methods, from Class Activation Map (CAM) [103", "104-107": "to other methods [108-110", "94": "an approach for textual explanation generation for AVs which was trained on the BDD-X dataset was proposed, and in [99", "111": ".", "112": "technique and visualization of learned attention. The illustrations provided for four carefully selected scenarios showed the importance of spatial representation of the nearest traffic participants. The use of the learned attention as a mechanism to generate explainable motion predictions was also proposed for transformer-based models [113", "114": "."}, {"114": "developed an iterative, graphical attention approach with interpretable geometric (actor-lane) and social (actor-actor) relationships that support the injection of counterfactual geometric targets and social contexts. The proposed method supports the study of hypothetical or unlikely scenarios, so-called counterfactuals. In [115"}]}