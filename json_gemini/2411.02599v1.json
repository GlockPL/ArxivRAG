{"title": "Vocal Sandbox: Continual Learning and Adaptation for Situated Human-Robot Collaboration", "authors": ["Jennifer Grannen", "Siddharth Karamcheti", "Suvir Mirchandani", "Percy Liang", "Dorsa Sadigh"], "abstract": "We introduce Vocal Sandbox, a framework for enabling seamless human- robot collaboration in situated environments. Systems in our framework are charac- terized by their ability to adapt and continually learn at multiple levels of abstrac- tion from diverse teaching modalities such as spoken dialogue, object keypoints, and kinesthetic demonstrations. To enable such adaptation, we design lightweight and interpretable learning algorithms that allow users to build an understanding and co-adapt to a robot's capabilities in real-time, as they teach new behaviors. For example, after demonstrating a new low-level skill for \u201ctracking around\" an object, users are provided with trajectory visualizations of the robot's intended motion when asked to track a new object. Similarly, users teach high-level planning behaviors through spoken dialogue, using pretrained language models to synthesize behaviors such as \u201cpacking an object away\" as compositions of low-level skills concepts that can be reused and built upon. We evaluate Vocal Sandbox in two settings: collaborative gift bag assembly and LEGO stop-motion animation. In the first setting, we run systematic ablations and user studies with 8 non-expert participants, highlighting the impact of multi-level teaching. Across 23 hours of total robot interaction time, users teach 17 new high-level behaviors with an aver- age of 16 novel low-level skills, requiring 22.1% less active supervision compared to baselines and yielding more complex autonomous performance (+19.7%) with fewer failures (-67.1%). Qualitatively, users strongly prefer Vocal Sandbox systems due to their ease of use (+20.6%), helpfulness (+10.8%), and overall performance (+13.9%). Finally, we pair an experienced system-user with a robot to film a stop- motion animation; over two hours of continuous collaboration, the user teaches progressively more complex motion skills to shoot a 52 second (232 frame) movie.", "sections": [{"title": "1 Introduction", "content": "Effective human-robot collaboration requires systems that can seamlessly work with and learn from people [1-3]. This is especially important for situated interactions [4-7] where robots and people share the same space, working together to achieve complex goals. Such settings require robots that continually adapt from diverse feedback, learning and grounding new concepts online. Yet, recent work [8-15] remain limited in the types of teaching and generalization they permit. For example, many systems use language models to map user utterances to sequences of skills from a static, predefined library [16-18]. While these systems may generalize at the plan level, they trivially fail when asked to execute new low-level skills \u2013 regardless of how simple that skill might be (e.g., \"hold this still\"). Instead, we argue that situated human-robot collaboration requires learning and adaptation at multiple levels of abstraction, empowering collaborators to continuously teach new high-level planning behaviors and low-level skills over the course of an interaction.\nBuilding on this insight, we present Vocal Sandbox, a framework for situated human-robot collabora- tion that enables users to teach through diverse modalities such as spoken dialogue, object keypoints, and kinesthetic demonstrations. Systems in our framework consist of a language model (LM) planner"}, {"title": "2 Vocal Sandbox: A Framework for Continual Learning and Adaptation", "content": "Vocal Sandbox is characterized by a language model planner that maps user utterances to sequences of high-level behaviors and a family of skill policies that ground behaviors to robot actions (Fig. 2)."}, {"title": "2.1 High-Level Planning with Language Models", "content": "We implement high-level planning using a language model (LM) prompted with an API specification $A_t = (F_t, C_t)$ (Fig. 2; Left) that consists of a set of functions $F_t$ (synonymous with \u201cbehaviors\") and argument literals $C_t$, indexed by interaction timestep t \u2013 this indexing is important as users can teach new behaviors (add to the API) over the course of a collaboration. Each argument $c \\in C_t$ is a typed literal \u2013 for example, we define Enums such as Location that associate a canonical name (e.g., Location.HOME in Fig. 2) to values that are used by the robot when executing a skill. We define each function $f \\in F_t$ as a tuple $(\\eta, \\sigma, d, b)$ consisting of a semantically meaningful name n (e.g., goto), a type signature $\\sigma$ (e.g., [ObjectRef] $\\rightarrow$ None), human-readable descriptive docstring d (e.g., \u201cMove above the specified obj\u201d), and function body b. Crucially, we assume we can define new function compositions \u2013 for example pickup (obj: ObjectRef) as goto(obj); grasp().\nGiven a user utterance $u_t$ at time t, the language model generates a plan $p_t$ that is com- prised of a program, or sequence of function invocations. For example, a valid plan for an instruction such as \"place the candy in the gift bag\" subject to the API in Fig. 2 would be $p_t = pickup(ObjectRef.CANDY); goto(ObjectRef.GIFT\\_BAG); release()$. We formalize planning as generating $p_t = LM(\\cdot \\vert u_t, A_t, h_t)$, where $u_t$ is the user's utterance, $A_t$ is the current API, and $h_t$ corresponds to the full interaction history through t. Concretely, $h_t = [(u_1, p_1), (u_2, p_2),... (u_{t-1}, p_{t-1})]$; see our project page for full prompts.\nNote that there are cases where the language model may not be able to generate a valid plan \u2013 for example, given an utterance describing a behavior that does not exist in the API (e.g., \"can you pack a gift bag with three candies\u201d). In such a situation, we raise an exception, and rely on the commonsense understanding of the LM to generate a helpful error message (e.g., \u201cI am not sure how to pack; could you teach me?\"). These exceptions (as well as the successfully generated plans) are shown to users via a custom graphical interface (GUI; \u00a73.3) to inform their next action.\""}, {"title": "2.2 Low-Level Skills for Action Execution", "content": "To ground plans $p_t$ to robot actions, we assemble skill policies $\\pi_{\\epsilon}(\\alpha \\vert O_t, [c_1, c_2, ...])$ that define the execution of a function $f \\in F$ as a mapping from a state observation $O_t \\in \\mathbb{R}^n$ and individual \u201cresolved\u201d arguments $[c_1, c_2, . . .]$ to a sequence of robot actions $a \\in \\mathbb{R}^{T\\times D}$ (e.g., end-effector poses). Of particular note is the resolution operator $[\\u2022]$ that maps an LM-generated plan $p_t$ to a sequence of skill policy invocations; to do this, we first iterate through each function invocation in $p_t$ and"}, {"title": "2.3 Teaching via Program Synthesis", "content": "A core component of Vocal Sandbox is the ability to synthesize new behaviors and skills from diverse modalities of teaching feedback such as spoken dialogue, object keypoints, and kinesthetic demonstrations. To do this, we leverage the commonsense priors and strong generalization ability of our underlying language model to synthesize new functions and arguments, updating the API $A_t$ in real-time. Given an failed plan, each \u201cteaching\u201d step first uses the language model to vocalize the \"missing\" concept(s) to be taught, followed by a interaction that prompts the user to provide targeted feedback. The language model then synthesizes new functions and arguments for the API. Fig. 2 (Right) shows the two types of teaching we develop: 1) argument teaching and 2) function teaching.\nThe goal of argument teaching is to teach and ground new literals $\\hat{c} \\in C$ \u2013 for example, identifying \"green toy car\" as a new argument given the utterance \"Grab the green toy car\" in Fig. 2 (Right). To do this, the LM parses as much of the utterance as possible subject to the current API, mapping \"grab\" to pickup. Because it is able to identify the correct function (but not the arguments), the LM then uses the corresponding type signature to infer that \"green toy car\" should be of type ObjectRef; it then automatically synthesizes an API update, adding a new literal ObjectRef.TOY\\_CAR. This addition is then shown to the user (as part of the second stage of teaching), who then then provides the supervision needed to successfully ground the literal (i.e., providing the keypoint supervision to localize the object). On successful execution, the LM commits this change, yielding $A_{t+1}$.\nThe goal of function teaching is to teach new functions $f \\in F$ \u2013 for example, defining pack from the utterance \"now can you pack the candy in the bag\" in Fig. 2 (Right). Here, the LM cannot partially parse the utterance \u2013 \u201cpack\u201d does not have an associated function, so there is no reliable way to infer a type signature. Instead, the LM highlights \u201cpack\u201d as a new behavior, and explicitly asks the user to teach its meaning through decomposition [23, 24], breaking \"pack\" down into a chain of existing skills. In this case, the user says: \"Pick up the candy; go above the bag; drop it\" with program pickup(ObjectRef.CANDY); goto(GIFT\\_BAG); release(). The LM then explicitly synthesizes the new function $f = (\\hat{n}, \\hat{\\sigma}, d)$, with name $\\hat{n} = pack$, signature $\\hat{\\sigma} = obj: ObjectRef \\rightarrow None$, and docstring d = \"Retrieve the object and place it in the gift bag.\u201d We also generate the \u201clifted\u201d body pickup(obj); goto(GIFT\\_BAG); release() via first-order abstraction [24, 25].\nThis combination of argument and function teaching enables the expressivity and real-time adaptivity of our framework; in defining lightweight algorithms for learning and synthesizing new API specifi- cations from interaction, we provide users with a reliable method of growing and reasoning over the robot's capabilities during the course of a collaboration."}, {"title": "3 Implementation & Design Decisions", "content": "While we introduce Vocal Sandbox as a general learning framework, this section provides implemen- tation details specific to the experimental settings we explore in our experiments. Notably, our first experimental setting involves a collaborative gift-bag assembly task (\u00a74.1) where a non-expert user and robot work together to assemble a gift bag with a set of known objects (visualized in Fig. 2). Our second setting (\u00a74.2) pairs an experienced system user (an author of this work) with a robot for the task of LEGO stop-motion animation (visualized in Fig. 1). For all settings, we use a Franka Emika"}, {"title": "3.1 Language Models for High-Level Planning", "content": "We use GPT-3.5 Turbo with function calling [v11-06; 27, 28] due to its high latency and cost-effective pricing. We encode $A_t$ as a Python code block (formatted as Markdown) in the system prompt $U_{prompt}$. To constrain the LM outputs to well-formed programs, we use the function calling feature provided by the OpenAI chat completion endpoint [28], formatting each function in $A_t$ as a \"tool\" that the LM can invoke in response to a user endpoint. Full code and prompts are on our project page."}, {"title": "3.2 Skill Policies for Object Manipulation and Dynamic Motions", "content": "We implement different families of skill policies $\\pi_f$ for each of our two experimental settings. In our first setting (\u00a74.1) we implement skills using a visual keypoints model, while for our second setting (\u00a74.2) we implement skills as dynamic movement primitives [DMPs; 29, 30].\nVisual Keypoint-Conditioned Policies for Object Manipulation. For the collaborative gift-bag assembly setting (\u00a74.1), we implement skills $\\pi_{goto}$ and pickup via learned keypoint-conditioned models that ground object referring expressions (e.g., \u201ca green toy car", "the green toy car\" always refer to the same object instance for the duration of the interaction (as opposed to predictions changing over time, confusing users). We provide further detail in \u00a7B.3.\nNote that we adopt this modular approach (predict keypoints from language, then extract a segmen- tation mask) for two reasons. First, we found learning a visual keypoints model to be extremely data efficient (requiring only a couple dozen examples) and reliable, significantly outperforming existing open-vocabulary detection and end-to-end models such as OWL-v2 [34, 35] as well as vision- language foundation models such as GPT-4V [36]; we quantify these results via offline evaluation in \u00a7B.3. Second, we found keypoints to be the right interface for interpretability and teaching: to specify new object, users need only \"click\" on the relevant part of the image via our GUI (\u00a73.3).\nLearning Dynamic Movement Primitives from Demonstration. For our stop-motion animation setting (\u00a74.2), the robot learns to control the camera to execute different cinematic motions such as panning, tracking, or zooming (amongst others); due to the dynamic nature of these motions, we implement skills as (discrete) DMPs [29, 30], where users teach new motions by providing a single kinesthetic demonstration; using DMPs allows us to generalize motions to novel start and goal positions, while also providing nice affordances for re-timing trajectories (e.g., speeding up a motion by a factor of 2x, or constraining that a motion executes in K steps). Furthermore, as rolling\"\n    },\n    {\n      \"title\": \"3.3 A Structured Graphical User Interface for Transparency and Intervention\",\n      \"content\": \"Finally, we design a graphical user interface (GUI; Fig. 5) to provide users with transparency into the system state and to enable teaching. Consider an utterance \u201cpack the candy\" and a failure mode that has the robot packing a different object (e.g., the ball) instead. Without any additional information, it is impossible for the user to identify whether this failure is a result of the LM planner (e.g., generating the incorrect plan pack(ball)), or a failure in the skill policy (e.g., incorrectly predicting a keypoint on the ball instead of the candy). The GUI counteracts this confusion by explicitly visualizing the plan and the interpretable traces produced by each skill (e.g., the predicted keypoints and segmentation mask, or the robot's intended path as output by a DMP) \u2013 these interpretable traces also double as interfaces for eliciting teaching feedback (e.g., \\\"clicks\\\" to designate keypoints). The GUI also provides 1) the transcribed user utterance, 2) the current \u201cmode\" (e.g., normal execution, teaching, etc.), and 3) the prior interaction history.\"\n    },\n    {\n      \"title\": \"4 Experiments\",\n      \"content\": \"We evaluate the usability, adaptability, and helpfulness of Vocal Sandbox systems through two experimental settings: 1) a real-world human-subjects study with N = 8 inexperienced participants for the task of collaborative gift bag assembly (\u00a74.1), and 2) a more complex setting that pairs an experienced system-user (author of this work) and robot to film a stop-motion animation (\u00a74.2). All studies were IRB-approved, with all participants providing informed consent.\"\n    },\n    {\n      \"title\": \"4.1 User Studies: Collaborative Gift Bag Assembly\",\n      \"content\": \"This study has participant work with the robot to assemble four gift bags with a fixed set of objects (candy, Play-Doh, and a toy car), along with a hand-written card (transcribed from a 96-word script). This task is repetitive and time-intensive, serving to study how well users can teach the robot maximally helpful behaviors to parallelize work and minimize their time spent supervising the robot.\nParticipants and Procedure. We conduct a within-subjects study with 8 non-expert user (3 female/5 male, ages 25.2 \u00b1 1.22). Each user assembled four bags with three different systems, with a random ordering of methods across users. Prior to engaging with the robot, we gave users a sheet describing the robot's capabilities (i.e., the base API functionality), and instructions for using any teaching interfaces (if applicable). Prior to starting the bag assembly task, users were allowed to practice using each method for the unrelated task of throwing (disjoint) objects from a table into a trash can.\nIndependent Variables \u2013 Robot Control Method. We compare the full Vocal Sandbox system (VS) to two baselines. First, VS - (Low, High) (without Low, High), a static version of Vocal Sandbox that ablates both low-level skill teaching and high-level plan teaching, reflecting prior work like MOSAIC\"\n    },\n    {\n      \"title\": \"4.2 Scaling Experiment: LEGO Stop Motion Animation\",\n      \"content\": \"Finally, to push the limits of our framework, we consider a LEGO stop motion animation setting, where an experienced system-user (author of this work) works with the robot over two hours of\"\n    },\n    {\n      \"title\": \"5 Related Work\",\n      \"content\": \"Vocal Sandbox engages with a large body of work proposing systems for human-robot collaboration that pair task planning with with learned skill policies for executing actions; we provide an extended treatment of related work in \u00a7C.3, focusing here on prior work that center language models for task planning, or introduce generalizable approaches for learning skills from different feedback modalities.\nTask Planning with Language Models. Recent work investigates methods for using LMs such as GPT-4 and PaLM [37-39] for general-purpose task planning [9, 16-18]. Especially relevant are approaches that use LMs to generate plans as programs [19, 40-43]. While some methods explore using LMs to generate new skills \u2013 e.g., by parameterizing reward functions [44, 45] - they require expensive simulation and offline learning. In contrast, Vocal Sandbox designs lightweight learning algorithms to learn new behaviors online, from natural user interactions.\nLearning Generalizable Skills from Mixed-Modality Feedback. A litany of prior approaches in robotics study methods for interpreting diverse feedback modalities, from intent inference in HRI [8, 46, 47] to learning from implicit expressions [11, 48] or explicit gestures [12, 49, 50]. With the advent of LMs, language has become an increasingly popular interaction modality; however, most methods are limited to specific types of language feedback such as goal specifications [13, 51] or corrections [14, 52, 53]. In contrast, Vocal Sandbox demonstrates that language alone is not sufficient when teaching new behaviors \u2013 especially for teaching new object groundings or dynamic motions. Instead, our framework leverages multiple feedback modalities simultaneously to guide learning.\"\n    },\n    {\n      \"title\": \"6 Discussion\",\n      \"content\": \"We present Vocal Sandbox, a framework for situated human-robot collaboration that continually learns from mixed-modality interaction feedback in real-time. Vocal Sandbox has two components: a high-level language model planner, and a family of skill policies that ground plans to actions. This decomposition allows users to give targeted feedback at the correct level of abstraction, in the relevant modality. We evaluate Vocal Sandbox through a user study with N = 8 participants, observing that our system is preferred by users (+13.9%), requires less supervision (-22.1%) and yields more complex autonomous performance (+19.7%) with fewer failures (-67.1%) than non-adaptive baselines.\nLimitations and Future Work. As execution relies on low-level skills that are quickly adapted from sparse feedback, this framework struggles in dexterous settings (e.g., assistive bathing) where more data is necessary to capture behavior nuances. Another shortcoming is that the collaborations enabled by our system are relatively homogeneous \u2013 users are teachers and robots are followers \u2013 which is not suited for all settings. Future work will explore algorithms for cross-user improvement as well as sample-efficient algorithms to learn more expressive skills. We will also further probe the user's model of robot capabilities to investigate questions about human-robot trust and collaboration styles.\"\n    },\n    {\n      \"title\": \"A Motivating Questions\",\n      \"content\": \"Q1. If I wanted to implement a Vocal Sandbox from scratch, what components would I need? How do the current experiments handle real-time speech-to-text and text-to-speech? What about pricing\nwhat was the cost of running the Gift-Bag Assembly User Study (N = 8)?\nBeyond the language model task planner that uses GPT-3.5 Turbo with function calling [v11-06; 27, 28], and the (lightweight) learned skill policy, we use a combination of Whisper [54] for real-time speech recognition (mapping user utterances to text), and the OpenAI text-to-speech (TTS) API [55] for vocalizing confirmation prompts and querying users for teaching feedback. All models, cameras, and API calls are run through a single laptop equipped with an NVIDIA RTX 4080 GPU (12 GB).\nFor the gift-bag assembly user study (N = 8), the total cost of all external APIs (Whisper, OpenAI TTS, GPT-3.5 Turbo) amounted to $0.47 + $0.08 + $1.24 = $1.79. For the entirety of the project, GPT-3.5 API spend was $5.79, with ~$4.00 spent on Whisper and TTS (< $10.00 total).\nQ2. Given the use of powerful closed-source foundation models such as GPT-3.5/GPT-4, why adopt a modular approach for implementing the visual keypoints (and similarly dynamic movement primitives for learning policies)? Why not adopt an end-to-end approach building on top of GPT-4 with Vision, or existing pretrained multitask policies?\nWe choose to adopt a modular approach in this work for two reasons. First, existing end-to-end models are still limited when it comes to fine-grained perception and grounding; we quantify this more explicitly through head-to-head static evaluations of our keypoint model vs. pretrained models such as OWLv2 [34, 35] in \u00a7B.3. Second, we argue that modularity allows users to systematically isolate failures and address them via multimodal feedback, at the right level of abstraction. We expand on this further in \u00a7C.1.\nQ3. The baseline methods in the user study (\u00a74.1) are framed as ablations, rather than instances of existing systems that combine language model planning with learning from multiple modalities (e.g., on-the-fly language corrections, gestures). How were these ablations chosen? What is their explicit relationship to prior work?\nIn our user studies, we constructed our baselines in a way that best represented the contributions of prior work while still fulfilling the necessary prerequisites to perform in our situated human-robot collaboration setting. Though we labeled these \u201cablations": "n the paper, each one is representative of prior work \u2013 connections we make explicit in \u00a7C.2.\nQ4. How does Vocal Sandbox fit into the context of prior human-robot interaction works? What are the new capabilities Vocal Sandbox is bringing to the table?\nWhile the main body of the paper situates our framework against prior work in task planning and skill learning from different modalities, Vocal Sandbox builds on a rich history of work that develops systems for different modes of human-robot interaction. We provide an extended treatment of related work, as well as directions for future work in \u00a7C.3."}, {"title": "B Implementing Vocal Sandbox", "content": "Implementing a system in the Vocal Sandbox framework requires not only the learned components for language-based task planning and low-level skill execution, but broader support for interfacing with users via automated speech-to-text, text-to-speech for vocalizing failures or confirmation prompts, as well as a screen for visualizing the graphical user interface. We describe these additional components, as well as provide more detail around the implementation of the learned components of the systems instantiated in our paper over the following sections."}, {"title": "B.1 System Architecture", "content": "For robust and cheap automated speech recognition (mapping user utterances to text), we use Whisper [27, 54], accessed via the OpenAI API endpoint. Whisper is a state-of-the-art model meant for natural speech transcription, and we find that the latency for a given transcription request (< 0.5s round-trip) is more than enough for all of our use-cases. API pricing is also affordable, with the Whisper API (through OpenAI) charging $0.006 / minute of transcription (less than $0.50 to run our entire gift-bag assembly user study). Note that we implement speech-to-text via explicit \u201cpush-to-talk", "always-listening\" approach; we find that this not only allows us to keep cost and word-error rate down, but improves user experience. By gating the listening and stop-listening features with explicit audio cues, users are more aware of what the system is doing, and can more quickly localize any failures stemming from malformed speech transcriptions.\nIn addition to automated speech-to-text, we adopt off-the-shelf solutions for real-time text-to-speech; this is mostly for implementing confirmation prompts (\u201cdoes this plan look ok to you?\") and for vocalizing the system state, but also includes an adaptive component when probing users to teach new visual concepts or behaviors (\u201cI'm sorry, I'm not sure what the 'jelly-candy thing' looks like, could you teach me?\"). For these queries, we use the OpenAI TTS API [55] with a similarly affordable pricing scheme of $15.00 per 1M characters (or approximately 200K words); to run our gift-bag assembly study, this cost fewer than $0.08. For hardware (for both speech recognition and text-to-speech), we use a standard USB speaker-microphone (the Anker PowerConf S3).\nTo visualize the graphical user interface to users, we use an external monitor (27 inches), placed outside of the robot's workspace. We drive the GUI, all API calls (speech recognition, text-to-speech, and language modeling via GPT-3.5 Turbo), ZED 2 camera, and all our learned models \u2013 including our visual keypoint-conditioned policy, FastSAM [32], and XMem [33] - from a single Alienware M16 laptop with an NVIDIA RTX 4080 GPU with 12 GB of VRAM; this laptop was purchased matching the DROID platform specification [26].\nModifications for Gift-Bag Assembly User Study. For the gift-bag assembly user study (N = 8) we implement the \u201cpush-to-talk": "peech recognition interface with physical buttons placed on the table; users are provided two buttons one for \"talking\" and one for \"cancelling\" the prior actions (which serves a dual function as a secondary, software-based emergency stop when the robot is moving). These buttons are placed on the side of the user's non-dominant hand, always within reach.\nModifications for LEGO Stop-Motion Animation. For the LEGO stop-motion animation study, we use the same components as above, with two additions. As the expert user is directing and framing individual camera shots during the course of the collaboration, they add an additional laptop (a MacBook, running Stop Motion Studio) to the workspace (disconnected from the rest of the system). As the user requires both hands free for this study (for articulating LEGO minifigures and structures, or editing the clip on their laptop), we replace the tabletop \u201cpush-to-talk"}, {"title": "B.2 Language Models for Task Planning and Skill Induction", "content": "As described in the main body of the paper, we use GPT-3.5 Turbo with function calling [v11-06; 27, 28] as our base language model for the task planner. This was the latest, most affordable, and highest latency language model at the time we began this work (prior to the release of GPT-4 and"}, {"title": "B.3 Visual Keypoint-Conditioned Policy Implementation", "content": "As described in the main body of the paper, we use three components to implement Vocal Sandbox's object manipulation skills: (1) a learned language-conditioned keypoints model, (2) a pretrained mask propagatation model [XMem; 33], and (3) a point-conditioned segmentation model [FastSAM; 32].\nOur learned keypoint model predicts object centroids from language, enabling us to generalize across object instances where XMem struggles. Given an RGB image $o_t \\in \\mathbb{R}^{H\\times W\\times 3}$ and natural language literal $C_{ref}$ from the high-level language planner, it predicts a matrix of per-pixel scores $H \\in [0,1]^{H\\times W}$. We take the coordinate-wise argmax of H as the predicted keypoint. We implement our model with a two-stream architecture following Shridhar et al. [57] that fuses pretrained CLIP [58] textual embeddings with a fully-convolutional architecture. We train this model on an small, cheap-to-collect dataset of 25 unique images each annotated with 3 keypoints (75 examples total). To fit our model, we create heatmaps from each ground-truth label, centering a 2D Gaussian around each keypoint with a fixed standard deviation of 6 pixels; we train our model by minimizing the binary cross-entropy between model predictions and these heatmaps, augmenting images with various label-preserving affine transformations (e.g., random crops, shears, rotations).\nOur mask propagation model, XMem [33] tracks object segmentation masks from one image frame to the next; we provide a brief overview here. XMem is comprised of three convolutional networks (a query encoder e, a decoder d, and a value encoder v) and three memory modules (a short-term sensory memory, a working memory, and a long-term memory). For a given image $I_t$, the query encoder outputs a query $q = e(I_t)$ and performs attention-based memory reading from working and long-term memory stores to extract features $F_{C_{ref}}$, where $C_{ref}$ is the language utterance (e.g., \"candy\"). The decoder d then takes as input q, $F$, and $h_{t-1}$ (the short-term sensory memory) to output a predicted mask $M_t$. Finally, the value encoder $v(I_t, M_t)$ outputs new features to be added to the memory history $h_t$. The query encoder e and value encoder v are instantiated with ResNet-50 and ResNet-18 [59] respectively. The decoder d concatenates the short-term memory history $h_{t-1}$ with the extracted features $F$, upsampling by a factor of 2x until reaching a stride of 4. While upsampling, the decoder fuses skip connections from the query encoder e at every level. The final feature map is"}, {"title": "B.4 Learning Discrete Dynamic Movement Primitives from Demonstration", "content": "For our LEGO stop-motion animation setting, we implement our low-level skill policy as a library of discrete Dynamic Movement Primitives [29, 30]. We adopt the traditional discrete DMP formulation from Ijspeert et al. [30], defining a second-order point dynamical system in terms of the system state $y$, a goal $g$, and phase variable x such that:\n$\\tau \\ddot{y} = \\alpha_y(\\beta_y(g - y) - \\dot{y}) + f(x, g); \\\\ \t\\dot{x} = -\\alpha_x x$\nwhere $\\alpha$ and $\\gamma$ define gain terms, $\\tau \\in (0,1]$ denotes a temporal scaling factor, and $f(x, g)$ is the learned forcing function that drives a DMP to follow a specific trajectory to the goal g; $f(x, g)$ is implemented as a learned linear combination of J radial basis functions and the phase variable x such that:\n$f(x,g) = \\frac{\\Sigma_{j=1}^J \\psi_j w_j}{\\Sigma_{j=1}^J \\psi_j} \\cdot x(g - y_0); \\\\ \\phi_j = exp(-h_j (x - c_j)^2)$\nwhere $c_j$ and $h_j$ are the heuristically chosen centers and heights of the basis functions, respectively. We fit the DMP weights $\\beta = \\{w_1, w_2 . . . w_j \\}$ with locally-weighted regression (LWR) from the provided kinesthetic demonstration. For all DMPs in this work, we use J = 32, with gain values $\\alpha_y = 25$, $\\gamma = \\frac{25}{4}$ and basis functions parameters set following prior work [30].\nWe choose (discrete) DMPs to implement skill learning as they permit efficient learning from a kinesthetic demonstration, and have two properties that enable rich generalization to 1) new goals (by"}, {"title": "B.5 Physical Robot Platform & Controller Parameters", "content": "We use a Franka Emika Panda 7-DoF robot arm with a Robotiq 2F-85 parallel jaw gripper following the platform specification from DROID [26]. The robot and its base are positioned at one side of a 3' x 5' table, across from the user, such that the user and robot share the tabletop workspace. We use an overhead ZED 2 RGB-D camera with known intrinsics and extrinsics. For robot control, we use a modified version of the DROID control stack based on Polymetis [61]. Low-level policies command joint positions at 10 Hz to a joint impedance controller which runs at 1 kHz. We implement two compliance modes: a stiff mode which is activated when the robot is executing a low-level skill, and a compliant mode for when the user provides a kinesthetic demonstration.\nSafety. We include multiple safeguards to ensure user safety. Users have the option to cancel any proposed behavior when an interpretable trace is presented with a physical Cancel button as described in \u00a7B.1 - this prevents execution and immediately backtracks the Vocal Sandbox system. Second, during execution of any low-level skill, the user can interrupt the robot's motion with this button as well. This halts the robot's motion and it immediately becomes fully compliant. Lastly, during user studies, both the user and proctor have access to the hardware emergency stop button which cuts the robot's power supply and mechanically locks the robot arm."}, {"title": "C Extended Discussion & Future Work", "content": "The following sections expand on the discussion from the main body of the paper, with a specific focus on the benefits of Vocal Sandbox's modular design, before providing an extended treatment of our contributions and future directions in the broader context of systems for human-robot collaboration."}, {"title": "C.1 On Modular vs. End-to-End Approaches", "content": "We"}]}