{"title": "Robust Markov Decision Processes: A Place Where AI and Formal Methods Meet", "authors": ["Marnix Suilen", "Thom Badings", "Eline M. Bovy", "David Parker", "Nils Jansen"], "abstract": "Markov decision processes (MDPs) are a standard model for\nsequential decision-making problems and are widely used across many\nscientific areas, including formal methods and artificial intelligence (AI).\nMDPs do, however, come with the restrictive assumption that the tran-\nsition probabilities need to be precisely known. Robust MDPs (RMDPS)\novercome this assumption by instead defining the transition probabili-\nties to belong to some uncertainty set. We present a gentle survey on\nRMDPs, providing a tutorial covering their fundamentals. In particular,\nwe discuss RMDP semantics and how to solve them by extending stan-\ndard MDP methods such as value iteration and policy iteration. We also\ndiscuss how RMDPs relate to other models and how they are used in\nseveral contexts, including reinforcement learning and abstraction tech-\nniques. We conclude with some challenges for future work on RMDPs.", "sections": [{"title": "1 Introduction", "content": "Markov decision processes (MDPs) are a fundamental model for tackling decision\nmaking under uncertainty across various areas, such as formal methods [72], op-\nerations research [99], and artificial intelligence [113]. At the core of MDPs is the\nassumption that the transition probabilities are precisely known, a requirement\nthat is often prohibitive in practice [11]. For example, in data-driven applica-\ntions in AI such as reinforcement learning (RL) [113], transition probabilities\nare unknown and can only be estimated from data. Furthermore, in formal ver-\nification problems, the state-explosion problem often prevents the model from\nbeing fully built [13, 14, 27]. As a remedy, sampling-based approaches that only\nestimate the transition probabilities, such as statistical model checking [6, 86],\nare used. Any sampling-based approach naturally carries the risk of statistical\nerrors and hence, incorrect estimates of the probabilities.\nRobust MDPs (RMDPs) overcome this assumption of precise knowledge of\nthe probabilities. An RMDP contains an uncertainty set that captures all pos-\nsible transition functions from which an adversary, typically called nature, may"}, {"title": "2 Markov Decision Processes and How to Solve Them", "content": "For a set X, we write |X| for its cardinality. Partial functions are denoted by\nf: XY, and we write I for undefined. A discrete probability distribution over"}, {"title": "2.1 Markov Decision Processes", "content": "We define Markov decision processes (MDPs) and their semantics.\nDefinition 1 (MDP). A Markov decision process (MDP) is a tuple of the form\n(S, S, A, P, R), where S is a finite set of states with s\u2081 \u2208 S the initial state, A is\na finite set of actions, P: S \u00d7 A \u2192 D(S) is the probabilistic transition function,\nand R: S \u00d7 A \u2192 R>o is the (non-negative) reward function.\nWe focus on expected reward objectives and hence omit a labelling function\nfrom our MDPs. We use partial functions for the transition and reward functions\nto allow for enabled actions. An action is enabled if P(s, a) is defined. We write\nA(s)\n A for the set of enabled actions at state s. We require that the transition\nand reward function are consistent with each other, that is, P(s, a) = 1 \u21d4\nR(s, a) = 1. For convenience, we write P(s, a, s') for the probability P(s, a)(s').\nA path in an MDP is an (in)finite sequence of successive states and actions:\nT = (80,A0, 81, . . . ) \u2208 (S \u00d7 A)* \u00d7 S where so = s, and Vi \u2208 N, P(Si, Ai, Si+1) > 0.\nA path is finite if the sequence is finite, T = (So, ao,..., Sk), for which we write\nlast(t) = sk for the last state. The set of all paths is denoted as Paths. The\nsequence of states in a path \u03c4 = (80, A0, 81, ...) is states(T) = (80, 81, ...).\nA discrete-time Markov chain (DTMC) is an MDP with only one enabled\naction in each state: \u2200s \u2208 S, |A(s)| = 1. For DTMCs, we omit the actions\naltogether from the tuple and write (S, s\u2081, P, R).\nA policy (also called scheduler or strategy) is a function that maps paths to\ndistributions over actions \u03c0: Paths \u2192 D(A). Such policies are called history-based\nand randomized. A policy is deterministic if it only maps to Dirac distributions\nover actions and stationary (also called memoryless) if it only considers finite\npaths of length one. Stationary deterministic (also called positional) policies are\nwritten as \u03c0: S \u2192 \u0391.\nGiven a policy for an MDP M, the action choices in M are resolved,\nresulting in a DTMC.\nDefinition 2 (Induced DTMC). Let M = (S, S\u0131, A, P, R) be an MDP and\n\u03c0: Paths \u2192 D(A) a policy. The induced DTMC is a tuple M\u2081 = (S*, \u03b4\u03b9, \u03a1\u03c0, R\u03c0),\nwhere S* is the (infinite) set of states, siis the initial state, and the transition\nand reward functions are defined as\n$P_\\pi(states(\\tau), states(\\tau): s') = \\sum_{a \\in A} \\pi(\\tau)(a) \\cdot P(last(\\tau), a, s'),$\n$R(states(\\tau)) = \\sum_{a \\in A} \\pi(\\tau)(a) \\cdot R(last(\\tau), a),$\nwhere T\u2208 Paths and states(T) : s' denotes concatenation of states(r) with s'."}, {"title": "Objectives.", "content": "The induced DTMC M has a unique probability measure PM that follows from\nthe standard cylinder set construction, see, e.g., [14, 40]. For stationary policies,\nthe set of states of the induced DTMC coincides with that of the MDP and is\nthus finite.\nObjectives. The primary objective we consider in this paper is cumulative\nreward maximization until reaching a state in some target set TCS [79]. We\nshall simply call this objective reach-reward. The goal is to compute the expected\nreward and an associated optimal policy for this objective in a given MDP M,\nwhich we formalize in the following subsection. Other common objectives include\nreachability, discounted expected reward, reach-avoid, and general temporal logic\nobjectives expressed in (probabilistic) LTL [96] or CTL [52]."}, {"title": "2.2 Classical Dynamic Programming", "content": "We review dynamic programming for MDPs with an infinite horizon undis-\ncounted reward objective in preparation for our discussion of robust dynamic\nprogramming in Section 3. We define state and state-action value functions\nV:SR and Q: S \u00d7 A \u2192 R, respectively. Dynamic programming updates\nthese value functions iteratively until the least fixed point is reached.\nWe preprocess the set of states S based on graph properties via the following\nstandard procedure [14] and PRISM-semantics for reward objectives [41, 79]. Let\nTCS be the target set of our objective, let S\u221e C S be the set of states for\nwhich there exists a policy that does not reach T almost-surely, and denote the\nremaining states by S? = S\\(TUS\u221e). For all a \u2208 A and target states s \u2208 T, let\nQ(s, a) = 0. Similarly, for all a \u2208 A and s \u2208 S\u221e, let Q(s, a) = \u221e. For all other\nstates s \u2208 S? and a \u2208 A, we initialize the state-action values as Q(s, a) = 0. We\niteratively update the state and state-action values for all (s, a) \u2208 S? \u00d7 A by:\n$V^{(n)}(s) = \\max_{a \\in A} Q^{(n)}(s,a), Q^{(n+1)}(s, a) = R(s, a) + \\sum_{s' \\in S} P(s, a, s')V^{(n)}(s').$\nThis process is also known as value iteration. When performing value iteration,\nwe do not need to keep track of the state-action values Q explicitly but instead\ncan directly compute the state-values V by setting V(s) = 0 for all s \u2208 T, for\nall s \u2208 S\u221e, V(s) = \u221e, and for all s \u2208 S? we iteratively compute:\n$V^{(n+1)}(s) = \\max_{a \\in A} \\{ R(s, a) + \\sum_{s' \\in S} P(s, a, s')V^{(n)}(s') \\}$       (1)\nThe optimal value function V* is the unique least fixed point of the Bellman\nequation in Equation (1).\nFor many objectives in MDPs, such as reach-reward maximization, optimal\npolicies are stationary and deterministic, i.e., of type \u03c0: S \u2192 A [99]. An optimal"}, {"title": "Policy evaluation and improvement.", "content": "stationary deterministic policy \u03c0* that achieves value V* can be extracted by\nperforming the following one-step dynamic programming procedure:\n$\\pi^*(s) = argmax_{a \\in A} \\{ R(s, a) + \\sum_{s' \\in S} P(s, a, s')V^*(s') \\}$\nPolicy evaluation and improvement. As an alternative to value iteration,\nMDPs can also be solved through policy iteration. Policy iteration consists of two\nalternating steps: policy evaluation and policy improvement. Policy evaluation\nis the process of computing the value of an MDP for a given policy, which\nis also known as verifying or model checking the induced Markov chain from\nDefinition 2 [14]. The value of a stationary policy \u03c0: S \u2192 D(A) is computed by\nthe following Bellman equation:\n$V^{(n+1)}_\\pi(s) = \\sum_{a \\in A} \\pi(s, a) \\cdot (R(s, a) + \\sum_{s' \\in S} P(s, a, s')V^{(n)}_\\pi(s'))$\nAlternatively, we may explicitly construct the induced DTMC (S, S\u2081, \u03a1\u03c0, \u03a1\u03c0)\nfrom Definition 2, whose set of states coincides with that of the MDP (and is\nthus finite) as the policy is stationary.\nAfter evaluating the current policy and determining its value function V*,\nthe policy improvement step looks for a new policy \u03c0' that outperforms the\ncurrent policy as follows. First, compute the state-action values under as\n$Q_\\pi(s, a) = R(s, a) + \\sum_{s'} P(s, a, s')V(s'), \\forall s \\in S, a \\in A(s).$\nThe new policy \u03c0' is extracted as \u03c0'(s) = argmaxa\u2208 A Q\u03c0(s, a) for all s \u2208 S and\nhas a value at least as good as the previous policy, i.e., V, \u2265 V*. This process\nterminates as soon as the policy does not change anymore: \u03c0' = \u03c0, after which\n\u03c0is guaranteed to be optimal.\nModifications towards other objectives. Many other objectives, such as\nreachability and discounted reward, can be solved by straightforward modifi-\ncations to the Bellman equation. For maximizing the reachability probability\nof a target set TCS, the reward function is removed, and the preprocessing\nstep is changed to set Q(s, a) = 1 for all (s, a) \u2208 T \u00d7 A, and Q(s, a) = 0 for all\n(s, a) \u2208 S\u221e \u00d7 A. For discounted reward, the preprocessing is removed altogether,\nand all state-action pairs are initialized with Q(s, a) = 0. The Bellman equation\nfrom Equation (1) is modified for both cases, respectively:\n$Q^{(n+1)}(s, a) = \\sum_{S' \\in S} P(s, a, s')V^{(n)}(s'),$ (reachability)\n$Q^{(n+1)}(s, a) = R(s,a) + \\gamma \\sum_{S' \\in S} P(s, a, s')V^{(n)}(s').$ (discounted)\nThese modifications can also be directly applied to the state-value function V\nfrom Equation (1)."}, {"title": "3 Theory of Robust Markov Decision Processes", "content": "Having briefly recapped the basics of MDPs and dynamic programming, we now\nmove to robust MDPs. In the following, let X be a set of variables. An uncertainty\nset U is a non-empty set of variable assignments subject to some constraints and\nis defined as U = {f : X \u2192 R | constraints on f}.\nDefinition 3 (RMDP). A robust Markov decision process (RMDP) is a tuple\n(S, S, A, P, R), where the states S, initial state s\u0131, actions A and reward function\nRare defined as in standard MDPs, and P:U \u2192 (S \u00d7 A \u2192 D(S)) is the\nuncertain transition function.\nEssentially, the uncertain transition function P is a set of standard transition\nfunctions P: S \u00d7 A \u2192 D(S), and we may thus also write P\u2208 P for a transition\nfunction P that lies inside the uncertain transition function.\nWhile strictly speaking not required, it is convenient to define the set of\nvariables X to have a unique variable for each possible transition of the RMDP,\nsuch that X = {xsas' | (s,a,s') \u2208 S \u00d7 A \u00d7 S}. The uncertainty set U is then\na set of variable assignments, i.e., functions that map each variable to a real\nnumber, subject to constraints. These constraints may, for example, define each\nvariable's allowed range and encode dependencies between different variables.\nNote that we do not explicitly add a constraint that each state-action pair is\nassigned a valid probability distribution but leave this implicit in the definition\nof the uncertain transition function P. Alternatively, one can define RMDPS\nby having the uncertain transition function assign a function over the variables\nto each transition, effectively encoding the dependencies there, and having the\nuncertainty set only define the range of each variable. This construction would,\nhowever, require additional adjustments to move most of the discussion that\nfollows (most notably around rectangularity) from the uncertainty set to the\nuncertain transition function."}, {"title": "Objectives.", "content": "Example 1. Figure 1 depicts an MDP and an RMDP. Below are three possible\nuncertainty sets for this RMDP:\n$U\u2081 = \\{x0a1 \\in [0.1, 0.9] \\land X0b1 \\in [0.1, 0.9] \\land X2a0 \\in [0.1,0.9]\\},$\n$U2 = \\{x0a1 \\in [0.1, 0.4] \\land xob1 = 2x0a1 \\land X2a0 \\in [0.1, 0.9]\\},$   (2)\n$\u0418\u0437 = \\{x0a1 \\in [0.1, 0.4] \\land xob1 = 2X0a1 / X2a0 = X0a1\\}.$\nObjectives. For ease of presentation, we again focus on reach-reward maxi-\nmization as an objective. However, as there is no single transition function, the\ngoal is now to compute an optimal robust policy, meaning optimal against the\nworst-case probabilities in the model. What this worst-case exactly is, depends\non the semantics of the RMDP."}, {"title": "3.1 RMDP Semantics and Structural Assumptions", "content": "RMDPs can be seen as a game between the agent, who aims to maximize their\nreward by selecting actions, and an adversarial nature, who aims to minimize\nthe agent's reward by selecting variable assignments from the uncertainty set.\nNature hence simulates the worst-case transition function that the agent should\nbe robust against. This game interpretation can be fully formalized into a zero-\nsum stochastic game (SG), as we shall discuss further in Section 4.2."}, {"title": "Last-action observability.", "content": "Intuitively, the game is constructed by adding a new set of states S \u00d7 A for\nnature that consists of tuples of the state-action pairs the agent was in. At each\nsuch state-action pair, nature selects a variable assignment from the uncertainty\nset that determines the transition function PEP.\nThe precise rules of the game, and with that the semantics of RMDPs, that\ndetermine which variable assignments nature is allowed to choose are controlled\nby two factors: (1) possible dependencies between nature's choice of the variable\nassignments between different states or actions, known as (non)-rectangularity;\nand (2) whether previous choices by nature restrict its future choices, known as\nthe static and dynamic uncertainty semantics. These two factors determine the\navailable policies, i.e., transition functions, for nature, and thus the worst-case\ntransition function that the agent must be robust against. We now discuss both\nconcerns in more detail.\nDependencies between the variables, or lack thereof, immediately follow from\nthe constraints used to define the uncertainty set U. Independence between states\nor state-action pairs is commonly referred to as rectangularity. Informally, an\nuncertainty set U is state-action or (s, a)-rectangular if there are no dependencies\nbetween the constraints on the variables at different state-action pairs, and state\nor s-rectangular if there are no dependencies between constraints on the variables\nat different states. More formally, following standard notation [118]:\nDefinition 4 (Rectangularity). The uncertainty set U is (s, a)-rectangular if\nit can be split into lower dimensional uncertainty sets U(s,a) that only relate to the\nvariables at the respective state-action pair (s,a), such that their product forms\nthe whole uncertainty set: U = X(s,a)\u2208S\u00d7AU(s,a). Similarly, an uncertainty set\nU is s-rectangular if U can be split into lower dimensional uncertainty sets U(s)\nthat only relate to variables at state s, such that U = Xs\u2208SU(s)."}, {"title": "Static and dynamic uncertainty semantics.", "content": "Example 2. We revisit the RMDP in Figure 1b and the three possible uncer-\ntainty sets in Equation (2). The set U\u2081 is an (s, a)-rectangular uncertainty set, as\neach variable influences the transition probabilities in only one state-action pair.\nIn other words, there are no dependencies between constraints on the variables\nat different state-action pairs. In U\u2082 the transition probabilities for state-action\npairs (so, a) and (so, b) both depend on variable 2001. Therefore, U2 no longer\nhas independence between actions but is still s-rectangular. The final uncertainty\nset, U3, has dependencies between all variables and is, therefore, non-rectangular.\nThe type of rectangularity has, together with whether the uncertainty set is\nconvex or not, direct consequences for the computational complexity of policy\nevaluation, i.e., computing the value for a given policy, and the type of policy\nthat is sufficient to be optimal for discounted reward objectives under static\nuncertainty semantics. These results are due to Wiesemann et al. [118] and\npresented in Table 1.\nUnder s-rectangularity, an additional assumption is made that nature can\nno longer observe the last action of the agent. This assumption is mentioned ex-\nplicitly in [58, 59, 118] but often left implicit. Note that this assumption on the"}, {"title": "Static and dynamic uncertainty semantics.", "content": "Example 3 (Last-action observability). Figure 2 depicts an RMDP. Below is an\ns-rectangular uncertainty set:\n$U = \\{x0a1 \\in [0.1, 0.9] \\land X0a1 = X0b2\\}.$\nWhether or not nature observed the agent's last action determines whether or\nnot nature has to take the dependency between 20a1 and 2062 into account. If\nnature observes the agent's last action, it can achieve an expected reward of 55\nby choosing the maximal xX0a1 = X0b2 = 0.9 when observing action a, and by\nchoosing the minimal x0a1 = X0b2 = 0.1 when observing action b. If nature does\nnot have this information, it has to account for both possible agent actions. The\nbest course of action for nature is then to choose X0a1 = X0b2 = 0.5, leading to\nan expected reward of 75 regardless of the agent's choice.\nStatic and dynamic uncertainty semantics. The second point about\nRMDP semantics is whether nature's previous choice at a certain state-action\npair should restrict its possible future choices. To that end, Iyengar [64] intro-\nduced the notions of static and dynamic uncertainty semantics. Static uncer-\ntainty semantics require nature to play a 'once-for-all' policy: if the state-action\npair is revisited, nature is required to use the same variable assignment from\nthe uncertainty set as before. In contrast, under dynamic uncertainty semantics\nnature plays 'memoryless' and is free to choose any variable assignment at every\nstep. Simultaneous but independently, Nilim and El Ghaoui introduced these\nsemantics as time-stationary and time-varying uncertainty models [93]. Note\nthat these notions have only been introduced for (s, a)-rectangular RMDPs and\nare only of concern in cyclic, infinite horizon models. Interestingly, Iyengar [64]\nalso shows that the distinction between static and dynamic uncertainty does not\nmatter for reward maximization in (s, a)-rectangular RMDPs. A similar result\nwas established for reachability in interval Markov chains in [26]. We state the\nresult in general in the following lemma."}, {"title": "3.2 Robust Dynamic Programming", "content": "Lemma 1 (Static and dynamic uncertainty coincide [64]). Consider an\n(s, a)-rectangular RMDP where both agent and nature are restricted to stationary\npolicies, i.e., policies of type \u03c0: S \u2192 D(A). Let \u00f8st be the optimal policy under\nstatic uncertainty, and ady be the optimal policy under dynamic uncertainty se-\nmantics. The robust values of these policies coincide, i.e., Vast = Vody.\n3.2 Robust Dynamic Programming\nIn this section, we discuss how value iteration and policy iteration can be adapted\nrather straightforwardly for (s, a)-rectangular RMDPs.\nRemark 1 (Graph preservation). For computational tractability of robust dy-\nnamic programming, especially of objectives that rely on preprocessing the un-\nderlying graph, such as the reach-reward objective we consider, it is often as-\nsumed that the uncertainty set should be graph preserving. That is, all variable\nassignments in the uncertainty set U imply the same topology for the under-\nlying graphs. Hence, if there exists some P\u2208P with P(s, a, s') = 0 for some\ntransition, then all other P' \u2208 P should also have P'(s, a, s') = 0.\nRecall Equation (1), describing value iteration in a standard MDP. In an\nRMDP, we do not have access to a precisely defined transition function P: S \u00d7\nA\u2192D(S). Instead, we have the uncertain transition function P that defines a\nset of such transition functions PEP.\nRobust value iteration adapts value iteration by accounting for the worst-\ncase P\u2208Pat each iteration. This is achieved by replacing the inner sum\nEs'es P(s, a, s')Vn(s') by an inner minimization problem:\n$V^{(n+1)}(s) = \\max_{a \\in A} \\{R(s, a) + inf_{P \\in P} \\{ \\sum_{S' \\in S} P(s, a, s')V^{(n)}(s') \\} \\}$  (3)"}, {"title": "Convex optimization.", "content": "We write V instead of V, which is now the worst-case or pessimistic value of the\nRMDP. That is, V is a lower bound on the value the agent can possibly achieve.\nBest-case or optimistic interpretations also exist, which we discuss later.\nUnder our assumption that the uncertainty set U is (s, a)-rectangular, we\nmay replace the global minimization problem infPEP by a local one:\n$V^{(n+1)}(s) = \\max_{a \\in A} \\{R(s, a) + inf_{P(s,a) \\in P(s,a)} \\{ \\sum_{S' \\in S} P(s, a, s')V^{(n)}(s') \\} \\}$           (4)\nIf, additionally, the uncertainty set U is convex, for instance, because all con-\nstraints are linear, the inner minimization problem can be solved efficiently via,\ne.g., convex optimization methods. Hence, robust value iteration extends regu-\nlar value iteration by solving an additional inner problem at every iteration. In\ngeneral, the computational tractability of RMDPs primarily relies on whether\nor not this inner problem is efficiently solvable.\nAs discussed in Section 3.1 and shown in Table 1, stationary deterministic\npolicies are sufficient for optimality in (s, a)-rectangular RMDPs with convex\nuncertainty sets. Thus, an optimal robust policy * can again be extracted via\n$\\pi^*(s) = argmax_{a \\in A} \\{R(s, a) + inf_{P(s,a) \\in P(s,a)} \\{ \\sum_{s' \\in S} P(s, a, s')V^{(n)}(s') \\} \\}$      (5)\nWe underline the policy \u03c0* to denote that it is an optimal robust policy, as we\nlater also touch upon optimal optimistic policies, which we shall denote by *.\nRobust policy iteration [64,76] extends standard policy iteration in a similar\nway. Policy evaluation, i.e., model checking the induced robust Markov chain, is\ndone by performing robust dynamic programming for some stationary policy \u03c0:\n$V^{(n+1)}_\\pi = \\sum_{\\pi(s, a)} \\cdot (R(s, a) + inf_{P(s,a) \\in P(s,a)} \\{ \\sum_{S' \\in S} P(s, a, s')V^{(n)}_\\pi (s') \\})$\nHere, we again use that our uncertainty set is (s, a)-rectangular and convex to\nensure an efficiently solvable inner minimization problem. After convergence, we\nuse the robust state values under the current policy V* to compute the robust\nstate-action values Q:\n$Q_\\pi (s, a) = R(s, a) + inf \\{ \\sum_{S' \\in S} P(s, a, s')V(s') \\}$\nThe policy improvement step is performed on these robust state-action values:\n$\\pi'(s) = argmax Q_\\pi (s, a).$\nThe process repeats until the policy stabilizes, i.e., \u03c0' = \u03c0, after which an optimal\nrobust policy * \u03c0' has been found."}, {"title": "3.3 Well-Known RMDP Instances", "content": "Optimistic dynamic programming. Instead of assuming the worst-case from\nthe uncertainty set, we may also assume the agent and nature play cooperatively.\nThat is, both players attempt to maximize the agent's reward, and we instead\nobtain optimistic values V that are computed in the same way as the pessimistic\nvalues were, except that the inner minimization problem from Equation (3) is\nnow replaced by an inner maximization problem:\n$V^{(n+1)}(s) = \\max_{a \\in A} \\{R(s, a) + sup_{P \\in P} \\{ \\sum_{S' \\in S} P(s, a, s')V^{(n)}(s') \\} \\}$\nThe optimal optimistic policy * is again extracted by one final step of dynamic\nprogramming, as in Equation (5):\n$\\pi^*(s) = argmax_{a \\in A} \\{R(s, a) + sup_{P \\in P} \\{ \\sum_{S' \\in S} P(s, a, s')V^{*}(s') \\} \\}.$\nConvex optimization. Since dynamic programming approaches for MDPs ex-\ntend with relative ease to RMDPs, especially in the case of (s, a)-rectangular\nuncertainty sets, a natural question to ask is whether the same goes for convex\noptimization approaches, and in particular the linear programming (LP) formu-\nlation for MDPs. As Iyengar [64] already notes, however, that is not the case, and\nthe natural analogue of the LP of MDPs for RMDPs yields, in fact, a noncon-\nvex optimization problem. In contrast, the optimistic setting does yield tractable\nLPs via standard dualization techniques, which have been applied to solve PCTL\nobjectives in (s, a)-rectangular RMDPs with convex uncertainty sets [98].\nMethods for s-rectangular RMDPs. For s-rectangular RMDPs, dynamic\nprogramming does not extend so straightforwardly, and a lot of research has fo-\ncused on finding efficient Bellman operators for various types of uncertainty sets.\nMost notably, s-rectangular L\u2081-MDPs [58], but also s-rectangular uncertainty\nsets defined by an Lo-norm [16] or -divergences [60]. In [59], a policy iteration\nalgorithm was introduced, while [42, 77, 115] employ policy gradient techniques.\nOther objectives. The RMDP literature primarily focuses on either finite hori-\nzon or discounted infinite horizon reward maximization. Adaptation to reacha-\nbility objectives, such as the reach-reward maximization we consider, is usually\nstraightforward, provided the graph preservation property of Remark 1 is met.\nTemporal logic objectives can be reduced to such reach-reward objectives via\na product construction [119]. Finally, recent works study average reward (also\nknown as mean payoff) and Blackwell optimality in RMDPs [25, 48, 49]. Aver-\nage reward objectives consider the problem of maximizing the average reward\ncollected in t time steps when limt\u2192\u221e, and Blackwell optimality balances the\nstandard discounted reward objective by also accounting for long-term reward.\nA policy is Blackwell optimal if it is optimal for all discount factors sufficiently\nclose to one, i.e., all y \u2208 [\u03b3*,1) [99]."}, {"title": "3.3 Well-Known RMDP Instances", "content": "We review common types of RMDPs often used in formal verification and AI,\nnamely interval MDPs, L1-MDPs, and multi-environment MDPs. We provide a\nmore commonly used tuple definition for each and explain how it fits the general\nRMDP framework.\nInterval MDPS\nInterval MDPs (IMDPs) [93], also referred to as bounded-parameter MDPs [46]\nor uncertain MDPs [111, 119], are a special instance of (s, a)-rectangular RMDPs.\nDefinition 5 (IMDP). An interval MDP (IMDP) is a tuple (S, s\u0131, A, P, P, R),\nwhere \u010e: S \u00d7 A\u00d7S \u2192 [0,1] and P: S \u00d7 A\u00d7S \u2192 [0,1] are two transition\nfunctions that assign lower and upper bounds to each transition, respectively,\nsuch that P < P and for all transitions \u00de(s, a, s') = 0 \u21d4 P(s, a, s') = 0.\nOur definition of an IMDP requires that a transition either does not exist (where\nboth P and P are zero) or is assigned an interval with a non-zero lower bound,\nthus ensuring graph preservation for tractable (standard) robust dynamic pro-\ngramming (Remark 1). For IMDPs, however, the statistical model checking lit-\nerature offers solutions to circumvent the need for this requirement [6, 35].\nIMDPs have a constraint that each state-action pair is required to have a\nvalid probability distribution. An IMDP is an RMDP (S, S\u0131, A, P, R) where the\nuncertainty set is of the form U = { f : X \u2192 R | \u2200(s, a, s') \u2208 S\u00d7A\u00d7S, f(xsas') \u2208\n[i, j]sas' \u2286 [0,1] ^ \u2200(s, a) \u2208 S \u00d7 A, s'esf(xsas') = 1}. An example IMDP is\ndepicted in Figure 3. Note that this IMDP is precisely the RMDP from Figure 1\nwith uncertainty set U\u00b9, see Example 1.\nIMDPs have the nice property that their inner problem can be solved effi-\nciently via a bisection algorithm [93], more explicitly given for interval DTMCs\nin [74] and presented in Algorithm 1. This algorithm sorts the successor states"}, {"title": "L1-MDPS", "content": "Algorithm 1 Algorithm to solve the IMDP inner problem inf P\u2208P(s", "a)\n1": "Sort S' = {1", "si+1)\n2": "Vs \u2208 S': P(s", "0\n3": "budget = 1 \u2212 \u2211s'es\u0131 \u010e(s", "s')\n4": "i1\n5: while budget \u2212 \u0158(s", "do\n6": "nP(s", "s)\n7": "nbudget \u2190 budget \u2013 P(s", "s'i)\n8": "nii+1\n9: end while\n10: P(s", "s'i)\n11": "j \u2208 {i + 1", "n}": "P(s", "s'j)\n12": "return P(s,a,.)\nS' = {$1,..., s'm} of state-action pair (s, a) by the current value V(n) in ascend-\ning order. A variable budget indicates how much probability mass is still free to\nassign when we start with assigning the lower bounds to each successor state.\nSuccessor states occurring at low indices, i.e., with low values V(n), will be as-\nsigned"}]}