{"title": "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization", "authors": ["Mingye Zhu", "Yi Liu", "Quan Wang", "Junbo Guo", "Zhendong Mao"], "abstract": "Recent breakthroughs in preference alignment have significantly improved Large Language Models' ability to generate texts that align with human preferences and values. However, current alignment metrics typically emphasize the post-hoc overall improvement, while overlooking a critical aspect: regression, which refers to the backsliding on previously correctly-handled data after updates. This potential pitfall may arise from excessive fine-tuning on already well-aligned data, which subsequently leads to over-alignment and degeneration. To address this challenge, we propose FlipGuard, a constrained optimization approach to detect and mitigate update regression with focal attention. Specifically, FlipGuard identifies performance degradation using a customized reward characterization and strategically enforces a constraint to encourage conditional congruence with the pre-aligned model during training. Comprehensive experiments demonstrate that FlipGuard effectively alleviates update regression while demonstrating excellent overall performance, with the added benefit of knowledge preservation while aligning preferences.", "sections": [{"title": "1 Introduction", "content": "As Large Language Models (LLMs) increasingly permeate and revolutionize various industries and professions, the need to guide LLM generations to align with human preferences and meet specific requirements becomes increasingly critical. Alignment in LLMs emerges as a pivotal topic and various techniques have been developed to build a safe and controllable AI system. Reinforcement Learning from Human Feedback (RLHF) is one of the most widely-used alignment techniques that involves explicitly fitting a reward model to human preferences and has demonstrated effectiveness in various applications. Alternatively, propose Direct Preference Optimization (DPO), which leverages a mapping between reward functions and optimal policies, eliminating the need for reward modelling.\nHowever, we discover that these popular alignment methods suffer from regression phenomenon, meaning the model's performance on a particular task or dataset deteriorates after an update, which it had previously performed well on\u00b9. One concrete example in Figure 1 is that, aligned models may fail to address certain questions that were previously successfully handled, despite overall improved alignment with human preference. These degraded instances are termed as negative flips\u00b2.\nThe occurrence of negative flips can have various detrimental consequences. Firstly, it diminishes the overall improvement achieved through the alignment process, thereby compromising its effectiveness. Furthermore, negative flips can lead to inconsistent and unreliable results, thus negative user experiences and reduced trust, especially during an era where LLMs are rapidly updating and iterating, posing a significant challenge to achieving a comprehensive and trustworthy AI system.\nIdeally, an alignment strategy should correct model outputs only when they misalign with human values or are considered inferior, while minimally affecting the model's output and preserving the model's integrity otherwise, since (excessive) alignment can potentially lead to underperformance and knowledge forgetting. However, imposing conditional constraints to achieve non-uniform alignment across different data points poses a significant challenge. In this paper, to alleviate the problem of update regression in alignment tasks, we propose FlipGuard, a constrained optimization approach to detect and mitigate update regression with focal attention. Specifically, as outlined in Figure 2, our approach involves 1. customizing a reward characterization to measure the model's performance, 2. determining the premise of negative flips and 3. finally applying a focal distillation to conform the aligned policy to the pre-aligned counterpart when certain conditions"}, {"title": "2 Related Work", "content": "A closely related research topic to our work is catastrophic forgetting in sequential learning"}, {"title": "3 Preliminaries", "content": "Next we give the preliminaries of the two alignment strategies that we focus on in this paper."}, {"title": "3.1 RLHF", "content": "RLHF is widely adopted in alignment tasks and involves three steps:\nStep 1. Supervised fine-tuning (SFT) on high-quality datasets for downstream tasks using next-token prediction loss.\nStep 2. Train a reward model using human feedback on pairwise preferences between chosen and rejected responses. Specifically, prompt the SFT model with queries x to generate response pairs, then have human evaluators label the chosen and rejected answers $y_c$ and $y_r$ for each query. In practice, we parametrize a reward model (RM) $r_f(x, y)$ to learn the latent preference through via negative log-likelihood loss.\n$L_R(r, D) = -E_{(x,y_c,y_r)\\sim D}[log \\sigma(r(x, y_c) \u2013 r(x, y_r))]$,\nwhere $\\sigma$ is a logistic function. The trained RM produces the log probability that a certain response is preferred by human labelers.\nStep 3. RL fine-tuning which utilizes the learned RM to provide feedback during learning. Specifically, every generated completion will be scored"}, {"title": "3.2 DPO", "content": "RLHF typically requires an RM to give explicit rewards to the generated completions. To bypass the training of RMs, propose to leverage implicit rewards defined by the policy and the reference model. Specifically, they define the implicit rewards as:\n$r(x, y) = \\beta log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}$,\nthen the alignment problem becomes maximizing the gap in implicit rewards of the response pair:\n$L_{DPO}(\\pi_\\theta; \\pi_{ref}) = -E_{(x,y_c,y_r)\\sim D}[log \\sigma(\\beta log \\frac{\\pi_\\theta(y_c|x)}{\\pi_{ref}(y_c|x)} - \\beta log \\frac{\\pi_\\theta(y_r|x)}{\\pi_{ref}(y_r|x)})]$"}, {"title": "4 Methodology", "content": "In this section, we delve into the technical intricacies of our approach, providing a formal definition of the problem and a detailed derivation of the theoretical framework of FlipGuard."}, {"title": "4.1 Notations", "content": "We begin by establishing the notation conventions used throughout this work. Specifically, we denote the pre-aligned and the aligned model as $\\pi_{\\theta_0}$ and $\\pi_\\theta$, respectively, and $\\pi_{\\theta}$ the FlipGuard-calibrated model. We sometimes use the post-aligned model and aligned model interchangeably depending on the context. Please note that $\\pi_{\\theta_0}$ is an SFT model in our experiments, and by \"alignment\" we primarily focus on PPO and DPO, leaving other alignment strategies for future research endeavors."}, {"title": "4.2 FlipGuard", "content": "The proposed FlipGuard objective relies on the definition of the reward. At its core, negative flips occur because the post-aligned policy produces responses with reduced human satisfaction, which can be characterized by lower rewards, compared to their pre-aligned counterparts.\nHow do we define the reward? In the literature, one popular method for illustrating the satisfaction level of an LM generation y given any query x is to assign a scalar \u201creward\" score R(x, y) to it. This is an explicit reward value that is widely adopted in standard RL methods such as REIN- FORCE and its variants. Alternatively, uses an implicit reward that is parameterized by the policy $\\pi_\\theta$ under a reference model to underline the relative preferred/dispreferred level of a certain response.\nFor standard RL methods such as PPO where the responses are sampled from the training policy, the most effective way is to directly compare the reward scores between the policy response $Y_{\\pi_\\theta}$ and the reference model response $Y_{\\pi_{\\theta_0}}$. If the latter has a higher score graded by RM, a negative flip occurs.\nFor RL-free methods such as DPO, we have labeled chosen and rejected responses at hand. Under this scenario, we need a different reward characterization. First we turn back to the optimal solution to the KL-constrained reward objective in RLHF derived mathematically by previous works. It shows:\n$\\pi^*(y | x) = \\frac{1}{Z(x)} \\pi_{\\theta_0} (y|x)e^{\\frac{1}{\\beta} r(x,y)}$,\nwhich is an explicit Energy Based Model representation uniquely determined by the original LM $\\pi_{\\theta_0}$, and $Z(x) = \\sum_{y} \\pi_{\\theta_0} (y|x)e^{\\frac{1}{\\beta}r(x,y)}$ is the partition function.\nIt is straightforward to show that the corresponding reward parameterization under the optimal policy is:\nr^*(x, y) = \\beta log \\frac{\\pi^*(y|x)}{\\pi_{\\theta_0} (y|x)} + \\beta log Z(x).\nTo this end, we have defined the reward characterization for both PPO and DPO, and we next develop the conceptual and theoretical framework for the FlipGuard objective.\nThe premise of negative flips.\nFor PPO, we assume negative flip happens when given some query x:\nR(x, Y_{\\pi_{\\theta_0}}) \u2013 R(x, Y_{\\pi_{\\theta}}) > \\epsilon,\nwhere R() is the reward score from some RM and \\epsilon is a small positive constant.\nFor DPO, assume y is the target (chosen) response from the supervised dataset, we formally define the premise of negative flips as:\nr_{\\pi_{\\theta_0}} (x, y) \u2013 r_{\\pi_{\\theta}} (x, y) > \\epsilon,\nThat is, for a given query x and target response y, if the reward characterization defined in Equation 6 under initial policy $\\pi_{\\theta_0}$ is higher than that under $\\pi_\\theta$, we assume there exists quality degradation for the aligned model. Building on this assumption, next we substitute Equation 6 into 8, through a little algebraic manipulation the intractable term log Z(x) cancels out and we are left with:\nlog \\pi_{\\theta_0} (y|x) \u2013 log \\pi_{\\theta}(y|x) > \\epsilon,\nthat is, a larger reward now boils down to a higher log likelihood under $\\pi_{\\theta_0}$ than $\\pi_\\theta$. This can also be perceived as a higher confidence score under target response defined by conditional probability distribution given a question x.\nTo summarize, we conclude both cases for PPO and DPO and denote A as the collection of events that conditions defined in Equation 7 or 9 are triggered, and r a specific reward relationship between $\\pi_{\\theta_0}$ and $\\pi_\\theta$, then we have:\n1_A(r) = {1, ifr\u2208A\n0, ifr\u2209A\nThis is our formal definition of negative flips.\nFocal constraint. When it comes to conforming one distribution to another, knowledge distillation (KD) is a natural approach. In our case, we only transfer knowledge from $\\pi_{\\theta_0}$ to $\\pi_\\theta$ when a negative flip occurs, which echos the concept of focal distillation. Compared to traditional KD, focal constraint has the advantage of reducing negative flips while preserving positive flips, because it would not bias the policy to the initial distribution \"uniformly\".\nTo summarize, FlipGuard has the following objective:\nL_{FlipGuard}(\\pi_\\theta; \\pi_{\\theta_0}) = L_{align}(\\pi_\\theta; \\pi_{\\theta_0}) + \\gamma 1_A(r) \u00b7 D[\\pi_{\\theta_0}(y|x)||\\pi_{\\theta}(y|x)],\nwhere $L_{align}(\\pi_\\theta; \\pi_{\\theta_0})$ is the original alignment objective and $\\gamma$ the hyperparameters controlling constraint weight. D(\u00b7||\u00b7) refers to distance function. In this paper, we simply set D(\u00b7||\u00b7) a KL-Divergence. Hereinafter, we move one step further by showing that minimizing the KL divergence between $\\pi_{\\theta_0}$ and $\\pi_\\theta$ is equivalent to minimizing the Cross-Entropy (CE) in terms of them. The resulting formulation of our FlipGuard objective becomes:\nL_{FlipGuard}(\\pi_\\theta; \\pi_{\\theta_0}) = L_{align}(\\pi_\\theta; \\pi_{\\theta_0}) \u2013 E_{x,y}1_A(r) \u00b7 [log \\pi_{\\theta}(y|x)].\nPlease note that y refers to the target (winning) response. In the case of PPO, it is the reference response if it has a higher reward score, otherwise the policy response, and for DPO, it is just the chosen response from the dataset.\nA deeper look at the FlipGuard objective. Apparently, FlipGuard objective is a flexible combination of the alignment loss and a CE (or SFT) loss. In practice, it is common to apply SFT first to equip the model with the ability to follow instructions before beginning the preference alignment process. However, it often happens that the model becomes \"overwhelmed\" during alignment training, resulting in a loss of its ability to follow instructions or forgetting its previously acquired knowledge. In this context, FlipGuard can be seen as performing an \"augmentation\" operation on the original alignment goal by transferring the abilities and knowledge it has previously acquired."}, {"title": "5 Experiments", "content": "Datasets. To comprehensively evaluate if the proposed FlipGuard can generalize to different tasks, we make use of four datasets that are widely used in alignment tasks. UltraFeedback is a large-scale, fine-grained, diverse preference dataset for training alignment models. We also leverage HH-RLHF, a human-labeled preference dataset on helpfulness and harmlessness from and Summarization dataset from. Besides, we employ a Chinese CVALUES dataset that aims at measuring the model values in terms of responsibility and safety in Chinese language. Please find more statistics of the datasets in Appendix \u0412.\nBaselines. We begin by fine-tuning the pre-trained Mistral 7B on a portion of the chosen responses in the datasets, which helps mitigate the distribution shift between the true data distribution and the reference policy. The resulting models, denoted as $\\pi_{\\theta_0}$, then serve as the pre-aligned policy for subsequent experiments. For the Chinese CVALUES dataset, ChatGLM3-6B is used as the base model. We also discard the \"filtering function\" in Equation 10 to apply a full CE loss, which is in contrast to our focal constraint, so we term this method as \"KD\" hereinafter. The experiments are conducted on 4 80GB Nvidia A100 GPUs. We set $\\gamma$ to 0.005 for Summarization and 0.01 for other datasets unless otherwise specified, with more discussion in Section 5.2. More method-specific hyperparameter settings are specified in Appendix C.\nEvaluation setup. We leverage the Negative Flip Rate (NFR) (our main goal) as the main metric to assess the model's ability to mitigate the negative flips, which is defined as the losing case of the aligned model compared to the SFT model, along with the win rate, i.e., the positive flip rate (extra bonus). Additionally, we also assess the models' general ability on academic benchmarks as well"}, {"title": "5.2 Experimental results and analysis", "content": "FlipGuard consistently mitigates negative flips without sacrificing win rates. We present NFR and win rates for all datasets in Table 1, disregarding the score changes within (-0.1, 0.1) to mitigate the influence of noise when evaluating with RM. Due to the absence of a widely adopted RM for CVALUES, direct comparisons with RM for this dataset are omitted.\nFlipGuard vs. KD. KD serves as an ablation study to test the effectiveness of the \"filtering mechanism\" in Equation 10. Our experiments demonstrate that FlipGuard's filtering mechanism is more flexible and introduces greater improvements across various alignment tasks and benchmarks. Both KD and FlipGuard significantly reduce NFR compared to PPO/DPO, supporting our argument that update regression is widespread during alignment. Moreover, FlipGuard consistently demonstrates superior or comparable performance across all datasets compared to KD. This can be attributed to its balanced approach, with the focal mechanism effectively mitigating negative flips by adhering to the pre-aligned policy while actively learning during alignment. This enables FlipGuard to explore new alignments and exploit existing knowledge. In contrast, the uniform constraint of naive KD may overly restrict the model's learning, resulting in suboptimal performance.\nFlipGuard increases most of the abilities on MT-Bench. To systematically evaluate the model's ability of instruction-following, we employ the widely-used MT-Bench. Figure 3 depicts that FlipGuard enhances the model in Coding, Writing, Roleplay,etc., with an exception in the Reasoning ability. Our hypothesis is that the dynamic incorporation of constraints ensures that the model adheres more closely to the desired output distribution, which is beneficial to tasks that rely heavily on structured and precise outputs, while might inadvertently restrict the model's flexibility in reasoning scenarios, where more nuanced and less predictable responses are often required.\nFlipGuard maintains or boosts model performance on academic benchmarks. Even though we have a distinct research focus than reducing \"alignment tax\", it is still worthwhile to investigate how the alignment strategy influences the models' general knowledge and ability.\nWe evaluated the models aligned with Ultra-Feedback against a series of academic benchmarks"}, {"title": "Trade-off between $ \\epsilon $ values and the win rates.", "content": "Specifically, a larger $\\epsilon$ value reduces the likelihood of FlipGuard being triggered (the reward score for the pre-aligned model needs to be far larger than that for the aligned model), whereas a smaller $\\epsilon$ value increases its likelihood, making the method more sensitive to changes in the reward score. Table 4 suggests that for PPO, the reward scores (scalar value from pre-trained RM) exhibit more noise than the reward (calculated as the log ratio) for DPO. Our experiments indicate that setting $\\epsilon$ to 0.1 for PPO and 0 for DPO is a good starting point. Larger values (0.2) tend to reduce the effectiveness of FlipGuard, as the objective degrades to the original alignment loss."}, {"title": "6 Conclusion", "content": "In this paper, we introduce FlipGuard, a framework aimed at mitigating model update regression in preference alignment for LLMs. By integrating reward-based focal constraints, FlipGuard minimizes performance degradation while preserving or even enhancing the overall performance. Extensive experiments demonstrate FlipGuard's effectiveness in this regard. In this paper, our primary goal is to highlight an underappreciated research scope in alignment tasks. This work marks our initial effort, and our future work will focus on refining these methods for broader applicability and enhanced optimizing performance."}, {"title": "7 Limitations", "content": "This paper acknowledges several limitations that warrant further investigation. Firstly, the applicability of FlipGuard has been validated only on PPO and DPO, leaving many other popular alignment algorithms untested. Future work should explore designing appropriate reward characterizations or leveraging other method-specific strategies to determine the premise of negative flips for these algorithms. Additionally, our focus has been on using KL divergence as the distance function to encourage congruence between two distributions. This approach can be expanded to include a broader range of distance functions, such as the Wasserstein distance and Jensen-Shannon divergence."}, {"title": "8 Impact Statements", "content": "With the enhanced capabilities of LLMs, there are heightened risks such as untruthful answers, deception, biased opinions, and harmful content, which can lead to severe consequences. To better manage and guide model outputs to align with human intentions and values, it is crucial to develop techniques that ensure ethical model behavior. Considerable research has been focused on creating ethical frameworks for AI systems, which span various stages including data collection and processing, algorithm design, and application implementation. We aspire that our work contributes to this field, making LLMs safer and more controllable for human use."}, {"title": "A KL divergence to CE loss", "content": "In this section, we prove that minimizing the KL divergence is equivalent to minimizing the CE loss (from Equation 11 to Equation 12) under our settings.\nFirst let $\\pi_{\\theta_0} (y|x)$ and $\\pi_{\\theta}(y|x)$ be the pre- and post-aligned policies. The KL divergence from $\\pi_{\\theta_0} (y|x)$ to $\\pi_{\\theta}(y|x)$ is defined as:\n$D_{KL}(\\pi_{\\theta_0} (y|x)||\\pi_{\\theta}(y|x)) = E_{x,y} [\\pi_{\\theta_0} (y|x) log \\frac{\\pi_{\\theta_0} (y|x)}{\\pi_{\\theta}(y|x)}].$\nWe can expand the definition of KL divergence:\n$D_{KL}(\\pi_{\\theta_0} (y|x)||\\pi_{\\theta}(y|x)) = E_{x,y} (\\pi_{\\theta_0} (y|x) log \\pi_{\\theta_0} (y|x)) \u2013 E_{x,y} (\\pi_{\\theta_0} (y|x) log \\pi_{\\theta}(y|x)).$\nThe first term is the negative entropy of $\\pi_{\\theta} (y|x)$, which we denote as \u0397($\\pi_{\\theta_0}$), thus, the KL divergence can be rewritten as:\n$D_{KL}(\\pi_{\\theta_0} (y|x)||\\pi_{\\theta}(y|x)) = H(\\pi_{\\theta_0}, \\pi_{\\theta}) \u2013 \u0397(\\pi_{\\theta_0})$\nwhere \u0397($\\pi_{\\theta_0}, \\pi_{\\theta}$) = \u2013$E_{x,y}\\pi_{\\theta_0} (y|x) log \\pi_{\\theta}(y|x)$ is the Cross-Entropy between $\\pi_{\\theta}$ and $\\pi_{\\theta}$. Since the entropy \u0397($\\pi_{\\theta_0}$) is a constant with respect to $\\pi_{\\theta}$, minimizing the KL divergence is equivalent to minimizing the Cross-Entropy.\nSince we only enforce the constraint when $\\pi_{\\theta_0}$ is considered superior, given a target sequence y, we can set $\\pi_{\\theta_0} (y|x) = 1$ , then minimizing the KL divergence is equivalent to minimizing the SFT loss."}, {"title": "B Dataset details", "content": "In this section, we give a detailed description of the datasets we use for the experiments. For PPO, we follow the default setting of DeepSpeed-Chat and split each training set into a ratio of 2:4:4 for SFT, reward modeling and RL training, respectively, and the SFT model in phase 1 is set as the starting point for DPO training to mitigate distribution discrepancy. Specifically, we directly use the datasets in the links provided in Section 5, except for the summarization task where we employ Summarize From Feedback from and follow the code\u00b3 to process the data. All the datasets are subject to the terms of the MIT License (Apache-2.0 license for CVALUES) and are utilized in accordance with their intended purposes. The statistics of the utilized datasets are listed in Table 5."}, {"title": "C Implementations", "content": "Across all the experiments, we set gradient accumulation to 2 steps and training epochs to 2. We apply a cosine learning rate schedule and weight decay of 0.1 on the preference optimization dataset. Lora with DeepSpeed ZeRO-2 is also applied for memory optimization. For PPO, we follow the default hyperparameter settings in DeepSpeed Chat, a fast and scalable framework for enabling end-to-end RLHF training. The experiments are run under a random seed. Specifically, we set the learning rate to 9.65e-6 for UltraFeedback and HH-RLHF, and 1e-6 for Summarization and CVALUES, with both PPO and DPO. We set $\\beta$ value in DPO as 0.05 for Summarization and 0.1 for other datasets. Please note that we did not run an extensive grid search to determine the best hyperparameter settings for different alignment strategies, so it is not appropriate to directly compare PPO results with DPO. However, we strictly maintain a consistent"}, {"title": "Qualitative analysis", "content": "In this section, we provide some model answers aligned with different strategies for a better understanding. Given the queries, the pre-aligned model provides satisfactory answers while the aligned model plays it conservatively and refrains from giving direct answers. With the calibration of FlipGuard, the model can provide well-aligned and informative answers. WARNING: this section may contain examples that may be considered offensive or upsetting."}]}