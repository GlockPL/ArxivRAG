{"title": "TSEML: A task-specific embedding-based method for few-shot classification of cancer molecular subtypes", "authors": ["Ran Su", "Rui Shi", "Hui Cui", "Ping Xuan", "Chengyan Fang", "Xikang Feng", "Qiangguo Jin"], "abstract": "Molecular subtyping of cancer is recognized as a critical and challenging upstream task for personalized therapy. Existing deep learning methods have achieved significant performance in this domain when abundant data samples are available. However, the acquisition of densely labeled samples for cancer molecular subtypes remains a significant challenge for conventional data-intensive deep learning approaches. In this work, we focus on the few-shot molecular subtype prediction problem in heterogeneous and small cancer datasets, aiming to enhance precise diagnosis and personalized treatment. We first construct a new few-shot dataset for cancer molecular subtype classification and auxiliary cancer classification, named TCGA Few-Shot, from existing publicly available datasets. To effectively leverage the relevant knowledge from both tasks, we introduce a task-specific embedding-based meta-learning framework (TSEML). TSEML leverages the synergistic strengths of a model-agnostic meta-learning (MAML) approach and a prototypical network (ProtoNet) to capture diverse and fine-grained features. Comparative experiments conducted on the TCGA Few-Shot dataset demonstrate that our TSEML framework achieves superior performance in addressing the problem of few-shot molecular subtype classification.", "sections": [{"title": "I. INTRODUCTION", "content": "Cancer is one of the deadliest diseases worldwide [1]. The classification of cancer molecular subtypes aids in a deeper understanding of the pathogenesis of cancer, which is beneficial for precise diagnosis and personalized treatment [2]. However, multiple pathogenetic mechanisms and heterogeneity in cancer pose significant obstacles to precise molecular subtyping [3], [4]. Automatically defining and separating cancer subtypes using computer-aided techniques is highly demanded to facilitate personalized therapy and improve prognosis [2], [5].\nVarious algorithms have been developed to effectively identify cancer subtypes [6]\u2013[9]. Despite these advancements, cancer molecular subtyping remains challenging, primarily due to the scarcity of samples for certain subtypes [10]. A prevalent strategy to address this small data (few-shot [11], [12]) problem is integrative analysis, which aims to enlarge the dataset by amalgamating data from different experiments and platforms [13]\u2013[15]. However, the complex nature of gene expression data in computational biology means that data aggregation from different studies can be negatively affected by batch effects and heterogeneity introduced by integrative approaches [16]. Thus, there is a critical need for datasets specifically designed for few-shot training, as well as for the development of novel methods that can accurately classify molecular cancer subtypes.\nRecently, meta-learning [17], also known as learning to learn, has emerged as a common few-shot learning framework. Two classical approaches in meta-learning are model-agnostic meta-learning (MAML) [18] and prototypical network (ProtoNet) [19], which demonstrate powerful ability in solving small data problems. However, these two classical methods may have limitations. Firstly, in the traditional MAML framework, meta-initialization contains useful features that can mostly be reused for new tasks, resulting in minimal task-specific adaptation. Secondly, ProtoNet has been observed to be prone to overfitting issues in gene expression data [20]. Based on these observations, we hypothesize that (1) introducing an auxiliary task could prevent the model from overfitting on the cancer subtyping task; (2) designing a task-specific learning paradigm that explores the correlation between the auxiliary task and the primary cancer subtyping task could facilitate knowledge transfer between tasks, potentially enhancing the overall performance and robustness of the model.\nTo this end, we first construct an N-way K-shot multi-task dataset for cancer molecular subtype classification and auxiliary cancer classification. Second, we propose a task-specific embedding-based meta-learning (TSEML) framework designed to leverage interrelated clinical tasks, facilitating the extraction of common knowledge and benefiting each individual task. The main contributions of this paper can be summarized as follows:"}, {"title": "II. MATERIALS AND METHODS", "content": ""}, {"title": "A. Datasets and task generation", "content": "1) Datasets and preprocessing: Due to the scarcity of few-shot learning datasets for molecular subtyping, we create a new TCGA Few-Shot dataset, derived from the TCGA [21] datasets. This dataset can be used to benchmark various few-shot learning methods, supporting cancer subtype classification and cancer classification tasks.\nWe utilize data from UCSC Xena [22], which performs a $\\log_2$ transformation on normalized counts downloaded from the TCGA Data Coordinating Center [21] and apply mean normalization for each gene across all TCGA cohorts. Then, we download 10,459 IlluminaHiSeq pan-cancer based normalized gene expression profiles, encompassing 33 cancer types and 20,530 genes, from UCSC Xena, with normal samples removed. These 33 cancers are used for cancer classification. For subtype classification, we select 14 cancer molecular subtypes from the 33 cancers. The remaining cancer molecular subtypes are excluded due to insufficient sample sizes. Detailed information on the selected types of cancers can be found in our GitHub repository 1.\n2) Few-shot learning setting: Given a dataset $D = \\{(x_k, y_k)\\}_{k=1}^D$, and its corresponding class set $Y = \\bigcup_{k=1}^D \\{y_k\\}$. We can define an N-way K-shot task $T_i = (S_i, Q_i)$ randomly sampled from $D$. The support set $S_i$ contains N classes with K samples per class, while the query set $Q_i$ includes the same N classes. The number of class types $YY$ in task $T_i$ is set to be $N = |Y|$. For each $Y_{i,j} \\in Y_i (j = 1, 2, ..., N)$, we have $S_{i,j} = \\{(x, y) | y = Y_{i,j}\\} \\subset D$, with $|S_{i,j}| = K$ and $S_i = \\bigcup_{j=1}^N S_{i,j}$. Regarding the Q-query setting, for every $Y_{i,j} \\in Y_i (j = 1,2,..., N)$, we have $Q_{i,j} = \\{(x,y) | y = Y_{i,j}\\} \\subset (D \u2013 S_{i,j}), |Q_{i,j}| = Q$ and $Q_i = \\bigcup_{j=1}^N Q_{i,j}$\n3) Task generation: To address the issue of insufficient molecular subtype samples and leverage additional knowledge from auxiliary tasks, we generate basic few-shot tasks and propose the TCGA Few-Shot dataset using the following steps (as depicted in Fig. 1):"}, {"title": "B. Proposed TSEML for few-shot classification", "content": "The TSEML is designed towards a two-fold goal: to enhance the flexibility of MAML, while also exploring various tasks with interrelated information in ProtoNet. The architecture of TSEML is depicted in Fig. 2.\n1) MAML framework: MAML [18] is a representative approach of optimization-based meta-learning, which consists of outer and inner gradient update loops. The outer loop handles the gradient update to find initial model parameters suitable for all tasks, while the inner loop optimizes the model for specific tasks.\nThe classification model in MAML is denoted as $f_\\theta$, where $\\theta$ represents the model parameters. Given the task distribution $p(T)$ and the task-level learning rate $\\alpha$, the optimization objective of MAML can be formulated as follows:\n$\\theta^* = \\arg \\min_\\theta E_{T\\sim p(T)} \\frac{1}{|Q_i|} \\sum_{(x^q, y^q) \\in Q_i} L(y^q, f_{\\theta_i^*}(x^q)),$ (1)\nwhere\n$\\theta_i = \\theta - \\alpha \\nabla_\\theta \\frac{1}{|S_i|} \\sum_{(x,y) \\in S_i} L(y, f_\\theta(x)).$ (2)\nAlternatively, the parameters of the model can also be updated using the support set:\n$\\theta_i = \\theta - \\alpha \\nabla_\\theta |\\frac{1}{|S_i|} \\sum_{(x,y) \\in S_i} L(y, f_{\\theta_i}(x)).$ (3)\n2) ProtoNet: ProtoNet [19] is a classical metric-based meta-learning approach. In ProtoNet, samples across various tasks are embedded into a high-level feature space. Within this space, features corresponding to the same class are close to each other, while features from different classes are distinctly separated. Classification is then performed by determining the proximity of sample features to the nearest class centroids, effectively leveraging the spatial relationships within the feature space for decision-making\nLet $g_\\phi$ denote the network body with model parameters $\\phi$, which can be considered as the embedding model for feature extraction. The centroid of sample features belonging to the j-th class of the support set $S_i$ is given by:\n$c_{i,j} = \\frac{1}{|S_{i,j}|} \\sum_{(x,y) \\in S_{i,j}} g_\\phi(x).$ (4)\nLet $d$ denote the distance function, for every $(x^q, y^q) \\in Q_i$, we use $-d(g_\\phi(x^q), c_{i,j})$ to estimate the distance between the prediction of $x^q$ and the class $Y_{i,j}$. The prediction vector $z^q$ of $x^q$ is calculated as follows:\n$z^q = [-d(g_\\phi(x^q), c_{i,1}), -d(g_\\phi(x^q), c_{i,2}), ..., -d(g_\\phi(x^q), c_{i,N})].$ (5)\nWe denote the entire process of computing $z^q$ by Eq (4) and (5) as the function $h_{\\phi}$, i.e.,\n$h_\\phi(x^q, S_i) = z^q.$ (6)\nTo this end, the optimization objective of ProtoNet is defined as follows:\n$\\phi^* = \\arg \\min_\\phi E_{T\\sim p(T)} \\frac{1}{|Q_i|} \\sum_{(x^q, y^q) \\in Q_i} L(y^q, h_\\phi(x^q, S_i)).$ (7)\n3) TSEML: The traditional MAML and ProtoNet suffer from several limitations: (1) The network head of MAML limits the number of classes for classification, and exacerbates the negative impact of low representative features with introduced uncertainties. (2) ProtoNet fails in modeling relevant knowledge, and changes across different classification tasks.\nTo alleviate these limitations, we propose the TSEML framework, as shown in Fig. 2. TSEML employs the classification method based on the nearest feature centroid of ProtoNet, which not only reduces the uncertainty of the method but also allows the model to handle classification tasks with an arbitrary number of classes. Additionally, TSEML utilizes the gradient update inner loop of MAML to perform task-specific embeddings for samples. By such, the learning ability of TSEML is enhanced significantly. The optimization objective of TSEML is calculated as follows:\n$\\phi^* = \\arg \\min_\\phi E_{T\\sim p(T)} \\frac{1}{|Q_i|} \\sum_{(x^q, y^q) \\in Q_i} L(y^q, h_\\phi(x^q, S_i)),$ (8)\nwhere\n$\\phi_i = \\phi - \\alpha \\nabla_\\phi \\frac{1}{|S_i|} \\sum_{(x,y) \\in S_i} L(y, h_\\phi(x^s, S_i)).$ (9)\nAs with MAML, we can also perform multiple updates using the support set:\n$\\phi_i = \\phi - \\alpha \\nabla_\\phi \\frac{1}{|S_i|} \\sum_{(x,y) \\in S_i} L(y^s, h_\\phi(x^s, S_i)).$ (10)\n4) Loss function: We use the cross-entropy as the N-way setting loss function:\n$L(y, z) = \\ln \\sum_{j=1}^N \\exp(z[j]) \u2013 z[l(y)],$ (11)\nwhere l(y) is the label corresponding to y, z is the output vector of the classifier, and z[j] represents the j-th element in z."}, {"title": "III. RESULTS", "content": ""}, {"title": "A. Experimental settings", "content": "1) Network architecture and hyper-parameters: Due to data scarcity and overfitting issues, we adopt a simple 1D-CNN architecture as our classification model, following the approach of Mostavi et al. [23]. Our method is implemented in PyTorch using an NVIDIA RTX 3090 graphic card. During the training phase of TSEML, we set the batch size of tasks to 10 and the meta-level learning rate to 1 \u00d7 10-4. TSEML is updated on the support set 5 times for training and 10 times for testing. We conduct 1,000 training iterations, testing the model after every 10 training iterations. The task-level learning rate is set to 0.01. Detailed model architecture and hyperparameter settings are available in our GitHub repository.\n2) Task settings: All methods, except those trained without tasks, are conducted under the same meta-learning training settings for fair comparisons. We utilize two meta-learning training configurations: 5-way 1-shot 15-query and 5-way 5-shot 15-query. For testing, the tasks are set to P-way 1-shot 1-query and Q-way 5-shot 5-query, where P and Q are equal to the number of classes in the specific task. For each classification challenge, we randomly generate 500 tasks and calculate the mean and standard deviation of the evaluation metrics.\n3) Evaluation metrics: We conduct ten-fold cross-validation experiments on the TCGA Few-Shot dataset. The performance of the multi-class classification is assessed using a comprehensive set of evaluation metrics, including accuracy (AC), macro-average precision (PREm), macro-average recall (RECm), macro-average F1-score (F1m), and macro-average area under the receiver operating characteristic curve (AUCm)."}, {"title": "B. Methods for comparison", "content": "To demonstrate the effectiveness of our TSEML, we compare it with typical models, which can be categorized into traditional deep learning based methods (VGG [24], deep neural network (DNN) [25], recurrent neural network (RNN) [26] and classifier-baseline (CB) [27]), and meta-learning based methods (meta-long short term memory (ML) [28], meta-stochastic gradient descent (MS) [29], contextual augmented meta-learning (CAML) [30], meta-baseline (MB) [27], MAML [31], and ProtoNet [19])."}, {"title": "C. Comparison results", "content": "1) Molecular subtype classification: As shown in Table I, our method, TSEML, consistently outperform other approaches in both 1-shot and 5-shot settings.\nIn the 1-shot setting, TSEML achieves the highest AUCm of 77.82%, showing a notable improvement over the closest competitor, ProtoNet, which records an AUCm of 75.67%. Furthermore, TSEML attains a PREm of 72.83% and an Flm of 41.91%, indicating the robustness of our methods in handling minimal and unbalanced data scenarios. In the 5-shot setting, the performance improvement of our methods is even more significant. TSEML achieves the best results across all metrics\nThese results underscore the effectiveness of our methods in enhancing the precision and reliability of cancer molecular subtype classification in few-shot learning scenarios.\n2) Cancer classification: Our experimental analysis on the auxiliary cancer classification tasks, as shown in Table II, presents a comparative study of various methods.\nIn the 1-shot setting, our TSEML achieves the best AUCm of 99.25%. In the 5-shot setting, TSEML further consolidates its lead by posting the highest scores in all metrics.\nIn summary, our first finding is that the TSEML framework outperforms other methods by a large margin on all tasks except cancer classification. We explain this result as the reason that the diversity of this dataset for cancer classification is limited, and the classical methods have reached the upper-bound performance on the dataset. However, TSEML demonstrates its capability to achieve precise predictions, highlighting its robust learning ability. As for cancer molecular subtype classification, our TSEML shows its powerful ability in learning highly representative embedding by a larger support set for gradient updates."}, {"title": "D. Ablation study for distance metric functions", "content": "To evaluate the impact of different distance metric functions within TSEML, we conduct ablation experiments using Euclidean distance (TSEML) and cosine distance (TSEML-C).\nMolecular subtype classification. In the 1-shot setting, TSEML achieves slightly higher metrics overall compared to TSEML-C, as shown in Table I, with an AC of 51.08% versus 51.72%, and an AUCm of 77.82% compared to 77.17%. TSEML also records a higher PREm at 72.83% compared to 72.26%, though TSEML-C achieves a higher Flm at 48.2% compared to 41.91%.\nIn the 5-shot setting, TSEML continues to outperform TSEML-C, achieving an AC of 70.84% versus 68.06%, and an AUCm of 89.34% compared to 87.10%. TSEML also maintains a higher PREm (74.26% versus 71.44%) and F1m (69.86% versus 66.84%). These results indicate that while both methods are highly effective, TSEML generally provides slightly better performance, particularly in terms of precision and overall accuracy.\nCancer classification. The performance variances between TSEML and TSEML-C on the cancer classification task, as shown in Table II, are minimal, indicating that both methods are nearly equally effective.\nOur finding is that different distance metric functions have a significant impact on various tasks. Specifically, molecular subtype classification is notably sensitive to the choice of metric functions. This sensitivity can be attributed to the complexity of the molecular subtype classification task, where the Euclidean distance tends to be more effective in capturing the inherent relationships between tasks."}, {"title": "E. t-SNE visualization results", "content": "We employ t-SNE [32] to visualize the distributions of samples within the high-level semantic latent feature embeddings constructed by the TSEML models under the 1-shot setting. The resulting visualization, as illustrated in Fig. 3, reveals that TSEML successfully clusters samples in the high-level semantic space."}, {"title": "IV. CONCLUSION", "content": "In this study, we introduce a task-specific embedding-based meta-learning framework (TSEML), specifically designed for the few-shot classification of cancer molecular subtypes. The proposed approach synergistically combines the strengths of the model-agnostic meta-learning framework and the prototypical network. TSEML is capable of extracting shareable knowledge from interconnected cancer molecular subtype classification tasks, effectively learning from a limited number of samples to enhance the network's task-specific learning capabilities. Additionally, we have developed a few-shot benchmark dataset, TCGA Few-Shot, specifically for cancer molecular subtype classification. Comparative experiments on the TCGA Few-Shot dataset demonstrate that TSEML gains superiority in addressing the challenges of few-shot classification of cancer molecular subtypes."}]}