{"title": "ZERO SHOT GENERALIZATION OF VISION-BASED RL WITHOUT DATA AUGMENTATION", "authors": ["Sumeet Batra", "Gaurav S. Sukhatme"], "abstract": "Generalizing vision-based reinforcement learning (RL) agents to novel environments remains a difficult and open challenge. Current trends are to collect large-scale datasets or use data augmentation techniques to prevent overfitting and improve downstream generalization. However, the computational and data collection costs increase exponentially with the number of task variations and can destabilize the already difficult task of training RL agents. In this work, we take inspiration from recent advances in computational neuroscience and propose a model, Associative Latent DisentAnglement (ALDA), that builds on standard off-policy RL towards zero-shot generalization. Specifically, we revisit the role of latent disentanglement in RL and show how combining it with a model of associative memory achieves zero-shot generalization on difficult task variations without relying on data augmentation. Finally, we formally show that data augmentation techniques are a form of weak disentanglement and discuss the implications of this insight.", "sections": [{"title": "1 INTRODUCTION", "content": "Training generalist agents that can adapt to novel environments and unseen task variations is a long-standing goal for vision-based RL. RL generalization benchmarks have focused on data augmentation to increase the amount of training data available to the agent while preventing model overfitting and increasing robustness to environment perturbations (Yarats et al., 2021a; Almuzairee et al., 2024; Hansen et al., 2021). This follows the current trend in the broader robot learning community of training large models at scale on massive datasets (Kim et al., 2024; Hansen et al., 2024; Team et al., 2024) with the hope that the model will generalize. However, a significant drawback of these approaches is, intuitively, that they require larger models, more training data, longer training times, and have greater training instability that must be dealt with care when training RL agents.\nYet when we examine biological agents, we find that humans and, indeed, many other primates are able to quickly adapt to task variations and environment perturbations DiCarlo et al. (2012); Friston (2010). While all aspects of biological intelligence that contribute to generalization have yet to be understood, there is some understanding in the recent neuroscience literature of aspects related to representation learning that we look to for inspiration. Many parts of the brain in human and non-human primates contain neurons that represent single factors of variation within the environment, such as grid cells (Hafting et al., 2005), object-vector cells (H\u00f8ydal et al., 2019), and border cells (Solstad et al., 2008) that represent euclidean spaces, distance to objects, and distance to borders, respectively. Such disentangled representations have been theorized to facilitate compositional generalization (Higgins et al., 2018) and have been studied with curated datasets where the factors of variation are known (Higgins et al., 2017a; Whittington et al., 2023), and even within the context of RL (Higgins et al., 2017b). It is then to our surprise, with limited exceptions (Dunion et al., 2023; Sax et al., 2018), that disentangled representation learning has not garnered much attention within robot learning or RL more generally. One potential reason for this is that learning disentangled representations while simultaneously learning an RL policy is extremely difficult. Indeed, Higgins et al. (2017b) required a two-stage approach where the disentangled representation was learned first, followed by policy learning. In addition, Yarats et al. (2021b) found that using a B-VAE directly led to training instability and worse performance, instead opting to use a deterministic autoencoder with softer constraints. Finally, there is counter-evidence by Schott et al. (2022) to suggest that, while"}, {"title": "2 BACKGROUND", "content": "We wish to learn a policy that maps states to optimal actions that maximize cumulative reward. The agent-environment interaction loop is typically formulated as a Markov Decision Process (MDP) (S, A, R, P, \u03b3), where S and A are the state and action spaces, R(s, a) is the reward function, P(s'|s, a) is the probabilistic transition function, and \u03b3 is the discount factor. The policy \u03c0 learns a mapping of state to action with the objective of maximizing cumulative discounted return\n\nGt = E[\u03a3 \u03b3^t R(st, at)].\n\nIn vision-based RL, we do not assume access to the low dimensional state st \u2208 S. Instead, we must infer st given high-dimensional image observations ot \u2208 O, making the problem a partially observable MDP, or POMDP (S, A, R, P, O, \u03b3), where O is the space of high-dimensional observations.\nSoft Actor-Critic (Haarnoja et al., 2018) is an off-policy actor-critic algorithm that jointly trains a policy \u03c0 and state-action value function Q using the maximum entropy framework. The policy opti-"}, {"title": "2.2 DISENTANGLED REPRESENTATION LEARNING.", "content": "Nonlinear ICA: The disentanglement problem is sometimes formulated in the literature (Hsu et al., 2023) through nonlinear independent component analysis (ICA) due to their conceptual similarity. We follow suit since the notation will be useful in later sections. Suppose there are ns nonlinear independent variables $1,..., Sns that are the sources of variation of the images in the data distribution. A data-generating model maps sources to images:\n\np(s) = \u220f p(si), o = g(s)\n\nwhere g: S\u2192 O is the non-linear data generating function. The nonlinear ICA problem is to recover the underlying sources given samples from this model. Similarly, the goal of latent disentangle-ment is to learn a latent representation z such that every variable 21, ..., Zn, \u2208 z corresponds to a distinct source 81, ..., Sn.. Unfortunately, nonlinear ICA is nonidentifiable \u2013 that is, there are many decompositions of the data into sets of independent latents that fit the dataset, and so recovering the true sources reliably is impossible. Thus, the field of disentangled representation learning has focused more on empirical results and evaluation metrics on toy datasets where the true sources of variation are known. Given a dataset of paired source-data samples (s, o = g(s)), the goal is to learn an encoder f: O \u2192 Z and a decoder \u011d: Z \u2192 O such that the disentanglement evaluation metrics are high while also maintaining acceptable reconstructions of the data. Disentanglement models are typically constructed as (variational) autoencoders (Whittington et al., 2023; Hsu et al., 2023; Higgins et al., 2017a) and are rarely applied outside of toy datasets."}, {"title": "2.3 GENERALIZATION IN VISION-BASED RL", "content": "Image augmentation methods have shown success and have become the go-to method for generalizing vision-based RL algorithms such as Soft Actor-Critic (SAC) (Haarnoja et al., 2018) and TD3 (Fujimoto et al., 2018), generally using augmentations such as random crops, random distortions, and random image overlays to simulate distracting backgrounds. Methods such as DrQ (Yarats et al., 2021a), SADA (Almuzairee et al., 2024), and SVEA (Hansen et al., 2021) regularize the Q function by providing the original and augmented images as inputs into the critic. In many cases, however, the image augmentations can put the training data within the support of the distributions of the evaluation environments. For example, the \"random convolution\" image augmentation changes the color of the agent and/or background, and the policy is evaluated on an environment where the color of the agent is randomized. This brings into question whether these methods are truly capable of zero-shot extrapolative generalization when the training data is made to be sufficiently similar to the test data."}, {"title": "2.4 ASSOCIATIVE MEMORY", "content": "An associative memory (AM) network stores a set of patterns with the intent to retrieve the most similar stored pattern given an input. The best-known form is a Hopfield network, originally proposed in Hopfield (1982), which was inspired by how the brain is capable of recalling entire memories given partial or corrupted input (e.g., recalling a food item given a particular smell). Classical Hopfield networks could only store and recall binarized memories, whereas modern (dense) Hopfield networks (Krotov & Hopfield, 2016) can work with continuous representations and are trainable as differentiable layers within existing Deep Learning frameworks (Ramsauer et al., 2021). The memory retrieval dynamics are typically formulated as a function of energy minimization. Let \u0121 \u2208 Rd be the input query pattern, and X := [x1, ...XM] \u2208 Rd\u00d7M be memory patterns. In AM models, memories are stored on the local minima of the energy landscape, where the goal is to retrieve the closest stored pattern to \u00a7 by minimizing energy. Modern Hopfield networks assume the following general form for the energy function:\n\nE = - \u03a3 F(x).\n\nIn particular, by setting F = -lse(\u1e9e, XT\u00a7) + +\u00a2T\u00a7 (lse = log-sum-exponent), the retrieval dynamics becomes gnew = Xsoftmax(\u03b2XTg), which is the attention mechanism (Vaswani, 2017) and the backbone of modern Hopfield networks. Follow-up works such as Bietti et al. (2023) show a tight connection between the learning dynamics of Transformers and models of associative memory."}, {"title": "3 ON THE RELATIONSHIP BETWEEN DISENTANGLEMENT AND DATA AUGMENTATION", "content": "We begin by motivating the case for learning a disentangled representation for RL agents by showing a connection between data augmentation and disentangled representation learning. Specifically, we formally prove that data augmentation is a weak disentanglement of the latent space. We define weak disentanglement as a partial factorization of some but perhaps not all latent dimensions of the latent space. Strong disentanglement, on the other hand, is a complete factorization where each latent dimension zk \u2208 z encodes for a unique source si \u2208 s and is thus linearly independent of other latent dimensions, which is the goal of disentangled representation learning. The full proof is provided in A.1.\nTheorem 1: Suppose we are given a z = fo(g(s)), where some latent dimension zk \u2208 z approximates one or more sources. We will denote the approximations as \u015di. We can categorize the sources s into two categories, D and E, which correspond to task-relevant and task-irrelevant sources, respectively. For any such zk, if Q*(z, a) is an optimality invariant optimal Q-function immune to distractor variables, then the following must be true of z:\n\ncov($i, Sj|zk) = 0 \u2200si \u2208 D, sj \u2208 E, zk \u2208 Z.\n\nIntuitively, if data augmentation enables learning a latent representation such that the Q(z, a), a function of z, is immune to distractor variables, then any dimension of the latent space that encodes for task-relevant variables cannot also encode for task-irrelevant variables. Otherwise, distribution shifts involving the task-irrelevant variables would affect the Q function and, thus, the performance of the agent. One of two conditions must be true: either z is partitioned, where some variables approximate only sources from D, and others only sources from E, or z contains no information about sources in E altogether, both of which are a form of weak disentanglement.\nWe take a probabilistic perspective to see why this relationship is important. Suppose $1...,k \u2208 D and $1+k,...,n. \u2208 E. In order to learn a latent representation that contains no information about task-irrelevant sources S1+k,...,ns, data augmentation methods essentially estimate the marginal distribution over task-relevant sources:\n\np($1, ..., Sk) = \u03a3\u03a7\u03a3\u03a7... \u00d7 \u2211p(S1, ..., Sk, Sk+1, \u2026\u2026\u2026Sn\u300f).\n\nThe implication of 3 is that we must collect data for every possible variation of the task-irrelevant sources, which may be prohibitively expensive for real-world applications. Instead, a model with strong priors that leads to disentanglement without data augmentation essentially achieves the same result without the additional costs. Although the latent representation of such a model may still contain task-irrelevant features, the policy can learn to simply ignore them or associate them with known values in the presence of OOD data, as is the case with ALDA. In addition, these task-irrelevant features may become relevant if the task changes (e.g., if the current task is for a manipulator to stack a blue cube, and the next task is to stack a red cube), and so it may, in fact, be important to keep them."}, {"title": "4 METHOD", "content": "Experimental Setup. We first describe the generalization benchmark and our evaluation criteria to provide additional context. We train on four challenging tasks from the DeepMind Control Suite (Tassa et al., 2018). To evaluate zero-shot generalization capability, we periodically evaluate model performance under challenging distribution shifts from the DMControl Generalization Benchmark (Hansen & Wang, 2021) and the Distracting Control Suite (Stone et al., 2021) throughout training. Specifically, we have two evaluation environments: color hard, which randomizes the color of the agent and background to extreme RGB values, and distracting cs, which applies camera shaking and plays a random video in the background from the DAVIS 2017 dataset (Pont-Tuset et al., 2017)."}, {"title": "4.1 DISENTANGLEMENT", "content": "We now describe our framework for jointly learning a disentangled representation and performing association. For latent disentanglement, we choose to use QLAE Hsu et al. (2023), the current SOTA disentanglement method, which trains an encoder fe that maps to a continuous disentangled latent space, a discrete, parameterized latent model l\u0173, and a decoder g\u00f8 that reconstructs the observation. Similar to VQ-VAE (van den Oord et al., 2017), QLAE uses a discrete codebook for the latent space, except that each dimension uses its own separate scalar codebook. Concretely, Z is the set of latent codes defined by the Cartesian product of nz scalar codebooks Z = V\u2081 \u00d7 ... \u00d7 Vn, i.e., there are nz latent variables and |V;| discrete categories per variable. The continuous outputs of the encoder are the latent variables, each of which is quantized to the nearest scalar value in their respective codebooks.\n\nZdj = argmin v jk\u2208vz|f(x)j - Vjk|, j = 1, ..., Nz.\n\nSince we cannot differentiate through argmin, as with VQ-VAE, the authors of QLAE use quantiza-tion and commitment losses and a straight-through gradient estimator (Bengio et al., 2013):\n\nLquantize = ||StopGradient fo(x)) \u2013 za||2, Lcommit = ||fo(x) \u2013 StopGradient(za)||2.\n\nThe authors claim that while this is a failure mode for vector quantization, Z is low-dimensional enough that, in practice, it does not meaningfully impact performance. While this may be true for standalone disentanglement benchmarks, we find that it causes training instability and performance degradation when jointly learning a policy for high-dimensional continuous control problems We propose a solution in section 4.2 from the viewpoint of associative memory.\nIt is common practice in many vision-based RL algorithms to utilize framestacking to incorporate temporal information into the latent space. This means that the encoder accepts as input, and the decoder produces a stack of RGB images in RB\u00d7Ck\u00d7H\u00d7W, where k is the number of frames. However, latent disentanglement models have only been shown to work on datasets of singular images and struggle to disentangle sources of individual images when given stacks of images as inputs. Evidence of this is presented in the appendix, Section A.5. To resolve this issue, we fold k into the batch dimension and encode/decode batches of single images in RBk\u00d7C\u00d7H\u00d7W, resulting in a batch size Bk of disentangled latent vectors zd \u2208 RBkxnsi. To incorporate temporal information, we reshape the batch of latent vectors into RB\u00d7knsi and feed it into a 1D convolutional neural network (CNN), producing our final latent vector z \u2208 RBxe. z is used as the state representation for the actor"}, {"title": "4.2 ASSOCIATION", "content": "The naive approach to performing association would be to feed the quantized latent rep-resentation through a Hopfield network. However, upon closer inspection of QLAE's latent dynamics, we find that most of the components of a generic associative mem-ory model are already present, i.e., QLAE is implicitly also a Hopfield network.\n\nz = P. sep(sim(X, \u03be)).\n\nP is a projection, sep is a separation function, and sim is a similarity function between the stored memories X and query \u00a7. While P is originally described as a projection matrix, we extend the definition of P to be any function that projects X and & into a shared embedding space. In this case, equation 4 can be interpreted as follows: fe is the projection function that projects high-dimensional images o into an embedding space shared by the scalar codebooks Z, which can be interpreted as predetermined memories. The closest memory is recovered by computing the L\u2081 distance, which serves as the similarity function, and sep is the argmin function. Through this view, we can rewrite the latent dynamics of QLAE in many ways, perhaps exchanging the similarity function for L2 distance or dot product, changing the separation function, etc., as long as it follows the framework of 6. Since Z is a product of scalar codebooks, L\u2081 distance remains an appropriate choice for the similarity function. Instead, we augment the latent dynamics with a Softmax separation function as follows:\n\nzd\u2081 = Softmax(-BL1(fo(0)j, vj)) \u00a9 Vj\n\nwhere \u1e9e is a scalar temperature parameter. Equation 7 can be interpreted in two ways. From an associative memory perspective, attention-based Hopfield models apply Softmax to separate the local minima (stored memories) on the energy landscape, where \u1e9e controls the degree of separation, and so we've recovered the modern Hopfield memory retrieval dynamics. From a purely mathematical perspective, we have what resembles the Gumbel-Softmax categorical reparameterization Jang et al. (2017), although we do not perform any sampling in our method. This lends a novel view on attention-based Hopfield networks \u2013 models with a high-temperature parameter can be interpreted as classifiers over X classes, where X is the number of stored memories whose local minima are well separated on the energy landscape.\nIn the limit, as \u1e9e goes to infinity, we achieve maximum separation between memories and recover equation 4. In practice, we choose a large value for \u1e9e such that we retrieve one scalar from each codebook, as originally intended, although our method works well with smaller values of \u1e9e (see appendix Section A.4 for additional results). Since large \u1e9e values can cause downstream gradients to vanish, we find that keeping the commitment loss from equation 5 helps keep the outputs of the"}, {"title": "5 EXPERIMENTS", "content": "We compare against several baselines that together represent the full range of different learning paradigms in the literature that attempt to elicit zero-shot generalization. DARLA (Higgins et al., 2017b) is, to the best of our knowledge, the only other algorithm that attempts to learn a disentangled representation of the image distribution towards zero-shot generalization of vision-based RL. SAC+AE (Yarats et al., 2021b) uses a deterministic autoencoder with an auxiliary reconstruction objective and strong regularization that demonstrates decent zero-shot generalization capability. RePo (Zhu et al., 2023) is a model-based RL algorithm that learns a task-centric latent representation immune to background distractors. Finally, SVEA (Hansen et al., 2021) is an off-policy RL algorithm that improves training stability and performance of off-policy RL under data augmentation. As in their paper, we use the random overlay augmentation for SVEA, where images sampled from the Places (Zhou et al., 2017) dataset of 10 million images are overlayed during training. The training curves and evaluation on \"color hard\" and DistractingCS are presented in Figure 5.\nExcluding SVEA, ALDA outperforms all baselines on both distribution shift environments. ALDA also maintains stability and high performance on the training environment, despite the disentanglement auxiliary objective and extremely strong weight decay (\u03bb\u03b8, \u03bb = 0.1) on the encoder and decoder. We do not expect to outperform SVEA since it uses additional data sampled from a dataset of 1.8 million diverse real-world scenes, likely putting the training data within the support of the data distributions of the evaluation environments. Nevertheless, ALDA performs comparably and, in some cases, is equal to SVEA despite only seeing images from the original task. Performance degrades severely for all algorithms on the DistractingCS environment. We suspect that, in addition to the"}, {"title": "6 DISCUSSION", "content": "As stated previously, the disentanglement problem by extension of nonlinear ICA is underdeter-mined, so there are many ways the latent space may factorize, perhaps by representing the sky and background with one latent or by separating them into two latents, etc. Given that both the task and reconstruction gradients of the critic/decoder affect the latent model/encoder, an interesting scientific and philosophical implication is that the model is potentially biased towards a disentangled representation that is useful, although there is no way to quantitatively or qualitatively show such a result at this time. Nevertheless, it remains an interesting line of further investigation from a scientific standpoint, and perhaps, philosophically, says something about whether the question \"What is the ground truth factorized representation that best explains the data?\" is even the right question to ask.\nRL agents deployed in the real world must constantly adapt to changing environmental conditions. Much of the variance can be captured with a sufficiently large dataset. However, there remains a portion of the distribution containing every possible edge case and unaccounted-for variation, commonly referred to as \"the long tail,\" that remains elusive because it is prohibitively expensive to account for every possible variation. Unfortunately, these uncaptured variations are frequent enough due to the ever-changing dynamical nature and complexity of the real world that deploying agents in the real world remains challenging. Therefore, it seems the case that data augmentation techniques, collecting massive datasets, and the like are not sufficient to develop generalist agents capable of adaptation the way humans and other animals are. That's not to say that data isn't important or a fundamental ingredient to training machine learning models. In fact, the method proposed in this paper can also utilize data augmentation techniques presented in prior work, as it should. Instead, our proposition is that if a data-driven model can generalize better with less data, then it will scale better with more data.\nIn Section 3, we showed how data augmentation and disentangled representation learning aim to achieve the same result a factorization of the latent space into separate components in order to improve downstream generalization performance. Given the additional computational and data collection costs and potential training instabilities that data augmentation methods may incur, it seems more fruitful to investigate models with inductive biases that elicit modular and generalizable representations without relying on data scaling laws. While presenting the model with sufficiently large and diverse datasets remains unquestionably important, we cannot rely solely upon the data in hopes that the model learns a good representation. As with any other inductive biases, such as using CNNs for vision tasks or transformers for NLP tasks, inductive biases that elicit modular representations while leveraging data are worth studying if we are to develop agents that can perform and adapt well in the real world.\nWe hope that the work presented here inspires future research into novel models and architectures to learn representations that enable the adaptability we see in our biological counterparts. We discuss some limitations of our method and promising directions for future research. One notable limitation is that our disentangled latent representation za does not explicitly account for temporal information since it primarily estimates the sources that produce the image distribution. Instead, we must capture temporal information in the downstream 1D-CNN layer as shown in Figure 2. How to learn a disentangled representation that contains sources of both the image data and temporal information for decision-making tasks remains an open question. Another limitation is that, while we introduce a simple Hopfield model as a modification to QLAE, we do not take advantage of the more recent literature involving learnable attention-based or energy-based Hopfield networks (Ramsauer et al., 2021; Hoover et al., 2024). Stronger Hopfield models that synergize well with disentangled representations is another potentially fruitful research direction.\nGiven that we use a very compact disentangled latent space with strong empirical evidence that individual latents capture information about specific aspects of the agent, an interesting research direction is to investigate whether all or parts of the proprioceptive state representation can be recovered from image observations. We provide some preliminary evidence of this in the appendix (A.2). Beyond interpretability, such a model may yield better performance since state-based RL"}]}