{"title": "Map-Free Visual Relocalization Enhanced by Instance Knowledge and Depth Knowledge", "authors": ["Mingyu Xiao", "Runze Chen", "Haiyong Luo", "Fang Zhao", "Juan Wang", "Xuepeng Ma"], "abstract": "Map-free relocalization technology is crucial for applications in autonomous navigation and augmented reality, but relying on pre-built maps is often impractical. It faces significant challenges due to limitations in matching methods and the inherent lack of scale in monocular images. These issues lead to substantial rotational and metric errors and even localization failures in real-world scenarios. Large matching errors significantly impact the overall relocalization process, affecting both rotational and translational accuracy. Due to the inherent limitations of the camera itself, recovering the metric scale from a single image is crucial, as this significantly impacts the translation error. To address these challenges, we propose a map-free relocalization method enhanced by instance knowledge and depth knowledge. By leveraging instance -based matching information to improve global matching results, our method significantly reduces the possibility of mismatching across different objects. The robustness of instance knowledge across the scene helps the feature point matching model focus on relevant regions and enhance matching accuracy. Additionally, we use estimated metric depth from a single image to reduce metric errors and improve scale recovery accuracy. By integrating methods dedicated to mitigating large translational and rotational errors, our approach demonstrates superior performance in map-free relocalization techniques.", "sections": [{"title": "Introduction", "content": "Visual relocalization is a technology that emulates the human ability to estimate the position and orientation of a camera from a single query image. This task has been extensively researched and offers numerous exciting applications in augmented reality (AR) and robotic navigation. By leveraging this technology, we can achieve more precise location awareness and navigation in complex environments, significantly enhancing both the technological capabilities and user experience in these domains. Traditional map-based visual relocalization techniques [31] require a large number of images and scale calibration to create detailed 3D maps of scenes. However, the necessity of creating detailed 3D"}, {"title": "Related work", "content": ""}, {"title": "Visual Relocalization", "content": "Visual relocalization, which involves estimating camera poses from visual inputs, has been a research topic for several decades [4, 18, 22, 28, 43]. Early map-based approaches [1,34,44] primarily relied on image retrieval strategies to directly estimate camera poses from the most similar images in a database. From a technical perspective, these methods are mainly divided into end-to-end and modular-based approaches. For the end-to-end method [5, 15, 16, 19, 26, 36, 48, 52, 57-59], the network is trained and outputs the results directly. Some methods [6,40, 60] calculate the absolute camera pose. For example, Chen [7] uses a scene geometry prediction module to generate the scene coordinate map, and then a Transformer-based pose regression network is used to directly retract the camera pose. Some methods [9,17, 29, 31] calculate the relative pose. Most of them"}, {"title": "Semantic Relocalization", "content": "Recent advancements in visual localization have significantly benefited from integrating semantic information, enhancing both accuracy and robustness. Multiple studies have employed deep learning, semantic segmentation, and geometric information to achieve these improvements.\nDynamic-SLAM [53] uses deep learning for semantic monocular visual localization and mapping, demonstrating improved performance in dynamically changing scenarios. Visual Localization Using Sparse Semantic 3D Map [37] maintains high localization accuracy under varying seasonal and lighting conditions, while Coarse-To-Fine [23] Visual Localization employs a coarse-to-fine approach using semantic compact maps for urban vehicle localization. SFD2 [55] and Visual Localization Using Semantic Segmentation and Depth Prediction [10] enhance localization accuracy through semantic-guided feature detection and the evaluation of semantic consistency during image retrieval. These methods leverage semantic information to improve the precision of visual localization. Monocular Localization with Semantics Map for Autonomous Vehicles [49] and"}, {"title": "Method", "content": ""}, {"title": "Overview", "content": "The overall framework of our method is shown in Figure 2. Given two input images, \\(I_1\\) and \\(I_2\\), our method first utilizes feature point matching to establish a set of correspondences, which illustrate the relationship between 2D images. To enhance matching accuracy, we incorporate instance segmentation knowledge to extract the main subjects from images. Specifically, our feature point matching"}, {"title": "Analysis of Intra-instance Matching", "content": "DUSt3R [50] leverages 3D prior knowledge to enhance feature point matching by processing two input images, \\(I_1\\) and \\(I_2\\), and outputting corresponding point and confidence maps that represent each pixel's 3D position. The network consists of two branches, each with an image encoder, decoder, and regression head. Initially, a shared-weight Vision Transformer (ViT) encoder extracts latent representations from the input images, generating \\(F_1\\) and \\(F_2\\). The decoders then use a cross-attention mechanism to facilitate information exchange between the views, refining these representations iteratively. Finally, the regression heads predict the point and confidence maps from the decoder outputs. The regression loss is based on the Euclidean distance between predicted and ground-truth 3D points, normalized by their average distances to the origin to handle scale ambiguity. This architecture effectively uses 3D prior knowledge to improve feature point matching and 3D reconstruction accuracy."}, {"title": "Instance-enhanced Matching", "content": "For a pair of images \\(I_1, I_2\\) and corresponding point maps \\(X_{1,1}, X_{2,1}\\), we aim to find the matching point. Our matching mechanism adopted the nearest neighbor matching and reciprocal matching from DUSt3R [50]. Firstly, we use the nearest neighbor search in the 3D point map space. For each pixel \\((i,j)\\) in image \\(I_1\\), we first find the corresponding 3D point \\(X_{1,1}(i, j)\\) in \\(X_{1,1}\\). Then we perform a nearest neighbor search in \\(X_{2,1}\\) to localize the 3D point \\(X_{2,1}(k,l)\\) that is closest to \\(X_{1,1}(i, j)\\), where \\((k,l)\\) are the coordinates of the matching point found in \\(X_{2,1}\\). Then, we use reciprocal matching to improve the accuracy of the matching. From each pixel \\((i, j)\\) in \\(I_1\\), we find the matching point \\((k,l)\\) in \\(I_2\\). Simultaneously, from each pixel \\((k,l)\\) in \\(I_2\\), we find the matching point \\((i, j)\\) in \\(I_1\\). A match is considered valid only if the results are consistent in both directions, i.e., \\((i,j) \\leftrightarrow (k,l)\\) and \\((k,l) \\leftrightarrow (i, j)\\). We denote the global mapping as \\(Map = [((i_1, j_1), (k_1, l_1)), ((i_2, j_2), (k_2, l_2)), ...((i_n, j_n), (k_n, l_n))]\\), where each tuple in the map represents the matching relation between points in \\(I_1\\) and \\(I_2\\), we define the \\(M()\\) as the feature point matching method:\n\\[\u039c\u03b1\u03c1\u00b3 = M(I_1, I_2, X_{1,1}, X_{2,1}),\\]\nwhere the \\(Maps\\) is the correspondences in global area. To significantly enhance the precision of feature point matching, instance segmentation technology was integrated, enabling the accurate extraction of primary objects from images and focusing on matching within identical instance objects. Specifically, we use Seg-GPT [51] to output the mask results of \\(Mask_1^o\\) and \\(Mask_2^o\\) for the same object in both the reference and query images. Here, \\(Mask_1^o\\) represents the mask of the oth in the reference frame, while \\(Mask_2^o\\) corresponds to the mask of the same instance in the query frame. Upon obtaining the results from instance segmentation, our feature point matching algorithm conducts refined matching within these masks. Specifically, for the \\(o\\)-th object, we denote corresponding image as \\(I_1^o\\) and \\(I_2^o\\), where \\(I_1^o = I_1 \\odot Mask_1^o\\), \\(I_2^o = I_2 \\odot Mask_2^o\\). Then, we perform point map reconstruction within the object and obtain corresponding point maps \\(X_{1,1}^o\\) and \\(X_{2,1}^o\\). Finally, we conduct matching within the mask object area:\n\\[\u039c\u03b1\u03c1^o = M(I_1^o, I_2^o, X_{1,1}^o, X_{2,1}^o),\\]\nwhere the \\(Map^o\\) is the correspondences in the same object. This modification aims for more precise matching within individual instances, ensuring that the"}, {"title": "Pose Estimation", "content": "For the pose solver, we selected the 5-point algorithm [27] combined with the essential matrix method as mentioned in the [2] to robustly compute the essential matrix E from matched point pairs:\n\\[E = U \\Sigma VT,\\]\nwhere U and V are orthogonal matrices representing rotation or reflection, and \\(\\Sigma\\) is a diagonal matrix representing the original matrix's stretching factor.\nThis matrix E is then decomposed into the rotation matrix and the scale-free translation vector. Subsequent to this decomposition, additional geometric constraints are utilized to select the correct R and t:\n\\[E = t \\times R,\\]\nwhere t is the scale-free translation vector, and R is the rotation matrix.Specifically, we use single-image depth estimation to back-project 2D matched points into 3D points and then evaluate which pair of R and t ensures that the majority of these 3D points have positive depth, indicating a feasible and physically consistent configuration."}, {"title": "Depth-enhanced Scale Recovery", "content": "We start by transforming the input image I into a canonical camera space image \\(I_c\\) using camera model parameters such as focal length \\(f_c\\) and principal point \\((u_c, v_c)\\). The intrinsic parameters of I, including focal length f and principal point (u, v), scale the image by the ratio \\(\\omega_r = \\frac{f}{f_c}\\):\n\\[I_c = \u03a4(I, \\omega_r),\\]\nwhere \\(T(.)\\) represents the image scaling operation.\nOnce in the canonical camera space, \\(I_c\\) is processed through an encoder-decoder network which initially yields a depth prediction \\(d_c\\):\n\\[d_c = Decoder(Encoder(I_c)).\\]"}, {"title": "Experiment", "content": ""}, {"title": "Dataset and Metrics", "content": "We conducted an evaluation of our method using the MapFree-Reloc dataset [2], which consists of 36,998 frames for validation and 14,778 frames for testing. The images in this dataset are from 65 validation scenes and 130 test scenes, spanning a diverse array of locations. The dataset presents difficult scenarios such as dynamic environmental conditions, dramatic viewpoint shifts, significant variability across locations, and minimal to no visual overlap with the reference image. Moreover, it is required to relocate within scenes represented by a singular reference image in map-free settings, making it more challenging. Following Map-free Relocalization [2], we use the following metrics to evaluate the performance: the Average Pose Error(the Average Median Rotation Error, the Average Median Translation Error), AUC@VCRE < 90px, and so on."}, {"title": "Performance Comparison", "content": "We analyze current models for insights into better relocalization algorithms. Specifically, we build the comparison method using three components: feature"}, {"title": "Main Results", "content": "Quantitative Evaluation As shown in Table 1, we first compared our method with existing approaches, and our method shows a significant reduction in Average Median Pose Error compared to the current state-of-the-art. For instance, the RPR[3D-3D] method, which performs best among the existing methods, has an Average Median Rotation Error of 22.623\u00b0, whereas ours is 9.030\u00b0. This improvement is attributed to our superior feature points matching technique. Specifically, we retain global matching information while using instance knowledge to focus the model on matching within instances. This strategy not only mitigates the impact of significant matching errors but also allows for more precise local matching. Moreover, the RPR[3D-3D] method has an Average Median Translation Error of 1.667m, while our method achieves 0.596m. This improvement is due to both our effective feature point matching and our precise depth estimation. Specifically, our method involves accurate prediction and processing of the original image depth, which is crucial for recovering the translation matrix from the scale-free translation vector. This not only confirms the effectiveness of our depth estimation approach but also highlights the importance of precise depth estimation for accurate scale recovery. Additionally, our method significantly outperforms all baseline methods in terms of the Average Median Reprojection Error, a combined measure of rotation and translation accuracy, further demonstrating the robustness and effectiveness of our approach.\nWe evaluated our method's efficacy relative to the baseline by computing the Cumulative Distribution Function (CDF) for pose estimation errors across all scenes (Figure 4. The CDF quantifies the cumulative probability of error values by sorting them in ascending order and determining the proportion below each threshold. A well-performing CDF rapidly ascends towards 1 at lower error magnitudes, indicating a predominance of lower errors. The comparative CDF curves show that our method outperforms the baseline, with our curve ascending more swiftly at reduced error levels. This demonstrates our method's consistent lower pose estimation errors and its robustness and accuracy across various scenarios.\nWe also compare our method with closed-source models submitted to the Mapfree Visual Relocalization Challenge, as shown in Table 2. Our method outperforms most others, demonstrating its superiority, particularly in Median Reprojection, Median Translation, and Median Rotation Error. Additionally, the performance of our method is close to state-of-the-art methods like MASt3R (Ess.Mat + D.Scale), highlighting the significant potential of our approach.\nQualitative Evaluation We visualized the camera poses for certain frames in specific scenes on Mapfree valid sets, represented by triangles, as shown in Figure 5. In this visualization, the horizontal axis represents the displacement in the x-axis direction, while the vertical axis corresponds to the displacement in the y-axis direction. The green triangles denote the ground truth values, the blue triangles represent the camera poses estimated by our method, and the red triangles are the estimates from the baseline method, LoFTR(Ess.Mat +D.Scale). Our method's estimates are closer to the true values, illustrating a more accurate"}, {"title": "Ablation Study", "content": "In this section, we conducted extensive ablation studies to investigate the significance of instance and depth knowledge within our framework. By isolating"}, {"title": "Conclusion", "content": "We propose a novel map-free relocalization method that estimates the relative pose of a query frame using reference and query images. By guiding the feature point matching model to focus on matches within instance objects via instance segmentation results, our approach significantly reduces incorrect matches and enhances accuracy. Additionally, we optimize 3D space point coordinates through a depth estimation for better scale recovery. Extensive experiments demonstrate the effectiveness of our collaborative optimization, paving the way for future improvements in map-free relocalization accuracy."}]}