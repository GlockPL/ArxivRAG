{"title": "PyOD 2: A Python Library for Outlier Detection with LLM-powered Model Selection", "authors": ["Sihan Chen", "Zhuangzhuang Qian", "Wingchun Siu", "Xingcan Hu", "Jiaqi Li", "Shawn Li", "Yuehan Qin", "Tiankai Yang", "Zhuo Xiao", "Wanghao Ye", "Yichi Zhang", "Yushun Dong", "Yue Zhao"], "abstract": "Outlier detection (OD), also known as anomaly detection, is a critical machine learning (ML) task with applications in fraud detection, network intrusion detection, clickstream analysis, recommendation systems, and social network moderation. Among open-source libraries for outlier detection, the Python Outlier Detection (PyOD) library is the most widely adopted, with over 8,500 GitHub stars, 25 million downloads, and diverse industry usage. However, PyOD currently faces three limitations: (1) insufficient coverage of modern deep learning algorithms, (2) fragmented implementations across PyTorch and TensorFlow, and (3) no automated model selection, making it hard for non-experts.\nTo address these issues, we present PyOD Version 2 (PyOD 2), which integrates 12 state-of-the-art deep learning models into a unified PyTorch framework and introduces a large language model (LLM)-based pipeline for automated OD model selection. These improvements simplify OD workflows, provide access to 45 algorithms, and deliver robust performance on various datasets. In this paper, we demonstrate how PyOD 2 streamlines the deployment and automation of OD models and sets a new standard in both research and industry. PyOD 2 is accessible at https://github.com/yzhao062/pyod.\nThis study aligns with the Web Mining and Content Analysis track, addressing topics such as the robustness of Web mining methods and the quality of algorithmically-generated Web data.", "sections": [{"title": "1 Introduction", "content": "Outlier detection (OD), also known as anomaly detection (AD), is a central task in data analysis and machine learning [5, 12]. It has critical applications in web-related domains, including fraud detection in e-commerce [7], clickstream analysis for user behavior modeling [2], software engineering [19], and social network moderation [19]. While traditional methods have been extensively studied and applied [1], deep learning methods have recently gained attention. Models based on autoencoders [1], generative adversarial networks (GANs) [3], mixture-of-experts [23], and transformers [20] have shown strong performance in detecting anomalies within complex, high-dimensional data [5, 8-10, 14].\nCurrent Landscape of Open-source OD Systems. Among the open-source libraries available for outlier and anomaly detection, PyOD [22] is not only the most widely used one, with more than 8,500 GitHub stars, 25 million downloads, and more than 1,000 citations, but it has also become a trusted resource in both academic and industrial communities. Its broad coverage of algorithms, ranging from classical statistical techniques to more advanced machine learning models, has earned it a strong following among developers, analysts, and researchers who rely on OD capabilities as a core component of their workflows. Despite its established position and extensive user base, PyOD still faces three key challenges: (i) a lack of modern deep learning methods, (ii) the coexistence of two different frameworks (PyTorch and TensorFlow) for neural models, and (iii) the absence of automated model selection. These limitations create integration issues, reduce consistency, and place a heavy burden on the domain expertise of outlier detection, thereby limiting its accessibility for less experienced users.\nWe address these challenges by introducing PyOD Version 2 (PyOD 2). As described in \u00a72, PyOD 2 provides:\n(1) Expanded Deep Learning Support: PyOD 2 integrates 12 state-of-the-art neural models, refactored into a single PyTorch framework, bringing the total number of OD models to 45.\n(2) Enhanced Performance and Ease of Use: All models are optimized for efficiency and consistency, ensuring reliable performance across various datasets.\n(3) First LLM-driven OD Model Selection: PyOD 2 introduces an automated model selection mechanism, using an LLM-based reasoning process. This reduces the need for manual tuning and supports users who may lack deep domain knowledge.\nDemonstration. In \u00a73, we demonstrate (1) how to use PyOD 2 to integrate the latest OD models such as LUNAR [4] into workflows in just five lines of code and (2) show how its LLM-based model selection provides effective guidance for choosing OD models from a pool of 10 models on 17 OD datasets."}, {"title": "2 Overview and Design of PyOD 2", "content": "The original PyOD [22] offers a unified API that covers a wide range of algorithms, including proximity-based methods, linear models, neural methods, and ensemble techniques. It provides JIT optimizations, parallelization, cross-platform compatibility, and extensive testing, making it robust and easy to integrate.\nBuilding on these foundations, PyOD 2 introduces major improvements in deep learning algorithm support and usability (\u00a72.1) while adding a novel model selection framework powered by LLMs (\u00a72.2). These updates modernize the framework and ensure PyOD 2 remains flexible, reliable, and adaptable."}, {"title": "2.1 Enhanced Coverage and Usability", "content": "PyOD 2 significantly strengthens its deep learning capabilities by integrating advanced algorithms and improving ease of use through a unified PyTorch-based design.\nIntegration and Optimization of Deep Learning Models. PyOD 2 adds over ten state-of-the-art deep learning OD models (see Table 1), including MO-GAAL [11], SO-GAAL [11], AE [1], VAE [6], AnoGAN [18], DeepSVDD [17], ALAD [21], AE1SVM [13], DevNet [15], and LUNAR [4]. Each model is optimized for unified usability, training stability, memory management, and performance.\nUnified PyTorch Framework. All neural-based models in PyOD 2 now run on a unified PyTorch framework [16], enabling consistent execution, simplified debugging, and efficient integration of future methods. The PyTorch environment also supports GPU acceleration, improving scalability and performance.\nStandardized Deep Learning Architecture. PyOD 2 introduces a base class, base_dl, providing a standard structure for all deep learning models. Common operations, including model initialization, training, and evaluation, are standardized, reducing code redundancy and simplifying the addition of new models."}, {"title": "2.2 Automated Model Selection Pipeline", "content": "We summarize the pipeline in Fig. 1. In this three-step process, the system first extracts symbolic metadata from each model to represent its strengths and weaknesses. It then profiles the dataset to produce a set of symbolic tags describing its statistical characteristics. Finally, it compares these model and dataset tags, ranking candidate models and applying LLM-based reasoning to choose the most suitable one. We now discuss each step in detail.\nStep 1: Encoding Symbolic Metadata for Models. The first step encodes each OD model's core properties as structured symbolic metadata. By analyzing model papers and source code, we extract details regarding their strengths and weaknesses, storing them as:\n$M_{meta}(m_i) = \\{strengths(m_i), weaknesses(m_i)\\}$,\nwhere each $m_i \\in M$ is associated with strengths that highlight its suitability for certain conditions (e.g., \"effective in high-dimensional data\") and weaknesses indicating scenarios where it may falter (e.g., \"computationally heavy\"). These symbolic descriptions establish a consistent basis for subsequent reasoning about model selection.\nStep 2: Generating Dataset Profiles and Symbolic Tags. Next, we profile each dataset D by computing descriptive statistics that characterize its complexity and distribution:\n$A_D = \\{dimensionality(D), skewness(D),\\\\ kurtosis(D), noise level(D), . . . \\}$.\nThese attributes are then mapped into a set of symbolic tags:\n$T_D = \\Psi_{LLM}(A_D)$,\nwhere $\\Psi_{LLM}()$ leverages LLMs to convert raw metrics into standardized tags (e.g., \"imbalanced data\" or \"noisy features\"). This transformation enables direct comparison between dataset properties and model metadata.\nStep 3: Model Selection via Symbolic-Neural Reasoning. In the final step, the pipeline applies symbolic reasoning and LLM-guided refinement to select a suitable model. Each model $m_i$ is evaluated by comparing its strengths and weaknesses against $T_D$:\n$S(m_i, T_D) = sim(strengths(m_i), T_D)\\\\ -penalty (weaknesses(m_i), T_D)$,\nwhere $sim()$ measures how well the model's strengths align with dataset tags, and $penalty (.)$ quantifies the impact of misaligned weaknesses.\nModels with scores $S(m_i, T_D)$ above a chosen threshold $\\delta$ form a candidate set (i.e., smaller $\\delta$ leads to more candidates):\n$M^* = \\{m_k | S(m_k, T_D) \\geq \\delta, m_k \\in M\\}$."}, {"title": "3 Demo of PyOD 2", "content": "Case 1: Outlier Detection in Five Lines of Code. This example shows how to train and evaluate a LUNAR model (Learning Universal Normal Abnormality Representations) with minimal code. The LUNAR model is one of the deep learning-based outlier detection algorithms included in PyOD 2. By integrating it into an existing PyOD workflow, users can quickly assess anomalies in their data.\nWith these few lines, the model is trained, and outlier scores can be easily obtained for both training and test datasets. Refer to the PyOD documentation for further examples and customization options.\nCase 2: Automated Model Selection Using LLMs. In this example, we demonstrate the automated selection and training of an outlier detection model using a LLM-powered framework. The AutoModelSelector class identifies the most suitable model from PyOD 2' extensive collection. It evaluates the dataset's statistical properties and uses symbolic-neural reasoning to produce a recommended model. By reducing the need for manual tuning, this approach is especially valuable for complex or unfamiliar datasets.\nAn optional parameter additional_notes allows users to provide domain insights about the dataset. Although these notes can guide the selection, the method can perform well using purely statistical attributes. After selecting a model, get_top_clf() returns the best candidate, which can then be trained and evaluated."}, {"title": "4 Experiment Settings and Results", "content": "Datasets and Models. We evaluated our approach on a diverse collection of 17 common datasets obtained from ADBench [5]: pima, cardio, mnist, arrhythmia, pendigits, shuttle, letter, musk, vowels, optdigits, satellite, lympho, ionosphere, wbc, glass, satimage-2, and vertebral. For each dataset, we consider 10 deep anomaly detection models from Table 1: MO-GAAL, AutoEncoder, SO-GAAL, VAE, AnoGAN, DeepSVDD, ALAD, AE1SVM, DevNet, and LUNAR. Our objective is to select a single model from these candidates for each dataset, thereby demonstrating how the AutoModelSelector can identify the most suitable method given the dataset's characteristics.\nBaselines and Evaluation. We compare five approaches: (1) the average performance of all models, which simulates random user choices; (2) an AutoEncoder and (3) LUNAR, representing a common baseline and a strong standalone deep learning method; and (4) two versions of the AutoModelSelector, one with and one without additional notes. We evaluate these methods based on their rankings regarding the area under the ROC curve (AUROC) across multiple datasets. Lower ranking values indicate better performance."}, {"title": "5 Conclusion and Future Directions", "content": "We introduced PyOD 2, a significant update to the PyOD library, emphasizing integrating state-of-the-art deep learning models under a unified PyTorch framework and introducing a large language model (LLM)-driven automated model selection process. By offering an expanded set of 45 outlier detection algorithms, including 12 recently developed deep learning methods, PyOD 2 streamlines the deployment and evaluation of outlier detection systems. These enhancements support both researchers and practitioners by simplifying model integration, improving accessibility, and reducing the barriers to identifying suitable methods for specific datasets.\nSeveral opportunities remain for further development. One direction is to explore the integration of domain-specific priors, leveraging knowledge from fields such as cybersecurity, healthcare, and online retail to guide automated model selection more efficiently. Another direction involves extending the framework to support continual learning and adaptive pipelines that adjust to shifting data distributions. Expanding the LLM-based reasoning module to incorporate advanced feedback loops and user interaction could also refine the decision-making process. Additionally, there is potential to improve computational scalability and incorporate distributed training strategies to handle larger and more complex datasets."}]}