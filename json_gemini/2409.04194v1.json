{"title": "Towards Privacy-Preserving Relational Data Synthesis via Probabilistic Relational Models", "authors": ["Malte Luttermann", "Ralf M\u00f6ller", "Mattis Hartwig"], "abstract": "Probabilistic relational models provide a well-established formalism to combine first-order logic and probabilistic models, thereby allowing to represent relationships between objects in a relational domain. At the same time, the field of artificial intelligence requires increasingly large amounts of relational training data for various machine learning tasks. Collecting real-world data, however, is often challenging due to privacy concerns, data protection regulations, high costs, and so on. To mitigate these challenges, the generation of synthetic data is a promising approach. In this paper, we solve the problem of generating synthetic relational data via probabilistic relational models. In particular, we propose a fully-fledged pipeline to go from relational database to probabilistic relational model, which can then be used to sample new synthetic relational data points from its underlying probability distribution. As part of our proposed pipeline, we introduce a learning algorithm to construct a probabilistic relational model from a given relational database.", "sections": [{"title": "1 Introduction", "content": "Probabilistic relational models such as parametric factor graphs (PFGs) combine first-order logic and probabilistic models and thereby provide an adequate formalism to represent relationships between objects in a relational domain. PFGs compactly encode a full joint probability distribution over a set of random variables (randvars) and hence allow to sample new relational data points from the encoded underlying full joint probability distribution. The generated synthetic relational data samples then follow the underlying full joint probability distribution and might be used for various purposes. Common applications of synthetic data include for example training machine learning models or sharing data without violating the privacy of individuals [8,27,35]. Another application could be to bootstrap PFG learning (using the given database to learn a PFG, which is then applied to generate additional synthetic relational data points to learn another PFG on the basis of a larger data set and then possibly repeating the procedure)."}, {"title": "2 Preliminaries", "content": "We start with the definition of factor graphs (FGs) as propositional probabilistic models and then continue to introduce PFGs, which combine first-order logic with probabilistic models. An FG is an undirected propositional probabilistic model to compactly encode a full joint probability distribution over a set of randvars [11,21]. Similar to a Bayesian network [28], an FG factorises a full joint probability distribution into a product of factors.\nDefinition 1 (Factor Graph). An FG $G = (V, E)$ is an undirected bipartite graph with node set $V = R \\cup \\Phi$ where $R = \\{R_1,..., R_n\\}$ is a set of variable nodes (randvars) and $\\Phi = \\{\\phi_1,..., \\phi_m\\}$ is a set of factor nodes (functions). The term range($R_i$) denotes the possible values of a randvar $R_i$. There is an edge between a variable node $R_i$ and a factor node $\\phi_j$ in $E \\subseteq R \\times \\Phi$ if $R_i$ appears in the argument list of $\\phi_j$. A factor is a function that maps its arguments to a positive real number, called potential. The semantics of G is given by\n$P_G = \\frac{1}{Z} \\prod_{j=1}^m \\phi_j(A_j)$\nwith Z being the normalisation constant and $A_j$ denoting the randvars connected to $\\phi_j$ (that is, the arguments of $\\phi_j$)."}, {"title": "3 Proposed Architecture", "content": "In this section, we provide an overview of the general architecture to generate synthetic relational data from a relational database using a PFG. We first take a look at the steps involved in the synthetic data generation approach and afterwards continue to investigate each of the steps in more detail.\nAn overview of the architecture of our proposed architecture is depicted in Fig. 3. The whole process consists of three primary steps, which can again be decomposed into various subroutines. The three primary steps consist of (i) constructing a propositional FG, (ii) transforming the propositional FG into a PFG, and (iii) sampling from the PFG to generate new synthetic relational data. Besides the generated synthetic data, the PFG is also a valuable output of the architecture (indicated by the + sign), as it can be used for various tasks such as probabilistic inference, causal inference, or bootstrap PFG learning, for example."}, {"title": "3.1 Construction of a Propositional Factor Graph", "content": "While there are learning algorithms for first-order probabilistic models such as MLNs, to the best of our knowledge, there is currently no approach to directly learn a PFG from a given relational database. However, there are well-known approaches to learn an FG from the given data [1] and an FG can be transformed into a PFG by running the so-called advanced colour passing (ACP) algorithm [23]. We therefore propose to first learn a propositional model, that is, an FG G, from the given relational database and afterwards run the ACP algorithm on G to transform G into a PFG entailing equivalent semantics as G.\nWhile such an approach seems straightforward at first glance, there are a few challenges to overcome. A major challenge is that applying a standard learning algorithm to obtain an FG from data does not fit our setting because standard learning algorithms do not include randvars and factors for individual objects into the FG. In other words, the relational structure of the data is neglected, which we illustrate in the upcoming example."}, {"title": "3.2 Transforming the Factor Graph into a Parametric Factor Graph", "content": "To obtain a PFG from a given FG, we need to find groups of identically behaving randvars and factors in the FG. Then, PRVs with logvars represent such groups of indistinguishable randvars and parfactors represent groups of identical factors. Replacing indistinguishable randvars by PRVs with logvars further abstracts from individuals and thus yields a promising foundation for privacy guarantees [12]. The ACP algorithm (which is a generalisation of the colour passing algorithm [2,15]) is able to construct a PFG from a given propositional FG [23]. The idea behind ACP is to exploit symmetries in a propositional FG and then group together symmetric subgraphs. ACP looks for symmetries based on potentials of factors, on ranges and evidence of randvars, as well as on the graph structure by passing around colours. A formal description as well as an example run of the ACP algorithm can be found in Appendix B. Figure 6 shows the PFG resulting from calling ACP on the FG depicted in Fig. 5 under the assumption that all potentials of the factors $\\phi_i$, $i \\in \\{1,...,4\\}$, are considered identical. Note that the assumption of identical factors is just for the sake of the example as in general, not all potentials are identical (and hence, not all of the factors $\\phi_i$ are grouped into a single group).\nWe remark that in its original form, ACP requires potentials of factors to identically match in order to group factors together. When learning an FG from"}, {"title": "3.3 Sampling from the Parametric Factor Graph", "content": "Every PFG compactly encodes a full joint probability distribution from which we can draw new samples. Note that, in contrast to previous sampling approaches (such as, e.g., [33]) that apply sampling for (approximate) query answering, we aim to generate new data samples that comply with the given ER model (that is, we wish to draw new data samples that contain a value for every attribute and every relationship in the ER model). As the PFG is inherently encoding a relational structure, we are able to synthesise relational data. More specifically, the semantics of a PFG is defined on a ground level, that is, sampling from"}, {"title": "4 Conclusion", "content": "We introduce a fully-fledged pipeline to deploy probabilistic relational models, in particular PFGs, for the generation of synthetic relational (i.e., multi-table) data. To construct a PFG from a given relational database, we propose a learning algorithm that learns both the graph structure as well as the parameters of a PFG from the relational database. We further elaborate on how the learned PFG can be applied to generate new samples of synthetic relational data. By ensuring certain privacy guarantees (e.g., differential privacy) during the construction process of the PFG, PFG provide a promising model to generate synthetic relational data in a privacy-preserving manner such that generated synthetic data can be publicly shared without leaking sensitive data of individuals.\nThere are three primary directions for future work. First, privacy guarantees for PFG learning need to be theoretically investigated. Second, the scalability of our proposed architecture should be assessed and improved to allow an efficient handling of large-scale relational databases, and finally, the practicality of the entire framework has to be tested empirically on real-world data sets."}, {"title": "A Augmented Full Join", "content": "We call the full join of the tables, where an additional column for each relationship is added and missing relationships are encoded by an additional row containing the value false in the corresponding relationship column, the augmented full join. The augmented full join can therefore be thought of as a cross join with an additional column for each relationship, which contains a Boolean value indicating which relationships are present in the database. For example, the augmented full join of the tables given in the example from Fig. 4 is illustrated in Fig. 7. In this particular example, the augmented full join contains the additional column Treat, which contains the value true if the relationship of a given combination of PatientId and MedicationId in a specific row actually exists. Otherwise, the column Treat contains the value false."}, {"title": "B Formal Description of the Advanced Colour Passing Algorithm", "content": "The ACP algorithm [23] builds on the colour passing algorithm [2,15] and solves the problem of constructing a PFG from a given FG. Algorithm 2 provides a formal description of the ACP algorithm.\n1 Assign each $R_i$ a colour according to $R(R_i)$ and $E$;\n2 Assign each $\\phi_i$ a colour according to order-independent potentials and rearrange arguments accordingly;\n$P(X,Y | Z) = P(X | Z) \\cdot P(Y | Z)$"}]}