{"title": "Atlas: A Framework for ML Lifecycle Provenance & Transparency", "authors": ["Marcin Spoczynski", "Marcela S. Melara", "Sebastian Szyller"], "abstract": "The rapid adoption of open source machine learning (ML) datasets and models exposes today's AI applications to critical risks like data poisoning and supply chain attacks across the ML lifecycle. With growing regulatory pressure to address these issues through greater transparency, ML model vendors face challenges balancing these requirements against confidentiality for data and intellectual property needs.\nWe propose Atlas, a framework that enables fully attestable ML pipelines. Atlas leverages open specifications for data and software supply chain provenance to collect verifiable records of model artifact authenticity and end-to-end lineage metadata. Atlas combines trusted hardware and transparency logs to enhance metadata integrity, preserve data confidentiality, and limit unauthorized access during ML pipeline operations, from training through deployment.\nOur prototype implementation of Atlas integrates several open-source tools to build an ML lifecycle transparency system, and assess the practicality of Atlas through two case study ML pipelines.", "sections": [{"title": "1. Introduction", "content": "In recent years, machine learning (ML) models, have become increasingly popular. The pervasive use of large language models (LLMs), in particular, and multi-stakeholder involvement in model creation and deployment exacerbate security and privacy risks. These considerations are emphasized by the global nature and the complexity of large-scale ML deployments with different lifecycle stages:\n1) Collection and sanitation of a training dataset from several public and proprietary sources.\n2) Provisioning of the training environment (hardware and software).\n3) Execution of training across many data centers.\n4) Construction of a testing dataset from several sources, and the evaluation.\n5) Deployment and use of the model for inference that is compliant with local laws or corporate policies.\nEach of these stages is vulnerable to malicious or dishonest parties. For example, data can be poisoned [1], [2] during collection or training. Service providers executing outsourced training can shorten or omit critical steps to reduce their cost. Model providers can serve smaller models in SaaS, or even distribute malicious ones.\nOn the other hand, responsible model builders and other stakeholders may be incentivised or required to provide security and trust guarantees. They may want to prove low bias in their training data, offer easily verifiable performance claims, or guarantee end-to-end integrity of the model creation in high risk domains.\nTo address these challenges, it is necessary to guarantee the integrity of the entire ML lifecycle \u2013 beginning with the data, through the training, and finally, the evaluation and deployment. Was the data modified? Did the hardware and software environment adhere to the specification? Did the contractor follow the specified training procedure? Can I trust the evaluation? How can I guarantee that I am interacting with the intended model? These are example questions that showcase the breadth of the involved challenges that must be tackled to provide end-to-end security.\nIn this work, we introduce Atlas, a framework for enhancing the security and transparency of the lifecycle of ML models. Atlas establishes the baseline of fundamental components and capabilities needed for comprehensive provenance tracking at each stage of the ML lifecycle. Subsequently, Atlas defines the core integrity requirements for verifiable ML lifecycle transparency. We provide a reference implementation that instantiates Atlas using hardware-based security mechanisms \u2013 with trusted execution environment (TEE), including attestations.\nWe claim the following contributions:\n1. We introduce Atlas, a framework designed for end-to-end ML lifecycle transparency.\n2. We instantiate Atlas using TEEs and metadata-based provenance tracking.\n3. We evaluate our Atlas prototype through two case studies: 1) fine-tuning of a BERT model [3], [4]; 2) fine-tuning of a bge-reranker model [5]."}, {"title": "2. Background & Related Work", "content": null}, {"title": "2.1. Data Provenance & Authenticity", "content": "While not a new concern, the authenticity, provenance and attribution of media has received renewed attention in recent years due to the wide online proliferation of images, videos and text that have been manipulated or forged using generative ML models [6], [7].\nOne common approach for addressing digital image and video manipulation involves cryptographic techniques such as hashing, digital signatures and blockchains, which enable provenance authentication, tamper detection, and checking for divergent views of the content [8].\nMost notably, the Coalition for Content Provenance and Authenticity (C2PA) specification [9], [10] introduced by the Content Authenticity Initiative (CAI) [11] and Project Origin [12] provides cryptographic binding of authenticity claims, verifiable assertions about content origin and modifications, and tamper-evident audit trails [13].\nInitially launched as a response to the growing challenge of deepfakes [11] and digital content manipulation, the C2PA standard first gained traction in digital photography and journalism workflows. The standard's extensible and interoperable metadata format has made C2PA an attractive option for ML dataset and model provenance tracking [14].\nOther prior work in this space builds upon digitally signed metadata by leveraging distributed ledger technologies to create transparent and immutable content or provenance records. Examples of this approach include AMP [7], the News Provenance Project [15] and Collomosse et al. [14].\nAtlas borrows techniques from C2PA and similar approaches, but integrates them directly into the ML system to track model artifact provenance closer to where the transformations occur to achieve higher metadata fidelity, rather than applying them in an ad hoc fashion after a particular dataset or model artifact has been created.\nThe Open Source Security Foundation (OpenSSF) Model Signing project [16] is a parallel effort in this space focusing specifically on cryptographic signing of model artifacts to ensure their authenticity and integrity. As such, it serves as a crucial building block but requires integration with other tools for more comprehensive supply chain security. While Model Signing might integrate with Atlas, additional mechanisms would be needed to address our goal of tracking full model lineage.\nLakeFS [17] combines Git-like semantics with concepts from object stores such as S3 to provide a version control system for data, including ML datsets. Thus, LakeFS aims to capture data lineage by tracking changes to stored data over time, and allowing ML applications to reference specific versions of the stored data. This approach is meant to integrate with existing first-party data processing pipelines, but does not facilitate verification of data provenance by downstream consumers. Atlas' metadata centered approach, on the other hand, enables first- and third-party ML dataset consumers to track changes and check their provenance, even when they may not have direct access to the data."}, {"title": "2.2. Supply Chain Integrity", "content": "While the ML supply chain is an emerging area of research, software supply chain security and integrity has been the subject of much prior work. Thus, several of these approaches are being explored for their applicability to enhancing the integrity of the ML model lifecycle.\nBills of Materials (BOM) have been employed to document the list of components of a hardware or software product for over three decades [18]. Due to recent regulations [19], [20], Software BOM (SBOM) have been the focus of many industry and academic efforts seeking to facilitate tracking software dependencies and other metadata to detect supply chain threats [21].\nThe AIBOM framework [22], [23] incorporates concepts from traditional software supply chain management while focusing on intended ML model use cases. Like SBOM, AIBOM provide a mechanism for tracking model software dependencies and maintaining model metadata. At the time of writing, we are not aware of any frameworks other than Atlas that utilize any sort of BOM data format to track ML model components.\nComplementing BOM, a number of efforts like OpenSSF Supply Chain Levels for Software Artifacts (SLSA) [24] and SPDX Build [25] have sought to collect build provenance, i.e., metadata describing the process through which a particular artifact was produced. With its incremental guidance on implementing integrity measures for software builds and its build provenance data format, the SLSA framework [24] is gaining traction among software vendors, and also beginning to see some use for ML model fine tuning [26]. Atlas' configurable metadata collection capabilities would enable us to collect build provenance at any stage of the ML model lifecycle, providing a more comprehensive view.\nA number of frameworks for capturing and verifying a variety of security claims and metadata about the supply chain have been proposed. in-toto [27] collects authenticated claims across supply chain steps, including SBOM and SLSA metadata. In particular, in-toto enables software development pipeline owners and downstream artifact consumers to specify end-to-end supply chain policies, and validate that only the expected parties carried out specific steps in the supply chain, and artifacts underwent transformations in the expected order. Given recent and upcoming enhancements that further generalize the framework, in-toto may be a suitable option for specifying and verifying end-to-end ML model lifecycle integrity policies in Atlas.\nSigstore [28] provides a transparency log-based infrastructure for issuing signing credentials and validating digital signatures on supply chain artifacts and metadata. Similarly, Supply Chain Integrity, Transparency and Trust (SCITT) [29] is an architecture for implementing distributed ledger to provide global visibility and auditing for supply chain operations and claims. The SCITT architecture also includes confidential computing technologies to help ensure that only authorized parties submit claims to the transparency ledger.\nAtlas builds upon several of these supply chain integrity and transparency approaches, but seeks to support multiple types of artifact provenance, supply chain metadata and integrity verification mechanisms all within a customizable, integrated framework designed for the ML model lifecycle."}, {"title": "2.3. Model Lineage Tracking", "content": "The EQTY Lineage Explorer [30] provides granular tracking of model artifacts throughout the training process. The tool captures relationships between datasets, model checkpoints and hyperparameters, enabling developers to trace model evolution. Unlike Atlas, it lacks cryptographic guarantees for artifact authenticity and focuses primarily on manually collected development-time lineage, rather than automatically capturing and linking information across the entire ML lifecycle.\nML experiment trackers like Weights and Biases [31], Neptune [32] and Kubeflow Pipelines [33] offer detailed run-time logging of model metadata about training runs, metrics, and model artifacts. While comprehensive for collaborative ML application development, these tools do not necessarily integrate transparently with common ML frameworks, and they typically provide only unauthenticated metadata that makes it less appropriate for establishing trust in a particular ML pipeline. Atlas, on the other hand, focuses making model lineage verifiable and on enabling adoption by supporting integration into existing open ML frameworks."}, {"title": "2.4. Hardware-Based Security for ML", "content": "Recent developments in trusted execution environments (TEEs) technologies have made it more practical to run large-scale systems and workloads. Thus, a growing number of proposals leverage TEEs to harden a variety of ML systems through hardware-enforced isolation and integrity verification mechanisms.\nChrapek et al. [34] use TEEs in a large language model (LLM) fine-tuning system running inside secure enclaves that help protect LLM code and data while in use. To this end, they evaluate their LLM system using the process-level Intel\u00ae Software Guard eXtensions (Intel\u00aeSGX) and virtual machine-level Intel\u00aeTrust Domain eXtensions (Intel\u00aeTDX) TEEs, showing that they are able to maintain practical performance overhead of less than 10% for models like Llama2 in both TEE configurations.\nLaminator [35] demonstrates the application of TEEs to ML property attestation for ML model regulatory compliance verification. Specifically, this work leverages TEEs' hardware-assisted remote attestation and cryptographic binding capabilities to generate verifiable ML model property cards. This work is complementary to Atlas and may enable us to extend our framework to support such ML property attestations.\nMo et al.'s survey [36] evaluates 38 works that use various TEE implementations to enhance the privacy and integrity of ML training and inference operations. The survey highlights several gaps that existing TEE-based frameworks do not address, including the protection of a full ML lifecycle, which is the primary focus of Atlas."}, {"title": "3. System Overview & Threat Model", "content": null}, {"title": "3.1. Terminology", "content": "In Atlas, an ML model is composed of a number of artifacts that include the training dataset, ML algorithm, ML framework (e.g., PyTorch), model configuration (e.g., hyperparameters, weights), and metadata (e.g., license).\nThe ML lifecycle consists of various stages, including data preparation, training, evaluation and deployment. A common synonym for ML lifecycle is \"ML supply chain\", so we use these terms interchangeably in the paper.\nThe ML pipeline defines the sequence of operations or a workflow that transforms or operates on a model artifact at a particular stage of the ML lifecycle [37]. To support the standardization and repeatability of a specific workflow, the pipeline is also used to facilitate workflow management and automation, e.g., training or evaluation pipelines.\nThe ML system is the set of hardware and software components that implement and execute an ML pipeline. For example, an ML system for training may include orchestration tools, an authentication service, storage systems, automation infrastructure, and specialized compute hardware (e.g., GPUs, TPUs, or custom accelerators)."}, {"title": "3.2. System Model", "content": "In Atlas, we target the multi-stakeholder ML model lifecycle setting."}, {"title": "3.2.1. Stakeholders", "content": "The Artifact Producers are individuals and organizations that create ML model artifacts to provide or sell them to other parties. Since the artifacts may represent intellectual property and/or make use of personally identifying information (PII), producers have business and regulatory reasons to preserve the confidentiality of their artifacts. To save costs, artifact producers often outsource the operation of ML systems to external MLaaS providers.\nML-as-a-Service (MLaaS) providers operate and maintain the compute infrastructure needed to run ML systems on behalf of artifact producers. MLaaS providers may offer ML-specific services that leverage general-purpose systems (e.g., [38]\u2013[40]), or provide special-purpose systems like orchestrators [33] or CI/CD (e.g. [41]) that can be used to build and run third-party ML systems.\nA Hub is a system that stores and distributes model artifacts. Thus, model pipelines typically ingest and output artifacts to and from hubs during their execution, enabled by interfaces exposed by MLaaS providers.\nHubs may be operated by artifact producers themselves or by third-party vendors, containing open or closed source artifacts (e.g.,HuggingFace [42], PyTorch Hub [43]).\nA Transparency Service in Atlas is responsible for generating, storing and distributing the information necessary to verify the authenticity and integrity of model artifacts, ML systems and lifecycles as a whole. We envision model vendors and independent parties to operate transparency services in practice.\nVerifiers are entities that seek to evaluate or audit a particular ML model's lifecycle with the goal of checking whether the model it produced was tampered with in an unintended or malicious way. To this end, verifiers in Atlas define a set of expectations about a producer's model artifacts, MLaaS providers, and their ML system components. In practice, model users, vendors or regulatory entities may act as verifiers.\nA Model User interacts with a model in an inferencing pipeline, or in a downstream ML pipeline as a dependency, such as a fine-tuning or evaluation pipeline (see \u00a73.2.2)."}, {"title": "3.2.2. ML Lifecycle", "content": "In Atlas, we consider four common high-level stages in the ML lifecycle. Each builds upon the outputs and feedback from the others, forming a continuous cycle that allows models to evolve and adapt based on real-world usage.\n1. Data processing: Raw data is collected, curated, processed into smaller units (e.g., tokens) and collated into a structure ingestable during training.\n2. Training: a training algorithm processes a given dataset using an ML framework. The output of the training pipeline is an ML model that may undergo further fine tuning and evaluation, prior to being deployed for inferencing.\n3. Evaluation: following training, model properties like its performance and accuracy are checked and further adjusted, prior to production release and deployment.\n4. Deployment: Once an ML model has been trained and evaluated, it is deployed to a production system configured for inferencing. The new data obtained from clients during inference is sent back to a data processing pipeline to enhance the training dataset, and the model."}, {"title": "3.3. Threat Model", "content": "Atlas is concerned with addressing risks to ML model artifacts introduced via the ML lifecycle, i.e., attack vectors in third-party MLaaS and hubs across different ML pipelines that lead to compromised ML artifacts. Specifically, while transparency services, verifiers and model users are trusted in Atlas, we consider threats by MLaaS providers, hubs and artifact producers.\nThe adversary's goal is to produce a tampered artifact or model, e.g., containing a hidden malicious component, so that a trusted transparency service generates a legitimate signature on the artifact or its metadata.\nCompromised MLaaS providers and hubs may involve malicious insiders, or external adversaries seeking to subvert these systems by exploiting vulnerable components. Given their central position, MLaaS providers and hubs may thus be able to compromise model integrity at various stages of the ML lifecycle.\nFor example, a malicous MLaaS provider can poison the training data during the curation step of the data processing stage leading to backdoors [1], [2]. A malicious hub may, for instance, present a dataset or model with a mismatched signature (e.g., to a different model, or any of its component artifacts) to a model user or MLaaS provider, so that pipelines in subsequent stages of the ML lifecycle may ingest compromised dependencies.\nAs a result, these compromises may propagate through the ML lifecycle if they go undetected, ultimately leading to vulnerable ML models at the deployment stage. This risk is exacerbated if a hub colludes with an MLaaS providers to introduce or accept compromised ML pipeline inputs.\nArtifact producers, on the other hand, may seek to compromise ML models in order to bypass regulatory requirements, introduce exploitable vulnerabilities or steal private information for profit (e.g, [44]). Thus, producers may collude with other untrusted participants, or inaccurately declare their dependencies leading to intentional omissions or misrepresentations their artifacts, to surreptitiously undermine the integrity of their artifacts."}, {"title": "3.3.1. Trusted Parties", "content": "As mentioned, Atlas considers the transparency services, verifiers and model users in an ML lifecycle to be trusted. We make the following assumptions about the security of the systems underlying and enabling the participants: 1) the hardware and cryptographic primitives are implemented correctly and do not contain known vulnerabilities; 2) a separate PKI system and organizations representing the participants follow best practices for key management, network security and access control.\nThis allows us to trust that transparency services correctly implement Atlas attestation mechanisms, detect tampering with them by a malicious MLaaS provider and maintain accurate golden values of model artifacts. Verifiers are trusted to properly evaluate Atlas attestations, including their digital signatures, against pre-specified expectations, and to publish a record of their evaluations."}, {"title": "3.3.2. Out of Scope", "content": "We consider the following threats to be out of scope for Atlas. While a critical threat to deployed AI applications, inference time black-box attacks (e.g., evasion attacks, model extraction, membership inference) are caused by malicious users, which we do not address; solutions proposed to reduce this particular risk [45]\u2013[47] are complementary to our work.\nSide-channel attacks against hardware enclaves, physical attacks on hardware infrastructure, and network-level denial of service attacks are also beyond the scope of Atlas. These attacks are the subject of a large body of prior work [36], and we acknowledge that these complementary security measures could be added to deployments of Atlas."}, {"title": "3.4. Design Requirements", "content": "We define the following integrity and operational requirements for Atlas:\nR1: Artifact tampering is detectable. To provide model artifact authenticity, Atlas must enable verifiers to detect unexpected modifications to model artifacts by malicious MLaaS providers or hubs.\nR2: Every model transformation is attested. Because adversaries may seek to tamper with model artifacts after they are produced by a pipeline, every model transformation in Atlas must produce an authenticated record as evidence for the process.\nR3: Verifiable model lineage. Atlas verifiers must be able to detect unauthorized operators and unintended or malicious changes to the expected stages of the lifecycle (e.g., pipelines operating out of order, or being omitted).\nR4: Strongly isolated ML systems. To reduce the risks of tampering with a pipeline during its execution, Atlas must restrict access to its ML system by unauthorized entities, and contain compromises from propagating beyond the execution environment.\nR5: Controlled artifact disclosure. Atlas must address data and model producer concerns about PII and intellectual property confidentiality by enabling participants to control which parties and systems are able to access datasets and model components.\nR6: Pipeline agnostic. To facilitate adoption, Atlas must be agnostic to any ML pipeline that integrates it.\nR7: Efficiency. We seek to minimize the computational and storage overheads incurred by Atlas to enable the implementation and deployment of Atlas in ML systems using commodity platforms and services."}, {"title": "4. Atlas Framework", "content": "Atlas introduces two core components to the ML lifecycle: 1) the transparency service interacting with MLaaS providers; 2) the verification service that model users and verifiers use to verify integrity and provenance. Fig. 1 depicts an example ML model lifecycle with Atlas."}, {"title": "4.1. Transparency Service", "content": "To provide transparency across stages of the ML lifecycle, MLaaS providers integrate an Atlas attestation client in their system. The transparency log makes all attestation clients' collected metadata available to verifiers. The four core techniques underlying the Atlas transparency service are designed to be general, allowing them to remain agnostic to the particular ML lifecycle stage or pipeline they are applied to (R6)."}, {"title": "4.1.1. Artifact Measurements", "content": "The Atlas attestation client within an MLaaS provider cryptographically measures every artifact ingested into and output by an ML pipeline. That is, we use a collision resistant cryptographic hash function to compute unique, immutable measurement values for every artifact. Because any changes to a given artifact result in a different hash value, Atlas verifiers wishing to check whether a given model artifact matches their expectation are able to detect tampering between stages of the lifecyle (R1)."}, {"title": "4.1.2. Model Transformation Integrity", "content": "In addition to capturing measurements about all inputs and outputs of an ML pipeline, the Atlas attestation client continuously verifies new inputs and monitors an ML system's execution to collect detailed information about pipeline state and operations that transform the input artifacts. That is, during data processing, the client tracks dataset modifications and preprocessing operations; during model training, Atlas captures state changes in model weights, hyperparameters, and configurations.\nTo enhance the integrity of the ML artifacts and system at runtime, Atlas runs ML pipeline code within trusted execution environments (TEEs) (e.g., Intel TDX [48] or AMD SEV-SNP [49]), which serve as a hardware-based root of trust for ML system component measurements extending from its hardware to the firmware and software.\nThese measures allow the Atlas attestation client to verify the integrity and authenticity of the compute environment before beginning the execution of the pipeline. Throughout execution, the TEE maintains isolated memory regions, reducing the risk of interference with the pipeline, as well as unauthorized disclosure of artifacts and pipeline code, by a compromised MLaaS provider (R4 & R5).\nAt the conclusion of an operation in a given pipeline, the client generates a record with all collected ML system information, and digitally signs it with a TEE-generated key along with all artifact measurements, providing a model transformation attestation (R2). If required by the artifact producer, Atlas may encrypt the produced model artifacts prior to digitally signing and uploading them to a given hub to further enhance artifact authenticity and confidentiality while in transit and at rest (R1 & R5)."}, {"title": "4.1.3. Provenance Chains", "content": "To allow verifiers to trace an ML model throughout all of its lifecycle stages from data preparation to deployment, Atlas embeds the cryptographic hash of the transformation attestation for every pipeline input that passed verification into the output attestation. These hash values are digitally signed as part of the transformation attestation. Thus, Atlas attestation clients establish an authenticated, cryptographically verifiable provenance chain representing a model's lineage relationships throughout its lifecycle (R3)."}, {"title": "4.1.4. Transparency Log", "content": "On the server side, a transparency log contains the golden values (i.e., known good values) of a producer's model artifacts, an MLaaS provider's system components, and the transformation attestations collected by the clients throughout the ML lifecycle. To enable efficient insertion and provenance verification while accommodating the cyclical nature of the ML lifecycle, Atlas relies on two data structures.\nFirst, to provide cryptographic tamper-evidence for the stored values, the transparency log is constructed using an append-only Merkle tree [50], meaning that pipeline metadata can be efficiently inserted in the right-most empty leaf node of the tree (e.g., as in [28]). Second, to enable more efficient verification of provenance across pipelines (or even cycles of the ML lifecycle), Atlas can represent each discreet pipeline/cycle using a different Merkle tree and chain the trees by embedding the Merkle root hash of the upstream pipeline or previous cycle into the \"current\" Merkle tree's root (e.g., as in [51]), limiting tree traversal operations when verifying a lifecycle over time (R7).\nWe note that the choice of whether and how to partition the transparency log's Merkle trees will result in different security and performance tradeoffs; depending on the party that operates the transparency service in practice, they may favor maintaining a smaller or larger number of trees according to their specific metadata access control and resource requirements."}, {"title": "4.2. Verification System", "content": "The Atlas verification system performs staged verification. When a pipeline ingests an input artifact, the attestation client requests the artifact's verification at the verification service, interacting with the transparency log to obtain the golden values and provenance information.\nFirst, the verification service validates that the received metadata was produced by the expected parties via their digital signatures, and checks whether the artifact matches the expected golden value measurements. If the first verification stage passes, Atlas inspects the received transformation metadata to confirm the recorded ML system and pipeline operations ran as expected based on their TEE-backed attestations. Third, the artifact's lineage is validated by tracing its provenance chain to check its transformations as far back as specified in the verifier's policy.\nTo enable efficient batch verification of input artifacts, Atlas groups related artifacts including training datasets, model weights, algorithm implementations, and evaluation results. By maintaining a cache of verified states and an incremental verification flow, the system avoids re-computing cryptographic operations for unchanged artifacts (R7). This particularly benefits ML pipelines where only small portions of the workflow change between iterations."}, {"title": "5. Implementation", "content": "Our proof-of-concept implementation integrates with Kubeflow [33] through standard APIs for metadata tracking and execution monitoring within ML pipelines. This integration approach enabled us to avoid significant modifications to our case study pipelines (\u00a76), while maintaining Atlas' security and transparency enhancements.\nThe Atlas attestation client is implemented as two components. First, the metadata sidecar (\u00a75) captures ML artifact and pipeline metadata in C2PA manifest format [10]. Due to current poor support for automated C2PA manifest generation and verification for ML models, we implemented Rust-based library in 2.3k LoC that captures cryptographic measurements, TDX attestation hashes, digital signatures, and temporal metadata. Second, the attestation client implementation leverages a dedicated TEE service (\u00a75, which interacts with Intel TDX [48], a virtualization-layer TEE, to provide the hardware-based security primitives for Atlas.\nTo implement the transparency log, our framework extends the Sigstore's Rekor [28] tamper-evident ledger. Building on Rekor's existing capabilities, we added support for storage and verification of Atlas C2PA-based model transformation attestations. The modified implementation validates C2PA signatures and attestation measurements during manifest submission, ensuring only properly signed and attested artifacts are stored in the log.\nFor ease of implementation, our prototype verification system (\u00a75.3) is integrated into the client's metadata sidecar.\nMetadata Sidecar. To integrate our C2PA tooling non-intrusively with ML pipelines, the Atlas attestation client uses a sidecar container architecture. This allows the client to monitor the pipeline for modifications, generate manifests and update relationships through an optimized storage layer. In addition to collecting C2PA manifest metadata, the sidecar generates the authenticated transformation attestations for model artifacts, and tracks artifact relationships by mapping datasets to checkpoints, checkpoints to models, and maintaining configuration bindings.\nThe sidecar container implementation monitors the pipeline for modifications, generating manifests that are first stored in a local cache layer before being committed to the transparency log. The local cache maintains an indexed hierarchy of manifests for efficient validation during pipeline execution, before the final storage in Rekor for tamper-evident provenance tracking. For storage optimization, our implementation decomposes manifests into constituent components. The assertion store, claim signatures, and metadata are stored separately, with relationships maintained through a reference system. This approach enables efficient updates to specific manifest components, reduced storage redundancy, optimized query performance, and scalable version tracking.\nTEE Service. To obtain the Intel TDX-based compute environment attestations that are included in the C2PA metadata, the sidecar interfaces with the TEE service. We implement a dedicated RESTful service that captures TEE state and validates runtime ML system component measurements, which are cryptographically anchored in hardware via Intel TDX. The TEE service implements the Confidential Containers (CoCo) Attestation format [52] for standardized claims used in TEE verification.\nWe optimize attestation operations through batching and caching mechanisms.The client coordinates with the verification system through standard Kubeflow interfaces, enabling seamless integration with the other Atlas compo-nents while preserving attestation integrity."}, {"title": "5.1. Kubeflow Integration", "content": "The integration with Kubeflow is achieved through custom operators and controllers that monitor pipeline execution through Kubeflow's Metadata V2 Beta API and KFP API. Through the //apis/v2betal/metadata endpoint, we track execution contexts and maintain verifiable records of pipeline runs.\nBy interfacing with/apis/v2betal/artifacts, we track model artifacts and their lineage. The metadata store provides structured information about component dependencies and data flow through the /apis/v2betal/connections endpoint. Our system correlates this information with integrity measurements and hardware attestations, creating verifiable records of pipeline execution states.\nThe metadata extraction leverages Kubeflow's event system through /apis/v2betal/events, enabling real-time capture of pipeline state transitions, component execution details, artifact generation events, and parameter updates. This structured approach enables verification of pipeline states while maintaining compatibility with existing workflows."}, {"title": "5.2. TEE Service Claims", "content": "For each attestation request, the client generates a JSON Web Token [53], which adds custom claims to represent Atlas-specific information including issuer identification and public key for signature verification, temporal validity through expiration and effective timestamps, hardware TEE information and software measurements, and golden values used in verification policy enforcement\nVerification Service Implementation. The metadata side-car also serves as a verification endpoint allowing pipeline components to validate artifact integrity against stored attestations. We optimize the performance of our staged verification system in three ways: 1) by processing changes incrementally and caching to avoid re-verifying unchanged components, 2) via batch processing of verification oper-ations, and 3) parallel verification paths for independent component classes.\nFor provenance chain validation, our implementation parallelizes verification of cross-component dependencies, version compatibility and evaluation result consistency. The system also maintains verification checkpoints that serve as trusted reference points, enabling partial verification from the last known good state instead of complete chain recomputation."}, {"title": "5.3. Verification Flow", "content": "Error handling focuses on computational efficiency through targeted cache invalidation rather than complete cache clearing. When verification failures occur, the system preserves verified states while ensuring security through selective invalidation. Preliminary testing shows these optimizations reduce verification time by up to 50% through parallelization and caching, while maintaining security guarantees."}, {"title": "5.4. Framework Adaptability", "content": "The BERT deployment validated our framework's flexibility across different ML environments. The abstraction layer successfully handled variations in framework-specific interfaces, from PyTorch's hook mechanisms to TensorFlow's Keras callbacks, while maintaining consistent security guarantees. Custom adapters enabled integration without modifying existing ML infrastructure, demonstrating the framework's ability to enhance security while preserving established workflows."}, {"title": "6. Evaluation", "content": "We validate our framework through a security analysis, preliminary performance testing and two case studies: BERT Meta [3], [4] and BGE reranker [5]."}, {"title": "6.1. Security Analysis", "content": "Atlas provides measures against the threats outlined in \u00a73.3 through multiple security mechanisms.\nFor MLaaS provider threats, the hardware-rooted TEEs in Atlas isolate sensitive computations and detect malicious insider tampering with executing ML pipelines. The attestation client continuously validates the runtime environment, generating ML system measurements that are cryptographically bound to model artifacts.\nAgainst hub threats, the Atlas verification system ensures artifact integrity through cryptographic measurements and signatures. The provenance chain maintains verifiable links between artifacts, preventing mismatched signatures or compromised dependencies from propagating.\nTo address artifact producer threats, Atlas relies on comprehensive provenance tracking that captures all dependencies and modifications. The transparency layer provides an immutable record of pipeline operations, enabling detection of undeclared dependencies or intentional omissions."}, {"title": "6.2. Preliminary Performance Analysis", "content": "Our implementation addresses the computational overhead of security mechanisms through several optimization strategies. Preliminary tests demonstrate that Atlas' security mechanisms introduce minimal overhead, with training overhead remaining under 8% across all test cases. The verification processes scale linearly with model size, while our caching strategies (see \u00a75) effectively optimize performance for iterative ML workflows; we observe a reduction of up to 50% of verification latency compared to the unoptimized implementation, and near-constant time for cached component verification.\nFor large-scale operations, parallel processing capabilities maintain performance at scale through: 1) concurrent verification operations when dependencies allow; 2) cache optimization to reduce repeated verification costs; 3) selective cache invalidation for error handling.\nWe plan to conduct more extensive performance benchmarking as part of future work."}, {"title": "6.3. Case Studies", "content": "BERT was selected for its complex architecture and widespread production use. Our implementation covers the complete lifecycle from pre-trained model fine-tuning through deployment, with isolated execution environments for attention computations and weight updates.\nTo validate Atlas' adaptability, we also implement security controls for BGE reranker on Intel Xeon processors. Atlas secures both instruction-based and layerwise reranking configurations, with training data structured as JSON records containing query-positive-negative text triplets. The provenance system tracks model adaptations across both base model variations (Gemma-2B and MiniCPM-2B), maintaining verifiable records of hyperparameters and training progressions.\nBoth implementations demonstrate performance characteristics consistent with our analysis in \u00a76.2, while preserving training efficiency and security guarantees."}, {"title": "7. Case Study: BERT Pipeline", "content": "Our proof-of-concept implementation demonstrates the framework's capabilities through securing a BERT model development and deployment pipeline. We chose BERT Meta [3], [4] for this case study due to its complex architecture, widespread use in production environments, and frequent fine-tuning requirements that test our framework's adaptability.\nThe implementation covers the complete model lifecycle from pre-trained model adaptation through deployment. For BERT's transformer architecture, we implemented isolated execution environments for attention computations and weight updates. The hardware-backed attestation system provides continuous runtime verification during fine-tuning operations, while our provenance system maintains verifiable records of all model evolution steps from the base model through task-specific adaptations.\nPerformance analysis shows practical viability with training overhead under 8%, primarily from manifest generation, and verification processes scaling linearly with model size. The system's parallel processing capabilities and cache optimization strategies maintained performance at scale."}, {"title": "8. Discussion and Conclusion", "content": "Our implementation and evaluation reveal several key considerations for practical deployment ML pipeline security. Organizations must balance security requirements with operational efficiency, considering factors like verification frequency, attestation depth, and computational overhead. Atlas' modular design enables customization of these tradeoffs while maintaining core security guarantees.\nCurrent hardware security architectures present integration challenges, particularly around GPU and accelerator attestation support. While Atlas provides secure boundaries"}]}