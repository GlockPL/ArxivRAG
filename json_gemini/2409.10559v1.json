{"title": "Unveiling Induction Heads: Provable Training Dynamics and Feature Learning in Transformers", "authors": ["Siyu Chen", "Heejune Sheen", "Tianhao Wang", "Zhuoran Yang"], "abstract": "In-context learning (ICL) is a cornerstone of large language model (LLM) functionality, yet its theoretical foundations remain elusive due to the complexity of transformer architectures. In particular, most existing work only theoretically explains how the attention mechanism facilitates ICL under certain data models. It remains unclear how the other building blocks of the transformer contribute to ICL. To address this question, we study how a two-attention-layer transformer is trained to perform ICL on n-gram Markov chain data, where each token in the Markov chain statistically depends on the previous n tokens. We analyze a sophisticated transformer model featuring relative positional embedding, multi-head softmax attention, and a feed-forward layer with normalization. We prove that the gradient flow with respect to a cross-entropy ICL loss converges to a limiting model that performs a generalized version of the \"induction head\" mechanism with a learned feature, resulting from the congruous contribution of all the building blocks. In the limiting model, the first attention layer acts as a copier, copying past tokens within a given window to each position, and the feed-forward network with normalization acts as a selector that generates a feature vector by only looking at informationally relevant parents from the window. Finally, the second attention layer is a classifier that compares these features with the feature at the output position, and uses the resulting similarity scores to generate the desired output. Our theory is further validated by experiments.", "sections": [{"title": "1 Introduction", "content": "In-context learning (ICL) (Brown et al., 2020) has emerged as a crucial aspect of large language model (LLM) (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; Anthropic, 2023; Team et al., 2023) functionality, enabling pre-trained LLMs to solve user-specified tasks during inference without updating model parameters. In ICL, a pre-trained LLM, typically a transformer, receives prompts containing a few demonstration examples sampled from a task-specific distribution and produces the desired output for that task. This capability is noteworthy because the tasks addressed during the ICL might not be part of the original training data set. The success of ICL requires the LLM to perform certain learning processes during inference.\nAlthough many previous works aim to demystify ICL from either empirical or theoretical perspectives, the theoretical foundations of ICL remain elusive. This is primarily due to the complexity of transformer architectures, which integrate token and position embeddings, multiple layers of multi-head softmax attention, layer normalization, and feedforward neural networks. When it comes to understanding how the ICL ability emerges in transformers after training, existing works often focus on simplified models, such as linear attention mechanisms or single-layer transformers (Von Oswald et al., 2023), and ICL tasks are typically confined to linear regression (Aky\u00fcrek et al., 2023). This leaves a gap in understanding how full-fledged transformer architectures facilitate ICL of more complex tasks, especially when latent causal structures exist among the tokens in a sequence.\nIn this paper, our aim is to narrow this gap by studying how a two-attention-layer trans- former is trained to perform ICL of an n-gram Markov chain model, where each token in the Markov chain statistically depends on the n tokens before it, known as the parent set. Specifically, we consider a transformer model with relative positional embedding (RPE) (He et al., 2020), multi-head softmax attention, and a feed-forward network (FFN) layer with normalization. We employ such a transformer model to predict the (L + 1)-th token of an n-gram Markov chain, with the first L tokens given as the prompt, where L + 1 is the sequence length. Here the L-token sequence is sampled from a random Markov chain model, where a random transition kernel obeying the n-gram Markov property is used to generate sequences. The token sequence is fed into the transformer model, which outputs a probability distribution over the vocabulary set to predict the (L + 1)-th token. To train the transformer model, we sample token sequences from these random Markov chain models and minimize the cross-entropy loss between the predicted token distribution and the true token distribution.\nUnder this setting, we aim to answer the following three questions: (i) Does the gradient flow with respect to the cross-entropy loss converge during training? (ii) If yes, how does the limiting model perform ICL? (iii) How do the building blocks of the transformer model contribute to ICL?\nMain Results. We provide an affirmative answer to the Question (i) by proving that the gradient flow converges during training. In particular, we identify three phases of training dynamics: in the first stage, FFN learns the potential parent set; in the second stage, each attention head of the first multi-head softmax attention layer learns to focus on a single parent token selected by FFN; and in the final stage, the parameter of the second attention layer increases, and the transformer approaches the limiting model. Moreover, for Questions (ii) and (iii), we show that the limiting model performs a specialized form of exponential kernel regression, dubbed \u201cgeneralized induction head\", which requires the congruous contribution of all the building blocks. Specifically, the first attention layer acts as a copier, copying past tokens within a given window to each position. The FFN layer acts as\""}, {"title": "1.1 Related Works", "content": "Our work adds to the rapidly growing literature on understanding ICL by transformers.\nIn Context Learning (ICL). Commercial Large Language Models (LLMs) such as ChatGPT (Brown et al., 2020), GPT-4 (Achiam et al., 2023), and Gemini (Team et al., 2023) typically operate in an autoregressive manner. These models exhibit remarkable ICL capabilities, without requiring further training. Previous research explores various aspects of the in-context learning (ICL) ability of these models. This includes their performance in zero-shot and few-shot learning scenarios (Honovich et al., 2022; Wei et al., 2021), the use of the chain of thought method to enhance reasoning (Wei et al., 2022; Zhou et al., 2022), and learning with multi-modalities (Alayrac et al., 2022). Moreover, recent research highlights the properties and advantages of using transformers beyond the traditional ICL setting, thereby broadening our understanding of their capabilities and applications (Edelman et al., 2022; Li et al., 2023; Jelassi et al., 2022; Sanford et al., 2023; Giannou et al., 2023; Liu et al., 2022; Tarzanagh et al., 2023a,b; Tian et al., 2023b,a; Song and Zhong, 2023; Deora et al., 2023; Chen and Li, 2024; Rajaraman et al., 2024b).\nThere is a large and growing body of literature on understanding how transformer architecture enables ICL. One strand of research proposes to understand ICL by casting it as a version of Bayesian inference expressed by the transformer architecture. See, e.g., Xie et al. (2021); Muller et al. (2021); Zhang et al. (2022, 2023b); Ahuja et al. (2023); Jeon et al. (2024); He et al. (2024) and the references therein. Another line of work investigates how transformers internally emulate specific algorithms to solve ICL tasks, where Aky\u00fcrek et al. (2023); Von Oswald et al. (2023); Fu et al. (2023); Ahn et al. (2023); Mahankali et al. (2023); Giannou et al. (2024); Wu et al. (2023) focus on learning with linear regression tasks and Bai et al. (2023); Cheng et al. (2023); Collins et al. (2024); Guo et al. (2023) investigate transformers' capabilities in learning with nonlinear functions. However, all of these works above focus on regression tasks where token (or token pairs) in the prompt sequences are i.i.d. or uncorrelated, which may not capture the more sophisticated data structures in real-world applications.\nIn addition, to study ICL with correlated data, there is also substantial interest in understanding how ICL operates over data drawn from Markov chains, providing insight into how transformer architectures contribute to ICL in these settings (Edelman et al., 2024; Makkuva et al., 2024b; Chen"}, {"title": "2 Problem Setup: In-Context Learning of Markov Chains", "content": "In this section, we present the details of the problem setting. In particular, we first introduce the statistical problem of ICL of n-gram Markov chains in \u00a72.1 and then lay out the details of the transformer model in \u00a72.2."}, {"title": "2.1 In-Context Learning and n-Gram Markov Chains", "content": "We study how autoregressive transformers are trained to perform in-context learning (ICL). A pre-trained transformer can be viewed as a conditional distribution \\( f_{tf}(\\cdot|prompt) \\) over a finite vocabulary set X, where prompt is a sequence of tokens in X. We consider an in-context unsupervised learning problem where the pre-trained transformer \\( f_{tf} \\) is used to predict the (L + 1)-th token \\( x_{L+1} \\) with the first L tokens being the prompt. Here L is a fixed number and the joint distribution of the sequence \\( x_{1:(L+1)} \\) is sampled from a random n-gram Markov chain. In other words, with \\( x_{1:(L+1)} \\) sampled from some distribution, we evaluate how well \\( f_{tf}(\\cdot|x_{1:L}) \\) predicts the distribution of \\( x_{L+1} \\).\nn-Gram Markov Chains. We assume the data comes from a mixture of n-gram Markov chain model, denoted by a tuple (X, pa, P, \\( \\mu_{0} \\)), where X is the state space and pa = (\\( -r_{1},..., -r_{n} \\)) is the parent set with positive integers \\( r_{1} < r_{2} < \\cdots < r_{n} \\). That is, for each l > \\( r_{n} \\), \\( x_{l} \\) only statistically depends on (\\( x_{l-r_{n}},...,x_{l-r_{1}} \\)), which is denoted by \\( x_{pa(l)} \\) and referred to as the parent tokens of \\( x_{l} \\). We let d = |X|"}, {"title": "Cross-Entropy Loss.", "content": "When \\( x_{1:(L+1)} \\) is generated, \\( x_{1:L} \\) is fed into the transformer \\( f_{tf} \\) to predict \\( x_{L+1} \\). To assess the performance of ICL, we adopt the population cross-entropy (CE) loss\n\n\n\n\n\\begin{equation}\\label{eq:2.1}\n\\mathcal{L}(f_{tf}) = -\\mathbb{E}_{\\pi\\sim P,x_{1:(L+1)}}[\\log(f_{tf}(x_{L+1}|x_{1:L}) + \\epsilon)],\n\\end{equation}\n\n\n\n\nwhere \\( \\epsilon > 0 \\) is a small constant introduced for numerical stability and in the sequel we will take \\( \\epsilon = O(L^{-1/2}) \\). Here, the expectation is taken with respect to the joint distribution of \\( x_{1:(L+1)} \\) (including the randomness of \\( \\pi \\sim P \\)). When setting \\( \\epsilon = 0 \\), we note that minimizing this cross-entropy loss is equivalent to minimizing the KL divergence\n\n\n\n\\begin{equation}\n\\mathbb{E}_{\\pi\\sim P,x_{1:L}} [KL(\\pi(\\cdot | x_{pa(L+1)}) || f_{tf}(\\cdot | x_{1:L}))].\n\\end{equation}\n\n\n\n\nAs a remark, we also relax a condition in Nichani et al. (2024) where the last token \\( x_{L} \\) has to be resampled from a uniform distribution. In addition, our analysis can also be extended to sequential CE loss, which corresponds to predicting every token in the sequence given the past rather than just the last token \\( x_{L+1} \\). This is closer to the training paradigm used in practice (Brown et al., 2020). See \u00a7B.3 for a further discussion on the sequential CE loss."}, {"title": "2.2 A Two-Layer Transformer Model", "content": "We consider a class of two-attention-layer transformer model, denoted by TF(M, H, d, D), which incorporates Relative Positional Embedding (RPE) (He et al., 2020), Multi-Head Attention (MHA) (Vaswani et al., 2017), and a Feed-Forward network (FFN) with normalization. Here M is an integer that specifies the window size of RPE, H is the number of heads in the first attention layer, d is the vocabulary size, and D is an integer that controls the complexity of FFN. The details of TF(M, H, d, D) are as follows.\nToken Embedding, Input and Output. Note that each token takes values in X with d = |X|. We embed the tokens into one-hot vectors in \\( \\mathbb{R}^{d} \\), and thus we can identify X as the canonical basis in \\( \\mathbb{R}^{d} \\), i.e., X = {\\( e_{1},...,e_{d} \\)}. A transformer model can be viewed as a mapping from \\( \\mathbb{R}^{(L+1)\\times d} \\) to \\( \\Delta(X) \\). In particular, given the input sequence \\( x_{1:L} \\), we denote X = (\\( x_{1},...,x_{L} \\)) \\( \\in \\) \\( \\mathbb{R}^{L\\times d} \\), and we append a zero vector 0 \\( \\in \\) \\( \\mathbb{R}^{d} \\) to the sequence, and define \\( \\hat{X} = (x_{1},...,x_{L},0)^{T} \\) \\( \\in \\) \\( \\mathbb{R}^{(L+1)\\times d} \\). The transformer takes \\( \\hat{X} \\) as input and outputs a probability distribution over X.\nRelative Positional Embedding. In each head of the first attention layer, we adopt RPE to incorporate positional information. Specifically, RPE is parameterized by a vector \\( w = (w_{-M},...,w_{-1}) \\) \\( \\in \\) \\( \\mathbb{R}^{M} \\), and it assigns a scalar value \\( W_{p}(i, j) \\) to a pair of positions (i, j) satisfying\n\n\n\n\n\\begin{align}\nW_{p}(i, j) &= w_{j-i} \\ \\text{ if } i-j \\in \\{1,...,M\\},\\\\\nW_{p}(i, j) &= -\\infty \\ \\text{ if } j > i \\text{ or } |j - i| > M.\n\\end{align}\n\n\n\n\nIn other words, as illustrated in Figure 3, the i-th token only attends to tokens with indices in {\\( i-1,...,i-M \\)}, referred to as the length-M window of the i-th token, and the trainable vector w determines the value of positional embedding. Here, we use -k to index the last k-th position."}, {"title": "3 Theoretical Results", "content": "In this section, we present the theoretical results. We first show in \u00a73.1 and \u00a73.2 that there exists a transformer in TF(M, H, d, D) that implements a generalized \u201cinduction head\" mechanism (Olsson et al., 2022) with a learned feature, which serves as a natural algorithm for learning n-gram Markov chains. Then in \u00a73.3 we prove that the gradient flow in (3.6) finds such a desired model asymptotically."}, {"title": "3.1 Generalized Induction Head Mechanism for Learning n-Gram Markov Chains", "content": "Recall that we define the mixture of n-gram Markov chain model (X, pa, P, \\( \\mu_{0} \\)) in \u00a72.1, where P is a distribution over the Markov transition kernels. For regularity, we assume existence of a unique stationary distribution for any \\( \\pi \\in supp(P) \\), where a rigorous statement is deferred to Assumption 3.5. We also assume the window size M > \\( r_{n} \\). For any n-gram Markov chain with transition kernel \\( \\pi \\sim P \\), we let \\( \\mu^{\\pi} \\) \\( \\in \\Delta(X^{M+1}) \\) denote the stationary distribution of the Markov chain over a window of size M + 1. Here we use {\\( z_{l} \\)}_{l>1} to denote a random sequence of tokens generated by the Markov chain. Then \\( \\mu^{\\pi} \\) denotes the joint distribution of a block of M + 1 tokens (\\( z_{l-M},...,z_{l-1},z_{l} \\)) under the stationary distribution of \\( \\pi \\), where l > M is an integer.\nIn the following, we introduce a generalized induction head (GIH) estimator for the task of predicting \\( x_{L+1} \\) given \\( x_{1:L} \\), which is based on the following simple idea: \\( x_{L+1} \\) should be similar to a previous token \\( x_{l} \\) if their parents are similar. As the parent set pa is unknown, GIH adopts an information-theoretic criterion to select a subset of previous tokens as a proxy of the parents. Specifically, GIH uses a modified version of \\( \\chi^{2} \\)-mutual information, which is defined as follows.\nDefinition 3.1 (Modified \\( \\chi^{2} \\)-Mutual Information). We take a length-(M+1) windows (\\( z_{l-M},...,z_{l-1},z_{l} \\)) for some l > M and suppose the sequence is sampled from stationary distribution \\( \\mu^{\\pi} \\) with \\( \\pi \\sim P \\). Let Z = (\\( z_{l-M},...,z_{l-1} \\)). For any subset S \\( \\subseteq \\) [M], we use \\( Z_{-S} \\) to denote the subvector of Z containing entries of the form \\( z_{l-s} \\), \\( \\forall s \\in S \\). For instance, suppose S = {2,5}, then \\( Z_{-S} = (Z_{l-5}, Z_{l-2}) \\). The modified \\( \\chi^{2} \\)-mutual information for S is defined as\n\n\n\n\\begin{equation}\\label{eq:3.1}\n\\hat{I}_{\\chi^{2}}(S) = \\mathbb{E}_{\\pi\\sim P,(z,Z)\\sim \\mu^{\\pi}} \\bigg(\\sum_{e \\in \\mathcal{X}} \\bigg[\\frac{\\mu^{\\pi}(z = e | Z_{-S})}{\\mu^{\\pi} (z = e)}\\bigg]^{2} - 1\\bigg) \\cdot \\mu^{\\pi}(Z_{-S})\\bigg].\n\\end{equation}\n\n\n\n\nwhere \\( \\mu^{\\pi}(z = \\cdot | Z_{-S}) \\) is the conditional distribution of z induced by \\( \\mu^{\\pi} \\) given the partial history \\( Z_{-S} \\), and \\( \\mu^{\\pi}(Z_{-S}), \\mu^{\\pi}(z) \\) are the marginal distributions of \\( Z_{-S} \\) and z under (z, Z) \\( \\sim \\) \\( \\mu^{\\pi} \\).\nIntuitively, \\( \\hat{I}_{\\chi^{2}}(S) \\) is modified from the vanilla \\( \\chi^{2} \\)-mutual information (\\( \\chi^{2} \\)-MI) between two random variables (Polyanskiy and Wu, 2024) and quantifies how much information the partial history \\( Z_{-S} \\) contains about z. In particular, we incorporate an additional \\( \\mu^{\\pi}(Z_{-S}) \\) term that decreases with the growing size of S. To see the rationality, we first introduce a GIH estimator based on the modified \\( \\chi^{2} \\)-mutual information.\nDefinition 3.2 (Generalized Induction Head). A GIH estimator with window size M \\( \\in \\) \\( \\mathbb{N} \\), feature size D \\( \\in \\) \\( \\mathbb{N} \\) is denoted by GIH(\u00b7; M, D), which maps \\( x_{1:L} \\) to a distribution over X. We let \\( S^* \\) be the information-optimal subset (referred to as the \u201cinformation set\u201d in the sequel) of [M] with size no more than D that maximizes the modified \\( \\chi^{2} \\)-mutual information \\( I_{\\chi^{2}}(S) \\) defined in (3.1). That is, we define the information set \\( S^* \\) as\n\n\n\n\\begin{equation}\\label{eq:3.2}\nS^* = \\underset{S\\in[M]_{\\leq D}}{\\operatorname{argmax}} \\hat{I}_{\\chi^{2}} (S).\n\\end{equation}\n\n\n\n\nThen GIH(\\( x_{1:L} \\); M, D) outputs\n\n\n\n\\begin{equation}\\label{eq:3.3}\ny^* := \\begin{cases}\n(L - M)^{-1} \\sum_{l=M+1}^{L} x_l \\cdot \\mathbb{1}(X_{l-S^*} = x_{L+1-S^*}), & \\text{ if } N \\geq 1, \\\\\n\\frac{1}{L} \\sum_{l=M+1}^{L} x_l, & \\text{ otherwise.}\n\\end{cases}\n\\end{equation}\n\n\n\n\nHere, we define \\( X_{l-S^*} \\) as the set {\\( x_{l-s} \\): \\( s \\in S^* \\)} and \\( N = \\sum_{l=M+1}^{L}\\mathbb{1}(X_{l-S^*} = x_{L+1-S^*}) \\).\nWith a slight abuse of notation, we also call \\( X_{l-S^*} := (x_{l-s}: s \\in S^*) \\) the information set of the l-th token \\( x_{l} \\)."}, {"title": "3.2 How Does Transformer Implement the GIH Mechanism?", "content": "In the following, we briefly illustrate how a two-attention-layer transformer model as introduced in (2.5) implements the GIH mechanism. As we will show in \u00a73.3, gradient flow with respect to the cross-entropy loss converges to this transformer in the limit.\nStep I: The First Attention Layer Copies the Information Set \\( S^* \\) to the Current Position. Suppose the number of heads is equal to the window size for simplicity, i.e., H = M. Then, attention head h \\( \\in \\) \\( S^* \\) can attend to the h-th parent token by setting the RPE weights in the softmax function to be \\( w^{(h)} = p e_{-h} \\) for a sufficiently large p, where \\( e_{-h} \\in \\mathbb{R}^{M} \\) is the canonical basis vector with the (M + 1 - h)-th entry being one and all other entries being zero. As a result, each \\( v^{(h)} \\) for h \\( \\in \\) \\( S^* \\) satisfies \\( v^{(h)} = x_{l-h} \\).\nStep II: FFN Generates the Polynomial Features of the Information Set \\( S^* \\). As we have introduced in (2.3), each learnable \\( c_{S} \\) in the FFN layer determines the contribution of the corresponding subset S to the output feature. To let the optimal information set \\( S^* \\) dominate the output, we set \\( c_{S^*} = 1 \\) whereas \\( c_{S} = 0 \\) for all \\( S \\) \\( \\neq \\) \\( S^* \\). The exact form of the output of the FFN"}, {"title": "3.3 Convergence Guarantee of Gradient Flow", "content": "In the following, we present the convergence guarantee for gradient flow. To simplify the discussion, we consider the case where H = M, meaning there are enough heads to implement the GIH mechanism by having each head copy a unique parent token from a window of size M. Let us first introduce the paradigm of training by gradient flow.\nTraining Paradigm. Consider training a transformer TF(M, H, d, D) in (2.5) with M = H to perform ICL on the n-gram Markov chain model introduced in \u00a72.1. Specifically, we define \\( \\mathcal{L}(\\Theta) \\) as the population cross-entropy loss in (2.1), where the transformer model \\( f_{tf} \\) is given by (2.5) with a parameter \\( \\Theta \\). Ideally, when training the parameter \\( \\Theta \\) with gradient flow, the dynamics with respect to the loss \\( \\mathcal{L}(\\Theta) \\) is given by:\n\n\n\n\\begin{equation}\\label{eq:3.6}\n\\frac{d \\Theta(t)}{dt} = -\\nabla \\mathcal{L}(\\Theta(t)).\n\\end{equation}\n\n\n\n\nWe consider a three-stage training paradigm where, in each stage, only a specific subset of the weights is trained by gradient flow. The three stages are outlined in Table 1. Specifically, in the first"}, {"title": "3.4 Further Discussions on the GIH Mechanism", "content": "We conclude this section with further discussions on the modified \\( \\chi^{2} \\)-mutual information and low-degree polynomial kernel for the FFN within the GIH mechanism.\nOn the Modified \\( \\chi^{2} \\)-Mutual Information. Now that we have shown how gradient flow approaches the desired GIH model, it is natural to ask the following questions: What is the optimal subset \\( S^* \\) that the model selects? How well does the model perform? For the purpose of illustration, let us consider a symmetric case where the stationary distribution \\( \\mu^{\\pi} \\) over a length-\\( r_{n} \\) window is uniform over \\( \\mathcal{X}^{n} \\). One can verify that in this case, the stationary distribution over a window of any other length is uniform as well, and the modified mutual information can be simplified into\n\n\n\n\n\\begin{equation}\n\\log \\hat{I}_{\\chi^{2}} (S) = \\log I_{\\chi^{2}} (S) - |S| \\log d,\n\\end{equation}\n\n\n\n\nwhere \\( I_{\\chi^{2}}(S) \\) is the standard \\( \\chi^{2} \\) mutual information between \\( \\mu^{\\pi}(z | Z_{-S}) \\) and \\( \\mu^{\\pi}(z) \\), and the second term |S| \\( \\log \\)d serves as a penalty on the model complexity. Thus, the GIH mechanism is reaching a balance between the model complexity and the information richness. Below we characterize two scenarios where the model will select the exact parent set, i.e., \\( S^* = pa \\).\n1. If n = 1, i.e., each token only has one parent, then \\( S^* = pa \\). This is because \\( S^* \\) simultaneously maximizes both terms in (3.9), thus reproducing the results in Nichani et al. (2024).\n2. If n is known a priori and restricting the polynomial kernel to \\( S \\in [H]_{=n} = \\{S \\in [H] : |S| = n\\} \\) for the FFN layer, then \\( S^* = pa \\). Here, the penalty term does not influence the selection and the exact parent set maximizes the mutual information by the data-processing inequality.\nIn the general case, however, the model could be much more flexible, and it is possible that the model selects only a subset of the true parent set or even some non-parent tokens that are also informative. The rationale is that with a more complex model, e.g., selecting a large S, the model"}, {"title": "4 Proof Sketch", "content": "In this section, we discuss the main ingredients of analysis of gradient flow. First, we show in \u00a74.1 how to simplify the model based on our choice of the initialization and the structure of the disentangled"}, {"title": "5 Experiments", "content": "In this section, we first detail the setup for the experiment in Figure 6, and then provide additional results for training a model that also incorporates the word embedding matrices WQ, WK, WV and the output embedding matrix Wo in the first attention layer. Let us first detail the data setup that is used for all the experiments in this work.\nData generation. The dataset for the ICL task is generated as n-gram Markov chains as described in \u00a72.1. We take pa = {-1, -2} as the parent set. Thus, the number of parents is n = 2 and the token embedding dimension is d = 3. Note that for each sequence, the transition matrix \u03c0(\u00b7|xpa) is of shape d x dn. We assign a prior distribution P for the transition matrix, which is defined such that each column of the transition matrix of kernel is independently drawn from a symmetric Dirichlet distribution with parameter \u03b1 = 0.01, i.e., \u03c0(\u00b7|xpa) ~ Dir(\u03b1 \u00b7 1d). Note that each chain has different transition kernel \u03c0 but follows the same prior distribution P. We randomly sample 10,000 Markov chains with L = 100 from the prior distribution P; 9,000 are used for training and 1,000 for validation."}, {"title": "5.1 Training with Stage Splitting", "content": "we present the simulation results with model TF(M, H, d, D) in (2.5) and training in the three-stage manner. We configure the model with window size M = 3, number of heads H = 3, vocabulary size d = 3 and maximal FFN degree D = 2."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we have studied the training dynamics of a two-attention-layer transformer model for learning n-gram Markov chains in an in-context way. Our theoretical analysis underscores a congruous interplay between the multihead attention mechanism, the feed-forward network, and layer normalization that yields a generalized version of the induction head mechanism during the training. In particular, we prove that the generalized induction head mechanism adopts a modified \\( \\chi^{2} \\)-mutual information criterion for parent selection that strikes a balance between information richness and model complexity. To our best knowledge, our work gives the first theoretical evidence for learning an induction head mechanism with n-gram Markov data, which potentially sheds light on the inner workings of large-scale transformer models.\nOur work opens new directions for developing a rigorous understanding of the transformer models. A natural direction would be that if one can find such a mechanism with standard FFN layer using multi-layer perceptron and standard layer normalization in the more practical transformer model. The intuition is that our FFN layer in (2.3), which is further instantiated in (B.1), lies in the space of low-degree polynomials and can be well represented by a MLP with sufficient dimensions and proper activation functions. Initial attempts to learn nonlinear features have also been made by Kim and Suzuki (2024). Another direction is to investigate the training dynamics beyond a single loop of this induction head mechanism, e.g., iteration head with recursively refined predictions (Cabannes et al., 2024), and how the induction head mechanism occurs in multi-layer transformer models."}, {"title": "7 Acknowledgement", "content": "We acknowledge Shaobo Wang for his help with the experiments. We also thank Jason D. Lee, Alex Damian, and Eshaan Nichani for their helpful discussions. Zhuoran Yang acknowledges the support of NSF under the award DMS-2413243."}, {"title": "A Additional Experiments", "content": "In this appendix, we present the auxiliary lemmas used to derive the approximation of the gradient flow dynamics in the proof of Theorem 3.6, which is presented in the previous appendix. The proofs of these lemmas are presented right below their statements."}, {"title": "A.1 Training without Stage Splitting", "content": "Previously in \u00a75, we show the simulation results on the simplified model (2.5). Now we present the results of additional experiments based on the full model defined as follows.\nFirst Attention: \\(\n\\sigma_{12L+1}=\\text{softmax}((X W_Q)(XW_K)^T+\\mathcal{W})"}, {"title": "B Additional Background and Discussions", "content": "In this appendix, we present the auxiliary lemmas used to derive the approximation of the gradient flow dynamics in the proof of Theorem 3.6, which is presented in the previous appendix. The proofs of these lemmas are presented right below their statements."}, {"title": "B.1 Feed-Forward Network for Polynomial Kernel", "content": "Lemma B.1. Recall the FFN satisfying (2.3), which maps a vector \\( z \\in \\mathbb{R}^{dH} \\) to a vector in \\( \\mathbb{R}^{d_e} \\). We write z as \\( (z^{(1)},...,z^{(H)}) \\) where \\( z^{(h)} \\in \\mathbb{R}^{d} \\) for all h \\( \\in \\) [H]. Let \\( z_{i}^{(h)} \\) be the i-th entry of \\( z^{(h)} \\). Then we can explicitly construct \\( \\phi(\\cdot) \\) by letting"}, {"title": "B.2 Perron-Frobenius Theorem", "content": "Next, we review the basics for the celebrated Perron-Frobenius theorem on non-negative matrices (Meyer, 2023, Chapter 7). We consider the following class of irreducible matrices."}, {"title": "B.3 Sequential CE Loss", "content": "In this work, we only consider the prediction"}]}