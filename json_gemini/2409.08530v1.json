{"title": "Integration of Mamba and Transformer \u2013 MAT for Long-Short Range Time Series Forecasting with Application to Weath Dynamics", "authors": ["Wenqing Zhang", "Junming Huang", "Ruotong Wang", "Changsong Wei", "Wenqian Huang", "Yuxin Qiao"], "abstract": "Long-short range time series forecasting is essential for predicting future trends and patterns over extended periods. While deep learning models such as Transformers have made significant strides in advancing time series forecasting, they often encounter difficulties in capturing long-term dependencies and effectively managing sparse semantic features. The state space model, Mamba, addresses these issues through its adept handling of selective input and parallel computing, striking a balance between computational efficiency and prediction accuracy. This article examines the advantages and disadvantages of both Mamba and Transformer models, and introduces a combined approach, MAT, which leverages the strengths of each model to capture unique long-short range dependencies and inherent evolutionary patterns in multivariate time series. Specifically, MAT harnesses the long-range dependency capabilities of Mamba and the short-range characteristics of Transformers. Experimental results on benchmark weather datasets demonstrate that MAT outperforms existing comparable methods in terms of prediction accuracy, scalability, and memory efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "Long-short range time series forecasting (LSRTSF) has demonstrated indispensable value across various modern domains, including demand prediction, signal processing and detection, and regulation of critical assets such as renewable energy, greenhouse farming, and advanced manufacturing facilities [1], [2]. Although several practical techniques for LSRTSF have been proposed in recent years, most of these techniques primarily focus on a single aspect. These aspects include handling long-term dependencies in complex scenarios, achieving linear scalability in model design, or enhancing computational efficiency and integration with edge computing. However, these techniques still face numerous challenges posed by the big data era and the need for better long-term forecasting capabilities.\nEffectively capturing long-term dependencies in multivariate time-series (MTS) data is paramount for accurate and robust forecasting. Linear models like DLinear and TiDE have demonstrated the capability to achieve performance comparable to that of Transformer-based models while maintaining low complexity and excellent scalability [3], [4]. These models primarily employ multilayer perceptrons and linear projection techniques, which, although efficient, may sometimes fall short in capturing subtle long-range correlations inherent in the data. On the other hand, Transformer-based models, including iTransformer, PatchTST, and Crossformer, excel in uncovering long-distance dependencies due to their sophisticated self-attention mechanisms. This capability significantly enhances the accuracy of LSRTSF. Nevertheless, despite their advanced features and superior performance, Transformer-based models often grapple with challenges related to scalability and practicality, especially in edge computing environments, due to their high computational complexity [5], [6]. This trade-off between accuracy and computational efficiency remains a critical consideration in the development and deployment of MTS forecasting models [7].\nRecently, state-space models (SSMs) have garnered significant attention due to their exceptional performance in sequence data inference [8]. These models handle very long sequences with linear complexity and demonstrate context-aware selectivity through hidden attention mechanisms [8]. While SSMs have shown potential in various domains such as genomics, table learning, graph data, and imagery, their application to LSRTSF remains underexplored [9]. The primary reasons for this are the recent development of SSM techniques with strong content and context selectivity and the ongoing challenge of efficiently representing contextual information in time series data [10]. Transformer-based models like Autoformer and Informer treat each time point as a separate unit, while newer models such as PatchTST and iTransformer process segments of the time series [9].\nMTS data typically comprises multiple channels, each representing a different variable. Models like Informer, FEDformer, and Autoformer utilize a mixed-channel approach, treating the input as a two-dimensional matrix defined by the number of channels and the length of histories [11]. This approach is beneficial when channels exhibit significant correlations, as it allows for the capture of these dependencies effectively. The temporal relationships inherent in time series data, which tend to be preserved even after downsampling, have been explored in models like SciNet but remain underutilized in many other approaches [12]. Due to the high redundancy in MTS data, relying solely on time points as markers can obscure context-based choices and overlook long-range dependencies. Instead, utilizing data snippets within a time window can provide richer contextual clues. To optimally capture long-range dependencies, it is crucial to provide context at multiple scales. This can be achieved by automatically generating global-level markers as context, similar to the approach used by iTransformer, which models the entire recall window to enhance understanding and prediction accuracy [13].\nThis study introduces a novel method designed to efficiently capture long- and short-range dependencies in time-series data by providing multi-scale contexts, with a particular focus on enhancing local contexts. The approach employs a selectively scanning SSM known as Mamba, coupled with attention mechanisms called Transformer to serve as the core inference engine, termed MAT. This architecture enables the capture of both long-term forecasting capabilities and short-range dependencies in MTS data while maintaining linear scalability and minimal memory usage. The model leverages the distinct characteristics of time-series data by employing a bottom-up strategy, which involves generating contextual cues at two distinct scales through linear mapping for resolution reduction or downsampling. The high-resolution level functions at the first scale, whereas the low-resolution level operates at the second scale. At each of these levels, the model utilizes four MAT modules: one for collecting local context clues and another for gathering both global context clues, which are then fused.\nThe main contribution of the proposed algorithm could be concluded as:\n\nThis research introduces an innovative model, MAT, representing a pioneering step in capturing both long-term and short-range dependencies in multivariate time-series data through exclusive use of MAT modules for context-aware prediction. MAT showcases linear scalability and a minimal memory footprint, delivering performance that is either superior or comparable to existing linear models.\nThe architecture of MAT is uniquely designed to uniformly handle variable sequence lengths and extract the long-short range relationships within the time series. It leverages inter-channel correlations using four MAT modules, integrating with Transformer and Mamba. Furthermore, MAT efficiently selects predictive content by targeting global and local contextual information at various scales of MTS data, thereby optimizing its ability to extract meaningful long-short range patterns from complex observations."}, {"title": "II. RELATED WORK", "content": "Various methods for the ubiquitous LSRTSF problem have been developed, falling into three primary categories regarding this article: Self-supervised methods, Transformer, and Mamba.\nSelf-Supervised Methods: Traditional solutions rely on statistical properties and patterns in data for prediction. Recently, there has been a shift towards deep learning approaches, which employ neural networks to capture complex dependencies and improve performance. These methods are categorized into variable-mixing and variable-independent approaches. The first branch, including RNNs, LSTM, and GRU, model dependencies across time and variables, despite challenges in optimization efficiency [14]\u2013[16]. Variable-independent methods assume variable independence, offering simplicity and efficiency but potentially oversimplifying the problem [17].\nTransformer: Transformer-based models have become widely adopted in LSRTSF for their remarkable accuracy [18]. Examples include iTransformer, PatchTST, Crossformer, FEDformer, Stationary, and Autoformer [19]. These models transform time series data into token sequences and utilize a self-attention mechanism to extract dependencies across various time steps, making them particularly adept at capturing complex temporal relationships. Additionally, Transformers enjoy the ability to process data in parallel in order to enhance their effectiveness in identifying long-term dependencies, sometimes with linear scalability. Despite their advantages, these models often struggle with quadratic time and memory complexity due to the nature of self-attention mechanisms [11].\nState Space Models: Recently, some studies have integrated SSMs with deep learning, showcasing substantial potential in addressing long-term dependency issues [8]. However, their computational and memory demands often hinder practical applications [20]. Efficient variants of SSMs, such as S4, H3, Gated State Space, and RWKV [21], aim to improve performance and efficiency in diverse tasks. Mamba [22] addresses a major limitation of traditional SSMs by introducing an S4-based data-dependent selection technique, effectively filtering specific inputs and capturing long-term context as the sequence length extends. Mamba achieves linear time efficiency in modeling long sequences and outperforms Transformer models in benchmark tests [22]. Mamba has been effectively employed across various domains, including audio signals, textual data, sensor readings, and genomics sequences [9]. This extensive application has greatly enhanced its capability to detect intricate patterns and long range dependencies in diverse types of data. In addition, a notable research direction involves integrating the Transformer and Mamba for language modeling [23]. Comparative studies [23] demonstrate that the Mambaformer is highly effective for in-context learning tasks. Jamba [10], the first production-grade attention-SSM hybrid model, boasts 12 billion active and 52 billion total available parameters, exhibiting excellent performance with long-context tasks."}, {"title": "III. METHODOLOGY", "content": "A. Preliminary\nIn this section, we provide a comprehensive overview of each component of our proposed architecture and elucidate how our model tackles the LSRTSF problem. Consider a collection of MTS samples, denoted as dataset D. Each sample consists of an input sequence x = [x1,...,xL], where each xt \u2208 RM represents a vector of M measurements at time point t. The sequence length L, also referred to as the look-back window, serves as the basis for predicting T future values, denoted by [xL+1,...,xL+T].\nB. Transformer\nAt the core of the proposed algorithm, as well as other transformer-based approaches, lies the concept of attention, which enables the models to concentrate on significant elements within a context, which has been visualized in Fgiure.1. One example is multi-head attention, which transforms a sequence of queries Q \u2208 Rlq\u00d7d of length lq into a sequence of outputs O = [o1,\u2026\u2026\u2026,oH] \u2208 Rlqxd of the same size by attending to lk key-value pairs K\u2208 Rlk\u00d7d, V \u2208 Rlk\u00d7d.\nOh = Attention(Qh, Kh, Vh) = Softmax( QhKhT\u221adk )Vh, (1)\nwhere Qh = QWh, Kh = KWh, V = VWh are the projected queries, keys, and values for head h\u2208 [1, H] with learning parameters Wh, Wk, and Wv, respectively. When Q = K = V, this attention mechanism is referred to as self-attention.\nGiven fully observed input sequences, the mapping can be computed efficiently without the sequential order constraints typically imposed by recurrent neural networks. Crucially, the mechanism inherently includes direct connections between distant time steps, allowing information from previous time steps to be accessed without compression into a fixed representation. This simplifies the optimization and learning of long-term dependencies [18], [24].\nWithout recurrence, the Transformer model [18] encodes information about each time step t using predefined sinusoidal positional embeddings Position(t) = [pt(1), ..., pt(d)] \u2208 Rd, where the i-th embedding is defined as pt(i) = sin(t\u00b7 ci/d) for even i and pt(i) = cos(t\u00b7 ci/d) for odd i, with e being a large constant.\nC. Mamba\nSSMs are commonly conceptualized as linear time-invariant (LTI) systems that transform continuous input signals u(t) into corresponding output signals y(t) via a system state representation x(t). This state space framework, which has been presented as Figure.2, delineates the temporal evolution of the state, which can be described by the following set of ordinary differential equations:\ndx(t)\n dt = Ax(t) + Bu(t)\ny(t) = Cx(t) + Du(t)\n(2)\nwhere x(t) = dx(t)dt, and A, B, C, and D are the parameters of the time-invariant SSMs.\nDiscretization: Solving SSMs analytically is exceedingly difficult due to their continuous nature. To address this, discretization techniques are employed to approximate the continuous-time SSM into a discrete-time counterpart. This involves sampling the input signals at fixed intervals, yielding their discrete-time equivalents. The resultant discrete-time SSM is expressed as:\nxk = Axk-1+ Buk\nYk = Cxk + Duk\n(3)\nwhere xk denotes the state vector at discrete time step k, and uk denotes the input vector at the same step. The matrices A and B in the discrete domain are derived from their continuous-time counterparts using discretization techniques like the Euler method or the Zero-Order Hold (ZOH) method. Specifically, A = exp(\u2206A) and B = (\u0394\u0391)-1(exp(\u0394\u0391) \u2212 I)AB, A is the sampling time interval.\nSelective Scan Mechanism: Mamba enhances traditional SSMs by incorporating a selective mechanism, allowing parameters to modulate interactions along the sequence contextually, which has been demonstrated by Figure.4. This selective approach enables Mamba to filter out irrelevant noise in time series tasks and selectively retain or discard information pertinent to the current input. Unlike previous SSM methodologies with static parameters, this approach deviates from the LTI characteristics. Consequently, Mamba employs a hardware optimization strategy and implements parallel scan training to effectively address this challenge.\nD. MAT\nThe framework of the proposed model, named MAT, is illustrated in Figure. 3. The core components of this algorithm are four combined Mambas and Transformer, which are employed to extract long-short range contextual information. It is noted that this configuration leverages both the long-term forecasting capability of Mamba and short range dependency of Transformer.\nBefore streaming the collected datasets into our procedure, the original MTS is normalized into x = [x1,\u2026\u2026,xL] \u2208 RM\u00d7L using x = Normalize(x). This normalization operation is processed by the reversible instance normalization (RevIN), which is widely utilized in MTS-related problems [25], [26]. Furthermore, the identical normalization process, e.g. RevIN, has been implemented in the data-preprocessing module of the baseline algorithm in the experiments.\nTo reduce the dimensionality of the original data and accelerate subsequent inference of MAT, a two-stage embedding representation technique of the input is employed as:\nx(1) = EMB1(x(0)), x(2) = EMB2(x(1))\n(4)\nwhere the embedding functions EMB1 : RM\u00d7L \u2192 RM\u00d7n1 and EMB2 : RM\u00d7n1 \u2192 RM\u00d7n2 are implemented through multi-layer perceptrons (MLPs). In addition, the dropout operation is introduced in the calculation of the embedding function E2 to mitigate overfitting phenomenon. To deal with the variable input sequence length L, a fixed-length parameters, which are n\u2081 and n2, are chosen from the set {512, 256, 128, 64, 32} such that n\u2081 > n2.\nTo be specific, the proposed algorithm has a similar procedure with the Mamba-based method in [27], there exists a main objective-level difference, the MAT approach in this paper aims at not only the long-term prediction capability of the prevalent Mamba methods, but also the short-range dependency learned from the Transformer module, which has been proved to be a pivotal component in the modern large language model [28]-[30]. This enhanced phenomenon has been observed and analyzed in the pioneering work in [31], where the reader could refer to more details. To be specific, the MTS problem could be decomposed into two main branches, one is the long term and another is short range, Mamba uses the long-proved SSMs to overcome the long-term prediction dilemma and Transformer uses the core attention mechanism to strengthen the short range dependent relationship. The proposed MAT enjoys the long-short prediction capability of them and outperforms many existing algorithms in this area."}, {"title": "IV. EXPERIMENTS", "content": "This section showcases the primary outcomes of our experiments using well-established weather benchmark datasets for MTS forecasting. Furthermore, we perform a comprehensive analysis to highlight the contribution of the proposed methodology.\nA. Dataset\nThe weather datasets, https://www.bgc-jena.mpg.de/wetter/, recorded every 10 minutes throughout the entire year of 2020, encompass 21 meteorological indicators, including air temperature and humidity. Provided by the Max Planck Institute for Biogeochemistry, this dataset includes detailed, long-term time series data on temperature, precipitation, and real-time weather conditions specifically for Jena, Germany. It features comprehensive statistical analyses, information on sun and moon phases, and integrates data from external weather stations and webcams. For a fair comparison, the results verified by the work [27] are used in this paper.\nB. Setting\nAll experiments were performed using the PyTorch framework on one NVIDIA V100 GPU with 32GB of memory. The model optimization was carried out using the ADAM algorithm, with L2 loss as the objective function and the learning rate \u03b7 = 0.0001. The batch size was adjusted according to the prediction requirement, and the training duration was set to 100 epochs. Prediction accuracy was evaluated using mean square error (MSE) and mean absolute error (MAE) metrics, with lower values signifying higher accuracy.\nC. Evaluation\nTable.I presents the results of MAT on supervised long-short range forecasting tasks. Adhering to the widely adopted settings, all baselines, including the MAT in this article, were fixed with L = 96 and T = {96, 192, 336, 720}. Additionally, default parameters for all Mambas were set as follows: Dimension factor D = 256, local convolutional width = 2, and state expand factor N = 1, for the transformer module, the multi head number is set H = 8, the Batch Size in the training process is set as Batch = 32.\nThe results in Table.I indicate that the proposed MAT outperforms many published baselines across weather datasets. Notably, Crossformer achieves superior results on these weather datasets, which have numerous channels and complicated neural network construction. Our method, however, shows comparable or superior performance on weather datasets, significantly surpassing the existing baselines. This underscores the effectiveness of the developed MAT in dealing with LSRTSF tasks across various scales and complexity."}, {"title": "D. Analysis", "content": "Table.I demonstrates the effectiveness of MAT using MAE and MSE metrics on the weather datasets. It is evident that MAT accurately captures real-world trends in the predicted future time horizon for the testing samples. Specifically, on the weather datasets, MAT shows superior performance compared to published prediction algorithms. However, it is notable that MAT was outperformed by Crossformer in shorter sequence lengths. A potential explanation for this behavior could be the conflict of interest between the long-term prediction capabilities of Mamba and the short-range dependency strengths of Transformer. Future research could focus on seamlessly integrating Mamba and Transformer to address this issue."}, {"title": "V. CONCLUSION", "content": "This study addresses the challenges in long-short range time series forecasting by comparing Transformers and the SSM, Mamba. While Transformers struggle with long-term dependencies and sparse semantic features, Mamba excels through selective input handling and parallel computing. We introduced MAT, a combined approach leveraging Mamba's long-range capabilities and Transformers' short-range strengths. Experiments on benchmark weather datasets demonstrate that MAT outperforms existing methods in prediction accuracy, scalability, and memory efficiency, making it a promising solution for forecasting in multivariate time series."}]}