{"title": "FAIRDD: FAIR DATASET DISTILLATION VIA SYNCHRONIZED MATCHING", "authors": ["Qihang Zhou", "Shenhao Fang", "Shibo He", "Wenchao Meng", "Jiming Chen"], "abstract": "Condensing large datasets into smaller synthetic counterparts has demonstrated its promise for image classification. However, previous research has overlooked a crucial concern in image recognition: ensuring that models trained on condensed datasets are unbiased towards protected attributes (PA), such as gender and race. Our investigation reveals that dataset distillation (DD) fails to alleviate the un-fairness towards minority groups within original datasets. Moreover, this bias typically worsens in the condensed datasets due to their smaller size. To bridge the research gap, we propose a novel fair dataset distillation (FDD) framework, namely FairDD, which can be seamlessly applied to diverse matching-based DD approaches, requiring no modifications to their original architectures. The key innovation of FairDD lies in synchronously matching synthetic datasets to PA-wise groups of original datasets, rather than indiscriminate alignment to the whole distri-butions in vanilla DDs, dominated by majority groups. This synchronized matching allows synthetic datasets to avoid collapsing into majority groups and bootstrap their balanced generation to all PA groups. Consequently, FairDD could effectively regularize vanilla DDs to favor biased generation toward minority groups while maintaining the accuracy of target attributes. Theoretical analyses and extensive experimental evaluations demonstrate that FairDD significantly improves fairness compared to vanilla DD methods, without sacrificing classification accuracy. Its consistent superiority across diverse DDs, spanning Distribution and Gradient Matching, establishes it as a versatile FDD approach.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning has witnessed remarkable success in computer vision, particularly with recent break-throughs in vision models (Oquab et al., 2023; Kirillov et al., 2023; Radford et al., 2021; Li et al., 2022; Zhou et al., 2023). Their vision backbones, such as ResNet (He et al., 2015) and ViT (Dosovit-skiy et al., 2020), are data-hungry models that require extensive amounts of data for optimization. Dataset Distillation (DD) (Wang et al., 2018; Zhao & Bilen, 2021; 2023; Cazenavette et al., 2022; Wang et al., 2022; Lee et al., 2022b; Cui et al., 2022; Loo et al., 2023; Guo et al., 2023; He & Zhou, 2024; Zhao et al., 2024) provides a promising solution to alleviate this data requirement by condensing the original large dataset into more informative and smaller counterparts (Mehrabi et al., 2021; Chung et al., 2023). Despite its appeal, existing DD researches focus on ensuring that models trained on condensed datasets perform comparable accuracy to those trained on the original dataset in terms of target attributes (TA) (Cui et al., 2024; Lu et al., 2024; Vahidian et al., 2024). However, they have overlooked enabling the fairness of trained models with respect to protected attributes (PA).\nUnfairness typically arises from imbalanced sample distributions among PA in the empirical training datasets. When the original datasets suffer from the PA imbalance, the corresponding datasets condensed by vanilla DDs inherit and amplify this bias in Fig. 1(a). Since vanilla DDs tend to cover TA distribution for image classification, and as a result, it naturally leads to more synthetic samples located in majority groups compared to minority groups w.r.t. PA, as shown in Fig. 1(b). In this case, these condensed datasets retain the imbalance between protected attributes, thereby rendering the model trained on them unfair. Moreover, the reduced size of the condensed datasets typically amplifies the bias present in the original datasets, especially when there is a significant gap in size between the original and condensed datasets, such as image per class (IPC) 1000 vs. 10. Therefore, it is worthwhile to broaden the scope of DD to encompass both TA accuracy and PA fairness. Recent"}, {"title": "2 PRELIMINARIES", "content": "Dataset Distillation. Given a vast dataset $\\mathcal{T} = \\{(\\mathbf{x}_i, \\mathbf{y}_i)\\}_{i=1}^M$, the goal of DD is to condense the original dataset $\\mathcal{T}$ into a smaller dataset $\\mathcal{S} = \\{(\\mathbf{x}_i, \\mathbf{y}_i)\\}_{i=1}^N$ via distillation algorithm Alg with a nerual network, parameterized by $\\theta$. Randomly initialized classification network $g_\\psi$ should maintain the same empirical risk whether it is trained on $\\mathcal{S}$ or $\\mathcal{T}$.\n\\begin{equation}\n\\mathcal{S}^* = \\arg \\min_{\\mathcal{S}} Alg(\\mathcal{S},\\mathcal{T}, \\theta),\n\\end{equation}\n\\begin{equation}\n\\mathbb{E}_{\\mathbf{x}, \\mathbf{y}\\sim \\mathcal{S}} [l(g_{\\psi}; \\mathcal{S})] \\approx \\mathbb{E}_{\\mathbf{x}, \\mathbf{y}\\sim \\mathcal{T}} [l(g_{\\psi}; \\mathcal{T})],\n\\end{equation}\nwhere $\\Theta$ and $l(\\cdot)$ represent the parameter space and loss function, respectively. The pioneering work (Wang et al., 2018) formulates Alg as a bi-level optimization problem. However, such an optimization process is time-consuming and unStable Recent works circumvent it and propose surrogate matching objectives to achieve comparable and even better performance. This research line is collectively referred to as the DMF, and our paper primarily studies one-stage GM (Zhao et al., 2020; Zhao & Bilen, 2021) and DM (Zhao & Bilen, 2023; Wang et al., 2022; Zhao & Bilen, 2022). We leave two-stage trajectory-matching (TM) for future exploration.\nVisual Fairness Visual fairness is an important field to mitigate discrimination against minority groups. Group fairness requires no statistical disparity to different groups in terms of PA, such as race and gender. This means that an ideal fair model should make independent predictions between TA and PA. One of the common fairness criteria is equalized odds (EO), which computes the prediction accuracy of PA conditioned on TA, to evaluate the level of conditional independence between PA and TA. We use two types of difference of equalized odds DEOM and DEOA from the worst and averaged levels. Formally, given the PA set $\\mathcal{A} = \\{a_1, a_2, ..., a_p\\}$, $DEO_M$ and $DEO_A$ (Jung et al., 2021) can be formulated mathematically as follows:\n\\begin{equation}\nDEO_M = \\max_{\\mathcal{Y} \\in \\mathcal{V}} \\max_{a_i,a_j \\in \\mathcal{A} \\& a_i \\neq a_j} |\\mathbb{P}(\\hat{Y} = y|Y = y, A = a_i) - \\mathbb{P}(\\hat{Y} = y|Y = y, A = a_j)|,\n\\end{equation}\n\\begin{equation}\nDEO_A = mean_{\\mathcal{Y} \\in \\mathcal{V}} \\max_{a_i,a_j \\in \\mathcal{A} \\& a_i \\neq a_j} |\\mathbb{P}(\\hat{Y} = y|Y = y, A = a_i) - \\mathbb{P}(\\hat{Y} = y|Y = y, A = a_j)|.\n\\end{equation}"}, {"title": "3 A CLOSE LOOK AT DATASET DISTILLATION FROM FAIRNESSs", "content": "A unified perspective for Data Match Framework. The essence of the DMF lies in choosing the target signs of original samples that effectively represent their characteristics for image recognition, and then aligning these signals as a proxy task to optimize the condensed dataset. The target signal $\\phi(\\mathbf{x}; \\theta)$ is typically the key information from feature extraction or optimization process using a randomly initialized network parameterized by $\\theta$. For example, GM aligns the gradient information produced by $\\mathcal{T}$ with that of the condensed $\\mathcal{S}$. Instead, DM matches the embedding distributions of $\\mathcal{T}$ and $\\mathcal{S}$. As for these approaches in DMF. we can unify the optimization objective as $\\mathcal{L}(\\mathcal{S}; \\theta, \\mathcal{T})$:\n\\begin{equation}\n\\mathcal{L}(\\mathcal{S}; \\theta, \\mathcal{T}) := \\sum_{y \\in \\mathcal{Y}} D(\\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_y} (\\mathbf{x}; \\theta)], \\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)]),\n\\end{equation}\nwhere $\\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_y} (\\mathbf{x}; \\theta)] \\in \\mathbb{R}^C$ and $\\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)] \\in \\mathbb{R}^C$ are represented expectation vectors of the target signs on $\\mathcal{T}$ and $\\mathcal{S}$, respectively. $D(\\cdot, \\cdot)$ is a distance function. In DMF, MSE is adopted in DM and DREAM, and MAE is used in IDC. Also, cosine distance is involved in DC.\nWhy does vanilla DD fail to mitigate PA imbalance? Given the PA set $\\mathcal{A}$ in $\\mathcal{T}$, let us define the class-level sample ratio $\\mathcal{R}_y = \\{r_{a_1}, r_{a_2}, ..., r_{a_p} \\}$, where $r_{a_i} = |\\mathcal{T}_{a_i}|/|\\mathcal{T}_y|$, and $|\\cdot|$ represents the cardinal number of a set. Current DD paradigms focus on preserving TA representativeness for image recognition. Here, we decompose the whole expectation into the expectation of PA-wise groups, i.e, $\\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_y} (\\mathbf{x}; \\theta)] = \\sum_{a_i \\in \\mathcal{A}} r_{a_i} \\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_{a_i}} (\\mathbf{x}; \\theta)]$, and thus Eq. 3 can be rewritten as follows:\n\\begin{equation}\n\\mathcal{L}(\\mathcal{S}; \\theta, \\mathcal{T}) := \\sum_{y \\in \\mathcal{Y}} D(\\sum_{a_i \\in \\mathcal{A}} r_{a_i} \\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_{a_i}} (\\mathbf{x}; \\theta)], \\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)]).\n\\end{equation}\nFrom Eq. 4, the optimization objective of class $y$ is weighted by the sample ratio $r_{a_i}$ from different groups. When $\\mathcal{T}$ suffers from PA imbalance, e.g., $r_{y_i} > \\sum_{i \\neq j}r_{y_i}$, the majority group indexed by $i$ contributes more to the alignment compared to minority groups, as present in Fig. 2(a). In"}, {"title": "4 FAIRDD", "content": "$\\mathcal{L}_{FairDD}(\\mathcal{S}; \\theta, \\mathcal{T}) := \\sum_{y \\in \\mathcal{Y}} \\sum_{a_i \\in \\mathcal{A}} D(\\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_{a_i}} (\\mathbf{x}; \\theta)], \\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)]).$\n\\end{equation}\nTherefore, the objective of vanilla DDs suffers from the PA imbalance within $\\mathcal{T}$.\nNext, we further study how the resulting $\\mathcal{S}$ is affected by sample ratio $r_{a_i}$ of different groups. To this end, we assume that the optimization process could reach the optimal solution for each class, and as a result, the final resulting $\\mathcal{S}$ satisfies the condition that the derivative of the objective function with respect to $\\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)]$ equals 0. Formally. we have the following mathematical equation:\n$\\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\mathcal{S}_y; \\theta, \\mathcal{T}_y)}{\\partial \\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)]} = 0 &\\Rightarrow \\frac{\\partial D(\\sum_{a_i \\in \\mathcal{A}} r_{a_i} \\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_{a_i}} (\\mathbf{x}; \\theta)], \\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)])}{\\partial \\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)]} = 0\n\\end{aligned}$$\nNow, let's delve into the specific distance metrics used in vanilla DDs, where the most commonly used metrics are MAE, MSE, and cosine distance. We respectively analyze that the optimal point of $\\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)]$ could reach under each of these metrics.\n\\begin{equation}\n\\frac{\\partial \\mathcal{L}(\\mathcal{S}_y; \\theta, \\mathcal{T}_y)}{\\partial \\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)]} = 0 \\Rightarrow \\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)] = \\begin{cases}\n\\sum_{a_i \\in \\mathcal{A}} \\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_{a_i}} (\\mathbf{x}; \\theta)], &\\text{For MAE}\\\\n\\sum_{a_i \\in \\mathcal{A}} \\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_{a_i}} (\\mathbf{x}; \\theta)], &\\text{For MSE}\\\\n\\lambda \\sum_{a_i \\in \\mathcal{A}} \\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_{a_i}} (\\mathbf{x}; \\theta)], &\\text{For cosine distance}\n\\end{cases}\n\\end{equation}\n$\\begin{aligned}\n\\mathcal{L}_{FairDD}(\\mathcal{S}; \\theta, \\mathcal{T}) := \\sum_{y \\in \\mathcal{Y}} \\sum_{a_i \\in \\mathcal{A}} D(\\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_{a_i}} (\\mathbf{x}; \\theta)], \\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)]).\n\\end{aligned}$\n\\begin{equation}\n\\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)] = \\begin{cases}\n\\frac{1}{|\\mathcal{A}|} \\sum_{a_i \\in \\mathcal{A}} \\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_{a_i}} (\\mathbf{x}; \\theta)], &\\text{For MAE}\\\\n\\frac{1}{|\\mathcal{A}|} \\sum_{a_i \\in \\mathcal{A}} \\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_{a_i}} (\\mathbf{x}; \\theta)], &\\text{For MSE}\\\\n\\lambda \\sum_{a_i \\in \\mathcal{A}} \\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_{a_i}} (\\mathbf{x}; \\theta)], &\\text{For cosine distance}\n\\end{cases}\n\\end{equation}\nWhere $\\lambda$ is a scalar, equaling $\\frac{||\\sum_{a_i \\in \\mathcal{A}} \\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_{a_i}} (\\mathbf{x}; \\theta)]||^2}{\\|\\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)] ||^2}$. Eq. 6 presents that the expectation of synthetic samples $\\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)]$ ultimately converges to an average on expectations of all PA groups, weighted by their respective sample ratios $r_{a_i}$. This indicates that vanilla DDs naturally favor majority groups, causing $\\mathcal{S}$ to shift towards them and inherit their biases.\nTherefore, when original datasets suffer from PA imbalance, e.g., $r_{y_i} > \\sum_{i \\neq j}r_{y_i}$, the unfairness of the synthetic dataset stems from two different aspects: 1) The majority group renders synthetic samples to locate its region from Eq. 6. 2) According to Eq. 4, the large sample quantities of the majority group contribute more to the total loss. As a result, minority groups experience higher loss during testing, which limits the model to represent them accurately. These factors prompt us to reduce the impact of PA imbalance on the generation of $\\mathcal{S}$.\n4.1\nOVERVIEW\nIn this paper, we propose a novel FDD framework that achieves both PA fairness and TA accuracy for the model trained on its generation $\\mathcal{S}$, regardless of whether the original datasets exhibit PA fairness. As illustrated in Fig. 2(b), FairDD first partitions the dataset into different groups w.r.t. PA and then introduces an effective synchronized matching to equally align $\\mathcal{S}$ with each group within $\\mathcal{T}$. Compared to vanilla DDs, which simply pull the synthetic dataset toward the whole dataset center that is biased toward the majority group in the synthetic dataset, FairDD proposes a group-level synchronized alignment, in which each group attracts the synthetic data toward itself, thus forcing it to move farther from other groups. This \"pull-and-push\" process prevents the generation from collapsing into majority groups (fairness) and ensures class-level distributional coverage (accuracy).\n4.2\nMETHODS\nAs mentioned in Sec. 3, vanilla DDs fail to mitigate PA imbalance and even amplify the discrimination. The relation behind the failure is that the majority group dominates the generation direction of $\\mathcal{S}$ and leads to the resulting $\\mathcal{S}$ inheriting the PA imbalance, i.e., preference to fitting to the majority group. To avoid the synthetic samples collapsing into the majority group, we decompose the single target (dominated by the majority group) into PA-wise sub-targets, and simultaneously align $\\mathcal{S}$ with these sub-targets, without incorporating the specific sample ratio of each group into the optimization objective. In this way, we obtain the unified objective function of FairDD:\n\\begin{equation}\n\\mathcal{L}_{FairDD}(\\mathcal{S}; \\theta, \\mathcal{T}) := \\sum_{y \\in \\mathcal{Y}} \\sum_{a_i \\in \\mathcal{A}} D(\\mathbb{E}[\\phi_{x \\sim \\mathcal{T}_{a_i}} (\\mathbf{x}; \\theta)], \\mathbb{E}[\\phi_{x \\sim \\mathcal{S}_y} (\\mathbf{x}; \\theta)]).\n\\end{equation}\nThe reformulation forms synchronized matching, where different sub-targets attempt to pull $\\mathcal{S}$ into their corresponding PA regions. Each PA group holds equal importance in generating $\\mathcal{S}$, ultimately converging to a balanced (fair) status. Subsequently, we present a theoretical analysis illustrating how FairDD effectively mitigates PA imbalance and guarantees coverage across the entire TA distribution."}, {"title": "5 EXPERIMENT", "content": "Datasets Comprehensive experiments have been conducted on publicly available datasets of diverse biases, including foreground bias (FG), background bias (BG), BG & FG bias, and real-world bias. C-MNIST (FG) is a variant of MNIST (LeCun et al., 2010) used to evaluate model fairness, where the handwriting numbers in each class are painted with ten different colors. To correlate the TA (digital number) and PA (color) within the training dataset, each training class is predominantly associated with one color according to the same biased ratio (BR), while the remaining samples are evenly painted with the other nine colors. BR is the ratio of the majority group samples to the total samples across all groups. For the test dataset, we evenly paint the numbers for each class with ten colors to test the model bias trained on S. C-MNIST (BG) adopts the same operation on the background and keeps the foreground unchanged. Colored-FMNIST (FG) is the modified version of Fashion-MNIST, originally aiming to classify object semantics. Like C-MNIST (FG), we color the objects for the training and test datasets. Colored-FMNIST (BG) paints the background similarly to C-MNIST (BG). CIFAR10-S (BG & FG) introduces a PA by applying grayscale or not to CIFAR10 samples. Following Wang et al. (2020), we grayscale a portion of the training images, correlating TA and PA among different classes. For fairness evaluation, we duplicate the test images, apply grayscale to the copies, and add them to the test dataset. We also test FairDD on the real-world facial dataset CelebA, a widely used fairness dataset. We follow the common practice of treating attractive attribute as TA and gender as PA (evaluations on other attributes refer to Appendix B).\nBaselines & Evaluation metrics FairDD is a general fairness framework applicable to diverse DDs in DMF. We apply FairDD to diverse DMF approaches including DM method DM (Zhao & Bilen, 2023) and GM methods DC (Zhao et al., 2020), IDC (DC version) (Kim et al., 2022), and DREAM (DC version) (Liu et al., 2023). To provide an overall evaluation for model bias toward PA, we use $DEO_M(\\downarrow) \\in [0, 100]$ and $DEO_A(\\downarrow) \\in [0, 100]$ to measure the worst and average fairness levels. Also, we report accuracy($\\uparrow$) to assess the model's prediction of TA. We also provide a comparison with MTT in Appendix L. Sometimes, we will abuse DM+FairDD and FairDD for clarification.\nImplementation details We default to BR of 0.9 for all synthetic original datasets to induce signifi-cant PA skew. In Table 24, we conduct the ablation study on BR. All baselines are reproduced using official implementations. FairDD doesn't introduce extra hyperparameters or learnable parameters. Experiments are conducted on PyTorch 2.0.0 with a single NVIDIA RTX 3090 24GB GPU.\n5.2 MAIN RESULTS\nWe use distilled datasets S from different DDs to train and evaluate ConvNet with the same parameters, and then report the corresponding fairness and accuracy. Random refers to sampling defined IPC from the original dataset to create smaller datasets. Besides, Whole means we train the model using the entire training dataset without distillation or sampling."}, {"title": "5.3 RESULT ANALYSIS", "content": "Representation loss\n4.1 Data augmentation (data is not used to calculate the loss)\n4.2 Data augmentation (data is used to calculate the loss)\nVisual Bias of the initializers\n6.4 Evaluation metrics and baselines"}, {"title": "5.4 ABLATION STUDY", "content": "Ablation on biased ratio of original datasets BR reflects the extent of unfairness in the original datasets and indicates the level of PA skew that the distillation process of S will encounter. We investigate the impact of BR values on fairness performance by setting BR to {0.85, 0.90, 0.95} on C-MNIST (FG), C-FMNIST (BG), and CIFAR10-S. The results at IPC = 50 in Table 24 show that DM is sensitive to the BR of original datasets, with its DEOM decreasing from 70.13 to 100.0 as BR increases from 0.85 to 0.95. A similar trend is observed in other datasets. Compared to DM, FairDD maintains consistent fairness and accuracy levels across different biases. This is attributed to the synchronized matching, which explicitly aligns each PA-wise subtarget, reducing sensitivity to group-specific sample numbers. This shows FairDD's robustness to PA skew in the original datasets."}, {"title": "6 RELATED WORK", "content": "Dataset distillation Dataset distillation has been broadly applied to many important fields (Lee et al., 2023; He et al., 2024; Feng et al., 2023; Chen et al., 2023). The first work (Wang et al., 2018) attempts to formulate dataset distillation as a bi-level optimization problem. However, the two folds of the optimization process are time-consuming. Neural tangent kernel (Jacot et al., 2018) are utilized to obtain the close form of the inner loop (Nguyen et al., 2021; Loo et al., 2022; Zhou et al., 2022). Some works propose surrogate objectives to achieve comparable even better performance, including matching-based methods like GM (Zhao et al., 2020; Zhao & Bilen, 2021; Lee et al., 2022a), DM (Zhao & Bilen, 2023; Wang et al., 2022; Zhao & Bilen, 2022), TM (Cazenavette et al., 2022; Cui et al., 2022), soft label learning (Bohdal et al., 2020; Sucholutsky & Schonlau, 2021) and factorization (Kim et al., 2022; Deng & Russakovsky, 2022; Liu et al., 2022; Lee et al., 2022a). Cui et al. (2024) and Zhao et al. (2024) focus on reducing the bias to improve the classification performance (Vahidian et al., 2024; Wang et al., 2024). In summary, current DD approaches only pursue classification accuracy that models trained on synthetic datasets, while they neglect fairness concerning PA. Therefore, we propose FairDD to improve fairness without sacrificing TA accuracy.\nVisual fairness With the advancement of computer vision, fair predictions without discrimination towards minority groups have become a crucial topic (Caton & Haas, 2024). According to the stage of bias mitigation, the research field of fairness algorithm (Bellamy et al., 2019) can be classified into three branches: Pre-processing (Creager et al., 2019; Louizos et al., 2015; Quadrianto et al., 2019; Sattigeri et al., 2019), In-processing (Agarwal et al., 2018; Jiang & Nachum, 2020; Zafar et al., 2017; Zhang et al., 2018; Jung et al., 2021; Wang et al., 2020; Jung et al., 2022; Zemel et al., 2013), and Post-processing (Alghamdi et al., 2020; Hardt et al., 2016). Our research falls within Pre-processing branch. Pre-processing aims to generate a fair version of datasets for downstream tasks. Several related works frame this issue as a data-to-data translation problem, utilizing generative models to produce fairer datasets concerning protected groups (Sattigeri et al., 2019; Quadrianto et al., 2019). However, unlike the traditional fairness approach that primarily focuses on reducing bias in original datasets Subramanian et al. (2021); Tarzanagh et al. (2023); Vogel et al. (2020); Rangwani et al. (2022), our work aims to integrate fairness into DD. Our objective is to mitigate the bias of original datasets while simultaneously condensing them into smaller and more informative counterparts."}, {"title": "7 CONCLUSION", "content": "This paper reveals for the first time that vanilla DDs fail to mitigate the bias of original datasets and even exacerbate the bias. To address the problem, we propose a unified fair dataset distillation frame-work called FairDD, broadly applicable to various DDs in DMF. FairDD requires no modifications to the architectures of vanilla DDs and introduces an easy-to-implement yet effective attribute-wise matching. This method mitigates the dominance of the majority group and ensures that synthetic datasets equally incorporate representative patterns with all protected attributes from both majority groups and minority groups. By doing so, FairDD guarantees the fairness of synthetic datasets while maintaining their representativeness for image recognition. We provide extensive theoretical analysis and empirical results to demonstrate the superiority of FairDD."}]}