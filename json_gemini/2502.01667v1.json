{"title": "REFINING ALIGNMENT FRAMEWORK FOR DIFFUSION MODELS WITH INTERMEDIATE-STEP PREFERENCE RANKING", "authors": ["Jie Ren", "Yuhang Zhang", "Dongrui Liu", "Xiaopeng Zhang", "Qi Tian"], "abstract": "Direct preference optimization (DPO) has shown success in aligning diffusion models with human preference. Previous approaches typically assume a consistent preference label between final generations and noisy samples at intermediate steps, and directly apply DPO to these noisy samples for fine-tuning. However, we theoretically identify inherent issues in this assumption and its impacts on the effectiveness of preference alignment. We first demonstrate the inherent issues from two perspectives: gradient direction and preference order, and then propose a Tailored Preference Optimization (TailorPO) framework for aligning diffusion models with human preference, underpinned by some theoretical insights. Our approach directly ranks intermediate noisy samples based on their step-wise reward, and effectively resolves the gradient direction issues through a simple yet efficient design. Additionally, we incorporate the gradient guidance of diffusion models into preference alignment to further enhance the optimization effectiveness. Experimental results demonstrate that our method significantly improves the model's ability to generate aesthetically pleasing and human-preferred images.", "sections": [{"title": "1 INTRODUCTION", "content": "Direct preference optimization (DPO), which fine-tunes the model on paired data to align the model generations with human preferences, has demonstrated its success in large language models (LLMs) (Rafailov et al., 2023). Recently, researchers generalized this method to diffusion models for text-to-image generation (Black et al., 2024; Yang et al., 2024a; Wallace et al., 2024). Given a pair of images generated from the same prompt and a ranking of human preference for them, DPO aims to increase the probability of generating the preferred sample while decreasing the probability of generating another sample, which enables the model to generate more visually appealing and aesthetically pleasing images that better align with human preferences.\nSpecifically, previous researchers (Yang et al., 2024a) leverage the trajectory-level preference to rank the generated samples. As shown in Figure 1(a), given a text prompt c, they first sample a pair of denoising trajectories $[x_1,...,x_0]$ and $[x_1,...,x_0^+]$ from the diffusion model, and then rank them according to the human preference on the final generated images $x_0^-$ and $x_0^+$. It is assumed that the preference order of $(x_t^-, x_t^+)$, at the end of the generation trajectory, can consistently represent the preference order of $(x_t^-, x_t^+)$ at all intermediate steps t. Then, the DPO loss function is implemented using the generation probabilities $p(x_{t-1}|x_t^-, c)$ and $p(x_{t-1}|x_t^+, c)$ at each step t to fine-tune the diffusion model, which is called the step-level optimization.\nHowever, we notice that the above trajectory-level preference ranking and the step-level optimization are not fully compatible in diffusion models. First, the trajectory-level preference ranking (i.e., the preference order of final outputs $(x_0^-, x_0^+)$ of trajectories) does not accurately reflect the preference order of $(x_t^-, x_t^+)$ at intermediate steps. Considering the inherent randomness in the denoising process, simply assigning the preference of final outputs to all the intermediate steps will detrimentally affect the preference optimization performance. Second, the generation probabilities $p(x_{t-1}|x_t^-, c)$"}, {"title": "2 RELATED WORKS", "content": "Diffusion models. As a new class of generative models, diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021) transform Gaussian noises into images (Dhariwal &"}, {"title": "3 METHOD", "content": null}, {"title": "3.1 PRELIMINARIES", "content": "Diffusion models. Diffusion models contain a forward process and a reverse denoising process. In the forward process, given an input $x_0$ sampled from the real distribution $p_{data}$, diffusion models gradually add Gaussian noises to $x_0$ at each step $t \\in [1, T]$, as follows:\n$x_t = \\sqrt{a_t}x_{t-1} + \\sqrt{1 - a_t}e_{t-1} = \\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 - \\bar{a}_t}e$\nwhere $e_t \\sim N(0, I)$ denotes the Gaussian noise at step t. $a_{0:T}$ denotes the variance schedule and $\\bar{a}_t = \\prod_{i=1}^{t} a_i$\nIn the reverse denoising process, the diffusion model is trained to learn $p(x_{t-1}|x_t)$ at each step t. Specifically, following (Song et al., 2021), the denoising step at step t is formulated as\n$x_{t-1} = \\frac{\\sqrt{a_{t-1}}}{\\sqrt{a_t}}x_t - \\frac{\\sqrt{1 - \\bar{a}_t}}{\\sqrt{a_t}}e_{\\theta}(x_t, t) + \\sqrt{1 - a_{t-1} - \\sigma_t^2}z\\sigma_t$,\nwhere $e_{\\theta}(\\cdot)$ is a noise prediction network with trainable parameters $\\theta$, which aims to use $e_{\\theta}(x_t, t)$ to predict the noise $e$ in Eq. (1) at each step t. $z \\sim N(0, I)$ is sampled from the standard Gaussian distribution. In fact, $x_{t-1}$ is sampled from the estimated distribution $N(\\mu_{\\theta}(x_t), \\sigma_t^2I)$. According to the reverse process, $x_0(x_t) = (x_t - \\sqrt{1 - \\bar{a}_t}e_{\\theta}(x_t, t)/\\sqrt{\\bar{a}_t}$ represents the predicted $x_0$ at step x.\nDirect preference optimization (DPO) (Rafailov et al., 2023). The DPO method was origi- nally proposed to fine-tune large language models to align with human preferences based on paired datasets. Given a prompt x, two responses $y_0$ and $y_1$ are sampling from the generative model $\\pi_{\\theta}$, i.e., $y_0, y_1 \\sim \\pi_{\\theta}(y|x)$. Then, $y_0$ and $y_1$ are ranked based on human preferences or the outputs $r(x, y_0)$ and $r(x, y_1)$ of a pre-trained reward model r(.). Let $y_w$ denote the preferred response in $(y_0, y_1)$"}, {"title": "3.2 MISMATCH BETWEEN TRAJECTORY-LEVEL RANKING AND STEP-LEVEL OPTIMIZATION", "content": "In this section, we first revisit how existing works implement DPO for diffusion models, using D3PO (Yang et al., 2024a) as an example for explanation. Then, we identify the mismatch between their trajectory-level ranking and step-level optimization from two perspectives.\nFor a text-to-image diffusion model $\\pi_{\\theta}$ parameterized by $\\theta$, given a text prompt c, D3PO first samples a pair of generation trajectories $[x_T^1,...,x_0^-]$ and $[x_T^1,...,x_0^+]$. Then, they compare the reward scores $r(c, x_0^-)$ and $r(c, x_0^+)$ of generated images, using the reward model r(\u00b7), and rank their preference order. The preferred image is denoted by $x_0^w$ and the dis-preferred image is denoted by $x_0^l$. Then, as Figure 1(a) shows, it is assumed that the preference order of final images $(x_t^0,x_t^+)$ represents the preference order of $(x_t^-, x_t^+)$ at all intermediate steps t. Subsequently, the diffusion model is fine-tuned by minimizing the following DPO-like loss function at the step level.\n$L_{D3PO}(\\theta) = -E_{(c,x_t^-,x_t^+,x_{t-1}^-,x_{t-1}^+)} [log \\sigma (\\beta log \\frac{\\pi_{\\theta} (x_{t-1}^-|x_t^-, c)}{\\pi_{ref}(x_{t-1}^-|x_t^-, c)} - \\beta log \\frac{\\pi_{\\theta} (x_{t-1}^+|x_t^+, c)}{\\pi_{ref}(x_{t-1}^+|x_t^+, c)})]$\nWe argue that there are two critical issues in the aforementioned process and loss function, which we will elaborate on and prove through theoretical analysis in the following sections.\nInaccurate preference order. The first obvious issue is that the preference order of final images $x_0$ at the end of the trajectory cannot accurately reflect the preference order of noisy samples $x_t$ at intermediate steps. Liang et al. (2024) demonstrated that early steps in the denoising process tend to handle layout, while later steps focus more on detailed textures. However, the preference order based on final images primarily reflects layout and composition preferences, misaligning with the function of later steps. Beyond these visual discoveries, we rethink this problem from another perspective and theoretically formulate the reward at intermediate steps.\nSimilar to (Yang et al., 2024a), we formulate the denoising process in a diffusion model as a Markov decision process (MDP), as follows.\n$S_t = (c, x_{T-t}), A_t \\equiv x_{T-t-1}, R_t = R(S_t, A_t) \\equiv R((c, x_{T-t}), x_{T-t-1})$\n$P(S_{t+1}|S_t, A_t) \\equiv (\\delta_c, \\delta_{x_{T-t-1}}), \\pi(A_t|S_t) \\equiv \\pi_{\\theta}(x_{T-t-1}|x_{T-t}, c)$"}, {"title": "3.3 TAILORED PREFERENCE OPTIMIZATION FRAMEWORK FOR DIFFUSION MODELS", "content": "To address the aforementioned issues, considering the characteristics of diffusion models, we pro- pose a Tailored Preference Optimization (TailorPO) framework for fine-tuning diffusion models in"}, {"title": "3.4 GRADIENT GUIDANCE OF REWARD MODEL FOR FINE-TUNING", "content": "In TailorPO, since noisy samples $(x_{t-1}^+,x_{t-1}^+)$ are generated from the same $x_t$, their similarity increases as t decreases. This increasing similarity potentially reduces the diversity of paired samples for training. On the other hand, Khaki et al. (2024) have shown that a large difference between paired samples is beneficial to the DPO effectiveness. Therefore, to enhance the DPO performance in this case, we propose enlarging the difference between two noisy samples from the reward perspective.\nTo this end, we consider how to adjust the reward of a noisy sample $x_{t-1}$. Similar to (Guo et al., 2024), we use $r_{high}$ to represent an expected higher reward. Then, the gradient of the conditional score function is $\\nabla_{x_{t-1}} log p(x_{t-1}|r_{high}) = \\nabla log p(x_{t-1}) + \\nabla_{x_{t-1}} log p(r_{high}|x_{t-1})$, where the first term $\\nabla log p(x_{t-1})$ is estimated by the diffusion model itself, and the second term is to be estimated by the guidance. Guo et al. (2024) further prove the following relationship for estimation.\n$\\nabla_{x_{t-1}} log P(r_{high} x_{t-1}) \\propto \\nabla_{x_{t-1}} log p(r'_{high}|x_0(x_{t-1})) \\propto -\\eta_t\\nabla_{x_{t-1}} (r_{high} - r_t(C, x_{t-1}))^2$\nTherefore, we can inject the gradient term $\\nabla_{x_{t-1}} (r_{high} - r_t(C, x_{t-1}))^2$ as the guidance to the gener- ation of $x_{t-1}$ to adjust its reward. Specifically, we update the noisy samples as follows.\n$x_{t-1} \\leftarrow x_{t-1} - \\eta_t\\nabla_{x_{t-1}} (r_{high} - r_t(c, x_{t-1}))^2$, to increase reward\n$x_{t-1} \\leftarrow x_{t-1} + \\eta_t\\nabla_{x_{t-1}} (r_{high} - r_t(c, x_{t-1}))^2$, to decrease reward\nTo demonstrate that the above gradient guidance is able to adjust the reward of noisy samples as expected, we compared the step-wise rewards of the original sample $x_{t-1}$, the increased sample $x_{t-1}^+$, and the decreased sample $x_{t-1}^-$. Specifically, we generated 100 noisy samples $x_{t-1}$ from Stable Diffusion v1.5 (Rombach et al., 2022), and then computed the corresponding $x_{t-1}^+$ and $x_{t-1}^-$. We set $\\eta_t = 0.2$ and $r_{high} = r_t(C, x_{t-1}) + d$ following Guo et al. (2024), where the constant $d = 0.5$ specified the expected increment of the reward value.\nThen, we computed the ratio of increased samples (satisfying $r_t(c, x_{t-1}^+) > r_t(c, x_{t-1})$) and the ratio of decreased samples (satisfying $r_t(c, x_{t-1}^-) < r_t(C, x_{t-1})$). Table 1 shows that for almost all samples, the gradient guidance successfully increased or decreased their reward as expected, demonstrating its effectiveness in adapting the reward of samples.\nFinally, we apply this method in our training process to enlarge the reward gap between a pair of noisy samples and develop the TailorPO-G framework. As shown in Figure 3 and Algorithm 1, we first modify the preferred sample $x_{t-1}^+$ to increase its reward value, and then use the modified sample"}, {"title": "4 EXPERIMENTS", "content": "Experimental settings. In our experiments, we evaluate the effectiveness of our method in fine- tuning Stable Diffusion v1.5 (Rombach et al., 2022). We compared our TailorPO method with the RLHF method, DDPO (Black et al., 2024), and DPO-style methods, including D3PO (Yang et al., 2024a) and SPO (Liang et al., 2024). For all methods, we used the aesthetic scorer (Schuhmann et al., 2022), ImageReward (Xu et al., 2023), PickScore (Kirstain et al., 2023), HPSv2 (Wu et al., 2023), and JPEG compressibility measurement (Black et al., 2024) as reward models. Considering that some reward models are non-differentiable, we evaluate both the effectiveness of TailorPO and TailorPO-G, respectively.\nFollowing the settings in D3PO (Yang et al., 2024a) and SPO (Liang et al., 2024), we applied the DDIM scheduler (Song et al., 2021) with $\\eta$ = 1.0 and T = 20 inference steps. The generated images were of resolution of 512 \u00d7 512. We employed LoRA (Hu et al., 2022) to fine-tune the UNet parameters on a total of 10,000 samples with a batch size of 2. The reference model was set as the pre-trained Stable Diffusion v1.5 itself. For SPO, we ran the officially released code by using the same hyper-parameters as in its original paper, and for other methods, we used the same hyper-parameters as in (Yang et al., 2024a), except that we set a smaller batch size for all methods. In particular, for all our frameworks, we generated images with T = 20 and uniformly sampled $T_{fine-tune}$ = 5 steps for fine-tuning, i.e., we only fine-tuned the model at steps t = 20, 16, 12, 8, 4. In addition, we set the coefficient $\\eta_t$ in gradient guidance using a cosine scheduler in the range of [0.1,0.2], which assigned a higher coefficient to smaller t (samples closer to output images). We have conducted ablation studies in Appendix E to show that our method is relatively stable with respect to the setting of $T_{fine-tune}$ and $\\eta_t$. We have also conducted ablation studies on each component in our framework in Appendix E."}, {"title": "4.1 EFFECTIVENESS OF ALIGNING DIFFUSION MODELS WITH PREFERENCE", "content": "In this section, we demonstrate that our frameworks outperform previous methods in aligning diffu- sion models with various preferences, from both quantitative and qualitative perspectives.\nQuantitative evaluation. We fine-tuned SD v1.5 on various reward models using a set of prompts of animals released by Black et al. (2024) and a set of complex prompts in the Pick-a-Pic"}, {"title": "4.2 GENERALIZATION TO DIFFERENT PROMPTS AND REWARD MODELS", "content": "In this section, we investigate the generalization ability of the fine-tuned model using our method. Here, we consider two types of generalization mentioned in (Clark et al., 2024): prompt generaliza- tion and reward generalization.\nPrompt generalization refers to the model's ability to generate high-quality images for prompts beyond those used in fine-tuning. To evaluate this, we fine-tuned Stable Diffusion v1.5 on 45 prompts of simple animal (Black et al., 2024) and evaluated its performance on 500 complex prompts (Kirstain et al., 2023). As shown in Table 3, the model fine-tuned on simple prompts exhibited higher reward values on complex prompts than the original SD v1.5, with our approach achieving the highest performance. Figure 8 presents examples of images generated from complex prompts, demonstrating that despite being fine-tuned on simple prompts, the model was also capable of gen- erating high-quality images given complex prompts. This highlights the effectiveness of our method in enhancing the model's generalization to human-preferred images across various prompts, rather than overfitting to simple prompts.\nReward generalization refers to the phenomenon where fine-tuning the model towards a specific reward model can also enhance its performance on another different but related reward model. We selected one reward model from the aesthetic scorer, ImageReward, HPSv2, and Pickscore for fine- tuning, and used the other three reward models for evaluation. Table 4 shows that after being fine- tuned towards the aesthetic scorer, ImageReward, and PickScore, the model usually exhibited higher performance on all these four reward models. In other words, our method boosted the overall ability of the model to generate high-quality images."}, {"title": "5 CONCLUSIONS", "content": "In this study, we rethink the existing DPO framework for aligning diffusion models and identify the potential flaws in these methods. We analyze these issues from both perspectives of preference"}, {"title": "A GRADIENT OF LOSS FUNCTIONS", "content": "Gradient of the original DPO loss function. Given the input (x, yw, y\u00b9) ~ D, the loss of DPO is as follows.\n$L = -E_{(x,yw,y1)~D}[log \\sigma (\\beta log \\frac{\\pi_{\\theta} (y_wx)}{\\pi_{ref}(y_wx)} - \\beta log \\frac{\\pi_{\\theta} (y_lx)}{\\pi_{ref}(y_lx)}))]$\nLet $h_{\\theta} (x, y_w, \\omega, \\psi_1) \\widehat{=} \\beta log \\frac{\\pi_{\\theta} (y_w|x)}{\\pi_{ref} (y_w|x)} - \\beta log \\frac{\\pi_{\\theta} (y_1|x)}{\\pi_{ref} (y_1|x)} $, and $f(x, y_w,Y_1) \\widehat{=} \\beta(1 - \\sigma(h_{\\theta} (x, y_w, Y_1)))$, then\n$\\frac{DL(x, y_{\\omega}, Y_1)}{\\partial \\theta} = \\frac{\\partial - log \\sigma (h_{\\theta} (x, Y_w, Y_1))}{\\partial \\theta}$\n$= \\frac{-1}{\\sigma(h_{\\theta}(x, \\zeta_w, \\zeta_1))} \\frac{\\partial \\sigma(h_{\\theta}(x, \\zeta_w, \\zeta_1))}{\\partial h_{\\theta}(x, \\zeta_w, \\zeta_1))} \\frac{\\partial h_{\\theta}(x, \\zeta_w, \\zeta_1))}{\\partial \\theta}$\n$= \\frac{-1}{\\sigma(h_{\\theta}(x, \\zeta_w, \\zeta_1))} \\sigma(h_{\\theta}(x, \\zeta_w, \\zeta_1))(1 - \\sigma(h_{\\theta}(x, \\zeta_w, \\zeta_1))) \\frac{\\partial [log \\pi_{\\theta}(y_w|x) - log \\pi_{ref}(y_w|x) - log \\pi_{\\theta}(y_1|x) + log \\pi_{ref}(y_1|x)]}{\\partial \\theta}$\n$= - f(x, y_w, Y_1) (\\frac{\\partial log \\pi_{\\theta}(y_w|x)}{\\partial \\theta} - \\frac{\\partial log \\pi_{\\theta}(y_1|x)}{\\partial \\theta})$\nGradient of the loss function of D3PO. To study the generative distribution in the denoising process of diffusion models, let x \u2252 (xt, c), y \u2252 xt\u22121, then we have\n$\\pi_{\\theta}(y|x) = \\pi_{\\theta}(x_{t-1}|x_t, C) = \\frac{1}{(2\\pi\\sigma_t^2)^{d/2}} exp(-\\frac{||x_{t-1} - \\mu_{\\theta}(x_t)||^2}{2\\sigma_t^2})$\nIn this case, the gradient of the loglikelihood log \u03c0\u03b8 (Xt\u22121 Xt, C) w.r.t. O is given as follows.\n$\\frac{\\partial log \\pi_{\\theta}(x_{t-1}|x_t, C)}{\\partial \\theta} = \\frac{\\partial \\frac{(-||x_{t-1} - \\mu_{\\theta}(x_t)||^2}{2\\sigma_t^2} - log((2\\pi\\sigma_t^2)^{d/2}))}{\\partial \\mu_{\\theta}(x_t)} \\frac{\\partial \\mu_{\\theta}(x_t)}{\\partial \\theta} = \\frac{(\\partial \\mu_{\\theta}(x_t))^{T} (x_{t-1} - \\mu_{\\theta}(x_t))}{\\sigma_t^2}$\nThen, we consider the gradient of the D3PO loss w.r.t. the model output \u03bc\u03b8.\n$\\frac{OL(x, x_1,x,x-1)}{\\partial \\theta} = -f_t( \\frac{\\partial log \\pi_{\\theta}(x_{t-1}^-|x_t^-, t,c)}{\\partial \\mu_{\\theta}(x_t)} - \\frac{\\partial log \\pi_{\\theta}(x_{t-1}^+|x_t^+, t,c)}{\\partial \\mu_{\\theta}(x_t)}) = -\\frac{f_t}{\\sigma_t^2} [(\\frac{\\partial \\mu_{\\theta}(x_t^l)}{\\partial \\mu_{\\theta}})^T (x_{t-1}^l - \\mu_{\\theta}(x_t)) - (\\frac{\\partial \\mu_{\\theta}(x_t^w)}{\\partial \\mu_{\\theta}})^T (x_{t-1}^w - \\mu_{\\theta}(x_t))]$\nSuppose $\\Delta \\theta = -L(x_{t-1}^l,x_{t-1}^w)$. After the update of $\\theta' \\leftarrow \\theta + \\Delta \\theta$, $\\Delta \\mu_{\\theta}(x_t) \\approx [(\\frac{\\partial \\mu_{\\theta}(x_t^w)}{\\partial \\mu_{\\theta}}) (\\frac{\\partial \\mu_{\\theta}(x_t^w)}{\\partial \\theta}) \\frac{(x_{t-1}^w - \\mu_{\\theta}(x_t))}{\\sigma_t^2}] - [(\\frac{\\partial \\mu_{\\theta}(x_t^l)}{\\partial \\mu_{\\theta}}) (\\frac{\\partial \\mu_{\\theta}(x_t^l)}{\\partial \\theta}) \\frac{(x_{t-1}^l - \\mu_{\\theta}(x_t))}{\\sigma_t^2}]$. If $x_t^w$ and $x_t^l$ are located in the same linear subspace of the model, i.e., $\\frac{\\partial \\mu_{\\theta}(x_t^l)}{\\partial \\mu_{\\theta}} \\approx \\frac{\\partial \\mu_{\\theta}(x_t^w)}{\\partial \\mu_{\\theta}}$, then the gradient can be written as follows.\n$\\frac{OL(x, x_1,x,x-1)}{\\partial \\theta} = -\\frac{f_t}{\\sigma_t^2} [(\\frac{\\partial \\mu_{\\theta}(x_t^w)}{\\partial \\mu_{\\theta}}) (x_{t-1}^l - \\mu_{\\theta}(x_t)) - (\\frac{\\partial \\mu_{\\theta}(x_t^w)}{\\partial \\mu_{\\theta}}) (x_{t-1}^w - \\mu_{\\theta}(x_t))]\n= -\\frac{f_t}{\\sigma_t^2} (\\frac{\\partial \\mu_{\\theta}(x_t^w)}{\\partial \\mu_{\\theta}})^T [(x_{t-1}^l-x_{t-1}^w) + (\\mu_{\\theta}(x_t) - \\mu_{\\theta}(x_t))]$\nSuppose $\\Delta \\theta = -\\eta \\frac{OL(x, x_1,x,x-1)}{\\partial \\theta}$. After the update of $\\theta' \\leftarrow \\theta + \\Delta \\theta$, $\\Delta \\mu_{\\theta}(x_t) \\approx (\\frac{\\partial \\mu_{\\theta}(x_t)}{\\partial \\theta}) \\eta = \\frac{L(x_{t-1},x_{t-1})^1}{\\partial \\theta} = -\\frac{f_t \\eta}{\\sigma_t^2} ((\\frac{\\partial \\mu_{\\theta}(x_t)}{\\partial \\mu_{\\theta}})^T (x_{t-1}^l-x_{t-1}^w) + (\\mu_{\\theta}(x_t) - \\mu_{\\theta}(x_t))))$\nGradient of our loss function. Then, we consider the gradient of our loss function w.r.t. the model output \u03bc\u03b8.\n$\\frac{OL(x_t, x_{t-1}^+, x_{t-1}^+)}{\\partial \\mu_{\\theta}} = -f_t (\\frac{\\partial log \\pi_{\\theta} (x_{t-1}^+|x_t, t, c)}{\\partial \\mu_{\\theta}(x_t)} - \\frac{\\partial log \\pi_{\\theta} (x_{t-1}^-|x_t, t, c)}{\\partial \\mu_{\\theta}(x_t)}) $\n=- \\frac{ft}{\\sigma_t^2} (\\frac{\\partial \\mu_{\\theta} (x_t)}{\\partial \\mu_{\\theta}})T (x_{t-1}^+-x_{t-1}^-) $\nSuppose $\\Delta \\theta = \u2013\\eta \\frac{\\partial L(x_t, x_{t-1}^+, x_{t-1}^+)}{\\partial \\theta}$. After the update of $\\theta' \\leftarrow \\theta + \\Delta \\theta$, $\\Delta \\mu_{\\theta}(x_t) \\approx (\\frac{\\partial \\mu_{\\theta}(x_t)}{\\partial \\theta}) \\eta = \u2013 \\eta \\frac{ft}{\\sigma_t^2} (\\frac{\\partial \\mu_{\\theta} (x_t)}{\\partial \\mu_{\\theta}} ) (x_{t-1}^+-x_{t-1}^-)$"}, {"title": "B TAILORPO AND TAILORPO-G", "content": "In this section, we provide another formulation for the loss function of TailorPO, and then discuss the difference between TailorPO and TailorPO-G from the perspective of gradient.\nFirst, Eq. (10) only shows a classic loss formulation of DPO, and does not reflect the preference selection procedure in TailorPO. To this end, we provide another formulation of the loss function, which incorporates the preference selection based on step-wise reward rt.\n$L(\\theta) = -E_{(c,x_t,x_{t-1},x_{t-1}^+) \\sim D} [log \\sigma ((-1)^{1(r_t(x_{t-1})<r_t(c,x_{t-1}))} . \\Delta)]$\nwith $ \\Delta = \\beta log \\frac{\\pi_{\\theta} (x_{t-1}|x_t, c)}{\\pi_{ref}(x_{t-1}|x_t, c)} - \\beta log \\frac{\\pi_{\\theta} (x_{t-1}^+|x_t, c)}{\\pi_{ref}(x_{t-1}^+|x_t, c)}$\nwhere 1(\u00b7) is the indicator function. The term (-1)^{1((x_{t-1}<rt(c,x_{t-1}))} represents the step-level preference ranking procedure.\nFurthermore, we compare TailorPO and TailorPO-G from the perspective of gradient, in order to understand their difference in effectiveness. In Eq. (11), we have shown that the gradient of the TarilorPO loss function can be written as follows.\n$\\nabla_{\\theta}L(\\theta) = -E [(\\frac{f_t}{\\sigma_t^2})\\cdot \\nabla_{\\mu_{\\theta}(x_t)}(x_{t-1}-x_{t-1}^+)]$\nFor TarlorPO-G, the term $x_{t-1}^+$ is modified by adding the gradient term $\\nabla_{z_{t-1}}log p(r_{high}|x_{t-1})$. Therefore, we can derive its gradient term as follows.\n$\\nabla_{\\theta}L_{TailorPO-G}(\\theta) = -E [(\\frac{f_t}{\\sigma_t^2})\\cdot \\nabla_{\\mu_{\\theta}(x_t)} ((\\nabla logp(r_{high}|x_{t-1}))+ x_{t-1}^+ -x_{t-1})]\n= -E[(\\frac{f_t}{\\sigma_t^2})\\cdot \\nabla_{\\mu_{\\theta}(x_t)} (\\nabla logp(r_{high}|x_{t-1})+(x_{t-1}-x_{t-1}^+))]$\nThe gradient term pushes the model towards the high-reward regions in the reward models. Therefore, TarlorPO-G further improves the effectiveness of TailorPO."}, {"title": "C EXPERIMENTAL SETTINGS AND ETHICS STATEMENT FOR THE USER STUDY", "content": "To verify that our framework generates more human-preferred images, we conducted a user study by requesting ten human users to label their preference for generated images from the perspective of visual appeal and general preference."}, {"title": "D MORE EXPERIMENTAL RESULTS", "content": null}, {"title": "D.1 EXPERIMENTS ON COMPLEX PROMPTS", "content": "We fine-tuned Stable Diffusion v1.5 on various reward models using 4k prompts in the training set of the Pick-a-Pic validation set (Kirstain et al., 2023), selected by Liang et al. (2024). We followed the same setting with Section 4 of the main text for TailorPO and TailPO-G. Then, we evaluated the fine-tuned model on 500 prompts from the Pick-a-Pic validation set. Table 5 compares our method with Diffusion-DPO (Wallace et al., 2024) and SPO (Liang et al., 2024)\u00b9. For these complex prompts, our methods also achieved the highest reward values. Visual demonstrations are shown in Figure 6."}, {"title": "D.2 VERIFICATION OF THE ESTIMATION FOR STEP-WISE REWARDS", "content": "In this section, we conducted experiments to verify the reliability of the estimation in Eq. (12) for step-wise rewards. We compared the estimated value $r(c, x_0(x_t))$ with $r_t(c, x_t) \\approx E[r(c, x_0)|C, x_t", "x_t": "as the ground truth of the step-wise reward. Then, we computed the estimated value $r(c, x_0(x_t))$ based on the fine-tuned parameters \u03b8'. Table 6 and Table 7 report the average relative error $E[\\frac{[", "xt))": ""}, {"c,xt)}": "at"}]}