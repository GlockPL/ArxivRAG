{"title": "Beyond Multiple-Choice Accuracy: Real-World Challenges of Implementing Large Language Models in Healthcare", "authors": ["Yifan Yang", "Qiao Jin", "Qingqing Zhu", "Zhizheng Wang", "Francisco Erramuspe \u00c1lvarez", "Nicholas Wan", "Benjamin Hou", "Zhiyong Lu"], "abstract": "Large Language Models (LLMs) have gained significant attention in the medical domain for their human-level capabilities, leading to increased efforts to explore their potential in various healthcare applications. However, despite such a promising future, there are multiple challenges and obstacles that remain for their real-world uses in practical settings. This work discusses key challenges for LLMs in medical applications from four unique aspects: operational vulnerabilities, ethical and social considerations, performance and assessment difficulties, and legal and regulatory compliance. Addressing these challenges is crucial for leveraging LLMs to their full potential and ensuring their responsible integration into healthcare.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have emerged as powerful tools in medical applications, offering unprecedented capabilities to process complex medical data, assist in decision-making, and streamline workflows1\u20136. Despite their immense potential, LLMs also present challenges that must be addressed to ensure their safe and effective integration into real-world clinical practice. These challenges range from technical issues such as hallucinations to ethical concerns around data privacy, fairness, and bias. As LLMs continue to being integrated into medical applications, it is essential to understand and address these challenges to utilize LLMs' capabilities effectively while minimizing potential risks.\nUnlike LLM applications in other domains, deploying LLMs in medical settings likely requires more caution because patients' lives are at stake. For instance, an erroneous recommendation from an LLM could lead to misdiagnosis or inappropriate treatment, resulting in death of patients\u201d. In addition to the technical challenges, deploying LLMs in medicine must also meet more stringent legal and regulatory requirements than general domains because medical applications directly impact patient safety, involve sensitive health data protected by privacy laws, and require compliance with strict standards for clinical accuracy and ethical responsibility to avoid harm or misdiagnosis.\nMany existing works either focus on summarizing various applications of medical LLMs\u00b2 and/or discuss one or two specific challenges and problems of LLMs in medicine9-13. Differently in this work, we aim to aggregate the challenges of applying LLMs from both general and medical domains, using medical-specific examples to provide a more comprehensive and relevant perspective."}, {"title": "Performance and evaluation challenges", "content": "Currently, most LLMs are evaluated on multi-choice questions (MCQs) 14-17 as a proxy for their medical capabilities, such as the United States Medical Licensing Examination (USMLE) subset of MedQA18, PubMedQA19, MedMCQA20, and medical subsets of MMLU21. These datasets are often used because the evaluation can be automatically performed by comparing the predicted answer choice with the ground truth at scale, without requiring any domain expertise. As can be seen in Figure 2, many LLMs have exhibited high performance on such benchmarks. However, there are significant limitations of evaluating LLMs using MCQs, which are unrealistic since no choices will be available in the real-world clinical setting.\nEvaluation frameworks that mimic physician-patient interactions, such as AgentClinic22 and Articulate Medical Intelligence Explorer (AMIE)23, represent a promising future direction to explore.\nAdditionally, there can be flaws hidden behind high MCQ scores, where the model predicts correct choices but presents wrong rationales13. Specifically, Jin et al evaluated the rationales of GPT-4 Vision for answering medical challenge questions from the NEJM Image Challenge13. They found that while GPT-4 Vision achieved expert-level performance measure by multi-choice selection accuracy, the model frequently presents flawed rationales even when it chooses the correct final answer. Such flawed rationales are most common in image comprehension, followed by step-by-step reasoning and the recall of medical knowledge, appearing in roughly 30% of the correct answers.\nWhile MCQ datasets do not reflect real-world tasks, they might still be useful as a screening tool \u2013 for example, if a model cannot even pass MedQA-USMLE with a 60% accuracy, the model might not be further considered for any downstream clinical evaluation. This is similar to the screening utility of medical examinations in real life.\nInstead of MCQs, real-world evaluations of LLMs on clinical utility often require open-ended questions and answers with real patient information as input24,25. In such evaluation scenarios, expert annotations are the ground truth, but obtaining them is time-consuming and labor-intensive, sometimes prohibitively expensive. Moreover, comparing human-annotated answers and LLM-generated text is a non-trivial task. Traditional automated metrics like BLEU26, ROUGE27 as well the semantic scores such as BERTScore28, which focus on word overlap or general meaning, fail to align well with expert judgments because they capture surface-"}, {"title": "Model Robustness and Generalization", "content": "Model generalizability is a machine learning model's ability to perform well on similar data that is from a different source 36. In medical Al, this is important because models may be deployed across diverse patient populations, imaging devices, and clinical settings. However, data heterogeneity poses a significant challenge: medical images can vary widely due to differences in equipment and imaging protocols\u00b37, as well as patient cohorts' characteristics like age, ethnicity, and health conditions38\u201340.\nTypically, models trained on one dataset demonstrate excellent performance on that specific dataset but often fail to generalize to other datasets with different characteristics41. General medical LLMs tend to excel in broad domains like MedQA18 but often underperform on specialized tasks. Meditron achieves 70.2% on MedQA, but achieves lower accuracy in identifying oligometastatic non-small cell lung cancer from radiology text42. Meditron's summarization performance is also lower on datasets such as MIMIC-CXR43, a radiology dataset of x-ray interpretation and reports, and MIMIC-IV44, an ICU dataset containing ultrasound, CT, and MRI reports45.\nThese results highlight the challenge that general medical LLMs face in handling specialized medical subdomains like radiology and ICU care. Hence, fine-tuning these models on domain-specific data is essential to enhance their performance in these specialized areas46,47. For example, in ophthalmology-related patient queries, a fine-tuned GPT-3.5 achieved a score of 87.1%, while Llama2-13b scored 80.9%, demonstrating that even smaller LLMs can perform well when specialized fine-tuning is applied48. Additionally, fine-tuned LLMs have successfully learned radiation oncology-specific information and generated physician letters in required styles, with clinical experts rating the benefit at 3.44 out of 449. These examples demonstrate the advantages of using fine-tuned models in various specialized medical domains."}, {"title": "Operational Vulnerability", "content": "When closely examining generated content by LLMs, one of the most concerning issues discussed in the literature is the occurrence of hallucinations, where LLMs generate content that is inaccurate, inconsistent, or completely fabricated11,52. For instance, Jin et al, showed that ChatGPT generated fake article titles and PMIDs in order to use them as evidence to support its answers53. Hallucinations in LLMs can be broadly classified into two categories: intrinsic and extrinsic52. Intrinsic hallucinations occur when the generated text directly contradicts the input data, such as producing inconsistent and inaccurate output when processing clinical notes\u00b9\u00b9. Extrinsic hallucinations refer to content that cannot be verified or refuted by the input source, which can occur when LLMs generate fabricated response when consulted for medical information53 or references for medical literature 54.\nIn medical applications, these hallucinations can have serious consequences, such as the misinterpretation of clinical trial results 11,53 or the misclassification of patient data 55,56. For instance, one recent study proposed to use GPT-4 to extract \"helmet status\" from patient clinical notes56. Although the model performed well in many cases, it exhibited hallucinations when it encountered negations like \"unhelmeted,\" resulting in classification errors. Furthermore, LLM can generate self-conflicting answers when responding to healthcare-related questions. Agarwal et al. showed that when asked \"Which foods cause the most allergies?\", GPT-3.5 initially identified \"fresh fruits and vegetables with high acidity\" but later recommended \"sticking to a diet of fresh, natural fruits,\" creating conflicting guidance 57.\nA primary challenge in mitigating hallucinations is the difficulty of verifying the accuracy of generated content, especially when the training data is incomplete or when access to key sources (e.g., clinical trials) is restricted by copyright or other limitations52. In such cases, LLMs may fill gaps with inferential content, leading to extrinsic hallucinations that undermine the reliability of the presented information. Ji et al. explore both intrinsic and extrinsic hallucinations in medical, where they highlight how LLMs tend to generate factually incorrect content when faced with incomplete or ambiguous data, often fabricating plausible sounding but unfaithful responses58. Similar concerns with hallucinations in medical"}, {"title": "Fairness and Bias", "content": "Fairness and bias are a major challenge for LLMs, with these models often reflecting and amplifying existing societal biases, such as those related to race, gender, and age. For example, in medical report generation, LLMs like GPT-3.5 and GPT-4 have been found to produce biased patient histories and racially skewed diagnoses, associating certain diseases disproportionately with specific racial groups 60. These biases may stem from the imbalanced or insufficient data used in training these models, leading to discrepancies in diagnostic outcomes and patient care. In biomedicine, such biases can exacerbate existing healthcare disparities, disproportionately affecting marginalized groups and contributing to unequal treatment. The inconsistency of LLM outputs also reveals the bias inherent to the model61. For example, while some responses correctly identified that race is a social construct with no genetic basis, other responses from the same model contradicted this, incorrectly suggesting that race reflects subtle genetic influences.\nRecognizing these issues, several studies emphasize the importance of using diverse evaluation methodologies and involving multiple stakeholders, such as physicians, health equity experts, and consumers, to identify biases that might otherwise remain undetected62\u201364. Pfohl et al. proposed EquityMedQA, a collection of adversarial datasets specifically designed to expose biases in LLM-generated responses to medical questions 62. Alongside this, they introduced a multifactorial framework for evaluating LLMs based on six dimensions of bias, including inaccuracy for certain demographic groups, stereotypical language, and the omission of structural factors driving health inequities.\nAddressing such biases in LLMs, particularly in biomedicine, is complex because bias can emerge at multiple stages\u2014from data collection to model deployment. This issue is particularly relevant in healthcare, where biased Al-driven decisions can worsen health outcomes for vulnerable groups. Techniques such as adversarial learning, data augmentation, and representation learning have been proposed to mitigate these biases, but they often come with trade-offs in model"}, {"title": "Ethical and Social Considerations", "content": "Recent research has revealed LLMs' concerning vulnerabilities to malicious manipulation that could potentially jeopardize patient safety and clinical decision-making practices. Both open source and proprietary LLMs are prone to manipulation in scenarios through targeted adversarial attacks12,68. These manipulative actions typically occur in two forms: crafted prompts (prompt-based attacks) and corrupted tuning data (fine-tuning attacks)12. By exploiting these techniques, malicious actors can manipulate LLM outputs to deliver targeted misinformation, and recommending incorrect or unnecessary medical procedures 12. Using adversarial statements to deliberately change the weights, modified LLMs generate misinformation such as incorrect maximum dosage of drugs or other medical information, potentially leading to organ injury and drug misuse, with attack success rate reaching up to 99.7%68. Commonly used commercial models, including GPT-4 and GPT-3.5-turbo, are vulnerable to both prompt-based and fine-tuning attacks, leading to significant changes in suggesting unnecessary medical tests. For example, Yang et al., showed that the attacked models increased their CT scan suggestions from 48.76% to 90.05%, and MRI suggestions from 24.38% to 88.56%12. In one case, the model even suggested an MRI for an unconscious patient with a pacemaker, potentially causing serious harm. Hidden prompt injection attacks in medical imaging can manipulate the outputs of visual language models (VLM) like GPT-40, with success rates as high as 70%, leading models to overlook critical conditions such as cancerous lesions during diagnosis69.\nThe impact of these vulnerabilities in clinical settings is significant, extending far beyond typical concerns seen in general domains. The smooth flow of language in LLMs increases the likelihood of this danger since they can create explanations for incorrect conclusions that might even deceive healthcare experts12,68. Moreover, multiple works find that models that have been compromised by attacks might not exhibit decreases in their overall performance when tested against standard medical benchmarks, which makes identifying them particularly difficult 12,68. As LLMs play a larger role in healthcare processes, from summarizing patient data to assisting with treatment choices, it is crucial to prioritize safeguard measures against malicious tampering. Prompt based attacks can be defended by making the system prompt of LLM applications transparent, reducing the likelihood of prompt injection by a third party. Fine-tuning attacks, however, currently lack reliable detection or robust defense methods. The best practice for"}, {"title": "Data Privacy and Security", "content": "As the development and implementation of medical LLMs often require private and domain-specific data, the use of LLMs in healthcare requires careful consideration of privacy and security needs. The misuse of healthcare LLMs, for example, could lead to unauthorized access to personally identifiable information (PII) or unintended consequences like membership inference, detecting a specific patient as a contributor to data. LLMs are also commonly deployed for consumers, through accessible interfaces including web applications. Though these consumer-facing LLMs enhance accessibility and engagement, they introduce additional privacy and security concerns. LLMs that interact directly with users who may input sensitive information pose an increased risk of exposing PII, especially if data handling and storage practices are not sufficiently secure.\nRegarding the deployment of LLMs in medical application, researchers have also emphasized the need for encryption, authentication, and access control mechanisms7\u00b9. Despite some proposed solutions, the privacy and security risks associated with LLMs in healthcare have contributed to hesitancy and caution among many 72,73. One of the concerns involves membership inference attacks (MIAs), where adversaries attempt to infer whether specific data was included in the model's training set74. Language Models trained on medical notes demonstrated that MIAs could reach an AUC of 0.90 when distinguishing whether"}, {"title": "Legal Considerations", "content": "Medical LLMs should comply with a range of existing laws that govern data privacy, intellectual property, and medical device regulations. For instance, in the U.S., the Health Insurance Portability and Accountability Act (HIPAA) mandates strict standards for protecting patient data, meaning that LLMs handling sensitive health information must be designed with robust data security measures to prevent unauthorized access or data breaches 76. Similarly, the General Data Protection Regulation (GDPR) in the European Union imposes requirements on data handling, including obtaining patient consent and ensuring data minimization, which can be particularly challenging for LLMs that rely on vast and diverse datasets for training77.\nAs LLMs may generate outputs that mirror copyrighted medical literature or clinical guidelines, this also raises concerns about potential violations of intellectual property laws78. Regulatory agencies like the U.S. Food and Drug Administration (FDA) have yet to clearly define whether LLMs fall under the category of medical devices, which makes it uncertain what safety and efficacy standards apply to their deployment. Given these legal complexities, it is essential that medical LLMs are developed with these regulatory challenges in mind to ensure compliance.\nThe integration of LLMs into clinical practice could introduce liability considerations for physicians, due to the challenges of LLMs discussed previously in this article and beyond. These issues can lead to erroneous medical decisions, raising concerns about malpractice liability. The lack of legal precedents for LLMs in healthcare further creates uncertainty in how courts will handle cases involving LLM-influenced decisions79. Because current law typically holds physicians liable only if they deviate from the standard of care and cause an injury result, the legal environment naturally discourages the use of LLMs and restricts them to a supplementary role, limiting their potential to improve care80. It has been suggested that physicians should use LLMs to supplement, rather than replace, their clinical judgment to mitigate these risks, as courts may scrutinize their reliance on LLM outputs when evaluating negligence claims81. Challenges in"}, {"title": "Conclusion", "content": "The growing integration of LLMs in healthcare holds significant potential for enhancing workflow, accuracy, and efficiency. However, acknowledging and addressing the challenges associated with LLMs is crucial for their safe and effective deployment in real-world medical settings. This article has outlined several key challenges, including operational vulnerabilities, ethical and social considerations, performance and assessment issues, and legal and regulatory compliances.\nAddressing these challenges will be essential for ensuring the responsible and safe use of LLMs in medical applications. By developing strategies that mitigate risks, improve reliability, and establish clear guidelines, the medical community can build trust and accountability in the use of LLMs, ultimately enabling LLMs' full potential to benefit patient care."}]}