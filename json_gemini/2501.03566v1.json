{"title": "Applying Large Language Models in Knowledge Graph-based Enterprise Modeling: Challenges and Opportunities", "authors": ["Benedikt Reitemeyer", "Hans-Georg Fill"], "abstract": "The role of large language models (LLMs) in enterprise modeling has recently started to shift from academic research to that of industrial applications. Thereby, LLMs represent a further building block for the machine-supported generation of enterprise models. In this paper we employ a knowledge graph-based approach for enterprise modeling and investigate the potential benefits of LLMs in this context. In addition, the findings of an expert survey and ChatGPT-40-based experiments demonstrate that LLM-based model generations exhibit minimal variability, yet remain constrained to specific tasks, with reliability declining for more intricate tasks. The survey results further suggest that the supervision and intervention of human modeling experts are essential to ensure the accuracy and integrity of the generated models.", "sections": [{"title": "1 Introduction", "content": "The utilization of Large Language Models (LLMs) in enterprise modeling has undergone a significant evolution in recent years, progressing from their initial role as a subject of academic research to their current status as a tool employed in industrial applications. Research has demonstrated the potential of LLM-based approaches, yielding impressive results across a range of use cases [18,3]. This has led to more complex investigations, e.g. including process mining techniques [4]. In the industrial context, tools such as ADONIS\u00b9 or SAP Signavio\u00b2 have introduced LLM-based artificial intelligence assistants to facilitate modeling activities. Another area of enterprise modeling where the use of LLMs appears promising is enterprise architecture modeling. The complex modeling languages employed in this domain, such as Open Group's ArchiMate, present"}, {"title": "2 Foundations", "content": "This section briefly introduces some foundations for our experiments in machine-supported enterprise model generation. This concerns the role of semantic in enterprise modeling, the relationship between large language models and semantics, and finally the current state of using large language models in enterprise modeling."}, {"title": "2.1 Enterprise Modeling and Semantics", "content": "Modeling the different aspects of enterprises has become a valuable task over the last decades and even more so in the context of digital transformation [36]. It aims at visualizing and formally representing enterprise structures and behaviors. Concepts captured in enterprise models include overviews on actors and their roles in the enterprise, business processes, information system landscapes, or application structures [41].\nIn the scope of this paper, enterprise modeling is seen as a sub-discipline of language-based conceptual modeling that bases on formal syntax in the form of a grammar, i.e. symbols and rules for their combination [43,17]. On top of syntax and symbols, semantics is defined as a mapping between elements of the grammar and a semantic domain [21]. Semantics can be further classified in type semantics and inherent semantics [25] \u2013 see Figure 13. Type semantics refers to the meaning of the elements of the modeling language, e.g. the concept of capabilities in business capability models. Inherent semantics refers to the meaning assigned at the time of instantiation to elements of the modeling language, e.g. typically in the form of labels and attribute values. Lastly, pragmatics relate to the context, goals and purposes, use and users, and effects of a modeling language and the models [49]."}, {"title": "2.2 Large Language Models and Semantics", "content": "Since the release of ChatGPT in 2022, Generative Pre-Trained Transformer (GPT) models have been widely adopted for various use cases. Especially Chat-GPT, with its specialization on conversations and artificial intelligence based responses to user input [9] has been applied in a wide range of private and business scenarios, leading quickly to over 100 million active users [11].\nGPTs are based on Large Language Models (LLM), which have also found wide acceptance in use cases such as image recognition, speech-to-text or text processing tasks [40,47]. In general, LLMs make experimentation with Artificial Intelligence more accessible due to their capability of specifying natural lan- guage prompts for triggering generations of text or images [46]. They thus seem optimally suited for tasks in language-based conceptual enterprise modeling.\nWhile traditional approaches in semantic mapping are mature, first approaches for LLM-based semantic mappings have emerged. For example, Wang et al. [44] developed an approach based on LLMs for biomedical concept linking. Their in- context approach follows a two stage procedure: at first the biomedical concepts are embedded into the overall context via a prompt and then similarity mapping is performed to get top candidates to match with an input concept.\nIn another use case, Hertling and Paulheim [23] developed an approach for concept matching in knowledge graphs. It addresses the problem that real-world objects may be contained in multiple knowledge graphs and one wants to deter- mine whether two objects are equivalent. Their approach uses open source LLMs to match candidate concepts from two different knowledge graph inputs using cardinality and confidence filters to improve result quality. As stated by the au- thors, the approach outperforms comparable approaches even though it is only based on natural language descriptions. They argue that semantics in knowledge graphs are typically described with either natural language in labels, comments or descriptions, relations in between concepts, or formal axioms. While in the past, the natural language semantics were only targeted at humans, LLMs now add powerful machine-processing capabilities for these natural language descrip- tions. The results so far showed that LLMs can lead to improvements in enter- prise modeling as well as in semantic concept mapping. Therefore, we will show in the next chapter how LLMs can be used in concept mapping for automated enterprise modeling. This will permit to evaluate which model element is most similar to a given real-world concept and to show the underlying explanation via the LLM."}, {"title": "2.3 Enterprise Modeling and Large Language Models", "content": "Recently, the application of LLMs in enterprise modeling has been explored. For example, Fill et al. [18] conducted experiments for investigating the capabilities of LLMs in the creation and interpretation of models in different enterprise contexts such as business process, systems, and data modeling. The authors concluded, that LLMs showed a huge potential for supporting modeling tasks with potential for improvement especially in terms of evaluation, in finding the right modeling languages and notations to use with LLMs, and in regard to the trade-offs between open source LLMs versus commercial ones.\nFurther on, H\u00e4rer [22] designed an architecture for generating PlantUML and Graphviz models based on LLMs in a conversational style. His study aimed at implementing a conceptual model interpreter for LLMs focussing on generating models with the correct syntax. He concludes that iterative modeling using GPT- 4 is generally possible in a conversational fashion.\nVidgof et al. [42] discuss the usage of LLMs in the Business Process Manage- ment lifecycle. They suggest the usage of LLMs in explaining business process models as a model chatbot to answer queries a user may have about a concrete model, or as process orchestrator.\nBarn et al. [3] investigate the adaptations that have to be made in enter- prise modeling languages for enabling prompt-based interactions. They develop a prompt engineering meta-model including domain concepts as well as modeling language elements for the 4EM method.\nWith a focus on software modeling, Camara et al. [8] investigated the ca- pabilities of ChatGPT in modeling UML by generating PlantUML code. They find that ChatGPT-based software modeling has limitations in terms of syn- tax, semantics, consistency, and scalability, especially when compared to code generation.\nFor better addressing the semantics of modeling languages, it is also being explored how knowledge graphs can be employed. Approaches in this regard in- clude three areas of research with a high potential for further evaluation, namely: (1) Knowledge Graph-enhanced LLMs, for improving the knowledge of LLMs during the pre-training and the inference phase, (2) LLM-augmented Knowledge Graphs, including LLMs for various tasks such as graph construction or question answering, and (3) Synergized LLMs + Knowledge Graphs, for bidirectional en- hancement of knowledge graphs and LLMs [30]. Luo et al. [28] argue that LLMs are skilled in reasoning in complex tasks, but struggle with up-to-date knowledge. Additional, they can lack from hallucinations in reasoning leading to negative impact in terms of performance and trustworthiness. Therefore, they developed the method reasoning on graphs to enable faithful and interpretable reasoning in LLMs. Finding the right balance between performance and efficiency is a key task for knowledge-based systems. LLMs could help to improve the performance for example in solving some of the knowledge-intensive sub tasks such as mention detection, entity disambiguation, or relation detection [24].\nThese early results show that knowledge graphs and LLMs can support each other in both directions. The use of knowledge graphs in the context of enterprise modeling could also have an impact on LLMs."}, {"title": "3 Integration of LLMs in Enterprise Modeling", "content": "In the following, three options for relating domain concepts with modeling lan- guage concepts will be explored: manual, knowledge graph-based, and LLM and knowledge graph-based. All options are described with reference to the exem- plary domain concept of Electronic Court Filing as part of the U.S. National Information Exchange Model (NIEM), which provides an open vocabulary for exchanging information between public and private organizations. Accordingly, the domain concept is to be mapped to the ArchiMate element designated Capa- bility. For all three options, we provide a description of the input, including the baseline information for the aforementioned concepts, the processing steps nec- essary for identifying a suitable relation, and the resulting output, as illustrated in Figure 2."}, {"title": "3.1 Manual", "content": "As previously outlined in Section 2, one of the illustrative examples is the mod- elling of an ArchiMate capability map viewpoint. A capability map is a tool that is typically employed to gain a structured overview of an enterprise's ca- pabilities. In the aforementioned viewpoint, three ArchiMate elements may be utilized: Outcome, Capability, and Resource. For example, a law firm may be confronted with evaluating the services offered in a service repository, such as NIEM for the justice domain. One of the service elements is Electronic Court Filing. An enterprise architect is consulted for the task and has to decide based on the natural language descriptions of the ArchiMate elements and the domain element Electronic Court Filing how to model it.\nIn NIEM, the term Electronic Court Filing is defined as: \"The LegalXML Electronic Court Filing 5.0 (ECF 5.0) specification consists of a set of non- proprietary XML and Web services specifications along with clarifying expla- nations and amendments to those specifications that have been added for the purpose of promoting interoperability among electronic court filing vendors and systems. ECF Version 5.0 is a major release and brings the specification into conformance with the National Information Exchange Model (NIEM) 4.0.\"5.\nIn the ArchiMate standard specification, element descriptions contain two parts: a short description and a more comprehensive one. The short description offers a brief overview of the concept, whereas the comprehensive description refers to the standards set forth in the ArchiMate standard. For the concept Capability, the short description is: \"A capability represents an ability that an active structure element, such as an organization, person, or system, possesses.\"6.\nThe two parts of the description are employed as inputs for the decision regarding instantiation.\nThe selection of the most appropriate ArchiMate element for a specific do- main concept, such as Electronic Court Filing, is typically based on a combina- tion of factors, including the descriptions, modeling language and business rules, experience, and the ability to address a stakeholder concern.\nThe modeling process, which considers the domain, model elements, and further context, results in the instantiation of a model element. For example, the Electronic Court Filing is an instantiated ArchiMate Capability as illustrated on the left side of Figure 1."}, {"title": "3.2 Knowledge Graph-based", "content": "The use of knowledge graphs (KGs) as a foundation for determining the in- stantiation of a domain concept as a modeling language element initially entails the adaptation of the format of the processed input. In the manual approach, the input descriptions of the domain element and ArchiMate elements were in the form of natural language. In contrast, in the KG-based approach, machine-processable formats are employed. For our example, an ArchiMate knowledge graph is employed, encompassing the ArchiMate modeling language, its con- stituent concepts, their interrelationships, and associated application rules. This is integrated with a NIEM enterprise KG, which includes the NIEM concepts."}, {"title": "3.3 LLM-based and KG-based", "content": "Next, the combination of LLM-based and KG-based approaches shall be dis- cussed. As outlined in section 3.2., they employ knowledge graphs as an input. The most significant alteration is the shift in processing methodology, moving away from semantic similarity approaches and towards the utilisation of LLMs for the assessment of domain concept instantiation within a modeling language. This also leads to the generation of the instantiated model element and the KG relation between the elements. In contrast to KG-based approaches, which typi- cally produce the same output format as the input, LLM-based approaches offer the option of generating formats such as JSON for subsequent processing. The use of KG-based inputs guarantees that the LLM processes curated and reliable knowledge sources, thereby ensuring the independence of the results from the training of the LLM. In terms of processing, the KG-based approach employs statistical measures based on the relation of the concepts in the KG, whereas LLM-based approaches utilize language-based probability. This results in altered outcomes, as LLM-based approaches do not incorporate a definitive measure of the relatedness between two elements; instead, they merely delineate it in natural language, as illustrated in Table 1. While KG-based approaches may necessitate preliminary processing to facilitate integration between two KGs and, thus, en- able statistical calculations, LLM-based processing does not require such steps, as it is capable of processing KG formats."}, {"title": "4 Empirical Evaluation", "content": "In order to examine the ability of large language models to instantiate a domain concept within a modeling language, we employed a two-pronged approach based on our previous work in this area [34]. First, an online survey was conducted with experts from various domains to establish a baseline for how human actors would complete the tasks. Subsequently, a series of experiments was conducted with ChatGPT-40. In both cases, domain concepts were observed in regard to ArchiMate elements and ranked according to a set of pre-defined criteria. The results were then investigated in terms of their proximity and relation types. This section outlines the general context of the investigations, followed by a detailed account of the survey and experimental procedures.\nIn the design of the general setting, five real-world use cases were selected for the framing of tasks. These cases were all situated within the domain of justice. For each of these use cases, one specific domain concept (Electronic Court Filing, Case Opening and Docketing, Court File, Court and Administration Mailbox, Participants) was identified as a requisite element for instantiation within an ArchiMate model. The model elements were constrained by the utilization of particular ArchiMate viewpoints, each of which sought to address a specific stakeholder concern and included a limited set of model elements.\nFor both approaches, the input was structured with a brief description of the functional context, a concrete description of the domain concept, the relevant ArchiMate viewpoint, and its associated model elements.\nFinally, the tasks were articulated: 1) Ranking of the ArchiMate elements in terms of their suitability to the domain concepts, from best to worst; 2) defining how close the relation between a domain concept and the ArchiMate concepts are considered, based on five values from very high to very low; 3) defining the relation type between domain concept and ArchiMate elements based on the relation types identical, similar, matches, related or none as defined in Table 1."}, {"title": "4.1 Expert Assessment", "content": "The survey was designed with the aforementioned considerations about the gen- eral setting in mind. The survey was constructed using LimeSurvey\u00b9. In the initial phase of the survey, a pre-test of the questionnaire was conducted, which served as the foundation for subsequent internal feedback and reviews. Follow- ing the incorporation of these insights, the final survey was constructed [26]. The participants were selected from academia, public and private industry [27]. Twelve experts responded to the survey. As illustrated in Fig. 4, their ArchiMate expertise ranges from first experience to experienced master.\nA preliminary data analysis and evaluation [31] was conducted using the internal tools of LimeSurvey and subsequently assessed. With regard to RQ1 ('prioritization of ArchiMate elements'), the experts demonstrated a high degree of clarity regarding the first two positions in the ranking. The percentage of experts ranged from 75% in Case 2 to 50% in Case 4 for the first rank and from 58% in Case 5 to 33% in Case 1 for the second rank. As the chosen ArchiMate viewpoints contained between three and five possible elements and the option to only include relevant elements in the ranking, a meaningful comparison of further ranks was not feasible. The option to refrain from selecting each and every element resulted in a range of 18% in Case 5 to 10% in Case 4 of unused elements in the rankings.\nIn regard to RQ2 ('probability of proposing an instance of an ArchiMate ele- ment'), the experts indicated a preference for the Very High, High and Very Low categories when asked to assess the likelihood of any of the potential viewpoint elements being instantiated. Conversely, the Low and Medium categories were perceived as less probable, although there was a degree of variation across all possible values. Figure 7 depicts the findings of RQ2 and RQ3, which sought to identify the optimal relationship between the ArchiMate elements of the view- points and the domain concept as outlined in Tab. 1. The most preferred relation was Related, followed by Similar and None."}, {"title": "4.2 LLM-based Assessment", "content": "To ensure the consistency of the results, the knowledge graph-based experiments were conducted 20 times on ChatGPT-40. The prompts were designed based on the aforementioned structure, but with the distinction that the context descrip- tion of domain, ArchiMate, and relation types were made accessible for the prompts through the use of knowledge graphs.\nSubsequently, the optimal prompting technique was explored. Three tech- niques were deemed feasible: 1) zero-shot prompting as a technique in which the task is based on natural language and entered in a single description at the time of inference. In this approach, no examples are provided [2]. 2) In few- shot prompting approaches, task examples are provided, including context and results, which support the LLM in its understanding [5]. 3) Chain-of-thought prompting, in which examples of the underlying thought process are provided, guiding the model to a series of reasoning steps necessary to reach the result [45]. To be consistent with the expert study, zero-shot prompting was chosen.\nFor each of the five use cases, the initial prompt delineated the context and posed the question for RQ1. Subsequently, prompts were presented for each of the questions for RQ2 and RQ3. To emulate the insights gleaned from the survey, all five use cases were conducted within the same chat."}, {"title": "5 Discussion", "content": "In general, the results demonstrated a certain degree of inconsistency and ambi- guity in the experts' opinions regarding the selection of the appropriate element for instantiation. ChatGPT exhibited a higher degree of accuracy, but often considered all elements to be relevant. In general, both experts and the LLM identified the same element as the most relevant. This demonstrates that LLMs are capable of selecting an appropriate element with minimal variation. How- ever, they exhibit a limitation in discerning irrelevant elements, which could potentially result in erroneous instantiation in more intricate domain descrip- tions. For instance, ChatGPT selected 25% of the irrelevant elements with a High probability of instantiation.\nThe results for RQ2 demonstrated a notable divergence from the judgments of human experts, particularly with regard to the assignment of Low and Very Low probabilities for the instantiation of ArchiMate elements. In light of the aforementioned inconsistency of ChatGPT with regard to irrelevant elements, it demonstrated remarkable consistency in mapping probabilities to specified ranks. All elements on rank 1 were specified to be Very High, while experts had just 75%. For rank 2 it was still 95% for ChatGPT and 57% for experts, while medium was 86% for ChatGPT and 41% for experts.\nThe experts and ChatGPT encountered difficulty with RQ3, where the ex- pected relation types should have been 'Related' or 'None'. 'Identical' requires two concepts to be the exact same thing, which was not given in this context. 'Similar' demands at least some shared properties, while 'Matches' refers to the same properties that can be substitutes for each other. The results indicate that experts selected narrower relations, such as 'Identical' and 'Similar', while ChatGPT favored lose relation types, such as 'Matches' and 'Related'.\nAs the relation types and their definitions are based on an ontological con- text [20], including formal definitions, the descriptions may lack sufficient clarity to be understood in an appropriate manner. Furthermore, a more comprehen- sive inquiry into the characteristics of relations, including their directionality, would facilitate the enhancement of the knowledge graph base. The results of the experiment demonstrate that, while ChatGPT performs the tasks with less variability than human experts, caution should be exercised when interpreting the results.\nThe remaining variability and inconsistencies let us conclude that, despite the precision with which LLMs can rank and instantiate, relying on them alone for modeling without the input of human modelers may be inadvisable, in par- ticular as the use cases were simplified for the experiments in comparison to real world scenarios. In conclusion, novel modeling approaches that integrate the strengths of LLMs in processing comprehensive data and proposing drafts of models, while leveraging human expertise for ensuring semantic correctness, are necessary. However, the challenge remains that a modeler must ultimately address the complexity of the utilized input sources. We can thus identify a number of limitations to inform future research. First, only structure-oriented viewpoints were selected so far, while flow-oriented viewpoints were excluded. Secondly, the meaningfulness of the results may be enhanced through a larger number of human expert participants. Thirdly, the discussion on reliability in more complex environments, as well as the relationship between modeling and domain concepts, could be more fully addressed through the use of more complex and detailed use cases."}, {"title": "6 Conclusion and Outlook", "content": "This paper examined the challenges and opportunities of using large language models in enterprise modeling. An experiment was conducted in which domain concepts and ArchiMate model elements as knowledge graphs were used as input for LLM-based modeling. As baseline, an expert survey was conducted. The results show that, although ChatGPT exhibited greater consistency than human experts, it still demonstrated variability and inconsistency. This indicates that human modelers are still necessary to validate the machine-created results. The conclusion and limitations indicate two areas for future research: 1) advanced experiments on the capabilities of LLMs in modeling, including more detailed use cases, and 2) developing a modeling process and demonstrating its conceptual and technical feasibility, integrating LLM capabilities with human expertise."}]}