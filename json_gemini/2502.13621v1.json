{"title": "Decentralized Planning Using Probabilistic Hyperproperties", "authors": ["Francesco Pontiggia", "Filip Mac\u00e1k", "Roman Andriushchenko", "Michele Chiari", "Milan \u010ce\u0161ka"], "abstract": "Multi-agent planning under stochastic dynamics is usually formalised using decentralized (partially observable) Markov decision processes (MDPs) and reachability or expected reward specifications. In this paper, we propose a different approach: we use an MDP describing how a single agent operates in an environment and probabilistic hyperproperties to capture desired temporal objectives for a set of decentralized agents operating in the environment. We extend existing approaches for model checking probabilistic hyperproperties to handle temporal formulae relating paths of different agents, thus requiring the self-composition between multiple MDPs. Using several case studies, we demonstrate that our approach provides a flexible and expressive framework to broaden the specification capabilities with respect to existing planning techniques. Additionally, we establish a close connection between a subclass of probabilistic hyperproperties and planning for a particular type of Dec-MDPs, for both of which we show undecidability. This lays the ground for the use of existing decentralized planning tools in the field of probabilistic hyperproperty verification.", "sections": [{"title": "1 INTRODUCTION", "content": "Decentralized planning under uncertainty. Markov decision processes (MDPs) are the ubiquitous model to describe sequential decision making (or planning) of a single agent operating in an uncertain environment: the outcomes of the agent's actions are determined by a probability distribution over the successor states. Decentralised MDPs (Dec-MDPs) naturally extend MDPs to the situation in which multiple agents are deployed in the same environment, each state of the environment generating a unique set of observations for the agents. Dec-MDPs can be seen as a special case of decentralised partially observable MDPs (Dec-POMDPs) [15], a classical model for decentralized planning under state uncertainty where the full information of the current state is not available to the agents. A classical synthesis task in Dec-(PO)MDPs is to compute for each agent a policy that maximises a given joint objective. Existing tool-supported approaches for Dec-(PO)MDPs typically handle only simple objectives such as time-bounded reachability or reward [38] or infinite-horizon (discounted) rewards [50].\nPlanning of multi-agent systems using temporal logics. In order to effectively formalise more complicated tasks and constraints, various temporal logics have been proposed in the context of multi-agent systems (MAS). The most prominent are Alternating Temporal Logic (ATL) [2] and Strategy Logic [21, 41]. Extensions deal with partial observability [13, 14, 16, 19], stochastic systems [3, 11, 22, 36, 37], and hyperproperties for MAS [17]. Belardinelli et al. [12] study model checking probabilistic variants of ATL on stochastic concurrent games with imperfect information. Despite their expressive power, most notably the ability to express coalitions or adversarial behaviours, these logics do not specialize to the particular type of partial observability arising in decentralized MAS.\nLinear Temporal Logic (LTL) [45] has too been used to specify objectives in decentralized planning under uncertainty [47, 52], but only via manually splitting the global goal into separate, agent-local goals.\nProbabilistic hyperproperties. In this paper, we propose a different approach for decentralized planning under uncertainty. The key idea is to use Probabilistic Hyperproperties (PHs) [1, 26] as the specification language. Hyperproperties [24] extend the traditional notion of trace properties by specifying requirements that put in relation multiple execution traces of a system at once. To this extent, they consider the self-composition of several copies of the same system, where the synchronised (step-wise paired) execution traces are analysed. For probabilistic systems, hyperproperties either relate the probability measures of different sets of traces (probabilistic computation trees) or constrain the joint probability of sets of computation trees [30]. PHs fit perfectly in a decentralized planning setting, where each different execution is interpreted as a different agent, and the hyperformula specifies a shared goal.\nOur approach is inspired by pioneering studies showing that PHs have novel applications in robotics and planning [4, 26, 48]. In order to formalise a decentralized multi-agent planning problem, we use an MDP to describe how a single agent operates in the environment and a PH to capture relative temporal constraints on the agents. We exemplify our approach using the following planning problem.\nMotivating example. Consider the 4x4 grid depicted in Fig. 1a, where obstacles are in orange. In every white cell, an agent can move in any of the four cardinal directions and will slip right from the selected course with a probability of 0.1 (see Fig. 1b). We also assume that each action can cause with some probability an ir-recoverable failure. Consider two agents, a\u2080 and a\u2081 that start in the bottom left and the upper left corners, respectively, and seek to reach the target location T: x = 2 & y = 1 with maximal probability (i.e. as soon as possible). Additionally, we require agent a\u2081 to reach the target first. The agents cannot communicate or observe the location of each other. We can see this problem as a more"}, {"title": "Contributions.", "content": "We propose a new logic, PHyperLTL, as a specification language for planning in decentralized, uncertain environments. We propose two approaches to construct decentralized policies satisfying PHyperLTL specifications that both rely on an automata-based synchronised product construction. For the full logic, we present an abstraction-refinement algorithm that constructs the optimising policy for the synchronised product MDP and attempts to factorise it into individual policies for each agent. If factorisation is not possible, we refine the abstraction to remove such a policy from the design space and continue the search.\nFor a fragment of PHyperLTL, the self-composition allows for translating it to a simple reachability problem for Dec-MDPs solvable by any off-the-shelf Dec-(PO)MDP solver. As a result, we establish a close connection between a subclass of probabilistic hyperproperties and decentralized planning. Using a novel undecidability proof for our logic, we obtain undecidability of infinite-horizon planning in locally fully observable Dec-MDPs and thus strengthen existing undecidability results. In the experimental evaluation, we demonstrate on a broad set of benchmarks that (i) PHyperLTL can succinctly capture complex temporal constraints over policies and their executions, (ii) the proposed synthesis algorithm can solve challenging planning problems, and (iii) the algorithm, in many cases, outperforms the state-of-the-art solver on Dec-MDPs."}, {"title": "Related work", "content": "Decentralized planning with complex shared objectives. There have been several attempts at finding formalisms for expressing objectives more general than reachability in decentralized planning. Neary et al. [43] use Mealy machines to encode a shared objective which is split into subtasks assigned to each agent, enabling decentralized reinforcement learning. They do not use logics, but rather automata, to express requirements.\nSchuppe and Tumova [47] use LTL over finite traces (LTLf) formulae to capture safety and liveness specifications for agents modelled as MDPs. They assume the global objective to be factored into individual agent tasks, plus a smaller task requiring their interaction. They use stochastic games to synthesize local policies separately. Such policies are not completely decentralized: the planner assumes that agents can share some limited information (called advisers) on their current state, violating local observability.\nLTL specifications have been considered in the context of reinforcement learning for Dec-POMDPs: Zhu et al. [52] propose a hierarchical approach that computes temporal equilibrium strategies via a parity game and encode the individual reward machines [43] used in the learning process.\nThese works propose different approaches to decompose the planning problem to tackle its high complexity. On the contrary, we perform centralized planning but tackle its complexity through abstraction refinement [5, 20]. Moreover, these approaches use logics to specify objectives, but aspects related to the multi-agent nature of the problem are expressed externally or in the underlying operational model because LTL is not sufficiently expressive.\nVerification of Hyperproperties. The deterministic hyperlogics HyperLTL and HyperCTL* [23] extend LTL and CTL* with path quantifiers that relate multiple execution traces of the same system. They can specify important security properties such as non-interference [40]. Beutner and Finkbeiner [18] show that model checking of a HyperLTL fragment can be cast as a nondeterministic planning problem for Qualitative Dec-POMDPs. HyperSL [17] is a variant of Strategy Logic that can relate multiple execution paths and was used for expressing planning objectives in MAS. In contrast to PHyperLTL, it does not support probabilistic reasoning and assumes perfect information.\nPHyperLTL can be seen as a probabilistic variant of HyperLTL, that replaces path quantifiers with probability operators and quantifies over policies. There are two other temporal logics for PHs: HyperPCTL [1] and PHL [26]. They have been devised mainly with the purpose of verification, while we designed PHyperLTL to conveniently specify planning problems. We compare them in Sec. 3.3. HYPERPROB [27] is an SMT-based tool for model checking probabilistic hyperproperties with limited scalability [4]. Two approaches tackle the high intractability of the problem: [28] proposes a statistical method for bounded HyperLTL model checking on MDPs based on sampling policies and traces, and [4] introduces an abstraction"}, {"title": "2 BACKGROUND", "content": "A distribution over a countable set A is a function \u03bc: \u0391 \u2192 [0,1] s.t. \u03a3\u03b1\u03b5\u0391 \u03bc(\u03b1)=1. The set Distr(A) contains all distributions on A. Single-agent planning problems in a fully observable stochastic environment can be represented as MDPs.\nDefinition 2.1 (MDP). A Markov decision process (MDP) is a tuple M = (S, Act, P, L) with a finite set S of states, a finite set Act of actions, a transition function P: S\u00d7Act \u2192 Distr(S), and a labelling function L: S \u2192 2AP where AP is a finite set of atomic propositions. We denote P(s, a, s') := P(s, a) (s'). A Markov chain (MC) is an MDP with |Act| = 1, denoted as (S, P, L) with P : S \u2192 Distr(S).\nNote that we do not specify initial states: these will be derived from the specification. A finite path of an MDP M is a sequence \u03c0 = s\u2070as\u00b9a\u00b9 ...sn where P(si, a\u00b9, si+1) > 0 for 0 < i < n. PathsM denotes the set of all finite paths of M.\nA deterministic policy (or controller, scheduler) is a function \u03c3: PathsM \u2192 Act. Policy \u03c3 is memoryless if the action selection only depends on the last state of the path, i.e. \u03c3 maps each state s \u2208 S to an action \u03c3(s). Policy \u03c3 for MDP M = (S, Act, P, L) induces an MC M\u03c3 = (PathsM, P\u03c3, L\u03c3) where for all paths \u03c0\u2208 PathsM ending with state s we set P\u03c3 (\u03c0, \u03c0as') = P(s, a, s') if a = \u03c3(\u03c0) and 0 otherwise, and L\u03c3 (\u03c0) = L(s).\nFor an MDP M and a policy \u03c3, \u03a1Mo (s = \u25c7T) denotes the probability of reaching from state s \u2208 S some state in a set T S of target states. Standard techniques [8] can efficiently compute policy \u03c3max that maximises this probability\u00b9.\nTo represent several agents acting in the same environment defined as an MDP, we introduce the MDP self-composition. In the following, given a set A, we denote its k-ary set product as Ak = X1 A, and its element tuples in bold face as a = (a1, . . ., ak).\nDefinition 2.2 (MDP self-composition). Given a positive integer m and an MDP M = (S, Act, P, L), its m-self-composition is an MDP Mm = (Sm, Actm, pm, Lm) where Pm : Sm \u00d7 Actm \u2192 Distr(Sm) is s.t. Pm (s, a, s') = \u03a0\u2081 P(si, ai, s), and Lm : Sm \u2192 (2AP)m where Lm (s) = (L(s1),..., L(sm)).\nIntuitively, m-self-composition Mm describes m independent instances (so-called replicas) of M that evolve simultaneously. The notions of paths, policies, and reachability probability trivially derive from those defined for MDPs. On the other hand, stochastic decentralized planning problems are usually modeled as decentralized POMDPS (Dec-POMDPs), where several agents jointly interact with a stochastic environment that they only partially observe. In this paper, we assume agents can fully observe their state but not the state of other agents. Thus, we introduce decentralized MDPs: the definition is derived from a standard Dec-POMDP definition (cf. [10, 15]) stripped of observations and rewards.\nDefinition 2.3. A factored, observation-independent, locally fully observable Decentralised MDP (Dec-MDP) with m agents is a tuple"}, {"title": "2.1 Temporal Logic", "content": "The syntax of LTL [9] is given as\n\u03c6 := \u03b1 | \u03c6 \u039b \u03c6 | \u00ac \u03c6 | \u039f \u03c6 | \u03c6U \u03c6 where a \u2208 AP\nfor a finite set of atomic propositions AP. We consider semantics over infinite traces, i.e., infinite sequences of subsets of AP. Intuitively, a holds in the current trace position if a is part of the subset of AP labeling it, Oq if q holds in the next position, and 41 U \u03c62 holds if 1 holds until 42 does. Propositional operators A and \u00ac have their usual semantics. We also use the standard abbreviations \u25c7 \u03c6 = T U q, and \u25a1 q = \u00ac \u00ac meaning that q will resp. eventually and always hold. An LTL formula o can be checked on a MC by building a deterministic Rabin automaton Ap that accepts exactly traces that satisfy \u03c6 [9].\nDefinition 2.4 (DRA). A deterministic Rabin automaton (DRA) is a tuple A = (Q, \u03a3, \u03b4, qo, Acc) where Q is a finite set of states, \u2211 is a finite input alphabet (\u2211 = 2AP when checking LTL formulae), \u03b4: Q\u00d7\u03a3 \u2192 Q is the transition function, qo \u2208 Q is the initial state, and Acc \u2264 2\u00ba \u00d7 2\u00ba is the Rabin acceptance condition.\nA run of A reading the infinite word a0a1 \u2208 \u2211 is a sequence of states p = q091 \u2208 Q such that 8(qi, ai) = qi+1 for all i \u2265 0. Run p is accepting if there exists a pair (L, K) \u2208 Acc such that states in L appear only finitely often in p, while those in K appear infinitely often. A accepts the language of all and only words whose run is accepting. Let PM (s = \u03c6) be the probability that the traces generated by labels of a MC M starting from a state s satisfy an LTL formula \u03c6. Computing PM (s\u25a1\u03c6) reduces to the reachability problem of a set of states UAcc in the synchronised product between M and A. We refer to [9] for more details."}, {"title": "3 PHyperLTL", "content": "In this section, we introduce the syntax and semantics of Probabilistic HyperLTL (PHyperLTL). The logic allows for expressing probabilistic hyperproperties on MDPs so that their model-checking problem can be easily interpreted as a planning problem."}, {"title": "3.1 PHyperLTL Syntax", "content": "Let Vpol and Vstate be sets of policy and state variables, respectively, and let M = (S, Act, P, L) be an MDP. Formulae in PHyperLTL are constructed with the syntax\n\u2203(\u00f4\u2081... \u00f4n). Q1\u015d1 \u2208 I\u2081 (\u00f4i) ... Qmsm \u2208 Im(\u00f4j). \u03c6\n\u03c6 := \u03c6^ \u00a2 | \u00ac | P(q) c"}, {"title": "3.2 Formal Semantics", "content": "Let n = |Vpol| be the number of policy variables. A policy assignment for Vpol is a vector \u2211 = ((\u00f41, 51), . . ., (\u00f4n, \u03c3\u03b7)), that assigns policies (\u03c3\u03b9... \u03c3\u03b7) to all variables (\u00f4\u2081... \u00f4n). We denote with \u2211(6) the policy assigned to variable \u00f4. Probability constraints are evaluated in the context of MDP M, a policy assignment \u2211, a sequence of m policies \u03b7, and a state s = (s1, ..., Sm) of the m-self-composition. We use () to denote the empty sequence and o for the concatenation operator. M satisfies formula \u2203 (\u00f4\u2081... \u00f4n). \u03a6iff there exists a policy assignment \u2211 such that M, \u03a3, (), () = \u03a6, where the satisfaction relation is defined as follows (we omit M, \u03a3 for clarity):\n\u03b7, \u03c2 \u039e\u2208 (\u00f4). \u03a6 iff / \u03b7\u03bf\u03a3(\u00f4), sos' = \u03a6[m + 1/\u015d]\ns'el\n\u03b7, s = V\u015d\u2208 (\u00f4). \u03a6iff \u03b7 \u03a3(\u2642), sos' = [m + 1/5]\ns'EI\n\u03b7, \u03c2 \u0395 1 \u039b \u03a62 iff n, s = $1 and n, s = $2\n\u03b7, \u03c2 \u03c6 iff n, s\n\u03b7, \u03c2 = P(\u03c6) c iff Pr({\u03c0\u03b5 \u03c9Paths(Mm, s) | \u039cm, \u03c0\u03ba\u03c6}) <c\nwhere m = |n|, and [m+1/\u015d] is the formula obtained by replacing every occurrence of a tagged proposition ps with the index m + 1; Mm is the MC generated by applying the m-tuple of policies \u03b7 to the m-self-composition of M, and wPaths(Mm, s) is the set of its infinite paths starting from state s. Given \u03c0 = s0s1... \u2208 \u03c9Paths(Mm, s), we set \u03c0\u00b2 = si, and n[i] = sisi+1 . . . . It is well-known that sets {\u03c0\u2208 wPaths(Mm,s) | \u039c,\u03c0 = \u03c6}, where q is an LTL formula,"}, {"title": "3.3 Related Logics", "content": "Policy quantification and P-operators are common to PHL [26], HyperPCTL [1], and PHyperLTL. PHL and HyperPCTL also offer alternation of policy quantifiers and comparison of P-operators. Quantifier alternation is useful for specifying adversarial problems, but in this paper we assume a cooperative environment. Moreover, it significantly complicates planning. Comparison of P-operators could be added to PHyperLTL by integrating the abstraction refinement approach by [4]. PHyperLTL and PHL allow for full LTL formulae in P-operators, a feature particularly suitable for planning objectives. HyperPCTL supports nested P-operators. PHyperLTL and PHL exclude them since they further complicate planning without offering significant benefits. PHyperLTL and HyperPCTL support state quantification, which is useful to express uncertainty in initial states of the agents. State quantification can be expressed in Probabilistic Hyper Logic (PHL) with nonprobabilistic constraints: PHyperLTL is thus strictly included in PHL."}, {"title": "3.4 Undecidability", "content": "Undecidability was proved for HyperPCTL [30] and PHL [26] by encoding the emptiness problem for probabilistic B\u00fcchi automata, which is undecidable [7]. The proof for HyperPCTL does not apply to PHyperLTL, because it employs a formula containing nested probability operators. The formula built in the proof for PHL contains some HyperCTL* parts, but it can be rewritten as a PHyperLTL formula containing, however, at least two probability constraints. In this section, we sketch an alternative proof that uses a PHyperLTL formula with just one probability constraint. We thus prove the undecidability of a smaller logic fragment, allowing us to obtain an undecidability result for locally fully observable Dec-MDPs in Sec. 5, which could not be easily obtained from the other proofs.\nTHEOREM 3.1. PHyperLTL model checking is undecidable.\nThe proof is based on a reduction of the emptiness problem for Probabilistic Finite Automata (PFA), which is undecidable [25, 32, 42], to model checking of a PHyperLTL formula on an MDP. A PFA can be seen as an MDP whose actions are input symbols, and it accepts finite strings (i.e., sequences of actions) for which it has a probability > \u03bb \u03b5 Q to reach an accepting state. The emptiness problem consists of deciding whether such a string exists.\nWe build an MDP that is the union of two disjoint MDPs M\u2081, that encodes the PFA, and M2, that is a deterministic finite-state automaton (FSA) with the same input alphabet as the PFA. A PHyperLTL formula O states the existence of two policies \u03c3\u2081 and 02, respectively controlling M\u2081 and M2, which are synchronised on the actions they"}, {"title": "4 PLANNING BY ABSTRACTION REFINEMENT", "content": "Assume an MDP M and a well-formed PHyperLTL formula containing n policy variables, m state variables and a single probability expression Pmax (4). In other words, we are interested in computing an n-tuple of policies that maximises the probability of satisfying an LTL formula o containing m state variables. The definition below presents the key device used to recast the satisfaction probability of q into a reachability probability.\nDefinition 4.1 (Synchronised product). Assume a Markov decision process M = (S, Act, P, L), an LTL formula q with m state variables and a DRA Ap = (Q, (2AP)m, d, q\u0ed0, Acc) accepting traces that satisfy q. The synchronised product of M and Ap is D(\u039c, \u03c6) = (Sm \u00d7 Q, Actm \u00d7 {a}, PD, LD), where a is a fresh action for transitions of Ap and for all s, s' \u2208 Sm, a \u2208 Actm, PD ((s, q), (a, a), (s', q')) = pm(s, a, s') where (q, Lm(s)) = q'. Lp(s,q) = Lm(s) for all s \u2208 Sm and q \u2208 Q.\nWe will denote D (M, q) simply as D whenever M, q are clear from the context. Intuitively, D is the product of the m-self-composition Mm with Ap. Each state tuple of D encodes a state of each of the m replicas of M modelling the m agents, and a state of Ap. The actions of D synchronise m actions of M with the (deterministic) update of Ap on m-tuples of labels of M. The theorem below asserts that computing the satisfaction probability of q on M is equivalent to computing a reachability probability on D. The proof follows directly from Def. 2.2, Def. 4.1 and [9].\nTHEOREM 4.2. Assume s \u2208 Sm and a policy \u03c3 \u03b5 \u03a3\u039c\u2122. Then\nP(Mm)o (s = \u03c6) = PDo' ((s, 8(90, Lm (s))) = \u25c7UA\u2084)\nwhere \u03c3' (\u03c0) = (\u03c3(\u03c0), a) for any path \u03c0, and UA, is the success set of Ap's acceptance condition."}, {"title": "4.1 Synchronised Product as an Abstraction", "content": "Assume the PHyperLTL specification from above where we seek an n-tuple of policies that maximises the probability of satisfying an LTL formula o containing m state variables. With standard methods for MDP model checking, we can compute policy \u014dmax maximising P(\u25c7UA\u2084) on D. \u014dmax maps state tuples (s1,..., $m, q) of D to action tuples (a1, ..., am, a). Under two conditions, max can be factorised into an n-tuple (01,..., \u03c3\u03b7) of policies for individual agents. Intuitively, a policy for D is a centralised policy that makes decisions while considering the states of all agents and thus may violate the local observability assumption. Additionally, ignores the fact that m might be larger than n, i.e. the case where multiple agents (replicas) must adhere to the same policy. The following definition formalises these concepts.\nDefinition 4.3 (Policy consistency). Let be a policy for D. We say that violates local observability if for some agent i there exist two states s and s' s.t. s\u2081 = s and\u2642(s)(i) \u2260 (s')(i). We say that violates policy bindings if for two agents i and j, i \u2260 j, bound to"}, {"title": "4.2 Abstraction Refinement", "content": "Assume we require\u00b2 P(\u03c6) > \u03bb. Since D is an abstraction, i.e. ED encompasses all policy tuples for M and q, the value V (max) of the maximizing policy for D is an over-approximation of the maximal value achievable by a policy n-tuple. If V (max) > \u03bb and \u014dmax is consistent, then it can be factorized into an n-tuple of policies that satisfy the property. If V (max) \u2264 1, then no n-tuple of policies can satisfy the property. If V (max > 1) and max is inconsistent, we optimistically obtain a policy by randomly resolving the inconsistencies in max, and check if the resulting policy n-tuple satisfies the threshold. If we fail, no conclusion can be deduced. Inspired by the abstraction refinement approach of [5, 20], we split D into sub-models, refining the abstraction and ensuring that the inconsistencies obtained for D are excluded from the refined model.\nTo split D, we inspect the inconsistent policy max. From Def. 4.3 it follows that there must exist two states s, s' and two (not necessarily different) agents i, j, bound to the same policy variable, for which s\u2081 = s'; and such that action selections a := \u014dmax(s) (i) and b := max(s')(j), a \u2260 b, violate local observability or policy binding. Let H be the set of all agents bound to the same policy variable as i and j. Splitting generates 3 submodels, where, for an arbitrary state s of D and agent k \u2208 H: D1 with Act(s) (k) = a; D2 with Act(s) (k) = b; and D3 with Act(s) (k) = Act(s)\\{a,b}.\nUpon splitting and obtaining a refined abstraction Di, its analysis proceeds as that of D. For optimality objectives, this yields a recursive anytime algorithm tracking the currently best result.\nCorrectness of the algorithm follows from the construction of D and, in particular, the synchronised product with DRA A\u03c6.\nTHEOREM 4.4. Abstraction refinement terminates and either returns a consistent tuple of memoryless policies \u03c3\u2081 ... \u03c3\u03b7 witnessing the specification or proves that no such tuple exists.\nComplexity. D\u2758 is doubly exponential in the length of q and singly exponential in m. Moreover, the number of (memoryless) policies is exponential in both n and the number of states in M and thus naive enumeration of policies is intractable. Indeed, Dec-MDP problems are NEXPTIME-hard [15], even for a finite horizon.\nBeyond memoryless policies. The presented algorithm analysing the synchronised product D can only find tuples of memoryless policies witnessing the specification. As we demonstrated in the motivating example, using memory might allow agents to satisfy specifications where simple behaviour described by memoryless policies is not enough, or to achieve better values when searching for the optimal behaviour. To enable the search for non-memoryless policies, we can unfold memory into the MDP M itself, allowing"}, {"title": "5 FROM PHyperLTL TO DEC-MDP PLANNING", "content": "We introduce PHyperLTLDEC, a fragment of PHyperLTL that can be reduced to Dec-MDP planning. It consists of all formulae in which i) each policy is associated with exactly one agent, ii) there is a single probability operator, and iii) all I\u2081... Im are singleton sets. PHyperLTLDEC formulae have the following syntax:\n\u03bb\n(01... \u00f4m). V\u00f4\u2081 \u2208 {31}(01)... V\u00f4m \u2208 {sm}(\u00f4m).P(\u03c6)\nGiven a PHyperLTLDEC formula O over an MDP M, it is easy to see that the synchronised product D described in Def. 4.1 is a Dec-MDP with m + 1 agents-one for each replica of M in the self-product, plus one for the DRA. Given s = ($1,..., Sm), the tuple of initial states associated to each policy variable in \u03a6, P(q) is equivalent to the probability of reaching the acceptance set of the DRA from s in the Dec-MDP. Despite the apparent simplicity of this connection, we are, to the best of our knowledge, the first to relate a class of probabilistic hyperproperties to Dec-MDP planning.\nThis fact has two main consequences. First, we can exploit existing planning algorithms for Dec-MDPs to model check the fragment. In Sec. 6, we compare our abstraction refinement algorithm from Sec. 4 to a state-of-the-art Dec-MDP planning tool.\nSecond, we derive a new undecidability result for a class of Dec-MDPs. Since a POMDP is a special case of Dec-MDP [15], the planning problem for general Dec-MDPs is known to be undecidable. The construction of D turns the verification of a PHyperLTLDEC formula on M into a planning problem for a locally fully observable Dec-MDP. So from Theorem 3.1 follows that planning is undecidable for Dec-MDPs even if they are locally fully observable:\nCOROLLARY 5.1. The planning problem for locally fully observable Dec-MDPs under the infinite-horizon total reward criterion is undecidable."}, {"title": "6 EXPERIMENTAL EVALUATION", "content": "We implemented the proposed abstraction refinement approach in the tool PHSYNT. The tool is built on top of PAYNT [6], a tool for verification approaches for families of MCs, using STORM [35] to solve MDP model-checking queries and SPOT [31] to generate Rabin automata for LTL formulae. The implementation and all the considered benchmarks are publicly available\u00b3. The evaluation focuses on the following questions:\n\u2022 Can PHyperLTL capture relevant planning specifications?\n\u2022 Can PHSYNT adequately solve such problems?\n\u2022 Can PHSYNT find even better solutions by searching for non-memoryless policies?\n\u2022 How does the performance of PHSYNT fare against state-of-the-art Dec-PMDP solver Inf-JESP [50] for PHyperLTLDEC?\nWe could not compare with the model-checking tool HYPER-PROB [27] because it does not support the self-composition."}, {"title": "6.1 Experimental setting", "content": "Benchmarks. We consider decentralized planning problems over a grid-world environment [34] with various obstacles. Agents move in the four cardinal directions (via four actions), and each action has a small probability of transitioning into an unintended neighbouring cell. We consider four variants of different dimensions and varying complexity of the specification. The particular specifications are described case by case later. In all models, we equip each action with a non-zero probability that the agent transitions to a trap state. This allows us to explicitly model discounting and thus conduct a fair comparison with Inf-JESP that currently does not support undiscounted specifications.\nMain tables. For every model, we report experimental results in Tables 1 and 2. Here |M| denotes the size of the underlying MDP, i.e. the state space of one agent. We report bounds on the values that can be achieved for the respective planning problem. A baseline is the value of a randomized policy (in each state, all agents uniformly pick one of the available actions), and an upper bound is given by solving the product MDP D, i.e. assuming a centralized planning problem where each agent has perfect information about other agents. The table further shows the size of the product |D| for memoryless policies (mem = 0) and for policies using 1-bit memory (mem = 1). For PHSYNT (and Inf-JESP in Table 1), we report the runtimes (in seconds) and the best values achieved. \u2020 indicates that PHSYNT was not able to explore the whole search space: we report the best value found within a time limit of 1 hour, and the time when the best value was found. When running the experiment with mem = 1, a value obtained for mem = 0 is given to the synthesiser as a reference point. The goal here is to see whether adding memory can help the synthesiser find a better value: \u2020 without the number in parentheses indicates that no better policy has been found.\nUsing Inf-JESP. Inf-JESP leverages point-based belief exploration methods, in particular, it repeatedly calls SARSOP [39] on sub-problems given as (single agent) POMDPs. Inf-JESP builds on randomised initialization and individual runs of the tool can vary massively. Inspired by the experimental evaluation of Inf-JESP presented in [50], we use 10 restarts and run the tool 30 times with an overall time limit of 2 hours. It means that we get at most 300 individual runs per experiment. We report the overall runtime of the experiment and the best value achieved."}, {"title": "6.2 Results", "content": "We present 6 planning problems, demonstrate their specification via probabilistic hyperproperties, and discuss the solutions found by our approach. The first three problems are within the fragment PHyperLTLDEC, so we will also evaluate these using Inf-JESP. All"}, {"title": "Meeting.", "content": "We consider a classical decentralized planning problem [33] where two agents, each starting from different initial locations s\u2081 and s2, seek to meet at a given target location T. Note that the two agents do not have the option to sit still upon reaching the target, and therefore the probability of both agents residing at the same location is not the product of individual reachability probabilities. In the following PHyperLTL formula, we require to maximise the probability of meeting, thus to meet as soon as possible (recall the transitions to the trap state)\n\u2203(102). V\u015d\u2081 \u2208 {$1}(01) \u00a5\u015d2 \u2208 {52}(82). Pmax(\u25ca (TT))"}, {"title": "Race.", "content": "We consider the specification from the motivating example (see the introduction) that can be seen as a probabilistic variant of the shortest path hyperproperty [48]. The race-3 variant reported in Table 1 considers three agents with a specific desired order of arrival. The obtained policies indicate again that often memoryless policies are good enough, depending on the initial locations of the agents that may already favour the intended winner of the race. For the bigger variants of race-3, PHSYNT fails to build the model for mem = 1 due to its size. On the other hand, the publicly available version of Inf-JESP cannot handle Dec-(PO)MDPs with more than"}, {"title": "Current state opacity.", "content": "Opacity with respect to the current state is a well-studied security/privacy planning requirement both in the deterministic [48", "51": "and in the probabilistic setting [4", "28": "."}, {"28": ".", "PHyperLTL": "n\u2203(102). V\u00f4\u2081 \u2208 {31}(81) V2 \u2208 {52}(02).\nPmax(\u00ac\u25a1 (act\u015d\u2081 = act\u015d2) \u2227 \u25a1 (reg\u015d\u2081 = reg\u015d\u2082) \u2227 \u25ca (T\u015d\u2081) ^ \u25ca (T\u015d\u2082))\nwhere act represents the action chosen by an agent in a state, and reg is the current region of the agent (intr"}]}