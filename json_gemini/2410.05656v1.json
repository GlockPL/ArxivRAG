{"title": "ON THE MODELING CAPABILITIES OF LARGE LANGUAGE MODELS FOR SEQUENTIAL DECISION MAKING", "authors": ["Martin Klissarov", "Devon Hjelm", "Alexander Toshev", "Bogdan Mazoure"], "abstract": "Large pretrained models are showing increasingly better performance in reasoning and planning tasks across different modalities, opening the possibility to leverage them for complex sequential decision making problems. In this paper, we investigate the capabilities of Large Language Models (LLMs) for reinforcement learning (RL) across a diversity of interactive domains. We evaluate their ability to produce decision-making policies, either directly, by generating actions, or indirectly, by first generating reward models to train an agent with RL. Our results show that, even without task-specific fine-tuning, LLMs excel at reward modeling. In particular, crafting rewards through artificial intelligence (AI) feedback yields the most generally applicable approach and can enhance performance by improving credit assignment and exploration. Finally, in environments with unfamiliar dynamics, we explore how fine-tuning LLMs with synthetic data can significantly improve their reward modeling capabilities while mitigating catastrophic forgetting, further broadening their utility in sequential decision-making tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) are generative models of natural language that can produce accurate general and domain-specific knowledge (Singhal et al., 2022; Imani et al., 2023; Manigrasso et al., 2024; Liu et al., 2024a), reason over long textual contexts (Reid et al., 2024), and generalize zero-shot (Kojima et al., 2022). These capabilities suggest that LLMs might be well-suited for complex sequential decision-making problems, such as in embodied settings where an agent acts in an environment. Recent research has begun exploring this potential, investigating how LLMs can serve as sources of intrinsic motivation (Wang et al., 2024; Klissarov et al., 2024), demonstrating world modeling capabilities (Lin et al., 2024; Liu et al., 2024b), and for acting and/or planning directly in an environment (Wang et al., 2023; Padalkar et al., 2023; Zhang et al., 2024b).\nHowever, as the predominant paradigm for training LLMs is not inherently aligned with the challenges of sequential decision-making problems, such as active exploration, it is not obvious how to best bridge their capabilities to tackle such challenges in a general manner. We study this problem through the lens of reinforcement learning (RL, Sutton & Barto, 2018), which formalizes how an agent interacts with an environment, receiving scalar rewards for each of its actions over a trajectory. We examine the capabilities of LLMs to solve RL tasks by comparing how they model policies 1) directly by generating action tokens, to 2) indirectly through a reward model derived from the LLM to be used within an RL algorithm. We perform a comprehensive evaluation on a diverse set of domains, including MiniWob (Liu et al., 2018), NetHack (K\u00fcttler et al., 2020), and Wordle (Lokshtanov &\nSubercaseaux, 2022), and MetaWorld (Yu et al., 2019). The environments we study present a variety of challenges, such as different action space granularities, observation modalities ranging from natural language to pixel data, and varying horizon lengths.\nWe first consider the off-the-shelf capabilities of LLMs for decision-making without updating them through additional gradient updates coming from the RL task. We find that indirectly modeling policies by first extracting knowledge from LLMs in the form of a Bradley-Terry model (Bradley &\nTerry, 1952; Christiano et al., 2017) provides the best and most consistent performance across the environments we study. We empirically analyze the various benefits, and limitations, provided by"}, {"title": "2 USING LANGUAGE MODELS TO SOLVE RL TASKS", "content": "We first introduce the types of RL problems as well as formalize the methodologies for using LLMs for RL tasks used in this work.\nReinforcement Learning. An RL task can be defined through a Markov Decision Process (MDP, Puterman, 2014), which is composed of a state space S, an action space A, a transition function p : S \u00d7 A \u2192 \u0394(S) which describes the forward dynamics of the system, a reward function r : S \u00d7 A \u2192 R and a discount factor \u03b3\u2208 [0,1]. Since it is often the case that the state is only partially observable, we also assume the environment emits an observation ot ~ po : S \u2192 \u0394(0) from observation space O. A policy, or actor, is a probability distribution \u03c0 : S \u2192 \u2206(A) which describes the action to be taken at every step. The objective of a rational actor is to maximize the expected cumulative rewards over horizon H > 0,\n$\\max_{\\pi} E_{S_{0}} [V^{\\pi}(s_{0})]$,\n(1)\nwhere the value function, $V^{\\pi}(s)$, represents the expected discounted sum of rewards over the entire trajectory, re-weighted by the environment's dynamics model, p, and the actor's policy, \u03c0.\nLarge Language Models. An LLM is a generative model of discrete random variables (i.e. tokens) conditioned on a history (i.e. context). The LLM models the data distribution autoregressively:\n$P(X_{t+1}|X_{1}, .., X_{t}) = \\prod_{t'=1}^{t} p(x_{t'} |X_{<t'}) = LLM(x_{<t}, l)$\n(2)\nwhere x \u2208 X are token variables taken from a valid vocabulary. The suitability of LLMs for solving RL tasks without additional fine-tuning primarily hinges on the hypothesis that LLMs contain information - i.e., knowledge \u2013 about the underlying MDP, for instance, through the policy or reward function. How that information is extracted depends on the data the LLM was trained on, the ability of the practitioner to properly prompt the model and interpret its responses to solve decision-making tasks."}, {"title": "2.1 PROMPTING", "content": "In this section, we describe the inputs, or prompts, to the LLM used in this work which allow to change the LLM's output distribution to be useful for solving RL tasks. All prompts in this work use 1) task specification using natural language as input to provide information about the MDP to the LLM as context and 2) episode history in order to address issues of partial-observability in some environments (similar to the Act-only baseline prompt found in Yao et al., 2022). We additionally use the following set of techniques,\n\u2022 Chain of Thought. By prompting the LLM to provide a step-by-step reasoning process for its output, rather than just the final answer, we can help surface its internal decision-making and improve the resulting performance (Wei et al., 2022)."}, {"title": "2.2 POLICY MODELING USING LLMS", "content": "As shown in Equation 1, the goal of a decision making agent is to learn a high performing policy \u03c0. This can be done either by maximizing the expected cumulative rewards and directly modeling the policy parameters (Sutton et al., 1999; Kakade & Langford, 2002). Equivalently, this can be done indirectly by first modeling the parameters of the value function and applying a greedy operator, such as in Q-Learning (Watkins & Dayan, 1992). A similar separation between direct and indirect approaches can be useful to study the capabilities of LLMs to model RL policies.\nDirect Policy Modeling. The most straightforward way to obtain a policy using LLMs is for the LLM to generate tokens that will be directly interpreted as actions from the environment, a \u2208 A (Yao et al., 2022; Shinn et al., 2023; Kim et al., 2024). To ensure the outputted actions adhere to the environment's action set, the LLM output tokens can be projected back onto A using projection operator proj(., A) (e.g., see Huang et al., 2022; Kim et al., 2024, for examples of projection operators). A variety of prompting techniques can be combined to increase the ability of the LLM to act, without task-specific fine-tuning, as a policy, which we detail in Section 2.1. This direct policy method will be referred to in our experiments as LLM Policy.\nIndirect Policy Modeling. On the other hand, we can prompt the LLM to output tokens representing intermediate quantities that will then be used to learn a policy. For example, one can model the forward dynamics of the environment for planning (Liu et al., 2024b), or an affordance model for action selection (Mullen Jr & Manocha, 2024). In this work, we focus on the case where these intermediate quantities will be used to generate rewards \u2013 i.e., a reward model \u2013 which will then be maximized by an off-the-shelf RL policy. In Section 2.3, we enumerate the different approaches for modeling reward functions with LLMs covered in our work.\nIn direct policy modeling experiments (LLM Policy), we found combining all of the prompting techniques in Section 2.1 to work the best, while for indirect modeling methods through reward we relied only on chain-of-thought prompting. Additional details, such specific prompt details and ablations on these choices are presented in the Appendix A.3."}, {"title": "2.3 INDIRECTLY MODELING POLICIES THROUGH REWARD MODELS", "content": "We consider a diversity of methods for modeling reward functions using LLMs, with a particular attention to methods that are applicable to a diversity of environments and modalities. We study the following set,\n\u2022 Direct Scalar. The LLM generates tokens that directly encode the reward (e.g., as a float or integer) given an observation (or a sequence of observations and actions). This reward is then given to the RL agent.\n\u2022 AI Feedback (Lee et al., 2023; Klissarov et al., 2024). Ask the LLM to express a preference y = {1,2, \u00d8} between two observations, 01 and 02, for the one showing the most progress to-wards a certain goal, or no preference if both observations are equally good. These labels can"}, {"title": "3 PERFORMANCE OF INDIRECT AND DIRECT POLICY MODELS", "content": "Due to fundamentally different challenges between direct and indirect policy modeling approaches, conducting a fair comparison requires care. For example, using the LLM directly as a policy requires grounding its outputs in the action space defined by the environment (Ahn et al., 2022; Huang et al., 2022). As the action space can vary significantly between environments and attempting to solve this problem adds additional algorithm- or domain-specific complexities (e.g. by crafting skills, see (Ahn et al., 2022; Wang et al., 2023)), we fix our experimental setting to the following\n1. Atomic actions. We only study approaches which can directly interface with the action space supported in the environment. In other words, the action space is at least a subspace of the space of language generated by the LLM. This allows for a more direct comparison across a variety of domains and study the relationship between an LLM's knowledge and the fixed action space defined by the environment.\n2. No finetuning. In most of the paper we assume that LLMs are used without any gradient updates, i.e. without fine-tuning from the RL task, and evaluate their off-the-shelf capabilities. In Section 5, we perform a preliminary study on the trade-offs between fine-tuning for direct and indirect policy modeling."}, {"title": "4 ANALYSIS OF AI FEEDBACK FOR RL", "content": "Our results so far suggest that, without additional fine-tuning, indirectly modeling policies by constructing reward functions through AI feedback is the most effective approach across the range of environments and modalities we studied. In this section, we examine how rewards shaped by this method can assist RL agents in addressing core decision-making challenges, such as credit assignment and exploration. Through this analysis, we also emphasize the ways in which reward misspecification can unintentionally arise and severely impair performance."}, {"title": "4.1 CREDIT ASSIGNMENT", "content": "AI feedback-based rewards depend on the prompt used to capture preferences. In the experiments conducted so far, these prompts were designed to elicit preferences by emphasizing states that contribute to task progress (see prompts Appendix A.2). Additionally, a key aspect of our methodology involved presenting the LLM with observations sampled randomly within trajectories. This enabled querying preference for any observation in the environment, rather limiting the focus to final states\na distinction also known as process-based and outcome-based reward models (Uesato et al., 2023;\nLightman et al., 2023). What are the resulting characteristics of the reward model under such choices?"}, {"title": "4.2 EXPLORATION", "content": "In the previous section, we investigated how our standard prompting strategy can ease the problem of credit assign-ment in downstream RL tasks. This outcome stemmed from the specific preferences we requested from the LLM, that is, promoting task progress. However, to address dif-ferent RL objectives, in particular the one of exploration, we may need to elicit alternative preferences.\nPreviously, Klissarov et al. (2024) employed AI feedback to design an effective reward function for an agent operat-ing in the open-ended environment of NetHack. However, before applying this reward to the RL agent, the authors implemented the following transformation:\nr(ot) \u00d7 rAIF(ot)/N(ot)\u03b2,\n(4)\nwhere TAIF is the reward model obtained from AI feed-back, N(ot) denotes the number of times a particular ob-"}, {"title": "5 BEYOND ZERO-SHOT REWARD MODELING", "content": "So far, we have explored the ability of LLMs to model policies, directly and indirectly, without any fine-tuning. However, in many cases the prior knowledge encoded in LLM might not contain the necessary information to do so successfully. In such instances, fine-tuning becomes an effective method for incorporating task-specific knowledge into the model."}, {"title": "6 DISCUSSION", "content": "In this paper, we explored two distinct approaches to leveraging LLMs for solving RL tasks: 1) directly, by modeling policies and 2) indirectly, by modeling rewards to be leveraged within a policy learning algorithm. Our results indicate that, without task-specific fine-tuning, current LLMs only show limited decision-making capabilities when directly generating actions. However, despite this limitation, LLMs are capable zero-shot reward modelers. In particular, when eliciting preferences to define rewards through the Bradley-Terry model, LLMs show strong performance across a wide range of domains presenting various challenges.\nIn cases where an LLM's prior knowledge is not enough to obtain useful reward functions, we also investigated fine-tuning with task-specific data to bridge this gap. Notably, fine-tuning to enhance reward modeling capabilities helps mitigate catastrophic forgetting, which is a crucial consideration for preserving the LLM's general-purpose abilities Maintaining these capabilities is essential for broad applicability to sequential decision-making tasks, including out-of-distribution tasks, and for supporting continued natural language interaction with users.\nThe reward modeling capabilities presented in this work offer potential solutions to challenges in RL. First and foremost, LLM-derived reward models alleviate the need for human-designed reward functions, which are often complex and costly to develop. Second, our empirical analysis reveals that AI-feedback based rewards produce dense functions which correlate positively with high-quality value functions. Such reward functions can significantly reduce the difficulty of assigning credit by redistributing rewards across different steps within a trajectory. Finally, distilling knowledge from LLMs into reward models opens new possibilities for applying RL in environments where simulators or symbolic features are unavailable\u2014such as embodied AI agents interacting with humans.\nSome notable limitations and caveats exist. For example, interacting with LLMs through natural language requires experimenting with various prompting techniques and specifications. However, this flexibility also enables the shaping of reward functions to incorporate valuable strategies (Knox et al., 2013), such as promoting exploration, which can further enhance the performance of RL agents."}, {"title": "A APPENDIX", "content": "A.1 ENVIRONMENT DETAILS\nIn our experiments, we investigate tasks from four different domains: MiniWob (Liu et al., 2018), NetHack (K\u00fcttler et al., 2020), and Wordle (Lokshtanov & Subercaseaux, 2022), and MetaWorld (Yu et al., 2019). The observation space for all these environments is text, except fro MetaWorld which consists of RGB pixels.\nIn the MiniWob domain, we sample the subset of the five tasks on which state-of-the-art results are low. Specifically, we carry experiments on: click-tab-2-hard, click-checkboxes-soft, count-shape, tic-tac-toe and use-autocomplete. To learn RL policies from LLM-based rewards, we leverage the experimental setup of Shaw et al. (2023). In NetHack, we use the same environment and the same algorithmic setup as in Klissarov et al. (2024). In Wordle, we build on the code made available by Snell et al. (2022) and use their proposed subset of 200 words from the official list of the game. Finally, in MetaWorld we study the same subset of environments presented in (Wang et al., 2024) consisting of drawer-open-v2, soccer-v2 and sweep-into-v2. Across all experiments where RL policies are learned, we use the original hyperparameter values defined in the respective experimental setups we are building upon.\nA.2 DETAILS ON INDIRECT POLICY MODELING THROUGH LLM-BASED REWARDS\nWe use the following prompt templates to query the agent for AI feedback, Scalar Reward and Reward as Code across various environments. For the Embedding-based approach, we use calculate the cosine similarity between the representation, provided by a BERT (Devlin et al., 2019) sentence encoder (specifically the same paraphrase-MiniLM-L3-v2 model) when environments are text-based, and otherwise we use the CLIP encoder (Radford et al., 2021). The similarity is measured between the current observation and the same goal description contained in the each of the following prompts given for the other baselines."}, {"title": "A.3 DETAILS ON DIRECT POLICY MODELING", "content": "We present the exact prompts used to query GPT-40 for each of the domains we have considered. These are presented through Prompt 13, 15, 14 and 16.\nAdditionally, in Figure 7, we ablate the prompting techniques used in our direct policy modeling approach. Results show that a combination of all prompting techniques presened in Section 2.1 works best."}, {"title": "A.4 ADDITIONAL INDIRECT POLICY MODELING METHODS", "content": "There are a number of other prompting methods for extracting information or knowledge from an LLM that may be relevant to solving RL tasks.\n\u2022 Direct State Generation. The model generates tokens that will represent next states (or other-future-time states). This is similar to world modeling. The next state prediction can be conditioned on an action, or marginalized over a policy distribution.\n\u2022 Action Preference. Ask the LLM to select, among two choices, the most likely action given previous and future observations.\n\u2022 State Preference. Ask the LLM to select, among two choices, the most likely next state or observation conditioned on prior history and/or actions.\nMany of the above could theoretically be used to construct a policy, yet a full implementation is out of scope from this paper due to the lack of available code-bases to build upon and we do not seek to build new algorithms from scratch. However, in Figure 2b we perform investigations into the capabilities of LLMs to perform Action Preference and State Preference. The results show that current LLMs struggle to achieve strong performance on any of these tasks. Additionally, in Table 1, we report the"}, {"title": "A.5 ABLATING REWARD AS CODE", "content": "In Table 2, we ablate the performance of the Reward as Code baseline across LLMs, observation spaces and additional assumptions. For pixel observations, we follow the methodology laid out in (Venuto et al., 2024), whereas for proprioceptive observations we follow the one from (Yu et al., 2023). Both methods heavily depend on access to a state-of-the-art, closed-source model to achieve performance comparable to that of AI Feedback, which uses the smaller, open-source model of Paligemma (Beyer et al., 2024). Additionally, each method requires expert demonstrations or specialized domain knowledge to guide the reward design process. While these assumptions may be viable in certain situations, such as in a controlled simulation environment, they can present"}, {"title": "A.6 LEARNING FROM ENVIRONMENT REWARDS", "content": "In Figure 8, we compare the performance of an RL agent trained using a reward function derived from AI feedback with that of an agent trained on human-designed rewards across different environments. We observe that AI feedback achieves comparable results, with an average score of 89.93 versus 86.3 for the human-designed reward. The objective of this experiment is not to argue that LLM-based rewards consistently outperform human-crafted ones-since expert human knowledge can always be encoded into a reward function\u2014but rather to contextualize the performance of LLM-based rewards. Notice that for MetaWorld we report the performance after fine-tuning the LLM as described in Section 5."}, {"title": "A.7 AI FEEDBACK AND HEURISTIC FUNCTIONS", "content": "While prior works have shown that rewards can be extracted from a language model (Brooks et al., 2024; Klissarov et al., 2024), it can be more generally thought of as encoding a heuristic function h. The function h contains high-level, multi-step information about the MDP M. To extract it, one can solve the re-shaped MDP M with \u0159(st, at) = r(st, at) + (1 \u2212 1)Est+1|st,at [h(st+1)] and \u1ef9 = \u03bb\u03b3 where \u03bb \u2208 [0, 1] Cheng et al. (2021). Solving M yields a policy \u03c0* that is also optimal in M - its value function's bias can be shown to converge to V* in M as a function of ||h - V*||\u221e.\nSpecifically, assume access to an initial dataset Do, from which a heuristic h can be computed. In the reshaped MDP M, one can learn a new policy \u03c0 which optimizes r with \u03bb \u2208 [0, 1]. Equation (5)"}, {"title": "A.8 ADDITIONAL CONSIDERATIONS FOR PREFERENCE-BASED REWARD MODELING", "content": "In Figure 9, we present the properties that were important to obtain effective exploration on NetHack, without the counting term shown in Equation 4."}, {"title": "A.9 IN-CONTEXT LEARNING FOR REWARD MODELING", "content": "In Figure 10, we present a variation on the Wordle game where the color code has been altered, which we refer to as Eldrow (reverse Wordle). Under this transformation, the off-the-shelf model provides feedback that correlates very poorly with the optimal value function. When we measure the perplexity"}]}