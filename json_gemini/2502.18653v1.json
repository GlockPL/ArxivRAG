{"title": "Enhancing Text Classification with a Novel Multi-Agent Collaboration Framework Leveraging BERT", "authors": ["Hediyeh Baban", "Sai Abhishek Pidapar", "Aashutosh Nema", "Sichen Lu"], "abstract": "We introduce a novel multi-agent collaboration framework designed to enhance the accuracy and robustness of text classification models. Leveraging BERT as the primary classifier, our framework dynamically escalates low-confidence predictions to a specialized multi-agent system comprising Lexical, Contextual, Logic, Consensus, and Explainability agents. This collaborative approach allows for comprehensive analysis and consensus-driven decision-making, significantly improving classification performance across diverse text classification tasks. Empirical evaluations on benchmark datasets demonstrate that our framework achieves a 5.5% increase in accuracy compared to standard BERT-based classifiers, underscoring its effectiveness and academic novelty in advancing multi-agent systems within natural language processing.", "sections": [{"title": "Introduction", "content": "Text classification is a fundamental task in natural language processing (NLP), with applications ranging from sentiment analysis and topic categorization to spam detection and intent recognition. Pre-trained models like BERT [Devlin et al., 2019] have achieved state-of-the-art performance in various classification tasks. However, challenges persist in handling ambiguous, domain-specific, and low-confidence predictions.\nTraditional approaches often rely on single-model architectures that may struggle with nuanced language patterns and context-dependent classifications. To address these limitations, we propose a novel multi-agent collaboration framework that integrates BERT with specialized agents to enhance classification accuracy and robustness. Our framework employs a threshold-based escalation mechanism, where low-confidence predictions from BERT are forwarded to a multi-agent system comprising Lexical, Contextual, Logic, Consensus, and Explainability agents. This collaborative process enables comprehensive analysis and consensus-driven decision-making, leading to significant performance improvements.\nOur contributions include:\n\u2022 Innovative Multi-Agent Framework: A generalizable system that integrates BERT with specialized agents to enhance text classification accuracy.\n\u2022 Dynamic Threshold-Based Escalation: A mechanism that intelligently escalates low-confidence predictions to a multi-agent system for further analysis.\n\u2022 Comprehensive Empirical Evaluation: Demonstrating a 5.5% increase in accuracy across multiple benchmark datasets compared to standard BERT-based classifiers.\n\u2022 Academic Novelty: Introducing a structured multi-agent collaboration approach in text classification, advancing the state-of-the-art in multi-agent systems within NLP.\nThe remainder of this paper is organized as follows: Section 2 reviews related literature, Section 3 details our methodology, Section 4 describes the experimental setup, Section 5 presents the results, Section 6 discusses the implications, and Section 7 concludes the paper."}, {"title": "Related Work", "content": "Text classification has been extensively studied, with models like BERT [Devlin et al., 2019, Kora and Mohammed, 2023] setting new performance benchmarks. Ensemble methods [Dietterich, 2000] and multi-model architectures [Kim, 2019] have been employed to enhance classification accuracy. However, these approaches often involve static model combinations without dynamic interaction mechanisms.\nMulti-agent systems have shown promise in various domains [Guo et al., 2024, Talebirad et al., 2023], enabling specialized agents to collaborate on complex tasks. Frameworks like CAMEL [Li et al., 2023] and multi-agent debate strategies [Du et al., 2023] demonstrate the benefits of role-based agent collaboration for reasoning and decision-making. Further, multi-agent systems balance the dynamic interplay between autonomy and alignment across various aspects inherent to architectural viewpoints such as goal-driven task management, agent composition, multi-agent collaboration, and context interaction [H\u00e4ndler, 2023].\nOur work distinguishes itself by integrating a multi-agent collaboration framework specifically tailored for text classification, leveraging BERT as the primary model and introducing specialized agents to handle low-confidence predictions dynamically. This approach not only improves accuracy but also enhances interpretability and robustness, addressing gaps in existing literature."}, {"title": "Methodology", "content": "Our proposed framework integrates BERT with a multi-agent system to enhance text classification accuracy and robustness. The system operates in two primary stages: initial classification and multi-agent collaboration for low-confidence predictions."}, {"title": "Workflow Overview", "content": "The workflow of our generalizable multi-agent text classification framework is illustrated in Figure 1."}, {"title": "Primary Classification with BERT", "content": "1. Data Input: The system receives input text from various sources, such as emails, chat messages, documents, or social media posts.\n2. Initial Classification with Primary Model: Each input text is processed by a primary classification model (e.g., BERT), which assigns a label along with a confidence score.\n3. Threshold Evaluation: If the confidence score of the primary model's classification is above a predefined threshold \\(T\\), the classification is accepted. Otherwise, the input is flagged for further analysis.\n4. Multi-Agent Collaboration for Low-Confidence Cases: Inputs with confidence scores below \\(T\\) are passed to a multi-agent system where specialized agents engage in a collaborative conversation to determine the correct label. This system is specifically designed to address false-positive predictions generated by the BERT model. Through iterative feedback from a large language model (LLM), the agents refine these predictions, resulting in enhanced overall accuracy.\n5. Decision Aggregation: The multi-agent system aggregates the outputs from various agents to arrive at a consensus classification.\n6. Output: The final classification is produced and can be used by downstream applications. Feedback from the outcome can be used to further train and improve the agents.\nWe employ BERT [Devlin et al., 2019] as the primary classifier due to its strong contextual understanding capabilities. For each input text \\(x_i\\), BERT assigns a label \\(y_i\\) along with a confidence score \\(c_i\\). If \\(c_i \\geq \\tau\\), where \\(\\tau\\) is a predefined confidence threshold, the classification is accepted. Otherwise, the input is escalated to the multi-agent system for further analysis."}, {"title": "Threshold-Based Escalation", "content": "The threshold-based mechanism ensures that only ambiguous or uncertain predictions are escalated, optimizing computational resources and focusing collaborative efforts where they are most needed. Mathematically, this can be expressed as:\nIf \\(c_i > \\tau\\), then accept \\(y_i\\) as the final label.\nElse, escalate \\(x_i\\) to the multi-agent system for further analysis."}, {"title": "Multi-Agent Collaboration", "content": "When a prediction falls below the confidence threshold, the input is processed by a multi-agent system comprising the following specialized agents:\n\u2022 Lexical Agent: Analyzes keywords and phrases to suggest potential labels based on lexical patterns.\n\u2022 Contextual Agent: Considers broader context, including historical data and situational factors, to refine label suggestions.\n\u2022 Logic Agent: Applies rule-based reasoning and domain-specific knowledge to infer the most probable label.\n\u2022 Consensus Agent: Aggregates the insights from other agents to reach a final classification decision.\n\u2022 Explainability Agent: Generates explanations for the classification decision to enhance interpretability."}, {"title": "Agent Interaction Protocol", "content": "Agents communicate iteratively, sharing insights and refining their analyses based on each other's input. This structured collaboration ensures comprehensive evaluation of low-confidence predictions. The interaction can be modeled as a graph where each agent is a node, and the edges represent communication channels."}, {"title": "Example Agent Conversation", "content": "To illustrate the collaborative process, consider the input text: \"I need more information about the order process.\" Suppose BERT assigns this to the \"Information Request\" label with a confidence score of 0.65 (\\(\\tau = 0.7\\)). The multi-agent system engages in the following detailed conversation:\nLexical Agent: \"The input contains keywords like 'more informa- tion' and 'order'. Based on my analysis, this suggests an 'Information Request' or 'General Inquiry'. Confidence in my analysis is 0.70.\"\nContextual Agent: \"Considering previous communication, the user has been asking about order-related issues. Given the context, the user might specifically be seeking more details regarding a process. I lean towards 'Information Request'. My confidence level is 0.75.\"\nLogic Agent: \"Using domain-specific rules, 'more information' paired with 'order process' typically maps to 'Information Request'. Additionally, the structure of the sentence indicates a formal inquiry. I agree with the others, and my confidence is 0.80.\""}, {"title": "Detailed Agent Conversation", "content": "Consensus Agent: \"Three agents suggest 'Information Request'. Based on confidence aggregation, the final decision is 'Information Request' with an overall confidence of 0.75.\"\nExplainability Agent: \"The classification is based on the presence of specific keywords related to information seeking and the contextual analysis of previous user interactions indicating a need for detailed process information. The logical structure of the request further supports this classification.\""}, {"title": "Mathematical Formulation of Agent Collaboration", "content": "To further illustrate the collaborative process, consider the following detailed interaction among the agents for the input text: \u201cI need more information about the order process.\u201d Suppose BERT assigns this to the \"Information Request\" label with a confidence score of 0.65 (\\(\\tau = 0.7\\)). The multi-agent system engages in the following conversation:\nLexical Agent: \"The input contains keywords such as 'more information' and 'order'. These suggest a need for details or clarification. Based on lexical patterns, possible labels are 'Information Request' or 'General Inquiry'. My confidence in 'Information Request' is 0.70.\"\nLet \\(A = \\{A_1, A_2, A_3, A_4, A_5\\}\\) represent the set of agents: Lexical (\\(A_1\\)), Contextual (\\(A_2\\)), Logic (\\(A_3\\)), Consensus (\\(A_4\\)), and Explainability (\\(A_5\\)). Each agent \\(A_j\\) provides a label suggestion \\(y_j\\) with an associated confidence score \\(c_j\\).\nThe Consensus Agent (\\(A_4\\)) aggregates these suggestions to determine the final label \\(y_{final}\\) and its confidence \\(c_{final}\\) as follows:\n\\[y_{final} = \\underset{y}{\\text{arg max}} \\sum_{j=1}^{M} \\delta(y_j = y) \\cdot w_j \\cdot c_j\\]\n\\[c_{final} = \\frac{\\sum_{j=1}^{M} \\delta(y_j = y_{final}) \\cdot w_j \\cdot c_j}{\\sum_{j=1}^{M} w_j}\\]\nwhere:\n\u2022 \\(M\\) is the number of collaborating agents (in this case, 3: Lexical, Contextual, Logic).\n\u2022 \\(\\delta(y_j = y)\\) is the indicator function that is 1 if agent \\(j\\) suggests label \\(y\\) and 0 otherwise.\n\u2022 \\(w_j\\) is the weight assigned to agent \\(j\\) based on its reliability or historical performance.\nThis weighted aggregation ensures that more reliable agents have a greater influence on the final decision."}, {"title": "Mathematical Justification for Improvement", "content": "The improvement in classification performance arises from the multi-agent collaboration framework's ability to leverage diverse perspectives and specialized analyses. Mathematically, this can be understood through ensemble learning principles, where combining multiple models typically results in better generalization and robustness.\nLet \\(f_{BERT}\\) represent the primary BERT classifier and \\(f_{MA}\\) represent the multi-agent collaboration system. The overall classification function \\(f\\) can be defined as:\n\\[f(x) = \\begin{cases} f_{BERT}(x) & \\text{if } c_{BERT}(x) > \\tau \\\\ f_{MA}(x) & \\text{otherwise} \\end{cases}\\]\nThe expected accuracy \\(E[Accuracy]\\) of the combined system can be expressed as:\n\\[E[Accuracy] = P(c_{BERT}(x) \\geq \\tau) \\cdot E[Accuracy_{BERT} | c_{BERT} \\geq \\tau] + P(c_{BERT}(x) < \\tau) \\cdot E[Accuracy_{MA} | c_{BERT} < \\tau]\\]\nGiven that\n\\[E[Accuracy_{MA} | c_{BERT} < \\tau] > E[Accuracy_{BERT} | c_{BERT} < \\tau],\\]\nthe overall expected accuracy of the system increases compared to using BERT alone.\nFurthermore, the robustness against adversarial examples is enhanced as the multi-agent system can cross-verify and validate predictions, reducing susceptibility to perturbations that might fool a single model."}, {"title": "Experiments", "content": "To evaluate the effectiveness of our proposed multi-agent collaboration framework, we conducted experiments on multiple benchmark text classification datasets, including sentiment analysis, topic categorization, spam detection, and intent classification. We compared our framework against baseline models, including standard BERT-based classifiers and ensemble methods."}, {"title": "Datasets", "content": "\u2022 Sentiment Analysis: The IMDb dataset [Maas et al., 2011] consists of 50,000 movie reviews labeled as positive or negative.\n\u2022 Topic Categorization: The AG News dataset [Zhang et al., 2015] contains 120,000 news articles categorized into four topics: World, Sports, Business, and Sci/Tech.\n\u2022 Spam Detection: The SMS Spam Collection dataset [Aloisio et al., 2011] includes 5,574 SMS messages labeled as spam or ham.\n\u2022 Intent Classification: A custom dataset comprising 10,000 organizational communication sentences categorized into five intents: Information Request, Action Directive, Expression of Concern, Feedback Provision, and General Inquiry."}, {"title": "Baselines", "content": "We compared our framework against the following baselines:\n\u2022 Standard BERT: A single BERT-based classifier fine-tuned on each dataset.\n\u2022 BERT Ensemble: An ensemble of BERT classifiers using majority voting.\n\u2022 Existing Multi-Agent Systems: Referencing frameworks like CAMEL [Li et al., 2023] for comparison."}, {"title": "Implementation Details", "content": ""}, {"title": "Agent Design", "content": "Each agent within our framework is designed with specific functionalities to contribute to the overall classification process:\n\u2022 Lexical Agent: Utilized a keyword map tailored to each dataset's labels, with a precision threshold of 0.7, and leveraged advanced NLP capabilities for context-aware communication [Zelenko, 2005].\n\u2022 Contextual Agent: Simulated contextual analysis based on historical data, considering the last 5 interactions per user, and employed diverse architectures such as BERT, RoBERTa, and XLNet [Liu et al., 2019, Yang et al., 2019], integrating reinforcement techniques [Shinn et al., 2023].\n\u2022 Logic Agent: Applied regex-based rules specific to each classification task, maintaining a rule set with 50 rules, while combining symbolic reasoning with neural networks [Madaan et al., 2023].\n\u2022 Consensus Agent: Aggregated agent outputs using weighted voting, assigning weights based on individual agent accuracies and confidences.\n\u2022 Explainability Agent: Generated textual explanations summarizing agent contributions, utilizing template-based responses, while also providing detailed explanations for classification decisions."}, {"title": "Computational Resources", "content": "Experiments were conducted on servers equipped with NVIDIA Tesla V100 GPUs and 32 GB RAM, and all models were implemented using Hugging Face's transformers library (version 4.12.3; [Wolf et al., 2021]). We fine-tuned the 'bert-base-uncased' model for 5 epochs with a learning rate of 2 \u00d7 10-5 and a batch size of 32. Agent pruning techniques reduced the computational load by approximately 15%, while the addition of multi-agent collaboration introduced a 10% increase in runtime, which is justified by the gains in accuracy and robustness.\nWe employed the Adam optimizer with \\(\\beta_1 = 0.9\\) and \\(\\beta_2 = 0.999\\), and utilized a linear learning rate scheduler with warmup steps set to 10% of the total training steps."}, {"title": "Evaluation Metrics", "content": "We evaluated models using the following metrics:\n\u2022 Accuracy: Correct predictions over total predictions.\n\u2022 Precision, Recall, F1-Score: To assess performance on each classification category."}, {"title": "Results", "content": "Our multi-agent collaboration framework consistently outperformed baseline models across all evaluated text classification tasks. Table 1 summarizes the performance metrics for each model and dataset."}, {"title": "Accuracy Improvement", "content": "Our framework achieved an average accuracy improvement of approximately 5.5% across all datasets compared to the standard BERT classifier. Specifically, in the Intent Classification task, our approach improved accuracy from 89% to 94.5%, demonstrating a substantial enhancement."}, {"title": "Robustness Enhancement", "content": "The multi-agent system enhanced the model's robustness against adversarial examples, with robustness scores increasing by 15%. For instance, in Topic Categorization, robustness improved from 0.72 to 0.88. The robustness score represents the F1 score computed on synthetic data generated through data augmentation, providing a reliable metric to measure model reliability and consistency."}, {"title": "Efficiency Considerations", "content": "While integrating the multi-agent system introduced a slight increase in runtime (approximately 10%), the trade-off between computational cost and performance gains is favorable. The enhanced accuracy and robustness justify the additional computational resources required."}, {"title": "Statistical Significance", "content": "To validate the significance of our results, we conducted paired t-tests comparing our framework against the standard BERT model across multiple runs. The improvements in accuracy and robustness were found to be statistically significant with p-values < 0.01."}, {"title": "Ablation Studies", "content": "We assessed the impact of each component. Removing adversarial training from the Lexical Agent reduced robustness from 0.78 to 0.62. Similarly, eliminating the multi-agent collaboration in the Consensus Agent decreased both accuracy and robustness, highlighting the significance of collaborative analysis."}, {"title": "Comparison with Baseline BERT on the IMDB Dataset", "content": "We also evaluated our model using the IMDB dataset, consisting of 50,000 labeled reviews. The evaluation metrics included Accuracy, Precision, Recall, and F1-score for both positive and negative classes.\nOur results demonstrate a notable increase in accuracy from 89% to 94.5% with the integration of the multi-agent system\u2014resulting in a 5.5% gain. Precision for both positive and negative labels saw a significant boost to 97%, while recall for positive labels remained steady at 80%. This adjustment highlights the model's advantage in achieving high precision, critical in applications where minimizing false positives is essential. The Precision-Recall Curve further illustrates the model's balance between precision and recall, leveraging the multi-agent collaboration to maintain high-confidence decisions while slightly trading off recall."}, {"title": "Discussion", "content": ""}, {"title": "Interpretation of Results", "content": "Our framework's design prioritizes precision through multi-agent consensus, which optimally suits environments requiring high confidence in positive classifications. While recall is lower, this trade-off is justified in cases where false positives are more detrimental than missing some positives. Applications in areas such as regulatory compliance, medical data analysis, or sensitive content classification can benefit significantly from this approach, as it ensures robust, highly accurate decision-making. Future work will explore strategies to enhance recall, potentially by tuning agent collaboration thresholds or integrating adaptive mechanisms that dynamically balance precision and recall."}, {"title": "Comparison with Existing Work", "content": "Compared to existing multi-agent frameworks like CAMEL [Li et al., 2023], our approach specifically targets low-confidence predictions, optimizing resource allocation and focusing collaborative efforts where they are most impactful. While CAMEL emphasizes role-playing and autonomous cooperation, our framework integrates a structured escalation mechanism that directly addresses classification uncertainty, leading to measurable performance gains."}, {"title": "Challenges and Limitations", "content": "Despite the significant improvements, our framework presents certain challenges:\n\u2022 Computational Overhead: The addition of multiple agents increases computational requirements, which may be a constraint in resource-limited environments.\n\u2022 Agent Configuration: Adjustments aimed at enhancing performance for one category often inadvertently disrupt the performance of other closely related categories. Effectively configuring and tuning each specialized agent for diverse classification tasks requires meticulous design and domain knowledge.\n\u2022 Scalability: Extending the framework to handle an extensive range of classification labels and complex datasets may necessitate further optimization."}, {"title": "Production Suitability and Efficiency Analysis", "content": "Assessing the suitability of our multi-agent collaboration framework for production environments and evaluating its speed and efficiency are crucial for real-world deployment. Our framework exhibits the following characteristics that contribute to its production readiness:"}, {"title": "Modularity and Scalability", "content": "\u2022 Modular Design: The separation of specialized agents (Lexical, Contextual, Logic, Consensus, Explainability) promotes modularity, allowing independent development, testing, and maintenance. This modularity facilitates scalability, enabling the addition of new agents or the expansion of existing ones without overhauling the entire system.\n\u2022 Scalability: The framework is designed to handle increased data volumes and more complex classification tasks. By distributing workloads across specialized agents, the system can scale horizontally by adding more instances or vertically by enhancing agent capabilities."}, {"title": "Maintainability and Extensibility", "content": "\u2022 Maintainability: Clear delineation of agent responsibilities simplifies debugging and updates. Each agent can be monitored and optimized independently, ensuring sustained performance."}, {"title": "Reliability and Robustness", "content": "\u2022 Extensibility: The framework's architecture allows for the integration of additional agents or the refinement of existing ones. Future enhancements, such as incorporating domain-specific knowledge agents, can be seamlessly integrated to address emerging classification challenges.\n\u2022 Fault Tolerance: Implementing redundancy for critical agents ensures system reliability. Backup instances of agents can take over in case of failures, maintaining uninterrupted operation.\n\u2022 Error Handling: Robust error-handling mechanisms are in place to manage unexpected inputs or agent malfunctions gracefully, preventing system-wide disruptions."}, {"title": "Security Considerations", "content": "\u2022 Data Privacy: The framework complies with data protection regulations (e.g., GDPR) by implementing encryption, access controls, and data anonymization where necessary.\n\u2022 Secure Communication: Agents communicate over secure channels, safeguarding data integrity and preventing unauthorized access or tampering."}, {"title": "Speed and Efficiency", "content": "\u2022 Computational Overhead: While BERT is computationally intensive, leveraging optimized versions like DistilBERT or applying quantization techniques can reduce latency and resource consumption. Additionally, the use of lightweight models or efficient rule-based systems for specialized agents (e.g., Lexical and Logic Agents) minimizes added computational overhead.\n\u2022 Parallel Processing: Agents operate in parallel where possible, reducing cumulative processing time and ensuring timely classification decisions.\n\u2022 Resource Management: Dynamic allocation of computational resources based on workload ensures optimal performance. Utilizing container orchestration tools like Kubernetes facilitates efficient scaling and load balancing."}, {"title": "Benchmarking and Profiling", "content": "Continuous monitoring of key performance indicators (KPIs) such as response time, throughput, and resource utilization is essential. Profiling tools help identify and address inefficiencies within agents, enabling targeted optimizations to enhance overall system performance."}, {"title": "Practical Recommendations for Production Deployment", "content": "\u2022 Model Optimization: Employ techniques like model distillation, quantization, and pruning to reduce the computational footprint of BERT and other agents without compromising accuracy.\n\u2022 Infrastructure Enhancements: Utilize hardware accelerators (GPUs/TPUs) and consider edge computing for applications requiring low latency.\n\u2022 Software Engineering Best Practices: Implement CI/CD pipelines for automated testing and deployment, and establish comprehensive monitoring and logging systems for performance tracking and anomaly detection.\n\u2022 User Experience Considerations: Design the system to degrade gracefully under resource constraints and incorporate user feedback mechanisms to facilitate continuous improvement.\nBy addressing these aspects, our multi-agent collaboration framework is well-positioned for deployment in production environments, offering a balance between performance gains and computational efficiency."}, {"title": "Implications and Future Work", "content": "The success of our framework opens avenues for further research and application:\n1. Adaptive Thresholding: Implementing dynamic threshold mechanisms that adjust based on real-time performance metrics and input characteristics could enhance efficiency and responsiveness.\n2. Agent Specialization: Developing agents specialized for sub-tasks or specific domains can further refine classification accuracy.\n3. Integration with External Knowledge Bases: Incorporating external data sources can enrich agent analyses, particularly in specialized domains.\n4. Continuous Learning: Establishing feedback loops where agents learn from ongoing interactions and user feedback can drive continuous performance improvements.\n5. Optimizing Computational Resources: Exploring lightweight agent architectures or parallel processing techniques can mitigate computational overhead while maintaining performance gains.\nFuture work will focus on expanding the current research by addressing limitations and exploring additional applications to enhance the overall impact of the findings:\n1. Adaptive Threshold in Agent Pruning: We plan to explore adaptive methods for setting the pruning threshold \\(\\tau\\), possibly using reinforcement learning techniques [Shinn et al., 2023]. This will allow the system to dynamically adjust thresholds based on real-time performance metrics and input complexity."}, {"title": "Extended Benchmarks", "content": "2. Extended Benchmarks: Testing our framework on larger and more diverse text classification datasets will further validate our approach. Additionally, benchmarking against other state-of-the-art text classification systems will provide deeper insights into comparative performance.\n3. Integration with Organizational Systems: We aim to integrate our framework with real-world organizational communication systems to assess its practical impact. This includes deploying the system in live environments, gathering user feedback, and iteratively refining agent behaviors and communication protocols.\n4. Exploration of Novel Agent Roles: Future research will investigate the introduction of new agent roles, such as sentiment analysis agents or domain-specific knowledge agents, to further enhance the system's understanding and classification capabilities."}, {"title": "Conclusion", "content": "We presented a novel multi-agent collaboration framework that integrates BERT with specialized agents to enhance text classification accuracy and robustness. By dynamically escalating low-confidence predictions to a collaborative multi-agent system, our approach leverages diverse analytical perspectives, resulting in significant performance improvements across various classification tasks. Achieving optimal performance across all categories required a nuanced and iterative approach to prompt tuning. This process highlighted the importance of balancing improvements within individual categories to avoid unintended trade-offs. Through continuous refinement, we were able to develop prompts that maintained high performance across all areas, ultimately enhancing the model's robustness and consistency. Empirical evaluations demonstrated that our framework outperforms standard BERT-based classifiers and existing multi-agent systems, achieving up to a 5.5% increase in accuracy.\nOur framework's generalizability allows it to be applied to a wide range of text classification applications, from sentiment analysis to intent recognition, making it a versatile tool in the NLP arsenal. Future work will focus on refining agent interactions, optimizing computational efficiency, and exploring adaptive mechanisms to further enhance performance and scalability."}]}