{"title": "PARTNR: A Benchmark for Planning and Reasoning in Embodied Multi-agent Tasks", "authors": ["Matthew Chang", "Gunjan Chhablani", "Alexander Clegg", "Mikael Dallaire Cote", "Ruta Desai", "Michal Hlavac", "Vladimir Karashchuk", "Jacob Krantz", "Roozbeh Mottaghi", "Priyam Parashar", "Siddharth Patki", "Ishita Prasad", "Xavier Puig", "Akshara Rai", "Ram Ramrakhya", "Daniel Tran", "Joanne Truong", "John M. Turner", "Eric Undersander", "Tsung-Yen Yang"], "abstract": "We present a benchmark for Planning And Reasoning Tasks in humaN-Robot collaboration (PARTNR) designed to study human-robot coordination in household activities. PARTNR tasks exhibit characteristics of everyday tasks, such as spatial, temporal, and heterogeneous agent capability constraints. We employ a semi-automated task generation pipeline using Large Language Models (LLMs), incorporating simulation in the loop for grounding and verification. PARTNR stands as the largest benchmark of its kind, comprising 100,000 natural language tasks, spanning 60 houses and 5,819 unique objects. We analyze state-of-the-art LLMs on PARTNR tasks, across the axes of planning, perception and skill execution. The analysis reveals significant limitations in SoTA models, such as poor coordination and failures in task tracking and recovery from errors. When LLMs are paired with real humans, they require 1.5x as many steps as two humans collaborating and 1.1x more steps than a single human, underscoring the potential for improvement in these models. We further show that fine-tuning smaller LLMs with planning data can achieve performance on par with models 9 times larger, while being 8.6x faster at inference. Overall, PARTNR highlights significant challenges facing collaborative embodied agents and aims to drive research in this direction.", "sections": [{"title": "Introduction", "content": "Imagine a domestic robot that collaborates with humans in daily activities using natural language, akin to human-to-human interactions. This scenario requires two key features: the dynamic collaboration between the robot and the human, and the use of natural language for interaction. Current benchmarks in embodied AI typically satisfy one or the other condition; either robots operate in isolation (Shridhar et al., 2020; Zhu et al., 2023; Krantz et al., 2020; Majumdar et al., 2024), or tasks are not specified in natural language (Yenamandra et al., 2023; Puig et al., 2024; Szot et al., 2023; Jain et al., 2020). Despite significant progress in the field of embodied AI, there remains a gap in realistic benchmarks that evaluate robots in collaborative settings. To bridge this gap, we introduce Planning And Reasoning Tasks in humaN-Robot collaboration (PARTNR), a novel benchmark that evaluates the ability of embodied AI agents to collaborate with humans across a range of household activities in simulated indoor environments (Figure 1).\nPARTNR consists of 100,000 natural language instructions paired with tailored evaluation functions, focusing on four task types: (1) constraint-free, where sub-tasks can be completed in any manner by either agent, (2) spatial tasks that contain spatial constraints, (3) temporal tasks that require ordered execution, and (4) heterogeneous tasks that include actions that cannot be completed by one of the agents. Beyond the conventional challenges of long-horizon planning, novel partially observed environments, and large state and action spaces, PARTNR emphasizes the need for effective collaboration dynamics, such as task division and tracking partner's progress.\nCurating such a benchmark of large-scale, natural language tasks with tailored evaluation functions presents significant challenges. Current benchmarks typically rely on either templated tasks (Shridhar et al., 2020;"}, {"title": "Related Work", "content": "Language-based benchmarks in Embodied Al. A large body of work on language benchmarks in Embodied AI has focused on navigation (Anderson et al., 2018; Krantz et al., 2020; Chen et al., 2019) or Embodied Question Answering (Das et al., 2018; Majumdar et al., 2024) which involve navigation and information gathering but do not require agents to modify their environments. Closer to our work are instruction-following benchmarks (Shridhar et al., 2020, 2021; Puig et al., 2018; Wang et al., 2024; James et al., 2020; Gong et al., 2023), where agents interact with environments to complete tasks described via language, though the diversity of tasks is limited. In contrast, we leverage LLMs to generate diverse task definitions and scene initializations, and extend them to multi-agent settings. The idea of scaling up task generation using LLMs has been explored in a few recent works (Katara et al., 2023; Wang et al., 2024; Xian et al., 2023; Nasiriany et al., 2024). However, these works tend to focus on single-agent tasks that span relatively short horizons, while we consider long-horizon, multi-agent problems.\nEmbodied multi-agent benchmarks. Multiple works have proposed embodied multi-agent benchmarks (Puig et al., 2023; Agashe et al., 2023; Zhang et al., 2024a; Jain et al., 2019; Suarez et al., 2019). Many of these benchmarks focus on coordination in simple 2D environments, limiting their applicability to real world settings (Agashe et al., 2023; Carroll et al., 2019). Recent works have developed benchmarks studying collaboration in more realistic environments and activities (Puig et al., 2021; Zhang et al., 2024a; Jain et al., 2019; Puig et al., 2024; Szot et al., 2023), focusing on rearranging objects or furniture in large, partially observable 3D environments (Puig et al., 2021, 2024; Jain et al., 2019; Szot et al., 2023), or manipulating objects in a counter-top space (Mandi et al., 2024). However, these benchmarks are typically limited to a predefined and reduced set of tasks, often not described in natural language and primarily involving object rearrangement. In contrast, PARTNR covers an open set of tasks, each described in natural language, requiring agents to rearrange objects with spatial and temporal constraints, as well as requiring heterogeneous actions that can only be done by the human agent, (e.g., washing dishes or turning on the oven).\nLLMs for decision making. Several works use LLMs as interactive policies, demonstrating their strong capabilities but also highlighting challenges in grounding them with observations and actions (Huang et al., 2022; Yao et al., 2023; Ahn et al., 2022; Huang et al., 2023; Zeng et al., 2022). Some approaches improve grounding by prompting LLMs with demonstrations and task-specific constraints (Huang et al., 2022; Yao et al., 2023), or by integrating LLMs with external modules for multi-modal reasoning (Ahn et al., 2022; Huang et al., 2023; Zeng et al., 2022). Toolformer (Schick et al., 2023) allows LLMs to call APIs for information retrieval or environmental interaction. For instance, APIs can be used to call low-level policies (Driess et al., 2023), to"}, {"title": "Benchmark Generation", "content": "We introduce PARTNR, a benchmark aimed at training and evaluating robots at solving natural language tasks in collaboration with humans. PARTNR covers four types of tasks: (1) Constraint-free tasks, where sub-tasks can be completed in any manner by either agent. For example, \"Let's move all dirty plates to the sink.\" (2) Spatial tasks that require reasoning about the spatial positioning of objects. For instance, \"Let's place the books on the shelf next to each other.\" (3) Temporal tasks, where the sequence in which sub-tasks are executed is important. For example, \"Let's remove the candles from the dining table before bringing the plates to the table.\" (4) Heterogeneous tasks, involving actions that are beyond the robot's capabilities. For example, \"Let's wash the dishes before putting them in shelves.\" In scenarios where the robot's skills do not support washing, completing this task requires reasoning about agent capabilities. Our benchmark consists of natural language instructions and corresponding evaluation functions, both of which are generated at-scale using LLMs. Specifically, we generate 1,000 human-verified instructions and corresponding evaluation functions and use them as in-prompt examples to scale to 100,000 tasks in other scenes with different layouts and objects. A unique aspect of our automatic generation is the integration of an embodied simulator within the generation loop, which significantly reduces LLM errors such as hallucinations and infeasible actions."}, {"title": "Simulation-in-the-loop task instruction generation", "content": "While LLM-based task generation has been studied in literature before (Katara et al., 2023; Wang et al., 2024; Xian et al., 2023; Nasiriany et al., 2024), these generations are not grounded beyond user-created in-context prompts. In PARTNR, we use a simulation-in-the-loop generation technique to ground the LLM in the environment, agents and available actions. Specifically, we instantiate a simulation environment in the Habitat 3.0 simulator (Puig et al., 2024), populated with the HSSD dataset (Khanna et al., 2024), consisting of 60 unique houses and 5,819 OVMM objects (Yenamandra et al., 2023). The simulated house is parsed into a list of rooms and available furniture, and passed to an LLM, along with all available objects. Using this information, the LLM is asked to generate free-form, viable tasks in the scene, along with an initial scene state description. For example, if the generated task is \"Clear dishes from the living room\", the LLM should generate an initial scene with multiple dishes in the living room. At this stage, additional objects are also added to the scene to create clutter in the environment. Once generated, the tasks, initial states, and clutter are instantiated in the simulator, and infeasible instructions are filtered. For example, if the house does not have a living room, \"Clear dishes from the living room\" is invalid. Similarly, if the generated task requires actions not supported by the simulator, such as folding, the task is filtered. Generally, the rate of hallucinations is high, leading to a significant number of episodes being discarded. We observe that after filtering for infeasible instructions, the diversity in generated instructions is typically limited. For example, most of the instructions use the same objects (e.g., dishes) or similar rooms (e.g., kitchen or dining room). To increase diversity of the generated tasks, we manually annotate them to ensure task and object diversity, such as maintaining a balanced distribution of constraint-free, spatial, temporal, and heterogeneous tasks by modifying the instructions to elicit specific characteristics. This process results in 1,000 human annotated and simulation-verified tasks.\nSuch manual annotation is not practical for large-scale generation. Instead, we leverage the human-annotated 1,000 instructions to scale generation by using them as in-prompt examples. We prompt the LLM with"}, {"title": "Evaluation function generation", "content": "To determine if an agent successfully completed the instruction \"Clear all dishes from the living room\", we need an evaluation function that can validate the removal of all spoons, forks, and other dishes from any of the living rooms. However, manually annotating all necessary rearrangements and state changes of a task is time intensive and since all tasks are unique, impractical at scale. Similar to instruction generation, we employ an LLM to create an evaluation function that assesses task completion without requiring any manual annotations. Specifically, we leverage the ability of LLMs to generate predicate-based Python programs using three types of APIs: a list of propositions indicating what relations between entities must be satisfied, a set of dependencies indicating when propositions should be queried, and a set of constraints indicating how propositions must be satisfied. We define an expressive vocabulary of each of these components to afford evaluation of all tasks in the benchmark (e.g., Figure 3). Closely related evaluation systems include defining tasks in PDDL (Ghallab et al., 1998) or BDDL (Srivastava et al., 2022). We choose to build a new Python-based evaluation system since neither have the expressivity to evaluate PARTNR tasks while maintaining human and LLM interpretability; for instance, BDDL does not support time-varying evaluation. Since PARTNR tasks have temporal dependencies (e.g. multi-step rearrangement), the input to the evaluation function is the complete sequence of simulator states during task execution. The evaluation function returns 3 metrics: (1) Percent Complete ($PC \\in [0, 1]$), the % of propositions that are satisfied w.r.t. constraints, (2) Success ($S \\in \\{True, False\\}$), measuring if a task was successfully completed, defined as $S := (PC = 1)$, and (3) Failure Explanation (FE), a human and LLM interpretable language description of why the agents failed to accomplish the task.\nWe use CodeLLama-70B-instruct (Roziere et al., 2023) for evaluation function generation. Exemplified in Figure 3, producing perfect evaluation functions is non-trivial. The LLM must correctly classify the entire space of possible actions against natural language instructions and the specific simulation environment, which can be quite complex. For example, in Figure 3, the instruction \"set the plants on the shelf\" refers to \"the shelf\", but two shelves exist in the room. The evaluation function must allow either shelf while requiring placement of all plants, and finally account for a next-to relation. Any error or missing value in either a proposition or constraint invalidates the evaluation function. Consequently, we observe a large error rate in LLM generation, particularly pertaining to incorrect propositions and temporal sequencing constraints.\nTo alleviate these inaccuracies, we follow a similar semi-automated procedure to instruction generation. We first generate evaluation functions for the 1,000 human-annotated instructions and perform manual annotation to correct them (Appendix A.6.3). This results in a dataset of 1,000 human-verified instruction and evaluation pairs. Next, we generate evaluations for the scaled 100,000 instruction set. Recall that the scaled instructions are generated by prompting the LLM with an example instruction from the annotated set. We retrieve the corresponding annotated evaluation function and prompt the LLM with it. This is similar to approaches such as retrieval-augmented generation (Lewis et al., 2020) and improves the accuracy of evaluation function generation from 50% to 92% as measured through manual inspection (Appendix A.6.2). As a final step, we ask human users to solve all PARTNR tasks using our human-in-the-loop evaluation tool (Appendix A.13). All tasks that cannot be solved by humans over 6 retries (3 single-user, 3 multi-user tries) are deemed infeasible, and removed from the dataset. We find that about 90% of instructions, and 92% of evaluation functions from automated generation are accurate, resulting in a combined generation accuracy of 90 \u00d7 92 = 83%."}, {"title": "The PARTNR Dataset", "content": "The PARTNR dataset comprises of 100,000 episodes in 37 train scenes, 1,000 episodes in 13 validation scenes, and 1,000 episodes in 10 test scenes from the HSSD dataset (Khanna et al., 2024). After scaled generation, all validation and test set episodes are human annotated for correctness, as well as a 2,000-episode subset of train. See Appendix A.6.1 for correctness analysis of scale-generated episodes. Below, we analyze the characteristics and diversity of this dataset.\nCharacteristics: As described earlier, PARTNR focuses on four task types: constraint-free, spatial, temporal, and heterogeneous. We show the distribution of these task types in the test split in Figure 4; validation split is similar. PARTNR evaluates collaboration along these axes both independently and jointly. Secondary characteristics of interest include dependent rearranges (e.g., \"Place them on the same table\") and multi-step rearrangement of the same object (e.g. \"Move the cup to the sink, wash it, then place it in the cabinet\"). 7% of tasks include dependent rearranges and 6% include multi-step rearrangement. Tasks average 4.7 propositions to be satisfied (indicative of number of steps required to complete tasks). For analysis of linguistic phenomena and more characteristics, see Appendix A.3.\nDiversity: The diversity of tasks in PARTNR is largely enabled by simulation-in-the-loop generation, which utilizes rich HSSD scenes, and the OVMM object set. Consequently, PARTNR tasks reference and require reasoning about 155 unique object types, 20 furniture classes and 13 room types. Note that each instruction, instantiated in each house, brings its own diversity. For example, \"move the laptop to the office table\", grounds office and table uniquely in each house, as well as different instances of laptop in different instructions. Further discussion can be found in Appendix A.3."}, {"title": "Experiments and Analysis", "content": "We investigate how state-of-the-art planning and perception methods handle natural language tasks in new environments and coordinate with unseen partners using PARTNR. Since PARTNR consists of diverse spatio-temporal tasks specified in language, we primarily use LLMs in our baselines for planning, and study variants in (1) zero-shot prompting, retrieval-augmented generation or fine-tuning, (2) centralized versus decentralized planning, (3) partially versus fully observed environment, (4) learned versus oracle low-level robot skills, and (5) privileged versus non-privileged perception.\nOur experiments are conducted in the Habitat 3.0 simulator (Puig et al., 2024) with a simulated Spot robot (Boston Dynamics; Yokoyama et al., 2023). We adopt a two-layer hierarchical control architecture, similar to (Puig et al., 2024; Szot et al., 2021), as illustrated in Figure 5, for the robot and simulated human. At the high level, a planner selects skills from a predefined skill library (e.g., navigate, pick, place, open, close). We also use a textual world graph with a three-layer hierarchy rep-"}, {"title": "Results and Analysis", "content": "Metrics. We evaluate performance across different settings using four key metrics. First, the simulation steps metric measures the number of steps required for agents to complete the task within the simulation environment, serving as an indicator of efficiency. Second, the success rate reflects the completion of the task i.e. whether 'all' task constraints are satisfied. Given the complexity and long-horizon nature of PARTNR tasks, agents often partially complete the task. To account for this, we also report percent complete, which quantifies the ratio of completed task 'propositions' (percent complete = 1 for successful tasks). Lastly, we assess the reasoning efficiency of the planners through the planning cycles metric, which counts the number of high-level LLM calls each planner makes in the course of an episode. We cap the maximum planner calls at 50 in all experiments.\nTask Performance Analysis\nPARTNR tasks are challenging for LLM-based planners. LLM-based baselines across all observability and controllability conditions perform worse than the privileged heuristic expert, due to errors in task tracking (not completing all steps, performing them in the wrong order, or undoing completed steps), and semantic errors (placing objects on the wrong furniture, or moving the wrong object), indicating a gap in LLM task planning.\nLLMs struggle with coordination in decentralized settings. Decentralized ReAct baselines which do not have privileged access to partner's intent are significantly slower at task completion than centralized ReAct (3295 steps with decentralized-partial in row(e) versus 2298 with centralized-partial in row(d)). This shows that reasoning about the partner e.g., knowing or inferring partner's intent can improve task efficiency in PARTNR tasks, and current SOTA LLMs perform poorly at this. Moreover, decentralized ReAct with two agents is even slower than ReAct with a single-agent (3295 steps with multi-agent in row(e) versus 2519 with single-agent in row(a)), indicating that LLMs suffer from a significant coordination \"burden\". This co-ordination burden is further highlighted in our analysis on extraneous effort in Section 4.2.2, where we find that agents end up repeating parts of the task or performing irrelevant actions with much higher frequency in decentralized settings.\nLLMs are unable to recover from learned skill failures. When replacing oracle skills with learned skills, the success rate decreases from 0.73 to 0.57 (row(e) vs. row (h)). This decline can be attributed to the higher failure rate and increased number of simulation steps required by learned skills compared to privileged oracle skills. The LLMs struggle to recover from skill errors like failing to pick up an object or performing incomplete exploration, resulting in a lower success rate. Future research could investigate training large models with low-level skills in the loop, enabling them to learn recovery and replanning strategies in the face of such failures.\nLLMs exhibit a high degree of sensitivity to errors in perception. When we replace privileged perception with off-the-shelf perception modules, success rate significantly declines (from 0.57 with a privileged, partial world graph in row(h) to 0.30 with Concept-Graphs (Gu et al., 2024) in row(i)). LLMs rely heavily on accurate world descriptions provided by the world graph and struggle to correct errors such as misclassification (e.g., shelves misidentified as tables) or incorrect room assignments (e.g., a table in the living room mislabeled as being in the bedroom). Multi-modal models like VLMs might be stronger at recovering from such failures, which we leave for future work.\nFinetuned 8B model performs on par with a ReAct with a 70B model, while being 8.6x faster. We find that the finetuned planner with a small 8B model performs on par with ReAct, which uses a much larger 70B model (a 0.73 success rate with the 70B model in row(e), versus 0.70 with the finetuned 8B model in row(g)). At the same time, we find that the finetuned model is 8.6 times faster at inference. This indicates that the finetuning effectively distills task-relevant information from the training set and generalizes to new test tasks. When"}, {"title": "Conclusion", "content": "We present PARTNR, a benchmark for reasoning and planning in multi-agent embodied tasks, featuring 100,000 natural language tasks instantiated in 60 simulated, multi-room houses with 5,819 unique objects. We use a semi-automated LLM-powered pipeline for large-scale instruction and evaluation function generation that uses simulation-in-the-loop grounding. PARTNR exhibits characteristics of everyday tasks, such as temporal and spatial constraints, and allows systematic evaluation of planning approaches. We find a significant gap between SOTA LLMs and human-level performance at PARTNR tasks. While our best LLM baseline only succeeds at 30% of tasks with no privileged information, humans are able to solve 93% of the tasks. Moreover, LLMs face challenges in coordinating with both LLM-based agents and real human partners. Human-in-the-loop evaluations, involving real humans collaborating with LLM-guided robots, reveal that LLM-guided partners decrease human efficiency compared to working solo. This suggests that LLM-based agents require significant improvements to become effective collaborative partners in embodied tasks. PARTNR serves as a challenging benchmark that highlights the substantial limitations of current models."}, {"title": "Appendix", "content": "We present PARTNR, a benchmark for reasoning and planning in multi-agent embodied tasks, featuring 100,000 natural language, everyday tasks. We show at-scale generation of these tasks using LLMs with simulation in the loop for grounding and human in the loop for filtering. We also evaluate several LLM-based planning models on these tasks and highlight avenues for future work. This appendix provides additional details on these contributions and is organized as follows:\nA.1 Code and PARTNR benchmark open-sourcing\nA.2 HSSD Scene Annotations\nA.3 Details and additional analysis on the PARTNR dataset\nA.4 Simulation features and prompts for the PARTNR task generation\nA.5 The PARTNR evaluation system\nA.6 Human annotation and quality assessment for the PARTNR tasks and evaluation functions\nA.7 World graph for perception in LLM agents\nA.8 Learned low-level robot skills\nA.9 Implementation details for ReAct agents\nA.10 Details on finetuning LLM agents for the PARTNR tasks\nA.11 Additional results\nA.12 Analysis of collaborative behavior and efficiency of LLM agents\nA.13 Human-in-the-loop (HITL) system and evaluation for the PARTNR tasks and LLM agents\nA.14 Prompts for task and evaluation generation\nA.16 Prompts for planner baselines"}, {"title": "Open-sourcing PARTNR Dataset and Codebase", "content": "Accompanying this paper, we have released the code and data necessary to reproduce our experiments. Released code includes our PARTNR benchmark tasks, metrics, baseline oracle skills, large planning model framework, and dataset generation utilities. Released data includes extensions of the Habitat Synthetic Scenes Dataset (HSSD) (Khanna et al., 2024), generated benchmark task episodes, and model weights for our trained neural network skills and fine-tuned large planning model.\nThe publicly released codebase accompanying PARTNR depends on the most recent version of the AI Habitat platform (habitat-lab and habitat-sim (v0.3.2)) (Puig et al., 2024) which it extends to define collaboration tasks and skills."}, {"title": "HSSD Scene Annotations", "content": "In order to model the space of rich indoor collaboration tasks we propose with PARTNR, we extended HSSD with additional asset authoring and annotation. To enable more realistic indoor object manipulation, we added articulated 3D furniture models such as drawers, cabinets, and appliances. These models were converted from rigid source assets in HSSD and swapped into the scenes. We prepared 60 scenes divided into train, val, and test splits to support our experiments. Each scene is manually adjusted by a human to ensure simulation robustness and minimize potential issues. Furniture is annotated with a set of Receptacles (surfaces which support small object placement such as shelves and drawers) and can be opened and closed by the agents. Receptacles are further filtered contextually in each scene to ensure that the active set is accessible to the agents. Additional annotations include point or marker sets for each furniture, region annotations, and semantic classification of objects. The marker sets indicate either a spread of surface points (for distance/occlusion checking) or the location of key points of interest such as faucets (for cleaning/filling) and handles (for opening/closing) necessary for low-level skill training and oracle skill execution. Region annotations included per-scene region volumes (e.g., kitchen, living-room, bedroom, etc.) for checking and specifying the location of objects and furniture. Semantic annotations indicate the object category or class (e.g. table, chair, cup, toy) to support open language prompt grounding and semantically guided task generation."}, {"title": "Dataset Details and Additional Analysis", "content": "We expand on the details and analysis of the PARTNR dataset of Section 3.3, including analysis of linguistic phenomena, secondary task characteristics, and the distribution of sampled entities in generated tasks.\nLinguistic Phenomena. In Table 4, we present an analysis of linguistic phenomena manually annotated over 50 random episodes (similar to Ku et al. (2020) and Chen et al. (2019)). We analyze the following phenomena:\nClass Reference: refers to a semantic class of objects, furniture, or rooms in the scene. These references typically, but not necessarily, follow from the classes defined in OVMM.\nInstance Reference: refers to a unique object, furniture, or room; this disambiguation between entities of a class is typically achieved by visual description.\nCo-Reference: an expression that refers to an entity defined elsewhere in the instruction.\nPassive Voice: the instruction is phrased such that the object is receiving the rearrangement or state change; the request is typically asked instead of commanded.\nActive Voice: the instruction is phrased such that the rearrangement or state change is to be completed with an object; the request is typically commanded instead of asked.\nHigh-Level Goal Spec: this sets the operating context for the task before the particulars of rearrangement or state change are specified.\nAgentic Reference: a reference to one of the agents performing the task. Typically used to incite a suggested task division between the human and robot.\nWe observe that PARTNR tasks have a high rate of entity class references, such as the table, (6.38/episode), and common occurrences of instance references and co-references. This signals a need for capable natural language understanding, scene grounding, and co-reference resolution. Tasks and sub-tasks are predominantly issued using active voice (92%), but 14% of tasks include at least one occurrence of passive voice. Half of tasks involve a high-level goal specification, which commonly serves to reduce the search space. For example, a task starting with Let's clean up all the toys in the playroom constrains object search to that room and softly constrains the placement of those objects to locations in which toys would commonly be stored. Finally, agentic references are present in 14% of episodes.\nSecondary Task Characteristics. In Table 5, We present an analysis of secondary task characteristics present in PARTNR as derived automatically from evaluation functions of all episodes in our dataset. We find rare but present occurrences of subset counts, where agents must reason about manipulating a subset of a set of"}, {"title": "Simulation-in-the-loop Large-Scale Task Generation", "content": "In this section, we describe in detail the simulation-in-the-loop task generation pipeline. We follow a 4-step generation pipeline:\nSimulation-in-the-loop small-scale free-form LLM generation:\nIn PARTNR, we initiate the process by setting up a simulation environment using the Habitat 3.0 simulator, populated with the HSSD dataset which includes 60 unique houses and 5,819 OVMM objects. This simulated environment is parsed to identify a list of rooms and available furniture. This information, along with a list of all available objects, is then passed to a Language Model (LLM). The LLM utilizes"}, {"title": "Habitat 3.0 and HSSD Extension", "content": "We generate PARTNR using modified HSSD scenes (Khanna et al., 2024) and the Habitat 3.0 (Puig et al., 2024) simulator due to its humanoid simulation capabilities and availability of features which support modeling of collaborative tasks as discussed in Appendix A.1.\nOur extensions to the Habitat platform include a set of features targeting: object state manipulation (e.g., clean/dirty, powered on/off and filled/un-filled), evaluation of object relative relationships (e.g., next-to, above, within, on-top, on-floor, in-region, etc), and procedural clutter generation utilities enabling generation of valid initial scene contents pre-conditioned on the output of LLM-generated requirements from open-language prompts (see Section 3). For example, in order to evaluate the task of rearranging a tea set from furniture A to furniture B, we must first generate a scene with both types of furniture in accessible locations and a tea set already sitting on or inside of furniture A."}, {"title": "The PARTNR Evaluation System", "content": "In this section, we formalize the components of the evaluation system, define the resulting metrics, and present the details and prompts used for LLM-based evaluation generation."}, {"title": "Evaluation Predicates", "content": "We use logical predicates to query the state of objects, furniture, and rooms at the current timestep in the simulator. The evaluation system operates on the resulting binary state values."}, {"title": "Propositions", "content": "The primary component of a task evaluation function is a list of propositions. We define a proposition as an evaluation predicate instantiated with argument values. Propositions additionally enable the evaluation of instructions with ambiguous references (\u201con a table\u201d which table?) and subset counts (\u201ctwo spoons\u201d any two of the n total spoons). Ambiguity is enabled by extending the predicate arguments to lists. Subset counts are enabled by optional arguments number, which defines the subset size, and arg_match, which is a boolean indicating whether all entities in the subset must be satisfied with the same second argument. Suppose we want to evaluate the task \"Bring a spoon to the table.\". If we have a single spoon and a single table, the proposition is straightforward:\nis_on_top([spoon_1], [table_1]).\nIf we have multiple spoons and need just one (an ambiguous instruction), the following proposition is used:\nis_on_top([spoon_1, spoon_2, spoon_3], [table_1]).\nIn the above case, a list is treated as a OR of entities. The same holds for multiple tables:\nis_on_top([spoon_1, spoon_2, spoon_3], [table_1, table_2]),\nin which any spoon may be placed on any table. If the instruction specifies bringing two spoons to the table, the number=2 argument is added:\nis_on_top([spoon_1, spoon_2, spoon_3], [table_1, table_2], number=2)."}, {"title": "Dependencies", "content": "Evaluation functions operate over a sequence of simulation states. By default", "Move the cup from the table to the sink. Then, return the cup to the table.\" This consists of a multi-step rearrangement where the proposition checking that the cup is on the table is dependent on a different proposition first being satisfied. We define a proposition dependency as the following": "nPropositionDependency(proposition_indices", "supported": "nafter_satisfied: evaluate propositions in proposition_indices after the propositions in depends_on have been satisfied.\nafter_unsatisfied: evaluate propositions in proposition_indices after propositions in depends_on have been satisfied at some point in the past but are no longer satisfied.\nwhile_satisfied: evaluate propositions in proposition_indices when propositions in depends_on are currently satisfied.\nAs a concrete example, suppose the instruction is \"Move the ball and bat to the kitchen and set them next to each other. Then, move them to the closet.\" The propositions for this task would be\n0 is_in_room([ball", "kitchen": "n1 is_in_room([bat"}, {"kitchen": "n2 is_next_to([ball", "bat": "n3 is_in_room([ball", "closet": "n4 is_in_room([bat"}, {"closet": ".", "PropositionDependency([2": [0, 1], "while_satisfied": "n1 PropositionDependency([3"}]}