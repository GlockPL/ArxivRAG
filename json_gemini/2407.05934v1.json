{"title": "Graph Anomaly Detection with Noisy Labels by Reinforcement Learning", "authors": ["Zhu Wang", "Shuang Zhou", "Junnan Dong", "Chang Yang", "Xiao Huang", "Shengjie Zhao"], "abstract": "Graph anomaly detection (GAD) has been widely\napplied in many areas, e.g., fraud detection in finance and robot\naccounts in social networks. Existing methods are dedicated to\nidentifying the outlier nodes that deviate from normal ones.\nWhile they heavily rely on high-quality annotation, which is hard\nto obtain in real-world scenarios, this could lead to severely\ndegraded performance based on noisy labels. Thus, we are\nmotivated to cut the edges of suspicious nodes to alleviate the\nimpact of noise. However, it remains difficult to precisely identify\nthe nodes with noisy labels. Moreover, it is hard to quantitatively\nevaluate the regret of cutting the edges, which may have either\npositive or negative influences. To this end, we propose a novel\nframework REGAD, i.e., REinforced Graph Anomaly Detector.\nSpecifically, we aim to maximize the performance improvement\n(AUC) of a base detector by cutting noisy edges approximated\nthrough the nodes with high-confidence labels. (i) We design a\ntailored action and search space to train a policy network to\ncarefully prune edges step by step, where only a few suspicious\nedges are prioritized in each step. (ii) We design a policy-\nin-the-loop mechanism to iteratively optimize the policy based\non the feedback from base detector. The overall performance\nis evaluated by the cumulative rewards. Extensive experiments\nare conducted on three datasets under different anomaly ratios.\nThe results indicate the superior performance of our proposed\nREGAD.", "sections": [{"title": "I. INTRODUCTION", "content": "Graphs have been prevalently adopted to effectively rep-\nresent relational information in many areas, e.g., social net-\nworks [1] and recommendation systems [2]. While graphs\ncould be at billion scales with tremendous nodes, errors are\ninevitably introduced [3], [4]. Graph anomaly detection (GAD)\nplays an important role in many real-world scenarios, e.g., fake\nnews detection [5]\u2013[7] and fraud detection [8], [9]. Research\nhas been conducted based on various designs to identify\nanomaly nodes that deviate from the majority in the graph.\nEarly studies utilize traditional methods [10], [11], i.e., matrix\nfactorization and KNN, to extract feature patterns of outliers.\nAfter that, GNNs are designed with loss function design\nor effective information aggregation strategies [12], [13] to\ncapture the features of anomaly nodes. Recently, various deep\nlearning-based methods, e.g., meta-learning, active learning,\nand contrastive learning, filter out information aggregation in\nGNNs to reduce the influence of anomalous nodes [14], [15]\non the patterns of normal nodes and minimize the assimilation\neffect from normal nodes.\nHowever, the performance of existing methods remains\nunsatisfactory to meet industrial needs. While they heavily\nrely on high-quality labels to facilitate supervised or semi-\nsupervised learning, this hypothesis can hardly stand in real-\nworld scenarios since the annotations are difficult to obtain.\nFirst, crowd-sourcing annotators can be unreliable in labeling\nthe complex graph structure [16]. Ensuring high-quality labels\nrequires careful data cleaning. Second, it is unaffordable to\nannotate extremely large graphs by involving human experts.\nIn Fig.1, we showcase the impacts of noisy labels on two\nbenchmark datasets. We stimulate the noise by label flipping\nin a heuristic way, and further details can be found in the\nExperiment Setting section. It is obvious that noisy labels\nsignificantly decline several semi-supervised detection models\u2019\nperformance based on the metric of the area under the ROC\ncurve. Consequently, noisy labels inevitably exist in the graphs\n[17]. The corresponding noise information would propagate\nthrough neighbors and affect the representation learning abil-\nity. A careful denoising method is urged for effective GAD.\nwe are motivated to cut the edges of suspicious nodes to\nalleviate the impact of noise during propagation. Nevertheless,\nthis task is challenging since determining the noise candidates\nto be pruned is laborious. Given the large scale of real-world\ngraphs, the search space is extremely big, making it difficult\nfor the edge pruner to prioritize suspicious nodes. Moreover,\nit is hard to quantitatively evaluate the regret of cutting the\nedges which may bring either positive or negative influences.\non one hand, we wish to cut as many suspicious nodes as\npossible. This over-prune may remove normal edges and result\nin a sparse structure, which may affect the graph learning\nability. On the other hand, we require sufficient information\nby preserving enough edges, while an under-prune can hardly\nsatisfy the purpose of noise mitigation.\nTo this end, we propose a novel policy-in-the-loop frame-\nwork, the REinforced Graph Anomaly Detection model, i.e.,\nREGAD, to learn from noisy labels for robust GAD effectively.\nSpecifically, we aim to maximize the performance improve-\nment (AUC) of a base detector by cutting noisy edges with\nan edge pruner. This is approximated through the nodes with\na set of high-confidence labels generated by the pre-trained\nbase detector since true labels are not readily available in\nthe real world. (i) We design a tailored action and search\nspace to train a policy network to carefully make decisions\nand cut the suspicious edges step by step, where only a few\nsuspicious edges are prioritized in each step. (ii) We design a\npolicy-in-the-loop mechanism to iteratively optimize the policy\nbased on the feedback from the base detector, while the base\ndetector correspondingly updates the sets of high-confidence\nlabels based on the reconstructed graph structure by the edge\npruner. The overall performance is evaluated by the cumulative\nrewards that have been received based on the performance\n(AUC).\nIn general, we summarize our contributions below:\n1) We formally define the problem of noisy label learning\nfor graph anomaly detection.\n2) A tailored policy network is designed to carefully identify\nnoisy labels and optimize the edge pruning by prioritizing\nthe most suspicious nodes.\n3) We design a novel policy-in-the-loop learning paradigm\nfor GAD with noisy labels. GAD and the policy comple-\nmentarily benefit each other to mitigate the impacts of\nnoisy labels.\n4) Extensive experiments are conducted to comprehensively\ndemonstrate the superiority of our framework under 50%\nerror rates on three datasets."}, {"title": "II. PROBLEM STATEMENT", "content": "We use G = (V,E,X) to denote an attributed graph, where\nV = {V1, V2, ..., Un} is the set of n nodes, and E \u2286 V \u00d7 V\nis the set of edges. Besides, X = {X1, X2, ..., Xn} is the node\nattributes, X \u2208 Rn\u00d7d, and d is the attribute dimension. A \u2208\u0454\nRnxn represents the adjacency matrix of G. If vi and vj are\nconnected, Aij = 1. Otherwise, Aij = 0. VL = {V1, V2, ..., vi}\nrepresents labeled nodes, and Vu = V \u2013 VL is the set of\nunlabeled nodes. V\u2081 includes normal nodes Vn and abnormal\nnodes Va. In practice, we only obtain anomaly nodes Va\nfollowing |Va| < |Vn|. YL = {Y1, Y2, ..., y\u0131} denotes the real\nground truth labels of VL. However, in our research problem,\nwe assume they are not always trustworthy and correct and\nutilize YL = {Y1, Y2, \u2026, Y\u0131} to represent the noisy ground truth\ncorrupted by noise."}, {"title": "B. Problem Definition", "content": "Graph Anomaly Detection. Given an attributed graph G =\n(V,E,X), the GAD task is formulated as:\n\\(f(G, V_L) \\rightarrow \\hat{S},\n(1)\nwhere anomaly scores \u015c reflect the likelihood or degree of\nbeing an anomaly node. Higher \u015d\u00bf means a higher possibility of\nbeing detected as an anomaly vi \u2208 Va. These predicted scores\nprovide evidence to identify more anomaly nodes except\nexisting Va with high accuracy, especially in Vu.\nGraph Anomaly Detection with Noisy Labels. In this task,\nthe ground truth labels Y\u2081 are not accurate, which means a\nsmall proportion of labeled nodes VL are erroneously labeled.\nIn noisy ground truth labels YL, Yi = 0, i \u2208 L is mistakenly\nlabeled, so the real one is y\u2081 = 1. However, it is difficult to\ndistinguish which is real from YL. Under this setting, we aim\nto predict correct matching anomaly scores \u015c for all nodes as\nmuch as possible supervised by VL, i.e.,\n\\(f(G\\', V_L) \\rightarrow \\hat{S},\n(2)\nwhere the efficient detection model f is to estimate the proba-\nbilities of nodes being anomalous. In contrast to classification\ntasks where multiple classes are considered, only two types,\nanomaly nodes Va and normal nodes Vn are included in\nVL. Besides, labeled nodes are significantly smaller than the\nnumber of unlabeled nodes, denoted as |VL| < |VU|. To\nstimulate YL, we will use label flipping introduced in the\nexperiment section in detail."}, {"title": "III. METHODOLOGY", "content": "In this section, we propose the REinforced Graph Anomaly\nDetection model in Fig. 2. Our model seeks to correct noisy\ntruth labels and minimize the impact of noisy labels on neigh-\nborhood nodes based on the reinforcement learning method,\ndesigned as a policy-in-the-loop framework introduced in Sec-\ntion III-C. From the above targets, there are three challenges:\n1) How can we identify noisy labels given that ground truths\nare not readily available? 2) Given the complicated graph\nstructure, how can we control the negative impacts from\nnodes with noisy labels to their neighbors? 3) How can we\nquantitatively evaluate the influences of the edge pruner, i.e.,\npositive (cut the real noisy edges) or negative (miscut edges)?\nTo address the first challenge, we employ a base detector\nto assess the anomaly score of each node supervised by noisy\nground truth YL. It takes node attributes as the input and gives\ntwo confident node sets, anomaly set AS, normal set NS,\nand a noisy-label candidate set NC. These candidates are the\nanchors in the following edge pruner, which is leveraged to\nsolve the second challenge. To control incorrect information\ntransmission, we apply reinforcement learning to learn a policy\n\u03c0\u03c1(\u03b1\u03c2) to cut edges of candidates to manage noisy labels\u2019\ninfluence. Additionally, we employ the base detector perfor-\nmance based on the refined structure to evaluate the pruner\nquantitatively by returning rewards. These two components\ncollaborate by exchanging information, forming the policy-in-\nthe-loop framework. Next, each part is introduced in detail."}, {"title": "A. Base Detector", "content": "The base detector fa is proposed to predict anomaly scores\nand generate pseudo labels, especially for unlabeled data su-\npervised by noisy ground truth labels YL. Although we employ\nthose training datasets, nodes with noisy labels, this detector\ncould provide more information by predictions for unlabeled\nnodes. From this perspective, not all predictions are reliable,\nand thus we identify trustworthy anomaly set AS, normal set\nNS for obtaining pseudo labels and modifying incorrect labels\nfrom score ranking. Nevertheless, due to supervision under\nnoisy labels, the effective identification of candidates to be\ntargeted by the pruner is made possible. To identify candidates\nNC for cutting edges and reduce noisy labels\u2019 influence, we\nuse a multi-armed bandit to determine which nodes are most\nlikely to be candidates. Most candidates consist of erroneously\nlabeled anomalous nodes hidden among the unlabeled ones,\nalthough there are also some normal nodes mislabeled as\nanomalous.\nThe base detector employs the Meta-GDN model [18]\nnot only because it provides accurate scores and superior\nperformance in GAD tasks but also because it addresses the\nissue of data imbalance [12], [13], [18]\u2013[20] through balanced\nbatches. Meta-GDN ensures that when using reliable sets\nfor label rectification, the goal is to correct erroneous labels\nsimultaneously, minimizing the production of incorrect labels.\nAdditionally, based on the predicted scores ranking from Meta-\nGDN, candidates are identified using a multi-armed bandit\napproach to find the optimal pace deviated from mid-range\nscores, thereby locating nodes with potentially false labels.\nMore importantly, rewards for the multi-armed bandit are\ndesigned based on the AUC corresponding to the selected\nnodes. As part of the iterative loop, Meta-GDN fulfills the\naforementioned functions, updates node representations based\non the refined graph, and calculates effective rewards for the\npruner.\n1) Predict anomaly scores: The base detector maps node\nfeatures into the low-dimensional latent space to learn node\nrepresentations. We apply a simple Graph Convolutional Net-\nwork (GCN) to provide node embeddings for score evaluation.\nTo represent aggregation in one layer formally, the complete\naggregation with an activation function of layer l is formally\nwritten as:\n\\(H^{l} = \\sigma(A H^{l-1} W^{l-1}),\n(3)\nwhere A = D-1/2\u00c3\u010e-1/2 and \u00c3 = A + I. To clarify, I is an\nidentity matrix and D is the degree matrix. \u03c3(\u00b7) denotes an\nactivation function, and the simplified expression is shown in\nEq. 4. Graph neural networks are usually designed with several\nlayers L to keep the long-range information in the network.\nFinally, node representations HL \u2208 Rd\u00d7r are obtained, i.e.,\n\\(H^{l} = GCN(A, H^{l-1}).\n(4)\nGiven the above node embeddings, the detector evaluates an\nanomaly score for each node, namely the probability of being\nanomalous nodes. The score evaluation is composed of two\nsimple linear layers as follows:\n\\(\\hat{S} = W_2(\\sigma(W_1H^L + b_1)) + b_2,\n(5)\nwhere \u015c denotes the predicted scores for all nodes and \u03c3(\u00b7)\nis an activate function. \u015di denotes the score of node vi. If\n\u015di \u2248 1 vi is anomalous, but if \u015di \u2248 0 node vi is more likely to\nbe normal. However, if the score is difficult to discern labels\n\u015di \u2248 5, vi has a high possibility to be a candidate with incorrect\nlabels.\n2) Explore reliable sets and the candidate set: By ranking\n\u015c, two high-confidence node sets are filtered out based on a\nhyper-parameter rate a, i.e., anomaly set AS = {U1,...UNas}\n(the most suspicious), normal set NS = {1,... UNns} (the\nleast suspicious). They are selected for label rectification to\nguarantee less noisy ground-truth labels:\nAS = {vi|si \u2208 ftop(\u015c)},\n(6)\nNS = {Vi|\u015di \u2208 ftop(-\u015c)},\n(7)\nwhere ftop is the node filtering function. Next, AS includes\nhighly possible anomaly nodes, and thus ground truth labels\nVAS should be updated as anomaly labels, i.e.,\n\\(Y\\'_i = \\begin{cases}\n1 & \\text{if } v_i \\in AS;\\\\\nY_i & \\text{else if } v_i \\in NS;\\\\\n\\end{cases}\n(8)\nwhere y' denotes more reliable ground-truth labels than the\nnoisy one \u00ddL, NS are normal node sets. Besides, the base\ndetector is pre-trained for higher reliability to predict scores\nand update two sets.\nSelecting a candidate set that may be anomalous from the\nremaining nodes is significant. Candidates directly determine\nthe search space and quality of the pruner since a small\nproportion has incorrect labels. Therefore, we propose a\nmethod based on the multi-armed bandit algorithm B(A, f, T)\nto choose an appropriate threshold & deviated from the mean\nvalue of the balanced batches\u2019 anomaly scores. A is the\naction space, f is the reward function, and T is the terminal\ncondition. This threshold especially assists in restricting the\nscore range of candidates, facilitating their edge-cutting as\npotential anomalies as:\n\\(N_C = \\{v_i | s_i \\in \\bar{s} \\pm \\delta\\}\n(9)\nConcretely, we formulated the threshold selection based on\ne-greedy computation as the following steps:\n\u2022 Action. In t-th iteration, a threshold value dt is randomly\nselected for exploration with a probability of e, while with\na probability of (1-\u20ac), the current threshold is exploited.\n\\(a_t = \\begin{cases}\n\\delta_{\\epsilon}, & {\\epsilon}\\\\\n\\argmax_d \\text{AUC}(S_{NC}, V_{NC}), & 1 - {\\epsilon}\n\\end{cases}\n(10)\nReward. For each threshold dt, the reward is the AUC\nvalue corresponding to the node candidate set whose\nscores fall within the range from (s \u2013 dt) to (s+ dt). The\nreward value rt is computed by comparing candidates\u2019\nscores with the ground-truth labels as:\nrt = AUC(SNC, Vvc), \u03b4 = \u03b4\u03b5\n(11)\nTo solve a special case with only one class, we randomly\nchoose an anomaly node and one normal node to put in\nthis candidate set to ensure reward computation."}, {"title": "B. Edge Pruner", "content": "After determining the noisy candidate set NC from the\nbase detector fa, we leverage edge pruner fe, as well as\nby reinforcement learning method, to remove edges centering\nnodes with noisy labels on the graph. The aim of the pruner\nis to learn a strategy to reshape the graph while keeping the\nsemantic representation. The refined graph has more accurate\npattern learning than before because edge-cutting prevents\ncorrupted information transmission and leads to fewer nodes\nbeing corrupted from noisy ground-truth labels\u2019 supervision.\nFor normal nodes, edge-cutting has few negative effects since\nthey typically have numerous edges connecting to other similar\nnormal nodes. However, for anomaly nodes, cutting off edges\naround anomaly nodes highlights the impact of node features\non score evaluation, reducing the assimilation of normal nodes.\nThereby obtaining more accurate anomaly node patterns for\ndirect detection.\nCutting edges to address noisy labels in graph structures\nis an effective way. However, the challenge of determining\nboth the quality and quantity of selecting edges should be\nconsidered, as shown in Fig. 3. This example displays that\ncutting edges excessively risks some isolated nodes, while too\nfew edges may fail to mitigate noisy information propagation.\nFaced with this difficulty, reinforcement learning [21]\u2013[23]\nemerges as a promising approach due to efficient decision-\nmaking ability. Specifically, we utilize a policy network to\nlearn the strategy of selecting functional edges, taking node\nrepresentations as input. The output is the updated graph\nstructure and the pruned edge set Ecut.\n1) Markov Decision Process: In this section, we reformu-\nlate the edge pruner module as a Markov Decision Process\n(MDP) to train a policy network. Specifically, an MDP is\ndenoted as a tuple with (S, A, P, R, \u03b3), where S is the set\nof states, A is the set of actions, P is the state transition\nprobability function, R is the reward function, and y is\nthe discounting factor. Fig. 2 illustrates how the policy \u043f\u04e9\nmanipulates actions based on current states. The specification\nof each component is detailed as follows:\n\u2022 State (S): In t-th step, state st = (\u00c2t, Ht) is transitioned\nto the next state st+1. The initial state so = (\u00c2, H\u00b2)\ncomprises the adjacency matrix \u00c2 and the initial node\nrepresentation H\u00b9 from the base detection.\n\u2022 Action (A): Given current state st, the agent selects\naction at centered around candidate nodes in NC as\nbelow:\n\\(a_t = \\{e_{N_1}, ..., e_{N_{|N_c|}}\\}, N_a \\leq n_t\n(13)\nwhich includes Na edges of anchors simultaneously, but\nless than limitation nt. The action space is stated as\nfollows:\n\\(E_{N_C} = \\{e_{ij}, \\forall v_i \\in N_C, v_j \\in N_i\\},\n(14)\nwhere Ni denotes all neighbors of node vi. Therefore,\nat ENC. Specifically, we utilize \u0415\u0442 = at to\naggregate all pruned edges in previous actions.\nReward (R): rt plays a vital role in guiding the agent to\nchoose actions efficiently and correctly. Specifically, we\ncompute reward rt by comparing detector performance\nbefore and after action at based on the metric of AUC:\nrt = R(St, at, St+1).\n(15)\nThe agent chooses the action at with the given state st and\nthe MDP will transit to the next state st+1. The objective of\nthe agent is to learn an optimal policy \u03c0* by solving:\n\\(\\arg \\max_\\theta \\sum_{t=1}^T \\mathbb{E}_{\\pi_\\theta} \\gamma^t r_t | s_0\n(16)\nwhere so is the initial state of the MDP. The process\nof graph reconstruction can be depicted as a trajectory\n{80, A1, r1, S1, a2, r2, ..., ST}, consisting of T steps. This MDP\ncould filter out the most optimal strategy, represented by a\nseries of actions, to cut edges to reduce negative impacts of\nnoisy labels.\n2) Policy Network Design: Having outlined the MDP\nabove, we describe the architecture of the policy network\ndetermining action at. Given the current state st, the agent\nselects edges as action at for all candidates to prune. The\npolicy network consists of two GCN layers, as detailed below:\n\\(Z_t = GCN(\\hat{A}_t, H_t)\\\nP_t = GCN(Z_t, H_t)\n(17)\nwhere GCN(\u00b7) represents one GCN layer. \u00c2t and Ht are\ncomponents of state st. The output Pt is a probability matrix,\nrepresenting probabilities of edges being pruned. In the next\nstage, action at is derived by selecting the top-k samples based\non Pt for each candidate in NC as follows:\n\\(a_t = \\bigcup_{\\forall v_i \\in N_C} \\text{Top}_k((P_t \\odot M)[i, :]),\n(18)\nwhere Pt[i,:] contains probabilities of edges connected to\ncandidate vi; M represents a mask matrix to exclude those\npruned edges, where a value of 1 indicates that the edge\nis eligible for selection, and a value of 0 indicates that the\nedge is not eligible; Topk(\u00b7) gives all existing top-k highest\nprobabilities; at aggregates selected edges for all nodes in NC.\nThis approach ensures that the edge selection process adheres\nto the constraints specified by the mask matrix M.\n3) State Transition: Once the action at is determined, the\ngraph is updated by pruning selected edges in at. The state\ntransition from st to st+1 is written as:\nSt+1 = Pat(St),\n(19)\nwhich involves: 1) \u00c2t is modified by setting the element\ncorresponding to the selected edges to zero, resulting in a\nnew adjacency matrix At+1; 2) The node embeddings Ht\nare updated by passing the modified adjacency matrix At+1\nthrough the base detection module. The new node embeddings\nHt+1 are stated as:\nSt+1 = (At - at, fa(At - at)).\n(20)\n4) Reward Design: After the state transition, the reward\nrt is computed to evaluate the effectiveness of the action at.\nSpecifically, The reward function R is defined by comparing\ndetector performance, represented by results under the metric\nof AUC before and after action at as:\nrt = AUC(fa(Gt+1), Y') \u2013 AUC(fa(Gt), V')\n(21)\nwhere AUC() calculates values by comparing truth labels and\npredicted scores. Most importantly, we utilize the confident\nupdated truth labels y' instead of noisy truth \u00ddL. Once the\npolicy chooses action at, including multiple edges around\nanchor nodes, the new graph Gt+1 can be easily obtained\nbased on st+1. If the new graph performs better, rt is positive.\nOtherwise, the agent gets a negative reward."}, {"title": "C. Policy-in-the-loop", "content": "The policy-in-the-loop frame is depicted as a cyclic frame-\nwork where the base detector and pruner interact, providing\nnecessary feedback to refine the graph structure. Concretely,\nthe base detector assigns anomaly scores to detect anomalies\naccording to graph structures, and the edge pruner manages to\nreconstruct the graph to address noisy information transmis-\nsion, which allows the detector to prioritize anomaly pattern\nidentification. In essence, edges linking normal nodes are\nmitigated and would lead to little influence due to numer-\nous connections with other normal nodes. Conversely, edges\naround anomaly nodes have significant impacts because of\nlimited edges.\n1) Policy Gradient Learning: The objective function is\nformulated to maximize the total reward during the Markov\nDecision Process (MDP) using the REINFORCE method,\nformulated as:\n\\(L_p = - \\text{softmax}(\\sum_{t=1}^T \\sum_{i,j \\in a_t} P_{e_{ij}} \\gamma^t r_t),\n(22)\nwhere Lp denotes the loss incurred during training of the\npolicy network and p is the probability value from matrix\nPt corresponding to edge eij in action at. Yt is the discount\nfactor, and rt is the reward received at time step t.\nThen, the training stage aims to update the policy network\nparameters 0 using the policy gradient:\n\\(\\theta \\leftarrow \\theta + l\\nabla_{\\theta} L_p,\n(23)\nwhere l is the learning rate. The quantity of pruned edges is\ncrucial as over-cutting may lead to numerous isolated nodes,\nwhile under-cutting may fail to reduce noisy information\npropagation. To address these issues, a terminal condition is\nset to ensure a balance in edge pruning and is terminated as:\nNecut = \u03b7\u039d\u03b7\u03c2,\n(24)\nwhere Necut represents the total edge count in previous\nactions, and Nnc is the number of nodes in NC. \u03b7 denotes\nthe rate to control the maximum number of pruned edges as\na hyper-parameter. This ensures the policy network finds a\nbalance in edge pruning to reduce the negative impacts of\nnodes with noisy labels."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we conduct experiments to evaluate REGAD\nperformance and answer the following questions:\n\u2022 RQ1: How effective is the proposed method REGAD for\nanomaly detection under the noisy label setting?\n\u2022 RQ2: What are the impacts of noisy label ratio on\nREGAD?\n\u2022 RQ3: What are the performances of REGAD under\ndifferent anomaly label numbers?\n\u2022 RQ4: Is REGAD sensitive to the hyper-parameters?\n\u2022 RQ5: How does our proposed method work in practice,\nespecially the cutting-edge mechanism?"}, {"title": "A. Experimental Settings", "content": "Datasets. We adopt three real-world attributed graphs that\nhave been widely used in related studies in Table I. We\nfollow the standard setup of graph anomaly detection research\n[24], [25] and regard the nodes from the smallest class(es) as\nanomaly data (i.e., rare categories) while taking nodes from\nthe other classes as \"normal\" data. The details of three datasets\nare as follows:\n\u2022 Clothing [26]: This dataset includes items such as cloth-\ning and jewelry from the Amazon website as nodes. The\nedges in this network represent instances in which two\nproducts are purchased together.\n\u2022 Computer [27]: This dataset is a segment of the co-\npurchase graphs about computer-related items. Nodes\nrepresent individual device products, and edges represent\nfrequent purchasing behaviors.\n\u2022 Photo [27]: This dataset focuses on photography-related\nproducts, similar to the Computer dataset.\nBaselines. We compare the proposed method with three\ngroups of methods for effectiveness evaluation, including\n(1) unsupervised GAD methods (e.g., DOM and ComGA),\n(2) semi-supervised GAD methods (e.g., Meta-GDN, Deep-\nSAD, CHRN, BWGNN), (3) noisy label learning methods\n(e.g., D2PT, RTGNN, PIGNN). The details are as follows:\nDOM [28] is an unsupervised method including the de-\ncoder and encoder by reconstructing adjacent matrix to detect\nanomalies. ComGA [19] designs a community-aware method\nto obtain representations to predict anomaly scores. Meta-\nGDN [15] is the simple version of an anomaly detection model\nbased on GCN leveraging balanced batch sizes and deviation\nloss between abnormal and normal nodes. DeepSAD [29]\nevaluates the entropy of the latent distribution for normal\nnodes and anomalous nodes to classify. BWGNN [20] is\nproposed to address the 'right-shift' phenomenon of graph\nspectrum by Beta Wavelet Graph Neural Network. CHRN [13]\naddresses the heterophily of the GAD task by emphasizing\nhigh-frequency components by graph Laplacian. D2PT [30] in-\nnovates a dual-channel GNN framework, increasing robustness\nconsidering the augmented and original global graphs. RT-\nGNN [31] focuses on noise governance by self-reinforcement\nsupervision module and consistency regularization after graph\naugmentation. PIGNN [32] leverages structural pairwise inter-\nactions (PI) to propose a PI-aware model to manage noise.\nEvaluation Metrics. Following previous papers [15], [33],\nwe adopt two metrics, i.e., AUC and AUPR, that have been\nwidely used for GAD. AUC denotes the area under the ROC\ncurve, which illustrates the true positive rate against the false\npositive rate. AUPR is the area under the Precision-Recall\ncurve, showing the trade-off between precision and recall.\nImplementation Details. All datasets are split into training\n(40%), validation (20%), and test (40%) sets. Our research\nproblem includes noisy labels, which are difficult to dis-\ntinguish during the training and validation phase. However,\ncurrent datasets are pre-processed and basically contain clean\nlabels. To simulate the real-world scenarios of noisy labels, we\nfollow related papers [34], [35] and induce noisy labels into the\ndatasets with label flipping and then mix corrupted labels with\ncorrect ones. Specifically, Label flipping is a common method\nfor introducing noisy labels, typically including uniform noise\nand pair noise, applied for node classification tasks. We\ngenerate noisy labels for anomaly detection by uniformly\nswapping normal nodes into the anomaly class at a designated\nrate.\nIn the experiments, if not further specified, the number of\nlabeled anomalies is set to 30, and the noisy label ratio is 50%.\nMoreover, we adopt a pre-trained base detector with 2 layers\nof GCN with 128 hidden units to learn node representations"}, {"title": "B. Effectiveness Analysis (RQ1)", "content": "We present the results of AUC and AUPR in Table II and\nhave the following observations: (1) Semi-supervised mod-\nels (e.g., DeepSAD) marginally outperform the unsupervised\nmodels (e.g., ComGA). This suggests that supervised ground\ntruth labels, even with noisy labels, can still guide learning\nabnormal patterns. (2) The noisy label learning GNN methods\n(e.g., RTGNN and D2PT) achieve sub-optimal performances\non the GAD datasets compared with semi-supervised GAD\nmodels. It verifies that the existing GNNs that exploit noisy\nlabels for node classification tasks are unsuitable for the GAD\ntask. (3) The performance of our method REGAD significantly\nsurpasses the baselines under the noisy label scenario. Inspired\nby semi-supervised and noisy label learning methods, REGAD\nrelies less on truth labels and mitigates the influence of noisy\nlabels from graph topology. This is because REGAD refines\nthe graph structure to reduce noise propagation and rectifies\nincorrect labels simultaneously, which helps to provide accu-\nrate supervision for learning abnormal patterns in the policy-\nin-the-loop framework."}, {"title": "C. Analysis of Noisy Label Ratio (RQ2)", "content": "In this section, we implement experiments with different\nnoisy label ratios, i.e., {10%, 30%, 50%, 70%, 90%} to\nanalyze the robustness of REGAD. Firstly, we investigate\nthe impacts of noisy label ratio under fixed labeled anomaly\nnodes. The corresponding number of nodes with clearly correct\nlabels are {27,21,15,9,3} on three datasets. In Fig 4(a), we\nobserve that the performance declines as the noisy label ratio\nincreases. In addition, REGAD demonstrates effective detec-\ntion results across different levels. Despite variations in the\nsize and sparsity of datasets, REGAD\u2019s performance remains\nconsistent. The results are likely attributed to the ability of\nREGAD to effectively mitigate the influence of nodes with\nnoisy labels on their neighboring nodes. Therefore, our model\nmaintains relatively good results even under extremely noisy\nrate conditions.\nIn Fig. 4(b), the results based on the AUPR metric are\npresented. The overall trend of this curve is downward as\nthe noisy label rate increases, which aligns with the expected\nnoisy label impacts. However, fluctuations are observed: for\ninstance, there is an initial increase followed by a continuous\ndecrease from 30% to 50%, particularly in the Computer and\nPhoto datasets. Introducing noisy labels within a proper range\n(30%-50%) may have a regularization effect, enabling the\nmodel to learn anomaly patterns better, thereby improving\nAUPR. Additionally, since the AUPR metric is more sensitive\nto predictions of minority classes, our model exhibits greater\nrobustness at moderate noise-label rates. The Clothing dataset\nremains stable under different settings, which is attributed to\nREGAD\u2019s effectiveness on sparse graphs."}, {"title": "D. Analysis of Labeled Anomaly Number (RQ3)", "content": "In this subsection, our main focus is to explore REGAD\nperformance under different Anomaly label numbers. We set\nlabeled anomaly nodes as {10, 20, 30, 40} respectively with a\nfixed noisy label ratio w.r.t. 50%. Under the AUC in Fig. 5(a),\nwhen labeled anomaly nodes increase, the performance of the\nthree datasets improves. This is expected because a higher\nnumber of reliable anomaly labels provides more accurate\nsupervised information. Even in extreme cases, such as with\nonly 10 anomaly labels, REGAD achieves relatively good\nresults. This indicates that rectifying some noisy ground\ntruth labels and cutting edges can help mitigate noise in\nprediction collaboratively. The model exhibits a similar trend\nwhen evaluated using AUPR, except for the Photo dataset,\nwhere the performance decreases as labeled outliers increase.\nWe speculate that increased labeled data can raise model\ncomplexity, particularly with many noisy labels, leading to\noverfitting of noisy-label nodes. Additionally, the cumulative\neffect of noise can hinder the ability of REGAD to extract\nuseful information, resulting in decreased performance."}, {"title": "E. Sensitivity Analysis (RQ4)", "content": "This section examines whether our model is sensitive to\nthe three key hyper-parameters: (1) rate a determines rec-\ntified label rate; (2) limitation nt constrains edges chosen\nby each action. The results of three datasets are shown in\nFig. 6 following the implementation setting (30 anomaly labels\nand 50% noisy labels). Furthermore, two parameters are set\nas {0.001, 0.003, 0.005, 0.007, 0.009}, {60,80,100,120,140}\nrespectively. Specifically, in Fig. 6a, Fig. 6c, and Fig. 6e, we\ncan observe that REGAD is not very sensitive to these hyper-\nparameters, especially on the Photo dataset. For the a, smaller\nvalues (e.g., 0.001-0.005) are generally recommended because\na determines the proportion of labels directly modified based\non the detector\u2019s prediction. If the proportion is too high, it\nmight introduce noise, requiring more actions to correct it,\ne.g., episode T=20. nt sets the maximum number of edges\nthat can be chosen per action, allowing for a more gradual\nand controlled edge selection process in Fig. 6b, Fig. 6d, and\nFig. 6f. The value is recommended in the middle level (i.e.,\n80-100). Conversely, the limitation is widened to 140, and the\nperformance falls sharply. This suggests our model\u2019s stable\nand robust behavior, reaffirming its effectiveness across varied\nsettings. In conclusion, findings underscore the reliability of\nREGAD in anomaly detection tasks."}, {"title": "F. Case Study (RQ5)", "content": "To further investigate how edge pruner functions effectively\nin REGAD, we analyze selected edges in actions of different\nepisodes and epochs. This case study explores how the number\nof edges selected in each MDP varies across different episodes\nand epochs in Fig 7. We aim to understand how the pruner\ncompletes the graph construction process in several steps and\nwhether the agent exhibits different strategies across episodes.\nWe observe some interesting phenomena. (1) The average\ncut edges have a small difference among epochs, indicating\na stable performance within one epoch. (2) There is a sig-\nnificant variation of selected edges among different epochs.\nThis variability is likely attributed to the process of policy\noptimization, where the model continuously explores different\nedge-pruning strategies. (3) Conversely, episodes within a\nsingle epoch tend to be more similar. The possible reason\ncould be the model\u2019s adaptation to newly learned patterns,\nwhich changes pruning decisions gradually. (4) The second\naction\u2019s edges decrease noticeably across three datasets. This\nsuggests that the agent can swiftly output edge-pruning actions\nwith appropriate hyper-parameter settings, leading to a more\nefficient and effective MDP. In short, the listed examples reveal\nthat REGAD refines graph structure to mitigate the influence\nof noisy labels for model learning and detecting anomaly\nnodes effectively."}, {"title": "V. RELATED WORK", "content": "Graph anomaly detection (GAD) is a crucial task to identify\ngraph objects that deviate from the main distribution [36], [37]\nwith inherent connections and complex structures. Previous\nGAD methods roughly fall into three types: traditional, GNN-\nbased, and hybrid methods.\nTraditional methods [38] distinguish anomaly nodes from\nnumerous nodes by learning feature patterns. Techniques in-\ncluding matrix factorization, KNN, and linear regression focus\non evaluating similarity in the feature space. For example, [39]\nand [40] map normal nodes into a hyper-sphere, which filters\nout abnormal nodes outside that space. To achieve better\ndetection performance, [20] and [18] converge on tackling\nthe right-shift phenomenon of graph anomaly nodes. GNN-\nbased methods [41], [42] utilize graph topology and manage\nneighborhood information, i.e., local node affinity [43]. A\nbenchmark [44] illustrates that simple supervised ensembles\nwith neighborhood aggregation also perform well on GAD.\nUnsupervised graph auto-encoder methods [28], [45] encode\ngraphs and reconstruct graph structure by the decoder to detect\nanomalies. Some research leverages anomalous subgraphs [46]\nor duel channel GNN [13] to provide graph national causing\nabnormality and augment neighbors based on similarity. As\nfor hybrid methods, approaches are applied in GNN-based\nmethods such as meta-learning [15], contrastive learning [4],\n[14], and active learning [3]. This method\u2019s biggest challenge\nis applying these deep-learning methods to instruct GAD.\nHowever, existing GAD methods often assume that clean\nand correct labels are available despite the expensive cost\nof confident annotations. Inspired by hybrid methods, this\npaper designs an effective detection model combined with\nreinforcement learning to address the challenge of noisy labels."}, {"title": "B. Noisy Label Learning on Graph", "content": "The impact of noisy labels, including incomplete, inex-\nact, and inaccurate labels in graphs, is relatively underex-\nplored [32], [35], [47]. Graphs would render noisy informa-\ntion to truth labels and lead to poor detection performance.\nCurrent research endeavors meta-learning and augmentation\nto improve robustness. MetaGIN [48] obtains noise-reduced\nrepresentations by interpolation and utilizes meta-learning to\nreduce label noise. Similarly, LPM [47] introduces meta-\nlearning to optimize the label propagation, thereby reducing\nthe negative effects of noisy information. RTGNN [31] and\nNRGNN [35] link unlabeled nodes with labeled nodes accord-\ning to high similarity to augment graphs and predict pseudo\nlabels for unlabeled nodes. Additionally, PIGNN [32] utilizes\npair interactions, and D2PT [30] proposes dual channels of the\ninitial graph to augment information to learn representations.\nHowever, the above methods are primarily designed for node\nclassification tasks and may achieve sub-optimal performance\nfor anomaly detection due to imbalanced distribution. Hence,\nwe formulate the research problem of GAD with noisy labels\nthat effectively leverage the available imperfect labels for\neffective anomaly detection."}, {"title": "C. Graph Reinforcement Learning", "content": "Recent research has revolved around integrating Deep Re-\ninforcement Learning (DRL) with GNN due to their com-\nplementary strengths in various tasks [49], [50], such as\nnode classification with generalization. GraphMixup [51] and\nGraphSR [21] both employ reinforcement learning to augment\nminority classes by generating edges for unlabeled nodes,\neffectively addressing imbalanced data. Moreover, some re-\nsearch has explored the use of Reinforcement Learning for\nadversarial attacks in graph [52], [53] that employ generate\nvirtual edges or detect attacked edges. However, designing ef-\nfective reward functions remains challenging. Graph reinforce-\nment learning is also utilized in tasks like explanation [23]\nand sparsification [54], identifying significant edges or prunes\nirrelevant ones. Furthermore, anomaly detection tasks based on\nthe assumption of clean labels leverage RL to select and filter\nneighborhood information propagation in GNN [9], [55], [56],\nthereby simplifying anomaly node pattern learning. However,\nthey consider all nodes as targets for information filtering,\nwhich poses a large search space for RL. These approaches are\nunsuitable for situations where noisy labels exist, especially\nin the case of imbalanced data. Since DRL has not yet\nbeen applied to address the issue of noisy labels, we tackle\nnoisy label influence by selecting edges to reconstruct graph\nstructure based on the policy-based method."}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose a policy-in-the-loop framework,\nREGAD, for the task of graph anomaly detection with noisy\nlabels. We apply reinforcement learning to reconstruct graph\nstructures, aiming to mitigate the negative impacts of noisy in-\nformation by maximizing the AUC performance before and af-\nter reconstruction. Confident pseudo-labels are used as ground\ntruth for suspicious labels to improve credibility. Experiments\nshow that REGAD generally outperforms baseline methods\non three datasets. REGAD demonstrates notable effectiveness,\nparticularly with the edge pruner module on large datasets due\nto the smaller search space compared to typical models with\nreinforcement learning in the GAD task. Future work should\nfocus on anomaly detection in sparse or complex graphs and\nselecting candidates from different base detectors by different\nstandards."}]}