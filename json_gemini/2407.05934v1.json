{"title": "Graph Anomaly Detection with Noisy Labels by Reinforcement Learning", "authors": ["Zhu Wang", "Shuang Zhou", "Junnan Dong", "Chang Yang", "Xiao Huang", "Shengjie Zhao"], "abstract": "Graph anomaly detection (GAD) has been widely applied in many areas, e.g., fraud detection in finance and robot accounts in social networks. Existing methods are dedicated to identifying the outlier nodes that deviate from normal ones. While they heavily rely on high-quality annotation, which is hard to obtain in real-world scenarios, this could lead to severely degraded performance based on noisy labels. Thus, we are motivated to cut the edges of suspicious nodes to alleviate the impact of noise. However, it remains difficult to precisely identify the nodes with noisy labels. Moreover, it is hard to quantitatively evaluate the regret of cutting the edges, which may have either positive or negative influences. To this end, we propose a novel framework REGAD, i.e., REinforced Graph Anomaly Detector. Specifically, we aim to maximize the performance improvement (AUC) of a base detector by cutting noisy edges approximated through the nodes with high-confidence labels. (i) We design a tailored action and search space to train a policy network to carefully prune edges step by step, where only a few suspicious edges are prioritized in each step. (ii) We design a policy-in-the-loop mechanism to iteratively optimize the policy based on the feedback from base detector. The overall performance is evaluated by the cumulative rewards. Extensive experiments are conducted on three datasets under different anomaly ratios. The results indicate the superior performance of our proposed REGAD.", "sections": [{"title": "I. INTRODUCTION", "content": "Graphs have been prevalently adopted to effectively rep- resent relational information in many areas, e.g., social net- works [1] and recommendation systems [2]. While graphs could be at billion scales with tremendous nodes, errors are inevitably introduced [3], [4]. Graph anomaly detection (GAD) plays an important role in many real-world scenarios, e.g., fake news detection [5]\u2013[7] and fraud detection [8], [9]. Research has been conducted based on various designs to identify anomaly nodes that deviate from the majority in the graph. Early studies utilize traditional methods [10], [11], i.e., matrix factorization and KNN, to extract feature patterns of outliers. After that, GNNs are designed with loss function design or effective information aggregation strategies [12], [13] to capture the features of anomaly nodes. Recently, various deep learning-based methods, e.g., meta-learning, active learning, and contrastive learning, filter out information aggregation in GNNs to reduce the influence of anomalous nodes [14], [15] on the patterns of normal nodes and minimize the assimilation effect from normal nodes.\nHowever, the performance of existing methods remains unsatisfactory to meet industrial needs. While they heavily rely on high-quality labels to facilitate supervised or semi- supervised learning, this hypothesis can hardly stand in real- world scenarios since the annotations are difficult to obtain. First, crowd-sourcing annotators can be unreliable in labeling the complex graph structure [16]. Ensuring high-quality labels requires careful data cleaning. Second, it is unaffordable to annotate extremely large graphs by involving human experts. In Fig.1, we showcase the impacts of noisy labels on two benchmark datasets. We stimulate the noise by label flipping in a heuristic way, and further details can be found in the Experiment Setting section. It is obvious that noisy labels significantly decline several semi-supervised detection models' performance based on the metric of the area under the ROC curve. Consequently, noisy labels inevitably exist in the graphs [17]. The corresponding noise information would propagate through neighbors and affect the representation learning abil- ity. A careful denoising method is urged for effective GAD. we are motivated to cut the edges of suspicious nodes to alleviate the impact of noise during propagation. Nevertheless, this task is challenging since determining the noise candidates to be pruned is laborious. Given the large scale of real-world graphs, the search space is extremely big, making it difficult for the edge pruner to prioritize suspicious nodes. Moreover, it is hard to quantitatively evaluate the regret of cutting the edges which may bring either positive or negative influences. on one hand, we wish to cut as many suspicious nodes as possible. This over-prune may remove normal edges and result in a sparse structure, which may affect the graph learning ability. On the other hand, we require sufficient information by preserving enough edges, while an under-prune can hardly satisfy the purpose of noise mitigation.\nTo this end, we propose a novel policy-in-the-loop frame- work, the REinforced Graph Anomaly Detection model, i.e., REGAD, to learn from noisy labels for robust GAD effectively. Specifically, we aim to maximize the performance improve- ment (AUC) of a base detector by cutting noisy edges with an edge pruner. This is approximated through the nodes with a set of high-confidence labels generated by the pre-trained base detector since true labels are not readily available in the real world. (i) We design a tailored action and search space to train a policy network to carefully make decisions and cut the suspicious edges step by step, where only a few suspicious edges are prioritized in each step. (ii) We design a policy-in-the-loop mechanism to iteratively optimize the policy based on the feedback from the base detector, while the base detector correspondingly updates the sets of high-confidence labels based on the reconstructed graph structure by the edge pruner. The overall performance is evaluated by the cumulative rewards that have been received based on the performance (AUC).\nIn general, we summarize our contributions below:\n1) We formally define the problem of noisy label learning for graph anomaly detection.\n2) A tailored policy network is designed to carefully identify noisy labels and optimize the edge pruning by prioritizing the most suspicious nodes.\n3) We design a novel policy-in-the-loop learning paradigm for GAD with noisy labels. GAD and the policy comple- mentarily benefit each other to mitigate the impacts of noisy labels.\n4) Extensive experiments are conducted to comprehensively demonstrate the superiority of our framework under 50% error rates on three datasets."}, {"title": "II. PROBLEM STATEMENT", "content": "A. Notation\nWe use G = (V,E,X) to denote an attributed graph, where V = {V1, V2, ..., Un} is the set of n nodes, and E \u2286 V \u00d7 V is the set of edges. Besides, X = {X1, X2, ..., Xn} is the node attributes, X \u2208 Rn\u00d7d, and d is the attribute dimension. A \u2208\u0454 Rnxn represents the adjacency matrix of G. If vi and vj are connected, Aij = 1. Otherwise, Aij = 0. VL = {V1, V2, ..., vi} represents labeled nodes, and Vu = V \u2013 VL is the set of unlabeled nodes. V\u2081 includes normal nodes Vn and abnormal nodes Va. In practice, we only obtain anomaly nodes Va following |Va| < |Vn|. YL = {Y1, Y2, ..., y\u0131} denotes the real ground truth labels of VL. However, in our research problem, we assume they are not always trustworthy and correct and utilize YL = {Y1, Y2, \u2026, Y\u0131} to represent the noisy ground truth corrupted by noise.\nB. Problem Definition\nGraph Anomaly Detection. Given an attributed graph G = (V,E,X), the GAD task is formulated as:\n\\(f(G, V_L) \\rightarrow \\hat{S},\\)\n(1)\nwhere anomaly scores \u015c reflect the likelihood or degree of being an anomaly node. Higher \u015d\u00bf means a higher possibility of being detected as an anomaly vi \u2208 Va. These predicted scores provide evidence to identify more anomaly nodes except existing Va with high accuracy, especially in Vu.\nGraph Anomaly Detection with Noisy Labels. In this task, the ground truth labels Y\u2081 are not accurate, which means a small proportion of labeled nodes VL are erroneously labeled. In noisy ground truth labels YL, Yi = 0, i \u2208 L is mistakenly labeled, so the real one is y\u2081 = 1. However, it is difficult to distinguish which is real from YL. Under this setting, we aim to predict correct matching anomaly scores \u015c for all nodes as much as possible supervised by VL, i.e.,\n\\(f(G', V_L) \\rightarrow \\hat{S},\\)\n(2)\nwhere the efficient detection model f is to estimate the proba- bilities of nodes being anomalous. In contrast to classification tasks where multiple classes are considered, only two types, anomaly nodes Va and normal nodes Vn are included in VL. Besides, labeled nodes are significantly smaller than the number of unlabeled nodes, denoted as |VL| < |VU|. To stimulate YL, we will use label flipping introduced in the experiment section in detail."}, {"title": "III. METHODOLOGY", "content": "In this section, we propose the REinforced Graph Anomaly Detection model in Fig. 2. Our model seeks to correct noisy truth labels and minimize the impact of noisy labels on neigh- borhood nodes based on the reinforcement learning method, designed as a policy-in-the-loop framework introduced in Sec- tion III-C. From the above targets, there are three challenges: 1) How can we identify noisy labels given that ground truths are not readily available? 2) Given the complicated graph structure, how can we control the negative impacts from nodes with noisy labels to their neighbors? 3) How can we quantitatively evaluate the influences of the edge pruner, i.e., positive (cut the real noisy edges) or negative (miscut edges)?\nTo address the first challenge, we employ a base detector to assess the anomaly score of each node supervised by noisy ground truth YL. It takes node attributes as the input and gives two confident node sets, anomaly set AS, normal set NS, and a noisy-label candidate set NC. These candidates are the anchors in the following edge pruner, which is leveraged to solve the second challenge. To control incorrect information transmission, we apply reinforcement learning to learn a policy \u03c0\u03c1(\u03b1\u03c2) to cut edges of candidates to manage noisy labels' influence. Additionally, we employ the base detector perfor- mance based on the refined structure to evaluate the pruner quantitatively by returning rewards. These two components collaborate by exchanging information, forming the policy-in- the-loop framework. Next, each part is introduced in detail.\nA. Base Detector\nThe base detector fa is proposed to predict anomaly scores and generate pseudo labels, especially for unlabeled data su- pervised by noisy ground truth labels YL. Although we employ those training datasets, nodes with noisy labels, this detector could provide more information by predictions for unlabeled nodes. From this perspective, not all predictions are reliable, and thus we identify trustworthy anomaly set AS, normal set NS for obtaining pseudo labels and modifying incorrect labels from score ranking. Nevertheless, due to supervision under noisy labels, the effective identification of candidates to be targeted by the pruner is made possible. To identify candidates NC for cutting edges and reduce noisy labels' influence, we use a multi-armed bandit to determine which nodes are most likely to be candidates. Most candidates consist of erroneously labeled anomalous nodes hidden among the unlabeled ones, although there are also some normal nodes mislabeled as anomalous.\nThe base detector employs the Meta-GDN model [18] not only because it provides accurate scores and superior performance in GAD tasks but also because it addresses the issue of data imbalance [12], [13], [18]\u2013[20] through balanced batches. Meta-GDN ensures that when using reliable sets for label rectification, the goal is to correct erroneous labels simultaneously, minimizing the production of incorrect labels. Additionally, based on the predicted scores ranking from Meta- GDN, candidates are identified using a multi-armed bandit approach to find the optimal pace deviated from mid-range scores, thereby locating nodes with potentially false labels. More importantly, rewards for the multi-armed bandit are designed based on the AUC corresponding to the selected nodes. As part of the iterative loop, Meta-GDN fulfills the aforementioned functions, updates node representations based on the refined graph, and calculates effective rewards for the pruner.\n1) Predict anomaly scores: The base detector maps node features into the low-dimensional latent space to learn node representations. We apply a simple Graph Convolutional Net- work (GCN) to provide node embeddings for score evaluation. To represent aggregation in one layer formally, the complete aggregation with an activation function of layer l is formally written as:\n\\(H^l = \\sigma (A H^{l-1} W^{l-1}),\\)\n(3)\nwhere A = D-1/2\u00c3\u010e-1/2 and \u00c3 = A + I. To clarify, I is an identity matrix and D is the degree matrix. \u03c3(\u00b7) denotes an activation function, and the simplified expression is shown in Eq. 4. Graph neural networks are usually designed with several layers L to keep the long-range information in the network. Finally, node representations HL \u2208 Rd\u00d7r are obtained, i.e.,\n\\(H^l = GCN(\\hat{A}, H^{l-1}).\\)\n(4)\nGiven the above node embeddings, the detector evaluates an anomaly score for each node, namely the probability of being anomalous nodes. The score evaluation is composed of two simple linear layers as follows:\n\\(\\hat{S} = W_2(\\sigma(W_1H^L + b_1)) + b_2,\\)\n(5)\nwhere \u015c denotes the predicted scores for all nodes and \u03c3(\u00b7) is an activate function. \u015di denotes the score of node vi. If \u015di \u2248 1 vi is anomalous, but if \u015di \u2248 0 node vi is more likely to be normal. However, if the score is difficult to discern labels \u015di \u2248 5, vi has a high possibility to be a candidate with incorrect labels.\n2) Explore reliable sets and the candidate set: By ranking \u015c, two high-confidence node sets are filtered out based on a hyper-parameter rate a, i.e., anomaly set AS = {U1,...UNas} (the most suspicious), normal set NS = {1,... UNns} (the least suspicious). They are selected for label rectification to guarantee less noisy ground-truth labels:\n\\(AS = \\{v_i | s_i \\in f_{top}(\\hat{S})\\},\\)\n(6)\n\\(NS = \\{v_i | s_i \\in f_{top}(-\\hat{S})\\},\\)\n(7)\nwhere ftop is the node filtering function. Next, AS includes highly possible anomaly nodes, and thus ground truth labels VAS should be updated as anomaly labels, i.e.,\n\\(\\hat{y}'_i = \\{\\begin{array}{ll}\n1 & \\text{if } v_i \\in AS; \\\\\n0 & \\text{if } v_i \\in NS; \\\\\ny_i & \\text{else},\\end{array}\\}\n(8)\nwhere y' denotes more reliable ground-truth labels than the noisy one \u00dcL, NS are normal node sets. Besides, the base detector is pre-trained for higher reliability to predict scores and update two sets.\nSelecting a candidate set that may be anomalous from the remaining nodes is significant. Candidates directly determine the search space and quality of the pruner since a small proportion has incorrect labels. Therefore, we propose a method based on the multi-armed bandit algorithm B(A, f, T) to choose an appropriate threshold \u03b4 deviated from the mean value of the balanced batches' anomaly scores. A is the action space, f is the reward function, and T is the terminal condition. This threshold especially assists in restricting the score range of candidates, facilitating their edge-cutting as potential anomalies as:\n\\(N_C = \\{v_i | s_i \\in \\bar{s} \\pm \\delta\\}\\)\n(9)\nConcretely, we formulated the threshold selection based on e-greedy computation as the following steps:\nAction. In t-th iteration, a threshold value dt is randomly selected for exploration with a probability of e, while with a probability of (1-\u20ac), the current threshold is exploited.\n\\(\\delta_t = \\{\\begin{array}{ll}\n\\delta_{\\epsilon}, & \\epsilon \\\\\n\\text{argmax}_\\delta AUC(S_{NC}, Y_{NC}), & 1 - \\epsilon\\end{array}\\}\\)\n(10)\nReward. For each threshold dt, the reward is the AUC value corresponding to the node candidate set whose scores fall within the range from (s \u2013 dt) to (s+ dt). The reward value rt is computed by comparing candidates' scores with the ground-truth labels as:\n\\(r_t = AUC(S_{NC}, Y_{NC}), \\delta = \\delta_t\\)\n(11)\nTo solve a special case with only one class, we randomly choose an anomaly node and one normal node to put in this candidate set to ensure reward computation.\nTerminal condition. The target is to minimize the candi- date prediction performance, and lower AUC represents these nodes as the most noisy ones to be classified by the following process:\n\\(T : r_t = min(f(\\delta_t)), t \\leq N\\)\n(12)\nThereby, the base detector could facilitate REGAD in two folds: (i) updated ground-truth labels y' provide confident supervision for GAD; (ii) the noisy candidate set NC = {1,..., VNne} guides the next edge pruner to reduce noisy information propagation.\nB. Edge Pruner\nAfter determining the noisy candidate set NC from the base detector fa, we leverage edge pruner fe, as well as by reinforcement learning method, to remove edges centering nodes with noisy labels on the graph. The aim of the pruner is to learn a strategy to reshape the graph while keeping the semantic representation. The refined graph has more accurate pattern learning than before because edge-cutting prevents corrupted information transmission and leads to fewer nodes being corrupted from noisy ground-truth labels' supervision. For normal nodes, edge-cutting has few negative effects since they typically have numerous edges connecting to other similar normal nodes. However, for anomaly nodes, cutting off edges around anomaly nodes highlights the impact of node features on score evaluation, reducing the assimilation of normal nodes. Thereby obtaining more accurate anomaly node patterns for direct detection.\nCutting edges to address noisy labels in graph structures is an effective way. However, the challenge of determining both the quality and quantity of selecting edges should be considered, as shown in Fig. 3. This example displays that cutting edges excessively risks some isolated nodes, while too few edges may fail to mitigate noisy information propagation. Faced with this difficulty, reinforcement learning [21]\u2013[23] emerges as a promising approach due to efficient decision- making ability. Specifically, we utilize a policy network to learn the strategy of selecting functional edges, taking node representations as input. The output is the updated graph structure and the pruned edge set Ecut.\n1) Markov Decision Process: In this section, we reformu- late the edge pruner module as a Markov Decision Process (MDP) to train a policy network. Specifically, an MDP is denoted as a tuple with (S, A, P, R, \u03b3), where S is the set of states, A is the set of actions, P is the state transition probability function, R is the reward function, and y is the discounting factor. Fig. 2 illustrates how the policy \u043f\u04e9 manipulates actions based on current states. The specification of each component is detailed as follows:\nState (S): In t-th step, state st = (\u00c2t, Ht) is transitioned to the next state st+1. The initial state so = (\u00c2, H\u00b2) comprises the adjacency matrix \u00c2 and the initial node representation H\u00b9 from the base detection.\nAction (A): Given current state st, the agent selects action at centered around candidate nodes in NC as below:\n\\(a_t = \\{e^{1}_{N_1}, ..., e^{m}_{N_a}\\}, N_a \\leq n_t\\)\n(13)\nwhich includes Na edges of anchors simultaneously, but less than limitation nt. The action space is stated as follows:\n\\(\\mathcal{E}_{NC} = \\{e_{ij}, \\forall i \\in N_C, v_j \\in N_i\\},\\)\n(14)\nwhere Ni denotes all neighbors of node vi. Therefore, at ENC. Specifically, we utilize \u0415\u0442 = at to aggregate all pruned edges in previous actions.\nReward (R): rt plays a vital role in guiding the agent to choose actions efficiently and correctly. Specifically, we compute reward rt by comparing detector performance before and after action at based on the metric of AUC:\n\\(r_t = R(S_t, a_t, S_{t+1}).\\)\n(15)\nThe agent chooses the action at with the given state st and the MDP will transit to the next state st+1. The objective of the agent is to learn an optimal policy \u03c0* by solving:\n\\(\\text{arg max}_\\theta \\mathbb{E}_{\\pi_{\\theta}} [\\sum_{t=1}^{T} \\gamma^t r_t | s_0],\\)\n(16)\nwhere so is the initial state of the MDP. The process of graph reconstruction can be depicted as a trajectory {80, A1, r1, S1, a2, r2, ..., ST}, consisting of T steps. This MDP could filter out the most optimal strategy, represented by a series of actions, to cut edges to reduce negative impacts of noisy labels.\n2) Policy Network Design: Having outlined the MDP above, we describe the architecture of the policy network determining action at. Given the current state st, the agent selects edges as action at for all candidates to prune. The policy network consists of two GCN layers, as detailed below:\n\\(Z_t = GCN(\\hat{A}_t, H_t),\\)\n(17)\n\\(P_t = GCN(Z_t, H_t),\\)\nwhere GCN(\u00b7) represents one GCN layer. \u00c2t and Ht are components of state st. The output Pt is a probability matrix, representing probabilities of edges being pruned. In the next stage, action at is derived by selecting the top-k samples based on Pt for each candidate in NC as follows:\n\\(a_t = \\bigcup_{\\forall i \\in N_C} Top_k((P_t \\odot M)[i, :]),\\)\n(18)\nwhere Pt[i,:] contains probabilities of edges connected to candidate vi; M represents a mask matrix to exclude those pruned edges, where a value of 1 indicates that the edge is eligible for selection, and a value of 0 indicates that the edge is not eligible; Topk(\u00b7) gives all existing top-k highest probabilities; at aggregates selected edges for all nodes in NC. This approach ensures that the edge selection process adheres to the constraints specified by the mask matrix M.\n3) State Transition: Once the action at is determined, the graph is updated by pruning selected edges in at. The state transition from st to st+1 is written as:\n\\(S_{t+1} = P_{a_t}(S_t),\\)\n(19)\nwhich involves: 1) \u00c2t is modified by setting the element corresponding to the selected edges to zero, resulting in a new adjacency matrix At+1; 2) The node embeddings Ht are updated by passing the modified adjacency matrix At+1 through the base detection module. The new node embeddings Ht+1 are stated as:\n\\(S_{t+1} = (A_t - a_t, f_a(A_t - a_t)).\\)\n(20)\n4) Reward Design: After the state transition, the reward rt is computed to evaluate the effectiveness of the action at. Specifically, The reward function R is defined by comparing detector performance, represented by results under the metric of AUC before and after action at as:\n\\(r_t = AUC(f_a(G_{t+1}), Y') - AUC(f_a(G_t), Y')\\)\n(21)\nwhere AUC() calculates values by comparing truth labels and predicted scores. Most importantly, we utilize the confident updated truth labels y' instead of noisy truth \u00ddL. Once the policy chooses action at, including multiple edges around anchor nodes, the new graph Gt+1 can be easily obtained based on st+1. If the new graph performs better, rt is positive. Otherwise, the agent gets a negative reward.\nC. Policy-in-the-loop\nThe policy-in-the-loop frame is depicted as a cyclic frame- work where the base detector and pruner interact, providing necessary feedback to refine the graph structure. Concretely, the base detector assigns anomaly scores to detect anomalies according to graph structures, and the edge pruner manages to reconstruct the graph to address noisy information transmis- sion, which allows the detector to prioritize anomaly pattern identification. In essence, edges linking normal nodes are mitigated and would lead to little influence due to numer- ous connections with other normal nodes. Conversely, edges around anomaly nodes have significant impacts because of limited edges.\n1) Policy Gradient Learning: The objective function is formulated to maximize the total reward during the Markov Decision Process (MDP) using the REINFORCE method, formulated as:\n\\(L_p = -softmax(\\sum_t \\sum_{ij \\in a_t} P_{e_{ij}} \\gamma^t r_t),\\)\n(22)\nwhere Lp denotes the loss incurred during training of the policy network and p is the probability value from matrix Pt corresponding to edge eij in action at. Yt is the discount factor, and rt is the reward received at time step t.\nThen, the training stage aims to update the policy network parameters 0 using the policy gradient:\n\\(\\theta \\leftarrow \\theta + l \\nabla_{\\theta} L_p,\\)\n(23)\nwhere l is the learning rate. The quantity of pruned edges is crucial as over-cutting may lead to numerous isolated nodes, while under-cutting may fail to reduce noisy information propagation. To address these issues, a terminal condition is set to ensure a balance in edge pruning and is terminated as:\n\\(N_{e_{cut}} = \\eta N_{NC},\\)\n(24)\nwhere Necut represents the total edge count in previous actions, and Nnc is the number of nodes in NC. \u03b7 denotes the rate to control the maximum number of pruned edges as a hyper-parameter. This ensures the policy network finds a balance in edge pruning to reduce the negative impacts of nodes with noisy labels."}, {"title": "IV. EXPERIMENTS", "content": "In this section", "questions": "nRQ1: How effective is the proposed method REGAD for anomaly detection under the noisy label setting?\nRQ2: What are the impacts of noisy label ratio on REGAD?\nRQ3: What are the performances of REGAD under different anomaly label numbers?\nRQ4: Is REGAD sensitive to the hyper-parameters?\nRQ5: How does our proposed method work in practice", "24": [25], "normal\" data. The details of three datasets are as follows": "nClothing [26", "27": "This dataset is a segment of the co- purchase graphs about computer-related items. Nodes represent individual device products"}, {"27": "This dataset focuses on photography-related products", "follows": "DOM [28", "19": "designs a community-aware method to obtain representations to predict anomaly scores. Meta- GDN [15", "29": "evaluates the entropy of the latent distribution for normal nodes and anomalous nodes to classify. BWGNN [20", "right-shift": "henomenon of graph spectrum by Beta Wavelet Graph Neural Network. CHRN [13", "30": "in- novates a dual-channel GNN framework", "31": "focuses on noise governance by self-reinforcement supervision module and consistency regularization after graph augmentation. PIGNN [32", "15": [33], "34": [35], "0.1": ".", "0.4": "with 100 iterations. We filter out edges simultaneously", "150": ".", "observations": 1, "observed": "for instance", "hyper-parameters": 1}]}