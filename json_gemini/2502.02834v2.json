{"title": "Task-Aware Virtual Training: Enhancing Generalization in Meta-Reinforcement Learning for Out-of-Distribution Tasks", "authors": ["Jeongmo Kim", "Yisak Park", "Minung Kim", "Seungyul Han"], "abstract": "Meta reinforcement learning aims to develop policies that generalize to unseen tasks sampled from a task distribution. While context-based meta-RL methods improve task representation using task latents, they often struggle with out-of-distribution (OOD) tasks. To address this, we propose Task-Aware Virtual Training (TAVT), a novel algorithm that accurately captures task characteristics for both training and OOD scenarios using metric-based representation learning. Our method successfully preserves task characteristics in virtual tasks and employs a state regularization technique to mitigate overestimation errors in state-varying environments. Numerical results demonstrate that TAVT significantly enhances generalization to OOD tasks across various MuJoCo and Meta-World environments.", "sections": [{"title": "1. Introduction", "content": "Research in meta reinforcement learning (meta-RL) aims to train policies on training tasks sampled from a training task distribution, with the goal of enabling the learned policy to adapt and perform well on unseen test tasks. Model-agnostic meta-learning (MAML) (Finn et al., 2017) seeks to find initial parameters that can generalize well to new tasks by using policy gradients to measure how well the current policy parameter can adapt to new tasks. On the other hand, to effectively distinguish tasks while capturing the task characteristics, context-based meta-RL methods that learn task latents through representation learning has been researched recently (Rakelly et al., 2019; Zintgraf et al., 2019; Fu et al., 2021). One of the most well-known context-based methods, PEARL (Rakelly et al., 2019), learns task representations from off-policy samples and uses these samples for reinforcement learning, resulting in sample-efficient learning and faster convergence compared to traditional methods. In contrast, another prominent context-based meta-RL method, VariBad (Zintgraf et al., 2019), employs a Bayesian approach to learn a belief distribution over environments, effectively managing the exploration-exploitation trade-off in previously unseen environments. Recently, to address this issue and better distinguish tasks based on their characteristics, advanced representation learning methods such as contrastive learning have been increasingly adopted in meta-RL (Fu et al., 2021; Choshen & Tamar, 2023). CCM (Fu et al., 2021) is a meta-RL method that integrates contrastive learning with PEARL. In CCM, task latents from the same task are treated as having a positive relationship and are trained to be close to each other, while latents from different tasks are treated as having a negative relationship and are trained to be distant.\nAdvanced representation learning in meta-RL improves the ability to distinguish training tasks through learned task latents. However, most existing methods assume that the test task distribution matches the training distribution, limiting their effectiveness on out-of-distribution (OOD) tasks. LDM (Lee & Chung, 2021) addresses this by training policies with virtual tasks (VTs) created via linear interpolation of task latents learned by VariBad, improving OOD generalization. Despite its benefits, we identify several key issues with the existing methods for VT construction. First, generated VTs often fail to capture task characteristics accurately. Second, LDM focuses solely on reward sample generation, which struggles in environments with task-dependent state transitions. To overcome these limitations, we propose Task-Aware Virtual Training (TAVT), a novel algorithm that generates VTs accurately reflecting task characteristics for both training and OOD scenarios. Using a Bisimulation metric (Ferns et al., 2011; Ferns & Precup, 2014), our method captures task variations, such as changing goal positions, and incorporates an on-off task latent loss to stabilize the task latents. In addition, we introduce task-preserving sample generation to ensure VTs generate realistic sample contexts while maintaining task-specific features. Finally, to address state-varying environments, our task decoder generates full dynamics including both rewards and next states, and we propose a state regularization method to mitigate overestimation errors from generated samples."}, {"title": "2. Preliminary", "content": ""}, {"title": "2.1. Meta Reinforcement Learning", "content": "In meta-RL, each task T is sampled from a task distribution p(T) and defined as a Markov Decision Process (MDP) (S, A, PT, RT, \u03b3, \u03c1\u2080), where S and A are the state and action spaces, PT represents state transition dynamics, RT is the reward function, \u03b3\u2208 [0,1) is the discount factor, and \u03c1\u2080 is the initial state distribution. At each time step t, the agent selects an action a\u209c based on the policy \u03c0, receives a reward r\u209c := RT(s\u209c, a\u209c), and transitions to the next state s\u209c\u208a\u2081 ~ PT(\u00b7|s\u209c, a\u209c). The MDP for each task may vary, but all tasks share the same state and action spaces. During meta-training, the policy \u03c0 is optimized to maximize the cumulative reward sum \u03a3\u209c\u03b3\u1d57r\u209c across tasks sampled from p(T\u1d57\u02b3\u1d43\u2071\u207f)."}, {"title": "2.2. Context-based Meta RL", "content": "Recent meta-RL methods focus on learning latent contexts to differentiate tasks. PEARL (Rakelly et al., 2019), a well-known context-based meta-RL approach, learns task latents z ~ q\u1d6a(\u00b7|c\u1d40) using a task encoder q\u1d6a with parameters \u03c8 and task context c\u1d40 := {(s\u1d62,a\u1d62,r\u1d62,s'\u1d62)}\u1d3a\u1d9c\u1d62=\u2081, where s' is the next state and N\u1d9c is the number of transition samples. PEARL defines task-dependent policies \u03c0(\u00b7|s, z) and Q-functions Q(s, a, z), using soft actor-critic (SAC) (Haarnoja et al., 2018) to train policies. The task encoder q\u1d6a is trained to minimize the encoder loss:\nE\u1d1b~\u209a(\u1d1b\u1d57\u02b3\u1d43\u2071\u207f) [E\ud835\udd6b~q\u1d6a [LQ(T,z)] + D\u2096\u2097(q\u03c8(\u00b7|c\u1d40)||\ud835\udca9(0, I))], where \ud835\udca9 is the multivariate Gaussian distribution, I is the identity matrix, and LQ is the SAC critic loss. Inspired by the variational auto-encoder (VAE) (Kingma & Welling, 2013), PEARL uses LQ instead of VAE's reconstruction loss to better distinguish tasks."}, {"title": "2.3. Virtual Task Construction", "content": "To improve the generalization of policies to various OOD tasks, LDM (Lee & Chung, 2021) defines the task latents z\u1d43 for a virtual task as a linear interpolation of training task latents z\u2071, i = 1,\u2026, M, expressed as:\nz\u1d43 = \u03a3\u1d62=\u2081\u1d39 \u03b1\u2071z\u2071\n(1)\nwhere \u03b1 = (\u03b1\u00b9,\u2026, \u03b1\u1d39) ~ \u03b2Dirichlet(1, 1, ..., 1)\u207b\u00b9 is the interpolation coefficient, Dirichlet(\u00b7) is the Dirichlet distribution, and M is the number of training tasks used for mixing. The parameter \u03b2 \u2265 1 controls the degree of mixing: with \u03b2 = 1, only interpolation within the training task latents occurs, while \u03b2 > 1 aallows extrapolation beyond the original latents. LDM further trains the policy using contexts generated from the task decoder based on the interpolated latents z\u1d43, enabling it to handle OOD tasks more effectively."}, {"title": "3. Related Works", "content": "Advanced Task Representation Learning:Advanced representation learning techniques have been widely explored to improve task latents that effectively distinguish tasks. Recent meta-RL methods use contrastive learning (Oord et al., 2018) to enhance task differentiation through positive and negative pairs, improving task representation (Laskin et al., 2020; Fu et al., 2021; Choshen & Tamar, 2023) and capturing task information in offline setups (Li et al., 2020b; Gao et al., 2023). The Bisimulation metric (Ferns et al., 2011) is employed to capture behavioral similarities (Zhang et al., 2021; Agarwal et al., 2021; Liu et al., 2023) and group similar tasks (Hansen-Estruch et al., 2022; Sodhani et al., 2022). Additionally, skill representation learning (Eysenbach et al., 2018) addresses non-parametric meta-RL challenges (Frans et al., 2017; Harrison et al., 2020; Nam et al., 2022; Fu et al., 2022; He et al., 2024), while task representation learning is increasingly applied in multi-task setups (Ishfaq et al., 2024; Cheng et al., 2022; Sodhani et al., 2021).\nGeneralization for OOD Tasks: Meta-RL techniques for improving policy generalization in OOD test environments have been actively studied (Lan et al., 2019; Fakoor et al., 2019; Mu et al., 2022). Model-based approaches (Lin et al., 2020; Lee & Chung, 2021), advanced representation learning with Gaussian Mixture Models (Wang et al., 2023b; Lee et al., 2023), and Transformers (Vaswani et al., 2017; Melo, 2022; Xu et al., 2024) have been explored. Additionally, some studies tackle distributional shift challenges through robust learning (Mendonca et al., 2020; Mehta et al., 2020; Ajay et al., 2022; Chae et al., 2022; Greenberg et al., 2023).\nModel-based Sample Relabeling: Model-based sample generation and relabeling techniques have gained attention in meta-RL (Rimon et al., 2024; Wen et al., 2024), enabling the reuse of samples from other tasks using dynamics models (Li et al., 2020a; Mendonca et al., 2020; Wan et al., 2021; Zou et al., 2024). These methods address sparse rewards (Packer et al., 2021; Jiang et al., 2023), mitigate distributional shifts in offline setups (Dorfman et al., 2021; Yuan & Lu, 2022; Zhou et al., 2024; Guan et al., 2024), and incorporate human preferences (Ren et al., 2022; Hejna III & Sadigh, 2023) or guided trajectory relabeling (Wang et al., 2023a), expanding their applications."}, {"title": "4. Methodology", "content": ""}, {"title": "4.1. Metric-based Task Representation", "content": "In this section, we propose a novel representation learning method to ensure task latents accurately capture differences in task contexts, enabling virtual tasks to effectively reflect task characteristics. To achieve this, we leverage the Bisimulation metric (Ferns et al., 2011), which measures the similarity of two states in an MDP based on the reward function RT and state transition PT. In meta-learning, the Bisimulation metric can quantify task similarity by comparing contexts (Zhang et al., 2021). Unlike Zhang et al. (2021), which considers tasks with different state spaces, we adapt the metric for tasks sharing the same state and action space, modifying it from Eq. (4) in Zhang et al. (2021).\nDefinition 4.1 (Bisimulation metric for task representation). For two different tasks Ti and Tj,\nd(Ti, Tj) = E(s,a)~\ud835\udc9f [|RT\u1d62(s, a) \u2212 RT\u2c7c(s, a)| + \u03b7\ud835\udcb2\u2082(PT\u1d62(\u00b7|s, a), PT\u2c7c(\u00b7|s, a))],                                                               (2)\nwhere \ud835\udc9f is the replay buffer that stores the sample contexts, RT, PT are the reward function and the transition dynamics for task T, \ud835\udcb2\u2082 is 2-Wasserstein distance between the two distributions, and \u03b7 \u2208 (0, 1] is the distance coefficient.\nProposition 4.2. d(\u00b7,\u00b7) defined in Eq. (2) is a metric.\nProof) The detailed proof is provided in Appendix A.\nThe Bisimulation metric d equals 0 when the contexts of two tasks perfectly match and increases as the context difference grows. Task latents learned using this metric more effectively capture task distances than existing representation methods, as shown in Fig. 1. We train the task encoder q\u03c8(z|c\u1d40) to ensure the task latent z ~ q\u03c8(\u00b7|c\u1d40) preserves the Bisimulation metric d in the latent space. Since the actual reward function R and transition dynamics P are generally unknown, we train the task decoder p\u03d5(s,a,z) = (R\u03d5(s,a,z), P\u03d5(\u00b7|s, a, z)) with parameters \u03d5 to approximate task dynamics using reconstruction loss.\nWe adopt the learning structure of PEARL, which uses two distinct policies: \u03c0\u1d49\u02e3\u1d56 for on-policy exploration to obtain task latents from contexts and \u03c0\u1d3f\u1d38 for off-policy RL to maximize returns. Contexts generated by \u03c0\u1d49\u02e3\u1d56 and \u03c0\u1d3f\u1d38 are stored in the on-policy buffer \ud835\udc9f\u1d52\u207f and the off-policy buffer \ud835\udc9f\u1d52\u1da0\u1da0, respectively. PEARL considers only on-policy task latents z\u1d52\u207f derived from c\u1d40 ~ \ud835\udc9f\u1d52\u207f, but z\u1d52\u207f can be unstable due to limited contexts in \ud835\udc9f\u1d52\u207f. To address this, we propose an on-off latent learning structure where off-policy task latents z\u1d52\u1da0\u1da0, derived from c\u1d40 ~ \ud835\udc9f\u1d52\u1da0\u1da0, maintain the Bisimulation distance for training tasks, and z\u1d52\u207f just aligns with z\u1d52\u1da0\u1da0. In summary, our encoder-decoder loss is defined as\n\u2112b\u1d62\u209b\u1d62\u2098(\u03c8, \u03d5) = E\u1d1b\u1d62,\u1d1b\u2c7c~\u209a(\u1d40\u1d57\u02b3\u1d43\u2071\u207f) [(||z\u1d52\u1da0\u1da0\u1d62 - z\u1d52\u1da0\u1da0\u2c7c|| - d(T\u1d62, T\u2c7c; p\u03d5))\u00b2] \n+ E(\u209b,\u2090,\u1d63,\u209b')~\ud835\udc9f\u1d40\u1d62,(\u1d63\u0302,\u015d')~\u209a\u03d5(\u209b,\u2090,z\u1d52\u1da0\u1da0) [(r \u2212 r\u0302)\u00b2 + (s\u2032 \u2212 \u015d\u2032)\u00b2]\n+(\u2112\u1d52\u207f\u207b\u1d52\u1da0\u1da0)(z\u1d52\u207f - z\u1d52\u1da0\u1da0)\u00b2, z\u1d52\u207f~ q\u03c8(c\u1d40\u1d62), c\u1d40\u1d62 ~ \ud835\udc9f\u1d40\u1d62, \u2200i,\n(3)"}, {"title": "4.2. Task Preserving Sample Generation", "content": "Using the task decoder p\u03d5(\u00b7, z\u1d52) and VT task latents z\u1d43, we generate virtual contexts \u0109\u1d43 := (s\u1d62, a\u1d62, r\u0302\u1d62, \u015d\u1d62)\u1d3a\u1d9c\u1d62=\u2081, where (s\u1d62, a\u1d62) are sampled from real contexts c\u1d40\u1d62, and r\u0302\u1d62, \u015d\u1d62 ~ p\u03d5(s\u1d62, a\u1d62, z\u1d43). Existing VT construction methods (Lee & Chung, 2021; Lee et al., 2023) use dropout-based regularization to ensure virtual contexts generalize to unseen tasks. However, we observed that task latents z\u1d43 ~ q\u03c8(\u00b7|\u0109\u1d43) often deviate significantly from z\u1d43, indicating that virtual contexts fail to effectively preserve task information. To address this, we propose a task-preserving loss to minimize the difference between z\u1d43 and z\u0302\u1d43, ensuring virtual contexts better retain task latent information. Despite the benefits of the task-preserving loss, virtual contexts \u0109\u1d43 may still differ from real contexts due to limitations in the task decoder p\u03d5(\u00b7, z\u1d43), which cannot fully capture actual task contexts. These differences can introduce instability and degrade performance for RL training. To address this issue, we use a Wasserstein generative adversarial network (WGAN) (Arjovsky et al., 2017), designed to reduce the distribution gap between real and generated data. In our setup, the task decoder p\u03d5(\u00b7, z\u1d43) acts as the generator, learning to produce samples that closely resemble real ones, while real contexts serve as the target for the discriminator. The discriminator f\u03b6 increases its value for real samples and decreases it for generated samples, while the generator aligns virtual contexts with real ones by increasing f\u03b6 for generated samples. This process ensures that virtual contexts not only preserve task information but also closely resemble real contexts, significantly reducing the gap between VT-generated samples and real OOD task samples. To summarize, the WGAN discriminator and generator losses, combined with the task-preserving loss, are defined as:\n\u2112d\u1d62\u209bc(\u03b6) = E\u1d1b~\u209a(\u1d40\u1d57\u02b3\u1d43\u2071\u207f),c\u1d40\u1d62~\ud835\udc9f\u1d40\u1d62 [-f\u03b6(c\u1d40\u1d62, z\u1d52\u1da0\u1da0) + E\u0109\u1d43~\u209a\u03d5[f\u03b6(\u0109\u1d43, z\u1d52\u1da0\u1da0)]] + \u03bb\u1d4d\u1d3e \u22c5 Gradient Penalty,\n\u2112g\u2091\u2099(\u03c8, \u03d5) = E\u0109\u1d43~\u209a\u03d5 [-\u03bb\u1d42\u1d33\u1d2c\u1d3a \u22c5 f\u03b6(\u0109\u1d43, z\u1d52\u1da0\u1da0) + \u03bb\u1d57\u1d56 \u22c5 E\u2097\u1d43q\u1d6a(\u2022|\u0109\u1d43) [(z\u1d43 \u2212 z\u0302\u1d43)\u00b2]],\n(4)\n(5)\nwhere the Gradient Penalty (GP), introduced in (Arjovsky et al., 2017), stabilizes training, with \u03bb\u1d4d\u1d3e as its coefficient. The detailed implementation of GP is provided in Appendix D.2. In this framework, the task decoder p\u03d5 is trained with input z\u1d52\u1da0\u1da0, as described in Eq. (3). Both the task-preserving loss and VT construction are always based on z\u1d52\u1da0\u1da0, derived from z\u1d52\u1da0\u1da0. Importantly, the off-policy task latent z\u1d52\u1da0\u1da0 conditions sample context generation, with its gradient disconnected to ensure stable training. By leveraging WGAN, we significantly reduce differences between generated and real samples, improving RL performance on OOD tasks."}, {"title": "4.3. Task-Aware Virtual Training", "content": "By combining metric-based representation learning with task-preserving sample generation, we propose the Task-Aware Virtual Training (TAVT) algorithm, which enhances the generalization of meta-RL to OOD tasks by leveraging VTs that accurately reflect task characteristics. The total encoder-decoder loss for TAVT is given by:\n\u2112t\u2092\u209c\u2090\u2097(\u03c8, \u03d5) = \u2112b\u1d62\u209b\u1d62\u2098(\u03c8, \u03d5) + \u2112g\u2091\u2099(\u03c8, \u03c6),\n(6)\nwith detailed loss scales provided in Appendix F.\nNow, we train the RL policy \u03c0\u1d3f\u1d38 using SAC, leveraging both real contexts c\u1d40\u1d62 ~ \ud835\udc9f\u1d40\u1d62 for each training task T\u1d62 and virtual contexts \u0109\u1d43 generated by the proposed TAVT method. For training tasks, the RL policy is defined as \u03c0\u1d3f\u1d38 = \u03c0(\u00b7|s, z\u1d52\u207f), where z\u1d52\u207f is the on-policy task latent. For VTs, the RL policy is defined as \u03c0\u1d3f\u1d38 = \u03c0(\u00b7|s, z\u1d52\u207f), where z\u1d52\u207f is the VT's task latent derived from z\u1d52\u207f. We train the RL policy \u03c0\u1d3f\u1d38 using SAC, leveraging both real contexts c\u1d40\u1d62 ~ \ud835\udc9f\u1d40\u1d62 for each training task T and virtual contexts \u0109\u1d43 generated by the proposed TAVT method. For training tasks, the RL policy is defined as \u03c0\u1d3f\u1d38 = \u03c0(\u00b7|s, z\u1d52\u207f), where z\u1d52\u207f is the on-policy task latent. For VTs, the RL policy is defined as \u03c0\u1d3f\u1d38 = \u03c0(\u00b7|s, z\u1d52\u207f), where z\u1d52\u207f is the VT's task latent derived from z\u1d52\u207f. The proposed TAVT method aligns task latents using metric-based representation and learns VTs that preserve task characteristics while generating samples similar to real ones through task-preserving loss with WGAN. This enables the policy to train on a diverse range of OOD tasks, improving generalization performance.\nSince the agent cannot access off-policy latents for test tasks, it relies on on-policy latents z\u1d52\u207f, obtained from \ud835\udca9\u2091\u2093\u209a episodes generated by the exploration policy \u03c0\u2091\u2093\u209a, as introduced in PEARL (Rakelly et al., 2019). While PEARL uses \u03c0(\u00b7|s, z) with z ~ \ud835\udca9(0, I) as the exploration policy, we propose a novel exploration policy \u03c0\u2091\u2093\u209a = \u03c0(\u00b7|s, z\u1d52\u207f) that leverages VT task latents z\u1d52\u207f. This approach enables exploration across both training tasks and VTs by varying the interpolation coefficient \u03b1, allowing the agent to explore a broader range of tasks."}, {"title": "4.4. State Regularization Method", "content": "The virtual contexts \u0109\u1d43 in TAVT include both rewards and next states, enabling it to handle state-varying environments. However, inaccuracies in the task decoder can introduce errors in the Q-function, leading to overestimation bias, a common issue in offline RL (Fujimoto et al., 2019; Yeom et al., 2024). While reward errors have minimal impact, next-state errors significantly contribute to overestimation. To mitigate this, we propose a state regularization method, replacing the next states \u015d\u2032\u1d43 in virtual contexts with \u015d\u02b3\u1d49\u1d4d, a mix of next states s' from training tasks and \u015d\u1d43 from virtual contexts:\ns\u0302\u02b3\u1d49\u1d4d := \u03f5\u02b3\u1d49\u1d4ds'\u1d43 + (1 \u2212 \u03f5\u02b3\u1d49\u1d4d)s',\nwhere (s, a, r, s\u2032) \u2208 c\u1d40\u1d62, r\u0302\u1d43, \u015d\u2032\u1d43 ~ p\u03d5(s, a, z\u1d52), and \u03f5\u02b3\u1d49\u1d4d \u2208 [0, 1] is the regularization coefficient."}, {"title": "5. Experiments", "content": "In this section, we compare the proposed TAVT algorithm with various on-policy and off-policy meta-RL methods across MuJoCo (Todorov et al., 2012) and MetaWorld ML1 (Yu et al., 2020) environments. For off-policy methods, we include PEARL, CCM which uses contrastive learning to enhance representation, MIER which incorporates a gradient-based dynamics model, and Amago which employs transformers for latent learning. For on-policy methods, we evaluate MAML which optimizes initial gradients for generalization, RL2 which encodes task information in RNN hidden states, VariBad which applies Bayesian methods for representation learning, and LDM which uses virtual tasks with a reconstruction loss and a DropOut layer. Additionally, we provide an analysis of the task representation in TAVT as well as an ablation study on its performance."}, {"title": "5.1. Environmental Setup", "content": "To evaluate generalization performance on OOD tasks, we used 6 MuJoCo environments (Todorov et al., 2012): Cheetah-Vel-OOD, Ant-Dir-2, Ant-Dir-4, and Ant-Goal-OOD, where only the reward function varies across tasks, and Walker-Mass-OOD and Hopper-Mass-OOD, where both the reward function and state transition dynamics vary. In addition, we considered 6 MetaWorld ML1 environments (Yu et al., 2020), including the original Reach and Push environments, as well as 4 OOD variations (Reach-OOD-Inter, Reach-OOD-Extra, Push-OOD-Inter, and Push-OOD-Extra). In these ML1 and ML1-OOD tasks, the final goal point varies across tasks while maintaining shared state dynamics. The task space is denoted by M, and detailed descriptions of these environments are provided below.\n\u2022 Cheetah-Vel-OOD: The Cheetah agent is required to run at target velocities \ud835\udc63\u209c\u2090\u1d63 in M.\n\u2022 Ant-Dir: The Ant agent is required to move in directions \u03b8d\u1d62\u1d63 in M. For Ant-Dir-2, the training tasks involve 2 directions (\u03b8d\u1d62\u1d63 = 0, \u03c0 \u20442 ), and for Ant-Dir-4, the training tasks involve 4 directions (\u03b8d\u1d62\u1d63 = 0, \u03c0 \u20444, \u03c0 \u20442 , 3\u03c0 \u20444).\n\u2022 Ant-Goal-OOD: The Ant agent should reach the 2D goal positions (\ud835\udc5f\u1d4d\u1d52\u1d43\u02e1 cos \u03b8\u1d4d\u1d52\u1d43\u02e1, \ud835\udc5f\u1d4d\u1d52\u1d43\u02e1 sin \u03b8\u1d4d\u1d52\u1d43\u02e1), where the radius \ud835\udc5f\u1d4d\u1d52\u1d43\u02e1 and angle \u03b8\u1d4d\u1d52\u1d43\u02e1 are selected from M.\n\u2022 Hopper/Walker-Mass-OOD: The Hopper/Walker agent are required to run forward with scale \ud835\udc5a\u209bc\u2090\u2097\u2091 in M multiplied to their body mass.\n\u2022 ML1/ML1-OOD: The agent is tasked with reaching or pushing an object to a target goal position \ud835\udc54\u209c\u2090\u1d63, sampled from the 3D goal space M, which varies depending on the environment. To create an OOD setup, inner areas \ud835\udc40\u1d62\u2099\u2099\u2091\u1d63 are defined within the goal space M."}, {"title": "5.2. Performance Comparison", "content": "We compare the performance of the proposed TAVT with other meta-RL algorithms. For MuJoCo environments, Fig. 6 shows the convergence performance of the test average return, while Table 1 presents the final average success rate for ML1/ML1-OOD environments. On-policy algorithms are trained for 200M timesteps in MuJoCo and 100M timesteps in MetaWorld environments. For off-policy algorithms, training timesteps vary in MuJoCo environments and are fixed at 10M timesteps for MetaWorld environments. Also, Fig. 6 includes the '-oracle' performance for VariBad (on-policy) and TAVT (off-policy) in MuJoCo environments. This represents the upper performance bound achieved by training on all tasks from both \ud835\udc40\u1d57\u02b3\u1d43\u2071\u207f and \ud835\udc40\u1d57\u1d49\u02e2\u1d57.\nFor MuJoCo environments, Fig. 6 shows that both VariBad and TAVT perform well in the '-oracle' setup across most environments. However, in OOD task scenarios, TAVT significantly outperforms other on-policy and off-policy methods. Notably, TAVT achieves performance close to the '-oracle' in Cheetah-Vel-OOD, Ant-Goal-OOD, and Walker-Mass-OOD, demonstrating strong generalization to unseen OOD tasks. While other algorithms struggle in challenging environments like Ant-Dir-2, which includes extrapolation tasks, TAVT remains robust. In environments such as Walker-Mass-OOD and Hopper-Mass-OOD, where state transition dynamics vary, LDM fails to adapt due to limitations in its VT construction. In contrast, TAVT excels, showcasing its ability to handle varying state transition dynamics.\nFor MetaWorld environments, Table 3 shows that TAVT consistently delivers superior performance across all setups. In the original ML1 environments, TAVT outperforms other methods, highlighting the advantages of its metric-based task representation. In ML1-OOD environments, PEARL-based methods such as PEARL, CCM, and MIER perform poorly in Push and Push-OOD tasks, while other off-policy algorithms also fail to achieve success rates near 1. Despite being a PEARL-based method, TAVT achieves significantly higher success rates, often approaching 1. Compared to on-policy algorithms like LDM, RL2, and VariBad, TAVT also demonstrates much better performance. These results highlight the effectiveness of our method in enhancing generalization for OOD tasks."}, {"title": "5.3. Task Representation of TAVT", "content": "The comparative experiments confirm the superiority of TAVT. To explore the factors behind this improvement, Fig. 7 illustrates the task representation of TAVT across various environments. As shown in the cased of Ant-Goal environment in Fig. 1, TAVT accurately aligns task latents for both training and OOD test tasks, reflecting task characteristics. For instance, task latents align linearly by target velocity in Cheetah-Vel-OOD and by target mass in Walker-Mass-OOD. Similarly, task latents align according to 2D target directions in Ant-Dir-4 and 3D target goals in Reach-OOD-Inter. These results demonstrate that the proposed metric-based task representation effectively distinguishes and generalizes task latents to OOD tasks."}, {"title": "5.4. Ablation Studies", "content": "Component Evaluation: To further analyze the proposed TAVT method, we perform a component evaluation on the Cheetah-Vel-OOD and Walker-Mass-OOD environments, comparing the final average return in Fig. 8. We consider the variations of TAVT including 'TAVT w/o VT', which removes virtual tasks entirely, \u2018TAVT w/o \u2112g\u2091\u2099', which excludes the \u2112g\u2091\u2099 component, \u2018TAVT w/o on-off loss', which omits the on-off loss, and 'Recon only', which uses only reconstruction loss with DropOut as in LDM. The results demonstrate that removing any component significantly degrades performance, underscoring the importance of task representation learning and sample generation for OOD task generalization. In addition, TAVT outperforms the 'Recon only' approach, showcasing its ability to construct more effective virtual tasks than existing methods.\nContext Differences: To evaluate the impact of context differences between VT-generated and actual contexts on performance, Fig. 9 presents these differences for the TAVT variations analyzed in the component evaluation. Since 'TAVT w/o VT' does not utilize virtual contexts, it is excluded from Fig. 9. The results show that removing any component increases context error, leading to degraded performance, as reflected in Fig. 8, underscoring the importance of each component in TAVT. Notably, omitting the task-preserving sample generation loss \u2112g\u2091\u2099 significantly increases context differences. These findings demonstrate that the proposed WGAN-based task-preserving sample generation method effectively reduces context errors and enhances generalization performance, as detailed in Section 4.2.\nState Regularization: To demonstrate the effectiveness of the proposed state regularization method, Fig. 5 shows the performance across different \u03f5\u02b3\u1d49\u1d4d values. As illustrated, a small \u03f5\u02b3\u1d49\u1d4d effectively reduces estimation bias. Fig. 10 further evaluates its impact on performance in Walker-Mass-OOD and Hopper-Mass-OOD environments, where \u03f5\u02b3\u1d49\u1d4d = 0.1 yields the best results. This setting minimizes both Q-function loss and overestimation bias, enabling the Q-function to accurately reflect expected returns and improving performance in OOD tasks. These findings highlight the importance of the proposed state regularization method."}, {"title": "6. Conclusion", "content": "Existing meta-RL methods either overlook OOD tasks or struggle with them, particularly when state transitions vary. Our proposed method, TAVT, addresses these challenges through metric-based latent learning, task-preserving sample generation, and state regularization. These components ensure precise task latents and enhanced generalization, overcoming previous limitations. Experimental results demonstrate that TAVT achieves superior alignment of OOD task latents with training task characteristics."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here"}, {"title": "A. Proof", "content": "Definition 3.1 (Bisimulation metric for task representation). For two different tasks Ti and Tj,\nd(Ti, Tj) = E(s,a)~\ud835\udc9f [|RT\u1d62(s, a) \u2212 RT\u2c7c(s, a)| + \u03b7\ud835\udcb2\u2082(PT\u1d62(\u00b7|s, a), PT\u2c7c(\u00b7|s, a))],                                                                  (1)\nwhere \ud835\udc9f is the replay buffer that stores the sample contexts, RT, PT are the reward function and the transition dynamics for task T, \ud835\udcb2\u2082 is 2-Wasserstein distance between the two distributions, and \u03b7 \u2208 (0, 1] is the distance coefficient.\nProposition 3.1. d(\u00b7,\u00b7) defined in Eq. (2) is a metric.\nProof of Proposition 3.1\nAs mentioned in Section 2, each task T is represented by an MDP (S, A, PT, RT, \u03b3, \u03c1\u2080), where S is the state space, A is the action space, PT represents the state transition dynamics, RT is the reward function, \u03b3 \u2208 [0, 1) is the discount factor, and \u03c1\u2080 is the initial state distribution. Since all tasks shares the same state space and action space in this paper, so Ti = Tj if and only if PT\u1d62 (s\u2032|s, a) = PT\u2c7c(s\u2032|s, a) and RT\u1d62(s, a) = RT\u2c7c(s, a), \u2200s, s\u2032 \u2208 S, a \u2208 A for any tasks Ti, Tj \u2208 M. Thus, from the definition of d given by Eq. (2), d is a metric since d satisfies the following axioms:\n1.  (Non-negativity) d(Ti, T\u2c7c) \u2265 0 since |\u00b7| and \ud835\udcb2\u2082(\u00b7,\u00b7) are non-negative,\n2.  d(Ti, Tj) = 0 \u21d4 RT\u1d62 = RT\u2c7c and PT\u1d62 = PT\u2c7c \u21d4 Ti = Tj,\n3.  (Symmetry) d(Ti, Tj) = d(Tj, Ti) from the definition,\n4.  (Triangle Inequality) d(Ti, Tk) \u2264 d(Ti, Tj) + d(Tj, Tk):\nd(Ti, Tk) = E(s,a)~\ud835\udc9f [|RT\u1d62(s, a) \u2212 RT\u2096(s, a)| + \u03b7\ud835\udcb2\u2082(PT\u1d62(\u00b7|s, a), PT\u2096(\u00b7|s, a))], \n\u2264 E(s,a)~\ud835\udc9f [|RT\u1d62(s,a) \u2212 RT\u2c7c(s,a)| + |RT\u2c7c(s, a) \u2212 RT\u2096(s, a)|+\n\u03b7[\ud835\udcb2\u2082(PT\u1d62(\u00b7|s, a), PT\u2c7c(\u00b7|s, a)) + \ud835\udcb2\u2082(PT\u2c7c(\u00b7|s, a), PT\u2096(\u00b7|s, a))], \n= E(s,a)~\ud835\udc9f [|RT\u1d62(s,a) \u2212 RT\u2c7c(s, a)| + \u03b7[\ud835\udcb2\u2082(PT\u1d62(\u00b7|s, a), PT\u2c7c(\u00b7|s, a))] +\nE(s,a)~\ud835\udc9f [|RT\u2c7c(s, a) \u2212 RT\u2096(s, a)| + \u03b7[\ud835\udcb2\u2082(PT\u2c7c(\u00b7|s, a), PT\u2096(\u00b7|s, a))],\n= d(Ti, Tj) + d(Tj, Tk),\nwhere \u2217 can be derived, as both the absolute value |\u00b7| and the Wasserstein distance \ud835\udcb2\u2082 satisfy the triangle inequality."}, {"title": "B. Task Representations Across All Environments", "content": "To present the task representation results for considered OOD environments,  shows the task representations for 6 MuJoCo O"}]}