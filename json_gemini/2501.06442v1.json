{"title": "ARES: Auxiliary Range Expansion for Outlier Synthesis", "authors": ["Eui-Soo Jung", "Hae-Hun Seo", "Hyun-Woo Jung", "Je-Geon Oh", "Yoon-Yeong Kim"], "abstract": "Recent successes of artificial intelligence and deep learning often depend on the well-collected training dataset which is assumed to have an identical distribution with the test dataset. However, this assumption, which is called closed-set learning, is hard to meet in realistic scenarios for deploying deep learning models. As one of the solutions to mitigate this assumption, research on out-of-distribution (OOD) detection has been actively explored in various domains. In OOD detection, we assume that we are given the data of a new class that was not seen in the training phase, i.e., outlier, at the evaluation phase. The ultimate goal of OOD detection is to detect and classify such unseen outlier data as a novel \"unknown\" class. Among various research branches for OOD detection, generating a virtual outlier during the training phase has been proposed. However, conventional generation-based methodologies utilize in-distribution training dataset to imitate outlier instances, which limits the quality of the synthesized virtual outlier instance itself. In this paper, we propose a novel methodology for OOD detection named Auxiliary Range Expansion for Outlier Synthesis, or ARES. ARES models the region for generating out-of-distribution instances by escaping from the given in-distribution region; instead of remaining near the boundary of in-distribution region. Various stages consists ARES to ultimately generate valuable OOD-like virtual instances. The energy score-based discriminator is then trained to effectively separate in-distribution data and outlier data. Quantitative experiments on broad settings show the improvement of performance by our method, and qualitative results provide logical explanations of the mechanism behind it.", "sections": [{"title": "1 INTRODUCTION", "content": "The big data era has opened the blooming of artificial intelligence and deep learning. A large set of collected data and development of learning algorithms are being applied in various domains, including computer vision [11, 27, 29, 30], natural language processing [3, 6], signal processing [7, 37], etc. However, most of the successes of deep learning have been accomplished in a closed-set learning scenario where we assume that both the training dataset and the test dataset follow the same distribution. This assumption leads the deep learning models to solely focus on effectively extracting the hidden representation from the given training dataset.\nIn the real-world setting, however, this assumption is hard to meet in various deployment phases. That is, the distribution of the training dataset and the test dataset may differ, which is called as open-set learning scenario. Various research branches have been developed to treat this realistic setting by slightly varying the application scenarios; including novelty detection [33], anomaly detection [32], open-set recognition [2], etc.\nOut-of-distribution (OOD) detection [13] is one such variant for open-set learning. In the OOD detection problem, we are given a training dataset as the resource for learning the model; and instances of unknown classes, which were unseen in the training dataset, are included in the test dataset. In this problem, we denote the training instances as the in-distribution dataset, while we denote the unseen instances as the out-of-distribution dataset. Having said that, the ultimate goal of the OOD detection is to recognize the instances of the unseen class and label the instances as a novel \"unknown\" class (i.e., out-of-distribution instance), instead of mislabeling the instance as one of the known classes (i.e., in-distribution instance) that belong to the training dataset. Simultaneously, the model should also predict the ground-truth label of the in-distribution dataset accurately.\nFocusing on the developments of the OOD detection methodologies, various branches of methodology have been proposed so far. One of the most simple but effective branches is the classifier-based method. This branch of methods uses the confidence value of the softmax output produced by the model, and labels each instance as out-of-distribution if the confidence value for the instance does not exceed a threshold. Moreover, energy score-based methods have also been proposed. In these branches of methods, an additional discriminator is introduced to discriminate in-distribution and out-of-distribution instances using energy-based score [22]. The introduction of the discriminator brings some advantages in that it does not harm the discriminative representation learned by the classifier model while effectively screening out the out-of-distribution instances. Recently, data generation-based methods are also being actively explored, adopting Generative Adversarial Networks (GAN) [10] or data augmentations [17]. These branches generate virtual instances that act as probable out-of-distribution instances, or outlier instances. The generated virtual outlier instances are further used for calculating the energy-based score that is distinguished from the given training dataset. Overall, these methods are jointly used in diverse directions, leading to an improved performance of OOD detection.\nHowever, there exist some limitations of data generation-based methods. First, these methods sometimes require additional effort to generate the virtual outlier. For example, adopting GAN takes computational cost and elaborate effort for training the generative model until its convergence. Moreover, the quality of the generated virtual outlier instances depends on the training quality of GAN. Second, the generation process starts with the given training dataset, i.e., in-distribution dataset. Hence, the generated virtual outlier instances are located near the boundary of in-distribution region, and these instances do not sufficiently exhibit the characteristics of true out-of-distribution dataset.\nThis paper aims to overcome the limitations of the conventional data generation-based methods above. First, we need to adopt a simple but effective generation process for virtual outliers, rather than training an additional network. Second, we need to get out of the in-distribution region and generate virtual outlier instances far away from the in-distribution region. These two views lead us to the proposal of our novel algorithm named Auxiliary Range Expansion for Outlier Synthesis (ARES). ARES uses a sampling-based method to generate virtual outliers, which does not require additional training cost or effort. The sampling of virtual outliers is conducted in the estimated out-of-distribution space that is expanded far away from the given in-distribution region. In specific, our virtual out-of-distribution space is estimated from the Mixup data [38, 43].\nThe remainder of this paper is as follows. Section 2 covers the background knowledge for the development of our method. Section 3 introduces the motivation of our method, ARES, and treats the detailed algorithms. Section 4 shows the improved performance of OOD detection by using ARES and Section 5 follows to conclude this paper."}, {"title": "2 BACKGROUND", "content": "2.1 Mixup-based Augmentation\nData augmentation is a widely used technique for enhancing the performance of deep learning models [17]. Traditional data augmentation includes geometric transformations such as rotations, horizontal/vertical flipping, cropping, etc. [15, 34].\nRecent advancements in data augmentation involve Mixup-based techniques that address the issues of ERM (Empirical Risk Minimization) [43]. Unlike ERM, which trains the model with the original training dataset, Mixup trains the model by mixing two data instances and their labels with a mixing coefficient, \\(\\lambda\\). Here, \\(\\lambda\\) is sampled from Beta(\\({\\alpha}\\), \\({\\alpha}\\)) distribution, where \\({\\alpha}\\) \\(\\in\\) (0,0); resulting in a mixed instance, \\(\\tilde{x} = \\lambda x_{i}+(1-\\lambda)x_{j}\\), and its mixed label, \\(\\tilde{y} = \\lambda y_{i} + (1 - \\lambda)y_{j}\\). Moreover, Manifold Mixup has been proposed to mix feature maps that are randomly selected, instead of raw input images, to learn a smoother decision boundary [38].\nRecently, the Mixup-based methods have been explored for enhancing the performance of deep learning models in various areas. PixMix is one such development of Mixup, which deals with Machine Learning safety including calibration, anomaly detection, corruption, consistency, and adversaries [14]. PixMix mixes original data with auxiliary data such as fractal images to utilize the natural structural complexity of pictures.\n2.2 Energy-based Score\nAn energy-based model maps d-dimensional inputs to a non-probabilistic scalar energy, which is called energy-based score. Energy-based score is widely adopted in OOD detection models as an indicator to discriminate between in-distribution and out-of-distribution instances. Energy-based score is advantageous in that it resolves the issue of arbitrarily high values of softmax confidence of OOD instances, leading to an effective discrimination of ID and OOD instances [22].\n2.3 Data Generation-based OOD Detection\nIn data generation-based out-of-distribution detection methods, virtual out-of-distribution instances are generated from the in-distribution dataset during the training process. By introducing an additional loss to discriminate the virtual instances from the original instances, the model can learn the representations of the out-of-distribution dataset during the training phase.\nData augmentation has been widely used for the generation of virtual OOD instances. Contrastive Shifted Instances (CSI) adopt hard augmentation (e.g., rotation) to produce a negative pair, and the augmented instances act as virtual OOD instances [35]. However, using traditional data augmentations also limits the quality of virtual OOD instances in that they are generated near the boundary of the in-distribution region and may not contain a sufficient representation of true OOD instances.\nMixture Outlier Exposure (MixOE) utilizes an auxiliary dataset for generating virtual instances [44]. In MixOE, virtual OOD instances are generated in the form of both fine-grained and coarse-grained by mixing ID instances with real OOD instances. However, the auxiliary dataset used for MixOE requires a heavy real OOD dataset, and it may appear as if the model is learning from actual OOD samples.\nGAN-based methods adopt Generative Adversarial Network (GAN) [10] to generate realistic virtual instances. In this line of research, the Kullback-Leibler divergence term serves to discriminate virtual instances as OOD instances by approximating a uniform distribution for the virtual instances [18]. However, synthesizing data in high-dimensional feature space is challenging to optimize; and due to the nature of GAN, it requires high computational costs as well as an increased training time.\nRecently, sampling-based methods have been proposed as a simple but effective way for generating OOD instances to overcome the burden of adopting GAN. Virtual Outlier Synthesis (VOS) estimates the distribution of the training dataset as a multivariate Gaussian distribution of N(\\({\\mu}\\), \\({\\sigma}\\)) [9]. Then, VOS"}, {"title": "3 METHOD", "content": "3.1 Problem Setting\nIn this paper, we solve out-of-distribution (OOD) detection task, where we are given a training dataset consisting of N number of instances, which is represented as \\(D = \\{(x_i, y_i)\\}_{i=1}^{N}\\). Compared to the training dataset, the test dataset may contain instances of unseen class labels as well as instances which share the same class labels with D. For clarity, we simply denote the instances with known classes as ID instances or inliers; while we denote the instances of unknown classes as OOD instances or outliers.\nThe goal of OOD detection task is training a model, which is a neural network \\(f_{\\theta}\\) parameterized by \\({\\theta}\\); where \\(f_{\\theta}\\) can be further represented as \\(f_{\\theta}(x) = f_{cls}(f_{ext}(x))\\). Here, \\(f_{ext}\\) is a feature extractor which extracts a d-dimensional feature embedding for x; and \\(f_{cls}\\) is a classifier that maps the feature embedding into a label space. The output of \\(f_{ext}\\) is also called as feature at the penultimate layer of \\(f_{\\theta}\\), and \\(f_{cls}\\) consists of fully-connected layers. Overall, \\(f_{\\theta}\\) should predict the correct classes for the inliers as well as predict outliers as \"unknown\" class rather than one of the ID classes.\n3.2 Motivation\nFor an effective OOD detection, we focus on the proposal of a sampling-based method. The basic assumption for our method is that we should sample virtual outliers at the farthest region as possible from the given ID region. Having said that, every stage of our proposed method commonly aims to depart the original ID region, contributing to enhancing the values of generated virtual outliers.\n3.3 ARES: Auxiliary Range Expansion for\nOutlier Synthesis\nIn this paper, we propose a novel approach named Auxiliary Range Expansion for Outlier Synthesis, or ARES, whose goal is to generate virtual OOD instances at the as farthest as possible region from ID instances. For the sake of this goal, ARES consists of four stages to generate OOD instances, which we name as 1) Escape, 2) Expansion, 3) Estimation, and 4) Divergence stages.\n3.3.1 Escape Stage. During the training phase, we are only allowed an access to the ID dataset which is given as a training set, by the natural assumption of OOD detection. The given ID instances become the only ingredient to generate OOD instances. Hence, ARES starts with the Escape stage, which generates a surrogate ID set, denoted as \\(D^*\\), apart from the original ID dataset, denoteod as D. \\(D^*\\) actually plays the role as the starting point for OOD generation.\nFor an effective generation of \\(D^*\\), we adopt PixMix [14], which mixes the original data with fractal images, denoted as \\(F = \\{(x_{frac})_{i=1}^{N}\\} \\) as the below.\n\\(x^* = \\lambda_1 x_{i} + (1 - \\lambda_1)x_{frac}, \\lambda_1 \\sim Beta(\\alpha_1, \\alpha_1)\\)  (1)\n\\(D^* = \\{(\\tilde{x}, y_i)\\}_{i=1}^{N}\\)  (2)\nThe generated \\(D^*\\) keeps the original label, \\(y_i\\), of the original instance, \\(x_i\\). Following prior work of PixMix, we apply either geometric transformation (e.g., rotation, solarization, etc.) or Mixup with a fractal image to each ID instance for up to four iterations. In this paper, we further restrict to choose Mixup with a fractal image for at least one iteration.\nPixMix has been proven to be effective in OOD detection by integrating diverse natural patterns from fractal images. Moreover, fractal images used in the Escape stage of ARES are class-agnostic and relatively easy to get, compared to previous work that requires a heavy real OOD dataset [44]. We have figured out that the generated \\(D^*\\) reflects the original characteristics of D but is located outside of the original ID region. That is, we have achieved the surrogate ID instances by escaping from the real ID region, as shown in Figure 1 (a).\n3.3.2 Expansion Stage. The generated surrogate ID instances are far away from the original ID region, so we can now start the actual process of generating OOD instances. To fully use the generated surrogate ID region \\(D^*\\), we expand this region by mixing multiple surrogate ID instances with each other, so that the expanded region could simulate the OOD region. For the Expansion stage, we adopt Manifold Mixup [38], which mixes feature representations rather than raw data instances. In specific, we select the feature map at the penultimate layer of the classifier for mixing as the below:\n\\(x^{**} = \\lambda_2 f_{ext} (x_i^*) + (1 - \\lambda_2) f_{ext} (x_j^*),\\) (3)\nwhere \\(\\lambda_2 \\sim Beta(\\alpha_2, \\alpha_2)\\) and \\(x_i^*, x_j^* \\in D^*\\). This allows us an efficient mixing process in a low-dimensional space as well as the utilization of rich information extracted by the feature extractor, \\(f_{ext}\\). We can assume this expanded region, which is represented as pink in Figure 1 (b), would contain OOD region which is far away from our given ID region.\nSpecifically, we need a sufficiently discriminative feature to get \\(x^{**}\\) in Eq. (3). Hence, we define a pre-defined epoch to pre-train the feature extractor, \\(f_{ext}\\), to start Eq. (3).\n3.3.3 Estimation Stage. Finally, we get the expanded set, denoted as \\(X = \\{x^{**}\\}\\). Given X, we assume a multivariate Gaussian distribution of X as N(\\({\\mu}\\), \\({\\Sigma}\\)), where we estimate the mean and variance of the distribution as the below.\n\\({\\mu}\\) = \\(\\frac{1}{N}\\sum_{i=1}^{N} x^{**}_{i}\\)  (4)\n\\({\\Sigma}\\) = \\(\\frac{1}{N}\\sum_{i=1}^{N} (x^{**}_{i} - {\\mu}) (x^{**}_{i} - {\\mu})^T \\)  (5)\nIt should be noted that the distribution on X is estimated as a class-agnostic form rather than a class-wise form, by using all \\(x^{**} \\in X\\) at once for calculating Eq. (4)\u2013(5). Modeling class-agnostic distribution further fits with the nature of OOD instances, which would be scattered across the whole data space. Following the prior work [9], we assume that OOD instance would have the lowest likelihood. Hence, we generate virtual outliers as below:\n\\(V = \\{v | \\frac{1}{(2\\pi)^{p/2} |{\\Sigma}|^{1/2}} exp(-\\frac{1}{2} (v - {\\mu})^T {\\Sigma}^{-1}(v - {\\mu})) < \\epsilon \\},\\)  (6)\nwhere p denotes the dimension of \\({\\mu}\\), and \\(\\epsilon\\) is a hyperparameter for thresholding the likelihood of outliers. To set the value of \\(\\epsilon\\), we randomly select M instances from X; and we use the t-th smallest likelihood as \\(\\epsilon\\). We describe the selection on M and t in Section 4.1.2. Figure 1 (c) confirms that the generated virtual outliers, v, which are denoted as blue stars, are located far away from the original ID region. The sampled v are generated from broad region, which is modeled via Escape and Expansion stages. That is, v covers from fine-grained (i.e., X) to coarse-grained (i.e., F) OOD, enhancing the robustness of ARES.\n3.3.4 Divergence Stage. Given the generated virtual outliers, we need to train a discriminator that distinguishes virtual outliers from the inliers that were given as the training set. Traditionally, an energy-based score has been widely used because of its simplicity.\nIn prior work [9], the discriminator on ID and OOD uses cross-entropy loss for energy-based score with further forwarding them through logistic regression. However, we conjecture that this loss modeling has limitations in that logistic regression requires additional parameters to train. Hence, we propose to directly discriminate energy-based scores between inliers and virtual outliers, by adopting Jensen-Shannon Divergence (JSD) loss [21]. JSD is widely used as loss in generative models like GAN when learning the difference between two distributions, because it allows for calculating the distance as divergence [10]. JSD often invokes problems when used as a minimization objective in generative models such as Wasserstein Generative Adversarial Networks (WGAN), because of its inaccurate expression of the distance metric when applied to already distinguished distributions [1]. However, these concerns are alleviated in ARES where JSD is used as a maximization objective to separate overlapping distributions.\nHence, we adopt JSD loss for ARES, which attempts to directly repel the energy-based scores between inliers and outliers. In specific, since \\(x^*\\) are in raw pixel space while v are in feature space extracted by \\(f_{ext}\\), we first extract features for \\(x^*\\) using \\(f_{ext}\\). Then, we train our OOD discriminator with loss as below:\n\\(L_{dis} = E_{v\\sim V,x^*\\sim D^*} [JSD (P (E(f_{ext} (x^*); {\\theta})) || Q (E(v; {\\theta}))) ],\\) (7)\nwhere P and Q denote the probability distributions of energy-based scores of ID instances and virtual OOD instances, respectively.\nTo construct the probability distribution of energy-based score, we sample virtual OOD instances using Eq. (6) with the same number as ID instances. That is, given ID instances with batch size of B, \\(\\epsilon\\) in Eq. (6) is set as the B-th lowest likelihood value. With the sampled virtual OOD instances and ID instances, we construct a frequency distribution of energy-based scores for both instances. Then, we assume Gaussian distributions for inliers and virtual outliers, denoted as \\(P \\sim N({\\mu}_{x^*}, {\\sigma}_{x^*}^{2})\\) and \\(Q \\sim N({\\mu}_{v}, {\\sigma}_{v}^{2})\\) respectively, by estimating the mean and variance as followings.\n{\\mu}_{x^*} = \\(\\frac{1}{B}\\sum_{i=1}^{B} E(f_{ext}(x^*_i); {\\theta})\\)  \\({\\sigma}_{x^*}^{2} = \\(\\frac{1}{B}\\sum_{i=1}^{B} (E(f_{ext}(x^*_i); {\\theta}) - {\\mu}_{x^*})^2 \\) (8)\n{\\mu}_{v} = \\(\\frac{1}{B}\\sum_{i=1}^{B} E(v_i; {\\theta})\\)  \\({\\sigma}_{v}^{2} = \\(\\frac{1}{B}\\sum_{i=1}^{B} (E(v_i; {\\theta}) - {\\mu}_{v})^2\\) (9)\nFinally, the JSD term in the discrimination loss of Eq. (7) is calculated using the estimated Gaussian distributions:\n\\(JSD (P || Q) = \\frac{1}{2} [KLD (P || M) + KLD (Q || M) ],\\) (10)\nwhere \\(M = \\frac{P+Q}{2} \\) follows N(\\({\\mu}_{M}\\), \\({\\sigma}_{M}^{2}\\)) and each KLD term is calculated in a closed-form solution as the followings.\n\\(KLD (P || M) = log \\frac{{\\sigma}_M}{{\\sigma}_{x^*}} + \\frac{{\\sigma}_{x^*}^2+({\\mu}_{x^*}-{\\mu}_M)^2}{2{\\sigma}_M^2} - \\frac{1}{2}\\) (11)\n\\(KLD (Q || M) = log \\frac{{\\sigma}_M}{{\\sigma}_{v}} + \\frac{{\\sigma}_{v}^2+({\\mu}_{v}-{\\mu}_M)^2}{2{\\sigma}_M^2} - \\frac{1}{2}\\)\nIn Eq. (7)-(11), E(x; \\({\\theta}\\)) denotes the energy-based score of an arbitrary input, x, as the below:\n\\(E(x; {\\theta}) = - log \\sum_{k=1}^{K} w_k exp(f_{cls}(x)),\\) (12)\nwhere k denotes the dimension of \\(f_{cls}(x)\\) and \\(w_k\\) is a learnable weight coefficient.\n3.4 Overall Algorithm\nThe model, \\(f_{\\theta}\\), is trained to minimize a total loss as the below:\n\\(L_{total} = E_{(x^*,y)\\sim D^*} [L_{cls}] + {\\beta}\\cdot L_{dis},\\) (13)\nwhich aims at both classifying inliers correctly by \\(L_{cls}\\); as well as discriminating between inliers and outliers by \\(L_{dis}\\). Here, \\({\\beta}\\) controls the weight for the discrimination loss of Eq. (7).\nAfter training the model, we discriminate the test dataset into inlier or outlier using the energy-based score as a criterion, which follows Eq. (14) of the below:\n\\(G(x_{test}) = \\begin{cases} 1 & \\text{if } E(x_{test}; {\\theta}) \\geq {\\gamma}, \\\\ 0 & \\text{if } E(x_{test}; {\\theta}) < {\\gamma}, \\end{cases}\\) (14)\nwhere 1 denotes the inlier and 0 denotes the outlier. If a test example is discriminated as an inlier, we predict its label using the prediction of \\(f_{\\theta}\\). Otherwise, we predict its label as \"unknown\". In Eq. (14), \\({\\gamma}\\) is a threshold that is used to separate ID instances from OOD instances. In this paper, we set \\({\\gamma}\\) to screen ID instances at FPR95.\nFinally, Algorithm 1 describes the overall process of ARES."}, {"title": "4 EXPERIMENTS", "content": "4.1 Experimental Settings\n4.1.1 Dataset. We train our model on the ID dataset with CIFAR-10 and CIFAR-100 [16]. For evaluation of the OOD detection, we use five datasets that can cover various real-world out-of-distribution; including 1) Texture [5], 2) SVHN [25], 3) LSUN-C [41], 4) iSUN [39], 5) Place365 [45]. Each dataset is exclusive to ID dataset.\n4.1.2 Model and Training Details. We use WideResNet40 [42] for the backbone network. We adopt Stochastic Gradient Descent (SGD) optimizer [28] with a learning rate of 1e-1 which decreases to 1e-6 using a cosine scheduler [23]. We use a batch size of 128 for both ID instances and OOD instances to calculate the loss of Eq. (13). In specific, to sample virtual outliers, v, we randomly selected M = 10,000 instances and used the instances with the t = 128-th smallest likelihood to set \\(\\epsilon\\) of Eq. (6). We train our model for 500 epochs. During the first 200 epochs, the model is trained with only classification loss, which is the first term of Eq. (13). Then, the model is trained with both classification loss and discrimination loss for the rest epochs.\nAdditionally, detailed settings for ARES are as follows. In the Escape stage, the Mixup with the fractal data uses \\(\\alpha_1\\)=3 in Eq. (1). In the Expansion stage, we use \\(\\alpha_2\\)=2 in Eq. (3) for the Mixup in the feature space. Finally, \\({\\beta}\\), which is the weight for the discrimination loss, in Eq. (13) is 0.1.\n4.1.3 Evaluation Metrics. We use various metrics to evaluate the performance of OOD detection: 1) FPR95, which is the false positive rate of OOD samples when the true positive rate of ID samples is at 95%, and 2) AUROC, which is the area under the receiver operating characteristic curve.\n4.1.4 Baseline Method. As presented in Table 1, we compare our ARES with other OOD detection methods. The methods compared include a total of 8, as follows: 1) MSP [13], 2) ODIN [20], 3) Mahalanobis [19], 4) CSI [35], 5) MixOE [44], 6) VOS [9], 7) RONF [12] and 8) NPOS [36]. We implemented most of the baselines through the Open-OOD benchmark framework [40]. For MSP and ODIN, we adopted the result from relevant research [36]. For RONF, we could not reproduce the model so we recorded the averaged results reported by the authors.\n4.2 Main Results\n4.2.1 Quantitative Analysis Results. We first provide quantitative results on evaluation metrics in Table 1. As shown in Table 1a, our methodology, ARES, demonstrates the best performance in all metrics across various OOD datasets when using CIFAR-10 as ID dataset.\nIf we contrast ARES with VOS [9] which is our most relevant baseline, the gap is apparent. The largest gap is seen on the Texture dataset, where FPR95 and AUROC improved from 45.98% to 4.32%, and from 87.80% to 99.01%, respectively. Based on the average values, improvements are obvious from 32.27% to 8.60% in FPR95, and from 92.44% to 98.10% in AUROC. The primary reason for these significant improvements lies in the process of estimating distributions of OOD region to synthesize virtual outliers. VOS estimates the region for virtual OOD by only utilizing given ID instances. This leads to a discernible value difference between the estimated virtual outliers and real outliers. However, ARES aims to model the OOD region as far from ID region as possible, which is accomplished by each stage of Escape, Expansion, Estimation, and Divergence stages. Hence, ARES generates more OOD-like virtual instances that deviate from original ID instances.\nAdditionally, we compare ARES to RONF [12], which has the second-best performance of AUROC. With regard to AUROC, ARES with 98.10% outperforms RONF with 95.34%, which shows an improvement of 2.76 percentage point. RONF mixes virtual ID instances found at the boundary to generate virtual OOD instances. That is, the ingredient of Mixup lies near the ID region. Also, RONF uses the Mixup instances directly as virtual outliers. Moreover, RONF still generates virtual OOD instances conditioned on class labels, which might limit the dispersion of OOD instances. Compared to RONF, ARES adopts the Escape stage to find the region deviated from ID space for Mixup. Also, ARES further estimates the distribution of OOD from the Mixup instances via Expansion stage. Finally, ARES generates virtual outliers in a class-agnostic way, which reflects the scattered nature of real OOD instances.\nWhen compared to NPOS [36], which has the second-best performance of FPR95 based on average values, ARES improved from 19.20% to 8.60%, showing an enhancement of 10.60 percentage point. NPOS proposes a non-parametric method to generate virtual outliers. Specifically, NPOS finds inliers lying at the boundary by utilizing k-NN distance and generates virtual outliers near the discovered inliers. Though NPOS differs from RONF in that it adopts a non-parametric method, NPOS still shares a similarity with RONF by limiting the generated virtual outliers near the ID region. Hence, ARES also outperforms NPOS by escaping from the ID region for the generation of OOD instances.\nIn Table 1b, it can be observed that even when the ID dataset is CIFAR-100, our method demonstrates the best performance across all metrics on Texture and iSUN as OOD. Also, our method shows the second-best performance on SVHN and LSUN-C. It should be noted that the average values across all OOD datasets are the best in ARES.\nEspecially, the improvement of ARES compared to other baselines was larger on CIFAR-100 than on CIFAR-10. For example, comparing ARES and CSI on Texture dataset, the improvement of FPR95 by ARES was 63.44 percentage point (i.e., from 91.99% to 28.55%) when using CIFAR-100 as ID dataset; while 53.57 percentage point (i.e., from 57.89% to 4.32%) were improved by ARES when using CIFAR-10 as ID dataset. This improvement by ARES is also observed on other datasets and baselines, suggesting that the effect of ARES becomes apparent in the more complex dataset.\n4.2.2 Qualitative Analysis Results. Now, we provide a qualitative analysis of the improvement by ARES. Figure 2 describes the distribution of actual inliers as well as virtual outliers using Uniform Manifold Approximation and Projection (UMAP) [24]. In the Figure, the colored dots indicate the inliers, while blue stars indicate the virtual outliers. As shown in Figure 2a, VOS synthesizes virtual outliers at the boundaries and further within each region of ID class. The overlap of virtual outliers with inliers may diminish their value as outliers. Conversely, ARES successfully synthesizes virtual outliers outside the ID class region, as shown in Figure 2b. We conjecture that the separation between inliers and outliers comes from each stage of ARES. As a result, ARES ensures the definitive value of virtual outliers, leading to an effective generation of OOD-like instances.\nNext, we describe the distribution of energy-based scores for inliers, outliers, and virtual outliers. In Figure 3, we compare VOS and ARES by selecting CIFAR-10 as ID and SVHN as OOD datasets. Figure 3a shows that in VOS, energy-based scores of virtual outliers are distributed by overlapping with inliers rather than outliers, indicating the limited characteristics of virtual outliers as OOD instances. This further results in insufficient discrimination between inliers and outliers. In contrast, Figure 3b shows that virtual outliers by ARES have a closer distribution of energy-based scores to outliers rather than inliers. Having said that, ARES successfully discriminates outliers from inliers."}, {"title": "4.3 Ablation Study Results", "content": "ARES is built upon the four main stages proposed in Section 3. Among them, the initial three stages, which are the Escape, Expansion, and Estimation stages, ultimately determine the distribution and shape of the generated virtual outliers. Then, Divergence stage serves as a discrimination loss. Hence, we provide a broad ablation study on each stage.\n4.3.1 Ablation on Each Stage of ARES. Figure 4 shows the results by excluding each stage from ARES. In the Figure, colored dots denote inliers; black dots denote fractal images used in Escape stage; pink diamonds denote Mixup instances generated in Expansion stage; and blue stars denote virtual outliers generated in Estimation stage.\nIn Figure 4a, which excludes the first Escape stage, we observe that the virtual outliers are synthesized near the ID region without significantly deviating from the ID boundary, which would have limited value of synthesized outliers to serve as an OOD instance. We conjecture that a similar phenomenon would be observed in RONF, leading to a lower performance as in Table 1a; because RONF first finds boundary instances following VOS and then further mixes them for virtual outliers. Next, we exclude the Expansion stage in Figure 4b, where Mixup instances do not exist. In the figure, the virtual outliers are synthesized from the escaped ID region but are not expanded, resulting in a less broad distribution. Finally, Figure 4c shows the result of excluding the Estimation stage and randomly selecting virtual outliers. In the figure, the UMAP is similar to Figure 2b which applies all the stages of ARES. However, some virtual outliers are synthesized within the ID region, which might have a high likelihood.\nThe effect of each stage is also confirmed in Table 2. In the table, we select CIFAR-10 as ID and LSUN-C as OOD datasets; and we report FPR95. FPR95 is the worst with 11.25% when the Escape stage is excluded, as shown in the first row of Table 2. If we exclude Expansion stage instead, the performance improves to 7.05% as shown in the second row, but it is still not the best performer. Next, the third row shows further improved performance of 6.75% by excluding Estimation stage instead. Finally, our findings indicate that ARES shows the best performance of 6.00% when all three main stages are harmoniously applied. We conclude from the table that the performance of OOD detection is affected by each stage in the order of Escape, Expansion, and Estimation.\n4.3.2 Ablation on Discrimination Loss. Another contributing factor of ARES is the discrimination loss of Eq. (7) of the Divergence stage, i.e., loss for discriminating the"}]}