{"title": "MHGNet: Multi-Heterogeneous Graph Neural Network for Traffic Prediction", "authors": ["Mei Wu", "Yiqian Lin", "Tianfan Jiang", "Wenchao Weng"], "abstract": "In recent years, traffic flow prediction has played a crucial role in the management of intelligent transportation systems. However, traditional forecasting methods often model non-Euclidean low-dimensional traffic data as a simple graph with single type nodes and edges, failing to capture similar trends among nodes of the same type. This paper proposes MHGNet, a new framework for modeling spatiotemporal multi-heterogeneous graphs, in which the STD Module decouples single-pattern traffic data into multi-pattern traffic data through feature mappings of timestamp embedding matrices and node embedding matrices. Then the Node Clusterer uses the Euclidean distance between nodes and different types of limit points for O(N) time complexity clustering, and the nodes within clusters undergo residual subgraph convolution in the spatiotemporal fusion subgraphs generated by the DSTGG Module before entering the SIE Module for node repositioning and redistribution of weights. This paper conducts a series of ablation and quantitative evaluations on four widely used benchmarks to validate the effectiveness of MHGNet.", "sections": [{"title": "I. INTRODUCTION", "content": "The task of traffic prediction (e.g., traffic flow prediction) is based on historical traffic conditions collected from sensors to forecast future traffic conditions [1]\u2013[3]. Graph Neural Networks (GNN) have been introduced to effectively manage the non-uniform topological structure of urban traffic networks [4]\u2013[6]. Spatiotemporal Graph Neural Networks (STGNN) combine GNN with various temporal learning methods to capture hidden patterns of spatially irregular signals that change over time, thus addressing the spatiotemporal heterogeneity of non-Euclidean urban data [7]\u2013[9]. However, existing models still cannot address the following issues in building spatiotemporal relationships between graph nodes:\n(1) Lack of handling the structural patterns of traffic. Traffic signal features are contributed by multiple traffic patterns (e.g., trucks, passenger cars, broken-down vehicles). As shown in Figure 1, nodes with similar traffic patterns have similar flow (speed) changes, regardless of physical spatial distance.\n(2) Lack of handling multi-dimensional spatiotemporal relationships. Current methods typically use Graph Neural Networks (GNN) to aggregate spatial features of single-dimensional flow or speed, and then use RNN or TCN for temporal feature extraction [10]\u2013[12]. In reality, true traffic data is dynamically complex, exhibiting multi-dimensional spatiotemporal heterogeneity [13]\u2013[15] within the spatial area of each node.\n(3) Lack of diversity in modeling. Existing methods typically model non-Euclidean traffic data as simple graphs with single-type nodes and edges [16]\u2013[18]. However, real-world complex traffic data, as shown in Figure 1, is a multigraph with various types of nodes and edges. The traffic flow of nodes of the same type shows similar trends, and node pairs are connected by two types of edges: temporal and spatial relationships.\nTo address issues one and two, the Spatiotemporal Decoupling Module in MHGNet is designed to decouple single-pattern traffic data into multi-pattern traffic data through the feature mapping of the timestamp embedding matrix and node embedding matrix, capturing dynamic multi-dimensional spatiotemporal information in the traffic pattern structure. For issue three, the complex traffic network is modeled as a multi-layer heterogeneous graph. A node clustering algorithm is employed, which clusters nodes by calculating the Euclidean distance between the nodes in a multi-dimensional feature space and different types of centroid points. The node sequence numbers are then stored in a node sequence pool.\nAs shown in Figure 2, the dynamic spatiotemporal graph generation module creates dynamic spatiotemporal subgraphs with weighted spatiotemporal relationships for each node cluster. This enables convolution operations on smaller, simpler homomorphic graphs rather than on large, complex heterogeneous graphs, effectively reducing time complexity and conserving computational resources. Finally, the clustered node IDs from the node sequence pool are used for"}, {"title": "II. METHODOLOGY", "content": "In this section, we will elaborate on the proposed framework and its components. An overview of our model architecture is illustrated in Figure 3.\nTo obtain heterogeneous spatiotemporal feature mappings for different traffic patterns, we introduce daily and weekly periodic feature matrices $T_D \\in \\mathbb{R}^{T_h \\times N \\times D_t}$ and $T_W \\in \\mathbb{R}^{T_h \\times N \\times D_t}$ into the STD Module, and then utilize a node embedding matrix $E \\in \\mathbb{R}^{N \\times D_s}$ to weight and partition the original traffic flow $X_{t-T_h:t}$ into $P$ types of traffic pattern features.\n$\\Omega_{(t,i)} = Sigmoid((ReLU(T_i)||T_i) || E_i)W_1)W_2)$\n$X_n = X_{t-T_h:t} \\oplus \\Omega_n$\n$X_p = X_{t-T_h:t} - \\sum_{n=1}^{p-1} X_n$\n(1)\nHere $\\Omega_{(t,i)}$ represents the output proportion of the specific traffic pattern at node i relative to total traffic at time step t."}, {"title": "A. STD Module", "content": "As illustrated in Figure 4a, the clustering feature space is constructed based on the P types of traffic patterns:\n$R_t = \\frac{(X_1W_1||X_2W_2...||X_pW_p)}{X_{t-T_h:t}W}$\n$\\bar{R} = \\frac{1}{T_h} \\sum_{t=1}^{T_h} R_t$\n$C_j = Max(\\bar{R}_{:,j}), j = 1, 2, ..., P$\n(2)\nThe weight matrices $W_i \\in \\mathbb{R}^{D \\times 1}$ for $i = 1,2,..., P$ transform the enriched multi-dimensional features into a single dimension. We determine the maximum value of $\\bar{R}$ across the $P$ dimensions, resulting in the limit point matrix $C\\in \\mathbb{R}^P$, where $C_p$ represents the maximum extremum state of the original traffic feature associated with feature $P$.\n$D_{i,j} = |\\bar{R}_{i,j} - C_j |, j = 1, 2, . . ., P$\n$T_i = argmin(D_i), i\\in S_T$\n(3)\nAs shown in Figure 4b, the Euclidean distance matrix $D \\in \\mathbb{R}^{N\\times P}$ is calculated between the feature tensor $\\bar{R}$ and the limit point matrix $C$ in the P-dimensional space. The type $T_i \\in \\mathbb{R}^1$ of node i is the type of the limit point with the minimum Euclidean distance, placing i in the node sequence pool $S_T$ for type $T_i$, which is used for subsequent node placement. The time complexity of this clustering algorithm is O(N), which significantly saves computational resources compared to traditional clustering algorithms."}, {"title": "B. Node Clusterer", "content": "We introduce the STGG Module to model the pairwise latent spatiotemporal relationships between subsets of nodes, generating a simple subgraph containing spatiotemporal fusion relationships.\n$M_1 = tanh(\\mathcal{E}EW_1)$\n$M_2 = tanh(\\mathcal{E}EW_2)$\n$\\hat{A} = \\alpha(M_1M_1^T - M_2M_2^T)$\n(4)\nThe matrices $\\mathcal{E}_b \\in \\mathbb{R}^{N_P \\times D_s}$ and $\\mathcal{E} \\in \\mathbb{R}^{N_P \\times D_s}$ represent the node embeddings for cluster P that are randomly initialized. The adjacency matrix $\\hat{A} \\in \\mathbb{R}^{N_P \\times N_P}$ containing the spatial relationships of cluster P is generated.\n$T_D^P = (T_D^{i_1} || ... || T_D^{i_{N_P}}), \\{i_1,...,i_{N_P}\\} \\subseteq V_p$\n$T_W^P = (T_W^{i_1} || ... || T_W^{i_{N_P}}), \\{i_1,...,i_{N_P}\\} \\subseteq V_p$\n$\\mathcal{A} = \\beta ( ReLU ( \\frac{1}{T_h} \\sum_{j=1}^{T_h} tanh (B(A)) ))$\n(5)\nThe matrix $\\mathcal{A} \\in \\mathbb{R}^{T_h \\times N_P \\times N_P}$ captures the periodic temporal features of daily and weekly patterns. By applying average pooling and non-linear transformations, we obtain the fused temporal information matrix $\\hat{\\mathcal{A}}$ within the $T_h$ time window for cluster P.\n$\\mathcal{A}_P = ReLU ( tanh (B(\\mathcal{A})))$\n$\\hat{\\mathcal{A}}_{[i,:]} = \\begin{cases}\n    \\mathcal{A}_{P,[i,:]} & \\text{if } j \\in argtopk(\\mathcal{A}_{[i,: ]}) \\\\\n    0 & \\text{otherwise}\n\\end{cases}$\n(6)\nUsing the sparsified spatial graph and the fused temporal information matrix, we construct a simple graph containing heterogeneous spatiotemporal information."}, {"title": "C. DSTGG Module", "content": "This article introduces the SIE module, which is used to capture clustering features and spatiotemporal patterns in dynamic fused subgraphs, as shown in Figure 5.\nTo achieve information control and capture long-term dependencies, MHGNet introduces a gating mechanism in the graph convolution operation:\n$\\mathcal{A}_P = \\mathcal{A}_P + I$\n$\\hat{H}_P^{(k)} = \\gamma \\hat{H}_P + (1 - \\gamma)(\\tilde{D}^{-1} \\tilde{A})\\hat{H}_P^{(k-1)}$\n$\\hat{H}_P = (H_P^{(0)} || H_P^{(1)} || ... || H_P^{(k-1)})W$\n(7)\nIn the formulas, let $\\mathcal{A}_P$ denote the adjacency matrix that includes self-loops. The degree matrix $\\tilde{D} \\in \\mathbb{R}^{N \\times 1}$ is defined such that $\\tilde{D}_{ii} = \\sum_j \\mathcal{A}_{ij}$ for each node $i$. Additionally, the parameter k represents the depth of propagation.\n$X_h = (\\hat{H}_1 || \\hat{H}_2 || ... || \\hat{H}_P)$\n$X_{h[:, i, :]} = X_{h[:, j,:]}, i\\in S_N, j\\in S_T$\n(8)\nIn these equations, $X_h$ represents the shuffled subgraph convolution results, $S_N$ denotes a continuous permutation from 0 to N, and $S_T$ is the node sequence pool generated by the Node Clusterer. The Sort Module regresses the node sequence, which is then used by the RNN to capture long-term temporal dependencies.\nThe model stacks the input over $T_c$ time steps in the GRU to form an output tensor $H_{out} \\in \\mathbb{R}^{T_c \\times N \\times (M \\times D)}$, and then performs weight redistribution on $H_{out}$:\n$H_{out} = dropout ([H^{(0)} H^{(1)} H^{(2)} ... H^{(T_c-1)}])$\n$\\bar{H}_{out} = (ReLU (H_{out} * W_1) * W_2)$\n$X_{out} = \\bar{H}_{out} W$\n(9)"}, {"title": "D. SIE Module", "content": "For the multi-scale spatiotemporal information captured by the SIE Module, we establish skip connections to incorporate additional feature information:\n$H = \\{X_{out} || X_{t-T_h:t} || X_1 || . . . || X_P || T_D || T_W \\}$\n(10)\nThe obtained tensor $H$ is fed into the regression layer for feature integration and nonlinear transformation to obtain the prediction state $X_{t:t+T_f}$:\n$X_{t:t+T_f} = [(ReLU (ReLU(H)W_1) W_2) * W]^T$\n(11)"}, {"title": "E. Output and prediction", "content": "As shown in Tables III and IV, our method achieves better performance across most metrics on all four datasets. We can draw the following conclusions: Compared to GNN-based models (such as DDGCRN and AGCRN), MHGNet improves performance through subgraph-based heterogeneous graph modeling. Compared to embedding-based models like STID, it demonstrates better adaptability in handling complex traffic data. Additionally, when compared to attention-based models (such as ASTGNN and PDFormer), it exhibits good robustness and generalization performance, particularly excelling in the traffic speed dataset (such as PEMS-BAY)."}, {"title": "A. Datasets", "content": "In this paper, we propose MHGNet, a new framework for modeling spatiotemporal heterogeneous graphs. The model utilizes a timestamp embedding matrix and a node embedding matrix to decouple one-dimensional features into a multi-dimensional feature space, enabling simple clustering. During the clustering process, a dynamic graph containing spatiotemporal information is generated for subgraph convolution. Moreover, experiments are conducted on multiple datasets."}, {"title": "IV. CONCLUSION", "content": "$\\hat{H}_P^{(k)} = \\hat{H}_P + (1 - \\gamma)(\\tilde{D}^{-1} \\tilde{A})\\hat{H}_P^{(k-1)}$"}, {"title": "B. Baseline Methods", "content": "Baseline Methods: GWNet [19], DCRNN [20], STGNCDE [21], GMAN [22], AGCRN [23], STNorm [24], ASTGNN [25], STID [26], DDGCRN [27], PDFormer [28]"}, {"title": "C. Experiment Settings", "content": "The experiment was conducted on a system equipped with an NVIDIA GeForce RTX 4090 GPU. The METR-LA and PEMS-BAY datasets were divided into training, testing, and validation sets in a 7:2:1 ratio. For the PEMS04 and PEMS08 datasets, we used a 6:2:2 split. The model was trained using the Adam optimizer, with a weight decay (wdecay) of 1.0 \u00d7 10-5 and an epsilon (eps) value of 1.0 \u00d7 10\u22128. Implement a warm-up strategy during the first 20 training cycles, followed by curriculum learning with a length of 3. The model parameters for the four datasets are detailed in Table II. The source code of MHGNet is available: https://github.com/meiwu5/MHGNet.git"}, {"title": "D. Experiment Results", "content": "\u2022\nw/o Node Clusterer: No node clustering, perform a single graph convolution\n\u2022\nMHGNetp=2: Node clustering into 2 patterns followed by subgraph convolution\n\u2022\nMHGNetp=3: Node clustering into 3 patterns followed by subgraph convolution\n\u2022\nw/o sg: Removing A in the DSTGNN Module.\n\u2022\nw/o tg: Removing A in the DSTGNN Module.\n\u2022\nMHGNetp=2: Using the spatiotemporal fused graph Ap."}, {"title": "E. Ablation Experiments", "content": "$\\bar{R} = \\frac{1}{T_h} \\sum_{t=1}^{T_h} R_t$"}]}