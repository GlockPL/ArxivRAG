{"title": "ElasticAI: Creating and Deploying Energy-Efficient Deep Learning Accelerator for Pervasive Computing", "authors": ["Chao Qian", "Tianheng Ling", "Gregor Schiele"], "abstract": "Abstract-Deploying Deep Learning (DL) on embedded end\ndevices is a scorching trend in pervasive computing. Since most\nMicrocontrollers on embedded devices have limited computing\npower, it is necessary to add a DL accelerator. Embedded\nField Programmable Gate Arrays (FPGAs) are suitable for\ndeploying DL accelerators for embedded devices, but developing\nan energy-efficient DL accelerator on an FPGA is not easy.\nTherefore, we propose the ElasticAI-Workflow that aims to help\nDL developers to create and deploy DL models as hardware\naccelerators on embedded FPGAs. This workflow consists of two\nkey components: the ElasticAI-Creator and the Elastic Node.\nThe former is a toolchain for automatically generating DL\naccelerators on FPGAs. The latter is a hardware platform for\nverifying the performance of the generated accelerators. With\nthis combination, the performance of the accelerator can be\nsufficiently guaranteed. We will demonstrate the potential of our\napproach through a case study.", "sections": [{"title": "I. INTRODUCTION AND RELATED WORK", "content": "Introducing Deep Learning (DL) to embedded devices en-\nables things to perceive and react to their surroundings with\nintelligence. Cloud/Edge-based DL approaches are now well-\nestablished and widely adopted, but they have shortcomings\nin accessibility, reliability, and privacy. Consequently, there\nis a tendency to deploy DL models on end devices, where\nMicrocontrollers (MCUs) are often employed as the processor\nto deal with general tasks due to their power efficiency. How-\never, their low-power feature results in the lack of resources\nand computational capacity required for the efficient execution\nof DL models. Sometimes, highly optimized models infer too\nslowly on such MCUs to reach real-time requirements.\nPrevious studies [1], [2] indicate that embedded Field\nProgrammable Gate Arrays (FPGAs) are advantageous for\naccelerating algorithms to gain greater speed or power effi-\nciency. Thus, we use an FPGA to execute highly optimized,\napplication-specific DL accelerators instead of employing a\nmore powerful MCU for faster DL operations. The low-power\nMCU is then always on and in charge of arranging onboard\nresources. At the same time, the FPGA can be activated to\naccelerate DL model inference when needed and will be de-\nactivated during idle states to conserve energy. By leveraging\nfeatures from the MCU and FPGA, the device can gain DL\nacceleration while meeting energy-efficiency requirements for\nthe entire system.\nTo infer a DL model on an FPGA, a corresponding DL hard-\nware accelerator should be implemented. Many approaches\n[3], [4] use High-Level-Synthesis (HLS) to simplify the de-\nvelopment of DL accelerators on FPGAs. However, it is known\nthat HLS introduces significant overhead. Blott et al. [5] report\nthat HLS introduced 45% resource overhead in their case.\nThus, we prefer to create DL accelerators using Register\nTransfer Level (RTL) template-based approaches. Although\ndesigning such templates incurs additional development work-\nload, it is worthwhile for low-energy devices since it saves\nmore resources and energy than HLS approaches.\nWith software, estimating the power consumption of gen-\nerated DL accelerators is feasible. However, noticing the\nunreliability of estimations, researchers also measure power\nconsumption on real hardware [2], [6]\u2013[9]. Nevertheless, major\nhardware platforms can only provide their overall power con-\nsumption, which is insufficient to verify the power estimation\nof the accelerator, let alone to guide the optimization of the\naccelerator. Thus, we favor developing special hardware to\nacquire fine-grained power consumption measurements.\nIn this work, we propose the ElasticAI-Workflow, which\nconsists primarily of the ElasticAI-Creator toolchain [10] and\nthe Elastic Node [9] hardware platform. We plan to demon-\nstrate the following:\n\u2022\na workflow (the ElasticAI) to help DL developers to\ncreate and deploy energy-efficient DL accelerators,\n\u2022\nan interactive process of solving a certain DL task using\nthe workflow to show the potential of our approach.\nIn the next section, we will introduce the details of our\nsystem. Then, a brief description of our demo will be provided.\nFinally, we will conclude with some thoughts and propose\nfuture work."}, {"title": "II. SYSTEM DESCRIPTION", "content": "This section first discusses the challenges of developing\nenergy-efficient DL accelerators on embedded FPGAs and\nthen explains our approach to structuring the ElasticAI-\nWorkflow."}, {"title": "A. Challenges", "content": "We have been researching emerging DL implementations\nfor embedded devices that own low-power MCU and FPGA\nfor several years. Reflecting on our experiences, we notice two\ncritical challenges to achieving energy efficiency.\n1) Solid FPGA expertise is required: To fit embedded\nFPGAS, DL developers need to aggressively optimize the DL\nmodel to simplify computation and reduce memory footprint\nwhile preserving the model's ability to give valid results. Then,\nto execute a particular (optimized) model on FPGAs, a cor-\nresponding digital circuit (accelerator) must be implemented,\nwhere optimizations for energy efficiency at the hardware\nlevel are required. For example, the constrained resources\nmake it impossible to naively instantiate all components of\nthe DL model on an embedded FPGA to run all components\nin parallel. Instead, reusing components over time to save\nresources is often applied for such FPGAs. Therefore, the\ndevelopers must have sufficient FPGA engineering expertise to\ncreate implementations that maximize the benefits of adding\nan FPGA by balancing resource utilization and energy effi-\nciency. However, it is non-trivial and time-consuming for DL\ndevelopers to master this knowledge.\n2) Reliable and fine-grained power consumption mea-\nsurements are required: Our previous work [11] indicates that\nbefore running an accelerator on an FPGA, it is possible to\nget a performance estimation by simulating the accelerator\non the circuit level and logging all switching activities using\nVivado\u00b9. However, the estimated power consumption can be\ninaccurate even with adequate simulation data. In addition,\nsuch simulations perform intensive computation and may take\na long time. A task that only takes a few seconds to execute on\nreal hardware may take hours in such a simulation. Therefore,\nwe prefer to perform real-world measurements on hardware\nto ensure reliability. To provide a good granularity of power\nconsumption measurements, similar to the ones estimated by\nVivado, special hardware with the capability of separately\nmeasuring the power consumption for each function region\nis required."}, {"title": "B. ElasticAI-Creator", "content": "To help DL developers cope with the lack of FPGA engi-\nneering knowledge, we propose the ElasticAI-Creator, which\nextends PyTorch with model components that are translatable\nto RTL components for FPGAs. Once the developers have\ndesigned a DL model with the supported components, this\nmodel can easily be translated into a hardware accelerator. The\ntrained and optimized model can be translated to a hardware\naccelerator in the RTL representation by simply pressing a\nbutton. Since the translation process can be taken care of by the\nElasticAI-Creator, the developers do not need to understand\nhow FPGAs work."}, {"title": "C. Elastic Node", "content": "The Elastic Node is a customized hardware platform that\ncan conveniently verify the energy efficiency of DL acceler-\nators in a real-world environment. In 2018 we demonstrated\nthe second version of the Elastic Node in PerCom [9] and"}, {"title": "D. ElasticAI-Workflow", "content": "Combining the ElasticAI-Creator and Elastic Node, we de-\nfine the ElasticAI-Workflow (see Figure 3) to develop energy-\nefficient DL accelerators on FPGAs. Currently, the ElasticAI-\nWorkflow involves three stages with a feedback loop that\ncould start from reports on multiple levels. According to\nthe information provided in the reports, DL developers can\nintervene with these stages to further optimize the DL model.\nIn the first stage, the DL developer can design a DL\nmodel with components supported by the ElasticAI-Creator.\nAfter model training/evaluation and testing under the PyTorch\nFramework, this model can be automatically translated to an\nRTL representation by the ElasticAI-Creator. Model optimiza-\ntion (such as hyper-parameters tweaking, layer replacement,\nand model quantization) can start with the information given\nin reports. The optimization loop will not terminate until the\ndevelopers are satisfied with the reports.\nIn the second stage, the RTL representation of the gen-\nerated accelerator is passed to the IDE of the FPGA vendor\n(such as Vivado from Xilinx) to synthesize and generate a"}, {"title": "III. DEMO INFORMATION", "content": "To demonstrate how our ElasticAI-Workflow works, we will\nuse a laptop and several Elastic Nodes to present it. On the\nlaptop, we will initially use the ElasticAI-Creator in a Jupyter\nnotebook to tune (optimize) a model design and perform\nmodel training with evaluation and testing for a particular DL\nassignment (e.g., time series analysis). Then, the ElasticAI-\nCreator will translate the optimized model into an accelerator,\nwhich Vivado will use to generate the bitfile for FPGAs. The\nperformance of the accelerator will be measured by loading\nthe bitfile to the Elastic Node. Reports from various stages\nwill be visualized and analyzed. In particular, measurements\non the Elastic Node will be shown in real-time on the laptop.\nIn addition, generating the bitfile takes several minutes\n(depending on the RTL design). While waiting, the audience\ncan suggest modifications to each stage of the workflow. The\nperformance of the newly generated accelerator will then be\nmeasured and compared to that of the previously generated\naccelerators. Potential reasons for performance changes can\nalso be discussed."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "Table I presents preliminary measurements from our previ-\nous work [11] of an accelerator generated by a Long Short-"}, {"title": "V. CONCLUSION AND OUTLOOK", "content": "Using ElasticAI-Workflow, DL developers without FPGA\nexpertise can create and deploy energy-efficient DL accel-\nerators on embedded pervasive devices. Moreover, the ac-\ncelerator's performance is promised due to the inclusion of\nverification on Elastic Node. Consequently, DL developers\ncan concentrate on enabling more DL models to be deployed\non embedded devices while addressing a broader range of\ntarget applications. In the near future, we intend to develop\nfurther model components to support more prevalent DL\nmodels, generating more DL accelerators. Moreover, we want\nto support FPGAs from other vendors."}]}