{"title": "Dynamic Fog Computing for Enhanced LLM Execution in Medical Applications", "authors": ["Philipp Zagar", "Vishnu Ravi", "Lauren Aalami", "Stephan Krusche", "Oliver Aalami", "Paul Schmiedmayer"], "abstract": "The ability of large language models (LLMs) to transform, interpret, and comprehend vast quantities of heterogeneous data presents a significant opportunity to enhance data-driven care delivery. However, the sensitive nature of protected health information (PHI) raises valid concerns about data privacy and trust in remote LLM platforms. In addition, the cost associated with cloud-based artificial intelligence (AI) services continues to impede widespread adoption. To address these challenges, we propose a shift in the LLM execution environment from opaque, centralized cloud providers to a decentralized and dynamic fog computing architecture. By executing open-weight LLMs in more trusted environments, such as the user's edge device or a fog layer within a local network, we aim to mitigate the privacy, trust, and financial challenges associated with cloud-based LLMs. We further present SpeziLLM, an open-source framework designed to facilitate rapid and seamless leveraging of different LLM execution layers and lowering barriers to LLM integration in digital health applications. We demonstrate SpeziLLM's broad applicability across six digital health applications, showcasing its versatility in various healthcare settings.", "sections": [{"title": "1 Introduction", "content": "The convergence of digital technology and healthcare has revolutionized medical monitoring and intervention, generating vast amounts of data through electronic health records (EHRs), wearable devices, and digital health applications. Used responsibly, this data can transform health care delivery, increasing patient engagement and improving outcomes [1,2]. To empower patients and digital health innovators to harness this data, the United States' 21st Century Cures Act, mandates EHR data accessibility via Fast Healthcare Interoperability Resources (FHIR) application programming interfaces (APIs) [3]. Nevertheless, challenges in efficiently leveraging this data persist.\nLLMs have the potential to harness the vast quantities of accessible health information, advancing healthcare objectives, reducing costs, and improving patient outcomes [4,5]. Such models generate human-like text from provided information, effectively bridging the gap between raw data and user interpretation and transforming vast amounts of heterogeneous, structured, or unstructured data into human-legible insights. LLMs can answer questions, summarize, paraphrase, and interpret text, outperforming human experts in certain contexts [6].\nEfficacy notwithstanding, the adoption of LLMs in healthcare contexts raises key data privacy and trust concerns [7]. The management of sensitive personal health data in cloud-hosted LLM execution environments has profound transparency, regulatory, and security implications [7,8]. Scaling LLMs is also resource-heavy, with high hardware costs that may deter would-be adopters.\nTo address these limitations, we present a paradigm shift in the digital health approach to LLMs: a fog-computing based architecture that dynamically relocates model execution closer to user devices [9]. Our approach creates the foundation for LLM inference environments on widely available, decentralized computing assets, such as users' mobile phones, laptops, or existing computing resources within secure, isolated hospital and clinic networks. We propose using multiple LLM execution environments to account for the capabilities of low-power resources running smaller models and the trust and financial concerns associated with remote cloud computing."}, {"title": "2 Background", "content": "Patient-accessible electronic health records (PAEHRs) have been shown to improve patient outcomes, improving engagement, self-management, and informed decision-making [1,10]. However, limited English proficiency (LEP) and health literacy disparities may hinder PAEHR utilization, even for those not facing technology or internet access barriers [11,12]. In contrast, LLM' sophisticated querying capabilities have the potential to enable patients to generate context-aware insights into their health data at any degree of complexity and in various languages. Preliminary validations have established the efficacy of LLMs such as OpenAI GPT models in this context [13,14]. One such demonstration is the open-source LLMonFHIR [15] application, designed to facilitate a \u201cdialogue\u201d with FHIR health information [16].\nAlthough LLMs frequently provide accurate information, their tendency to generate erroneous content-\u201challucinations\u201d-prevents them from serving as infallible or singular sources of truth in production environments, especially in high-stakes care delivery contexts. Ongoing investment in AI alignment and hallucination mitigation is reducing their frequency [17]. Accordingly, SpeziLLM is LLM-agnostic, allowing the continuous integration of newer, lower-hallucinogenic models [18].\nEdge computing, a distributed paradigm, brings computation and data storage closer to data sources, enhancing trust, security, and privacy by processing data locally [19]. Adapting compute-intensive LLMs for edge computing can be challenging, due to the disparity in computing resources between consumer-grade mobile edge devices and specialized cloud-based infrastructures. Edge devices' limited computational power, resulting from physical dimensions, heat dissipation, battery life, and cost constraints, restricts memory allocation and GPU capabilities. Ongoing efforts to overcome these limitations and enhance mobile inference performance include the introduction of more compact LLMs and model compression techniques (4-bit or 1-bit quantization) [20-23].\nFog computing leverages the strengths of both edge and cloud computing [9,24]. This architectural paradigm brings the substantial computational capabilities of central instances closer to consumers in a distributed fashion, lowering latency and optimizing network resources [9]. Fog computing extends the cloud computing paradigm to the network's edge, offering dynamic dispatch, agile computing resource allocation, increased efficiency, and enhanced trust in the execution environment [25]. Central to this approach are fog nodes: heterogeneous devices stationed near the network's edge [26]. Fog computing architectures are divided into three layers [24]:\n\u2022 Edge Layer (Local): Low-power IoT and end-user devices at the network's edge where data is generated and utilized.\n\u2022 Fog Layer: Positioned between the cloud and the edge, fog nodes with substantial computational power process data nearer to the source-a more trusted environment than distant servers [27].\n\u2022 Cloud Layer (Remote): Centralized units with massive computational resources, raising privacy, trust, and financial concerns."}, {"title": "3 Architecture", "content": "Our goal is to transparently and dynamically shift the inference environment of LLMs closer to the user's device. We aim to provide a uniform and interchangeable interface for interacting with LLMs, regardless of execution locality."}, {"title": "3.1 Retrieval-Augmented Generation Capabilities", "content": "Limited LLM context size poses a key challenge for LLM-based interactions with large amounts of data. Injecting all relevant data into the entire context window may also prove financially burdensome, given that the pricing of LLM inference is typically based on a per-token cost for LLM output and input.\nTo mitigate these challenges, major LLM service providers like OpenAI and Anthropic and openly available LLMs like Llama3 [31] have introduced function calling mechanisms [32], specific instantiations of Retrieval-Augmented Generation (RAG) [33], that enable LLMs to have structured and reliable interaction with external systems.\nWe integrated these mechanisms as a core feature in our proposed system architecture. We provide developers with a convenient and declarative LLM-agnostic domain-specific language (DSL) [34] that abstracts technical complexities and state management (Figure 3). The runner (or, more specifically, the active session) is initially configured with a collection of LLM Functions. Upon submission of the request message into the runner context, the inference process on the LLM Service is executed. The service may return a selection of tools, including the function name and encoded parameters, in addition to a humanly legible output stream of tokens. The runner identifies the configured LLM functions, injects the supplied parameters, and executes them concurrently. The results are seamlessly reintegrated into the runner's context."}, {"title": "3.2 Dynamic LLM Task Dispatching", "content": "The fog layer provides significant decentralized computing resources near the user's edge device, where health data resides and prompts are devised. Edge devices use dynamic dispatch to allocate inference tasks to the fog node with the highest proximity measurement, thus accurately identifying the most capable and reliable computing resource.\nFigure 4 illustrates the process by which an LLM inference job is assigned to a computing resource within the fog layer. It requires two components: A fog node that advertises LLM inference services (acting as the server) and a client that discovers and consumes the LLM resource. Upon receipt of a user message, the runner discovers Fog LLM Services available within the local network. Once a service is discovered, the LLM FogSession dispatches an inference job with proper authorization credentials to the previously discovered fog LLM service via a secure connection. If the use of that resource is permitted by the Fog Auth Service, the fog node will set up the language model and stream the content back to the client, who displays the response."}, {"title": "4 SpeziLLM", "content": "Based on the fog computing LLM architecture described in section 3, we propose a platform enabling developer access to LLMs across edge, fog, and cloud layers. Our goal is to empower developers to integrate LLMs securely, privately, and cost-efficiently, simplifying the complexities of the decentralized architecture for digital health innovators. To that end, we introduce SpeziLLM\u00b9 [35]: an open-source, MIT-licensed Swift software framework offering modularized, ready-to-use LLM capabilities that can be combined strategically to suit a breadth of patient care and clinical research goals. SpeziLLM is embedded in the Stanford Spezi ecosystem [36,37], which enables the rapid development of component-based, interoperable, and reusable digital health applications. As a first demonstration, SpeziLLM supports all major Apple operating systems, allowing seamless LLM integration across iOS, macOS, and visionOS. The framework additionally leverages Apple's hardware and software ecosystem, integrating with HealthKit and utilizing Apple Silicon and Metal acceleration.\nThe SpeziLLM framework encompasses a suite of convenience components essential for most LLM applications, including context data models, onboarding facilitators, LLM chat interfaces, and state and error management mechanisms. All layers are based on the shared mental model presented in section 3, facilitating a reusable and extensible ecosystem across the various execution layers."}, {"title": "4.1 Cloud or remote layer", "content": "OpenAI and its LLM inference API have become the blueprint among cloud service providers. SpeziLLM's cloud layer has been designed for interaction with remote LLMs via the OpenAI API schema, allowing it to engage with any cloud-layer LLM service that mirrors or bridges to the OpenAI API, such as Anthropic's Claude model.\nAs an additional abstraction, SpeziLLM provides a declarative function-calling DSL that simplifies integration, remaining LLM-agnostic to ensure consistent application code across different providers (see Figure 5). It translates function-calling definitions into the appropriate format for specific providers, such as OpenAI, facilitating seamless integration, enhancing code reliability, and bypassing the complexities associated with handling untyped function-calling JSON definitions."}, {"title": "4.2 Fog layer", "content": "SpeziLLM is designed to streamline the complex fog-based, decentralized LLM inference system implementation process. The Swift-based framework (the client) is complemented by a server-side Docker-packaged fog node component, which includes scripts to facilitate the rapid deployment and systemic integration of new fog nodes. The fog node's API aligns with the OpenAI API, enabling dynamic substitution of the underlying LLM inference service.\nThe client, represented by the SpeziLLM framework, initiates service discovery by advertising fog nodes (Figure 4). This resource announcement and"}, {"title": "4.3 Edge or local layer", "content": "SpeziLLM's local execution builds on the open-source llama.cpp library, which now supports all major open-weight models. The library leverages Apple's Silicon hardware acceleration and software frameworks like Accelerate and Metal. It supports vectorization, quantization, hybrid CPU/GPU inference, and the offloading of specific inference tasks to optimized chip components. An open-source XCFramework version of llama.cpp\u00b9\u2070 was forked, enabling binary distribution for all Apple platforms, proper semantic versioning, and dependency management via the Swift Package Manager (SPM).\nSpeziLLM manages resource tasks for local execution, freeing developers to focus on application logic. The LLM LocalPlatform enforces one sequential execution job at a time, ensuring proper local inference. The framework also includes utility components, like an LLM download and persistence manager for efficient model file retrieval and setup.\nSpeziLLM's local execution layer supports the open-weight language models listed in Table 1 with minimal configuration. Developers must provide a LocalSchema configuration and an LLM model file in the llama.cpp GPT-Generated Unified Format (GGUF) format. \u00b9\u00b9 SpeziLLM's local execution component additionally supports the integration of any GGUF format model and configuration of varying prompt structures."}, {"title": "4.4 Methods", "content": "We present six diverse case studies showcasing the application of SpeziLLM in the development of various mobile platforms, each tailored toward distinct digital health objectives (Table 2). Each selected application used the Llama 2 model (7B variant) [42] in the local and fog layers. The local layer used an iPhone 15 Pro with an A17 Pro System on a Chip (SoC) and 8GB RAM, and the fog node used a MacBook Pro 16\" with an M1 Pro chip, ten cores, and 16GB RAM. The fog node ran in a Docker container, which simplified deployment but added performance overhead as a result of limited hardware acceleration. The cloud layer used OpenAI's GPT-4 (gpt-4-0125-preview).\nWe also evaluated SpeziLLM's utility in mobile application development for those without prior experience with SpeziLLM or the Stanford Spezi ecosystem. To that end, CS students enrolled in CS342\u00b9\u00b2 at Stanford University were voluntarily sampled. CS342 is a ten-week, team-based course in which students learn to design and build secure digital solutions to unmet health needs. An anonymous survey was distributed via Google Forms to 15 students whose final applications were developed using SpeziLLM. The survey consisted of eight five-point Likert-scale questions (detailed in Table 3), and was designed to assess the usability and functionality of SpeziLLM. Data collection spanned five days. Participants were notified via direct messages (with one \"reminder\") to maximize response rates. Responses were analyzed using descriptive statistics.\nThe primary objective of this study was to assess the usability, functionality, and adaptability of SpeziLLM among novices in Swift and mobile app development. Limitations include a small sample size, poor generalizability (the sample is composed of Computer Science students in a particular course at a particular institution), and varying levels of respondent engagement with SpeziLLM, the broader Stanford Spezi ecosystem, and the Swift language."}, {"title": "4.5 Results", "content": "The case studies shown in Table 2 showcase SpeziLLM's versatility across various digital health use cases. LLMonFHIR (1), an open-source iOS application [15] that facilitates interactive dialogue between users and their FHIR records [16], benefited greatly from the integration of SpeziLLM's LLM capabilities and prebuilt UI elements, particularly the API token capture and chat components. Application of the SpeziLLM framework enabled the adoption of OpenAI's function calling declarative DSL (see Figure 3), reducing code complexity and improving performance via parallel processing of function calls.\nThis implementation exhibits SpeziLLM's uniform interface for LLM interactions, which uses function calling to retrieve FHIR resources and enable resource summarization-a functionality that has been extracted into the SpeziFHIR [43] Swift package (also used by OwnYourData (2))."}, {"title": "5 Discussion", "content": "LLMs have the capacity to transform vast, unstructured data from EHRS, wearable devices, and digital health applications into actionable insights, improving individual health and health care delivery systems alike [4, 5, 16,47]. Where their use involves the transmission of patient data to remote clouds, however, valid privacy, trust, and cost concerns arise (section 1). To that end, SpeziLLM is designed to move LLM computations closer to the user using a decentralized, layered fog computing model. SpeziLLM's uniformity facilitates the division of extensive LLM tasks into smaller segments, which are assessed for complexity and assigned to execution layers accordingly. The output generated by a more trusted layer serves as input for more capable, less trusted layers, pre-processing sensitive health data prior to its exposure to opaque cloud LLM providers.\nThroughout the development of our case studies, we observed an inverse correlation between context size and inference speed. The memory swapping associated with larger contexts served to degrade performance, even using advanced devices like the iPhone 15 Pro with 8GB RAM [?] (subsection 4.5). Similar reductions occur in fog nodes within Docker containers. The high cost of inference-capable hardware may pose a challenge to the widespread adoption of LLM capabilities.\nWe found that smaller, locally-executed LLMs handle straightforward tasks"}, {"title": "6 Conclusion", "content": "This work demonstrates the potential of our open-source, dynamic, and uniform inference framework, SpeziLLM, to enhance the execution of LLM tasks across edge, fog, and cloud environments. By shifting LLM execution closer to the user's device within a decentralized fog computing architecture, SpeziLLM addresses critical privacy, trust, and financial concerns related to cloud-based solutions, all while maintaining robust LLM functionalities near the data source. \nSpeziLLM's architecture encompasses three tiers: local/edge, fog, and cloud. The framework enables efficient local inference on Apple's mobile platforms, establishes a decentralized fog layer with discoverable LLM nodes, and seamlessly integrates with the OpenAI API through a declarative DSL for function calling. SpeziLLM's instantiation across six mobile applications highlights its versatility and utility in an array of digital health contexts. \nSpeziLLM's primary contribution is its uniform, LLM-agnostic interface, which facilitates transparent transitions between inference environments. This approach offers a viable solution to the challenges of LLM execution in sensitive and resource-constrained environments [21-23], facilitating the segmentation of extensive LLM tasks into smaller components and the distribution of such segments across complexity- and trust-matched layers. Lightweight models, such as Llama 2, can efficiently handle simpler tasks within the local and fog layers, while more complex tasks can be processed in the cloud. Future developments in efficient LLM execution with limited memory will further refine SpeziLLM's performance and applicability."}]}