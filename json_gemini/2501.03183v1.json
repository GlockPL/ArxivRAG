{"title": "Classifier-Guided Captioning Across Modalities", "authors": ["Ariel Shaulov", "Tal Shaharabany", "Eitan Shaar", "Gal Chechik", "Lior Wolf"], "abstract": "Most current captioning systems use language models trained on data from specific settings, such as image-based captioning via Amazon Mechanical Turk, limiting their ability to generalize to other modality distributions and contexts. This limitation hinders performance in tasks like audio or video captioning, where different semantic cues are needed. Addressing this challenge is crucial for creating more adaptable and versatile captioning frameworks applicable across diverse real-world contexts. In this work, we introduce a method to adapt captioning networks to the semantics of alternative settings, such as capturing audibility in audio captioning, where it is crucial to describe sounds and their sources. Our framework consists of two main components: (i) a frozen captioning system incorporating a language model (LM), and (ii) a text classifier that guides the captioning system. The classifier is trained on a dataset automatically generated by GPT-4, using tailored prompts specifically designed to enhance key aspects of the generated captions. Importantly, the framework operates solely during inference, eliminating the need for further training of the underlying captioning model. We evaluated the framework on various models and modalities, with a focus on audio captioning, and report promising results. Notably, when combined with an existing zero-shot audio captioning system, our framework improves its quality and sets state-of-the-art performance in zero-shot audio captioning.", "sections": [{"title": "I. INTRODUCTION", "content": "Captioning is the task of generating descriptive text for modalities like images or audio. It has numerous applications from information retrieval to accessibility, but is fundamentally challenging because it may require deep semantic understanding of the content. Current baseline models rely on language models (LMs) as the core component for caption generation. However, the use of LMs presents challenges for certain modalities due to shifts in data distribution, as LMs are trained primarily on textual data and thus struggle to adapt effectively to modality-specific captioning tasks.\nIn audio captioning, it is crucial to generate captions that capture nuanced audio-specific semantics, such as distinguishing between audible actions, like speaking or clapping, and non-audible ones, like walking. Additionally, models often struggle to adapt to diverse real-world contexts, where varying environmental and cultural factors influence audio interpretation and captioning requirements, making accurate and contextually relevant captions more challenging to generate.\nTo differentiate between audible and non-audible actions, we trained a text classifier on a dataset automatically generated by directing GPT-4. This classifier is employed during inference to guide the LM, allowing it to select more audible words in the caption generation process.\nWe validate the effectiveness of our framework through a series of experiments, particularly in the domain of audio captioning. Our method is evaluated on the AudioCaps [9] and Clotho [12] datasets, where we demonstrate that incorporating audibility guidance significantly enhances performance across all metrics compared to baseline models. Additionally, we conducted an ablation study to show the applicability of our approach to other modalities, such as image captioning.\nOur contributions can be summarized as follows: (1) We propose a novel approach to adapt captioning networks to the semantics of alternative settings by incorporating classifier guidance, (2) We introduce a carefully curated dataset, generated by GPT-4, containing both audible and non-audible caption samples, and (3) We demonstrate that our framework achieves state-of-the-art results in zero-shot audio captioning and significantly enhances performance in image captioning across both zero-shot and fully supervised settings."}, {"title": "II. RELATED WORK", "content": "Captioning models, which generate descriptive text for input modalities, have been widely studied in artificial intelligence. Traditional methods focused on image captioning, using convolutional neural networks (CNNs) for feature extraction and recurrent neural networks (RNNs) or transformers for sequence generation [1]\u2013[3], with improvements from attention mechanisms [4]\u2013[6].\nAudio captioning, which generates descriptions for audio inputs, initially adapted techniques from image captioning, using spectrogram-based CNNs combined with RNNs or transformers [7], [8]. Recent advancements introduced attention mechanisms tailored to audio data, improving contextual relevance [9], [10].\nZero-shot methods like NoAudioCaptioning [24], WSAC [25], and Zhang et al. [26] propose using text-only data for training, bypassing the need for audio inputs and leveraging pre-trained language models to simulate training. While these methods achieve impressive results, they lack modality-specific guidance, which is crucial for accurately capturing nuanced audio features. Specifically, these approaches struggle with capturing audibility-the ability to describe only the elements directly inferred from the audio-due to their reliance on text-based training that overlooks the unique characteristics and dynamics of audio data. In contrast, our inference-time"}, {"title": "III. METHOD", "content": "The task of captioning can be mathematically formulated as a sequence generation problem, where we aim to infer the conditional probability of the i-th word, denoted as $x_i$, in a sentence. Specifically, the objective is to optimize the probability distribution $P(x_i|[X_t]_{t<i}, A)$, where $x_t$ represents the preceding words in the sentence, and $A$ denotes the input, in our case, an audio clip.\nIn this work, we focus on zero-shot audio-captioning by introducing a novel inference-time optimization method designed to generate high-fidelity, audibly-relevant captions. This approach leverages audibility classifier-based guidance to direct a language model (LM) towards producing more audibly interpretable outputs.\nNotably, our method is modality-agnostic, allowing seamless integration across various modalities such as audio and images, thus offering broad applicability and flexibility.\nThe proposed framework consists of two main components: (i) a pre-trained audio captioner, which incorporates a language model (LM), and (ii) a binary audibility classifier that provides guidance during the caption generation process.\nThe sentence generation process is mathematically expressed as:\n$x_{i+1} = LM (x_i, [(K_j), V_j)]_{j<i,1<l<L})$,\nwhere $x_i$ denotes the i-th word in the generated sentence, and $K$ and $V$ represent the context transformer's key and value for the j-th token across L layers.\nTo enhance the captioner's ability to generate sentences that reflect the audibility of the audio inputs, we introduce a calibrated audibility loss, $L_{classifier}$, which encourages the"}, {"title": "A. Audibility Classifier", "content": "To address the challenge of insufficient audibility in sentences generated by audio captioning models, we generated two distinct sets of sentences using ChatGPT: one set representing audible captions and the other representing non-audible captions. This newly generated dataset serves as the foundation for training a classifier ($h_a$), specifically designed to distinguish between audible and non-audible captions.\nThe trained classifier $h_a$ offers guidance to the Language Model (LM) in a manner that aligns with our audibility objectives. This guidance is attained through optimizing the following term over the context cache $C_i$:\n$L_{classifier} = -log (h_a (LM(x_i; C_i)[1])\nWhere [1] indexes the classifier's output for the pseudo-probability for the audibility label (the positive label). The audibility classifier evaluates the sentence in its entirety, and accordingly, the guidance pertains to the sentence as a whole."}, {"title": "B. Loss Function", "content": "Our methodology employs a pre-trained audio captioner that contains a LM (GPT-2 in our case) to deduce the subsequent word in a sentence.\nTo ensure that the captioner generates sentences that are influenced by the classifier, we incorporate an additional term, $L_{classifier}$, from Eq. 1 into the main loss function.\nFurthermore, an additional regularization term denoted as $L_{CE}$ (Eq. 2) is added (as is often done) to ensure that the distribution of the next token remains consistent with that of the original language model.\n$L_{CE} = CE(LM(x_i; C_i), LM(x_i; C_i))$\nwhere i is the index of the currently generated token, CE is the cross entropy loss, and $C_o$ is a context cache of the relevant keys, queries, and values as they are computed based on the embedding and $K Q$ and $V$ projections of the LM model (without the inference time optimization over $C_i$).\nTo conclude, the loss function of the optimization process can be represented as:\n$L = \\lambda_0 L_{CE} + \\lambda_1 \\cdot L_{classifier}$\nas default parameters, we set $\\lambda_0$ to be 0.2, and $\\lambda_1$ to be 0.6.\nThis optimization process is executed iteratively during auto-regression, with each token being addressed in sequence. At every generation step, we optimize the current context cache $C_i$ using gradient descent, generate the next token, and continue to the next iteration. Importantly, this process does not involve any updates to the model weights, which remain fixed throughout."}, {"title": "IV. EXPERIMENTS", "content": "The evaluation of audio captioning models heavily relies on high-quality datasets that provide paired audio and text data. Two widely recognized datasets in this domain are AudioCaps [9] and Clotho [12].\nAudioCaps [9], derived from the AudioSet ontology, is a large-scale dataset specifically curated for audio captioning tasks. It comprises over 46k audio clips and 49k human-generated captions, each describing the acoustic events present in the corresponding audio.\nIn contrast, Clotho [12] offers a more diverse and challenging set of audio-caption pairs. The Clotho dataset contains nearly 5k audio files and 19k captions, with each audio file ranging from 15 to 30 seconds in length.\nWe train our audibility classifier using our self-collected dataset, which contains two categories: audible, and non-audible. This data set comprises 10k captions generated by ChatGPT, with an equal distribution of 5k captions labeled as audible and 5k labeled as non-audible, see Sec. III-A\nTo validate the effectiveness of our approach, we integrate our novel guided decoding technique with the zero-shot audio captioning model, NoAudioCaptioning [24].\nOur pipeline is implemented on a single Titan X GPU, utilizing a single beam search for caption generation. The system evaluates 512 candidate tokens within approximately 2 seconds per token, with a target sequence length set to 30 tokens.\nThe classifier $h_a$ in equation 1 is based on the Distil-BERT architecture [11]. The optimization process employs the AdamW algorithm, configured with a batch size of 64 and a learning rate of 0.0003, over 40 epochs. A learning rate scheduler is applied to reduce the learning rate by a factor of 10 every 10 epochs.\nTo ensure a fair zero-shot evaluation, we conduct our experiments within an out-"}, {"title": "V. LIMITATIONS", "content": "Despite the improvements of our framework, several limitations persist. First, relying on pre-trained models like GPT-2 may introduce biases that affect performance and generalization. Additionally, inference-time optimization adds computational overhead, making it less suitable for real-time or resource-constrained environments."}, {"title": "VI. CONCLUSION", "content": "We propose a modality-agnostic framework that adapts captioning networks to the semantics of different distributions or contexts through classifier-driven guidance, focusing on audibility in zero-shot audio captioning. By combining pre-trained language models with an audibility classifier, our method significantly outperforms baseline models in zero-shot settings.\nFuture work will explore applying this framework to additional modalities, such as video, and investigate methods to reduce optimization overhead."}, {"title": "VII. ACKNOWLEDGMENTS", "content": "This work was supported by a grant from the Tel Aviv University Center for AI and Data Science (TAD), and by the Israeli Ministry of Science, Israel-Singapore binational grant 207606."}]}