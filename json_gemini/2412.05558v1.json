{"title": "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition", "authors": ["Feng Li", "Jiusong Luo", "Wanjun Xia"], "abstract": "Speech emotion recognition (SER) remains a challenging yet crucial task due to the inherent complexity and diversity of human emotions. To address this problem, researchers attempt to fuse information from other modalities via multimodal learning. However, existing multimodal fusion techniques often overlook the intricacies of cross-modal interactions, resulting in suboptimal feature representations. In this paper, we propose WavFusion, a multimodal speech emotion recognition framework that addresses critical research problems in effective multimodal fusion, heterogeneity among modalities, and discriminative representation learning. By leveraging a gated cross-modal attention mechanism and multimodal homogeneous feature discrepancy learning, WavFusion demonstrates improved performance over existing state-of-the-art methods on benchmark datasets. Our work highlights the importance of capturing nuanced cross-modal interactions and learning discriminative representations for accurate multimodal SER. Experimental results on two benchmark datasets (IEMOCAP and MELD) demonstrate that WavFusion succeeds over the state-of-the-art strategies on emotion recognition.", "sections": [{"title": "1 Introduction", "content": "Recently, speech emotion recognition (SER) is a fascinating field that utilizes technology to analyze and identify different emotions present in human speech [1]. This technology has various applications, including in customer service and market research [2], learning and education [3], mental health [4], and social media analytics [5]. In real-life scenarios, humans express emotions not only through speech but also through alternative modalities, such as text and visuals [6,7]. Previous studies on SER typically rely on speech information. However, different modalities provide complementary information for emotion recognition, and emotion recognition of the single modality is not inadequate to meet real-world demands. To address this problem, researchers utilize multimodal information to identify emotional states [8]. In the domain of Multimodal Emotion Recognition (MER), the information of diverse modalities is complementary, providing additional cues to mitigate semantic and emotional ambiguities.\nIn addition to multimodality, another challenge of SER is achieving better interaction during the fusion of different modalities. Firstly, multimodal data often exhibit asynchrony [9]. For instance, visual signals typically precede audio signals by approximately 120 ms in emotional expressions [10]. This asynchronicity poses a challenge to feature fusion and model design, necessitating methods to address temporal alignment and matching issues. To address this issue, Tsai et al. [11] have proposed specific asynchronous models and cross-modal attention mechanisms. Zheng et al. [12] solved heterogeneity among different encoder output features by employing unsupervised training of a multi-channel weight-sharing autoencoder. This approach minimizes the differences among features extracted from different modalities. Additionally, the interactions are simulated by supervised training of cascaded multi-head attention mechanisms. However, most methods with cross-modal attention mechanisms ignore redundant information during the fusion process, thus restricting the performance of MER. Additionally, samples with the same emotion in multimodal data may exhibit differences across modalities, referred to as homogeneous feature differences. For instance, some features in speech and text may exhibit formal similarity but convey different emotional states [13]. Hazarika et al. [14] projected each modality into two different subspaces capturing modality-invariant and modality-specific features. However, they only considered the differences between the different emotion of same modalities and ignored the differences between different modalities with the same emotion. DialogueTRM explores intra- and inter-modal emotional behaviors in conversations, using Transformers to model the context [15]. MMGCN proposes a multimodal fusion approach via a deep graph convolution network, modeling the interactions between different modalities using a graph [16]. MM-DFN introduces a dynamic fusion network that leverages intra- and inter-modal information at different levels of representation [17]. M2FNet proposes a multimodal fusion network that learns and fuses complementary information from audio, visual, and textual modalities [18].\nTherefore, in this paper, we propose a novel arhitecture called WavFusion for emotion recognition. Unlike DialogueTRM, WavFusion specifically focuses on incorporating wav2vec2.0 [19] with a gated cross-modal attention mechanism to dynamically fuse multimodal features. Additionally, WavFusion introduces multimodal homogeneous feature discrepancy learning to distinguish between same-emotion but different-modality representations. WavFusion does not rely on graph-based modeling but instead uses a transformer architecture with a modified cross-modal attention mechanism. WavFusion also emphasizes capturing both global and local visual information through the A-GRU-LVC module. While MM-DFN focuses on dynamic fusion strategies, WavFusion emphasizes the use of wav2vec2.0 pre-trained representations and a gated cross-modal attention mechanism to mitigate redundant information during fusion. Additionally, WavFusion incorporates multimodal homogeneous feature discrepancy"}, {"title": "2 Proposed Method", "content": "2.1 Problem Statement\nGiven a multimodal signal $S_j = \\{S_j^a, S_j^t, S_j^v\\}$, we can represent the unimodal raw sequence extracted from the video fragment $j$ as $S_j^m, m \\in \\{a, t, v\\}$. Here, the modalities are denoted by $\\{a, t, v\\}$, which refer to audio, text, and visual modalities.\nIn WavFusion, we aim to predict the emotion category for each utterance. It focuses on categorizing the emotion conveyed in each utterance, assigning it to a specific emotion class or category, $y_j \\in R^c$. c is the number of emotion categories. Figure 1 illustrates the overall structure of WavFusion, including an auxiliary modal encoder, a primary modal encoder, and multimodal homogeneous feature discrepancy learning. The orange color represents positive emotions and the green color represents negative emotions.\n2.2 Auxiliary Modality Encoder\nVideo Representation For the visual modality, we use the EfficientNet pre-trained model as a feature extractor to obtain visual features $e_v$. This model is a self-supervised framework for visual representation learning. In this paper, we attempt to extend EfficientNet to emotion recognition. $e_v$ can be formulated as:\n$e_v = \\phi_{visual}(S_j^v)$ (1)\nwhere $\\phi_{visual}$ denotes the function of EfficientNet model.\nOn the other hand, we consider the context and situation conveyed by the global information in the visual modality, along with the specific details of actions and expressions from local information. The visual feature is fed into the proposed A-GRU-LVC module, which aims to extract both global and local features.\n$X_v^1 = F_{SA}(F_{GRU}(e_v))$ (2)\nwhere $F_{SA}$ and $F_{GRU}$ denote the learning functions of GRU and self-attentive mechanism, respectively.\nSimultaneously, to preserve local corner point regions and extract local information, a learnable visual center (LVC) is implemented on the visual features [20]. This LVC aggregates features from local areas, ensuring that important local information is retained. In contrast to the approach, we utilize one-dimensional convolution instead of two-dimensional convolution.\n$X_v^2 = F_{LVC}(e_v)$ (3)\nwhere $F_{LVC}$ denotes the learning functions of the LVC block.\nFinally, the output of the A-GRU-LVC block is obtained by connecting the output of the self-attention module $X_v^1$ and the output of the LVC block $X_v^2$ along the last dimension.\n$X_v^v = X_v^1 \\bigoplus X_v^2$ (4)\nContextualized Word Representation To capture rich contextual information from textual data, we utilize the RoBERTa-base model, which belongs to the transformer family, as a contextual encoder. The architecture of RoBERTa consists of multiple Transformer layers, including a stack of encoders. Each encoder layer contains a multi-head self-attention mechanism and a feed-forward neural network. ROBERTa is designed to capture contextualized representations of words in a sentence, allowing it to understand the meaning and relationships between different words. $e_t$ can be formulated as:\n$e_t = \\phi_{text}(S_j^t)$ (5)\nwhere $\\phi_{text}$ denotes the function of the RoBERTa pre-train model.\nTo further consider context-sensitive dependence for text features, we feed it into the GRU and the self-attention mechanism to obtain global features of the text information. Due to the strong temporal continuity present in textual information, we opted not to employ the LVC mechanism to capture local feature.\n$X_t^t = F_{SA}(F_{GRU}(e_t))$ (6)\nwhere $F_{SA}$ and $F_{GRU}$ denote the learming functions of GRU and self-attentive mechanism, respectively.\nMajor Modality Encoder In WavFusion, we encode low-level audio features through the shallow transformer layer, followed by combining text and visual features through the deep transformer layer to form a comprehensive multimodal representation. We define the original transformer layer as a shallow transformer layer and the modified transformer layer as a deep transformer. The incorporation of text and vision into wav2vec 2.0 detects relevant information within the extensive pre-trained audio knowledge, thereby enhancing emotional information within the multimodal fusion representation. The low-level acoustic features $X_a$ extracted by the shallow transformer block are calculated as follows:\n$X_a^a = F_{ST}(S_j^a)$ (7)\nwhere $F_{ST}$ is the learning function of shallow transformer layers.\n$X^{F1} = CMA-T(X_a^a, X_t^t)$ (8)\n$X^{F2} = CMA-V(X_a^a, X_v^v)$ (9)\nFinally, the augmented features $X^{F1}$ and $X^{F2}$ are processed through the following gated filtering mechanism. The ratio of each channel can be dynamically defined by a learnable parameter that filters out misinformation generated during cross-modal interactions.\nP^* = sigmoid(FC(X^{F1} + X^{F1})) (10)\nX^* = P^* \\bigodot X_a^a + (1 \u2212 P^\u2217) \\bigodot X^{F2} (11)\nMultimodal Homogeneous Feature Discrepancy Learning Multimodal homogeneous feature discrepancy learning has made significant progress in multimodal emotion recognition. It can optimize the modal representation ability and extract richer and more accurate emotional information by learning the relationships and differences between homogeneous features. First, we feed unfused audio features $X_a^a$, text features $X_t^t$ and visual features $X_v^v$ into a shared"}, {"title": "3 Evaluation", "content": "3.1 Dataset\nWe evaluate our proposed method on two prevalent benchmark datasets for ERC, including IEMOCAP [21] and MELD [22], respectively. The IEMOCAP dataset consists of 12 hours of improvised and scripted audio-visual data from 10 UC"}, {"title": "3.3 Comparative Analysis", "content": "In Tables 1 and 2, we show the performance of different approaches on the IEMOCAP and MELD datasets. The evaluation metrics are Accuracy (ACC) and Weighted F1 score (WF1). On the IEMOCAP dataset, our method outperformed the state-of-the-art by 0.84% in ACC and 0.74% in WF1. Similarly, on the MELD dataset, our method surpassed the state-of-the-art by 0.43% in ACC and 0.44% in WF1. The reasons are probably twofold. Firstly, we argue that this is because most of these models do not explicitly consider redundant information in the cross-modal fusion process, but our proposed method considers these through a gated cross-modal attention mechanism. Secondly, most of them only take into account the distances of different emotion samples of the same modality, but not the distances of the same emotion samples of different modalities."}, {"title": "3.4 Ablation Studies", "content": "To verify the effectiveness of WavFusion model, we conduct ablation studies on the IEMOCAP dataset. First, we reveal the importance of each modality in this section. Specifically, when utilizing a single modality, we omitted the gated cross-modal attention and multimodal homogeneous feature discrepancy learning. The results in Table 3 illustrate that the highest accuracy and weighted average F1 scores are attained when incorporating all three modalities. Due to the complexity of emotion recognition, recognizing emotions using a single modality is challenging to meet the demands of reality. We can achieve better recognition performance by integrating multimodal information.\nAdditionally, we introduce LVC blocks to capture local information related to visual features. To assess the significance of LVC blocks, we conducted an experiment where we omitted the LVC blocks from the model, thus failing to capture local information about visual features. From Table 4, we observe that the model with the LVC block outperforms the model without the LVC block. The inclusion of LVC blocks improves ACC by 0.63% and the WF1 by 0.76%. The experiment demonstrates that the LVC blocks are beneficial for capturing relevant contextual details and spatial dependencies.\nAlso, define: $L_{mar} = \\frac{1}{\\mid M \\mid}\\sum_{(i,j,k)\\in M} max(0, \\alpha \u2013 cos(X^{m[i],c[i]}, X^{m[j],c[j]})) + cos(X^{m[i],c[i]}, X^{m[k],c[k]}))$.\n$L^{emotion}_{total} = \\frac{1}{N_D} \\sum_{j=1}^{N_D} y_j . log \\hat{y_j}$.\n$L^{emotion}_{task} = L^{emotion}_{total} + X L_{mar}$"}, {"title": "4 Conclusion", "content": "In this paper, we propose a novel SER approach, which is designed a gated cross-modal attention alternative to self-attention in the wav2vec 2.0 pre-trained model to dynamically fuse features from different modalities. Additionally, we introduce a novel LVC block to efficiently capture the local information of visual features. The model can more effectively utilize the spatial characteristics of visual data, resulting in more comprehensive representations. Finally, we design the concept of multimodal homogeneous feature discrepancy learning, which helps the model to effectively learn and distinguish representations of the same modalities but different emotions. The effectiveness of the proposed model is demonstrated on the IEMOCAP and MELD datasets. The results show promising performance compared to state-of-the-art methods. In the future, we plan to utilize the leveraging large amounts of unlabeled audio and video data available to recognize the different emotion."}]}