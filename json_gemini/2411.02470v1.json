{"title": "BENCHMARKING \u03a7\u0391\u0399 EXPLANATIONS WITH HUMAN-ALIGNED EVALUATIONS", "authors": ["R\u00e9mi Kazmierczak", "Steve Azzolin", "Elo\u00efse Berthier", "Anna Hedstr\u00f6m", "Patricia Delhomme", "Nicolas Bousquet", "Goran Frehse", "Massimiliano Mancini", "Baptiste Caramiaux", "Andrea Passerini", "Gianni Franchi"], "abstract": "In this paper, we introduce PASTA (Perceptual Assessment System for explana- Tion of Artificial intelligence), a novel framework for a human-centric evaluation of XAI techniques in computer vision. Our first key contribution is a human evaluation of XAI explanations on four diverse datasets-COCO, Pascal Parts, Cats Dogs Cars, and MonumAI-which constitutes the first large-scale benchmark dataset for XAI, with annotations at both the image and concept levels. This dataset allows for robust evaluation and comparison across various XAI methods. Our second major contribution is a data-based metric for assessing the in- terpretability of explanations. It mimics human preferences, based on a database of human evaluations of explanations in the PASTA-dataset. With its dataset and metric, the PASTA framework provides consistent and reliable comparisons be- tween XAI techniques, in a way that is scalable but still aligned with human eval- uations. Additionally, our benchmark allows for comparisons between explana- tions across different modalities, an aspect previously unaddressed. Our findings indicate that humans tend to prefer saliency maps over other explanation types. Moreover, we provide evidence that human assessments show a low correlation with existing XAI metrics that are numerically simulated by probing the model.", "sections": [{"title": "1 INTRODUCTION", "content": "As Deep Neural Networks (DNNs) systems are being used in increasingly high stakes domains (e.g., legal, medical) (Surden, 2021; Litjens et al., 2017), it is essential that humans are able to interpret how they reach their conclusions. (Bender et al., 2021). Their lack of transparency has led them to be characterized as \u201cblack boxes\u201d (Castelvecchi, 2016), which is particularly problematic in critical applications where understanding the decision-making process is essential for trust and accountabil- ity (Vereschak et al., 2024), leading to the creation of a relatively new field: explainable AI (XAI) (Gunning et al., 2019). XAI aims to make the workings of deep learning models more transparent and interpretable. XAI methods fall into two main categories: post-hoc techniques (Selvaraju et al., 2017; Ribeiro et al., 2016; Lundberg & Lee, 2017) and ante-hoc techniques (Bennetot et al., 2022; Koh et al., 2020). Post-hoc techniques generally explain the output of a frozen, pretrained DNN, while ante-hoc techniques modify the architecture of the DNN to improve its interpretability from the outset. Each of these categories can be further subdivided into various sub-families, offering a wide array of XAI approaches.\n\nThe diversity of XAI techniques calls for an effort to standardize their evaluation and compari- son. Although there are toolkits in computer vision that offer a range of computational evaluation techniques (Hedstr\u00f6m et al., 2023; Fel et al., 2022a), to our knowledge there has been no effort to standardize their evaluation from a perceptual point of view (Nauta et al., 2023), i.e., the way the explanation is perceived by the human for whom it was intended. Currently, prevalent approaches (Dawoud et al., 2023; Colin et al., 2022) to evaluating XAI techniques involve human annotators"}, {"title": "2 RELATED WORK", "content": "Automated scoring. Automated scoring involves developing models that assign scores to inputs based on a reference dataset, often derived from human ratings. A particularly active area of re- search in this domain is automated essay scoring. Traditionally, this has been addressed through handcrafted feature extraction (Yannakoudakis et al., 2011), but modern methods tend to be closer to model as a judge (Lee et al., 2024; Taghipour & Ng, 2016; Chiang et al., 2024). More recently, there has been a growing interest in using embeddings from large language models (LLMs) as features for scoring. The first successful attempt in this direction was made by Yang et al. (2020). Building on this trend, other approaches have incorporated LLM embeddings with models like LSTMs (Wang et al., 2022b), integrated text generation into the training loop (Xiao et al., 2024), or introduced multi-scale aspects to enhance performance (Li et al., 2023a).\n\nExplainable AI. Explainability refers to computational models designed to provide specific de- tails or reasons to ensure clarity and ease of understanding regarding their functioning (Arrieta et al.,"}, {"title": "3 CREATING A HUMAN PREFERENCE DATASET ON \u03a7\u0391\u0399 INTERPRETABILITY", "content": "To evaluate the quality of XAI explanations from a human-centric point of view, we proceed along the following steps, which are detailed in the subsections below. First, we constitute a dataset from annotated images. Using our codebase of classifiers and XAI techniques, we compute label predictions along with XAI explanations. The explanations are then evaluated by human annotators following a rigorous evaluation protocol. Finally, we compare the different XAI techniques with the human evaluations to assess quality of their explanations. We also investigate how human scores correlate with popular automated XAI metrics to see whether they are complementary."}, {"title": "3.1 DATASET COMPOSITION", "content": "To evaluate the performance of different perceptual metrics, we collect a large-scale dataset com- posed of images of four highly heterogeneous datasets: COCO (Lin et al., 2014), Pascal Part (Chen et al., 2014), Cats Dogs Cars (Kazmierczak et al., 2024), and Monumai (Lamas et al., 2021). This collection is referred to as the classifier's dataset A.1.1. It is important to distinguish this from the PASTA-dataset, which comprises the final set of images, annotations, labels, and explanations ob- tained through our evaluation process. Each dataset includes two levels of annotations: image-based annotations and concept-based annotations. This dual-level annotation framework allows for the application of Concept-Based and other XAI methods, enabling a robust evaluation across differ- ent approaches. For this, we use for the computation of explanations a subset of 25 images of the classifier's dataset test split per dataset, resulting in a comprehensive evaluation of 100 images, that serve as the basis of the PASTA-dataset. This diverse selection ensures a broader generalization of the XAI techniques across datasets being assessed. Note that, unlike traditional datasets, our benchmark dataset comprises a triplet of images, explanations, and labels. This triplet enables us to quantitatively assess the quality of XAI techniques"}, {"title": "3.2 \u03a7\u0391\u0399 METHODS", "content": "To ensure representativity, we consider two distinct types of explanations. The first type comprises saliency methods, which generate explanations by assigning an importance score to each pixel of the input image, indicating the significance of each pixel in the prediction process. The second type consists of concept-based explanations, which highlight the importance of human-understandable concepts in the explanation. A detailed list of the methods and more details on each technique are given in Appendix A.2."}, {"title": "3.3 TRAINING CLASSIFIERS AND COMPUTING THE XAI DATASET", "content": "The initial phase in constructing the sample set involves training the various classifier models on which explanations will be generated. Specifically, we utilize ResNet50 (He et al., 2016), ViT-B (Dosovitskiy, 2020), ResNet50-BCos (B\u00f6hle et al., 2024), CLIP-Linear (Yan et al., 2023), CLIP- QDA (Kazmierczak et al., 2024), X-NeSyL (D\u00edaz-Rodr\u00edguez et al., 2022), and ConceptBottleneck (Koh et al., 2020). These models are trained separately on the four datasets. Further details regarding the datasets and training procedures are provided in Section A.1. The final assessment of XAI techniques is conducted on samples of the test set. To provide a diverse range of explanations, our framework incorporates both black-box models, explained using post-hoc methods, and ante- hoc methods, specifically CBMs. The specific details regarding these two families of methods are presented in Appendix A.1.2.\n\nOur codebase includes the 21 XAI methods described in Sect. 3.2 and the 7 backbone models de- scribed above. Some XAI methods are incompatible with certain backbones, see Table 6, this leaves 46 distinct combinations of XAI methods and backbones, which we refer to as XAI techniques. We apply each technique to 100 images. This leads to an XAI dataset of 4600 instances, each of which is associated with an image and its ground truth label, the label prediction from a classifier instance, and the explanation from a particular XAI technique. In the next section, we present an approach for evaluating the human perception of these instances."}, {"title": "3.4 HUMAN EVALUATION PROTOCOL", "content": "We aim to quantify the interpretability and usefulness of XAI techniques accurately, using a human evaluation of the quality of evaluations. The resulting dataset serves as a benchmark, enabling us to compare and validate current and future XAI methods. Our human-centric approach complements existing approaches that focus primarily on assessing the model's internal behavior. For exam-"}, {"title": "3.5 HUMAN EVALUATION AND RESULTS", "content": "We now present a brief summary of the human evaluations obtained in the PASTA-dataset, using the previously described protocol. Full results and values are available in Appendices B.2 and B.3. Our XAI dataset, described in Sect. 3.3, contains 4600 instances with images, predictions, and ex- planations. From this set, we select 2200 samples randomly for the saliency maps as detailed in Appendix A.1 and let them be evaluated by humans according to the protocol above. Each instance receives five evaluations from different annotators. We aggregate these evaluations using majority voting to favor consensus opinions.\n\nAs illustrated by Figure 3a, we observe that these results indicate a preference for image-based tech- niques, suggesting that saliency maps are perceived as more interpretable than CBMs. Additionally, Figure 3b shows the average score among techniques that share the same backbone. Interestingly, CLIP and ViT have similar scores, likely due to the architectural similarities between the two mod- els. ResNet 50, which played a pivotal role in the development of many XAI methods, consistently scores higher. This could suggest a potential bias toward ResNet 50 in the design and effectiveness of current XAI methods."}, {"title": "3.6 CORRELATION WITH OTHER METRICS.", "content": "We turn to the question of how the human scores in the PASTA-dataset correlate with standard XAI metrics. An analysis based on the Pearson Correlation Coefficient and the Spearman rank Correlation Coefficient for different perturbation strategies, shown in Table 1 indicates a rather weak correlation between human scores and ROAD (Rong et al., 2022), a popular metric to evaluate faithfulness. We conclude that our human scores indeed cover an aspect of explanation quality unrelated to that of perceptual quality, as predicted by Biessmann & Refiano (2021). Additional results, including results for other axioms, are available in Appendix B.1 and B.2."}, {"title": "4 DEVELOPING A METRIC FOR PERCEPTUAL EVALUATION", "content": "Our human preference dataset contains evaluations of the fidelity, complexity, objectivity, and ro- bustness of each evaluation. These scores were painstakingly attributed by human annotators. To"}, {"title": "4.1 COMPUTATION OF EMBEDDINGS", "content": "Drawing inspiration from recent literature in automated scoring (Yang et al., 2020; Wang et al., 2022b), we use a foundation model to generate embeddings. Given its multimodal capabilities, we select CLIP (Yan et al., 2023) as the embedding model. This choice allows for a unified integration of both concept-based explanations, which can be transformed into text, and saliency map-based explanations, which can be projected into the same embedding space.\n\nLet us denote by $x_i \\in \\mathbb{R}^{H\\times W\\times 3}$ the i-th test image of height H and width W in the dataset and by $e_{\\text{saliency}} \\in \\mathbb{R}^{H\\times W}$ any explanation produced by a saliency-based XAI method for this image. We also note the CLIP image encoder as $CLIP_{\\text{image}}$. For saliency explanations, the resulting embedding based on a saliency map can be obtained using the following formula:\n\n$\\varphi_{\\text{image}} (e_{\\text{saliency}}) = CLIP_{\\text{image}} (\\text{Heatmap}(x_i, e_{\\text{saliency}})),$"}, {"content": "Drawing inspiration from recent literature in automated scoring (Yang et al., 2020; Wang et al., 2022b), we use a foundation model to generate embeddings. Given its multimodal capabilities, we select CLIP (Yan et al., 2023) as the embedding model. This choice allows for a unified integration of both concept-based explanations, which can be transformed into text, and saliency map-based explanations, which can be projected into the same embedding space.\n\nLet us denote by $x_i \\in \\mathbb{R}^{H\\times W\\times 3}$ the i-th test image of height H and width W in the dataset and by $e_{\\text{saliency}} \\in \\mathbb{R}^{H\\times W}$ any explanation produced by a saliency-based XAI method for this image. We also note the CLIP image encoder as $CLIP_{\\text{image}}$. For saliency explanations, the resulting embedding based on a saliency map can be obtained using the following formula:\n\n$\\varphi_{\\text{image}} (e_{\\text{saliency}}) = CLIP_{\\text{image}} (\\text{Heatmap}(x_i, e_{\\text{saliency}})),$ where Heatmap is the process generating the saliency related heatmap to the image. More details are given in Appendix C.3.1.\n\nFor CBMs, let us denote with $e_{BM} \\in \\mathbb{R}^{K}$ any explanation produced by a saliency-based XAI method for this image $x_i$, where K is the length of the concept set. These attributions are converted into a sentence, which is then embedded. Let $CLIP_{\\text{text}}$ denote the CLIP text encoder and Sentence the process of converting the CBM explanation into text, more details about which Sentence are"}, {"title": "4.2 SCORING NETWORK", "content": "where Heatmap is the process generating the saliency related heatmap to the image. More details are given in Appendix C.3.1.\n\nFor CBMs, let us denote with $e_{BM} \\in \\mathbb{R}^{K}$ any explanation produced by a saliency-based XAI method for this image $x_i$, where K is the length of the concept set. These attributions are converted into a sentence, which is then embedded. Let $CLIP_{\\text{text}}$ denote the CLIP text encoder and Sentence the process of converting the CBM explanation into text, more details about which Sentence are\n\n$text(EBM) = CLIP_{text}(Sentence(e^{CBM})).$ Once the embeddings are computed, a scoring network composed of a linear layer is used to predict scores. Inspired by Automated Essay Scoring (Yang et al., 2020; Wang et al., 2022b), we use a loss $L$ that combines a similarity loss $L_s$, a mean squared error (MSE) loss $L_{mse}$, and a ranking loss $L_r$. From a set of ground truth scores obtained from majority voting $\\{m_k\\}_{k\\in [0,N_s]}$ and the predictions given by the scoring network $\\{\\hat{m}_k\\}_{k\\in [0,N_s]}$, the resulting loss is defined as:\n\n$L = \\alpha L_s + \\beta L_{mse} + \\gamma L_r,$\n\nwhere $\\alpha$, $\\beta$, and $\\gamma$ are hyperparameters controlling the relative importance of each component. Details about the different losses are given in Appendix C.3"}, {"title": "4.3 CLASSIFIER RESULTS", "content": "Once the embeddings are computed, a scoring network composed of a linear layer is used to predict scores. Inspired by Automated Essay Scoring (Yang et al., 2020; Wang et al., 2022b), we use a loss $L$ that combines a similarity loss $L_s$, a mean squared error (MSE) loss $L_{mse}$, and a ranking loss $L_r$. From a set of ground truth scores obtained from majority voting $\\{m_k\\}_{k\\in [0,N_s]}$ and the predictions given by the scoring network $\\{\\hat{m}_k\\}_{k\\in [0,N_s]}$, the resulting loss is defined as:\n\n$L = \\alpha L_s + \\beta L_{mse} + \\gamma L_r,$\n\nwhere $\\alpha$, $\\beta$, and $\\gamma$ are hyperparameters controlling the relative importance of each component. Details about the different losses are given in Appendix C.3\n\nIn all experiments, we employed the Adam optimizer with a batch size of 128, training for 500 epochs at a learning rate of 0.001. Additionally, we configured the parameters as follows: $\\alpha$ was set to 1, $\\beta$ to 0.01, and $\\gamma$ to 0.1. We split our dataset in 1540 training samples, 330 validation samples, and 330 testing samples. The experiments were conducted using a V100-16GB GPU. The training and inference times are summarized in Table 2. It is important to note that, for both inference and testing, the majority of the computational time is dedicated to precomputing CLIP embeddings, as the scoring network itself is relatively lightweight and requires minimal computational resources.\n\nFor questions 1 to 6 in Section 3, we calculated the Mean Square Error (MSE), Quadratic Weighted Kappa (QWK), and Spearman Correlation Coefficient (SCC) between the predicted and ground truth labels on the test set. The results are presented in Table 3. Additional details about these compu- tations can be found in Appendix C.1.2. For comparison, we also include the scores from a linear regression model and report the inter-annotator agreement values, which correspond to the metrics computed between a randomly selected annotator's score and the mode. Additional experiments, including ablation studies, are presented in Appendix C.2."}, {"title": "4.4 GENERALIZATION CAPABILITIES OF THE PASTA-METRIC", "content": "In the main study, to constitute training, validation, and test sets, we shuffled all the samples without considering the XAI technique or the dataset they belong to. This means that the same images with different XAI techniques, or the same XAI techniques applied to different images, possibly from the same dataset, could be found in the training and test set. In this section, we investigate the impact of shuffling solely the image indices and the explanation indices. By doing so, we ensure that samples from the same image (resp. same XAI technique) cannot be in two different splits. This will help us investigate the generalization capabilities of the model in two distinct ways: can it generalize to new XAI techniques, and to new images? The results of the two setups for Q1 are shown in Table 4.\n\nThe results indicate a decrease of 0.04 in QWK when shuffling across XAI techniques and a more significant drop of 0.07 when shuffling across image IDs. This opens a discussion on the potential for applying the PASTA-metric to other image datasets. Regarding the generalization to new XAI methods, the relatively moderate drop in performance supports the feasibility of testing our metric on novel XAI techniques."}, {"title": "5 CONCLUSIONS", "content": "In this paper, we introduce PASTA, a novel perceptual assessment system designed to benchmark explainable AI (XAI) techniques in a human-centric manner. We integrate four diverse datasets, COCO, Pascal Parts, Cats Dogs Cars, and Monumai to form a large-scale benchmark dataset for XAI, and used it for an assessment of XAI explanations by human annotators. We also develop an automated evaluation metric that mimics human preferences based on a comprehensive database of human evaluations. This framework offers a scalable and reliable way to compare different XAI methods, facilitating robust evaluations across modalities previously unaddressed.\n\nOur findings demonstrate a clear preference for saliency-based explanations, particularly techniques such as LIME and SHAP, which align well with human intuition. These results affirm the scalability and reliability of our perceptual metric, which provides consistency with human assessment while automating much of the evaluation process.\n\nHowever, there are limitations to our approach. The current study focuses on a fixed set of datasets and XAI techniques. Human evaluations can be influenced by subjective factors that may affect the consistency of results. Furthermore, annotators can inadvertently introduce biases. Perceptual metrics like the ones proposed here are therefore not intended to serve as absolute measures of XAI performance. Rather, we consider them simply as complementary to other XAI metrics.\n\nLooking ahead, dynamic scoring approaches could be explored to capture the evolving nature of XAI techniques and their use in real-world applications. In conclusion, PASTA intends to take a step towards creating a transparent and trustworthy AI ecosystem. By aligning AI explanations with human cognitive processes, we aim to foster the development of more interpretable AI systems that can be understood and trusted by users across various domains."}]}