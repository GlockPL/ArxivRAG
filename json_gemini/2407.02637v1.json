{"title": "Change My Frame: Reframing in the Wild in r/ChangeMyView", "authors": ["Arturo Mart\u00ednez Peguero", "Taro Watanabe"], "abstract": "Recent work in reframing, within the scope of text style transfer, has so far made use of out-of-context, task-prompted utterances in order to produce neutralizing or optimistic reframes. Our work aims to generalize reframing based on the subreddit r/ChangeMyView (CMV). We build a dataset that leverages CMV's community's interactions and conventions to identify high-value, community-recognized utterances that produce changes of perspective. With this data, we widen the scope of the direction of reframing since the changes in perspective do not only occur in neutral or positive directions. We fine tune transformer-based models, make use of a modern LLM to refine our dataset, and explore challenges in the dataset creation and evaluation around this type of reframing.", "sections": [{"title": "Introduction", "content": "Reframing is a text style transfer technique that alters the frame, perspective, or focus of an utterance, aiming to highlight different aspects of the content while preserving its original meaning. A reframed sentence is compatible with the original, yet it shifts the perspective to emphasize different features of the situation, making specific contextual aspects more salient. Reframing does not contradict the core content and meaning of the original sentence; rather, it often introduces new content derived from a broader context, providing a distinct perspective or focus. A typical example of reframing involves a glass being half full or half empty, respectively denoting optimism or pessimism.\nThe perspective that an utterance implicitly carries can dramatically change the impact it has on a listener. The way we frame our communications has uses as benign as therapy, and as harmful as manipulative propaganda.\nRecent work has shown the possibility of crowdsourcing reframing datasets and performing positive reframing, where a sentence can be rephrased with a different and positive perspective while preserving its meaning. In this work, we avoid data that is obtained in unnatural experiment-task-based contexts both in order to make efficient use of publicly available forums and to reduce costs of hiring crowdsource workers or experts. Our (original, reframing) pairs are not prompted artificially but rather observed in the more naturally-occuring environment of one of the most popular public discussion forums in the world: Reddit.\nIn particular, we look at the subreddit r/ChangeMyView (CMV) since it self-describes as a forum where different perspectives on a single issue are shared.\nOur dataset currently consists of 32,306 (post, comment) pairs identified by CMV community members to be of high value and to have changed a view, as shown on Figure 1. Fine-tuning on our dataset has limited but increasing success as the dataset is pared down. Finally, challenges with proper reframing evaluation remain."}, {"title": "Previous Work", "content": "Some recent studies have explicitly addressed positive reframing. Ziems et al. (2022) constructed a parallel dataset of original sentences reflecting negative, stress-related tweets, along with crowdsourced, manually crafted novel reframings. With these dataset and psychological strategies, they used transformer-based models and succeeded in performing positive reframings.\nMore recently, Sharma et al. (2023) created a mental-health-expert-defined framework of linguistic-attributes that contribute to reframing. Experts generated reframes from a database of negative thoughts and the framework helped automatically evaluate reframing attributes. Additionally, Maddela et al. (2023) asked crowdsourced to generate original sentences, to label such sentences and to reframe them. Both studies built reframing models from their data.\nSince the studies mentioned addressed mostly positive reframing and used crowdsourcing as their main source of reframes, that has opened the door to other kinds of reframings."}, {"title": "Dataset Building", "content": ""}, {"title": "r/ChangeMyView", "content": "The subreddit r/ChangeMyView (CMV) is an online social forum that hosts a community around the goal of changing each others' views on particular topics.\nThe basic mechanics of CMV consist of an original post (OP) author submitting a concept about which they hold an original view. When they post in CMV, they are asking the community to change their view. Other CMV members then reply to the OP presenting different perspectives on the issue. Any submission that is not the OP is referred to as a comment.\nStarting from the OP, comments themselves can have their own subcomments and so on recursively. Indeed, any particular comment can be embedded in an arbitrary heavily-nested depth below the OP.\nWhen a particular contribution is deemed to be valuable to the Reddit community, recognitions such as upvotes, gold and flair are awarded. On top of these, the CMV community also assigns deltas. Borrowing from the mathematical use of the greek letter A (uppercase delta) to indicate change, CMV members award deltas to recognize comments that have changed their perspective relative to the OP."}, {"title": "Leveraging the delta system", "content": "Given the CMV specifics discussed above, we build a dataset on comments that have changed an OP author's particular perspective.\nWe filter out moderation posts, as well as [deleted], [removed] and empty posts or comments, since they are not helpful for reframing purposes, We also only consider comments that have been awarded deltas. We exploit the delta-awarding mechanics and DeltaBot comment structure to obtain (post,comment) pairs consisting of (1) an OP, and (2) a delta-awarded comment. Given moderator-enforced rules around post and comment lengths, we filter out any pairs where the post is shorter than 500 characters and where the comment is shorter than 100 characters. Further, we consider only posts that explicitly indicate that the OP author themselves has awarded a delta. To tighten the constraints, we limit consideration of (post,comment) pairs to those where the delta-awarded comment is a direct reply (i.e. not nested deep in the replies) to the OP. Additionally, only delta-awarding comments by the OP author are taken into account. This is illustrated in Figure 1."}, {"title": "r/ChangeMyView reframing data", "content": "The full initial dataset consisted of around 255,287 posts and 11,461,626 comments from CMV ranging from early 2012 to the end of 2022. Post and comment bodies as well as metadata was included in the dataset. Since flair labels indicating that an OP author awarded deltas only appear from 2015 onward, only 2015-2022 data was used.\nGiven the restrictions described in the previous section, we built a dataset of 32,306 (post,comment) pairs where the original author awarded a delta to a direct reply to their OP."}, {"title": "Experiments & Evaluation", "content": "For our experiments, for ease of comparison with previous work in reframing, we follow the experiment design by Ziems et al. (2022) using our dataset to fine-tune the encoder-decoder models BART (Lewis et al., 2020) and T5 (Raffel et al., 2019) with greedy decoding. We assign 80% of the (post,comment) pairs to the training split,"}, {"title": "Results & Discussion", "content": "As shown on Table 1, our work so far has only managed to achieve performance similar to that of GPT or no-pretrain-GPT-2 from Ziems et al. (2022) on overlap-similarity measures. BERTScore increased as we took away data, either with fewer sentences or by trimming data, assisted by GPT-4. Additional cross-dataset experiments were conducted by (A) fine-tuning on Ziems et al. (2022) data and evaluating on our test data, and (B) fine-tuning on our data and evaluating on test data from Ziems et al. (2022). These results are shown on Table 2 and suggest that our dataset affects performance negatively, causing suboptimal fine-tuning and setting up a difficult testing environment.\nFine-tuning on our data and testing on Ziems et al. (2022) data seemed to yield better results on overlap-similarity measures than the other way around, indicating potentially more universality and tolerance for diversity from our data.\nThese results suggests that our dataset still contains too much context around the original exposition of a particular point of view and around the substring of the comment that contains the reframing. The experiments with GPT-4-aided pair trimming seem to confirm this, obtaining the highest BERTscore and overlap-based performances once unnecessary text had been reduced or eliminated."}, {"title": "Conclusion and future work", "content": "Our results point to the need to pare down our data, either manually or with the help of LLMs. This will also open the doors to few-shot learning, explored in some of the studies mentioned in Section 2.\nReframing, which demands meaning preservation but keeps open the possibility of new content, is not completely adequately served by surface-form similarity evaluations. While BERTScore provides some degree of richer semantic tolerance and diversity of formulation of an idea, measures that automatically assess different parts of a re-frame, such as the linguistic attribute framework suggested in Sharma et al. (2023), can contribute to a fuller evaluation.\nFinally, extending this work to other languages,"}]}