{"title": "A Taxonomy of Stereotype Content in Large Language Models", "authors": ["Gandalf Nicolas", "Aylin Caliskan"], "abstract": "This study introduces a taxonomy of stereotype content in contemporary large language models\n(LLMs). We prompt ChatGPT 3.5, Llama 3, and Mixtral 8x7B, three powerful and widely used\nLLMs, for the characteristics associated with 87 social categories (e.g., gender, race,\noccupations). We identify 14 stereotype dimensions (e.g., Morality, Ability, Health, Beliefs,\nEmotions), accounting for ~90% of LLM stereotype associations. Warmth and Competence\nfacets were the most frequent content, but all other dimensions were significantly prevalent.\nStereotypes were more positive in LLMs (vs. humans), but there was significant variability\nacross categories and dimensions. Finally, the taxonomy predicted the LLMs' internal\nevaluations of social categories (e.g., how positively/negatively the categories were represented),\nsupporting the relevance of a multidimensional taxonomy for characterizing LLM stereotypes.\nOur findings suggest that high-dimensional human stereotypes are reflected in LLMs and must\nbe considered in AI auditing and debiasing to minimize unidentified harms from reliance in low-\ndimensional views of bias in LLMs.", "sections": [{"title": "A Taxonomy of Stereotype Content in Large Language Models", "content": "Humans create and place each other into social categories (e.g., in terms of gender, race,\nage, occupations) to simplify and navigate the social world, often via potentially harmful\nstereotypes (1). A stereotype is defined here and in general psychological models as a\ncharacteristic associated with a social category (e.g., through explicit beliefs, implicit\nassociations, 2). These stereotypes can vary in content (i.e., what they are about) and valence\n(i.e., how positive or negative they are), among other properties (3). Recent models have used\ntext analysis to characterize the diversity of stereotypes across salient social categories in human\nsurvey data (4). However, stereotype content has not been systematically described for\ncontemporary Artificial Intelligence (AI) large language models (LLMs). Effective auditing and\npotential debiasing solutions for AI bias require first a more comprehensive taxonomy of the\nvarious stereotypes that are associated with social categories in LLMs. Thus, the current paper\nuses a multimethod approach, including methods such as cluster and dictionary analyses, to\nintroduce a taxonomy of stereotype content in three state-of-the-art LLMs.\nThe taxonomy includes various dimensions of content (i.e., distinct semantics groupings\nin terms of what the stereotypes are about, including: Sociability, Morality, Ability,\nAssertiveness, socioeconomic Status, political-religious Beliefs, Health, Occupation, Emotion,\nDeviance, Social Groups, Geographic origin, and Appearance), and we characterize how well\nthese dimensions account for the LLMs' associations with social categories (i.e., coverage). We\nalso describe the content dimensions' properties (direction and valence), change over responses,\nand correspondence with human evaluations of social categories (i.e., their predictive value).\nStereotype Content in Human Data"}, {"title": null, "content": "The best-established stereotype dimensions are Warmth and Competence, which are\nevolutionarily plausible, have been found cross-culturally and over time, and are predictive of\nemotions and behaviors (5, 6). Warmth (also called communion or the horizontal dimension)\nrefers to attributions about a target's sociability and morality. Competence (also called agency or\nthe vertical dimension) refers to attributions about a target's abilities and assertiveness. In other\nwords, when evaluating others, humans prioritize understanding: is this person a friend or foe\n(Warmth), and can they act on their intentions (Competence)? Expanding into a more\ncomprehensive taxonomy, the Spontaneous Stereotype Content Model (SSCM, 4) suggested that\nabout 14 content dimensions account for 80-95% of freely-generated stereotypes about salient\nsocial categories. These dimensions are: Sociability and Morality (facets of Warmth), Ability and\nAssertiveness (facets of Competence), socioeconomic Status, political-religious Beliefs (7, 8),\nAppearance, Emotion, Occupation, Health, Deviance, Geography, Family relations, and Social\nGroups (e.g., \"rich people are men\u201d). Additionally, these dimensions vary in representativeness\n(i.e., prevalence of association), with Warmth and Competence facets being highly representative\nacross social categories, while associations about Health and Geographic origin are less\nprevalent (4). Representativeness relates to primacy and importance of stereotype dimensions\n(3).\nIn addition to representativeness, many stereotype dimensions vary in direction, which\nrefers to how \u201clow\u201d to \u201chigh\u201d a dimension is, along opposite ends of a semantic differential. To\nillustrate, a category can be evaluated as low Morality (e.g., \u201cimmoral,\u201d \u201cdishonest\u201d) or high\nMorality (e.g., \u201cmoral,\u201d \u201chonest); low Deviance (e.g., \u201cunremarkable,\u201d \u201caverage\u201d), or high\nDeviance (e.g., \u201cunique,\u201d \u201cweird,\u201d 4)."}, {"title": null, "content": "A related property is valence, that is, how positive vs. negative the stereotype is.\nDirection and valence often correlate. For example, being \u201cuneducated\u201d or \u201cunintelligent\u201d (low\nAbility) is evaluated more negatively than being \u201ceducated\u201d or \u201cintelligent\u201d (high Ability).\nHowever, the two constructs are differentiable. For example, high Assertiveness can sometimes\nbe seen negatively (\u201caggressiveness\u201d), and some dimensions, such as political-religious Beliefs\nvary in direction from liberal/secular to conservative/religious (whether liberal or conservative is\nat the low or high endpoint is arbitrary; as used, this dimension effectively measures\nconservatism), which tend not to strongly correlate with valence on average. Additionally,\nvalence is more general, which has strengths, such as being applicable to more content\ndimensions and to overall evaluations, and thus making it a strong signal in language (9).\nHowever, this generality also has limitations, such as having more variability across contexts\n(e.g., what is positive/negative may vary based on domain, 4).\nFinally, the representativeness and direction/valence of a dimension are differentiable\nproperties of stereotypes. For example, using numerical scales, Americans evaluate nurses and\ndoctors as being similarly high/positive Warmth and high Competence (i.e., direction/valence).\nHowever, when open-endedly describing these targets, associations for nurses are mostly about\nWarmth, while associations for doctors are mostly about Competence. That is, despite similar\ndirection/valence, Warmth is more representative of stereotypes about nurses, while Competence\nis more representative of stereotypes about doctors\nStereotype Content in LLMs\nLLMs (e.g., ChatGPT, 10\u201312) are generative Al models trained on vast amounts of text\ndata which learn contextualized semantics and can generate human language in response to\nlinguistic input. Given their training data from the internet and other text sources, LLMs"}, {"title": null, "content": "reproduce many human stereotypes and biases (13). However, almost all the research on the\ntopic has examined either general valence associations, a very limited number of stereotype\ndimensions, or simple word associations (without identifying more generalizable dimensions of\nmeaning). For example, research shows that many social categories have negative\nrepresentations in AI models paralleling human stereotypes (14\u201316), and that Warmth and\nCompetence valence differences emerge in LLM stereotypes (17\u201320). More recent papers have\nlooked at 3-dimensional models (e.g., Warmth/communion, Competence/status, and Beliefs, 8,\n21). However, these are still low-dimensional models that may not account for a near-totality of\nstereotypical associations in LLMs. Whether more dimensions (and which) are needed to\nunderstand social representations in AI, has yet to be systematically examined.\nAdditionally, most research has focused on specific social categories, such as gender or\nrace (22, 23). However, understanding more generalizable patterns of stereotype content requires\nexamining larger and more representative samples of categories (5). Previous research has also\nfocused on examining text embeddings directly (24, 25), the numerical representations of text\nthat underlie the more conversational output of LLMs that users interact with through chatbots.\nHere, we focus on the text output directly, as these constitute the final product in most\napplications and have the most direct impact on the general public.\nConsequences of Stereotypical Associations\nStereotypes may be inaccurate, over-generalized, essentializing, and self-fulfilling,\namong other well-documented problematics (26), often resulting in discrimination, conflict, and\nadverse health impacts for stigmatized groups (27, 28). Both positive and negative stereotypes\ncan be harmful (29), and their effects have been thoroughly documented. For example,\nstereotype content predicts outcomes such as emotional responses and interpersonal behaviors"}, {"title": null, "content": "(30), hiring and performance evaluations (31), interactions across societal and organizational\nhierarchies (32, 33), and attitudes towards AI (34).\nThese consequences may be amplified, and stereotypes reinforced, via biased Al models.\nLLMs have become ubiquitous in applications with real-world impact, from healthcare (35) to\nhiring (36). As with research and auditing, efforts to minimize harms from LLM stereotypes\nhave so far focused on general valence, or in a few cases, on a limited number of dimensions\nand/or a small number of social categories (17, 18). However, a more comprehensive taxonomy\nof the various LLM stereotype dimensions may be needed for more effective auditing and\npotential debiasing solutions for AI fairness.\nCurrent Study\nIn the current study we introduce a taxonomy of stereotype content in contemporary\nLLMs, including identifying dimensions using both data- and theory-driven approaches, and\ncharacterizing the coverage, representativeness, direction, valence, change over responses, and\npredictive value of the diverse set of proposed content dimensions. We derive this taxonomy\nbased on the models' semantic associations to several U.S. social category terms, in line with\nsocial psychological models focusing on generalizable stereotype properties that are applied\nacross social categories. We focus on three recent and widely used LLMs: ChatGPT 3.5 Turbo\n(primary model), as well as Llama 3 and Mixtral 8x7B Instruct (replication models for a subset\nof analyses), providing convergent evidence for the taxonomy.\nWe prompted the LLMs to list 50 characteristics associated with social categories and\ncoded responses into semantic dimensions using cluster and dictionary analyses. We examine\nhow many of the LLM associations are coded into meaningful dimensions (i.e., coverage) and\nhow frequently the different dimensions occurred in stereotypes across categories (i.e.,"}, {"title": null, "content": "representativeness/prevalence). Additionally, we explore changes in content representativeness\nacross the 50 characteristics requested. We also measured how high vs. low (direction) and\npositive vs. negative (valence) the different dimensions are. Finally, we examined whether the\nfull taxonomy predicts general valence evaluations of the categories (by both the LLMs and\nhumans).\nWe expected to find significant overlap with the human SSCM data (4), where\ndimensions related to Warmth (Sociability, Morality) and Competence (Ability, Assertiveness)\nare highly representative, but not sufficient to characterize stereotype content, with additional\ndimensions (e.g., Emotion, Deviance) showing significant prevalence across categories. Given\nchatbots' safety features (37), we expected either similar or more positive stereotypes than\nhuman data for ChatGPT and Mixtral, and more negative stereotypes for Llama 3's base model.\nFinally, we hypothesized the expanded taxonomy would add significant predictive value above\ncurrent valence-exclusive approaches, suggesting better capture of both internal and human\nsocial category representations in LLMs.\nResults\nCluster Analysis\nA majority of fit metrics suggested either 2 or 59 clusters. To capture meaningful\ndiversity in content, we used a 59-cluster solution (see online repository for more information;\nAs expected, this data-driven approach revealed multiple clusters related to dimensions\nfrom psychological models. A replication cluster analysis using Llama3 with both Llama3 and\nSBERT embeddings also showed evidence for a taxonomy composed of these dimensions (see\nSupplement)."}, {"title": "Direction", "content": "The direction of the various dimensions differed significantly, ChatGPT: $F(7, 4008.02) =\n130.64, p < .001$; Llama 3: $F(7, 3838.52) = 107.03, p < .001$; Mixtral: $F(7, 4483.30) = 229.79, p$\n< .001$, see and Figure 2. Most dimensions were high-directional on average, with\nhighest scores for Deviance, Ability, and Assertiveness, and negative direction for Health (as\nwell as for Sociability and Morality in Llama 3)."}, {"title": "Valence", "content": "Collapsing across dimensions, the valence of the associations was positive for the\nchatbots with safeguards, ChatGPT: $M = .187, t(84.3) = 6.154, p < .001$; Mixtral: $M = .262,$\n$t(85.8) = 10.31, p < .001$; and neutral for the Llama 3 base model: $M = -.03, t(84.8) = 0.837, p =$\n$.405$. However, the valence of specific dimensions differed significantly, ChatGPT: $F(13,$\n$6200.68) = 19.1, p < .001$; Llama 3: $F(13,5707.56) = 11.8, p < .001$; Mixtral: $F(13, 7107.1)$\n$=35.6, p < .001$, see and Figure 3. Valence showed more significant differences between\nLlama 3 and the other models, potentially as a result of its lack of chatbot finetuning, which may\nreduce its level of safeguards targeting valence. Llama 3 was more negative across dimensions,\nmore in line with human data. However, the average relative valence of specific dimensions were\nmore similar between human data and ChatGPT and Mixtral."}, {"title": "Specific Illustrations", "content": "In Figure 4, we show examples of stereotype prevalence and direction/valence for\nspecific salient social categories. These results illustrate how understanding both prevalence and\ndirection/valence across multiple dimensions reveals stereotype differences, including higher\nprevalence of Sociability content for women (vs. men), despite similar direction, or high\nfrequency of content for dimensions beyond Warmth and Competence, such as Deviance\nstereotypes for \u201cheterosexual\u201d and Emotion stereotypes for \u201cpoor\u201d categories. Additional\nexamples are available in the online repository."}, {"title": "Change Over Responses", "content": "For this exploratory analysis we found that there were significant increases (Sociability,\nMorality, Assertiveness, Emotion, and Other content, $ps < .007$) and decreases (Ability, Status,\nBeliefs, and Social Groups, $ps < .027$) in content representativeness over the 50 responses for\nvarious dimensions. In fact, these patterns (e.g., Ability more common in earlier responses,\nWarmth facets more common later on) replicate human patterns (4). However, the changes were\nnot large enough to meaningfully change the taxonomy prevalence if fewer than 50 responses are\nconsidered. Extrapolating from these patterns (with caveats for predicting beyond the data\nrange), we would not expect that moderate increases beyond 50 would be particularly impactful\neither. Perhaps the exception for change-over-response adjustments would be for Sociability\ncontent, which shows the largest change by far, from 7.8% of first responses being about\nSociability, increasing to 17.5% of 50th responses. See the Supplement for further information.\nPredictive Value\nFinally, we examine whether the full taxonomy improves predictions of LLM and human\ngeneral valence evaluations of social categories, as compared to using only the valence of the\nassociations as a predictor. As expected (see Table 5), a baseline linear regression model\npredicting internal general LLM valence about a category from the valence of the responses\nabout the category was significant. However, a regularized model adding all the taxonomy's\ndictionary-coded dimensions significantly improved predictivity. All dimensions of the\ntaxonomy were retained in at least one LLM, and most were also significant in at least one linear\nmodel, in support for both the importance of an extended taxonomy in improving predictions of\ncategory representations, and the potential for cross-model variability."}, {"title": "Discussion", "content": "The present study introduces a nuanced taxonomy of the stereotype content in\ncontemporary LLMs. We prompted ChatGPT 3.5, Llama 3, and Mixtral 8x7b to provide cultural\nstereotypes about a large number of salient social categories. We then content-coded these\nassociations to understand stereotype associations encoded by the LLMs.\nThe dimensions of the taxonomy were initially identified via a data-driven cluster\nanalysis of text embeddings of the associations, and largely align with the content of human\nstereotypes described by the SSCM (4). Various dimensions shifted slightly in representativeness\nbetween LLMs, analytical approach, and human data, but the general pattern involved very high\nrepresentativeness of Warmth and Competence facets, followed by Status, Beliefs, Emotions,\nand Appearance, and then a variety of smaller yet significantly prevalent content, such as Health,\nOccupations, Deviance, Social group membership, and Geographical stereotypes.\nWe demonstrate significant comprehensiveness of these dimensions by establishing that\nthey account for ~90% or more of the social category associations. For comparison, the \u201cbig\ntwo\u201d of Warmth and Competence, recently examined in LLM studies (17, 18), accounted for\nonly about 54-63% of content. Even then, our results suggest differences in representativeness,\ndirection, valence, change over time, and predictivity for facets of Warmth and Competence\n(Sociability, Morality, Ability, Assertiveness), suggesting that breaking down the \u201cbig two\u201d into\ntheir theoretical facets (3) may be preferrable to characterize stereotype content.\nDespite similarity to human models in the representativeness of dimensions, the LLMs\nhad noticeable departures in direction and valence: while human stereotypes tend to be negative,\nthe LLMs showed more positivity across dimensions. This may be the result of reinforcement\nlearning from human feedback and other safeguards put into place after initial model training"}, {"title": null, "content": "(i.e., they may not reflect positivity in the training data, 10, 12). For Llama 3 this pattern was\nattenuated, potentially due to the use of a base (vs. chatbot) model, including fewer valence\nsafeguards. Regardless, these findings indicate neutral-to-positive valence for stereotype content\nacross categories and dimensions, on average.\nThis positivity, however, does not signify a lack of potential for valence-based bias. We\nfind between-category direction/valence differences aligning with expected biased valence\nevaluations, such as poor people being stereotyped more negatively across most dimensions as\ncompared to rich people (4). Additionally, direction and valence differences between dimensions\nunderscores that dimension-specific valence is more informative than the general valence often\nused in LLM auditing. Some of these differences between dimensions (particularly for direction)\nalign with human patterns such as higher positivity for Competence-related dimensions (Ability,\nAssertiveness, Status) than Warmth-related dimensions (Sociability, Morality). Future research\nshould further examine these patterns in models with differing levels of safeguards.\nIn addition, we find differences in prevalence between categories across dimensions,\neven when the categories are relatively similar on valence. For example, ChatGPT's stereotypes\nabout women focused more on sociability, while men stereotypes focused more on assertiveness;\nstereotypes about gay people were more about social category membership, while stereotypes\nabout heterosexual people were more about (lack of) deviance (e.g., \u201caverage,\u201d \u201cnormal\u201d). This\nfinding underscores the need to consider both direction/valence and representativeness to more\nfully capture biased representations (38).\nAnalyses of change over responses suggests that the taxonomy is largely robust to the\nnumber of responses requested, with all dimensions remaining statistically significant. However,\nwe did find evidence for dynamic changes across dimensions, which largely align with human"}, {"title": null, "content": "patterns. For example, Ability and stereotypes that seem to be more structural (e.g., Status,\nBeliefs) tend to happen earlier in the list of responses, whereas more interpersonal stereotypes\ntend to happen later. Earlier responses may also have a bigger impact in AI applications, since\nshorter text may only retrieve initial stereotypes. Future studies can further examine these\ntemporal dynamics.\nFinally, supporting the relevance of the taxonomy in understanding LLMs' stereotypes,\nthe dimensions predicted the LLMs' internal general valence representation of the social\ncategories, above-and-beyond the valence across stereotypical associations. That is, these\ndimensions provide unique predictive value about the positivity/negativity with which the LLMs\n(and humans, as suggested by exploratory results) represent social categories. These results\nsupport the validity of the taxonomy as representing consequential dimensions, that improve\nconnections between human and LLMs' stereotypes to improve auditing efforts, and between\ndimension-specific and broader (e.g., general valence-based) representations of social categories.\nImplications\nThe taxonomy introduced here provides a more nuanced view of stereotype associations\nin LLMs. While most previous research, auditing, and debiasing efforts have tended to focus on\ngeneral valence patterns, our paper suggests that understanding the content of LLMs\u2019\nassociations with social categories requires a much wider set of content dimensions. These same\ndimensions have been found to describe open-ended stereotypes in humans (4), as well as face\nimpressions and other person perceptions (39), in support of their relevance.\nIn previous research, these dimensions have been shown to predict prejudice towards and\ndecision making about social targets. Warmth and Competence facets are well-established\npredictors of outcomes ranging from hiring and performance evaluations to negative"}, {"title": null, "content": "interpersonal behaviors (30, 32). Moreover, the rest of the taxonomy predicts scenario-based\ndecision-making outcomes such as which social categories to prioritize for policies guaranteeing\naccess to healthcare (Health dimension), protection from hiring discrimination (Social groups,\nGeography), or protection from discrimination in facial recognition technologies (Appearance, 4,\n39). Understanding how these dimensions are reflected in LLMs can expand the ways in which\nwe measure stereotypes relevant to these outcomes (e.g., over time, across languages), with\nimplications for social psychological theory and interventions (40\u201343). However, such inferences\nfrom LLM to human cognition must carefully consider training data (transparency and biases),\nfine-tuned safeguards that may distort cultural patterns, and the potential for LLMs reflecting\nnovel or distinct stereotypes due to how they synthesize and process information in non-\ntransparent ways (e.g., 44).\nMore directly relevant to LLM development and use, a deeper understanding of the\nnuances of stereotypes can help prevent bias percolating through auditing and debiasing\napproaches focused on general indicators. Developing benchmarks and debiasing procedures that\naddress higher-dimensional stereotypes and multiple properties, including representativeness,\ndirection, and valence, will provide a more accurate picture of fairness in LLMs and responsible\napplications. For example, auditing efforts should pay attention not just to general positivity in\ncategories' representations in LLMs, but valence across many relevant stereotype dimensions,\nand our taxonomy provides an initial set of content to evaluate. Similarly, auditing should go\nbeyond valence/direction for these dimensions, to also measure prevalence, providing fuller\nprofiles of the representation of social categories (38). This would identify, for example, men\nand women showing similar valence across dimensions, yet being described more often based on\nsome dimensions over others (e.g., women associated more with Warmth and emotions, and men"}, {"title": null, "content": "associated more with Competence). These steps may reduce harmful exposure to stereotypes for\nstigmatized groups, reductions in the perpetuation of stereotypes via AI, and improved human-\ncomputer interaction, among others.\nLimitations and Future Directions\nThe current research is not without limitations. First, our results are US- and English-\ncentric, based on the training data of the LLMs selected biasing heavily towards these data.\nHowever, initial cross-cultural research with human participants showed fair stability of the\nSSCM taxonomy (4). Second, the LLMs used show a significant lack of transparency regarding\ntraining data and implemented safeguards. As such, our ability to connect LLM representations\nto cultural representations is limited. Third, we restrict our results to three models in a growing\nfield of LLMs. However, the striking consistency between these independent models, and human\ndata, suggests that this taxonomy may be robust, with variability across specific properties (e.g.,\nvalence) to be studied in future research. Fourth, our prompts focused on cultural associations\nrather than \u201cpersonal\u201d ones. However, LLMs may not make significant distinctions in their\nrepresentations of concepts based on whether instructions request cultural vs. an artificial\n\"personal\" representation. In fact, preliminary analyses suggest that at least some stereotype\nproperties are invariant to these prompt variations (38). Nonetheless, more research is needed.\nHere, we focused on maximizing convergence with human data, which has most often asked\nabout cultural stereotypes, in part to minimize socially-desirable responding (c.f., chatbot\nsafeguards), but also because cultural stereotypes have been shown to predict the myriad of\noutcomes discussed previously. A related future direction should explore the taxonomy in less\nexplicit prompts that may elicit more negative stereotypes (e.g., 45). Fifth, open-ended data are\ndifficult to analyze. In line with the SSCM, we employed a multimethod approach to examine"}, {"title": null, "content": "robustness to analytical methods and find minimal differences (text embeddings clustering and\ndictionary analyses). However, future studies may improve coding methods to potentially extract\nmore information from the models.\nAdditional future directions include expanding the taxonomy to other multimodal and\nintersectional targets (46, 47) and developing auditing and debiasing methods incorporating the\ntaxonomy. Finally, we note that the current taxonomy includes dimensions that could always be\nfurther broken down or combined. This taxonomy aims to balance nuance with a manageable\nnumber of dimensions, but for those interested, additional subdimensions exist in the dictionaries\n(or instruments could be developed). For example, Appearance may be broken down into\nAttractiveness, Clothing, Body Properties (see online repository, and previous research, 48). On\nthe other hand, dimensions may be further combined for more parsimonious solutions, depending\non the need for nuance (e.g., creating Warmth and Competence dimensions instead of using their\nfacets).\nA complete understanding of the biases encoded into increasingly influential AI\ntechnologies requires acknowledging the multidimensional nature of stereotyping. The LLM\nstereotype taxonomy we identified and characterized largely aligns with human models, such as\nthe SSCM, while showing differences in valence and direction of stereotypes. As LLMs continue\nto be developed and deployed, our findings suggest that auditing and debiasing efforts should\nattend to the complexities of stereotypes, in an effort to minimize their harmful consequences.\nMaterials and Methods\nThe study uses data from LLMs to conduct a quantitative content analysis via text\nembeddings and dictionary coding, revealing information about the coverage, prevalence,\ndirection, and valence of the identified taxonomy of LLM stereotypes."}, {"title": "Language Models", "content": "We primarily focus on ChatGPT for analyses but provide results from two independent\nmodels, Mixtral 8x7B, and Llama 3 for coverage, prevalence, direction, valence, and predictive\nanalyses for insights across current LLMs.\nChatGPT\nWe use GPT 3.5 turbo as implemented in freely-available versions of ChatGPT as of July\n30th, 2024 (10). The ChatGPT model was trained on vast amounts of data, including the\nCommon Crawl (a large scraping of internet webpages), books, Reddit, and Wikipedia (11), as\nwell as human feedback in reinforcement learning (12), and potentially others. We used the\nPython OpenAI API to access the model.\nMixtral\nWe use Mixtral 8x7B (with instruct fine-tuning for chatbot functionality), accessed via\nPython Transformers and HuggingFace. Mixtral is a \u201csparse mixture of experts\u201d model (37, 49).\nUnlike the ChatGPT model used here, Mixtral has open weights, providing some additional level\nof transparency. Mixtral also has either similar or superior performance to ChatGPT 3.5 on\nvarious benchmarks (37). As with ChatGPT, the training data for the model is not transparently\ndisclosed by the developers, but is described as being extracted \u201cfrom the open Web.\u201d (49)\nLlama 3\nWe use Llama 3 8B (50) as another open-source LLM, accessed via Python\nHuggingFace. Llama 3 uses a decoder transformer architecture and achieves similar or superior\nperformance to other similarly-sized open-source LLMs. Unlike for the previous models, the\nLlama 3 model used here is the base model (i.e., not fine-tuned to behave as a chatbot). Using a\nbase model allowed us to reduce the influence of potential safeguards introduced via chatbot"}, {"title": null, "content": "fine-tuning. However, prompts about internal valence (see below) produced low-quality data\n(e.g., primarily, failure to provide a response) for the base model, so we relied on the fine-tuned\n\"instruct\" model for this variable.\nAvailable information about the Llama 3's training data includes that it consisted of over\n15T tokens from public data, over 5% non-English data, involving some filtering (e.g., NSFW\nfilters), and other quality checks (50).\nPrompts\nAll prompts use language from human research requesting the LLMs to provide\ncharacteristics that most Americans associate with social categories. In survey research, this\napproach allows for measurement of (perceptions of) \u201ccultural stereotypes\u201d shared across people,\nand helps reduce social desirability concerns of providing personal stereotypes (6). These\ncultural stereotypes are nonetheless highly predictive of bias and discrimination intent (4, 5, 30).\nPrompt language referring to cultural stereotypes may also reflect the aggregate (vs.\nindividualized) nature of the associations LLMs learn.\nAs a \"system prompt\", we entered: \u201cYou will be given information about a series of\ntarget groups of people. Answer how most people in American society would respond.\"\nFollowing the \u201csystem prompt,\u201d for each stimulus term, we retrieved their stereotype\nassociations by prompting: \u201cList 50 characteristics that you believe most Americans would think\ndescribe [target]. Use single words\u201d. We prompted the LLM for 50 characteristics to obtain an\nexpansive representation of the top stereotypical associations. Human research usually asks for\nfar fewer associations (e.g., one to ten, 4), and in general, the LLMs often provided fewer than\n50 responses, potentially when no additional associations were available. To further examine the\nrole of number of responses on the taxonomy, we also present a change-over-response analysis\""}, {"title": null, "content": "of content in the Supplement (Changes in prevalence tended to be small, not significantly\nimpacting the ranking of most dimensions, with the exception of Sociability associations which\nbecame more common in later responses and may thus be less prevalent if fewer responses are\nrequested).\nTo retrieve the model's internal general valence evaluations of the categories, we\nprompted: \u201cIn general, how does American society view people who are [target]? Use a scale\nranging from 1) Very negatively to 5) Very positively. Do not provide an explanation, only a\nsingle-number response using the scale.\"\nTo obtain the most deterministic results, we set the temperature (a parameter that\nmanipulates the randomness of the models' output) to 0, when possible. Repeated associations\nwithin a response were removed if they occurred. Some output included warnings about bias,\nwhich were removed. In addition to warnings, the models failed to return lists for terms it\nindicated are \"not commonly used or understood in American society\u201d (e.g., \u201cmahanaya\u201d for\nChatGPT). For all 87 categories, except the \u201cBlack\u201d category in ChatGPT (which returned only\nwarnings and was thus removed from subsequent analyses), we successfully retrieved the\nrequested output for at least one term. Because the Llama 3 base model is trained for sentence\ncompletion rather than chat (50), we slightly modified the prompt for sentence-completion rather\nquestion answering (see Supplement for these variations).\nHuman Data\nFor obtaining human stereotype dimension prevalence, direction, and valence (as\ndescriptive baselines; from open-ended items requesting the characteristics that come to mind to\ndescribe the categories), as well as general valence evaluations of social categories (how\nparticipants evaluated the categories in general, on a scale from 1 \u2013 Very Negatively to 5 \u2013 Very"}, {"title": null, "content": "positively), we use two published studies that contain parallel data on the same social categories\n(4, studies 3 and 3R, N = 797, including 4,782 category ratings). All human measures used the\nsame format as for the LLMs.\nStatistical Analysis\nWe preprocessed all responses for the stereotype associations by transforming words\nfrom plural to singular, removing capitalization, and replacing dashes with spaces.\nObtaining Text Embeddings\nTo run the initial cluster analysis, we first obtained the text embeddings for the LLMs\u2019\nresponses. Text embeddings are numerical vector representations of each stereotype association,\nencoding information about the semantic relations between words.\nHere, we use the embedding model SBERT (51, 52). SBERT embeddings have fewer\ndimensions than those underlying the LLMs used here (making them more suitable for cluster\nanalysis), are openly available (unlike ChatGPTs'), and have shown validity in previous cluster\nanalyses of social perceptions (4, 39). In addition, by using a different model to code the\nresponses, our coding is independent from the internal representations of the LLMs used here,\navoiding potential \u201cdouble dipping\u201d on a model's bias. However, we note that because the\ncluster analysis uses only the responses, without connecting them to the categories they were\nprovided for, it captures the semantic structure of the responses, not biases based on the\ncategory-response association.\nCluster Analysis\nWith the embeddings, we computed a (dis)similarity matrix using pairwise cosine\nsimilarities between all of the unique response embeddings (N = 5,871). For example, words\nsuch as fit and healthy received higher"}]}