{"title": "A Taxonomy of Stereotype Content in Large Language Models", "authors": ["Gandalf Nicolas", "Aylin Caliskan"], "abstract": "This study introduces a taxonomy of stereotype content in contemporary large language models (LLMs). We prompt ChatGPT 3.5, Llama 3, and Mixtral 8x7B, three powerful and widely used LLMs, for the characteristics associated with 87 social categories (e.g., gender, race, occupations). We identify 14 stereotype dimensions (e.g., Morality, Ability, Health, Beliefs, Emotions), accounting for ~90% of LLM stereotype associations. Warmth and Competence facets were the most frequent content, but all other dimensions were significantly prevalent. Stereotypes were more positive in LLMs (vs. humans), but there was significant variability across categories and dimensions. Finally, the taxonomy predicted the LLMs' internal evaluations of social categories (e.g., how positively/negatively the categories were represented), supporting the relevance of a multidimensional taxonomy for characterizing LLM stereotypes. Our findings suggest that high-dimensional human stereotypes are reflected in LLMs and must be considered in AI auditing and debiasing to minimize unidentified harms from reliance in low-dimensional views of bias in LLMs.", "sections": [{"title": "A Taxonomy of Stereotype Content in Large Language Models", "content": "Humans create and place each other into social categories (e.g., in terms of gender, race, age, occupations) to simplify and navigate the social world, often via potentially harmful stereotypes (1). A stereotype is defined here and in general psychological models as a characteristic associated with a social category (e.g., through explicit beliefs, implicit associations, 2). These stereotypes can vary in content (i.e., what they are about) and valence (i.e., how positive or negative they are), among other properties (3). Recent models have used text analysis to characterize the diversity of stereotypes across salient social categories in human survey data (4). However, stereotype content has not been systematically described for contemporary Artificial Intelligence (AI) large language models (LLMs). Effective auditing and potential debiasing solutions for AI bias require first a more comprehensive taxonomy of the various stereotypes that are associated with social categories in LLMs. Thus, the current paper uses a multimethod approach, including methods such as cluster and dictionary analyses, to introduce a taxonomy of stereotype content in three state-of-the-art LLMs.\nThe taxonomy includes various dimensions of content (i.e., distinct semantics groupings in terms of what the stereotypes are about, including: Sociability, Morality, Ability, Assertiveness, socioeconomic Status, political-religious Beliefs, Health, Occupation, Emotion, Deviance, Social Groups, Geographic origin, and Appearance), and we characterize how well these dimensions account for the LLMs' associations with social categories (i.e., coverage). We also describe the content dimensions' properties (direction and valence), change over responses, and correspondence with human evaluations of social categories (i.e., their predictive value).\nStereotype Content in Human Data"}, {"title": "Stereotype Content in Human Data", "content": "The best-established stereotype dimensions are Warmth and Competence, which are evolutionarily plausible, have been found cross-culturally and over time, and are predictive of emotions and behaviors (5, 6). Warmth (also called communion or the horizontal dimension) refers to attributions about a target's sociability and morality. Competence (also called agency or the vertical dimension) refers to attributions about a target's abilities and assertiveness. In other words, when evaluating others, humans prioritize understanding: is this person a friend or foe (Warmth), and can they act on their intentions (Competence)? Expanding into a more comprehensive taxonomy, the Spontaneous Stereotype Content Model (SSCM, 4) suggested that about 14 content dimensions account for 80-95% of freely-generated stereotypes about salient social categories. These dimensions are: Sociability and Morality (facets of Warmth), Ability and Assertiveness (facets of Competence), socioeconomic Status, political-religious Beliefs (7, 8), Appearance, Emotion, Occupation, Health, Deviance, Geography, Family relations, and Social Groups (e.g., \"rich people are men\u201d). Additionally, these dimensions vary in representativeness (i.e., prevalence of association), with Warmth and Competence facets being highly representative across social categories, while associations about Health and Geographic origin are less prevalent (4). Representativeness relates to primacy and importance of stereotype dimensions (3).\nIn addition to representativeness, many stereotype dimensions vary in direction, which refers to how \u201clow\u201d to \u201chigh\u201d a dimension is, along opposite ends of a semantic differential. To illustrate, a category can be evaluated as low Morality (e.g., \u201cimmoral,\u201d \u201cdishonest\u201d) or high Morality (e.g., \u201cmoral,\u201d \u201chonest); low Deviance (e.g., \u201cunremarkable,\u201d \u201caverage\u201d), or high Deviance (e.g., \u201cunique,\u201d \u201cweird,\u201d 4)."}, {"title": "Stereotype Content in LLMs", "content": "A related property is valence, that is, how positive vs. negative the stereotype is.\nDirection and valence often correlate. For example, being \u201cuneducated\u201d or \u201cunintelligent\u201d (low Ability) is evaluated more negatively than being \u201ceducated\u201d or \u201cintelligent\u201d (high Ability). However, the two constructs are differentiable. For example, high Assertiveness can sometimes be seen negatively (\u201caggressiveness\u201d), and some dimensions, such as political-religious Beliefs vary in direction from liberal/secular to conservative/religious (whether liberal or conservative is at the low or high endpoint is arbitrary; as used, this dimension effectively measures conservatism), which tend not to strongly correlate with valence on average. Additionally, valence is more general, which has strengths, such as being applicable to more content dimensions and to overall evaluations, and thus making it a strong signal in language (9). However, this generality also has limitations, such as having more variability across contexts (e.g., what is positive/negative may vary based on domain, 4).\nFinally, the representativeness and direction/valence of a dimension are differentiable properties of stereotypes. For example, using numerical scales, Americans evaluate nurses and doctors as being similarly high/positive Warmth and high Competence (i.e., direction/valence). However, when open-endedly describing these targets, associations for nurses are mostly about Warmth, while associations for doctors are mostly about Competence. That is, despite similar direction/valence, Warmth is more representative of stereotypes about nurses, while Competence is more representative of stereotypes about doctors\nStereotype Content in LLMs\nLLMs (e.g., ChatGPT, 10\u201312) are generative Al models trained on vast amounts of text data which learn contextualized semantics and can generate human language in response to linguistic input. Given their training data from the internet and other text sources, LLMs"}, {"title": "Consequences of Stereotypical Associations", "content": "reproduce many human stereotypes and biases (13). However, almost all the research on the topic has examined either general valence associations, a very limited number of stereotype dimensions, or simple word associations (without identifying more generalizable dimensions of meaning). For example, research shows that many social categories have negative representations in AI models paralleling human stereotypes (14\u201316), and that Warmth and Competence valence differences emerge in LLM stereotypes (17\u201320). More recent papers have looked at 3-dimensional models (e.g., Warmth/communion, Competence/status, and Beliefs, 8, 21). However, these are still low-dimensional models that may not account for a near-totality of stereotypical associations in LLMs. Whether more dimensions (and which) are needed to understand social representations in AI, has yet to be systematically examined.\nAdditionally, most research has focused on specific social categories, such as gender or race (22, 23). However, understanding more generalizable patterns of stereotype content requires examining larger and more representative samples of categories (5). Previous research has also focused on examining text embeddings directly (24, 25), the numerical representations of text that underlie the more conversational output of LLMs that users interact with through chatbots. Here, we focus on the text output directly, as these constitute the final product in most applications and have the most direct impact on the general public.\nConsequences of Stereotypical Associations\nStereotypes may be inaccurate, over-generalized, essentializing, and self-fulfilling, among other well-documented problematics (26), often resulting in discrimination, conflict, and adverse health impacts for stigmatized groups (27, 28). Both positive and negative stereotypes can be harmful (29), and their effects have been thoroughly documented. For example, stereotype content predicts outcomes such as emotional responses and interpersonal behaviors"}, {"title": "Current Study", "content": "(30), hiring and performance evaluations (31), interactions across societal and organizational hierarchies (32, 33), and attitudes towards AI (34).\nThese consequences may be amplified, and stereotypes reinforced, via biased Al models.\nLLMs have become ubiquitous in applications with real-world impact, from healthcare (35) to hiring (36). As with research and auditing, efforts to minimize harms from LLM stereotypes have so far focused on general valence, or in a few cases, on a limited number of dimensions and/or a small number of social categories (17, 18). However, a more comprehensive taxonomy of the various LLM stereotype dimensions may be needed for more effective auditing and potential debiasing solutions for AI fairness.\nCurrent Study\nIn the current study we introduce a taxonomy of stereotype content in contemporary LLMs, including identifying dimensions using both data- and theory-driven approaches, and characterizing the coverage, representativeness, direction, valence, change over responses, and predictive value of the diverse set of proposed content dimensions. We derive this taxonomy based on the models' semantic associations to several U.S. social category terms, in line with social psychological models focusing on generalizable stereotype properties that are applied across social categories. We focus on three recent and widely used LLMs: ChatGPT 3.5 Turbo (primary model), as well as Llama 3 and Mixtral 8x7B Instruct (replication models for a subset of analyses), providing convergent evidence for the taxonomy.\nWe prompted the LLMs to list 50 characteristics associated with social categories and coded responses into semantic dimensions using cluster and dictionary analyses. We examine how many of the LLM associations are coded into meaningful dimensions (i.e., coverage) and how frequently the different dimensions occurred in stereotypes across categories (i.e.,"}, {"title": "Results", "content": "representativeness/prevalence). Additionally, we explore changes in content representativeness across the 50 characteristics requested. We also measured how high vs. low (direction) and positive vs. negative (valence) the different dimensions are. Finally, we examined whether the full taxonomy predicts general valence evaluations of the categories (by both the LLMs and humans).\nWe expected to find significant overlap with the human SSCM data (4), where dimensions related to Warmth (Sociability, Morality) and Competence (Ability, Assertiveness) are highly representative, but not sufficient to characterize stereotype content, with additional dimensions (e.g., Emotion, Deviance) showing significant prevalence across categories. Given chatbots' safety features (37), we expected either similar or more positive stereotypes than human data for ChatGPT and Mixtral, and more negative stereotypes for Llama 3's base model. Finally, we hypothesized the expanded taxonomy would add significant predictive value above current valence-exclusive approaches, suggesting better capture of both internal and human social category representations in LLMs.\nResults\nCluster Analysis\nA majority of fit metrics suggested either 2 or 59 clusters. To capture meaningful diversity in content, we used a 59-cluster solution (see online repository for more information; Table 1 shows cluster examples).\nAs expected, this data-driven approach revealed multiple clusters related to dimensions from psychological models. A replication cluster analysis using Llama3 with both Llama3 and SBERT embeddings also showed evidence for a taxonomy composed of these dimensions (see Supplement)."}, {"title": "Representativeness", "content": "Two of the ChatGPT clusters captured linguistic regularities (e.g., words starting with \u201cun\") rather than stereotype content, and nine clusters had a mixture of content. In general, the cluster solution may be influenced by length of phrases, syntax, punctuation, and other non-content features encoded in the embeddings. These limitations are addressed by alternative methods. Specifically, the dimensions identified were largely overlapping with existing dictionaries. Thus, the cluster analysis provided a data-driven set of dimensions that align with the instruments we use next.\nCoverage\nThe taxonomy dimensions, as measured through dictionaries, accounted for 93.5% of the ChatGPT, 94.3% of the Llama 3, and 88.4% of the Mixtral responses. Thus, the taxonomy characterizes the vast majority of top stereotypical associations for a large sample of salient social categories across three widely-used LLMs, in line with human studies(4). Unaccounted-for responses tended to be idiosyncratic, include names or other non-trait information, among other patterns. For comparison, the \u201cbig two\u201d of Warmth and Competence accounted for only ~54% of responses in ChatGPT and Mixtral, and 63% of Llama 3 responses.\nRepresentativeness\nThe various taxonomy dimensions differed in how representative they were of stereotypes across social categories, ChatGPT: F(14, 9518.37) = 295.73, p < .001; Llama 3: F(14, 9705) = 357.66, p < .001; Mixtral: F(14, 10263.92) = 263.61, p < .001, see Table 2 and Figure 1. Table results include descriptive information from the recent human studies with a similar design described previously (4), as a baseline for comparison. Across LLMs, the most representative dimensions were Ability and Assertiveness (facets of Competence), as well as Morality and Sociability (facets of Warmth). Beliefs and Status content followed, in line with"}, {"title": "Direction", "content": "their relevance in human research (7). Appearance and Emotions were also highly prevalent.\nAssociations about Geographic origin (e.g., \u201cforeign\u201d) were present, but less frequently. An \"Other\" category groups less frequent content related to culture, family, fortune, arts, and science.\nFor representativeness, we show response rates in figures and proportions in tables and main analyses (for clarity of presentation, but results are congruent across both metrics/methods).\nDirection\nThe direction of the various dimensions differed significantly, ChatGPT: F(7, 4008.02) = 130.64, p < .001; Llama 3: F(7, 3838.52) = 107.03, p < .001; Mixtral: F(7, 4483.30) = 229.79, p < .001, see Table 3 and Figure 2. Most dimensions were high-directional on average, with highest scores for Deviance, Ability, and Assertiveness, and negative direction for Health (as well as for Sociability and Morality in Llama 3)."}, {"title": "Valence", "content": "Valence\nCollapsing across dimensions, the valence of the associations was positive for the chatbots with safeguards, ChatGPT: M = .187, t(84.3) = 6.154, p < .001; Mixtral: M = .262, t(85.8) = 10.31, p < .001; and neutral for the Llama 3 base model: M = -.03, t(84.8) = 0.837, p = .405. However, the valence of specific dimensions differed significantly, ChatGPT: F(13, 6200.68) = 19.1, p < .001; Llama 3: F(13,5707.56) = 11.8, p < .001; Mixtral: F(13, 7107.1) =35.6, p < .001, see Table 4 and Figure 3. Valence showed more significant differences between Llama 3 and the other models, potentially as a result of its lack of chatbot finetuning, which may reduce its level of safeguards targeting valence. Llama 3 was more negative across dimensions, more in line with human data. However, the average relative valence of specific dimensions were more similar between human data and ChatGPT and Mixtral."}, {"title": "Specific Illustrations", "content": "Specific Illustrations\nIn Figure 4, we show examples of stereotype prevalence and direction/valence for specific salient social categories. These results illustrate how understanding both prevalence and direction/valence across multiple dimensions reveals stereotype differences, including higher prevalence of Sociability content for women (vs. men), despite similar direction, or high frequency of content for dimensions beyond Warmth and Competence, such as Deviance stereotypes for \u201cheterosexual\u201d and Emotion stereotypes for \u201cpoor\u201d categories. Additional examples are available in the online repository."}, {"title": "Change Over Responses", "content": "Change Over Responses\nFor this exploratory analysis we found that there were significant increases (Sociability, Morality, Assertiveness, Emotion, and Other content, ps < .007) and decreases (Ability, Status, Beliefs, and Social Groups, ps < .027) in content representativeness over the 50 responses for various dimensions. In fact, these patterns (e.g., Ability more common in earlier responses, Warmth facets more common later on) replicate human patterns (4). However, the changes were not large enough to meaningfully change the taxonomy prevalence if fewer than 50 responses are considered. Extrapolating from these patterns (with caveats for predicting beyond the data range), we would not expect that moderate increases beyond 50 would be particularly impactful either. Perhaps the exception for change-over-response adjustments would be for Sociability content, which shows the largest change by far, from 7.8% of first responses being about Sociability, increasing to 17.5% of 50th responses. See the Supplement for further information.\nPredictive Value\nFinally, we examine whether the full taxonomy improves predictions of LLM and human general valence evaluations of social categories, as compared to using only the valence of the associations as a predictor. As expected (see Table 5), a baseline linear regression model predicting internal general LLM valence about a category from the valence of the responses about the category was significant. However, a regularized model adding all the taxonomy's dictionary-coded dimensions significantly improved predictivity. All dimensions of the taxonomy were retained in at least one LLM, and most were also significant in at least one linear model, in support for both the importance of an extended taxonomy in improving predictions of category representations, and the potential for cross-model variability."}, {"title": "Discussion", "content": "An exploratory analysis using human survey evaluations of general valence provided congruent results, supporting an improvement in predictive power from the expanded taxonomy.\nA simple model using the valence of the responses to predict general human valence evaluations had $R^2$ = .553 and AIC = 128.12 for ChatGPT, $R^2$ = .277 and AIC = 1360.84 for Llama 3, and $R^2$ = .29 and AIC = 1417.9 for Mixtral (with significant effects for the predictor, p < .001). A model adding all the dictionary-coded prevalences had R\u00b2 =.620 and AIC =, $\\chi2(15)$ = 6.5, p = .01 for ChatGPT, $R^2$=.325 and AIC = 1340.39, $\\chi2(14)$ = 22.92, p < .001 for Llama 3, and $R^2$=.4 and AIC = 1329.4, $\\chi2(15)$ = 51.105, p <.001 for Mixtral.\nDiscussion\nThe present study introduces a nuanced taxonomy of the stereotype content in contemporary LLMs. We prompted ChatGPT 3.5, Llama 3, and Mixtral 8x7b to provide cultural stereotypes about a large number of salient social categories. We then content-coded these associations to understand stereotype associations encoded by the LLMs.\nThe dimensions of the taxonomy were initially identified via a data-driven cluster analysis of text embeddings of the associations, and largely align with the content of human stereotypes described by the SSCM (4). Various dimensions shifted slightly in representativeness between LLMs, analytical approach, and human data, but the general pattern involved very high representativeness of Warmth and Competence facets, followed by Status, Beliefs, Emotions, and Appearance, and then a variety of smaller yet significantly prevalent content, such as Health, Occupations, Deviance, Social group membership, and Geographical stereotypes.\nWe demonstrate significant comprehensiveness of these dimensions by establishing that they account for ~90% or more of the social category associations. For comparison, the \u201cbig two\u201d of Warmth and Competence, recently examined in LLM studies (17, 18), accounted for only about 54-63% of content. Even then, our results suggest differences in representativeness, direction, valence, change over time, and predictivity for facets of Warmth and Competence (Sociability, Morality, Ability, Assertiveness), suggesting that breaking down the \u201cbig two\u201d into their theoretical facets (3) may be preferrable to characterize stereotype content."}, {"title": "Limitations and Future Directions", "content": "Despite similarity to human models in the representativeness of dimensions, the LLMs had noticeable departures in direction and valence: while human stereotypes tend to be negative, the LLMs showed more positivity across dimensions. This may be the result of reinforcement learning from human feedback and other safeguards put into place after initial model training (i.e., they may not reflect positivity in the training data, 10, 12). For Llama 3 this pattern was attenuated, potentially due to the use of a base (vs. chatbot) model, including fewer valence safeguards. Regardless, these findings indicate neutral-to-positive valence for stereotype content across categories and dimensions, on average.\nThis positivity, however, does not signify a lack of potential for valence-based bias. We find between-category direction/valence differences aligning with expected biased valence evaluations, such as poor people being stereotyped more negatively across most dimensions as compared to rich people (4). Additionally, direction and valence differences between dimensions underscores that dimension-specific valence is more informative than the general valence often used in LLM auditing. Some of these differences between dimensions (particularly for direction) align with human patterns such as higher positivity for Competence-related dimensions (Ability, Assertiveness, Status) than Warmth-related dimensions (Sociability, Morality). Future research should further examine these patterns in models with differing levels of safeguards.\nIn addition, we find differences in prevalence between categories across dimensions, even when the categories are relatively similar on valence. For example, ChatGPT's stereotypes about women focused more on sociability, while men stereotypes focused more on assertiveness; stereotypes about gay people were more about social category membership, while stereotypes about heterosexual people were more about (lack of) deviance (e.g., \u201caverage,\u201d \u201cnormal\u201d). This finding underscores the need to consider both direction/valence and representativeness to more fully capture biased representations (38).\nAnalyses of change over responses suggests that the taxonomy is largely robust to the number of responses requested, with all dimensions remaining statistically significant. However, we did find evidence for dynamic changes across dimensions, which largely align with human"}, {"title": "Materials and Methods", "content": "patterns. For example, Ability and stereotypes that seem to be more structural (e.g., Status, Beliefs) tend to happen earlier in the list of responses, whereas more interpersonal stereotypes tend to happen later. Earlier responses may also have a bigger impact in AI applications, since shorter text may only retrieve initial stereotypes. Future studies can further examine these temporal dynamics.\nFinally, supporting the relevance of the taxonomy in understanding LLMs' stereotypes, the dimensions predicted the LLMs' internal general valence representation of the social categories, above-and-beyond the valence across stereotypical associations. That is, these dimensions provide unique predictive value about the positivity/negativity with which the LLMs (and humans, as suggested by exploratory results) represent social categories. These results support the validity of the taxonomy as representing consequential dimensions, that improve connections between human and LLMs' stereotypes to improve auditing efforts, and between dimension-specific and broader (e.g., general valence-based) representations of social categories.\nImplications\nThe taxonomy introduced here provides a more nuanced view of stereotype associations in LLMs. While most previous research, auditing, and debiasing efforts have tended to focus on general valence patterns, our paper suggests that understanding the content of LLMs\u2019 associations with social categories requires a much wider set of content dimensions. These same dimensions have been found to describe open-ended stereotypes in humans (4), as well as face impressions and other person perceptions (39), in support of their relevance.\nIn previous research, these dimensions have been shown to predict prejudice towards and decision making about social targets. Warmth and Competence facets are well-established predictors of outcomes ranging from hiring and performance evaluations to negative\nMaterials and Methods"}, {"title": "Stimuli", "content": "interpersonal behaviors (30, 32). Moreover, the rest of the taxonomy predicts scenario-based decision-making outcomes such as which social categories to prioritize for policies guaranteeing access to healthcare (Health dimension), protection from hiring discrimination (Social groups, Geography), or protection from discrimination in facial recognition technologies (Appearance, 4, 39). Understanding how these dimensions are reflected in LLMs can expand the ways in which we measure stereotypes relevant to these outcomes (e.g., over time, across languages), with implications for social psychological theory and interventions (40\u201343). However, such inferences from LLM to human cognition must carefully consider training data (transparency and biases), fine-tuned safeguards that may distort cultural patterns, and the potential for LLMs reflecting novel or distinct stereotypes due to how they synthesize and process information in non-transparent ways (e.g., 44).\nMore directly relevant to LLM development and use, a deeper understanding of the nuances of stereotypes can help prevent bias percolating through auditing and debiasing approaches focused on general indicators. Developing benchmarks and debiasing procedures that address higher-dimensional stereotypes and multiple properties, including representativeness, direction, and valence, will provide a more accurate picture of fairness in LLMs and responsible applications. For example, auditing efforts should pay attention not just to general positivity in categories' representations in LLMs, but valence across many relevant stereotype dimensions, and our taxonomy provides an initial set of content to evaluate. Similarly, auditing should go beyond valence/direction for these dimensions, to also measure prevalence, providing fuller profiles of the representation of social categories (38). This would identify, for example, men and women showing similar valence across dimensions, yet being described more often based on some dimensions over others (e.g., women associated more with Warmth and emotions, and men\nMaterials and Methods\nThe study uses data from LLMs to conduct a quantitative content analysis via text embeddings and dictionary coding, revealing information about the coverage, prevalence, direction, and valence of the identified taxonomy of LLM stereotypes.\nStimuli"}, {"title": "Language Models", "content": "All data and materials available in the online repository:\nhttps://osf.io/bdu6g/?view_only=733e364b751a4736af2c3c897cf476c8\nStimuli\nWe used a list of 1,366 different terms referring to 87 salient social categories in the U.S. For example, terms such as \u201cwealthy,\u201d or \u201cmillionaire\u201d were stimuli used to represent the \u201crich\u201d social category. These terms have been validated and used successfully in previous LLM studies to elicit stereotype content (38). See Table 6 for categories and the Supplement for a full list of terms.\nLanguage Models\nWe primarily focus on ChatGPT for analyses but provide results from two independent models, Mixtral 8x7B, and Llama 3 for coverage, prevalence, direction, valence, and predictive analyses for insights across current LLMs.\nChatGPT\nWe use GPT 3.5 turbo as implemented in freely-available versions of ChatGPT as of July 30th, 2024 (10). The ChatGPT model was trained on vast amounts of data, including the Common Crawl (a large scraping of internet webpages), books, Reddit, and Wikipedia (11), as well as human feedback in reinforcement learning (12), and potentially others. We used the Python OpenAI API to access the model.\nMixtral\nWe use Mixtral 8x7B (with instruct fine-tuning for chatbot functionality), accessed via Python Transformers and HuggingFace. Mixtral is a \u201csparse mixture of experts\u201d model (37, 49). Unlike the ChatGPT model used here, Mixtral has open weights, providing some additional level of transparency. Mixtral also has either similar or superior performance to ChatGPT 3.5 on various benchmarks (37). As with ChatGPT, the training data for the model is not transparently disclosed by the developers, but is described as being extracted \u201cfrom the open Web.\u201d (49)\nLlama 3\nWe use Llama 3 8B (50) as another open-source LLM, accessed via Python HuggingFace. Llama 3 uses a decoder transformer architecture and achieves similar or superior performance to other similarly-sized open-source LLMs. Unlike for the previous models, the Llama 3 model used here is the base model (i.e., not fine-tuned to behave as a chatbot). Using a base model allowed us to reduce the influence of potential safeguards introduced via chatbot"}, {"title": "Prompts", "content": "fine-tuning. However, prompts about internal valence (see below) produced low-quality data (e.g., primarily, failure to provide a response) for the base model, so we relied on the fine-tuned \"instruct\" model for this variable.\nAvailable information about the Llama 3's training data includes that it consisted of over 15T tokens from public data, over 5% non-English data, involving some filtering (e.g., NSFW filters), and other quality checks (50).\nPrompts\nAll prompts use language from human research requesting the LLMs to provide characteristics that most Americans associate with social categories. In survey research, this approach allows for measurement of (perceptions of) \u201ccultural stereotypes\u201d shared across people, and helps reduce social desirability concerns of providing personal stereotypes (6). These cultural stereotypes are nonetheless highly predictive of bias and discrimination intent (4, 5, 30). Prompt language referring to cultural stereotypes may also reflect the aggregate (vs. individualized) nature of the associations LLMs learn.\nAs a \"system prompt\", we entered: \u201cYou will be given information about a series of target groups of people. Answer how most people in American society would respond.\"\nFollowing the \u201csystem prompt,\u201d for each stimulus term, we retrieved their stereotype associations by prompting: \u201cList 50 characteristics that you believe most Americans would think describe [target]. Use single words\u201d. We prompted the LLM for 50 characteristics to obtain an expansive representation of the top stereotypical associations. Human research usually asks for far fewer associations (e.g., one to ten, 4), and in general, the LLMs often provided fewer than 50 responses, potentially when no additional associations were available. To further examine the role of number of responses on the taxonomy, we also present a change-over-response analysis"}, {"title": "Human Data", "content": "of content in the Supplement (Changes in prevalence tended to be small, not significantly impacting the ranking of most dimensions, with the exception of Sociability associations which became more common in later responses and may thus be less prevalent if fewer responses are requested).\nTo retrieve the model's internal general valence evaluations of the categories, we prompted: \u201cIn general, how does American society view people who are [target]? Use a scale ranging from 1) Very negatively to 5) Very positively. Do not provide an explanation, only a single-number response using the scale.\"\nTo obtain the most deterministic results, we set the temperature (a parameter that manipulates the randomness of the models' output) to 0, when possible. Repeated associations within a response were removed if they occurred. Some output included warnings about bias, which were removed. In addition to warnings, the models failed to return lists for terms it indicated are \"not commonly used or understood in American society\u201d (e.g., \u201cmahanaya\u201d for ChatGPT). For all 87 categories, except the \u201cBlack\u201d category in ChatGPT (which returned only warnings and was thus removed from subsequent analyses), we successfully retrieved the requested output for at least one term. Because the Llama 3 base model is trained for sentence completion rather than chat (50), we slightly modified the prompt for sentence-completion rather question answering (see Supplement for these variations).\nHuman Data\nFor obtaining human stereotype dimension prevalence, direction, and valence (as descriptive baselines; from open-ended items requesting the characteristics that come to mind to describe the categories), as well as general valence evaluations of social categories (how participants evaluated the categories in general, on a scale from 1 \u2013 Very Negatively to 5 \u2013 Very"}, {"title": "Statistical Analysis", "content": "positively), we use two published studies that contain parallel data on the same social categories (4, studies 3 and 3R, N = 797, including 4,782 category ratings). All human measures used the same format as for the LLMs.\nStatistical Analysis\nWe preprocessed all responses for the stereotype associations by transforming words from plural to singular, removing capitalization, and replacing dashes with spaces.\nObtaining Text Embeddings\nTo run the initial cluster analysis, we first obtained the text embeddings for the LLMs\u2019 responses. Text embeddings are numerical vector representations of each stereotype association, encoding information about the semantic relations between words.\nHere, we use the embedding model SBERT (51, 52). SBERT embeddings have fewer dimensions than those underlying the LLMs used here (making them more suitable for cluster analysis), are openly available (unlike ChatGPTs'), and have shown validity in previous cluster analyses of social perceptions (4, 39). In addition, by using a different model to code the responses, our coding is independent from the internal representations of the LLMs used here, avoiding potential \u201cdouble dipping\u201d on a model's bias. However, we note that because the cluster analysis uses only the responses, without connecting them to the categories they were provided for, it captures the semantic structure of the responses, not biases based on the category-response association.\nCluster Analysis\nWith the embeddings, we computed a (dis)similarity matrix using pairwise cosine similarities between all of the unique response embeddings (N = 5,871). For example, words such as fit and healthy received higher cosine similarity scores than pairs such as fit and black"}, {"title": "Dictionary Coding", "content": "hair. We computed a k-means clustering solution from the dissimilarity matrix. To select an appropriate number of clusters (k), we used the R package NBclust (53), which runs multiple metrics of cluster fit. NBClust suggested k = 2 (6 metrics) and k = 59 (5 metrics) most frequently from the ks tested (2 through 60). Given a preference for higher-cluster solutions to capture more diversity of content, we set k = 59. Then, to facilitate labeling by the researchers, we obtained, for each cluster, the responses that were most semantically similar to their centroid (i.e., those that were most prototypical of the cluster). Finally, we used the k-means results to code all responses and compute cluster analyses.\nDictionary Coding\nDictionary approaches allow measurement of coverage, direction, and valence, and provide an independent coding approach from the cluster analysis. Dictionaries are lists of words coding for content referring to the dimensions identified in the cluster analysis, and which can be matched to the LLM responses. The dictionaries used here have been validated (48) and used successfully in previous studies of stereotypes in LLMs (38). The dictionaries include over 15,700 terms coded into over"}]}