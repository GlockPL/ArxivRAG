{"title": "RETHINKING Data SELECTION AT SCALE: RANDOM\nSELECTION IS ALMOST ALL YOU NEED", "authors": ["Tingyu Xia", "Bowen Yu", "Kai Dang", "An Yang", "Yuan Wu", "Yuan Tian", "Yi Chang", "Junyang Lin"], "abstract": "Supervised fine-tuning (SFT) is crucial for aligning Large Language Models\n(LLMs) with human instructions. The primary goal during SFT is to select a small\nyet representative subset of training data from the larger pool, such that fine-tuning\nwith this subset achieves results comparable to or even exceeding those obtained\nusing the entire dataset. However, most existing data selection techniques are de-\nsigned for small-scale data pools, which fail to meet the demands of real-world\nSFT scenarios. In this paper, we replicated several self-scoring methods-those\nthat do not rely on external model assistance on two million-scale datasets, and\nfound that nearly all methods struggled to significantly outperform random selec-\ntion when dealing with such large-scale data pools. Moreover, our comparisons\nsuggest that, during SFT, diversity in data selection is more critical than sim-\nply focusing on high-quality data. We also analyzed the limitations of several\ncurrent approaches, explaining why they perform poorly on large-scale datasets\nand why they are unsuitable for such contexts. Finally, we found that filtering\ndata by token length offers a stable and efficient method for improving results.\nThis approach, particularly when training on long-text data, proves highly benefi-\ncial for relatively weaker base models, such as Llama3. The code is available at\nhttps://github.com/xiatingyu/SFT-DataSelection-at-scale.", "sections": [{"title": "1 INTRODUCTION", "content": "With the advent of large language models (LLMs) such as ChatGPT, we have observed signifi-\ncant advancements in tasks involving instruction following (Wang et al., 2023b), intent compre-\\hension (Lu et al., 2023), and text generation (Zhao et al., 2023). One of the primary objectives of\ndeveloping LLMs is to harness their potential for generalizing to unseen natural language processing\n(NLP) tasks. To achieve this aim, many LLMs focus on precisely aligning with human instructions.\nRecent studies indicate that supervised fine-tuning (SFT) can customize LLMs for specific domains,\ntasks, or applications by utilizing well-crafted data. According to the study in Zhou et al. (2024a), it\nis feasible to fine-tune a pre-trained language model with a relatively small set of examples. Building\non this insight, several papers have explored data selection strategies for SFT of LLMs (Wang et al.,\n2024; Qin et al., 2024), emphasizing the importance of enhancing the quality of instruction tuning\n(IT) data or increasing data diversity. These strategies can be classified into two primary categories:\n(1) Extenral-scoring methods, which require support from more sophisticated external models like\nGPT-4 to score the data for the subsequent selection (Lu et al., 2023; Chen et al., 2023; Du et al.,\n2023; Liu et al., 2023; Zhou et al., 2024b); (2) Self-scoring methods, which leverage LLMs them-"}, {"title": "2 RELATED WORK", "content": "External-scoring Method. Lu et al. (2023) introduced an open-set instruction tagging method\ncalled INSTAG, which employed ChatGPT to generate detailed tags to measure and examine the\nvariety and intricacy of human instructions for LLMs during SFT. Chen et al. (2023) presented the"}, {"title": "3 SELF-SCORING STRATEGIES", "content": "In this paper, we focus on self-scoring methods that do not rely on external advanced LLMs to score\ndata. We refer Qin et al. (2024)'s work and categorize existing resourceful data selection methods\ninto two main perspectives: data quality-based methods and data diversity-based methods."}, {"title": "3.1 QUALITY-BASED SELECTIONS", "content": "In this section, we introduce 4 methods based on data quality assessment and selection. \u201cQuality\u201d\nhere refers primarily to the complexity, completeness, score, and influence of the datapoints. Dif-\nferent from Qin et al. (2024), we believe that the influence of a datapoint in the target dataset is\nalso a reflection of data quality, especially in practical scenarios, where we are required to deal with\ndiverse tasks rather than a single task. We thus regard the influence as a quality category as well.\nLESS Xia et al. (2024) instroduced low-rank gradient similarity search to select influential data for\nthe target application. Concretely, a model was trained with LoRA (Hu et al., 2021) for a warmup\nperiod on a small subset $D_{warmup} \\subset D$. Then, the Adam LoRA gradient features for each data point\nwere computed and stored in a gradient database.\nNext, a gradient datastore of projected low-dimensional gradient features was constructed which\ncan be reused for different target tasks. For training datapoints z, they computed d-dimensional\nprojection of the LoRA gradient $\\nabla l(z; \\theta^*_0) = \\Pi \\nabla l(z; \\theta^*_0)$, where $\\Pi$ is computed and applied by\nmemory-efficient online implementation of random projections proposed by Park et al. (2023). For\nvalidation datapoint z', they computed $\\hat{\\Gamma}(z', \\cdot) = \\Pi \\hat{\\Gamma}(z', \\cdot)$.\nFinally, LESS computed $\\max_z Inf_{Adam}(z, D_{val})$ for the training set z across all sub-validation sets\n$D_{val}$. Then it selected the highest score examples to construct $D_{train}$."}, {"title": "3.2 DIVERSITY-BASED SELECTIONS", "content": "In this section, we introduce methods that emphasize the diversity of instruction datasets, where\ndiversity refers to the overall diversity of the entire training dataset.\nDiverseEvol iteratively sampled training subsets to improve its own performance (Wu et al., 2023).\nIt selected new data points most distinct from any existing ones according to its current embedding\nspace in each iteration phase.\nGiven a training set D, DiverseEvol first randomly selected a data pool $P_0$ and trained an initial\nmodel $M_0$. In each iteration, it consisted of two operations: 1. Deduce new data points $D_t$ to merge\ninto $P_{t+1}$, informed by the previously trained model $M_t$. 2. Train the subsequent chat model $M_{t+1}$,\nwith the updated data pool $P_{t+1}$.\nDiverseEvol used K-Center-Sampling to select data. From a candidate pool, it chose k data points in\nsuch a way that the distances to their respective nearest existing training data points were maximized.\narg max min \u0394 (ni, pj)\ni\u2208Nt j\u2208Pt\nAt each step, the input parameters to K-Center-Sampling were the model Mt, the current training\npool Pt, and Dt. The selection function K-Center-Sampling then outputs the new data point Nt,\nwhich was added to the training pool for the next iteration Pt+1.\nZIP presented that model performance is negatively correlated to the compression ratio of training\ndata, which usually yields a lower training loss. Yin et al. (2024) proposed a quite efficient and\nuniversal data selection method named ZIP for training LLMs, which aimed to prioritize data subsets\nexhibiting a low compression ratio.\nZIP is initialized by calculating the sample-level compression ratio for the entire dataset D, where\nHD shows the information redundancy state of D. In each iteration, it selected K\u2081 samples with the\nlowest HD\u2081 to form an initial candidate pool DK\u2081. Then, it calculated the compression ratio of a\nmerged set that adds each sample in DK\u2081 to the selected set Dtrain, to update the redundancy state\nof the information TD\u2081.\nBased on the scores of the samples in DK1, ZIP selected DK\u2082 samples with the lowest scores. After\nthat, it initialized an empty selected set DK3, and computed the compression ratio of the union of\nDK3 and each sample in DK2. Then, the sample with the lowest compression ratio was added to\nDK3, and removed from DK2. Finally, each sample in DK3 was added to the selected set Dtrain. In\nZIP, the compression ratio calculation g(C(D)) is defined as:\ng(C(D)) = Bits(D)\nBits(C(D))"}, {"title": "4 EXPERIMENT", "content": "In practical applications, researchers frequently encounter extensive datasets from various sources\nduring SFT, which may also contain imperfections. Thus, in this study, rather than using the typically\nemployed IT datasets such as alpaca (Taori et al., 2023), we select two large-scale IT datasets at the\nmillion-record level, Openhermes2.5 (Teknium, 2023) and WildChat-1M (Zhao et al., 2024), to\nexamine the efficiency of existing data selection techniques in handling large datasets and to assess\ntheir performance in real-world scenarios.\nOpenhermes2.5 is presented by Teknium (2023), which comprises over 1 million data points. It\nis significantly more comprehensive and of higher quality, predominantly consisting of generated\nguides and chats. The dataset's information is sourced from 16 distinct origins, including meta-\nmath (Yu et al., 2023), CamelAI (Li et al., 2023a), among others. It encompasses a wide variety of\nsubjects such as mathematics, programming, and authentic user dialogues.\nWildChat-1M is introduced by Zhao et al. (2024) and features solely non-toxic user inputs and\nChatGPT responses. The dataset comprises 1 million dialogues between human users and ChatGPT,\nwith 25.53% of the interactions stemming from the GPT-4 model, and the remainder from GPT-3.5.\nIt encompasses a diverse range of user-chatbot exchanges, including ambiguous user inquiries, code-\nswitching, topic-switching, and political discussions. In this study, we extract English dialogues\nfrom the WildChat dataset, resulting in over 440k interactions."}, {"title": "4.2 BENCHMARKS", "content": "To thoroughly evaluate the capabilities of LLM, we explored various approaches across different\ndownstream tasks. We assess the reasoning abilities of LLMs using two commonly used datasets:\nthe Grade School Math dataset (GSM) (Cobbe et al., 2021) and Big-Bench-Hard (BBH) (Suzgun\net al., 2022) within the CoT setting (Wei et al., 2022). We evaluate the code generation capability\nwith the HumanEval dataset (Chen et al., 2021) and report pass@1 results. To determine the fac-\ntual knowledge of LLMs, we use the Massive Multitask Language Understanding dataset (MMLU)\n(Hendrycks et al., 2021) and provide 5-shot results. We also assess instruction-following ability\nusing the IFEval (Zhou et al., 2023b) dataset and report both strictly and loosely followed scores.\nAdditionally, we utilize scripts from OpenInstruct, which includes a collection of standard bench-\nmarks focusing on core capabilities (Wang et al., 2023a; Ivison et al., 2023; 2024)."}, {"title": "4.3 IMPLEMENTATION DETAILS", "content": "Specifically, we leverage the widely-used LLaMA3-8B (AI@Meta, 2024) and Qwen2-7B (qwe,\n2024) as our base models, and fine-tune them using the Llama-Factory framework (Zheng et al.,\n2024). We train these models for 3 epochs with a batch size of 128. Our training process employs a\ncosine learning rate scheduler beginning at 7e \u2013 6, which decays to 0.1, warms to 0.01, and utilizes\nan input length of 4096. To replicate our baseline methods on Openhermes and WildChat, we adjust\nsome original parameters and implementations to fit the large-scale datasets.\nIn term of LESS, individual models are built and trained on specific tasks. However, in practical\napplications, our goal is to train a model that enhances performance across various scenarios. Thus,\ngiven that the two datasets we select are both extensive and diverse, we randomly select 1000 data\npoints from each dataset as Dval. Additionally, due to the volume of our data, we randomly pick\n10,000 data points for warm-up training, differing from the method described in (Xia et al., 2024).\nAs for IFD, we initially generate 1000 clusters on instruction embeddings, which differs from the\nsettings given in Li et al. (2023b). For SelectIT, we adopt model-level selection as the final strategy\nfor the Qwen2 model and evaluate the model-level score on Qwen2-1.5B and Qwen2-7B. While\nfor Llama3, we employ sentence-level selection as the final approach. Considering that the Llama3\nfamily only has two public variants, Llama3-8B and Llama3-70B, and to mitigate time costs, we\ncompute the score based solely on Llama3-8B.\nWithin DiverseEvol, during each iteration's K-Center-Sampling stage, data points are selected based\non maximizing their distance to the nearest existing training data points, one at a time, until the"}, {"title": "5 DISCUSSION", "content": "In this section, we reproduce all baseline methods in experiments involving LLaMA3-8B and\nQwen2-7B on OpenHermes2.5, the experimental results are presented in Table 1, and results on\nWildChat are detailed in Table 2. We assess LLaMA3-8B and Qwen2-7B with and without fine-\ntuning on the entire dataset. All mentioned SFT data selection methods are employed to select\n10,000 samples as described in Section 4.3. We randomly run 5 times and all of the results are pro-\nvided in the tables. Furthermore, 50,000 samples obtained through various methods are also shown\nin the Appendix Table 5, 6.\nAs indicated in Table 1 and 2, it is evident that when dealing with extensive and diverse IT datasets,\nno data selection techniques consistently outperform random sampling by a substantial margin,\nwhich implies that the average score exceeds the random score by more than 1%. In most cases,\nthe results of the baseline method are within the range of the results obtained by 5 random runs, and\na few methods are even worse than the worst random result, For instance, when evaluating Cross-\nEntropy on Qwen2-7B using Openhermes2.5, the average result is a mere 54.02, significantly below\nthe lowest score of 57.04 obtained in the 5 random trials.\nBased on the experimental results, when dealing with an extensive SFT dataset, it is more ef-\nficient to randomly select training data instead of spending significant time and resources to\nmeticulously choose seemingly optimal training data. Random selection reduces costs and yields\nsuperior training results."}, {"title": "5.2 QUALITY VS DIVERSITY", "content": "Tables 1 and 2 demonstrate that the diversity-based selection strategy outperforms the quality-based\none. To examine whether prioritizing diversity over data quality improves data selection, we de-"}, {"title": "5.3 BASELINE ANALYSIS", "content": "In this part, we mainly analyze several methods and try to find the reasons why these methods fail\nin large-scale data sets and why these methods are not applicable to practical applications.\nThe lack of availability of Less is primarily evident in how its influence score is calculated. Since it\nrequires computing the score for the final data point in the target task, it is essential to meticulously\ndesign a target set for each task to filter the data. However, in practical applications, we face a\nvariety of training tasks that require our target data to be comprehensive and diverse. Hence, the\neffectiveness of LESS is strongly related to the quality of Dval.\nThe IFD approach determines the ultimate IFD score by evaluating the perplexity (ppl) of the re-\nsponse. However, the length of the data significantly affects the ppl value. In particular, shorter data\ntend to produce excessively high ppl values, which contradicts with our expected results. Ultimately,\nwe note that the IT data instructions selected by the IFD approach are quite brief, averaging merely\n42 tokens on Openhermes, which aligns with the findings reported by Liu et al. (2023)."}, {"title": "5.4 WHICH METHOD IS THE BEST?", "content": "By examining the average results of all methods, we notice that the majority of methods perform\nbetter with WildChat as the data source compared to OpenHermes, as illustrated in Figure 2, which\nis rather unexpected. Nonetheless, from a quality perspective, WildChat's conversation data tends to\nbe noisy, particularly since the context of multiple conversation rounds is sometimes unrelated, while\nOpenHermes's data quality should be substantially higher than WildChat. However, the performance\nof the same data selection methods on these two types of data contradicts with our expectations. It is\nobserved that the average token length for WildChat data is 1142, whereas for OpenHermes data, it is\n354. Drawing inspiration from the work of Shen (2024), we devise a new experiment concentrating\non data selection by token length. Initially, we obtain N clusters through the K-Means process and\nsubsequently select a certain amount of data based on the token length from each cluster proportional\nto its size. The results are presented in Table 3.\nBased on Table 3, it is evident that using token length as the criterion for data selection generally\nyields optimal results. Specifically, for Llama3, regardless of whether the data source is Open-\nHermes or WildChat, the results are significantly superior to those achieved by other methods. In\naddition, the average score on WildChat (55.51) surpasses that obtained by fine-tuning with the\nentire dataset (54.58).\nSelecting data by token length can stably obtain a relatively high training benefit, reduce the\nuncertainty caused by randomness, and reduce costs. This approach is particularly beneficial\nfor BASE language models which generally have limited capabilities, as they tend to derive the most\nsignificant benefits from training on longer texts."}, {"title": "6 CONCLUSION", "content": "In this study, we observe that many SFT data selection methods depend on small-scale data sets,\nwhich do not meet the actual needs in real-world scenarios. This finding makes us rethink whether\nSFT data selection methods can work when they are required to handle large-scale IT datasets. We\nreproduce some existing self-scoring data selection approaches that do not need external LLMs'\nsupport on two million-scale datasets and find that almost all present methods do not significantly\nsurpass random selection when dealing with large-scale datasets. Moreover, our analyses show that\nduring the SFT phase, data diversity in data selection plays a more significant role than data quality.\nIn addition, using token length as the quality metric is more appropriate for SFT data selection\ncompared to other carefully crafted quality metrics."}]}