{"title": "Training Verification-Friendly Neural Networks via Neuron Behavior Consistency", "authors": ["Zongxin Liu", "Zhe Zhao", "Fu Song", "Jun Sun", "Pengfei Yang", "Xiaowei Huang", "Lijun Zhang"], "abstract": "Formal verification provides critical security assurances for neural networks, yet its practical application suffers from the long verification time. This work introduces a novel method for training verification-friendly neural networks, which are robust, easy to verify, and relatively accurate. Our method integrates neuron behavior consistency into the training process, making neuron activation states consistent across different inputs in a local neighborhood, reducing the number of unstable neurons and tightening the bounds of neurons thereby enhancing neural network verifiability. We evaluated our method using the MNIST, Fashion-MNIST, and CIFAR-10 datasets across various network architectures. The results of the experiment demonstrate that networks trained using our method are verification-friendly across different radii and different model architectures, whereas other tools fail to maintain verifiability as the radius increases. We also show that our method can be combined with existing methods to further improve the verifiability of networks.", "sections": [{"title": "Introduction", "content": "Neural networks are increasingly applied in safety-critical domains such as autonomous driving (Urmson and Whittaker 2008) and flight control (Julian, Kochenderfer, and Owen 2019). However, they frequently struggle with a lack of robustness, as even minor perturbations to their inputs can lead to incorrect predictions (Bu et al. 2022; Chen et al. 2021; Song et al. 2021; Chen et al. 2023, 2022b; Zhao et al. 2021; Chen et al. 2022a). This is unacceptable in safety-critical applications, where consistent performance is imperative. Thus, it is desirable to develop methods to systematically advance the robustness verification of neural networks.\nExisting methods for analyzing robustness can be divided into two categories: empirical analysis through adversarial attacks and mathematical proof via formal verification. While adversarial attacks generate misleading examples, they merely demonstrate the presence of adversarial samples without affirming their absence. In contrast, formal verification ensures the correctness of neural networks using logical and mathematical methods, allowing us to formally verify the robustness of neural networks. This rigorous verification is essential for safety-critical systems.\nAdvanced formal verification tools (Wang et al. 2021; Zhang et al. 2022a; Bak 2021; Katz et al. 2017), typically employ branch-and-bound algorithms for neural network verification. At the beginning of verification, abstract interpretation (Gehr et al. 2018; Mirman, Gehr, and Vechev 2018; Zhang et al. 2021, 2023; Guo et al. 2021) is usually used to abstract neurons. If the properties of the network remain undetermined after using symbolic propagation (Singh et al. 2019) to calculate neuron boundaries and using MILP (Tjeng, Xiao, and Tedrake 2019; Tran et al. 2020; Zhang et al. 2022b; Zhang, Song, and Sun 2023; Zhang et al. 2024) or SMT methods (Ehlers 2017; Huang et al. 2017; Katz et al. 2017; Zhao et al. 2022; Liu et al. 2024) for constraint solving, further branching is required. The branching process involves enumerating the activation states of unstable neurons, whose activation status cannot be determined through bound calculations, to introduce additional constraints, thereby refining the abstraction. Typically, neural networks contain numerous unstable neurons, and exploring these combinations of activation states requires exponential time, which limits the widespread application of formal verification techniques in practice.\nIn addition to developing ever-more sophisticated methods for post-training verification, researchers have investigated the idea of training neural networks that are easier to verify, as known as verification-friendly neural networks. Ideally, a training method for verification-friendly neural networks must satisfy the following requirements. First, (accuracy) the resultant neural network must have an accuracy that is comparable to neural networks trained conventionally. Second, (robustness) the resultant neural network must have improved robustness, which could be measured using existing adversarial attacks. Third, (verifiability) it must be easier to verify using existing or dedicated neural network verification techniques, which can be measured using the effectiveness of selected neural network verification methods.\nThere are mainly two existing approaches to promoting the verification-friendliness of neural networks. One approach involves post-processing the network through methods that modify the weights of the networks (Xiao et al. 2019; Baninajjar, Rezine, and Aminifar 2023). Another involves altering the neural network design and training process with considerations for verification (Xiao et al. 2019; Narodytska et al. 2019; Xu et al. 2024). However, these methods still have limitations. The effectiveness of post-training methods is limited by the network itself. Moreover, certain post-training methods such as using MILP to make the network more sparse (Baninajjar, Rezine, and Aminifar 2023), are computationally expensive and may not be adaptable for large networks. The ReLU Stable methods, which introduce ReLU Stable (RS) loss (Xiao et al. 2019) to reduce the number of unstable neurons, depend on the bounds of neurons, and when the perturbation radius changes, it often causes these bounds to shift, affecting the verification efficiency of the network. Certified training, mainly based on the heavy Interval Bound Propagation (IBP) method (Mirman, Gehr, and Vechev 2018; Gowal et al. 2018; Zhang et al. 2019a; Xu et al. 2020) suffers from a long training time and the gradient explosion or vanish problem. Adversarial training (Madry et al. 2018; Zhang et al. 2019b; Ganin et al. 2016; Zhu et al. 2017) typically increase network complexity but do not contribute to improving network verifiability.\nIn this work, we introduce a straightforward yet effective training method that enhances the verifiability of neural networks by enforcing the consistency of neuronal behavior, which we refer to as neuron behavior consistency (NBC), throughout the training process as a regularization term. A neuron is called behavior consistent if its activation state remains the same in a given input neighborhood. By maximizing the consistency of neurons, the unstable neurons are decreased, reducing the search space of the verification process. NBC can also help to tight the the bounds of neurons, as fewer unstable neurons introduce less error during bound calculation algorithms. Our approach can be scaled to larger networks compared to MILP-based methods. Moreover, the core of our method lies in the consistency of neuronal behavior without relying on heavy IBP methods, making a reduction in training epochs and ensuring the trained network maintains verifiability across different perturbation radii.\nWe evaluate our method using Fashion-MNIST, MNIST, and CIFAR-10 datasets across different architectures at various perturbation radii. Our method oputperforms in stable neuron ratio and achieved up to a speedup of 450% in verification time. Importantly, our method accelerates the verification while preserving the accuracy of the model, which is not commonly achieved by existing methods. In summary, our contributions are as follows:\n\u2022 We introduce a method of training verification-friendly networks by integrating neuron behavior consistency.\n\u2022 We evaluated our methodology on three well-known datasets. Experiments show that networks trained using our method can maintain verification-friendly properties across different radii and different model architectures.\n\u2022 We demonstrate that our method can be combined with existing methods to further improve the verifiability of networks, especially in the case of large networks.\n\u2022 We show that our method accelerates the verification process while preserving model accuracy and robustness."}, {"title": "Preliminary", "content": "In this section, we introduce the background of neural network verification problems and the general branch and bound verification framework."}, {"title": "Neural Networks Verification Problems", "content": "Given a neural network $f : \\mathbb{R}^{m_{in}} \\rightarrow \\mathbb{R}^{m_{out}}$, with $m_{in}$ input neurons and $m_{out}$ output neurons, the goal of the neural network verification problem is to determine whether the output of the network satisfies a set of output constraints $P$ for all inputs that meet the input constraints $C$, formally defined as:\nDefinition 1 (Neural Network Verification Problem). The neural network verification problem $(f,C,P)$ is to determine whether:\n$\\forall x \\in \\mathbb{R}^{m_{in}}, x \\in C \\Rightarrow f(x) \\in P$,\nwhere $C \\subseteq \\mathbb{R}^{m_{in}}$ is the input constraints and $P \\subseteq \\mathbb{R}^{m_{out}}$ is the output constraints.\nWe focus on the local robustness verification problem $(f, C_{\\varepsilon}(x), P_c)$, which checks if the classification result $c$ is robust to input perturbations $\\varepsilon$ in the norm $l_\\infty$, where the input constraints $C_{\\varepsilon}(x)$ defined as $\\{x \\in \\mathbb{R}^{m_{in}} | |x - x_0| \\le \\varepsilon \\}$ and the output constraints $P_c$ defined as $\\{y \\in \\mathbb{R}^{m_{out}} | \\forall i \\neq c, y_i - y_c \\le 0\\}$."}, {"title": "General Verification Framework", "content": "State-of-the-art methods for solving neural network verification problems (Wang et al. 2021; Zhang et al. 2022a; Katz et al. 2019; Bak 2021) are typically based on branch-and-bound algorithms, consisting of three critical components: constraint solving, bound calculation, and branch selection.\nAs shown in Figure 1, the verification process starts with the bound calculation, where the upper and lower bounds of neuron output are estimated under specified input constraints. If these bounds are sufficiently precise, the properties of the network can be directly verified. Due to non-linear activation functions such as ReLU, computing these bounds can be complex, which is a problem often addressed through neuron-wise abstraction (Singh et al. 2019; Bak 2021). This abstraction uses linear bounds to approximate neuron outputs, thereby simplifying the bound calculations.\nWhen the bound calculation does not suffice to verify the network's properties, constraint solving is employed. This process involves determining if the constraints of the abstracted network can be satisfied, using Linear Programming (LP) or Mixed Integer Linear Programming (MILP) methods. Constraints typically include the input constraints $C$, the negated output constraints $\\neg P$ and the constraints of the abstracted network itself. If these constraints are UNSAT (unsatisfiable), the property holds, which proves that the network can not be successfully attacked within $C$; otherwise, the returned counterexample must be examined. If"}, {"title": "Training Verification-Friendly Networks via Neuron Behavior Consistency", "content": "In this section, we propose our method, neuron behavior consistency, to train verification-friendly neural networks. Given a network $f$ with parameters $\\theta$ and an underlying distribution $D$, the traditional training process aims to optimize the parameters $\\theta$ to minimize the expected loss:\n$\\min_{\\theta} E_{(x,y)\\sim D}(loss(x, y))$,\nwhere $(x, y)$ denotes the input and target label sampled from the distribution $D$, and $y$ is the one-hot vector representation of the target label $y$, that is, a vector with a single 1 at the index of the target label and 0 elsewhere. The most common loss function is the cross-entropy function for classification tasks, defined as $CE(f(x), y) = \\sum_i y_i \\log f(x)_i$.\nThe ordinary training objective does not impose constraints on neurons, potentially resulting in a large number of unstable neurons. We thus propose an alternative training objective that aims to maximize the consistency of neurons. A neuron is consistent if its activation state remains the same in a given input neighborhood. Formally, given an input $x$ and a neighboring input $x'$, the consistency of the $j$-th neuron of the $i$-th layer $n^{(i)}_j$ is defined as:\n$NBC(n^{(i)}_{j},x,x') = \\begin{cases} 1, & \\text{if } sign(f^{(i)}(x)_j) = sign(f^{(i)}(x')_j), \\\\ 0, & \\text{otherwise}, \\end{cases}$\nwhere $f^{(i)}(x)_j$ denotes the pre-activation value of the $j$-th neuron of the $i$-th layer when fed with input $x$. Intuitively, for any input within a given neighborhood, if the activation states of individual neurons are highly consistent, the calculated boundaries (upper and lower bounds) are more likely to be tight. This coherence may reduce the occurrence of unstable neurons in the neural network.\nTo maximize (minimize the negative) this consistency, we incorporate a regularization term into the optimization objective, which can be represented as:\n$\\min_{\\theta} E_{(x,y)\\sim D,x'\\in C(x)} [CE(f(x), y) - \\beta \\sum_{n_i \\in N} NBC(n_i,x,x')]$,\nwhere $\\beta$ is a hyperparameter that controls the importance of the regularization term, and $N$ denotes the set of neurons in the network.\nAdapting concepts from adversarial training, the loss function can be reformulated to maximize the minimal (minimize the negative minimal) consistency of neural behavior across different inputs within the neighborhood domain:\n$\\min_{x' \\in C_{\\varepsilon}(x)} [NBC_{\\varepsilon}(x, y) = CE(f(x), y) - \\beta \\min_{x' \\in C_{\\varepsilon}(x)} \\sum_{n_i \\in N} NBC(n_i,x,x')]$.\nThe consistency metric presented in Equation 3 is a discrete measure that cannot be directly integrated into the loss function. Therefore, we employ a continuous metric to approximate the consistency of neural behavior across different inputs within the neighborhood domain, which is shown in Algorithm 1. This algorithm calculates the NBC between the neural network $f$ when fed with $x$ and $x'$.\nDuring the calculation of NBC, the NBC value s is initially set to zero, then iterating over the hidden layers of f, the algorithm assesses the consistency for each neuron between the original and adversarial images, incrementally updating NBC. The final NBC is calculated as the sum of the scaled neuron consistencies across all layers.\nDue to the characteristics of gradient backpropagation, the layer close to the output layer has a greater impact on the network's behavior. Therefore, we use the KL divergence as a consistency metric for the output layer as a regularization term to ensure that the network's output remains consistent across different inputs, which can be calculated as:\n$KL(f(x)||f(x')) = \\sum f(x)_i \\log \\frac{f(x)_i}{f(x')_i}$\nFor the hidden layers, to prevent gradient explosion or vanishing, we use cosine similarity as a continuous metric"}, {"title": "Evaluation", "content": "In this section, we evaluate our methods to answer the following research questions:\nRQ1: Can networks trained with our method maintain their verification-friendly properties across various network architectures and radii?\nRQ2: Can our method be effectively integrated with existing training methods?\nRQ3: How does our method compare with existing methods under close accuracy ranges?"}, {"title": "Experimental Setup", "content": "Experiments are conducted on a server with 128 Intel Xeon Platinum 8336C CPUs, 128GB memory, and four NVIDIA GeForce RTX 4090 GPUs, running Debian GNU/Linux 10 (Buster). We use Python 3.11.7 and PyTorch 2.1.2 for implementation. Other settings are as follows.\nDataset. Networks are trained on three widely used datasets, MNIST (LeCun et al. 1998), Fashion-MNIST (Xiao, Rasul, and Vollgraf 2017), and CIFAR-10 (Krizhevsky 2009).\nNetwork Architecture. We select neural networks of varying sizes from VNN-COMP (Bak, Liu, and Johnson 2021; M\u00fcller et al. 2023) to assess the effectiveness of the methods used. For the MNIST and Fashion-MNIST datasets, we chose the M1 (cnn_4_layer), M2 (relu_stable), and M3 (conv_big) models, with approximately 0.16M, 0.17M, and 1.9M parameters, respectively. For the CIFAR-10 dataset, we select the C1 (marabou_medium), C2 (marabou_large), and C3 (conv_big) models, containing about 0.17M, 0.34M, and 2.4M parameters, respectively. Network architectures are detailed in supplementary material.\nBaselines. We choose Relu Stable (Xiao et al. 2019) as a baseline, which shares the most similarities with our approach. TRADES (Zhang et al. 2019b) and Madry (Madry et al. 2018), two commonly used adversarial training methods, are selected to show that directly using our method can at least achieve robustness comparable to classical robust training methods and combining our method with existing methods can improve verification performance.\nTraining. We set the batch size to 128, the learning rate to 0.001, and employing the Adam optimizer. For RQ1, networks are trained under default settings for 400 epochs. For RQ2, each network underwent 200 epochs of training with the original method followed by another 200 epochs using a combination of the original and our methods, or in the reverse order. For RQ3, we use CE loss to train a base model and further fine-tune using RS, Madry, Trades, and our method separately to maintain each method's accuracy within a specified range.\nVerification. We use a, \u03b2-CROWN, a state-of-the-art verification tool that performs overall best in VNN-COMP competitions, to verify the properties of the trained networks. For each dataset, we select k images from each of the 10 categories in the test set. For each image x, and ground truth label y, we verified the property that the network's output"}, {"title": "Results of the Last Epoch", "content": "shows the overall evaluation results of networks trained on MNIST, Fashion-MNIST, and CIFAR-10. As some metric values are too large or small, we set the best-performing value as 100 and other values are scaled proportionally to get a better visualization. Note that for metric Time and Timeu+T, we use the reciprocal of the values to make the visualization more intuitive. Therefore, the larger the metric in the table, the better the performance.\nWe observe that our method outperforms others in terms of verification time, stable neuron ratio, and verified ratio (UNSAT%). While our method slightly lags in accuracy and PGD accuracy, it generally maintains comparable accuracy to other methods. This result is a comprehensive evaluation across various perturbation radii and models, indicating that networks of various architectures trained using our method are more verification-friendly at different perturbation radii than those trained using other methods.\n shows the detailed results of networks trained on MNIST. Detailed results of Fashion-MNIST and CIFAR-10 are provided in the supplementary material. As shown in Ta-"}, {"title": "Combination with Existing Methods", "content": "shows the results of networks trained with our method combined with other methods on MNIST dataset. All examples show that our method combined with existing methods improves the ratio of stable neurons. In most cases, our method combined with existing methods increases the ratio of verified properties (UNSAT%) and reduces the time required for verification greatly.\nIt is worth noting that when the baseline method is combined with our method, the performance is particularly outstanding in larger network structures. For example, in the M3 model. Our method combined with Madry and RS methods respectively increased the verified ratio by 83.2% and 87.0% at \u03b5 = 0.2 and also decreased the time by 94.4 seconds and 102.5 seconds. This means that our method makes it possible to verify larger network structures. The same results are also reflected in the Fashion-MNIST and CIFAR-10 datasets provided in the supplementary material.\nCombining our method with existing methods sacrifices some accuracy and adversarial accuracy, but improves verification and neural stability. In particular, the improvement is more pronounced in larger models. This means that our method makes it possible to verify larger network structures using existing verification tools. The same results are also reflected in the Fashion-MNIST and CIFAR-10 datasets provided in the supplementary material.\nAnswer RQ2: Our method combined with other methods demonstrates to improve the verification-friendly properties of the network, however, a trade-off between accuracy and verifiability is required."}, {"title": "Comparison under Close Accuracy", "content": "The accuracy of networks trained using default parameters varied significantly on the CIFAR-10 dataset. To compare the performance of networks with close accuracy, we fine-"}, {"title": "Related Work", "content": "Traditional adversarial methods such as TRADES (Zhang et al. 2019b) and Madry et al. (Madry et al. 2018) use adversarial training to improve the robustness of neural networks. However, these methods most impose constraints on the network output, while our method imposes neuron behavior constraints on all layers of the network. Recent works, HYDRA (Sehwag et al. 2020) integrates the pruning process with adversarial training, using a criterion that considers adversarial robustness during the pruning decision, leading to more compact yet robust models and Wu et al. (Wu, Xia, and Wang 2020) found that adversarial weight perturbation can improve the robustness of the network.\nCertified training (De Palma et al. 2022; Mirman, Gehr, and Vechev 2018; Jovanovic et al. 2022; Zhang et al. 2019a; Xu et al. 2020) introduces interval bound propagation (IBP) into the training process to improve the robustness of the network but suffers from the long training time.\nThe ReLU Stable method (Xiao et al. 2019) ensures that the upper and lower bounds of the neurons have the same sign, pushing the upper and lower bounds away from zero in the same direction. This method can also incorporate ternary loss to train a SAT-friendly network (Narodytska et al. 2019). In contrast, our method emphasizes the stability of neurons before and after perturbation, focusing on the similarity and consistency of neuron behavior. Furthermore, RS loss requires additional calculations for each neuron's bounds, whereas our method does not require extra information, making it easier to implement. Linearity grafting (Chen et al. 2022c) replaces unstable neurons with linear neurons to improve the robustness of the network. However, this method will modify network architectures, while our method keeps the architecture unchanged. MILP-based method (Baninajjar, Rezine, and Aminifar 2023) uses MILP to post-process the network to make it sparse to improve the verification efficiency, which may be time-consuming. Pruning-based methods (Xiao et al. 2019; Xu et al. 2024) heuristically prune the neurons that are inactive or unstable and have little impact on the network's performance. Bias shaping method (Xu et al. 2024) changing the bias of the unstable neurons during training to stabilize them. These methods are either post-processing methods or training tricks that can be incorporated with other methods to further improve the verifiability of networks."}, {"title": "Conclusion", "content": "In this work, we propose a novel training method to train a verification-friendly neural network by preserving the neuron behavior consistency of the network. Our experiments on the MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our method consistently enhances the network's verification-friendliness, as evidenced by a stable neuron ratio, comparable robustness, and faster verification speed across different perturbation radii. Additionally, existing methods, when combined with ours, show improved verification efficiency. Our method also speeds up the verification process while maintaining the accuracy of the model, which is not commonly achieved by existing methods."}, {"title": "Detailed Experimental Setup", "content": "Dataset Selection.\nWe select three datasets: Fashion-MNIST, MNIST, and CIFAR-10 datasets for our experiments. The MNIST and CIFAR-10 datasets are widely used in early adversarial training research (Madry et al. 2018; Zhang et al. 2019b) and similar studies (Xiao et al. 2019; Xu et al. 2024). To demonstrate the generality of our method, we additionally included the Fashion-MNIST dataset. The categories in the Fashion-MNIST dataset are similar to those in the MNIST dataset, but the images in the Fashion-MNIST dataset are more complex and challenging compared to the MNIST dataset.\nNetwork Architecture Selection.\nWe select the following network architectures based on the network sizes used in previous works (Xiao et al. 2019; Xu et al. 2024), the verification capabilities of neural network verification tools, and the generality of the networks. M1 and C1 are smaller networks, M2 and C2 are medium-sized networks, and M3 and C3 are larger networks. Compared to previous works, we mostly use convolutional networks instead of fully connected networks. This is because the use of convolutional networks is more common in real applications. Moreover, M3 and C3 are even larger than the largest network structures used in similar works (Xiao et al. 2019; Xu et al. 2024) and are very close to the limits of neural network verification tools.  provides detailed descriptions of the network architectures used in our experiments.\nTraining Details.\nFor the MNIST and the Fashion-MNIST datasets, we directly use the original pictures to train each network. For the CIFAR-10 dataset, we randomly crop images to 32x32 pixels and augment the data by flipping images horizontally with a probability of 0.5. We also add padding of 4 pixels on each side before cropping.\nWe use the Adam optimizer to train all networks with a batch size of 128. The learning rate is set to 0.0001 for the MNIST and Fashion-MNIST datasets and 0.00001 for the CIFAR-10 dataset. We use the same number of training iterations for all datasets, which is 400 iterations. The random seed is set to 0 for reproducibility. All PGD-like adversarial training process uses the same hyperparameters as 10 steps and a step size of 8/10.\nIn the last epoch experiment, for the RS (Xiao et al. 2019), Madry (Madry et al. 2018), and TRADES (Zhang et al. 2019b) methods, we used the same hyperparameters as in the original papers. As for our model, we use the hyperparameters \u03b2 = 1 for M1, M2 and M3 models, and \u03b4 = 2,5,3 for C1, C2, and C3 models, respectively. We select the hyperparameters based on the performance of the model on the validation set (generated by the original training set).\nIn the combination experiment, we find that different methods have different performances when using different combination orders. Based on the performance of the model on the validation set. For the RS method, we first train 200 epochs using the original RS loss and then combine it with our NBC loss for another 200 epochs. For the Madry method, we use the method combined with NBC loss for 200 epochs and then use the original Madry loss for another 200 epochs. For the TRADES method, we use the method combined with the NBC loss to train directly for 400 epochs. The parameters are the same as the original training process. The application of these methods results mainly in a decrease in accuracy compared to direct training without these methods, which may be unacceptable in practical applications. Therefore, our objective is to evaluate the effectiveness of these methods while maintaining relatively high accuracy. We specified two sets of accuracy ranges for each model: one where the accuracy difference from the 'Natural' method is within 1%-2%, and another where the difference is within 6%-7%. We believe this setup effectively evaluates the effectiveness of each method under different accuracy requirements. Besides, as shown in Table 1 and 5, various metrics drop as the robustness radius increases, making the differences in metrics between various methods less apparent. The use of \u03b5 = 2/255 to train the model and the same \u03b5 to verify the model reveals the differences more clearly and better illustrates the issue. In this experiment, we initially trained a baseline network using only cross-entropy loss (referred to as 'Natural' in the tables and figures). We then used this model as a pre-trained baseline and applied various methods to fine-tune it separately, aiming to achieve a relatively high accuracy.\nVerification Tasks.\nTo demonstrate that our methods have generalization capabilities, we train on the training set and generate local robustness verification tasks under different perturbation radii on the test set. We use the same verification tasks for all methods to ensure fairness."}, {"title": "Detailed Experimental Results", "content": "The following tables provide detailed results of the experiments conducted for RQ1, RQ2, and RQ3. Typically, Table 4, 5 and  show the results of networks trained for RQ1 and RQ2, on Fashion-MNIST and CIFAR-10 datasets, respectively. Table 8 shows the results of networks trained for RQ3 on CIFAR-10 datasets."}, {"title": "Extra Results for RQ1", "content": "Table 4 shows the evaluation results of networks trained in Fashion-MNIST.\nIn the M1 model, our method demonstrates moderate accuracy. Although our method is not the best in terms of UNSAT%, it is within 1% of the best performing RS method. Our method performs best in terms of UNSAT% at \u03b4 = 0.3.\nOur method consistently maintains the best performance in terms of stability, with a stable ratio of over 50%. Our method is the fastest in terms of verification time, and its advantage becomes more pronounced as the perturbation radius increases. Our method performs moderately under the PGD attack.\nIn the M2 model, our method still performs best in terms of Stable%. However, various methods have their advantages and disadvantages.\nIt should be noted that in the M3 model, our method performs best in terms of UNSAT%, Stable%, and the average time of UNSAT and timeout issues. Especially at \u03b5 = 0.3,"}, {"title": "Extra Results for RQ2", "content": "Table 6 and shows the results of networks trained with each method combined with our method on the Fashion-MNIST and CIFAR-10 datasets, respectively.\nOur method significantly increases the ratio of stable neurons, which implies a smaller theoretical upper bound for solving. In most cases, the verification time is greatly reduced after combining our method. The UNSAT% slightly decreases when combined with our method at the smallest \u03b5, but as the robustness radius \u03b5 increases, our method performs almost the best in terms of UNSAT%. It is worth not-"}, {"title": "Discussion for Hyperparameters", "content": "Table 11 shows the results of C1 networks trained with NBC at \u03b5 = 2/255 on CIFAR-10 dataset with different B. We evaluated the models with 50 verification tasks. Overall, as \u1e9e increases, the test accuracy decreases, UNSAT% increases, stable neuron ratio increases, verification time decreases, which is consistent with our expectations. The results show that a larger \u1e9e can improve the verification performance of the network, but it may reduce the accuracy of the network. Therefore, it is necessary to choose an appropriate B according to the specific requirements of the application scenario.\n shows the results of networks trained with different \u03b3[i] on CIFAR-10 dataset and MNIST dataset. We evaluated the models with 50 verification tasks. As discussed in the main paper, neurons in smaller layers, when behavior is consistent, may constrain subsequent layers through propagation. Moreover, layers near the input and output often have fewer neurons, applying constraints to these layers can directly affect the forward or backward propagation process and accelerate the convergence of target loss. In contrast, middle layers typically have more neurons and are responsible for extracting complex features; over-constraining these layers could adversely affect the model's expressive power. The results show that using a looser penalty on layers with more neurons may slightly decrease accuracy but increase the proportion of stable neurons and improve verification speed, which is consistent with our expectations."}, {"title": "Compare with Certified Training", "content": "Table 10 shows the comparison of networks trained with NBC and SABR. We evaluated the models with 50 verification tasks. In the MINST dataset, the network trained with NBC achieves better performance in all metrics compared to the network trained with SABR. In the CIFAR-10 dataset, the network trained with NBC achieves better performance in terms of Test accuracy, UNSAT% and the accuracy under the PGD attack. These results shows that under the same network architecture, our method-despite its lower computational cost and ease of implementation-achieves performance comparable to, or even surpasses, that of certified training methods which demand more computational resources."}]}