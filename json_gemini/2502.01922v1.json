{"title": "LAST STOP For Modeling Asynchronous Time Series", "authors": ["Shubham Gupta", "Thibaut Durand", "Graham Taylor", "Lilian W. Bia\u0142okozowicz"], "abstract": "We present a novel prompt design for Large Language Models (LLMs) tailored to Asynchronous Time Series. Unlike regular time series, which assume values at evenly spaced time points, asynchronous time series consist of timestamped events occurring at irregular intervals, each described in natural language. Our approach effectively utilizes the rich natural language of event descriptions, allowing LLMs to benefit from their broad world knowledge for reasoning across different domains and tasks. This allows us to extend the scope of asynchronous time series analysis beyond forecasting to include tasks like anomaly detection and data imputation.\nWe further introduce Stochastic Soft Prompting, a novel prompt-tuning mechanism that significantly improves model performance, outperforming existing fine-tuning methods such as QLORA. Through extensive experiments on real-world datasets, we demonstrate that our approach achieves state-of-the-art performance across different tasks and datasets.", "sections": [{"title": "Introduction", "content": "An asynchronous time series (also named temporal event sequence or continuous-time event sequence) is a temporally ordered set of events that describe the progression of actions or occurrences. Asynchronous time series are ubiquitous in daily life, such as healthcare (Lorch et al., 2018; Rizoiu et al., 2018), finance (Bacry et al., 2015; Jin et al., 2020), e-commerce (Hernandez et al., 2017), and social media (Zhang et al., 2022; Kong et al., 2023). In each of those domains, predicting the next events plays a crucial role.\nUnlike regular time series, which consist of values at evenly spaced time intervals (like weather measurements), asynchronous time series consist of multiple types of discrete events occurring sporadically over time. For example, in the context of social media platforms like X (Twitter), user interactions (likes, comments, shares, and follows) happen sporadically and at irregular intervals (Zhao et al., 2015). Each such type of interaction with a user's profile represents an event type, and together with their timestamps, form an asynchronous time series (Xue et al., 2024). Modeling such asynchronous time series is challenging due to the irregular timing and the diversity of event types, which contrasts with the uniformity and regularity of traditional time series data (Schirmer et al., 2022; Horn et al., 2020; Zhang et al.).\nTraditionally, to model asynchronous time series, events are grouped into a fixed, small number of categorical types (Xue et al., 2024). Separate stochastic processes\u2014such as Poisson processes or Hawkes processes\u2014are then modeled for each event type to predict which event will occur next and when (Mei et al., 2022; Hawkes, 1971). However, this approach presents several significant drawbacks. Firstly, it inherently limits research to datasets with a small number of event types because modeling each event type separately becomes increasingly computationally intensive as the number of event types grows (Zuo et al., 2020). Secondly, events can vary widely and may not fit neatly into predefined categories. Thirdly, this method leads to the loss of meaningful natural language descriptions associated with the events. Fourthly, these methods treat each event type independently, ignoring any interactions between them for example, likes and shares of a tweet are not independent events. Lastly, extending these methods to other tasks require significant theoretical development (Shchur et al., 2021).\nDeep learning models have significantly revolutionized techniques for time series modeling, and even more so with the introduction of transformers (Vaswani et al., 2017). However, there are often limitations due to the scarcity of training data, overfitting in specific domains, and the highly specialized architectural designs. In response to those challenges, Large Language Models (LLMs) have emerged as a powerful and promising direction to model time series data. For example, Gruver et al. (2023); Zhou et al. (2023); Xue & Salim (2023); Jin et al. (2024) have illustrated how LLMs can be used as time series forecasters when the input time series is encoded as a string of numeric digits, by casting the time series forecasting problem as a next-token prediction in text, hence unlocking the use of powerful pre-trained models. LLMs have also been explored in other domains"}, {"title": "Related Work", "content": "Temporal Point Processes (TPPs). TPPs (Hawkes, 1971; Daley & Vere-Jones, 2007) have emerged as the standard method to model asynchronous time series data. Over the last decade, a large number of neural temporal point processes have been proposed to capture complex dynamics of stochastic processes in time by using neural networks. Du et al. (2016); Mei & Eisner (2017) proposed to use models based on Recurrent Neural Networks (RNNs) to model the sequence of events. Then, more advanced models (Mehrasa et al., 2019; L\u00fcdke et al., 2023) were proposed to better model uncertainty when predicting the future. Recently, several neural TPP models incorporate Transformers in order to improve performance by using attention to better model long-term dependencies. These include the Self-attentive Hawkes process (SAHP) (Zhang et al., 2020), Transformer Hawkes process (THP) (Zuo et al., 2020), and Attentive Neural Hawkes Process (Att-NHP) (Mei et al., 2022).\nTransformers for Time Series. Transformers (Vaswani et al., 2017) have become popular to model regularly-sampled time series because of their ability to capture long-range dependencies and to extract semantic correlations among the elements of a long sequence. Informer (Zhou et al., 2021) introduced a novel self-attention architecture to reduce the quadratic complexity of the original self-attention. Autoformer (Wu et al., 2021) used a novel decomposition architecture with an auto-correlation mechanism to identify more reliable temporal patterns. Crossformer (Zhang & Yan, 2023) proposed a novel architecture to model both the cross-time and cross-dimension dependencies multivariate time"}, {"title": "Background", "content": "Notations. We observe $n$ events over a fixed time interval $[0, T)$, with each event being denoted as $(e,t)$, where $e \\in E$ is the event type (or attributes) and $E$ represents the space of event types. An asynchronous time series is a sequence of events $X_{1:n} = ((e_1, t_1), (e_2, t_2),..., (e_n, t_n))$ where $t_i$ is an increasing sequence in $[0, T)$ that does not necessarily"}, {"title": "Proposed Method", "content": "Unlike ordinary time series, often represented as sequences of numerical values (Gruver et al., 2023), asynchronous time series are represented as sequences of events $x_i = (e_i, t_i)$, where $e_i$ is the event type, and $t_i$ is a representation of the timestamp of this event. Normally, $t_i$ is expressed as an inter-arrival time, which is the time elapsed between event $x_{i-1}$ and $x_i$.\nIn prior work on modeling asynchronous time series (Du et al., 2016; Mehrasa et al., 2019; Zhang et al., 2020; Mei"}, {"title": "Model analysis", "content": "Comparison of SP and StoP learned token representations. Stochastic Soft Prompt (StoP) and Soft Prompt (SP) learn distinct token distributions due to differences in training. Figure 5 shows t-SNE projections of the first 100 tokens from 400-length prompts. We observe that the tokens learned through StoP training are more spread out, indicating greater diversity, while those learned through SP training tend to cluster more closely. StoP follows a coarse-to-fine approach, with early embeddings that are diverse and cover a larger space. This difference is further highlighted in the last plot of Figure 5, where StoP tokens have lower adjacent cosine similarity than SP. As a result, StoP outperforms SP even when using only the first few tokens, with further improvements as more tokens are utilized (Figure 6).\nAll prefixes are valid prompts in StoP The training paradigm of StoP forces all prefixes of StoP to act as valid standalone prompts, as they are used as prompts during training for some batches (if trained for long enough). (see Figure 6). This further strengthens our belief that tokens in StoP are arranged from coarse, independent tokens at the beginning to tokens with tokens containing finer information"}, {"title": "Conclusion and Future Work", "content": "We presented a novel approach to modeling asynchronous time series with an LLM, introducing a flexible alternative to traditional TPP methods. By encoding an asynchronous time series in a prompt, our approach enables LLMs to leverage their world knowledge for various downstream tasks, including forecasting, anomaly detection, and imputation.\nAdditionally, we proposed Stochastic Soft Prompt (StoP), an efficient PEFT technique for adapting LLMs to asynchronous time series data. This approach not only improves adaptability but also suggests broader applicability to other data modalities such as image or natural language sequences.\nOur findings highlight the potential of LLM-based representations for asynchronous time series and suggest new directions for future research, including refining LLM adaptation strategies and exploring hybrid approaches that combine neural architectures with prompt-based modeling."}, {"title": "Appendix", "content": "A.1 Dataset Preparation\nWe remove any sequence in the dataset that is very small (< 4 elements). We split the dataset in a random 70/10/20 train, validation and test split. Each sequence is expanded into multiple sequences based on the task:\n\u2022 Forecasting: We convert a sequence into multiple prediction tasks. For each element of the series, the prediction task is to predict the element given the preceding elements. We impose a minimum and maximum length requirements on the number of preceding elements used.\n\u2022 Imputation: For every element in the series, we replace the element by a mask, and the imputation task is to predict the masked element given the remaining sequence.\n\u2022 Anomaly Detection: For every element in the sequence, we replace the action by a random different action. the anomaly detection task is to identify the element of the sequence that has been tampered with.\nFor the three test based datasets - Breakfast, MultiTHUMOS and EPIC-KITCHENS, the event types are already represented as text. The remaining 5 datasets from the temporal point processes domain lack a textual component, and the event types are represented by integers. For these datasets, we simply treat each integer event type as a string, allowing the LLM to process it similarly to text-based data.\nA.2 Dataset Class Imbalance\nWe observe significant class imbalance in our datasets, as shown in Figure 8 for the Breakfast and MultiTHUMOS datasets. This imbalance motivates our choice of Macro-F1 as the primary metric, as it treats all classes equally, unlike Accuracy, which is heavily influenced by the dominant class.\nA.3 LASTS representation of Asynchronous time series for Zero Shot\nHere we present the LASTS prompt structure for use with LLMs for various tasks. The structure of the LASTS prompts is shown in Figure 2.\nSystem Prompt The system prompt is very similar across tasks, except for the task specific portions of the prompt. The system prompt used for Forecasting is:\nYou are a helpful assistant. Your task is to complete an asynchronous time series. dataset_description. Each series is given in the format (inter_arrival_time, action_name). This indicates that the action_name started inter_arrival_times milliseconds after the start of the previous action or the beginning of time if it's the first action. The allowable actions are: valid_vocab. Given the first few elements of an asynchronous time series, your task is to provide the next action with its inter arrival time as (inter_arrival_time, action_name). You generate all your response as a single python tuple. Be sure to provide only that one python tuple and nothing else."}, {"title": "Evaluating LLM Interaction with LASTS Components", "content": "We considered various variants of framing the LASTS prompt and present a few interesting ones here, evaluated on Breakfast dataset.\nTesting LLMs use of world knowledge We want to test whether LLMs can understand a prompt like LASTS and provide a meaningful response to the task on the sequence using their world knowledge. To this end, we study a variant where each event description is replaced by a uniquely mapped gibberish 4-letter string. This unique mapping ensures that while any semantic meaning in the descriptions is removed, the structure of the time series remains intact.Table 3 shows that all tracked metrics degrade considerably in the scrambled names variant. This confirms that LLMs not only understand LASTS properly but also leverage their world knowledge to perform the specified tasks.\nSequence Representation We probe about the right representation for the time series events - should they be represented as $(e_i, t_i)$ or $(t_i, e_i)$. Our results in Table 4 show that its better to have time first, followed by the event description. This is what we adopt in LASTS.\nTime Representation We investigate if simplifying the series representation would improve LLM performance. For the Breakfast dataset, we replace inter-arrival times with durations, since we hypothesize that most actions occur contiguously for this dataset. We hypothesize that durations may be easier for the LLM to model rather than inter arrival. From the results in Table 5, we observe that while we have a favourable impact on forecast, both imputation and anomaly detection suffer from this change. This suggests that while durations help with forecasting, more precise inter-arrival times are crucial for more involved tasks like imputation and anomaly detection."}, {"title": "LASTS representation used for LLM Adaptation", "content": "For our experients on LLM adaptation, we keep the LASTS representation very similar to our zero shot experiments:\n\u2022 System prompt in this case is a very concise description of just the task. We skip any dataset description as we expect the model to learn that during the fine tuning process.\n\u2022 User prompt is represented as a comma separated sequence of tuples of event description and inter arrival times.\n\u2022 Assistant prompt contains the expected prediction.\nThe exact system prompt used for each of the tasks are as follows:\n\u2022 Forecasting: \"Predict the next element of this asynchronous time series where each element is of the form (inter_arrival_time, action_name).\"\n\u2022 Imputation: \"Predict the element marked 'MISSING' in this asynchronous time series where each element is of the form (inter_arrival_time, action_name).\"\n\u2022 Anomaly Detection: \"One of the element in this asynchronous time series is anomalous, find this element. Each element of the series is of the form (inter_arrival_time, action_name).\""}, {"title": "Baselines", "content": "Random Baseline To evaluate our methods on the three text-based datasets and the three tasks, we establish a random baseline simulating random guesses. For forecasting and imputation, given an input asynchronous time series, the baseline predicts the inter-arrival time as the average of all inter-arrival times in the sequence and selects a random event type from the valid event descriptions. For anomaly detection, it randomly labels an event from the series as anomalous (see Table 10)."}, {"title": "Dataset Class Imbalance", "content": "We observe significant class imbalance in our datasets, as shown in Figure 8 for the Breakfast and MultiTHUMOS datasets. This imbalance motivates our choice of Macro-F1 as the primary metric, as it treats all classes equally, unlike Accuracy, which is heavily influenced by the dominant class."}]}