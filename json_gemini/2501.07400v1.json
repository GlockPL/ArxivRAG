{"title": "DERIVATION OF EFFECTIVE GRADIENT FLOW EQUATIONS\nAND DYNAMICAL TRUNCATION OF TRAINING DATA IN\nDEEP LEARNING", "authors": ["THOMAS CHEN"], "abstract": "We derive explicit equations governing the cumulative biases and\nweights in Deep Learning with ReLU activation function, based on gradient\ndescent for the Euclidean cost in the input layer, and under the assumption\nthat the weights are, in a precise sense, adapted to the coordinate system dis-\ntinguished by the activations. We show that gradient descent corresponds to a\ndynamical process in the input layer, whereby clusters of data are progressively\nreduced in complexity (\"truncated\") at an exponential rate that increases with\nthe number of data points that have already been truncated. We provide a de-\ntailed discussion of several types of solutions to the gradient flow equations. A\nmain motivation for this work is to shed light on the interpretability question\nin supervised learning.", "sections": [{"title": "1. INTRODUCTION", "content": "The importance and technological impact of Machine Learning (ML) and Deep\nLearning (DL) in recent times has been extraordinary, accompanied by steep ad-\nvancements in the design of algorithms, computational implementations, and appli-\ncations across a vast range of disciplines. Nevertheless, a mathematically rigorous\nconceptual understanding of the core reasons underlying the functioning of DL algo-\nrithms (the question of \"interpretability\u201d) has, to a large extent, remained elusive.\nIn fact, it is presently an accepted practice to use DL algorithms as a \"black box\".\nIn this paper, we continue our investigation of the interpretability problem in\nsupervised learning in DL, [2, 3, 4, 5] (joint with P. Mu\u00f1oz Ewald) and [1]. Our\nfocus in the work at hand is to derive the effective equations for the cumulative\nweights and biases, and to understand the action of the gradient flow in terms of a\ndynamical system acting on the training data in the input layer, where we choose\nthe ReLU activation function. We analyze several classes of solutions, and show\nthat the gradient flow in DL is equivalent to the action of dynamical truncations\nin input space, by which clusters of training data are progressively reduced in their\ngeometric complexity; under the right circumstances, they are contracted to points.\nThe latter corresponds to a dynamical realization of neural collapse [12], and leads\nto zero loss training.\nWe will now summarize in more detail the results of this paper. For simplicity\nof exposition, we consider a DL network with equal dimensions in all layers. Ac-\ncordingly, we associate training vectors $x^{(0)} \\in \\mathbb{R}^Q$ with the input layer, and define\nhidden layers, indexed by $l = 1, ..., L$, where recursively,\n$x^{(l)} = \\sigma(W_l x^{(l-1)} + b_l) \\in \\mathbb{R}^Q$. (1.1)"}, {"title": null, "content": "The map in the \u2113-th layer is parametrized by the weight matrix $W_\\ell \\in \\mathbb{R}^{Q \\times Q}$ and\nbias vector $b_\\ell \\in \\mathbb{R}^Q$. We choose the activation function \u03c3 to be ReLU (the ramp\nfunction, $\u03c3(x) = \\max\\{0, x\\}$), and use the convention that its (weak) derivative is\ngiven by\n$\u03c3'(x) = h(x) = \\begin{cases}\n1 & x > 0 \\\\\n0 & x \\leq 0\n\\end{cases}$ (1.2)\nBoth \u03c3 and h are defined to act component-wise on $x \\in \\mathbb{R}^Q$.\nAssuming that all $W_\\ell \\in GL(Q)$ are invertible, we define the cumulative parameters\n$\\begin{aligned}\nW^{(\u2113)} &:= W_\u2113W_{\u2113\u22121} \u00b7 \u00b7 \u00b7 W_1 \\\\\nb^{(\u2113)} &:= W_\u2113 \u00b7 \u00b7 \u00b7W_2b_1 + \u00b7 \u00b7 \u00b7 + W_2b_{\u2113\u22121} + b_\u2113 \\\\\n\\beta^{(\u2113)} &:= (W^{(\u2113)})^{-1} b^{(\u2113)} = \\sum_{j=1}^{\u2113} (W^{(j)})^{-1} b_j\n\\end{aligned}$ (1.3)\nfor $\u2113 = 1, . . . , L$, and\n$\\beta^{(L+1)} := (W_{L+1})^{-1} \\beta^{(L)}$ (1.4)\nin the output layer. Introducing the affine maps\n$a^{(\u2113)}(x) := W^{(\u2113)}x + b^{(\u2113)}$ (1.5)\nwe define the truncation maps\n$\\begin{aligned}\n\\tau^{(\u2113)}(x) &:= (a^{(\u2113)})^{-1} \u25e6 \u03c3 \u25e6 a^{(\u2113)}(x) \\\\\n&= (W^{(\u2113)})^{-1} (\u03c3(W^{(\u2113)}x + b^{(\u2113)}) \u2212 b^{(\u2113)}) \\\\\n&= (W^{(\u2113)})^{-1}\u03c3(W^{(\u2113)}(x + \\beta^{(\u2113)})) \u2212 \\beta^{(\u2113)},\n\\end{aligned}$ (1.6)\nin the same way as in [3]. The \u2113-th truncation maps is the pullback of the activation\nmap under $a^{(\u2113)}$; that is, $a^{(\u2113)}$ maps a vector x in input space to the \u2113-th layer where\n\u03c3 acts on it, and subsequently, $(a^{(\u2113)})^{-1}$ maps the resulting vector back to the input\nlayer. Accordingly, the gradient flow of cumulative weights and biases induces a\ndynamics of time-dependent truncation maps acting on the training data in the\ninput layer.\nWe assume that the reference outputs (labels) are given by $y_\u2113 \\in \\mathbb{R}^Q$, $\u2113 = 1, . . . , Q$,\nand denote the training inputs belonging to the label $y_\u2113$ by $x^{(0)}_{\u2113,i} \\in \\mathbb{R}^Q$, $\u2113 = 1, . . . , Q$,\n$i = 1, . . . , N_\u2113$. We will write $N := (N_1, . . . , N_Q)$.\nAs we will explain in detail in Section 2, the standard $L^2$ cost\n$C_N = \\frac{1}{2} \\sum_{j=1}^{Q} \\frac{1}{N_j} \\sum_{i=1}^{N_j} \\lVert W^{(L+1)} \\tau^{(L)}(x^{(0)}_{j,i}) \u2212 (W^{(L+1)})^{-1} y_j \\rVert_{\\mathbb{R}^Q}^2$ (1.7)\nis defined with the pullback metric in input space with respect to the map $W^{(L+1)}$\nfrom input to output space. In gradient descent algorithms, $W_{L+1}$ (and thus,\n$W^{(L+1)}$) are often treated as dynamical parameters, and the non-Euclidean, time\ndependent metric introduces many of the known complications (\u201dcost landscape\u201d).\nHere, we propose to investigate the Euclidean $L^2$ cost in the input space,\n$C_N := \\frac{1}{2} \\sum_{j=1}^{Q} \\frac{1}{N_j} \\sum_{i=1}^{N_j} \\lVert \\tau^{(L)}(x^{(0)}_{j,i}) \u2212 (W^{(L+1)})^{-1} y_j \\rVert_{\\mathbb{R}^Q}^2$ (1.8)"}, {"title": null, "content": "where we study the gradient flow at fixed $W^{(L+1)}$.\nMoreover, we note that the choice of the activation map \u03c3 distinguishes a specific\ncoordinate system. The polar decomposition of the cumulative weight yields\n$W^{(\u2113)} = |W^{(\u2113)}| R_\u2113$ (1.9)\nwhere $R_\u2113 = |W^{(\u2113)}|^{-1}W^{(\u2113)} \\in O(Q)$ is an orthogonal matrix, and $|W^{(\u2113)}|$ is symmetric.\nAccordingly,\n$|W^{(\u2113)}| = R_\u2113^T W^{(\u2113)}_* R_\u2113$ (1.10)\nwhere $W^{(\u2113)}_*$ is diagonal, and $R_\u2113 \\in SO(Q)$ accounts for the degree of freedom of\nrotating the coordinate system in which \u03c3 is defined. In this paper, we choose the\ncumulative weights to be adapted to the activation in that $R_\u2113 = 1$, so that\n$W^{(\u2113)} = W^{(\u2113)}_* R_\u2113$ (1.11)\nwith $W^{(\u2113)}_* \\geq 0$ diagonal. One then observes that the truncation maps become independent of $W^{(\u2113)}_*$ (a consequence of $(W^{(\u2113)}_* )^{-1}\u03c3(W^{(\u2113)}_* x) = \u03c3(x)$), and that therefore,\n$\\beta^{(\u2113)} \\in \\mathbb{R}^Q$ and $R_\u2113 \\in O(Q)$ parametrize the DL network. The analysis of more general situations including variable layer dimensions and general weights with $R_\u2113 = 1$\nare left for future work.\nWe denote the empirical probability distribution on $\\mathbb{R}^Q$, associated to the \u2113-th\ncluster of training inputs, by\n$\u00b5_\u2113(x) := \\frac{1}{N_\u2113} \\sum_{i=1}^{N_\u2113} \u03b4(x \u2212 x^{(0)}_{\u2113,i}),$ (1.12)\nwhere \u03b4 is the Dirac delta distribution. We write\n$y_\u2113 := (W^{(Q+1)})^{-1} y_\u2113$ (1.13)\nfor notational convenience, with $W^{(Q+1)}$ fixed.\nWe define the notion of cluster separated truncations that accounts for the \u2113-th truncation map acting nontrivially only on training inputs in the \u2113-th cluster,\nbut acting on all other clusters as the identity, $\u03c4^{(\u2113)}(x^{(0)}_{\u2113',i}) = x^{(0)}_{\u2113',i}$ for all $\u2113' \\neq \u2113$.\nThis property was crucially used in [3] and [4]. It requires the supports of $\u00b5_\u2113$,\n$\u2113 = 1, . . . , Q$, to be sufficiently separated from one another.\nWe then prove, in Theorem 3.2, that for cluster separated truncations, the gradient flow for the cumulative weights and biases is given by the effective equations\n$\\begin{aligned}\n\u2202_s(\\beta^{(\u2113)} + y_\u2113) &= \u2212R_\u2113^T J_{0}^{(\u2113)\u22a5} R_\u2113(\\beta^{(\u2113)} + y_\u2113) \\\\\n\u2202_s R_\u2113 &= \u2212 \u2126_\u2113R_\u2113\n\\end{aligned}$ (1.14)\nwhere\n$J_{0}^{(\u2113)\u22a5} = \\int_{\\mathbb{R}^Q \\setminus \\mathbb{R}^Q_{+}} dx \u00b5_\u2113(a_{R_\u2113,\\beta^{(\u2113)}}^{-1}(x)) H^\\bot(x),$ (1.15)\nis a diagonal matrix with\n$\\begin{aligned}\nH^\\bot(x) &= \\mathbb{1}_{Q \\times Q} \u2212 H(x) \\\\\nH(x) &= diag(h(x_i); i = 1, . . . , Q)\n\\end{aligned}$ (1.16)"}, {"title": null, "content": "and\n$\u2126_\u2113 = \\int_{\\mathbb{R}^Q \\setminus (\\mathbb{R}^Q_{+} \\cup \\mathbb{R}^Q_{-})} dx \u00b5_\u2113(a_{R_\u2113,\\beta^{(\u2113)}}^{-1}(x)) \\bigg[H(x), M^{(\u2113)}(x)\\bigg],$ (1.17)\nwhere [A, B] = AB \u2212 BA is the commutator of A, B \u2208 $\\mathbb{R}^{Q \\times Q}$, and\n$M^{(\u2113)}(x) := \\frac{1}{2} \\bigg[ x(\\beta^{(\u2113)} + y_\u2113)^T R_\u2113^T + R_\u2113(\\beta^{(\u2113)} + y_\u2113)x^T \\bigg].$ (1.18)\nIn Section 4, we prove that\n\u2022 The pair $(\\beta^{(\u2113)}, R_\u2113)$ is an equilibrium solution if\n$\\operatorname{supp} \\big(\u00b5_\u2113 \u25e6 a_{R_\u2113,\\beta^{(\u2113)}}^{-1}\\big) \\subset \\mathbb{R}^Q_+$ (1.19)\nor\n$\\operatorname{supp} \\big(\u00b5_\u2113 \u25e6 a_{R_\u2113,\\beta^{(\u2113)}}^{-1}\\big) \\subset \\mathbb{R}^Q_-.$ (1.20)\nIn the first case, $\u03c4^{(\u2113)}$ acts as the identity on the \u2113-th cluster, while in the\nsecond case, the \u2113-th cluster is contracted to a point.\n\u2022 If the initial data $(\\beta^{(\u2113)}(0), R_\u2113(0))$ is such that\n$\\operatorname{supp} \\big(\u00b5_\u2113 \u25e6 a_{R_\u2113(0),\\beta^{(\u2113)}(0)}^{-1} \\cap \\mathbb{R}^Q \\setminus (\\mathbb{R}^Q_+ \\cup \\mathbb{R}^Q_-)\\big) = \\emptyset,$ (1.21)\nand the support of $\u00b5_\u2113 \u25e6 a_{R_\u2113,\\beta^{(\u2113)}}^{-1}$ is suitably geometrically positioned, and\nsufficiently concentrated in $\\mathbb{R}^Q_-$ in a manner that\n$J_{0}^{(\u2113)\u22a5} > 1 \u2212 \u03b7$ (1.22)\nfor a small constant \u03b7, the following holds.\nThe solution of the gradient flow translates $\u00b5_\u2113 \u25e6 a_{R_\u2113(s),\\beta^{(\u2113)}(s)}^{-1}$ into $\\mathbb{R}^Q_-$ in finite time $s = s_1 < \u221e$, and $\\beta^{(\u2113)}(s) \\rightarrow \u2212y_\u2113$ converges exponentially as\n$s \\rightarrow \u221e$. For $s > s_1$, the weight matrix $R_\u2113(s) = R_\u2113(s_1)$ is stationary. In particular, this implies that the entire \u2113-th cluster is collapsed into the point\n$\\beta^{(\u2113)}(s)$ for $s > s_1$.\nThis provides an interpretation of the phenomenon of neural collapse on\nthe level of training data in input space, as computationally evidenced in\n[12]. See also [5, 6].\nIn Section 4.3, we present a detailed analysis of the dynamics of the cumulative\nbias $\\beta^{(\u2113)}(s)$ at fixed $R_\u2113$, and show that it converges exponentially to $\u2212y_\u2113$, at a rate\nthat increases with every additional training input that is truncated.\nIn Theorem 5.1, we derive the gradient flow equations in the general case, without\nthe assumption of cluster separated truncations. They describe dynamical trunca tions of clusters that are renormalized by the intersection of the positive sectors\nof all truncation maps. The geometry of the resulting configurations of data is significantly more complicated than the case discussed above, see for instance [8].\nAn analysis of the dynamics is left for future work.\nIn Section 8, we address two situations in which the gradient flow for the standard cost (1.7) can be explicitly controlled. In Proposition 8.3, we prove, for fully\ncollapsed initial data, that there exist matrix valued integrals of motion providing\na spectral gap that drives the cost to exponentially converge to zero."}, {"title": null, "content": "We remark that the situations considered in this work cover underparametrized\nDL networks as in [3, 4], while overparametrized networks are customarily used in\napplications, [1, 5, 7, 11, 13].\n2. Definition of the mathematical model\nWe consider the setting of supervised learning in a deep network with L hidden\nlayers. We associate the space $L_0 = \\mathbb{R}^{M_0}$ to the input layer, the spaces $L_\u2113 = \\mathbb{R}^{M_\u2113}$\nto the hidden layers, $\u2113 = 1, . . . , L$, and $L_{L+1} = \\mathbb{R}^{M_{L+1}}$ to the output layer, with\n$M_0, . . . , M_{L+1} \\in \\mathbb{N}$.\nWe will specifically assume that the reference outputs (labels) are given by $y_\u2113 \\in \\mathbb{R}^Q$, $\u2113 = 1, . . . , Q$, so that $M_{L+1} = Q$. We denote the training inputs belonging to\nthe label $y_\u2113$ by $x^{(0)}_{\u2113,i} \\in L_0 = \\mathbb{R}^{M_0}$, $\u2113 = 1, . . . , Q$, $i = 1, . . . , N_\u2113$.\nWe will refer to $\\{x^{(0)}_{\u2113,i}\\}_{i=1}^{N_\u2113} \\subset \\mathbb{R}^Q$ as the \u2113-th cluster of training inputs, and will\nuse the multiindex notation $N := (N_1, . . . , N_Q) \\in \\mathbb{N}_0^Q$, with $N := \\sum_{j=1}^Q N_j$.\nThe \u2113-th layer, defined on $L_\u2113 = \\mathbb{R}^{M_\u2113}$, recursively determines the map\n$x^{(\u2113)}_j = \u03c3(W_\u2113 x^{(\u2113\u22121)}_j + b_\u2113) \\in L_\u2113 = \\mathbb{R}^{M_\u2113}$ (2.1)\nparametrized by the weight matrix $W_\u2113 \\in \\mathbb{R}^{M_\u2113 \\times M_{\u2113\u22121}}$ and bias vector $b_\u2113 \\in \\mathbb{R}^{M_\u2113}$. We\nchoose the activation function \u03c3 to be the same for every \u2113. Accordingly, we define\nthe \u2113-th layer cluster averages\n$x_j^{(\u2113)} := \\frac{1}{N_j} \\sum_{i=1}^{N_j} x^{(\u2113)}_{j,i}$ (2.2)\nand deviations\n$\u2206x^{(\u2113)}_{j,i} := x^{(\u2113)}_{j,i} \u2212 x_j^{(\u2113)}$ (2.3)\nfor $j = 1, . . . , Q$. The output layer is associated with the map\n$x^{(L+1)}_j = W_{L+1} x^{(L)}_j + b_{L+1} \\in L_{L+1} = \\mathbb{R}^Q,$ (2.4)\nand includes no activation function. We assume that $M_\u2113 \\leq M_{\u2113\u22121}$ are non-increasing.\nWe denote the vector of parameters by\n$\u03b8 \\in \\mathbb{R}^K, \\quad K = \\sum_{\u2113=1}^{L+1} (M_\u2113 M_{\u2113\u22121} + M_\u2113)$ (2.5)\ncontaining the components of all weights $W_\u2113$ and biases $b_\u2113, \u2113 = 1, . . . , L+1$, including\nthose in the output layer.\nTo begin with, we consider the case $M_\u2113 = Q, \u2113 = 1, . . . , L$, in which the dimensions of the input and hidden layer spaces are all Q.\nAssuming that all $W_\u2113 \\in GL(Q)$, we define the cumulative parameters\n$\\begin{aligned}\nW^{(\u2113)} &:= W_\u2113W_{\u2113\u22121} \u00b7 \u00b7 \u00b7 W_1 \\\\\nb^{(\u2113)} &:= W_\u2113 \u00b7 \u00b7 \u00b7W_2b_1 + \u00b7 \u00b7 \u00b7 + W_2b_{\u2113\u22121} + b_\u2113 \\\\\n\\beta^{(\u2113)} &:= (W^{(\u2113)})^{-1} b^{(\u2113)} = \\sum_{j=1}^{\u2113} (W^{(j)})^{-1} b_j\n\\end{aligned}$ (2.6)\nfor $\u2113 = 1, . . . , L$, and\n$\\beta^{(L+1)} := (W_{L+1})^{-1} \\beta^{(L)}$ (2.7)"}, {"title": null, "content": "in the output layer. Introducing the affine maps\n$a^{(\u2113)}(x) := W^{(\u2113)}x + b^{(\u2113)}$ (2.8)\nwe define the truncation maps\n$\\begin{aligned}\n\\tau^{(\u2113)}(x) &:= (a^{(\u2113)})^{-1} \u25e6 \u03c3 \u25e6 a^{(\u2113)}(x) \\\\\n&= (W^{(\u2113)})^{-1} (\u03c3(W^{(\u2113)}x + b^{(\u2113)}) \u2212 b^{(\u2113)}) \\\\\n&= (W^{(\u2113)})^{-1}\u03c3(W^{(\u2113)}(x + \\beta^{(\u2113)})) \u2212 \\beta^{(\u2113)}.\n\\end{aligned}$ (2.9)\nWe note that\n$a^{(\u2113)}: L_0 \\rightarrow L_\u2113,$ (2.10)\nand\n$\\tau^{(\u2113)}: L_0 \\stackrel{a^{(\u2113)}}{\\longrightarrow} L_\u2113 \\stackrel{\u03c3}{\\longrightarrow} L_\u2113 \\stackrel{(a^{(\u2113)})^{-1}}{\\longrightarrow} L_0.$ (2.11)\nThat is, the vector $x \\in L_0$ in the input layer is mapped to the \u2113-th layer via $a^{(\u2113)}$\nwhere the activation function \u03c3 acts on its image, and is subsequently pulled back\nto the input space via $(a^{(\u2113)})^{-1}$.\nDefinition 2.1. We denote the sets $S^+_\u2113, S^\u2212_\u2113$, defined by\n$S^+_\u2113 := \\{x \\in \\mathbb{R}^Q | a^{(\u2113)}(x) \\in \\mathbb{R}^Q_+\\}$\n$S^\u2212_\u2113 := \\{x \\in \\mathbb{R}^Q | a^{(\u2113)}(x) \\in \\mathbb{R}^Q_-\\}\\$ (2.12)\nas the positive, respectively, negative sector of the truncation map $\u03c4^{(\u2113)}$, and\n$S^\\bot_\u2113 := \\mathbb{R}^Q \\setminus S^+_\u2113.$ (2.13)\nWe say that $x \\in L_0 \\simeq \\mathbb{R}^Q$ is\n\u2022 untruncated by $\u03c4^{(\u2113)}$ if $x \\in S^+_\u2113$,\n\u2022 partially truncated by $\u03c4^{(\u2113)}$ if $x \\in S^\\bot_\u2113$,\n\u2022 fully truncated by $\u03c4^{(\u2113)}$ if $x \\in S^\u2212_\u2113$, and\n\u2022 truncated in the r-th coordinate direction if $(W_\u2113(x + \u03b2^{(\u2113)}))_r \\in \\mathbb{R}_-$.\nMoreover, we say that a set $\\{x_i\\}_i$ is fully truncated or untruncated if all $x_i$ are\nfully truncated, respectively untruncated. Otherwise, we say that $\\{x_i\\}_i$ is partially\ntruncated.\nClearly, if x is untruncated, then\n$\\tau^{(\u2113)}(x) = x \\Leftrightarrow x \\in S^+_\u2113,$ (2.14)\nthat is, $S^+_\u2113 \\subset \\mathbb{R}^Q$ is the fixed point set of $\u03c4^{(\u2113)}$. On the other hand, if x is fully\ntruncated,\n$\\tau^{(\u2113)}(x) = \u2212\u03b2^{(\u2113)} \\Leftrightarrow x \\in S^\u2212_\u2113.$ (2.15)\nThus in particular, if $x \\in S^+_\u2113 \u222a S^\u2212_\u2113$, it follows that $\u03c4^{(\u2113)}(x)$ is independent of $W^{(\u2113)}$,\nand if $x \\in S^+_\u2113$, then $\u03c4^{(\u2113)}(x)$ is also independent of $\u03b2^{(\u2113)}$.\nThen, defining\n$\\tau^{(\u2113)} := \u03c4^{(\u2113)} \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c4^{(1)},$ (2.16)\nthe vectors in the \u2113-th hidden layer are given by\n$x_j^{(\u2113)} = W^{(\u2113)} \\tau^{(\u2113)}(x^{(0)}),$ (2.17)\nfor $\u2113 = 1,..., L$."}, {"title": null, "content": "The vectors in the output layer are obtained by\n$x^{(L+1)}_{j,i} = W_L x^{(L)}_{j,i} = W^{(L+1)} \u03c4^{(L)}(x^{(0)}_{j,i}),$ (2.18)\nfor all $j = 1, . . . , L$, and $i = 1, . . . , N_j$. That is, the vectors $\u03c4^{(\u2113)}(x^{(0)}_{j,i})$ in the input\nlayer are mapped by $W^{(L+1)} = W_{L+1}W_L \u00b7 \u00b7 \u00b7W_1$, via\n$W^{(L+1)} : L_0 \\stackrel{W_1}{\\longrightarrow} L_1 \\stackrel{W_2}{\\longrightarrow} \u00b7 \u00b7 \u00b7 \\stackrel{W_{L+1}}{\\longrightarrow} L_{L+1},$ (2.19)\nto the output layer. We will assume that $W^{(L+1)}$ has full rank.\nIn the input layer, there are two natural metrics associated with this problem.\nThe Euclidean metric on $L_0$ on one hand,\n$\\lVert x \\rVert_{L_0} = \\lVert x \\rVert_{\\mathbb{R}^{M_0}},$ (2.20)\nand on the other hand, the pullback metric under $W^{(L+1)} : L_0 \\rightarrow L_{L+1}$ obtained\nfrom the Euclidean metric on $L_{L+1}$,\n$\\lVert x \\rVert_{L_0,W^{(L+1)}} := \\lVert W^{(L+1)}x \\rVert_{\\mathbb{R}^{M_{L+1}}}$ (2.21)\nwith metric tensor $(W^{(L+1)})^T W^{(L+1)} : L_0 \\rightarrow L_0$.\n2.1. Standard cost is pullback cost in input layer. The standard $L^2$ cost (or\nloss) function is given by\n$\\begin{aligned}\nC_N &= \\frac{1}{2} \\sum_{j=1}^Q \\frac{1}{N_j} \\sum_{i=1}^{N_j} \\lVert x^{(L+1)}_{j,i} \u2212 y_j \\rVert_{\\mathbb{R}^{M_{L+1}}}^2 \\\\\n&= \\frac{1}{2} \\sum_{j=1}^Q \\frac{1}{N_j} \\sum_{i=1}^{N_j} \\bigg\\lVert W_{L+1} x^{(L)}_{j,i} \u2212 W_{L+1}^{-1}y_j \\bigg\\rVert_{\\mathbb{R}^{M_{L+1}}}^2 \\\\\n&= \\frac{1}{2} \\sum_{j=1}^Q \\frac{1}{N_j} \\sum_{i=1}^{N_j} \\bigg\\lVert W^{(L+1)} \u03c4^{(L)}(x^{(0)}_{j,i}) \u2212 (W^{(L+1)})^{-1} y_j \\bigg\\rVert_{\\mathbb{R}^{M_{L+1}}}^2 \\\\\n&= \\frac{1}{2} \\sum_{j=1}^Q \\frac{1}{N_j} \\sum_{i=1}^{N_j} \\lVert \u03c4^{(L)}(x^{(0)}_{j,i}) \u2212 (W^{(L+1)})^{-1} y_j \\rVert_{L_0,W^{(L+1)}}^2\n\\end{aligned}$ (2.22)\nThat is, the standard cost is defined by use of the pullback metric (2.21) in $L_0$\nunder $W^{(L+1)} : L_0 \\rightarrow L_{L+1}$ obtained from the Euclidean metric in the output\nspace $L_{L+1}$. For every $j = 1, . . . , Q$, it measures, relative to the pullback metric,\nthe distance of the points $\u03c4^{(L)}(x^{(0)}_{j,i}), i = 1, . . . , N_j,$ to the preimage of the reference\nvectors (labels), $(W^{(L+1)})^{-1}y_j$.\nTraining of the DL network corresponds to finding global, or at least sufficiently\ngood local minimizers of the cost function. The predominant approach is to employ\nthe gradient flow $\u2202_s\u03b8 = \u2212\u2207_\u03b8 C_N$ in parameter space (2.5), see [1, 5] for a discussion\nof geometric aspects of this problem.\nThis choice of the cost function introduces the following challenges:\n\u2022 It defines a metric in input space with metric tensor $(W^{(L+1)})^T W^{(L+1)}$,\nwhich is itself a time dependent parameter under the gradient flow.\n\u2022 Because of\n$x^{(L)}_{j,i} = W^{(L)} \u03c4^{(L)}(x^{(0)}_{j,i}),$ (2.23)"}, {"title": null, "content": "it follows that the definition of\n$x^{(L+1)}_{j,i} = W_{L+1}W^{(L)}\u03c4^{(L)}(x^{(0)}_{j,i})$ (2.24)\nexhibits the issue that since $W^{(L)}$ is unknown, multiplication with the un known $W_{L+1}$ introduces a degeneracy; namely, the pullback metric in $L_0$\nis invariant under\n$\\begin{aligned}\nW^{(L)} &\\rightarrow AW^{(L)} \\\\\nW^{(L+1)} &\\rightarrow W^{(L+1)} A^{-1}\n\\end{aligned}$ (2.25)\nfor any $A \\in GL(M_L)$. The relevance of including $W_{L+1}$ in the output space\n$L_{L+1}$ is to match $x^{(L)}$ to the reference outputs $y_j$, but including it in the\ndefinition of the pullback metric introduces a redundance.\n\u2022 The presence of $W_{L+1}$ in this form unnecessarily complicates the geometric\nstructure of the gradient flow, as we will see below.\n2.2. Euclidean cost in input layer. For the above reasons, our key objective\nin this paper is to study the gradient flow generated by the Euclidean cost (loss)\nfunction in input space,\n$C_N = \\frac{1}{2} \\sum_{j=1}^Q \\frac{1}{N_j} \\sum_{i=1}^{N_j} \\lVert \u03c4^{(L)}(x^{(0)}_{j,i}) \u2212 (W^{(L+1)})^{-1} y_j \\rVert_{\\mathbb{R}^{M_0}}^2$ (2.26)\nNotably, in (2.26), the reference output vectors $y_j \\in L_{L+1}$ are pulled back to $L_0$ via\n$(W^{(L+1)})^{-1}$. Combined with weight matrices adapted to the activation function \u03c3,\nwe will elucidate the natural geometrical interpretation of the action of the gradient\nflow in input space. It turns out to be quite intuitive and simple; the geometric\nunderstanding thus obtained will open up the path to gradient descent algorithms\nthat do not require backpropagation.\nFor simplicity of exposition, we will assume that the number of hidden layers is\n$L = Q$, and that all layers have the same dimension, $M_\u2113 = Q, \u2113 = 1,...,Q$. The\ngeneral case will be addressed in future work.\nInstead of the parameters $(W_\u2113, b_\u2113)_\u2113$ that are usually used for the gradient flow,\nwe will instead study the gradient flow of the cumulative parameters $(W^{(\u2113)}, \u03b2^{(\u2113)})_\u2113$.\nFor different values of $\u2113,\u2113'$, the cumulative weights and biases $(W^{(\u2113)}, \u03b2^{(\u2113)})$ and\n$(W^{(\u2113')}, \u03b2^{(\u2113')})$ are independent parameters.\n2.3. Weights adapted to the activation. It is important to note that the ac tivation function (which we will think of as ReLU or a smooth mollification of\nReLU) singles out a distinguished coordinate system. Namely, in the \u2113-th layer,\nthe definition of\n$\u03c3: L_\u2113 \\rightarrow L_\u2113, \\quad (x_1,...,x_Q) \\rightarrow ((x_1)_+,\u2026\u2026\u2026, (x_Q)_+)^T$ (2.27)\ndepends on the choice of the coordinate system.\nBy assumption, for $\u2113 = 1, . . ., Q + 1$, the cumulative weight matrix $W^{(\u2113)} : L_0 \\rightarrow L_\u2113$ is an element of $\\mathbb{R}^{Q \\times Q}$ and we assume it to be invertible. It admits the polar decomposition\n$W^{(\u2113)} = |W^{(\u2113)}| R_\u2113$ (2.28)\nwhere $R_\u2113 = |W^{(\u2113)}|^{-1}W^{(\u2113)} \\in O(Q)$ is an orthogonal matrix. Since $|W^{(\u2113)}|$ is sym metric,\n$|W^{(\u2113)}| = R_\u2113^T W^{(\u2113)}_* R_\u2113$ (2.29)"}, {"title": null, "content": "where $W^{(\u2113)"}, "is diagonal, and $R_\u2113 \\in SO(Q)$ maps the eigenbasis of $|W^{(\u2113)}|$ to the orthonormal coordinate system distinguished by the definition of the activation\nmap, (2.27).\nGiven $x \\in L_0$, the map\n$\u03c3(W^{(\u2113)}x) = \u03c3(R_\u2113^T W^{(\u2113)}_* R_\u2113R_\u2113x)$ (2.30)\nallows for a misalignment of the coordinate system (2.27) with the eigenbasis of\n$|W^{(\u2113)}|$. This introduces additional degrees of freedom that account for a rotation,\nvia $R_\u2113$, of the coordinate system in which \u03c3 is defined.\nTherefore, we introduce the following definition.\nDefinition 2.2. We say that the cumulative weight matrix $W^{(\u2113)} : L_0 \\rightarrow L_\u2113$ is\naligned with the activation function \u03c3 if $|W^{(\u2113)}|$ is diagonal in the coordinate system\n(2.27), for $\u2113 = 1, . . . , Q$. That is,\n$W^{(\u2113)} = W^{(\u2113)}_* R_\u2113$ (2.31)\nwith $W^{(\u2113)}_*$ diagonal, and $R_\u2113 \\in O(Q)$.\nWe will see that for weight matrices aligned with the activation function, the\ngradient flow generated by the Euclidean cost in the input layer has a transparent\nform amenable to a clear understanding of the geometry of the minimization process\nvia the dynamical reduction of the complexity of data clusters.\n3. Gradient flow in input space for cluster separated truncations\nIn this section, we prove that gradient descent flow generated by the Euclidean\ncost is equivalent to a time dependent flow of truncation maps in input space\n$L_0$ determined by the averages of input data clusters as they are progressively\ntruncated.\n3.1. Definitions and notations. We define the matrices\n$\\begin{aligned}\nX^{(\u2113)}_j &:= [x^{(\u2113)}_{j,1} \u00b7 \u00b7 \u00b7 x^{(\u2113)}_{j,N_j}"]}