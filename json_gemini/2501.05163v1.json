{"title": "Explainable AI based System for Supply Air Temperature Forecast", "authors": ["Marika Eik", "Ahmet Kose", "Hossein Nourollahi Hokmabad", "Juri Belikov"], "abstract": "This paper explores the application of Explainable AI (XAI) techniques to improve the transparency and understanding of predictive models in control of automated supply air temperature (ASAT) of Air Handling Unit (AHU). The study focuses on forecasting of ASAT using a linear regression with Huber loss. However, having only a control curve without semantic and/or physical explanation is often not enough. The present study employs one of the XAI methods: Shapley values, which allows to reveal the reasoning and highlight the contribution of each feature to the final ASAT forecast. In comparison to other XAI methods, Shapley values have solid mathematical background, resulting in interpretation transparency. The study demonstrates the contrastive explanations-slices, for each control value of ASAT, which makes it possible to give the client objective justifications for curve changes.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial Intelligence (AI) solutions are rapidly expanding and increasingly integrating with conventional systems. However, as the complexity of these models grows\u2014particularly deep learning (DL) models with billions of parameters\u2014their reasoning and outcomes are becoming difficult for humans to interpret and justify [1]. This opaque decision-making process limits the applicability of this promising technology in high-risk fields, such as the aviation industry, healthcare, military, and very large-scale systems like electricity grid, where any error or unanticipated risk could result in unpredictable consequences.\nTo mitigate concerns and reduce risk factors associated with AI models, researchers have introduced Explainable AI (XAI) methods. These techniques bridge the gap between AI-generated outcomes and expert assessments, helping to prevent unintended negative consequences. XAI focuses on creating tools that make black-box models more transparent, clarify complex systems, and improve human reasoning [2]. In this context, two key concepts are interpretability and explainability. Interpretability refers to how easily humans can understand a model's predictions based solely on its structure and design, while explainability focuses on how well humans can reason about the factors influencing the model's predictions [3].\nTechniques such as SHapley Additive exPlanations (SHAP) [4] and Local Interpretable Model-Agnostic Explanations (LIME) [5], aim to clarify the contribution of each feature to the final model decisions and thus enhance the model explainability. For example, DeepLIFT approach is introduced to approximate Shapley values in an recurrent neural network (RNN)-based energy forecasting model to reveal how features at different time intervals influence neuron activation across deep layers, thereby providing explanations for the model's output forecasts [6]. Additionally, XAI techniques can assist in feature selection by detecting and removing unnecessary features, which enhances both the model's performance and its interpretability [7].\nIn commercial buildings, heating, ventilation, and air conditioning (HVAC) systems are the primary energy consumers, and optimizing their control can significantly reduce energy usage. Given that modern HVAC systems are highly complex, involving numerous sensors, actuators, and control parameters, achieving optimal performance requires careful planning and real-time adjustments. As a result, AI models have recently gained considerable attention and demonstrated promising performance in this field. These models are employed for tasks such as forecasting ambient parameters like temperature and humidity, as well as generating control commands to optimize and orchestrate HVAC operations, and diagnosing faults. However, without the aid of XAI tools, the reasoning behind these models' outcomes may be hard to understand for a human operator. This lack of transparency impedes effective human interpretation and evaluation during the control process, limiting the ability to make informed decisions or appropriately respond to issued warnings.\nRecognizing this gap, this paper leverages one of the XAI techniques-Shapley values, to clarify how a regression model predicts automated supply air temperature (ASAT), a critical input for HVAC system control. By employing Shapley values-based approach, the study reveals the influence of various features on the ASAT control curve, offering insights into the physical and/or semantic reasoning behind these forecasts. The study presents contrastive explanations-slices, for each value forecasted, supporting the transparency of any changes in the control curve. The approach presented improves the understanding of modelling results from a physical point of view, providing field experts with valuable knowledge on how different features can affect the ASAT values."}, {"title": "II. TECHNICAL BACKGROUND AND METHODOLOGY", "content": "Univariate time series data are typically composed of single scalar observations that are recorded over equal timestamps. They can be modelled by typical classical methods of statistical analysis, such as Autoregressive Integrated Moving Average (ARIMA) [8], Seasonal ARIMA (SARIMA) [9], Holt-Winters [10], and Kalman filtering [11]. The analysis and modelling of the multivariate data, when multiple variables are recorded over time, is more complex, and classical methods are usually less effective in this case [12].\nThe forecasting of time series using simpler machine learning methods may in some situations be more effective compared to the advanced ones, including neural network models [13]. Work [14] considered RNN and LSTM methods among the less accurate ones, meaning that the complexity does not necessarily guarantee improvements in forecasting performance. To forecast time series, it is necessary to transform historical data into a supervised learning problem. This can be done by using the time steps of previous/historical observations as input values and the next time step observations as outputs. A sliding window method uses the values of the prior time steps to predict the values in the next time steps. This method is also called the lag method. A window width or size of the lag corresponds to the number of previous time steps. A sliding window approach is the basis for how the time series data are transformed to supervised learning. A walk-forward validation method employs true observation values as input and makes the forecast.\n\nA typical way to understand importance of features of parametric models is to consider the model coefficients- weights learned, \\( \\beta_i \\). Depending on the coefficient magnitude, representing the effect of a feature, a change in model output, while changing the feature, can be estimated. However, the model coefficients are not the best way of estimating the overall importance of a feature [17]. This is because the value of each coefficient depends on the scale of the input feature. A prediction \\( f(x) \\), received by a linear model for one data-point \\( x \\) can be represented as:\n\\[ f(x) = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_nx_n. \\]  (1)\nThe i-th feature contribution \\( \\phi_i \\) to the prediction \\( f(x) \\) can be given as:\n\\[ \\phi_i(f) = \\beta_ix_i - E(\\beta_ix_i), \\]  (2)\nwhere \\( E(x_i) \\) is the average effect estimate for the feature i. Considering the sum of all feature contributions for one data-point, it leads to the following:\n\\[ \\sum_{i=1}^M \\phi_i(f) = \\sum_{i=1}^M (\\beta_ix_i - E(\\beta_ix_i)) \\]\n\\[ = \\left( \\beta_0 + \\sum_{i=1}^M \\beta_ix_i \\right) - \\left( \\beta_0 + \\sum_{i=1}^M E(\\beta_ix_i) \\right) \\]\n\\[ = f(x) - E(f(X)), \\] (3)\nwhich is the predicted value for the data-point x minus the average predicted value.\n\nA vector of Shapley values defines the principle of optimal distribution of gain-predicted value, between the players-features in the theory of cooperative games. It represents a distribution, where the gain of each player is equal to its average contribution to the welfare of the total coalition. Thereby, using the Shapley values, all possible combinations and options are revealed and the features, which are relevant, can be identified.\nThe contribution of a feature value, e.g., feature X3 = x31, can be demonstrated using a small example, which makes it possible to interpret the meaning of Shapley value more clearly. For example, the total number of features is 4, and there is a need to determine the contribution of X3 and its value x31. As a first step, the number of all possible coalitions (combinations) with the 3 remaining features is estimated. This way, M coalitions are formed. If in considered \u201ccoalition-1\u201d there are, for example, only 2 features, then X3 = x31 is added to this coalition and the missing fourth feature is selected randomly from other samples. A randomly selected feature acts as a 'donor'. Then, a prediction P1 of the target value is made. As a next step, the value of X3 = x31 is removed and selected randomly from other instances, and a new prediction P2 is made. The contribution of X3 = x31 is equal to the difference between the predictions made at the first and second iterations. Basically, the prediction values depended on a randomly selected X4 = x41 and further changed value of X3 = x32. To estimate the contribution of X3 = x31 more accurately, it is necessary to repeat this sampling step and then average the contributions received. Thereby, the Shapley value represents the average of all contributions of the feature value in all possible coalitions M (combinations of features). It should be highlighted that Shapley value is not the difference in predictions while removing the feature from the feature vector [18].\nThe Shapley value is a value function \\( f(S) \\) of features-players, in the subset-coalition S:\n\\[ f(S) = \\int [f(x_S, X_{\\bar{S}})-E_{x_{\\bar{S}}}(f(X)], \\]  (4)\nwhere \\( x_S \\) represents the values of the features that are in coalition S, \\( X_{\\bar{S}} \\) is the vector of feature values to be explained and considered as complimentary/unobserved features in random variables, i.e., those features that are not in the coalition S, and \\( X \\) is the random variable in the background dataset [18], [19].\nEquation (4) performs multiple integrations for each feature that is not included into the coalition S. For example, the model includes four features, i.e., X1, X2, X3, X4 and the coalition considered includes two feature values: x11, x21, which leads to the following expression:\n\\[ f(\\{1,2\\}) = \\int\\int [f(x_{11}, x_{21}, X_3, X_4)dP_{X_3X_4}-E_X(f(X)). \\] (5)\nNote that equation (5) is similar to the one defined for the linear model, i.e., (3). Essentially, the evaluation of a feature value occurs by averaging the differences in predictions made when fixing the values of the coalition features and randomly sorting through the values of features that are not included in the coalition. On top of that, all possible combinations of coalitions should be taken into account.\nOne of the advantages of Shapley values compared to other methods, e.g., LIME, is its efficiency. This means that the difference between the actual prediction and the average prediction is fairly distributed among the feature values:\n\\[ \\sum_{i=1}^M \\phi_i = f(x) - E_x(f(X)). \\] (6)\nThe axioms of efficiency, symmetry, dummy, and additivity underlying the Shapley values provide a theoretical basis for feature value explainability [18]. The calculation of Shapley values require high computer resources due to \\( 2^M \\) of all possible coalitions of the feature values. This shortcoming, however, can be overcome by computing the contributions for a few samples of the possible coalitions, which reduces the number of iterations but increases the variance of Shapley values. As an alternative, Shapley value can be approximated using the Monte-Carlo sampling.\nSHAP (SHapley Additive exPlanations) method splits the forecast into parts to reveal the contribution of each feature [4], and is based on the Shapley vector. However, it represents the Shapley value explanation as a linear model of coalitions:\n\\[ g(x') = f(x) = \\Phi_0 + \\sum_{i=1}^M \\Phi_ix_i \\]  (7)\nIn case \\( \\Phi_0 = E_x(f(X)) \\) and the coalition vector x is set to all ones, meaning that all feature values are present. Then, the Equation (7) turns to a form of Shapley efficiency property (6):\n\\[ f(x) = \\Phi_0 + \\sum_{i=1}^M \\phi_ix_i = E_x(f(x)) + \\sum_{i=1}^M \\phi_i, \\] (8)\nwhere \\( \\phi_i \\) are the Shapley values. The main difference of SHAP from the classical Shapley values is the coalition vector, that is, if a feature is present in the coalition, then its taken as equal to 1, if it is absent, then 0. Thus, the coalition vector is a vector of zeros and ones. The function \\( h_x(z') = z \\), where \\( h_x: \\{0,1\\}^M \\rightarrow \\mathbb{R}^M \\) maps a coalition such that for the present features the corresponding/actual value is assigned and for the absent features the values are assigned by random feature values from the data.\nThe model predictive control of ASAT employs sliding window approach, which can be interpreted as Iterative Learning Control (ILC), meaning that the next iteration of a system component is predicted using its historical data. This way the learning is implemented via learning from the past [20]. In the present study the feature vector of a data point included the Ambient Temperature (AT), Room Temperature average (RT-avg) and history of ASAT values. The configuration and encoding of the feature vector is presented in Fig. 2."}, {"title": "IV. NUMERICAL RESULTS AND ANALYSIS", "content": "Explainability of a control curve of ASAT was made based on the forecast for the October 10, 2024, which was a working day. The forecasted and true ASAT control values, as well as the difference between the control curves are shown in Fig. 4. The maximum difference between the true (y = 20.1) and forecasted (y = 18.00) values of ASAT occurred at 14:45 and was equal to 2.1 [\u00b0C]. Reviewing the distributions of model parameters of ASAT, the following highlights can be marked:\nRelevant features (referring to the absolute order of magnitude) appear in the beginning and end of the feature vector, indicating the relevance of: AverageRoomTemperature and ASAT historic data corresponding to the end of the working day (only operational hours of AHU were considered while building the model, i.e., 8:00\u201317:00);\nAn interesting observation is the lower effect of AmbientTemperature.\nEasy explanation and visualisation of ASAT values forecasted considering the 2D dynamic array of features can be made by shap.plots.waterfall(), see [4]. The number of such explanations and visualisation slices is equal to 37, i.e., daytime hours from 08:00-17:00 with a resolution 15 min. Figure 6 demonstrates the importance of the features for forecasting the ASAT value at 9:45. The feature importance given in Fig. 6 using the semantic explanation can be given in the form represented in Table I.\nAn interpretation of, for example, a value forecasted for the daytime at 9:45, having the relevant features, such as: 'f37_2D:16:45', 'f9_1D:9:45' can be as: those are the thirty-seventh and ninth features in the ordered feature vector (counting starts from '0'), which correspond to ASAT historical values of the timestamps at '16:45 minus 2-DAYS' and '9:45 minus 1-DAY', respectively. Using the semanitic explanation given above, it can be represented as: ['9:45', ('f37-2D:16:45', 'f9-1D:9:45')]. The possible explanation for the matter of maximum difference maybe be as: the feature that played the most significant role for the forecast was equal to ASAT temperature of the previous day at the same and 15 min earlier timestamps."}, {"title": "V. CONCLUSIONS", "content": "The study presented answered the following questions:\nForecast explanations produced by shap.plots.waterfall() made it possible to receive the local-contrastive explanations, of forecasts: slices. This added a clarity to each forecast point of the ASAT curve. Besides, the slicing explanations made it possible to justify from the physical and semantic point of views any control curve changes by making this functionality as a customer friendly service.\nIn comparison with LIME, SHAP, i.e. application of Shapley values, always use all the features to explain the prediction. The efficiency property of Shapley value allows to estimate the difference between the final and the average predictions, which is fairly distributed among the contributions of the feature values. LIME, in turn, does not allow such transparency.\nSHAP enabled for global model interpretations with all features always present, whereas in the case of LIME only local explanations are possible."}]}