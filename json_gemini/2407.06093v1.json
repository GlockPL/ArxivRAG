{"title": "Artificial Intuition: Efficient Classification of Scientific Abstracts", "authors": ["Harsh Sakhrani", "Naseela Pervez", "Anirudh Ravi Kumar", "Fred Morstatter", "Alexandra Graddy Reed", "Andrea Belz"], "abstract": "It is desirable to coarsely classify short scientific texts, such as grant or publication abstracts, for strategic insight or research portfolio management. These texts efficiently transmit dense information to experts possessing a rich body of knowledge to aid interpretation. Yet this task is remarkably difficult to automate because of brevity and the absence of context. To address this gap, we have developed a novel approach to generate and appropriately assign coarse domain-specific labels. We show that a Large Language Model (LLM) can provide metadata essential to the task, in a process akin to the augmentation of supplemental knowledge representing human intuition, and propose a workflow. As a pilot study, we use a corpus of award abstracts from the National Aeronautics and Space Administration (NASA). We develop new assessment tools in concert with established performance metrics.", "sections": [{"title": "1 Introduction", "content": "Analyzing technical documents is a crucial strategic task, enabling the management of research portfolios, tracking investment trends, and exploring scientific advancement. On a more tactical level, it can aid the preliminary screening of scientific abstracts in systematic reviews (Buchlak et al., 2020; Rios and Kavuluru, 2015; Ambalavanan and Devarakonda, 2020).\nSeveral approaches are possible. First, authors can label their own work, but this presents several challenges: (1) authors that self-label their own texts may make idiosyncratic decisions, (2) authors in close disciplines may use different terms for related concepts, such as \u201crobotics\u201d and \u201cautonomy\", and (3) multidisciplinary projects may require novel or multiple labels.\nA second method is to impose an external ontology. However, these schemes often have both fine- and coarse-granularities (e.g., \u201cnetworks\u201d versus \"ad-hoc networks\"). Another concern is that the scheme simply lacks appropriate labels, especially for emerging fields.\nAutomated processes do exist. Those with a large number of parameters are now customizable at lower computational cost (Hu et al., 2021; Ben Zaken et al., 2022). Although dedicated pre-trained models can yield robust results, they incur significant expenses in manual annotation due to reliance on supervised learning (Beltagy et al., 2019; Chang et al., 2008; Cohan et al., 2020).\nIn summary, we face two distinct needs in the analysis of scientific documents: (1) a unified, coarse-grained, non-overlapping taxonomy, tailored to uniquely classify a set of documents; and (2) an unsupervised methodology that circumvents the reliance on manual annotation while effectively managing the peculiarities of scientific text. These challenges are exacerbated for abstracts.\nIn manual labeling, an expert's rapid progress often hinges on integrating prior knowledge, crucial for effective comprehension (Reid Smith and Hammond, 2021). In so doing, the expert rapidly identifies the phrases conveying the most information and uses those for classification. Importantly, this process is not a simple frequency or statistical analysis; indeed, the most important phrase may appear only once. Moreover, multigrams carrying high semantic value may not appear systematically in the same place in a sentence or paragraph.\nHere we describe \u201cartificial intuition,\" a method mimicking the expert's process to execute two objectives: generating an optimal label space and producing accurate predictions within this new space. We integrate tools into a novel workflow to identify important terms, augment them with relevant background information, then aggregate these enhanced documents into clusters for classification purposes.\nAs a pilot case to evaluate our methodology, we analyze award abstracts of federally funded projects from the National Aeronautics and Space Administration (NASA) Small Business Innova-\""}, {"title": "2 Related Work", "content": "Various methodologies have been proposed for text classification. Bayesian approaches (Tang et al., 2016) classify the text by extracting features. One method is to first select document features with discriminative power, then compute the semantic similarity between features and documents (Zong et al., 2015), but this becomes more difficult as the number of features grows. Support Vector Machines (SVMs) can be used for document classification (Cai and Hofmann, 2004). However, these approaches are constrained by the requirement for manual feature engineering, limiting their ability to capture the complexity of natural language.\nNew deep learning techniques have advanced scientific document classification. Neural network-based architectures (Lee and Dernoncourt, 2016), particularly Convolutional Neural Networks (CNNs) (Sun et al., 2019) and Recurrent Neural Networks (RNNs) (Xun et al., 2019; Liu et al., 2016), outperform some traditional machine learn-"}, {"title": "3 Approach", "content": "3.1 Problem Formulation\nThe scientific literature tagging task can be conceptualized as a multi-label classification (each paper can be relevant to more than one label) problem, where all candidate tags (e.g., \u201cAerodynamics,\u201d \"Superconductance/Magnetics\u201d) constitute a label space Y of arbitrary size. We seek to:\n\u2022 Construct a new label space Y comprising coarse-grained labels and aggregating correlated labels (e.g., merging \u201cOptics\" and \"Photonics\" into \"Optical technologies\").\n\u2022 Develop an unsupervised multi-label classifier that can effectively map an abstract to the new\""}, {"title": "3.2 Implementation Components", "content": "\u2022 Yet Another Keyword Extractor (YAKE) (Campos et al., 2020) is a lightweight, unsupervised keyword extraction algorithm that uses statistical properties and contextual information.\n\u2022 Mistral 7B is a Large Language Model (LLM) with strong performance of Llama-2 13B on key benchmarks (Jiang et al., 2023).\n\u2022 Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) iteratively selects candidate items that simultaneously maximize their relevance to the query and their novelty compared to previously selected items.\n\u2022 Sentence Transformer (S-Transformer) (Reimers and Gurevych, 2019) constructs dense vector representations of sentences to enable efficient comparison of text semantics."}, {"title": "3.3 Document Corpus", "content": "The NASA SBIR program publishes abstracts of funded projects. We used 1,230 abstracts from 2010 to 2015 extracted online from the publicly available archive. The average abstract length is about 450 words. All abstracts were pre-processed by removing stop words, which were variations and combinations of: \u201cNASA\u201d, \u201cspace\u201d, \u201cmission(s)\u201d, \"research", "SBIR": "spacecraft", "future": "and \"science\". These words and multigrams comprising these words appeared in a large number of the abstracts, and therefore they provided little information to assist in classification. We randomly drew 100 abstracts (roughly 10%) for manual classification, described below."}, {"title": "3.4 Label Space Generation", "content": "We generate the label space as illustrated in Figure 1. Initially, pre-processed abstracts are submitted to YAKE to extract keywords. One hyperparameter of our workflow is the number of keywords \u0109. We estimated that \u0109 should be approximately 5 as it represents 1-2% of the abstract length. We confirmed that the F1 results, described in more detail below, showed a general lack of sensitivity to this parameter (Figure 2), and therefore we set \u0109 = 5 in our main analyses.\nWe sought to supplement these keywords with contextual definitions to form metadata. We used Mistral-7B Instruct v0.2 with hyperparameters set at default values and submitted the following prompt:\nThis combined data-keyword concatenation is processed using the S-Transformer model to produce embeddings. A critical aspect of this process is that the metadata generated for each keyword is tailored specifically to the context of the related document, ensuring that the embeddings are context-specific rather than generic. We use k-means clustering (Habibi and Popescu-Belis, 2015) to partition these embeddings into clusters, represented by the keyword closest to their centroids and"}, {"title": "4 Results", "content": "4.1 Label Space Orthogonality: Redundancy\nOur first task is estimate the degree of overlap within the label space. To do so, we define the redundancy, $R$, as a measure of the orthogonality between labels. This figure of merit (FOM) is intrinsic to the label space and assessed independently of individual document projections.\nThe labels are transformed into normalized embeddings using the S-Transformer model, resulting in a label matrix $L$ of dimensions $k \\times v$ (in our case, $v = 768$). Each element $L_{ij}$ represents the j-th dimension of the i-th label embedding.\nTo measure the orthogonality, we calculate the cosine similarity between each pair of distinct label embeddings. If the labels are orthogonal and distinct, the cosine similarity should approach 0; on the other hand, two labels capturing closely related ideas will give a cosine similarity that approaches 1. Formally, for normalized label vectors $T_i$ and $T_j$ in $L$, we define redundancy $R$ as the maximum cosine similarity among all pairs:\n$R = \\max_{i\\neq j} (\\text{cosine similarity}(T_i, T_j))$ (1)\nwhere\n$\\text{cosine similarity}(T_i, T_j) = \\frac{T_iT_j}{||T_i||||T_j ||}$\nA value of $R$ close to 0 is desirable because orthogonal label embeddings suggest that each label contributes unique information without redundancy. Conversely, a value of $R$ approaching 1 shows that at least one pair of labels shares a high degree of overlap. Overlap implies that multiple labels may be describing similar features within the documents, thus complicating the interpretability and utility of the label space. Our goal is to represent each key concept with a unique label.\nTo understand the redundancy in our basis vector set, we executed the label space generation process"}, {"title": "4.2 Spanning the Document Space: Coverage", "content": "We defined the redundancy $R$ to characterize the orthogonality of our proposed label space basis vectors. Next, we study how comprehensively these labels describe the documents, essentially determining if our labels can span the document space.\nWe architected a second workflow (Figure 4). Again we begin with YAKE usage for a single document. We submit these keywords to Mistral-7B for document-specific contextual definitions as supplementary metadata. Both the document itself and the keyword-metadata concatenations are subsequently processed through the S-Transformer model to generate their individual embeddings, refined using MMR. This forms a new keyword embedding matrix $C$ of dimensions $v \\times \\hat{c}$, where $v$ (768 in our case) represents the embedding dimension, and the extracted keywords are still parameterized by $\\hat{c}$.\nLikewise, we still have the label embedding $L$ of dimensions $k$ x $v$. As our goal is to understand the overlap between the labels and the corpus embeddings, we define a new matrix, termed \u201ccoverage\", $W$ with elements $w_{ij}$:\n$w_{ij} = \\sum_{v} L_{iv} C_{vj}$ (2)\nThe coverage matrix $W$ has the resulting dimension $k$ x $\\hat{c}$, where $k$ represents the number of labels and $\\hat{c}$ represents the number of keywords. In other words, $W$ is the projection of the keywords onto the label space. (Strictly speaking, the S-Transformer embeddings of length $v$ can be understood as creating a coordinate system to facilitate projections.) Each $w_{ij}$ element ranges from -1 to 1.\nA high value of any element $w_{ij}$ indicates that a label and keyword are highly aligned. Therefore, finding the maximum value that appears in this matrix $W$ will signify how well the label space describes the keywords of an individual document in the best case. Consequently, we define the coverage $S$ for a given document $d$ (where $d$ is a member of the document corpus $D$):\n$S_d = \\max(w_{ij})$ (3)\nThe coverage for the corpus $D$ is simply the average of the individual documents' coverage:\n$S_D = \\frac{\\sum_D S_d}{D}$ (4)\nThis proposed figure of metric, coverage, provides critical validation that the new label space is pertinent to the knowledge domain encompassed by the documents. One would expect for coverage to be small if the label space is not large enough namely, for small values of $k$. An intermediate regime would appear if each new label adds significant new information. Eventually, a final regime would be reached wherein the new information provided by an additional label is marginal as the segmentation becomes finer, such as comparing 'Chemical Propulsion Technologies' and 'Electronic Propulsion Technologies'. In other words, a general analytical form for coverage should start near 0, then experience rapid growth until the space is largely covered and it tapers off. The corpus coverage is bounded by 1 because the individual documents' coverage is given by a cosine similarity of two normalized vectors, thus limited to 1."}, {"title": "4.3 Label Assignment: Precision and Recall", "content": "We seek to create a label space with high coverage, indicating relevance; and low redundancy or overlap. However, these measures act in opposition as higher coverage naturally can lead to greater redundancy. That is, these two measures form a trade space in which we strive to optimize $k$.\nWe revisited the workflow generating the coverage matrix (Figure 4) and developed a prediction"}, {"title": "5 Discussion and Future Research", "content": "Scientific communication is designed to efficiently carry rich information between experts. The abstract of a grant or publication is perhaps the most striking example, wherein sophisticated concepts"}, {"title": "6 Conclusion", "content": "For labeling short scientific documents, such as abstracts, pre-existing domain-specific taxonomies are ambiguous. Defining a label space spanning the set of documents is an important task that humans execute easily. In this paper, we demonstrate that the text of the documents is insufficient to either define the label space or predict the labels. We present evidence that an LLM can provide critical metadata to address this gap, forming the basis for artificial intuition. Additionally, we propose both an architecture to address this and two novel measures to evaluate the constructed label spaces. Testing our model with a corpus of NASA award abstracts, we demonstrate a workflow that integrates the LLM's supplemental data successfully."}]}