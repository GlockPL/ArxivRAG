{"title": "Improving Representation of High-frequency\nComponents for Medical Foundation Models", "authors": ["Yuetan Chu", "Yilan Zhang", "Zhongyi Han", "Changchun Yang", "Longxi Zhou", "Gongning Luo", "Xin Gao"], "abstract": "Foundation models have recently attracted significant attention for their impressive generalizability across diverse\ndownstream tasks. However, these models are demonstrated to exhibit great limitations in representing high-frequency components\nand fine-grained details. In many medical imaging tasks, the precise representation of such information is crucial due to the inherently\nintricate anatomical structures, sub-visual features, and complex boundaries involved. Consequently, the limited representation of\nprevalent foundation models can result in significant performance degradation or even failure in these tasks. To address these\nchallenges, we propose a novel pretraining strategy, named Frequency-advanced Representation Autoencoder (Frepa). Through\nhigh-frequency masking and low-frequency perturbation combined with adversarial learning, Frepa encourages the encoder to\neffectively represent and preserve high-frequency components in the image embeddings. Additionally, we introduce an innovative\nhistogram-equalized image masking strategy, extending the Masked Autoencoder (MAE) approach beyond ViT to other architectures\nsuch as Swin Transformer and convolutional networks. We develop Frepa across nine medical modalities and validate it on 32\ndownstream tasks for both 2D images and 3D volume data. Without fine-tuning, Frepa can outperform other self-supervised pretraining\nmethods and, in some cases, even surpasses task-specific trained models. This improvement is particularly significant for tasks\ninvolving fine-grained details, such as achieving up to a +15% increase in dice score for retina vessel segmentation and a +7%\nincrease in loU for lung nodule detection. Further experiments quantitatively reveal that Frepa enables superior high-frequency\nrepresentations and preservation in the embeddings, underscoring its potential for developing more generalized and universal medical\nimage foundation models.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, the rapid development of medical foundation\nmodels has attracted considerable attention and emerged\nas a novel paradigm in artificial intelligence (AI). These\nmodels typically involve pretraining on unlabeled datasets\nusing self-supervised learning techniques [1]\u2013[4] or employ\npaired images, annotations, and text for supervised train-\ning [5], [6]. By training on large-scale datasets, foundation\nmodels can exhibit emergent capabilities [7] and facilitate\nimproved generalization by transferring the knowledge ac-\nquired during the pretraining phase to specific tasks or data\nsources. In the field of healthcare, many foundation models\nhave achieved significant progress across various medical\ntasks [4]-[6], [8], [9], marking a substantial advancement in\nmedical Al research.\nHowever, both our findings and previous works sug-\ngest that these foundation models often result in limited\nrepresentations of high-frequency components and fine-\ngrained details [5], [10], [11]. Fig. 1 provides two illus-\ntrative examples. Fig. 1a shows the classification accuracy\nof different models on the MedMNIST dataset [12] after\napplying high-pass filters of varying sizes. The performance\nof these models degrades rapidly with increasing filter size,\nsuggesting an over-reliance on low-frequency components\nand a limited ability to capture high-frequency components.\nFig. 1b showcases the segmentation and detection results\nof tasks involving fine-grained details using the fine-tuned\nMedSAM [5] and our proposed methods. This highlights\nthat even segmentation-specific foundation models struggle\nwith fine-grained structures. In diagnostic practices, many\ndiseases and lesions manifest as subtle differences from\nnormal tissue or other disease subtypes, making accurate\ndisease identification heavily reliant on detailed informa-\ntion confined to small regions [13]. Moreover, anatomical\nstructures such as small lesions, blood vessels, and fibrotic\ntissues, which are critical for both physiological functions\nand disease diagnosis [14], [15], also involve high-frequency\nand fine-grained information. The limited representation\nof such information is an inherent shortcoming of current\nfoundation models, leading to poor performance or even\nfailure in these tasks and reducing their generalizability to\nreal-world medical applications.\nThe limitations in high-frequency representation are pri-\nmarily due to the dominant role of global information in\ncurrent training strategies. For instance, autoencoder pre-\ntraining methods like the masked autoencoder (MAE) [1]\ntypically produce reconstructions that capture only general\nsemantic information while lacking specific details [11].\nIn fact, during the reconstruction process, low-frequency,\nglobal information tends to be more substantial, causing the\nmodel to emphasize these components over high-frequency\ndetails [16], resulting in limited detailed information. A\nsimilar issue is observed in contrastive learning-based meth-"}, {"title": "2 RELATED WORK", "content": "The recent advancements in foundation models signify a\ntransformative leap in deep learning, showcasing excep-\ntional generalization and transfer learning capabilities. The\nmulti-modal visual foundation models based on contrastive\nlearning, such as CLIP [3], Flamingo [22], and LLaVA [23],\nhave made significant strides in efficient transfer learning\nand exhibit impressive zero-shot performance. These mod-\nels, however, rely on large-scale image-text paired datasets\nand are limited in their applicability to other visual tasks like\nsegmentation or image restoration. On the other hand, au-\ntoencoder pretraining strategies enable the network to learn\nmeaningful feature representations from large-scale data in\na self-supervised manner via reconstructing raw images\nfrom corrupted images. Among these approaches, MAE [1]\nis particularly notable, which pretrains a ViT encoder by\nrandomly masking parts of an image and reconstructing\nthe original image from the remaining patches. However,\nrecent studies have highlighted the limitation of autoen-\ncoders in that they tend to focus more on low-frequency\ninformation and fail to effectively utilize the full spectrum\nof frequency information [10], [11]. This issue is especially\ncritical in medical imaging tasks, where accurate diagnosis\nand numerous physiological structures often depend on\nhigh-frequency components and fine-grained details. To the\nbest of our knowledge, Frepa is the first study to system-\natically investigate the frequency representation capabilities\nof foundation models and to validate the feasibility in the\nmedical field."}, {"title": "2.2 Medical foundation model", "content": "As foundation models for natural images have achieved\nsignificant success, their applications in the medical field are\ngaining increasing attention. Current research on medical\nfoundation models can be broadly classified into specialist\nmodels and generalist models. Specialist models are typ-\nically designed to address clinical tasks within a single\nimaging modality or specific disease type. Examples include\nUSFM [4] for ultrasound, RETFound [24] for retinal images,\nand EndoFM [25] for endoscopy videos. Although specialist\nmodels can achieve higher accuracy and efficiency within\ntheir specific modalities, they often face challenges in gener-\nalizing to other modalities. In contrast, generalist models,\nsuch as MedSAM [5], MedDr [6], and LLAVA [23], offer\nbroader applicability and generalizability across various\nmodalities and tasks. These methods rely on large amounts\nof training data, computational resources, and parameters,\nachieving progress in generalization. However, in the pro-\ncess of transferring pretraining techniques from natural\nimages, these models often don't adequately consider the\ndata characteristics of medical images, such as fine anatom-\nical structures, multi-scale objects, and complex, uncertain\nboundaries. Consequently, they may perform poorly on\nmedical imaging tasks that depend on fine-grained details\nand high-frequency information [5], [6] (Fig. 1), posing a\nsignificant barrier to their practical application in clinical\nsettings. In this work, we focus on lightweight generalist\nmodels and introduce a novel pretraining strategy designed\nto enhance the ability of foundation models to represent\nhigh-frequency information. Our approach aims to improve\nperformance on fine-grained tasks, thereby extending the\npractical applicability and generalizability of these models\nin clinical practice."}, {"title": "3 METHODS", "content": "To enhance image understanding and high-frequency rep-\nresentation, we introduce the frequency dual-component\nmasking strategy. This approach consists of two steps: high-\nfrequency masking and low-frequency perturbation. By pre-\ndicting image components of the masked frequencies, the\nimage encoder can learn the high-frequency components\nand their realistic distributions. Simultaneously, the low-\nfrequency perturbation prevents the encoder from overly re-\nlying on simple low-frequency components, instead encour-\naging it to utilize medium-frequency and high-frequency\ncomponents for image reconstruction. This pretraining strat-\negy can be seamlessly applied to both vanilla ViT, multi-\nscale ViTs, and convolutional networks without requiring\nchanges to the framework (Fig. 2,a)."}, {"title": "3.1.1 Frequency dual-component masking", "content": "Given an image $x \\in$\\n$\\mathbb{R}^{H \\times W}$, we can obtain its frequency spectrum $f \\in \\mathbb{R}^{H \\times W}$ via\nthe 2D discrete centered Fourier transform:\n$f_{uv} = \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} x_{hw} (-1)^{h+w}.e^{-i2\\pi(\\frac{uh}{H}+\\frac{vw}{W})}$.\nHere, the subscript hw denotes the value of the image at the\ncoordinate (h, w), and similarly, the subscript uv denotes the\nfrequency spectrum at the coordinate (u, v). Masking the\nlow-frequency components can result in significant image\ndistortion that is extremely challenging for reconstruction;\ntherefore, we adopt a progressive masking strategy. Our\ngoal is to make patch masking more likely to occur in\nthe high-frequency components and less likely in the low-\nfrequency components. To achieve this, we set the size of\npatches in the frequency spectrum to 16 \u00d7 16 and use an ex-\nponential decay function to control the masking probability\n(Fig. 2,b).\n$p_{ij} = 1 - exp(-\\frac{d_{ij}^2}{d_c^2})$\nHere $p_{ij}$ means the masking probability of patch ij, and $d_{ij}$\nis the Euclidean distance from the center of patch ij to the\ncenter of the frequency spectrum. $d_c$ is the cut-off distance,\nand we set $d_c = 0.2 \\times min (H, W)$ (Fig. 2,b)."}, {"title": "High-frequency domain masking", "content": "Given an image $x \\in$\n$\\mathbb{R}^{H \\times W}$, we can obtain its frequency spectrum $f \\in \\mathbb{R}^{H \\times W}$ via\nthe 2D discrete centered Fourier transform:\n$f_{uv} = \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} x_{wh} (-1)^{h+w}.e^{-i2\\pi(\\frac{uh}{H}+\\frac{vw}{W})}$. (1)\nHere, the subscript hw denotes the value of the image at the\ncoordinate (h, w), and similarly, the subscript uv denotes the\nfrequency spectrum at the coordinate (u, v). Masking the\nlow-frequency components can result in significant image\ndistortion that is extremely challenging for reconstruction;\ntherefore, we adopt a progressive masking strategy. Our\ngoal is to make patch masking more likely to occur in\nthe high-frequency components and less likely in the low-\nfrequency components. To achieve this, we set the size of\npatches in the frequency spectrum to 16 \u00d7 16 and use an ex-\nponential decay function to control the masking probability\n(Fig. 2,b).\n$p_{ij} = 1 - exp(-\\frac{d_{ij}^2}{d_c^2})$ (2)\nHere $p_{ij}$ means the masking probability of patch ij, and $d_{ij}$\nis the Euclidean distance from the center of patch ij to the\ncenter of the frequency spectrum. $d_c$ is the cut-off distance,\nand we set $d_c = 0.2 \\times min (H, W)$ (Fig. 2,b)."}, {"title": "Low-frequency perturbation", "content": "After the aforementioned\nstage, the high-frequency components are severely dis-\ntorted. However, most low-frequency components remain\nunaffected because $p_{ij}$ is zero at the center patch. To further\nreduce the dependence of the image encoder on the low-\nfrequency components, we introduce low-frequency pertur-\nbation. Specifically, we add intensity-attenuated, zero-mean\nnoise to the masked frequency spectrum:\n$\\begin{cases}\n\\delta_{uv} \\sim \\mathbb{N} (0, \\sigma^2) \\times exp(-\\frac{d_{uv}^2}{\\gamma^2}) \\\\\n\\delta_{0,0} = 0\n\\end{cases}$ (3)"}, {"title": "3.1.2 Equaled-histogram image-domain masking", "content": "Although advanced multi-scale ViTs and convolutional net-\nworks have demonstrated superior high-frequency captur-\ning abilities compared to the vanilla ViT [19], [20], the suc-\ncess of MAE relies on the global property and asymmetric\nstructure of the vanilla ViT, which enables the application of\nthe self-attention mechanism to discrete, non-overlapping\nimage patches [27]. In contrast, multi-scale ViTs and con-\nvolutional networks typically incorporate local window op-\nerators, such as convolutional layers, which prevent them\nfrom this same capability. It has been shown that directly\nsetting all masked patches to zero is inefficient [28], as this\napproach significantly distorts the image histogram distri-\nbution (Fig. 3). Additionally, the convolutional operator can\nresult in the \"mask pattern vanishing\" issue [28], causing\ninconsistent masking effects and ratios across different en-\ncoding layers.\nTo address this problem, we propose an innovative\nand straightforward image masking strategy, termed the\nhistogram-equalized masking strategy. Specifically, we first\nuniformly sample 30% of the visible image patches, each\nwith a size of 32 \u00d7 32. Unlike MAE [1], we do not discard\nthe masked patches; instead, we replace them with random\nnoise that has the same histogram distribution to the origi-\nnal patches. Denoting the one masked patch as $m_{ij}$, such an\noperation can be formulated as\n$M_{ij} \\leftarrow \\xi_{ij} \\sim D (m_{ij})$\nwhere $\\xi_{ij}$ is the histogram-equalized noise, and $D (\\cdot)$ de-\nnotes the histogram distribution. Practically, such distri-\nbutions can be approximated by a Gaussian distribution\nwith the same mean and variance as the original patches.\nImages corrupted via this strategy can keep the histogram\nconsistency before and after the masking (Fig. 3). Notably,\nthe frequency dual-component masking can also preserve\nthe histogram distribution."}, {"title": "3.2 Pretraining pipeline of Frepa", "content": "Given a raw 2D image x, we randomly use the frequency-\ndomain mask and the image-domain masking with equal\nprobability to perform corruption and obtain the degraded\nimage x. Our objective is to reconstruct x from x using an\nencoder-decoder network N:\n$N (\\hat{x}) \\rightarrow x$. (5)"}, {"title": "Loss function for reconstruction", "content": "We utilize the RMSE to guide the reconstruction in the\nimage domain.\n$\\mathcal{L}_{RMSE} = \\sqrt{\\frac{1}{H \\times W} ||N (\\hat{x}) - x||^2}$. (7)\nWe also incorporate the image gradient loss to restrict the\nreconstruction of details and textures.\n$\\mathcal{L}_{Grad} = \\frac{1}{H \\times W} ||\\nabla N (\\hat{x}) - \\nabla x||$, (8)\nwhere $\\nabla$ is the gradient operator.\nTo further enhance the representation of high-frequency\ncomponents, we aim for the reconstructed images to achieve\nsatisfactory alignment with the raw images in the frequency\ndomain. Initially, we employ the commonly used focal fre-\nquency loss (FFL) [30], specifically designed to enhance at-\ntention to high frequencies. However, in practice, we found\nthat directly incorporating the FFL can result in aliasing\nartifacts in the reconstructed images (Fig. 4), which cannot\nbe ideal reconstruction outcomes. We tend to consider that\nthese artifacts may arise from the degeneration of high-\nfrequency information due to overfitting in the frequency\ndomains [31], as FFL could impose excessively strong con-\nstraints on high frequencies [4]. To address this issue, we\npropose a novel hierarchical frequency-to-spatial loss (HFL),\nwhich can be formulated as\n$\\mathcal{L}_{HFL} = \\sum_{k=1}^5 \\frac{1}{H \\times W} ||O_k N (\\hat{x}) - O_k x||$ (9)\n$O_k$ denotes the high-pass exponential filter, while a higher\nk represents less preservation of the low-frequency com-\nponents. This loss function enables the optimization of\nhigh-frequency reconstruction in the image domain without\ndirectly imposing restrictions on the frequency spectrum,\nthereby preventing the formation of artifacts.\nTo further enhance the representation of high-frequency\ncomponents, we propose an adversarial training strategy"}, {"title": "3.3 Extending to 3D volume decoding", "content": "The network and pretraining strategy above are designed\nfor 2D images. However, in clinical practice, many datasets\nconsist of 3D volumes and image sequences, such as com-\nputed tomography (CT) and magnetic resonance imaging\n(MRI) scans. The current image encoder architecture cannot\nbe directly applied to these types of data. To address this\nchallenge, inspired by the design of spatial-temporal net-\nworks [32], we have developed an image-fusion architecture\nto seamlessly transfer pretrained 2D encoders to 3D volume\ndata (Fig. 2 c).\nGiven a volume data $v \\in \\mathbb{R}^{N \\times C \\times H \\times W}$, where N and C\nrepresent the number of images and channels, respectively,\nwe first apply the encoder to each image to obtain image\nembeddings $e \\in [\\mathbb{R}^{(N \\times n_c) \\times n_H \\times n_W}$ with feature channels $n_e$\nand embedding size $n_H \\times n_W$. We then incorporate the\nintra-image position embedding $P_{intra}$ into the embeddings\nas $e + P_{intra}$, and feed them into an inter-image transformer\nblock to extract intra-image features, resulting in output\n$c_{intra} \\in \\mathbb{R}^{(N \\times n_c) \\times (n_H \\times n_W) \\times n_d}$, where $n_d$ is the token di-\nmension. The transformer block follows the commonly used"}, {"title": "4 EXPERIMENTS AND RESULTS", "content": "We conducted experiments on two popular architectures:\nvanilla ViT-B [43] (86.9M parameters) and SwinT-B [19]\n(86.9M parameters) as the image encoders. The image de-\ncoder for reconstruction consisted of four convolutional\nlayers, while the classification head was composed of a\nmultilayer perception. The models were pretrained on a\ncomprehensive dataset, including modalities of CT, MRI, X-\nray, ultrasound, optical coherence tomography (OCT, projec-\ntion image), retina imaging, and dermatoscopy.\u00b9. All images\nwere resized to 512 \u00d7 512 \u00d7 3 using bi-cubic interpolation\nand zero-padding. For single-channel grayscale images, the\nimage was duplicated along the channel dimension to main-\ntain consistency. The model was trained using the Adam\noptimizer with a learning rate of 1 \u00d7 10\u22124. We employed\nimage augmentation techniques including random flipping,\nrandom rotation, random affine transformations, and ran-\ndom histogram shifting during the pretraining phase. The\nmodels were trained on a Linux workstation with two A100\n(80G) GPUs for 100 epochs.\nWe compared Frepa with other pretrained and task-\nspecific foundation models, including MAE [1], MFM [2]\n(a self-supervised strategy performed in the frequency do-\nmain), MedSAM [5], and CLIP [3]. The backbones for MAE\nand MFM were ViT-B, consistent with Frepa, and were\npretrained on our collected dataset. The training of CLIP\nfollowed the method proposed in [6]. For MedSAM, we\nused the model and checkpoint published in [5] without\nmodification. We also included one large visual-language\nmodel, MedDr [6] (40 billion parameters) for the comparison\nof some classification tasks."}, {"title": "4.1 Implementation details", "content": "We conducted experiments on two popular architectures:\nvanilla ViT-B [43] (86.9M parameters) and SwinT-B [19]\n(86.9M parameters) as the image encoders. The image de-\ncoder for reconstruction consisted of four convolutional\nlayers, while the classification head was composed of a\nmultilayer perception. The models were pretrained on a\ncomprehensive dataset, including modalities of CT, MRI, X-\ndata. The raw datasets were randomly divided into training,\nvalidation, and testing sets in an 8:1:1 ratio. Other training\nsettings remain the same as above."}, {"title": "4.2 Experiment settings for downstream tasks", "content": "We froze the encoders and fine-tuned only the task heads\nfor downstream tasks. To ensure a fair comparison, we\nemployed consistent training strategies and decoder archi-\ntectures across all foundation models. The loss functions\nused were Cross Entropy loss for classification, Dice loss\nfor segmentation, and GIoU loss [44] for detection tasks.\nFor classification evaluation, we utilized accuracy (ACC)\nand the area under the curve (AUC) metrics. Segmentation\nperformance was assessed using the DSC. Detection ratio\nand IoU were used for the evaluation of detection tasks. No-\ntably, we incorporated two modalities\u2014OCT reconstruction\ndata and electron microscopy (EM) data\u2014for segmentation\ntasks. These modalities were not included in the pretraining\nphases, thereby evaluating the generalizability to real-world\ndata. The raw datasets were randomly divided into training,\nvalidation, and testing sets in an 8:1:1 ratio. Other training\nsettings remain the same as above."}, {"title": "4.3 Visualization for reconstruction", "content": "We present several visualizations of reconstruction results\non external datasets. Specifically, we compare reconstruction\nfor image masking against MAE and reconstruction for fre-\nquency masking, with low-pass filtering, against MFM. As\nshown in Fig. 5, Frepa demonstrates the highest fidelity in\nreconstructing original images, with exceptional detail infor-\nmation. In contrast, images generated by other autoencoders\nappear relatively blurred, lacking sufficient detail in textures\nand edges. These results are further corroborated by the\nfrequency spectrum and RMSE metrics of each image. The\nsuperior reconstruction quality and clear detail retention\nindicate that Frepa effectively captures medium- and high-\nfrequency information."}, {"title": "4.4 Comparison with previous methods", "content": "4.4.1 Classification and detection\nTables 1 and 2 present the results of classification and\nlung nodule detection on 2D images and 3D volumes,\nrespectively. It shows that Frepa, when implemented with\nboth ViT and SwinT, overall outperforms MAE and MAF.\nSpecifically, Frepa exhibits an average improvement on ACC"}, {"title": "4.4.1 Classification and detection", "content": "Specifically, Frepa exhibits an average improvement on ACC\nof 2.3% and 2.4% over MAE, and 3.0% and 3.1% over MFM,\nrespectively. AUC metric also shows a similar conclusion.\nFurthermore, when compared to the classification-specific\nfoundation model, CLIP, Frepa demonstrates comparable\nperformance across most tasks and excels in certain areas\nsuch as the classification of chest cancer sub-types and\npneumonia sub-types. Additionally, it is noteworthy that\nin fine-grained tasks, such as lung nodule and lung nod-\nule detection, Frepa achieves more substantial improve-\nments, attributed to its superior capability of capturing\nhigh-frequency details. Specifically, For 2D images, there\nis a 4.0% increase in lung cancer detection ratio and a\n7% improvement in IoU (Table 1), while for 3D volumes,\nthe lung nodule detection ratio improves by 3.5% and the\nIoU by 6.4% (Table 2). Collectively, in classification tasks\nreliant on global-level information, Frepa achieves perfor-\nmance comparable to CLIP and MAE, affirming its robust\nglobal-level representation. Moreover, for tasks requiring\nfine-grained information, Frepa consistently outperforms,\nunderscoring its effectiveness in capturing detailed, high-\nfrequency information."}, {"title": "4.4.2 Segmentation", "content": "Tables 3 and 4 present the segmentation results on 2D\nimages and 3D volumes, respectively. These results en-\ncompass both large-scale anatomical segmentations, (lung,\nheart, and pneumonia infection in 2D segmentation, Tables\n3, and abdomen1K [45], AMOS22 [46], and ADCD [47] in\n3D segmentation, Tables 4), as well as fine-grained and\nvascular-structure segmentation (OCT artery-vein [48], reti-\nnal vessels [49]\u2013[51], and intercellular gaps [14] in 2D seg-\nmentation, Tables 3, and pulmonary artery-vein [15] in 3D\nsegmentation, Tables 4). In large-scale anatomical segmenta-"}, {"title": "4.4.2 Segmentation", "content": "ctions, Frepa shows a slight improvement over other models,\nincluding MedSAM, attributed to its enhanced ability to\ncapture high-frequency details, thereby improving bound-\nary identification. For the fine-grained segmentation tasks,\nFrepa exhibits more markedly superior performance, with\na significant improvement ranging from +7% to +15% in\nDSC compared to the best-performing models. Notably, in\nvessel segmentation for retina and OCT (Tables 3), other\nmodels yield almost failed segmentation results (Fig. 1),\nwhereas Frepa consistently achieves exceptional results,\nmaintaining topological structures and morphology that\nclosely match the ground truth. Moreover, this advancement\nis preserved when applied to external modalities, such as\nthe OCT reconstruction dataset and EM dataset, which\nexhibit entirely different imaging characteristics from other\nmodalities. These findings robustly demonstrate Frepa's su-\nperior representation of fine-grained information and high-\nfrequency components, along with better generalizability\nthan other foundation models when applied to a broader\nrange of realistic medical tasks."}, {"title": "4.5 Ablation study", "content": "We gradually perform the ablation study of the components\nin Frepa to evaluate their respective contributions. The\nvalidation is performed on datasets including Pneumonia\nMNIST [12] for classification, Lung cancer [37] for lesion\ndetection, and OCTA arteries and veins [48] for segmenta-\ntion. The four most critical components of Frepa examined\nare frequency dual-component masking (FM), equalized-\nhistogram image-domain masking (IM), HFL loss, and ad-\nversarial training (AT). For the ablation, these components\nare individually replaced or removed as follows: FM branch\nis deleted, IM is replaced by zero-patch masking (Fig. 3,\nsecond column), HFL loss, and AT are excluded during the\npretraining phase.\nThe results of the ablation study are presented in Table 5.\nRemoving each component leads to a decline in model\nperformance across all tasks. Notably, the FM has the most\nsignificant impact; its removal causes the performance to\ndegrade to be comparable to that of the MAE, suggesting\na failure in high-frequency representation. Similarly, the\nresults indicate that the zero-patch masking is less effective\nthan our proposed equalized-histogram IM strategy. Addi-\ntionally, the exclusion of HFL loss and AT also contributes to\na reduction in performance. These findings are consistent for\nboth ViT and SwinT architectures. In summary, the masking\nstrategy and loss function designs are crucial components\nthat significantly enhance the performance of Frepa."}, {"title": "4.6 Frequency representation in image embedding", "content": "In this section, instead of using downstream tasks, we\ndesign a more straightforward approach to evaluate the\nrepresentation and preservation of high-frequency compo-\nnents in the image embeddings. The experimental pipeline\nis illustrated in Fig. 6a. In this setup, raw images are fed\ninto fixed encoders (offline), and then decoders (online) are\ntrained to reconstruct the raw images\n$\\hat{x}' = D_{on} E_{off} (x) \\rightarrow x$.\nMSE loss is employed during training. Subsequently, we\ncompare the reconstructed images with the raw images\nacross different frequency components, utilizing the RMSE\nas the metric for reconstructed ratios.\n$p_k = \\frac{1}{|H \\times W|} ||O_k (\\hat{x}') - O_k (x)||^2 \\times 100\\%$.\nwhere $O_k \\in \\{O_l, O_m, O_h \\}$ represents low-pass, medium-\npass, and high-pass filtering, respectively. We compared\nFrepa, implemented with both ViT and SwinT, against two\nother autoencoder methods: MAE and MFM. The results\nof this comparison are presented in Fig. 6b. It is evident\nthat Frepa consistently outperforms the other autoencoder\nmethods in medium-frequency and high-frequency compo-\nnents while maintaining comparable performance in low-\nfrequency components. This underscores Frepa's superior\nability to represent and preserve high-frequency informa-\ntion without compromising low-frequency representation.\nThese findings align with our experiments on downstream\ntasks, providing a direct and intuitive evaluation of the level\nof embedding spaces."}, {"title": "5 DISCUSSION AND CONCLUSION", "content": "Foundation models have achieved notable success with\nnatural images by allowing the encoder to understand\nimages from a global-level perspective. This approach is\nbeneficial for tasks such as classification and object detec-\ntion. However, in medical image analysis, this method faces\nsignificant challenges due to the anatomical similarities and\n(12)\n(13)\nlow intra-modality variance among individuals [52], which\nresult in less significance of global-level perception within\none modality. Additionally, accurate diagnosis commonly\ndepends on detailed information from specific anatomical\nstructures, requiring more refined representations. Conse-\nquently, capturing only global-level and low-frequency in-\nformation is inadequate for medical image analysis.\nOur proposed Frepa addresses these limitations by\nenhancing high-frequency components and fine-grained\nfeatures without sacrificing image-level perception. Our\nexperiments demonstrate that Frepa can achieve a gen-\neral performance improvement, especially for those requir-\ning high-frequency details, surpassing both SOTA models\nand task-specific models. Our experiments further directly\ndemonstrate that Frepa better captures and preserves high-\nfrequency components from various perspectives, suggest-\ning its potential use in other fields requiring fine-grained in-\nformation, such as geospatial learning and spectral imaging.\nWhile our study shows excellent performance, future work\ncould focus on optimizing Frepa for coarse-grained tasks\nand enhancing robustness to image noise, particularly in\nlow-dose CT scans. Frepa's versatile and effective approach\npaves the way for advancements in medical AI foundation\nmodels."}]}