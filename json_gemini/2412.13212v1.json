{"title": "An introduction to reservoir computing", "authors": ["Michael te Vrugt"], "abstract": "There is a growing interest in the development of artificial neural networks that are implemented in a physical system. A major challenge in this context is that these networks are difficult to train since training here would require a change of physical parameters rather than simply of coefficients in a computer program. For this reason, reservoir computing, where one employs high-dimensional recurrent networks and trains only the final layer, is widely used in this context. In this chapter, I introduce the basic concepts of reservoir computing. Moreover, I present some important physical implementations coming from electronics, photonics, spintronics, mechanics, and biology. Finally, I provide a brief discussion of quantum reservoir computing.", "sections": [{"title": "1 Introduction", "content": "You know that it is possible to perform machine learning tasks on a computer, but did you know that it is also possible to do so with a bucket of water? Precisely that was demonstrated in Ref. [12]. Input data was mechanically fed into a bucket, recordings of the water surface then could be used for classification tasks. This is a form of reservoir computing (RC), which this chapter will provide an introduction to.\nThe core idea of RC is that a significant portion of the computing task is performed not by a trained network, but by a very high-dimensional system (reservoir) that is essentially treated as a black box and whose output is fed into a single readout layer, which is the only component of the network that is trained. This setup is illustrated in"}, {"title": "2 Basic concepts of reservoir computing", "content": "Since physical systems are generally much more difficult to train than neural networks on a computer, RC is a very promising approach for implementing artificial intelligence in a physical system (for example a bucket of water or \u2013 more relevant in practice - optical or magnetic systems). Consequently, RC has attracted considerable interest among physicists in recent years. From a computer science point of view, on the other hand, RC is useful for the otherwise difficult task of training recurrent neural networks.\nThe foundations of what is nowadays referred to as reservoir computing were independently developed by Jaeger [23], who referred to it as echo state network, and by Maass et al. [34], who called it liquid state machine. (There were some earlier ideas in this direction see Refs. [28, 27, 44] \u2013 that are largely unknown today [38].) The name \u201creservoir computing\" was coined in Refs. [52, 53] to unify these concepts. RCs are particularly useful for tasks involving some temporal dynamics without needing external memory, since what they do depends also on the value of the input signal at previous times. Reviews of RC can be found in Refs. [32, 42, 10, 33, 11], a book on the topic was edited by Nakajima and Fischer [38]."}, {"title": "2.1 How reservoir computing works", "content": "The discussion follows Refs. [10, 36] (and also adapts the notation used there)."}, {"title": null, "content": "Suppose that we have an n-dimensional training input signal $u_{train}(t)$ (depending on time t) that is supposed to lead to a certain m-dimensional output signal $y_{train}(t)$. For instance, if the system is supposed to predict time series data, $u_{train}(t)$ would be the first part of a time series and $y_{train}(t)$ the second part (that we want to predict from the first part) [10]. The input signal is used to drive a high-dimensional nonlinear dynamical system that is referred to as the reservoir.\nSpecifically, the state X of the reservoir at time $t_i$ takes the form [36]\n$X(t_i) = f(X(t_{i-1}), u(t_i))$ (1)\nwith the input signal u and a function f. A sufficiently large subset of the state variables, summarized in a vector x(t), must be accessible. Finally, a readout function F maps the vector x(t) to the output y(t), i.e.,\n$F(x(t)) = y(t)$. (2)\nThis readout function is the only thing that is touched during the training process. It is designed to minimize a loss function that typically that depends on the difference between the actual output y(t) and the desired output $y_{train}(t)$. In most cases, F is obtained via linear regression. Once F is known, one can apply the system to new input signals u(t).\nThis strategy has a number of advantages:\n\u2022\nThe training usually only consists in linear regression, which is simple, compu-\ntationally inexpensive, and easy to implement.\n\u2022\nOne only needs to train the readout function F and not the entire system. This is helpful if we are dealing with a physical system where the precise interactions between the parts are difficult to modify and perhaps not even fully known.\n\u2022\nOne can use the same reservoir for different computing tasks by simply using different readout functions F.\n\u2022\nIt is a good approach for dealing with time series data.\nAlmost every physical system can be described by an equation of the form (1), and therefore a large variety of physical system can be used as reservoirs (although in practice there are restrictions, see Section 3). A notable feature of RCs is their temporal dynamics. Iterating Eq. (1) gives [36]\n$X(t_i) = f(f(f(X(t_{i-3}), u(t_{i-2})), u(t_{i-1})), u(t_i)),$ (3)\nshowing that the system's state at a certain time depends on the system states and inputs at previous times. This allows the system to react to temporal input signals, such as spoken language."}, {"title": "2.2 Recurrent neural networks", "content": "An essential distinction in this context (see also the chapter by Tobias Wand in this volume) is that between a feed-forward neural network (FNN) and a recurrent neural network (RNN). In a FNN, signals propagate in one direction - the first layer activates the second layer, the second layer the third layer and so on. In a RNN, on the other hand, information can also \u201cmove backwards\". This allows the system to have memory: If a system is supposed to process a signal u(t), then reacting to $u(t_2)$ might require also information about $u(t_1)$ with $t_1 < t_2$. This can be achieved by feeding back information about previous states of nodes. This distinguishes RC from the related concept of an extreme learning machine [21, 54], which employs FNNs and do therefore not have memory of this type [7].\nThe existence of closed loops implies in particular that RNNs can have a tem- poral dynamics even in the absence of external inputs. Therefore, while FNNs are (mathematically speaking) functions \u2013 they map a certain input to a certain output \u2013 RNNs are (mathematically speaking) dynamical systems [32]. This already indi- cates how they might be related to RC, which, as discussed in Section 2.1, is based on dynamical systems. Frequently, in computer science applications, the dynamical system constituting the reservoir is a RNN.\nIn general, RNNs are difficult to train. FNNs are typically trained via gradient descent methods, where the parameters giving the connection weights are gradually changed to move the network's output closer to the target output. Gradient descent methods are also frequently, and in many different forms, applied to RNNs [2]. However, such methods are considerably more difficult to apply in the context of RNNs. Reasons for this include that a gradual parameter change might lead to a bifurcation that spoils convergence and that training times are very long since a single parameter change requires running the temperal dynamics for some time. RC was proposed as a new method for training RNNs that allows to avoid these problems by changing the weights only in a non-recurrent readout layer. The RNN itself can be created randomly and is unchanged during the training [32]."}, {"title": "2.3 Why does this work?", "content": "At first sight, the idea of RC is somewhat counterintuitive. While it is of course easier to train only the final layer of a neural network rather than the entire one, there is certainly a reason why one usually trains all layers. Usually one would not get away with only changing the final layer, so why is it possible here?\nWhat is exploited here is the high dimensionality of the reservoir. The information that we are interested in is in principle encoded in the input signal, but it is mixed up, nonlinearly, with a huge amount of other stuff that we are not interested in. The projection onto a higher-dimensional space allows for separability. This idea is illustrated in Fig. 2. Suppose we want to classify some inputs into stars and circles. Unfortunately, due to the way the data is distributed, it does not exhibit"}, {"title": "3 What is a good reservoir?", "content": "While the procedure discussed above can in principle be applied very generally, not every dynamical system does in practice make a good reservoir. A number of criteria have been developed that a reservoir should satisfy to be useful in this"}, {"title": null, "content": "context - although the broad range of systems that have been used for this purpose (consider, for example, the water bucket mentioned above) suggests that in practice these criteria are quite flexible [46].\nFirst, reproducibility is of course important. If two input signals are very similar, the output signals should be similar as well. This is related to the ability of the system to generalize from the data it has been trained on to other input data [46]. Second, if the input signals are sufficiently different, the output signals should also be different (separation). This determines the reservoir's ability to distinguish different sorts of inputs. A reservoir computer that always produces the same output regardless of the input obviously satisfies the reproducibility property, but would not be really useful.\nMoreover, fading memory is an important property of reservoir computers [10]. The computer is supposed to process a time series u(t), not just he instantaneous value of u (the latter would not require a RNN). Therefore, it needs to have memory. On the other hand, the current value of u should still be what is most important, values at earlier times become gradually less important. In other words, the memory should gradually fade. The computer should relax to a quiescent state if there is no external input [42], and its behavior should not depend on the initial condition X(0) of the reservoir. This is referred to as the echo state property - the influence of initial conditions should gradually vanish [57]. The timescale of the fading memory should be comparable to the timescales of the input signal [26]. Since it depends on the application what the timescale of the input signal is, the adequate reservoir system may differ depending on the application.\nAlso, it is often considered advantageous to operate the reservoir computer in a parameter range close to an instability (for example, in the vicinity of a transition to chaos), since there its behavior is particularly complex and therefore has a very high computational complexity. Intuitively, the reason is that such regimes present a compromise between ordered phases (where the behavior is stable and thus reliable, but where initial conditions are also quickly erased by the approach to an attractor making the system less able to react to an input) and chaotic phases (where the system's behavior is strongly affected by small differences in the initial conditions, harming separability) [46]. There are, however, also counterexamples, i.e., systems where moving close to an instability actually decreases the performance of a reservoir computer [8]. In fact, Herbert Jaeger (in his foreword for Ref. [38]) argues that the idea that RC should operate close to chaos or criticality is a \"myth\", neither mathematically well-defined nor empirically confirmed."}, {"title": "4 Physical reservoir computing", "content": "The discussions so far were concerned with general concepts in artificial intelligence. This book, however, has a specific focus on the relation of AI to physics, and there is a reason that RC features so prominently in the introductory part of this book. This reason is physical reservoir computing."}, {"title": "4.1 Electronic reservoir computing", "content": "Standard computers are based on electronics, and therefore it is a very natural idea to use electronic systems. This has been achieved in a broad number of ways (see Refs. [49, 31] for an overview), of which I discuss here just one, namely memristors. A memristor is a resistor that possesses memory, i.e., whose resistance changes based on the current that has passed through it. Memristors are therefore useful devices in applications like RC where memory is important. They can be used to mimic the plasticity of biological neurons, and allow for nonlinear transformations of input signals [49]. A memristor-based RC was first proposed by Kulkarni and Teuscher [30]."}, {"title": "4.2 Photonic reservoir computing", "content": "In photonics, information processing is based not (solely) on electric currents, but on light (photons). RC has developed into a widely used approach in photonics, see Ref. [42] for a review (I am loosely following this reference here). An important approach is the implementation of RC in on-chip-photonics, where photonic systems are integrated into chips. This allows the systems to be produced and sold on industrial scales and to therefore use them for high-speed low-power-consumption computing. This approach was theoretically suggested in Ref. [50] and later realized in hardware [51].\nA further interesting approach is delay reservoir computing [22]. Delay systems have been of considerable interest for optics in the past years [45, 29]. They are described mathematically by delay differential equations, which differ considerably from ordinary differential equations since they possess an infinite-dimensional phase space - for solving it, one needs to specify not only the state of the system at a single initial time, but on an entire time interval. Using delay systems, one can therefore achieve a high-dimensional phase space (which is advantageous in the context of RC) even with a very simple setup [42]."}, {"title": "4.3 Spintronic reservoir computing", "content": "Spintronics is a field of technology where information processing is based not only on the electric charges (as in electronics), but also on the spins (elementary magnetic moments) of electrons. Spintronics has become increasingly popular in neuromor- phic computing in general and RC in particular, see Refs. [58, 13, 18, 11, 56, 31] for reviews. Spintronic systems can be used to build artificial synapses, thereby mim- icking the structure and functionality of biological brains [18]. An introduction is provided in the chapter by Atreya Majumdar/Karin Everschor-Sitte in this book.\nAn interesting recent proposal in this context is Brownian reservoir computing based on skyrmions [41, 3, 4, 5]. Brownian motion [6] is the random thermal motion of particles, which is a central phenomenon in soft matter physics, but also arises in magnetic systems. An example are magnetic skyrmions, which are whirl-like topological magnetic nanostructures that have particle-like diffusion behavior rem- iniscent of neurotransmitters and can be used as information carriers in spintronics [18]. In Brownian computing, one employs thermal fluctuations \u2013 which in most systems are present anyway for computing purposes to achieve a high energy efficiency. It is of course helpful if the employed Brownian system can be easily integrated into a computer, which is why magnetic nanosystems such as skyrmions are useful here [4]. Brownian RC based on skyrmions was realized experimentally by Raab et al. [41], who demonstrated that this approach is very promising for energy-efficient computing."}, {"title": "4.4 Mechanical reservoir computing", "content": "Mechanical systems can make for useful reservoirs. Robotic systems, in particular from soft robotics (where the bodies of the robots are flexible), have been repeatedly used in this context. Hauser et al. [20] have modeled this using the example of a nonlinear mass-spring-damper system connected to a mechanical network, which was intended to represent in a simple way the body of a soft robot (or biological system) and which exhibits the complex nonlinear dynamics required for successful RC. Nakajima et al. [39] employed a silicon-based robot arm inspired by the arm of an octopus, with the input being the rotation of the arm and the output being measured strain. While the noisy and nonlinear dynamics of soft robots is often perceived as disadvantageous, it can be very useful in the context of RC. (This paragraph follows Ref. [19].)"}, {"title": "4.5 Biological and chemical reservoir computing", "content": "Reservoir computing has always had a close connection to neurobiology. In particu- lar, work on RC has been motivated by attempts to understand information processing in mammalian brains [47]. For instance, it has been proposed that the cerebellum might work like a liquid state machine [55], and experiments on mice [9] suggest that the mouse brain exploits principles of RC [9, 31]. Moreover, RC \u2013 which is frequently based on random neural networks and noisy systems \u2013 might explain why the brain works so accurately despite being a rather noisy system [32]. It is therefore a promising direction of work to use biological neural networks for RC tasks [47]. Biological neurons are in a sense the most obvious, but not the only approach to biological RC. For example, it has been proposed to realize RC based on Escherichia coli bacteria [24]. Another variant, namely DNA reservoir computing [17], will be discussed in the chapter on DNA neural networks in this book. This approach is based on employing chemical systems for RC, an idea that is also used in non-biological contexts (for example based on electrolyte solutions [25]). See the chapter by Julian Jeggle/Raphael Wittkowski in this volume for a discussion of the related concept of active matter RC."}, {"title": "5 Quantum reservoir computing", "content": "In the wake of the currently growing interest in quantum computing in general and quantum-mechanical approaches to machine learning in particular, quantum reservoir computing [15, 35, 16, 48, 36] has attracted some interest. Here, one employs quantum-mechanical reservoirs and thereby aims to exploit the advantages of quantum computers for RC. In this section, I will introduce the elementary ideas of how this works. The discussion follows Ref. [15], which was one of the first articles on this topic. A more general introduction to quantum machine learning can be found in the chapter by Ivana Nikoloska in this volume.\nQuantum states are represented by vectors in complex Hilbert states. In the context of quantum computing, the minimal information unit is a qubit, corresponding to a two-dimensional complex vector in a vector space spanned by the vectors |0) and |1). In general, the state of a system of N qubits is described by a $2^N \\times 2^N$ Hermitian matrix $\\rho$, the density matrix. The quantum system is said to be in a pure state if $\\rho$ can be written as $\\rho = |\\psi\\rangle \\langle \\psi|$, where $|\\psi\\rangle$ is a $2^N$-dimensional vector and $\\langle \\psi|$ is a covector to $\\psi$. (For instance, if $|\\psi\\rangle = (1, 2, 3)^T$, then $\\langle \\psi| = (1, 2, 3)$.) If the density matrix at time t is $\\rho(t)$, then the density matrix at time t + $\\tau$ is\n$\\rho(t + \\tau) = e^{-iH\\tau}\\rho(t)e^{iH\\tau}$ (4)\nwith the Hamiltonian H (a $2^N \\times 2^N$ Hermitian matrix that determines the dynamics and whose eigenvalues correspond to the energy levels of the quantum system). For an arbitrary observable $A_i$, which is also represented by a $2^N \\times 2^N$ Hermitian matrix,"}, {"title": null, "content": "the expectation value is given by\n$\\langle A_i(t) \\rangle = Tr(\\rho(t)A_i)$ (5)\nwith the trace Tr.\nWhat is now required is a way to feed an input signal u(t) into the system and to get an output signal x(t) that can then be fed into the readout function F. Let us consider for simplicity a one-dimensional input signal u(t), which we sample in M discrete time intervals of length $\\tau$ to get a sequence {$u_k$} with $u_k = u(k\\tau)$ and k = 0, 1, ...M. At each time k$\\tau$, the state of the first qubit is changed to $\\rho_{u_k}$ = $|\\psi_{u_k}\\rangle \\langle \\psi_{u_k}|$ with\n$|\\psi_{u_k}) = \\sqrt{1 \u2013 u_k} |0) + \\sqrt{u_k} |1) .$ (6)\nThe density matrix $\\rho$ is thereby replaced by\n$\\rho_{u_k} \\otimes Tr_1(\\rho),$ (7)\nwhere $\\otimes$ is a tensor product and $Tr_1$ is a trace over the degrees of freedom of the first qubit. Afterwards, the density matrix is time evolved via Eq. (4) for a time $\\tau$. This time has to be optimized in order to optimize the performance of the computer (see Ref. [15]). For the output x(t), we then pick some observables $A_i$ and assemble them in a vector A. Then, we can obtain the output vector from their expectation values as\n$x(t) = Tr(\\rho(t)A).$ (8)\nSpecifically, Fujii and Nakajima [15] choose $A_i$ as the Pauli operator acting on the ith qubit.\nThe dimension of the quantum-mechanical Hilbert space increases exponentially with the number of qubits N, giving rise to an exponentially increasing number of nodes in the reservoir. For readout purposes, these are split into true nodes (the observed ones) and hidden nodes (the rest). The signals are sampled not only at the time k$\\tau$, but also at several times in between. Dividing the time interval into V parts gives rise to V virtual nodes and allows to increase the number of nodes from N to NV via temporal multiplexing. Thereby, the exponentially large Hilbert space is monitored via a polynomial number of signals. This is the distinguishing feature of quantum RC compared to other RC approaches [15]. Changing $\\tau$ corresponds to a change of the dynamics of the reservoir, whereas changing V corresponds to a change of the way it is observed [40].\nAn important feature of quantum systems is also the way in which they interact with the environment. Such interactions lead to dissipation and decoherence [43], where quantum states are destroyed by interactions with the environment. Moreover, performing a measurement of a quantum state generally changes it, a phenomenon giving rise to the famous quantum measurement problem [14]. Usually, interactions with the environment are not beneficial for the performance of quantum computers. One can, however, also try and exploit such effects in quantum reservoir computing, as has recently been demonstrated for both measurements [37] and dissipation [43]."}, {"title": "6 Outlook: Relation to intelligent matter", "content": "If we loosely understand \"intelligent matter\" as \"physical materials perform tasks similar to those expected from computer systems that we would refer to as (artifi- cially) intelligent\", then RC seems to be, if not an instance of it, then at least an important step towards it. We have here physical systems that can be employed in computational tasks of the form that appear in machine learning.\nNevertheless, according to Kaspar et al. [26], reservoir computing systems in the form described here do not constitute \"intelligent matter\" in the technical sense:\n\u2022\nThe systems possess fading memory, whereas intelligent matter needs to have long-term memory.\n\u2022\nThe readout function F still needs to be trained manually, the system does not adapt on its own.\nRegarding the first point, it should be noted, however, that the fading memory can be tuned to fade rather slowly if this is desired in a certain context.\nRC does nevertheless have significant potential for the development of \"true\" intelligent matter, in particular when considering its relation to evolutionary dynam- ics (a topic reviewed in Ref. [46]). After all, RC is a possible working principle of biological brains. It is conceivable that RC emerges in evolutionary contexts, as it has certain advantages (such as the low cost of learning and the fact that external sytems can be used to carry out computations) that could give biological systems exploiting this paradigm a fitness advantage. An evolutionary evolving RC system would be a system that evolves its computing capabilities in adaptation to the en- vironment, bringing it closer to actual intelligence. A possible disadvantage of RC in evolutionary contexts, Seoane [46] suggests, is that (since the reservoir needs to be high-dimensional), it requires systems to perform a lot of activity that is not really used for computing, making it energetically costly (which leads to a fitness disadvantage). A fine-tuned neural network can have a smaller number of nodes."}, {"title": "7 Summary", "content": "In this chapter, I have introduced the basic ideas of reservoir computing. Here, one uses a very high-dimensional recurrent neural network and trains only the final layer. This makes it possible to use for the rest of the network a physical system whose properties might be difficult to tune or not fully known. A variety of systems have been used here, ranging from buckets of water to optical and magnetic setups. Reservoir computing is a very promising tool for implementing artificial intelligence in nanosystems, and will continue to be a thriving field of research in the coming years."}]}