{"title": "Bridging Language Barriers in Healthcare: A Study on Arabic LLMs.", "authors": ["Nada Saadi", "Tathagata Raha", "Cl\u00e9ment Christophe", "Marco AF Pimentel", "Ronnie Rajan", "Praveen K Kanithi"], "abstract": "This paper investigates the challenges of developing large language models (LLMs) proficient in both multilingual understanding and medical knowledge. We demonstrate that simply translating medical data does not guarantee strong performance on clinical tasks in the target language. Our experiments reveal that the optimal language mix in training data varies significantly across different medical tasks. We find that larger models with carefully calibrated language ratios achieve superior performance on native-language clinical tasks. Furthermore, our results suggest that relying solely on fine-tuning may not be the most effective approach for incorporating new language knowledge into LLMs. Instead, data and computationally intensive pretraining methods may still be necessary to achieve optimal performance in multilingual medical settings. These findings provide valuable guidance for building effective and inclusive medical AI systems for diverse linguistic communities.", "sections": [{"title": "Introduction", "content": "The evolution of multilingual language models like Llama3 and GPT-4 marks a significant advancement in natural language processing. However, most LLMs are primarily trained on English and other common European languages, often neglecting low-resource languages with different alphabets, such as Arabic. This limitation poses a significant challenge, particularly in specialized domains like healthcare, where accurate language understanding is crucial.\nOne major obstacle is the scarcity of high-quality, domain-specific data for these languages. In this paper, we address this challenge by evaluating and improving the capabilities of LLMs for clinical tasks in Arabic. We first conduct a comprehensive evaluation of existing open-source LLMs to assess their performance on medical tasks in both English and Arabic. This analysis provides valuable insights into the current state of LLMs in handling clinical information across different languages.\nTo further enhance the capabilities of these models, we investigate various techniques, including leveraging exist-"}, {"title": "Related Work", "content": "The emergence of large language models (LLMs) has marked a significant advancement in artificial intelligence, demonstrating impressive capabilities in natural language understanding and generation. Initially developed for general use-cases such as text summarization, translation, and dialogue generation, LLMs have quickly been adopted across diverse industries, including finance, law, and education.\nOne domain where LLMs have shown considerable promise is healthcare (Zhang, Wang, and Chen 2023). Recent studies have explored the application of LLMs to a variety of medical tasks, including clinical decision support, medical question answering, and diagnosis assistance. For instance, GPT-4 has demonstrated proficiency in medical knowledge evaluation, achieving scores comparable to human experts on standardized medical exams (Nori et al. 2023). Other models like Meditron (Chen et al. 2023), Open-BioLLM (Ankit Pal 2024) and Med42 (Christophe et al. 2024) have further advanced the field, with many surpassing GPT-4's performance on specific medical tasks and releasing open-source models usable by the research community and facilitating further advancements in the field.\nEvaluating the performance of clinical LLMs, however,"}, {"title": "Evaluating Large Language Models' Capabilities in Arabic Medical Applications", "content": "While many large language models claim to work well in multiple languages and medical tasks, most testing focuses on either general language skills or English medical knowledge separately. Few studies look at how well these models handle medical content in specific non-English languages (Jin et al. 2024). In this study, we test several popular models of different sizes on Arabic medical benchmarks. Our results show that these models still have a long way to go to match their English performance levels."}, {"title": "Arabic Evaluation Datasets", "content": "We utilize a set of Arabic-translated medical datasets for our zero-shot and fine-tuning evaluations. These datasets, originally developed for training and evaluating question-answering systems in the medical domain, include PubMedQA, MedMCQA, MedQA, and Medical MMLU. PubMedQA: This dataset is derived from biomedical research articles (Jin et al. 2019). MedMCQA (Pal, Umapathi, and Sankarasubbu 2022) and MedQA (Jin et al. 2021) consists of multiple-choice questions coming from Indian and United States medical license exams. Medical MMLU is derived from the MMLU benchmark, specifically focusing on biomedical subsets including Clinical Knowledge, College"}, {"title": "Modifications to Harness Pipeline", "content": "Our research utilizes the Harness evaluation framework (Gao et al. 2024), which calculates log-likelihood scores to evaluate predictive model performance. To accommodate the Arabic language, we made significant modifications to handle its unique attributes, including its distinctive script, complex morphology, and syntactic structure, ensuring accurate processing of Arabic data.\nArabic text runs right-to-left (RTL), unlike English (LTR). We updated the framework to display and process Arabic text correctly. This meant reformatting our dataset while keeping its structure intact. To conduct zero-shot evaluations in Arabic, we adapted the entire framework for zero-shot testing, converting all prompts, responses, and multiple-choice options to Arabic, ensuring an accurate display and functionality of the Arabic script. Arabic's linguistic complexity makes context crucial for accurate understanding. A single word can have multiple meanings depending on its grammatical form and context. For instance, the root word \u062c\u0631\u0628 )j-r-b) can transform into various derivatives that range from \"to try\" to \"to test\" to other nuanced meanings, highlighting why machine learning models must carefully consider contextual cues when processing Arabic text.\nAdditionally, rather than just calculating the probability of generating answer choice labels (e.g., a, b, c, or d), we calculate the probability of generating the full answer text. This modification provides a more detailed understanding of the model's performance by taking into account the entire answer generation process."}, {"title": "Results", "content": "As shown in Table 1, large language models in all model families exhibit limited performance on Arabic medical benchmarks. While leading models like Llama3.1 achieve high accuracy in English (62.0 and 78.2 on MedQA), their performance significantly degrades when applied to Arabic (29.5 and 56.6). Although Qwen2.5 models demonstrate relatively better performance in Arabic, accuracy remains suboptimal.\nWe will focus on improving Arabic performance, using Llama3.1 as a case study to explore strategies to achieve English-language proficiency on Arabic medical benchmarks."}, {"title": "LLM Adaptation Through Translation Pipeline", "content": "A straightforward approach to enhance large language model performance across languages is to implement a"}, {"title": "Language Specific Finetuning", "content": "We aim to improve large language models' performance by finetuning on bilingual domain-specific data not encountered during pretraining. In this section, we detail our finetuning pipeline, present the datasets utilized, and analyze the optimal balance between English and Arabic training data.\nFinetuning Datasets Due to the scarcity of high-quality Arabic clinical data, we developed a comprehensive data preparation pipeline. This section details our methodology for cleaning existing datasets, generating new data, and performing translations to ensure robust data quality. The size of each dataset is described in Table 3.\n\u2022 Arabic Health Questions & Answers Dataset (AHQAD): The AHQAD dataset, with its 90 richly diverse categories, offers a comprehensive landscape of medical and healthcare-related themes tailored specif-"}, {"title": "Fine-tuning Pipeline", "content": "The bilingual medical fine-tuning pipeline explores the optimal combination of Arabic and English medical datasets to enhance model performance on both English and Arabic medical tasks. The pipeline incorporates two distinct data streams: high-quality Arabic medical content obtained through rigorous cleaning and filtering of native Arabic medical datasets, and carefully translated English medical datasets that maintain clinical accuracy in both languages. For different ratios, we maintain a constant number of 469.97M tokens, randomly sampling from our dataset presented in Table 3."}, {"title": "Results", "content": "Our results in Table 4 show that different ratio of Arabic-English data yield to different performance levels depending on the evaluation task. For PubMedQA, training with exclusively Arabic data produces the best accuracy (71.2). While for MedMCQA and MedQA, the models perform best with strong Arabic majority and English Majority, respectively (35.1 and 29.8). Surprisingly, for the MMLU datasets, which focuses on testing direct knowledge application, using only English data, achieves 42.4 compared to the 39.8 zero-shot accuracy.\nThese patterns remain consistent across both the base and instruct models. These results highlight the fact that the relationship between the language distribution used for finetuning and performance is fundamentally linked to the nature of each task. For instance, PubMedQA requires complex analytical reasoning within medical contexts, while MMLU focuses on structured knowledge assessment through multiple-choice questions. This difference suggests that tasks requiring a deeper understanding of context need stronger language-specific training.\nInterestingly, our findings on the larger 70B parameter models show more consistent behavior across all Arabic tasks, with Arabic-only training data consistently achieving the best results. This suggests that larger models may handle language-specific tasks more uniformly than their smaller counterparts, given their greater capacity to abstract and generalize linguistic features across different training distributions. Llama3.1-70B base model exhibits poor performance on the PubMedQA test set due to its inability to follow the chat-template for a highly context-based task. Intriguingly, our fine-tuned version of Llama3.1-70B-Instruct rarely outperforms the original model, suggesting that it has"}, {"title": "Conclusion", "content": "Our research into Arabic-English medical AI reveals critical insights for developing truly effective multilingual language models.\nFirst, we highlight the significant performance gap between English and Arabic, especially pronounced in smaller models. This disparity underscores the need for models deeply trained in specific languages to achieve genuine language understanding and complex medical reasoning. Smaller models, with their limited capacity, struggle to capture the nuances of different languages and medical terminology, resulting in a substantial performance gap between languages like English and Arabic.\nSecond, while some general language models demonstrate superior translation capabilities compared to specialized translation models, they come with high computational costs and are not perfect. General language models, despite their broader training data, still have limitations in accurately translating medical terminology and complex linguistic structures, highlighting the need for further research in this area.\nThird, fine-tuning models do not always guarantee improved performance compared to the baseline, and the results are highly dependent on the distribution of languages in the training data. The effectiveness of fine-tuning can vary significantly depending on the specific language mix used in the training data, suggesting that a careful balance of languages is crucial for optimal performance.\nWe acknowledge that our evaluation primarily focuses on close-ended question benchmarks, which, while valuable for assessing domain knowledge, do not fully capture the generation capabilities, safety, and bias aspects of a model. These aspects are crucially important for any healthcare model. Therefore, we advocate for new benchmarks, such as MEDIC (Kanithi et al. 2024), to include multilingual capabilities tests to address these critical dimensions.\nIt is important to note that the impact of language mixing is particularly significant when dealing with languages that have vastly different alphabets, such as Arabic, Chinese, or Latin-based languages. The non-overlapping nature of their tokens can lead to unique challenges in training and optimization. Moreover, the performance of a model in one domain should ideally transfer seamlessly across multiple languages. It should be easier for a model to learn technical vocabularies in a new language if it is already trained on that domain and possesses a good understanding of the language. Therefore, the transfer capabilities of a model for a specific domain from one language to another should be high.\nThus, we need to continue relying on extensive pretraining for models to learn a new language effectively. At the same time, exploring the transfer capabilities of models for specific domains across languages is crucial. The ultimate goal extends beyond technical achievement: we aim to create AI systems that can break down language barriers, provide accurate medical insights, and expand healthcare access, especially in underserved and linguistically diverse communities. This means developing models that do not just translate words, but truly comprehend the intricate cultural and linguistic subtleties of medical communication. Achieving this goal will require models that can not only translate medical information accurately but also understand the cultural context and linguistic nuances associated with different languages, ensuring effective communication and healthcare access for diverse populations."}]}