{"title": "Efficiently Solving Turn-Taking Stochastic Games with Extensive-Form Correlation", "authors": ["Hanrui Zhang", "Yu Cheng", "Vincent Conitzer"], "abstract": "We study equilibrium computation with extensive-form correlation in two-player turn-taking stochastic games. Our main results are two-fold: (1) We give an algorithm for computing a Stackelberg extensive-form correlated equilibrium (SEFCE), which runs in time polynomial in the size of the game, as well as the number of bits required to encode each input number. (2) We give an efficient algorithm for approximately computing an optimal extensive-form correlated equilibrium (EFCE) up to machine precision, i.e., the algorithm achieves approximation error \\(\\varepsilon\\) in time polynomial in the size of the game, as well as log(1/\\(\\varepsilon\\)).\nOur algorithm for SEFCE is the first polynomial-time algorithm for equilibrium computation with commitment in such a general class of stochastic games. Existing algorithms for SEFCE typically make stronger assumptions such as no chance moves, and are designed for extensive-form games in the less succinct tree form. Our algorithm for approximately optimal EFCE is, to our knowledge, the first algorithm that achieves 3 desiderata simultaneously: approximate optimality, polylogarithmic dependency on the approximation error, and compatibility with stochastic games in the more succinct graph form. Existing algorithms achieve at most 2 of these desiderata, often also relying on additional technical assumptions.", "sections": [{"title": "Introduction", "content": "Equilibrium computation is one of the most important topics in algorithmic game theory. Decades of effort has painted a fairly complete landscape for the computational complexity of various equilibrium concepts in normal-form games: Roughly speaking, computing an (optimal) equilibrium is computationally tractable, if either correlation is allowed between both players' actions, or one of the two players has the power to commit to a strategy. In other words, in normal-form games, there are polynomial-time algorithms for computing an optimal (i.e., maximizing a convex combination of the two players' utilities) correlated equilibrium, and for computing a Stackelberg equilibrium (see, e.g., [Papadimitriou, 2007]).\nThe situation is subtler for games in dynamic environments, where the two players iteratively take actions, each affecting the state of the world, and together determining the overall payoff of each player. Such games are conventionally modeled as stochastic games or extensive-form games (we will discuss the differences between the two formulations momentarily). In these games,"}, {"title": "1.1 Our Results", "content": "Throughout the paper, we focus on two-player, finite-horizon, turn-taking stochastic games. Put in different words, we focus on two-player, perfect-information extensive-form games in the graph form. Our main results are twofold:\n\u2022 We give an algorithm for computing an SEFCE, which runs in time polynomial in the size of the game, as well as the number of bits required to encode each input number.\n\u2022 We give an efficient algorithm for computing an approximately optimal approximate EFCE up to machine precision, i.e., the algorithm achieves approximation error \\(\\varepsilon\\) in time polynomial in the size of the game, as well as log(1/\\(\\varepsilon\\)).\nOur algorithm for SEFCE is, to our knowledge, the first polynomial-time algorithm for equilibrium computation with commitment in such a general class of stochastic games (the main assumption being that the game is turn-taking). As discussed in Section 1.2, existing algorithms for SEFCE typically make stronger assumptions such as no chance moves, and are designed for extensive-form"}, {"title": "1.2 Further Related Work", "content": "Equilibrium computation in normal-form games has been extremely well-studied. For example, without commitment, Daskalakis et al. [2009] and Chen et al. [2009] show that computing a Nash equilibrium is PPAD-complete in two-player normal-form games, and computing optimal Nash equilibria is generally NP-hard [Conitzer and Sandholm, 2008, Gilboa and Zemel, 1989]. In contrast, when correlation is allowed, one can compute an optimal correlated equilibrium efficiently (see, e.g., [Papadimitriou, 2007]). With commitment, Conitzer and Sandholm [2006] give an efficient algorithm for computing a Stackelberg equilibrium in two-player normal-form games (see also von Stengel and Zamir [2010]), and that this becomes hard with 3 players; however, if the committing player can also send signals to the other players, thereby effectively taking over the role of the mediator in correlated equilibrium, then the problem is again efficiently solvable with any number of players [Conitzer and Korzhyk, 2011]. So in short, efficient equilibrium computation is possible if either correlation or commitment is allowed.\nEquilibrium computation becomes more difficult in dynamic environments, such as extensive-form games and stochastic games. There, commitment does not imply efficient computation anymore: Letchford and Conitzer [2010] show that it is NP-hard to compute a Stackelberg equilibrium in two-player extensive-form games, even with perfect information. Moreover, their hardness result holds even if normal-form correlation (as opposed to extensive-form correlation to be discussed momentarily) is allowed. Similar hardness results hold for various structured families of stochas-"}, {"title": "2 Preliminaries", "content": "Stochastic games. We focus on two-player finite-horizon turn-taking stochastic games in this paper. There is a finite set of states \\(S = [n]\\), and a finite set of actions \\(A = [m]\\). \\(S_{init} = 1\\) and \\(S_{term} = n\\) are the initial and terminal states, respectively. For each state \\(s \\in S\\), there is an acting player \\(ap(s) \\in \\{1, 2\\}\\), who unilaterally decides which action to play in state \\(s\\). For each player \\(i \\in \\{1, 2\\}\\), there is a reward function \\(r_i : S \\times A \\rightarrow \\mathbb{R}_+\\), which specifies the immediate reward \\(r_i(s, a)\\) that player \\(i\\) receives when action \\(a\\) is played in state \\(s\\). We assume rewards are normalized, i.e., \\(r_i(s, a) \\in [0, 1]\\) for each \\(i \\in \\{1, 2\\}\\), \\(s \\in S\\) and \\(a \\in A\\). A transition operator \\(P : S \\times A \\rightarrow \\Delta(S)\\) specifies the distribution \\(P(s, a)\\) of the next state when action \\(a\\) is played in state \\(s\\), where for each \\(s'\\), \\(P(s, a, s')\\) is the probability that the next state is \\(s'\\).\nUnless otherwise specified, we assume the transition operator is acyclic, i.e., \\(P(s, a, s') > 0\\) only if \\(s' > s\\) or \\(s = s' = n\\). For the terminal state \\(s_{term} = n\\) in particular, we assume \\(r_i(n, a) = 0\\) and \\(P(n, a, n) = 1\\) for each action \\(a \\in A\\). In other words, there is no meaningful action in the terminal state \\(S_{term} = n\\). These assumptions essentially mean the game is finite-horizon. In particular, note that in the finite-horizon case, the acyclicity assumption is without loss of generality, as the state could include the index of the current period (with a blowup proportional to the time horizon \\(T\\)).\nHistories, strategies, and utilities. Fix a stochastic game \\((S, A, ap, r_1, r_2, P)\\). A history \\(h\\) of length \\(t\\) is a sequence of \\(t\\) states and \\(t\\) actions \\(h = (s_1, a_1, s_2, a_2, ..., s_t, a_t)\\) which fully describes \\(t\\) consecutive steps of a play. Let \\(H\\) be the collection of all histories of all lengths not exceeding \\(n\\) (so \\(H\\) is finite). For brevity, we also let \\(|h|\\) denote the length of \\(h\\), and \\(h + (s, a)\\) be the history obtained by appending \\((s, a)\\) to the end of \\(h\\).\nA deterministic (history-dependent) strategy \\(\\pi : H \\times S \\rightarrow A\\) maps each history-state pair \\((h, s)\\) to the action to be played in \\(s\\) given history \\(h\\). Note that we do not explicitly partition a strategy into two parts corresponding to the two players, since such a partition is induced by the mapping \\(ap\\) from each state to the corresponding acting player. A randomized strategy \\(\\Pi\\) is a distribution over deterministic strategies. For any deterministic strategy \\(\\pi\\), we say a history \\(h = (s_1, a_1, ..., s_t, a_t)\\) is admissible if the action played in each step is the one specified by \\(\\pi\\), i.e., for each \\(t' \\in [t]\\), \\(\\pi((s_1, a_1,..., s_{t'-1}, a_{t'-1}), s_{t'}) = a_{t'}\\). For any randomized strategy \\(\\Pi\\), we say a history \\(h\\) is admissible under \\(\\Pi\\) if \\(h\\) is admissible under some \\(\\pi\\) in the support of \\(\\Pi\\). Let \\(H^{\\pi}\\) (resp. \\(H^{\\Pi}\\)) be the set of admissible histories under \\(\\pi\\) (resp. \\(\\Pi\\)). For a randomized strategy \\(\\Pi\\) and an admissible history \\(h\\in H^{\\Pi}\\) under \\(\\Pi\\), let \\(\\Pi | h\\) denote the conditional version of \\(\\Pi\\) given that the states reached and actions played in the first \\(|h|\\) steps are \\(h\\).\nFor each \\(i \\in \\{1, 2\\}\\), the onward utility \\(u_\\pi^i(h, s)\\) of a player \\(i\\), under a deterministic strategy \\(\\pi\\),"}, {"title": "Extensive-form correlated equilibria.", "content": "Extensive-form correlated equilibria (EFCE) and their Stackelberg version (Stackelberg EFCE, or SEFCE) are the natural generalizations of correlated equilibria and Stackelberg equilibria to dynamic settings such as stochastic games and extensive-form games. In the original definition of EFCE for extensive-form games by von Stengel and Forges [2008], a mediator specifies a distribution over deterministic strategies (i.e., a randomized strategy according to our definition above), where each deterministic strategy specifies a recommended action in each node of the game tree (corresponding to a history-state pair in our formulation). A deterministic strategy is drawn and fixed at the beginning of the play, but the recommended action in each node given by this strategy is revealed to the acting player only when the node is actually reached. If a player decides to not follow a recommended action, that player will not receive recommended actions in the rest of the play.\nFor any \\(\\varepsilon \\geq 0\\), we say a player is \\(\\varepsilon\\)-best responding under a randomized strategy if that player cannot increase their onward utility by more than \\(\\varepsilon\\) by deviating from the recommended action at any point of a recommended path of play, i.e., an admissible history. A randomized strategy is an \\(\\varepsilon\\)-EFCE if both players are \\(\\varepsilon\\)-best responding. Moreover, consider a Stackelberg setting where player 1 is the leader and player 2 is the follower. Then, a randomized strategy is an SEFCE if player 1's utility is maximized subject to the constraint that player 2 is 0-best responding (or simply best responding).\nPut in our language, a player \\(i \\in \\{1, 2\\}\\) is \\(\\varepsilon\\)-best responding under a randomized strategy \\(\\Pi\\) iff for any admissible history \\(h\\in H^{\\Pi}\\), state \\(s\\) where \\(ap(s) = i\\), action \\(a\\) where \\(h + (s, a) \\in H^{\\Pi}\\), and deterministic strategy \\(\\pi'\\) where \\(\\pi'(h, s) \\neq a\\):\n\nHere, \\((i : \\pi', 3 - i : \\pi)\\) denotes a strategy obtained by combining \\(\\pi'\\) restricted to player \\(i\\)'s actions and \\(\\pi\\) restricted to (3 \u2013 i)'s actions (note that 3 - i = 2 when i = 1, and vice versa; 3 - i simply"}, {"title": "3 Stackelberg Extensive-Form Correlated Equilibria", "content": "3.1 Overview of Our Approach\nMaximum punishment without loss of generality. Our algorithm is based on the standard observation that in an SEFCE, it is without loss of generality to maximally punish the follower when they deviate from the prescribed path of play, regardless of how that would affect the leader's utility at that point. In fact, for any SEFCE, there exists an effectively equivalent SEFCE where deviation always immediately triggers maximum punishment, so once the follower deviates, the game immediately becomes effectively zero-sum. This is because intuitively, the sole purpose of the leader's equilibrium strategy in parts of the game where the follower has deviated is to threaten the follower and cancel out any potential incentive to deviate. In particular, such a threat would never be actually executed, because in equilibrium no player would deviate in the first place. As such, it never hurts to threaten with the worst punishment possible. This greatly simplifies the problem from a computational perspective, since computing a strategy for maximum punishment is no harder than solving turn-taking zero-sum stochastic games, which can be done by simple backward induction.\nReducing to constrained planning. Once the punishment strategy is fixed, we only need to optimize over strategies where the follower never faces worse utility than what they would face after deviating in the optimal way and being maximally punished thereafter. One key observation here is that in any state, regardless of the recommended action, the optimal way to deviate is always the same. So, to prevent the follower from deviating, we only need to guarantee that conditioned on the recommended action, the onward utility of the follower is at least the utility resulting from deviating optimally. In particular, the latter utility depends only on the state (which is only true in turn-taking stochastic games). Given this observation, the problem becomes a constrained planning problem, where we want to find an optimal strategy subject to the constraint that in each state where the follower is the acting player, the onward utility of the follower (conditioned on the recommended action) is at least some state-dependent quantity that can be efficiently pre-computed. This is very similar to planning in constrained MDPs, except for one key difference: In constrained"}, {"title": "3.2 Reduction to Constrained Planning", "content": "We first provide a formal reduction from computing an SEFCE to the constrained planning problem.\nThe punishment amplifier. Fixing a stochastic game \\((S, A, ap, r_1, r_2, P)\\), our reduction involves the punishment amplifier \\(pa\\), which maps each deterministic strategy to its maximally punishing version against a subset of players for SEFCE this subset is \\(\\{2\\}\\), and as we will see later, for EFCE this subset is \\(\\{1, 2\\}\\). For each player \\(i \\in \\{1, 2\\}\\), consider the zero-sum stochastic game \\((S, A, ap, r'_1, r'_2, P)\\), where \\(r'_i = r_i\\) and \\(r'_{3-i} = -r_i\\). Let \\(\\pi_i\\) be a deterministic subgame-perfect equilibrium strategy in this zero-sum game here, \\(i\\) is the player being punished, but note that \\(\\pi_i\\) comprises both players' actions. Such a strategy can be found by backward induction. Note that without loss of generality, \\(\\pi_i\\) is history-independent, so we write \\(\\pi_i(s)\\) for simplicity. If there are multiple candidates for \\(\\pi_i\\), we pick an arbitrary one among them (the choice does not affect our results).\nGiven a deterministic strategy \\(\\pi\\) and a subset of players \\(S \\subseteq \\{1, 2\\}\\), the punishment-amplified version \\(\\pi' = pa(\\pi, S)\\) of \\(\\pi\\) is given by: For each \\(h = (s_1, a_1, ..., s_t, a_t) \\in H\\) and \\(s \\in S\\), \n\u2022 If \\(h\\in H^{\\pi}\\) (i.e., \\(h\\) is feasible under \\(\\pi\\)), then \\(\\pi'(h, s) = \\pi(h, s)\\).\n\u2022 If \\(ap(s) \\notin S\\), then \\(\\pi'(h, s) = \\pi(h, s)\\).\n\u2022 Otherwise, \\(\\pi'(h, s) = \\pi_i(s)\\), where \\(h' = (s_1, a_1,..., s_t)\\) is the longest feasible prefix of \\(h\\), and \\(i = ap(s_{t'})\\) (i.e., \\(i\\) is the first player who deviated, in state \\(s_{t'}\\)).\nThe punishment amplifier can be naturally extended to randomized strategies: For a randomized strategy \\(\\Pi\\) and a subset of players \\(S\\), \\(pa(\\Pi, S)\\) is obtained by mapping every deterministic strategy \\(\\pi\\) in the support of \\(\\Pi\\) to \\(pa(\\pi, S)\\), and assign the latter the same probability mass in \\(\\Pi'\\) as \\(\\pi\\) has in \\(\\Pi\\).\nWe first prove maximum punishment is without loss of generality, which is formally captured by the following lemma:\nLemma 1. Fix a stochastic game \\((S, A, ap, r_1, r_2, P)\\). For any \\(\\Pi\\) under which the follower is best responding, the follower is also best responding under \\(pa(\\Pi, \\{2\\})\\), and moreover,\n\nWe defer the proof of Lemma 1, as well as all other missing proofs, to Appendix A. The lemma suggests that when optimizing over strategies where the follower is best responding, we can focus on those with the maximum punishment structure as described above. Next we show this gives us a reduction to the constrained planning problem."}, {"title": "3.3 Pareto Frontier Curves", "content": "Before proceeding to the full description of our algorithm, we first quickly (and somewhat informally) define Pareto frontier curves and discuss some useful properties. Intuitively, the Pareto frontier curve \\(f_{s, a}\\) for a state-action pair \\((s, a)\\) is the curve capturing all Pareto-optimal pairs of"}, {"title": "3.4 Evaluating the Pareto Frontier Curves", "content": "Observe that if we can evaluate \\(f_{s_{init}, a}\\) for each \\(a \\in A\\), then it is not hard to find the optimal utilities of the two players induced by a feasible strategy, given a particular objective direction \\(a \\in \\mathbb{R}^2\\) (for an SEFCE in particular, we want \\(a = (1, 0)\\)): Without loss of generality, an optimal strategy picks a deterministic action in the initial state \\(s_{init}\\), so we only need to try every one of the actions. For each action \\(a \\in A\\), the optimal utilities induced by a strategy that is feasible after \\(s_{init}\\) is \\(f_{s, a}(a)\\). If \\(ap(s_{init}) = 1\\) or \\(f_{s, a}(a)_{ap(s_{init})} \\geq u_p(s_{init})\\), then this strategy is a feasible strategy, and \\(f_{s, a}(a)\\) gives the optimal utilities if the first action is \\(a\\). Otherwise, since \\(f_{s, a}\\) is concave, the optimal utilities induced by a feasible strategy must correspond to the point on \\(f_{s, a}\\) where the constraint in \\(s_{init}\\) is binding. More specifically, suppose \\(ap(s_{init}) = 2\\), and let \\(y_{s_{init}} = u_p(s_{init})\\). Then the optimal utilities when the first action is \\(a\\) must be \\((f_{s, a}(y_{s_{init}}), y_{s_{init}})\\).\nPivotal points. The above discussion suggests that the point \\((f_{s, a}(y_{s_{init}}), y_{s_{init}})\\) plays a partic-ularly important role in finding the optimal utilities. More generally, we define the pivotal point \\(pp(s, a)\\) on \\(f_{s, a}\\) for each \\((s, a)\\) where \\(ap(s) = 2\\) to be the rightmost point (if there is one) on \\(f_{s, a}\\) such that \\(pp(s, a)_{ap(s)} \\geq u_p(s)\\). If such a point does not exist, then we let \\(pp(s, a) = (-\\eta, -\\eta)\\)"}, {"title": "3.5 Algorithm and Analysis", "content": "Now we are ready to formally describe and analyze our full algorithm, Algorithm 1, which calls Algorithm 2 as a subroutine.\nBelow we analyze our algorithm. First we show that the binary search in Algorithm 1 is exact, in the sense that in line 18, \\(q_l\\) and \\(q_r\\) are adjacent turning points to each other on \\(f_{s, a}\\)."}, {"title": "3.6 Decoding the Output Strategy", "content": "Now we discuss the final missing piece of our algorithm: extracting the strategy encoded in the output of Algorithm 1. We present a procedure, Algorithm 3, which, given the output of Algorithm 1, computes a random action for any given history-state pair. We will prove that the strategy implicitly given by Algorithm 3 is the one encoded in the output of Algorithm 1. In particular, it is feasible, and achieves the leader's utility in an SEFCE computed by Algorithm 1."}, {"title": "4 Approximately Optimal Extensive-Form Correlated Equilibria", "content": "Now we proceed to the computation of approximately optimal EFCE. We present a bi-criteria algorithm that, given an objective direction (i.e., a combination of the two players' utilities), computes an \\(\\varepsilon\\)-EFCE whose objective value is at least that of the optimal EFCE minus \\(\\varepsilon\\), in time log(1/\\(\\varepsilon\\)).\nThe idea and structure of our algorithm for approximately optimal EFCE is overall quite similar to that for SEFCE. There are two key differences:\n\u2022 Recall that for SEFCE, we optimize over strategies where player 2 is best responding. This reduces to optimizing over feasible strategies, where feasibility means that when player 2 is the acting player, their onward utility must be at least the utility under punishment. For \\(\\varepsilon\\)-EFCE, both players need to be \\(\\varepsilon\\)-best responding, which leads to a different definition for feasible strategies. The definition and structural properties of Pareto frontier curves also need to be modified accordingly. Such modifications lead to minor changes in the proofs of the structural properties and the algorithm.\n\u2022 A more substantial difference is in the numerical resolution of the Pareto frontier curves. For SEFCE, the feasibility constraints are all in the same direction, i.e., parallel to the x-axis. This is no longer true for \\(\\varepsilon\\)-EFCE, where the direction of the feasibility constraint in a state depends on the acting player. Such alternating constraints break the asymmetry between the"}, {"title": "4.1 Useful Facts", "content": "Before stating the full algorithm, we quickly state the new reduction from \\(\\varepsilon\\)-EFCE to constrained planning, as well as modified definitions and properties of Pareto frontier curves. The proofs of these properties are similar to those of their counterparts for SEFCE.\nReduction to constrained planning.\nLemma 8. Fix a stochastic game \\((S, A, ap, r_1, r_2, P)\\). For any \\(\\varepsilon > 0\\) and \\(\\Pi\\) under which both players are \\(\\varepsilon\\)-best responding, both players are also \\(\\varepsilon\\)-best responding under \\(pa(\\Pi, \\{1, 2\\})\\), and moreover,\nfor each \\(i \\in \\{1, 2\\}\\),\n\nLemma 9. Fix a stochastic game \\((S, A, ap, r_1, r_2, P)\\). For any \\(\\varepsilon > 0\\) and randomized strategy \\(\\Pi\\), both players are \\(\\varepsilon\\)-best responding under \\(pa(\\Pi, \\{1, 2\\})\\) if the following condition holds: For each admissible history \\(h \\in H^{\\Pi}\\), state \\(s \\in S\\), and action \\(a \\in A\\) such that \\(h + (s, a) \\in H^{\\Pi}\\), \n\nwhere \\(\\tau_{ap(s)}\\) is the subgame perfect equilibrium when player 3-\\(ap(s)\\) tries to minimize player \\(ap(s)\\)'s utility, as defined above.\nTheorem 4. Fix a stochastic game \\((S, A, ap, r_1, r_2, P)\\). For any \\(\\varepsilon \\geq 0\\) and \\((x, y) \\in \\mathbb{R}^2\\), there exists a strategy \\(\\Pi\\) under which both players are \\(\\varepsilon\\)-best responding such that \\(u^{\\Pi}_1(\\emptyset, s_{init}) > x\\) and \\(u^{\\Pi}_2(\\emptyset, s_{init}) \\geq y\\), if and only if there exists a strategy \\(\\Pi'\\) such that \\(u^{\\Pi'}_1(\\emptyset, s_{init}) > x\\), \\(u^{\\Pi'}_2(\\emptyset, s_{init}) \\geq y\\), and for each admissible history \\(h \\in H^{\\Pi'}\\), state \\(s \\in S\\), and action \\(a \\in A\\) such that \\(h + (s, a) \\in H^{\\Pi'}\\), \n\nFeasible strategies. We say a strategy \\(\\Pi\\) is \\(\\varepsilon\\)-feasible (we will omit \\(\\varepsilon\\) when it is clear from the context) if it satisfies the condition in the corollary which involves the utility under punishment, i.e., for each \\(h\\in H^{\\Pi}\\), \\(s \\in S\\), and \\(a \\in A\\) such that \\(h + (s, a) \\in H^{\\Pi}\\), \n\nWe say a strategy \\(\\Pi\\) is \\(\\varepsilon\\)-feasible after \\(s\\), if for each \\(h\\in H^{\\Pi}\\), \\(s' > s\\), and \\(a \\in A\\) such that \\(h + (s', a) \\in H^{\\Pi}\\),"}, {"title": "4.2 Algorithm and Analysis", "content": "Now we are ready to present and analyze our algorithm for approximately optimal EFCE, Algorithm 4, which uses Algorithm 5 as a subroutine. We defer both these algorithms, as well as Algorithm 6 to be mentioned later, to Appendix B, since these algorithms are similar to their counterparts in Section 3.\nThe key differences between Algorithms 4 and 5, and Algorithms 1 and 2, are:"}]}