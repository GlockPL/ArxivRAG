{"title": "RGMDT: Return-Gap-Minimizing Decision Tree Extraction in Non-Euclidean Metric Space", "authors": ["Jingdi Chen", "Hanhan Zhou", "Yongsheng Mei", "Carlee Joe-Wong", "Gina Adam", "Nathaniel D. Bastian", "Tian Lan"], "abstract": "Deep Reinforcement Learning (DRL) algorithms have achieved great success in solving many challenging tasks while their black-box nature hinders interpretability and real-world applicability, making it difficult for human experts to interpret and understand DRL policies. Existing works on interpretable reinforcement learning have shown promise in extracting decision tree (DT) based policies from DRL policies with most focus on the single-agent settings while prior attempts to introduce DT policies in multi-agent scenarios mainly focus on heuristic designs which do not provide any quantitative guarantees on the expected return. In this paper, we establish an upper bound on the return gap between the oracle expert policy and an optimal decision tree policy. This enables us to recast the DT extraction problem into a novel non-euclidean clustering problem over the local observation and action values space of each agent, with action values as cluster labels and the upper bound on the return gap as clustering loss. Both the algorithm and the upper bound are extended to multi-agent decentralized DT extractions by an iteratively-grow-DT procedure guided by an action-value function conditioned on the current DTs of other agents. Further, we propose the Return-Gap-Minimization Decision Tree (RGMDT) algorithm, which is a surprisingly simple design and is integrated with reinforcement learning through the utilization of a novel Regularized Information Maximization loss. Evaluations on tasks like D4RL show that RGMDT significantly outperforms heuristic DT-based baselines and can achieve nearly optimal returns under given DT complexity constraints (e.g., maximum number of DT nodes).", "sections": [{"title": "1 Introduction", "content": "Deep Reinforcement Learning (DRL) has significantly advanced real-world applications in various domains [5, 12, 13, 16, 17, 29, 44, 69, 71]. However, the black-box nature of DRL's"}, {"title": "2 Related Work", "content": "Effort on Interpretability for Understanding Decisions. To enhance interpretability in decision-making models, one strategy involves crafting interpretable reward functions within inverse reinforcement learning (IRL), as suggested by [10, 68, 67]. This approach offers insights into the underlying objectives guiding the agents' decisions. Agent behavior has been conceptualized as showing pref-erences for certain counterfactual outcomes [7], or as valuing information differently when under time constraints [34]. However, extracting policies through black-box reinforcement learning (RL) algorithms often conceals the influence of observations on the selection of actions. An alternative is to directly define the agent's policy function with an interpretable framework. Reinforcement learning policies have thus been articulated using a high-level programming language [60], or by framing explanations around desired outcomes [66], facilitating a more transparent understanding of decision-making processes.\nInterpretable RL via Decision Tree-based models. Since their introduction in the 1960s, DTs have been crucial for interpretable supervised learning [50, 47, 53]. The CART algorithm [8], established in 1984, is foundational in DT methodologies and underpins Random Forests (RF) [9] and Gradient Boosting (GB) [23], which are benchmarks in predictive modeling. These techniques are central to platforms like ranger and scikit-learn and continue to evolve, as seen in the iterative random forest, which explores stable interactions in data [64, 52, 4, 70]. To interpret an RL agent, Frosst et al [25] explain the decisions made by DRL policies by using a trained neural net to create soft decision trees. Coppens et al. [19] propose distilling the RL policy into a differentiable DT by imitating a pre-trained policy. Similarly, Liu et al. [39] apply an imitation learning framework to the Q-value function of the RL agent. They also introduce Linear Model U-trees (LMUTs), which incorporate linear models in the leaf nodes. Silva et al. [57] suggest using differentiable DTs directly as function approximators for either the Q function or the policy in RL. Their approach includes a discretization process and a rule list tree structure to simplify the trees and enhance interpretability. Additionally, Bastani et al. [3] propose the VIPER method, which distills policies as neural networks into a DT policy with theoretically verifiable capabilities that follow the Dataset Aggregation (DAGGER) method proposed in [54], specifically for imitation learning settings and nonparametric DTs. Ding et al. [20] try to solve the instability problems when using imitation learning with tree-based model generation and apply representation learning on the decision paths to improve the decision tree-based explainable RL results, which could achieve better performance than soft DTs. Milani et al. extend VIPER methods into multi-agent scenarios [48] in both centralized and decentralized ways, they also summarize a paper about the most recent works in the fields of explainable AI [49], which confirms the statements that small DTs are considered naturally interpretable.\nHowever, traditional DT methods are challenging to integrate with RL due to their focus on cor-relations within training data rather than accounting for the sequential and long-term implications in dynamic environments. Imitation learning approaches have been explored in single-agent RL settings [42, 55, 58]. For instance, VIPER [3] trains DTs with samples from a trained DRL policy, but its return gap depends on the number of samples needed and does not apply to DTs with arbitrary size constraints (e.g., maximum number of nodes L). DT algorithms remain under-explored in multi-agent settings, except for MAVIPER [48] that extends VIPER to multi-agent scenarios[46] while lacking performance guarantees and remaining a heuristic design."}, {"title": "3 Preliminaries and Problem Formulation", "content": "A Dec-POMDP [6] models cooperative MARL, where agents lack complete information about the environment and only have local observations. We formulate a Dec-POMDP as a tuple D = (S, \u0391, \u03a1, \u03a9, \u03b7, R, \u03b3), where S is the joint state space and A = A\u2081 \u00d7 A2 \u00d7 \uff65\uff65\uff65 \u00d7 An is the joint action space, where a = (a1,a2,..., an) \u2208 A denotes the joint action of all agents. P(s'|s, a) : S\u00d7A\u00d7S \u2192 [0, 1] is the state transition function. \u03a9 is the observation space. n is the total number of agents. R(s, a) : S \u00d7 A \u2192 R is the reward function in terms of state s and joint action a, and \u03b3 is the discount factor. Given a policy \u03c0, we consider the average expected return J(\u03c0) = lim_{T\u2192\u221e}(1/T)\u0395\u03c0[\u03a3_{t=0}^T Rt]. The goal of this paper is to minimize the return gap between"}, {"title": "4 Theoretical Results and Methodology", "content": "We first recast the problem as clustering in a non-Euclidean Metric Space and then prove a single-agent result. Then we expand the single-agent result to multi-agent settings. Since directly minimizing the return gap is intractable, we bound the performance of RGMDT with the return gap between the oracle policy \u03c0* = [\u03c0\u0390(ai|01,..., On), \u2200i] corresponding to obtaining the action-values Q\u03c0* and optimal decision tree policy TL = [T\u00a3,\u2026\u2026\u2026,T\u2193], where each T\u2081 can only have L nodes, and n is the total number of agents. For simplicity, we use V* to represent V\u3160*, and Q* to represent Q. We assume that observation/action spaces defined in the Dec-POMDP tuple are discrete with finite observations and actions, i.e., |\u03a9| < \u221e and |A| < \u221e. For Dec-POMDPs with continuous observation and action spaces, the results can be easily extended by considering cosine-distance between action-value functions and replacing summations with integrals, or sampling the action-value functions as an approximation.\nLemma 4.1. (Policy Change Lemma.) For any policies \u03c0* and DT policy TL with L leaf nodes, the optimal expected average return gap is bounded by:\n$J(\u03c0*) \u2013 J(TL) \u2264\\sum_{o\\sim\\mu}\\sum_{l}[Q^* (o, a^{\u03c0*}) \u2013 Q^{T_L} (o, a^{T_L})]d^\u03bc (o),$\nwhere $d^\u03bc (o) = (1 - \u03b3) \\sum_{t=0}^{\u221e}\u03b3^t \u00b7 P(o_t = o|T_L, \u03bc),$ is the y-discounted visitation probability under decision tree $T_L$ and initial observation distribution \u00b5, and $\\sum_{o\\sim l}$ is a sum over all observations corresponding to the decision path from the parent node to the leaf node l, where l indicates its class.\nProof Sketch. Our key idea is to leverage the state value function $V^{T_L} (o)$ and its corresponding action-value function $Q^{T_L} (o, a)$ in Eq.(2) to unroll the Dec-POMDP from timestep t = 0 and onward. Detailed proof is provided in the Appendix.\nThen we define the action-value vector corresponding to observation oj, i.e.,\n$Q^* (o_j) = [Q^* (o_j, o_{-j}), \\forall o_{-j}],$\nwhere o_j are the observations of all other agents and $Q^*(o_j, o_{-j})$ is a vector of action-values weighted by marginalized visitation probabilities $d^\u03bc(o_{-j}|o_j)$ and corresponding to different actions, i.e., $Q^* (o_j, o_{-j}) = [Q^*(o_j, o_{-j}, \u03c0^*(a_j|o_j), a_j) d(o_{-j}|o_j)]$. At the initial iteration step of the iteratively-grow-DT process, since we did not grow the DT for agent j yet, we use oracle policy \u03c0* to give a deterministic action $a_j^* = argmax_{a_j} Q^*(o_j, a_j)$ based on oj for obtaining action-value"}, {"title": "5 Constructing SVM-based Decision Tree to Minimize Return Gaps", "content": "The result in Thm. 4.4 inspires a iteratively-grow-DT framework - RGMDT, which constructs return-gap-minimizing multi-agent decentralized DTs of arbitrary sizes. This is because RGMDT grows a binary tree iteratively (in both single and multi-agent cases) until the desired complexity is reached. The method addressed two challenges: (1). RGMDT constructs the optimal DT that minimizes the return gap given the complexity of the DT (e.g., the number of the leaf nodes). (2). RGMDT addressed the scalability problems of multi-agent DT construction with provided theoretical guarantees. We summarize the pseudo-code in the Appendix. E.\nNon-Euclidean Clustering Labels Generation. We approximate the non-Euclidean clustering labels lj = g(oj) for each agent using DNNs parameterized by \u03be = {\u00a71,..., \u03be\u03b7}. Prior to growing the decision tree (DT) for each agent j, we sample a minibatch of K\u2081 transitions Xj from the replay buffer R, which includes observation-action-reward pairs from all agents. We then identify the top K2 most frequent observations ok2, in Xj, and retrieve their associated actions using the pre-trained policy \u03c0*, forming a set X_j. We combine these samples with (0j, \u03c0*(0;)) from X; to form the dataset D for training. The oracle critic networks, parameterized by w, compute the action-values for this dataset, approximating the vectors Q* (oj). g\u00a3; (oj) is updated by optimizing a Regularized Information Maximization (RIM) loss function [32]:\n$L(g_{\\xi_j}) = \\sum_{p=1}^{K_1} \\sum_{q \\in N_{K_3} (p)} [D_{cos}(Q^* (o_p), Q^* (o_q))] ||l_{g_{\\xi_j} (o_p)} - l_{g_{\\xi_j} (o_q)}||^2 - [H(m_j) - H(m_j|o_j)],$\nthe first term is a locality-preserving clustering loss, which enhances the cohesion of clusters by encouraging action-value vectors close to each other to be grouped together. This is achieved using the cosine distance Dcos to identify the K3 nearest neighbors of each action-value vector. The second term, the mutual information loss, quantifies the mutual information between the observation oj and the cluster label mj. It aims to balance cluster size and clarity by evaluating the difference between the marginal entropy H(mj) and the conditional entropy H(mj|0j)."}, {"title": "6 Evaluation and Results", "content": "Experiment Setup and Baselines. We test RGMDT on both discrete and continuous state space problems in the maze environments and the D4RL [26]. To explore how well RGMDT scales in multi-agent environments, we also constructed similar target-chasing tasks in the maze following the same settings as the Predator-Prey tasks in the Multi-Agent Particle Environment (MPE) [40] (detailed in F.1.2). We use Centralized Q Learning and Soft Actor-critic (SAC) [30] to obtain the action-value vectors. We compare RGMDT against strong baselines. The baselines include different types of Imitation DTs [48]: Each DT policy is directly trained using a dataset collected by running the expert policies for multiple episodes. No resampling is performed. The observations of an agent are the features, and the actions of that agent are the labels. (1). Single Tree Model using CART [8], which is the most famous traditional DT algorithm whose splits are determined by choosing the best cut that maximizes the gain. (2). DT algorithms with Bootstrap Aggregating error correction methods: Random Forest (RF) [9] that reduces variance by averaging multiple deep DTs, trained on different parts of the same training set; Extra Trees (ET) [28] that uses the whole dataset to grow the trees which make the boundaries more randomized. (3). Boosting For Error Correction, Gradient Boosting DTs [24] that builds shallow trees in succession where each new tree helps to correct errors made by the previously trained tree. (4). Multi-agent Verifiable Reinforcement Learning via Policy Extraction (MAVIPER): A centralized DT training algorithm that jointly grows the trees of each agent [48], which extends VIPER [3] in multi-agent settings. All the experiments are repeated 3-5 times with different seeds. More details about the evaluations are in Appendix F.\nSingle Agent Task. In the single-agent task, an agent is trained to navigate to a target without colliding with walls to complete the task. We increase the complexity of the environment and evaluate the RGMDT across three different levels of complexity (detailed in"}, {"title": "7 Conclusion and Future Works", "content": "This paper proposes a DT extraction framework, RGMDT, that is proven to achieve a closed-form guarantee on the expected return gap between a given RL policy and the resulting DT policy. Each decision path of the extracted DT maps a subset of observations to an action attached to the leaf node. We consider DT extraction as an iterative non-euclidean clustering problem of the observations into different decision paths, with actions at leaf nodes as labels, as guided by action-value vectors. Therefore, RGMDT supports an iterative algorithm to generate return-gap-minimizing DTs of arbitrary size constraints. For multi-agent DT extraction, we develop an iteratively-grow-DT process, which iteratively identifies the best step to grow the DT of each agent, conditioned on the current DTs of other agents by revising the resulting action-value function until the desired complexity is reached. Experimental results show that RGMDT significantly outperforms the baselines and achieves nearly optimal returns. Further research will explore regression trees more suitable for continuous state/action spaces and incorporate a re-sampling module to enhance RGMDT's performance."}, {"title": "A Impact Statements", "content": "The proposed framework can iteratively grow decision trees for multi-agent environments, while we do not see an immediate threat of ethical and societal consequences, it is essential to address and mitigate any risks of misuse or unintended consequences from applying malicious training datasets or improper use of certain scenarios."}, {"title": "B Additional Related Work Discussion", "content": "Effort on Interpretability for Understanding Decisions. To enhance interpretability in decision-making models, one strategy involves crafting interpretable reward functions within inverse rein-forcement learning (IRL), as suggested by [10]. This approach offers insights into the underlying objectives guiding the agents' decisions. Agent behavior has been conceptualized as showing pref-erences for certain counterfactual outcomes [7], or as valuing information differently when under time constraints [34]. However, extracting policies through black-box reinforcement learning (RL) algorithms often conceals the influence of observations on the selection of actions. An alternative is to directly define the agent's policy function with an interpretable framework. Reinforcement learning policies have thus been articulated using a high-level programming language [60], or by framing explanations around desired outcomes [66], facilitating a more transparent understanding of decision-making processes.\nDecision Trees. Since their introduction in the 1960s, DTs have been crucial for interpretable supervised learning [50, 47, 53]. The CART algorithm [8], established in 1984, is foundational in DT methodologies and underpins Random Forests (RF) [9] and Gradient Boosting (GB) [23], which are benchmarks in predictive modeling. These techniques are central to platforms like ranger and scikit-learn and continue to evolve, as seen in the iterative random forest, which explores stable interactions in data [64, 52, 4, 70]. However, traditional DT methods are challenging to integrate with RL due to their focus on correlations within training data rather than accounting for the sequential and long-term implications in dynamic environments. Imitation learning approaches have been explored in single-agent RL settings [42, 55, 58]. For instance, VIPER [3] trains DTs with samples from a trained DRL policy, but its return gap depends on the number of samples needed and does not apply to DTs with arbitrary size constraints (e.g., maximum number of nodes L). DT algorithms remain under-explored in multi-agent settings, except for MAVIPER [48] that extends VIPER to multi-agent scenarios[46] while lacking performance guarantees and remaining a heuristic design.\nInterpretable RL via Tree-based models. To interpret an RL agent, Frosst et al [25] explain the decisions made by DRL policies by using a trained neural net to create soft decision trees. Coppens et al. [19] propose distilling the RL policy into a differentiable DT by imitating a pre-trained policy. Similarly, Liu et al. [39] apply an imitation learning framework to the Q-value function of the RL agent. They also introduce Linear Model U-trees (LMUTs), which incorporate linear models in the leaf nodes. Silva et al. [57] suggest using differentiable DTs directly as function approximators for either the Q function or the policy in RL. Their approach includes a discretization process and a rule list tree structure to simplify the trees and enhance interpretability. Additionally, Bastani et al. [3] propose the VIPER method, which distills policies as neural networks into a DT policy with theoretically verifiable capabilities that follow the Dataset Aggregation (DAGGER) method proposed in [54], specifically for imitation learning settings and nonparametric DTs. Ding et al. [20] try to solve the instability problems when using imitation learning with tree-based model generation and apply representation learning on the decision paths to improve the decision tree-based explainable RL results, which could achieve better performance than soft DTs. Milani et al. extend VIPER methods into multi-agent scenarios [48] in both centralized and decentralized ways, they also summarize a paper about the most recent works in the fields of explainable AI [49], which confirms the statements that small DTs are considered naturally interpretable."}, {"title": "C Background and Preliminaries", "content": "C.1 Decision Tree\nMultivariate Decision Tree: Wang et al. [61] introduced two variants of multivariate decision tree classifiers structured as full binary trees: the Randomly Partitioned Multivariate Decision Tree (MDT1) and the Principal Component Analysis (PCA)-Partitioned Multivariate Decision Tree"}, {"title": "D Theoretical Proofs", "content": "D.1 Proofs for Lemma 4.1\nProof. To simplify, we denote the policy generated by decision tree TL as \u03c0. We prove the result in this lemma by leveraging observation-based state value function V\u2122 (o) in Eq.(2) and the 100corresponding action value function Q* (o, a) = \u0395 [\u03a3=Rtti Otot = o, at = a] to unroll the Dec-POMDP. Here we consider all other agents i \u2260 j as a conceptual agent denoted by -j.\n$J(\u03c0^*) \u2013 J(\u03c0) = \u0395_\u03bc(1 \u2212 \u03b3)[V^*(0(0)) \u2013 V^\u03c0 (0(0))],$\n$= \u0395_\u03bc(1 \u2212 \u03b3)[V^*(o_{-j}(0), o_j (0)) \u2013 V^\u03c0 (o_{-j} (0), o_j (0))],$\n$= \u0395_\u03bc(1 \u2212 \u03b3)[V^*(o_{-j} (0), o_j (0)) \u2013 Q^* (o_{-j} (0), o_j (0), a^* ) + Q^* (o_{-j} (0), o_j (0), a^*) \u2013 Q^\u03c0 (o_{-j} (0), o_j (0), a^\u03c0 )],$\n$= \u0395_\u03bc(1 \u2212 \u03b3)[\u0394^\u03c0 (o_{-j} (0), o_j (0), a) + (Q^* - Q^\u03c0 )],$\n$= \u0395_\u03bc(1 \u2212 \u03b3)[\u0394^\u03c0 (o_{-j} (0), o_j (0), a) + E_{o_{-j}(1),o_j(1)~P(:1 o_j(0),o_{-j}(0),a)} [(V^*(o_{-j}(1), o_j(1)) \u2013 V^\u03c0 (o_{-j}(1), o_j(1))],$\n$= E_{\u03bc,o_{-j}(t),o_j (t)~p}(1 \u2212 \u03b3)\\sum_{t=0}^{\u221e}[\u2206(o_{-j} (t), o_j (t), a^*) | a^\u03c0 ],$\n$= \\sum_{l}E_{o_{-j},o_j\\forall d_j}\u03c0(o_{-j},o_j,a^\u03c0)=\u03c0(o_{-j},g(o_j)) [\u2206(o_{-j}, o_j, a^*) | a^\u03c0 ],$\n$= \\sum_l\\sum_i\\sum_j[Q^*(o_{-j}, o_j, a^*) \u2013 Q^\u03c0 (o_{-j}, o_j, a)] d(o_{-j}, o_j),$\n$\\leq \\sum_l\\sum_i\\sum_j[Q^*(o_{-j}, o_j, a^*) \u2013 Q^\u03c0 (o_{-j}, o_j, a)] \u00b7d(o_{-j}, o_j),$\n$\\leq \\sum_{o\\sim\\mu} [Q^*(o, a^*) \u2013 Q^\u03c0 (o, a)] d(o),$\nStep 1 is to use Eq.(3) with initial observation distribution o(0) ~ \u00b5 at time t = 0 to re-write the average expected return gap; Step 2 is separate the observations as agent j and the other agents as a single conceptual agent -j, then o = (0-j, 0j); Step 3 and step 4 are to obtain the sub-optimality-gap of policy \u03c0, defined as \u2206\u03c0 (o, a) = V*(o) \u2013 Q*(o, a", "a": "by substracting"}, {"title": "E Algorithm", "content": "Iteratively-Grow-DT Process: To simplify, consider agents i \u2260 j as a conceptual super agent -j. With a limit of L leaf nodes, we iteratively grow DTs for agent j and super agent -j, adding two nodes for each agent per iteration with a total of log2(L) + 1 iterations. For agent j at iteration i = 0, $Q^{*i=0}(o_j, o_{-j}) = [Q^*(o_j, o_{-j}, \u03c0^*(a_j|o_j), a_j) d(o_{-j}|o_j)]$, apply Lemma. 4.3, we got l=0 = g(oj) and DT Ti=0 for iteration 0. For agent -j, we con-sider the clustering for $Q^{*i=0}(o_{-j}, o_j) = [Q^*(o_{-j}, o_j, \u03c0^*(a_{-j}|o_{-j}), T^{i=0}(o_j)) \u00b7d(o_j|o_{-j})]$. We use Ti=0(oj) to replace aj in $Q^{*i=0}(o_{-j}, o_j)$, since the agent j is restricted to taking the same actions for all of in the same cluster with label lj, apply Lemma. 4.3, we got DT T(o_j) in the interaction 0 for agent -j. For each iteration k, for growing DT for agent j, agent j takes actions based on Ti=k-1(oj), agent -j takes actions based on Tik-1(o_j), then $Q^*(o_j, o_{-j}) = [Q^*(o_j, o_{-j}, T^{i=k-1}(o_j), T^{i=k-1} (o_{-j})]$ is used for clustering and training DT Ti=k(oj); for growing DT for agent -j in the same iteration k, since agent j updates its"}]}