{"title": "OPEN LLAMA2 MODEL FOR THE LITHUANIAN LANGUAGE", "authors": ["Art\u016bras Nakvosas", "Povilas Daniu\u0161is", "Vytas Mulevi\u010dius"], "abstract": "In this paper, we propose and describe the first open Llama2 large language models (LLMs) for the Lithuanian language, including an accompanying question/answer (Q/A) dataset and translations of popular LLM benchmarks. We provide a brief review of open regional LLMs and detailed information on the proposed LLMs and their training process. We also conduct an empirical evaluation, comparing the perplexities of the proposed LLMs with those of other modern open LLMs. In addition, benchmarking the proposed LLMs against language understanding tasks reveals that high-quality pretraining datasets may be essential for achieving models that perform efficiently on these benchmarks. The full realisations of the described LLMs are available in the accompanying open repository https://huggingface.co/neurotechnology.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), relying on Transformer architecture [47] have shown remarkable effectiveness in many natural language processing (NLP) tasks [28, 30]. This has primarily been fuelled by increasingly large model parameterisations and training datasets, which are deemed essential according to neural scaling laws [16]. On the other hand, with the consistent advancement of computational linguistics and NLP, there were recently released open LLMs with performance characteristics comparable with their state-of-the-art (SOTA) commercial counterparts [45], [19], [20], [1], [12]. Although these open models are useful for further fine-tuning for various downstream problems, training of LLMs usually requires both massive datasets and considerable computational resources.\nIn the context of the current SOTA, open and commercial LLMs are usually trained on largely English texts [45], which results in the lack of performance for less common languages. In addition, commercial LLMs as a rule are not fully accessible (e.g. they are exposed only via APIs, which do not include the model's parameters nor its intermediate representations). Consequently, there have been multiple recent attempts to achieve efficient open LLMs, tailored for various regional languages other than English (e.g., Section 2, Table 1). Besides an improved performance for corresponding regional languages, compared to their predecessor LLMs, such open models also are potentially useful for research, as their internal mechanism is fully transparent, and there are related applications, both inside and outside the scope of NLP [28].\nThis article describes Neurotechnology's \u00b9 contribution to the direction of regional LLM research, encompassing\n\u2022\n\u2022 Llama2-based [45] 7 and 13 billion parameter LLMs for the Lithuanian language, and their empirical evaluation,\na new dataset, consisting of 13, 848 Q/A pairs primarily about Lithuania and Lithuanian history (in the Lithuanian language) [31],\n\u2022 translations of popular LLM benchmarks to the Lithuanian language,\n\u2022\nopen repository, containing all the mentioned components."}, {"title": "Related work", "content": "Llama2 model. Transformer-based Llama2 is available in different parameter sizes (e.g. 7B, 13B, and 70B parameters), and modifications (e.g. it also includes Llama2-chat version, which is optimised for dialogue use cases). The model is first pretrained using a 2 trillion token set, collected from public sources, and utilising a self-supervised autoregressive approach with cross-entropy loss. Afterwards, it is fine-tuned using publicly available instruction datasets, augmented with human-annotated data, and Reinforcement Learning with Human Feedback (RLHF) methodologies [45].\nThis model can support the maximum context length of 4096 tokens. According to benchmarks, Llama2 generally performs on par with many open alternatives (e.g. Falcon [1], Mistral [19], Mixtral [20], PaLM [4], etc.). As is common with large foundational models, it can be further successfully tuned for various downstream tasks, including regional language modelling.\nLLMs for regional languages. Table 1 summarises LLMs tailored for common European languages, reflecting the recent contributions from the research and engineering community working in this direction. We include only those regional LLMs, that meet the following criteria:\n\u2022 The model should be published in an open repository (e.g. Hugging Face\u00b2),\n\u2022 It should contain at least a minimal description (architecture, training data, and other details).\nAccording to Table 1, open LLMs are released for the majority of common European languages. Table 1 shows that Llama2 [45] and Mistral [19] are the leading architectures for open LLMs for regional European languages, and 7 billion parameter models are the most common. Table 1 also reveals that full-parameter training is conducted in the majority of cases (19 cases from 20), instead of the parameter-efficient fine-tuning (PEFT) based approach. However, in some instances (2 cases from 20) regional LLMs were trained using PEFT methods (e.g., LoRA [17], MoRA [21]), which may result in less accurate models compared to full-parameter training, although with the lower computational costs. In addition, quite often only the model itself is published (11 out of 20 cases), without an accompanying citable document (e.g. technical report/peer-reviewed publication), or training and evaluation datasets. In our opinion, the lack of accompanying scientific documentation limits the potential usefulness of the released regional LLMs in various important aspects, including their reproducibility, empirical performance assessment, and establishing a connection to the existing related results."}, {"title": "Proposed open LLMs and their evaluations", "content": "Proposed open LLMs and their training details. The proposed models (including tokenizers) are trained from Llama2 73 and 134 billion parameter checkpoints (further denoted by Llama2-7B and Llama2-13B, correspondingly). The training consists of two phases: the first one is standard autoregressive pretraining on the Lithuanian component of the CulturaX dataset [32], and the second one is fine-tuning on the Alpaca [43] dataset, which has been translated into Lithuanian using ChatGPT (gpt-4-1106-preview) and [31] dataset. We train the full model without using PEFT. Figure 5 shows the source distribution of the Lithuanian component of the CulturaX dataset, and Figure 6 shows the record length distribution in tokens. We use 2048 token context length during model training for both models. The models are trained on 8xH100 GPUs. Figure 1 shows loss during the model pretraining process. The training details for both LLMs are provided in Table 2 and the fine-tuning was conducted with the same parameters, except the learning rate, which was set to 0.00001. The download links for all the proposed LLMs are provided in Table 5.\nProposed open Q/A dataset [31]. This dataset was constructed from the ChatGPT [34] summarisations of a set of Lithuanian Wikipedia pages, and represents a collection of 13, 848 Q/A pairs primarily about Lithuania and Lithuanian history. It was not used during pretraining process. Table 3 showcases a set of examples from this dataset."}, {"title": "Perplexity evaluation benchmarks", "content": "We further conduct an empirical evaluation of the proposed Lithuanian LLMs using an open Lithuanian language dataset [31]. We analyse LLMs by examining their perplexity, which is defined as\n$P(W) := exp \\left( -\\frac{1}{N} \\sum_{i=1}^{N} log p(w_i | w_1, w_2,..., w_{i-1}) \\right)$\nwhere\n\u2022 W = w1, w2, ..., wn is the sequence of tokens,\n\u2022 N is the number of tokens in the sequence,"}, {"title": "Language understanding benchmarks", "content": "In this experiment, we evaluate the proposed LLMs using LM evaluation harness (LMEH) language understanding benchmarks [10] translated into Lithuanian language (see Table 5 for the download links). These benchmarks are created to assess LLMs across a wide range of evaluation tasks including Massive Multitask Language Understanding (MMLU) set [15], primarily covering diverse academic disciplines. Figure 3 and Figure 4 showcases the accuracies for a sequence of checkpoints, which correspond to the percentage of the pretraining data from CulturaX Lithuanian component, starting with 0% (which corresponds to the initial Llama2-7B), with the step of 10%. Similarly, Figure 7 and Figure 8 provide information about individual benchmarks from the MMLU set.\nAlthough for some tasks (e.g., arc_easy_lt, hellaswag_lt, winogrande_lt) we see consistent improvement throughout the entire pretraining process, this benchmark surprisingly reveals that in most cases of MMLU (see Figure 7 and Figure 8), there is no improvement compared to the initial model. We hypothesise that this may be because the Lithuanian component of CulturaX is almost exclusively collected through web crawling of common websites (see Figure 5), which does not include data that is relevant to those specific tasks. Therefore, the extensions of regional components of CulturaX with high-quality data may improve LLMs, tailored for the corresponding regional languages."}, {"title": "Conclusions", "content": "We presented the first Llama2-based open LLMs tailored especially for the Lithuanian language. Our model is released with the accompanying QA dataset [31] and translated standard LLM benchmarks.\nThe motivation for our research was to achieve Lithuanian LLMs, which were either nonexistent (e.g., Llama2 [45]) or quite weak (e.g., Llama3 [7]). Although the latest open multilingual models (e.g., Llama3.1 [7], Gemma2 [44]), released during our research, have a strong Lithuanian component, we trained our model based on Llama2 to investigate whether an efficient Lithuanian LLM can be trained from an LLM without any Lithuanian component.\nWe also conducted an overview of the existing regional LLMs. It shows that most regional models follow Llama2 or Mistral architecture. In addition, some authors do not train a full parameter set but instead rely on PEFT approaches\nmodern LLM architectures. We also translated these benchmarks into Lithuanian and published them in an open repository (see Table 5), contributing to the standardisation of Lithuanian language model evaluation.\nIn the context of regional LLMs, the proposed models open further research perspectives not only for NLP, but also for other directions since LLM representations are potentially useful in various scenarios (e.g. sentiment analysis [49], robotics [22], causality [24], and multimodality [35]). Our future work will include fully trained small language models tailored for Baltic languages and English."}]}