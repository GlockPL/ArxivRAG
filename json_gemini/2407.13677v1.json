{"title": "PASTA: Controllable Part-Aware Shape Generation with Autoregressive Transformers", "authors": ["SONGLIN LI", "DESPOINA PASCHALIDOU", "LEONIDAS GUIBAS"], "abstract": "The increased demand for tools that automate the 3D content creation process led to tremendous progress in deep generative models that can generate diverse 3D objects of high fidelity. In this paper, we present PASTA, an autoregressive transformer architecture for generating high quality 3D shapes. PASTA comprises two main components: An autoregressive transformer that generates objects as a sequence of cuboidal primitives and a blending network, implemented with a transformer decoder that composes the sequences of cuboids and synthesizes high quality meshes for each object. Our model is trained in two stages: First we train our autoregressive generative model using only annotated cuboidal parts as supervision and next, we train our blending network using explicit 3D supervision, in the form of watertight meshes. Evaluations on various ShapeNet objects showcase the ability of our model to perform shape generation from diverse inputs e.g. from scratch, from a partial object, from text and images, as well size-guided generation, by explicitly conditioning on a bounding box that defines the object's boundaries. Moreover, as our model considers the underlying part-based structure of a 3D object, we are able to select a specific part and produce shapes with meaningful variations of this part. As evidenced by our experiments, our model generates 3D shapes that are both more realistic and diverse than existing part-based and non part-based methods, while at the same time is simpler to implement and train.", "sections": [{"title": "1 INTRODUCTION", "content": "The ability to generate realistic and diverse 3D shapes has the potential to significantly accommodate the workflow of artists and content creators and potentially enable new levels of creativity through \"generative art\" [Bailey 2020]. The tremendous progress in generative modelling and implicit-based representations gave rise to several works [Chan et al. 2022, 2021; Gao et al. 2022; Gu et al. 2022; Schwarz et al. 2020] that generate objects with high realism in terms of geometric details and texture. Nevertheless, as these pipelines represent objects holistically, i.e. without taking into consideration the underlying part-based structure of each object, they only support few interactive applications that typically require deep technical knowledge of each model. However, shape editing and manipulation involves controlling what parts of the object need to be changed. To enable this level of control, an active area of research proposes to consider the decomposition of shapes into parts [Deprelle et al. 2019; Gadelha et al. 2020; Hao et al. 2020; Hertz et al. 2022; Li et al. 2021; Mo et al. 2019a, 2020; Wu et al. 2020].\nExisting part-based generative models, represent 3D shapes as a collection of simple shapes parametrized with cuboids [Mo et al."}, {"title": "2 RELATED WORK", "content": "3D Representations Learning-based approaches for 3D reconstruction employ a neural network that learns a function from the input to a mesh [Groueix et al. 2018; Kanazawa et al. 2018; Liao et al. 2018; Pan et al. 2019; Wang et al. 2018b; Yang et al. 2019], a pointcloud [Achlioptas et al. 2018; Fan et al. 2017; Jiang et al. 2018; Qi et al. 2017; Thomas et al. 2019; Yang et al. 2019], a voxel grid [Brock et al. 2016; Choy et al. 2016; Gadelha et al. 2017; Rezende et al. 2016; Riegler et al. 2017; Stutz and Geiger 2018; Xie et al. 2019] or an implicit surface [Chen and Zhang 2019; Mescheder et al. 2019; Michalkiewicz et al. 2019; Park et al. 2019; Saito et al. 2019; Xu et al. 2019]. Unlike explicit representations that discretize the output space, using voxels, points or mesh vertices, implicit representations represent shapes in the weights of a neural network that learns a mapping between a query point and a context vector to a signed distance value [Atzmon and Lipman 2020; Gropp et al. 2020; Michalkiewicz et al. 2019; Park et al. 2019; Takikawa et al. 2021] or a binary occupancy value [Chen and Zhang 2019; Mescheder et al. 2019]. As these methods require 3D supervision, several works propose combining them with surface [Niemeyer et al. 2020; Yariv et al. 2020] or volumetric [Mildenhall et al. 2020] rendering to learn the 3D object geometry and texture directly from images. In this work, we introduce a part-aware generative model that parametrizes shapes as an occupancy field [Mescheder et al. 2019].\nPrimitive-based Representations Shape abstraction techniques represent shapes using semantically consistent part arrangements and seek to recover the 3D geometry using simple shapes such as cuboids [Dubrovina et al. 2019; Li et al. 2017; Mo et al. 2019a;"}, {"title": "3 METHOD", "content": "PASTA is a part-aware generative model for synthesizing 3D shapes. Our model comprises two main components that are trained independently: An object generator that sequentially generates objects as unordered sequences of labelled parts, where each part is parametrized using a 3D cuboidal primitive (Sec. 3.1), and a blending network that composes part sequences in a meaningful way and synthesizes high quality implicit shapes. The object generator is an autoregressive model trained to maximize the log-likelihood of all possible part arrangements in the dataset. We use part-level supervision in the form of part labels and 3D cuboids that define the size and the pose of each part (Sec. 3.2). The blending network is an occupancy network [Mescheder et al. 2019], implemented with a transformer decoder that takes a sequence of cuboids and a query 3D point and predicts whether this point is inside or outside the surface boundaries (Sec. 3.3). To train the blending network, we assume explicit 3D supervision in the form of a watertight mesh."}, {"title": "3.1 Object Parametrization", "content": "We define each object $S = \\{P, B\\}$ using a collection of N parts, $P = \\{P_1,..., P_N\\}$, and a 3D bounding box B that specifies the object's boundaries. Each part is represented with a 3D labelled cuboid and is parametrized using four values describing its label, size, translation and rotation, $p_j = \\{c_j, s_j, t_j, o_j\\}$. To model the label of each part, i.e. the back or the arm of a chair, we use a categorical distribution defined over the total number of part labels in the dataset, whereas for the rest of the attributes we use a mixture of logistic distributions [Salimans et al. 2017; van den Oord et al. 2016]. In our setup, the rotation $o_j \\in \\mathbb{R}^6$ is the 6D representation [Zhou et al. 2019] of the 3D cuboid that contains the part. Likewise, the translation $t_j \\in \\mathbb{R}^3$ is the center of the 3D cuboid containing the part and the size $s_j \\in \\mathbb{R}^3$ is its width, height and depth. Similarly, the bounding box B is defined using 12 parameters: three for its size along each axis, three for the translation, which is the center of the box and six for the rotation, which is a 6D representation.\nSimilar to ATISS [Paschalidou et al. 2021a], we predict the components of $p_j$ autoregressively, namely part label first, followed by translation, rotation and size. Hence, the probability of generating the j-th part, conditioned on the previous parts and box B becomes\n$P_{\\theta}(P_j | P_{<j}, B) = p(c_j|p_{<j},B)p(t_j|c_j, p_{<j}, B)$\n$p_{\\theta}(o_j|c_j, t_j, p_{<j}, B)p_{\\theta}(s_j|c_j, t_j, o_j, p_{<j}, B),$"}, {"title": "3.2 Object Generator", "content": "The input to our object generator is a set of objects in the form of 3D labelled cuboids and their corresponding bounding boxes. We implement our generator using an autoregressive transformer architecture similar to ATISS. The transformer model takes as input a sequence of embedding vectors that represent the conditioning sequence and generates the features F that will be used to predict the attributes of the next part. We map the per-part attributes to embedding vectors using a part encoder and the features F to part attributes using a part decoder. Our model is illustrated in Fig. 2.\nThe part encoder network $s_{\\theta}(\\cdot)$ takes the attributes for each part $p_j = \\{c_j, s_j, t_j, o_j\\}$ and maps them to an embedding vector $z_j$\n$z_j = s_{\\theta} ( [\\lambda(c_j); \\gamma(s_j); \\gamma(t_j); \\gamma(o_j)]),$\nwhere $\\lambda(\\cdot)$ is a learnable embedding, $\\gamma(\\cdot)$ is a positional encoding layer [Vaswani et al. 2017] that is applied separately on each attribute's dimension and $[;]$ denotes concatenation. To predict an embedding vector $z_B$ for B, we pass its attributes to $s_{\\theta}(\\cdot)$.\nSimilar to ATISS [Paschalidou et al. 2021a], we implement our transformer encoder $\\tau_{\\theta}(\\cdot)$ as a multi-head attention transformer without positional encoding [Vaswani et al. 2017], as we want to model objects as unordered sets of parts. Our transformer encoder takes as input $\\{z_j\\}_{j=1}^N$ the N embeddings for all parts in the sequence, $z_B$ the embedding for the bounding box and a learnable embedding vector q, which is used to predict the feature vector F that will be used to generate the next part in the sequence. More formally,\n$F = \\tau_{\\theta} (z_B, \\{z_j\\}_{j=1}^N, q).$\nThe last component of the object generator is the part decoder that takes as input the feature vector F and autoregressively predicts the attributes of the next part to be generated. For the part label, we define a function $c_{\\theta}(\\cdot)$, implemented using a linear projection layer, that takes F and predicts the per-part label probability. We predict the size, translation and rotation in two-stages. First, we cluster the values of each attribute from the training set into 20 clusters using K-Means. Subsequently, we predict a cluster for each attribute, which is then used to predict the specific values. More formally, for the translation, we learn $t^{coarse}_{ \\theta} (\\cdot)$ that predicts the per-cluster probability from F using a linear projection layer and $t^{fine}_{ \\theta} (\\cdot)$ that predicts the $7 \\times K$ parameters that define the mixture of logistics"}, {"title": "3.3 Blending Network", "content": "The input to the blending network is a sequence of labelled cuboids and a set of 3D query points X, for which we want to predict their occupancy probabilities, namely whether they lie inside or outside the surface boundaries. In detail, our blending network consists of two main components: (i) a part-encoder that maps the part attributes into an embedding vector, which is implemented as discussed in Sec. 3.2 and (ii) a transformer decoder without self-attention that takes the part embeddings and the query points and predicts whether they are inside or outside the surface boundary (see Fig. 3). The transformer comprises only cross attention layers and MLPs, thus, each query 3D point attends to the per-part embeddings using cross attention, without attending to the other points. The transformer output is passed to a linear layer followed by a sigmoid non-linearity to get an occupancy probability for each query point. We follow common practice and before passing the query points to the decoder we map them to a higher dimensional space using positional encoding [Vaswani et al. 2017]."}, {"title": "3.4 Training and Inference", "content": "Unlike prior autoregressive models [Paschalidou et al. 2021a; Ritchie et al. 2019] that are trained with teacher forced embeddings, we train PASTA using scheduled sampling [Bengio et al. 2015]. The key idea is that during training we feed our model with a mix of the teacher forced embeddings and the actual model's predictions from the previous generation step. In particular, we choose an object from the dataset and apply the permutation function $\\pi(\\cdot)$ on its elements. Next, we randomly select the first M objects and pass them to the object generator that is used to predict the next part. The newly generated part is appended to the initial sequence of M parts and passed once again to the object generator to predict the attribute distribution of the next part. Our model is trained to maximize the likelihood of the M + 2 object in the permuted sequence. A pictorial representation of our scheduled sampling is provided in Fig. 4. We follow common practice and during training, we train with both scheduled sampling and teacher forcing."}, {"title": "3.5 Conditional Generation", "content": "Here, we discuss how PASTA can be used to perform language- and image-guided generation. Instead of conditioning the generation only on the bounding box B, we now condition also on a textual description of the shape to be generated. In particular, we utilize the pre-trained CLIP [Radford et al. 2021] model to extract text embeddings and pass them to the transformer encoder as an additional input. Note that during training the pre-trained CLIP text encoder remains frozen, namely is not optimized with the rest of our network. Once we train PASTA with text embeddings from CLIP, we can use it, without any re-training, also for image-guided generation. This is possible because the CLIP model has a joint latent space for text and images. While, in our experiments, we only demonstrate language- and image-guided generations, our model can be extended to other types of conditioning such as depth maps or pointclouds etc. by utilizing an appropriate encoder that generates embeddings from the input."}, {"title": "4 EXPERIMENTAL EVALUATION", "content": "In this section, we provide an extensive evaluation of our method comparing it to relevant baselines. Additional results and implementation details are provided in the supplementary.\nDatasets We report results on three PartNet [Mo et al. 2019b] categories: Chair, Table and Lamp, which contain 4489, 5705, and 1554 shapes respectively. For the Chair category, there are 47 different types of parts, while for the Lamp and the Table we have 32 and 43 respectively. We train our model and our part-based baselines using the part annotations and train/test splits from PartNet. Moreover, for our model, we use the object bounding boxes specified in PartNet.\nBaselines In our evaluation, we include PQ-NET [Wu et al. 2020] that is a generative model that generates 3D shapes using an RNN"}, {"title": "4.1 Shape Generation", "content": "We evaluate the performance of our model on the shape generation task on chairs, tables and lamps and perform category-specific training for our model and our baselines. Conditioned on different bounding boxes, our model can successfully synthesize meaningful part arrangements (see 4th column in Fig. 5, Fig. 6 and Fig. 11), which are fed to our blending network that combines them and yields plausible 3D meshes (see 5th column in Fig. 5, Fig. 6 and Fig. 11). We observe that PASTA consistently generates diverse and realistic part arrangements (Ours-Parts) for all object categories, which are, in turn, converted into meshes (Ours) that faithfully capture the initial part representation with cuboids. On the contrary, ATISS struggles to synthesize meaningful part sequences, i.e. the synthesized part arrangements consist of parts positioned in unnatural positions, especially for the case of chairs and tables. While synthesized objects sampled from PQ-NET and IM-NET are more realistic than the part arrangements produced by ATISS, they lack diversity, as indicated by the coverage score in Tab. 1. Note that our model, even without"}, {"title": "4.2 Shape Completion", "content": "Starting from an incomplete sequence of parts, we evaluate whether our model and our baselines can complete the input sequence in a meaningful way. For this experiment, we only consider our part-based baselines and all models are trained in a category-specific"}, {"title": "4.3 Applications", "content": "In this section, we present several applications of our model, such as conditional generation from text and images. In both experiments, our model is trained in a category-specific manner.\nLanguage-guided Generation For this experiment, we use the part labels provided in PartNet [Mo et al. 2019b] and generate utterances that describe the part-based structure of each object. We train a variant of our model that conditions on CLIP [Radford et al. 2021] embeddings produced from our textual descriptions in addition to the object bounding box as described in Sec. 3.5. In Fig. 8, we provide text-guided generations of our model and observe note that they consistently match the input text (e.g. the table with the two pedestals, or the lamp connected to the ceiling).\nImage-guided Generation: We now test the ability of our model to perform image-guided generation using the same model that was trained for language-guided generation, without any re-training. In particular, we take advantage of the CLIP's joint latent space, and condition our model on image embeddings produced by CLIP's image encoder. Fig. 10 shows examples of image-guided synthesis. While PASTA was never trained with images, we showcase that it generates shapes of various object categories that faithfully match the input image. Notably, the recovered parts capture fine geometric details such as the three legs of the first table in the Fig. 10."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced PASTA a part-aware generative model for 3D shapes. Our architecture consists of two main components: the object generator that autoregressively generates objects as sequences of labelled cuboids and the blending network that combines a sequence of cuboidal primitives and synthesizes a high-quality implicit shape. Unlike traditional autoregressive models that are trained with teacher forcing, we demonstrate that relying on scheduled sampling [Bengio et al. 2015] improves the generation performance of our model. Our experiments, showcase that PASTA generates more meaningful part arrangements and plausible 3D objects than both part-based [Paschalidou et al. 2021a; Wu et al. 2020] and non part-based generative models [Chen and Zhang 2019]. In future work, we plan to extend our architecture to generate parts with textures. Note that this is a straight-forward extension of our method if we simply replace our blending network with a NeRF-based decoder [Mildenhall et al. 2020] that instead of predicting occupancies, predicts colors and opacities. Another exciting direction for future research, is to explore learning such part-based autoregressive models without explicit part annotations."}, {"title": "6 IMPLEMENTATION DETAILS", "content": "In this section, we provide a detailed description of the several components of our network architecture (Sec. 6.1). Next, we describe our training procedure (Sec. 6.2) and the generation protocol (Sec. 6.3). Finally, we detail our metrics computation (Sec. 6.4) and discuss our baselines (Sec. 6.5)."}, {"title": "6.1 Network Architecture", "content": "Here we describe the individual components of our network architecture and provide additional implementation details. Our model comprises two main components: the object generator that sequentially generates objects as unordered sequences of labelled parts, where each part is parametrized using a 3D cuboidal primitive, and a blending network that composes part sequences in a meaningful way and synthesizes high quality implicit shapes.\nObject Generator We implement our object generator using an autoregressive transformer architecture similar to ATISS [Paschalidou et al. 2021a]. In particular, it comprises three main components: (i) the part encoder that takes the per-part attributes and maps them to an embedding vector, (ii) the transformer encoder that takes the embeddings for each part in the sequence, the embedding of the bounding box and a learnable query embedding q and predicts the feature vector F and (iii) the part decoder that takes F and predicts the attributes of the next object to be added in the scene.\nThe part encoder simply takes the attributes of each part $p_j = \\{c_j, s_j, t_j, o_j\\}$ and maps them to an embedding vector $z_j$ as follows:\n$z_j = s_{\\theta} ( [\\lambda(c_j); \\gamma(s_j); \\gamma(t_j); \\gamma(o_j)] ).$\nFor the part label $c_j$, we use a learnable embedding, denoted as $\\lambda(\\cdot)$, which is simply a matrix of size $C \\times 64$, where C is the total number of part labels in the dataset. The positional encoding layer, denoted as $\\gamma(\\cdot)$, that is applied on the remaining attributes can be expressed as follows:\n$\\gamma(p) = (sin(2^0 \\pi p), cos(2^0 \\pi p), ..., sin(2^{L-1} \\pi p), cos(2^{L-1} \\pi p))$\nwhere p can be any of the size, translation or rotation. We follow [Paschalidou et al. 2021a] and set L = 32. Once the attributes are embedded to a higher dimensional space either using $\\lambda(\\cdot)$ or $\\gamma(\\cdot)$, we concatenate them to a 512-dimensional feature vector, which is then passed to another linear layer and generates the final embedding vector $z_j \\in \\mathbb{R}^{64}$.\nSimilar to [Paschalidou et al. 2021a; Vaswani et al. 2017] we implement our transformer encoder as a multi-head attention transformer"}, {"title": "6.2 Training Protocol", "content": "As already discussed in our main submission, we train the two components of our model independently. To train the autoregressive transformer encoder, we use the Adam optimizer [Kingma and Ba 2015] with learning rate $\\eta = 10^{-4}$ and weight decay $10^{-3}$. For the other hyperparameters of Adam we use the PyTorch defaults: $\\beta_1 = 0.9, \\beta_2 = 0.999$ and $\\epsilon = 10^{-8}$. For both category-specific training as well as joint training experiments, we train the autoregressive transformer encoder with a batch size of 128 for 700k iterations. During training, we do not perform any type of rotation augmentation. To determine when to stop training, we follow [Paschalidou et al. 2021a] and evaluate the validation metric every 1000 iterations and use the model that performed best as our final model. Likewise, to train our blending network that is implemented using a transformer decoder we use the Adam optimizer [Kingma and Ba 2015] with learning rate $\\eta = 10^{-4}$ with no weight decay. For the other hyperparameters of Adam we use the PyTorch defaults: $\\beta_1 = 0.9, \\beta_2 = 0.999$ and $\\epsilon = 10^{-8}$. For the case of category-specific training, we train the transformer on each object category with a batch size of 32 for 150k iterations. For the case of the joint-training on multiple object categories, we train the blending network with a batch size of 32 for"}, {"title": "6.3 Generation Protocol", "content": "In this section, we discuss the sampling process for generating a novel part arrangement. When we perform generation from scratch, we condition our generation on a bounding box that specifies the object boundaries. Note that when we want to generate novel shapes from a model that was trained jointly on multiple object categories, we do not have to explicit condition on a specific category. For the case of shape completion from an incomplete sequence of parts, we condition our generation on the cuboidal primitives as well as the bounding box that specifies the object boundaries. A similar concept is adapted for the case of the language- and image-guided generation. As soon as a sequence of cuboidal parts is produced, we pass it to the transformer decoder that composes the cuboids and synthesizes an implicit 3D shape of high quality."}, {"title": "6.4 Metrics", "content": "As mentioned in the main submission, to evaluate the plausibility and the diversity of the generated shapes using our model and our baselines, we report the Coverage Score (COV) and the Minimum Matching Distance (MMD) [Achlioptas et al. 2018] using the Chamfer-L2 distance between points sampled on the surface of the real and the generated shapes. In particular, MMD measures the quality of the generated shapes by computing how likely it is that a generated shape looks like a shape from the reference set of shapes. On the other hand, COV measures how many shape variations are covered by the generated shapes, by computing the percentage of reference shapes that are closest to at least one generated shape.\nLet us denote G the set of generated shapes and R the set of reference shapes from the test set. To estimate the similarity between two shapes from the two sets, we use the Chamfer Distance (CD), which is simply the distance between a set of points sampled on the surface of the reference and the generated mesh. Namely, given a set of N sampled points on the surface of the reference $X = \\{x_i\\}_{i=1}^N$ and the generated shape $Y = \\{y_i\\}_{i=1}^M$, the Chamfer Distance (CD) becomes\n$CD(X, Y) = \\frac{1}{N} \\sum_{x \\in X} \\min_{y \\in Y} || x - y ||_2 + \\frac{1}{M} \\sum_{y \\in Y} \\min_{x \\in X} || y - x ||_2.$"}, {"title": "6.5 Baselines", "content": "In this section, we provide additional details regarding our baselines. We compare our model with IM-NET [Chen and Zhang 2019], PQ-NET [Wu et al. 2020] and ATISS [Paschalidou et al. 2021a]. For all our experiments, we retrain all baselines, using the released code provided by the authors. For the case of ATISS, which is an autoregressive transformer originally introduced for indoor scene synthesis, we adapt the original code for the task of part-based object generation and train ATISS using the per-object part sequences from PartNet [Mo et al. 2019b]. Note that IM-NET [Chen and Zhang 2019] is not directly comparable to our model as it does not consider any parts. However, we include it in our evaluation as a powerful implicit-based generative model for 3D shapes."}, {"title": "6.6 Mesh Extraction", "content": "To extract meshes from the predicted occupancy field, we employ the Marching Cubes algorithm [Lorensen and Cline 1987]. In particular, we start from a voxel grid of $128^3$ initial resolution for which we predict occupancy values. Next, we follow the process proposed in [Mescheder et al. 2019] and extract the approximate isosurface with Marching Cubes using the code provided by Mescheder et al. [Mescheder et al. 2019]. Note that the same process is followed to extract meshes from the implicit representations learned both with PQ-NET [Wu et al. 2020] and IM-NET [Chen and Zhang 2019]."}, {"title": "7 DATA PROCESSING", "content": "We use PartNet [Mo et al. 2019a] as the dataset to evaluate our model and our baselines. We remove 123 objects in total across all categories due to their invalid part hierarchical structures (i.e. missing nodes etc.). To train our blending network, we assume explicit 3D supervision in the form of a wateright mesh. To acquire this, we align ShapeNet [Chang et al. 2015] objects with PartNet objects using the scripts provided in the official PartNet repository. We then convert aligned objects into watertight meshes using the code provided by Stutz et al. [Stutz and Geiger 2018]. To train the variant of our model for language-guided generation, we remove samples, whose descriptions are too long and cannot be handled by CLIP [Radford et al. 2021]. After this processing step, we have 4000 training samples for chairs, 5439 for tables, and 1424 for lamps."}, {"title": "8 IMPACT OF SCHEDULE SAMPLING", "content": "In this section, we investigate how schedule sampling affects the generation capabilities of our model. For this experiment, we perform category specific training on the Chair category. In particular, we train two variants of our model, one with teacher forcing and one with schedule sampling until convergence. We compare the two model variants wrt. their generation performance in Tab. 3. We note that training our network only with teacher forcing, significantly deteriorates performance and results in generations of lower quality. This is also validated from our qualitative comparison in Fig. 16, where we visualize randomly generated chairs using both models. We observe that the variant of our model trained only with teacher forcing tends to produce part arrangements, with parts"}, {"title": "9 ADDITIONAL EXPERIMENTAL RESULTS", "content": "In this section, we provide additional information regarding our experiments on PartNet [Mo et al. 2019b]. In particular, we consider three categories: Chair, Table and Lamp, which contain 4489, 5705, and 1554 shapes respectively. For the Chair category, there are 47 different classes (e.g. back surface horizontal bar, arm holistic frame etc.) in total, while for the Lamp and Table, we have 32 and 43 part categories respectively. Note that unlike prior works such as PQ-NET [Wu et al. 2020] that only consider shapes that have less than 10 parts, we consider shapes with a significantly larger number of components. In particular, chairs can have up to 144 parts, tables 164 parts and lamps up to 191 parts. For more details, regarding our training data, we refer reader to Sec. 7. In this section, we provide additional qualitative results for all experiments discussed in our main submission."}, {"title": "9.1 Shape Generation", "content": "In this experiment, we investigate the ability of our model to generate plausible part-aware 3D geometries, conditioned on various bounding boxes that specify the object's boundaries. Fig. 18 shows seven randomly generated chairs using our model, ATISS [Paschalidou et al. 2021a], PQ-NET [Wu et al. 2020] and IM-NET [Chen and Zhang 2019]. Note that for this experiment, we perform category-specific training, namely we train a different model for each object type. For our model, we visualize both the synthesized part arrangements (Ours-Parts) and the output of our blending network"}, {"title": "9.2 Shape Completion", "content": "Starting from a partial object, parameterized with a set of cuboidal parts, we want to evaluate whether our model and our baselines can generate plausible part arrangements. Since IM-NET cannot be used to complete 3D shapes from a partial sequence of parts, we exclude it from our evaluation. To measure the quality of the generated parts, in this experiment, we also report the classification accuracy of a classifier trained to discriminate real from synthetic objects. In particular, as we represent objects using a collection of cuboidal primitives, we implement our classifier using a transformer encoder [Vaswani et al. 2017] trained to discriminate real from generated 3D labelled cuboids. Furthermore, in our evaluation, we also the FID score [Heusel et al. 2017]. For the FID score computation, we generate the same amount of objects as in the test set and render them at 512x 512 resolution using 5 random camera views. To evaluate the realism of the generated parts, we render objects using their part-based representation, as show in Fig. 17, and we compare with corresponding part-based renderings from the ground-truth objects"}, {"title": "9.3 Size-guided Generation", "content": "Now we examine whether our model can generate objects of different sizes. Note that as we condition the generation of parts on a bounding box that defines the object boundaries, our model can generate shapes of arbitrary sizes. In this experiment, we generate several bounding boxes, with different size parameters and demonstrate the ability of our model to generate short and tall lamps (see 1st and 3rd lamp in 3rd row in Fig. 21, respectively), or smaller and bigger tables (see 1st and 2nd tables in 2nd row in Fig. 21). We believe that this is an important application of our model that allows users to precisely specify the size of the generated object. For all experiments presented in Fig. 21, we perform category-specific training per object type."}, {"title": "9.4 Language-guided Generation", "content": "Starting from a text prompt and a bounding box that defines the object's boundaries, we want to examine the ability of our model"}, {"title": "9.5 Image-guided Generation", "content": "For this task, we utilize the variant of our model that was trained for language-guided generation without any re-training. CLIP [Radford et al. 2021"}]}