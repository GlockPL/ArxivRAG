{"title": "PASTA: Controllable Part-Aware Shape Generation with Autoregressive Transformers", "authors": ["SONGLIN LI", "DESPOINA PASCHALIDOU", "LEONIDAS GUIBAS"], "abstract": "The increased demand for tools that automate the 3D content creation process led to tremendous progress in deep generative models that can generate diverse 3D objects of high fidelity. In this paper, we present PASTA, an autoregressive transformer architecture for generating high quality 3D shapes. PASTA comprises two main components: An autoregressive transformer that generates objects as a sequence of cuboidal primitives and a blending network, implemented with a transformer decoder that composes the sequences of cuboids and synthesizes high quality meshes for each object. Our model is trained in two stages: First we train our autoregressive generative model using only annotated cuboidal parts as supervision and next, we train our blending network using explicit 3D supervision, in the form of watertight meshes. Evaluations on various ShapeNet objects showcase the ability of our model to perform shape generation from diverse inputs e.g. from scratch, from a partial object, from text and images, as well size-guided generation, by explicitly conditioning on a bounding box that defines the object's boundaries. Moreover, as our model considers the underlying part-based structure of a 3D object, we are able to select a specific part and produce shapes with meaningful variations of this part. As evidenced by our experiments, our model generates 3D shapes that are both more realistic and diverse than existing part-based and non part-based methods, while at the same time is simpler to implement and train.", "sections": [{"title": "1 INTRODUCTION", "content": "The ability to generate realistic and diverse 3D shapes has the potential to significantly accommodate the workflow of artists and content creators and potentially enable new levels of creativity through \"generative art\" [Bailey 2020]. The tremendous progress in generative modelling and implicit-based representations gave rise to several works [Chan et al. 2022, 2021; Gao et al. 2022; Gu et al. 2022; Schwarz et al. 2020] that generate objects with high realism in terms of geometric details and texture. Nevertheless, as these pipelines represent objects holistically, i.e. without taking into consideration the underlying part-based structure of each object, they only support few interactive applications that typically require deep technical knowledge of each model. However, shape editing and manipulation involves controlling what parts of the object need to be changed. To enable this level of control, an active area of research proposes to consider the decomposition of shapes into parts [Deprelle et al. 2019; Gadelha et al. 2020; Hao et al. 2020; Hertz et al. 2022; Li et al. 2021; Mo et al. 2019a, 2020; Wu et al. 2020].\nExisting part-based generative models, represent 3D shapes as a collection of simple shapes parametrized with cuboids [Mo et al."}, {"title": "2 RELATED WORK", "content": "3D Representations Learning-based approaches for 3D reconstruc-\ntion employ a neural network that learns a function from the input\nto a mesh [Groueix et al. 2018; Kanazawa et al. 2018; Liao et al. 2018;\nPan et al. 2019; Wang et al. 2018b; Yang et al. 2019], a pointcloud\n[Achlioptas et al. 2018; Fan et al. 2017; Jiang et al. 2018; Qi et al. 2017;\nThomas et al. 2019; Yang et al. 2019], a voxel grid [Brock et al. 2016;\nChoy et al. 2016; Gadelha et al. 2017; Rezende et al. 2016; Riegler\net al. 2017; Stutz and Geiger 2018; Xie et al. 2019] or an implicit sur-\nface [Chen and Zhang 2019; Mescheder et al. 2019; Michalkiewicz\net al. 2019; Park et al. 2019; Saito et al. 2019; Xu et al. 2019]. Un-\nlike explicit representations that discretize the output space, using\nvoxels, points or mesh vertices, implicit representations represent\nshapes in the weights of a neural network that learns a mapping\nbetween a query point and a context vector to a signed distance\nvalue [Atzmon and Lipman 2020; Gropp et al. 2020; Michalkiewicz\net al. 2019; Park et al. 2019; Takikawa et al. 2021] or a binary oc-\ncupancy value [Chen and Zhang 2019; Mescheder et al. 2019]. As\nthese methods require 3D supervision, several works propose com-\nbining them with surface [Niemeyer et al. 2020; Yariv et al. 2020]\nor volumetric [Mildenhall et al. 2020] rendering to learn the 3D\nobject geometry and texture directly from images. In this work, we\nintroduce a part-aware generative model that parametrizes shapes\nas an occupancy field [Mescheder et al. 2019].\nPrimitive-based Representations Shape abstraction techniques\nrepresent shapes using semantically consistent part arrangements\nand seek to recover the 3D geometry using simple shapes such\nas cuboids [Dubrovina et al. 2019; Li et al. 2017; Mo et al. 2019a;"}, {"title": "3 METHOD", "content": "PASTA is a part-aware generative model for synthesizing 3D shapes.\nOur model comprises two main components that are trained in-\ndependently: An object generator that sequentially generates ob-\njects as unordered sequences of labelled parts, where each part is\nparametrized using a 3D cuboidal primitive (Sec. 3.1), and a blend-\ning network that composes part sequences in a meaningful way\nand synthesizes high quality implicit shapes. The object generator\nis an autoregressive model trained to maximize the log-likelihood\nof all possible part arrangements in the dataset. We use part-level\nsupervision in the form of part labels and 3D cuboids that define\nthe size and the pose of each part (Sec. 3.2). The blending network is\nan occupancy network [Mescheder et al. 2019], implemented with a\ntransformer decoder that takes a sequence of cuboids and a query\n3D point and predicts whether this point is inside or outside the\nsurface boundaries (Sec. 3.3). To train the blending network, we\nassume explicit 3D supervision in the form of a watertight mesh."}, {"title": "3.1 Object Parametrization", "content": "We define each object $S = {P, B}$ using a collection of N parts,\n$P = {P_1,..., P_N}$, and a 3D bounding box B that specifies the\nobject's boundaries. Each part is represented with a 3D labelled\ncuboid and is parametrized using four values describing its label,\nsize, translation and rotation, $p_j = {c_j, s_j, t_j, o_j}$. To model the label\nof each part, i.e. the back or the arm of a chair, we use a categorical\ndistribution defined over the total number of part labels in the\ndataset, whereas for the rest of the attributes we use a mixture of\nlogistic distributions [Salimans et al. 2017; van den Oord et al. 2016].\nIn our setup, the rotation $o_j \\in \\mathbb{R}^6$ is the 6D representation [Zhou\net al. 2019] of the 3D cuboid that contains the part. Likewise, the\ntranslation $t_j \\in \\mathbb{R}^3$ is the center of the 3D cuboid containing the\npart and the size $s_j \\in \\mathbb{R}^3$ is its width, height and depth. Similarly,\nthe bounding box B is defined using 12 parameters: three for its size\nalong each axis, three for the translation, which is the center of the\nbox and six for the rotation, which is a 6D representation.\nSimilar to ATISS [Paschalidou et al. 2021a], we predict the com-\nponents of pj autoregressively, namely part label first, followed by\ntranslation, rotation and size. Hence, the probability of generating\nthe j-th part, conditioned on the previous parts and box B becomes\n$$P_\\theta(P_j | P_{<j}, B) = p_\\theta(c_j|p_{<j},B)p_\\theta(t_j|c_j, p_{<j}, B)$$\n$$p_\\theta(o_j|c_j, t_j, p_{<j}, B)p_\\theta^s(s_j|c_j, t_j, o_j, p_{<j}, B),$$\n(1)"}, {"title": "3.2 Object Generator", "content": "The input to our object generator is a set of objects in the form\nof 3D labelled cuboids and their corresponding bounding boxes.\nWe implement our generator using an autoregressive transformer\narchitecture similar to ATISS. The transformer model takes as input\na sequence of embedding vectors that represent the conditioning\nsequence and generates the features F that will be used to predict\nthe attributes of the next part. We map the per-part attributes to\nembedding vectors using a part encoder and the features F to part\nattributes using a part decoder. Our model is illustrated in Fig. 2.\nThe part encoder network $s_\\theta(\\cdot)$ takes the attributes for each part\n$p_j = {c_j, s_j, t_j, o_j}$ and maps them to an embedding vector $z_j$\n$$z_j = s_\\theta ( [\\lambda(c_j); \\gamma(s_j); \\gamma(t_j); \\gamma(o_j)]),$$\n(3)\nwhere $\\lambda(\\cdot)$ is a learnable embedding, $\\gamma(\\cdot)$ is a positional encoding\nlayer [Vaswani et al. 2017] that is applied separately on each at-\ntribute's dimension and $[;]$ denotes concatenation. To predict an\nembedding vector $z_B$ for B, we pass its attributes to $s_\\theta(\\cdot)$.\nSimilar to ATISS [Paschalidou et al. 2021a], we implement our\ntransformer encoder $\\tau_\\theta(\\cdot)$ as a multi-head attention transformer\nwithout positional encoding [Vaswani et al. 2017], as we want to\nmodel objects as unordered sets of parts. Our transformer encoder\ntakes as input ${z_j}_{j=1}^N$ the N embeddings for all parts in the sequence,\n$z_B$ the embedding for the bounding box and a learnable embedding\nvector q, which is used to predict the feature vector F that will be\nused to generate the next part in the sequence. More formally,\n$$F = \\tau_\\theta (z_B, {z_j}_{j=1}^N, q).$$\n(4)\nThe last component of the object generator is the part decoder\nthat takes as input the feature vector F and autoregressively predicts\nthe attributes of the next part to be generated. For the part label, we\ndefine a function $c_\\theta(\\cdot)$, implemented using a linear projection layer,\nthat takes F and predicts the per-part label probability. We predict\nthe size, translation and rotation in two-stages. First, we cluster\nthe values of each attribute from the training set into 20 clusters\nusing K-Means. Subsequently, we predict a cluster for each attribute,\nwhich is then used to predict the specific values. More formally,\nfor the translation, we learn $t_\\theta^{coarse}(\\cdot)$ that predicts the per-cluster\nprobability from F using a linear projection layer and $t_\\theta^{fine}(\\cdot)$ that\npredicts the $7 \\times K$ parameters that define the mixture of logistics"}, {"title": "3.3 Blending Network", "content": "The input to the blending network is a sequence of labelled cuboids\nand a set of 3D query points X, for which we want to predict their\noccupancy probabilities, namely whether they lie inside or out-\nside the surface boundaries. In detail, our blending network con-\nsists of two main components: (i) a part-encoder that maps the\npart attributes into an embedding vector, which is implemented as\ndiscussed in Sec. 3.2 and (ii) a transformer decoder without self-\nattention that takes the part embeddings and the query points and\npredicts whether they are inside or outside the surface boundary\n(see Fig. 3). The transformer comprises only cross attention lay-\ners and MLPs, thus, each query 3D point attends to the per-part\nembeddings using cross attention, without attending to the other\npoints. The transformer output is passed to a linear layer followed\nby a sigmoid non-linearity to get an occupancy probability for each\nquery point. We follow common practice and before passing the\nquery points to the decoder we map them to a higher dimensional\nspace using positional encoding [Vaswani et al. 2017]."}, {"title": "3.4 Training and Inference", "content": "Unlike prior autoregressive models [Paschalidou et al. 2021a; Ritchie\net al. 2019] that are trained with teacher forced embeddings, we\ntrain PASTA using scheduled sampling [Bengio et al. 2015]. The key\nidea is that during training we feed our model with a mix of the\nteacher forced embeddings and the actual model's predictions from\nthe previous generation step. In particular, we choose an object from\nthe dataset and apply the permutation function $\\pi(\\cdot)$ on its elements.\nNext, we randomly select the first M objects and pass them to the\nobject generator that is used to predict the next part. The newly\ngenerated part is appended to the initial sequence of M parts and\npassed once again to the object generator to predict the attribute\ndistribution of the next part. Our model is trained to maximize the\nlikelihood of the M + 2 object in the permuted sequence. A pictorial\nrepresentation of our scheduled sampling is provided in Fig. 4. We\nfollow common practice and during training, we train with both\nscheduled sampling and teacher forcing."}, {"title": "3.5 Conditional Generation", "content": "Here, we discuss how PASTA can be used to perform language- and\nimage-guided generation. Instead of conditioning the generation\nonly on the bounding box B, we now condition also on a textual\ndescription of the shape to be generated. In particular, we utilize\nthe pre-trained CLIP [Radford et al. 2021] model to extract text em-\nbeddings and pass them to the transformer encoder as an additional\ninput. Note that during training the pre-trained CLIP text encoder\nremains frozen, namely is not optimized with the rest of our network.\nOnce we train PASTA with text embeddings from CLIP, we can use\nit, without any re-training, also for image-guided generation. This is\npossible because the CLIP model has a joint latent space for text and\nimages. While, in our experiments, we only demonstrate language-\nand image-guided generations, our model can be extended to other\ntypes of conditioning such as depth maps or pointclouds etc. by\nutilizing an appropriate encoder that generates embeddings from\nthe input."}, {"title": "4 EXPERIMENTAL EVALUATION", "content": "In this section, we provide an extensive evaluation of our method\ncomparing it to relevant baselines. Additional results and implemen-\ntation details are provided in the supplementary.\nDatasets We report results on three PartNet [Mo et al. 2019b] cat-\negories: Chair, Table and Lamp, which contain 4489, 5705, and 1554\nshapes respectively. For the Chair category, there are 47 different\ntypes of parts, while for the Lamp and the Table we have 32 and 43\nrespectively. We train our model and our part-based baselines using\nthe part annotations and train/test splits from PartNet. Moreover, for\nour model, we use the object bounding boxes specified in PartNet.\nBaselines In our evaluation, we include PQ-NET [Wu et al. 2020]\nthat is a generative model that generates 3D shapes using an RNN"}, {"title": "4.1 Shape Generation", "content": "We evaluate the performance of our model on the shape genera-\ntion task on chairs, tables and lamps and perform category-specific\ntraining for our model and our baselines. Conditioned on different\nbounding boxes, our model can successfully synthesize meaningful\npart arrangements (see 4th column in Fig. 5, Fig. 6 and Fig. 11), which\nare fed to our blending network that combines them and yields plau-\nsible 3D meshes (see 5th column in Fig. 5, Fig. 6 and Fig. 11). We\nobserve that PASTA consistently generates diverse and realistic part\narrangements (Ours-Parts) for all object categories, which are, in\nturn, converted into meshes (Ours) that faithfully capture the initial\npart representation with cuboids. On the contrary, ATISS struggles\nto synthesize meaningful part sequences, i.e. the synthesized part\narrangements consist of parts positioned in unnatural positions, es-\npecially for the case of chairs and tables. While synthesized objects\nsampled from PQ-NET and IM-NET are more realistic than the part\narrangements produced by ATISS, they lack diversity, as indicated\nby the coverage score in Tab. 1. Note that our model, even without"}, {"title": "4.2 Shape Completion", "content": "Starting from an incomplete sequence of parts, we evaluate whether\nour model and our baselines can complete the input sequence in a\nmeaningful way. For this experiment, we only consider our part-\nbased baselines and all models are trained in a category-specific"}, {"title": "4.3 Applications", "content": "In this section, we present several applications of our model, such as\nconditional generation from text and images. In both experiments,\nour model is trained in a category-specific manner.\nLanguage-guided Generation For this experiment, we use the part\nlabels provided in PartNet [Mo et al. 2019b] and generate utterances\nthat describe the part-based structure of each object. We train a\nvariant of our model that conditions on CLIP [Radford et al. 2021]\nembeddings produced from our textual descriptions in addition\nto the object bounding box as described in Sec. 3.5. In Fig. 8, we\nprovide text-guided generations of our model and observe note that\nthey consistently match the input text (e.g. the table with the two\npedestals, or the lamp connected to the ceiling).\nImage-guided Generation: We now test the ability of our model to\nperform image-guided generation using the same model that was\ntrained for language-guided generation, without any re-training.\nIn particular, we take advantage of the CLIP's joint latent space,\nand condition our model on image embeddings produced by CLIP's\nimage encoder. Fig. 10 shows examples of image-guided synthesis.\nWhile PASTA was never trained with images, we showcase that it\ngenerates shapes of various object categories that faithfully match\nthe input image. Notably, the recovered parts capture fine geometric\ndetails such as the three legs of the first table in the Fig. 10."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced PASTA a part-aware generative model\nfor 3D shapes. Our architecture consists of two main components:\nthe object generator that autoregressively generates objects as se-\nquences of labelled cuboids and the blending network that combines\na sequence of cuboidal primitives and synthesizes a high-quality\nimplicit shape. Unlike traditional autoregressive models that are\ntrained with teacher forcing, we demonstrate that relying on sched-\nuled sampling [Bengio et al. 2015] improves the generation per-\nformance of our model. Our experiments, showcase that PASTA\ngenerates more meaningful part arrangements and plausible 3D ob-\njects than both part-based [Paschalidou et al. 2021a; Wu et al. 2020]\nand non part-based generative models [Chen and Zhang 2019]. In fu-\nture work, we plan to extend our architecture to generate parts with\ntextures. Note that this is a straight-forward extension of our method\nif we simply replace our blending network with a NeRF-based de-\ncoder [Mildenhall et al. 2020] that instead of predicting occupancies,\npredicts colors and opacities. Another exciting direction for future\nresearch, is to explore learning such part-based autoregressive mod-\nels without explicit part annotations."}]}