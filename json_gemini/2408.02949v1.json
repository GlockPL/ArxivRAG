{"title": "Few-shot Scooping Under Domain Shift via Simulated\nMaximal Deployment Gaps", "authors": ["Yifan Zhu", "Pranay Thangeda", "Erica L Tevere", "Ashish Goel", "Erik Kramer", "Hari D Nayar", "Melkior Ornik", "Kris Hauser"], "abstract": "Autonomous lander missions on extraterrestrial bodies need to sample granu-\nlar materials while coping with domain shifts, even when sampling strategies\nare extensively tuned on Earth. To tackle this challenge, this paper studies\nthe few-shot scooping problem and proposes a vision-based adaptive scoop-\ning strategy that uses the deep kernel Gaussian process method trained with\na novel meta-training strategy to learn online from very limited experience on\nout-of-distribution target terrains. Our Deep Kernel Calibration with Maxi-\nmal Deployment Gaps (kCMD) strategy explicitly trains a deep kernel model\nto adapt to large domain shifts by creating simulated maximal deployment\ngaps from an offline training dataset and training models to overcome these de-\nployment gaps during training. Employed in a Bayesian Optimization sequen-\ntial decision-making framework, the proposed method allows the robot to per-\nform high-quality scooping actions on out-of-distribution terrains after a few\nattempts, significantly outperforming non-adaptive methods proposed in the\nexcavation literature as well as other state-of-the-art meta-learning methods.\nThe proposed method also demonstrates zero-shot transfer capability, success-\nfully adapting to the NASA OWLAT platform, which serves as a state-of-the-\nart simulator for potential future planetary missions. These results demon-\nstrate the potential of training deep models with simulated deployment gaps\nfor more generalizable meta-learning in high-capacity models. Furthermore,\nthey highlight the promise of our method in autonomous lander sampling mis-\nsions by enabling landers to overcome the deployment gap between Earth and\nextraterrestrial bodies.", "sections": [{"title": "Introduction", "content": "Terrain sampling with landers and rovers during extraterrestrial scientific explorations is typ-\nically done with humans in the loop where a team of experts would carefully teleoperate the\nrobot from the earth (1), which is prohibitively slow, suffers from long delays, and can be in-\nterrupted for long durations. Meanwhile, autonomous sampling has the potential to increase\nefficiency drastically but faces daunting challenges, including large uncertainties in terrain ma-\nterial properties and composition, restrictions in onboard computation, and a limited sampling\ncapacity. As illustrated in Fig. 1, our work is inspired by the proposed NASA missions to send"}, {"title": "Background and Related Work", "content": "The problem of robotic scooping is highly related to the broader field of granular material ma-\nnipulation, which encompasses a diverse range of real-world robotic applications from food\npreparation to construction and outdoor navigation. While recent research explored various as-\npects of granular manipulation\u2014including pushing (18), grasping (19), untangling (20), and\nlocomotion (21, 22)\u2014our focus aligns most closely with scooping (23) and excavation (24)\nstudies. These related but distinct areas operate at different scales: scooping typically involves\nsmaller volumes and more precise control, while excavation deals with larger-scale earth mov-\ning. The task proposed by Schenck et al. focuses on manipulating a granular terrain to a\ncertain shape (23) by learning a predictive function of terrain shape change given an action.\nAn optimization-based method is proposed by Yang et al. to generate excavation trajectories to\nexcavate desired volumes of soil based on the intersection volume between the digging bucket\nswept volume and the terrain (12). Dadhich et al. propose to use imitation-learning for rock\nexcavation by wheel loaders, given expert demonstrations (25). All of these past works are de-\nveloped on a single type of material. In contrast to these methods, our work directly addresses\nthe large deployment gaps that are likely to be found in extraterrestrial terrain sampling.\nLeveraging deep learning and its ability to process raw high-dimensional inputs such as\nimages has become increasingly popular in the robotics domain (3, 4). However, the perfor-\nmance of neural networks can deteriorate significantly when there is a deployment gap (5), and\nfor deep models to be deployed in real-world robotics tasks where deployment gaps are fre-\nquent and inevitable, they have to be adaptive. Simply fine-tuning neural networks based on\nonline data observed during deployment to combat deployment gaps (26) is a possible approach\nfor adaptation but high-capacity models tend to overfit on sparse data. A promising class of\nmethods to train adaptive deep models is few-shot meta-learning (6), which involves training\ndeep models on a diverse offline dataset comprising multiple related tasks. This approach en-\nables the extraction of shared information across tasks, allowing the model to rapidly adapt to\nnovel, yet related, tasks using only a few examples. Few-shot meta-learning has been studied\nin low-dimensional function regression (27\u201329), high-dimensional vision tasks (30\u201333), and re-\ninforcement learning tasks (34\u201336). However, these meta-learning methods do not necessarily\nwork well right out of the box when applied to robot manipulation problems. As we will show\nin the results, current state-of-the-art methods fail to adapt quickly to OOD tasks in the terrain\nsampling domain. Our proposed meta-training method kCMD is designed for training deep\nkernel GP models. This idea of using the non-parametric deep kernel GP model for few-shot\nlearning meta-learning has been explored before (9, 37), where kernels are trained to maximize\nthe data likelihood on the training tasks. These methods have demonstrated robustness against\noverfitting when sparse online data is given. Different from these methods, kCMD explicitly\ntrains the kernel to perform well on out-of-distribution (OOD) tasks, and as we will show in the\nexperiments, improves the performance for few-shot scooping on novel terrains.\nOnce the model is trained, we use it in the Bayesian Optimization (BO) framework for\ndecision-making. BO is a popular approach for sequential optimization where the objective\nfunction is modeled with a surrogate probabilistic model, and the action is selected in each\niteration by maximizing some acquisition function that balances exploitation and exploration.\nUsing GP as the surrogate model for the objective function is common practice in BO. Meta-\nlearning GP in the context of BO has also been explored before, for both GP (38, 39) and\ndeep mean and kernels (40). Closest to our approach is the work that meta-learns deep kernels\nand means for use in BO (40). Compared to this work, where there are dozens to hundreds\nof online samples, our work focuses on the few-shot regime. In addition, while this work\noptimizes a meta-objective for the task distribution that is computationally intractable for high-\ndimensional inputs, our work deals with real-world high-dimensional inputs and challenging\ntesting scenarios that are drastically different from training scenarios."}, {"title": "Results", "content": "The model in our scooping system takes as input a parametrized scooping action and a local\nRGB-D image patch of the terrain at the scooping location aligned with the scooping direction\nand predicts the mean and variance of the scooped volume. The model architecture consists of a\ndeep mean function and a deep kernel, preceded by a shared feature extractor. The residual and\nvariance of the volume predicted by the deep kernel are summed with the deep mean prediction\nto give the mean and variance of the volume. The scooping action consists of 5 parameters:\nthe x, y scooping location, the scooping yaw angle, the scooping depth, and a binary variable\nindicating whether the stiffness of the robot impedance controller is high or low. Then the model\nis used by a Bayesian optimization decision-maker that chooses an action from the action set (a\nuniform grid over action parameters) that would maximize an acquisition function that balances\nthe scoop volume prediction and its uncertainty. Once an action is selected, an impedance\ncontroller tracks a reference trajectory generated from the action parameters.\nOur model is trained by executing a total of 5,100 random scoops on 51 terrains with dif-\nferent compositions and materials on the UIUC testbed, where the materials and compositions\nare shown in Fig. 3. Each terrain has a unique combination of one or more materials used and\ntheir composition. Eight materials, Sand, Pebbles, Slates, Gravel, Paper Balls, Corn, Shredded\nCardboard, and Mulch, are composed in three different ways to form the training data, includ-\ning Single, Mixture, and Partition. The materials are placed manually in a scooping tray that\nis approximately 0.9 m x 0.6 m x 0.2 m with varying surface features such as slopes and ridges.\nSome terrain examples are shown in Fig. 3(C).\nWe first evaluate our method on the UIUC testbed, where there are 16 test terrains that con-\ntain out-of-distribution materials and compositions. We introduce 4 novel materials, which are\nRock, Packing Peanuts, Cardboard Sheet, and Bedding, and a new Layers composition, de-\nscribed in Fig. 3. On terrains with the Layers composition, observations do not directly reflect\nthe composition of the terrain, and online experience is needed to infer it. Note that the Card-\nboard Sheet material is not scoopable. For each of the Single, Partition, Mixture, and Layers\ncompositions, we consider 4 terrains, resulting in 16 test terrains. The 4 Single terrains are\ncreated with each of the 4 new testing materials. Material combinations on terrains with the\nMixture, Partition, and Layers compositions are randomly generated from all materials but with\nthe constraints that 1) each of the 4 novel materials is selected at least once; 2) each terrain con-\ntains at least 1 novel material. We exclude Cardboard Sheet from Mixture since it is physically\nimpossible to create.\nWe first perform 2 types of simulated experiments, simulated de-\nployment and prediction accuracy, on a static test database to evaluate the performance of our\nmethods against the state of the art. The test database consists of 100 randomly chosen scoops\non each of the 16 testing terrains.\nFor simulated deployment, we evaluate how the model's prediction accuracy impacts adap-\ntive decision-making performance. In this experiment, we implement a policy that only selects\nfrom the 100 actions in the dataset for the given test terrain, and the robot receives the corre-\nsponding reward observed in the dataset. A trial begins by observing a single RGB-D image as\ninput, and the agent executes the policy until the sample reward is above a threshold B. B is\ncustomized for a given terrain and is defined as the 5th largest reward in that terrain's dataset in\nthe test database. The Single Cardboard Sheet terrain is excluded in these experiments because\nit is not scoopable.\nFor prediction accuracy, we are evaluating how well each model predicts scoop volume in\nthe k-shot setting. For each testing terrain, the dataset of 100 samples is first randomly split\ninto a query set of 80 samples. Then the support set with k shots is randomly drawn from the\nremaining 20 samples. The model prediction accuracy in terms of mean absolute error (MAE)\non the query set when conditioned on the support set is evaluated.\nWe compare our method against three state-of-the-art meta-learning methods and one non-\nadaptive supervised learning (SL) baseline:\n1. The SL baseline uses the same network architecture as ours except that it does not contain\nthe deep kernel. It is trained on all training data with supervised learning, and does not\nadapt during online testing.\n2. The first meta-learning method is implicit model-agnostic meta-learning (iMAML) (41),\nwhich is a variant that improves over the MAML (34) algorithm. It is a gradient-based\nmeta-learning algorithm that optimizes the initial weights of a neural network such that\nthey quickly adapt to the training tasks in a few gradient descent steps.\n3. The second method is DKMT (37), which is a meta-training method for deep kernel\nGPs that meta-trains the mean and kernel jointly by minimizing aggregated negative log\nmarginal likelihood loss on all training tasks. We use the same network architecture as\nours.\n4. The third method is conditional neural processes (CNP) (42), which is a non-kernel-\nbased approach that learns a task representation using the support set and conditions the\nprediction on the query set on the learned task representation.\nIn addition, we evaluate the effectiveness of leveraging OT for splitting to create maximal\ndeployment gaps. We compare against two other ways of splitting: Random Split and Manual\nSplit. In Random Split, the kernel and mean splits are created randomly. In Manual Split, the\nsplitting process is created manually based on the knowledge of the underlying materials. The\nsplits are created such that their terrain materials are different.\nThe results are summarized in Fig. 4(A) and (B). Each model is trained 3 times with dif-\nferent random seeds and average results across all tasks aggregated over 3 random seeds are\nreported. For simulated deployment, the adaptive methods are used by a UCB decision maker\nwith y = 2, while the SL baseline sorts actions by the reward predicted by the mean model\nand greedily proceeds down the list. For prediction accuracy, the mean absolute error (MAE) is\nreported with 0, 5, and 10 shots. We find that kCMD significantly outperforms all baselines for\nsimulated deployment, in terms of both average and maximum attempts used. For prediction\naccuracy, DKMT has the best performance while kCMD comes to a close second, both signif-\nicantly outperforming all the other baselines. While the kCMD and DKMT have similar MAE\nreduction from 0-shot to 10-shot on average for the prediction accuracy task, DKMT performs\na lot worse on the simulated deployment task. We find that this is because DKMT exhibits\na high variance, even degrading significantly in performance for some terrains from 0-shot to\n10-shot adaptation. On the Single Rocks testing terrain where DKMT suffers the largest degra-\ndation, BO with the DKMT model takes as many as 44 attempts to reach the threshold for one\nof the random seeds. This performance degradation is due to incorrect correlations between\nlow-quality support set samples and samples that are potentially of high quality on novel mate-\nrials. Compared to using OT for splitting, Random Split and Manual Split are worse in terms of\nboth simulated deployment and prediction accuracy. This highlights the importance of ensuring\nthat the simulated deployment gap is maximal during training.\nWe evaluate the real-world performance of our\nmethod in physical deployments on the UIUC testbed. Here, the robot executes the scooping\nsequence as determined from the action set by the optimizer, and each action introduces terrain\nshifting for the subsequent action, so the RGB-D image is re-captured after every scoop. The\naction set is a uniform grid over the action parameters, with 15 x positions (3 cm grid size), 12\ny positions (2 cm grid size), 8 yaw angles, 4 scooping depths, and 2 stiffness settings, totaling\n11520 actions. Policies are deployed on the same 15 testing terrains as the simulated deploy-"}, {"title": "Discussion", "content": "This paper introduced a novel method for granular material manipulation under domain shift\nthat uses a vision-based few-shot learning approach to adapt quickly to small amounts of on-\nline data. Our novel meta-training procedure, Deep Kernel Calibration with Maximal Deploy-\nment Gaps, demonstrates encouraging results for meta-training generalizable and adaptive high-\ncapacity models.\nAlthough our method performed well in the observed experiments, a deployed robot may\nencounter exotic materials in which correlations between appearance, action, and result are\ndrastically different than those observed in training. In such cases, the learned model may\nmislead the robot to perform poorly, and possibly even worse than exhaustive uninformed sam-\npling. A potential remedy for this problem would be to adapt the GP kernel online. We leave\ninvestigating this approach to future work.\nOur current method relies on impedance control for reactive movement to track a target tra-\njectory, and it frequently jams in challenging rocky terrains. A possible area of future work is\nutilizing the visual appearance of particle movements and the contact force experienced when\nexecuting a scoop action, which could be very informative about the underlying terrain to im-\nprove performance. We hope to adopt terrain-adaptive feedback controllers that alter the move-\nment strategy during a scoop to further improve sample volumes.\nFinally, we would like to explore more complex rewards other than sampling volume, such\nas the outcome of a scientific assay from a sample analysis instrument."}, {"title": "Method and Materials", "content": "In this section, we first detail the problem formulation and the scooping setups, and then de-\nscribe our system and method.\nWe formulate the scooping problem as a sequential decision-making\ntask, where the robot in each episode observes the terrain RGB-D image $o \\in O$ and uses a\nscooping policy to apply $a \\in A(0)$ where A(0) is a discrete set of parameterized, observation-\ndependent scooping motions. The reward $r \\in R$ of a scoop is the scooped volume. Throughout\nthe paper, a terrain is defined as a unique composition of one or more materials, where a mate-\nrial is composed of particles with consistent geometry and physical properties.\nPresented with a target terrain $T^*$, the robot's goal is to find a scoop whose reward is above\na threshold B. In planetary missions, for example, B could be the minimal volume of materials\nneeded to perform an analysis. During the n-th episode, the robot knows the history of scoops\non this terrain $H = {(2,a_i,r_i) | j = 1, ..., n - 1}$, which we also refer to as the online sup-\nport set. Note that the support set only contains samples of low quality, i.e. below B, because\notherwise the goal would already have been achieved.\nThe robot has access to an offline prior scooping experience, which consists of a set of M\nterrains ${T_1, ..., T_M }$, and a training dataset $D_i = {(2,a_i, r_i) | j = 1, ..., N_i}$ of past scoops\nand their rewards for each terrain i = 1, ..., M.\nFor a terrain, we suppose a latent variable a characterizes its composition, material proper-\nties, and topography, which are only indirectly observed. Let $a_*$ characterize $T^*$ and $a_i$ charac-\nterize $T_i$ for i = 1, . . ., M. Moreover, the observation is dependent on the latent variable, and\nan action's reward r = r(a,a) is also an unknown function of the action and latent variable.\nStandard supervised learning applied to model $r \u2248 f(o, a)$ will work well when $a_*$ is within\nthe distribution of training terrains, and $a_*$ is uniquely determined by the observation o or the\nreward is not strongly related to unobservable latent characteristics. However, when $T^*$ is out of\ndistribution or the observation o leaves ambiguity about latent aspects of the terrain that affect\nthe reward, the performance of the learned model will degrade.\nConsidering the limitations of supervised learning in this setting, online learning from H\nhas the potential to help the robot perform better on $T^*$. Meta-learning attempts to model the\ndependence of the reward or optimal policy on a, either with explicit representations of a (e.g.,\nconditional neural processes (42)) or implicit ones (e.g., kernel methods (9), which are used\nhere).\nWe first train and test our method on a university (UIUC) testbed, then di-\nrectly deploy the model on the NASA Ocean Worlds Lander Autonomy Testbed (OWLAT) (44)\nwithout finetuning. The two setups are shown in Fig. 1. The UIUC testbed includes a UR5e arm\nwith a scoop mounted on the end-effector, an overhead Intel RealSense L515 RGB-D camera,\nand a scooping tray that is approximately 0.9m x 0.6m x 0.2m. OWLAT is a high-fidelity\ntestbed developed to validate autonomy algorithms for future ocean world missions. It serves\nas a state-of-the-art platform for simulating various potential future planetary missions over a\nwide range of dynamic environments, including surface operations on small bodies where recre-\nating the dynamics in low gravity is critical. The testbed hardware consists of a 7-DOF Barrett\nWAM7 robotic arm with a host of interchangeable end-effector tools including the manipulator,\nan Intel Realsense D415 mounted on a pan-tilt mount for 3D perception, and force-torque sen-\nsors located at the interface between the arm and the platform and also at the end of the arm's\nwrist.\nWe consider a variety of materials and compositions for the offline database collected on\nthe UIUC testbed. The materials used in this project are listed in Fig. 3. The offline database\ncontains materials Sand, Pebbles, Slates, Gravel, Paper Balls, Corn, Shredded Cardboard, and\nMulch. The testing terrains on the UIUC testbed also include Rock, Packing Peanuts, Cardboard\nSheet and Bedding, which significantly differ from the offline materials in terms of appearance,\ngeometry, density, and surface properties. The terrain compositions used are listed in Fig. 3.\nThe offline database contains the Single, Mixture, and Partition compositions, while the testing\nset also contains the Layers composition. On terrains with the Layers composition, observations\ndo not directly reflect the composition of the terrain, and online experience is needed to infer\nit. All terrains are constructed manually, with varying surface features (e.g. slopes, ridges,\netc.) with a maximum elevation of about 0.2m and a maximum slope of 30\u00b0. Some terrain\nexamples are demonstrated in Fig. 3. We also observe that the scooping outcomes show high\nvariance because many terrain properties are not directly observable, such as the arrangement\nand geometry of the particles beneath the surface. For evaluation on OWLAT, we used a terrain\ndesigned by subject matter experts using out-of-distribution materials, Comet and Regolith,\nwhich is detailed in the Results section.\nA scoop action is a parameterized trajectory for a scoop end effector that is tracked by an\nimpedance controller. We follow the common practice in the excavation literature (11, 45) to\ndefine a scooping trajectory, shown in Fig. 7(A), where the scoop has a roll angle of 0 and stays\nin a plane throughout the trajectory. The scoop starts the trajectory at a location p, penetrates\nthe substrate at the attack angle a to a penetration depth of d, drags the scoop in a straight line\nfor length l to collect material, closes the scoop to an angle \u03b2, and lifts the scoop with a lifting\nheight h. We assume that the scoop always starts scooping at the terrain surface, which can be\ndetermined from the depth image. The impedance controller of the end-effector is configured\nwith stiffness parameters b.\nTo reduce the action space, we manually tuned the parameters that have minimal impact on\nthe scooping outcome, fixing the attack angle a at 135\u00b0, the dragging length l at 0.06 m, the\nclosing angle \u03b2 at 190\u00b0, and the lifting height h at 0.02 m. In addition, we set two options for\nthe impedance controller stiffness b, corresponding to soft and hard stiffness, where the linear\nspring constants are 250N/m and 750N/m and the torsion spring constants are 6 Nm/rad and\n20 Nm/rad, respectively. Therefore, the action is specified by the starting x, y position and yaw\nangle of the scoop, the scooping depth d, and stiffness b.\nTo measure the scooped volume, the scoop is moved to a fixed known pose, after which a\nheight map within the perimeter of the scoop is obtained from the depth image. The volume is\nthen calculated by integrating the difference between this height map and the height map of an\nempty scoop at the same pose collected beforehand.\nThe offline database contains data on 51 terrains, all with unique combinations of materials\nand compositions. Out of these terrains, 8 are Single, 25 are Partition, and 18 are Mixture. The\nmaterials used are randomly selected from the training materials. For each terrain, we collect\n100 random scoops, sampled uniformly with random x, y positions in the terrain tray, random\nyaw angle from a set of 8 discretized yaw angles, 45\u00b0 apart, random depth in the range of 0.03 m\nto 0.08 m, and random stiffness (either \u201chard\u201d or \u201csoft\u201d). Sometimes trajectory planning of the\nrobot manipulator for a sampled scoop can fail due to kinematic constraints. If so, the scoop\naction is discarded and sampling continues until planning is successful. The average scooped\nvolume across the offline database is 31.3 cm\u00b3, and the maximum volume is 260.8 cm\u00b3.\nOur approach models the reward's dependence on\nthe observation o, action a, and history H as a deep kernel Gaussian process (GP) model. Our\nDeep Kernel Calibration with Maximal Deployment Gaps (kCMD) method meta-trains the deep"}, {"title": "Deep Kernel Gaussian Process Model", "content": "Our approach models the reward's dependence on\nthe observation o, action a, and history H as a deep kernel Gaussian process (GP) model. Our\nDeep Kernel Calibration with Maximal Deployment Gaps (kCMD) method meta-trains the deep\nregression predicts the function values at new point x* as a Gaussian distribution:\n$P(y^*|x, y, x^*) \\sim N(m(x^*) + kK^{-1}\\overline{y}, k(x^*, x^*) \u2013 kK^{-1}k^T)$.\nHere,\n$K =\\begin{bmatrix}\nk(x_1, x_1) & ... & k(x_1, x_n) \\\\\n: & : & : \\\\\nk(x_n, x_1) & ... & k(x_n, x_n)\n\\end{bmatrix} + \\sigma_n^2I$,\n$k = [k(x^*, x_1), ..., k(x^*, x_n)]$,\n$\\overline{y} = [y_1 - m(x^*), ..., y_n \u2013 m(x^*)]^T$,\nwhere $\\sigma_n$ is the standard deviation of noise at an observation and $\\overline{y}$ is the residual. A typical\nchoice for the mean function is a constant mean and the radial basis function kernel (RBF) is\na popular kernel of choice (46). The mean constant, kernel function parameters, and $\\sigma_n$ can\nbe hand-picked if there is prior knowledge of f. In practice, such knowledge is usually not\navailable and they are estimated from data with type-II maximum likelihood by minimizing the\nnegative log marginal likelihood (NLML):\n$- log P(y|x, \\theta) = \\frac{1}{2} log |K + \\sigma_n^2I|$\n$+ \\frac{1}{2}(y - m(x))^T (K + \\sigma_n^2 I)^{-1}(y \u2013 m(x))$\n$ + c$,\nwhere @ denotes all the parameters to be determined and c is a constant.\nDeep kernels leverage neural networks to improve the scalability and expressiveness of\nkernels (8) and have been proposed for the few-shot setting (9). For deep kernels, an input\nvector is mapped to a latent vector using a neural network before going into the kernel function\n$k(g_{\\theta}(\u00b7), g_{\\theta}(\u00b7))$, where $g_{\\theta}(\u00b7)$ is a neural network with weights \u03b8. Additionally, deep kernels were\nextended to use deep mean functions $m_{\\theta}(\u00b7)$ (37) to learn more expressive mean functions. Our\nproposed deep GP contains both a deep kernel and a deep mean. The model takes in the RGB-D\nand the kernel parameters of a deep GP can be jointly trained over the entire training set with\nthe same NLML loss as Eqn. 3, where @ contains the neural network parameters. However,\nthis approach does not typically train kernels that are well-tuned to individual tasks because it\naggregates the data from all tasks together. Instead, meta-training may be realized with stochas-\ntic gradient descent with each batch containing the data for a single task, i.e. minimizing the\nfollowing aggregate loss:\n$\\min_{\\theta} \\sum_{j} log P(\\mathcal{D}_y^j| \\mathcal{D}_x^j, \\theta)$,\nwhere $\\mathcal{D}_y^j$ and $\\mathcal{D}_x^j$ are the target variables and input variables of task j. This approach, here-\nin-after referred to as Deep Kernel and Mean Transfer (DKMT), has been proposed in the\nmeta-learning literature (37, 40).\nDKMT has potential problems with out-of-distribution tasks. During training, the residuals\nseen by the kernels are residuals of the deep mean on the training tasks, which could be very\ndifferent compared to the residuals on tasks out of distribution of the training terrains. As\nour experiments will show, this feature leads to the kernels being poorly calibrated. Another\npotential issue is the over-fitting of the deep mean function. The first two terms of NLML in\nEqn. 3 are often referred to as the complexity penalty and data fit terms, where the complexity\npenalty regularizes the deep kernels (46). However, there is no regularization of the deep mean\nfunction. As a result, the deep mean can potentially overfit on all the training data, so the\nresiduals will be close to zero.\nkCMD addresses these issues by encouraging the residuals seen in kernel training to be\nrepresentative of the residuals seen in out-of-distribution tasks with novel materials. The idea\nis to split the training terrains into a mean training set and a kernel training set, where each\nset contains terrains that are maximally different from each other. This is achieved by using\noptimal transport (OT) (10), which is a principled approach for calculating the distance between\ntwo probability distributions, to measure the difference between two datasets. To perform the\nsplitting given the pairwise distance between datasets, each split is obtained by first randomly\nselecting a task dataset as the reference task dataset Dr, and splitting all task datasets into one\nthat contains the most similar datasets to Dr, according to OT, and one that contains the most\ndifferent. Then, the mean is trained on the mean training set to minimize error and the GP is\ntrained on the residuals of the mean model on the kernel training set. As we showed in the\nresults, such a simulated deployment process with maximal deployment gaps leads to trained\nkernels that generalize better to novel terrains. The optimal transport between two probability\ndistributions can be calculated given samples and a corresponding cost function that computes\nthe distance between sample points. To take into account both the observation, action, and the\ncorresponding volume of the data samples, we consider the following cost function, similar to\nwhat is done in (7, 47):\n$d((O_i, a_i, r_i), (O_j, a_j, r_j)) = ((d_{img}(O_i, O_j)/C_1)^2 + (||a_i-a_j ||/C_2)^2 + (||r_i - r_j ||/C_3)^2)^{1/2}$,\nwhere dimg measures the distance between the RGB and depth image patches and C1, C2, and\nC3 are normalization constants, which are calculated as the largest norm of observation, action,\nand volume across the entire dataset, respectively. dimg is calculated by first obtaining feature\nvectors for the RGB-D image patch pair, where each feature vector is obtained by computing\nthe histogram of each of the RGB and depth channels with 32 bins and concatenating into a\n128-dimensional feature vector. Then dimg is the L2 norm between the two feature vectors.\nDirectly solving for the optimal transport plan between the two distributions is computationally\nintensive and we opt to approximate it with the Sinkhorn divergence (48), which is calculated\nusing an efficient implementation from the Geomloss library (48).\nOne concern for splitting the training dataset is that it limits the amount of training data\navailable for the mean and kernel. Therefore, we repeat this process similarly to k-fold cross-\nvalidation, in which each fold has a separate mean model trained on the mean split for that fold,\nand then the residuals for that model on the kernel split are used to define the kernel loss for\nthat fold. A common kernel is trained using losses aggregated across folds.\nThe overall training procedure for our proposed kCMD method is detailed in Algorithm 1.\nThe feature extractor and the deep mean (0f and 0m) are first jointly trained on all training tasks\nwith standard supervised learning and saved to disk (Line 2). Subsequently, the kernel residuals\nare collected for each of the K splits (Line 3-9). Some splits are visualized in Fig. 8. For\neach mean split, the feature extractor and deep mean with weights $O_f^j$ and $O_m^k$ are trained from\nscratch, and the kernel splits' residuals are collected. The deep kernel parameters \u03b8k are then\nmeta-trained with the NLML loss on all the collected residuals across the N splits. Finally, the\nOf and Om are loaded from disk to return the final model, where $O_k^j$ and $O_m^j$ are discarded (Line\n10-12). Note that to encourage the kernel weights \u03b8k to be tuned to the feature extractor \u03b8f,\nwhen training the feature extractor from scratch for collecting the kernel residuals during each\nsplit (Line 7) we also add an L2 regularization term with a coefficient of 1 to encourage the\nweights to stay close to \u03b8f."}]}