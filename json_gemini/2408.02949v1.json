{"title": "Few-shot Scooping Under Domain Shift via Simulated Maximal Deployment Gaps", "authors": ["Yifan Zhu", "Pranay Thangeda", "Erica L Tevere", "Ashish Goel", "Erik Kramer", "Hari D Nayar", "Melkior Ornik", "Kris Hauser"], "abstract": "Autonomous lander missions on extraterrestrial bodies need to sample granular materials while coping with domain shifts, even when sampling strategies are extensively tuned on Earth. To tackle this challenge, this paper studies the few-shot scooping problem and proposes a vision-based adaptive scooping strategy that uses the deep kernel Gaussian process method trained with a novel meta-training strategy to learn online from very limited experience on out-of-distribution target terrains. Our Deep Kernel Calibration with Maximal Deployment Gaps (kCMD) strategy explicitly trains a deep kernel model to adapt to large domain shifts by creating simulated maximal deployment gaps from an offline training dataset and training models to overcome these deployment gaps during training. Employed in a Bayesian Optimization sequential decision-making framework, the proposed method allows the robot to perform high-quality scooping actions on out-of-distribution terrains after a few attempts, significantly outperforming non-adaptive methods proposed in the excavation literature as well as other state-of-the-art meta-learning methods. The proposed method also demonstrates zero-shot transfer capability, successfully adapting to the NASA OWLAT platform, which serves as a state-of-the-art simulator for potential future planetary missions. These results demonstrate the potential of training deep models with simulated deployment gaps for more generalizable meta-learning in high-capacity models. Furthermore, they highlight the promise of our method in autonomous lander sampling missions by enabling landers to overcome the deployment gap between Earth and extraterrestrial bodies.", "sections": [{"title": "Introduction", "content": "Terrain sampling with landers and rovers during extraterrestrial scientific explorations is typically done with humans in the loop where a team of experts would carefully teleoperate the robot from the earth (1), which is prohibitively slow, suffers from long delays, and can be interrupted for long durations. Meanwhile, autonomous sampling has the potential to increase efficiency drastically but faces daunting challenges, including large uncertainties in terrain material properties and composition, restrictions in onboard computation, and a limited sampling capacity. As illustrated in Fig. 1, our work is inspired by the proposed NASA missions to send autonomous landers to Europa and Enceladus for collecting and analyzing terrain samples, exploring whether these bodies exhibit conditions that could support extraterrestrial life (2). In this mission, not only is autonomous sampling desirable, but it is necessary: the mission is designed to last only 20 to 40 days due to the adversarial conditions on Europa and Enceladus that would greatly limit teleoperated operations. However, while engineers can implement autonomous sampling by tuning scripted sampling policies on terrain simulants on Earth, these policies will inevitably face a deployment gap when operating on an extraterrestrial body when the terrain properties are significantly different from the materials the policies were tested on. In such cases, the lander could degrade or completely fail to collect samples. In order to address these challenges, it is essential for a robot to quickly adjust its behavior based on a few failed attempts in unknown environments. To this end, we have developed a novel approach for a robot sampling system that learns from raw vision data and adapts quickly to drastically different scenarios.\nRecently, deep-learning-based approaches have demonstrated the potential of allowing robots to solve vision-based problems (3,4), and learn extremely complex mappings with high-capacity neural networks. However, the performance of neural networks can deteriorate significantly when there is a deployment gap (5), and adapting high-capacity networks on sparse online data tends to result in overfitting (6). The future of having robots deployed ubiquitously in the real world, where deployment gaps are frequent and inevitable, necessitates strategies that train high-capacity models that are adaptive to large domain gaps. One promising approach is few-shot meta-learning (6), which involves extracting useful information from an offline dataset consisting of multiple different but related tasks such that the model adapts to a novel task quickly. However, it is challenging to apply current few-shot meta-learning methods that are mainly designed for computer vision tasks to the robot manipulation domain, and as shown in the results, current state-of-the-art methods struggle with terrain sampling. Our solution is a few-shot meta-learning approach that explicitly trains deep models to overcome large domain gaps, where the training procedure creates large simulated deployment gaps from an offline training dataset and forces models during training to learn to overcome these deployment gaps."}, {"title": "Results", "content": "The model in our scooping system takes as input a parametrized scooping action and a local RGB-D image patch of the terrain at the scooping location aligned with the scooping direction and predicts the mean and variance of the scooped volume. The model architecture consists of a deep mean function and a deep kernel, preceded by a shared feature extractor. The residual and variance of the volume predicted by the deep kernel are summed with the deep mean prediction to give the mean and variance of the volume. The scooping action consists of 5 parameters: the x, y scooping location, the scooping yaw angle, the scooping depth, and a binary variable indicating whether the stiffness of the robot impedance controller is high or low. Then the model is used by a Bayesian optimization decision-maker that chooses an action from the action set (a uniform grid over action parameters) that would maximize an acquisition function that balances the scoop volume prediction and its uncertainty. Once an action is selected, an impedance controller tracks a reference trajectory generated from the action parameters.\nOur model is trained by executing a total of 5,100 random scoops on 51 terrains with different compositions and materials on the UIUC testbed, where the materials and compositions are shown in Fig. 3. Each terrain has a unique combination of one or more materials used and their composition. Eight materials, Sand, Pebbles, Slates, Gravel, Paper Balls, Corn, Shredded Cardboard, and Mulch, are composed in three different ways to form the training data, including Single, Mixture, and Partition. The materials are placed manually in a scooping tray that is approximately 0.9 m x 0.6 m x 0.2 m with varying surface features such as slopes and ridges. Some terrain examples are shown in Fig. 3(C).\nWe first evaluate our method on the UIUC testbed, where there are 16 test terrains that contain out-of-distribution materials and compositions. We introduce 4 novel materials, which are Rock, Packing Peanuts, Cardboard Sheet, and Bedding, and a new Layers composition, described in Fig. 3. On terrains with the Layers composition, observations do not directly reflect the composition of the terrain, and online experience is needed to infer it. Note that the Cardboard Sheet material is not scoopable. For each of the Single, Partition, Mixture, and Layers compositions, we consider 4 terrains, resulting in 16 test terrains. The 4 Single terrains are created with each of the 4 new testing materials. Material combinations on terrains with the Mixture, Partition, and Layers compositions are randomly generated from all materials but with the constraints that 1) each of the 4 novel materials is selected at least once; 2) each terrain contains at least 1 novel material. We exclude Cardboard Sheet from Mixture since it is physically impossible to create.\nWe first perform 2 types of simulated experiments, simulated deployment and prediction accuracy, on a static test database to evaluate the performance of our methods against the state of the art. The test database consists of 100 randomly chosen scoops on each of the 16 testing terrains.\nFor simulated deployment, we evaluate how the model's prediction accuracy impacts adaptive decision-making performance. In this experiment, we implement a policy that only selects from the 100 actions in the dataset for the given test terrain, and the robot receives the corresponding reward observed in the dataset. A trial begins by observing a single RGB-D image as input, and the agent executes the policy until the sample reward is above a threshold B. B is customized for a given terrain and is defined as the 5th largest reward in that terrain's dataset in the test database. The Single Cardboard Sheet terrain is excluded in these experiments because it is not scoopable.\nFor prediction accuracy, we are evaluating how well each model predicts scoop volume in the k-shot setting. For each testing terrain, the dataset of 100 samples is first randomly split into a query set of 80 samples. Then the support set with k shots is randomly drawn from the remaining 20 samples. The model prediction accuracy in terms of mean absolute error (MAE) on the query set when conditioned on the support set is evaluated.\nWe compare our method against three state-of-the-art meta-learning methods and one non-adaptive supervised learning (SL) baseline:\n1.  The SL baseline uses the same network architecture as ours except that it does not contain the deep kernel. It is trained on all training data with supervised learning, and does not adapt during online testing.\n2.  The first meta-learning method is implicit model-agnostic meta-learning (iMAML) (41), which is a variant that improves over the MAML (34) algorithm. It is a gradient-based meta-learning algorithm that optimizes the initial weights of a neural network such that they quickly adapt to the training tasks in a few gradient descent steps.\n3.  The second method is DKMT (37), which is a meta-training method for deep kernel GPs that meta-trains the mean and kernel jointly by minimizing aggregated negative log marginal likelihood loss on all training tasks. We use the same network architecture as ours.\n4.  The third method is conditional neural processes (CNP) (42), which is a non-kernel-based approach that learns a task representation using the support set and conditions the prediction on the query set on the learned task representation.\nIn addition, we evaluate the effectiveness of leveraging OT for splitting to create maximal deployment gaps. We compare against two other ways of splitting: Random Split and Manual Split. In Random Split, the kernel and mean splits are created randomly. In Manual Split, the splitting process is created manually based on the knowledge of the underlying materials. The splits are created such that their terrain materials are different.\nThe results are summarized in Fig. 4(A) and (B). Each model is trained 3 times with different random seeds and average results across all tasks aggregated over 3 random seeds are reported. For simulated deployment, the adaptive methods are used by a UCB decision maker with \u03b3 = 2, while the SL baseline sorts actions by the reward predicted by the mean model and greedily proceeds down the list. For prediction accuracy, the mean absolute error (MAE) is reported with 0, 5, and 10 shots. We find that kCMD significantly outperforms all baselines for simulated deployment, in terms of both average and maximum attempts used. For prediction accuracy, DKMT has the best performance while kCMD comes to a close second, both significantly outperforming all the other baselines. While the kCMD and DKMT have similar MAE reduction from 0-shot to 10-shot on average for the prediction accuracy task, DKMT performs a lot worse on the simulated deployment task. We find that this is because DKMT exhibits a high variance, even degrading significantly in performance for some terrains from 0-shot to 10-shot adaptation. On the Single Rocks testing terrain where DKMT suffers the largest degradation, BO with the DKMT model takes as many as 44 attempts to reach the threshold for one of the random seeds. This performance degradation is due to incorrect correlations between low-quality support set samples and samples that are potentially of high quality on novel materials. Compared to using OT for splitting, Random Split and Manual Split are worse in terms of both simulated deployment and prediction accuracy. This highlights the importance of ensuring that the simulated deployment gap is maximal during training.\nWe evaluate the real-world performance of our method in physical deployments on the UIUC testbed. Here, the robot executes the scooping sequence as determined from the action set by the optimizer, and each action introduces terrain shifting for the subsequent action, so the RGB-D image is re-captured after every scoop. The action set is a uniform grid over the action parameters, with 15 x positions (3 cm grid size), 12 y positions (2 cm grid size), 8 yaw angles, 4 scooping depths, and 2 stiffness settings, totaling 11520 actions. Policies are deployed on the same 15 testing terrains as the simulated deployment experiments and with the same termination threshold B. For each trial, a budget of 20 attempts is enforced, beyond which the trial is considered a failure. If robot trajectory planning fails for a scooping action, the next action that has the highest score according to the decision maker is selected until planning succeeds. This is done for both our method and all the baseline methods.\nIn addition to the baselines we compared against in the simulated experiments, we additionally employ a heuristic volume-maximizing (Vol-Max) policy, where the action is chosen to maximize the intersection between the scoop's swept volume and the terrain following a strategy proposed recently in the excavation literature (12). We note that Vol-Max also does not adapt. Our method and the baseline approaches use a UCB decision maker with \u03b3 = 2, except for Vol-Max and SL that use a greedy decision maker.\nEach method is run on each terrain three times. Each method except for Vol-Max is tested with three models trained with different random seeds, while Vol-Max, being a heuristic method without a learned model, is simply tested 3 times. When deploying the policies on a testing terrain, the terrain is manually reset at the start of each deployment so that surface features are consistent across trials. Note that slight terrain variations are introduced naturally during the reset.\nThe average and maximal number of attempts before termination are reported in Fig. 4(C). Our method outperforms the other baselines significantly. We show two representative trials on two terrains for each of the three methods in Fig. 5. The first terrain is Layers with Packing Peanuts over Slates (left) and Shredded Cardboard (right). The Packing Peanuts material is unseen during training, and the deep mean function of kCMD predicts higher volumes on Packing Peanuts, but due to the layer of Slates underneath the scoop jams easily. After one failed scoop, kCMD quickly adapts and predicts low volumes for Packing Peanuts, and selects a scoop directed towards shredded cardboard that results in large volumes. SL takes many samples on Packing Peanuts, but eventually stops because the Slates become exposed, and Slates are in the training database and predicted to yield low volume. iMAML is also able to quickly adapt on this terrain. The mean function for DKMT happens to predict low volumes for the novel Packing Peanuts and succeeds within a few attempts. Because Shredded Cardboard has more prominent terrain features, resulting in large intersection volumes, Vol-Max always selects to scoop on Shredded Cardboard but takes 6 attempts to obtain high volumes because Vol-Max ignores the arrangement of granular particles, which has a substantial effect on the scoop outcome. The other terrain is Single Rocks. We visualize only the trials for kCMD and the two strong baselines iMAML and DKMT due to restrictions in space. While kCMD obtains a large volume quickly, iMAML and DKMT adapt the scores incorrectly, lowering scores for potentially good actions very quickly, after merely one or two actions in the support set. The ideal location on this terrain allows the scoop to stick into a gap between rock pieces to avoid jamming and contains a big piece of rock in the direction of the scoop motion that can be scooped.\nWe apply our trained model directly on NASA OWLAT testbed, without any fine-tuning. This deployment was made possible because the action definition is independent of the robot arm kinematics and the scoop used on the UIUC testbed is replicated on OWLAT. OWLAT has one terrain simulant consisting of Comet and Regolith, illustrated in Fig. 6. Comet is an unscoopable composition of grey comet simulant material (43) surrounded by 3D printed PLA features with rugged terrain features from a 3D scan of Devil's Golf Course in Death Valley National Park, painted to match the Regolith's color. Regolith is a fine sand-like material that is visually distinct from the sand used in training. The two materials are composed together to create a hypothetical representation of the ocean world terrain. The scoopable Regolith region is designed with mounds that have heights comparable to those of the unscoopable Comet regions.\nOur UIUC testbed experiments demonstrated kCMD's superior performance among adaptive methods. To evaluate its effectiveness in OWLAT, we compare kCMD with SL and Vol-Max, the two non-adaptive baselines that represent approaches in the excavation domain (11, 12), and report the average volumes collected per attempt with a fixed budget of 5 attempts, emulating rover missions (2). The quantitative results are shown in Fig. 4 (D). Due to the large prominent features of the Comet, Vol-Max selects to scoop at the Comet region and obtains low volumes. SL starts with the Comet region and fails to modify its policy in response to the data observed online, continuing the ineffective scooping attempts in the Comet region, akin to Vol-Max. kCMD initially targeted the Comet region but quickly adapted to scoop at the Regolith, and we show a representative trial on the OWLAT testbed with kCMD in Fig. 6."}, {"title": "Discussion", "content": "This paper introduced a novel method for granular material manipulation under domain shift that uses a vision-based few-shot learning approach to adapt quickly to small amounts of online data. Our novel meta-training procedure, Deep Kernel Calibration with Maximal Deployment Gaps, demonstrates encouraging results for meta-training generalizable and adaptive high-capacity models.\nAlthough our method performed well in the observed experiments, a deployed robot may encounter exotic materials in which correlations between appearance, action, and result are drastically different than those observed in training. In such cases, the learned model may mislead the robot to perform poorly, and possibly even worse than exhaustive uninformed sampling. A potential remedy for this problem would be to adapt the GP kernel online. We leave investigating this approach to future work.\nOur current method relies on impedance control for reactive movement to track a target trajectory, and it frequently jams in challenging rocky terrains. A possible area of future work is utilizing the visual appearance of particle movements and the contact force experienced when executing a scoop action, which could be very informative about the underlying terrain to improve performance. We hope to adopt terrain-adaptive feedback controllers that alter the movement strategy during a scoop to further improve sample volumes.\nFinally, we would like to explore more complex rewards other than sampling volume, such as the outcome of a scientific assay from a sample analysis instrument."}, {"title": "Method and Materials", "content": "We formulate the scooping problem as a sequential decision-making task", "hard": "r \u201csoft\u201d). Sometimes trajectory planning of the robot manipulator for a sampled scoop can fail due to kinematic constraints. If so", "function": "n$f(x) \\sim GP(m(x)", "y_n": "T$ at $x = [x_1", "x_n": "T$", "distribution": "n$P(y^*|x", "n": "nk(x_n", "x_n)": "n\\bar{y"}, ["y_1 - m(x^*), ..., y_n - m(x^*)"], "T,$\nwhere $\\sigma_n$ is the standard deviation of noise at an observation and $\\bar{y}$ is the residual. A typical choice for the mean function is a constant mean and the radial basis function kernel (RBF) is a popular kernel of choice (46). The mean constant, kernel function parameters, and $\\sigma_n$ can be hand-picked if there is prior knowledge of $f$. In practice, such knowledge is usually not available and they are estimated from data with type-II maximum likelihood by minimizing the negative log marginal likelihood (NLML):\n$-\\log P(y|x, \\theta) = \\frac{1}{2} \\log |K + \\sigma_n^2I| + \\frac{1}{2} (y - m(x))^T (K + \\sigma_n^2 I)^{-1} (y - m(x)) + c,$\nwhere $\\theta$ denotes all the parameters to be determined and $c$ is a constant.\nDeep kernels leverage neural networks to improve the scalability and expressiveness of kernels (8) and have been proposed for the few-shot setting (9). For deep kernels, an input vector is mapped to a latent vector using a neural network before going into the kernel function $k(g_{\\theta}(.), g_{\\theta}(.))$, where $g_{\\theta}(.)$ is a neural network with weights $\\theta$. Additionally, deep kernels were extended to use deep mean functions $m_{\\theta}(.)$ (37) to learn more expressive mean functions. Our proposed deep GP contains both a deep kernel and a deep mean. The model takes in the RGB-D image observation and action parameters to predict the scooped volume. Specifically, we use a localized RGB-D image patch - a small portion of the full image starting at the sampling location and aligned with the yaw angle - rather than the entire image, along with the scooping depth and binary stiffness variable action parameters as input. This image patch contains most of the information needed to evaluate a scoop, and is much more computationally efficient than processing the entire image. The model architecture is shown Fig. 7(B). The kernel and mean function share the same feature extractor, which is a convolutional neural network, and have separate fully connected layers.\nThe neural network parameters and the kernel parameters of a deep GP can be jointly trained over the entire training set with the same NLML loss as Eqn. 3, where $\\theta$ contains the neural network parameters. However, this approach does not typically train kernels that are well-tuned to individual tasks because it aggregates the data from all tasks together. Instead, meta-training may be realized with stochastic gradient descent with each batch containing the data for a single task, i.e. minimizing the following aggregate loss:\n$\\min_{\\theta} - \\sum_{j} \\log P(D_j^y \\vert D_j^x, \\theta),$\nwhere $\\mathcal{D}^y_j$ and $\\mathcal{D}^x_j$ are the target variables and input variables of task j. This approach, herein-after referred to as Deep Kernel and Mean Transfer (DKMT), has been proposed in the meta-learning literature (37, 40).\nDKMT has potential problems with out-of-distribution tasks. During training, the residuals seen by the kernels are residuals of the deep mean on the training tasks, which could be very different compared to the residuals on tasks out of distribution of the training terrains. As our experiments will show, this feature leads to the kernels being poorly calibrated. Another potential issue is the over-fitting of the deep mean function. The first two terms of NLML in Eqn. 3 are often referred to as the complexity penalty and data fit terms, where the complexity penalty regularizes the deep kernels (46). However, there is no regularization of the deep mean function. As a result, the deep mean can potentially overfit on all the training data, so the residuals will be close to zero.\nkCMD addresses these issues by encouraging the residuals seen in kernel training to be representative of the residuals seen in out-of-distribution tasks with novel materials. The idea is to split the training terrains into a mean training set and a kernel training set, where each set contains terrains that are maximally different from each other. This is achieved by using optimal transport (OT) (10), which is a principled approach for calculating the distance between two probability distributions, to measure the difference between two datasets. To perform the splitting given the pairwise distance between datasets, each split is obtained by first randomly selecting a task dataset as the reference task dataset $D_r$, and splitting all task datasets into one that contains the most similar datasets to $D_r$, according to OT, and one that contains the most different. Then, the mean is trained on the mean training set to minimize error and the GP is trained on the residuals of the mean model on the kernel training set. As we showed in the results, such a simulated deployment process with maximal deployment gaps leads to trained kernels that generalize better to novel terrains. The optimal transport between two probability distributions can be calculated given samples and a corresponding cost function that computes the distance between sample points. To take into account both the observation, action, and the corresponding volume of the data samples, we consider the following cost function, similar to what is done in (7, 47):\n$d((o_i, a_i, r_i), (o_j, a_j, r_j)) = ((\\frac{d_{img}(o_i, o_j)}{C_1})^2 + (\\frac{||a_i - a_j||}{C_2})^2 + (\\frac{||r_i - r_j||}{C_3})^2)^{1/2},$\nwhere $d_{img}$ measures the distance between the RGB and depth image patches and $C_1$, $C_2$, and $C_3$ are normalization constants, which are calculated as the largest norm of observation, action, and volume across the entire dataset, respectively. $d_{img}$ is calculated by first obtaining feature vectors for the RGB-D image patch pair, where each feature vector is obtained by computing the histogram of each of the RGB and depth channels with 32 bins and concatenating into a 128-dimensional feature vector. Then $d_{img}$ is the $L_2$ norm between the two feature vectors. Directly solving for the optimal transport plan between the two distributions is computationally intensive and we opt to approximate it with the Sinkhorn divergence (48), which is calculated using an efficient implementation from the Geomloss library (48).\nOne concern for splitting the training dataset is that it limits the amount of training data available for the mean and kernel. Therefore, we repeat this process similarly to k-fold cross-validation, in which each fold has a separate mean model trained on the mean split for that fold, and then the residuals for that model on the kernel split are used to define the kernel loss for that fold. A common kernel is trained using losses aggregated across folds.\nThe overall training procedure for our proposed kCMD method is detailed in Algorithm 1. The feature extractor and the deep mean (${\\theta}_f$ and ${\\theta}_m$) are first jointly trained on all training tasks with standard supervised learning and saved to disk (Line 2). Subsequently, the kernel residuals are collected for each of the K splits (Line 3-9). Some splits are visualized in Fig. 8. For each mean split, the feature extractor and deep mean with weights ${\\theta}^k_f$ and ${\\theta}^k_m$ are trained from scratch, and the kernel splits' residuals are collected. The deep kernel parameters $\u03b8k$ are then meta-trained with the NLML loss on all the collected residuals across the N splits. Finally, the ${\\theta}_f$ and ${\\theta}_m$ are loaded from disk to return the final model, where ${\\theta}^k_f$ and ${\\theta}^k_m$ are discarded (Line 10-12). Note that to encourage the kernel weights \u03b8k to be tuned to the feature extractor ${\\theta}_f$, when training the feature extractor from scratch for collecting the kernel residuals during each split (Line 7) we also add an $L_2$ regularization term with a coefficient"]}