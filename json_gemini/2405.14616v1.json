{"title": "TIMEMIXER: DECOMPOSABLE MULTISCALE MIXING\nFOR TIME SERIES FORECASTING", "authors": ["Shiyu Wang", "Haixu Wu", "Xiaoming Shi", "Tengge Hu", "Huakun Luo", "Lintao Ma1*", "James Y. Zhang", "Jun Zhou"], "abstract": "Time series forecasting is widely used in extensive applications, such as traffic\nplanning and weather forecasting. However, real-world time series usually present\nintricate temporal variations, making forecasting extremely challenging. Going\nbeyond the mainstream paradigms of plain decomposition and multiperiodicity\nanalysis, we analyze temporal variations in a novel view of multiscale-mixing,\nwhich is based on an intuitive but important observation that time series present\ndistinct patterns in different sampling scales. The microscopic and the macroscopic\ninformation are reflected in fine and coarse scales respectively, and thereby complex\nvariations can be inherently disentangled. Based on this observation, we propose\nTimeMixer as a fully MLP-based architecture with Past-Decomposable-Mixing\n(PDM) and Future-Multipredictor-Mixing (FMM) blocks to take full advantage of\ndisentangled multiscale series in both past extraction and future prediction phases.\nConcretely, PDM applies the decomposition to multiscale series and further mixes\nthe decomposed seasonal and trend components in fine-to-coarse and coarse-to-fine\ndirections separately, which successively aggregates the microscopic seasonal and\nmacroscopic trend information. FMM further ensembles multiple predictors to\nutilize complementary forecasting capabilities in multiscale observations. Conse-\nquently, TimeMixer is able to achieve consistent state-of-the-art performances in\nboth long-term and short-term forecasting tasks with favorable run-time efficiency.", "sections": [{"title": "1 INTRODUCTION", "content": "Time series forecasting has been studied with immense interest in extensive applications, such as\neconomics (Granger & Newbold, 2014), energy (Mart\u00edn et al., 2010; Qian et al., 2019), traffic planning\n(Chen et al., 2001; Yin et al., 2021) and weather prediction (Wu et al., 2023b), which is to predict\nfuture temporal variations based on past observations of time series (Wu et al., 2023a). However, due\nto the complex and non-stationary nature of the real world or systems, the observed series usually\npresent intricate temporal patterns, where the multitudinous variations, such as increasing, decreasing,\nand fluctuating, are deeply mixed, bringing severe challenges to the forecasting task.\nRecently, deep models have achieved promising progress in time series forecasting. The representative\nmodels capture temporal variations with well-designed architectures, which span a wide range of\nfoundation backbones, including CNN (Wang et al., 2023; Wu et al., 2023a; Hewage et al., 2020),\nRNN (Lai et al., 2018; Qin et al., 2017; Salinas et al., 2020), Transformer (Vaswani et al., 2017;\nZhou et al., 2021; Wu et al., 2021; Zhou et al., 2022b; Nie et al., 2023) and MLP (Zeng et al., 2023;\nZhang et al., 2022; Oreshkin et al., 2019; Challu et al., 2023). In the development of elaborative\nmodel architectures, to tackle intricate temporal patterns, some special designs are also involved in\nthese deep models. The widely-acknowledged paradigms primarily include series decomposition and\nmultiperiodicity analysis. As a classical time series analysis technology, decomposition is introduced\nto deep models as a basic module by (Wu et al., 2021), which decomposes the complex temporal\npatterns into more predictable components, such as seasonal and trend, and thereby benefiting\nthe forecasting process (Zeng et al., 2023; Zhou et al., 2022b; Wang et al., 2023). Furthermore,"}, {"title": "2 RELATED WORK", "content": "As the key problem in time series analysis (Wu et al., 2023a), temporal modeling has been widely\nexplored. According to foundation backbones, deep models can be roughly categorized into the\nfollowing four paradigms: RNN-, CNN-, Transformer- and MLP-based methods. Typically, CNN-\nbased models employ the convolution kernels along the time dimension to capture temporal patterns\n(Wang et al., 2023; Hewage et al., 2020). And RNN-based methods adopt the recurrent structure\nto model the temporal state transition (Lai et al., 2018; Zhao et al., 2017). However, both RNN-\nand CNN-based methods suffer from the limited receptive field, limiting the long-term forecasting\ncapability. Recently, benefiting from the global modeling capacity, Transformer-based models have\nbeen widely-acknowledged in long-term series forecasting (Zhou et al., 2021; Wu et al., 2021;\nLiu et al., 2022b; Kitaev et al., 2020; Nie et al., 2023), which can capture the long-term temporal\ndependencies adaptively with attention mechanism. Furthermore, multiple layer projection (MLP)\nis also introduced to time series forecasting (Oreshkin et al., 2019; Challu et al., 2023; Zeng et al.,\n2023), which achieves favourable performance in both forecasting performance and efficiency.\nAdditionally, several specific designs are proposed to better capture intricate temporal patterns,\nincluding series decomposition and multi-periodicity analysis. Firstly, for the series decomposition,"}, {"title": "2.2 \u039c\u0399\u03a7\u0399NG NETWORKS", "content": "Mixing is an effective way of information integration and has been applied to computer vision and\nnatural language processing. For instance, MLP-Mixer (Tolstikhin et al., 2021) designs a two-stage\nmixing structure for image recognition, which mixes the channel information and patch information\nsuccessively with linear layers. FNet (Lee-Thorp et al., 2022) replaces attention layers in Transformer\nwith simple Fourier Transform, achieving the efficient token mixing of a sentence. In this paper, we\nfurther explore the mixing structure in time series forecasting. Unlike previous designs, TimeMixer\npresents a decomposable multi-scale mixing architecture and distinguishes the mixing methods in\nboth past information extraction and future prediction phases."}, {"title": "3 TIMEMIXER", "content": "Given a series x with one or multiple observed variates, the main objective of time series forecasting\nis to utilize past observations (length-P) to obtain the most probable future prediction (length-F). As\nmentioned above, the key challenge of accurate forecasting is to tackle intricate temporal variations. In\nthis paper, we propose TimeMixer of multiscale-mixing, benefiting from disentangled variations and\ncomplementary forecasting capabilities from multiscale series. Technically, TimeMixer consists of a\nmultiscale mixing architecture with Past-Decomposable-Mixing and Future-Multipredictor-Mixing\nfor past information extraction and future prediction respectively."}, {"title": "3.1 MULTISCALE MIXING ARCHITECTURE", "content": "Time series of different scales naturally exhibit distinct properties, where fine scales mainly depict\ndetailed patterns and coarse scales highlight macroscopic variations (Mozer, 1991). This multiscale\nview can inherently disentangle intricate variations in multiple components, thereby benefiting\ntemporal variation modeling. It is also notable that, especially for the forecasting task, multiscale time\nseries present different forecasting capabilities, due to their distinct dominating temporal patterns\n(Ferreira et al., 2006). Therefore, we present TimeMixer in a multiscale mixing architecture to utilize\nmultiscale series with distinguishing designs for past extraction and future prediction phases.\nAs shown in Figure 1, to disentangle complex variations, we first downsample the past observations\n$x \\in \\mathbb{R}^{P \\times C}$ into M scales by average pooling and finally obtain a set of multiscale time series\n$\\mathcal{X} = \\{x_0,\\dots, x_M\\}$, where $x_m \\in \\mathbb{R}^{[\\frac{P}{2^m}] \\times C}, m \\in \\{0,\\dots, M\\}$, C denotes the variate number. The\nlowest level series $x_0 = x$ is the input series, which contains the finest temporal variations, while the\nhighest-level series $x_M$ is for the macroscopic variations. Then we project these multiscale series\ninto deep features $\\mathcal{X}^0$ by the embedding layer, which can be formalized as $\\mathcal{X}^0 = \\text{Embed}(\\mathcal{X})$. With\nthe above designs, we obtain the multiscale representations of input series."}, {"title": "3.2 PAST DECOMPOSABLE MIXING", "content": "We observe that for past observations, due to the complex nature of real-world series, even the coarsest\nscale series present mixed variations. As shown in Figure 1, the series in the top layer still present\nclear seasonality and trend simultaneously. It is notable that the seasonal and trend components hold\ndistinct properties in time series analysis (Cleveland et al., 1990), which corresponds to short-term\nand long-term variations or stationary and non-stationary dynamics respectively. Therefore, instead\nof directly mixing multiscale series as a whole, we propose the Past-Decomposable-Mixing (PDM)\nblock to mix the decomposed seasonal and trend components in multiple scales separately.\nConcretely, for the l-th PDM block, we first decompose the multiscale time series $\\mathcal{X}_{l}$ into seasonal\nparts $\\mathcal{S}^l = \\{s_0^l,\\dots, s_M^l\\}$ and trend parts $\\mathcal{T}^l = \\{t_0^l,\\dots, t_M^l\\}$ by series decomposition block from\nAutoformer (Wu et al., 2021). As the above analyzed, taking the distinct properties of seasonal-trend\nparts into account, we apply the mixing operation to seasonal and trend terms separately to interact\ninformation from multiple scales. Overall, the l-th PDM block can be formalized as:\n$s_m^l, t_m^l = \\text{SeriesDecomp}(x_m^{l-1}), m \\in \\{0,\\dots, M\\},\\\\\n\\mathcal{X}^{l} = \\mathcal{X}^{l-1} + \\text{FeedForward}\\Big( \\text{S-Mix} (\\{s_m^l\\}_{m=0}^{M}) + \\text{T-Mix} (\\{t_m^l\\}_{m=0}^{M})\\Big),$\nwhere FeedForward(\u00b7) contains two linear layers with intermediate GELU activation function for\ninformation interaction among channels, S-Mix(\u00b7), T-Mix(\u00b7) denote seasonal and trend mixing.\nIn seasonality analysis (Box & Jenkins, 1970), larger periods can be seen as\nthe aggregation of smaller periods, such as the weekly period of traffic flow formed by seven daily\nchanges, addressing the importance of detailed information in predicting future seasonal variations.\nTherefore, in seasonal mixing, we adopt the bottom-up approach to incorporate information from\nthe lower-level fine-scale time series upwards, which can supplement detailed information to the\nseasonality modeling of coarser scales. Technically, for the set of multiscale seasonal parts $\\mathcal{S}^l =$"}, {"title": "Seasonal Mixing", "content": "$\\mathcal{S}^l = \\{s_0^l,\\dots, s_M^l\\}$, we use the Bottom-Up-Mixing layer for the m-th scale in a residual way to achieve\nbottom-up seasonal information interaction, which can be formalized as:\n$\\text{for } m: 1 \\rightarrow M \\text{ do: } s_m^l=s_m^l + \\text{Bottom-Up-Mixing}(s_{m-1}^l).$\nwhere Bottom-Up-Mixing(\u00b7) is instantiated as two linear layers with an intermediate GELU activa-\ntion function along the temporal dimension, whose input dimension is $[\\frac{P}{2^{m-1}}]$ and output dimension\nis $[\\frac{P}{2^m}]$.\nTrend Mixing\nContrary to seasonal parts, for trend items, the detailed variations can introduce\nnoise in capturing macroscopic trend. Note that the upper coarse scale time series can easily provide\nclear macro information than the lower level. Therefore, we adopt a top-down mixing method to\nutilize the macro knowledge from coarser scales to guide the trend modeling of finer scales.\nTechnically, for multiscale trend components $\\mathcal{T}^l = \\{t_0^l,\\dots, t_M^l\\}$, we adopt the Top-Down-Mixing\nlayer for the m-th scale in a residual way to achieve top-down trend information interaction:\n$\\text{for } m: (M-1) \\rightarrow 0 \\text{ do: } t_m^l=t_m^l + \\text{Top-Down-Mixing}(t_{m+1}^l),$\nwhere Top-Down-Mixing(\u00b7) is two linear layers with an intermediate GELU activation function,\nwhose input dimension is $[\\frac{P}{2^{m+1}}]$ and output dimension is $[\\frac{P}{2^m}]$ as shown in Figure 2.\nEmpowered by seasonal and trend mixing, PDM progressively aggregates the detailed seasonal\ninformation from fine to coarse and dive into the macroscopic trend information with prior knowledge\nfrom coarser scales, eventually achieving the multiscale mixing in past information extraction."}, {"title": "3.3 FUTURE MULTIPREDICTOR MIXING", "content": "After L PDM blocks, we obtain the multiscale past information as $\\mathcal{X}^L = \\{x_0^L,\\dots, x_M^L\\}, x_m^L\\in\n\\mathbb{R}^{[\\frac{P}{2^{m}}]\\times d_{model}}$. Since the series in different scales presents different dominating variations, their\npredictions also present different capabilities. To fully utilize the multiscale information, we propose\nto aggregate predictions from multiscale series and present Future-Multipredictor-Mixing block as:\n$\\hat{x}_m = \\text{Predictor}_m(x_m^L), m \\in \\{0,\\dots, M\\}, \\quad \\hat{x} = \\sum_{m=0}^M \\hat{x}_m,$\nwhere $\\hat{x}_m \\in \\mathbb{R}^{F \\times C}$ represents the future prediction from the m-th scale series and the final output is\n$\\hat{x} \\in \\mathbb{R}^{F \\times C}$. Predictor$\\text{Predictor}_m(\\cdot)$ denotes the predictor of the m-th scale series, which firstly adopts one\nsingle linear layer to directly regress length-F future from length-$[\\frac{P}{2^{m}}]$ extracted past information\n(Figure 2) and then projects deep representations into C variates. Note that FMM is an ensemble of\nmultiple predictors, where different predictors are based on past information from different scales,\nenabling FMM to integrate complementary forecasting capabilities of mixed multiscale series."}, {"title": "4 EXPERIMENTS", "content": "We conduct extensive experiments to evaluate the performance and efficiency of TimeMixer, covering\nlong-term and short-term forecasting, including 18 real-world benchmarks and 15 baselines. The\ndetailed model and experiment configurations are summarized in Appendix A."}, {"title": "4.1 MAIN RESULTS", "content": "As shown in Table 2, TimeMixer achieves consistent state-of-the-art\nperformance in all benchmarks, covering a large variety of series with different frequencies, variate\nnumbers and real-world scenarios. Especially, TimeMixer outperforms PatchTST by a considerable\nmargin, with a 9.4% MSE reduction in Weather and a 24.7% MSE reduction in Solar-Energy. It is\nworth noting that TimeMixer exhibits good performance even for datasets with low forecastability,\nsuch as Solar-Energy and ETT, further proving the generality and effectiveness of TimeMixer."}, {"title": "4.2 MODEL ANALYSIS", "content": "To verify the effectiveness of each component of TimeMixer, we provide detailed ablation\nstudy on every possible design in both Past-Decomposable-Mixing and Future-Multipredictor-Mixing\nblocks on all 18 experiment benchmarks. From Table 5, we have the following observations.\nThe exclusion of Future-Multipredictor-Mixing in ablation \u2461 results in a significant decrease in the\nmodel's forecasting accuracy for both short and long-term predictions. This demonstrates that mixing\nfuture predictions from multiscale series can effectively boost the model performance.\nFor the past mixing, we verify the effectiveness by removing or replacing components gradually. In\nablations and \u2463 that remove seasonal mixing and trend mixing respectively, also cause a decline"}, {"title": "Seasonal and trend mixing visualization", "content": "To provide an intuitive understanding of PDM, we\nvisualize temporal linear weights for seasonal mixing and trend mixing in Figure 3(a)~(b). We find\nthat the seasonal and trend items present distinct mixing properties, where the seasonal mixing layer\npresents periodic changes (repeated blue lines in (a)) and the trend mixing layer is dominated by\nlocal aggregations (the dominating diagonal yellow line in (b)). This also verifies the necessity of\nadopting separate mixing techniques for seasonal and trend terms. Furthermore, Figure 3(c) shows\nthe predictions of season and trend terms in fine (scale 0) and coarse (scale 3) scales. We can observe\nthat the seasonal terms of fine-scale and trend parts of coarse-scale are crucial for accurate predictions.\nThis observation provides insights for our design in utilizing bottom-up mixing for seasonal terms\nand top-down mixing for trend components."}, {"title": "Analysis on number of scales", "content": "We explore the impact from\nthe number of scales (M) in Figure 6 under different series\nlengths. Specifically, when M increases, the performance gain\ndeclines for shorter prediction lengths. In contrast, for longer\nprediction lengths, the performance improves more as M increases. Therefore, we set M as 3 for\nlong-term forecast and 1 for short-term forecast to trade off performance and efficiency."}, {"title": "5 CONCLUSION", "content": "We presented TimeMixer with a multiscale mixing architecture to tackle the intricate temporal\nvariations in time series forecasting. Empowered by Past-Decomposable-Mixing and Future-\nMultipredictor-Mixing blocks, TimeMixer took advantage of both disentangled variations and\ncomplementary forecasting capabilities. In all of our experiments, TimeMixer achieved consis-\ntent state-of-the-art performances in both long-term and short-term forecasting tasks. Moreover,\nbenefiting from the fully MLP-based architecture, TimeMixer demonstrated favorable run-time\nefficiency. Detailed visualizations and ablations are included to provide insights for our design."}, {"title": "6 ETHICS STATEMENT", "content": "Our work only focuses on the scientific problem, so there is no potential ethical risk."}, {"title": "7 REPRODUCIBILITY STATEMENT", "content": "We involve the implementation details in Appendix A, including dataset descriptions, metric calcu-\nlation and experiment configuration. The source code is provided in supplementary materials and\npublic in GitHub (https://github.com/kwuking/TimeMixer) for reproducibility."}, {"title": "A IMPLEMENTATION DETAILS", "content": "We summarized details of datasets, evaluation metrics, experiments and visualizations in this section."}, {"title": "Datasets details", "content": "We evaluate the performance of different models for long-term forecasting on\n8 well-established datasets, including Weather, Traffic, Electricity, Solar-Energy, and ETT datasets\n(ETTh1, ETTh2, ETTm1, ETTm2). Furthermore, we adopt PeMS and M4 datasets for short-term\nforecasting. We detail the descriptions of the dataset in Table 6."}, {"title": "Metric details", "content": "Regarding metrics, we utilize the mean square error (MSE) and mean absolute\nerror (MAE) for long-term forecasting. In the case of short-term forecasting, we follow the metrics\nof SCINet (Liu et al., 2022a) on the PeMS datasets, including mean absolute error (MAE), mean\nabsolute percentage error (MAPE), root mean squared error (RMSE). As for the M4 datasets, we\nfollow the methodology of N-BEATS (Oreshkin et al., 2019) and implement the symmetric mean\nabsolute percentage error (SMAPE), mean absolute scaled error (MASE), and overall weighted\naverage (OWA) as metrics. It is worth noting that OWA is a specific metric utilized in the M4\ncompetition. The calculations of these metrics are:\n$\\text{RMSE} = \\sqrt{\\frac{1}{F} \\sum_{i=1}^{F} (X_i - \\hat{X}_i)^2},$\n$\\text{MAE} = \\frac{1}{F} \\sum_{i=1}^{F} |X_i - \\hat{X}_i|,$ \n$\\text{SMAPE} = \\frac{200}{F} \\sum_{i=1}^{F} \\frac{|X_i - \\hat{X}_i|}{|X_i + \\hat{X}_i|},$\n$\\text{MAPE} = \\frac{100}{F} \\sum_{i=1}^{F} \\frac{|X_i - \\hat{X}_i|}{|X_i|},$\n$\\text{MASE} = \\frac{1}{F} \\sum_{i=1}^{F} \\frac{|X_i - \\hat{X}_i|}{\\frac{1}{s} \\sum_{j=1}^{s}|X_{j+s} - X_j|},$\n$\\text{OWA} = \\frac{1}{2} \\big(\\frac{\\text{SMAPE}}{\\text{SMAPE}_{Naive2}} + \\frac{\\text{MASE}}{\\text{MASE}_{Naive2}}\\big),$\nwhere s is the periodicity of the data. $X, \\hat{X} \\in \\mathbb{R}^{F \\times C}$ are the ground truth and prediction results of\nthe future with F time pints and C dimensions. $X_i$ means the i-th future time point."}, {"title": "Experiment details", "content": "All experiments were run three times, implemented in Pytorch (Paszke et al.,\n2019), and conducted on a single NVIDIA A100 80GB GPU. We set the initial learning rate as $10^{-2}$\nor $10^{-3}$ and used the ADAM optimizer (Kingma & Ba, 2015) with L2 loss for model optimization.\nAnd the batch size was set to be 8 between 128. By default, TimeMixer contains 2 Past Decomposable\nMixing blocks. We choose the number of scales M according to the length of the time series to achieve\na balance between performance and efficiency. To handle longer series in long-term forecasting, we\nset M to 3. As for short-term forecasting with limited series length, we set M to 1. Detailed model\nconfiguration information is presented in Table 7."}, {"title": "Visualization details", "content": "To verify complementary forecasting capabilities of multiscale series, we\nfix the PDM and train a new predictor for the feature at each scale with the ground truth future as\nsupervision in Figure 4; Figure 3(c) also utilizes the same operations. Especially for Figure 4, we also\nprovide the visualization of directly plotting the output of each predictor, i.e. $\\hat{x}_m$, $m\\in \\{0,\\dots, M\\}$\nin Eq. 6. Note that in FMM, we adopt the sum ensemble $\\hat{x} = \\sum_{m=0}^M \\hat{x}_m$ as the final output, the scale\nof each plotted cure is around $\\frac{1}{M+1}$ of ground truth, while we can still observe the distinct forecasting\ncapability of series in different scales. For clearness, we also plot the (M + 1) \u00d7 $\\hat{x}_m$ in the second\nrow of Figure 7, where the visualizations are similar to Figure 4."}, {"title": "B EFFICIENCY ANALYSIS", "content": "In the main text, we have ploted the curve of efficiency in Figure 5. Here we present the quantitive\nresults in Table 8. It should be noted that TimeMixer's outstanding efficiency advantage over\nTransformer-based models, such as PatchTST, FEDformer, and Autoformer, is attributed to its fully\nMLP-based network architecture."}, {"title": "C ERROR BARS", "content": "In this paper, we repeat all the experiments three times. Here we report the standard deviation of our\nmodel and the second best model, as well as the statistical significance test in Table 9, 10, 11."}, {"title": "D HYPERPARAMTER SENSITIVITY", "content": "In the main text, we have explored the effect of number of scales M. Here, we further evaluate the\nnumber of layers L. As shown in Table 12, we can find that in general, increasing the number of\nlayers (L) will bring improvements across different prediction lengths. Therefore, we set to 2 to trade\noff efficiency and performance."}, {"title": "E FULL RESULTS", "content": "To ensure a fair comparison between models, we conducted experiments using unified parameters\nand reported results in the main text, including aligning all the input lengths, batch sizes, and training\nepochs in all experiments. Here, we provide the full results for each forecasting setting in Table 13.\nIn addition, considering that the reported results in different papers are mostly obtained through\nhyperparameter search, we provide the experiment results with the full version of the parameter\nsearch. We searched for input length among 96, 192, 336, and 512, learning rate from 10-5 to 0.05,\nencoder layers from 1 to 5, the dmodel from 16 to 512, training epochs from 10 to 100. The results are\nincluded in Table 14, which can be used to compare the upper bound of each forecasting model.\nWe can find that the relative promotion of TimesMixer over PatchTST is smaller under comprehensive\nhyperparameter search than the unified hyperparameter setting. It is worth noticing that TimeMixer\nruns much faster than PatchTST according to the efficiency comparison in Table 8. Therefore,\nconsidering perfromance, hyperparameter-search cost and efficiency, we believe TimeMixer is a\npractical model in real-world applications and is valuable to deep time series forecasting community."}, {"title": "F FULL ABLATIONS", "content": "Here we provide the complete results of ablations and alternative designs for TimeMixer."}, {"title": "F.1 ABLATIONS OF EACH DESIGN IN TIMEMIXER", "content": "To verify the effectiveness of our design in TimeMixer, we conduct comprehensive ablations for all\nbenchmarks. All the results are provided in Table 15, 16, 17 as a supplement to Table 5 of main text.\nImplementations We implement the following 10 types of ablations:\n\u2022 Offical design in TimeMixer (case 1).\n\u2022 Ablations on Future Mixing (case \u2461): In this case, we only adopt a single predictor to the\nfinest scale features, that is $\\hat{x} = \\text{Predictor}_0(x_0^L)$.\n\u2022 Ablations on Past Mixing (case 3-7): Firstly, we remove the mixing operation of TimeMixer\nin seasonal and trend parts respectively (case \u2462-\u2463), that is removing Bottom-Up-Mixing\nor Top-Down-Mixing layer. Then, we reverse the mixing directions for seasonal and\ntrend parts (case 5-7), which means adopting Bottom-Up-Mixing layer to trend and\nTop-Down-Mixing layer to seasonal part.\n\u2022 Ablations on Decomposition (case 8-10): In these cases, we do not adopt the decomposition,\nwhich means that there is only one single feature for each scale. Thus, we can only try one\nsingle mixing direction for these features, that is bottom-up mixing in case \u2467, top-down\nmixing in case. Besides, we also test the case that is without mixing in 10, where the\ninteractions among multiscale features are removed."}, {"title": "Analysis", "content": "In all ablations, we can find that the official design in TimeMixer performs best, which\nprovides solid support to our insights in special mixing approaches. Notably, it is observed that\ncompletely reversing mixing directions for seasonal and trend parts (case \u2466) leads to a seriously per-\nformance drop. This may come from that the essential microscopic information in finer-scale seasons\nand macroscopic information in coarser-scale trends are ruined by unsuitable mixing approaches."}, {"title": "F.2 ALTERNATIVE DECOMPOSITION METHODS", "content": "In this paper, we adopt the moving-average-based season-trend decomposition, which is widely\nused in previous work, such as Autoformer (Wu et al., 2021), FEDformer (Zhou et al., 2022b) and\nDLinear (Zeng et al., 2023). It is notable that Discrete Fourier Transformer (DFT) have been widely\nrecognized in time series analysis. Thus, we also try the DFT-based decomposition as a substitute.\nHere we present two types of experiments.\nThe first one is DFT-based high- and low-frequency decomposition. We treat the high-frequency part\nlike the seasonal part in TimeMixer and the low-frequency part like the trend part. The results are\nshown in Table 18. It observed that DFT-based decomposition performs worse than our design in\nTimeMixer. Since we only explore the proper mixing approach for decomposed seasonal and trend\nparts in the paper, the bottom-up and top-down mixing strategies may be not suitable for high- and\nlow-frequency parts. New visualizations like Figure 3 and 4 are expected to provide insights to the\nmodel design. Thus, we would like to leave the exploration of DFT-based high- and low-frequency\ndecomposition methods as the future work."}, {"title": "F.3 ALTERNATIVE DOWNSAMPLING METHODS", "content": "As we stated in Section 3.1, we adopt the average pooling to obtain the multiscale series. Here\nwe replace this operation with 1D convolutions. From Table 19, we can find that the complicated\n1D-convolution-based outperforms average pooling slightly. But considering both performance and\nefficiency, we eventually use average pooling in TimeMixer."}, {"title": "F.4 ALTERNATIVE ENSEMBLE STRATEGIES", "content": "In the main text, we sum the outputs from multiple predictors towards the final result (Eq. 6). Here\nwe also try the average strategy. Note that in TimeMixer, the loss is calculated based on the ensemble\nresults, not for each predictor, that is $||x - \\hat{x}|| = ||x - \\sum_{m=0}^M \\hat{x}_m||$. When we change the ensemble\nstrategy as average, the loss will be $||x - \\hat{x}|| = ||x - \\frac{1}{M+1} \\sum_{m=0}^M \\hat{x}_m||$. Obviously, the difference\nbetween average and mean strategies is only a constant multiple.\nIt is common sense in the deep learning community that deep models can easily fit constant multiple.\nFor example, if we replace the \"sum\" with \"average\", under the same supervision, the deep model\ncan easily fit this change by learning the parameters of each predictor equal to $\\frac{1}{M+1}$ of the \"sum\"\ncase, which means these two designs are equivalent in learning the final prediction under the deep\nmodel aspect. Besides, we also provide the experiment results in Table 20, where we can find that the\nperformances of these two strategies are almost the same."}, {"title": "F.5 ABLATIONS ON LARGER SCALES AND LARGER INPUT LENGTH SETTINGS", "content": "In the previous section , we have conducted comprehensive ablations under the\nunified configuration presented in Table 7, which M is set to 1 for short-term forecasting and input\nlength is set to 96 for short-term forecasting. To further evaluate the effectiveness of our proposed\nmodule, we also provide additional ablations on larger scales for short-term forecasting and larger\ninput length settings in Table 21 as a supplement to Table 5 of the main text. Besides, we also provide\na detailed analysis of relative promotion (Table 22), where we can find the following observations:"}, {"title": "J LIMITATIONS AND FUTURE WORK", "content": "TimeMixer has shown favorable efficiency in GPU memory and running time as we presented in the\nmain text. However, it should be noted that as the input length increases, the linear mixing layer may\nresult in a larger number of model parameters, which is inefficient for mobile applications. To address\nthis issue and improve TimeMixer's parameter efficiency, we plan to investigate alternative mixing\ndesigns, such as attention-based or CNN-based in future research. In addition, we only focus on the\ntemporal dimension mixing in this paper, and also plan to incorporate the variate dimension mixing\ninto model design in our future work. Furthermore, as an application-oriented model, we made a\ngreat effort to verify the effectiveness of our design with experiments and ablations. The theoretical\nalysis to verify the optimality and completeness of our design is also a promising direction."}]}