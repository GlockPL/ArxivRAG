{"title": "AutoGameUI: Constructing High-Fidelity Game Uls via Multimodal Learning and Interactive Web-Based Tool", "authors": ["ZHONGLIANG TANG", "MENGCHEN TAN", "FEI XIA", "QINGRONG CHENG", "HAO JIANG", "YONGXIANG ZHANG"], "abstract": "We introduce an innovative system, AutoGameUI, for efficiently constructing cohesive user interfaces in game development. Our system\nis the first to address the coherence issue arising from integrating inconsistent UI and UX designs, typically leading to mismatches\nand inefficiencies. We propose a two-stage multimodal learning pipeline to obtain comprehensive representations of both UI and UX\ndesigns, and to establish their correspondences. Through the correspondences, a cohesive user interface is automatically constructed\nfrom pairwise designs. To achieve high-fidelity effects, we introduce a universal data protocol for precise design descriptions and\ncross-platform applications. We also develop an interactive web-based tool for game developers to facilitate the use of our system. We\ncreate a game UI dataset from actual game projects and combine it with a public dataset for training and evaluation. Our experimental\nresults demonstrate the effectiveness of our system in maintaining coherence between the constructed interfaces and the original\ndesigns.", "sections": [{"title": "1 INTRODUCTION", "content": "With the widespread adoption of personal computers and smart mobile devices, the gaming industry has generated\nconsiderable economic value and cultural influence around the world. However, as a critical step in game development,\ngame UI development still faces various challenges that need to be addressed. Compared to the numerous literature\n[10, 16, 21, 45, 47] in the computational UI domain for mobile apps and web pages, few academic research and industrial\napplications discovered and truly resolved the issues encountered in game UI development until now.\nIn conventional game UI development, two specialized teams collaborate in parallel to create a cohesive game user\ninterface (GameUI) that can be played in the game engine. The first team, made up of UI designers or artists, focuses on"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Representation of Uls", "content": "How to extract and represent the data attributes of graphical UIs is crucial for downstream tasks such as UI recognition\n[21], UI completion [16], UI generation [15], UI structuring [47], etc. The essential attributes include spatial layout,\nsemantics, visual appearance, textual content, view hierarchy, and rendering order. Most of the existing methods\nfor UI representation exhibit equal treatment toward spatial layout and semantics but overlook others. For example,\nLayoutTrans [13] developed a transformer model for layout generation based on the 2D bounding box and the semantic\nlabel of each element. LayoutDM [15] also trained a transformer model as the backbone in the reverse diffusion process,\nto formulate the mapping between latent representation and original layout. Other efforts [16, 44, 51] attempted to\nenhance representation performance by integrating visual appearance and textual content, but they still fell short in\ncomprehending the hierarchy and rendering order. Some studies [3, 24] delved into the modeling of the hierarchy,\nwhile the rendering order has not been explored yet. Few works extend adequate consideration to the rendering order\nfor visual consistencies. This neglect is associated with the inherent limitations of public datasets like the RICO [10]\nand PubLayNet [56]. In these datasets, the layout of UI elements does not involve complex occlusion relationships.\nMoreover, the datasets only provide RGB screenshots, not the raw RGBA images used in the rendering pipeline.\nCompared to previous works, we incorporate spatial layout, semantics, textual content, view hierarchy, and rendering\norder to comprehend the graphical UIs, which lay a robust foundation for subsequent computational tasks. Besides, we\ncreate a real-world game UI dataset based on actual game projects to measure and demonstrate the effectiveness of our\nmethod."}, {"title": "2.2 Correspondence between Uls", "content": "Identifying corresponding elements between pairs of UIs has been investigated for some time now. Earlier studies\n[18, 19] tried to establish the correspondences between hierarchical web pages with the probabilistic optimization, and\nautomatically transfer design and contents from one to another side. Dayama et al. [8] presented an integer programming\nalgorithm to optimize the correspondences and developed an interactive tool for layout transfer. LayoutBlending [48]\nrecursively found the optimal correspondences between two hierarchical structures, which maintains the consistencies\nof hierarchy and avoids unsuitable rendering effects in layout blending applications. However, this work solved the\ncorrespondence problem using the Hungarian algorithm [17], which is restricted by the bipartite matching setting. Otani\net al. [30] introduced a criterion to compute and measure the similarity between arbitrary pairs in layout generations.\nAlthough these works do not rely on extensive training data, the integrity of handcrafted features, heuristics, constraints,\nand optimization algorithms still affect their generalization performance in practical applications.\nSome works also applied deep learning methods for correspondence estimation. Patil et al. [32, 33] separately\nintroduced a recursive neural network and a graph neural network to generate layouts and measure their structural\nsimilarity. The graph neural network, named LayoutGMN, utilizes the fully-connected graph to organize layout elements\nand employs metric learning to determine the similarity of both the entire layout and individual elements. Wu et\nal. [46] carried out a multimodal transformer model to generate the latent representations of layout elements and\nthen infer correspondences across two UIs. This work pruned the dissimilar matches in the optimization process and\navoided making sub-optimal decisions. Most of these works computed the similarity matrix between pairwise elements\nfor correspondences. Once numerous elements are designed within the two homogeneous structures, incorporating"}, {"title": "2.3 Artificial Intelligence in Gaming Industry", "content": "Al technology has been increasingly applied in the gaming industry, involving game-playing bots and procedural\ncontent generation. Prior works [11, 43, 52, 53] utilized reinforced AI to develop and manage agents based on player\ndata, enabling collaborative playing or controlling Non-Playable Characters (NPCs) to enhance the player experience.\nBesides, more works employed generative AI to assist in the production of game assets, such as 2D graphics [39, 55],\n3D character and scene models [26, 35], music and voices [1, 41], facial expression and animations [6, 7, 28].\nSome efforts [12, 22, 54, 57] appeared to intersect with human-computer interaction and game development, but they\nchiefly centered around gameplay design and interaction experience, with limited relevance to game UI development\nitself. The applications of AI in game UI development remain a relatively unexplored field, with both academic research\nand industrial systems still in their early stages. Existing intelligent tools for UI development primarily focus on web\npages or mobile apps such as Uizard\u00b9 and Framer\u00b2. These tools can generate UI/UX prototypes based on prompts,\nscreenshots, or wireframes, which are not as urgently required in game UI development. Moreover, these tools often\nperform poorly in real-world game UIs which have highly stylized and aesthetic design. In comparison, our work\npioneers an automated system for GameUI construction, offering a real-time solution that directly addresses the core\nbottleneck encountered in game UI development."}, {"title": "3 GAMEUI CONSTRUCTION", "content": "Constructing GameUI fundamentally involves estimating correspondences between UI and UX designs and integrating\nthe visual appearance of UI design into UX design through these correspondences. This section provides an overview\nof the proposed system, as illustrated in Fig. 2 and Fig. 3. Given a pair of UI and UX designs, we first clarify the data\nattributes by factorizing them into multiple modalities with discrete parameters. Next, we employ a self-supervised\nlearning approach to generate comprehensive multimodal representations from the discrete parameters. Utilizing\nthe multimodal representations of UI and UX designs, we formulate the GameUI construction task as a constrained\ncorrespondence matching problem. To effectively address this problem, we introduce a novel cross-attention module and\nflexible integer programming. The novel cross-attention module learns the matching probability for identifying potential\nmatches and the integer programming acquires optimal matching results. With the established correspondences, we\ndesign a universal protocol to facilitate data description and attribute integration across UI and UX designs. Lastly, we\nintroduce a web-based tool to help users interactively construct GameUI."}, {"title": "3.1 Learning Multimodal Representation", "content": ""}, {"title": "3.1.1 Data Attributes", "content": "In game development, UI and UX designs are typically cached and managed in a tree structure,\nwith some common attributes like spatial layout, image texture, textual content, hierarchy, rendering order, and\nsemantics. Spatial layout is essential for describing the 2D geometry of leaf and non-leaf nodes. Image texture and\ntextual content are the renderable attributes exclusively presented in leaf nodes. Unlike previous works [16, 21, 46], we\nexclude image texture to avoid uncontrollable noise, as the visual appearance of UI and UX designs are usually different.\nHierarchy reveals the depth and arrangement of nodes that high-level nodes have a broader scope, while lower-level\nnodes have a narrower scope. Rendering order defines the display priority that nodes are rendered from top to bottom\nor conversely on the screen. Semantics indicate the interaction of the nodes in the tree structure such as TEXT, IMAGE,\nBUTTON, SLIDER, LIST, etc.\nMathematically, we can denote the tree structure as $T = (V, \\&)$ where V is the node set and & is the edge set. Let n\nbe the number of nodes, the kth node can be defined as:\n$Uk = (9k, Sk, tk),$\nwhere 1 \u2264 k \u2264 n. $gk = [\\frac{xk}{w}, \\frac{yk}{h}, \\frac{wk}{w}, \\frac{hk}{h}]$ is a four-dimensional parameter that details the 2D geometry of uk. Each value\nof gk is normalized with the height and width of the entire layout, and helps improve scale stability. sk is an integer\nthat indicates the semantic label. tk is the textual content and can be transformed into a fixed-length integer sequence\nwith a particular vocabulary.\nInspired by [16, 21, 29, 33], we also explore the integration of the tree structure and the fully-connected graph\nstructure into the edge. We can define the directed edge eij from node vi to node vj as:\n$eij = (\\Delta gij, \\Delta hij, \\Delta rij),$\nwhere 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 n, i \u2260 j. Agij indicates the spatial relationship between vi and vj. We adopt the identical\nformula utilized in [29, 33], which measures the translation, IoU area, aspect ratio, and orientation. Ahij \u2208 {0,1,2}\nindicates the hierarchical relationship in T and the values 0, 1, 2 correspond to three conditions respectively: vi is the\nancestor of vj; vi is the descendant of vj; vi does not have any hierarchical relationship with vj. Arij \u2208 {0, 1, 2} indicates"}, {"title": "3.1.2 Multimodal Representation", "content": "Upon defining the tree representation with discrete parameters, we decide to seek\na continuous representation with fixed-size length, which is more suitable for multimodal learning. To meet the\nrequirements, we incorporate a graph neural network \u00deg to embed the spatial parameter gk and the semantic parameter\nsk. Besides, a pre-trained language model It is employed to embed the content parameter tk. Notably, the edge\nparameters Ahij and Arij are not embedded into continuous feature vectors, which will be discussed later in Section\n3.2.3.\nThe graph neural network \u00deg is broadly similar to [29, 33] and has been proven effective in graph encoding tasks. We\nuse Ig to embed the node parameters sk, gk, and the adjacent edge parameters {$\u2206gk1,..., Agkj, ..., Agkn$} for yielding\na 128-dimensional feature vector f as:\n$f = g(9k, Sk, {\\Delta 9k1\u00b7\u00b7\u00b7, \\Delta gkj, ..., \\Delta gkn}),$\nThe pre-trained language model It is the sentence BERT model [36]. With the vocabulary of sentence BERT, every\ntext sentence is segmented into multiple tokens and transformed into an integer sequence. After feeding the integer\nsequence into the encoder-only model, the original text sentence is embedded as a 768-dimensional feature vector. It is\nimportant to note that some nodes may be devoid of textual content, resulting in an empty tk. To address this issue, we\nemploy the special token [UNK] to handle such cases. The [UNK] token performs as a padding element, ensuring every\nnode can be encoded as an equal-sized feature vector f:\n$f = Pt(tk).$\nLeveraging the modules Og and It, we acquire two heterogeneous feature vectors ff and ff, which offer different\ninsights into multiple modalities. To fuse them into a unified representation, we combine principal component analysis\n(PCA) projection and dimensional concatenation, which are widely used in works [16, 21, 44], to initialize a 256-\ndimensional feature vector fi:\n$f = [f; p (f)],$\nwhere Op is a trainable linear layer for PCA operation. We use \u00dep to project the 768-dimensional feature vector fas\nthe same as the 128-dimensional feature vector ff and make them have equivalent significance in the fusion phase."}, {"title": "3.2 Learning Correspondence Matching", "content": ""}, {"title": "3.2.1 Objective and Constraints", "content": "The foundation of GameUI construction is to establish correspondences between UI\nand UX designs. After receiving the multimodal representation of UI design TA and UX design TB, we can easily\nfind the correspondence by evaluating the similarity between pairwise multimodal representation. However, the naive\nnode-to-node similarity fails to satisfy the real-world correspondence rules and may lead to unreasonable matches.\nThis arises from the fact that UI and UX designers often work concurrently with unaligned perspectives. Due to the\nlack of timely and clear communication, they may craft mismatches with missing or redundant elements on both ends.\nAs depicted in the Fig. 4, we have identified several real-world rules as below:\nRules 1. From the viewpoint of tree structures, each node in TA has at most one matching node in TB. This suggests\nthat the leaf nodes in TA (e.g text or image) may either have matching leaf nodes in TB, or be undefined children of\nsome non-leaf nodes in TB, or be entirely redundant without matching nodes in TB."}, {"title": "3.2.2 Grouped Cross-Attention Module", "content": "In contrast to prior studies [46, 48], we do not directly compute the node-to-\nnode similarity matrix to solve the optimal transport problem. Given the trees TA and TB with hundreds or even"}, {"title": "3.2.3 Integer Programming", "content": "Although the variables P, S, r, c have been defined in Section 3.2.2, we notice that the\nregularization function \u03a9 is not determined yet. The function \u03a9 aims to eliminate suboptimal solutions, especially\nambiguous matches depicted in Fig. 6, which often arise from the varying hierarchical relationship or converse rendering\norder between pairwise matches. Assume that two nodes va and in the UI design have an ancestor-descendant\nrelationship (i.e. h = 0), but after matching, the corresponding nodes and in the UX design have a descendant-\nancestor hierarchical relationship (i.e. Ah, = 1). In this case, the pairwise matches are recognized as ambiguous.\nConsidering that the ambiguity in rendering order can be explained similarly, we determine the regularization function\nwith penalty terms as follows:\n\u03a9(P, S) =$\\sum_{P_{ii'} =1,P_{jj'}=1} \\frac{S_{ii'} + S_{jj'}}{\\tau}$"}, {"title": "3.3 Interactive Construction", "content": ""}, {"title": "3.3.1 Universal Data Protocol", "content": "Based on the estimated correspondences, we begin to construct GameUI by integrating\nthe visual attributes of the elements in the UI design into the logic structure of the UX design. This process involves\nresizing the height and width of each node, updating text content, font, and color, and replacing image textures, etc. As\ndepicted in Fig. 7, we introduce a universal protocol for data description and attribute integration.\nThe protocol is composed of two main components: language and compiler. The language defines a set of UI entities\nranging from low-level to high-level semantics. Each entity is also characterized by a sequence of attributes that precisely\ndescribe common UI attributes such as position, rotation, scale, canvas anchor, opacity, image texture, text fonts, etc. The\ncompiler is to create a parser that interprets the language into another programming language and enables execution\nacross various platforms. We employ Antlr43, a powerful parser generator to support multiple programming languages,\nincluding C++, C#, Python, and TypeScript. Benefiting from Antlr4's capabilities, we can seamlessly adapt the protocol\nfor mainstream game engines such as Unity and Unreal Engine (UE), as well as some digital content creation (DCC)\nsoftware like Photoshop and Figma. To streamline the usage, we have developed accompanying plug-ins with embedded\ncompilers to enable cross-platform applications."}, {"title": "3.3.2 Interactive Web-based Tool", "content": "We implemented a web-based interactive tool that follows a client-server architecture\nto remotely execute our proposed algorithm and display the matching results on the user's desktop PC. The input\nof the tool comprises two description files (ends with UIPROTO and UXPROTO) as well as the RGBA images that\ncomposite the UI design. The output is the updated UXPROTO, with the visual attributes integrated from the UI design."}, {"title": "4 EXPERIMENTS", "content": "In this section, we primarily evaluate the performance of our proposed approach on the GameUI construction task.\nBesides, we use the UI retrieval task as a supplementary evaluation to strengthen our assessment. To conduct these\nexperiments, we build a specialized dataset and combine it with the public RICO dataset, to compare our method against\nthe state-of-the-art methods. Finally, We demonstrate the effectiveness of our approach in the GameUI construction\nthrough both quantitative and qualitative analyses."}, {"title": "4.1 Datasets", "content": "To the best of our knowledge, there is no existing work in academia and industry that truly formulates and tackles the\nGameUI construction task. Most relevant research, such as those on UI generation or UI retrieval, primarily focuses on\nthe RICO dataset [10] or ENRICO dataset [20]. These public datasets are largely crawled from mobile Apps in categories\nsuch as communication, medicine, and finance, which are quite simple in aesthetics without complex visual elements.\nCompared to real-world game UIs, these datasets exhibit differences in data attributes and distribution. Therefore, we\nadditionally collected pairs of UI and UX designs from actual game projects to create a specialized dataset, which is\ncalled GAMEUI, aiming to better evaluate the effectiveness of our approach.\nThe GAMEUI dataset comprises 42 sets of game UIs provided by different game projects. Each set includes a pair of\nUI and UX designs. Some of these sets have already been launched in operation, while others remain in the development\nphase. We use the interactive web-based tool mentioned earlier for data annotation and cleansing. As shown in Fig. 8(d),\nwe can switch to the annotation mode and manually establish the correspondences between UI and UX designs, starting\nfrom the root node of the UX design and iteratively recording the correspondence between the image or text resources\nin the left UI tree and the secondary-level node in the right UX tree. All annotations within the same UX secondary level\nare exported as a single data sample. After removing low-quality samples from the 42 interfaces, we gather 381 samples\nin total. These samples are split into 282/33/66 for model training, validation, and testing. Especially, the samples in\ndifferent splits cannot belong to the same interface. To increase the validity and credibility of our results, we also\nconduct experiments on the RICO dataset, which is divided into 35851/2109/4218 samples for training, validation, and\ntesting."}, {"title": "4.2 Implementation Details", "content": "In the experiments with the GAMEUI dataset, we train our models in two stages. The first stage concentrates on learning\nthe multimodal representations of UI and UX designs, while the second stage aims to learn their correspondences. In the\nfirst stage, we freeze the weights of the pre-trained language model It due to the lack of sufficient text data. Meanwhile,\nwe adopt several data augmentation schemes to avoid overfitting. These data augmentation schemes include using a\nlarge language model to generate positive and negative samples for each text sentence in advance, randomly masking a\nfew words in the text sentences, and randomly translating or scaling the 2D bounding box within a limited range. We\nset the batch size to 4 and train the first-stage model for 300 epochs. The data in each batch is padded and processed to"}, {"title": "4.3 Evaluation", "content": ""}, {"title": "4.3.1 Setup", "content": "In the following studies, we select LayoutGMN [33], LayoutTrans [13], LayoutDM [15], and LayoutBlending\n[48] as baselines in the GameUI construction. We measure these methods on the GAMEUI dataset's test set and estimate\nthe correspondences between pairwise UI and UX designs. Using these correspondences, we can integrate the raw\nRGBA images for image re-rendering and compute the differences between the rendered image and the original UI\nscreenshot. It's worth noting that the raw RGBA images are exported from the UI design, and the original UI screenshot\nserves as the ground truth image.\nAmong the baselines, LayoutBlending is the unique heuristic approach that does not require training. It exploits\nspatial layout, semantics, and hierarchical structure to compute node-to-node similarity and adopts the Hungarian\nalgorithm [17] to determine correspondences. LayoutGMN, LayoutTrans, and LayoutDM are based on deep neural\nnetworks and exclusively combine spatial layout and semantics for multimodal representation learning. The DNNs-\nbased methods are individually trained in contrastive, autoregressive, and generative ways. We adopt the same learning\nconfigurations as our first-stage model to train these baselines on the RICO and GAMEUI datasets. Then, we use\nthe trained models to extract the multimodal representation of each element in UI and UX designs, and compute the\nnode-to-node cosine similarity as the matching probability matrix. Particularly, we extend the maximum length of the\nabsolute positional embeddings in LayoutDM and LayoutTrans from 25 to 200, to better accommodate the GAMEUI"}, {"title": "4.3.2 Results", "content": "We qualitatively and quantitatively compare the results of our first-stage model against LayoutGMN\n[33], LayoutTrans [13], LayoutDM [15] and Screen2Vec [24]. Table 1 illustrates that our first-stage model performs\nbetter than other baselines in the GameUI construction, even if only using a node-to-node similarity matrix based on\nthe multimodal representations for matching. Given the comparison of the identical models trained on the RICO and\nGAMEUI datasets, we can observe that this advanced performance is not limited to a specific dataset. Moreover, the\nsupplementary validations in Fig. 9 demonstrate that our first-stage model is still competitive in the UI retrieval task,\ncovering not only spatial layout but also textual content. Especially, the middle retrieval result in Fig. 9(d) is the only\none with high similarity to the query content, both of which are related to time and date. Fig. 9(e) shows that even when"}, {"title": "4.4 User Study", "content": ""}, {"title": "4.4.1 Participants", "content": "We invited three game UI developers from different game project teams to test our system. Each\nparticipant had 2-5 years of experience in game UI development, with an average age of under 30."}, {"title": "4.4.2 Environments", "content": "We provided participants with the same local desktop PC and identical remote server configurations.\nBefore testing, we completed the installation of all necessary plug-ins used in the procedure."}, {"title": "4.4.3 Materials", "content": "We prepared five sets of game UIs for testing. The first four sets were gathered from previously\ncompleted game UIs, each with a pair of UI and UX designs. The fifth set was brand-new and contained 12 pairs of\nUI and UX designs. Notably, 10 of these pairs were designed for simple pop-up windows that required little time to\nconstruct. Within the five sets, the UI designs were Photoshop files and the UX designs were Unity prefabs."}, {"title": "4.4.4 Criteria", "content": "Our evaluation criterion was based on man-days, which refers to the working hours required for a\nsingle person to construct a single GameUI. A full man-day is equivalent to eight hours."}, {"title": "4.4.5 Procedure", "content": "Two developers were assigned to build the first four sets, and the third engineer was responsible\nfor handling the brand-new set. They were asked to complete the GameUI construction under two conditions: the\nfirst one being the traditional approach, and the second one using our innovative system. The traditional approach\nrefers to manually exporting the protocol and resources from Photoshop and building the cohesive game interface in\nUnity according to the prefab's hierarchy. In contrast, the innovative approach involves using plug-ins to automatically\nexport the protocols and resources from Photoshop and Unity, which are then assembled in the interactive tool. Once\nconstruction is finished, the integrated protocol and resources are re-imported into the Unity engine for visual and\nfunctional testing. The three developers used our system before proceeding with the traditional manual method, which\nensures that the time statistics do not favor our system, even if they are already familiar with the sets."}, {"title": "4.4.6 Findings", "content": "As shown in Fig. 13, the manual construction took an average of 0.437 man-days for the four previous\nsets, while using our system only consumed 0.086 man-days. On the other hand, the manual construction took an"}, {"title": "5 LIMITATIONS & FUTURE WORK", "content": "Until now, our system is in the alpha release stage, and we are constantly refining the interactive web-based tool based on\nusers' feedback. For example, although we have introduced convenient interactive features like bidirectional confidence\nvalues to guide and support users in manual construction, users still need to frequently expand the hierarchical structure\nof the UX design to verify the matching results. The verification process slows down the construction speed in real-world\nworkflows. This is because the hierarchical tree and 2.5D main canvas (i.e. RGBA and depth rendering mode) cannot\nfully represent the 3D UI or UX design. Users have expressed their desire for the interactive tool to provide more\nintuitive visualization capabilities to aid manual reconstruction, such as a 3D hierarchical preview on the main canvas.\nAdditional limitations pertain to our algorithm and dataset. Compared to previous works, our method considers\ntwo crucial attributes: hierarchy and rendering order. However, we do not explicitly embed them into the multimodal\nrepresentation. Instead, they serve as the objective constraints in correspondence matching. Future work can integrate\nthe hierarchical relationship and the rendering order into the transformer-based models using positional encoding,\npotentially enhancing the performance of the multimodal representation. Furthermore, when we attempt to tackle\nthe cohesive issue in game UI development, the game project teams cannot provide sufficient UI and UX designs for\nour GAMEUI dataset. The reason is that earlier UI and UX designs have been lost as game design scripts are updated,\nwhich inevitably affects the building of the GAMEUI dataset. To augment the GAMEUI dataset, we have employed the\ninteractive tool to record the submitted data of internal users.\nIn game UI development, there remain many topics that deserve to be explored, such as how to automatically optimize\nthe memory usage of game UI resources and how to automatically generate UX prototypes from UI designs. Some of\nthese topics are closely related to classic computational UIs but need more in-depth investigation."}, {"title": "6 CONCLUSION", "content": "We present a novel system to integrate UI and UX designs into cohesive user interfaces in game UI development.\nOur system allows users to construct cohesive user interfaces in a web-based tool, which supports both automatic\nand manual operations. For automatic construction, our system exploits a two-stage multimodal learning pipeline\nto capture the comprehensive representations of UI and UX designs and determine their correspondences by using\ngrouped cross-attention modules and flexible integer programming. Based on the correspondence, the visual attributes\nof the UI design are seamlessly integrated into the logic structure of the UX design. For manual construction, our system\noffers multiple perspectives and interactive features to gain a user-friendly experience. To guarantee the visual and\nlogical consistency of the cohesive user interfaces, we introduce a universal data protocol, which also facilitates the\napplication of our system across different platforms. Utilizing our newly built GAMEUI dataset and RICO dataset, we\ndemonstrate the effectiveness of our system through extensive experimental results. These results show that our system\nperforms well in maintaining the coherence of the constructed interfaces with the original UI and UX designs."}]}