{"title": "Recurrent Aggregators in Neural Algorithmic Reasoning", "authors": ["Kaijia Xu", "Petar Veli\u010dkovi\u0107"], "abstract": "Neural algorithmic reasoning (NAR) is an emerging field that seeks to design neural networks that mimic classical algorithmic computations. Today, graph neural networks (GNNs) are widely used in neural algorithmic reasoners due to their message passing framework and permutation equivariance. In this extended abstract, we challenge this design choice, and replace the equivariant aggregation function with a recurrent neural network. While seemingly counter-intuitive, this approach has appropriate grounding when nodes have a natural ordering\u2014and this is the case frequently in established reasoning benchmarks like CLRS-30. Indeed, our recurrent NAR (RNAR) model performs very strongly on such tasks, while handling many others gracefully. A notable achievement of RNAR is its decisive state-of-the-art result on the Heapsort and Quickselect tasks, both deemed as a significant challenge for contemporary neural algorithmic reasoners\u2014especially the latter, where RNAR achieves a mean micro-F\u2081 score of 87%.", "sections": [{"title": "Introduction", "content": "Neural algorithmic reasoning [1, NAR] is an area of research that explores how neural networks can learn algorithms from data. This seeks to combine the benefits of both neural networks and classical algorithms and gives rise to the possibility of designing better neural networks that can learn and develop stronger algorithms for challenging real-world reasoning problems [2-7].\nGraph neural networks [8, GNNs] are the most commonly used class of models in NAR due to their algorithmic alignment [9] to dynamic programming [10]. Algorithmic alignment is the observation that an increase in the structural similarity between an algorithm and a neural network tends to result in an increase in the neural network's ability to learn the algorithm\u2014and GNNs can offer a high degree of flexibility in how this alignment is designed [11, 12]. Indeed, GNNs are capable of generalising out-of-distribution (OOD) on standard algorithmic benchmarks like CLRS [13] to a significantly higher degree [14] than, e.g., Transformer-based LLMs [15, 16].\nWhile it is evident that this improvement is largely due to the permutation equivariance properties of GNNs [17], so much so that often it is important for OOD generalisation to leverage strictly less expressive categories of permutation equivariant aggregators [11, 18], it is also worth noting that such an approach forces all neighbours of a node to be treated symmetrically\u2014and many tasks of interest to algorithms do not include such a symmetry. This is especially the case for sequential algorithms, where the input comes in the form of a list and hence a natural ordering between the elements exists. Indeed, such algorithms are frequent in CLRS-ten out of thirty of its tasks [19\u201323] are sequential.\nIn this extended abstract, we detail our attempt to leverage a recurrent aggregator in a state-of-the-art neural algorithmic reasoning architecture (leaving all other components the same). Specifically, we leverage long short-term memory (LSTM) networks [24, 25] as the aggregation function. The result-ing recurrent NAR (RNAR) model yielded a serendipitous discovery: it significantly outperformed prior art on many sequential tasks in CLRS, while also gracefully handling many algorithms without such a bias! Further, RNAR sets a dominating state-of-the-art result on the Quickselect task [21], which was previously identified as a major open challenge in neural algorithmic reasoning [26]."}, {"title": "Towards RNAR", "content": "Let G = (V, E) denote a graph, where V is the set of vertices and E is the set of edges in the graph. Let the one-hop neighbourhoods of node u be defined as $N_u = \\{v \\in V | (v, u) \\in E\\}$, and $x_u \\in \\mathbb{R}^k$ be the features of node u. With reference to the definition of GNNs in Bronstein et al. [17], we can formalise the message passing framework over this graph as:\n$X'_u = \\bigoplus_{v \\in N_u} \\psi(X_u, X_v)$ (1)\nThe message function $\\psi : \\mathbb{R}^k \\times \\mathbb{R}^k \\rightarrow \\mathbb{R}^m$ first computes the message to be sent along edge (v, u) based on node u and its neighbour node v. Then, the receiver node u will aggregate the messages along incoming edges using the aggregation function: $\\bigoplus : bag(\\mathbb{R}^m) \\rightarrow \\mathbb{R}^m$. Lastly, the update function $\\phi : \\mathbb{R}^k \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^k$ updates the features of the receiver node u based on its current features and the aggregated messages. Typically, $\\psi$ and $\\phi$ are deep multilayer perceptrons.\nWhile various forms of $N_u$ have been explored by prior art [27], nowadays it is standard practice to use a fully-connected graph [13], i.e. $N_u = V$, and allow the GNN to infer the important neighbours by itself. This assumption also makes it easier to learn multiple algorithms in the same model [14].", "subsections": [{"title": "Aggregation functions", "content": "The choice of aggregation function, $\\bigoplus$, is often central to the success of NARs. While it is well-known that aggregators such as summing are provably powerful for structural tasks in GNNs [28], in practice a more aligned selection of aggregator-such as maximisation-tends to be superior, especially out-of-distribution [11, 18]. In nearly all cases, $\\bigoplus$ is chosen to be permutation invariant\u2014i.e. yielding identical answers for any permutation of the way in which neighbours are presented to it. Such a class of models is well understood and known to be universal over unordered sets of neighbours under certain conditions [29, 30].\nPermutation invariance is challenging to learn from data due to the high degrees-of-freedom induced by the permutation group and, as such, it is believed that this is a key reason for why GNNs tend to extrapolate better on algorithmic tasks compared to autoregressive Transformers [15]. Invariance to permutations also grants the model invariance to a certain kind of asynchronous execution [12]."}, {"title": "Why would we ever drop permutation invariance in NARS?", "content": "With all of the above reasons in favour, it might seem extremely counter-intuitive to ever consider setting $\\bigoplus$ to something which is not permutation invariant. So, why did we even bother attempting it?\nThere are three key reasons:\n\u2022 Firstly, permutation invariance is a property typically most desired when inputs are assumed to be given without any order. In many algorithmic tasks, this is frequently enough not the case, making this direction worth studying. Many classical algorithm categories, such as sorting [23] and searching [21], assume that an input is a list, inducing a natural order between the nodes. Previous research [31] highlighted how such sequential algorithms are not favourable for GNNs.\n\u2022 Secondly, imposing permutation symmetry forces all neighbours to be treated equivalently, limiting expressive power and the scope of functions that could be learnt. Recently there have been trends to eliminate various kinds of equivariances from models, leading to surprising improvements [32, 33], which may also be considered motivating for our attempt.\n\u2022 Lastly, using a permutation-invariant aggregator is typically realised by fixing a commutative monoid structure. If the target task requires a substantially different monoid choice-often the case in more complex tasks this can pose a unique challenge for NARs [34]."}, {"title": "The RNAR architecture", "content": "Motivated by these reasons, in RNAR we drop the commutative monoid assumption, and instead treat $\\bigoplus :list(\\mathbb{R}^m) \\rightarrow \\mathbb{R}^m$ as an arbitrary list reduction function. We will hence assume that the N = |V| node features are pre-arranged in a list [x1, x2, ..., xN]. Such an ordering will always be provided by the CLRS benchmark through its pos node input feature [13]."}]}, {"title": "", "content": "A popular, theoretically expressive choice of such a sequential aggregator is the long short-term memory (LSTM) network [24], which we employ in this work.\nIn a typical fully-connected GNN, each node receives messages from all other nodes, and therefore receives a total of N messages in one computational step. In a GNN with an LSTM as its aggregation function, the messages from the neighbouring nodes are fed into the LSTM in a particular order.\nThe LSTM will therefore run for N time steps, with the input to the LSTM at each time step, 1 \u2264 t \u2264 N, being one of the N messages computed using:\n$z_t^{(u)} = LSTM(\\psi(x_u, x_t), z_{t-1}^{(u)})$ (2)\nwhere the initial LSTM cell state, $z_0^{(u)}$, is initialised to a fixed zero vector, 0.\nThe final updated embeddings are then computed using the output of the LSTM at the last time step, which is considered to be the aggregation of the N messages:\n$X'_u = \\phi(X_u, z_N^{(u)})$ (3)\nSince the choice of the initial node ordering clearly affects $z_N^{(u)}$, LSTM as an aggregator is not invariant to message receiving order, breaking permutation invariance.\nWhile our work offers the first comprehensive study of such a recurrent aggregator on a benchmark like CLRS-and reveals surprising results\u2014we stress that we are far from the first work to attempt replacing a GNN's aggregator with a recurrent neural network.\nThree key works to consider here include GraphSAGE [35]\u2014one of the earliest GNNs to attempt an LSTM aggregator; Janossy pooling [36]-illustrating how such models can be made permutation equivariant in expectation by applying them to randomly-sampled permutations; and LCM [34]\u2014which showed how GRU aggregators [37] can effectively learn a challenging commutative monoid."}, {"title": "Evaluating RNAR", "content": "We evaluate RNAR using the CLRS-30 algorithmic reasoning benchmark [13]. Since we want to examine whether the use of RNAR enables the emergence of novel capabilities not covered by previous state-of-the-art, we insert RNAR into the state-of-the-art Triplet-GMPNN architecture [14] with hint reversals [38], and compare its performance against the baseline Triplet-GMPNN, as well as two additional state-of-the-art neural algorithmic executors, which both offer inventive ways to boost performance: Relational Transformers [39] and G-ForgetNets [40].\nWe remark that there are several very interesting recent works improving NARs [41, 42] which we exclude because they leverage a different learning regime and/or CLRS environment assumptions."}, {"title": "Conclusion", "content": "Still, RNAR proves itself a worthy element in the NAR toolbox: with its outperformances on Find Max Subarray, Heapsort and especially Quickselect, there are now only three tasks in CLRS-30 (Floyd-Warshall, Knuth-Morris-Pratt and Strongly Connected Components) for which there is no known OOD result above 80%-indicating that we soon may need a new test split for CLRS-30.\nAs such, it is our hope that RNAR inspires future research into non-commutative aggregators in NAR. We note two obvious limitations worth exploring in the future: the memory considerations of LSTM aggregators, which caused OOMs in conjunction with triplets on four of the tasks, and the fact that the Knuth-Morris-Pratt algorithm proves challenging in spite of being a string algorithm. For the former, one may consider alternatives to recurrent aggregators such as Binary-GRUs [34]; for the latter, seeking out better alignment with automata may be desirable."}]}