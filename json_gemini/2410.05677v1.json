{"title": "T2V-TURBO-V2: ENHANCING VIDEO GENERATION MODEL POST-TRAINING THROUGH DATA, REWARD, AND CONDITIONAL GUIDANCE DESIGN", "authors": ["Jiachen Li", "Qian Long", "Jian Zheng", "Xiaofeng Gao", "Robinson Piramuthu", "Wenhu Chen", "William Yang Wang"], "abstract": "In this paper, we focus on enhancing a diffusion-based text-to-video (T2V) model during the post-training phase by distilling a highly capable consistency model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2, introduces a significant advancement by integrating various supervision signals, including high-quality training data, reward model feedback, and conditional guidance, into the consistency distillation process. Through comprehensive ablation studies, we highlight the crucial importance of tailoring datasets to specific learning objectives and the effectiveness of learning from diverse reward models for enhancing both the visual quality and text-video alignment. Additionally, we highlight the vast design space of conditional guidance strategies, which centers on designing an effective energy function to augment the teacher ODE solver. We demonstrate the potential of this approach by extracting motion guidance from the training datasets and incorporating it into the ODE solver, showcasing its effectiveness in improving the motion quality of the generated videos with the improved motion-related metrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2 establishes a new state-of-the-art result on VBench, with a Total score of 85.13, surpassing proprietary systems such as Gen-3 and Kling.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion-based (Sohl-Dickstein et al., 2015; Ho et al., 2020) neural video synthesis has been advancing at an unprecedented pace, giving rise to cutting-edge text-to-video (T2V) systems like Sora (Brooks et al., 2024), Kling (Kuaishou, 2024), DreamMachine (Luma, 2024) and Gen-3 (Gen-3, 2024). These models are capable of generating high-quality videos with detailed motion dynamics. However, the majority of these systems are proprietary, as pretraining them from scratch requires significant computational resources and access to extensive, human-curated video datasets, which are not readily accessible to the academic community. Consequently, a notable performance disparity has emerged between proprietary models and currently available open-source alternatives (Chen et al., 2024a; Guo et al., 2023; Wang et al., 2023a;c; Open-Sora, 2024; Lab & etc., 2024), which are trained on datasets with varying video quality, e.g., WebVid-10M (Bain et al., 2021).\nEfforts have been directed toward bridging the performance gap. On the one hand, various video datasets (Tan et al., 2024; Nan et al., 2024; Chen et al., 2024b; Ju et al., 2024) with higher visual quality and detailed captions are released to the public domain. On the other hand, several methods have been proposed to enhance the sample quality of pretrained T2V models during the post-training phase. For example, InstructVideo (Yuan et al., 2023) and Vader (Prabhudesai et al., 2024) propose aligning pretrained T2V models by backpropagating gradients from image-text reward models (RMs). More recently, Li et al. (2024b) proposes T2V-Turbo, achieving fast and high-quality video generation by incorporating feedback from a mixture of RMs into the consistency distillation (CD) process (Song et al., 2023; Luo et al., 2023). However, these methods still employ the WebVid data to train the model and thus solely rely on the reward feedback to improve the generation quality. In addition, training-free methods (Feng et al., 2023; Mo et al., 2024; Xie et al., 2023) have emerged that seek to enhance generation quality by introducing conditional guidance during inference. For example, MotionClone (Ling et al., 2024) extracts the temporal attention matrix from a reference video as the motion prior and uses it to formulate an energy function that guides the T2V model's sampling process, achieving impressive results. However, this approach requires calculating the additional guidance term at each sampling step, imposing significant computational overhead, and requiring access to a high-quality reference video.\nIn this paper, we aim to advance the post-training of the T2V model by incorporating supervision signals from various sources. To this end, we introduce T2V-Turbo-v2, integrating various supervision signals from high-quality video datasets, reward feedback, and conditional guidance into the CD process. Additionally, we highlight the vast design space of conditional guidance strategy, which can be boiled down to designing an energy function to augment the teacher ODE solver together with the classifier-free guidance (CFG) (Ho & Salimans, 2021). In this paper, we empirically showcase the potential of this approach by leveraging the motion guidance from MotionClone to formulate our energy function. Regarding the reference video, our key insight is that, given a pair of video and caption, the video itself naturally serves as the ideal reference when the generation prompt is its own caption. Moreover, we remedy the substantial computational cost of calculating the conditional guidance by dedicating a data preprocessing phase before training the consistency model. Implementation-wise, we improve upon T2V-Turbo by eliminating the target network used to calculate the distillation target without experiencing training instability. The saved memory from the preprocessing phase and removing the target network allows us to perform full model training, whereas T2V-Turbo can only train its model with LORA (Hu et al., 2021).\nWe design comprehensive experiments to investigate how different combinations of training datasets, RMs, and conditional guidance impact the performance of our T2V-Turbo-v2. Our design of RMs is more rigorous compared to T2V-Turbo, leveraging feedback from vision-language foundation models, e.g., CLIP (Radford et al., 2021) and the second stage model of InternVideo2"}, {"title": "2 PRELIMINARY", "content": "Diffusion Sampling The sampling process from a latent video diffusion model (Ho et al., 2022b) can be treated as solving an empirical probability flow ordinary differential equation (PF-ODE) (Song et al., 2020b). The sampling process executes in a reversed-time order, starts from a standard Gaussian noise $z_T$, and returns a clean latent $z_0$.\n$dz_t = \\frac{1}{2} \\mu(t) z_t + \\sigma(t)^2 \\epsilon(z_t, c, t) dt, z_T \\sim N(0, 1),$\nwhere $z_t \\sim p_t(z_t)$ is the noisy latent at timestep $t$, $c$ is the text prompt, and $\\mu(\\cdot)$ and $\\sigma(\\cdot)$ are the drift and diffusion coefficients, respectively. The denoising model $\\epsilon_\\theta(z_t, c, t)$ is trained to approximate the score function $-\\nabla \\log p_t (z_t)$ via score matching, and is used to construct an ODE solver $\\Psi$.\nTo improve the quality of conditional sampling, we can augment the denoising model $\\epsilon$ with classifier-free guidance (CFG) (Ho & Salimans, 2021) and the gradient of an energy function $G$\n$\\epsilon(z_t, c, t) = \\epsilon_{\\Psi} (z_t, c, t) + w (\\epsilon_{\\Psi} (z_t, C, t) - \\epsilon_{\\Psi} (z_t, \\O, t)) + \\lambda \\nabla_{z_t}G (z_t, t),$\nwhere $w$ and $\\lambda$ are parameters controlling the guidance strength.\nConsistency Model (CM) (Song et al., 2023; Luo et al., 2023) is proposed to accelerate the sampling from a PF-ODE. Specifically, it learns a consistency function $f : (z_t, c,t) \\rightarrow x_{\\epsilon}$ to directly map any $z_t$ on the PF-ODE trajectory to its origin, where $\\epsilon$ is a fixed small positive number. We can model $f$ with a CM $f_\\theta$ and distill it from a pretrained diffusion model, e.g., a denoising model $\\epsilon_\\Psi$, by minimizing the consistency distillation (CD) loss.\nConsider discretizing the time horizon into $N - 1$ sub-intervals with $t_1 = \\epsilon < t_2 < ... < t_n = T$, we can sample any $z_{t_{n+k}}$ ($k$ is the skipping interval of the ODE solver $\\Psi$) from the PF-ODE trajectory and obtain its solution with $\\Psi$. Conventional methods (Luo et al., 2023; Wang et al., 2023b; Li et al., 2024a;b) augment $\\Psi$ with CFG and the solution is given as below\n$z_{t_n}^{\\Psi,\\omega} \\leftarrow z_{t_{n+k}} + (1+w) \\Psi (z_{t_{n+k}}, t_{n+k}, t_n, c; \\psi) - w\\Psi(z_{t_{n+k}}, t_{n+k}, t_n, \\O; \\psi).$"}, {"title": "3 INTEGRATE REWARDS AND CONDITIONAL GUIDANCE INTO CONSISTENCY DISTILLATION", "content": "Conventional CD methods (Song et al., 2023; Luo et al., 2023; Li et al., 2024a;b) typically neglect the design space of conditional guidance methods and do not employ an energy function $G$ to augment the ODE solver $\\Psi$. With the conditional guidance from G, we can derive the solution of the augmented ODE solver as $z_{t_n}^{\\Psi,\\omega,\\lambda} \\leftarrow z_{t_n}^{\\Psi,\\omega} + \\lambda \\nabla_{z_t} G (z_t, t)$ with $\\lambda$ controlling the conditional guidance strength. And similarly, we further condition our CM $f_\\theta$ on $\\lambda$, i.e., $f_\\theta : (z_t, w, \\lambda, c, t) \\rightarrow z_0$.\nIn this paper, we are particularly interested in leveraging the PTA guidance as our motion guidance $G_m$. With CFG, the solution of the augmented solver can be given by\n$z_{t_{n+k}}^{\\Psi,\\omega,\\lambda} \\leftarrow z_{t_{n+k}} + (1+w) \\Psi (z_{t_{n+k}}, t_{n+k}, t_n, c; \\psi) - w\\Psi(z_{t_{n+k}}, t_{n+k}, t_n, \\O; \\psi) + \\lambda \\cdot \\nabla_{z_{t_{n+k}}}G_m (z_{t_{n+k}}, z_{t_{n+k}}^{ref}, t_{n+k}),$\nThe core insight of our method is that given video datasets with decent motion quality, the training video itself naturally serves as the ideal reference when the generation prompt is its own caption. Therefore, we can always extract the motion information from the training video and employ it to guide the video generation. Following MotionClone (Ling et al., 2024), we only apply motion guidance to the first $\\tau$ percent of the sampling steps, i.e., we explicitly set $\\lambda = 0$ for $n < N(1 - \\tau)$. Note that for $n \\geq N(1 - \\tau)$, we can still set $\\lambda = 0$ for video without good motion quality."}, {"title": "3.1 LEARNING OBJECTIVES", "content": "Our preliminary experiment shows that the target network $f_{\\theta^-}$ can be removed without affecting the training stability. This is empirically significant, as it frees us from maintaining the EMA $\\theta^-$, saving a substantial amount of GPU memory. With our $f_\\theta$ conditioned $\\lambda$, our new CD loss can be derived by slightly modifying Eq. 4\n$L_{CD} (\\theta; \\Psi) = E_{z,c,w,n} [d (f_\\theta (z_{t_{n+k}}, w, \\lambda, c, t_{n+k}), stop\\_grad (f_{\\theta^-} (z_{t_n}^{\\Psi,\\omega}, w, \\lambda, c, t_n)))],$\nNote that we do not distill the LAS guidance term from the original MotionClone, which has been demonstrated to enhance generation quality. Instead, we mitigate the potential quality loss by following T2V-Turbo (Li et al., 2024b), augmenting the conventional consistency distillation with an objective to maximize a mixture of RMs, including an image-text RM $R_{img}$ and a video-text RM $R_v$. The reward optimization objective $J$ can be formulated as below\n$J (\\theta) = E_{z,c,n} [\\beta_{img} \\sum_{m=1}^M R_{img} (x_0^m, c) + \\beta_v R_v (x_0, c) ], x_0 = D (f_\\theta (z_{t_{n+k}}, w, \\lambda, c, t_{n+k})),$\nwhere $\\beta_{img}$ and $\\beta_v$ are the weighting parameters. Note that we can optimize multiple $R_{img}$ and $R_v$ with minimal change to Eq. 10. In Sec. 4.3, we investigate the effects of different choices of RMs. Our total training loss combines the CD loss and the reward optimization objective as follows:\n$L (\\theta; \\Psi) = L_{CD} (\\theta; \\Psi) - J(\\theta).$"}, {"title": "3.2 DATA PROCESSING AND TRAINING PROCEDURES", "content": "It is important to note that calculating the gradient of an energy function $G$ can be computationally expensive, consuming substantial memory. For example, using the motion guidance $G_m$ as the energy function further requires performing DDIM inversion to obtain $z_{t_{n+k}}^{ref}$, which is too expensive to be done during each training iteration. For example, MotionClone's original implementation can consume over 40GB of GPU memory and require 3 minutes to perform the DDIM inversion.\nFortunately, we identify that the solution $z_{t_{n+k}}^{ref}$ can be pre-calculated before training the CM $f_\\theta$. Appendix A describes the detailed procedures of our preprocessing procedures in Algorithm 1 and includes the pseudo-codes for training in Algorithm 2. Our training pipeline is depicted in Fig. 2."}, {"title": "4 EXPERIMENTAL RESULTS", "content": "Our experiments aim to demonstrate our T2V-Turbo-v2's ability to generate high-quality videos and unveil the key design choices contributing to our superior performance. Sec. 4.1 evaluate our method on VBench (Huang et al., 2024) from various dimensions against a broad array of baseline methods. We then perform thorough ablation studies to demonstrate the importance of carefully selecting training datasets (Sec. 4.2), reward models (Sec. 4.3), and guidance methods (Sec. 4.4).\nSettings. We distill our T2V-Turbo-v2 from VideoCrafter2 (Chen et al., 2024a). All our models are trained on 8 NVIDIA A100 GPUs for 8K gradient steps without gradient accumulation. We use a batch size of 3 to calculate the CD loss and 1 to optimize the reward objective on each GPU device. During optimization of the image-text reward model $R_{img}$, we randomly sample 2 frames from each video by setting $M = 2$. The learning rate is set to $1e-5$, and the guidance scale is defined within the range $[W_{min}, W_{max}] = [5, 15]$. We use DDIM (Song et al., 2020a) as our ODE solver $\\Psi$, with a skipping step parameter of $k = 5$. For motion guidance (MG), we set the motion guidance percentage $\\tau = 0.5$ and strength $\\lambda = 500$."}, {"title": "4.1 COMPARISON WITH SOTA METHODS ON VBENCH", "content": "We train two variants of our T2V-Turbo-v2. Specifically, T2V-Turbo-v2 w/o MG is trained using the CFG-augmented solver (Eq. 3) without motion guidance, whereas T2V-Turbo-v2 w/ MG includes motion guidance by using solver in Eq. 8. We train on a mixed dataset VG + WV, which consists of equal portions of VidGen-1M (Tan et al., 2024) and WebVid-10M (Bain et al., 2021). While the CD loss is optimized across the entire dataset, the reward objective Eq. 10 is optimized using only WebVid data. We utilize a combination of HPSv2.1 (Wu et al., 2023a) and ClipScore (Radford et al., 2021) as our $R_{img}$, applying the same weight of $\\beta_{img} = 0.2$. Additionally, we employ the second-stage model of InternVideo2 (InternV2) (Wang et al., 2024) as the our $R_v$ with $\\beta_v = 0.5$. The rationale behind these design choices is elaborated in the ablation sections.\nBoth variants of our T2V-Turbo-v2 consistently surpass all baseline methods on VBench in terms of Total Score, outperforming even proprietary systems such as Gen-3 and Kling. Our mod-"}, {"title": "4.2 ABLATION STUDIES ON THE DESIGN OF TRAINING DATASETS", "content": "To demonstrate the importance of training data selection, we experiment with VidGen-1M (Tan et al., 2024) (VG), OpenVid-1M (Nan et al., 2024) (OV), WebVid-10M (Bain et al., 2021) (WV), and their combinations. Specifically, OV and VG contain videos with high visual quality and detailed captions, whereas the conventional WV contains videos with mixed quality and short captions. VG + WV combines equal portions of VG and WV. OV + WV combines equal portions of OV and WV. We train VCM and our T2V-Turbo-v2 w/o MG using the same set of RMs as in Sec. 4.1 for each dataset and report the evaluation results in Table 2 (per-dimension scores are in Table 8).\nSurprisingly, T2V-Turbo-v2's performance does not scale in parallel with VCM's performance. While VCM attains its highest performance on OV, incorporating reward feedback only leads to modest performance gain: a gain of 0.41 (83.63 $\\rightarrow$ 84.04) on Quality Score, 5.80 (61.9 $\\rightarrow$ 68.73) on Semantic Score, and 2.45 (78.52 $\\rightarrow$ 80.97) on Total Score. A similar phenomenon is observed with VG. In contrast, VCM's performance on WV is relatively low, but integrating reward"}, {"title": "4.3 ABLATION STUDIES ON THE DESIGN OF REWARD MODELS", "content": "Before investigating the impact of different selections of RMs, we can conclude that VCM falls short in aligning text and video from the results in Table 2. None of the VCM variants achieve a satisfactory Semantic Score, which motivates us to incorporate vision-language foundation models (Bommasani et al., 2021), such as CLIP and InternV2, to enhance text-video alignment in addition to feedback from HPSv2.1. In this section, we perform comprehensive ablation studies to assess the effectiveness of feedback from each model by training T2V-Turbo-v2 w/ MG with different combinations of HPSv2.1, CLIPScore, and InternV2 on VG + MV.\nAs illustrated in Table 3 (per-dimension scores are in Table 9) and Fig. 4, learning from a more diverse set of RMs results in better performance. Relying solely on feedback from HPSv2.1 results in only minimal enhancements in video quality compared to the baseline VCM trained without reward feedback. This contrasts with the findings of Li et al. (2024b), where HPSv2.1 significantly contributed to T2V-Turbo's performance when purely trained on the WV datasets. This discrepancy underscores that the impact of reward feedback is highly dependent on the dataset, emphasizing the importance of carefully designing the RM sets to achieve optimal performance."}, {"title": "4.4 ABLATION STUDIES ON THE EFFECT OF MOTION GUIDANCE", "content": "Table 1 demonstrates that augmenting $\\Psi$ with motion guidance, $G_t$, improves performance on VBench. To further evaluate the improvements in terms of video motion quality, we provide scores for Human Action, Dynamic Degree, and Motion Smoothness across different methods in Table 4. For a more comprehensive assessment of motion quality, we further report the Motion Binding, Action Binding and Dynamic Atribute scores from T2V-CompBench (Sun et al., 2024). Experimental results indicate that incorporating motion guidance enhances performance and improves all metrics listed in Table 4. Fig. 5 further includes a qualitative comparison between different the two variants. Additional qualitative results are included in Fig. 7 and Fig. 8."}, {"title": "5 RELATED WORK", "content": "Diffusion-based T2V Models. Conventional diffusion based T2V models often rely on large-scale image datasets for training (Ho et al., 2022b; Wang et al., 2023a; Chen et al., 2023; 2024a; Ho et al., 2022a) or adopt weights from pre-trained text-to-image (T2I) models (Zhang et al., 2023; Blattmann et al., 2023; Khachatryan et al., 2023). For example, LaVie (Wang et al., 2023c) begins training with WebVid-10M and LAION-5B before fine-tuning on a curated internal dataset of 23 million videos. Text-image datasets, such as LAION (Schuhmann et al., 2022), tend to be more than ten times larger than open-source video-text datasets like WebVid-10M (Bain et al., 2021), offering higher spatial resolution and greater diversity (Wang et al., 2023a). Recently, high-quality video datasets (Tan et al., 2024; Nan et al., 2024; Chen et al., 2024b; Wang et al., 2023d; Yang et al., 2024) with dense captions are collected and released to the public. In this paper, we aim to leverage the supervision signals from high-quality video to improve a pretrained T2V model during the post-training phase.\nVision-and-language Reward Models. Several open-source image-text reward models (RMs) have been developed to mirror human preferences for a given image-text pair, including HPS (Wu et al., 2023b;a), ImageReward (Xu et al., 2024), and PickScore (Kirstain et al., 2024). These models are typically fine-tuned from image-text foundation models like CLIP (Radford et al., 2021) and BLIP (Li et al., 2022) with human preference data. Recently, VideoScore (He et al., 2024) is released to reflect human preference on video-text pair. In this paper, we choose our RMs based on the fact that VCM struggles to achieve satisfactory text-video alignment on all of our training dataset variants. Thus, we leverage vision-language foundation models CLIP and InternV2, along with HPSv2.1, to improve the semantic quality of the generated videos. We perform thorough experiments to show that our model benefits from a diverse set of RMs.\nLearning from Human/AI Feedback is an effective method for aligning generative models with human preferences (Leike et al., 2018; Ziegler et al., 2019; Ouyang et al., 2022; Stiennon et al., 2020; Rafailov et al., 2024; Li et al., 2024a). In the domain of image generation, various approaches have been introduced to align text-to-image models with human preferences, including reinforcement learning (RL)-based methods (Fan et al., 2024; Prabhudesai et al., 2023; Zhang et al., 2024) and backpropagation-based reward fine-tuning techniques (Clark et al., 2023; Xu et al., 2024; Prabhudesai et al., 2023). Recently, InstructVideo (Yuan et al., 2023) and Vader (Prabhudesai et al., 2024) extended reward fine-tuning to optimize text-to-video (T2V) models. Our method extends T2V-Turbo (Li et al., 2024b), integrating supervision from various domains, including high-quality datasets, diverse reward feedback, and conditional guidance.\nTraining-Free Conditional Guidance has been widely adopted in controlling T2I generations (Feng et al., 2023; Mo et al., 2024; Cao et al., 2023; Epstein et al., 2023; Ge et al., 2023; Patashnik et al., 2023; Xie et al., 2023) and achieves great success. MotionClone (Ling et al., 2024) tackles T2V generation by leveraging temporal attention from a reference video to guide the video generation process. In this paper, we leverage the same motion guidance by extracting motion prior from training videos and using them to formulate the energy function that augments the ODE solver. Our results show that this method enhances the motion quality in the generated videos."}, {"title": "6 CONCLUSION AND LIMITATIONS", "content": "In this paper, we present T2V-Turbo-v2, which integrates additional supervision signals from high-quality data, diverse reward feedback, and conditional guidance into the consistency distillation process. Notably, the 16-step generations from T2V-Turbo-v2 establish a new state-of-the-art result on VBench, surpassing both its teacher VideoCrafter2 and proprietary systems such as Gen-3 and Kling. Through comprehensive ablation studies, we demonstrate the critical importance of tailoring data to specific training objectives. Additionally, we observe that VCM, without reward feedback, struggles to align text with the generated videos, highlighting the need to align with vision-language foundation models to improve text-video alignment. Furthermore, we identify and explore the vast design space of energy function, which can be used to augment the teacher ODE solver. We empirically validate the potential of this approach by showing that incorporating motion priors extracted from training data enhances the motion quality of the generated videos.\nWhile our T2V-Turbo-v2 demonstrates impressive empirical results, it is important to acknowledge certain limitations. One key constraint is that our approach cannot fully capitalize on high-quality datasets, such as OpenVid-1M, due to the limited context length of existing reward models. Additionally, the teacher model used in this work, VideoCrafter2, also relies on the CLIP text encoder for processing the generation text prompt, which may limit its ability to serve as the teacher ODE solver to handle dense video captions. Therefore, developing long-context reward models would benefit future post-training research in video generation models. Furthermore, future T2V models should incorporate text encoders capable of comprehending longer and more detailed prompts, thereby enhancing their capacity to generate richer and more aligned video content."}, {"title": "A PSEUDO-CODES OF OUR T2V-TURBO-v2'S DATA PREPROCESSING AND TRAINING PIPELINE", "content": "Algorithm 1 and Algorithm 2 presents the pseudo-codes for data preprocessing and training, respectively."}, {"title": "B FURTHER DETAILS ABOUT VBENCH", "content": "In this section, we provide an overview of the metrics used in VBench (Huang et al., 2024), followed by a discussion of how the Quality Score, Semantic Score, and Total Score are derived. For further details, we encourage readers to consult the original VBench paper.\nThe Quality Score is determined using the following metrics:\n*   Subject Consistency (Subject Consist.): This metric measures the similarity of DINO (Caron et al., 2021) features across frames.\n*   Background Consistency (BG Consist.): This is calculated based on the CLIP (Radford et al., 2021) feature similarity across frames.\n*   Temporal Flickering (Temporal Flicker.): The mean absolute difference between frames is used to quantify flickering.\n*   Motion Smoothness (Motion Smooth.): Motion priors from the video frame interpolation model (Li et al., 2023b) assess the smoothness of motion.\n*   Aesthetic Quality: This metric is based on the average of aesthetic scores generated by the LAION aesthetic predictor (Schuhmann et al., 2022).\n*   Dynamic Degree: The RAFT model (Teed & Deng, 2020) calculates the dynamic level of the video.\n*   Imaging Quality: The MUSIQ (Ke et al., 2021) predictor is used to obtain the results.\nThe Quality Score is the weighted sum of these normalized metrics, with all metrics assigned a weight of 1, except for Dynamic Degree, which is weighted at 0.5.\nThe following metrics contribute to the Semantic Score:\n*   Object Class: The success rate of generating the intended object is assessed using GRIT (Wu et al., 2022).\n*   Multiple Object: GRiT (Wu et al., 2022) also evaluates how well multiple objects are generated as specified by the prompt.\n*   Human Action: UMT (Li et al., 2023a) is used to assess the depiction of human actions.\n*   Color: This metric checks if the color in the output matches the expected color using GRIT (Wu et al., 2022).\n*   Spatial Relationship (Spatial Relation.): Calculated via a rule-based method similar to (Huang et al., 2023a).\n*   Scene: The video caption generated by Tag2Text (Huang et al., 2023b) is compared to the scene described in the prompt.\n*   Appearance Style (Appear Style.): This metric uses ViCLIP (Wang et al., 2023d) to match the video's appearance style to the prompt's style description.\n*   Temporal Style: The similarity between the video feature and the temporal style description from ViCLIP (Wang et al., 2023d) is evaluated.\n*   Overall Consistency (Overall Consist.): The overall alignment between the video feature and the full text prompt is measured using ViCLIP (Wang et al., 2023d).\nThe Semantic Score is the mean of the normalized values of the above metrics. Finally, the Total Score is computed by taking the weighted sum of the Quality Score and the Semantic Score, using the formula:\n$Total Score = \\frac{4 \\cdot Quality Score + Semantic Score}{5}$"}, {"title": "C ADDITIONAL ABLATION RESULTS", "content": "C.1 ABLATION STUDIES ON THE NUMBER OF INFERENCE STEPS.\nWe investigate the impact of varying the number of inference steps in Table 5. In general, increasing the number of inference steps leads to improved visual quality and better text-video alignment for our T2V-Turbo-v2. Our inference is performed using the BFloat16 data type. The 4-step sampling takes approximately 1 second, the 8-step sampling takes around 1.5 seconds, and the 16-step sampling takes about 3 seconds.\nNote that the results of T2V-Turbo (Li et al., 2024b) reported in VBench (Huang et al., 2024) leaderboard is obtained with 4 function evaluation steps. To ensure a fair comparison between our T2V-Turbo-v2 and T2V-Turbo, we also evaluate the 4-step generation of our T2V-Turbo-v2 and compare it with T2V-Turbo in Table 6. Our T2V-Turbo-v2 still outperforms T2V-Turbo in terms of Quality Score, Semantic Score, and Total Score."}, {"title": "C.2 ABLATION STUDIES ON THE DATASETS FOR REWARD OPTIMIZATION", "content": "When training on the mixed dataset VG + WV, we propose minimizing CD loss using both VG and WV data and only maximizing the reward objectives Eq. 10 on the WV data with short captions. In this section, we further experiment with the other combination: 1) minimizing the CD loss on VG data and maximizing reward objectives on WV data. 2) minimizing the CD loss and maximizing reward objectives using both VG and WV data.\nOur experiments show that the first setting led to reward over-optimization and color distortion in the generated videos, as shown in Fig. 6. Table 7 compares the second setting with the setting used in the main paper, demonstrating that optimizing rewards using both VG and WV reduces the benefit of aligning with RMs."}, {"title": "D ADDITIONAL QUALITATIVE RESULTS", "content": ""}]}