{"title": "Weakly Supervised Video Anomaly Detection and Localization with Spatio-Temporal Prompts", "authors": ["Peng Wu", "Xuerong Zhou", "Guansong Pang", "Zhiwei Yang", "Qingsen Yan", "Peng Wang", "Yanning Zhang"], "abstract": "Current weakly supervised video anomaly detection (WSVAD) task aims to achieve frame-level anomalous event detection with only coarse video-level annotations available. Existing works typically involve extracting global features from full-resolution video frames and training frame-level classifiers to detect anomalies in the temporal dimension. However, most anomalous events tend to occur in localized spatial regions rather than the entire video frames, which implies existing frame-level feature based works may be misled by the dominant background information and lack the interpretation of the detected anomalies. To address this dilemma, this paper introduces a novel method called STPrompt that learns spatio-temporal prompt embeddings for weakly supervised video anomaly detection and localization (WSVADL) based on pre-trained vision-language models (VLMs). Our proposed method employs a two-stream network structure, with one stream focusing on the temporal dimension and the other primarily on the spatial dimension. By leveraging the learned knowledge from pre-trained VLMs and incorporating natural motion priors from raw videos, our model learns prompt embeddings that are aligned with spatio-temporal regions of videos (e.g., patches of individual frames) for identify specific local regions of anomalies, enabling accurate video anomaly detection while mitigating the influence of background information. Without relying on detailed spatio-temporal annotations or auxiliary object detection/tracking, our method achieves state-of-the-art performance on three public benchmarks for the WSVADL task.", "sections": [{"title": "1 Introduction", "content": "As a challenging and long-standing problem, video anomaly detection (VAD) has garnered significant attention from the computer vision community. The core objective of VAD is to detect various real-world anomalous events, holding immense potential for numerous practical applications, particularly in the realm of surveillance. For example, an intelligent video surveillance system equipped with anomaly detection capabilities can promptly perceive potential dangers, thereby facilitating timely interventions to enhance public security. Early studies have primarily focused on semi-supervised VAD [6, 11, 14, 31, 37, 47], where the task is to learn normal patterns by solely utilizing normal videos, with abnormal events identified as those deviating from the learned normal pattern. However, these methods encounter limitations as they lack knowledge about abnormal videos, potentially leading to a high false alarm rate.\nWeakly supervised video anomaly detection (WSVAD) has emerged as a prominent research topic in recent years, distinct from its semi-supervised counterpart in that both normal and abnormal videos are available during the training stage. The objective of WSVAD is to achieve frame-level anomaly detection with weak or coarse annotations (i.e., video-level labels). Existing works typically involve extracting features from full-resolution frames using pre-trained models such as I3D [2], Transformer [7], and CLIP [45], followed by training a classifier based on the multiple instance learning (MIL) mechanism to predict anomalous events at the frame level. While these approaches yield promising results as a standard practice, they often overlook a crucial aspect: abnormal events tend to occur in localized spatial regions rather than spanning the entire full-resolution frame, especially in surveillance scenarios. Drawing insights from the popular benchmark UCF-Crime [52], we illustrate this observation with examples depicted in Figure 1. For clarity, the anomalous regions are delineated using orange bounding boxes. It is evident that various types of anomalies manifest in diverse spatial locations and sizes; however, the spatial extent of anomalies is typically small compared to the dimensions of the full-resolution video frame. Nevertheless, existing works compress entire frames into single features, thereby neglecting crucial region-level abnormal details, and leading subsequent classifiers to rely heavily on dominant background information. Moreover, such operations further cause that the VAD model lacks reliability and interpretability, as it does not verify whether the detection align with the actual spatial location of anomalies. Consequently, certain true positive detection may be merely \"lucky\u201d coincidences of erroneous detection (such as irrelevant background or events) to the abnormal events [31].\nTherefore, how to explicitly exploit the spatio-temporal fusion [15, 21], and detect anomalies from when and where perspectives, is an important direction for exploration. It is worth noting that our work is not the first attempt to address weakly supervised video anomaly detection and localization (WSVADL). Several previous studies [28, 33, 53, 64] have also endeavored to leverage spatio-temporal relation capabilities to enhance frame-level VAD under weak supervision. However, these approaches often necessitate complex and resource-intensive relation modeling. For example, Liu et al. [33] re-annotated the UCF-Crime benchmark, subsequently training detection models with full spatio-temporal annotations. On the other hand, Wu et al. [64] paved an another way for weakly supervised spatio-temporal VAD, which took inspirations from spatio-temporal action localization [26, 42], and used a tubelet-level detector based on pre-trained object detectors and hierarchical clustering to detect possible spatio-temporal anomalies. These methods have demonstrated improved frame-level anomaly detection results over conventional WSVAD methods by mitigating the influence of irrelevant background through intricate spatio-temporal modeling processes. Nevertheless, these solutions are either complicated, e.g., multi-scale spatial pyramid training [28] and spatio-temporal coherence modeling [53] or heavily dependent on auxiliary modules for object detection and tracking [64] and detailed spatio-temporal annotations [33].\nIn this paper, enlighted by the success of large pre-trained vision-language models (VLMs) on industrial defect detection [4, 23, 81, 82], we propose a novel method STPrompt that learns spatio-temporal prompt embeddings built on the top of VLMs for WSVADL. Different from previous works [28, 33, 53, 64], our approach is both conceptually straightforward and practically effective. Specifically, we first segment the video into frames and further then split each frame into patches, and video anomaly detection and localization can thus be conceptualized as frame-level and patch-level classification, obviating the reliance on object detection and tracking. Concurrently, to alleviate the complexity associated with spatio-temporal relation modeling, we explicitly decompose spatio-temporal VAD into two distinct sub-tasks: temporal anomaly detection and spatial anomaly localization. For temporal anomaly detection, alongside standard temporal modeling, we introduce a simple yet effective spatial attention aggregation (SA\u00b2) mechanism aimed at enhancing background denoising. This approach leverages motion priors derived from the intrinsic attributes of videos. Regarding spatial anomaly localization, we capitalize on the established image-to-concept capability of VLMs, taking a significant step forward towards training-free spatial anomaly localization without full supervision. By leveraging related concepts, we identify abnormal patches. Our approach addresses the limitations of prior works while achieving superior performance across three public benchmarks: UCF-Crime [52], ShanghaiTech [38], and UBnormal [1].\nTo summarize, the main contributions of this paper are threefold:\n\u2022 A novel model named STPrompt is proposed to address spatiotemporal video anomaly detection under weak video-level supervisions. To our knowledge, STPrompt represents the first endeavor to efficiently transfer pre-trained vision-language knowledge from VLMs to simultaneously tackle temporal anomaly detection and spatial anomaly localization.\n\u2022 To mitigate the requirement of extra auxiliary information and intricate modeling strategies, STPrompt decouples the WSVADL task into temporal anomaly detection and spatial anomaly localization. A spatial attention aggregation mechanism is devised in STPrompt to filter irrelevant background for temporal anomaly detection. Besides, a large language models (LLMs)-enabled, training-free anomaly localization method is introduced to obtain fine-grained text prompts for spatial anomaly localization.\n\u2022 Extensive experiments on three widely-used benchmarks show the superiority of STPrompt over state-of-the-art competing methods. It performs substantially better than, or on par with, the recent competing methods in anomaly detection, while largely outperforming them in TIoU for anomaly localization across all three"}, {"title": "2 Related Work", "content": "2.1 Video Anomaly Detection\n2.1.1 Semi-supervised VAD. The advent of deep learning revolutionized the field of semi-supervised VAD, with the mainstream of research focusing on convolutional neural networks (CNNs) [10, 22, 29, 35, 43, 61, 66, 69, 75], recurrent neural networks (RNNs) [51, 74], and transformers [63, 73], with many of these approaches adopting self-supervised learning principles. For example, several studies [17, 38, 77] utilize 2D-CNNs, 3D-CNNs, and RNN-based autoencoders to reconstruct normal events and identify abnormal events based on the magnitude of the reconstruction error. Liu et al. [34] proposed a CNN-based video prediction network to predict future video frames based on previous frames, while Yang et al. [73] employed transformers to extract video features and then reconstructed video events based on key-frames. Yu et al. [75] introduced a novel approach called video event completion to address gaps existing in reconstruction or frame prediction methods. Several of these approaches also address spatial anomaly localization. For instance, Li et al. [31] divided the visual field into overlapping regions and learned a global mixture model using only patches around the current frame, with regions least similar to their surroundings deemed most likely to be abnormal. Wu et al. [66] similarly divided the visual field into overlapping regions and trained a deep one-class model to discriminate abnormal regions.\n2.1.2 Weakly supervised VAD. Weakly supervised video anomaly detection [3, 9, 33, 52, 55, 64, 76] has emerged as a prominent research focus in recent years. Sultani et al. [52] were among the pioneers, introducing a deep MIL model that treats a video as a bag and its segments as instances. By utilizing ranking loss with bag-level labels, their model aims to maximize the separation between the most anomalous instances in positive bags and negative bags. Subsequent studies have endeavored to enhance the positive design aspect of WSVAD. For instance, Zhong et al. [78] proposed a graph convolutional network (GCN)-based method to model feature similarity and temporal consistency between video segments. Tian et al. [55] devised robust temporal feature magnitude learning, significantly improving the MIL approach's robustness to negative instances from abnormal videos. Li et al. [30] and Huang et al. [19] introduced transformer-based multi-sequence learning frameworks to capture temporal relationships between frames. Zhou et al. [79] proposed dual memory units and an uncertainty learning scheme to better distinguish patterns of normality and anomaly. More recently, pre-trained vision-language models have garnered significant attention in the VAD community. VadCLIP [71] was the first to efficiently transfer pre-trained language-visual knowledge from CLIP [45] to weakly supervised VAD, achieving state-of-the-art performance. Pu et al. [44] attempted to enhance WSVAD by learning prompt-enhanced context features.\n2.2 Image Anomaly Detection with Prompts\nGenerally, image anomaly detection aims to localize anomalies in images like industrial defect images, predicting an image or a pixel as normal or anomalous. Typical works [5, 18, 20, 48, 54, 57, 72] mainly focused on one-class or self-supervised anomaly detection, which only requires normal images. Recently, exploiting VLMs with prompts has emerged as a successful enabler for this task, especially for the zero/few-shot setting. WinCLIP [23] introduced a language-guided paradigm for zero-shot industrial defect detection. AnomalyCLIP [81] adapted CLIP for zero-shot industrial defect detection across different domains, which learn object-agnostic text prompts that capture generic normality and abnormality. InCTRL [82] learned residual features between query images and few-shot in-context normal images to build generalist models for image anomaly detection. These CLIP-based works inspired us to spatially local anomaly in videos, but our method is more succinct and does not require learnable parameters in the prompts."}, {"title": "3 Method", "content": "3.1 Overview\nPrevious WSVAD task supposes that only video-level labels are given for model training, and encourages the model to predict whether each video frame is abnormal at the test time, where the detection granularity falls into the frame level. In comparison to WSVAD, WSVADL is a more challenging task, which assumes that the model is supposed to detect anomalies at a finer level, i.e., the pixel level, while keeping the supervisions unchanged. Mathematically, given a set of training samples {V, \u0423, \u0423c}, where V, \u0423\u044c, and y denote the sets of video, video-level binary label, and video-level category label, respectively. For each video sample v, it has two corresponding labels, namely, yb and yc. Here yb \u2208 {0, 1}, and yb = 1 indicates that v includes anomalies; and yc \u2208 R1+C, in which C is the number of abnormal categories.\nAs aforementioned, the main limitation of previous spatiotemporal VAD works [28, 33, 53, 64] is that they rely on labor-intensive spatio-temporal annotations, detector-dependent pre-processing, and computationally expensive spatio-temporal modeling. Compared to these works, our STPrompt is conceptually simple yet practically effective, which is demonstrated in Figure 2. To move beyond the above limitations, there are a series of dedicated designs in our STPrompt. Firstly, based on this routine operation of splitting a video into multiple frames, we further split each frame into multiple patches. Through such an operation, WSVADL can be considered as a coarse frame-level and patch-level classification task without the requirement of any detection pre-processing. On this case, a natural way is to directly treat all spatial patches as instances, and then use the MIL mechanism to predict the anomaly confidence of each patch. However, such a readily implemented way is computationally heavy and can not be easily optimized [13, 27]. Therefore, to reduce the spatio-temporal modeling complexity and optimization difficulty, we then factor the WSVADL task into two sub-tasks, i.e., temporal anomaly detection and spatial anomaly localization. For temporal anomaly detection, we introduce a dual-branch model built on the top of CLIP, meanwhile, we design two key modules, on the one hand, a spatial attention aggregation assists the temporal detection model in focusing on potential spatial location of anomalies. On the other hand, a typical temporal adapter enhances the temporal context capture capabilities. For spatial anomaly localization, to address the challenges posed by insufficient supervisions,"}, {"title": "3.2 Motion Prior-aware Spatio-Temporal Prompt Learning for Anomaly Detection", "content": "Inspired by the pioneer work VadCLIP [71], we also introduce a dual-branch framework, namely, classification branch and alignment branch. Specifically, given a video v, we employ a frozen image encoder of CLIP to extract the frame-level feature XCLIP \u2208 RT\u00d7D, where T is the length of video v, and D is the feature dimension. Then these feature are fed into two branches after a series of information enhancements, classification branch is to directly predict the anomaly confidence A \u2208 RT\u00d71 by a binary classifier, another align branch is to compute the anomaly category probability M\u2208RT\u00d7(1+C) by means of the image-to-concept alignment. With A and M in hands, we adopt the typical TopK [67] and the recent MIL-Align [71] strategies to compute the video-level anomaly prediction and category prediction, respectively, these predictions are subsequently used to calculate losses and provide data support for model optimization. Throughout the whole process, we devise two modules to encourage the model to focus on anomalies from the spatial and temporal dimensions, which are illustrated in the following sections.\n3.2.1 Motion prior-aware spatial attention aggregation. Although we explicitly disentangle WSVADL into two independent tasks, i.e., temporal anomaly detection and spatial anomaly detection, for the temporal anomaly detection task, we still require the critical spatial local anomalies as assistance information. This is because potential spatial anomalies can eliminate the noise effect caused by the irrelevant backgrounds, after all, most anomalies may occupy a small spatial region. For this problem, a majority of previous works completely ignore spatial anomaly information, and a tiny minority attempts to learn the interaction between spatial patches and video frames. The former works lacks the consideration of the use of individual spatial contents, while the latter works inevitably incurs excessive computational costs. Therefore, we propose a novel spatial attention aggregation (SA\u00b2) scheme to capture key spatial information with low computational costs. As we know, the whole frames consist of the background of the scene and foreground of action, and anomalous events often occur with foreground objects, thus focusing on the spatial foreground captures potentially anomalous events. The common methods for locating the foreground include object detection algorithms [46] or optical flow [8], but these require high computation costs. Here, we propose a considerably simple and efficient method named SA\u00b2, inspired by motion priors based works [59, 65]. Specifically, given the frame-level feature XCLIP and its corresponding spatial feature XPATCH \u2208 RT\u00d7H\u00d7W\u00d7D, where H and W are the height and width of the spatial feature, we argue that when most abnormal events occur, the corresponding location within spatial feature would change significantly [65]. Therefore, we compute the frame difference to obtain the motion magnitude:\nMo[i] = L2(XPATCH[i]\u00d72-XPATCH [i-1]-XPATCH [i+1]), (1)\nwhere the size of Mo is T\u00d7 H \u00d7 W, L2 is the L2 normalization applied in the channel dimension, and i denotes the i-th frame. Then we use the TopK mechanism to select a fixed number of patch-level feature xMo \u2208 RT\u00d7K\u00d7D with the highest motion magnitude MOTOP \u2208 RT\u00d7K\u00d71, where K < H \u00d7 W, and then compute the attention to obtain the aggregate spatial feature:\nAttention[i] = SoftMax (MOTO\u041e\u0420 [i]), (2)\nXAS[i] = Attention[i]xMo[i]. (3)\nDifferent from XCLIP in which all pixels in each frame have nearly equal influence for anomaly detection, XxAS places a heavy focus on potential anomaly locations. No matter how the spatial region of abnormal events changes, these two features, i.e., XCLIP and XAS, can extract key abnormal information from the local and global perspectives. In other words, they are complementary.\n3.2.2 Temporal CLIP adapter. As aforementioned, we adopt the pre-trained image encoder of CLIP to extract frame-level features, which contain momentary information but lacks a global temporal context critical for the VAD task. This motivates us to study temporal context modeling. We propose the temporal adapter that is similar to a vanilla multi-layer Transformer encoder, consisting of self-attention, layer normalization (LN), and feed-forward networks (FFN). Following [40], positional encoding is not applied. The main difference between temporal adapter and Transformer encoder lies"}, {"title": "3.2.3 Dual-branch prompt learning.", "content": "After obtaining the deeply processed features, we require the model to predict the frame-level anomaly confidence. Due to the proven performance of VadCLIP, we further adopt its dual-branch detection framework. The one branch is a classification branch (C-Branch), a simple linear layer with the number of neuron of one, which takes XTA as input, and generates the anomaly confidence A. Another branch is an alignment branch (A-Branch), which takes video features and textual embedding of labels as input, and yields the anomaly category probability M. To be specific, we create the image feature by adding the original CLIP feature XCLIP and the output of temporal adapter XTA together, combining the pre-trained knowledge from CLIP and newly learned contextual information. For the textual embedding of labels, we take inspiration from CoOp [80], we add a learnable prefix prompt embedding into the category embeddings, where the category embeddings are created by transforming original text categories, e.g, Fighting, Shooting, Car accident, into class tokens through CLIP Tokenizer, and then put them into text encoder of CLIP. Mathematically, we concatenate the category embedding for class i, tc,, with the learnable embedding {e1, ..., et} that consists of I context tokens to form a complete sentence token, and thus, the input of text encoder for one class is presented as {e1,..., el, tc; }. The overall label prompt embedding Prompt \u2208 R(1+C)\u00d7D_is the CLS token output of text encoder. With Prompt and XCLIP + XTA in hands, M is generated by,\nM =\n[XCLIP + XTA]Prompt\n||XCLIP + XTA||2||Prompt||2\n.(5)"}, {"title": "3.2.4 Objective function.", "content": "Following the setup of [71], TopK-based classification objective function is adopted for classification branch, which can be presented as follows,\n\u0440\u044c = Mean(\u0422\u043e\u0440K(A)), (6)\nLclass = -ybLog\u0440\u044c \u2013 (1 \u2013 \u0443\u044c)Log(1 \u2013 \u0440\u044c), (7)\nwhere TopK means select the set of k-max frame-level confidences in A for the video v. Lclass is the binary cross-entropy between pb and video-level binary labels yb.\nMIL-Align based objective function is used for alignment branch, which is based on anomaly category probability M. For each column of M, we select k-max similarities and compute the average to measure the alignment degree between v and the current class. Then we can obtain a vector S = {$1,..., $(1+C)} that represents the similarity between v and all classes. Then we compute the loss Lalign as follows,\nPci =\nexp (si/t)\n\u03a3jexp (sj/t)\u1fbf\n1+C\nLalign = -\n\u03a3Yc; Logpci\n(8)\nwhere pc; is the prediction of the i-th class, and t refers to the temperature hyper-parameter for scaling.\nTo learn discriminative prompt embeddings, we also introduce a contrastive loss to make all textual embeddings more dispersible. Specifically, we calculate cosine similarity between label prompt embeddings, and compute the contrastive loss Lconst as follows,\nLconst = -\n\u03a3\u03a3 max\n{0, \nPrompte Promptcj\n||Prompte ||2||Prompte,12\n}\n(9)\nThe final objective function is the weighted sum of the above three loss functions:\nL = Lclass + aLalign + BLconst. (10)"}, {"title": "3.3 LLM-Enabled Text Prompting for Spatial Anomaly Localization", "content": "The core of this operation is that how to locate anomaly regions. Thanks to the emerging paradigm of pre-trained VLMs, we take a step forward to training-free spatial anomaly localization. Inspired by CLIP-based industrial defect detection works [23, 81, 82], we regard the spatial anomaly localization as a spatial patch retrieval process given text queries. Specifically, we suppose a test video frame is deemed as an anomaly frame due to its high anomaly score. We then obtain its patch-level feature map xp \u2208 RH\u00d7W\u00d7D by the sliding window scheme, in which the patches are generated in the same way as XPATCH. Here the sliding windows scheme means that we first generate a set of image patches with a fixed-size window of PXP by sliding the window with a stride of S, i.e., a operation similar to convolutions, and then feed these image patches into the image encoder of CLIP to obtain the corresponding embedding of the CLS token. Notably, we do not adopt the natural dense representations, i.e., the penultimate feature maps in CLIP, though its generation is simpler than the sliding-window based scheme. This is because those features are not directly supervised with language in CLIP, and moreover, these patch features have already aggregated the global context due to self-attention, hindering the modeling of local region details for localization [23].\nAs for text queries, we generate several normal and abnormal descriptions. For the normal generation, the specifics are as follows, compared to industrial defect detection tasks, using textual labels to describe normal behavior under WSVAD task is more challenging. This is because videos in WSVAD task typically include multiple scenes, especially numerous real-world scenarios that are difficult to accurately summarize with textual labels directly. On the other hand, in terms of spatial fine-grained description, there may be semantic ambiguities between normal and abnormal behaviors due to the limited coverage range of spatial patches. Considering that most anomalies target intense human behavior, we believe it is more appropriate to use textual captions that describe the background of the image as normal descriptions. Therefore, we query LLMs about common indoor and outdoor items and selected 13 of the most common text descriptions as normal text descriptions. For example, \u201ca picture of sky, a picture of ground, a picture of road, a picture of grass, a picture of building, a picture of wall, a picture of tree, a picture of floor tile, a picture of desk, a picture of cabinet, a picture of chair, a picture of door, a picture of blank\u201d.\nFor abnormal descriptions, in addition to the original abnormal categories, we also use LLMs with a template \u201cProvide phrases similar to [abnormal category]\u201d to obtain augmented descriptions. For example, \u201c[abnormal category]\u201d can be set as \u201cpeople knockout someone\u201d for the category Fighting, \u201cpeople lying on the ground\u201d for Car accident, \u201csomeone ignite fire\u201d for Arson, \u201cpeople shooting someone\u201d for Shooting. The augmented prompts, along with the original textual categories, are used as final abnormal prompts for spatial anomaly localization.\nWith xp and text queries qr in hands, we perform a patch-level retrieval process, namely, using normal descriptions and abnormal descriptions to locate the background regions and potential abnormal regions, respectively. Mathematically, this process can be represented as,\nMs [i, j] =\n\u03a3\nr[r] Anomaly\nexp(xp[i, j]qt [r]/\u03c4)\nEk exp(xp[i, j]qt [k]/\u03c4\n]. (11)\nA spatial heat map of anomalous events Ms with size of H \u00d7 W is created, and it is resized to the size of original frames, and can generate the predicted bounding box by shape detection algorithm. Notably, we create two different scale feature maps for xp with P&S set to 32&32 and 80&48, and use a fusion hyper-parameter \u03bb to average their detection results as the final result."}, {"title": "4 Experiments", "content": "4.1 Datasets and Evaluation Metrics\n4.1.1 Datasets. We conduct extensive experiments on three popular WSVAD benchmarks in which the spatio-temporal anomaly annotations of test videos are provided. UCF-Crime is a large-scale benchmark for WSVAD task. It consists of 1900 long and untrimmed real-world surveillance videos, where the total duration is 128 hours, and the number of training videos and test videos is 1610 and 290, respectively. ShanghaiTech is a medium-scale dataset of 437 videos, including 130 abnormal videos on 13 scenes. This dataset is originally designed for semi-supervised video anomaly detection, and we follow Zhong et al. [78] and reorganize the dataset into 238 training videos and 199 test videos. UBnormal is a synthesized dataset. There are total 543 videos with 22 abnormal event types, in which 6 types are visible in the training set, and 12 types are visible in the test set. Following WSVAD settings, only video-level labels are available during the training stage.\n4.1.2 Evaluation metrics. For the temporal anomaly detection, we follow previous works [52], and utilize the area under the curve (AUC) of the frame-level receiver operating characteristics (ROC). The higher AUC indicates the better performance. For the spatial anomaly localization, following the previous work [33], we use TIoU (Temporal Intersection-over-Union) as the evaluation metric."}, {"title": "4.3 Comparison with State-of-the-art Methods", "content": "To ensure fairness in comparison, we re-implement most methods using the same CLIP features as ours, given that several works utilize different feature extractors.\n4.3.1 Temporal anomaly detection results. As listed in Tables 1 to 3, our method demonstrates superior performance on the UCF-Crime and UBnormal benchmarks, while also achieving competitive results on ShanghaiTech. Specifically, our method attains 88.08% AUC on UCF-Crime, outperforming other comparison counterparts without using CLIP features by a wide margin, and also excelling CLIP-based methods by a clear margin. Compared to the best competitor VadCLIP [71], although our STPrompt only utilizes the spatial attention aggregation instead of multi-crop augmentation, it easily achieves a performance improvement. Besides, our method obtains 97.81% AUC, a competitive result on ShanghaiTech dataset. On UBnormal dataset, our method achieves 63.98% AUC, achieving an absolute gain of 1.0% in terms of AUC over the best competitor OPVAD [70]. The above results demonstrate the compelling ability of our method for WSVAD task, which can outperform current competition counterparts on three commonly-used benchmarks.\n4.3.2 Spatial anomaly localization results. On the other hand, our method exhibits superior performance in spatial anomaly localization. Since there are few works exploring spatial anomaly localization, we modify a classical method [13] and an emerging method [71] as baselines, where the former is a variant of Sultani et al. [52], a simple patch-level detection, and the latter employs the learned label prompt embeddings to locate spatial anomalies. From Tables 1 to 3, we found that STPrompt achieves best performance in terms of TIoU on all the three benchmarks. For example, compared to fully-supervised method Liu et al. [33] and the baseline Sultani et al. [52], our STPrompt can achieve a substantial improvement of 7.5% and 7.2% TIoU on UCF-Crime. Besides, STPrompt also comprehensively outperforms VadCLIP, showing the advantages of purpose-built prompts created by LLMs with respect to vanilla learnable label prompts for the spatial anomaly localization."}, {"title": "4.4 Ablation Studies", "content": "4.4.1 Effectiveness of dual-branch structure. As the result shown in Table 4, we investigate the performance of dual-branch structure in various situations. It is evident that employing both the C-branch and A-branch leads to improved performance compared to using a single branch. After adding other modules, A-branch has sustainable advantages over C-branch, which indicates that A-branch possesses superior capabilities in temporal anomaly detection, Consequently, we use the results generated from A-branch (1 minus the similarity between the video and normal class) as the final frame-level anomaly score.\n4.4.2 Effectiveness of spatial aggregation attention. We employ the spatial aggregation attention not only for aggregating spatial features for temporal detection but also for assisting spatial anomaly location. According to the result in Table 4, using spatial features yields a notable improvement of 2.0% AUC on UCF-Crime and 1.0% AUC on UBnormal, respectively. This underscores the efficacy of spatial information in enhancing the ability of model to distinguish anomalies from normal events or background within the same frame. Consequently, abnormal regions are highlighted while redundant background is suppressed. Table 5 shows the effects of using different strategies to integrate spatial features. We observe that simply using the average features or attention-based weighted average features can slightly improve the performance on UCF-Crime, but leads to a performance drop on UBnormal. Using motion-based selection strategy can reduce redundant spatial features and achieves a improvement on both UCF-Crime and UBnormal. Our SA\u00b2 is the combination of motion-based selection and attention-based weighted average. Such a simple yet effective operation can contribute to clear improvements.\n4.4.3 Effectiveness of temporal adapter. Previous works have proven the effectiveness of temporal relation modeling on WSVAD task. As presented in Table 4, temporal adapter gets significant performance boosts both with or without SA2. Furthermore, we also perform ablation studies on how to learn temporal modeling. From Table 6, we found that the vanilla transformer that performs well when being trained on large-scale datasets with full-supervised supervision, however, is not suitable for WSVAD task. Besides, experimental results reveal that transformer based on fixed distance relation performs better than traditional self-attention transformer. This indicates that salient priors are required for WSVAD task with insufficient supervisory signals."}, {"title": "4.5 Computational Complexity", "content": "We conduct a detailed analysis of the parameter count and computational cost of our model, juxtaposing it with previous related works in Table 7. Upon reviewing the comparison results, we note that our method, particularly when compared to spatio-temporal modeling based VAD method SSRL [28], exhibits lighter weight and greater efficacy. It is worth highlighting that, despite the presence of shared parameters between different modules in SSRL denoted by the symbol *, the parameter count and computational cost of our model are notably lower. In general, the comparison results presented in Tables 1 to 3 underscore the accurate anomaly detection and localization capabilities of our method. Furthermore, these findings underscore the favorable balance between speed and accuracy achieved by our approach in spatio-temporal modeling."}, {"title": "4.6 Qualitative Analyses", "content": "4.6.1 Qualitative results of temporal anomaly detection. We illustrate qualitative results of temporal anomaly detection in Figure 3. The top row depicts the results of UCF-Crime, the first two samples in the bottom row present the results of ShanghaiTech, and the remaining samples show the results of UBnormal. It is clear that STPrompt can detect different types of anomalies on three public benchmarks, including human-centric anomalies, e.g., Fighting and Robbery, and scenario-centric anomalies such as Explosion, showcasing the effectiveness of STPrompt in anomaly detection.\n4.6.2 Qualitative results of spatial anomaly localization. We illustrate qualitative results of spatial anomaly localization in Figure 4."}, {"title": "5 Conclusion", "content": "In this work, we present STPrompt, a novel approach utilizing frozen vision-language models, for weakly supervised video anomaly detection and localization. To tackle this challenging task, we adopt a divide-and-conquer strategy by decomposing this task into two distinct sub-tasks: temporal anomaly detection and spatial anomaly localization. For the former task, we design a spatial attention aggregation strategy and temporal adapter to efficiently capture potential spatial anomaly information as well as contextual information, and then employ a dual-branch network to detect anomalies by binary classification and cross-modal alignment. For the latter task, we devise a training-free query-and-retrieve method based on the pre-trained concept knowledge from VLMs. Without bells and whistles, our STPrompt achieves state-of-the-art performance on three benchmarks in both temporal anomaly detection and spatial anomaly localization. In the future, how to further reduce the spatial false alarm rate and improve spatial localization accuracy is a problem worthy of long-standing research."}]}