{"title": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot", "authors": ["Ruixiang Jiang", "Changwen Chen"], "abstract": "We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability shall be elicited to evaluate the aesthetics of artworks. To facilitate this investigation, we construct MM-StyleBench, a novel high-quality dataset for benchmarking artistic stylization. We then develop a principled method for human preference modeling and perform a systematic correlation analysis between MLLMs' responses and human preference. Our experiments reveal an inherent hallucination issue of MLLMs in art evaluation, associated with response subjectivity. ArtCoT is proposed, demonstrating that art-specific task decomposition and the use of concrete language boost MLLMs' reasoning ability for aesthetics. Our findings offer valuable insights into MLLMs for art and can benefit a wide range of downstream applications, such as style transfer and artistic image generation. Code available at https://github.com/songrise/MLLM4Art.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancement of computer vision (CV) algorithms, particularly in style transfer [1]\u2013[3] and image generation models [4], [5], have democratized artistic creation, allowing users to personalize artworks with minimal expertise. However, the development of quantitative and interpretable evaluation protocols on artwork aesthetics has not kept pace. Without human-aligned evaluation metrics, it is challenging to systematically refine and enhance these generation methods, thereby limiting their potential to produce truly aesthetically pleasing artworks.\nUnlike most CV tasks, aesthetics evaluation is not well-defined with pure vision features alone. It requires higher-level assessments that consider factors such as cultural background, emotional impact, and storytelling. However, in existing aesthetic evaluation protocol, vision-feature-based metrics (e.g., Style loss [6], Aesthetic Predictor [7], and Art Score [8]) dominate the landscape. These metrics significantly undermine the complexity of aesthetics and are proven to be misaligned with human preference [8]\u2013[10]. Determining how to evaluate aesthetic quality holistically thus remains an open question.\nWe draw inspiration from the \u201cFormal Analysis\" [11], [12], a practice frequently adopted by art critics for formal aesthetics assessment. It involves using concise language to describe visual elements in artwork, relating them to domain-specific knowledge such as history, cultural background, and aesthetic principles, and collectively arguing about the artwork's aesthetic quality. This technique effectively translates the underlying reasoning processes of human aesthetic perception into explicit language, facilitating more objective assessments. Intriguingly, this approach parallels the multimodal inference- time reasoning of MLLMs [13]\u2013[15], thereby presenting a compelling research question:\nCan MLLMs reason about the aesthetic quality of artistic images in a manner aligned with human preferences?\nTo explore this question, we focus on the evaluation of artistic stylization and introduce MM-StyleBench, a large- scale and densely annotated dataset with diverse content and style instances. To quantify aesthetics alignment, we model rank-based human aesthetic preferences on MM-StyleBench in a principled manner and perform a correlation study with responses from MLLMs. By prompting various MLLMs with different strategies, we identify a key challenge in aligning their outputs with human preferences: the tendency of MLLMs to produce hallucinations through the frequent use of subjective language associated with art. To address this issue, we propose ArtCoT, a prompting method with explicit decomposition of art evaluation to reduce hallucination and enhance the reasoning capability of MLLMs.\nWe implement ArtCoT across three different MLLMs, measure the models' alignment in two preference ranking settings, and observe a significant and consistent gain in aesthetic alignment. Furthermore, our empirical analysis reveals that art-specific task decomposition with clearly defined sub-tasks facilitates MLLMs' reasoning abilities, leading to a more objective thinking trace with reduced hallucination. Overall, our findings offer valuable insights into how MLLMs should be applied in art evaluation tasks. In a broader sense, the feedback signals provided aesthetics-aligned MLLMs can benefit downstream applications such as reinforcement-learning- enhanced stylization or image generation [16]. We summarize our contributions as follows:\n1) We introduce MM-StyleBench, the first large-scale dataset for multimodal stylization with dense annotations.\n2) We propose a principled approach for modeling human aesthetic preference.\n3) Our analysis provides valuable insights into reducing hallucinations in MLLMs during art evaluation.\n4) We introduce ArtCoT that significantly enhances the aesthetic alignment of MLLMs for artistic evaluation."}, {"title": "II. RELATED WORKS", "content": "Aesthetic Evaluation. Early aesthetic evaluation algorithms are mainly built on heuristics [17] or established aesthetic principles [18]. Recent learning-based methods [7], [19] regress human preference scores from large-scale image collections. Metrics like ArtFID [20] and Art Score [8] approach this problem by targeting image stylization tasks, where they finetune image encoders for aesthetic prediction. These methods measure aesthetics as feature-space distance, which is insufficient for aesthetics perception that requires multimodal reasoning [9]\u2013[11]. Recent methods [21]\u2013[23] attempt to incorporate the strong inference capability of MLLMs to facilitate image aesthetic evaluation. Nevertheless, they usually target at general image aesthetics, which has different criteria than artwork evaluations.\nArtistic Stylization. Neural stylization methods aim to map the style of content images to other styles. Early approaches [1], [24]\u2013[26] employ pretrained image encoders to extract and merge vision features from content and reference style images. However, these methods are often limited in style fidelity and are prone to filtering artifacts, resulting in low aesthetic quality. Recent approaches [2], [3], [27] take aesthetics into model design considerations to enhance the style fidelity. Benefiting from multimodal embedding techniques, text-driven stylization methods emerge [3], [28], [29], where the style is specified through text prompts. As stylization methods evolve, evaluating these methods presents challenges, which can hinder the method's advancement. Our work offers a timely solution for human-aligned stylization assessment."}, {"title": "III. MM-STYLEBENCH", "content": "To construct a validation set for model comparison, previous stylization methods have relied on ad-hoc sampling from established datasets such as MS-COCO [30] and WikiART [31]. These samples are often limited in size and diversity, making performance measurements sensitive to instance selection and thereby challenging reproducibility and fairness [9], [10]. Additionally, the limited annotation granularity impedes a comprehensive understanding of the methods' strengths and weaknesses.\nTo circumvent these challenges, we construct a large-scale benchmarking dataset by harvesting from diverse sources (detailed in Appendix. A). The resulting dataset, MM-StyleBench, contains multi-modal content and style with dense attribute annotations, enabling unified stylization comparisons. Compared with existing benchmarks and evaluation protocols, MM-StyleBench offers strength in three aspects: (1) Scale it offers 1k content and styles, respectively, which is two orders of magnitude larger than existing datasets. (2) Quality MM-StyleBench contains multi-modal (image and text) and fine-grained attribute annotations, facilitating comprehensive model comparisons, and (3) Diversity Our dataset is built from diverse sources (SA-1B [32], MSCOCO [30], WikiART [31], and DiffusionDB [33]), covering a wide range of domains to eliminate potential bias. We also utilized LLMs to create variation, further enhancing the diversity."}, {"title": "IV. MODELING HUMAN PREFERENCE", "content": "Objective. In this step, we aim to derive a statistically robust global ranking of artists (stylization models) or artworks (stylized images) from human feedback. Formally, let C, S, M denote the sets of content, style, and stylization models, respectively. For each instance (c, s) \u2208 C \u00d7 S, the stylization result yi = mi(c,s), for mi \u2208 M, constitutes the candidate outcomes Yc,s = {Y1,..., Yk}. Human preference is represented as a ranking:\n$\\Upsilon_{\\pi(1)} \\succ \\Upsilon_{\\pi(2)} \\succ \\cdot\\cdot\\cdot \\succ \\Upsilon_{\\pi(k)}$,\nwhere \u03c0(i) maps the candidate index to its global rank, and \u27a4 means preferred over.\nPreference Modeling. To model human aesthetics preference, previous methods [3], [7], [8] employ Likert scale or similar ordinal-valued psychometrics in preference collection. However, these metrics are suboptimal for rank-based modeling due to issues like central tendency [37]. Recent psychological study [9] further suggests multi-scale metrics can impair human perception, especially for artistic evaluation.\nIn response to this challenge, we design two-alternative forced choice (2AFC) task to collect preferences. Specifically, we present users with the content, style and two candidate stylization results. Users are required to choose the preferred one without the option of tie and skip. The probability of preferring yj over yi is modeled as:\nP(Yi \\succ Yj |c, s) = \\frac{Qi}{Qi + Qj}\nwhere Qi represents the latent competence of yi, conditioned on the specific instance c, s.\nSampling Strategies for 2AFC. Given the combinatorial complexity of C \u00d7 S \u00d7 M, exhaustive pairwise comparisons are infeasible. Therefore, we generate 2AFC questions by sampling. For a specific combination Yes, all possible comparisons can be modeled as a complete undirected graph G(V, E), where the node V is the candidate results and E stand for pair-wise comparison. We consider two strategies for sampling E' \u2286 E.\n1) Global sampling: Uniformly sample arbitrary number of edges without replacement. This approach is suitable for covering a wider range of content and style within a fixed total budget, facilitating model-level ranking.\n2) Per-instance sampling: Sample edges |E' \u2208 [|V| \u2013 1,|E|] such that the sub-graph is connective with maximum node degree uniformness, meaning that each candidates shall be compared with for similar times. This is suitable for deriving instance-level ranking.\nFor the per-instance sampling, we solve it by designing a greedy algorithm (described in Appendix. C) that initialize from a minimum spanning tree using Kruskal's algorithm [38] and iteratively add edges with lowest node degree imbalance.\nDealing with Subjectivity in Human Preference. We deal with two special cases of subjectivity to enhance the consistency of preference annotation in per-instance setup:\n1) Pair disagreement: We exclude pairwise comparisons where preferences tie (P(yi \u27a2 yj|c, s) \u2248 50%).\n2) Non-transitive relationship: To deal with cyclic preference, such as (a > b, b > c, c > a), we apply feedback arc set (FAS) algorithm [39] to detect feedback arcs E'f and drop the instance when |E'f||/|E'| \u2265 \u03b7.\nGlobal rank derivation. We utilize Bradley-Terry (B-T) model [40] and Elo [41] algorithm to derive global rankings. In the B-T model, Qi = exp(0\u2081), where the latent parameter {0;} is optimized. The Elo algorithm utilize Qi = 10^{Ri/400}, where Ri is the rating of candidate-i that is updated on-line."}, {"title": "V. ZERO-SHOT REASONING ABOUT AESTHETIC QUALITY", "content": "We leverage CoT prompting [42] to elicit the reasoning ability of MLLMs during inference time. Our key finding is that explicit decomposition of art evaluation tasks and utilization of concrete language collectively reduce hallucination, as detailed in Sec. VII. Specifically, we propose ArtCoT, which encourages MLLMs to concretely describe visual features and reason the aesthetic quality, akin to the formal analysis in art. The ArtCoT takes the same input as other prompting techniques, but involves three specialized MLLM inferences, each tailored to an art-specific role.\nIn the initial stage, the MLLM evaluates content preservation and style fidelity by providing a detailed description of the visual features of stylized images and references. This involves identifying visual elements in images at different levels, such as color schemes, strokes, structural components, and composition. The difference with the existing feature- based aesthetic metrics [7], [8] is that we explicitly map vision features into semantics (as tokens) and link with domain knowledge to facilitate further reasoning. The response from this stage is concatenated with the original input and forwarded to the next stage.\nThe second stage, termed as the art critic phase, prompts the MLLM to re-evaluate the combined input and connect with domain knowledge, akin to the formal analysis in art. Here, the MLLM shall critically examine the previously described visual features against artistic principles and domain-specific background information such as the cultural background. Notice that instead of determining a winner, this stage aims to produce a detailed thinking trace to facilitate the final decision. In the final stage, the MLLM summarizes the information from the first two stages and determines the winner in this 2AFC question. The overall workflow is visualized in Fig. 2- (c)."}, {"title": "VI. EXPERIMENTS", "content": "A. Experiment Setup\nStylization models. We inspect a total of 10 text-driven and image-driven with default configurations, including AdaIn [24], ArtFlow [26], ControlNet [43], DDIM [4], DiffArtist [3], DiffStyler [2], InstantStyle [28], Instruct-pix2pix [29], StyleID [34], and Sty-Tr2 [1].\nPreference collection. 12 human experts with general knowledge of art are recruited for preference annotation. We collected a total of 21k responses. For per-instance sampling, we sample O(klog(k)) for each content/style combination. We prune uncertain preferecne with P(yi \u27a2 yj|c,s) \u2208 [0.4, 0.6]. Highly non-transitive instances with \u03b7 > 0.15 are removed. As a result, 24.8% of the feedbacks are filtered out due to pairwise divergence, while the remaining 18.6% are removed due to a high non-transitive preference.\nAlignment Metrics. Following [3], [8], [21], we use the spearman's rank correlation [44] to quantify the alignment between human preference and responses from MLLMs. A spearman's p closer to 1 indicates a stronger positive linear correlation of ranking. We calculate p on the rank of methods for global sampling scheme, while for per-instance sampling, we rank each instance and report averaged p and combined p value using Fisher's method [45]. A lower p value means stronger statistical significance.\nB. Main Result\nWe inspect three main-stream MLLMs, including GPT-4 [13], Gemini 1.5 [14], and Claude 3.5 [15] with different prompting methods. We also compare the result of random guess and the linear aesthetic predictor trained on LAION [7]. We compare the alignment as spearman's p for the two sampling strategies and report results in Tab. II.\nThe experimental results demonstrate the advantage of applying ArtCoT for aesthetic alignment, achieving an average improvement of 56% in the per-method setup and 29% in the"}, {"title": "VII. ANALYSIS AND DISCUSSION", "content": "A. Zero-shot CoT reinforces hallucination in art evaluation\nIn multimodal art evaluation, zero-shot CoT may adversely impact the reasoning capability of MLLMs. Our empirical findings suggest that this problem is rooted in the hallucination issue, where MLLMs tend to \"feel\" instead of to \"reason\".\nEmpirically, in the zero-shot CoT, MLLMs tend to adopt a subjective and less concrete language for art evaluation. Sensational (e.g., \"feels\" and \"sense\") and hedge words (e.g., \"appears\" and \"somewhat\") are frequently picked to justify their preference, which is more frequently misaligned with human preference. Moreover, the MLLMs may hallucinate \"artistic interpretations\" that deviate from the actual visual features present in the stylized images.\nB. ArtCoT elicit the ability of formal analysis for art evalua- tion\nCompared with base prompting and zero-shot CoT, the responses generated by ArtCoT are more concise, objective, and consistent with the input images. To quantify this improvement, we calculate the frequency of subjective words in the lemmatized responses from the MLLMs and perform lexicon- based subjectivity analysis using the TextBlob package. The result demonstrates the reduced subjectivity of MLLM prompted with ArtCoT. This reduction can be attributed to task decomposition, allowing the MLLMs to think in steps like an art critic.\nSpecifically, (1) the content/style analyzer is prompted to describe and assess the level of content preservation and style fidelity. Unlike direct artistic evaluation, these subtasks are more close-ended and relate closer to visual attributes, encouraging the use of concrete words. (2) The art critic phase enables MLLMs to re-evaluate the candidate images. The key difference from zero-shot CoT is that this critic stage can break potential self-reinforcing hallucinations, particularly when the previous responses contain factual errors.\nC. ArtCoT boost aesthetic alignment in all circumstances.\nLeveraging the fine-grained annotations provided in MM- StyleBench, we conduct a comprehensive comparison of various prompting methods. For instance, in Fig. 3, we visualize the alignment performance on different content complexity and style prompt categories.\nThe proposed ArtCoT outperforms both base and zero- shot CoT prompting in all situations. The most notable improvements are observed in stylization tasks that involve concrete instructions. This is evidenced by the significant improvement of \"long prompt\" and prompt specifying particular \"art movement\". We postulate that long and specific instructions set a more concrete objective for both stylization and evaluation. This facilitates the MLLM to more objectively describe the expected visual features in target style and link with multidisciplinary background information. Consequently, the MLLM is less likely to hallucinate."}, {"title": "D. Ablations.", "content": "Components of ArtCoT. We ablate the key components of ArtCoT, specifically the content/style analyzer and the art critic phase, and report the results in Tab. IV. The complete ArtCoT prompt achieves the highest aesthetic alignment. Particularly, removing the art critic phase induces the most significant decline in performance, underscoring its critical role.\nInput to MLLM. We study how different input modalities and image resolutions impact aesthetic alignment and report the result in Tab. V. We set the resolution to be 1/2 of the original resolution by default due to a balance in performance and the number of tokens. For input modalities, providing style information is important in per-method alignment, while including the reference image affects per-instance alignment most. For either setting, providing the MLLMs with all input modalities achieves the best alignment, meaning that both"}, {"title": "VIII. CONCLUSION", "content": "In this work, we present the first comprehensive analysis of how MLLMs' reasoning ability should be evoked for aesthetic evaluation of artworks. Utilizing the newly developed MM- StyleBench dataset, our extensive study reveals the significant hallucination issue within MLLMs. We demonstrate that decomposing art evaluation into specific, art-focused sub-tasks and employing concrete, precise language in prompts significantly enhance the models' ability to reason about aesthetic quality. A wide range of downstream applications could benefit from our insights, including but not limited to stylization evaluation, image generation, and reinforcement learning from AI feedback."}, {"title": "APPENDIX", "content": "A. Detail on MM-StyleBench\nWe construct MM-StyleBench by harvesting from existing open-source datasets with the help of MLLMs.\nFor the Content category, 50% of the content images are generated using the Ideogram-v1 text-to-image (T2I) diffusion model with diverse prompts produced by GPT-4. The remaining 50% are randomly sampled from the SA-1B [32] and MS-COCO [30] datasets, with captions generated by Gemini-v1.5 pro [14]. Images from SA-1B are downsampled by a factor of two, while those from MS-COCO retain their original resolution. Overall, all images have an average height and width of 895.7 and 811.9 pixels, respectively. The 5th percentiles for height and width are 480 and 427 pixels, respectively, and the 95th percentiles for both dimensions are 1248 pixels.\nTo synthesize fine-grained attribute annotations for the content images and their associated prompts, we first employ Gemini v1.5-pro to automate the annotation process based on a predefined attribute set. Each annotation is manually reviewed to ensure accuracy and correctness. The Style subset of MM-StyleBench is derived from WikiArt [31] and DiffusionDB [33]. The process involves two main steps:\n1) WikiArt Processing: We extract keywords from WikiArt, focusing on specifications such as art movement, artist, and genre. These base keywords are then expanded and combined using GPT-4 [13] to create a diverse set of style descriptors.\n2) DiffusionDB Processing: Given the high noise level in DiffusionDB annotations, we utilize an MLLM to preprocess the text prompts within the dataset. Specifically, we extract the style descriptions from each prompt and merge those with similar style specifications. Subsequently, we subsample from these processed style prompts to ensure quality and diversity.\nFor both datasets, style reference images are generated using StableDiffusion [5], with all reference images standardized to a resolution of 512\u00d7512 pixels. Similar to content annotations, we employ MLLMs to annotate the attributes of style prompts.\nThe actual number of sampled content and style images is summarized in Table VI. We plan to open-source MM- StyleBench to facilitate reproducibility and support future research endeavors.\nB. Stylization\nWe present the stylization result generated by different methods on two content and style combination in Fig. 5."}]}