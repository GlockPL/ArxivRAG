{"title": "Robust Robot Walker: Learning Agile Locomotion over Tiny Traps", "authors": ["Shaoting Zhu", "Runhan Huang", "Linzhan Mou", "Hang Zhao"], "abstract": "Quadruped robots must exhibit robust walking capabilities in practical applications. In this work, we propose a novel approach that enables quadruped robots to pass various small obstacles, or \"tiny traps\". Existing methods often rely on exteroceptive sensors, which can be unreliable for detecting such tiny traps. To overcome this limitation, our approach focuses solely on proprioceptive inputs. We introduce a two-stage training framework incorporating a contact encoder and a classification head to learn implicit representations of different traps. Additionally, we design a set of tailored reward functions to improve both the stability of training and the ease of deployment for goal-tracking tasks. To benefit further research, we design a new benchmark for tiny trap task. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness and robustness of our method.", "sections": [{"title": "I. INTRODUCTION", "content": "Humans and animals have the ability to walk robustly in complex environments, relying on proprioception to avoid various obstacles such as strings, poles, and ground pits. However, these same challenges present significant difficulties for robots. In real-world scenarios, seemingly minor traps can severely impact a robot's mobility. Many of these obstacles are tiny or positioned below or behind the robot, making them difficult to detect with external sensory devices like depth cameras, as shown in Fig. 2. Small objects like narrow bars or poles often produce unreliable data in depth images, appearing intermittently noisy or as a dense patch at zero distance, making them indistinguishable from the edge noise of other obstacles. Additionally, since realistic RGB images cannot be accurately rendered in simulations, using them in real-world applications is limited due to a significant sim-to-real gap. This arises the need for developing control policies that enable robots to overcome such trap-type obstacles without relying on additional sensory equipment.\nLearning agile locomotion over tiny traps presents several challenges. First, frameworks [1], [2] that rely on exteroceptive inputs are ineffective for tasks involving tiny traps, as both RGB and depth images are difficult to leverage in these scenarios. Second, with incomplete perceptual information, it is difficult to learn a blind walking policy directly from scratch. Some privileged information is required to guide the training. Lastly, while some goal-tracking frameworks [3], [4] have addressed the above issues, they often lack omnidirectional movement capabilities or rely heavily on external localization techniques. Moreover, these frameworks frequently employ sparse rewards, which cause the instability of training and complicate real-world deployment.\nTo address the challenges of learning agile locomotion over tiny traps using proprioception, we propose a novel solution with several key contributions.\n\u2022 First, we introduce a two-stage training framework that relies solely on proprioception, enabling a robust policy that successfully passes tiny traps in both simulation and real-world environments.\n\u2022 Second, we develop an explicit-implicit dual-state estimation paradigm, utilizing a contact encoder to estimate contact forces on different robot links and a classification head to enhance the learning of contact representations.\n\u2022 Third, we redefine the task as goal tracking, rather than velocity tracking, and incorporate carefully designed dense reward functions and fake goal commands. This approach achieves approximate omnidirectional movement without motion capture or additional localization techniques in real-world, significantly improving training stability and adaptability across environments."}, {"title": "II. RELATED WORKS", "content": "A. Learning-Based Locomotion\nThe deep reinforcement learning (DRL) paradigm has demonstrated its ability to learn legged locomotion behaviors in simulators [5]. Unlike model-based control methods [6], DRL enables robots to handle corner cases in simulation through an end-to-end learning process, resulting in robust, transferable policies. These simulation-trained policies allow robots not only to walk on flat terrain [7], but also to achieve high-speed running [8], [9], traverse muddy or uneven terrains [10], stand on rear legs [11], [12], open doors [11], climb stairs [13], [14], scale rocky terrain [4], [15], and even perform high-speed parkour [2], [3], [16]\u2013[18].\nB. Collision Detection\nCollisions in robots can be classified into several stages: the pre-collision phase [19]\u2013[21], collision detection phase [19], [22], [23], collision isolation phase [19], [24], collision identification phase [19], [24], [25], collision classification phase [26], collision reaction phase [27]-[29], and post-collision phase [26], [29]. Collision detection methodologies for quadruped robots are generally divided into model-based and model-free approaches. Model-based methods typically employ state estimation techniques [30]\u2013[32]. Some approaches leverage exteroceptive sensors [33], while others rely purely on proprioception for estimation [34]. Model-free methods, on the other hand, involve training neural network-based contact estimators through deep reinforcement learning, which can be either implicit [35] or explicit [15]."}, {"title": "III. METHOD", "content": "A. Task Definition\nThe task is defined as passing through a set of tiny traps, denoted as T. An onboard camera typically fails to detect such traps. These traps are categorized into three main types: Bar, Pit, and Pole, as illustrated in Fig. 4. A Bar refers to a horizontal thin bar positioned over a plane at a height below the quadruped robot's head. A Pit is a small depression between two planes that can cause the robot's legs to slip, and it is not visible from the normal front view. A Pole is a thin, upright pole standing on a plane. When encountering these traps, the robot is provided with constant control commands and must adjust its speed autonomously to pass them. It's important to note that in training and demonstrations, the traps include both thin and thick Bars/Poles. While the onboard camera can detect thicker ones, they are included only to showcase the generalization capabilities of our control policy.\nB. Reinforcement Learning Setting\nWe decompose the locomotion control problem into discrete locomotion dynamics, with a discrete time step $d_t = 0.02s$. We use the Proximal Policy Optimization algorithm (PPO) [36] to optimize our policy. Inspired by [3], [4], we formulate the problem as goal tracking instead of velocity tracking.\nState Space: The entire process includes the following four types of observation: proprioception $p_t$, privileged state $\\hat{s}_t$, contact force $c_t$, and goal command $g_t$. 1) Proprioception $p_t$ contains gravity vector and base angular velocity from IMU, joint positions, joint velocities, and last action. 2) Privileged state $\\hat{s}_t$ contains base linear velocity (unreliable from IMU) and the ground friction. 3) Contact force $c_t$ includes contact force with environment meshes of each joint link, refer to the robot link in different colors in Fig. 3. Each contact force is clipped, and normalized to [-1,1]. 4) Goal command $g_t$ includes goal position $\\Delta G = (\\Delta x, \\Delta y, \\Delta z)$ relative to the current robot frame, and the time remaining to complete $\\Delta t$. At the beginning of one episode, we randomly sample a goal position fixed in the world frame and set $\\Delta t$ to the episode length. In every time step, we calculate $\\Delta G$ based on the robot pose, and update $\\Delta t$ to the last time of the current episode. Since our task does not involve height change, $\\Delta z$ is always set to zero. To facilitate the training of a standstill state, we set $\\Delta G = (0,0,0)$ when $||\\Delta G||_2 < 0.2$.\nAction Space: The action space $a_t \\in \\mathbb{R}^{12}$ is the desired joint positions of 12 joints.\nC. Reward Function\nThe reward function has three components: task reward $r^\\tau$, regularization reward $r^r$, and style reward $r^s$. The total reward is the sum of three items: $r_t = r^\\tau + r^r + r^s$.\n1) Task reward. $r_{goal}$, $r_{heading}$, and $r_{finish}$ are the main components. Unlike previous works, we define the $r_{goal}$ as a dense reward throughout the entire episode. In our setup, the robot can only perceive tiny traps upon contact, meaning it cannot predict and avoid them from a distance against the reward function. This eliminates the need for free movement exploration as seen in [4], and significantly enhances the stability of the training process.\n$r_{goal} = \\frac{1}{0.4 + ||\\Delta G||_2},$ (1)\nWe aim for the robot to always move toward the goal. Without the heading reward, it may encounter traps horizontally or backward, which is equivalent to multi-task learning, and greatly increases the difficulty of training. Although heading reward allows the robot to only pass through the trap in a straight or with a small angle, it greatly speeds up convergence and improves the passing success rate. $\\epsilon = 10^{-6}$.\n$r_{heading} = \\begin{cases} \\frac{\\Delta x + \\epsilon}{| | \\Delta G ||_2 + \\epsilon}, & | | \\Delta G ||_2 \\neq 0 \\\\ 1, & | | \\Delta G ||_2 = 0 \\end{cases}$ (2)\nBesides, the policy should be able to operate stably for a long time on the real robot. Therefore, we introduce a finish reward to encourage the robot to stand still when near the goal. When $\\Delta G = (0,0,0)$:\n$r_{finish\\_pos} = \\sum_{i=1}^{12} | q_i - q_{default} |,$ (3)\n$r_{finish\\_vel} = ||V|| + ||\\Omega||,$ (4)\n$r_{finish} = \\lambda_1 \\cdot r_{finish\\_pos} + \\lambda_2 \\cdot r_{finish\\_vel}.$ (5)\n2) Regularization reward. Regularization reward is designed to make the robot move smoothly, safely, and naturally. A key component of this is the velocity limit reward. Unlike previous work, our goal is not for the robot to reach a limited speed, which is unsafe in a real deployment.\n$r_{vel\\_limit} = (\\omega_z < \\omega_{limit}) \\cdot (||V_{x,y}||_2 < V_{limit})$ (6)\nOther regularization rewards include stall, leg energy, dof vel, etc. The details can be found in Appendix A.\n3) Style reward. We use the Adversarial Motion Priors (AMP) style reward to gain a natural gait and speed up convergence following [13], [37].\nD. Training and Deployment\n1) Training. To effectively learn the privileged state and improve the performance, we employ a two-stage Probability Annealing Selection (PAS) framework [38] instead of the traditional teacher-student imitation learning approach, as illustrated in Fig. 3. Further details are in Appendix B.\nIn the first step of training, the policy can access all the information $[p_t, \\hat{s}_t, c_t, g_t]$ as observation. In addition, we use explicit-implicit dual-state learning similar to [18]. The contact force $\\hat{c}_t$ is first encoded by a contact encoder to an implicit latent, and concatenated with explicit privileged state $\\hat{s}$, to the dual-state $\\hat{I}_t = [Enc(\\hat{c}_t), \\hat{s}_t]$. In addition, we introduce a classification head to guide the policy in learning the connection between the contact force distribution and the trap category. In fact, the category of trap can be seen as a very strong explicit privileged state. We use cross-entropy loss as the classification loss. Previous work [15] uses boolean values as the collision state for prediction. We find this will cause the sim-to-real problem, referring to the experiment Sec. IV-D. Besides, we pre-train the estimator module in the first step using L2 loss to reconstruct $\\hat{I}_t$. In summary, the total optimization target is:\n$L_{surrogate} + L_{value} + L_{recon} + L_{classify} + L_{discriminator}.$ (7)\nIn the second step of training, the policy can only access $[p_t, g_t]$ as observation. We initialize the weight of the estimator and the low-level RNN copied from the first training step. Probability Annealing Selection is used to gradually adapt policies to inaccurate estimates while reducing the degradation of the Oracle policy performance.\n$\\hat{I}_t = Estimator(p_t, g_t),$ (8)\n$\\bar{I}_t = ProbabilitySelection(P_t, \\hat{I}_t, I_t),$ (9)\n$a_t = Actor_{Elow}(\\bar{I}_t, p_t, g_t),$ (10)\nProbability $P_t = q^{iteration}.$ (11)\nTo enhance training stability, we increase the batch size by using 4,096 parallel robots, each performing 50 steps. Moreover, the episode length is reduced from 20 seconds to 8 seconds. More details of dynamic randomization and trap terrain curriculum are in the Appendix. C and Appendix D.\n2) Deployment. Unlike previous works, our policy achieves approximate omnidirectional movement by teleoperation without motion capture or other auxiliary localization techniques."}, {"title": "IV. EXPERIMENTS", "content": "A. Experiment Setup\nWe use the IsaacGym [5] for policy training and deploy 4,096 quadruped robot agents on a single NVIDIA RTX 3090. We first train 12,000 iterations on plane terrain, then train 30,000 iterations for both stages on trap terrain. The control policy within both the simulator and the real world operates at a frequency of 50 Hz. We deploy our policy on the Unitree A1 quadruped robot which has an NVIDIA Jetson Xavier NX as the onboard computer. In the real-world deployment, the robot receives the goal command from the teleoperator through ROS message and runs the low-level control policy to predict desired joint positions for PD control ($K_p = 40, K_d = 0.5$).\nB. Tiny Trap Benchmark\nWe design a new Tiny Trap Benchmark in simulation. As shown in Fig. 5, the benchmark consists of a 5m\u00d760m runway with three types of traps evenly distributed along the path. The traps include 10 bars with heights ranging from 0.05m to 0.2m, 50 randomly placed poles, and 10 pits with widths ranging from 0.05m to 0.2m. For each experiment, 1,000 robots are deployed, starting from the left side of the runway and passing through all the traps to reach the right side. We refer to this as the \u201cMix\u201d benchmark. Additionally, there are separate \"Bar,\" \"Pit,\" and \"Pole\" benchmarks, each focusing on one specific type of trap, but with triple the number of traps.\nThe benchmark uses three metrics: Success Rate, Average Pass Time, and Average Travel Distance. A robot is considered successful if it reaches within 0.2m of the target point within 300 seconds, at which point we record its pass time. Failure cases include falling off the runway, getting stuck, or rolling over. For failed cases, the pass time is set to a maximum of 300 seconds. We then calculate the overall success rate and average pass time. At the end of the evaluation, we average the lateral travel distance of all robots. During the evaluation, we use the same fake goal commands as in real-world deployment to guide the robot to stay in the center.\nC. Simulation Experiments\nWe conducted experiments with the following methods:\n\u2022 Different combinations of joint links. (Prop: Only trained with proprioception).\n\u2022 Ours w/o goal command: use traditional velocity command, but only train moving forward towards the trap.\n\u2022 Ours w/ Boolean: without encoder, boolean-value collision states are directly feed into low-level RNN, partly like [15]. Other settings are same as our method.\n\u2022 RMA [10]: A 1D-CNN serves as an asynchronous adaptation module in the teacher-student training framework.\n\u2022 MoB [39]: \"Learning a single policy that encodes a structured family of locomotion strategies that solve training tasks in different ways.\"\n\u2022 HIMLoco [40]: \"HIM only explicitly estimates velocity and implicitly simulates the system response as an implicit latent embedding by constrastive learning.\"\nFor RMA and HIMLoco, they are designed to deal with common terrains like stairs or uneven planes. We retrain the policy in our trap terrain, but the rewards and training settings have not changed (e.g. randomly sample velocity). This may cause the relatively poor performance of these two methods.\nWe report the results in Table. I and Table. II. In comparison experiments, our method outperforms others in all metrics. In ablation experiments, policy that utilizes all joint links performs best on \u201cMix\u201d and perform relatively well on other benchmarks. This proves contact force of every joint is helpful in improving the policy performance.\nD. Real-world Experiments\nWe deploy the policy on real robots and conduct real-world experiments. The robot achieves zero-shot omnidirectional movement using the teleoperation strategy from Sec. III-D. The visualized results are shown in Fig. 7. 1) Bar: The robot learns to step back its front legs after contacting the bar. Even when the hind legs are intentionally trapped, the robot can still detect the collision and lift its hind legs across the bar. In addition, the bar is somewhat elastic unlike simulations, which also proves the generalization ability of our policy. 2) Pit: The robot learns to support its body with the other three legs when one leg steps into a void, lifting the dangling leg out of the pit. Additionally, our robot has learned to forcefully kick its legs to climb out when multiple legs are stuck in the pit. 3) Pole: The robot learns to sidestep to the left or right after colliding with a pole, avoiding the pole by a certain distance before moving forward.\nIn addition, we deploy and evaluate some other policies for quantitative comparison. For each test, we repeatedly conduct 20 trials and calculate the success rate. The results are in Fig. 6. It shows our method obtains the best performance compared to other baselines. The method with velocity command is not much worse. This is because in the training we only sample forward velocity command instead of traditional omnidirectional velocity, which greatly increases the performance.\nWe also show that when a boolean value collision state is directly sent to low-level Actor RNN, the robot will face severe sim-to-real gap in deployment. There are ineliminable gaps between sim and real such as friction and motor damping. When the gap occurs, the latent space will have some noise. The latent space of boolean value state is very sparse, as shown in Fig. 11, this will cause policy to misjudge the current state more easily and operate irregularly. As shown in Fig. 9, when the robot moves forward on the plane and suddenly stops for a short time, the gap occurs, and estimated collision state rises to a large number. If we move backward the robot at that time, the robot will assume it has hit a trap. Since there is no trap practically, the robot may touch the ground or even fall down. After introducing contact forces and the contact encoder, the sim-to-real gap is mitigated. The robot can better identify traps and operate more stably.\nE. Additional Experiments\nWe conduct additional experiments to further illustrate our method. All experiments in this section are done in simulation.\n1) Different $\\Delta t$ of the fake goal command. We tested different $\\Delta t$ values for the fake goal command when passing one 0.2m high bar in Fig. 10. Unlike previous work [3], $\\Delta t$ doesn't indicate policy aggressiveness due to the velocity limit reward. A constant $\\Delta t \\in [3, 5]$ yields the best performance, with fake goal commands performing comparable or better than real ones. Too large or small $\\Delta t$ values degrade performance, likely due to fewer samples in ranges in the middle of the episode.\n2) Different $\\Delta G$ of the fake goal command. As discussed in Sec. III-D, different constant goal commands result in different movements. We recorded $V_x$, $V_y$, and $\\omega$ over one second and averaged the results, as shown in Fig. 8, where $\\Delta G = (d i s \\cdot cos \\theta, dis \\cdot sin \\theta, 0)$. The results indicate that: a. $V_x$ is approximately proportional to $\\Delta x$ (i. e. $cos \\theta$). b. $V_y$ is proportional to $\\omega$ with small $dis$; $V_y$ remains proportional to small $\\theta$ but drops quickly as $\\theta$ approaches $\\pi/2$ with large $dis$. c. $\\omega$ is near zero but increases with $dis$. In theory, to execute a command of $(V_x, V_y, \\omega)$, we can first calculate $\\theta$ based on the $V_x : V_y$, then select the appropriate $dis$ for $\\omega$. The limitation of this approach is that $||V_{x,y}||_2$ remains constant, allowing control over velocity direction only."}, {"title": "V. CONCLUSION", "content": "In this work, we propose a novel method for enabling a quadruped robot to pass through tiny traps using only proprioception, avoiding the imprecision of camera images. A contact encoder and classification head are used to capture contact forces latent. With well-designed rewards and fake goal commands, we enable approximate omnidirectional movement without localization techniques. We deploy our policy on the real robot and demonstrate robustness through extensive experiments.\nHowever, limitations include the inability to control velocity magnitude due to reward settings and struggles with highly deformable traps like rubber bands. Future work could address these with velocity adjustments and real-world fine-tuning."}, {"title": "APPENDIX", "content": "In Appendix, we further illustrate the technical details of training, deploying, and experiment setting. Appendix mainly has these sections:\n\u2022 A. Reward Functions: the formula of each reward, the definition and analysis of regularization reward, and style reward.\n\u2022 B. Network Architectures: the network architectures of Actor RNN, Critic RNN, Estimator Module, Latent Encoder, and Contact Encoder.\n\u2022 C. Dynamic Randomization: details of the domain randomization and Gaussian noise.\n\u2022 D. Trap Terrain Setting in Simulation: the trap terrain details for training.\n\u2022 E. Training Hyperparameters: the hyperparameters of PPO, contact force, and t-SNE visualization.\n\u2022 F. Importance Analysis: the method and formula for importance analysis.\n\u2022 G. Real-world Experiment Settings and Additional Results: details of the experimental equipment and deployment, and additional experiments in the low-light environment.\nA. Reward Functions\nThe reward function has three components: task reward $r^\\tau$, regularization reward $r^r$, and style reward $r^s$.\nIn Sec. III-C, we have already introduced the task reward, which plays a major role in the training. In addition, regularization reward is used to optimize the performance of the robot. \"Stall\" reward is used to prevent the robot from stopping in the middle of the journey. \"Velocity limit\" reward is used to slow down the robot and ensure safety. \u201cJoint velocity\u201d reward and \u201cJoint acceleration\u201d reward are used to make the joint movements more stable and smooth. \"Angular velocity stability\" reward is used to make the base of the robot more stable. \"Feet in air\" reward is used to improve the gait and prevent the feet from rubbing on the ground. \u201cBalance reward\u201d is used to improve the left-right symmetry. For style reward, we first collect a dataset using an MPC controller. The dataset contains a state transition $(s_t, s_{t+1})$, with a time interval same as the RL policy. $s_t \\in \\mathbb{R}^{19}$ includes joint positions, base height, base linear velocity, and base angular velocity. We randomly select 200 velocity commands in the simulator. Each command lasts for two seconds and is converted to the next command continually. Following [13], [37], we train a Discriminator $D_{amp}$ by the following loss function:\n$L_{discriminator} = \\mathbb{E}_{(s_t, s_{t+1}) \\sim MPC} [(D_{amp} (s_t, s_{t+1})-1)^2] \\\\ + \\mathbb{E}_{(s_t, s_{t+1}) \\sim Policy} (D_{amp} (s_t, s_{t+1})+1)^2 \\\\ + Q \\mathbb{E}_{(s_t, s_{t+1}) \\sim MPC}[||D_{amp} (s_t, s_{t+1})||^2],$ (12)\nAnd then we use $D_{amp}$ to score the gait performance from policy output $(s_t, s_{t+1})$:\n$r'_{style} = max [0,1-0.25 (D_{amp}(s_t, s_{t+1})-1)^2].$\nB. Network Architectures\nThe details of network architectures are shown in Tab. IV.\nC. Dynamic Randomization\nFor better sim-to-real transfer, we introduce dynamic randomization including domain randomization and Gaussian noise. We have a series of domain randomizations including base mass, mass position, friction, initial joint positions, initial base velocity, and motor strength. the random ranges are shown in the Tab. V.\nBesides, we add Gaussian noise to the input observation, as shown in Tab. VI. This aims to simulate the noise of real robot sensors. Lots of experiments show that with dynamic randomization, the policy can be easily transferred from simulation to the real world without additional training.\nD. Trap Terrain Setting in Simulation\nWe employ \"Terrain Curriculum\" introduced in previous work [43] for better policy training. Due to the instability of reinforcement learning in early training, it is difficult for the policy to learn the movement in complex traps at once. Therefore, we design a trap curriculum to guide the policy from easy to difficult. The terrain is distributed in 10 rows and 10 columns. The terrains are divided into 4 categories. Each categorey has different traps ranging from easy to difficult, consisting of 10 variations. The column numbers of Bar, Pit, Pole, and Plane are 3,2,3,2. To prevent the robot from cheating by detouring, we put the bar and pit in a circle. As shown in Fig. 13, the robot is born inside the circle (blue point) and needs to reach outside the circle (yellow point). The height of the bar increases evenly from 0.05m to 0.25m, with a width randomizing in the range [0.025m, 0.1m]. The width of the pit increases evenly from 0.05m to 0.30m. The number of the pole increases evenly from 10 to 60, with a width randomizing in the range [0.025m, 0.1m]. In addition, we add perlin noise to all of the terrains with an amplitude in the range [0.05m, 0.15m].\nE. Training Hyperparameters\nIn our work, we conduct a Policy Optimization algorithm (PPO) as our reinforcement learning method. The hyperparameters are shown in Tab. VII.\nF. Importance Analysis\nAssume the input $I \\in \\mathbb{R}^N$ and the output (action) $O \\in \\mathbb{R}^M$.\n$O = Policy (I),$ (13)\nFirst, we obtain the Jacobian matrix $J \\in \\mathbb{R}^{M \\times N}$ by calculating the partial derivative.\n$J = \\begin{bmatrix} \\frac{\\partial O_1}{\\partial I_1} & \\frac{\\partial O_1}{\\partial I_2} & ... & \\frac{\\partial O_1}{\\partial I_N} \\\\ \\frac{\\partial O_2}{\\partial I_1} & \\frac{\\partial O_2}{\\partial I_2} & ... & \\frac{\\partial O_2}{\\partial I_N} \\\\ : & : & & : \\\\ \\frac{\\partial O_M}{\\partial I_1} & \\frac{\\partial O_M}{\\partial I_2} & ... & \\frac{\\partial O_M}{\\partial I_N} \\end{bmatrix}$ (14)\nWe take the absolute value for each term of the matrix.\n$J_{abs} = |J|,$ (15)\nFor each input $I_i$, we sum the corresponding output dimensions and align them with the upper $U_i$ and lower bounds $L_i$ to get the importance vector $S \\in \\mathbb{R}^N$.\n$S_i = (U_i - L_i) \\cdot \\sum_{j=1}^M J_{abs}(j, i), i = 1,2,...,N$ (16)\nFor a group of input $G \\subset I$, such as Contact force, Linear velocity, etc, we average importance for every input in $G$.\n$S_G = \\frac{\\sum_{I_i \\in G} S_i}{num(I_i \\in G)},$ (17)\nThe group $G_1, G_2,......,G_k$ is for comparison, we normalize them to get relative importance $R \\in \\mathbb{R}^k$ for each group.\n$R_i = \\frac{S_{G_i}}{\\sum_{i=1}^k S_{G_i}}, i = 1,2,..., k$ (18)\nG. Real-world Experiment Settings and Additional Results\nWe use common easily accessible items as traps for our real-world experiments, as shown in Fig. 14. For the Bar trap, there are two variations: thin bars and thick bars. The thin bars have a diameter of 6mm, while the thick bars measure 20mm in diameter. For the Pit trap, we separate two wooden boxes with a height of 40mm by some distance. For the Pole trap, there are also thin and thick poles. The thin poles are the legs of a iron table with a diameter of 8mm, while the thick poles include a range of obstacles made from thick iron poles and sticks of varying diameters.\nWe also conduct experiments in low-light environment, as shown in Fig. 15, Fig. 16, and Fig. 17. The robot can robustly move through different traps even if there is little light. The results demonstrate the effectiveness and importance of proprioception locomotion in scenarios where there is no visual input, such as during nighttime."}]}