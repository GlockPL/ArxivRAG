{"title": "Reasoning Robustness of LLMs to Adversarial Typographical Errors", "authors": ["Esther Gan", "Yiran Zhao", "Liying Cheng", "Yancan Mao", "Anirudh Goyal", "Kenji Kawaguchi", "Min-Yen Kan", "Michael Shieh"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning using Chain-of-Thought (CoT) prompting. However, CoT can be biased by users' instruction. In this work, we study the reasoning robustness of LLMs to typographical errors, which can naturally occur in users' queries. We design an Adversarial Typo Attack (ATA) algorithm that iteratively samples typos for words that are important to the query and selects the edit that is most likely to succeed in attacking. It shows that LLMs are sensitive to minimal adversarial typographical changes. Notably, with 1 character edit, Mistral-7B-Instruct's accuracy drops from 43.7% to 38.6% on GSM8K, while with 8 character edits the performance further drops to 19.2%. To extend our evaluation to larger and closed-source LLMs, we develop the R2ATA benchmark, which assesses models' Reasoning Robustness to ATA. It includes adversarial typographical questions derived from three widely-used reasoning datasets-GSM8K, BBH, and MMLU-by applying ATA to open-source LLMs. R2ATA demonstrates remarkable transferability and causes notable performance drops across multiple super large and closed-source LLMs.", "sections": [{"title": "1 Introduction", "content": "Chain-of-Thought (CoT) prompting (Wei et al., 2022) enables Large Language Models (LLMs) to break down a complex problem into a series of intermediate steps to solve complex problems. Answering users' queries in a step-by-step fashion has been implemented in many state-of-the-art AI systems such as ChatGPT (OpenAI, 2022), Mistral (Jiang et al., 2023) and Gemini (Team et al., 2023). Despite being carefully trained and aligned, LLMs' sensitivity to the prompt is evident when employing CoT reasoning. It was shown that CoT reasoning can be biased by users' instructions (Perez and Ribeiro, 2022; Lanham et al., 2023; Wang et al., 2024; Xiang et al., 2024) and be confused by irrelevant context (Shi et al., 2023; Turpin et al., 2024). For example, Turpin et al. (2024) found that models tend to justify answers as correct if the majority of previous examples suggest that answer, even when it's incorrect. These scenarios demonstrate the importance of evaluating LLMs' reasoning robustness at the contextual level, such as sentence structure or information correctness. However, it is crucial to recognize that non-contextual mistakes also naturally occur in users' queries, significantly influencing LLMs' performance.\nIn this work, we study the robustness of CoT reasoning against seemingly innocuous errors: typographical errors or typos. We found that typos can significantly undermine the CoT reasoning process. For instance, in Figure 1, the user made two typographical errors in the input: omitting a letter (year to yar) and duplicating a letter (has to haas), yet these minor typos initiate a cascade of errors. Recognizing the impact of such typos, we propose the Adversarial Typo Attack (ATA) algorithm. It is designed to effectively identify typographical er-"}, {"title": "2 Adversarial Typo Attack (ATA)", "content": "ATA employs an iterative process to introduce typographic errors in prompt words, selecting replacements based on their performance in guiding the model to generate the desired attacking target. Unlike traditional adversarial attacks that aim to prompt models to produce harmful outputs, our objective with ATA is to influence LLMs to generate incorrect reasoning responses while preserving the naturalness and coherence of the text. Therefore, to ensure universal adaptability to diverse user queries, we designate our target response as \"Sorry, I'm unable to answer the question.\", which leverages the negative semantic connotation to signal the model not to generate a satisfactory answer, reinforcing our adversarial strategy. Furthermore, candidates considered in each iteration are limited to those that contain only typographical errors, as thoroughly explained in Section 2.2."}, {"title": "2.1 Overview", "content": "ATA employs an iterative process to introduce typographic errors in prompt words, selecting re- placements based on their performance in guiding the model to generate the desired attacking target. Unlike traditional adversarial attacks that aim to prompt models to produce harmful outputs, our objective with ATA is to influence LLMs to generate incorrect reasoning responses while preserving the naturalness and coherence of the text. Therefore, to ensure universal adaptability to diverse user queries, we designate our target response as \"Sorry, I'm unable to answer the question.\", which leverages the negative semantic connotation to signal the model not to generate a satisfactory answer, reinforcing our adversarial strategy. Furthermore, candidates considered in each iteration are limited to those that contain only typographical errors, as thoroughly explained in Section 2.2."}, {"title": "2.2 Typographical Errors used in ATA", "content": "To accurately simulate real user scenarios, we restrict word modifications to those commonly encountered during user interactions. In chatbot interactions powered by LLMs, users frequently make typing errors due to keyboard usage. These mistakes often remain undetected in the absence of a grammar check tool."}, {"title": "Keyboard Proximity Errors.", "content": "One common error occurs when users accidentally strike keys adjacent to the intended key. For instance, when intending to type the letter 'S', users may inadvertently touch the keys \u2018A', \u2018W', 'D', 'Z', or \u2018X\u2019."}, {"title": "Keyboard Double-Typing Errors.", "content": "Another type of error that often goes unnoticed is repeated typing, where a word is mistakenly typed with repeated characters, such as transforming \u201cflop\" into \"floop\". However, this particular error only occurs with words, as users typically recognize and correct repeated typing when it involves numbers."}, {"title": "Keyboard Omission Errors.", "content": "In contrast to double typing, typing omission refers to the unintentional omission of a letter from a word."}, {"title": "Extra Whitespace Error.", "content": "Another common oversight users encounter involves unintentionally inserting multiple spaces between words. This often stems from typing hastily, where users may inadvertently strike the space bar more than once or fail to notice extra spaces as they type swiftly.\nThese errors are hard to detect as they don't trigger conventional spelling or grammar checks, leading to unnoticed text inconsistencies."}, {"title": "2.3 ATA Algorithm", "content": "For a LLM, let Q represent the original question. Our objective is to create imperceptible adversarial perturbations in Q to generate an adversarial example, denoted as Qadv, which induces the model to produce a target answer T."}, {"title": "Task Definition.", "content": "For a LLM, let Q represent the original question. Our objective is to create imperceptible adversarial perturbations in Q to generate an adversarial example, denoted as Qadv, which induces the model to produce a target answer T."}, {"title": "Algorithm Description.", "content": "For each original question Q1:n = {W1,W2,..., Wn} comprising of words wi, we initiate our algorithm by identifying the most influential words in the question using the loss function \u2207L(Q1:n).\nWe then rank these words by their influence and select the top-k, denoted as {W(1), W(2), ..., W(k) }.\nFrom this influential word set, we randomly sample a word ws and uniformly select a letter ls within ws for potential modification. This selected letter undergoes potential modification through the Edit(\u00b7) function, introducing errors based on the operations listed in the mistake dictionary M, which covers four types of typographical errors in Table 1. To create a batch size of B candidates, we repeat this sampling process B times and calculate the loss for each modified question, denoted as L(Q1:n), for b\u2208 {1,\u2026\u2026, B}.\nWe finally select the modified question with the lowest loss:"}, {"title": "3 Experiment", "content": null}, {"title": "3.1 Experimental Setup", "content": null}, {"title": "Dataset.", "content": "For our experiments, we have selected three widely recognized reasoning datasets: GSM8K (Cobbe et al., 2021), BBH (Suzgun et al., 2023), and MMLU (Hendrycks et al., 2021), which cover evaluation of comprehensive reasoning capabilities, including logical reasoning, symbolic reasoning, mathematical reasoning, and commonsense reasoning. We include all test questions from the GSM8K dataset in our evaluation. For the BBH and MMLU datasets, due to computational constraints, we will select a subset of 50 questions from each topic."}, {"title": "Generation of adversarial test cases.", "content": "We conduct ATA on both zero-shot and few-shot prompts, focusing specifically on editing the questions (and options, if applicable). Notably, we avoid attacking the standardized prompt, \u201cLet's think step by step.\u201d to ensure the model retains its understanding of the need for CoT. For few-shot prompts, we retain the original examples without edits, simulating human behavior of directly copying examples."}, {"title": "Models.", "content": "To evaluate the reasoning robustness of LLMs, we select LLMs ranging from smaller parameters to larger parameters to attack. We use Gemma-2B-It, Gemma-7B-It (Team et al., 2024), Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), Llama2-7B-Chat (Touvron et al., 2023), Vicuna- 13B-v1.5, Vicuna-33B-v1.3 (Chiang et al., 2023), Mixtral-8\u00d77B-Instruct-v0.1 (Jiang et al., 2024), ChatGPT (gpt-3.5-turbo-0613) (OpenAI, 2022),"}, {"title": "Implementation details.", "content": "We present accuracy results for both the original and edited scores, represented on a logarithmic scale ranging from 1 to 8 edits applied to each question. The primary metric for assessing the effectiveness of an adversarial attack is the reduction in accuracy. All experiments are conducted on the A800-SMX-80GB GPU."}, {"title": "3.2 Main results", "content": "The main results of the attacks on the GSM8K, BBH, and MMLU datasets and comparison of the performance of the baselines models are summarized in Table 2 and Table 3."}, {"title": "Performance Degradation under ATA.", "content": "As shown in Table 2 and Table 3, our method consistently reduces model performance across various datasets, demonstrating the significant vulnerability of LLMs to such errors. For instance, in Table 2, small models like Gemma-2B2, Llama2-7B, Mistral-7B and Gemma-7B show striking average absolute reductions of 7.0%, 10.6%, 13.6% and 7.8% respectively for GSM8K. Similar declines are observed across four models on other datasets shown by 8.8%, 7.6%, 9.1%, and 6.5% respectively for BBH, and 6.6%, 5.6%, 7.6%, and 5.7%"}, {"title": "Transferability.", "content": "To further explore the impact of adversarial typographical errors on LLMs, we evaluated the transferability of adversarial prompts crafted for Mistral-7B to larger models. The results reveal a similar vulnerability to smaller models, as larger models shown in Table 3: Vicuna- 13B, Vicuna 33B, and Mixtral-8\u00d77B show average absolute reductions of 5.0%, 9.0%, and 8.3% respectively for GSM8K, 8.7%, 8.4%, and 5.2% respectively for BBH, 5.2%, 6.9%, and 5.1% respectively for MMLU. This consistent decrease in performance across various larger models underscores the high transferability of our adversarial attacks, demonstrating that typographical errors not only disrupt smaller models but also significantly impair the reasoning processes of more complex systems. These findings emphasize that the vulnerabilities exploited by our attacks are fundamental, affecting a broad spectrum of model architectures and sizes, thereby highlighting the critical need for robust defense mechanisms in the development of future LLMs."}, {"title": "3.3 Attack Performance Analysis", "content": "Effectiveness. We compare ATA-4 with two baselines to evaluate its effectiveness. The first"}, {"title": "4 Benchmark: Reasoning Robustness to Adversarial Typo Attacks (R2ATA)", "content": "To enable a comprehensive evaluation of LLMs\u2019 Reasoning Robustness to ATA, including future new models, super-large models, and closed-source models, we propose the establishment of a benchmark named R\u00b2ATA. This benchmark utilizes adversarial typographical questions derived from transfer experiments conducted in Section 3, specifically GSM8K, BBH, and MMLU. Concrete examples of R2ATA for each dataset are shown in Tables 6 to 8 in Appendix A.1."}, {"title": "4.1 R2ATA Statistics", "content": null}, {"title": "Representative Example.", "content": "Figure 3 compares the model's responses to an original and an adversarially edited GSM8K question. In the original question, the model follows a logical reasoning pathway to reach the correct answer. Meanwhile, the adversarially edited question introduces subtle typo-"}, {"title": "Distribution of Typographical Edits.", "content": "One of the key analyses involves examining the distribution of the edit operations used in R2ATA. Notably, the predominance of the whitespace error operation adopted by ATA highlights its significance in exploiting model vulnerabilities. This suggests that LLMs are particularly susceptible to errors stemming from additional whitespace, possibly due to a lack of robustness in handling such perturbations. The frequency of whitespace errors implies that patterns involving multiple whitespaces between words are likely infrequent in the training data, resulting in heightened sensitivity and errors in reasoning outputs.\nThe variation in error operation distribution across the three datasets, as depicted in Figure 4, indicates that task complexity influences the prevalence of specific error operations. The GSM8K dataset focuses on mathematical reasoning, while MMLU and BBH cover a broader range of tasks, including logical and commonsense reasoning (Suz- gun et al., 2023). By systematically evaluating LLMs' performance under these conditions, the benchmark aims to provide insights into improving model robustness across diverse reasoning tasks."}, {"title": "4.2 R2ATA Analysis", "content": "The R2ATA benchmark is analyzed at various levels to provide comprehensive insights into the types and patterns of typographical errors that impact model performance."}, {"title": "Type of Edited Words.", "content": "Figure 5 illustrates the distribution of edited word types across all three datasets. The data reveals that nouns are the most frequently edited word type, accounting for 48.9% of the edits. Verbs follow at 16.7%, and adjectives at 14.9%. This distribution reflects the significant roles these word types play in conveying meaning. Nouns, as primary subjects and objects, are often targeted for edits due to their substantial semantic weight, which can profoundly alter sentence meaning and context. Verbs, crucial for actions and states, similarly impact sentence meaning when modified. Adjectives, providing descriptive nuances, can subtly change the tone or implication of text upon editing. In contrast, stop words such as"}, {"title": "conjunctions and prepositions primarily contribute", "content": "to grammatical structure rather than semantic content, making them less frequently edited and thus less impactful on overall meaning. This goes to show that models need to be more robust to subject perturbations to ensure more robustness to these typographical errors."}, {"title": "Edited Words Statistics.", "content": "Figure 6 shows the word cloud of edited words with size reflecting edit frequency. To ensure a fair comparison, we applied Inverse Document Frequency (IDF) normalization, calculated using: where t is the term, N is the total number of prompts, and dft is the number of prompts containing the term t.\nWe adjust each word's frequency by multiplying it with its IDF weight to highlight words disproportionately edited relative to their overall frequency.\nIn the GSM8K dataset, frequent edits of words like \"many,\u201d \u201cpeople,\u201d \u201cmuch,\u201d \u201ctwo,\u201d \u201ceach,\u201d and \"total\" suggest their semantic importance in mathe-"}, {"title": "Impact on the Token Level.", "content": "Figure 7a illustrates how accuracy varies with edit distance for adversarially edited prompts across three datasets: GSM8K, BBH, and MMLU. Meanwhile, Figure 7b shows how accuracy varies with the Jaccard coefficient, with each data point representing 0, 1, 2, 4, and"}, {"title": "8 edits.", "content": "It is evident that even a small number of edits leads to a substantial increase in edit distance, resulting in a significant decline in accuracy. However, despite this increase in edit distance, the Jaccard coefficient remains relatively stable, consistently exceeding 0.8 across all edits. This high degree of similarity between the edited and original prompts suggests that the edits are likely imperceptible to humans, underscoring the challenge of detecting adversarial modifications."}, {"title": "Impact on Attention", "content": "Figure 8 illustrates the changes in attention distribution before and after an adversarial attack on a question. In the original question, attention was focused on critical words such as \"much,\u201d \u201cincreased,\u201d and \u201cby 150%\u201d. However, after the question was edited, there was a noticeable shift in attention. For instance, the attention on \"much\u201d decreased significantly due to it being altered to \u201cmuxh\u201d. Similarly, attention on \"increased\" and \"by 150%\" was entirely lost. Instead, the attention was redirected to irrelevant words like \"the house\". This misallocation of attention led to errors in the reasoning steps, as the model focused on less important parts of the text, thereby compromising its ability to understand and answer the question correctly. The detailed implementation code for attention calculation using PyTorch is shown in Appendix A.2."}, {"title": "5 Related Work", "content": "Textual Adversarial Attacks have garnered significant attention due to their ability to exploit vulnerabilities in LLMs. These attacks, which manipulate input text to mislead models into incorrect predictions or misleading responses, have been studied extensively at various levels of input granularity: character-level (Gao et al., 2018; Li et al., 2019; Pruthi et al., 2019), word-level (Garg and Ramakrishnan, 2020; Jin et al., 2020; Zhou et al., 2024), sentence-level (Shi et al., 2023; Xu et al., 2024; Turpin et al., 2024; Lanham et al., 2023) and semantic-level Zhu et al. (2023); Parcalabescu and Frank (2023), as noted by Zhu et al. (2023). However, these approaches often generate adversarial examples that are easily detectable by human users, limiting their real world applicability. Our approach instead introduces imperceptible modifications to prompts similar to Brown et al. (2018); Richards et al. (2021), offering a more realistic assessment of adversarial risks.\nFurthermore, while some defenses address related threats, such as malware detection adversaries (Fleshman et al., 2018; \u00cdncer Romeo et al., 2018), they operate in more constrained spaces and do not directly apply to the nuanced edits (Lowd and Meek, 2005) that we explore."}, {"title": "6 Conclusion", "content": "This study examined the robustness of LLMs to typographical errors using the ATA algorithm and the R2ATA benchmark. By focusing on imperceptible, real-world attacks in NLP, our work fills a key gap in adversarial research, moving beyond the artificial constrains of prior approaches and offering insights into more practical vulnerabilities in LLMs. Our findings show that even minor typographical changes significantly reduce model accuracy. Specifically, we observe that adversarial prompts from Mistral-7B similarly affect larger models like Vicuna-13B, Vicuna-33B, and Mixtral-8\u00d77B, indicating that both smaller and larger models are vulnerable. This highlights the need for improved robustness in LLMs against typographical errors. The R2ATA benchmark is a valuable tool for developing more resilient models capable of reliable performance despite minor errors, emphasizing the critical need for robust defense mechanisms in future LLMs."}, {"title": "Limitation", "content": "Our algorithm primarily focuses on typographical errors common in languages that use alphabets and whitespaces, such as English. This excludes languages with different writing systems, such as Chinese, where typographical errors may involve character substitutions or stroke omissions. The typographical errors considered may not cover all possible real-world scenarios. For instance, whitespace errors only apply to languages that use spaces, while letter addition and deletion errors are relevant only to alphabetic languages. Therefore, future research should extend the scope to encompass a broader range of linguistic diversity to ensure the applicability of findings across various languages and writing systems. Exploring language-specific modifications will provide a more comprehensive understanding of LLM robustness across diverse linguistic contexts. Developing and testing adversarial attacks tailored to these languages will help in creating more universally resilient language models. Additionally, our evaluation primarily relies on open-source and commercially available LLMs due to accessibility constraints. While the R2ATA benchmark effectively demonstrates vulnerabilities in these models, the performance of many closed-source LLMs remains unexplored."}, {"title": "2.3 ATA Algorithm", "content": "Task Definition. For a LLM, let Q represent the original question. Our objective is to create imperceptible adversarial perturbations in Q to generate an adversarial example, denoted as Qadv, which induces the model to produce a target answer T.\n\nThis can be formulated as follows:\n\nmin L(T|Qadv),\n Qadv\n\n\nwhere L(T|Qadv) = -logp(T|Qadv) is the negative log-likelihood of the LLM generating the target answer T given the adversarial prompt Qadv.\n\nAlgorithm Description. For each original question Q1:n= {W1,W2,...,Wn} comprising of words wi, we initiate our algorithm by identifying the most influential words in the question using the loss function \u2207L(Q1:n).\nWe then rank these words by their influence and select the top-k, denoted as {W(1), W(2), ..., W(k) }.\nFrom this influential word set, we randomly sample a word ws and uniformly select a letter ls within ws for potential modification. This selected letter undergoes potential modification through the Edit(\u00b7) function, introducing errors based on the operations listed in the mistake dictionary M, which covers four types of typographical errors in Table 1. To create a batch size of B candidates, we repeat this sampling process B times and calculate the loss for each modified question, denoted as L(Q1:n), for b\u2208 {1,\u2026\u2026, B}.\nWe finally select the modified question with the lowest loss:\nQin= arg min L(Qin).\n\nThis process is repeated for E iterations, depending on the desired number of edits to execute the tar-"}]}