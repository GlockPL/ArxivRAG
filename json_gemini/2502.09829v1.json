{"title": "Efficient Evaluation of Multi-Task Robot Policies With Active Experiment Selection", "authors": ["Abrar Anwar", "Rohan Gupta", "Zain Merchant", "Sayan Ghosh", "Willie Neiswanger", "Jesse Thomason"], "abstract": "Evaluating learned robot control policies to determine their physical task-level capabilities costs experimenter time and effort. The growing number of policies and tasks exacerbates this issue. It is impractical to test every policy on every task multiple times; each trial requires a manual environment reset, and each task change involves re-arranging objects or even changing robots. Naively selecting a random subset of tasks and policies to evaluate is a high-cost solution with unreliable, incomplete results. In this work, we formulate robot evaluation as an active testing problem. We propose to model the distribution of robot performance across all tasks and policies as we sequentially execute experiments. Tasks often share similarities that can reveal potential relationships in policy behavior, and we show that natural language is a useful prior in modeling these relationships between tasks. We then leverage this formulation to reduce the experimenter effort by using a cost-aware expected information gain heuristic to efficiently select informative trials. Our framework accommodates both continuous and discrete performance outcomes. We conduct experiments on existing evaluation data from real robots and simulations. By prioritizing informative trials, our framework reduces the cost of calculating evaluation metrics for robot policies across many tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "With the growth of large-scale robot datasets and pretrained policies, robot systems have become capable of achieving good performance across many tasks; however, this diversity makes evaluating these policies increasingly more difficult. Unlike fields such as computer vision or natural language processing, physical robotics experiments are conducted sequentially, with each policy rollout taking experimenter effort. Considering the effort to change task setups, it becomes impractical to evaluate every policy on every task.\nIn practice, experimenters are typically interested in selecting the best checkpoints, tuning hyperparameters, or comparing model architectures, which do not necessarily require a full evaluation across every policy across every task. A robot policy that can \u201cpick up an apple\u201d is likely capable of \"picking up an orange\" for an analogous scene. Our insight is to take advantage of relationships between tasks and frame evaluation as a population parameter estimation problem, which lets us design more efficient experiment sampling strategies.\nManipulation [34, 23, 6] and navigation [43, 42, 3] approaches continue to improve. Simulation-based evaluation has become a common approach to measure that improvement [28], but simulation has often been insufficient for understanding real-world performance [1, 10, 28]. The combinatorial growth of tasks with scene complexity makes an exhaustive evaluation even more impractical. As such, there is a need for efficient evaluation strategies that can enable systematic and scalable testing of multi-task robot policies in the real world.\nWhen evaluating a robot policy, it is common to consider only the mean of some metric. However, since robot performance often has high variance, we instead consider the evaluation of a policy on a specific task as a distribution of outcomes. Thus, every policy-task pair is characterized by a distribution reflecting the experiment conditions, for example a Bernoulli distribution for binary success or a Gaussian distribution for a reward outcome. In this work, as an experimenter conducts evaluations sequentially, we learn a surrogate model that estimates the parameters for this distribution for every policy-task pair under consideration.\nTo build an efficient evaluation strategy, we take advantage of latent shared structure between tasks. As we sample new experiments, we learn a surrogate model conditioned on latent task and policy embeddings. We show that better representations of a policy and a task, including language-based priors for tasks, improves estimates of the outcome distributions, indicating that there is shared information between tasks and policies learnable from policy performance.\nSince evaluation is expensive, we want to minimize the cost of evaluation while still estimating the performance of all policies across all tasks of interest. Then, with our surrogate model, we leverage strategies from the active learning literature to integrate cost-efficient sampling heuristics like expected information gain. We show that our approach is able to efficiently estimate the performance of robot policies across tasks.\nIn particular, we:\n\u2022\tformalize multi-task robot policy evaluation as a population parameter estimation problem;\n\u2022\tfind that there are performance relationships between tasks for estimating the performance of a policy-task pair;\n\u2022\tcreate an active testing protocol that leverages these performance relationships between tasks and policies, allowing us to efficiently evaluate multiple robot policies;\n\u2022\tand create cost-aware sampling strategies that can estimate the performance of robot policies with lower cost."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Past work in machine learning model evaluation and active learning have considered how to compare model performance; however, as more robot policies become easier to develop, it is critical to develop better strategies for evaluating robot strategies. We discuss approaches for testing models in machine learning and its relevance to evaluation for robotics.\nEvaluation in Machine Learning. In fields such as computer vision or NLP, it is common to characterize the out-of-distribution performance of a single model [49, 19, 40, 18, 30, 8, 14], some of which create a standard for comparing different models as well [30]. These approaches allow for experimenters to quickly understand the performance of their model, and in some cases compare between models. However, in robotics, each task is expensive to evaluate and each policy evaluation is difficult. In this work, we look at use methods from active learning to improve experiment selection during evaluation.\nActive Testing. Similar to active learning which aims to select training labels, active testing approaches [41, 39, 51] focus on selecting test instances to evaluate to better predict model performance. Though these settings focus on classification or regression labeling tasks, this formulation is important to robotics as evaluation is expensive. Various Bayesian optimization, active learning, and active testing approaches use surrogate models to estimate the value of a training or test instance [11, 7, 44, 9, 38, 24], often incorporating cost-aware sampling [27, 36]. In robotics, surrogate models have been used to predict outcomes of a human-robot interaction scenarios in simulation for policy learning [4]; however, that past work did not consider the cost evaluating each scenario. Additionally, most of these works focus on active learning and active testing for regression models. Since robot evaluation can have high variance, we take inspiration from past work [45] to focus on active learning of probablistic models using a surrogate model. We then apply these cost-aware active testing strategies on multi-task, multi-policy robot evaluation by learning a task and policy conditioned surrogate model.\nEvaluation of Robot Policies. Simulation is often used to evaluate the performance of a real-robot system [10, 1, 22, 15] by recreating a simulated counterpart to a real environment, but shows ineffective direct sim2real performance without domain randomization or real-world finetuning strategies. There exist correlations between simulation and real-world performance even if they do not exactly match [37, 28]; however there are no guarantees about real-world performance. Other recent work focuses on real world evaluation such as carefully selecting the initial conditions of an experiment [25], evaluating LLM-based task planners [21], active capability assessment of black-box symbolic planners [46, 47, 33], or providing bounds on policy performance by assuming some underlying distribution for outcomes [48]. Other work has investigated how changes to these initial conditions can provide information about policy sensitivity [35, 50, 2] or has used factors of the initial conditions and naive sampling strategies to more efficiently collect data [13]. In this work, we consider the setting of evaluating multiple policies across various tasks while also learning the parameters of an underlying distribution. We then leverage this learned distribution to more efficiently sample experiments for evaluation."}, {"title": "III. PROBLEM FORMULATION AND NOTATION", "content": "The objective of this work is to design an efficient strategy to evaluate robot policies across tasks while balancing the cost of experimentation. Consider a fixed set of $M$ robot policies, denoted by $P = {\u03c0_1,\u03c0_2,...,\u03c0_M}$ and a set of $N$ tasks $T = {T_1, T_2, ...,T_N}$. Each task $T_j \u2208 T$ is a finite-horizon MDP defined by states, actions, and a high-level natural language instruction $L_i$.\nOur framework is policy-agnostic and does not assume access to policy model weights, and can be applied to engineered robot systems in addition to end-to-end models.\nPopulation Parameter Estimation. We formulate the problem as population parameter estimation, similar to probabilistic matrix factorization [32]. Let the performance of a policy $\u03c0_i \u2208 P$ on a task $T_j \u2208 T$ be represented by the random variable $X_{ij}$ with distribution $P_{ij}$, from which we can sample evaluations $X_{ij} \u223c P_{ij}$. Here, $P_{ij}$ represents the \"true\" performance distribution. Since the underlying distribution $P_{ij}$ is unknown, the goal of population parameter estimation is to estimate a distribution $Q_{ij}$ that models real-world evaluation outcomes from $P_{ij}$. We use $\u03b8_{ij}$ to represent the parameters of the learned distribution $Q_{ij}$. For example, $\u03b8_{ij} = [\u03bc, \u03c3]$ if $Q_{ij}$ is a Gaussian distribution. Given a limited number of observed samples from the true distribution, $x_1,x_2,...,x_n \u223c P_{ij}$, the goal is to estimate the parameters of an estimated distribution $\u03b8_{ij}$.\nOur setting also has samples from other random variables, $X_{kl}$ corresponding to different policy-task pairs. Therefore, in this work we want to estimate $\u0398 = {\u03b8_{ij}}_{i=1...M,j=1...N}$ for all policy-task pairs given a dataset $D = {x}$. These distributions can be visualized as a grid of policy-task pairs as shown in Figure 2.\nThe aim is to estimate the parameters of $Q_{ij}$ of all policy-task combinations by leveraging shared information across this matrix. However, it is infeasible to directly evaluate all policy-task pairs due to cost constraints. Therefore, we adopt an active testing approach, where the objective is to iteratively select the most informative experiments $(\u03c0_i, T_j)$ to efficiently learn $\u0398$.\nActive Testing. We apply an active learning paradigm to learn a population parameter estimator $f(\u03c0_i, T_i)$. As such, we define acquisition functions to guide the selection of task-policy pairs or tasks alone, and then sample experiments that are most informative. First, we define an acquisition function $\u03b1(\u03c0_i, T_i)$, and the next experiment is selected by maximizing this function over all possible experiments:\n$(\u03c0^*_i, T^*_j) = arg \\underset{(\u03c0_i, T_j)}{max} \u03b1(\u03c0_i, T_j)$.\nAlthough these acquisition functions are informative, we want a balance between selecting informative experiments and their costs.\nEvaluation Cost. In real-world evaluation, each policy-task evaluation incurs a cost. Let $C_{eval}(T_j)$ denote the cost of a single evaluation of a policy on task $T_j$. We make a simplifying assumption that this cost is agnostic to changes in the policy under evaluation, that often being a configurable software option. This cost could include the time required to execute the policy, the resources consumed during evaluation, or the manual supervision required to reset the scene. Furthermore, switching between tasks typically incurs a larger cost involving a reconfiguring the scene or the robot. We define this switching cost $C_{switch}(T_j, T_k)$ as the cost associated with transitioning from task $T_j$ to $T_k$. For a sequence of tasks that have been evaluated $T_{i_1},..., T_{i_r}$ (where each $i_j \u2208 N$), we compute the total cost of evaluation as:\n$C_{total} = \\sum_{j=1}^{N-1} C_{eval}(T_{i_j}) + \\sum_{j=1}^{N-1} C_{switch}(T_{i_j}, T_{i_{j+1}})$\nGiven these costs, the problem is to design an evaluation strategy that minimizes the total cost of evaluation while learning the population parameters of test instances."}, {"title": "IV. METHOD", "content": "We aim to design a framework for sampling experiments for multi-task robot policies. Our framework consists of two parts: (1) learning a surrogate model to estimate the population parameters of a test instance and (2) designing strategies to sample experiments in a cost-efficient manner. The surrogate model leverages task and policy representations that define an experiment to have a better estimate of the overall performance distributions. Then, we use this surrogate model to compute the expected information gain of different experiments. We then use the expected information gain along with the cost of switching tasks to conduct active testing.\nA. Surrogate Model\nAs we evaluate our robot policies across tasks, we track the outcomes of each trial to aggregate a dataset $D$ over time. Each of these outcomes are realizations of a true underlying distribution $P_{ij}$. Our goal is to learn a surrogate model from $D$ that predicts the population parameters $\u03b8_{ij}$ of a performance distribution $Q_{ij}$. As more evaluation rollouts are conducted, we add the outcomes to $D$ and continue training the surrogate model.\nTo train an effective surrogate model, we use notions of similarity between tasks and policies. Thus, we need a representation that captures the similarities between policies and tasks with respect to their performance distributions. We define a policy embedding $e_{\u03c0_i}$ and task embedding $e_{T_j}$, where similar performance distributions in task and policy can be captured based on the embeddings. These policy and task representations are then provided as input to an MLP that predicts the estimated population parameters:\n$\u03b8_{ij} = f(\u03c0_i, T_j) = MLP(e_{\u03c0_i}, e_{T_j})$.\nTask and Policy Representation. To define the task and policy embeddings $e_{\u03c0_i}, e_{T_j}$, we design various types of embeddings. In practice, we cannot know the relationship between policies in advance as we are conducting evaluation. Therefore, we define the policy embedding to be a fixed, randomly initialized embedding to act as an identifier for the policy in a given experiment.\nFor the task embedding $e_{\u03c0_i}$, we leverage language embeddings from MiniLMv2 [16] which we reduce to 32 dimensions using PCA over all tasks. However, we found that language embeddings overly focus on nouns as opposed to verbs, which causes issues as actions with similar nouns but different verbs would be closer together verbs with the same nouns. Thus, we apply the following procedure to mitigate this issue. We (1) use part-of-speech tagging to extract all verbs and verb phrases, (2) compute a language embedding for the verb $e_{verb}$ and for the entire task description $e_{task}$, and then (3) compute the task embedding\n$e_{T_i} = 0.8e_{verb} + 0.2 e_{task} + 0.1 \u22c5 N(0,1)$.\nWe also found that the embeddings were often too close across multiple tasks, and we found that adding a slight noise term helped separate close embeddings. Experiments on this result are in Section VI.\nPopulation Parameter Estimation. Outcomes in robot learning can take the form of continuous values like rewards, time to completion, or task progress, and binary values like task success. Thus, the underlying distribution from the surrogate model depends on the type of task. We consider two types of underlying distributions. When $X_{ij}$ is continuous, $Q_{ij}$ takes the form of a mixture of Gaussians with $K$ components,\n$X_{ij} \u223c Q_{ij} = \\sum_{k=1}^{K} p_kN(\u03bc_k, \u03c3_k)$,\nwhere $\u03c0_k, \u03bc_k,$ and $\u03c3_k$ are the mixing coefficients, means, and standard deviations of the Gaussian components respectively that are predicted from the surrogate model $\u03b8_{ij} = f(\u03c0_i, T_j)$. We thus train the surrogate model with a mixture density loss [5, 17] to minimize the negative log-likelihood of the observed data under the mixture model. In our experiments on continuous outcome distributions, we use $K = 2$ Gaussian components, as robotics performance is often bimodal; robots either fail catastrophically or they maintain non-zero performance.\nIn the case where $X_{ij}$ is binary, indicating success or failure, $Q_{ij}$ takes the form of a Bernoulli distribution:\n$X_{ij} \u223c Q_{ij} = p^{x_{ij}}(1 \u2212 p)^{1-x_{ij}}$,\nwhere $\u03b8_{ij} = {p \u2208 [0, 1]}$ represents the success probability predicted by the surrogate model trained using cross-entropy loss.\nB. Cost-aware Active Experiment Selection\nWe explore cost-aware, active-experiment acquisition functions that guide selection of experiments based on their expected utility while considering associated costs. To define the acquisition function, we first focus on how to measure the informativeness of a policy-task evaluation, which we capture through expected information gain.\nExpected Information Gain. Expected Information Gain (EIG) quantifies the value of an experiment by estimating how much it reduces the predictive uncertainty of the performance distribution for a policy-task pair. Since the surrogate model estimates performance distributions, we define the EIG of a policy-task pair using a Bayesian Active Learning by Dis-agreement (BALD) [20] formulation for probabilistic models\n$I(\u03c0_i, T_j) = H[Q_{ij}] \u2212 E_{\u03b8_{ij}\u223cf(\u03b8_{ij}|D)}[H[Q_{ij}|\u03b8_{ij}]]$.\nmarginal entropy\nexpected conditional entropy\nThe first term represents the marginal entropy over $Q_{ij}$, which quantifies the total uncertainty in $Q_{ij}$. The second term corresponds to the expected conditional entropy over multiple samples of parameters $\u03b8_{ij}$. Thus, $I(\u03c0_i, T_j)$ captures the disagreement between multiple samples of distributions. For example, if 10 sampled parameters for a Gaussian have very different distributions, then their disagreement will be high. Since the entropy of a mixture of Gaussians generally lacks a closed-form solution, we estimate the entropy by discretizing the empirical distribution into $n = 25$ bins for which to compute entropy over.\nBALD ensures the EIG score is higher in test instances where there is disagreement in the predicted distributions across sampled parameters. In this case, we define the acquisition functions $\u03b1(\u03c0_i, T_j) = I(\u03c0_i, T_j)$.\nTo compute the expected information gain, we require multiple samples of $\u03b8_{ij}$; however, we only train a single MLP. Inspired by Monte Carlo dropout [12] and past literature [31, 26], we apply dropout only at test-time to compute multiple samples of $\u03b8_{ij}$ from the surrogate model $f(\u00b7)$.\nCost-Aware EIG. While EIG effectively quantifies the informativeness of an experiment, it does not consider the costs of conducting evaluation. To make EIG cost-aware, we design the following acquisition function based on prior work that simply integrates cost with a multiplicative factor [36, 27]:\n$\u03b1_{cost-aware}(\u03c0_i, T_j, T_{current}) = \\frac{I(\u03c0_i, T_j)}{(\u03bb \u22c5 C_{switch}(T_{current}, T_j)) + 1}$,\nwhere $I(\u03c0_i, T_j)$ represents EIG for the policy $\u03c0_i$ on task $T_j$, $C_{switch}(T_{current}, T_j))$ is the cost of switching from the current"}, {"title": "V. EVALUATION ON OFFLINE DATASETS", "content": "To evaluate our active testing framework, we leverage evaluations that have already been conducted which we then sample offline. We use experiments from the HAMSTER paper [29], the OpenVLA paper [23], and from MetaWorld [52], as visualized in Figure 3. For MetaWorld, we train two versions, one focused on understanding our framework's ability in evaluating different policies and another on evaluating multiple checkpoints of a single policy. Each of these datasets can be modeled with different underlying distributions and have varying costs, semantic diversity, and skills. More details on training for MetaWorld, switching costs for the datasets, and other details can be found in Appendix A.\nHAMSTER. We use evaluations from the HAMSTER paper [29], which evaluates a hierarchical VLA model against 4 other policies such as OpenVLA [23] and Octo [34] across 81 tasks. These 81 tasks are of varying complexity, with diverse task types, objects, and linguistic variation that were evaluated once each. Their work uses a continuous task progress metric; however, since they only evaluated each policy-task pair once, we treat the single continuous value as the mean of a Gaussian distribution with a fixed standard deviation. For switching cost, we add an additional cost if the policy switches from one task type to another. More details on this cost can be found in Appendix A.\nOpenVLA. We use evaluations from the OpenVLA paper [23], which compares 4 policies over 29 tasks. In their paper, some tasks allow for partial success (0.5). For simplicity, we round the partial successes down to maintain a binary success metric. OpenVLA also provides results across two embodiments. Therefore, in addition to a higher cost term to switching tasks that require a large scene reset, we add an additional cost term to switch between embodiments. More details in Appendix A.\nGiven these datasets, we show that the types of policy and task representations that are useful for active learning, and then we can leverage the surrogate model for cost-aware active experiment selection.\nMetaWorld Policies. MetaWorld [52] is an open-source simulated benchmark containing a set of 50 different manipulation environments for multi-task learning. We train 10 poli-"}, {"title": "VI. TASK AND POLICY REPRESENTATION", "content": "As we define an experiment based on a task and a policy, we must design different embedding strategies for each of them. We first discuss baselines and upper bounds on task and policy representations, then we show results on how these representations impact our surrogate model.\nA. Experimental Setup\nAs it is unclear what an ideal representation for a policy or task is, we compute an upper bound for a task and policy representation by taking all the pre-evaluated outcomes, and then training learnable embeddings on the task of estimating performance. Thus, these task and policy representations have specifically been tuned for this prediction task. We can then use these learned embeddings as optimal representations of the task and policy.\nHowever, this optimal approach requires all the data a priori. Thus, we need a way to represent both a task and a policy. The most direct way to represent a task is based on the language description of a task. As described in Section IV-A, we define our task representation as a weighted sum between the language embeddings of the task description and the verbs. We call this approach Verb. Overall, we consider the following task representation types as upper bounds and baselines:\n1)\tOptimal: Leverage all the data a priori to learn embeddings that are useful for predicting performance;\n2)\tVerb: Use a weighted sum of the language embedding of the task and the language embedding of its verbs;\n3)\tLanguage: Use a language embedding of the task as its representation; and\n4)\tRandom: Assume no relationship between policies and tasks by using random embeddings.\nUnlike a task representation through language, there is no clear representation for a policy. We leave the exploration of new policy representations to future work and focus on two policy representations: Optimal and Random.\nAll experiments in this section were run for 750 evaluation steps over three seeds. To evaluate how much these embeddings improve the performance of population parameter estimation during active experiment selection, we look at the log likelihood of all the outcomes in our offline dataset against a probability distribution represented by the predicted population parameters from the surrogate model. Each experiment is sampled similar to how researchers typically evaluate: we select a random task and test each policy three times."}, {"title": "VII. COST-AWARE EXPERIMENT SELECTION", "content": "To evaluate the effectiveness of our cost-aware active experiment selection methods, we assess the population parameter estimation capability of our framework across various datasets using continuous and binary performance distributions.\nA. Experimental Setup\nSampling Strategies. To select the most informative experiment based on an acquisition function $\u03b1(\u03c0_i, T_j)$, we must design acquisition functions to define our sampling strategy. We consider two types of sampling strategies. The first is to select both a policy and a task to run an evaluation on. Given the EIG formulation in Section IV-A, we define three sampling strategies with this approach:\n\u2022\tRandom Sampling: Select a task-policy pair uniformly at random $\u03b1(\u03c0_i, T_j) = 1/(|P| \u00d7 |T|)$;\n\u2022\tEIG: Select a task-policy pair $(\u03c0_i,t_j)$ with the highest EIG: $\u03b1(\u03c0_i, T_j) = I(\u03c0_i, T_j)$;\n\u2022\tCost-aware EIG: Select a task-policy pair that maximizes the cost-aware EIG according to Equation 7.\nThe second type of sampling strategy is to select a task, and then evaluate every policy in that task d = 3 times.\n\u2022\tRandom Task: Select a task uniformly at random and evaluate all policies on that task: $\u03b1(t_j) = 1/|T|$\n\u2022\tTask EIG: Select a task $T_j$ that maximizes the summed EIG across all policies: $\u03b1(t_j) = \u03a3_iI(\u03c0_i,T_j)$\n\u2022\tCost-aware Task EIG: Select a task $T_j$ that maximizes the summed cost-aware EIG across all policies: $\u03b1(T_j) = \u03a3_i \u03b1_{cost-aware} (\u03c0_i, T_j, T_{current})$\nThe task-based sampling strategies is more realistic to how experimenters evaluate their robots today, as experimenters typically select a task and then evaluate every policy.\nWe evaluated each method for 1500 evaluation steps over three seeds using Random policy embeddings and Verb task embeddings. To evaluate these methods, we consider two metrics: (1) the log likelihood of all the outcomes in our offline dataset against the predicted population parameters of the model, and (2) the L1 error between the mean from all the data for a policy-task pair against the mean derived from the estimated population parameters.\nB. Results\nEIG-based approaches struggle to learn population parameters that represent all the data, but better estimate the mean. In Figure 5, we show the average log likelihood of all the outcomes in our offline dataset against the probability distribution represented by the predicted population parameters from the surrogate model. In both task- and policy-task sampling approaches, we find that EIG-based approaches fit the original data marginally better than random baselines. In some cases, such as for MetaWorld Policies with success rate, cost-aware EIG is able to maintain a larger improvement; however, this result is not consistent across other datasets. This result indicates that learning this full underlying distribution remains challenging, particularly in the early stages of evaluation when data is sparse. However, in Figure 6, EIG-based approaches clearly dominate when estimating the mean of these distributions, and often are able to estimate the mean at a lower cost compared to random baselines. If the cost is fixed at a lower value, as if it was a maximum cost-budget, then we find that EIG-based approaches better estimate the means.\nTradeoffs between task- and policy-task sampling. Both"}, {"title": "VIII. CONCLUSION AND LIMITATIONS", "content": "We present a framework for the efficient evaluation of multitask robot policies. By framing evaluation as an active testing problem, we develop techniques that use relationships between tasks to predict policy performance distributions. In particular, we focus on methods that select experiments based on the expected information gain. Our experiments demonstrate that task similarities can indeed be used to predict policy performance in an efficient manner, compared to standard evaluation approaches. As evaluation settings and policy comparisons continue to scale in size, our methods for active testing can help lower the cost of effective evaluation without sacrificing too much information about policy performance.\nFuture Work. To properly be cost-aware, a single look-ahead step is often not enough, as it may be beneficial to plan future evaluations with respect to cost and potential information gain. Future work can extend our methods by developing look-ahead algorithms that can select longer sequences of experiments at a time. In addition, other types of acquisition functions, such as those that batch experiments at once, can be explored. We focused on ensuring that our surrogate model is able to estimate the landscape of performance across tasks and policies, but future work can focus on other types of comparison, such as finding the best average policy, finding a ranked ordering of policies, or finding the worst performing tasks. Each of these would require different active sampling strategies. Additionally, learning policy embeddings may better predict performance, and policy embedding priors might be formed by encoding the training data of those policies, analogous to \u201ctask embeddings\u201d in multi-task learning. There are also hierarchical relationships between tasks such as \u201cpour milk\" likely depending on being able to \"pick up the milk\" that would be exciting to explore in future work.\nLimitations. Though our approach to mitigating the cold-start problem with test-time dropout appears to have improved performance during sampling, this approach has not been rigorously tested by the Bayesian optimization community. We had also tried other approaches, such as ensembling and variational prediction, but these approaches also overfit to the small size of the dataset early in the evaluation procedure. We also represented execution costs naively at a fixed cost; however, different tasks may have different execution costs that may depend on whether a policy fails on its task or not, such as having to clean up spilled milk. Additionally, we chose to use a simple MLP to learn our surrogate model; however, other work often used Bayesian neural networks and Gaussian processes. We made this decision because these alternative approaches typically do not scale to larger inputs; however, we did not consider the state-of-the-art for those approaches."}]}