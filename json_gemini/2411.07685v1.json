{"title": "Fast Disentangled Slim Tensor Learning for Multi-view Clustering", "authors": ["Deng Xu", "Chao Zhang", "Zechao Li", "Chunlin Chen", "Huaxiong Li"], "abstract": "Tensor-based multi-view clustering has recently received significant attention due to its exceptional ability to explore cross-view high-order correlations. However, most existing methods still encounter some limitations. (1) Most of them explore the correlations among different affinity matrices, making them unscalable to large-scale data. (2) Although some methods address it by introducing bipartite graphs, they may result in sub-optimal solutions caused by an unstable anchor selection process. (3) They generally ignore the negative impact of latent semantic-unrelated information in each view. To tackle these issues, we propose a new approach termed fast Disentangled Slim Tensor Learning (DSTL) for multi-view clustering Instead of focusing on the multi-view graph structures, DSTL directly explores the high-order correlations among multi-view latent semantic representations based on matrix factorization. To alleviate the negative influence of feature redundancy, inspired by robust PCA, DSTL disentangles the latent low-dimensional representation into a semantic-unrelated part and a semantic-related part for each view. Subsequently, two slim tensors are constructed with tensor-based regularization. To further enhance the quality of feature disentanglement, the semantic-related representations are aligned across views through a consensus alignment indicator. Our proposed model is computationally efficient and can be solved effectively. Extensive experiments demonstrate the superiority and efficiency of DSTL over state-of-the-art approaches. The code of DSTL is available at https://github.com/dengxu-nju/DSTL.", "sections": [{"title": "I. INTRODUCTION", "content": "In many real-world applications, data can originate from different sources and feature collectors. For instance, we can utilize text posts, images, videos, user profiles, and social network connections to depict user behavior and interactions on social media platforms. To analyze sensor signals, we can decompose them into time and frequency domains. These are known as multi-view data, which often detail an object from various perspectives and provides a richer and more comprehensive understanding compared to single-view data. In recent years, the surge in multi-view data has sparked considerable interest in multi-view learning (MVL). MVL seeks to leverage the potential consistent and complementary information across multiple views to enhance generalization performance in downstream machine learning activities such as classification and clustering [1]\u2013[8]. Single-View Clustering (SVC) refers to the clustering of data comprising of a single view, while Multi-View Clustering (MVC) is to partition the multi-view data, leading to superior results than those obtained with SVC [9]\u2013[11].\nMost existing methods for MVC have demonstrated success, such as those employing matrix factorization [12]\u2013[14], graph learning [15]\u2013[17], and subspace learning [18]\u2013[20]. Multi-view matrix factorization (MultiMF) effectively reduces the dimensionality of high-dimensional data and captures diverse underlying representations of multiple views. For example, Ma et al. [13] integrated multi-view linear discriminant analysis with MultiMF to leverage the intrinsic low-dimensional structure within the projection subspace. Liu et al. [14] unified MultiMF with partition generation to improve the clustering performance and efficiency for large-scale datasets. Graph-based methods strive to learn a unified graph from multiple views to delineate the pairwise similarities among data points. Huang et al. [16] proposed to formulate both the multi-view consistent graph and diverse graph in a unified framework. In [17], Wang et al. performed both anchor learning and graph construction to acquire an anchor graph to promote clustering efficiency. Another category is subspace-based MVC methods, which aim to infer latent representations from different views in a shared subspace. One representative example is the work of LMSC [18]. It integrated multiple views into a comprehensive latent representation subspace that encodes complementary information across different views. Unlike LMSC, Sun et al. [19] combined anchor learning and graph construction into a unified subspace to get a more discriminative clustering structure. Recently, several deep MVC approaches [21]\u2013[24] have also emerged to bolster clustering performance by harnessing the robust feature learning capabilities of deep neural networks.\nAlthough these MVC methods have achieved good results, they fail to fully explore the high-order correlations between each view. To solve this problem, tensor-based MVC methods have developed recently [25]\u2013[30], which usually stack the similarity graphs from all views into a three-order tensor to capture the cross-view correlations with tensor-based regularization. For instance, Zhang et al. [25] integrated self-expression based subspace representations from various"}, {"title": "II. NOTATIONS AND RELATED WORK", "content": "In this section, we explain the commonly used notations and provide necessary tensor preliminaries. Then we present two central contents related to our work: multi-view matrix factorization and tensor-based multi-view clustering.\n#### A. Notations and Preliminaries\nWe adopt the following notation conventions through- out this paper: bold lowercase letters (e.g., x) represent vectors, uppercase letters (e.g., X) denote matrices, and calligraphic letters (e.g., \\(\\mathcal{X}\\)) signify tensors. For a matrix \\(X \\in \\mathbb{R}^{N_1 \\times n_2}\\), its Frobenius norm and nuclear norm are defined as \\(||X||_F = \\sqrt{\\sum_{ij} x_{ij}^2}\\) and \\(||X||_* = \\sum_{i} \\delta_i(X)\\), respectively, where \\(x_{ij}\\) is the element of X at position (i, j), and \\(\\delta_i(X)\\) is the i-th singular value of X. Tr(\u00b7) denotes the matrix trace, 1 represents an all-one vector, and \\(I_k\\) is ak-dimensional identity matrix. sign(\u00b7) denotes the sign of the input. For a tensor \\( \\mathcal{X} \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}\\), its l1-norm is defined as \\(||\\mathcal{X}||_1 = \\sum_{ijk} |\\mathcal{X}_{ijk}|\\), where \\(\\mathcal{X}_{ijk}\\) is the element of \\(\\mathcal{X}\\) at position (i, j, k). The i-th frontal, lateral, and horizontal slice of \\(\\mathcal{X}\\) are denoted as \\(\\mathcal{X}(:,:,i)\\), \\(\\mathcal{X}(:,i,:)\\), and \\(\\mathcal{X}(i,:,:)\\), respectively. Additionally, for convenience, we use \\(\\mathcal{X}(i)\\) to represent \\(\\mathcal{X}(:,:,i)\\). \\(\\mathcal{X}_f\\) denotes the fast Fourier transformation (FFT) of \\(\\mathcal{X}\\) along the third dimension, i.e., \\(\\mathcal{X}_f = fft(\\mathcal{X}, [], 3)\\), and \\(\\mathcal{X}\\) can be recovered from \\(\\mathcal{X}_f\\) by the inverse FFT operation, i.e., \\(\\mathcal{X} = ifft(\\mathcal{X}_f, [], 3)\\) [34].\n**Definition 1 (t-SVD [35]).** For a tensor \\(\\mathcal{X} \\in \\mathbb{R}^{N_1 \\times n_2 \\times n_3}\\), its t-SVD is defined as\n\\[\\mathcal{X} = \\mathcal{U} * \\mathcal{S} * \\mathcal{V}^T,\\]\nwhere \\(\\mathcal{U} \\in \\mathbb{R}^{N_1 \\times n_1 \\times n_3}\\) and \\(\\mathcal{V} \\in \\mathbb{R}^{N_2 \\times n_2 \\times n_3}\\) are orthogonal tensors, \\(\\mathcal{S} \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}\\) is an f-diagonal tensor, whose frontal slices are all diagonal matrices, and \"*\" denotes the t-product.\n**Definition 2 (t-SVD based tensor nuclear norm [36]).** Given a tensor \\(\\mathcal{X} \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}\\), its t-SVD based tensor nuclear norm is defined as\n\\[|\\|\\mathcal{X}\\|\\|_* = \\sum_{k=1}^{n_3} ||\\mathcal{X}^{(k)}||_* = \\sum_{k=1}^{n_3} \\sum_{i=1}^{min(n_1, n_2)} \\delta_i(\\mathcal{X}^{(k)}),\\]\n#### B. Multi-view Matrix Factorization\nGiven the data from m views \\(\\{X^v\\}_{v=1}^m\\), in which \\(X^v \\in \\mathbb{R}^{d_v \\times n}\\) represents the v-th view with \\(d_v\\) feature dimensions and n samples. MultiMF tries to obtain a consensus latent representation G for all distinct views. The general objective function of MultiMF can be described as\n\\[\\min_{W^U, G^U} \\sum_{v=1}^m ||X^v - W^vG^v||_F^2 + \\alpha T(W^v) + \\lambda R(G, G^v).\\]"}, {"title": "III. THE PROPOSED METHOD", "content": "To address the above challenges, we propose a novel fast MVC approach called DSTL in this section. Then, we present the optimization algorithm and analyze the algorithm's computational complexity. The overall framework of our DSTL is shown in Fig. 1.\n#### A. Model Formulation\nIn modern multi-view applications, where data tends to be both massive (i.e., \\(n_r\\) is very large) and high-dimensional (i.e., \\(d_r\\) is very large), posing a great challenge to efficient data clustering analysis [42]. Similarity graph based methods usually explore the similarity relations between pairwise data points for spectral clustering, making them unscalable to large data. Matrix factorization provides an effective solution to it, which directly obtains the latent low-dimensional representation for downstream tasks. As illustrated in Eq. (1), most previous MultiMF based approaches learn a latent representation for each view and fuse them into a single one. However, such a strategy ignores the potentially irrelevant"}, {"title": "B. Optimization", "content": "To optimize the objective function involving multiple variables, we adopt an alternate optimization strategy. Specifically, we update one variable in each step while keeping the others fixed.\n**Updating Wv:** When other variables are fixed, the optimization problem w.r.t. \\(W^v\\) can be formulated as follows:\n\\[W_{t+1}^v = \\arg \\min_{W^v} \\sum_{v=1}^m ||X^v - W^v(S^v + H^v)||_F^2\\]\ns.t. \\((W^v)^T W^v = I_k\\).\nBy transforming the Frobenius norm to the trace and eliminating terms irrelevant to \\(W^v\\), the above formula can be equivalently reformulated as:\n\\[\\max_{W^v} Tr(W^vM^v) \\quad s.t. (W^v)^T W^v = I_k,\\]\nwhere \\(M^v = X^v(S^v + H^v)^T\\). This subproblem can be efficiently solved by Theorem 1.\n**Theorem 1.** Let \\(B = U\\Sigma V^T\\) be the singular value decomposition (SVD) of a matrix B. The optimal solution to the problem\n\\[\\max_{A^v} Tr(A^vB) \\quad s.t. (A^v)^T A^v = I,\\]\nis given by \\(B^+ = VU^T\\).\n*Proof.* Substituting \\(B = U\\Sigma V^T\\) into Eq. (10), we get\n\\(Tr(AB) = Tr(AUEV^T) = Tr(\\Sigma V^T AU)\\\n= Tr(\\Sigma O) = \\sum_{i} S_{ii}O_{ii},\\)\nwhere \\(O = V^T AU\\). It can be easily verified that \\(OO^T = I\\). Thus, we have -1 < Oii < 1, and then\n\\[Tr(AB) = \\sum_{i} S_{ii}O_{ii} \\le \\sum_{i} S_{ii}.\\]\nWithout of generality, let \\(O = V^TAU = I\\), then \\(B = VU^T\\), and Tr(AB) attains its maximum. This completes the proof.\n**Updating Cv:** When other variables are fixed, the optimization problem w.r.t. \\(C^v\\) is\n\\[C_{t+1}^v = \\arg \\min_{C^v} \\sum_{v=1}^m ||H^v - C^vY_t||_F^2 \\quad s.t. (C^v)^T C^v = I_k.\\]\nSimilar to the \\(W^v\\) problem, Eq. (13) is converted to\n\\[\\max_{C^v} Tr(C^vN) \\quad s.t. (C^v)^T C^v = I_k,\\]\nwhere \\(N = H^vY_t^T\\). As in solving Eq. (9), the optimal solution for \\(C^v\\) can be obtained by Theorem 1.\n**Updating S:** When other variables are fixed, the optimization problem w.r.t. S is\n\\[S_{t+1} = \\arg \\min_{S} \\lambda_1 ||S||_1 + ||S - P_t||_F^2,\\]"}, {"title": "Algorithm 1 DSTL algorithm", "content": "Input: Multi-view data matrices \\(\\{X^v\\}_{v=1}^m\\), parameters\n\\(\\lambda_1\\), \\(\\lambda_2\\), \\(\\lambda_3\\), and k.\nOutput: Perform k-means on Y.\n1: Initialize \\(W^v = 0, C^v = 0, Y = 0, S = H = 0\\), \\(\\epsilon = 1e-4\\).\n2: while not converged do\n3: \tUpdate \\(W^v\\) by solving (8);\n4: \tUpdate \\(C^v\\) by solving (13);\n5: \tUpdate S by Eq. (17);\n6: \tUpdate H by solving. (18);\n7: \tUpdate Y by solving (21);\n8: \tCheck the convergence conditions:\n||Yt - Yt-1||/||Yt-1|| \u2264 \\(\\epsilon\\);\n9: \tt \\(\\leftarrow\\) t + 1;\n10: end while\n11: return The alignment indicator matrix Y.\nwhere \\(P_t = P(P_1, ..., P_m)\\) is constructed by\n\\[P^v = W^vX^v - H^v.\\]\nEq. (15) can be solved by\n\\[S_{t+1}^{v} = D_{\\lambda_1}(P_{t}^v),\\]\nwhere \\(D_{\\gamma}\\) is the soft-threshold operator [44] defined as\n\\([D_{\\gamma}(A)]_{ij} = sign (a_{ij}) \\cdot \\{max(|a_{ij}| - \\gamma,0)\\} \\).\n**Updating H:** When other variables are fixed, we can update H by working out the following subproblem\n\\[H_{t+1} = \\arg \\min_{H} \\lambda_2 ||H||_* + (\\lambda_3 + 1)||H - Q_t||_F^2,\\]\nwhere \\(Q_t = P(Q_1, ..., Q_m)\\) is constructed using the following equation:\n\\[Q^v = \\frac{1}{\\lambda_3 + 1}(W^vX^v - S_{t+1}^v) + \\frac{\\lambda_3}{\\lambda_3 + 1}C_{t+1}^vY_t.\\]\nEq. (18) is a typical low-rank tensor norm minimization problem that has a closed-form solution and can be easily solved by the following Theorem 2 [45].\n**Theorem 2.** Given two tensor \\(K \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}\\) and \\(L \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}\\) with a constant \\(\\rho\\), the globally optimal solution of the problem\n\\[\\min_{K} \\rho||K||_* + \\frac{1}{2}||K - L||_F^2\\]\ncan be obtained by the tensor tubal-shrinkage operator\n\\[K = C_{n_3 \\rho}(L) = U * C_{n_3 \\rho}(Z) * V^T,\\]\nwhere \\(L = U * Z * V^T\\) and \\(C_{n_3 \\rho}(L) = Z * D\\). \\(D \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}\\) is a f -diagonal tensor and its diagonal element in the Fourier domain is \\(D_f(i,i,j) = (1 - \\frac{\\rho}{\\sigma_{ii}^{(j)}})_+\\)."}, {"title": "C. Computational Complexity Analysis", "content": "1) **Space Complexity:** The major memory costs of our algorithm are variables \\(X^v \\in \\mathbb{R}^{d_v \\times n}\\), \\(W^v \\in \\mathbb{R}^{d_v \\times k}\\), \\(S^v \\in \\mathbb{R}^{k \\times n}\\), \\(H^v \\in \\mathbb{R}^{k \\times n}\\), \\(C^v \\in \\mathbb{R}^{k \\times k}\\), and \\(Y \\in \\mathbb{R}^{k \\times n}\\). Therefore, the space complexity of Algorithm 1 is linear to the number of samples, i.e., O(n).\n2) **Time Complexity:** The most time-consuming steps for updating \\(W^v\\) and \\(C^v\\) are the SVD operation and matrix multiplication, which require \\(O(k^2d + kdn)\\) and \\(O(k^3 + k^2n)\\), respectively. When updating S and Y, the primary time cost is matrix multiplication, which takes O(kdn) and \\(O(k^2n)\\), respectively. In terms of updating H, we mainly consider the computational complexity of matrix multiplication, FFT, inverse FFT, and SVD operations. It takes \\(O(kdv n + k^2n)\\) in matrix multiplication. For a k \\(\\times\\) m \\(\\times\\) n tensor, it takes O(kmnlog(n)) to conduct FFT and inverse FFT operations, and needs \\(O(knm^2)\\) for SVD operation. Since k and m are much smaller than n and \\(d_v\\), the overall computational complexity of Algorithm 1 is \\(O(\\tau(kmnlog(n) + kdn))\\), where \\(d = \\sum_{v=1} d_v\\), \\(\\tau\\) is the number of iterations. In addition, the post-processing of the k-means step has linear complexity to n. However, most existing tensor-based MVC approaches often have \\(O(n^2log(n))\\) at the FFT stage and \\(O(n^3)\\) at the spectral clustering stage. Therefore, the DSTL reduces the complexity of tensor-based MVC methods and is scalable to large-scale datasets."}, {"title": "D. Convergence Analysis", "content": "When considering all the variables jointly, the overall objective function is non-convex, but the convergence of each sub-problem in Algorithm 1 can be guaranteed, and each closed-form solution reduces the objective function value. Let \\((\\Omega_t) = \\Omega(W^v, C^v, S, H, Y_t)\\) be the objective function value after the t-th iteration, and we can conclude \\((\\Omega_t) = \\Omega(W^v, C^v, S, H, Y_t) \\ge \\Omega(W_{t+1}^v, C^v, S, H, Y_t) \\ge \\Omega(W_{t+1}^v, C_{t+1}^v, S, H, Y_t) \\ge \\Omega(W_{t+1}^v, C_{t+1}^v, S_{t+1}, H, Y_t) \\ge \\Omega(W_{t+1}^v, C_{t+1}^v, S_{t+1}, H_{t+1}, Y_t) \\ge \\Omega(W_{t+1}^v, C_{t+1}^v, S_{t+1}, H_{t+1}, Y_{t+1})\\). Since the overall objective function value is lower-bounded, it can converge to a local minimum eventually."}, {"title": "IV. EXPERIMENT", "content": "#### A. Experimental Setup\n**Datasets:** Nine popular datasets from diverse applications were used to validate our DSTL, consisting of text datasets NGs\u00b9 and BBCSport, digit dataset HW\u00b2, scene datasets Scene15 [47] and MITIndoor [48], video dataset CCV [17], animal dataset Animal\u00b3, as well as object datasets Caltech101-all4 and NUSWIDEOBJ [49]. More details are shown in Table I."}, {"title": "B. Experimental Results", "content": "Table II provides a comprehensive overview of the clustering results for all methods across 9 datasets. The best and second-best results are highlighted using **bold** and *underlined* values, respectively. \"OM\" denotes unavailable results due to out-of-memory errors. Based on the results, we can draw the following conclusions:\n(1) DSTL outperforms the baselines across most datasets, with particularly notable improvements observed on the CCV and Animal datasets. For example, on the CCV dataset, DSTL achieves enhancements of 29.06%, 36.06%, 41.78%, 39.89%, and 33.06% over the second-best method t-SVD-MSC across five evaluation metrics. It can be attributed to the presence of considerable semantic-unrelated information in these datasets, which adversely affects the clustering performance of the baseline methods. However, DSTL effectively disentangles the latent features, mitigating the negative impact of such irrelevant information and resulting in superior clustering performance, demonstrating the efficiency and advantages of our feature disentanglement strategy.\n(2) Both our proposed DSTL and other tensor-based methods (i.e., t-SVD-MSC, ETLMSC, TBGL and LTBPL) consistently outperform non-tensor approaches (i.e., AWMVC, FDAGF, FSMSC, and UDBGL) on the first six datasets. This is primarily due to the better capacity of tensor regulaization in leveraging complementary information and the capturing high-order correlations among multiple views, which non-tensor methods lack.\n(3) The DSTL outperforms the majority of the other tensor-based methods on the first six datasets. This is attributed to its unique ability to consider both disentangled semantically unrelated and related features. By integrating a consensus alignment indicator into the disentanglement process, DSTL optimizes the clustering outcomes by ensuring that the extracted features are aligned across multiple views, leading to superior performance.\n(4) Additionally, compared to other tensor-based methods that stack affinity graphs as tensors, DSTL offers a significant advantage in terms of memory efficiency. Traditional tensor stacking approaches often lead to high space complexity of O(n\u00b2), resulting in memory errors on large-scale datasets. However, DSTL directly operates on latent low-dimensional features, which have a linear space complexity of O(n), making it more memory-efficient and scalable to larger datasets."}, {"title": "C. Parameter Analysis", "content": "In this section, we investigate the impact of different parameters in DSTL (i.e., \\(\\lambda_1, \\lambda_2, \\lambda_3\\), and k) on clustering performance and examine their interrelationship across the NGS, BBCSport, HW, and NUSWIDEOBJ datasets. Specifically, we first fix \\(\\lambda_3\\) and k, then analyze the influence of \\(\\lambda_1\\) and \\(\\lambda_2\\) on the ACC. The results are depicted in Fig. 2. Notably, we observe that DSTL exhibits higher sensitivity to variations in \\(\\lambda_1\\) compared to \\(\\lambda_2\\). This phenomenon can be attributed to the fact that \\(\\lambda_1\\) encodes the regularization"}, {"title": "D. Convergence and Time Comparison", "content": "1) **Convergence Analysis:** As illustrated before, the objective value of DSTL decreases monotonically with variables alternate update and the objective function is lower-bounded. In this subsection, we experimentally demonstrate the convergence property of the optimization algorithm for Algorithm 1. We set the convergence condition as Loss(Y) = ||Yt - Yt-1||/||Yt-1|| \\le 1e \u2013 4. Fig. 4 displays the change in Loss(Y) and ACC values w.r.t. iterations"}, {"title": "E. Ablation Study", "content": "To assess the effectiveness of multi-view features disentanglement and tensor-based regularization, as well as the consensus semantic alignment indicator learning, we derive three variants: DSTL-S, DSTL-T, and DSTL-Y.\n(1) DSTL-S drops the semantic-unrelated features learning and just focus on the semantic-related representations. The loss function of DSTL-S is\n\\[\\min_{\\Omega} \\sum_{v=1}^m ||X^v - W^vH^v||_F^2 + \\lambda_2||H||_* + \\lambda_3 \\sum_{v=1}^m ||H^v - C^vY||_F^2 \\quad s.t. \\Omega,\\]\n(2) DSTL-T dose not construct two silm tensor and utilizes traditional nuclear norm to replace tensor nuclear-norm regularization:\n\\[\\min_{\\Omega} \\sum_{v=1}^m ||X^v - W^v(S^v + H^v)||_F^2 + \\lambda_1 ||S^v||_1\\\\\n+ \\lambda_2 ||H||_* + \\lambda_3 ||H^v - C^vY||_F^2.\\]\n(3) DSTL-Y omits the consensus alignment indicator and does not use it to guide the disentanglement process:\n\\[\\min_{\\Omega} \\sum_{v=1}^m ||X^v - W^v(S^v + H^v)||_F^2 + \\lambda_1||S||_1 + \\lambda_2||H||_*.\\]\nSimilar constraints are imposed on the three variants as those in our DSTL. Fig. 5 shows the t-SNE visualizations [53] of learned consensus alignment indicator embedding Y obtained by DSTL-S and DSTL on BBCSport, HW, Scene15, and Animal datasets. It can be observed that when using DSTL-S to obtain Y, there are some clusters entangled with each other in the four datasets. However, we can see our DSTL is more robust and can achieve excellent performance in BBCSport and HW datasets when the negative influence of disentangled semantic-unrelated information is considered.\nFig. 6 shows the clustering performance of the three variants and our DSTL on nine datasets. We can observe that the performance of DSTL-S is not desirable on all datasets, eapecially on MITIndoor, CCV, and Animal. We know that there are considerable semantic-unrelated information in these datasets, which adversely affects the clustering performance. These results prove the effectiveness of the disentangled semantic-unrelated features learning once again. When dropping the tensor-based regularization, the clustering results of DSTL-T are notably inferior to DSTL, which demonstrates the effctiveness of utilizing tensor learning to explore high-order correlations. In NUSWIDEOBJ, it is evident that DSTL can achieve competitive results compared to DSTL-Y, confirming the effectiveness of learning consensus semantic alignment indicator to help align semantic-related representation across views, then guiding the process of"}, {"title": "V. CONCLUSION", "content": "This paper proposes a novel fast tensor-based MVC method named disentangled slim tensor learning (DSTL), which integrates multi-view features disentanglement and slim tensor learning into a unified framework. To mitigate the negative influence of feature redundancy, DSTL disentangles the features of each view to learn both semantic-unrelated and semantic-related representations. It then constructs two slim tensors to alleviate the negative impact of semantic-unrelated information while also capturing the high-order consistency of different views. Additionally, the semantic-related representations are aligned across views to guide the disentanglement process by learning a consensus semantic alignment indicator. DSTL effectively reduce the space and time complexity compared with traditional tensor-based methods. Experimental results demonstrate the effectiveness and efficiency of our approach. One limitation of our method is that DSTL may struggle with incomplete multi-view data due to missing samples in practice. To address this, we will consider incorporating some recovery and completion learning strategies [54], [55] in future work."}]}