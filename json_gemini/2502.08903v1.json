{"title": "3D-Grounded Vision-Language Framework for\nRobotic Task Planning: Automated Prompt\nSynthesis and Supervised Reasoning", "authors": ["Guoqin Tang", "Qingxuan Jia", "Zeyuan Huang", "Gang Chen", "Ning Ji", "Zhipeng Yao"], "abstract": "Vision-language models (VLMs) have achieved re-\nmarkable success in scene understanding and perception tasks,\nenabling robots to plan and execute actions adaptively in dynamic\nenvironments. However, most multimodal large language models\nlack robust 3D scene localization capabilities, limiting their\neffectiveness in fine-grained robotic operations. Additionally,\nchallenges such as low recognition accuracy, inefficiency, poor\ntransferability, and reliability hinder their use in precision tasks.\nTo address these limitations, we propose a novel framework that\nintegrates a 2D prompt synthesis module by mapping 2D images\nto point clouds, and incorporates a small language model (SLM)\nfor supervising VLM outputs. The 2D prompt synthesis module\nenables VLMs, trained on 2D images and text, to autonomously\nextract precise 3D spatial information without manual interven-\ntion, significantly enhancing 3D scene understanding. Meanwhile,\nthe SLM supervises VLM outputs, mitigating hallucinations and\nensuring reliable, executable robotic control code generation.\nOur framework eliminates the need for retraining in new\nenvironments, thereby improving cost efficiency and operational\nrobustness. Experimental results that the proposed framework\nachieved a 96.0% Task Success Rate (TSR), outperforming other\nmethods. Ablation studies demonstrated the critical role of both\nthe 2D prompt synthesis module and the output supervision\nmodule (which, when removed, caused a 67% TSR drop). These\nfindings validate the framework's effectiveness in improving 3D\nrecognition, task planning, and robotic task execution.", "sections": [{"title": "I. INTRODUCTION", "content": "HE integration of robotics and artificial intelligence\n(AI) has catalyzed significant advancements in the au-\ntonomous execution of complex tasks, marking a new era\nof embodied intelligence where robots are not only capable\nof physical interactions but also capable of reasoning and\ndecision-making through multi-modal data fusion and intel-\nligent planning systems [1], [2]. In industrial automation,\nrobotic systems have demonstrated exceptional efficiency, pre-\ncision, cost-effectiveness, and safety, enabling diverse appli-\ncations such as industrial equipment installation, precision\ninstrument assembly, and logistics automation, forming the\nbackbone of modern smart manufacturing processes [3], [4].\nHowever, achieving robust autonomy in complex environments\npresents one fundamental challenge: reasonable task planning.\nTask planning, as stated in [5], encompasses environmental\nperception, task understanding, and action planning, resulting\nin executable sequences of robotic actions that achieve speci-\nfied objectives. Common perception techniques [6], ranging\nfrom image-based [7] and point cloud-based [8] methods\nto multimodal fusion [9], provide essential environmental\nmodeling and serve as key constraints for subsequent planning\nsteps. However, traditional approaches, such as rule-based\nmethods and state-transition strategies, rely heavily on expert\nknowledge, predefined robot states, and fixed interaction rules\n[5]. This dependency limits their adaptability and scalability\nin complex, uncertain environments, prompting researchers to\nexplore alternative strategies [10].\nBuilding on these explorations, recent work has investigated\nmultimodal fusion [11], [12] for richer environmental repre-\nsentation and large-scale models [13]-[15] for more flexible,\ndata-driven task planning. In these frameworks, perception is\nnot an isolated step but an integral component of the planning\npipeline, providing critical spatial and semantic cues that guide\nsubsequent reasoning. Yet, despite these efforts, significant\nchallenges persist in bridging the gap between high-level com-\nmands and actionable robot behaviors. For example, consider"}, {"title": "II. RELATED WORK", "content": "Achieving robust task planning in robotics requires ad-\ndressing two interrelated challenges: precise 3D spatial un-\nderstanding and the effective utilization of large pre-trained\nmodels. Precise 3D perception provides the geometric and\nspatial foundation necessary for fine-grained operations, such\nas grasping and trajectory planning. However, integrating these\nperceptual insights into large models, such as Vision-Language\nModels (VLMs) and Large Language Models (LLMs), is\nequally critical to ensure task-relevant outputs and dynamic\nadaptability. This section reviews progress in these areas,\nfocusing on multimodal perception and model adaptation\nstrategies, and highlights their limitations."}, {"title": "A. Multimodal Perception for Robotic Operations", "content": "Multimodal perception enhances 3D scene understanding by\ncombining RGB, depth, and textual inputs. Current approaches\ncan be broadly categorized into End-to-End Models and\nModular Architectures. End-to-End Models, for example\n\"RT series\" [24], [25] and \"RobotFlamingo\" [26], employ\ntransformer-based frameworks to encode sensory inputs into\nunified cross-modal embeddings, enabling robust multimodal\nreasoning. These methods excel in learning rich representa-\ntions but often lack geometric fidelity, struggle with task-\nspecific adaptation, and require costly retraining on large\nannotated datasets, making them less effective in dynamic\nenvironments.\nModular architectures decouple perception and reasoning\nprocesses, leveraging pre-trained VLMs to reduce computa-\ntional demands. For example, CLIP-based methods extract\nsemantic features, while auxiliary modules handle spatial rea-\nsoning [27], [28]. Another approach integrates Reinforcement\nLearning (RL) modules to generate desired outputs directly\n[29]. Despite their flexibility, these architectures face signif-\nicant challenges in multimodal data fusion, particularly in\naddressing sensor uncertainties and prioritizing task-relevant\nfeatures in cluttered or dynamic scenarios. These limitations\nhighlight the need for adaptive frameworks capable of inte-\ngrating 3D perception with dynamic task planning."}, {"title": "B. Large Pre-trained Models In Robotic Tasks", "content": "Large pre-trained models (VLMs and LLMs) have emerged\nas powerful tools for robotic task planning [30] and navigation\n[31] through their multimodal reasoning capabilities [32].\nTheir integration into robotics employs two complementary\napproaches: prompt engineering that steers models through"}, {"title": "III. METHODOLOGY", "content": "This paper introduces a modular framework (Fig. 2) to\naddress the limitations of Vision-Language Models (VLMs) in\nfine-grained robotic task planning. The proposed framework\nenhances spatial reasoning, logical consistency, and adapt-\nability by integrating three complementary modules, enabling\nprecise and reliable execution of complex robotic tasks."}, {"title": "A. VLM-Based Robotic Task Planning Framework", "content": "The proposed framework leverages a frozen Vision-\nLanguage Model (VLM) pre-trained on large-scale datasets,"}, {"title": "B. Confidence-Based Registration Strategy", "content": "Accurate multimodal data fusion is essential for robotic\nperception systems, yet it remains challenging due to segmen-\ntation errors, sensor noise, and spatial misalignment. These\nuncertainties are especially detrimental in dynamic environ-\nments or tasks requiring high spatial accuracy. To address these\nissues, we propose a confidence-based registration strategy that\nevaluates and prioritizes spatial points based on an entropy-\nguided probabilistic framework. This strategy ensures that only\nreliable information contributes to downstream tasks, such as\nobject localization and trajectory planning."}, {"title": "1) Mathematical Definition of Confidence Score", "content": "The con-\nfidence score Ci quantifies the reliability of each spatial point\nxi, integrating multiple sources of uncertainty into a unified\nprobabilistic measure:\nCi = exp(- \\sum_{n=1}^{K} \\lambda_n \\cdot H_n(x_i) ), (1)\nwhere Hn(xi) represents the normalized entropy capturing\nuncertainty in the n-th dimension, An is the task-specific\nweighting coefficient reflecting the importance of the n-th di-\nmension, and K is the total number of uncertainty dimensions.\nThis formulation draws from information theory, where\nentropy quantifies the level of uncertainty in a probability\ndistribution. By exponentially scaling the summed entropies,\nthis model ensures that points with higher uncertainty have sig-\nnificantly lower confidence scores, prioritizing reliable points\nfor fusion and downstream tasks."}, {"title": "2) Entropy Components and Their Physical Significance", "content": "The confidence score integrates four entropy components, each\naddressing distinct uncertainty sources. These components are\nnormalized to the range [0,1] to ensure uniform scaling and\ncompatibility across heterogeneous data modalities."}, {"title": "a) Spatial Consistency Entropy H\u2081(xi)", "content": "This component\nmeasures the deviation of a point xi from the centroid of its\nsegmentation mask ck. Using the 2D Euclidean distance:\nd2D(xi) = ||Pi - Ck ||2, (2)\nwhere pi is the 2D position of xi. The normalized probability\nand entropy are:\nP1(xi) = \\frac{1}{1+d_{2D}(x_i)}, H1(xi) = -P1(xi) log(P\u2081 (xi)). (3)\nPhysically, H\u2081(xi) quantifies the consistency of xi within its\nsegmentation context, penalizing outliers far from the centroid."}, {"title": "b) Geometric Consistency Entropy H2(xi)", "content": "Geometric\nconsistency evaluates the deviation of a point x, from the local\npoint cloud structure using the Mahalanobis distance:\nDM(xi) = (x \u2212 \u03bc)\u03a4\u03a3\u22121(x \u2212 \u03bc), (4)\nwhere \u03bc and \u2211 represent the local mean and covariance\nmatrix. The normalized probability and entropy are:\nP2(xi) = \\frac{DM(x_i)}{D_{max}}, H2(xi) = -P2(xi) log(P2(xi)). (5)"}, {"title": "c) Depth Measurement Entropy H3(xi)", "content": "Depth uncer-\ntainty is modeled using local depth variance 02:\nP3(Xi) = \\frac{\\sigma_{max}^2}{\\sigma^2},,\nH3(xi) = -P3(xi) log(P3(xi)). (6)\nHere, H3(xi) reflects the reliability of depth measurements,\npenalizing inconsistent or noisy depth readings."}, {"title": "d) Temporal Stability Entropy H4(xi)", "content": "In dynamic envi-\nronments, temporal stability measures the variability of a point\nxi across frames:\nSi = \\frac{1}{T-1} \\sum_{t=1}^{T-1} |\\frac{x_i^{(t)} - x_i^{(t-1)}|}{D_{max}} |, (7)\nwhere |x_i^{(t)} - x_i^{(t-1)}| represents the displacement between\nconsecutive frames. The normalized probability and entropy\nare:\nP4(xi) = \\frac{Si}{Si+1}, H4(xi) = -P4(xi) log(P4(xi)). (8)\nThis entropy component penalizes points exhibiting high tem-\nporal instability, ensuring that only stable points are priori-\ntized."}, {"title": "3) Task-Driven Weight Allocation", "content": "The weighting coeffi-\ncients An are tailored to specific tasks, prioritizing relevant\nuncertainty dimensions based on operational requirements:\n\u2022 High spatial resolution tasks: Emphasize H\u2081(xi) and\nH2(xi) for precise spatial alignment.\n\u2022 Dynamic environments: Prioritize H4(xi) for motion\ntracking and stability.\n\u2022 Cluttered environments: Adjust 3 to suppress noisy\ndepth information.\nFor instance, in robotic navigation, temporal stability\n(H4(xi)) is critical for maintaining trajectory coherence, while\nprecision assembly tasks emphasize spatial (H1(xi)) and ge-\nometric (H2(xi)) consistency."}, {"title": "4) Integration with Downstream Tasks", "content": "The confidence-\nbased registration strategy ensures that high-confidence points\ncontribute to downstream processes, such as trajectory plan-\nning and control generation. By systematically integrating\nmultiple dimensions of uncertainty, this approach enhances the\nrobustness and reliability of multimodal data fusion, signifi-\ncantly improving robotic perception and decision-making in\ncomplex environments."}, {"title": "C. 2D Prompt Synthesis and Textual Prompt Design", "content": "This subsection presents a framework(fig 4) that integrates\n2D prompt synthesis with textual prompt design to enhance\nthe Vision-Language Model (VLM)'s spatial reasoning capa-\nbilities. By embedding reliable 3D spatial information into 2D\ninputs and dynamically refining textual prompts, the frame-\nwork constrains the high-dimensional space of VLM encoding,\nensuring accurate outputs for robotic control tasks."}, {"title": "1) Nearest Neighbor Selection Strategy", "content": "The nearest neigh-\nbor strategy is designed for tasks requiring quick responses\nwith moderate precision. It embeds selected 3D points into 2D\ninputs, reducing visual redundancy while preserving essential\nspatial details. For each segmentation mask Mk, the centroid\nCk in the 2D plane is computed as:\nCk = \\frac{1}{|M_k|} \\sum_{p \\in M_k} P (9)\nwhere Mk is the number of pixels in Mk, and p = (x,y)\nrepresents pixel coordinates.\nUsing the nearest neighbor (NN) algorithm, candidate 3D\npoints Nk near ck are identified:\nNk = {p \u2208 P | p \u2208 NN4(Ck)}, (10)\nwhere P is the point cloud. A confidence score C(p) deter-\nmines the most reliable point:\nP = arg max C(p).\nPENK (11)\nThe selected point pris annotated onto the 2D image,\nproviding critical spatial cues to the VLM.\nThis process constrains the high-dimensional space of VLM\nencoding by incorporating key 3D spatial information into the\nimage vector v1, which refines the probability distribution of\nthe output vector vo:\np(vo | VI)\u2192p(vo | VI, P). (12)\nThe reduced uncertainty allows the decoder to generate more\nprecise robotic control commands."}, {"title": "2) Iterative Multi-Step Prompting Strategy", "content": "This strategy\nsystematically refines spatial understanding and task precision\nthrough recursive visual-linguistic processing. It integrates\nmultimodal inputs (such as images and textual descriptions)\nwith iterative updates to the task prompt. The process consists\nof the following stages:\nInitial Encoding and Prompt Formulation:\nThe initial task prompt T(1) is generated by combining\nthe input image I with the corresponding textual description\nD, along with a predefined text template T that specifies\nthe output format and interaction guidelines for the Vision-\nLanguage Model (VLM). This multimodal information is\nmapped into a task vector vr within a high-dimensional space\nV, as shown below:\nT(1) : (I,D,T) \u2192 VT, VT \u2208 V\nHere, I is the input image, D is the associated textual\ndescription (e.g., task instructions or goals), and T is the\npredefined text template that outlines the expected output\nformat, enabling the VLM to generate responses that include\nself-evaluation and check for the need for further information."}, {"title": "D. Supervisory Feedback via SLM", "content": "To address hallucinations and logical inconsistencies in\nVision-Language Models (VLMs) for robotic control tasks,\nthis study proposes a supervisory feedback mechanism based\non a Small Language Model (SLM). The mechanism ensures\nthe precision, consistency, and safety of control commands\nthrough iterative optimization. In practice, we identify three\nprimary types of errors: parameter errors, logical errors, and\nconstraint violations, which involve control values exceeding\noperational ranges, task execution sequence disorders, and\nviolations of safety and task-specific conditions, respectively.\nWe specifically train the SLM to retain this critical information\nand validate key parameters in the VLM's output, such as joint\nangles, end-effector positions, and grip forces, ensuring logi-\ncal consistency and adherence to environmental constraints.\nThrough continuous queries and verification processes, the\nSLM leverages its knowledge base to guide the VLM in\ncorrecting its outputs, thereby enhancing task success rates.\nThe overall architecture of the supervisory system is illus-\ntrated in Fig. 6, revealing the feedback loops and information\nflow between system components. Subsequent sections will\ndiscuss in detail the following four crucial aspects: \"Model\nFinetuning\", \"SLM Prompt Template Design\", \"Supervi-\nsion Strategy\" and \"SLM-VLM Interaction Framework\",\nall of which are essential for constructing the supervisory\nsystem."}, {"title": "1) Domain-Specific Model Fine-Tuning", "content": "In practical oper-\national tasks, errors in VLM outputs can be categorized into\nthree primary defined as follows:\n\u2022 Parameter Errors: These occur when control val-\nues exceed operational ranges (e.g., joint angles 0\n[0min, Omax]).\n\u2022 Logical Errors: These pertain to violations of task se-\nquence constraints (e.g., releasing an object before grasp\ncompletion).\n\u2022 Constraint Violations: These refer to breaches of safety\nor task-specific conditions (e.g., end-effector colliding\nwith a barrier).\nTo optimize the supervisory capabilities of the SLM, we\ntrain it specifically to recognize and address these errors.\nThis training involves fine-tuning the SLM using a curated\ndataset that includes examples of these error types, along with\ncorresponding corrections and confidence scores."}, {"title": "a) Dataset", "content": "The fine-tuning process leverages a dataset\nD constructed as follows:\nD = {(xi, Yi, Ci)}=1 (18)\nwhere xi represents input commands, Yi denotes correct\noutputs, and ci indicates confidence scores. It includes four\ncritical components:\n\u2022 Natural-language task descriptions with validated control\nparameters\n\u2022 Expert-annotated correction examples with confidence\nscores\n\u2022 Historical execution logs with success/failure cases\n\u2022 Task-specific constraints and safety thresholds"}, {"title": "b) Fine-Tuning Method", "content": "The fine-tuning process lever-\nages Low-Rank Adaptation (LoRA), which introduces train-\nable matrices A \u2208 Rr\u00d7d and B\u2208 Rd\u00d7r with rank r into the\npre-trained weights W, as shown below:\nW+BA = W + AW (19)\nThis parameter-efficient adaptation balances domain speci-\nficity and computational efficiency [42]. The rank r controls\nthe adaptation capacity, with lower values preserving more\noriginal knowledge. During fine-tuning, a cross-entropy loss\nfunction is employed to minimize prediction errors while\npreserving logical coherence in task outputs. Critical hyper-\nparameters, such as rank and learning rate, are empirically\noptimized."}, {"title": "2) SLM Prompt Template for SLM-VLM Interaction", "content": "To standardize the supervision process, we define the SLM\nPrompt Template structure, see TABLE I. This template\nprovides necessary information to the SLM for generating\naccurate feedback and adjustment suggestions. Details are\npresented in Section IV."}, {"title": "3) Supervision Strategies for SLM-VLM Interaction", "content": "To\nensure stability and convergence within the feedback loop, the\nSLM employs two complementary strategies:\na) Single-Dimension Adjustments: This strategy focus\neach iteration on refining one parameter or logical step at\na time, thereby avoiding interference among interdependent\nvariables:\nApn) = dj,i(n) f(en)) (20)\nwhere i(n) denotes the parameter selected for adjustment in\nthe n-th iteration, pj represents the j-th parameter, and ej is its\nassociated error. This multi-stage optimization ensures mono-\ntonic improvement through incremental parameter updates."}, {"title": "b) Feedback History Tracking", "content": "This strategy maintains a\nrecord of past adjustments, preventing redundant or contradic-\ntory corrections:\nH(n) = {p{*) | k = 1,...,n-1} (21)\nHere, H(n) represents the feedback history. Given that the\nSLM is a small model, it is solely responsible for adjusting\nthe VLM's outputs in complex tasks. By caching historical ad-\njustments, the SLM can quickly provide optimal feedback for\ntasks it has previously corrected, thereby enhancing efficiency."}, {"title": "c) Implementation Example", "content": "Consider a VLM-generated\ncommand for robotic manipulation:\n\"Apply 15N grip force on fragile component\"\nThe SLM identifies a safety violation (max 5 N threshold) and\ngenerates corrective feedback:\n\"A grip force of 15N exceeds the safety threshold\nfor fragile components (maximum allowable force:\n5 N). Please adjust the grip force to at most 5 N and\nreattempt the grasping action while maintaining the\ncurrent approach vector and speed.\"\nThis feedback is reintegrated into the text prompt helping\nVLM to regenerate commands until either the commands\nsatisfy all task requirements or a predefined iteration limit\nNmax is reached. If convergence remains unattainable after\nNmax iterations, a fallback mechanism ensures task safety by\neither reverting to a default safe state or requesting human\nintervention, depending on the task's criticality level."}, {"title": "4) SLM-VLM Interaction Framework", "content": "The interaction\nbetween the Small Language Model (SLM) and Vision-\nLanguage Model (VLM) constitutes a closed-loop feedback\nsystem for iterative refinement of robotic control commands.\nThis dual-model approach enables robust command generation\nthrough continuous validation and refinement. See Algorithm2.\na) System Formalization: The interaction is formalized\nas a state transition system S = (X,U, F), where X C Rn\nstands for the state space of possible commands, U \u2286 RM\nis the feedback space and F:X\u00d7U \u2192 X refers to state\ntransition function."}, {"title": "b) Interaction Components", "content": "The SLM generates feed-\nback Y(n) at iteration n:\nY(n) = {flag(n), C(n), I(n), S(n), p(n)}\nwhere Y(n) represents the feedback generated by the SLM\nin the n-th iteration, encompassing the acceptance flag, con-\nfidence score, detected issues, corresponding suggestions, and\nthe prompt for the VLM."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we evaluate the effectiveness of the proposed\nmodular framework in enhancing scene understanding and\ntask execution for robotic systems. We focus on assessing\nthe contributions of the fusion and supervision modules in\nterms of 3D spatial reasoning, task planning, and control code\ngeneration."}, {"title": "A. Experimental Setup", "content": "We conducted experiments using a FRANKA robotic arm\nequipped with a RealSense RGB-D camera and a LiDAR\nsensor. The robot performed a series of manipulation tasks in\nan indoor environment featuring moderate clutter and varying\nlighting conditions. Camera and LiDAR data were used for\nscene perception and task planning."}, {"title": "B. Prompt Template", "content": "In this experiment, prompt templates play a critical role in\nguiding the Vision-Language Model (VLM) and Small Lan-\nguage Model (SLM) to perform robotic task planning and val-\nidation. These templates standardize input-output interactions,\nensuring task-specific requirements are met while addressing\nlogical consistency and safety constraints. The VLM template\nfocuses on generating task steps from visual and textual inputs,\nwhile the SLM template supervises the VLM outputs and\nprovides necessary adjustments. The following sections detail\nthe configuration of these templates."}, {"title": "1) VLM Template", "content": "The Vision-Language Model (VLM)\nis tasked with generating task plans based on visual and\ntextual inputs. Its output directly influences the robotic actions,\nmaking it critical to standardize the input and output structure\nfor consistent performance. This template provides the VLM\nwith task descriptions, prior knowledge of the robot and the\nenvironment, and necessary constraints, ensuring safe and\nprecise task execution. See A."}, {"title": "2) SLM Template", "content": "The Small Language Model (SLM)\nensures that the VLM outputs adhere to logical consistency,\nsafety, and task-specific requirements. Acting as a validation\nlayer, the SLM identifies potential issues in the VLM outputs\nand provides corrective feedback to refine the task plan. The\ntemplate defines the input-output structure and specifies the\nfeedback format to guide the VLM iteratively. See B."}, {"title": "C. Output Examples", "content": "This section provides example outputs from the Vision-\nLanguage Model (VLM) and the Small Language Model\n(SLM) for a specific robotic task. These outputs illustrate\nhow the models interact and refine their responses during task\nexecution. The task is defined as follows:\nTask Description: Identify and remove a headphone\nfrom a headphone stand in the scene, and place\nit into the user's hand. Prompt Template: {PT},\nHistorical Responses: {H(n)}, VLM Output: Grasp\nthe headphone."}, {"title": "D. SLM Dataset Construction", "content": "To train the Small Language Model (SLM), we constructed\na dataset combining public datasets, custom experimental data,\nand augmented data. All raw data were processed to conform\nto the SLM's input-output format, ensuring compatibility with\nnext token prediction training."}, {"title": "1) Positive Data Collection", "content": "We utilized BridgeData V2\n[44], focusing on task descriptions and task decompositions\nwhile removing trajectory-related data. From this dataset,\nwe extracted 240 samples, representing 24 unique scenes,\nwith 10 frames uniformly sampled from each scene. Each\nsample includes a task description, task decomposition, and\nobject coordinates within the scene. Additionally, we collected\n320 task samples in a controlled environment featuring a\nrobotic arm, headphone stand, and headset. Object positions\nwere systematically varied to generate scene descriptions with\nprecise 3D coordinates relative to the robot base, along with\ncorresponding task decompositions."}, {"title": "2) Data Augmentation and Negative Data Generation", "content": "To diversify the dataset and simulate challenging scenarios,\nwe generated 3,000 augmented samples. The augmentation\nstrategies included:\n\u2022 Positive Data Augmentation:\nModifying task parameters to exceed operational\nthresholds (e.g., increasing grip force beyond safe\nlimits).\nRemoving or adding steps in task decomposition to\nsimulate incomplete or invalid sequences (e.g., omit-\nting the release step in a pick-and-place task).\n\u2022 Negative Data Generation:\nReversing task flows, such as swapping grasp and\nrelease actions.\nIntroducing parameter violations, such as invalid object\npositions outside the robot's workspace.\nSimulating occlusions by removing parts of the object's\nlocation data in the scene description.\nEach augmented sample was paired with its predefined error\ntype, and GPT-4 was tasked with generating feedback strictly\nbased on the given error and task information. This ensured\nthat the feedback aligned with the predefined errors while\nconforming to the SLM input-output format."}, {"title": "3) Data Processing and Final Dataset Composition", "content": "All\ncollected and generated data were initially raw data and were\ntransformed into complete input-output pairs to prepare the\ndataset for next token prediction training. Each sample follows\nthe input-output structure detailed in Appendix B. The final\ndataset comprises 240 samples from BridgeData V2, 320"}, {"title": "E. Evaluation Tasks", "content": "The proposed framework was evaluated on four manipula-\ntion tasks using the FRANKA robotic arm equipped with an\nRGB-D camera. These tasks were designed with increasing\ncomplexity to assess the framework's performance in scene\nunderstanding, task planning, and control code generation:\n\u2022 Task 1: Hanging a headphone on a stand. The task begins\nwith the headphone placed on a flat surface and the\nstand fixed at a predefined location. This task evaluates\nthe system's spatial perception and object manipulation\ncapabilities.\n\u2022 Task 2: Placing the headphone in a designated position.\nStarting with the headphone hanging on the stand, the\nrobot must identify and remove it, then place it at a\nspecified 3D location. This task tests the framework's\nability to perform object segmentation, spatial coupling,\nand precise placement.\n\u2022 Task 3: Moving the stand and hanging the headphone.\nThe robot must first relocate the stand to a specified\nposition and then hang the headphone on it. This task\nevaluates the framework's dual-task reasoning, coordina-\ntion, and sequential execution capabilities.\n\u2022 Task 4: Generating control code from high-level instruc-\ntions (e.g., \"listen to music\"). The robot must interpret the\ninstruction, generate executable control commands, and\nperform the corresponding actions, such as placing the\nheadphone in the appropriate position. This task evaluates\nthe framework's ability to translate abstract commands\ninto logically consistent and executable actions."}, {"title": "F. Evaluation Metrics", "content": "To evaluate the proposed framework, four key metrics were\nused: mIoU, ROUGE-L, Executability, and TSR. These met-\nrics assess spatial localization accuracy, task decomposition\nability, command executability, and overall task success rate."}, {"title": "1) Mean Intersection over Union (mIoU)", "content": "mIoU measures\nthe localization accuracy by comparing the overlap between\nthe predicted and ground-truth object regions in the 3D space.\nThe formula is:\nmIoU = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{A_i \\cap B_i}{A_i \\cup B_i} (24)\nwhere A and Bi represent the predicted and ground-truth\npoints, respectively, constrained by a 3D localization error\nof less than 0.2. This adaptation ensures mIoU evaluates the\nspatial precision required for accurate object manipulation."}, {"title": "2) ROUGE-L", "content": "ROUGE-L evaluates the semantic consis-\ntency of generated control sequences by computing the longest\ncommon subsequence (LCS) between the predicted sequence\nP and the ground-truth sequence G:\nROUGE-L = \\frac{2 \\cdot LCS(P,G)}{|P| + |G|} (25)\nThis metric focuses on high-level task command consistency,\nsuch as evaluating instructions like \"listen to music.\""}, {"title": "3) Executability", "content": "Executability quantifies the proportion\nof control commands that can be successfully parsed and\nexecuted by the robot:\nExecutability = \\frac{Successful Executions}{Total Executions} \u00d7 100%. (26)\nParsing ensures that each generated command conforms to\nthe robot's syntax and functional constraints, reflecting the\ncompatibility of instructions with physical capabilities."}, {"title": "4) Task Success Rate (TSR)", "content": "TSR measures the overall\nsuccess rate of task completion:\nTSR = \\frac{Successful Tasks}{Total Tasks} \u00d7 100%. (27)\nA task is considered successful only if all steps in the\ngenerated plan are executed without failure. Repeated trials\nensure that TSR captures the robustness and reliability of the\nframework under real-world conditions."}, {"title": "G. Results and Analysis", "content": "Experimental results, summarized in Table II, highlight\nthe key limitations of existing models and demonstrate the\nproposed framework's advantages."}, {"title": "1) Task Execution Comparison", "content": "The experimental results\nreveal fundamental limitations across different categories of\nmodels, highlighting the advantages of the proposed frame-\nwork. For models incapable of autonomously acquiring 3D\ncoordinates (e.g., LLM, VLM, VLM with COT), precise object\npositions were directly provided to simulate scenarios where\nsuch information might be obtained via alternative means\n(e.g., prompt engineering, manual input, or pre-training on\nspecific scenes). This setup effectively isolates the models'\nspatial data acquisition capabilities, allowing for a fair and\nfocused evaluation of their spatial understanding, task plan-\nning, and execution performance. It should be noted, however,\nthat providing spatial data does not equate to enhancing the\nmodels' inherent spatial reasoning capabilities, although the\nuse of prompts indirectly facilitates task understanding to some\nextent."}, {"title": "\u2022 Coordinate-Guided Models (e.g., LLAMA3.3 70B)", "content": "These models, rooted in high-dimensional text embed-\nding spaces, lack the capacity to autonomously integrate\nspatial data into coherent 3D environment representations.\nEven with precise object coordinates provided, their in-\nability to understand spatial relationships results in exe-\ncution errors. For example, in Task 1 (hanging the head-\nphone) and Task 3 (moving and hanging the headphone),\nthese models fail to predict spatial constraints, frequently\ncausing collisions. This limitation reflects the inherent\ndisconnect between text-based reasoning and physical\ntask requirements in dynamic, multi-object scenarios."}, {"title": "\u2022 End-to-End Vision-Language Models (e.g., GPT-40)", "content": "While incorporating 2D visual inputs improves task\nplanning, these models exhibit unclear understanding of\nspatial coupling relationships. In Task 2 (placing the\nheadphone), for instance, the lack of clear comprehension\nof spatial coupling leads to inconsistent object place-\nment and suboptimal task execution. This highlights a\nkey limitation in their ability to extrapolate 3D spatial\nrelationships from 2D inputs, particularly in unstructured\nenvironments."}, {"title": "\u2022 Text-Guided Reasoning Models (e.g., GPT-01 with COT)", "content": "Incorpor"}]}