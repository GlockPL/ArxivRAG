{"title": "Personalizing Education through an Adaptive LMS with Integrated LLMs", "authors": ["Kyle Spriggs", "Meng Cheng Lau", "Kalpdrum Passi"], "abstract": "The widespread adoption of large language models (LLMs) marks a transformative era in technology, especially within the educational sector. This paper explores the inte- gration of LLMs within learning management systems (LMSs) to develop an adaptive learning management system (ALMS) personalized for individual learners across various educational stages. Traditional LMSs, while facilitating the distribution of educational materials, fall short in addressing the nuanced needs of diverse student populations, particularly in settings with limited instructor availability. Our proposed system leverages the flexibility of AI to provide a customizable learning environment that adjusts to each user's evolving needs. By integrating a suite of general-purpose and domain-specific LLMs, this system aims to minimize common issues such as factual inaccuracies and out- dated information, characteristic of general LLMs like OpenAI's ChatGPT. This paper details the development of an ALMS that not only addresses privacy concerns and the limitations of existing educational tools but also enhances the learning experience by maintaining engagement through personalized educational content.", "sections": [{"title": "I. INTRODUCTION", "content": "WIDESPREAD adoption of large language models (LLMs) is a recent phenomenon. Previous technologies lack the powerful automation utility LLMs provide. Simple wrappers over LLMs are novel but already commonplace. Unique innovations therefore require the combination of multiple technologies. In education, institutions distribute course materials and analyze student performance using digital learn- ing management systems (LMSs) [1]. Course-specific SaaS platforms also assist in the learning process [2]. In K-12, teacher attention is spread too thin to meet every student's needs; a problem worsened in post-secondary where self-study dominates [3]. Passive learning, such as rereading textbooks, is less effective than active methods [4]. Standard LMSs rely on passive content, making the status quo sub-optimal. SaaS platforms, though helpful, are expensive and suffer from the same limitations. One-on-one instruction is the pinnacle of educational methods [5], as the learning experience can be per- sonalized to actively moderate a student's cognitive load [6]. Achieving this balance is hard, even for trained educators, and nearly impossible with a generalized approach [7]. An optimal solution should adapt to the student, both during lessons and throughout a course.\nWe envision an adaptive learning management system per- sonalized for each user. An Al agent can customize the experience via consistent user profiling. Such a system could overshadow conventional solutions. Traditional LMSs serve as delivery methods for academic materials, acting as proxies between teachers and students, allowing educators to manage content for student consumption, collect completed work, track academic progress, and communicate [1]. This framework can be augmented to provide adaptive content tailored to users' goals. LLMs such as OpenAI's ChatGPT could be pivotal in such a system, but face issues like hallucinations, privacy concerns, knowledge gaps, and high costs [8]. This paper discusses a prototype designed to address these issues. It integrates custom-trained, domain-specific LLMs for various tasks, balancing privacy and cost by choosing when to use secure models or third-party APIs. By extending models with specialized datasets and using retrieval augmented generation (RAG), the system reduces hallucinations and outdated infor- mation.\nDevelopment was divided into three phases. In Phase I, command-line scripts were designed for tasks like OCR and data management, and a Django web backend and React frontend were built. The system was then refined and con- tainerized using Docker. Phase II integrated LLMs, developing a command-line interface with ChatGPT's API and testing various configurations with RAG and vector embedding. Phase III compared the performance of various LLMs against human test scores in mathematics, reading, writing, reasoning, and coding. Data on resource utilization and performance was ana- lyzed to benchmark the LLMs. This research and development process sought to evaluate the utility of expert systems and LLMs in an adaptive learning management system.\nThe remainder of this paper is structured as follows: Sec- tion II provides a literature review, examining related work and foundational concepts. Section III details the implementation of the system, including the architecture and technologies used. Section V describes the experimental design, outlin- ing the methodologies and procedures followed. Section VI presents the results and discussion, analyzing the findings and their implications. Finally, Section VII concludes the paper and discusses future work, suggesting directions for further research and development."}, {"title": "II. LITERATURE REVIEW", "content": "Studies dating back decades before the advent of generative AI indicate that intelligent tutoring systems have a positive"}, {"title": "A. Artificial Intelligence in Education", "content": "Studies dating back decades before the advent of generative AI indicate that intelligent tutoring systems have a positive impact on student learning [9], in some cases comparable to that of human tutors [10]. Now that Al has begun a societal boom, implementations using the technology to assist in the classroom are becoming increasingly popular. Current applications have been shown to benefit both students and teachers. Many of these systems are built around the concept of formative assessment, where evidence of students' learning is continuously logged and subsequently used to guide instruc- tional adjustments and provide actionable feedback [11].\nStudent-oriented personalized learning systems may include additional features including generation of exercises, recom- mendation systems, and adaptive conversation [12][13]. Some systems can detect how individual students learn best, helping them make informed learning choices, increasing student en- gagement, and lending to the generation of adaptive learning materials [14]. Applications oriented towards teachers con- tain features like automated curriculum design, performance prediction for flagging struggling students, AI-generated les- son plans, frameworks for assessment, statistics tracking and analytics, generation of homework and test questions, and adaptive professional development systems [15]. Technolo- gies used to make these features possible consist primarily of machine learning, expert systems, and natural language processing [12]. AI tutoring systems can provide noticeable improvements to the quality of education across many domains and institutions, including K-12, post-secondary, and in the workplace [13][16]. Trials have demonstrated tangible benefits such as increased student participation, increased learning performance, and reduced teacher workload [13].\nModern AI learning systems contain a combination of standout capabilities and debilitating shortfalls. Due to their natural language processing (NLP) backbone, LLMs have a proven track record of strong reading and writing skills, as well as reasonable competence in answering general knowledge and science-based questions [17]. The same core building blocks provide AI learning systems relying on LLMs with com- petence in language-to-language translation, validating their inclusion in foreign language classes [18]. Some prototype systems are designed to track user performance or categorize their preferences, and then use these cues to adapt the system's behaviour to match the user [19]. LLM-based systems are reported to be capable of producing small blocks of code at or above the skill level of a post-secondary CS1 course. Test groups remark on the learning advantages of generating code samples, tutoring advice from chatbots during assignments, and LLM analysis of erroneous code [20].\nDespite their promise, educational AI systems have many shortfalls. LLM-based systems are reputed to lack competency in both problem solving and tutoring on the topic of math- ematics [21]. The performance of LLMs also varies widely depending on the wording of semantically similar prompts, independent of the problem type [22]. Furthermore, LLMs are plagued by over-confident generation of inaccurate or outdated information, known as hallucinations [23] (see section II-C). Some prototype systems have had success avoiding halluci- nations by using RAG and vector embedding, although this may require preventing LLMs from utilizing their training data when generating responses [24]. LLMs are also held back by either the financial cost of API calls to proprietary models, or in the case of self-hosted models, the requirement of high- spec hardware for LLMs with large parameter sizes. Finally, users with privacy concerns are rightfully cautious of using proprietary LLMs on account of how prompts and data fed into such models are often used to train future versions, allowing for the possibility that segments of one user's data may be relayed to another user, or stored on a company's server where it could be vulnerable to a data breach [25]."}, {"title": "B. Expert Systems", "content": "An expert system is a digital knowledge storage and retrieval system intended to assist in the resolution of problems using expert knowledge embedded into the system. They consist of a data pipeline that mirrors some aspects of the fundamental data science process [26]. First, raw data is collected from a reputable source, typically a human expert with a comprehen- sive understanding of the fine details of a particular domain. This data is initially stored in some format, such as a database, a collection of physical documents, or audio recordings. The information from this intermediate source is then codified and processed into practical knowledge that is relevant to a business or system requirement, or some general use case. Once translated into this more useful format, the knowledge is stored in a digital knowledge base. Rules for the retrieval of the stored knowledge are then established and programmed into the system, allowing end-users who lack relevant knowledge for a given problem to interact with the system and retrieve the knowledge that is specific to their issue [27][28]."}, {"title": "C. Large Language Models", "content": "Large language models (LLMs) are a branch of artificial intelligence (AI) based on transformer architecture [29]. We will build an understanding of the fundamental nature and resulting characteristics of LLMs by beginning with their \u201catomic,\u201d low-level conceptual precursors, and progressively scaling back towards more macroscopic, corollary technical concepts. Deep learning is a machine learning (ML) process that makes use of neural networks [30]. Their function is beyond the scope we will explore. Transformers are ML models trained using deep learning to perform specialized tasks like computer vision or natural language processing (NLP) [31]. LLMs are a type of transformer that is trained for NLP tasks, making them specialized for text-completion and question-answering tasks [32]. This requires an extensive training process. Enormous quantities of sample text are passed into the LLM as input, and parameter settings are adjusted to produce consistently more desirable output [33].\nTraining of transformers, LLMs, and other AI technologies is performed using a combination of several ML techniques, including supervised and unsupervised learning. Supervised learning involves the input of large quantities of labelled datasets to train algorithms to accurately classify data or predict the next output in a sequence [32]. In terms of NLP models, this entails predicting the most suitable word or phrase in a partially completed sentence, paragraph, or larger body of text. Unsupervised learning is characterized by training"}, {"title": "III. METHODOLOGY", "content": "The development methodologies of DevOps were adopted very early in the initial research and planning stage of the project, and referenced as an overarching philosophy through- out the development process. Short for \"development and operations,\" DevOps builds on many principles used in Agile project management, breaking away from the rigidity and bureaucratic ideas that hamper progress within the waterfall model [39].\nDevOps accelerates development through strategies like continuous integration and continuous delivery (CI/CD). In continuous integration, a version control system is utilized to merge code changes made by each developer into a central repository, where they can be tested immediately after their initial design, and integrated into the main project branch shortly thereafter. This allows changes to become functional parts of the project without the delay of waiting for the next development stage, as in the waterfall model. This strategy also highlights the DevOps principle of communication and collaboration, by uniting the functions of developers and operations into a streamlined, integrated process [39]. Git version control software and GitHub repositories were used during this project to achieve continuous integration.\nIn continuous delivery, code changes are built, tested, and prepared for deployment shortly after they are designed, often be the same developer who drafted the changes. This creates a trend towards rapid delivery by allowing bugs to be identified immediately after they are introduced. The DevOps principle of reliance on automation also comes into play here, using automated testing tools to speed up the process [39]. We employed these practices by testing each functional unit of code before moving on to subsequent sections, often on the same day that they were produced.\nAutomation tools are used in DevOps throughout the de- velopment process, contributing to rapid deployment goals. In this project, this took the form of frameworks like Django, which accelerated the design and maintenance of the web backend. DevOps also takes the stance that we should begin with the end goal in mind, and consistently work towards that goal. This entails an emphasis on modular design, implying encapsulation of processes so that all aspects of the project can be scaled as necessary [39]. We used a modular approach in our design, encapsulating sections of the project using Docker containerization and virtual environments."}, {"title": "IV. IMPLEMENTATION", "content": "In Phase I of our implementation, a prototype expert system was developed using a combination of Python, JavaScript/TypeScript, JSX/HTML, CSS, Shell, and SQL pro- gramming languages. Functioning as a web app using a Django backend and React frontend, this system combines multiple functions involving OCR, text-based search, and HTML scraping. At present the system effectively solves user questions by matching their query with the four closest options loaded into the system's knowledge base."}, {"title": "A. Expert System", "content": "In Phase I of our implementation, a prototype expert system was developed using a combination of Python, JavaScript/TypeScript, JSX/HTML, CSS, Shell, and SQL pro- gramming languages. Functioning as a web app using a Django backend and React frontend, this system combines multiple functions involving OCR, text-based search, and HTML scraping. At present the system effectively solves user questions by matching their query with the four closest options loaded into the system's knowledge base."}, {"title": "B. LLM Question-Answering Machine", "content": "Phase II of the project began with the implementation of a command-line interface written in Python that interacted with OpenAI's API, allowing for a single query to be relayed to ChatGPT-3.5 Turbo. This was expanded with a simple loop to provide an unlimited series of query-response cycles, along with basic commands to break the loop, adjust the temperature (randomness) of the Al's responses, limit the maximum number of tokens in the response, and switch to other OpenAI models. A Python list was added to store a log of messages between the user and agent throughout the conversation. By piping a set number of recent messages from the conversation history into the context of each subsequent prompt, the AI developed a basic form of short-term memory, able to respond with an understanding of the persistent topic.\nThis basic loop was re-created using the altered syntax of the LangChain Python library. LangChain facilitates the use of multiple LLM models from a single command, increasing test- ing speed and modularity. Within this framework, OpenAI's suite of LLMs were tested, including ChatGPT-3.5 Turbo, GPT-3.5 Turbo Instruct, and GPT-4. The \"instruct\u201d version of GPT-3.5 was found to return semantically similar responses to the baseline version, but with less verbose output.\nLangChain's capabilities were expanded with the integration of the Ollama extension. Ollama opened the door to exper- imentation with self-hosted LLMs, which are downloaded to the host machine and run locally, without API calls or bandwidth usage. Initial testing was done using Mistral-7B and Llama2-7B. The 7 billion parameter models were chosen due to their reduced system requirements compared to higher- performance but lower-speed versions of the same LLMs. See subsection II-C for a table I summarizing hardware requirements corresponding to different parameter ranges.\nInformal experimentation was performed on the self-hosted and proprietary models' capabilities in system prompt mod- ification, RAG, vector embedding, and prompt engineering. System prompt modification involved context injection to aug- ment the personality, speech style, and flavour of responses. Using internal system prompts like \"You are Luke Skywalker\" established an identity for the agent, which it would emulate insofar as it had knowledge about that persona. In the above example, a user query of \u201cWho's your daddy?", "My father is Darth Vader, otherwise known as Anakin Skywalker.\"\nFor RAG, two directions were investigated. First, RAG from text documents were tested. Here we used output from PDF transcriptions obtained from the Phase I system as an input for the LangChain pipeline. The text loader module parsed the .txt input files, split the document into overlapping chunks, translated each chunk's semantic data into a vector, and stored the collection of vectors into a FAISS vectorstore in memory. Embedding models tested during the intermediate stage of this process include OpenAI's Ada-002, SFR-Embedding-Mistral, voyage-lite-02-instruct, and mxbai-embed-large-v1. Ada-002 was used for its speed and consistency in all subsequent tests.\nNext, we explored RAG from webpage URLs. In these tests, we used the same webpage scraping library as in Phase I; BeautifulSoup. By inputting a URL into the appropriate field, the scraper extracted all text on the page, and passed it into LangChain's pipeline, onward to the same RAG toolkit discussed above. In both the RAG from local document and RAG from URL tests, a consistent system prompt was used:": "Answer the following question based only on the provided context : (context) {context} </context) Question: {input}\u201d\u201d\u201d\nUser queries related to the parsed input source were an- swered with surprising accuracy. To test for hallucinations, queries unrelated to the source were used. In all cases, the sys- tem responded that the requested information was not present in the source articles, rather than misrepresenting falsehoods with unwavering confidence as was the case with some non- RAG tests. Some experimentation with prompt engineering was also performed. This proved valuable as a means of adjusting the Al's responses to specific questions. Lessons learned in these experiments were employed to great effect in Phase III of the project.\nIn terms of hardware bottlenecks, testing on multiple sys- tems revealed RAM, CPU, and GPU quality to be major factors in execution time. Larger parameter-size models took upwards of four minutes to complete on lower-spec computers without GPU assistance. AI tutors typically use a single LLM as the core of their system, adding techniques like prompt engineering and fine-tuning to modify the LLM to behave appropriately. Data retrieved by the system often relies on training datasets as a source. RAG was shown to offer a superior alternative, allowing for the source to be curated for the specific use case, and for sources to be provided with the results.\nFurther optimization can be achieved by using multiple AI models depending on the sub-task. For example, Mistral for NLP, CodeLlama for programming, OpenAI's Ada as an encoding model, and Nougat as a vision transformer. This delegation of sub-tasks separates the data pipeline into pieces that can be managed with a higher degree of control. When"}, {"title": "V. EXPERIMENTAL DESIGN", "content": "Phase III of the project consisted of an analysis of the strengths and weaknesses of a choice selection of LLMs. The intention was to discern which models could be relied on for a given task type, while acknowledging the associated costs in their operation, including financial costs and hardware resource usage. In addition to comparing LLMs to one another, the research aimed to clarify what the general capabilities of LLMs were, and how these capabilities fared against typical human performances in similar tests.\nTo achieve this, an extensive battery of tests was designed. Most question sets were based on standardized tests applied in schools across Canada, ranging from the K-12 level up- wards to post-secondary. Five subject categories were tested: mathematics, reading, writing, reasoning, and coding. From each of these categories, a subset of three question sets were assembled according to estimations of their difficulty, from easy, medium, and hard, based on informal testing in Phase II of the project. For each of the three difficulties, ten questions were selected to be tested, except for the coding and writing categories, due to limitations on grading speed. The questions were distributed as shown in Table II.\nNote the question count for the hard writing question and the three coding question sets. This is due to the time-intensive nature of manually grading these questions. All question sets denoted above with 10 questions were arranged in multiple- choice format, with an automatic grading system implemented using Python. Since LLM responses are somewhat unpre- dictable, with a tendency toward verbosity given their NLP underpinnings, consistent output of a single character response (i.e. A, B, C, D, or E) was not possible for most models. Therefore, a combination of filtering the AI responses using regular expressions, combined with manual grading for many questions, was necessary.\nTo gain the desired insight, ten LLM models were chosen from a stratified sample of over fifty candidates researched. Each model was tested on the entire battery three times over. As noted above, some questions were graded for only one of the three rounds. This made for a total of 3,780 data points that had to be carefully tested, filtered, verified, and graded. Table III lists the LLMs tested.\nThe question sets themselves were either sourced directly from standardized tests or manually designed to match the intended difficulty and style. Each question was formatted into the appropriate JSON file, where it was read by the testing module one at a time. Results were stored in the local filesystem in batch sizes of 30 questions per file, to prevent data loss if a test was interrupted. This was a relevant factor since each LLM could take over an hour to complete its circuit. After testing, test results were re-formatted to extract specific fields, which were then converted into a Pandas DataFrame and exported to Excel. The Excel files could then be filtered, verified and graded. Details on the contents of the aptitude tests follow in sections V-B to V-F. In addition to LLM problem- solving aptitude, the system's resource usage was also tracked across a range of datapoints (see section V-G)."}, {"title": "A. Benchmarking System Design", "content": "Phase III of the project consisted of an analysis of the strengths and weaknesses of a choice selection of LLMs. The intention was to discern which models could be relied on for a given task type, while acknowledging the associated costs in their operation, including financial costs and hardware resource usage. In addition to comparing LLMs to one another, the research aimed to clarify what the general capabilities of LLMs were, and how these capabilities fared against typical human performances in similar tests.\nTo achieve this, an extensive battery of tests was designed. Most question sets were based on standardized tests applied in schools across Canada, ranging from the K-12 level up- wards to post-secondary. Five subject categories were tested: mathematics, reading, writing, reasoning, and coding. From each of these categories, a subset of three question sets were assembled according to estimations of their difficulty, from easy, medium, and hard, based on informal testing in Phase II of the project. For each of the three difficulties, ten questions were selected to be tested, except for the coding and writing categories, due to limitations on grading speed. The questions were distributed as shown in Table II.\nNote the question count for the hard writing question and the three coding question sets. This is due to the time-intensive nature of manually grading these questions. All question sets denoted above with 10 questions were arranged in multiple- choice format, with an automatic grading system implemented"}, {"title": "B. Mathematics Test Design", "content": "Informal tests performed in Phase II suggested that LLMs had significant difficulty with mathematics problems. In an- ticipation of this, question sets were chosen primarily from standardized elementary school tests, rather than including advanced problems from the post-secondary level. For easy difficulty, problems from the elementary school EQAO Grade 3 Mathematics test were selected. Practice problems from the EQAO Grade 6 Mathematics test were chosen for medium difficulty. For hard, the ACT Mathematics Test, representative of skills required for entrance into post-secondary institutions, was used as a source [40][41][42]. All of these sources included a mixture of text-based and diagram-based problems. Since the models to be tested are not capable of interpreting images, any questions containing diagrams were omitted. Ten questions were selected for each difficulty. All questions were multiple choice style."}, {"title": "C. Reading Test Design", "content": "Tests performed in Phase II indicated that all models possessed very strong capabilities in answering reading ques- tions, regardless of difficulty level. In light of this, attempts were made to source especially challenging problems, many of which required parsing through and interpreting lengthy passages. For easy difficulty, selections were taken from the reading section of the 2015 Ontario Secondary School Literacy Test (OSSLT). Medium-level questions were sourced from the ACT Reading test, and hard questions from practice problems at Isac.org for the LSAT Reading Comprehension test [43][44][45].\nTen multiple-choice questions were chosen for each diffi- culty. Questions from medium difficulty were typically be- tween 700 and 800 words long; many hard-level problems re- quired reading two separate passages and answering questions comparing the contents of each passage."}, {"title": "D. Writing Test Design", "content": "In this category, a variety of question types was selected to investigate different dimensions of models' writing ability. For easy difficulty, ten short general knowledge questions were designed to test how accurately each LLM could follow simple writing instructions. These questions required a small amount of knowledge, which was expected to be easily accessible due to the breadth of training data each model was exposed to. A simple pass/fail marking scheme was devised to assess their performance.\nFor medium difficulty, ten multiple choice practice questions from the OSSLT Writing Test, which assesses knowledge of grammatical structure, semantics, and word choice were selected. Since practice question sets for these tests contain only a few multiple choice questions, problems from 2015, 2019, and 2023 were compiled to reach the desired count of ten questions [46][44][47][48].\nThe hard writing test was given special consideration. A second multiple-choice test was regarded as a poor means of assessing the true writing ability of the assembled LLMs. Instead, models were tasked with writing a short essay, sourced from the ACT Writing Test [49]. Several teachers volunteered to grade the essays; results from section VI-A3 represent the mean of their scoring.\nThe essay writing question involved reading a short pas- sage, analyzing three perspectives related to the passage, and then writing an essay within the specifications provided. The instructions given to the LLM models were identical to those in the source material, with the addition of a 500-word limit to reduce marking time and add consistency between responses. Note that of the sample essays included in the source material, most samples averaged well below 500 words.\nGrading for this topic mimicked the marking scheme used on the actual ACT. A rubric was carefully compiled based on the parameters established from the source test materials [49] and all marking was done according to these stipulations. Scoring was recorded in the same format as for human subjects taking the ACT writing test; grades (out of six marks each) were assigned independently across four performance categories: ideas and analysis, development and support, or- ganization, and language use. This produced a total out of a maximum of 24 points. As per ACT guidelines, this total was then represented as a proportionate score out of 36 points, following the standard procedure outlined in the ACT marking instructions [50], allowing scores to be compared to the other categories of the ACT.\nThe ACT's College Readiness Benchmark Scores are em- pirically derived benchmarks, where students who meet the minimum benchmark score have a 50% chance of achieving a B grade or higher in related college courses [51]. Writing scores are a part of the ELA score but do not impact English or Composite scores. The minimum benchmark score on this scale for ELA is 20. Since the writing test scores are used to calculate ELA totals, we will use a cut-off of 20/36 in our final pass/fail assessment of each LLM's writing ability, to establish which LLMs are at or above the minimum viable level for post-secondary success according to the College Readiness Benchmark scale.\nOne essay was produced by each of the ten LLM models, and conscientiously graded according to the rubric. The names of the LLM models that produced each essay were omitted from the copies provided to the graders, and organized in random order, to eliminate marker bias."}, {"title": "E. Reasoning Test Design", "content": "Informal tests in Phase II of the project suggested a signifi- cantly stronger ability in verbal reasoning than with problems involving arithmetic and mathematics. Based on these results, it was hypothesized that this trend would carry over to testing in Phase III. As such, verbal classification questions were selected for the easy difficulty of this category, and numerical reasoning problems were sourced for the medium difficulty level. Ten questions were selected for each of these difficulties, from a test bank of interview practice questions [52][53]. All questions were in multiple-choice format.\nFor the hard difficulty in this category, ten questions from the Law School Admission Test (LSAT) pertaining to logical reasoning were selected. This test is known for being very difficult for human subjects, and so was assumed to be especially challenging for LLMs as well. The question set used was obtained from online practice questions at lsac.org; since they are intended to prepare law students for the entry exam, these problems are unwavering in difficulty. A consistent string containing the directions provided on the source webpage was prepended to the query for each question [54]."}, {"title": "F. Coding Test Design", "content": "For this category, multiple sources were consulted to pro- duce five custom problems per category. All problems were based on searching algorithms, for consistency [55][56][57]. Each difficulty level was made incrementally more challenging by requiring a minimum solution corresponding to progres- sively higher time complexities. Within easy difficulty, most problems had time complexities of O(n) or O(n log n). For medium and hard, either time complexities were greater, or similar problems were reputed to be suitably difficult for human subjects. All questions specified a solution to be written in the Python programming language.\nFor all problems, the AI models were instructed to write a single Python function named test() with specific input and return types. This was intended to provide consistency, make testing more empirical, and accelerate the grading process. Rather than including ten questions per difficulty, as in most other categories, only five per difficulty were included here due to the time required to design and test each problem. Special considerations were needed in the grading of this problem type. Analysis of style was beyond the scope of these tests. To ensure consistent, empirical results were gathered, unit tests were written for each problem, outputting the result as a binary pass/fail. The appropriate portion of the AI's response was manually copied into the matching unit testing module, and results were logged for later analysis. Given five problems per difficulty, across three difficulties, with ten LLMs, this amounted to: 5 * 3 * 10 = 150 coding problems that had to be manually graded. Note that the questions selected can be considered to be of easy, medium, and hard difficulty for beginner programmers. This design choice was intended to mirror the type of questions encountered in a post-secondary CS1 course, to compare the ability of LLMs against students at this skill level."}, {"title": "G. Resource Utilization Test Design", "content": "In addition to benchmarking of problem-solving abilities, the LLMs' system resource usage was recorded across a range of metrics, for each question solved. These records were averaged across all attempts at a given question and then arranged into tables and charts for analysis. Collecting this data was considered valuable in gaining an understanding of the relative system demands across each LLM tested, and to determine how computationally intensive each question in our test bank was to solve. Such readings can also be useful in identifying aberrant behavior such as memory leaks. Table IV below lists tested metrics, along with an explanation of what they represent.\nMany strategies were attempted to achieve this detailed level of system data collection. Initially, Celery was used as a task queue. Celery's broker sends and receives messages to the system kernel, managing thread usage and launching tasks as subprocesses. RabbitMQ was used as a broker over Redis and other options, due to its more robust and stable nature. A SQLite3 database was employed as a backend for passing results from celery workers back to the test manager. Flower, a Celery worker monitoring agent, was set up to collect the desired resource metrics. However, Flower cannot access all of the above usage details. Furthermore, Celery disallows the creation of subprocesses via Python's subprocess library, which was integral to the testing module. Thus, Celery and Flower were abandoned.\nNext, control groups were investigated as a potential so- lution. These are used in systemd-based operating systems. Systemd is the first process that runs after boot on post- SystemV UNIX systems. It manages all other services and processes on the system and uses control groups (cgroups) to manage processes, track their resource utilization, and set limits on allowed usage of system hardware components. Since cgroups isolate parent processes and any subprocess hierarchy spawned by them from the rest of the system, they are ideal for accurately identifying an application's resource draw.\nIn this project, after excluding Celery and Flower as options, the intention was to manually create a cgroup for each test in- stance, which would then encapsulate all related subprocesses. The cgroup could then be accessed at the end of each test to return readings of total and mean usage across the entire process hierarchy during its lifespan.\nDue to difficulties in directly creating and accessing cgroups using low-level systemd commands, the power of cgroups was exploited by accessing groups created automatically by the kernel upon the initialization of our testing application using the getrusage() function from Python's \"resource\" library. This function is capable of accessing cgroup usage data of any input process identifier (PID). We made use of this in order to retrieve the system time and user time of our application's exe- cution. Because getrusage() provides results from the queried PID and its child process hierarchy separately, these values were summed to find the total usage of the testing application.\nHowever, the LLMs tested were not considered to be sub- processes of the testing application's PID in terms of cgroup encapsulation. This is because Ollama runs as a backgrounded daemon between executions of the locally hosted LLMs it serves, even after the parent testing application process is terminated. Thus, a workaround had to be devised to collect data from the local Ollama server's usage, unique to each separate question tested. The solution implemented included the identification of the Ollama server's PID, termination of the process before each test, verification of the process's termination, initialization of a new Ollama server instance, and then access to the new Ollama server's usage data after each test was completed. This method granted access to the CPU system time and user time across all associated processes during each separate test.\nWhile the above method could be used to monitor some hardware usage metrics, we also desired access to the average CPU percentage, average virtual memory percentage, and overall execution times for each question tested. Therefore, a separate method was designed to collect these data points, us- ing the psutil (Python system and process utilities) library. This library is capable of returning system-wide CPU and virtual"}, {"title": "H. Data Representation", "content": "To represent all accumulated data in a useful format for analysis, data science principles were applied. Each question first had to be graded, before any statistics could be evaluated. This was accomplished through the creation of a custom Python grading module, involving the filtering of AI responses using regular expressions to detect whether they included the character denoted in the solution column for any given question (i.e. A, B, C, D, or E). This allowed large portions of the raw data to be automatically graded in a binary pass/fail format. Inconclusive results were later analyzed and graded manually.\nAll data points were then merged into a single frame, flattening the representation into a single table with 3,780 rows and 19 columns, in first normal form. This was accomplished by parsing intermediate test output stored in a series of raw text files into multi-dimensional Python lists using regular expressions. The process was simplified due to these inter- mediate text files having variables encapsulated in a mock- HTML format in anticipation of this step. This method was used rather than moving data directly from memory into tables during testing to avoid data loss in the event of system errors part-way into the testing process. Such measures were deemed necessary due to the extent of testing performed; in total, the runtime required to test all questions on all ten LLMs exceeded 16 hours. This was partly caused by an enforced 5-second delay with continuous readings of less than 10% total system- wide CPU load, preventing damage to hardware, and ensuring consistency of resource usage readings.\nIntermediate results were loaded into Pandas DataFrames, allowing specific columns to be extracted from the complete dataset, and exported into independent tables according to the tested category (i.e. mathematics, coding, etc.), in second and third normal form. Aggregate functions were applied to many"}]}