{"title": "SUDS: A Strategy for Unsupervised Drift Sampling", "authors": ["Christofer Fellicious", "Lorenz Wendlinger", "Mario Gancarski", "Jelena Mitrovi\u0107", "Michael Granitzer"], "abstract": "Supervised machine learning often encounters concept drift, where the data distribution changes over time, degrading model performance. Existing drift detection methods focus on identifying these shifts but often overlook the challenge of acquiring labeled data for model retraining after a shift occurs. We present the Strategy for Drift Sampling (SUDS), a novel method that selects homogeneous samples for retraining using existing drift detection algorithms, thereby enhancing model adaptability to evolving data. SUDS seamlessly integrates with current drift detection techniques. We also introduce the Harmonized Annotated Data Accuracy Metric (HADAM), a metric that evaluates classifier performance in relation to the quantity of annotated data required to achieve the stated performance, thereby taking into account the difficulty of acquiring labeled data.\nOur contributions are twofold: SUDS combines drift detection with strategic sampling to improve the retraining process, and HADAM provides a metric that balances classifier performance with the amount of labeled data, ensuring efficient resource utilization. Empirical results demonstrate the efficacy of SUDS in optimizing labeled data use in dynamic environments, significantly improving the performance of machine learning applications in real-world scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "In classical machine learning, most models train on a static dataset with the underlying assumption that the data distribution is static. This assumption works for speech detection, optical character recognition, and so forth, where little to no changes occur over the years. However, many domains such as credit card fraud detection [1], energy consumption [2], production forecasting [3], and many others are very dynamic and do not adhere to the static distribution model. These domains are examples where the underlying data distribution changes over time and this is known as concept drift. \u017dliobait\u0117 et al. [4] explain the different application domains of concept drift in detail.\nConcept drift alters data distribution, impacting model performance and necessitating model updates. However, obtaining labeled data for retraining can be costly in terms of time, effort, and resources, often requiring manual expert labeling. For instance, in credit card fraud detection, delays in acquiring labeled data may justify the use of semi-supervised methods that exploit future unlabeled transactions to improve fraud detection [5]. Additionally, the scarcity of positive instances further complicates the labeling process [6]. Therefore, it is crucial to adapt to distribution changes with minimal training data. While algorithmic performance typically garners significant attention, the need for annotated ground truth data often remains overlooked. Sample size estimation, a common practice in fields like medicine and economics, highlights the expense and challenge of obtaining ground truth data [6].\nReal-world datasets typically exhibit gradual rather than abrupt drifts, which can contaminate the model's training data when drift is detected in between distribution changes.\nOur primary contribution in this paper is SUDS, a method that piggybacks on existing unsupervised drift detection methods to select homogeneous data points for retraining a model after the occurrence of a drift. SUDS operates on the idea, that we can benefit from an analysis of the data present when a drift is detected, and exploit the most recent data to produce a more homogeneous collection of data points for retraining the classifier. Using SUDS, we demonstrate that our method can outperform existing algorithms while requiring only 20% of the training data compared to these algorithms.\nOur secondary contribution is a performance metric, the Harmonized Annotated Data Accuracy Metric (HADAM), that focuses on the performance of a model along with the amount of annotated samples required to achieve that performance. With HADAM, SUDS outperforms its counterparts in most datasets, showing excellent performance particularly in real-world datasets.\nWe organize the paper as follows: we introduce the problem and research statement in Section I with the related work explained Section II. Our proposed approach is presented in Section III with Section IV containing the results of our experiments. We then present our conclusions in Section V. We provide an extended version of this paper along with the hyperparameter experiments in the supplementary material."}, {"title": "II. RELATED WORK", "content": "We look at some of the unsupervised methods in concept drift detection as it pertains more towards this work. Sethi et al. [7] proposed the Margin Density Drift Detection(MD3) method for detecting concept drifts from unlabelled data streams. \"The proposed methodology uses the number of samples in a classifier's region of uncertainty as a metric for detecting drift.\" Koh [8] introduces a distinction between two types of drifts: local and global drifts. Koh's contribution relies on two different drift detection methods based on the two different drift types. The contributions are using fixed windows for their purpose. Mustafa et al. [9] proposes a denoising autoencoder to rebuild an input vector from a corrupted version and a nonparametric multidimensional change point detection method. The method works by maintaining several ensembles using KNN models and testing whether the new instances belong to one of the ensemble's classifiers or are outliers. If the new instance is an outlier, it is stored in a frequently checked buffer to verify if the algorithm should add a new classifier to the ensemble. The One-Class Drift Detector proposed by G\u00f6z\u00fca\u00e7ik et al. [10] consists of a sliding window of the DataStream to which we append a sliding window that they called \"outlier's window.\" This window gives the result of the classification of each instance by a One Class SVM trained on a current data set. A drift is detected whenever the outlier's rate exceeds a threshold parameter. We can take the following proposed methods as examples of the first type: Costa et al. [11] developed the Drift Detection Method based on Active Learning (DDAL) that uses active learning to detect changes in data streams. We divide the data stream into fixed-size batches and train a model on the initial batch. We then predict the labels of subsequent batches. The algorithm computes the uncertainties, updating the minimum and maximum values. The algorithm signals a drift if the updated difference between min and max is higher than a threshold. The algorithm updates the model using the newly labelled instances in the current batch. Liu et al. [12] proposed a method based on the Nearest Neighbor-based Density Variation Identification algorithm. It relies on a k-nearest neighbor-based space partitioning schema, a distance function over the density discrepancies, and a statistically significant test to perform the detection. G\u00f6z\u00fca\u00e7ik et al. [13] proposed the Discriminative Drift Detector(D3). The algorithm works by accumulating the samples into two windows that are not the same size. The algorithm then trains a classifier on both windows, with each window getting its label. If the classifier can distinguish between the two windows with a certain threshold, then the algorithm signals a concept drift. The underlying assumption is that if both windows contain data points of the same underlying distribution, the classifier will fail to assign labels correctly, and the new window will contain samples that belong to the drifted distribution. Gulcan and Can proposed the Label Dependency Drift Detector(LD3) method in multi-label classification. The authors use a label ranking method and exploit the temporal dependencies between labels using this ranking. The method exploits correlations between labels in multi-label data streams. The algorithm considers two windows, one for the old and the other for the new samples. Once these windows are full, the algorithm generates co-occurrence matrices. We obtain these matrices by counting the occurrences of each class label. Each label is then ranked based on the frequencies. The similarity between rankings provides an idea of whether a concept drift occurred. In recent years neural networks and Generative Adversarial Networks are used to detect concept drifts [14], [15]. Especially Fellicious et al. uses a GAN to identify historical drifts and use the past data to increase the training data size. This allows for better performance of the model and less susceptible to noise. Cerquiera et al. proposed a method using the student-teacher model [16]. The method uses the teacher model to learn and the output of the teacher model is learnt by the student. For incoming instances the difference is then computed and based on the change in the output of the student a drift is signalled."}, {"title": "III. METHOD", "content": "In many machine learning algorithms, the data available at the point of drift detection could be from different data distributions [17]. This heterogeneous data is because data distribution changes are usually gradual, and the drift detection point may not precisely align with the actual point of drift as drift detection depends on specific hyperparameters used by the detection algorithm. Training on such heterogeneous data can reduce the classifier's efficacy and negatively impact future drift detection by poisoning the training set. To address this issue, we propose a method that leverages the existing classifier to select a more homogeneous subset of data for retraining. Our approach aims to enhance overall model performance by ensuring that the training data is homogeneous in its distribution, thereby improving both the retraining process and future drift detection accuracy.\nIn our work, we introduce terms that we use throughout this work. We refer to the current distribution or $D_{curr}$ as the distribution on which the model was trained. A concept drift occurs when the current distribution ($D_{curr}$) changes into a newer distribution. We refer to this changed distribution as the newer distribution or $D_{next}$. Once the model identifies a concept drift from $D_{curr}$, we retrain the model on $D_{next}$. Also, we use X to denote input feature vectors and W to denote a window where we consider the input vectors within that window.\nReal-world observations from different domains show that concept drift is usually a gradual drift where the data distribution changes over time as in Equation 1 [17].\n$X_{t} \\in D_{curr} \\land X_{t+1...k} \\in {D_{curr}, D_{next}}/X_{k+1...n} \\in D_{next}$ (1)\nwhere, $X_{t}$ is the input features at time t, $D_{curr}$ being the current distribution under consideration and $D_{next}$ being the changed distribution after the concept drift. As for our method, we also do not expect a sudden change in the data distribution (abrupt drift) as given in Equation 2.\n$X_{t} \\in D_{curr}X_{t+1...n} \\notin D_{curr}$ (2)\nIn the case of a non-abrupt drift, the samples used for training could belong to a heterogeneous data distribution as shown in Equation 3:\n$W_{k...k+n} \\in {D_{curr}, D_{next}}$ (3)\nwhere $W_{k..k+n}$ is the selected training window for the model. From Equation 3, we know that the training window comprises a heterogeneous distribution instead of the optimal homogeneous distribution due to the tolerance and threshold hyperparameters of the corresponding drift detection algorithm. In such scenarios, training the model on mixed distributions can degrade its performance, in both label prediction and drift detection. To address this issue, one solution is to discard an arbitrary number of samples before collecting data to retrain the model. However, such a waiting strategy could be problematic as it delays model updates and we need to be sure that the next concept drift is sufficiently far into the future. Therefore, a strategy that selects only the homogeneous data points belonging to the new distribution ($D_{next}$) could provide significant advantages. This approach should ensure better quality training data for the model, potentially improving the prediction accuracy while lowering the labeling costs in time and human effort.\nOur method \"Strategy for Unsupervised Drift Sampling (SUDS)\" builds on the assumption that a more homogeneous training data provides better drift detection and performance. Our method works by selecting a window around the point of detected drift occurrence and use a classifier to identify the data points that only belong to the newer distribution. We do this by using the drift detection model itself to detect out of distribution samples w.r.t. the current data distribution and rejecting them. This is given in Equation 4.\n${W_{train} \\in X | X \\in D_{next}}$ (4)\nwhere $W_{train}$ refers to the samples for retraining the model and $D_{next}$ refers to the new data distribution. This approach assumes that the majority of data points in $D_{next}$ belong to the new class which allows us to train a selector for similar examples in the distribution prior to the shift.\nAs our method essentially treats the drift detector as a black-box model and only modifies the training data selection, it is compatible with all established drift detection techniques. While there are several established supervised drift detection methods such as Drift Detection Method (DDM), Early Drift Detection Method (EDDM), Adaptive Windowing (ADWIN), and DDE, we focus on unsupervised methods due to their greater applicability in real-world scenarios [18]\u2013[21].\nTo ensure reproducibility and practical application, we prioritize methods with publicly available source code and datasets. Despite recent advances in unsupervised drift detection, obtaining experimental source code and datasets remain a significant challenge. Given our limited resources, implementing multiple algorithms from scratch is not feasible and may not yield consistent results due to differences in dependencies and optimizations. We select two algorithms that meet our criteria of having open-source code and public datasets: the Discriminative Drift Detector (D3) and the One Class Drift Detector (OCDD). The specific adaptations of these algorithms for our plugin method are detailed in Section III-A and Section III-B. The datasets used by the authors consist of real-world and artificial datasets from Losing et al. [22]. We use nineteen datasets, including both real-world and artificial datasets."}, {"title": "A. Adapting the approach to D3", "content": "Discriminative Drift Detector (D3) works by obtaining a window W of size $w(1 + \u03c1)$ of continuous data samples, from which two sub-windows of size w and $w * p$ are considered [13]. One of the sliding windows, $W_{curr}$ of size w represents the current distribution ($D_{curr}$), and the second sliding window represents the new distribution ($D_{next}$) of size $w * p$, and we represent this sub window as $W_{next}$ in this work. The underlying theory here is that if a classifier can distinguish between $W_{curr}$ and $W_{next}$, there is a separation between the two sub-windows, and that means a change in underlying distribution, which is a concept drift.\nTo implement SUDS in D3, we take both the sub-windows $W_{curr}$ and $W_{next}$. Since $W_{curr}$ is larger in default parameter settings, we subsample $W_{curr}$ so that $|W_{curr}|$ = $|W_{next}|$. A classifier, logistic regression in this case, is trained on the both $W_{curr}$ and $W_{next}$ as label 0 and label 1 respectively, and predict confidence scores for the whole window W. To train for the new distribution $D_{next}$, we take the data points with the highest confidence scores for label 1. We then train the Hoeffding tree on these data points instead of simply taking the $W_{next}$ as done in D3. We use the Hoeffding tree as the class predictor as in the original D3 and OCDD algorithms."}, {"title": "B. Adapting the approach to OCDD", "content": "One-Class Drift Detector is based on a One class SVM classifier performing the drift detection [10]. OCDD works by taking a window, considering it as the current distribution, and then searching for outliers. The algorithm signals a drift once the number of outliers reaches a threshold. This detection is under the assumption that when the data distribution changes, the number of outliers detected will increase.\nTo incorporate our retraining strategy, we do the following: we fit a new classifier with the data contained in the new context sub-window. We train another One-Class Support Vector Machine(SVM) classifier with only the data points marked as outliers. We then predict on the whole window using the trained One-Class SVM. The theory is that we need homogeneous samples for the retraining, and the outliers should be part of the new distribution. We then decide based on the predictions of the old and new classifiers as given in Algorithm 2. The samples need to be in distribution based on the classifier trained on the new incoming data distribution and out of distribution for the classifier trained on the old or previous data distribution. We require homogeneous samples to train the Hoeffding tree and this is achieved by selecting the common data samples that are predicted as outliers according to the classifier trained on the previous data distribution and as indistribution according to the classifier trained on the new incoming data distribution."}, {"title": "C. Harmonized Annotated Data Accuracy Metric(HADAM)", "content": "Most evaluation of drift detection methods focuses on accuracy metrics, often neglecting the crucial aspect that is the amount of labeled data required for effective model retraining [10], [13], [23]. This oversight presents a significant challenge, as data labeling is expensive and can add noise [24]\u2013[28]. In real-world applications, the usability of a machine learning system depends on both efficiency and effectiveness. Although there are a few other custom metrics for drift detection, none of them address the data annotation requirement [29], [30]. Thus, there is a pressing need for a more holistic approach that balances model performance with the practical constraints of data annotation.\nThere is research in this direction such as sample size estimation for classifier accuracy but this area mainly focuses on static data distributions where data collection is time-consuming, expensive, or both [31].Considering all these factors, we introduce a metric Harmonized Annotated Data Accuracy Metric(HADAM) to evaluate the performance of the classifiers based on the training data requirements and the effectiveness, accuracy in this case. We propose the metric as a harmonic mean based on the Pythagorean means of the performance metric (e.g. Accuracy) and the amount of unannotated samples in the dataset. HADAM is calculated as follows.\n$\\frac{1}{HADAM} = \\frac{1}{2} (\\frac{1}{\\psi^2} + \\frac{1}{\\epsilon^2}) \\Rightarrow HADAM = \\frac{2 \\times \\psi \\times \\epsilon}{\\psi + \\epsilon}$ (5)\nwhere,\n\u03c8 = the performance metric\n\u03f5 = 1 \u2212 (|Annotated Samples|/|Dataset|).\nWe integrate \u03f5 as the percentage of unannotated samples instead of the raw number of samples to remove the influence of dataset size.\nThe higher the value, the better the model's overall performance on the dataset. Performance metrics such as accuracy, precision, recall, and f1-score are in [0, 1], and realistically, the percentage of samples will be (0, 1] and thus puts the score also in [0, 1). The metric, therefore, balances the raw performance and the amount of annotated samples. All our methods are publicly available\u00b9."}, {"title": "IV. RESULTS", "content": "For our comparisons, we required algorithms that used open datasets and publicly available code. Even though there are many recent advances in concept drift detection methods, such as Student Teacher concept for Unsupervised Drift Detection(STUDD) [16], we were not able to obtain the source code and hyperparameters necessary to implement and test it ourselves. We also want to reduce the influence the random factors on the results and run our experiments multiple times with the default hyperparameters provided by the authors. The accuracy metrics are computed based on the Interleaved Test-Then-Train method adopted by most drift detection algorithms including D3 and OCDD [17] For D3, the hyperparameter values are w = 100, \u03c1 = 0.1 and \u03c4 = 0.7, where w is the window size, \u03c1 is the percentage of w considered as the new window, and \u03c4 is the Area Under Curve(AUC)as explained by G\u00f6z\u00fca\u00e7ik et al. [13]. For OCDD, there are two hyperparameters: the window size w and \u03c1 denote the percentage of the window that could be outliers. The authors use w = 250 and \u03c1 = 0.3 [10]. The SUDS modifications of D3 and OCDD use the same parameters given by the authors. We chose accuracy as the metric, as the authors of the original papers used the same metric for comparison with existing methods.\nour model is proven to be more effective. We could compare our method against the original methods by ranking but ranking algorithms does not provide significant insights in our case due to the discrete nature of ranking. Analyzing the performance differences between these methods would yield better insights beyond mere rankings. Therefore, we compute the average difference of a method based on Equation 6.\n$Avg \\ Diff_{Method} = \\frac{1}{N} * \\sum_{i=1}^{N} Max_{i} - Method_{i}$ (6)\n$Method \\in \\{D3, D3(SUDS), OCDD, OCDD(SUDS)\\}$\n$Max_{i}$ = The best accuracy for Dataset i\n$Method_{i}$ = Accuracy for Dataset i with Method\nFrom Equation 6 we know that the average difference will be in [0,1] with 0 being the best and 1 being the worst. Looking at the scores, we see that D3 performs the best with only 3.6% average difference in accuracy to the best performing method of each dataset. Here, the SUDS modifications perform the worst with third and fourth places respectively. A deeper inspection reveals that this is due to the abrupt drifts present in the artificial datasets that causes the performance to drop. We validate this assumption by looking at only the real world datasets where the SUDS modifications are close to the best performing method overall which is D3. The OCDD method drops places even though it requires the most training data. Overall D3(SUDS) performs at a deficiency of approximately 1.5%, the method requires only 20% of the annotated data when compared to D3.\nWe also look at the difference between the original algorithms and their SUDS counterparts as well. In this case, the average difference in accuracy between D3 and its SUDS modification is 3.56% with D3 exhibiting better performance while that of OCDD and corresponding SUDS modification is 7.01%. When evaluating only real-world datasets, the difference shrinks to 1.53% between D3 and D3(SUDS), with the gap between OCDD and OCDD(SUDS) reducing to 1.61%. When we put this into the amount of annotated data required as well from Table V, we get a better picture. Although we have to accept an average performance drop of less than 2%, we require less than 20% of the annotated data of the corresponding algorithm. This is a huge factor for real-world applications where a slight drop in performance might be acceptable for a less time expensive method in terms of annotation time and its cost.\nWe can explain this reduced requirement for annotated data by examining the number of detected drifts in Table IV. It significantly affects the training data needed in real-world scenarios and also has an impact on the retraining time of the classifier. We see that our modifications D3(SUDS) and OCDD(SUDS) require the least amount of training data for every dataset, as shown in Table IV. If we look at a percentage-wise comparison of annotated data required, OCDD requires the most (over 90%), while D3 requires considerably less. Introducing our modifications further reduces the amount of annotated data required to the original methods by approximately 80%. OCDD(SUDS) detects fewer drifts for the artificial datasets likely due to the absence of noise in some datasets. However, detecting fewer drifts only sometimes means worse performance. For instance, in the \"Rotating Hyperplane,\" \"Sea Big,\" and \"Sea Stream\" datasets, OCDD(SUDS) requires approximately 150 annotated instances but either outperforms the best approach or is on par.\nThese examples demonstrate that an improved data sampling strategy can enhance performance while reducing the effort required for data labeling. We show the two different views with Table II and Table IV. The HADAM metric we introduce in Equation 5 and detailed in Table I balances performance and annotated data requirements. Therefore, using our method provides a better understanding on how an algorithm performs in a real-world scenario. Our modifications outperform existing methods in eleven of nineteen datasets. Notably, our methods struggle in artificial datasets lacking noise or with abrupt drifts. However, their performance improves notably when noise is introduced, as seen in the SEA concepts synthetic dataset with 10% noise added. Beyond raw performance, our formulated metric in Table I considers training data volume alongside performance. SUDS outperforms original algorithms concerning annotated data for drift events. OCDD predicts more drifts than D3, demanding more annotated data and retraining. Despite OCDD's superior raw performance, the increased labelling effort often outweighs its performance advantage. We can see this from Table V where our method incorporated into D3 requires the least amount of training data(01.68%). Here, we do not consider the labels required by the Interleaved-test-then-train method [23]. The amount of training data required by D3(SUDS), our modification to D3, is approximately one-third the amount of annotated data required by the original D3. Compared to OCDD, our modification OCDD(SUDS) requires one-sixth of the annotated data required by OCDD."}, {"title": "A. Hyperparameter Search", "content": "We further evaluate the performance of SUDS across different hyperparameter combinations. For the D3 algorithm, the authors identify three hyperparameters which are:\n\u2022 w is the window size considered as the old distribution\n\u2022 p is the fraction of the window size considered as the new distribution\n\u2022 AUC is the area under the curve acting as the threshold to signal a drift\nWe consider w = [50, 100, 150], \u03c1 = [0.1, 0.25, 0.5, 0.75, 1.0], and AUC = [0.6, 0.65, 0.7, 0.75, 0.8] as the range of hyperparameters for D3 with the bold value indicating the default values. For the window size (w), we evaluate both smaller and larger windows compared to the default value. Parameter p includes five different values to assess whether a larger sample size for the new distribution is beneficial overall. Additionally, we examine different threshold values to determine if raising the threshold and thus reducing the number of detected drifts improves the overall algorithm performance. We run the same hyperparameter combination three times and average it out to minimize the effect of outliers resulting in 225 runs for each dataset of the nineteen datasets. We then compute the HADAM for each hyperparameter combination based on the average accuracy and average training data required. The average of HADAM is computed across all datasets for both D3 and D3 with SUDS modification. We then plot this difference, where positive values indicate better performance by D3 with SUDS, and negative values indicate better performance by the original D3.\nFrom Figure 2a, our algorithm performs worse than the D3 algorithm only for the smallest \u03c1 value (0.1). The \u03c1 parameter decides the percentage of the window to be considered as the new distribution to check against the samples in the window. Having too few samples could cause the sampling mechanism to perform worse as it is the case with most machine learning models. Another reason could be that our model performs worse on the artificial datasets and this contributes to lowering the mean. To verify whether artificial datasets primarily contribute to the degradation of the SUDS algorithm, we tabulate the results for only the real-world datasets.\nFrom Figure 2b, the results show an improvement. All values are positive, indicating that our method performs better on average when compared to D3 w.r.t real world datasets. This could be due to the absence of sudden drifts which help in a better sampling strategy overall. Another observation is that SUDS performs better with a larger number of samples from the newer distribution.\nFor OCDD, there are only three hyperparameters,\n\u2022 v parameter to the One-Class SVM that defines \"the upper bound on the fraction of training errors and a lower bound of the fraction of support vectors\" [32].\n\u2022 w is the size of the window under consideration\n\u2022 p is the threshold for drift detection\nWe consider w = [150, 200, 250, 300], \u03c1 = [0.25, 0.3, 0.35] and v = [0.4, 0.5, 0.6] as the parameter values for the hyper-parameter search with the bold values indicating the default parameter values for the algorithm. From Figure 1, we see that our method performs better overall for all hyperparameter combinations."}, {"title": "V. CONCLUSION", "content": "We address the challenges of concept drift in unsupervised learning by proposing a novel approach to enhance an algorithm's adaptability to changing data distributions. We validate our approach through extensive experiments, which demonstrate its effectiveness. The results show that the strategic acquisition of homogeneous data points for retraining reduces the need for extensive annotated training data while maintaining performance levels comparable to conventional methods. Additionally, we introduce a new metric that combines the quantity of annotated data with model performance, providing a more practical assessment framework for real-world applicability. These findings highlight the potential of our approach to improve the efficiency and adaptability of unsupervised learning algorithms in dynamic environments, offering both enhanced performance and efficiency."}]}