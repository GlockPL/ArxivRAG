{"title": "ADVANCING ENTERPRISE SPATIO-TEMPORAL FORE-CASTING APPLICATIONS: DATA Mining MEETS IN-STRUCTION TUNING OF LANGUAGE MODELS FOR MULTI-MODAL TIME SERIES ANALYSIS IN LOW-RESOURCE SETTINGS", "authors": ["Sagar Srinivas Sakhinana", "Geethan Sannidhi", "Chidaksh Ravuru", "Venkataramana Runkana"], "abstract": "Spatio-temporal forecasting is crucial in transportation, logistics, and supply chain management. However, current methods struggle with large, complex datasets. We propose a dynamic, multi-modal approach that integrates the strengths of traditional forecasting methods and instruction tuning of small language models for time series trend analysis. This approach utilizes a mixture of experts (MoE) architecture with parameter-efficient fine-tuning (PEFT) methods, tailored for consumer hardware to scale up AI solutions in low resource settings while balancing performance and latency tradeoffs. Additionally, our approach leverages related past experiences for similar input time series to efficiently handle both intra-series and inter-series dependencies of non-stationary data with a time-then-space modeling approach, using grouped-query attention, while mitigating the limitations of traditional forecasting techniques in handling distributional shifts. Our approach models predictive uncertainty to improve decision-making. Our framework enables on-premises customization with reduced computational and memory demands, while maintaining inference speed and data privacy/security. Extensive experiments on various real-world datasets demonstrate that our framework provides robust and accurate forecasts, significantly outperforming existing methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Multivariate time series forecasting (MTSF) has many applications, but it faces challenges such as complex relationships between time series variables, non-linearity, sparsity, and non-stationarity. Spatio-temporal graph neural networks (STGNNs) improve forecast accuracy by modeling temporal dependencies within variables and interdependencies between variables. STGNNs utilize both explicit relationships based on predefined graphs provided by domain expert knowledge and implicit relationships derived from data-driven relational inference methods. While 'Human-in-the-loop' STGNNS Yu et al. (2017); Li et al. (2017); Guo et al. (2020) use prior knowledge in predefined graphs, they do not take into account latent variable relationships underlying the substantial data. On the other hand, 'Human-out-of-the-loop' STGNNs Deng & Hooi (2021); Wu et al. (2020); Kipf et al. (2018) jointly infer variable dependency graph structures and learn spatio-temporal dynamics from data, but they may underutilize expert-defined graphs, especially in noisy data scenarios, which can impact forecasting performance. However, existing methods rely on fixed historical window lengths that may not capture the diverse and complex time series patterns of varying lengths, and lack reliable uncertainty estimates. Transformers Vaswani et al. (2017), without a built-in bias towards pairwise variable dependencies, provide greater flexibility in modeling long-range dependencies beyond local spatial relationships, enabling the capture of global relationships. STGNNS introduce a stronger spatio-temporal inductive bias, while Transformers provide enhanced representational flexibility. Recent research indicates an opportunity to develop hybrid methods that combine explicit domain knowledge in priori-known graph structures with data-driven relational"}, {"title": "2 PROBLEM DEFINITION", "content": "Our study focuses on a dynamic system with N sensors collecting sequential data over T time intervals across F features, forming a spatio-temporal matrix $X \\in R^{N \\times T \\times F}$. These features include traffic attributes, such as speed, flow, and density. We denote the historical data for each sensor as"}, {"title": "3 EXPERIMENTS AND RESULTS", "content": ""}, {"title": "3.1 DATASETS", "content": "The study focuses on evaluating two frameworks: MultiTs Net and its variant w/Unc-MultiTs Net, using large-scale spatial-temporal traffic datasets (PeMSD3, PeMSD4, PeMSD7, PeMSD7(M), PeMSD8) from the Caltrans Performance Measurement System (PeMS)Chen et al. (2001). PeMS provides critical real-time and historical traffic data for California's freeways, aiding in traffic management, monitoring, and analysis. Our study converts 30-second interval data into 5-minute averages, following previous research methodsChoi et al. (2022), and also uses additional traffic flow datasets (METR-LA and PEMS-BAY)Li et al. (2018) converted into the same format. This approach allows for a consistent data format, enhancing the study's ability to analyze and model complex spatial-temporal data, and demonstrate superior performance over existing methodologies."}, {"title": "3.2 EXPERIMENTAL SETUP", "content": "In our study, we divided traffic-related datasets (PEMS-BAY and METR-LA) into three parts: training (70%), validation (10%), and testing (20%). Other datasets followed a 60%/20%/20% split. Before training, we standardized all time-series variables to a zero mean and unit variance. The models"}, {"title": "3.3 MULTISTEP FORECASTING RESULTS", "content": "Tables 2 and 3 compare two models, MultiTs Net and w/Unc-MultiTs Net, against various baselines in the MTSF task. The baseline results are reported from earlier studiesChoi et al. (2022); Wu et al. (2020). In a standard benchmark setting, we utilize historical data to predict estimates in a popular 12-sequence-to-12-sequence forecasting task. It involves using 12 time steps of historical data to forecast the value at the 12th future time step and then computing the forecasting errors. Both the MultiTs Net and its variant with local uncertainty estimation, w/Unc-MultiTs Net, demonstrate superior performance over the baselines. They achieve lower forecast errors and effectively capture complex MTS data dynamics. However, w/Unc-MultiTs Net, despite providing uncertainty estimates, slightly underperforms compared to MultiTs Net. shows the uncertainty estimations on framework forecasts on the benchmark datasets. Our framework forecasts consistently outperform baselines, as seen across all prediction horizons in Figure 2."}, {"title": "3.4 ABLATION STUDY RESULTS", "content": "The MultiTs Net is a unified framework designed to improve the accuracy and reliability of forecasting in multi-time series (MTS) data. An ablation study was conducted to evaluate the importance"}, {"title": "4 CONCLUSION", "content": "We propose a dynamic, cost-effective, and privacy-conscious hybrid approach for multi-horizon forecasting, specifically designed for private enterprise adoption. This approach integrates time series trend analysis using instruction-tuning of smaller language models with prompt-augmented, time series representation learning. This combination enables accurate and reliable forecasts, even in the presence of complex inter-variable relationships and non-stationarity. The method leverages"}, {"title": "5 TECHNICAL APPENDIX", "content": ""}, {"title": "5.1 PROPOSED METHOD", "content": ""}, {"title": "5.1.1\nMIXTURE OF PARAMETER-EFFICIENT EXPERTS", "content": "Low-Rank Adaptation (LoRA) Hu et al. (2022) is a parameter-efficient fine-tuning method for pre-trained language models that does not increase inference latency. It enables efficient, task-specific customization by incorporating a set of additional, lightweight trainable parameters into the existing architecture, of pretrained models, without modifying the original pretrained weights. Freezing the original weights helps mitigate catastrophic forgetting by preserving the extensive knowledge acquired by the pretrained models while learning new information. LoRA introduces a pair of low-rank weight matrices, known as adapters, alongside the frozen pretrained weights to capture task-specific information. Specifically, LoRA approximates the weight update of a linear layer as follows:\n$Y = (W_o + \\Delta W)X = (W_o + \\alpha B A)X$\nwhere $Y \\in R^{b \\times d_{out}}$ and $X \\in R^{b \\times d_{in}}$ are the output and input of a linear layer, respectively. $d_{in}$, $d_{out}$ are the input and output dimensions, respectively, and b is the batch size. $W_o \\in R^{d_{in} \\times d_{out}}$ is the pretrained weight matrix, $\\Delta W$ is the low-rank approximation of the weight update, and $\\alpha$ is a scaling constant. $B \\in R^{d_{in} \\times r}$, $A \\in R^{r \\times d_{out}}$ are projection-down and projection-up weight matrices, respectively. For $d_{in} = d_{out} = d$, the low-rank decomposition technique reduces the number of trainable parameters from $O(d^2)$ to $O(2dr)$, where $r < d$, thus yielding substantial memory savings. The rank r is a key hyperparameter for effective fine-tuning of large pretrained models using LORA Hu et al. (2022) for niche tasks, impacting computational complexity and adaptability to new tasks. However, LoRA suffers from high activation memory costs during task-specific adaptation, which are comparable to full-parameter fine-tuning due to the need to store large input activations (or intermediate outputs, like $X \\in R^{b \\times d_{in}}$) for the computation of gradients of low-rank matrices B and A during backpropagation. Current solutions include selective layer adaptation Hu et al. (2022) or activation recomputation Chen et al. (2016), but these methods may impact performance. In summary, vanilla LoRA enables efficient LLM adaptation through low-rank weight decomposition but faces challenges related to fine-tuning memory overhead. We further enhance the original LORA method by reducing the activation memory footprint further, without incurring extra computational costs. We achieve this by freezing the projection-down weight B, while redefining the projection-up weight A as the product of a pair of low-rank matrices, D and C, where $D \\in R^{r \\times \\delta}$ remains static, and $C \\in R^{\\delta \\times d}$ is updated during fine-tuning. This approach reduces trainable parameters and minimizes the size of input activations stored during fine-tuning, which are required for backward propagation during gradient computation, all without adding inference latency. In this approach, the input $X \\in R^{b \\times d_{in}}$ is initially mapped through $B \\in R^{d_{in} \\times r}$ and $D \\in R^{r \\times \\delta}$ to reduce its dimension to $\\delta$, before being projected back up through C. This approach significantly reduces the activation memory requirements by limiting the storage of input activation to the output of X transformed by matrix D, which is retained from the feed-forward pass to compute the gradient of C during backward propagation. We start with B and D initialized from a normal distribution, and C set to zero, while keeping the adaptation weight matrix $\\Delta W = BA = B(DC)$ initially at zero. During fine-tuning, only C is updated, which limits weight updates to a reduced column rank space ($\\delta$) defined by the output of D. In this work, we propose using a mixture of parameter-efficient experts (MoPEsZadouri et al. (2023)) to synergistically combine the advantages of applying a mixture of experts (MoEs) to parameter-efficient fine-tuning (PEFT) methods. This results in a parameter-efficient adaptation of the MoE approach. The LoRA variant achieves parameter efficiency and activation memory reduction, and the MoE architecture utilizes specialized experts (multiple LoRA variants) tailored for adapting to distinct aspects of the input data. With a mixture of experts, each targeting specific patterns in the input data, MoPEs enable more efficient fine-tuning of large pretrained models and enhance overall performance on complex tasks. MoPEs represent a family of neural network architectures that enable conditional computation through multiple experts (LoRA variants), activated based on a gating mechanism (router R). We denote the set of K experts as {$C_0 = E(X; \\theta_0),...,C_K = E(X; \\theta_K)$}, where $C_k$ is the k-th expert weight"}, {"title": "5.1.2\nFINE-TUNING SMALL-SCALE LMS", "content": "The Llama 2Touvron et al. (2023) is an advanced autoregressive, language-optimized transformer architecture, fine-tuned using supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human-centric values and preferences. It incorporates RMSNorm pre-normalization, PaLM-inspired SwiGLU activation functions, and rotary positional embeddings. Additionally, it utilizes a grouped-query attention mechanism, extending the input context length to 4096 tokens. The architecture consists of 32 layers and 32 attention heads, with a hidden size of 4096, and supports batch sizes of up to 32 for sequences up to 2048 tokens. We utilize zero-shot prompting of the Llama2-70B model to generate training data for time series trend analysis, enabling task-specific fine-tuning of smaller models. We perform instruction tuning on smaller models, such as Llama2-7B through the Quantized MoPEs technique, using the machine-generated data mentioned earlier, to efficiently customize them for niche time series trend analysis through transfer learning. This approach allows us to achieve both accuracy and efficiency comparable to that of the larger model. We have integrated MoPEs modules into each linear layer of the grouped-query attention layers in the Llama2-7B model architecture for efficient fine-tuning. Each layer typically captures different aspects of language, with lower layers often capturing basic syntactic information and higher layers capturing more complex semantic relationships, allowing for task-specific adaptation. Furthermore, the original weights of the Llama2-7B model hosted by Meta AI are in 16-bit format to reduce memory usage. We also apply 8-bit quantizationDettmers et al. (2023) to further compress the pretrained language model's parameters, significantly reducing memory and computational costs. We leverage paired input time series data and their corresponding Llama2-70B-generated textual summaries to instruct-tune a smaller Llama2-7B model and minimize standard cross-entropy loss to achieve similar performance with reduced resource consumption and increased interpretability. The Llama2-7B model compute expressive token embeddings to encapsulate both contextual information and semantic relationships between words or phrases. We freeze the fine-tuned Llama2-7B model and use a downstream, forecasting task-based, differentiable softmax attention pooling mechanism to derive text-level embeddings, represented as $H_{text} \\in R^{N \\times W \\times d}$, across a historical time window to compliment traditional forecasting method. Our innovative method aims to demystify the 'black-box' nature of the Llama2-70B by generating instruction-following data, thereby enhancing the Llama2-7B's capabilities in interpreting and analyzing time series data with increased precision and explainability through task-specific customization."}, {"title": "5.1.3\nDYNAMIC PROMPTING MECHANISM DESIGN", "content": "We present a dynamic prompting mechanism designed to enhance the adaptability and accuracy of traditional forecasting methods when dealing with complex time series data. The dynamic prompting mechanism consists of a predefined set of shared pools of prompts, stored as key-value pairs, with each prompt associated with specific time series characteristics, such as periodic trends, seasonality, cyclicality, and more. The prompting mechanism enables traditional methods to retrieve relevant prompts based on the evolving nature of time series data and apply learned patterns for forecasting tasks. This allows them to draw upon appropriate past knowledge and adapt to new, similar time series trends or patterns, ultimately leading to improved forecast accuracy. Traditional methods often struggle to adapt to dynamic, non-stationary data with distributional shifts. The ability to access and utilize the most relevant prompts from the shared pool to introduce appropriate time-series-specific prior knowledge significantly improves upon traditional methods. The shared pool of prompts encodes contextual information and insights learned from historical time series data stored as key-value pairs $(k_m, V_m)$ described as follows:"}, {"title": "5.1.4\nMODELING INTRA-SERIES DEPENDENCIES", "content": "We model the dependencies within each individual time series to enhance pointwise forecasts. We employ the Grouped-query multi-head attention (GQ-MHA) mechanism to capture non-linear, time-evolving dependencies. Our approach involves projecting the time series embedding $S_t \\in R^{N \\times W \\times d}$ for each of the N sensors into shared keys $(K_g)$, shared values $(V_g)$, and unique queries $(Q_{g,h})$ for each head(h) in the group(g), as follows:\n$K_g = S_t W_{Kg}, i = 1,..., N$\n$V_g = S_t W_{Vg}, i = 1, ..., N$\n$Q_{g,h} = S_t W_{Qgh}, i = 1, ..., N$.\nwhere the weight matrices $W_{K_g}$, $W_{V_g}$, and $W_{Q_{gh}}$ have dimensions $R^{d \\times d}$. Consequently, the dimensions of $K_g$, $V_g$, and $Q_{g,h}$ for each sensor i are $R^{W \\times d}$, respectively. The transformed time series embeddings are computed using the scaled dot-product attention mechanism, as follows:\n$Attention(Q, K, V) = softmax(\\frac{Q_{gh}(K_{gw})^T}{\\sqrt{d_k}}) V_w$\nwhere $d_k = \\frac{d}{H}$ is a scaling factor, and H is the total number of heads. We then perform aggregation across heads and groups to synthesize a concise representation of the time series data for each sensor as follows:\n$S = \\frac{1}{G} \\sum_{g=1}^G (Concat(Attention_1, ..., Attention_H)W_o)$"}, {"title": "5.1.5\nMODELING INTER-SERIES DEPENDENCIES", "content": "In highly intricate multivariate systems with intertwined dynamics, a hybrid approach that iteratively learns both intra-series and inter-series dependencies might be the most effective way to adequately"}, {"title": "5.1.6\nGRAPH CHEBYSHEV CONVOLUTION", "content": "Graph convolution is an effective method for processing graph-structured data, with spectral graph convolution Tanaka (2021) being notable but computationally intensive. To address this, Chebyshev Graph Convolution (CGC) Defferrard & Vandergheynst (2016) offers a more scalable alternative, leveraging Chebyshev polynomials to approximate spectral graph convolution, facilitating efficient convolutional filtering on graph-structured data using the Chebyshev polynomial approximation of the graph Laplacian. The Chebyshev polynomials are calculated based on the normalized Laplacian matrix of the predefined graph, denoted as $\\tilde{L} = \\tilde{D})^{-1/2} \\tilde{A}\\tilde{D}^{-1/2}$, where $\\tilde{A}$ is the normalized adjacency matrix, and $\\tilde{D}$ is the diagonal degree matrix of the graph. The Chebyshev approximation of the graph Laplacian to any degree is obtained using Chebyshev polynomials $T_k(L)$, where k represents the degree of the polynomial. The GCC operation can be defined as follows:\n$\\tilde{S} = \\sigma (\\sum_{k=0}^{K-1} T_k(\\tilde{L}) \\Theta_k)$\nwhere $\\sigma(\\cdot)$ is a non-linear activation function applied element-wise, $\\Theta_k \\in R^{d \\times d}$ is the trainable weight matrix for the k-th order Chebyshev polynomial, and K denotes the maximum order of the Chebyshev polynomials, which influences the expressive power of the approximation. $\\tilde{S}$ is the transformed time series embedding, which captures the spatial relationships within the graph. To regulate the information flow from $S_t$ (refer Equation equation 1) and $\\tilde{S_t}$ (refer Equation equation 2), we employ a gating mechanism that generates a weighted combination of these representations, denoted as $S_t$. Our hybrid architecture combines explicit domain-expert knowledge with implicit knowledge, extending beyond pairwise dependencies to capture the full complexity of spatio-temporal dependencies. Our approach, by modeling both local and global relationships in spatio-temporal data, enables accurate forecasting"}, {"title": "5.1.7\nOUTPUT LAYER", "content": "We employ the multi-head attention mechanism (MHA) Vaswani et al. (2017) to merge text-level and time series embeddings, thereby enhancing contextual understanding and alignment across different multi-domain embeddings. This integration improves the analysis and understanding of multi-time-"}, {"title": "5.1.8\nUNCERTAINITY ESTIMATION", "content": "We present a variant and extension of our proposed framework, $S_t^{\\phi+1}$ - Time Net for time series forecasting: w/Unc-MultiTs Net, with a focus on uncertainty estimation. The MultiTs utilizes a supervised learning approach to minimize the Mean Absolute Error (MAE) which quantifies the deviation between the framework's forecasts and actual data. The w/Unc-MultiTs Net extends this by assessing uncertainties in forecasts, with predictions modeled as a heteroscedastic Gaussian distribution, characterized by mean $\\mu_{\\phi}(S_t)$ and variance $\\sigma_{\\phi}(S_t)$. These parameters are derived using $\\mu_{\\phi}(S_t), \\sigma_{\\phi}(S_t) = f_\\theta(S_t)$, from a linear layer function $f_\\theta$ applied to the output of a multi-modal alignment output layer.\n$L_{GaussianNLLLoss} = \\frac{1}{T} \\sum_{t=1}^T (\\frac{log(\\sigma_{\\phi}(S_t)^2)}{2} + \\frac{(S_{t+1} - \\mu_{\\phi}(S_t))^2}{2\\sigma_{\\phi}(S_t)^2})$"}, {"title": "5.1.9\nIRREGULAR TIME SERIES", "content": "We focus on evaluating the effectiveness of the $S_t^{\\phi+1}$ - Time Net framework in handling missing data in large, complex sensor networks. The study simulates two types of missingness patterns: MCAR (Missing Completely At Random), representing random sensor failures, and block-missing, where data points are missing for a contiguous period. Block-missing patterns are defined by their length (the number of missing points) and frequency (how often they occur). These simulations help assess the framework's performance in 12-sequence-to-24-sequence forecasting tasks, with data missing-ness ranging from 10% to 50%. Tables 5 and 6 show the imputation results. The performance of the proposed models in the forecasting task was assessed by calculating the average error between pre-dicted and actual values over 12 future time steps. The results reveal that while the MultiTs Net performs well with lower missing data percentages, its accuracy declines as the level of missingness increases. However, its ability to condition forecasts on available observations without relying on imputed values demonstrates its resilience and effectiveness in capturing nonlinear spatio-temporal dependencies in sensor network data."}]}