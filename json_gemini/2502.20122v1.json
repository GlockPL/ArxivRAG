{"title": "Self-Training Elicits Concise Reasoning in Large Language Models", "authors": ["Tergel Munkhbat", "Namgyu Ho", "Seohyun Kim", "Yongjin Yang", "Yujin Kim", "Se-Young Yun"], "abstract": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, relative to their default behavior. To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings. Our combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy. By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training.", "sections": [{"title": "Introduction", "content": "Chain-of-thought (CoT) reasoning has significantly improved the ability of large language models (LLMs) to perform complex tasks (Wei et al., 2022b). The effectiveness of CoT reasoning has been attributed to the additional computation allocated during inference, as each intermediate reasoning token enables the model to perform an additional forward pass through its parameters (Nye et al., 2021; Wei et al., 2022b). On the other hand, this inherently incurs additional inference cost and latency, roughly proportional to the number of output tokens (Agrawal et al., 2024; Ho et al., 2024). We posit that current models often generate more tokens than necessary to accomplish the task, incurring extraneous inference costs. Typical reasoning chains sampled from the original model distribution, exemplified in the blue box of Figure 1, includes many tokens that do not contribute to the solution, specifically verbose explanations and repetitive phrasing. Similar observations have also been made in prior work (Renze and Guven, 2024; Zhang et al., 2024; Chiang and Lee, 2024a). We argue that the redundancy in reasoning chains of current models is not surprising. LLMs have not been explicitly trained to utilize intermediate tokens for reasoning in the most efficient manner. Rather, CoT reasoning is an emergent ability of LLMs (Wei et al., 2022a) first uncovered through novel few-shot prompting (Wei et al., 2022b). Post-hoc analysis suggests that reasoning ability is derived from procedural knowledge in pretraining data (Ruis et al., 2024), which is not optimized for conciseness. Therefore, it is natural that the default behavior of LLMs does not optimize for efficient use of tokens for reasoning. However, this does not preclude the possibility that LLMs possess the ability to reason more efficiently. Indeed, the distribution of reasoning path lengths shown in Figure 1 reveals a compelling insight: the presence of shorter, more efficient reasoning paths within the model's output distribution, marked in orange, suggests a latent capacity for concise reasoning. This observation motivates our central hypothesis: that by selectively leveraging these existing concise examples, we can fine-tune the model to elicit this latent ability and shift its default output distribution towards more efficient reasoning. In this paper, we propose a simple yet effective fine-tuning method to elicit efficient, concise reasoning in models for a given target task. We note that existing zero-shot prompting methods fails to reliably elicit concise reasoning, exhibiting near negligible impact on task-specialized models (Section 2.2). On the other hand, we can leverage best-of-N (BoN) sampling and few-shot conditioning (FS) to reliably generate concise training data (Section 3). We can then apply standard fine-tuning to distill the length-reducing benefits of BoN sampling and few-shot prompting back into the model itself while avoiding their associated overheads at inference time. Across GSM8K and MATH datasets and a wide variety of model families, our unified few-shot conditioned best-of-N sampling (FS-BoN) method achieves a substantial reduction in output length of 30% on average, a 2.4x improvement over previous fine-tuning baselines (De Sabbata et al., 2024), while preserving overall accuracy. Notably, our analysis shows that trained models adaptively adjust output length based on question complexity, preserving detail for difficult problems while simplifying responses to easier ones. We confirm that these results remain consistent across various model scales. These results strongly suggest that fine-tuning with carefully curated, self-generated data effectively can unlock latent concise reasoning abilities within LLMs, leading to significantly more cost-effective inference for complex tasks."}, {"title": "Preliminary Investigation", "content": "In this section, we probe the potential of current models to perform concise reasoning (Section 2.1) and investigate whether zero-shot prompting can reliably elicit such behavior (Section 2.2). We start with a definition of concise reasoning and key experimental setup.\nConcise reasoning We define concise reasoning in relative terms: for a given model, concise reasoning is to correctly reason through a given problem using less output tokens, compared to its default output. What is the default output? For our preliminary investigation, we measure the default output length of a model on a given question by taking the average length of correct paths over multiple stochastic generations. We use this measure instead of greedy decoding length, as the greedy path may be incorrect. We also consider the greedy output length as a noisy estimate, as it is highly sensitive to superficial input perturbations and numerical errors (Holtzman et al., 2019).\nKey experimental setup We analyze a wide range of moderately sized models, including both instruction-tuned language models and models specifically fine-tuned for mathematical reasoning, to assess robustness. We consider two reasoning tasks of moderate difficulty to assess preservation of model performance: GSM8K and MATH. We explain our full experimental setup in Section 4."}, {"title": "Method", "content": "To address the limitations of zero-shot prompting, we propose a simple yet robust fine-tuning (FT) approach to reliably elicit concise reasoning in LLMs while preserving accuracy. We focus on self-training approaches, to unlock and refine the latent concise reasoning abilities of current LLMs, observed in Section 2.1. Self-training not only removes external dependencies, but we posit that it helps preserve reasoning capability, as the training data originates from the model's own distribution.\nNaive Best-of-N Sampling (BoN) We first consider naive BoN sampling to collect relatively concise reasoning samples from the original output distribution, corresponding to the probability mass toward the left side of the distribution in Figure 2, for fine-tuning. Specifically, we generate N reasoning paths for each question in the original training dataset and select the shortest correct reasoning path for each question\u00b9. In contrast to selecting the shortest subset of correct reasoning paths from a combined pool of questions, our question-wise selection scheme ensures supervision across a wide range of difficulty levels, as difficult questions may require longer absolute reasoning lengths. Corresponding experimental results are in Section D.3.\nFew-Shot Conditioning For Effective Reduction While naive BoN sampling is a straightforward approach, its sample inefficiency (Xiang et al., 2025) makes it infeasible to achieve significant length reduction beyond a certain point. Figure 3 shows a log-linear relationship between N and output length reduction, indicating that length reduction through BoN incurs exponential generation costs.\nFew-shot conditioned sampling (FS) To mitigate the sample inefficiency of naive BoN, we can leverage few-shot prompting to bootstrap the reduction in output length. We consider three sources for few-shot exemplars: human-annotation (FS-Human), proprietary frontier LLMs (FS-GPT40), and self-generated samples (FS-Self). We use human annotated examples from Wei et al. (2022b), as they are readily available and demonstrate very concise reasoning. We provide additional details on few-shot example acquisition in Appendix B."}, {"title": "Sample Augmentation for Accuracy Boost", "content": "While few-shot prompting elicits concise reasoning, its adaptability is limited due to the small number of given examples. It may (1) prohibit generation of correct paths for very complex questions that require longer reasoning paths, while (2) eliciting extraneous steps that are not required for very easy questions. To address this, for each question, we augment the set of {1, N} sample(s) generated for FS and FS-BON, respectively, with N samples generated for naive BoN, and select the shortest correct path from the combined set. We find that this retains the length reduction from FS and FS-BON while better preserving accuracy, likely due to better coverage of difficult questions. We apply this augmentation by default to few-shot conditioned methods, with its effectiveness ablated in Figure 4."}, {"title": "Experimental setup", "content": "Models To account for realistic task-specific deployment settings, we select recent moderately sized post-trained models. We also consider math-specialized models to evaluate on models that have been optimized for specific task domains. We consider five main models for our key experiments: Llama-3.2-3B (Dubey et al., 2024), Gemma-2-2B (Team et al., 2024), Qwen2.5-3B (Yang et al., 2024a), Qwen2.5-Math-1.5B (Yang et al., 2024b), and DeepSeekMath-7B (Shao et al., 2024). We investigate scaling on Llama-3.2-{1B, 3B} and Llama-3.1-8B.\nTasks and datasets We focus on challenging reasoning tasks where (1) CoT reasoning significantly improves model performance, (2) only the final answer is relevant, and (3) models achieve moderate performance. Reasoning length reduction is desirable under the first and second conditions as it can reduce inference latency without affecting utility. The third condition is necessary to assess accuracy preservation. We consider two mathematical reasoning datasets: GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), where the models achieve accuracies of 40-90% and 20\u201370%, respectively. For evaluation, we utilize the test set of the GSM8K and MATH500 dataset (Lightman et al., 2024). We explain details in Section C.1.\nEvaluation metrics We evaluate methods using two primary metrics: accuracy and length. Accuracy is evaluated using Python-based parsing code, described in Section C.2. Length is defined as the average number of output tokens in all reasoning paths, including incorrect ones, as output tokens incur inference costs in deployment scenarios regardless of their correctness. We focus on number of output tokens for clear comparison, as number of input tokens are similar across methods, and output tokens affect wall-clock latency to a higher degree (Agrawal et al., 2024). We further justify this choice in Section C.3. We also employ relative accuracy and relative length metrics to better evaluate how well each method elicits concise reasoning while maintaining accuracy. Specifically, relative accuracy is the ratio of the given method's accuracy to the baseline accuracy, and relative length is the ratio of the given method's average length to the baseline average length, using a strong baseline zero-shot prompt (Pang et al., 2024). We use greedy decoding throughout evaluation to ensure reproducibility\u00b2."}, {"title": "Results", "content": "Main results\nOur main results, presented in Table 2 and Figure 4, demonstrate the performance of our self-training methods against baseline approaches.\nNaive BoN fine-tuning is effective but sample inefficient. Naive BoN fine-tuning effectively reduces output length without significantly degrading model performance. This also holds true for Qwen2.5-Math-1.5B and DeepSeekMath-7B (Table 8 and Table 9), which failed to achieve length reduction through zero-shot prompting. However, the length reduction from naive BoN with N = 16 is limited to 12% on average. Furthermore, as illustrated in Figure 3, achieving more compression with BoN becomes progressively less efficient.\nIterative baseline yields similar results as naive BoN fine-tuning. Rational Metareasoning, an iterative baseline, yields similar relative length reduction and relative accuracy to BoN fine-tuning. This suggests that the utility reward proposed by De Sabbata et al. (2024) may not effectively achieve both accuracy gains and token length reduction.\nFew-shot conditioning outperforms BoN in length reduction. The results demonstrate that few-shot conditioning achieves a greater relative length reduction compared to naive BoN, including math-specialized models (Table 8 and Table 9). This is in line with the superior length reduction of few-shot conditioning, compared to naive BoN as shown in Figure 3. Notably, self-training on generations conditioned on human-annotated examples (FS-Human) achieves an average relative length of 67.96% on GSM8K, compared to 87.17% with naive BoN.\nSelf-training better preserves accuracy than training with external data. Table 2 shows fine-tuning with external data (FT-External Data) leads to a significant reduction in relative length but causes a severe drop in relative accuracy. Figure 4 further highlights the accuracy preservation of self-training: fine-tuning with external concise reasoning supervision from GPT-40 (FT-GPT40) lies below the Pareto-curve of relative accuracy and relative length reduction, established by our self-training methods.\nFew-shot conditioned BoN achieves best length reduction while maintaining accuracy. FS-BoN elicits the largest length reduction among our self-training methods, while maintaining relative accuracy, on average. Notably, for math-specialized models, FS-GPT40-BoN achieves the greatest reduction among all methods, except those fine-tuned on external data which greatly sacrifice the accuracy (Table 8 and Table 9).\nAugmentation boosts accuracy for few-shot conditioning. Figure 4 compares few-shot conditioning, i.e., FS and FS-BoN, with and without augmentation (\u2020). Augmentation improves accuracy by providing solutions for previously unsolvable hard questions as discussed in Section 3.3. While augmentation may slightly affect reduction rates, they remain superior to naive BoN and RM. Even when matching the budget (Budget-Matched) with other self-training methods in Table 2, it achieves the greatest length reduction among them with minimal accuracy degradation. The effect of augmentation on training data length is analyzed in Section D.5."}, {"title": "Analysis", "content": "In this section, we analyze the length reduction effects in depth. We exclude DeepSeekMath-7B from quantiative analysis due to cost.\nTokens are reduced adaptively according to question complexity. The MATH dataset's difficulty levels range from 1 (basic algebra) to 5 (advanced calculus and complex mathematical reasoning). As shown in Figure 5, our method adaptively reduces tokens based on question difficulty, with higher difficulty leading to less reduction. The higher reduction (20%-40%) at easier difficulty levels (1-2) suggests that the original model outputs for these easier questions contained unnecessary tokens. This reveals a gap in current models' ability to tailor their inference budget to problem complexity. Our method effectively closes this gap by reducing redundancy, allowing for more precise token allocation based on question difficulty.\nSelf-training maintains consistency across model scales. We conduct a scaling study on Llama-3.2-1B, 3B, and Llama-3.1-8B to examine consistency across different model sizes (Figure 6). Overall, token reduction increases as the model size increases, while the maintenance of accuracy does not show a strong correlation with model size. RM exhibits lower reduction rates compared to our few-shot conditioned self-training methods across all models and shows a decrease in accuracy with increasing model size. Our standalone few-shot conditioning method (FS-GPT40) also shows a similar trend in length reduction, but better preserves accuracy. Our joint FS-GPT40-BoN method achieves the greatest reduction across all models, maintaining relative accuracy across different model sizes, especially in the largest 8B model.\nSample study Table 3 presents qualitative examples of reasoning paths generated by the model before and after fine-tuning with our method. The original reasoning exhibits verbosity, containing redundant processes such as question confirmation and repeated instructions. In contrast, the reasoning generated by our method includes only the necessary steps, significantly reducing the number of tokens while still arriving at the correct answer. More examples are provided in the Appendix E.\nLength reduction is transferred through fine-tuning. As shown in Figure 7, fine-tuning with shorter rationales results in shorter model outputs, showing a strong correlation between test and training lengths. We note that the length of test outputs (incorrect and correct) are longer than the length of training samples (only correct) on average. This is mainly because incorrect paths are generally longer than correct ones. We find a closer correspondence between train length and test length of correct samples only, indicated by the lighter datapoints. This discrepancy suggests the need to terminate incorrect paths early to minimize redundant inference overhead. We consider this for future work."}, {"title": "Discussion", "content": "Default reasoning behavior of LLMs Previous research has shown that the CoT reasoning ability of LLMs originates from procedural knowledge in pretraining data (Ruis et al., 2024). Modern LLM training pipelines utilize high-quality math, code, and synthetic reasoning data to enhance reasoning, but these do not promote conciseness (Dubey et al., 2024; Yang et al., 2024a). Furthermore, recent 'thinking' models are reinforced to use additional tokens to improve reasoning performance, rather than save on token budget (OpenAI, 2024). Thus, current LLMs naturally exhibit redundant reasoning. We believe that incorporating concise reasoning supervision or rewards in training pipelines can be beneficial for model efficiency, especially for 'thinking' models with lengthy internal reasoning.\nLightweight fine-tuning for concise reasoning elicitation In this paper, we focused on standard fine-tuning based on self-generated samples. Our analysis shows that LLMs already possess the ability to correctly reason in a relatively concise manner. Therefore, we posit that lightweight fine-tuning is sufficient to achieve significant reasoning length reduction, echoing the Superficial Alignment Hypothesis in the field of LLM alignment (Zhou et al., 2023), Indeed, our post-hoc analysis in Figure 7 shows that standard fine-tuning can reduce the output length of models in proportion to that of training samples."}, {"title": "Conclusion", "content": "We tackle redundancy in CoT reasoning, hypothesizing LLMs possess a latent capacity for concise reasoning, evidenced by shorter correct reasoning paths. We introduce fine-tuning methods, leveraging self-generated data from BoN sampling and few-shot conditioning, to elicit this capacity. Our FS-BON method significantly reduces reasoning length by 30% reduction while maintaining accuracy. This implies fine-tuning with curated self-generated data can reliably unlock latent concise reasoning, enabling more efficient inference.\nLimitations and Future Work\nAdvanced Training Schemes While our focus was on standard fine-tuning with self-generated samples, exploring advanced reinforcement learning (RL) based training schemes could potentially maximize efficiency further. Our preliminary experiments with expert iteration and BoN sampling (without a utility reward) showed promising results, boosting both accuracy and length reduction compared to non-iterative BoN. Further investigation of iterative or more advanced RL methods is warranted.\nFew-Shot Prompting Exploration Although we leveraged few-shot prompting to bootstrap length reduction during data generation, we did not explore advanced few-shot prompting methods as a middle ground between zero-shot and fine-tuning. While direct few-shot prompting with our prompts achieved comparable length reduction to fine-tuning on single-path few-shot generations, it incurred a slight accuracy loss. Critically, fine-tuning enabled us to incorporate BoN sampling and sample augmentation, leading to significantly greater length reduction without sacrificing accuracy. However, future work on integrating advanced exemplar selection (Fu et al., 2022) and many-shot prompting (Agarwal et al., 2024) could further enhance our fine-tuning approach.\nExtended Scaling Studies Our scaling study was limited to Llama 3.x models at 1B, 3B, and 8B parameters. While these model sizes are relevant for our task-specific setting, further empirical studies are needed to evaluate the effectiveness of our method on models exceeding 8B parameters. Investigating scaling trends across a wider range of model sizes is crucial for understanding the full potential of our approach.\nConcise Reasoning in General LLMs This study focused on task-specific fine-tuning. While existing zero-shot and fine-tuning methods often struggle with reliability and efficacy even in such settings, and our method proved effective, generalizing our approach to a broader range of tasks is an important direction. Exploring techniques like multi-task training could enable efficient reasoning without task-specific tuning. This could be particularly beneficial for \u2018thinking' models (Guo et al., 2025) where reasoning enhances the final response, similar to our math reasoning tasks but in a more general context. Furthermore, while prior work has explored reducing the number of reasoning stages (Chen et al., 2024), our method focuses on reducing verbal redundancy within sentences, offering a complementary approach."}, {"title": "Methodological Details", "content": "Self-Generation of Few-Shot Exemplars (FS-Self)\nWe devise a systematic approach to obtain few-shot exemplars using the default prompt (Appendix F.1) of concise reasoning from the target model itself, motivated by BoN sampling, through a two-phase process: sampling and selection.\nSampling Phase In the sampling phase, 128 questions were randomly selected from the training set, and for each selected question, 128 diverse reasoning paths were generated with a sampling temperature of T = 0.7. This process created a diverse set of candidate paths for both the GSM8K and MATH datasets.\nSelection Phase The selection process was executed through the following steps:\nSorting candidate paths by token count in ascending order.\nFiltering only correct samples by comparing oracle labels with the models' answers using the parsing code (Section C.2).\nSequential validation using GPT-4o with the following specialized prompt was performed until eight valid and concise examples were obtained. To ensure question uniqueness, alternative solutions from previously selected questions were excluded.\nYou are a math reasoning validator. Given a math problem, its answer, and a proposed rationale: First, determine if the problem requires step-by-step reasoning to solve.\nThen, validate if the rationale contains explicit step-by-step reasoning that correctly solves the problem.\nQuestion: [question here]\nExpected Answer: [label here]\nProposed Rationale: [rationale here]\nRespond with ONLY ACCEPT or REJECT:\nACCEPT - if the problem requires step-by-step reasoning AND the rationale contains explicit step-by-step reasoning that correctly leads to the answer.\nREJECT - if the problem is too simple (can be solved in one step).\nREJECT - if the rationale lacks explicit reasoning steps or contains incorrect logic.\nREJECT - if the rationale only states the answer.\nDataset-Specific Selection Criteria For GSM8K, eight valid examples were selected using the above selection phase. An example is shown in Table 17. For MATH, to avoid selecting only very easy questions, the selection was distributed across different categories: two examples from algebra (the largest category) and one example from each of the other categories (geometry, intermediate algebra, prealgebra, number theory, counting and probability, and precalculus). An example is shown in Table 18.\nGeneration of Few-Shot Exemplars From GPT-40 (FS-GPT40) To obtain few-shot exemplars of concise reasoning from GPT-40, we use our zero-shot prompt Hand Crafted 3 (Appendix F.1) to generate short reasoning paths. For GSM8K, we randomly sample eight questions from the training set. For MATH, we randomly select two questions from the largest category, Algebra and one from each of the other categories (Geometry, Intermediate Algebra, Pre-algebra, Number Theory, Counting & Probability, Precalculus), all from the training set. We manually filter low-quality outputs to ensure the quality of few-shot examples. However, we found that GPT-40 reliably followed the zero-shot prompt in the majority of cases. Generated examples of GPT-40 are in Tables 15 and 16."}, {"title": "Experimental Details", "content": "Datasets\nA summary of the datasets used in our experiments, along with their original licenses, is provided in Table 4. Both datasets are in English and focus on mathematical reasoning tasks. GSM8K contains grade school math word problems requiring multi-step reasoning, while MATH covers more advanced problems across various categories (algebra, geometry, precalculus, etc.) with different difficulty levels. Both datasets use straightforward language and standard mathematical notation. The original train/test splits are used for fine-tuning and evaluation. Results for MATH are reported on the MATH-500 subset, following the previous work (Lightman et al., 2024).\nAnswer Parsing\nFor accuracy, we extract and verify the final numerical answer using Python-based parsing code. We build previous implementations for GSM8K (Kojima et al., 2022) and MATH (Lai et al., 2024), with additional rules to ensure consistent parsing across all models. Our implementation additionally handles mathematical expressions by standardizing numerical formats, removing units and mathematical notation, and cleaning model-specific formatting to ensure consistent evaluation across models.\nDuring our experiments, we observed that different models require different parsing approaches to extract answers correctly for the MATH dataset. We implemented a model-specific parsing strategy where Gemma-2-2b and Qwen2.5-3B models utilize an enhanced parser, while all other models (including Llama, DeepSeekMath, and Qwen2.5-Math) use our standard parser. This adjustment led to more accurate model performance evaluation. All experiments use the appropriate parser for each model to ensure consistent evaluation.\nJustification For Length Metric\nWe define our length metric for model evaluation as the average number of output tokens in model responses across the entire evaluation set. For clear comparison, we do not include input tokens because (1) the number of input tokens do not differ significantly between zero-shot prompting and direct prompting on fine-tuned models, but more importantly (2) the real-world wall-clock latency incurred by each output token is significantly greater than that of each input token. The greater latency of output tokens stems from the sequential nature of their processing (the decode stage). Input tokens, in contrast, are processed in parallel (the pre-fill stage). The decode stage is heavily memory-bound, requiring parameter fetching for every forward pass, which significantly limits compute utilization (Agrawal et al., 2024). This difference in processing is also reflected in the pricing structures of commercial LLM API providers, where input tokens are cheaper than output tokens\u00b3."}, {"title": "Generation and Fine-Tuning", "content": "We implemented all model fine-tuning using the HuggingFace Trainer library and conducted generation using vLLM for optimized inference. For evaluation, we use greedy decoding, while for distribution analysis and training data generation we use temperature sampling with T=0.7, following Cobbe et al. (2021); Wang et al. (2023). We generate up to 512 output tokens on the GSM8K dataset and up to 1024 output tokens on the MATH dataset, following the token limits established in prior works (Dubey et al., 2024; Ren et al., 2024). For fine-tuning, we use a batch size of 16 and train for one epoch with a learning rate of le-5. The fine-tuning phase requires at maximum 469 training steps, making it computationally modest compared to the generation phase, as demonstrated in Table 5. For comparison with the iterative fine-tuning baseline MR (De Sabbata et al., 2024), we limit to 4 iterations as the authors observe performance generally plateaus after iteration 4. All experiments are conducted using bfloat16 precision for both generation and fine-tuning. We conduct our experiments on 8 NVIDIA H100 GPUs. Our total usage for main experiments is approximately 1,000 H100 GPU hours."}, {"title": "Additional results", "content": "Full Analysis on Reasoning Length Distribution (Section 2.1)\nWe analyze the distribution of output lengths across different model families using three metrics. First, we examine the absolute output token count distribution, which directly reflects the computational cost during inference. Second, we analyze the raw character count distribution to verify our findings are consistent across different tokenization schemes. Third, we study the normalized output token count, where each solution's output length is divided by the mean output length of correct solutions for that specific question, allowing us to identify relatively concise solutions.\nFigure 8 reveals significant variations in how different model families approach mathematical reasoning. On GSM8K, DeepSeekMath-7B demonstrates notably concise reasoning with a mean output length of 179 tokens, while Qwen2.5-3B uses substantially more tokens, averaging 298 tokens per solution. This pattern persists in the MATH dataset, though with generally longer output lengths due to the increased complexity of the problems."}, {"title": "Analysis", "content": "The presence of significant probability mass below 100% in the normalized distributions highlights an intriguing aspect of model behavior: while models may tend toward verbose outputs, they are not constrained to this verbosity and can successfully solve problems with more compact reasoning in some cases.\nFull Analysis on Efficacy of Zero-Shot Prompting Methods (Section 2.2)\nTable 6 shows evaluation results for zero-shot prompting methods on MATH. Prompting methods exhibit similar limitations as those on the GSM8K dataset.\nImpact of Question-Wise Selection We compare the performance of FS-GPT40-BON when selecting the shortest samples per question versus selecting the shortest samples across all questions. Both methods are tested without augmentation. The average relative accuracy across all models and datasets is 99.03% with question-wise selection and 96.03% without it, while the average relative length is 70.54% and 64.33%, respectively. Although selecting the shortest samples across all questions reduces relative length, it comes at the cost of accuracy. Notably, the accuracy of Gemma-2-2B drops significantly from 90.53% with question-wise selection to 75.81% without it. Therefore, we adopt question-wise selection to maintain question diversity and accuracy.\nFull Result Tables on Main Methods We present full results on the accuracy and reasoning length elicited by main methods for GSM8K in Table 8 and MATH in Table 9. We show absolute accuracy and output token length for individual models and show average over relative accuracy and relative length for aggregates across models.\nImpact of Augmentation on Training Data Length Table 7 demonstrates the impact of augmentation on training data length. Augmentation effectively expands the dataset by leveraging samples from zero-shot BoN. This is achieved by incorporating previously unsolvable long solutions, while also replacing solutions that can be solved with shorter solutions with zero-shot prompting. Consequently, the average solution length remains relatively stable, while the number of correct samples for fine-tuning increases, leading to improved model accuracy."}, {"title": "Sample Study", "content": "Tables 10-13 present additional examples of generated reasoning paths from both the initial model and the model fine-tuned using FS-GPT40-BoN. We primarily showcase results for the general model Llama-3.2-3b and the math-fine-tuned model Deepseek-7b, evaluated on the GSM8k and MATH500 datasets. One can observe a consistent reduction in token length in our fine-tuned models, achieved through decreased verbosity. This demonstrates the effectiveness of our method in improving inference efficiency."}, {"title": "Prompts", "content": "Zero-Shot Prompts\nFor each question, we use the following instructions as the system input to the language model if the model's chat template includes a system role. If no system role exists, we append the prompt to the user input instead.\nPrompting Template We used the default chat template for all models except Llama. For Llama, we modified its default chat template by removing the automatically added date information:\nCutting Knowledge Date: December 2023\nToday Date: 23 July 2024\nThis modification was made to ensure fair comparison across all models by maintaining consistent system prompt formatting.\nSystem Inputs:\nDefault (Pang et al., 2024)\nYour task is to answer the question below. Give step by step reasoning before you answer, and when you're ready to answer, please use the format \u2018The answer is'.\nBe Concise (Renze and Guven, 2024)\nYour task is to answer the question below. Give step by step reasoning before you answer, and when you're ready to answer, please use the format 'The answer is'. Be concise.\nEst. Budget (Han et al., 2024)\nBudget Estimation: Analyze the given question and estimate the minimum number of tokens required to generate a complete and accurate response. Please give the response by strictly following this format: [[budget]], for example, Budget: [[12]].\nToken-budget-aware CoT: Let's think step by step and use less than {budget_estimate} tokens.\nFixed Budget (Nayab et al., 2024)\nLet's think a bit step by step and limit the answer length to 100 words.\nHand Crafted 1 (ours)\nYour task is to answer the question below. Give step by step reasoning before you answer, and when you're ready to answer, please use the format 'The answer is'. You don't need unnecessary explanations; you can solve problems using only essential words and expressions. Summarize your thought process as simply as possible and provide your answer. Do not generate only the final answer.\nHand Crafted 2 (ours)\nYour task is to answer the question below. Give step by step reasoning before you answer, and when you're ready to answer, please use the format 'The answer is'. Use less words and more compact expressions to be concise.\nHand Crafted 3 (ours)\nGive concise step by step reasoning before you answer. Only retain key steps such as names, objects, numbers and mathematical operations. Use short plain words. Don't use any formatting such as emphasis, lists, or enumeration. Make sure that the intermediate results are presented in the order that they are calculated. After your reasoning, please use the format 'The answer is' to answer.\nHand Crafted 4 (ours)\nYour task is to answer the question below. Carefully solve the problem step by step, while using *as few words are possible*. Be careful about your choice of words. Use only necessary and essential steps. Avoid extra words. Avoid repetition. Avoid verbose statements. Avoid introductory remarks. When you're ready to answer, please use the format 'The answer is'.\nUser Input:\nQuestion: [question here]\nSolution:\nFew-Shot Prompts\nWe provide 8-shot examples for each question, following the approach of (Wei et al., 2022b), without any system instructions given to the language model. Each example is formatted in a chat-based template that includes both user and assistant roles. The order of examples for each question is randomly shuffled. We present one representative example for each prompt type, while the full set of examples is available in our public repository. Table 14-18 display one of eight examples used in FS-Human, FS-GPT40, and FS-Self."}, {"title": "Table 11", "content": "Evaluate $(1+2i)6-3i$\nQuestion\nReasoning Paths\nTo evaluate the expression, we need to follow the order of operations (PEMDAS): 1. Multiply 6 by the complex number 1+2i:\nSubtract 3i from the result:\nThe answer is 6 + 9i"}, {"title": "Zero-Shot", "content": "Table 13: Example rationales generated from the original Deekseek-7B model and the model fine-tuned using FS-GPT40-BON. The question is randomly sampled from MATH500.\nQuestion Convert the point (0, 3) in rectangular coordinates to polar coordinates. Enter your answer in the form (r, \u03b8), where r > 0 and 0 < \u03b8 < 2\u03c0.\nReasoning Paths To convert from rectangular coordinates to polar coordinates, we use the formulas $r = \\sqrt{x^{2} + y^{2}}$ and $\u03b8 = arctan(\\frac{y}{x})$. In this case, x ="}]}