{"title": "Self-Training Elicits Concise Reasoning in Large Language Models", "authors": ["Tergel Munkhbat", "Namgyu Ho", "Seohyun Kim", "Yongjin Yang", "Yujin Kim", "Se-Young Yun"], "abstract": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, relative to their default behavior. To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings. Our combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy. By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training.", "sections": [{"title": "Introduction", "content": "Chain-of-thought (CoT) reasoning has significantly improved the ability of large language models (LLMs) to perform complex tasks (Wei et al., 2022b). The effectiveness of CoT reasoning has been attributed to the additional computation allocated during inference, as each intermediate reasoning token enables the model to perform an additional forward pass through its parameters (Nye et al., 2021; Wei et al., 2022b). On the other hand, this inherently incurs additional inference cost and latency, roughly proportional to the number of output tokens (Agrawal et al., 2024; Ho et al., 2024). We posit that current models often generate more tokens than necessary to accomplish the task, incurring extraneous inference costs. Typical reasoning chains sampled from the original model distribution, exemplified in the blue box of Figure 1, includes many tokens that do not contribute to the solution, specifically verbose explanations and repetitive phrasing. Similar observations have also been made in prior work (Renze and Guven, 2024; Zhang et al., 2024; Chiang and Lee, 2024a). We argue that the redundancy in reasoning chains of current models is not surprising. LLMs have not been explicitly trained to utilize intermediate tokens for reasoning in the most efficient manner. Rather, CoT reasoning is an emergent ability of LLMs (Wei et al., 2022a) first uncovered through novel few-shot prompting (Wei et al., 2022b). Post-hoc analysis suggests that reasoning ability is derived from procedural knowledge in pretraining data (Ruis et al., 2024), which is not optimized for conciseness. Therefore, it is natural that the default behavior of LLMs does not optimize for efficient use of tokens for reasoning. However, this does not preclude the possibility that LLMs possess the ability to reason more efficiently. Indeed, the distribution of reasoning path lengths shown in Figure 1 reveals a compelling insight: the presence of shorter, more efficient reasoning paths within the model's output distribution, marked in orange, suggests a latent capacity for concise reasoning. This observation motivates our central hypothesis: that by selectively leveraging these existing concise examples, we can fine-tune the model to elicit this latent ability and shift its default output distribution towards more efficient reasoning. In this paper, we propose a simple yet effective fine-tuning method to elicit efficient, concise reasoning in models for a given target task. We note that existing zero-shot prompting methods fails to reliably elicit concise reasoning, exhibiting near negligible impact on task-specialized models (Section 2.2). On the other hand, we can leverage best-of-N (BoN) sampling and few-shot conditioning (FS) to reliably generate concise training data (Section 3). We can then apply standard fine-tuning to distill the length-reducing benefits of BoN sampling and few-shot prompting back into the model itself while avoiding their associated overheads at inference time. Across GSM8K and MATH datasets and a wide variety of model families, our unified few-shot conditioned best-of-N sampling (FS-BoN) method achieves a substantial reduction in output length of 30% on average, a 2.4x improvement over previous fine-tuning baselines (De Sabbata et al., 2024), while preserving overall accuracy. Notably, our analysis shows that trained models adaptively adjust output length based on question complexity, preserving detail for difficult problems while simplifying responses to easier ones. We confirm that these results remain consistent across various model scales. These results strongly suggest that fine-tuning with carefully curated, self-generated data effectively can unlock latent concise reasoning abilities within LLMs, leading to significantly more cost-effective inference for complex tasks."}, {"title": "Preliminary Investigation", "content": "In this section, we probe the potential of current models to perform concise reasoning (Section 2.1) and investigate whether zero-shot prompting can reliably elicit such behavior (Section 2.2). We start with a definition of concise reasoning and key experimental setup.\nConcise reasoning We define concise reasoning in relative terms: for a given model, concise reasoning is to correctly reason through a given problem using less output tokens, compared to its default output. What is the default output? For our preliminary investigation, we measure the default output length of a model on a given question by taking the average length of correct paths over multiple stochastic generations. We use this measure instead of greedy decoding length, as the greedy path may be incorrect. We also consider the greedy output length as a noisy estimate, as it is highly sensitive to superficial input perturbations and numerical errors (Holtzman et al., 2019).\nKey experimental setup We analyze a wide range of moderately sized models, including both instruction-tuned language models and models specifically fine-tuned for mathematical reasoning, to assess robustness. We consider two reasoning tasks of moderate difficulty to assess preservation of model performance: GSM8K and MATH. We explain our full experimental setup in Section 4."}, {"title": "Method", "content": "To address the limitations of zero-shot prompting, we propose a simple yet robust fine-tuning (FT) approach to reliably elicit concise reasoning in LLMs while preserving accuracy. We focus on self-training approaches, to unlock and refine the latent concise reasoning abilities of current LLMs, observed in Section 2.1. Self-training not only removes external dependencies, but we posit that it helps preserve reasoning capability, as the training data originates from the model's own distribution.\nNaive Best-of-N Sampling (BoN) We first consider naive BoN sampling to collect relatively concise reasoning samples from the original output distribution, corresponding to the probability mass toward the left side of the distribution in Figure 2, for fine-tuning. Specifically, we generate N reasoning paths for each question in the original training dataset and select the shortest correct reasoning path for each question\u00b9. In contrast to selecting the shortest subset of correct reasoning paths from a combined pool of questions, our question-wise selection scheme ensures supervision across a wide range of difficulty levels, as difficult questions may require longer absolute reasoning lengths. Corresponding experimental results are in Section D.3.\nFew-Shot Conditioning For Effective Reduction While naive BoN sampling is a straightforward approach, its sample inefficiency (Xiang et al., 2025) makes it infeasible to achieve significant length reduction beyond a certain point. Figure 3 shows a log-linear relationship between N and output length reduction, indicating that length reduction through BoN incurs exponential generation costs.\nFew-shot conditioned sampling (FS) To mitigate the sample inefficiency of naive BoN, we can leverage few-shot prompting to bootstrap the reduction in output length. We consider three sources for few-shot exemplars: human-annotation (FS-Human), proprietary frontier LLMs (FS-GPT4o), and self-generated samples (FS-Self). We use human annotated examples from Wei et al. (2022b), as they are readily available and demonstrate very concise reasoning. We provide additional details on few-shot example acquisition in Appendix B."}, {"title": "Sample Augmentation for Accuracy Boost", "content": "While few-shot prompting elicits concise reasoning, its adaptability is limited due to the small number of given examples. It may (1) prohibit generation of correct paths for very complex questions that require longer reasoning paths, while (2) eliciting extraneous steps that are not required for very easy questions. To address this, for each question, we augment the set of {1, N} sample(s) generated for FS and FS-BON, respectively, with N samples generated for naive BoN, and select the shortest correct path from the combined set. We find that this retains the length reduction from FS and FS-BON while better preserving accuracy, likely due to better coverage of difficult questions. We apply this augmentation by default to few-shot conditioned methods, with its effectiveness ablated in Figure 4."}, {"title": "Experimental setup", "content": "Models To account for realistic task-specific deployment settings, we select recent moderately sized post-trained models. We also consider math-specialized models to evaluate on models that have been optimized for specific task domains. We consider five main models for our key experiments: Llama-3.2-3B (Dubey et al., 2024), Gemma-2-2B (Team et al., 2024), Qwen2.5-3B (Yang et al., 2024a), Qwen2.5-Math-1.5B (Yang et al., 2024b), and DeepSeekMath-7B (Shao et al., 2024). We investigate scaling on Llama-3.2-{1B, 3B} and Llama-3.1-8B.\nTasks and datasets We focus on challenging reasoning tasks where (1) CoT reasoning significantly improves model performance, (2) only the final answer is relevant, and (3) models achieve moderate performance. Reasoning length reduction is desirable under the first and second conditions as it can reduce inference latency without affecting utility. The third condition is necessary to assess accuracy preservation. We consider two mathematical reasoning datasets: GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), where the models achieve accuracies of 40-90% and 20\u201370%, respectively. For evaluation, we utilize the test set of the GSM8K and MATH500 dataset (Lightman et al., 2024). We explain details in Section C.1.\nEvaluation metrics We evaluate methods using two primary metrics: accuracy and length. Accuracy is evaluated using Python-based parsing code, described in Section C.2. Length is defined as the average number of output tokens in all reasoning"}, {"title": "Results", "content": "Main results Our main results, presented in Table 2 and Figure 4, demonstrate the performance of our self-training methods against baseline approaches.\nNaive BoN fine-tuning is effective but sample inefficient. Naive BoN fine-tuning effectively reduces output length without significantly degrading model performance. This also holds true for Qwen2.5-Math-1.5B and DeepSeekMath-7B (Table 8 and Table 9), which failed to achieve length reduction through zero-shot prompting. However, the length reduction from naive BoN with N = 16 is limited to 12% on average. Furthermore, as illustrated in Figure 3, achieving more compression with BoN becomes progressively less efficient.\nIterative baseline yields similar results as naive BoN fine-tuning. Rational Metareasoning, an iterative baseline, yields similar relative length reduction and relative accuracy to BoN fine-tuning. This suggests that the utility reward proposed by De Sabbata et al. (2024) may not effectively achieve both accuracy gains and token length reduction.\nFew-shot conditioning outperforms BoN in length reduction. The results demonstrate that few-shot conditioning achieves a greater relative length reduction compared to naive BoN, including math-specialized models (Table 8 and Table 9). This is in line with the superior length reduction of"}, {"title": "Analysis", "content": "In this section, we analyze the length reduction effects in depth. We exclude DeepSeekMath-7B from quantiative analysis due to cost.\nTokens are reduced adaptively according to question complexity. The MATH dataset's difficulty levels range from 1 (basic algebra) to 5 (advanced calculus and complex mathematical reasoning). As shown in Figure 5, our method adaptively reduces tokens based on question difficulty, with higher difficulty leading to less reduction. The higher reduction (20%-40%) at easier difficulty levels (1-2) suggests that the original model outputs for these easier questions contained unnecessary tokens. This reveals a gap in current models' ability to tailor their inference budget to problem complexity. Our method effectively closes this gap by reducing redundancy, allowing for more precise token allocation based on question difficulty.\nSelf-training maintains consistency across model scales. We conduct a scaling study on Llama-3.2-1B, 3B, and Llama-3.1-8B to examine consistency across different model sizes (Figure 6). Overall, token reduction increases as the model size increases, while the maintenance of accuracy does not show a strong correlation with model size. RM exhibits lower reduction rates compared to our few-shot conditioned self-training methods across all models and shows a decrease in accuracy with increasing model size. Our standalone few-shot conditioning method (FS-GPT40) also shows a similar trend in length reduction, but better preserves accuracy. Our joint FS-GPT40-BoN method achieves the greatest reduction across all models, maintaining relative accuracy across different model sizes, especially in the largest 8B model.\nSample study Table 3 presents qualitative examples of reasoning paths generated by the model before and after fine-tuning with our method. The original reasoning exhibits verbosity, containing redundant processes such as question confirmation and repeated instructions. In contrast, the reasoning generated by our method includes only the necessary steps, significantly reducing the number of tokens while still arriving at the correct answer. More examples are provided in the Appendix E."}, {"title": "Discussion", "content": "Default reasoning behavior of LLMs Previous research has shown that the CoT reasoning ability of LLMs originates from procedural knowledge in pretraining data (Ruis et al., 2024). Modern LLM training pipelines utilize high-quality math, code, and synthetic reasoning data to enhance reasoning, but these do not promote conciseness (Dubey et al., 2024; Yang et al., 2024a). Furthermore, recent 'thinking' models are reinforced to use additional tokens to improve reasoning performance, rather than save on token budget (OpenAI, 2024). Thus, current LLMs naturally exhibit redundant reasoning. We believe that incorporating concise reasoning supervision or rewards in training pipelines can be beneficial for model efficiency, especially for 'thinking' models with lengthy internal reasoning."}, {"title": "Conclusion", "content": "We tackle redundancy in CoT reasoning, hypothesizing LLMs possess a latent capacity for concise reasoning, evidenced by shorter correct reasoning paths. We introduce fine-tuning methods, leveraging self-generated data from BoN sampling and few-shot conditioning, to elicit this capacity. Our FS-BON method significantly reduces reasoning length by 30% reduction while maintaining accuracy. This implies fine-tuning with curated self-generated data can reliably unlock latent concise reasoning, enabling more efficient inference."}, {"title": "Limitations and Future Work", "content": "Advanced Training Schemes While our focus was on standard fine-tuning with self-generated samples, exploring advanced reinforcement learning (RL) based training schemes could potentially maximize efficiency further. Our preliminary experiments with expert iteration and BoN sampling (without a utility reward) showed promising results, boosting both accuracy and length reduction compared to non-iterative BoN. Further investigation of iterative or more advanced RL methods is warranted.\nFew-Shot Prompting Exploration Although we leveraged few-shot prompting to bootstrap length reduction during data generation, we did not explore advanced few-shot prompting methods as a middle ground between zero-shot and fine-tuning. While direct few-shot prompting with our prompts achieved comparable length reduction to fine-tuning on single-path few-shot generations, it incurred a slight accuracy loss. Critically, fine-tuning enabled us to incorporate BoN sampling and sample augmentation, leading to significantly greater length reduction without sacrificing accuracy. However, future work on integrating advanced exemplar selection (Fu et al., 2022) and many-shot prompting (Agarwal et al., 2024) could further enhance our fine-tuning approach.\nExtended Scaling Studies Our scaling study was limited to Llama 3.x models at 1B, 3B, and 8B parameters. While these model sizes are relevant for our task-specific setting, further empirical studies are needed to evaluate the effectiveness of our method on models exceeding 8B parameters. Investigating scaling trends across a wider range of model sizes is crucial for understanding the full potential of our approach.\nConcise Reasoning in General LLMs This study focused on task-specific fine-tuning. While existing zero-shot and fine-tuning methods often struggle with reliability and efficacy even in such settings, and our method proved effective, generalizing our approach to a broader range of tasks is an important direction. Exploring techniques like multi-task training could enable efficient reasoning without task-specific tuning. This could be particularly beneficial for \u2018thinking' models (Guo et al., 2025) where reasoning enhances the final response, similar to our math reasoning tasks but in a more general context. Furthermore, while prior work has explored reducing the number of reasoning stages (Chen et al., 2024), our method focuses on reducing verbal redundancy within sentences, offering a complementary approach."}, {"title": "Methodological Details", "content": "Self-Generation of Few-Shot Exemplars (FS-Self) We devise a systematic approach to obtain few-shot exemplars using the default prompt (Appendix F.1) of concise reasoning from the target model itself, motivated by BoN sampling, through a two-phase process: sampling and selection.\nSampling Phase In the sampling phase, 128 questions were randomly selected from the training set, and for each selected question, 128 diverse reasoning paths were generated with a sampling temperature of T = 0.7. This process created a diverse set of candidate paths for both the GSM8K and MATH datasets.\nSelection Phase The selection process was executed through the following steps:\nSorting candidate paths by token count in ascending order.\nFiltering only correct samples by comparing oracle labels with the models' answers using the parsing code (Section C.2).\nSequential validation using GPT-4o with the following specialized prompt was performed until eight valid and concise examples were obtained. To ensure question uniqueness, alternative solutions from previously selected questions were excluded."}, {"title": "Experimental Details", "content": "Datasets A summary of the datasets used in our experiments, along with their original licenses, is provided in Table 4. Both datasets are in English and focus on mathematical reasoning tasks. GSM8K contains grade school math word problems requiring multi-step reasoning, while MATH covers more advanced problems across various categories (algebra, geometry, precalculus, etc.) with different difficulty levels. Both datasets use straightforward language and standard mathematical notation. The original train/test splits are used for fine-tuning and evaluation. Results for MATH are reported on the MATH-500 subset, following the previous work (Lightman et al., 2024).\nAnswer Parsing For accuracy, we extract and verify the final numerical answer using Python-based parsing code. We build previous implementations for GSM8K (Kojima et al., 2022) and MATH (Lai et al., 2024), with additional rules to ensure consistent parsing across all models. Our implementation additionally handles mathematical expressions by standardizing numerical formats, removing units and mathematical notation, and cleaning model-specific formatting to ensure consistent evaluation across models. During our experiments, we observed that different models require different parsing approaches to extract answers correctly for the MATH dataset. We implemented a model-specific parsing strategy where Gemma-2-2b and Qwen2.5-3B models utilize an enhanced parser, while all other models (including Llama, DeepSeekMath, and Qwen2.5-Math) use our standard parser. This adjustment led to more accurate model performance evaluation. All experiments use the appropriate parser for each model to ensure consistent evaluation.\nJustification For Length Metric We define our length metric for model evaluation as the average number of output tokens in model responses across the entire evaluation set. For clear comparison, we do not include input tokens because (1) the number of input tokens do not differ significantly between zero-shot prompting and direct prompting on fine-tuned models, but more importantly (2) the real-world wall-clock latency incurred by each output token is significantly greater than that of each input token. The greater latency of output tokens stems from the sequential nature of their processing (the decode stage). Input tokens, in contrast, are processed in parallel (the pre-fill stage). The decode stage is heavily memory-bound, requiring parameter fetching for every forward pass, which significantly limits compute utilization (Agrawal et al., 2024). This difference in processing is also reflected in the pricing structures of commercial LLM API providers, where input tokens are cheaper than output tokens\u00b3."}, {"title": "Generation and Fine-Tuning", "content": "We implemented all model fine-tuning using the HuggingFace Trainer library and conducted generation using vLLM for optimized inference. For evaluation, we use greedy decoding, while for distribution analysis and training data generation we use temperature sampling with T=0.7, following Cobbe et al. (2021); Wang et al. (2023). We generate up to 512 output tokens on the GSM8K dataset and up to 1024 output tokens on the MATH dataset, following the token limits established in prior works (Dubey et al., 2024; Ren et al., 2024). For fine-tuning, we use a batch size of 16 and train for one epoch with a learning rate of le-5. The fine-tuning phase requires at maximum 469 training steps, making it computationally modest compared to the generation phase, as demonstrated in Table 5. For comparison with the iterative fine-tuning baseline MR (De Sabbata et al., 2024), we limit to 4 iterations as the authors observe performance generally plateaus after iteration 4. All experiments are conducted using bfloat16 precision for both generation and fine-tuning. We conduct our experiments on 8 NVIDIA H100 GPUs. Our total usage for main experiments is approximately 1,000 H100 GPU hours."}, {"title": "Additional results", "content": "Full Analysis on Reasoning Length Distribution (Section 2.1) We analyze the distribution of output lengths across different model families using three metrics. First, we examine the absolute output token count distribution, which directly reflects the computational cost during inference. Second, we analyze the raw character count distribution to verify our findings are consistent across different tokenization schemes. Third, we study the normalized output token count, where each solution's output length is divided by the mean output length of correct solutions for that specific question, allowing us to identify relatively concise solutions. Figure 8 reveals significant variations in how different model families approach mathematical reasoning. On GSM8K, DeepSeekMath-7B demonstrates notably concise reasoning with a mean output length of 179 tokens, while Qwen2.5-3B uses substantially more tokens, averaging 298 tokens per solution. This pattern persists in the MATH dataset, though with generally longer output lengths due to the increased complexity of the problems."}, {"title": "Sample Study", "content": "To evaluate the expression, we need to follow the order of operations (PEMDAS): 1. Multiply 6 by the complex number 1+2i: $ [ (1+2i)6 = 6 + 12i ] $ 2. Subtract 3i from the result: $ [ (6 + 12i) - 3i = 6 + 9i ] $. The answer is 6 + 9i.\n[FS-GPT40-BoN] While working at the restaurant, each of the forty customers who came into the restaurant gave Rafaela a $20 tip. Julieta received 10% less money in tips than Rafaela. How much money did Julieta and Rafaela receive as tips altogether?\n[FS-GPT40-BoN] The answer is 6 + 9i."}, {"title": "Prompts", "content": "Zero-Shot Prompts For each question, we use the following instructions as the system input to the language model if the model's chat template includes a system role. If no system role exists, we append the prompt to the user input instead.\nPrompting Template We used the default chat template for all models except Llama. For Llama, we modified its default chat template by removing the automatically added date information:\nYour task is to answer the question below. Give step by step reasoning before you answer, and when you're ready to answer, please use the format \u2018The answer is'.\nFixed Budget (Nayab et al., 2024) Let's think a bit step by step and limit the answer length to 100 words."}]}