{"title": "Multi-View Majority Vote Learning Algorithms: Direct Minimization of PAC-Bayesian Bounds", "authors": ["Mehdi Hennequin", "Abdelkrim Zitouni", "Khalid Benabdeslem", "Haytham Elghazel", "Yacine Gaci"], "abstract": "The PAC-Bayesian framework has significantly advanced our understanding of statistical learning, particularly in majority voting methods. However, its application to multi-view learning remains underexplored. In this paper, we extend PAC-Bayesian theory to the multi-view setting, introducing novel PAC-Bayesian bounds based on R\u00e9nyi divergence. These bounds improve upon traditional Kullback-Leibler divergence and offer more refined complexity measures. We further propose first and second-order oracle PAC-Bayesian bounds, along with an extension of the C-bound for multi-view learning. To ensure practical applicability, we develop efficient optimization algorithms with self-bounding properties.", "sections": [{"title": "1. Introduction", "content": "Multi-view learning leverages multiple sets of features, or views, to enhance algorithmic performance and robustness (Sun, 2013; Xu et al., 2013; Zhao et al., 2017; Fang et al., 2023). For example, in image processing, combining visual data with depth or thermal information can improve object recognition tasks.\nHowever, while multi-view learning can improve learning outcomes, ensuring reliable generalization\u2014particularly across multiple views\u2014remains a critical challenge. To address this, researchers have explored generalization bounds for multi-view learning, often through the PAC (Probably Approximately Correct) framework (Blum & Mitchell, 1998; Dasgupta et al., 2001), and more recently using Rademacher complexity (Farquhar et al., 2005; Szedmak & Shawe-Taylor, 2007; Rosenberg & Bartlett, 2007; Sindhwani & Rosenberg, 2008; Rosenberg et al., 2009; Sun & Shawe-Taylor, 2010; Sun, 2011; Tian et al., 2021; Tang et al., 2023; Ma et al., 2024). Although these approaches have improved our understanding of multi-view learning, the PAC-Bayesian (PAC-Bayes) framework has emerged as especially effective for producing tighter generalization bounds in practical applications (P\u00e9rez-Ortiz et al., 2021).\nSun et al. (2017) laid the theoretical groundwork for integrating multiple views in the PAC-Bayes framework, introducing the first PAC-Bayes bounds for multi-view learning by combining weight vectors from different views. This approach leveraged complementary information across views for consistent predictions. They later incorporated stability (Bousquet & Elisseeff, 2002) into their analysis (Sun et al., 2022). While Sun et al. (2017) made significant strides by introducing PAC-Bayes bounds for multi-view learning, their approach is constrained to two views, limiting its applicability in scenarios where data comes from numerous sources. Goyal et al. (2017) addressed this by proposing a more flexible PAC-Bayes approach for multiple views using a two-level process: view-specific classifiers are learned first, then combined to produce final predictions, making it more applicable to real-world scenarios.\nDespite the promising multi-view PAC-Bayes bounds proposed by Goyal et al. (2017), the lack of a direct optimization method complicates their practical utility. In response, Goyal et al. (2019b) proposed minimizing the PAC-Bayes C-Bound (Lacasse et al., 2006) for individual views (Multi-view C-Bound, given in Lemma 1, Equation 3 of (Goyal et al., 2019b)) rather than addressing the more complex C-Bound in Theorem 2, Equation 3. This shift underscores the challenge of optimizing the C-bound, as stated by Viallard et al. (2021).\nAdditionally, Masegosa et al. (2020) introduced the concept of second-order oracle PAC-Bayes bounds in majority vote. These advanced bounds provide significantly improved precision over the first-order oracle bounds (Lacasse et al., 2006; Germain et al., 2015b), offering a novel analysis of the risk associated with weighted majority vote in multiclass classification, addressing the limitations of previous methods. Notably, while the work of Goyal et al. (2017; 2019b) primarily focuses on binary classification, extending their approach to incorporate these advanced second-order oracle PAC-Bayes bounds could yield valuable insights. Therefore, we propose the following contributions in the context of multi-view majority vote learning:"}, {"title": "2. Multi-view PAC-Bayesian Learning", "content": "We stand in the context of learning a weighted majority vote for multiclass classification. Consider a d-dimensional input space $X \\subset \\mathbb{R}^d$ and a finite label space $Y$. We assume an unknown data distribution $D$ on $X \\times Y$, with $D_x$ denoting the marginal distribution on $X$. A learning sample $S = \\{(x_i, y_i)\\}_{i=1}^m \\sim D^m$ is provided to the learning algorithm. Let $H$ be a hypothesis set consisting of so-called voters $h : X \\rightarrow Y$. The learner aims to find a weighted combination of the voters in $H$, where the weights are represented by a distribution over $H$. In the PAC-Bayes framework, we postulate a prior distribution $P$ over $H$. After observing $S$, the goal is to learn a posterior distribution $Q$ over $H$ used to construct a Q-weighted majority vote classifier, $B_Q(x) = \\arg \\max_{y\\in Y} [\\mathbb{E}_{h\\sim Q} [\\mathbb{I}(h(x) = y)]]$ (a.k.a. Bayes classifier), that minimizes the true risk $R_D \\triangleq \\mathbb{E}_{(x,y)\\sim D} [l(B_Q(x), y)]$, with the 0-1 loss $l(h(x),y) = \\mathbb{I}(h(x) \\neq y)$, where $\\mathbb{I}(.)$ is the indicator function. Since $D$ is unknown, a common way to try to minimize the true risk is the minimization of its empirical counterpart defined as $R_{D_S} \\triangleq \\frac{1}{m} \\sum_{i=1}^m l(B_Q(x_i), y_i)$.\nIn multi-view learning, data instances are represented or partitioned across $V > 2$ different views, where each view $\\upsilon \\in [V]$ (which denotes the set $\\{1,2,...,V\\}$) contains elements from $X^\\upsilon \\subset \\mathbb{R}^{d_\\upsilon}$. The combined dimensions of all views are represented by $d = d_1 \\times \u00b7\u00b7\u00b7 \\times d_V$. Each view contributes to the labeled sample as $S = \\{(x^\\upsilon, y_i)\\}_{i=1}^m \\sim D$. For each view $\\upsilon \\in [V]$, we consider a view-specific set $H^\\upsilon$ of voters $h : X^\\upsilon \\rightarrow Y$, with an associated prior distribution $P^\\upsilon$ for each view.\nAdditionally, a hyper-prior distribution $\\pi$ is defined over the set of views. The learner's dual objective is to optimize both the view-specific posterior distributions $Q^\\upsilon$ and the hyper-posterior distribution $\\rho$ over the views. This strategy aims to minimize the true risk $R_\\flat$ and its empirical counterpart $R_{D_S}$ of the multi-view weighted majority vote, defined as, $B(x) \\triangleq \\arg \\max_{y\\in Y} \\mathbb{E}_{\\upsilon \\sim \\rho} [\\mathbb{E}_{h\\sim Q^\\upsilon} [\\mathbb{I}(h(x^\\upsilon) = y)]]$. Here, the weighted majority vote is computed by taking the expectation over both the hyper-posterior $\\rho$ on the views and the posterior $Q^\\upsilon$ on the voters within each view.\nTo simplify the following sections, we introduce several abbreviations. In particular, we use $\\mathbb{E}E[\u00b7]$ to denote $\\mathbb{E}[\\mathbb{E}[.]]$, abbreviate $\\mathbb{E}_{(x,y)\\sim D}[\u00b7]$ to $\\mathbb{E}_D[\u00b7]$, $\\mathbb{E}_{x\\sim D_x}[\u00b7]$ to $\\mathbb{E}_{D_x}[\u00b7]$, represent $\\mathbb{E}_{S\\sim D^m}[\u00b7]$ by $\\mathbb{E}_{D_S} [\u00b7]$, simplify $\\mathbb{E}_{\\upsilon \\sim \\rho}$ to $\\mathbb{E}_\\rho [\u00b7]$, $\\mathbb{E}_{h\\sim Q}[\u00b7]$ to $\\mathbb{E}_Q[\u00b7]$, $\\mathbb{E}_{(\\upsilon,\\upsilon')\\sim \\rho^2} [\u00b7]$ to $\\mathbb{E}_{\\rho^2} [\u00b7]$ and $\\mathbb{E}_{(h,h')\\sim Q^2} [\u00b7]$ to $\\mathbb{E}_{Q^2} [\u00b7]$."}, {"title": "2.1. General Multi-view PAC-Bayesian bounds", "content": "The risk of $B_Q$ is known to be NP-hard (Lacasse et al., 2006; Redko et al., 2019); therefore, PAC-Bayes generalization bounds do not directly focus on the risk of $B_Q$. Instead, it provides an upper bound on the expectation of the true risks of all individual hypotheses under $Q$, which is known as the Gibbs risk $R_D \\triangleq \\mathbb{E}_D\\mathbb{E}_Q [l((h(x), y)]$. We propose PAC-Bayesian analysis in a multi-view setting to estimate the Gibbs risk $R_D \\triangleq \\mathbb{E}_{D,\\mathbb{E}_{Q^\\upsilon}} [l((h(x^\\upsilon), y)]$ from the empirical Gibbs risk $R_{D_S} \\triangleq \\frac{1}{m} \\sum_{i=1}^m \\mathbb{E}\\mathbb{E}_{Q^\\upsilon} [l((h(x_i^\\upsilon), y_i)]$ building on the work of Begin et al. (2016), who employed R\u00e9nyi divergence for PAC-Bayesian bounds, by extending it to multi-view learning. R\u00e9nyi divergence offers a broader, more adaptable measure compared to the traditionally used Kullback-Leibler divergence, thereby enhancing the flexibility of divergence measures between distributions (van Erven & Harremo\u00ebs, 2012; B\u00e9gin et al., 2016; Viallard et al., 2023). Following Goyal et al. (2017), we derive three foundational PAC-Bayesian approaches\u2014McAllester (1998), Catoni et al. (2007), Seeger (2003) and Langford (2005)\u2014to formulate bounds that are specifically tailored for multi-view settings using R\u00e9nyi divergence. Specifically, we present the Seeger/Langford bound (Seeger, 2003; Langford, 2005), known as the tightest bound (Germain et al., 2015a; Foong et al., 2021), in detail within the main text. Additional bounds based on the works Catoni et al. (2007) and McAllester (1998) are discussed in the Appendix B.\nCorollary 2.1 (PAC-Bayes-kl Inequality based on R\u00e9nyi Divergence, in the idea of Seeger/Langford's theorem (Seeger, 2003)). Let $V \\geq 2$ be the number of views. For"}, {"title": "2.2. First Order Multi-view PAC-Bayesian Bounds", "content": "When $B_Q(.)$ misclassifies an instance $x$, it implies that at least half of the classifiers (according to the distribution $Q$) have made an error on that instance. As a result, we can bound the true risk $R_D$ by twice the Gibbs risk $R_D$, i.e., $R_D \\leq 2R_D$. This is commonly referred to as the first-order oracle bound (Germain et al., 2015a; Masegosa et al., 2020). This relationship can also be generalized to the multi-view learning framework, yielding the inequality:\nTheorem 2.2 First Order Multi-view Oracle Bound (Goyal et al., 2017)\n$R_D \\leq 2R_D$.\nIn this section, we extend the first-order multi-view oracle bound to empirical bounds by leveraging the PAC-Bayes-kl inequality, with the R\u00e9nyi Divergence previously introduced. The next theorem provides a relaxation of the PAC-Bayes-kl inequality, which is more convenient for optimization. The upper bound is due to Thiemann et al. (2017), while the lower bound was proposed by Masegosa et al. (2020). Therefore, we propose adapting Thiemann et al. (2017)'approches to the multi-view PAC-Bayes. See the Appendix C for the proof.\nTheorem 2.3 Multi-view PAC-Bayes-\\$\\lambda$ Inequality, in the idea of Thiemann et al. (2017)'s theorem. Under the same assumption of Corollary 2.1 and for all $\\lambda\\in (0,2)$ and $\\gamma > 0$ we have:"}, {"title": "2.3. Second Order Multi-view PAC-Bayesian Bounds", "content": "The first order oracle bound ignores the correlation of errors, which is the main power of the majority vote. Furthermore, this bound is tight only when the Gibbs risk is low (Langford & Shawe-Taylor, 2002). In order to take correlation of errors into account, Lacasse et al. (2006) derived the C-Bound, which is based on the Chebyshev-Cantelli inequality. The concept was further developed by Laviolette et al. (2011; 2017), Germain et al. (2015a), and extended to multi-view learning by Goyal et al. (2017). Masegosa et al. (2020) extended this idea with a second-order oracle bound, based on the second-order Markov's inequality, positing that $R_D \\leq 4 e_b$. For multi-view, we propose the followin theorem (a proof of this relation is available in the appendices, see D),\nTheorem 2.6 Second Order Multi-view Oracle Bound (Goyal et al., 2017)\n$R_D \\leq 4e_b$.\nAs stated in Section 2.2, we propose the following corollary"}, {"title": "3. Multi-view PAC-Bayesian C-Bounds", "content": "In this section, we recall PAC-Bayesian generalization bounds on the C-Bound referred to as the PAC-Bayesian C-Bounds. The first, is based on the Seeger (2003)'s approach that we adapt in multi-view as proposed by Goyal et al. (2017). The second is the C-Tandem Oracle Bound using the form proposed by Lacasse et al. (2006) (PAC-bound 1). We adapt this bound to multi-view with Seeger (2003)'s approach.\nTheorem 3.1 (Multi-view PAC-Bayesian C-Bound)\nUnder the same assumption of Theorem 2.11, we have:"}, {"title": "4. Self-Bounding Algorithms", "content": "In this section, we propose self-bounding algorithms that directly minimize the multi-view PAC-Bayesian bounds.\n4.1. Optimization of PAC-Bayes-\\$\\lambda$ inequality Bounds\nFirst, we propose an optimization approach based on the PAC-Bayes-\\$\\lambda$ inequality, incorporating methods from Thiemann et al. (2017). Unlike the optimization procedure proposed by Masegosa et al. (2020), we impose constraints based on the work of Germain et al. (2015a) and Viallard et al. (2021). The bounds proposed with Thiemann et al. (2017)'s approach parameterize the trade-off between empirical risk and divergence, introducing the $\\lambda$ parameter. In the optimization procedure, the choice of $\\lambda$ as a gradient parameter can be made in two ways: the primary method is outlined in our Algorithm 1, with the secondary choice being to calculate it using the methods described by Thiemann et al. (2017) and Masegosa et al. (2020). The entire optimization procedure is detailed in Algorithm 1.\nWe aim to minimize the following constrained objective functions derived from the Pac-Bayes-\\$\\lambda$ inequality bounds from Corollaries 2.4, 2.7, 2.8:"}, {"title": "4.2. Optimization of Inverted KL Bounds", "content": "The main challenge in optimizing the multi-view first- and second-order inverted KL bounds is to evaluate KL and $\\overline{KL}$ and to compute their derivatives. To achieve this, we employ the bisection method proposed by Reeb et al. (2018) (also used in (Viallard et al., 2021)) for calculating KL. This method is outlined in the functions Compute-KL($q||\\psi$), Compute-KL($q||\\overline{\\psi}$) of Algorithm 3 in the Appendix F. It involves iteratively refining an interval [$p_{min}$, $p_{max}$] such that $p \\in [p_{min}, p_{max}]$ and KL($q || p) = \\psi$.\nWe aim to minimize the following constrained objective functions derived from the inverted KL bounds from Corollaries 2.5, 2.9, 2.10 and 2.10:"}, {"title": "4.3. Optimization of Multi-view PAC-Bayesian C-Bound", "content": "In this section, we present self-bounding algorithms to directly minimize the PAC-Bayesian C-Bounds. We aim at minimizing the following constraint optimization problem:"}, {"title": "5. Experiments", "content": "Multi-view Datasets. We use a total of 10 datasets\u00b9 to evaluate the effectiveness of our method. Some datasets were originally multi-view, while others were mono-view and required transformation and feature extraction to fit our multi-view setting.\u00b2 Datasets with multiple classes were utilized to optimize both multi-classification and binary classification bounds. For more detailed information about each dataset and its source, refer to Table 1 in the Appendix.\nExperimental Setup.\u00b3 Inspired by (Masegosa et al., 2020) and (Goyal et al., 2017), we assessed the practical effectiveness of our bounds using standard random forests (Breiman, 2001), each consisting of 100 trees. We experimented with three configurations (Stump, Weak, and Strong learners). For each dataset, we reserved 20% of the data as a test set, denoted as $S_{test}$. To ensure reliability, we conducted extensive experiments and report the mean and standard deviation across 10 repetitions of each experiment. Further details on the experimental setup and hyperparameters are provided in Appendix G.2 and G.3.\nResults. Figures 1, and 2 displays the optimized Bayes risk and bound values for each of our proposed self-bounding algorithms, allowing comparisons across individual views, the concatenated view, and the multi-view setting. For single-view experiments, some methods were adapted from previous work Masegosa et al. (2020); Viallard et al. (2021), while others, such as the first- and second-order inverted KL bounds, are newly implemented.\nWe primarily report results for the \"mfeat-large\" dataset in both binary and multi-class classification scenarios. This dataset offers the most views and the largest number of samples among those we considered, providing a rich multi-view setting and enhancing the statistical significance of our results. Additionally, our approach demonstrates strong performance on this dataset, effectively showcasing the advantages of our methods over single-view and concatenated-view approaches.\nBy focusing on the \"mfeat-large\" dataset in the main paper, we aim to illustrate the benefits of our proposed algorithms in a complex, multi-view context. Results on the other datasets are included in the Appendix G. To save space, only the concatenated and multi-view subplots are included for the multi-class plot; complete results can be found in the Appendix.\nWe note that the slashed bars (\\) represent the Bayes risks"}, {"title": "6. Conclusion and Limitations", "content": "Our approach has several limitations. First, the R\u00e9nyi divergence used in our bounds is limited to $\\alpha > 1$, restricting its utility. Specifically, for $\\alpha > 1$, it exceeds the Kullback-Leibler (KL) divergence (van Erven & Harremo\u00ebs, 2014), potentially leading to looser bounds. We cannot use $\\alpha = \\frac{1}{2}$ (the Hellinger distance) due to theoretical constraints, which may prevent our bounds from being tighter than those based on the KL divergence. Second, we do not include the R\u00e9nyi divergence on the left side of our equations, limiting our bounds' flexibility. Symmetrically incorporating it could yield tighter and more adaptive bounds. Future work could explore integrating the R\u00e9nyi divergence on both sides to assess its impact on bound tightness and generalization.\nTo avoid the complexity from the hierarchy of views, merging the views before learning\u2014early fusion\u2014might be advantageous, perhaps using a Variational Autoencoder (VAE) as in (Mbacke et al., 2024).\nLastly, we explored poisoning the views by adding noise (see Appendix H.1.2 and Figure 15). We observed that the weight assigned to the views shifted towards those without noise, making our multi-view methods particularly effective in this context (Viallard et al., 2024). However, we have not fully exploited adversarial PAC-Bayes approaches; we only considered poisoning the views with noise.\nIn summary, while our method has limitations and doesn't always yield the expected results\u2014especially compared to the concatenated view\u2014we have introduced new multi-view PAC-Bayes bounds and optimization algorithms based on majority voting. This represents a new direction in algorithm design and their generalization guarantees."}, {"title": "A. Mathematical Tools", "content": "Theorem A.1 Markov's Inequality. For any random variable X such that $\\mathbb{E}[|X|] = \\mu$, for any $a > 0$, we have\n$\\mathbb{P}\\{|X| \\geq a\\} < \\frac{\\mu}{a}$\nTheorem A.2 Second Order Markov's Inequality. For any random variable X with a finite second moment, i.e., $\\mathbb{E}[X^2] < \\infty$, and for any $a > 0$, we have\n$\\mathbb{P}\\{X \\geq a\\} \\leq \\frac{\\mathbb{E}[X^2]}{a^2}$\nTheorem A.3 Jensen's Inequality. For any random variable X, and for any concave function $\\varphi$, we have\n$\\varphi(\\mathbb{E}[X]) \\geq \\mathbb{E}[\\varphi(X)]$.\nAdditionally, for any convex function $\\varphi$, we have\n$\\varphi(\\mathbb{E}[X]) \\leq \\mathbb{E}[\\varphi(X)]$.\nTheorem A.4 Cantelli-Chebyshev Inequality. For any random variable X such that $\\mathbb{E}[X] = \\mu$ and $Var[X] = \\sigma^2$, and for any $a > 0$, we have\n$\\mathbb{P}\\{X - \\mu \\geq a\\} \\leq \\frac{\\sigma^2}{\\sigma^2 + a^2}$\nTheorem A.5 H\u00f6lder's Inequality. For any random variables X and Y, and for any positive real numbers p and q such that $\\frac{1}{p} + \\frac{1}{q} = 1$, we have\n$\\mathbb{E}[|XY|] \\leq (\\mathbb{E}[|X|^p])^{\\frac{1}{p}} (\\mathbb{E}[|Y|^q])^{\\frac{1}{q}}$\nProposition A.6\n$D_\\alpha(Q^2 || P^2) = 2 D_\\alpha(Q || P)$"}, {"title": "B. A proof of General Multiview PAC-Bayesian Theorem based on the R\u00e9nyi Divergence", "content": "To demonstrate the three most popular PAC-Bayes approaches\u2014 McAllester (1998); Catoni et al. (2007); Seeger (2003); Langford (2005)\u2014we rely on a general PAC-Bayesian theorem, as proposed by Germain et al. (2009; 2015a), adapted to the multi-view learning framework with a two-hierarchy of distributions on views and voters, as proposed by Goyal et al. (2017). In our study, we integrate the R\u00e9nyi divergence, as suggested by B\u00e9gin et al. (2016). An important step in PAC-Bayes proofs involves the use of a measure-change inequality, based on the Donsker-Varadhan inequality (Donsker & Varadhan, 1975). The lemma below extends this tool to our multi-view framework using the R\u00e9nyi divergence.\nLemma B.1 (Multi-view R\u00e9nyi change of measure).\nFor any set of priors $\\{P^\\upsilon\\}_{\\upsilon=1}^V$ and any set of posteriors $\\{Q^\\upsilon\\}_{\\upsilon=1}^V$, for any hyper-prior distribution $\\pi$ over $[V]$ and hyper-posterior distribution $\\rho$ over $[V]$, and for any measurable function $\\phi : H^\\upsilon \\rightarrow \\mathbb{R}$, we have:\n$\\mathbb{E}_{\\upsilon\\sim \\rho} \\mathbb{E}_{h\\sim Q^\\upsilon} [\\phi(h)] \\leq \\mathbb{E}_{\\upsilon\\sim \\rho} [D_\\alpha(Q^\\upsilon||P^\\upsilon)] + D_\\alpha(\\rho||\\pi) + \\ln ( \\mathbb{E}_{h\\sim P^\\upsilon} [e^{\\alpha \\phi(h)}])^{\\frac{1}{\\alpha}}$\nwhere $D_\\alpha (Q||P)$ is the R\u00e9nyi divergence of order $\\alpha > 1$ between the distributions $Q$ and $P$.\nThe proof utilizes techniques from the proof presented in Theorem 17 by Germain et al. (2016) and incorporates the method used in Theorem 8 proposed by B\u00e9gin et al. (2016):"}]}