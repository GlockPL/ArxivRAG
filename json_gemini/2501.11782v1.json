{"title": "Experiment Report: Human-AI Collaborative Game Testing with Vision Language Models", "authors": ["Boran Zhang", "Muhan Xu", "Zhijun Pan"], "abstract": "As modern video games become increasingly complex, traditional manual testing methods are proving costly and inefficient, limiting the ability to ensure high-quality game experiences. While advancements in Artificial Intelligence (AI) offer the potential to assist human testers, the effectiveness of AI in truly enhancing real-world human performance remains underexplored. This study investigates how AI can improve game testing by developing and experimenting with an AI-assisted workflow that leverages state-of-the-art machine learning models for defect detection. Through an experiment involving 800 test cases and 276 participants of varying backgrounds, we evaluate the effectiveness of AI assistance under four conditions: with or without AI support, and with or without detailed knowledge of defects and design documentation. The results indicate that AI assistance significantly improves defect identification performance, particularly when paired with detailed knowledge. However, challenges arise when AI errors occur, negatively impacting human decision-making. Our findings show the importance of optimizing human-AI collaboration and implementing strategies to mitigate the effects of AI inaccuracies. By this research, we demonstrate AI's potential and problems in enhancing efficiency and accuracy in game testing workflows and offers practical insights for integrating AI into the testing process.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid growth of the gaming industry has led to the development of increasingly complex and immersive video games, characterized by dynamic mechanics, interactive narratives, and expansive open worlds [1]\u2013[4]. Ensuring the functionality and quality of such intricate games requires comprehensive testing. However, traditional manual game testing approaches are becoming insufficient due to the scale and complexity of modern games. These methods are time-consuming, resource-intensive, and prone to human error, reducing their effectiveness in identifying a broad range of defects [5]\u2013[7].\nWhile average gamers can serve as video game playtesters, Artificial Intelligence (AI) has emerged as a promising solution to enhance game testing workflows by automating defect detection processes and reducing reliance on manual efforts. Early AI approaches in game testing focused on rule-based automation to manage repetitive tasks like regression testing. However, these systems struggled to adapt to the emergent gameplay dynamics and player behaviors found in complex gaming environments [1], [8], [9]. Recent advancements in machine learning, particularly reinforcement learning, have enabled AI systems to autonomously simulate player interactions and learn from game environments, improving both the realism and robustness of test cases [6], [10]\u2013[13]. These AI systems are capable of detecting a wide spectrum of defects, ranging from visual glitches to gameplay inconsistencies, enhancing the depth and efficiency of game testing.\nA key advancement in AI-driven testing is the integration of multimodal inputs-visual, audio, and textual data which allows AI systems to comprehensively analyze gameplay environments. By leveraging these diverse data types, AI systems can detect graphical anomalies, such as texture misalignments and rendering errors, while simulating player behaviors to uncover gameplay issues [3], [7], [14]\u2013[18]. However, while AI systems offer enhanced capabilities, they also present challenges, particularly in misinterpreting complex game elements, which can lead to inaccurate defect identification or even hallucinations\u2014where the AI erroneously flags non-defective elements as bugs. These hallucinations, compounded by human testers' reliance on AI, can further complicate the testing process [19]\u2013[23]. Solutions to mitigate AI hallucinations are actively being explored to enhance the reliability of AI systems [24], [25].\nCurrent AI-driven game testing approaches can be classified into two main categories: rule-based automation and AI-driven perception systems [26]. Rule-based systems, which simulate player actions by interacting directly with the game's source code, require substantial technical expertise and may interfere with game performance during testing [3], [27]. In contrast, AI-driven perception systems utilize machine vision and multimodal models to detect visual defects, such as rendering errors, and analyze gameplay mechanics [12], [15]. While effective, these systems may still struggle with unpredictable player behavior and nuanced gameplay scenarios, areas where human testers excel [13], [16], [28].\nDespite the advances in AI, the extent to which AI enhances human performance in real-world game testing, particularly under various conditions, remains underexplored. This study aims to address this gap by investigating the impact of AI on human testers' performance across different levels of knowledge and experience, and by exploring how AI-generated insights can improve defect detection while managing risks like hallucinations.\nThe contributions of our research are as follows:\n\u2022 AI-Assisted Game Testing Workflow: We present an AI-assisted workflow that integrates machine learning to detect defects in game visuals, user interfaces, and gameplay mechanics.\n\u2022 Empirical Insights into Human-AI Collaboration: Through a comprehensive user study, we provide empirical evidence demonstrating how AI enhances game testers under various conditions, especially when paired with detailed knowledge and human oversight.\n\u2022 Analysis of AI Hallucinations and Error Impact: We analyze how AI hallucinations and other errors impact human testers' performance, offering strategies to mitigate over-reliance and reduce the effects of AI inaccuracies.\n\u2022 Practical Guidelines for AI Integration: Based on the findings, we propose practical recommendations for incorporating AI into game testing workflows, emphasizing the balance between AI assistance and human judgment."}, {"title": "II. RELATED WORK", "content": "Multimodal AI involves integrating diverse data types such as visual, audio, and textual inputs into AI systems, enabling more robust and comprehensive analysis [15], [16]. In the context of game testing, multimodal AI facilitates the detection of a wide array of defects, from visual glitches to inconsistencies in gameplay and narrative [3], [14], [29]. This approach allows AI systems to assess multiple dimensions of a game simultaneously, improving the breadth and depth of testing.\nRecent advancements in deep learning models designed for multimodal inputs have significantly enhanced Al's real-time, multidimensional capabilities in game testing [30]. These models can process complex game environments, evaluating everything from graphical fidelity to narrative coherence [6], [12], [31], [32]. However, integrating multiple data types introduces challenges, including the need for large and diverse datasets and the computational complexity required for real-time analysis [7], [17]. Moreover, the \"black-box\" nature of many deep learning models raises concerns about explainability, which can hinder human testers' trust in AI systems [20], [33].\nAnother notable challenge with multimodal AI systems is hallucinations-where the AI misinterprets game elements and flags non-existent bugs [22]. Addressing this issue is crucial for improving AI reliability. Recent research has explored techniques like contrastive learning and model fine-tuning to reduce hallucinations and improve AI accuracy in defect detection [23]\u2013[25]. These efforts are essential to ensuring that AI-assisted systems remain trustworthy and efficient in complex game testing environments.\nHuman-Al collaboration in software testing combines the strengths of automated AI systems and human testers' intuitive judgment. Al excels at automating repetitive tasks, such as generating test cases and identifying specific types of defects, thereby enhancing the efficiency and scalability of testing processes [11], [34]. Human testers, on the other hand, contribute critical thinking, intuition, and the ability to manage complex or edge-case scenarios that are often difficult to automate [35]\u2013[37].\nIn game testing, which often involves dynamic environments and unpredictable player behaviors, balancing human-AI collaboration is essential for thorough defect detection [1], [38]. Studies have shown that over-reliance on Al systems can lead testers to overlook subtle defects or unquestioningly accept incorrect AI outputs, underscoring the importance of workflows that allow for human oversight and critical assessment [20], [21]. To mitigate these issues, explainable AI (XAI) techniques have been explored to improve the transparency of AI systems, especially when errors, such as hallucinations, occur [19], [39], [40].\nRecent studies indicate that machine input can heavily influence human decision-making, especially under cognitive limitations [41]. Trust between human testers and AI systems is vital for effective collaboration [42], and dissonance between human and machine understanding can cause inefficiencies [43]. Well-designed interfaces should enable testers to adjust or override AI decisions, minimizing the impact of AI errors [44], [45]. Improved AI training methods and handling uncertainty are essential to mitigate these issues [25].\nAI-assisted game testing has evolved from rule-based automation to more advanced machine learning and reinforcement learning techniques [1], [8], [18], [46], [47]. Deep learning architectures, particularly convolutional neural networks (CNNs), have proven effective in detecting visual defects, such as texture misalignments and rendering errors [15]. Systems like AstroBug [13] and SUPERNOVA [16] utilize these models to analyze gameplay footage in real time, offering scalable solutions that outperform manual testing in several aspects.\nReinforcement learning, which simulates diverse player behaviors, has also been successfully applied to uncover bugs related to game physics, balance, and interaction dynamics, often missed by traditional testing methods [12], [27], [33], [48]. This approach allows AI to explore edge cases and emergent gameplay behaviors, improving the thoroughness of game testing [6], [14].\nNevertheless, AI-assisted game testing faces limitations. AI systems may struggle with the unpredictability of complex game environments, potentially missing nuanced defects that require human interpretation [19], [45]. Human oversight is therefore critical for validating AI-generated reports and refining models to better handle complex scenarios [15], [49]."}, {"title": "III. METHODOLOGY", "content": "The primary objective of our research is to investigate how AI can effectively assist human testers in identifying defects in video games, focusing on integrating AI-driven automation with human oversight. We employed two principle workflows: a traditional manual testing process and an AI-assisted testing process.\nIn the traditional game testing workflow (illustrated in Fig 1), the process starts with game development and the creation of a detailed test plan. This test plan outlines the objectives, requirements, and scope of testing, covering both game mechanics and graphical elements [50]. Human testers manually execute the test cases, aiming to identify defects such as graphical glitches, gameplay inconsistencies, and physics issues. Defects identified by testers are documented in a test report, which is then forwarded to developers for resolution. After the resolution, regression testing is conducted to ensure no new issues have been introduced. This workflow, although thorough, is resource-intensive, time-consuming, and prone to human error, particularly for large and complex games [5], [51].\nIn the AI-assisted testing workflow (illustrated in Fig 2), the process begins similarly with game development and test planning. However, during the test execution phase, an AI component captures in-game screenshots and analyzes them using state-of-the-art models. The AI system detects visual defects such as texture misalignments, clipping errors, and UI inconsistencies, and compiles these findings into an initial test report for human reviewers. Human testers then validate the AI-identified defects and focus on more complex issues like gameplay mechanics and physics behaviors, which may not be easily detectable by AI. This hybrid approach enables AI to handle repetitive, surface-level defect detection, allowing human testers to spend more time on in-depth analysis. The final test report combines both AI-generated and human-verified results, which are submitted to developers for resolution.\nThis hybrid workflow represents our core research framework: an integration of automated detection with human validation in game testing. The setup allows us to explore the effectiveness of such a system in improving testing efficiency, accuracy, and overall human performance.\nOur experiment was designed to validate the AI-assisted game testing system's reliability and examine the effects of human-AI collaboration in identifying game defects. The experiment setup consists of the following components: the system, test cases, execution procedures, and scoring mechanisms.\n1) System: The AI-assisted system is based on the state-of-the-art GPT-40 model [30], which was employed to process a diverse dataset of video game visuals, including different defect types and gameplay elements. The AI component functions by processing game screenshots, generating descriptions of the game environment, and detecting a set of predefined defect categories (see Table II). Human participants interact with this system during the testing phases, working the AI's outputs to identify game defects. The system's performance metrics include the accuracy of visual descriptions and defect identifications, with an 87.5% accuracy rate for scene descriptions and a 97.1% accuracy rate for defect detection under controlled conditions.\n2) Test Cases and Execution: The experiment involved a total of 800 test cases, divided equally across four experimental conditions: No Knowledge, No AI Assistance (Manual); Knowledge, No AI Assistance (KA); No Knowledge, With AI Assistance (AIA); and Knowledge, With AI Assistance (KA+AIA). Each condition consisted of 200 test cases, and each participant took a total of 40 random cases-10 from each condition. This ensured that all participants experienced every possible combination of AI assistance and the availability of knowledge.\nIn this experiment, \u201cknowledge\u201d refers to detailed information generated by the AI system, including explanations of potential defects identified by the AI (when AI involves), as well as the game design documentation related to the specific test cases. This knowledge helps participants understand the types of defects that may occur and provides context from the game's design to assist in defect identification.\nEach test case involved the identification of defects from game images, with participants required to identify up to two defects per case. Defects were selected from four primary categories: graphical rendering errors, user interface (UI) errors, scene and object consistency errors, and object and physics behavior errors, which were further subdivided into specific types as described in Table II. While most test cases contains at least one defect to be identifies, around 10% of all cases are bug-free.\nTest execution followed a structured sequence, ensuring participants experienced both AI-assisted and non-AI-assisted conditions, as well as conditions with and without the detailed knowledge provided by the AI system.\n3) Scoring Mechanism: Participants were scored based on their ability to correctly identify defects within each test case. The scoring system was designed to allocate a maximum of 10 points per test case, with 5 points awarded for each correctly identified defect. Incorrect or missed defect identifications resulted in no points for that particular defect. Bug-free cases come with 10 points initially. If the participant identified bugs for bug-free cases, there will be no point. Therefore, each participant could score a maximum of 400 points, with the average score recorded as 211 points and the highest as 395. This scoring mechanism enabled a comparative analysis of participants' performance across different test conditions.\nHuman Participants:: A total of 276 human participants were recruited via an online platform, with the average test duration being 6 minutes and 5 seconds. Among the participants, 55.4% were between 18 and 24 years old, and 97.5% reported prior gaming experience. For a detailed breakdown of participant demographics, see Table I.\nthe AI component functions as an additional participant in the testing process, utilizing advanced machine vision techniques to analyze and detect defects. Its performance, as mentioned, was rigorously evaluated for accuracy in both scene description and defect identification.\nThe primary task for participants was to identify defects in the provided game visuals and classify them based on the defined categories. Defects were categorized into graphical rendering errors, UI errors, scene and object consistency errors, and object and physics behavior errors [52], as detailed in Table II. Participants were required to complete the task under four different experimental conditions.\nOur experiment was structured into four testing conditions that combined the AI system with the availability of detailed defect information and design documentation (knowledge):\n\u2022 No Knowledge, No AI Assistance (Manual): Participants relied solely on observation without any detailed defect information or AI assistance.\n\u2022 Knowledge, No AI Assistance (KA): Participants were provided with detailed explanations of potential defects identified by the AI, along with game design documentation related to the test cases. However, they performed the tests without direct AI assistance during execution.\n\u2022 No Knowledge, With AI Assistance (AIA): Participants used the AI system for defect identification but did not have access to detailed defect information or game design documentation.\n\u2022 Knowledge, With AI Assistance (KA+AIA): Participants had both the AI system's assistance and access to detailed explanations of potential defects, as well as game design documentation for the test cases.\nThe experimental procedure followed these steps:\nPre-Test Briefing: Participants were briefed on the experiment's content and procedure.\nTest Execution: Participants evaluated 10 randomly assigned game screenshots in each of the four conditions (Manual, KA, AIA, KA+AIA), following a predefined sequence.\nData Collection: Participants' responses, including the difficulty of the game screenshots and the AI system's evaluations, were recorded.\nPost-Test Feedback: Participants provided feedback on their experience, including their understanding of the AI system and defect identification process.\nIncentive Mechanism: Participants were incentivized with monetary rewards based on their performance, with bonuses for higher accuracy scores.\n1) Data Preprocessing: To ensure the validity and accuracy of the data, the following preprocessing steps were conducted:\nExclusion of Anomalous Times: Each participant's response duration was recorded, with an average analysis time per case exceeding 8 seconds. Therefore, any response time under 300 seconds was considered indicative of insufficient engagement. A minimum threshold of 300 seconds was established, and all records falling below this threshold were excluded."}, {"title": "IV. EXPERIMENT RESULTS AND ANALYSIS", "content": "The results of this study indicate that AI-assisted systems significantly enhance the effectiveness of human testers in identifying game defects, especially under challenging conditions. The integration of AI assistance with detailed knowledge of defects including information generated by AI and access to game design documentation-yielded the highest effectiveness in bug detection. However, the study also uncovered unexpected patterns regarding the impact of Al errors and hallucinations on human decision-making, highlighting the complexities of human-AI collaboration in game testing.\nTo evaluate the impact of AI assistance and knowledge provision on defect detection accuracy, we compared participants' performance across the four experimental conditions: Manual (M), Knowledge Assistance (KA), AI Assistance (AIA), and Knowledge plus AI Assistance (KA+AIA). As shown in Figure 3, AI assistance significantly improved defect detection accuracy. Specifically, participants in the AIA condition achieved an average accuracy of 62.7%, compared to 41.3% in the Manual condition\u2014a statistically significant increase (p < 0.001). When AI assistance was combined with knowledge provision (KA+AIA), the average accuracy further increased to 64.4%.\nInterestingly, while knowledge provision alone (KA) improved average accuracy to 43.8% compared to the Manual condition, this difference was not statistically significant (p = 0.127). This suggests that AI assistance has a more substantial effect on improving defect detection accuracy than knowledge provision alone.\nFurthermore, the interquartile range (IQR) for participants in the AI-assisted conditions was narrower than in the Manual and KA conditions, indicating more consistent performance among participants when AI assistance was provided. This consistency likely results from the AI system's ability to reduce cognitive load by automating the detection of surface-level defects, allowing human testers to focus on more complex issues.\nThese findings demonstrate that AI assistance significantly enhances human testers' effectiveness in identifying game defects, and that combining AI assistance with detailed knowledge yields the highest accuracy. The results indicate the potential of AI to augment human capabilities in game testing, leading to more efficient and accurate defect detection processes.\nParticipants without prior gaming experience showed a similar boost in accuracy from AI assistance, achieving 61.4% with AI and 45.7% without AI when no extra knowledge was provided (AIA vs. Manual). However, with the addition of knowledge, accuracy decreased to 59.3% with AI and 35.0% without AI (KA+AIA vs. KA). This suggests that while AI consistently improves accuracy, unfamiliarity with video games may make knowledge provision overwhelming, reducing effectiveness. Despite this, AI assistance helped this group achieve accuracy comparable to participants with prior gaming experience.\nGiven the gaming scenario, we attracted few non-gamers. Due to the small sample size of 7 participants in this group, these findings are not statistically significant. Future work should involve a more diverse participant pool to better understand how gaming experience influences the effectiveness of AI and knowledge in game testing.\nTo further assess the value of AI assistance, we analyzed participants' accuracy in identifying defects under challenging conditions, such as low visibility (LV), multiple defects (MD), subtle defects (SD), and ambiguous boundaries (AB). Table III summarizes the accuracy across different conditions.\nIn low-visibility conditions, manual testers achieved an accuracy of 42.4%, indicating the difficulty of detecting defects when visibility is compromised. AI assistance substantially improved accuracy to 71.6% in the AIA condition (p < 0.001 compared to Manual), demonstrating Al's effectiveness in processing visual information that may be challenging for humans. However, the KA+AIA condition saw a slightly lower accuracy of 65.3%, suggesting that additional knowledge may not significantly enhance performance in conjunction with Al assistance under low-visibility conditions, possibly due to information overload or cognitive interference.\nWhen test cases contained multiple defects, manual testers achieved 44.0% accuracy. AI assistance again led to a significant improvement, with 67.9% accuracy in the AIA condition (p < 0.001). The KA+AIA condition achieved 65.6% accuracy, comparable to the AIA condition. This indicates that AI assistance helps testers manage the complexity of identifying multiple defects simultaneously, likely by systematically highlighting potential issues.\nDetecting subtle defects proved challenging for manual testers, with an accuracy of 36.6%. AI assistance improved accuracy to 54.7% in the AIA condition (p < 0.001), and further to 62.3% in the KA+AIA condition. The significant improvement in the condition KA + AIA suggests that the combination of AI assistance and detailed knowledge enables testers to detect subtle defects more effectively, perhaps by providing context that enhances the interpretation of AI output.\nIn cases with ambiguous defect boundaries, manual testers had the lowest accuracy (36.2%). The AIA condition improved accuracy to 65.4% (p < 0.001), while the KA+AIA condition achieved the highest accuracy of 72.9%. This substantial improvement indicates that when defects are difficult to classify, the combination of AI assistance and knowledge provision is particularly beneficial, likely because the detailed information helps testers resolve ambiguities in defect categorization.\nOverall, these results highlight that AI assistance significantly enhances defect detection accuracy under challenging conditions, and that the addition of knowledge provision can further improve performance in certain scenarios. The findings emphasize the importance of integrating AI tools with human expertise to address complex testing challenges effectively.\nWhile AI assistance generally improved defect detection accuracy, it is crucial to understand how Al errors affect human testers. We analyzed the impact of AI descriptive errors (DE), judgment errors (JE), cases with correct description but judgment errors (CDJE), and cases with both description and judgment errors (DEJE) on participant accuracy. Table IV presents the accuracy across different conditions.\nIn cases where the AI provided incorrect scene descriptions, participant accuracy decreased in the AIA condition (35.2%) compared to the Manual condition (41.5%). This suggests that misleading descriptions from the AI can negatively impact testers who rely on AI outputs. However, in the KA+AIA condition, accuracy improved to 44.5%, indicating that access to detailed knowledge helps participants identify and correct AI descriptive errors.\nWhen the AI misclassified defects, participant accuracy was significantly lower in the AIA condition (20.3%) compared to the Manual condition (32.2%), highlighting the potential for AI judgment errors to mislead testers. The KA+AIA condition showed a slight improvement (27.4%), suggesting that knowledge provision partially mitigates the impact of AI judgment errors by enabling testers to critically assess AI outputs.\nIn scenarios where the AI provided accurate descriptions but incorrect defect judgments, participants in the AIA condition achieved higher accuracy (51.7%) than in the Manual condition (37.0%). This implies that accurate descriptions allow testers to override incorrect AI judgments. The highest accuracy was observed in the KA+AIA condition (54.6%), reinforcing the benefit of combining AI assistance with knowledge provision.\nWhen the AI made both descriptive and judgment errors, participant accuracy was notably low in the AIA condition (9.8%), significantly lower than in the Manual condition (30.6%). This shows the detrimental effect of compounded Al errors on human performance. The KA+AIA condition showed some improvement (18.4%), indicating that knowledge provision can help testers identify and correct AI errors to some extent.\nThese findings highlight that while AI assistance generally enhances performance, AI errors can adversely affect human testers, particularly when they rely heavily on AI outputs. Providing testers with additional knowledge and training to critically evaluate AI outputs is essential to mitigate the impact of Al errors.\nAI hallucinations\u2014instances where the AI incorrectly identifies defects in non-defective visuals or fails to detect actual defects-pose a significant challenge in AI-assisted game testing. We examined how AI hallucinations affected participant accuracy, focusing on false negative detections (FND) and false positive detections (FPD). Table V summarizes the results.\nIn cases where the AI failed to detect actual defects, participants in the AIA condition had lower accuracy (24.6%) compared to the Manual condition (34.8%). This indicates that reliance on AI can lead to missed defects when the AI overlooks issues. However, the KA+AIA condition showed an accuracy of 35.7%, similar to the Manual condition, suggesting that knowledge provision helps participants compensate for AI's false negatives by relying on their own judgment informed by detailed information.\nWhen the AI incorrectly identified defects in non-defective visuals, participant accuracy was low across all conditions. The Manual condition achieved slightly higher accuracy (27.3%) compared to the AIA (26.3%) and KA+AIA (21.8%) conditions. This suggests that AI hallucinations of false positives can mislead testers, even when additional knowledge is provided. Participants may trust the Al's assessment, especially if the AI presents plausible explanations for the supposed defects, leading to incorrect defect reports."}, {"title": "V. DISCUSSION", "content": "In our analysis, one of the key outcomes is that participants utilizing AI assistance consistently achieved higher accuracy compared to those relying solely on manual testing. As shown in Table III, AI-assisted conditions (AIA and KA+AIA) led to substantial improvements in defect detection across scenarios with low visibility, multiple defects, subtle defects, and ambiguous boundaries. This demonstrates the potential of AI systems to augment human capabilities by efficiently handling surface-level defect detection, allowing testers to focus on more complex issues.\nHowever, the study also indicates that Al assistance is not infallible. Al errors both descriptive and judgmental-negatively impacted human performance. When the AI provided incorrect descriptions or misclassified defects, participants' accuracy decreased significantly in the AI-assisted conditions (Table IV). Moreover, AI hallucinations, where the AI incorrectly identified non-defective visuals as containing bugs or failed to detect actual defects, further misled testers (Table V). These findings align with existing literature on the risks of over-reliance on AI systems, where users may accept AI outputs without sufficient critical evaluation [22], [23].\nImportantly, combining AI assistance with detailed knowledge provision (KA+AIA) mitigated some of the negative effects of Al errors. Participants with access to additional knowledge were better equipped to critically assess AI outputs, correct errors, and maintain higher accuracy levels, even when the AI made mistakes. For instance, in cases of correct description but judgment errors (CDJE), the KA+AIA condition achieved the highest accuracy (54.6%), indicating that knowledge provision helps testers override incorrect AI judgments (Table IV). This emphasizes the crucial role of human oversight and the value of equipping testers with sufficient context to make informed decisions.\nThe study also reveals that while knowledge provision alone (KA) did not significantly improve accuracy compared to manual testing, its combination with AI assistance led to the highest performance. This suggests that AI assistance enhances human performance most effectively when testers are supported by both AI tools and adequate knowledge resources. However, there is a need to balance the amount of information provided to avoid cognitive overload, which can hinder decision-making efficiency [54], [55].\nOur findings highlight the importance of designing Al systems that are not only accurate but also transparent and interpretable, enabling testers to understand and critically evaluate AI outputs. Additionally, training testers to maintain a critical perspective towards AI assistance and to effectively use the provided knowledge resources is essential to maximize the benefits of AI integration."}, {"title": "VI. CONCLUSION", "content": "Through a comprehensive user study, we demonstrated that AI assistance significantly enhances real-world human testers' effectiveness in identifying game defects, particularly when combined with detailed knowledge resources. By integrating Al-driven defect detection with human oversight, we experimented a hybrid testing workflow that optimizes efficiency and accuracy in game testing processes.\nHowever, the research also highlights challenges associated with Al errors and hallucinations, which can negatively impact performance if not properly managed. These findings emphasize the critical need for effective human-AI collaboration, where testers are equipped to critically assess and correct AI outputs. Developing improved collaborative frameworks and interaction models is essential to address the problems associated with Al errors and to facilitate better and easier human judgment.\nOur study contributes to the understanding of human-AI interaction in complex decision-making environments, revealing the dual role of AI as both an enhancer and a potential source of error in game testing. Practically, the study suggests that providing testers with manageable knowledge resources and training in critical evaluation can maximize the benefits of AI assistance while mitigating risks.\nLimitations of this study include a participant pool with predominantly gaming experience, which may affect generalizability, and a focus on specific defect types and testing conditions. Future research should include a more diverse range of participants and expand the scope to various game genres and complex scenarios. Additionally, improving AI transparency and developing strategies to enhance human oversight will be crucial for advancing AI-assisted game testing methodologies.\nIn conclusion, by addressing the identified challenges and fostering effective human-AI collaboration, the gaming industry can leverage AI to achieve more efficient, accurate, and reliable testing processes."}]}