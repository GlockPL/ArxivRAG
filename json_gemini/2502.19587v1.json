{"title": "NeoBERT: A Next-Generation BERT", "authors": ["Lola Le Breton", "Quentin Fournier", "Mariam El Mezouar", "Sarath Chandar"], "abstract": "Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and ROBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERTlarge, ROBERTalarge, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.", "sections": [{"title": "Introduction", "content": "Auto-regressive language models have made tremendous progress since the introduction of GPT (Radford et al., 2018), and modern large language models (LLMs) such as LLaMA 3 (Dubey et al., 2024), Mistral (Jiang et al., 2023), OLMo (Groeneveld et al., 2024), and DeepSeek-r1 (DeepSeek-AI et al., 2025) now exhibit remarkable reasoning and in-context learning capabilities. These improvements result from scaling both the models and the web-scraped text datasets they are trained on, as well as from innovations in architecture and optimization techniques. However, while decoders have continuously evolved, encoders have not seen the same level of progress. Worse, their knowledge has become increasingly outdated despite remaining critical for a wide range of downstream NLP tasks that depend on their embeddings, notably for retrieval-augmented generation (Ram et al., 2023) and toxicity classification (Hartvigsen et al., 2022). Despite being five years old, BERT (Devlin et al., 2019) and ROBERTa (Liu et al., 2019) remain widely used, with more than 110 million combined downloads from Hugging Face as of the writing of this paper.\nSimilar to decoders, which undergo multi-stage processes of pre-training, instruction-tuning, and alignment, encoders also require successive training phases to perform well on downstream tasks. First, encoders go through self-supervised pre-training on large corpora of text with the masked language modeling objective. By predicting masked or replaced tokens, this stage enables models to learn the structural patterns of language and the semantics of words. However, the pre-training task is disconnected from downstream applications, and models require additional training to achieve strong performance in clustering or retrieval. Thus, a second fine-tuning phase is often achieved through multiple stages of semi-supervised contrastive learning, where models learn to differentiate between positive and negative sentence pairs, refining their embeddings in the latent space."}, {"title": "Related work", "content": "In 2019, Devlin et al. (2019) introduced BERT, a novel approach to embedding text using bi-directional Transformers pre-trained without supervision on large corpora. Shortly after, Liu et al. (2019) improved over BERT's pre-training by removing the next-sentence prediction objective and drastically increasing the amount of data, leading to RoBERTa. Since then, the primary focus of the community has shifted towards optimizing the fine-tuning phase of these models through contrastive learning, where the model is trained to maximize the similarity between positive text pairs while pushing them apart from negative samples.\nAmong the earliest contrastive learning approaches designed for encoders, SimCSE (Gao et al., 2022) demonstrated that sentence pairs could be easily generated by feeding the same input to the model twice and applying dropout to introduce noise. However, this simple approach was soon outperformed by models like GTE (Li et al., 2023b), which introduced more advanced contrastive learning techniques. GTE employed a weakly supervised stage that takes advantage of the vast number of successive sentence pairs available in traditional unlabeled datasets, followed by a semi-supervised stage incorporating labeled sentence pairs from high-quality datasets such as NLI (Bowman et al., 2015) and FEVER (Thorne et al., 2018). Recently, fine-grained strategies have emerged to better adapt models to both task and context. For instance, Jina-embeddings (Sturua et al., 2024) introduced task-specific Low-Rank Adaptation (LoRA) adapters. As of January 2025, CDE (Morris & Rush, 2024) ranks at the top of the MTEB leaderboard for models under 250M parameters thanks to two key innovations: grouping samples with related contexts into the same batch and providing contextual embeddings for the entire corpus in response to individual queries.\nHowever, pre-training has not seen the same level of effort, and thus progress, most likely due to its prohibitively high computational cost. ROBERTa, for instance, required a total of 1,024 V100 days for its pre-training. As a result, GTE, Jina-embeddings, and CDE all rely on pre-trained BERT, XLM-ROBERTa (Conneau et al., 2020), and NomicBERT (Nussbaum et al., 2024) to initialize their respective models. The latter, NomicBERT, represents a recent effort to refine BERT's architecture and pre-training. NomicBERT incorporates architectural improvements such as SwiGLU and RoPE, utilizes FlashAttention, and extends the context length to 2,048 tokens. Despite these innovations, NomicBERT still relied on sub-optimal choices, as discussed in section 3. In parallel with the development of NeoBERT, Warner et al. (2024) released ModernBERT with the goal of further refining the pre-training of NomicBERT. Although we share some of the modifications, we make distinct design choices and conduct thorough ablations that ultimately lead to greater performance on MT\u0415\u0412."}, {"title": "NeoBERT", "content": "The following section describes NeoBERT's improvements over BERT and ROBERTa, as well as the recent NomicBERT and ModernBERT models. Since GTE and CDE use BERT and NomicBERT as their pre-trained backbone, they inherit their respective characteristics. "}, {"title": "Architecture", "content": "The Transformer architecture has been refined over the years and has now largely stabilized, with models like LLaMA 3 being essentially the same as the original LLaMA. NeoBERT integrates the latest modifications that have, for the most part, become standard.\nDepth-to-Width The concept of depth efficiency has long been recognized in neural network architectures. In the case of Transformers, stacking self-attention layers is so effective that it can quickly saturate the network's capacity. Recognizing this, Levine et al. (2020) provided theoretical and empirical evidence for an optimal depth-to-width ratio in Transformers. Their findings suggested that most language models were operating in a \"depth-inefficiency\" regime, where allocating more parameters to width rather than depth would have improved performance. In contrast, small language models like BERT, ROBERTa, and NomicBERT are instead in a width-inefficiency regime. To maximize NeoBERT's parameter efficiency while ensuring it remains a seamless plug-and-play replacement, we retain the original BERTbase width of 768 and instead increase its depth to achieve this optimal ratio.\nPositional Information Transformers inherently lack the ability to distinguish token positions. Early models like BERT and RoBERTa addressed this by adding absolute positional embeddings to the token embeddings before the first Transformer block. However, this approach struggles to generalize to longer sequences and requires the positional information to be propagated across layers. To overcome these limitations, Su et al. (2023) proposed Rotary Position Embeddings (RoPE), which integrate relative positional information directly into the self-attention mechanism. RoPE has quickly become the default in modern Transformers due to its significant improvements in performance and extrapolation capabilities. NeoBERT, like all newer encoders, integrates ROPE. Nevertheless, degradation still occurs with sequences significantly longer than those seen during training. As a solution, Peng et al. (2023) introduced Yet Another ROPE Extension (YaRN), which allows to efficiently fine-tune models on longer contexts. NeoBERT is readily compatible with YaRN, making it well-suited for tasks requiring extended context.\nLayer Normalization Consistent with standard practices in modern Transformer architectures, we move the normalization layer inside the residual connections of each feed-forward and attention block, a technique known as Pre-Layer Normalization (Pre-LN). Pre-LN improves stability, allows for larger learning rates, and accelerates model convergence (Xiong et al., 2020). While all newer encoder models adopt Pre-LN, they typically continue to use the classical LayerNorm rather than Root Mean Square Layer Normalization (RMSNorm). In NeoBERT, we substitute the classical LayerNorm with RMSNorm (Zhang & Sennrich, 2019), which achieves comparable training stability while being slightly less computationally intensive, as it requires one fewer statistic.\nActivations BERT and RoBERTa utilize the standard Gaussian Error Linear Unit (GELU) activation function. However, Shazeer (2020) demonstrated the benefits of the Gated Linear Unit in Transformer architectures. These activation functions have since been adopted in several language models, including the LLaMA family. Following previous works, NeoBERT incorporates the SwiGLU activation function, and because it introduces a third weight matrix, we scale the number of hidden units by a factor of 2/3 to keep the number of parameters constant."}, {"title": "Data", "content": "Data has emerged as one of the most critical aspects of pre-training, and datasets with increasing quantity and diversity are frequently released. NeoBERT takes advantage of the latest datasets that have proven to be effective.\nDataset BERT and NomicBERT were pre-trained on two carefully curated and high-quality datasets: Wikipedia and BookCorpus (Zhu et al., 2015). As Baevski et al. (2019) demonstrated that increasing data size can improve downstream performance, Liu et al. (2019) pre-trained RoBERTa on 10 times more data from BookCorpus, CC-News, OpenWebText, and Stories. However, RoBERTa's pre-training corpus has become small in comparison to modern web-scraped datasets built by filtering and deduplicating Common Crawl dumps. Following the same trend, we pre-trained NeoBERT on RefinedWeb (Penedo et al., 2023), a massive dataset containing 600B tokens, nearly 18 times larger than RoBERTa's. Although RefinedWeb does not have strict high-quality constraints, we believe that exposing the model to such a large and diverse dataset will improve its real-world utility.\nSequence Length BERT and RoBERTa were pre-trained on sequences up to 512 tokens, which limits their downstream utility, especially without ROPE and YaRN. NomicBERT increased the maximum length to 2,048 and employed Dynamic NTK interpolation at inference to scale to 8192. To further broaden NeoBERT's utility, we seek to increase the context length. However, due to the computational cost associated with pre-training, we adopt a two-stage pre-training procedure similar to LLMs like LLaMA 3. In the first stage, we train the model for 1M steps (2T tokens) using sequences truncated to a maximum length of 1,024 tokens, referring to this version as NeoBERT1024. In the second stage, we extend the training for an additional 50k steps (100B tokens), increasing the maximum sequence length to 4,096 tokens. We refer to this final model as NeoBERT4096. To ensure the model encounters longer sequences during this stage, we create two additional sub-datasets, Refinedweb1024+ and Refinedweb2048+, containing only sequence lengths greater than 1,024 and 2,048 tokens, respectively, alongside the original Refinedweb dataset. Each batch is sampled from Refinedweb, Refinedweb1024+ and Refinedweb2048+ with probabilities 20%, 40%, and 40%. Since longer sequences tend to represent more complex or academic content, this strategy helps mitigate the distribution shift typically observed when filtering for longer sequences. We explore the benefits of this two-stage training strategy in subsection 5.3."}, {"title": "Pre-Training", "content": "Encoder pre-training has received less attention than the data and architecture. However, many improvements made to decoders also apply to encoders. NeoBERT combines encoder-specific modifications with widely accepted decoder improvements.\nObjective In light of ROBERTa's findings that dropping the next-sentence prediction task does not harm performance, both NomicBERT and NeoBERT were only pre-trained on masked language modeling. Moreover, Wettig et al. (2023) challenged the assumption that the 15% masking rate of BERT and ROBERTa is universally optimal. Instead, their findings suggest that the optimal masking rate is actually 20% for base models and 40% for large models. Intuitively, a model learns best when the difficulty of its training tasks aligns with its capabilities. Based on their insight, we increase the masking rate to 20%, while NomicBERT exceeds it by opting for 30%.\nOptimization Following standard practice, we use the AdamW optimizer (Loshchilov & Hutter, 2019) with the same hyperparameters as LLAMA 2: \u03b2\u2081 = 0.9, \u03b22 = 0.95, and e = 10-8. In preliminary experiments, we also considered SOAP (Vyas et al., 2025), a recent extension of the Shampoo optimizer, but it failed to outperform Adam and AdamW and has been omitted from the list of ablations. We employ a linear warmup for 2,000 steps to reach a peak learning rate of 6 \u00d7 10-4, followed by a cosine decay to 10% of the peak learning rate over 90% of the training steps. Once fully decayed, the learning rate remains constant for the last 100k steps at a sequence length of 1,024 and 50k steps at a sequence length of 4,096. We use a weight decay of 0.1 and apply gradient clipping with a maximum norm of 1.0.\nScale Recent generative models like the LLaMA family (Touvron et al., 2023; Dubey et al., 2024) have demonstrated that language models benefit from being trained on significantly more tokens than was previously standard. Recently, LLaMA-3.2 1B was successfully trained on up to 9T tokens without showing signs of saturation. Moreover, encoders are less sample-efficient than decoders since they only make predictions for masked tokens. Therefore, it is reasonable to believe that encoders of similar sizes can be trained on an equal or even greater number of tokens without saturating. For NeoBERT's pre-training, we use batch sizes of 2M tokens over 1M steps in the first stage and 50k steps in the second, resulting in a theoretical total of 2.1T tokens. Note that because sequences are padded to the maximum length, this represents a theoretical number of tokens. In terms of tokens, this is comparable to ROBERTa and represents a 2x increase over"}, {"title": "Effect of Design Choices", "content": "We conduct a series of ablations in controlled settings to evaluate our improvements to the original BERT architecture. We fully train each model for 1M steps, controlling for the seed and dataloader states to ensure successive models are trained with identical setups. These resource-intensive ablations were crucial to confirm our design choices, as they are based on the literature of pre-training decoder models. The baseline model, referred to as M0, is similar to BERTbase but includes pre-layer normalization. Following RoBERTa, MO also drops the next sentence prediction objective. We introduce modifications iteratively, resulting in a total of ten different models. To mitigate computational costs, the ablations are evaluated on the GLUE benchmark with a limited hyperparameter grid search of batch sizes \u2208 {16,32} and learning rates \u2208 {1e - 5, 2e - 5,3e - 5}. For the final model M10, we extend the grid search, as detailed in Appendix C.\nKey Performance-Enhancing Modifications As expected, the two modifications that had the greatest impact on the average GLUE score were related to scale. In M2, replacing Wikitext and BookCorpus with the significantly larger and more diverse RefinedWeb dataset improved the score by +3.6%, while increasing the model size from 120M to 250M in M7 led to a +2.9% relative improvement. Note that to assess the impact of the depth-to-width ratio, we first scale the number of parameters in M7 to 250M while maintaining a similar ratio to BERTbase, resulting in 16 layers of dimension 1056. In M8, the ratio is then adjusted to 28 layers of dimension 768.\nModifications That Were Discarded In M3, replacing the Google WordPiece tokenizer with LLaMA BPE results in a -2.1% performance decrease. We hypothesize that while the heterogeneous and multilingual nature of the LLaMA 2 vocabulary enhances broader applicability in decoders, it trades off performance for more compact encoder representations. In M6, we un-pad the sequences by concatenating samples of the same batch. While this removes unnecessary computation on padding tokens, packing sequences without accounting for cross-sequence attention results in a relative performance drop of -2.8%. Although this modification was discarded from our subsequent ablations, we incorporate methods of un-padding with accurate cross-attention in our released codebase, following Kundu et al. (2024).\nModifications Retained Despite Performance Trade-offs Unexpectedly, using AdamW (Loshchilov & Hutter, 2019) and cosine decay (Loshchilov & Hutter, 2017) in M4 decreases performance by -0.5%. As AdamW introduces additional regularization with weight decay, we expect that it will become beneficial when extending training by mitigating overfitting. Similarly, increasing the masking ratio from 15% to 20% in M5 decreases performance by -0.7%. We hypothesize that increasing the task difficulty initially hinders downstream task performance but is likely to become advantageous when training larger models on more tokens. Consequently, we retain both modifications despite being unable to verify these hypotheses at scale due to the computational costs."}, {"title": "Experiments", "content": "Selecting appropriate metrics and benchmarks is crucial for properly assessing the downstream performance and utility of language models. Following both early and recent studies, we include the GLUE and MTEB benchmarks in our evaluations."}, {"title": "GLUE", "content": "The GLUE benchmark (Wang et al., 2019) is a cornerstone of language modeling evaluations and has played a significant role in the field. Although GLUE is now 6 years old and the community has long recognized its limitations, we report the GLUE score due to its widespread adoption and to facilitate the comparison of NeoBERT with existing encoders. Following standard practices, we fine-tune NeoBERT on the development set of GLUE with a classical hyperparameter search and introduce transfer learning between related tasks. Complete details of the fine-tuning and best hyperparameters are presented in Appendix C. NeoBERT achieves a score of 89.0% comparable to previous large models while being 100M to 150M parameters smaller. We present the results in Table 3."}, {"title": "MTEB", "content": "Beyond the GLUE benchmark, we consider the more recent and challenging MTEB benchmark (Muennighoff et al., 2023), which has quickly become a standard for evaluating embedding models, with a wide coverage of 7 tasks and 56 datasets in its English subset.\nMTEB tasks rely on the cosine similarity of embeddings pooled across tokens in a sentence. The most common and straightforward pooling strategy is computing the average of each token's final hidden representation. However, because out-of-the-box encoders are trained with the masked language modeling objective, they provide no guarantee that mean embeddings will produce meaningful representations without further fine-tuning. As a result, models require expensive fine-tuning strategies crafted for MTEB to achieve strong scores. For instance, GTE (Li et al., 2023b) with multi-stage contrastive learning, Jina-embeddings (Sturua et al., 2024) with task-specific Low-Rank Adaptation (LoRA) adapters, and CDE (Morris & Rush, 2024), with batch clustering and contextual corpus embeddings all pushed the limits of the leaderboard in their respective categories.\nThese coupled stages make it challenging to disentangle the respective impacts of pre-training and fine-tuning on the final model's performance. To isolate and fairly evaluate the improvements introduced during pre-training, we implemented an affordable, model-agnostic fine-tuning strategy based on classical contrastive learning. This fine-tuning approach was designed in accordance with established methods in the literature. Its controlled settings ensured that all models were fine-tuned and evaluated under identical conditions.\nGiven a dataset of positive pairs D = {qi, d+ }=1, a similarity metric s, a temperature parameter\n\u03c4, and a set of negative documents Ng for each query q, the contrastive loss is defined as:\nL = - log \\frac{e^{s(q,d^+)/\tau}}{e^{s(q,d^+)/\tau} + \\sum_{d^- \\in N_q} e^{s(q,d^-)/\tau}}\nNegative documents can be either generic samples of the same format or tailored hard negatives, which exhibit a high degree of similarity to the contrasted sample in their original representation. We constructed a dataset of positive query-document pairs with optional hard negatives based on open-source datasets previously employed in the literature (Li et al., 2023a) for a total of nine million documents. In addition to the optional hard negatives, we also leverage in-batch, task-homogeneous negatives. In line with prior research (Li et al., 2023a), we employ task-specific instructions and temperature-scaled sampling of the datasets. Complete details about the data, training, and evaluation can be found in Appendix D.\nResults We found that training for more than 2,000 steps resulted in minimal performance gains.  presents the complete MTEB-English evaluation of all fine-tuned models. Although NeoBERT is 100M parameters smaller than all large baselines, it is the best model overall with a +4.5% relative increase over the second best model, demonstrating the benefits of its architecture, data, and pre-training improvements."}, {"title": "Sequence Length", "content": "Following previous literature, NeoBERT underwent an additional 50k pre-training steps, during which it was exposed to extended sequences of up to 4,096 tokens. To assess the impact of this additional training, we randomly sampled 2,467 long sequences from the English subset of Wikipedia. For each sequence, we independently masked each token at position i and computed its cross-entropy loss, li. The pseudo-perplexity of the entire sentence is then defined as P = exp(\u03a3i=1li).\nAlthough NeoBERT1024 was trained exclusively on sequences of up to 1,024 tokens, it generalizes effectively to context lengths approaching 3,000 tokens. This demonstrates the robustness of ROPE embeddings to out-of-distribution inputs. Moreover, after an additional 50k training steps with sequences up to 4,096 tokens, NeoBERT 4096 successfully models longer sequences. This approach provides a compute-efficient strategy for extending the model's maximum context window beyond its original length."}, {"title": "Efficiency", "content": "To assess model efficiency, we construct a synthetic dataset consisting of maximum-length sequences of sizes {512, 1024, 2048, 4096, 8192}. For each sequence length, we scale the batch size from 1 to 512 samples or until encountering out-of-memory errors. Inference is performed for 100 steps on a single A100 GPU, and we report the highest throughput achieved for each model and sequence length. "}, {"title": "Discussion", "content": "Encoders are compact yet powerful tools for language understanding and representation tasks. They require fewer parameters and significantly lower training costs compared to their decoder counterparts, making them strong alternatives for applications that do not involve text generation. Traditionally, the representational capacity of these models has been assessed through downstream tasks such as classification, in particular through the GLUE benchmark.\nWhile GLUE has played a pivotal role in guiding model adoption, it includes only nine sequence classification datasets, four of which are entailment tasks. Moreover, its small dataset sizes and occasionally ambiguous labeling make it prone to overfitting, with models long surpassing human performance on the benchmark. Although DeBERTa-v3 achieves state-of-the-art performance on GLUE by a significant margin, our fine-tuning experiments reveal its comparatively poor performance on the more recent MTEB benchmark. M\u0422\u0415\u0412 encompasses a broader range of datasets and tasks, but attaining high performance on its leaderboard necessitates carefully crafted fine-tuning strategies with costly training requirements. As more complex fine-tuning strategies emerge, it becomes unclear what the source of score improvements is. Moreover, these strategies are not easily reproducible or accessible, limiting the possibility of fair comparison between pre-trained backbones.\nThis underscores the limitations of current evaluation paradigms and highlights the need for more affordable and standardized frameworks. We advocate for future research to focus on the development of standardized fine-tuning protocols and the exploration of new zero-shot evaluation methodologies to ensure a more comprehensive and unbiased assessment of encoder-only models."}, {"title": "Conclusion", "content": "We introduced NeoBERT, a state-of-the-art encoder pre-trained from scratch with the latest advancements in language modeling, architecture, and data selection. To ensure rigorous validation, we systematically evaluated every design choice by fully training and benchmarking ten distinct models in controlled settings. On GLUE, NeoBERT outperforms BERTlarge and NomicBERT and is comparable with ROBERTalarge despite being 100M parameters smaller and supporting sequences eight times longer. To further validate its effectiveness, we conducted a comprehensive evaluation on MTEB, carefully isolating the effects of pre-training and fine-tuning to provide a fair comparison against the best open-source embedding models. Under identical fine-tuning conditions, NeoBERT consistently outperforms all baselines. With its unparalleled efficiency, optimal depth-to-width, and plug-and-play compatibility, NeoBERT represents the next generation of encoder models. To foster transparency and collaboration, we release all code, data, model checkpoints, and training scripts, making NeoBERT the only fully open-source model of its kind.\nBroader Impact Statement\nDespite its improvements, NeoBERT inherits the biases and limitations of its pre-training data. Moreover, the greatest jump in performance stems from the pre-training dataset, and as newer, larger, and more diverse datasets become available, retraining will likely be needed to further improve its performance. Nonetheless, NeoBERT stands today as an affordable state-of-the-art pre-trained encoder with great potential for downstream applications."}, {"title": "Training details", "content": "NeoBERT was trained on 8 H100 for 1,050,000 steps, for a total of 6,000 GPU hours. In the first stage of training, we used a local batch size of 32, 8 gradient accumulation steps, and a maximum sequence length of 1,024, for a total batch size of 2M tokens. In the second stage of training, we keep the theoretical batch size constant and increase the maximum sequence length to 4,096."}, {"title": "Ablations", "content": "Our first model, MO is modeled after BERTbase in terms of architecture. The only two differences are the absence of the next-sentence-prediction objective, as well as Pre-Layer Normalization. Each successive model, up until M8 is identical to the previous one on every point except for the change reported in "}, {"title": "GLUE", "content": "We perform a classical parameter search with learning rates in {5e - 6, 6e - 6, 1e - 5, 2\u0435 \u2013 5, \u0417\u0435 \u2013 5}, batch sizes in {4, 8, 16, 32} and weight decay in {1e - 2, 1e - 5}. In addition, we start training from the best MNLI checkpoint for RTE, STS, MRPC, and QNLI.\nWe fine-tune on the training splits of every glue dataset for 10 epochs, with evaluation on the validation splits every n steps, n being defined as min(500, len(dataloader) // 10) with early stopping after 15 evaluation cycles if scores have not improved.\nFollowing BERT, we exclude WNLI from our evaluation. For tasks with two scores and for MNLI matched and mismatched, we report the average between the two metrics."}, {"title": "MTEB", "content": "As demonstrated in Figure 4, evaluating out-of-the-box pre-trained models on MTEB is inconclusive. In that setting, BERTbase outperforms both BERTlarge and ROBERTalarge, highlighting the importance of fine-tuning to ensure representative evaluation on the MTEB benchmark.\nFollowing the existing literature, we designed a simple fine-tuning strategy entirely agnostic to the models evaluated. We used cosine similarity and \u03c4 = 0.07 as a temperature parameter in the contrastive learning loss. Additionally, we sampled datasets with a multinomial distribution based on their sizes (nj)j=1 with\n\u03b1 = 0.5:\n\u03c0i = \\frac{n_i^{\\alpha}}{\\sum_{j=1}^m n_j^{\\alpha}}\nWe trained on the following fully-open datasets: AG-News (Zhang et al., 2016), All-NLI (Bowman et al., 2015; Williams et al., 2018), AmazonQA (Gupta et al., 2019), ConcurrentQA (Arora et al., 2022), GitHub Issues (Li & Li, 2023), GooAQ (Khashabi et al., 2021), MedMCQA (Pal et al., 2022), NPR4, PudMedQA (Jin et al., 2019), SentenceCompression (Filippova & Altun, 2013) StackExchange, TriviaQA (Han et al., 2019), Wikihow (Koupaee & Wang, 2018), Yahoo! Answers (Zhang et al., 2016) as well as the available training splits of MTEB datasets (StackOverFlowDupQuestion, Fever (Thorne et al., 2018), MS MARCO (Bajaj et al., 2018), STS12, and STSBenchmark (Cer et al., 2017)).\nWe fine-tune every model for 2,000 steps and evaluate on MTEB in float16. The complete results are presented."}]}