{"title": "BREAKING THE RECLUSTERING BARRIER IN CENTROID-BASED DEEP CLUSTERING", "authors": ["Lukas Miklautz", "Timo Klein", "Kevin Sidak", "Collin Leiber", "Thomas Lang", "Andrii Shkabrii", "Sebastian Tschiatschek", "Claudia Plant"], "abstract": "This work investigates an important phenomenon in centroid-based deep clustering (DC) algorithms: Performance quickly saturates after a period of rapid early gains. Practitioners commonly address early saturation with periodic reclustering, which we demonstrate to be insufficient to address performance plateaus. We call this phenomenon the \u201creclustering barrier\u201d and empirically show when the reclustering barrier occurs, what its underlying mechanisms are, and how it is possible to Break the Reclustering Barrier with our algorithm BRB. BRB avoids early over-commitment to initial clusterings and enables continuous adaptation to reinitialized clustering targets while remaining conceptually simple. Applying our algorithm to widely-used centroid-based DC algorithms, we show that (1) BRB consistently improves performance across a wide range of clustering benchmarks, (2) BRB enables training from scratch, and (3) BRB performs competitively against state-of-the-art DC algorithms when combined with a contrastive loss. We release our code and pre-trained models at https://github.com/Probabilistic-and-Interactive-ML/\nbreaking-the-reclustering-barrier.", "sections": [{"title": "1 INTRODUCTION", "content": "\"To live is to change; to be perfect is to change often\" (Newman, 1845): Though not originally about ML, this proverb underscores the importance of adaptability. This adaptability is critical for the success of machine learning algorithms, especially during training. It is particularly crucial in clustering \u2013 a family of unsupervised learning algorithms that partition samples into multiple groups based on their similarity. Deep clustering (DC) involves not only assigning samples to a particular group but also jointly learning a representation of the data using deep learning. As this representation improves, the algorithm enhances its ability to identify clusters in the data, requiring it to adjust its assignments frequently.\nIn this paper, we analyze whether centroid-based DC algorithms can sufficiently adapt during training to facilitate further improvements. The central finding of our analysis is that they cannot and that commonly used techniques, such as reclustering with k-Means, are insufficient to address this issue. Specifically, we find that reclustering by itself is not enough to enable late-training improvements to the clustering because it fails to change the structure of the underlying embedded space. A bad initial representation or clustering exacerbates the effect and potentially decreases final performance by more than 10%, as our experiments demonstrate. When a clustering algorithm cannot improve late in training despite frequent reclustering, we refer to this as the reclustering barrier."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Deep clustering (DC) is the combination of clustering and deep learning. Popular deep learning approaches for DC include self-supervised networks such as autoencoders (AEs), or SimCLR (Chen et al., 2020) as they can be trained without given labels. The idea of centroid-based DC is to obtain an initial clustering, typically by running the clustering algorithm k-Means in the latent space of a pre-trained network. The algorithm proceeds to update this result by optimizing \\(L(0, M) = \\Lambda_1L_{SSL}(0) + \\Lambda_2L_c(0, M)\\), where \\(L_c\\) refers to the clustering loss and \\(L_{SSL}\\) refers to the self-supervised loss (e.g., contrastive or reconstruction loss, for SimCLR and AE respectively). Here, the embedding and the clustering can be updated simultaneously or iteratively (Zhou et al., 2022). A well-known representative utilizing simultaneous optimization is DEC (Xie et al., 2016). It uses a kernel based on the Student's t-distribution to minimize the Kullback-Leibler divergence between the data distribution and an auxiliary target distribution, which can be trained end-to-end. As \\(L_{SSL}\\) is not used during the clustering optimization, a distorted embedding could occur (Guo et al., 2017). This issue is tackled by IDEC (Guo et al., 2017), which concurrently optimizes \\(L_c\\) and \\(L_{SSL}\\). DCN (Yang et al., 2017) pursues an iterative optimization, where the embedding is frozen when the clustering is updated, and vice versa. Since the cluster centers are not learned via the neural network but explicitly updated, it is feasible to use k-Means to obtain non-differentiable hard cluster labels. The well-established algorithms DEC, IDEC, and DCN are the building blocks for a number of follow-up works and have diverse applications, making them a natural choice for this study. We provide more details on these algorithms in Appendix B.1."}, {"title": "3 PROBLEM SETUP", "content": "Given an unlabeled dataset \\(X = \\{x_i\\}_{i=1}^n\\) containing \\(n\\) instances \\(x_i \\in \\mathbb{R}^D\\), our objective is to partition \\(X\\) into \\(k\\) distinct clusters. Each cluster is represented by a centroid \\(\\mu_j \\in \\mathbb{R}^d\\), with \\(j \\in \\{1,...,k\\}\\). This partitioning requires learning assignments \\(s_{i,j} \\in \\{0,1\\}\\) of instances \\(x_i\\) to centroids \\(\\mu_j\\) subject to \\(\\sum_{j=1}^k s_{i,j} = 1\\) for all \\(i\\). Additionally, deep clustering methods aspire to learn a \u201ccluster-friendly\u201d (Yang et al., 2017) latent feature space \\(H \\subseteq \\mathbb{R}^d\\), where \\(d \\ll D\\) utilizing a non-linear encoder \\(f_\\theta: X \\rightarrow H\\). For example, a k-Means friendly latent space consists of compressed spherical clusters (low intra-cluster distance) that are well-separated (high inter-cluster distance).\nMany methods utilize a self-supervised auxiliary objective to prevent trivial solutions in DC, e.g., by including a reconstruction or contrastive loss in addition to the clustering loss. This auxiliary objective is applied in a task-dependent output space \\(Z\\) and is often learned with a separate task head \\(g: H\\rightarrow Z\\). In the case of reconstruction, the function \\(g\\) serves as a decoder network, where \\(Z\\subseteq \\mathbb{R}^D\\) corresponds to the space of reconstructed data points. For contrastive learning, \\(g\\) acts as a projector network (e.g., a shallow MLP), where \\(Z\\) is the projector's output space.\nA deep centroid-based clustering algorithm as outlined above can be formalized as the iterative application of a function \\(C: (\\theta_t, S_t, M_t) \\rightarrow (\\theta_{t+1}, S_{t+1}, M_{t+1})\\). Here, \\(\\theta_t \\in \\Theta\\) represents the network parameters at step \\(t\\), \\(M_t \\in \\mathbb{R}^{k \\times d}\\) the centroid matrix and \\(S_t \\in [0,1]^{n \\times k}\\) a matrix encoding the cluster assignments for each sample. The function \\(C\\) optimizes a clustering objective \\(L(\\theta_t, S_t, M_t): \\Theta \\times [0,1]^{n \\times k} \\times \\mathbb{R}^{k \\times d} \\rightarrow \\mathbb{R}\\), utilizing a gradient-descent optimizer. Its output are improved parameters \\(\\theta_{t+1}\\) as well as new assignments \\(S_{t+1}\\) and centroids \\(M_{t+1}\\). Typically, \\(C\\) is iterated until a local minimum of \\(L(\\theta, S, M)\\) is reached."}, {"title": "4 THE RECLUSTERING BARRIER", "content": "Before introducing BRB in Section 5, we examine why reclustering by itself fails to enhance performance beyond a certain threshold during DC training. We assess the effect of reclustering on the embedded space using intra-class distance (intra-CD) to measure variation within ground truth classes and inter-class distance (inter-CD) to assess separation between ground truth classes; this follows the embedding space analysis in (Lehner et al., 2024). We quantify the impact of reclustering on the clustering targets by the cluster label change (CL Change) between epochs in terms of the normalized mutual information (NMI) (Kvalseth, 1987) as \\((1 - NMI(S_t, S_{t-1})) \\cdot 100\\). A value of 0 means no change between two consecutive clusterings, while a value of 100 indicates complete change."}, {"title": "5 OUR APPROACH - BRB", "content": "Our goal with this work is to overcome the reclustering barrier in a centroid-based deep clustering algorithm \\(C\\). To this end, we establish three desiderata for our approach:\n1.  Exploration: The proposed modification should increase the exploration of clustering solutions.\n2.  Knowledge preservation: The knowledge encoded in network parameters \\(\\theta\\) should be preserved while exploring a wider range of solutions.\n3.  Generality: The proposed modification should be applicable to existing DC approaches.\nWe now define our modified algorithm as\n\\[C_{BRB}: (\\theta_t, S_t, M_t) \\rightarrow C(\\iota_\\omega(\\theta_t), \\iota_c(S_t, M_t)),\\]\nconsisting of two modifications \\(\\iota_\\omega\\) and \\(\\iota_c\\) as well as optional momentum resets, each impacting different components of the deep clustering algorithms.\nWeight resets Section 4 motivates the need for structured changes in the embedded space to enable effective reclustering. BRB achieves such changes by applying a soft weight reset (Ash &\nAdams, 2020; D'Oro et al., 2023) to the the non-linear encoder \\(f_\\theta\\). In particular, we obtain modified"}, {"title": "6 EXPERIMENTS", "content": "We integrate BRB into DEC, IDEC, and DCN, presenting experiments that demonstrate the performance improvements resulting from this integration for a wide range of datasets. The first part"}]}