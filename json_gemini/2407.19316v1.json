{"title": "AResNet-ViT: A Hybrid CNN-Transformer Network for Benign and Malignant\nBreast Nodule Classification in Ultrasound Images", "authors": ["ZHAO XIN", "ZHU QIANQIAN", "WU JIALING"], "abstract": "Background Breast cancer is one of the most common types of cancer in women worldwide. Early diagnosis is crucial for\nimproving survival rates. In recent years, there has been extensive research on computer-aided diagnosis (CAD) using\nartificial intelligence techniques, specifically deep neural networks, to accurately and automatically predict the benign or\nmalignant nature of breast nodules in ultrasound images. However, accurate classification faces challenges due to the\nsimilarity between breast lesions and surrounding tissues, as well as the overlapping appearances of partially benign and\nmalignant nodules. To address these challenges, this study proposes a deep learning network that integrates convolutional\nneural networks (CNN) and Transformer by leveraging the complementary strengths of CNN in local feature extraction and\nTransformers in global feature extraction, aiming to enhance the classification of benign and malignant breast lesions in\nultrasound images.\nMethod The proposed network AResNet-ViT adopts a dual-branch architecture for comprehensive local-global features\nextraction. In the local feature extraction branch, a residual network with multiple attention-guided modules is utilized. This\ndesign effectively captures local details and texture features specific to breast nodules, enhancing sensitivity to subtle\nchanges within the nodules and aiding in accurate classification of their benign or malignant nature. The global feature\nextraction branch leverages a multi-head self-attention ViT network to capture the overall shape, boundary, and\nrelationships with surrounding tissues. This enhances the understanding and modeling of both nodule-specific and global\nimage features. The experiments were conducted on the publicly available BUSI dataset and the performance of the\nexperiments was evaluated using metrics of ACC, TPR, TNR, and AUC. The binary Cross Entropy (BCE) loss function was\napplied in training process.\nResults We conducted ablation experiments on the attention-guided design of the residual CNN branch. We also\nperformed architecture ablation experiments on the CNN branch and the transformer branch separately, as well as using\nboth branches together. Additionally, we compared the performance of our proposed AResNet-ViT network with that of\nclassical classification models and the results from three recent papers published in the past three years. The experimental\nresults demonstrate that the AResNet-ViT network, with its hybrid CNN-transformer structure and multi-attention\n1", "sections": [{"title": "1 INTRODUCTION", "content": "Breast nodules, which may manifest as cystic or solid masses, are frequently encountered in breast tissue\nand represent a prevalent condition among women. These nodules are categorized as either benign or\nmalignant. Benign breast nodules do not pose a substantial health risk, whereas malignant breast nodules\nindicate the presence of cancerous proliferation, thereby posing a significant threat to women's overall\nphysical and emotional well-being.\nRegular breast screening, including mammography X-ray, breast ultrasound, and breast magnetic\nresonance imaging (MRI), plays a crucial role in early detection of breast nodules and diagnosis of breast\ncancer. Mammography X-ray imaging, despite its significant radiation exposure and limited imaging angles, is\nprimarily employed for further screening of malignant nodules. MRI imaging, on the other hand, is time-\nconsuming and expensive, making it impractical for routine outpatient examinations. Ultrasound imaging, with\nits advantages of radiation-free, affordability, convenience, speed, and versatility in imaging from various\nangles, has emerged as the primary modality for evaluating breast nodules [1]. Nevertheless, the diagnostic\naccuracy of ultrasound for breast nodules heavily relies on the clinical expertise of ultrasound practitioners.\nConsequently, variations in physician experience or the impact of visual fatigue often lead to misdiagnosis or\nmissed diagnoses.\nWith the continuous advancement of artificial intelligence technology, researchers have extensively\nexplored computer-aided diagnosis of ultrasound breast nodules. Their efforts focus on developing intelligent\nalgorithms capable of automatically identifying and characterizing nodule areas in ultrasound images as\nbenign or malignant. These algorithms leverage techniques such as deep learning and machine learning to\ntrain models for nodule identification and classification. This Al-assisted diagnostic approach holds the\npotential to enhance the accuracy and efficiency of ultrasound-based assessment of breast nodules,\nequipping clinicians with reliable auxiliary tools to support clinical decision-making and treatment planning.\nConversely, certain malignant nodules may display clear borders and an aspect ratio smaller than 1, aligning\nwith characteristics typically associated with benign nodules, which cause difficulty in Al identification.\nIn the past decade, deep learning-based approaches have achieved remarkable success in natural image\nclassification and garnered widespread attention in the field of medical image recognition. Specifically, in the\ndomain of ultrasound breast image classification and recognition, several studies have employed various\nCNN-based deep learning models to learn and extract features specific to breast nodules in ultrasound\nimages. in 2016, Huynh et al. [2] used the ImageNet data set to preprocess VGGNet, ResNet and DenseNet,\nsubsequently comparing the classification performance of these networks on breast ultrasound images. In\n2017, Han et al. [3] utilized the GoogLeNet algorithm to discern benign/malignant ultrasonic breast nodules.\n2"}, {"title": "2 METHOD", "content": "The dual-branch architecture of AResNet-ViT, consisting of two branches, is shown in Figure 1. The upper\nbranch of the network utilizes a residual network guided by multiple attentions to effectively capture the local\ndetails and texture features of breast nodules. This capability enhances sensitivity to subtle changes within\nthe nodules, contributing to the accurate determination of benign and malignant nodules. On the other hand,\nthe lower branch of the network utilizes a multi-head self-attention-based Vision Transformer (ViT) to capture\nthe overall shape, boundary, and relationship between the nodule and the surrounding tissue and enhances\nthe understanding of both the nodule and the overall image characteristics. By combining and encoding the\nfeatures extracted from the local feature extraction branch and the global dependency feature extraction\nbranch, the network can effectively utilize both local and global information to improve the accuracy of breast\nnodule classification. Each branch of the network outputs a one-dimensional feature, which is subsequently\nconcatenated and then encoded by a fully connected multi-layer perceptron (MLP). Finally, the classification\nresult is obtained through Sigmoid activation.\nAResNet\nResNet Model 1\nResNet Model 2\nResNet Model 3\nResNet Model 4\nPatch-Embrdding\n3\u00d73Conv CA\nChannel\nAttention\nAverage\nAP Pooling\nFC\nMulti-head Self-Attention ViT\n0\nPosition\nEmbedding\n2\n3\nLN\nMSA\n6\n14\n15\nLNMLP\n6\nTransformer-Block\n12x\nFully\nConnected\nLN\nLayer\nNormalization\nMSA\nMulti-Head\nSelf-Attention\nMLP\nMultilayer\nPerceptron\n14\n15\n\u2022 Benign\nSigmoid\nC MLP\n\u2022 Malignant\nMaxPooling\nFigure 1: The network architecture of AResNet-ViT for ultrasound breast lesion classification\n2.1 Local feature extraction\nTo enhance the network's capacity for focusing on and learning internal features of ultrasound breast nodules,\nwe propose a locally-guided attention-based residual network named AResNet as the local feature extraction\nbranch. This architecture is constructed based on the ResNet18 framework and comprises four residual\nblocks, each incorporating attention mechanisms, as illustrated in Figure 1. Within the structure of residual\nblocks 1 and 2, the network emphasizes intricate details, such as texture and edges, present in the ultrasound\nimages. Given the substantial image size and the abundance of intricate details, the integration of spatial\nattention mechanisms becomes imperative to facilitate the network in effectively capturing and\ncomprehending internal nodule information. The segmentation mask of the ultrasound breast nodule provides\npositional information and can act as a guide for spatial attention. Consequently, in residual blocks 1 and 2,\nwe introduce ultrasound breast nodule segmentation mask attention (ROI-mask Attention, RA) [19].\n4"}, {"title": "2.2 Global feature extraction", "content": "Convolutional neural networks (CNNs) primarily emphasize local receptive fields for information filtering while\nneglecting the global pixel-level self-correlation when processing ultrasound breast images. To augment the\nnetwork's capability to acquire comprehensive global contextual information, this study incorporates a Vision\nTransformer (ViT) network that leverages the multi-head self-attention mechanism. The ViT network extracts\nboth global image features and pixel-level self-correlation, as depicted in the lower branch of Figure 1. The\nnetwork comprises 12 Transformer blocks connected in series. Each Transformer block independently\nperforms self-attention and feed-forward neural network operations to iteratively extract features from the\n5"}, {"title": "2.3 Loss function and evaluation metrics", "content": "2.3.1 Loss function\nGiven that breast ultrasound image classification is a binary classification task, the Binary Cross Entropy\n(BCE) loss function is employed, as denoted by formula 2.\n$L_{BCE} = \\frac{1}{N} \\sum_{i=1}^N - [y_i \\log (P_i) + (1 - y_i) \\log(1-p_i)] $\nWhere N represents the total number of pixels in the input image. $y \\in \\{0,1\\}$represents the true label of the\ni-th pixel, where 1 represents the pixel corresponding to the positive class and 0 represents the negative class.\np\u2081 \u2208 {0,1}indicates the probability that pixel i is predicted to be a positive class.\n3 RESULTS\n3.1\nExperimental settings\n3.1.1 Image Dataset\nWe assessed the performance of AResNet-ViT using the publicly available BUSI dataset, which was acquired\nfrom two ultrasound devices (LOGIQ E9 and LOGIQ E9Agile) at Bachia Hospital[20]. The images in the\ndataset have dimensions of 500x500, encompassing a total of 780 images. Among these, 487 images contain\nbenign nodules, 210 images contain malignant nodules, and 133 images depict normal breast tissue. All\nimages have been meticulously annotated by experienced medical professionals. Given the primary focus of\nthis study on breast lesion recognition, experiments were conducted exclusively employing benign and\nmalignant images. To ensure a rigorous evaluation, these images were meticulously partitioned into training\nand testing sets, maintaining a well-balanced 8:2 ratio. To address the issue of overfitting due to limited data\nsamples, data augmentation techniques, including horizontal flipping and rotations at 90\u00b0, 180\u00b0, and 270\u00b0, are\nemployed on the training set images. This effectively increases the sample size by a factor of five.\nSubsequently, all augmented and original images are resized from their original 500x500 dimensions to\n224x224.\n3.1.2 Experimental environment\nThe experiments in this work were all performed using Python 3.7 under a Windows 10 operating system. All\ndeep learning models were developed using the Keras framework. The specific workstation parameters were\n6"}, {"title": "3.2 Evaluation Metrics", "content": "The performance of all experiments was evaluated using accuracy rate (ACC), true positive rate (TPR), true\nnegative rate (TNR), and area under the curve (AUC). ACC provides an overall assessment of the model's\nclassification performance. TPR represents the probability of correctly classifying a malignant nodule as\nmalignant. TNR represents the probability of accurately labeling a benign nodule as benign. AUC measures\nthe area under the Receiver Operating Characteristic (ROC) curve, with TPR plotted on the vertical axis and\nFalse Positive Rate (FPR) on the horizontal axis. The AUC value ranges between 0 and 1, where a higher\nvalue indicates better classification performance. These evaluation metrics were defined as formulas (3)-(5).\n$ACC=\\frac{TP+TN}{TP+FP+FN+TN}$ (3)\n$TPR = \\frac{TP}{TP + FN}$ (4)\n$TNR = \\frac{TN}{TN+FP}$ (5)\nWhere TP represents the number of pixels with real labels as breast lesions and classified as breast\nlesions; TN represents the number of pixels with real labels as non-breast lesions and classified as non-\nbreast lesions; FP represents the number of pixels with real labels as non-breast lesions but classified as non-\nbreast lesions. The number of pixels of breast lesions; FN represents the number of pixels whose true label is\nbreast lesions and is classified as non-breast lesions.\n7"}, {"title": "3.3 Ablation experiments", "content": "3.3.1 Effectiveness of the attention mechanism\nTo validate the rationality and effectiveness of the attention guidance module, five ablation experiments were\nconducted, and the corresponding results are presented in Table 1. \"Network 1\" refers to the ResNet18\nnetwork without any attention added. \"Network 2\" incorporates segmentation mask attention after the\ncompletion of the first two residual blocks of the ResNet18 network, while \"Network 3\" incorporates\nsegmentation mask attention after the completion of the last two residual blocks. \"Network 4\" integrates\nsegmentation mask attention after the completion of all residual blocks in the ResNet18 network. Lastly,\n\"Network 5\" extends \"Network 2\" by further adding channel attention after the completion of the last two\nresidual blocks. The same set of parameters was used across all experiments.\n8"}, {"title": "3.4 The heat-maps of classification results", "content": "The visual classification results for the test samples obtained from the AResNet-ViT model are depicted in\nFigure 3, in which the original ultrasound breast nodule images are displayed in the top row, while the feature\nattention heat map generated by the AResNet-ViT model is presented in the bottom row. The feature attention\nheat map assigns weights to each position of the input data, indicating the areas or features to which the\nmodel pays greater attention. This visualization enables us to identify the specific regions within the input data\nthat the model finds most significant. It can be observed from the figure that the nodule area receives primary\nattention from the model, as indicated by the highly weighted regions in the heat map.\nFurthermore, in breast ultrasound images where the internal ultrasound characteristics of the nodule\nresemble those of the surrounding tissue, the model demonstrates accurate discrimination between the\nnodule area and the background. Additionally, the prediction results of the AResNet-ViT model for benign and\nmalignant nodule samples with overlapping representations were found to align with the gold standard,\nindicating the model's capability to achieve precise classification.\n9"}, {"title": "3.5 Comparative analysis", "content": "to investigate whether AResNet-ViT outperforms the existing classical models and other methods published in\nthe field, we conducted a comparative analysis. The analysis is divided into two parts: an initial comparison\nwith four well-established classical models (VGG16 [21], ResNet34 [22], DenseNet [23], and InceptionV3 [24]),\nfollowed by a comparison with three recently published methods. except for the dataset used in reference [25],\nall other methods, including the one proposed in the study, utilize the same BUSI dataset.\n10"}, {"title": "4 DISCUSSION", "content": "In this study, a hybrid CNN-Transformer architecture called AResNet-ViT was proposed for the benign-\nmalignant classification of breast nodules in breast ultrasound images. The AResNet-ViT model combines the\nability of CNNs to extract localized features and the capability of Transformers to model global features,\nresulting in a more discriminative feature representation for accurate classification. AResNet-ViT is designed\nwith a dual-branch architecture. One branch focuses on extracting local detailed features from images using a\nresidual network based on the ResNet18 framework. This branch consists of four residual blocks, each of\nwhich incorporates an attention mechanism. The other branch leverages a Vision Transformer (ViT) for global\nfeature extraction. We use segmentation mask attention and channel attention for the shallow and deep\nmodules of residual networks respectively, because the shallow layers of residual networks mainly extract\nlow-level semantic features, paying more attention to nodule location information, whereas the deep residual\nnetworks extract high-level semantic features, and the weight of channels is more important than nodule\nlocation. Ablation experiments conducted on the residual network verify that the combined use of both types\nof attention achieves higher evaluation metrics compared to using segmentation mask attention or channel\nattention alone.\nAblation experiments were conducted to evaluate the performance of different architectures in ultrasonic\nbreast nodule classification. The study compared ResNet18, ViT, AResNet (ResNet18 with segmentation\nmask and channel attention), and the fusion of AResNet and ViT. The results showed that the combination of\nResNet18 and ViT (ResNetViT) outperformed individual networks, indicating the benefit of network integration.\nAResNet exhibited improvements over ResNet18 in accuracy, true positive rate, true negative rate, and area\nunder the curve. The fusion of AResNet and ViT further enhanced performance, particularly in identifying\nsamples with malignant characteristics but are actually benign, which are critical for accurate clinical\ndiagnosis.\nThe heat maps of the classification results demonstrate that the AResNet-ViT model can accurately\ndiscriminate between the nodule area and the background, even in cases where the internal ultrasound\ncharacteristics of the nodule resemble those of the surrounding tissue. This further confirms the AResNet-ViT\nmodel's ability to learn and recognize features specific to the nodule region, indicating its precise classification\ncapability. When compared to classical models and recently published methods in the field of ultrasonic\nbreast nodule classification, our AResNet-ViT model demonstrates superior performance across all metrics,\nincluding ACC, TPR, TNR, and AUC of 0.889, 0.861, 0.896, and 0.925, respectively. The results indicate that\nthe CNN-Transformer hybrid architectures can significantly improve ultrasound breast nodule classification.\nMoreover, integrating attention mechanisms in the convolutional stages enhances the extraction of local\nfeatures.\nDespite its outstanding performance, our method had some limitations, such as the requirement for a large\ndataset for effective training. Breast ultrasound images are intricate and vary greatly between individuals,\nmaking it challenging to build a robust classifier with limited data. Future work should focus on collecting a\nlarger and more diverse dataset to improve model generalization. In addition, the computational complexity of\nthe hybrid model can be high, making it challenging to deploy in real-time clinical settings. Therefore, future\nwork should also optimize the model for efficient computation and reducing inference time is essential for\npractical applications.\n11"}]}