{"title": "AResNet-ViT: A Hybrid CNN-Transformer Network for Benign and Malignant Breast Nodule Classification in Ultrasound Images", "authors": ["ZHAO XIN", "ZHU QIANQIAN", "WU JIALING"], "abstract": "Background Breast cancer is one of the most common types of cancer in women worldwide. Early diagnosis is crucial for improving survival rates. In recent years, there has been extensive research on computer-aided diagnosis (CAD) using artificial intelligence techniques, specifically deep neural networks, to accurately and automatically predict the benign or malignant nature of breast nodules in ultrasound images. However, accurate classification faces challenges due to the similarity between breast lesions and surrounding tissues, as well as the overlapping appearances of partially benign and malignant nodules. To address these challenges, this study proposes a deep learning network that integrates convolutional neural networks (CNN) and Transformer by leveraging the complementary strengths of CNN in local feature extraction and Transformers in global feature extraction, aiming to enhance the classification of benign and malignant breast lesions in ultrasound images.\nMethod The proposed network AResNet-ViT adopts a dual-branch architecture for comprehensive local-global features extraction. In the local feature extraction branch, a residual network with multiple attention-guided modules is utilized. This design effectively captures local details and texture features specific to breast nodules, enhancing sensitivity to subtle changes within the nodules and aiding in accurate classification of their benign or malignant nature. The global feature extraction branch leverages a multi-head self-attention ViT network to capture the overall shape, boundary, and relationships with surrounding tissues. This enhances the understanding and modeling of both nodule-specific and global image features. The experiments were conducted on the publicly available BUSI dataset and the performance of the experiments was evaluated using metrics of ACC, TPR, TNR, and AUC. The binary Cross Entropy (BCE) loss function was applied in training process.\nResults We conducted ablation experiments on the attention-guided design of the residual CNN branch. We also performed architecture ablation experiments on the CNN branch and the transformer branch separately, as well as using both branches together. Additionally, we compared the performance of our proposed AResNet-ViT network with that of classical classification models and the results from three recent papers published in the past three years. The experimental results demonstrate that the AResNet-ViT network, with its hybrid CNN-transformer structure and multi-attention", "sections": [{"title": "1 INTRODUCTION", "content": "Breast nodules, which may manifest as cystic or solid masses, are frequently encountered in breast tissue and represent a prevalent condition among women. These nodules are categorized as either benign or malignant. Benign breast nodules do not pose a substantial health risk, whereas malignant breast nodules indicate the presence of cancerous proliferation, thereby posing a significant threat to women's overall physical and emotional well-being.\nRegular breast screening, including mammography X-ray, breast ultrasound, and breast magnetic resonance imaging (MRI), plays a crucial role in early detection of breast nodules and diagnosis of breast cancer. Mammography X-ray imaging, despite its significant radiation exposure and limited imaging angles, is primarily employed for further screening of malignant nodules. MRI imaging, on the other hand, is time-consuming and expensive, making it impractical for routine outpatient examinations. Ultrasound imaging, with its advantages of radiation-free, affordability, convenience, speed, and versatility in imaging from various angles, has emerged as the primary modality for evaluating breast nodules [1]. Nevertheless, the diagnostic accuracy of ultrasound for breast nodules heavily relies on the clinical expertise of ultrasound practitioners. Consequently, variations in physician experience or the impact of visual fatigue often lead to misdiagnosis or missed diagnoses.\nWith the continuous advancement of artificial intelligence technology, researchers have extensively explored computer-aided diagnosis of ultrasound breast nodules. Their efforts focus on developing intelligent algorithms capable of automatically identifying and characterizing nodule areas in ultrasound images as benign or malignant. These algorithms leverage techniques such as deep learning and machine learning to train models for nodule identification and classification. This Al-assisted diagnostic approach holds the potential to enhance the accuracy and efficiency of ultrasound-based assessment of breast nodules, equipping clinicians with reliable auxiliary tools to support clinical decision-making and treatment planning. Conversely, certain malignant nodules may display clear borders and an aspect ratio smaller than 1, aligning with characteristics typically associated with benign nodules, which cause difficulty in Al identification.\nIn the past decade, deep learning-based approaches have achieved remarkable success in natural image classification and garnered widespread attention in the field of medical image recognition. Specifically, in the domain of ultrasound breast image classification and recognition, several studies have employed various CNN-based deep learning models to learn and extract features specific to breast nodules in ultrasound images. in 2016, Huynh et al. [2] used the ImageNet data set to preprocess VGGNet, ResNet and DenseNet, subsequently comparing the classification performance of these networks on breast ultrasound images. In 2017, Han et al. [3] utilized the GoogLeNet algorithm to discern benign/malignant ultrasonic breast nodules."}, {"title": "2 METHOD", "content": "The dual-branch architecture of AResNet-ViT, consisting of two branches, is shown in Figure 1. The upper branch of the network utilizes a residual network guided by multiple attentions to effectively capture the local details and texture features of breast nodules. This capability enhances sensitivity to subtle changes within the nodules, contributing to the accurate determination of benign and malignant nodules. On the other hand, the lower branch of the network utilizes a multi-head self-attention-based Vision Transformer (ViT) to capture the overall shape, boundary, and relationship between the nodule and the surrounding tissue and enhances the understanding of both the nodule and the overall image characteristics. By combining and encoding the features extracted from the local feature extraction branch and the global dependency feature extraction branch, the network can effectively utilize both local and global information to improve the accuracy of breast nodule classification. Each branch of the network outputs a one-dimensional feature, which is subsequently concatenated and then encoded by a fully connected multi-layer perceptron (MLP). Finally, the classification result is obtained through Sigmoid activation."}, {"title": "2.1 Local feature extraction", "content": "To enhance the network's capacity for focusing on and learning internal features of ultrasound breast nodules, we propose a locally-guided attention-based residual network named AResNet as the local feature extraction branch. This architecture is constructed based on the ResNet18 framework and comprises four residual blocks, each incorporating attention mechanisms, as illustrated in Figure 1. Within the structure of residual blocks 1 and 2, the network emphasizes intricate details, such as texture and edges, present in the ultrasound images. Given the substantial image size and the abundance of intricate details, the integration of spatial attention mechanisms becomes imperative to facilitate the network in effectively capturing and comprehending internal nodule information. The segmentation mask of the ultrasound breast nodule provides positional information and can act as a guide for spatial attention. Consequently, in residual blocks 1 and 2, we introduce ultrasound breast nodule segmentation mask attention (ROI-mask Attention, RA) [19].\nThe input features undergo two consecutive convolutions with a kernel size of 3x3 to extract local contextual information. Residual connections are introduced to expedite network optimization. Finally, the output features are element-wise multiplied with the segmentation mask image. Residual block 1 or 2 is formulated as follows.\n$Y(x) = (F(x) + x) \\times R(x) \\times C\\text{      }(1)$\nWhere x represents the input, F(x) represents the features learned through the convolution block, R(x) represents the nodule mask feature map, Y(x) represents the learning feature output under the attention guidance of the segmentation mask, and C is used to match the dimensions of the residual block and the segmentation mask map.\nResidual blocks 3 and 4 further extract high-level semantic features based on the information derived from residual blocks 1 and 2. Each output channel within these blocks represents a distinct high-level semantic representation, contributing differently to the overall high-level semantics. Therefore, a Channel Attention (CA) module, as illustrated in Figure 2, is employed in residual blocks 3 and 4 to enhance the network's focus on channel outputs and amplify the informative channel representations. The CA module conducts global average pooling and global maximum pooling operations on the input feature map. The resulting one-dimensional feature vectors from both pooling operations are then combined and encoded using a multi-layer perceptron (MLP). Subsequently, the encoded result is subjected to a Sigmoid activation function to obtain a vector representing the weights assigned to each channel. This vector is then element-wise multiplied with the deep features of the input module. The primary objective of this module is to assign varying weights to each channel, thereby amplifying the channel-specific information that effectively captures the high-level semantic features exhibited by the ultrasound breast nodules."}, {"title": "2.2 Global feature extraction", "content": "Convolutional neural networks (CNNs) primarily emphasize local receptive fields for information filtering while neglecting the global pixel-level self-correlation when processing ultrasound breast images. To augment the network's capability to acquire comprehensive global contextual information, this study incorporates a Vision Transformer (ViT) network that leverages the multi-head self-attention mechanism. The ViT network extracts both global image features and pixel-level self-correlation, as depicted in the lower branch of Figure 1. The network comprises 12 Transformer blocks connected in series. Each Transformer block independently performs self-attention and feed-forward neural network operations to iteratively extract features from the input sequence. This design enables multiple iterations of self-attention and feature extraction at various levels, thereby enhancing the model's expressiveness and performance.\nThe process of global feature extraction is as follows: First, the input image of size 224x224 is divided into blocks of size 16x16. Each image patch is linearly mapped and converted into a one-dimensional vector, preserving the spatial information between patches by adding position encoding. The data with position encoding is then fed into the Transformer blocks for layer-by-layer operations to perform feature encoding. By incorporating the self-attention mechanism, the network can capture the inter-dependencies among different locations in the image, facilitating a comprehensive understanding of the overall image context. This, in turn, improves the network's ability to extract the overall features and correlations of ultrasound breast images."}, {"title": "2.3 Loss function and evaluation metrics", "content": "2.3.1 Loss function\nGiven that breast ultrasound image classification is a binary classification task, the Binary Cross Entropy (BCE) loss function is employed, as denoted by formula 2.\n$L_{BCE} = -\\frac{1}{N}\\sum_{i=1}^{N} [y_i \\log(p_i) + (1 - y_i) \\log(1-p_i)] \\text{      }(2)$\nWhere N represents the total number of pixels in the input image. $y \\in \\{0,1\\}$represents the true label of the i-th pixel, where 1 represents the pixel corresponding to the positive class and 0 represents the negative class. $p_i \\in \\{0,1\\}$indicates the probability that pixel i is predicted to be a positive class."}, {"title": "3 RESULTS", "content": "3.1 Experimental settings\n3.1.1 Image Dataset\nWe assessed the performance of AResNet-ViT using the publicly available BUSI dataset, which was acquired from two ultrasound devices (LOGIQ E9 and LOGIQ E9Agile) at Bachia Hospital[20]. The images in the dataset have dimensions of 500x500, encompassing a total of 780 images. Among these, 487 images contain benign nodules, 210 images contain malignant nodules, and 133 images depict normal breast tissue. All images have been meticulously annotated by experienced medical professionals. Given the primary focus of this study on breast lesion recognition, experiments were conducted exclusively employing benign and malignant images. To ensure a rigorous evaluation, these images were meticulously partitioned into training and testing sets, maintaining a well-balanced 8:2 ratio. To address the issue of overfitting due to limited data samples, data augmentation techniques, including horizontal flipping and rotations at 90\u00b0, 180\u00b0, and 270\u00b0, are employed on the training set images. This effectively increases the sample size by a factor of five. Subsequently, all augmented and original images are resized from their original 500x500 dimensions to 224x224.\n3.1.2 Experimental environment\nThe experiments in this work were all performed using Python 3.7 under a Windows 10 operating system. All deep learning models were developed using the Keras framework. The specific workstation parameters were"}, {"title": "3.2 Evaluation Metrics", "content": "The performance of all experiments was evaluated using accuracy rate (ACC), true positive rate (TPR), true negative rate (TNR), and area under the curve (AUC). ACC provides an overall assessment of the model's classification performance. TPR represents the probability of correctly classifying a malignant nodule as malignant. TNR represents the probability of accurately labeling a benign nodule as benign. AUC measures the area under the Receiver Operating Characteristic (ROC) curve, with TPR plotted on the vertical axis and False Positive Rate (FPR) on the horizontal axis. The AUC value ranges between 0 and 1, where a higher value indicates better classification performance. These evaluation metrics were defined as formulas (3)-(5).\n$ACC=\\frac{TP+TN}{TP+FP+FN+TN} \\text{      }(3)$\n$TPR = \\frac{TP}{TP + FN} \\text{      }(4)$\n$TNR = \\frac{TN}{TN+FP} \\text{      }(5)$\nWhere TP represents the number of pixels with real labels as breast lesions and classified as breast lesions; TN represents the number of pixels with real labels as non-breast lesions and classified as non-breast lesions; FP represents the number of pixels with real labels as non-breast lesions but classified as non-breast lesions. The number of pixels of breast lesions; FN represents the number of pixels whose true label is breast lesions and is classified as non-breast lesions."}, {"title": "3.3 Ablation experiments", "content": "3.3.1 Effectiveness of the attention mechanism\nTo validate the rationality and effectiveness of the attention guidance module, five ablation experiments were conducted, and the corresponding results are presented in Table 1. \"Network 1\" refers to the ResNet18 network without any attention added. \"Network 2\" incorporates segmentation mask attention after the completion of the first two residual blocks of the ResNet18 network, while \"Network 3\" incorporates segmentation mask attention after the completion of the last two residual blocks. \"Network 4\" integrates segmentation mask attention after the completion of all residual blocks in the ResNet18 network. Lastly, \"Network 5\" extends \"Network 2\" by further adding channel attention after the completion of the last two residual blocks. The same set of parameters was used across all experiments."}, {"title": "3.3.2 Effectiveness of the dual-branch architecture", "content": "To assess the performance of each individual branch, as well as the combined architecture of the dual-branch architecture in ultrasonic breast nodule classification, ablation experiments were conducted across four experimental groups. The first group exclusively utilized the ResNet18 network for classification, while the second group employed the ViT network to classify benign and malignant breast nodules. In the third group, the ResNetA network was employed, incorporating a segmentation mask attention mechanism at the end of the first two residual blocks of the ResNet18 network, and channel attention at the end of the last two residual blocks, to conduct breast nodule classification experiments. The fourth and fifth groups were based on the ViT network architecture and involved the parallel fusion of the ResNet network and the AResNet network to classify benign and malignant breast nodules. The results of the ablation experiment are presented in Table 2.\nAs observed from the Table 2, the performance metrics of a single network (ResNet18 or ViT) were inferior to the combination of ResNet18 and ViT (ResNetViT network), signifying that network integration enables the learning of more relevant features specific to breast nodules. Furthermore, in comparison to the ResNet18 classification network, the AResNet classification network exhibited improvements in accuracy (ACC), true positive rate (TPR), true negative rate (TNR), and area under the curve (AUC), with respective increments of 0.061, 0.072, 0.062, and 0.066. This suggests that the AResNet classification network outperforms in guiding and learning nodule region features. Additionally, the parallel fusion of AResNet and ViT further enhanced performance metrics, with the most significant improvement observed in TNR. This indicates a higher recognition capability of AResNet-ViT model in identifying samples that manifest malignant nodules but are actually benign, which are crucial for clinical diagnosis as they represent the most easily misjudged cases."}, {"title": "3.4 The heat-maps of classification results", "content": "The visual classification results for the test samples obtained from the AResNet-ViT model are depicted in Figure 3, in which the original ultrasound breast nodule images are displayed in the top row, while the feature attention heat map generated by the AResNet-ViT model is presented in the bottom row. The feature attention heat map assigns weights to each position of the input data, indicating the areas or features to which the model pays greater attention. This visualization enables us to identify the specific regions within the input data that the model finds most significant. It can be observed from the figure that the nodule area receives primary attention from the model, as indicated by the highly weighted regions in the heat map.\nFurthermore, in breast ultrasound images where the internal ultrasound characteristics of the nodule resemble those of the surrounding tissue, the model demonstrates accurate discrimination between the nodule area and the background. Additionally, the prediction results of the AResNet-ViT model for benign and malignant nodule samples with overlapping representations were found to align with the gold standard, indicating the model's capability to achieve precise classification."}, {"title": "3.5 Comparative analysis", "content": "to investigate whether AResNet-ViT outperforms the existing classical models and other methods published in the field, we conducted a comparative analysis. The analysis is divided into two parts: an initial comparison with four well-established classical models (VGG16 [21], ResNet34 [22], DenseNet [23], and InceptionV3 [24]), followed by a comparison with three recently published methods. except for the dataset used in reference [25], all other methods, including the one proposed in the study, utilize the same BUSI dataset."}, {"title": "4 DISCUSSION", "content": "In this study, a hybrid CNN-Transformer architecture called AResNet-ViT was proposed for the benign-malignant classification of breast nodules in breast ultrasound images. The AResNet-ViT model combines the ability of CNNs to extract localized features and the capability of Transformers to model global features, resulting in a more discriminative feature representation for accurate classification. AResNet-ViT is designed with a dual-branch architecture. One branch focuses on extracting local detailed features from images using a residual network based on the ResNet18 framework. This branch consists of four residual blocks, each of which incorporates an attention mechanism. The other branch leverages a Vision Transformer (ViT) for global feature extraction. We use segmentation mask attention and channel attention for the shallow and deep modules of residual networks respectively, because the shallow layers of residual networks mainly extract low-level semantic features, paying more attention to nodule location information, whereas the deep residual networks extract high-level semantic features, and the weight of channels is more important than nodule location. Ablation experiments conducted on the residual network verify that the combined use of both types of attention achieves higher evaluation metrics compared to using segmentation mask attention or channel attention alone.\nAblation experiments were conducted to evaluate the performance of different architectures in ultrasonic breast nodule classification. The study compared ResNet18, ViT, AResNet (ResNet18 with segmentation mask and channel attention), and the fusion of AResNet and ViT. The results showed that the combination of ResNet18 and ViT (ResNetViT) outperformed individual networks, indicating the benefit of network integration. AResNet exhibited improvements over ResNet18 in accuracy, true positive rate, true negative rate, and area under the curve. The fusion of AResNet and ViT further enhanced performance, particularly in identifying samples with malignant characteristics but are actually benign, which are critical for accurate clinical diagnosis.\nThe heat maps of the classification results demonstrate that the AResNet-ViT model can accurately discriminate between the nodule area and the background, even in cases where the internal ultrasound characteristics of the nodule resemble those of the surrounding tissue. This further confirms the AResNet-ViT model's ability to learn and recognize features specific to the nodule region, indicating its precise classification capability. When compared to classical models and recently published methods in the field of ultrasonic breast nodule classification, our AResNet-ViT model demonstrates superior performance across all metrics, including ACC, TPR, TNR, and AUC of 0.889, 0.861, 0.896, and 0.925, respectively. The results indicate that the CNN-Transformer hybrid architectures can significantly improve ultrasound breast nodule classification. Moreover, integrating attention mechanisms in the convolutional stages enhances the extraction of local features.\nDespite its outstanding performance, our method had some limitations, such as the requirement for a large dataset for effective training. Breast ultrasound images are intricate and vary greatly between individuals, making it challenging to build a robust classifier with limited data. Future work should focus on collecting a larger and more diverse dataset to improve model generalization. In addition, the computational complexity of the hybrid model can be high, making it challenging to deploy in real-time clinical settings. Therefore, future work should also optimize the model for efficient computation and reducing inference time is essential for practical applications."}]}