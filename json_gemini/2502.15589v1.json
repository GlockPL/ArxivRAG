{"title": "LightThinker: Thinking Step-by-Step Compression", "authors": ["Jintian Zhang", "Yuqi Zhu", "Mengshu Sun", "Yujie Luo", "Shuofei Qiao", "Lun Du", "Da Zheng", "Huajun Chen", "Ningyu Zhang"], "abstract": "Large language models (LLMs) have shown remarkable performance in complex reasoning tasks, but their efficiency is hindered by the substantial memory and computational costs associated with generating lengthy tokens. In this paper, we propose LightThinker, a novel method that enables LLMs to dynamically compress intermediate thoughts during reasoning. Inspired by human cognitive processes, LightThinker compresses verbose thought steps into compact representations and discards the original reasoning chains, thereby significantly reducing the number of tokens stored in the context window. This is achieved by training the model on when and how to perform compression through data construction, mapping hidden states to condensed gist tokens, and creating specialized attention masks. Additionally, we introduce the Dependency (Dep) metric to quantify the degree of compression by measuring the reliance on historical tokens during generation. Extensive experiments on four datasets and two models show that LightThinker reduces peak memory usage and inference time, while maintaining competitive accuracy. Our work provides a new direction for improving the efficiency of LLMs in complex reasoning tasks without sacrificing performance\u00b9.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models (LLMs) have demonstrated their remarkable capabilities in complex reasoning tasks. As research in this domain progresses, the reasoning patterns of these models have gradually evolved from \"fast thinking\u201d to \u201cslow thinking\". This transition is exemplified by methods such as Chain-of-Thought (CoT) (Wei et al., 2022) prompting, which enhances reasoning by breaking down complex problems into sequential sub-steps. Building on this, the o1-like thinking mode (Jaech et al., 2024; Qwen., 2024; DeepSeek-AI et al., 2025) introduces multiple reasoning abilities such as trial-and-error, backtracking, correction, and iteration, further improving the success rate of models in solving complex problems. However, this performance improvement comes at the cost of generating a large number of tokens (Wu et al., 2024b). Given that current LLMs are predominantly based on the Transformer architecture (Vaswani et al., 2017), the computational complexity of the attention mechanism grows quadratically with the context length, while the storage overhead of the KV Cache increases linearly with the context length. For example, in the case of Qwen32B (Yang et al., 2024), when the context length reaches 104, the KV Cache occupies a space comparable to the model itself. Consequently, the increase in token generation leads to a sharp rise in memory overhead and computational costs, severely limiting the practical efficiency of LLMs in long-text generation and complex reasoning tasks.\nTo mitigate this issue, two main approaches have been proposed, primarily differentiated by their intervention requirements during inference. The first category requires no additional intervention during inference, achieving efficiency through prompt engineering (Han et al., 2024; Ding et al., 2024; Nayab et al., 2024) or specialized training (Liu et al., 2024a; Kang et al., 2024; Arora and Zanette, 2025; Luo et al., 2025; Cheng and Durme, 2024; Hao et al., 2024) to guide LLMs in generating fewer or even zero (Deng et al., 2023, 2024) intermediate tokens during reasoning. The second category operates through real-time token-by-token intervention during inference (Zhang et al., 2023; Chen et al., 2024), reducing memory usage by selectively retaining important parts of the KV Cache while discarding less critical ones. However, both approaches face distinct challenges: the first typically requires careful data construction and iterative refinement, while the second introduces substantial inference latency due to the computational overhead of token-wise importance assessment.\nIn this work, we propose a new approach by training LLMs to dynamically compress historical content during reasoning. Our motivation stems from two observations: 1) Tokens generated by the LLM serve dual purposes: ensuring linguistic fluency and facilitating actual reasoning (as highlighted in yellow in Figure 1(a)), making compression feasible. 2) When humans solve problems similar to the one in Figure 1(a), they typically write only key steps (highlighted in yellow), while storing the rest of the thought process mentally.\nBased on these insights, we introduce LightThinker, a method that dynamically compresses intermediate thoughts during generation. As illustrated in Figure 1(b), after generating a lengthy thought step (e.g., Thought i), it is compressed into a compact representation (e.g., C Ti), and the original thought chain is discarded, with reasoning continuing based solely on the compressed content. This approach significantly reduces the number of tokens stored in the context window, thereby lowering memory overhead and computational costs. In practice, we train the LLM to learn when and how to compress. Specifically, we construct data to teach the model when to compress; the hidden states of the thoughts to be compressed are reduced to a set of hidden states corresponding to a small number of special tokens (i.e., gist tokens (Mu et al., 2023)). Through carefully designed attention masks, the LLM then learns how to compress and how to continue generating based on the compressed content. To quantify the amount of information used during reasoning, we further propose the Dependency (Dep) metric, defined as the total number of historical tokens each generated token depends on (see Figure 4). This metric effectively measures the degree of compression, with a lower Dep value indicating reduced reliance on the original long context and more significant compression.\nWe conduct extensive experiments across four datasets using two different models. The results indicate that, with the Qwen model, LightThinker reduces the peak token usage by 70% and decreases inference time by 26% compared to the Vanilla model, while maintaining comparable accuracy (with only a 1% drop). Our contributions are as follows: 1) We propose LightThinker, a method that dynamically compresses thought chains during reasoning, significantly reducing memory overhead and inference time. 2) We introduce the Dependency (Dep) metric to measure the compression ratio and the amount of information used during reasoning. 3) We demonstrate that LightThinker achieves a good balance between reasoning efficiency and accuracy, offering new insights for future LLM inference acceleration."}, {"title": "2 Background", "content": "Slow Thinking. The reasoning ability of LLMs is crucial (Qiao et al., 2023), especially in solving complex problems, necessitating a shift from the fast-thinking System 1 to the slow-thinking System 2 (Sloman, 1996; Kahneman, 2011; Booch et al., 2021). For instance, Chain-of-Thought (CoT) (Wei et al., 2022) approaches decompose complex problems into sub-problems and solve them step-by-step. o1-like thinking mode (Jaech et al., 2024; Qwen., 2024; DeepSeek-AI et al., 2025) goes a step further by incorporating abilities such as trial, reflection, backtracking, and correction on top of the divide-and-conquer strategy. Empirical evidence (Jaech et al., 2024; DeepSeek-AI et al., 2025)"}, {"title": "3 Methodology", "content": "We propose LightThinker, a method designed to accelerate the reasoning process of LLMs, as illustrated in Figure 2. The core idea is to train LLMs to dynamically compress the current thought during reasoning, enabling subsequent generation to be based on the compact compressed content rather than the original long thought.\n3.1 Overview\nNotation. For clarity, we define the following notations. Lowercase letters, such as xi, denote a single token. Uppercase letters, such as X, denote sequences of tokens. The notation \u2018[\u00b7]' denotes a special token, such as \u2018[c]', while \u2018<\u00b7>' denotes an optional special token, such as \u2018<w>'. The o1-like thinking mode dataset \\(D = \\{(X, Y)_i\\}_{i=1}^{|D|}\\) consists of |D| samples, where \\(X = \\{x_i\\}\\_{i=1}^{|X|}\\) represents a question, and \\(Y = \\{y_i\\}\\_{i=1}^{|Y|}\\) represents the corresponding thought and final answer. Recent works (Team, 2025; DeepSeek-AI et al., 2025) show that SFT on D significantly enhances LLM reasoning capabilities.\nDesign. To achieve the core idea, we focus on addressing two key questions: i) When to compress? The timing of compression significantly impacts reasoning efficiency and compression quality. We explore two different strategies. The first is token-level (Zhang et al., 2024b), where compression is performed after a fixed number of tokens. This strategy is straightforward to implement but may ignore semantic boundaries. The second is thought-level (Pang et al., 2024), where compression is performed after a complete \"thought\", defined as a sentence or paragraph. This strategy better preserves semantic information but requires a more complex segmentation function. ii) How to compress? The goal of compression is to encode the current lengthy thought into a more compact representation. We investigate two different approaches. The first is text compression, where the current thought is encoded into a shorter text (Jiang et al., 2023) or a chunk of continuous vectors (Chevalier et al., 2023; Ge et al., 2024). This method requires an additional encoding model and increases computational overhead. The second is hidden state compression, where the hidden state of the current thought is compressed into the hidden states of a few special tokens (i.e., gist tokens (Mu et al., 2023)). We choose this method as it does not require additional models. Specifically, in our work, we address the first question by reconstructing data and the second by constructing thought-based attention mask.\n3.2 Light Thinker\nData Reconstruction. To enable LLMs to dynamically compress during generation, we reconstruct the original dataset D as follows. First, we segment the output. Given the input X and output Y, we use a segmentation function Seg() to divide Y into k subsequences S, i.e., Y = \\{\\mathbb{S}_i\\}\\_{i=1}^{k}\\). The function can be based on token-level or thought-level. Then, we insert the special tokens. Specifically, we insert a set of special tokens \\{\\langle \\mathbb{w} \\rangle, \\mathbb{C}, [0]\\} between adjacent subsequences Si, where <w> is an optional compression trigger, indicating the need to compress the preceding thought. It can be omitted if the Seg() is token-level or if <w> \u2208 Si. The token C = \\{\\[c_i]\\}\\_{\\mathbb{i=1}^{|C|}}\\) consists of |C| special tokens, serving as gist tokens to store compressed content. Here we refer to C as cache tokens and denote |C| as the cache size. The token [o] is a mandatory output token, enabling continual generation based on compressed content, inspired by Zhang et al.. Finally, the enhanced data is\n\\(Y = \\{\\mathbb{S}_1, \\langle w \\rangle, \\mathbb{C}, [o], \\mathbb{S}_2, \\langle w \\rangle, \\mathbb{C}, [o],...,\\mathbb{S}_k\\}\\),\nand the enhanced dataset is defined as D = \\{(X, Y)\\}_{i=1}^{N}\\). For simplicity, we assume <w> \u2208 Si, so we omit it. Additionally, we use superscripts to distinguish different special tokens at different positions, such as C(1) and [o](1) for tokens following S1, though they are the same across different positions.\nThought-based Attention Mask Construction. To enable LLMs to learn how to compress and how to generate based on the compressed content (i.e., how to understand the compressed content), we manipulate Thought-based Mask Construction as shown in Figure 2(b). Specifically, let S<i = \\{\\mathbb{S}_1,..., \\mathbb{S}_{i-1}\\}\\) denotes the sequence before the i-th thought Si.\nDuring compression, C(i) tokens can only attend to the question X, previous compressed content \\{\\mathbb{C}, [o]\\}^{(\\langle \\mathbb{i}\\rangle)}\\, and the current thought Si, that is,\n\\(\\mathbb{C}^{(i)} \\leftarrow \\text{Cmp}(X, \\{\\mathbb{C}^{(1)}, [o]^{(1)}, ..., \\mathbb{C}^{(i-1)}, [o]^{(i-1)}\\}, \\mathbb{S}_i)\\),\nwhere Cmp() is compression operation. This allows the LLM to compress the key content of Si into C(i).\nDuring generation, token [o](i) can only attend to the question X and the previous compressed content \\{\\mathbb{C}, [o]\\}^{(\\leq i)}\\, that is,\n\\(\\mathbb{S}_{i+1} \\leftarrow \\text{Gen}(X, \\{\\mathbb{C}^{(1)}, [o]^{(1)}, ..., \\mathbb{C}^{(i)}, [o]^{(i)}\\}\\),\nwhere Gen() is generation operation. This enables the LLM to continue reasoning based on the question and previous compressed content.\nTraining and Inference. Our training objective is to maximize the following probability distribution:\n\\(P_{\\theta}(\\mathbb{S}_1|X) \\cdot P_{\\theta}(\\mathbb{S}_2|X, \\mathbb{C}^{(1)}, [o]^{(1)}) \\cdot .... \\cdot P_{\\theta}(\\mathbb{S}_k|X, \\{\\mathbb{C}^{(i)}, [o]^{(i)}\\}_{i=1}^{k})\\),\nwhere \u03b8 represents the LLM parameters. Notably, during training, LLM is not allowed to predict the input X and the special tokens C and [o]. The training samples are drawn from the D, and we employ an attention mask to encourage the LLM to learn to compress and comprehend the compressed content. The entire training process remains based on next token prediction. The detailed inference procedure is illustrated in Fig. 1(b) and Fig. 2(c).\n3.3 Discussions\nMore discussion of LightThinker is detailed in the Appendix D due to page constraints.\nWhat content has been compressed? Our cache tokens do not aim for lossless compression, but rather focus on preserving predictive information crucial for subsequent inference. As illustrated in the dashed box of Figure 1(b), the cache tokens"}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\nBaselines. We conduct experiments on two types of LLMs: the Qwen2.5-7B series (Yang et al., 2024) and the Llama3.1-8B series (Dubey et al., 2024). To establish an upper bound on performance, we perform full parameter instruction tuning using the Bespoke-Stratos-17k dataset (abbr. BS17K, with a data sample shown in Figure 15), and the fine-tuned model is denoted as Vanilla. Notably, we initialize the training with the R1-Distill (DeepSeek-AI et al., 2025) (e.g., DeepSeek-R1-Distill-Qwen-7B) model, as we found that finetuning on instruction models (e.g., Qwen2.5-7B-instruct) yields limited improvements. We introduce five baselines for comparison: two training-free acceleration methods applied to Vanilla (H2O (Zhang et al., 2023) and SepLLM (Chen et al., 2024), which retain important KV Cache through specific strategies), one training-based method (AnLLM (Pang et al., 2024)), and the remaining two use CoT (Wei et al., 2022) to prompt the instruction model and the R1-Distill model. More detailed descriptions of the baselines can be found in Appendix B.2.\nEvaluation Metrics and Datasets. We evaluate LightThinker on four datasets: GSM8K (Cobbe et al., 2021), MMLU (Hendrycks et al., 2021), GPQA (Rein et al., 2024), and BBH (Suzgun et al., 2023). For MMLU and BBH, we randomly sample a portion of the data for evaluation. The evaluation focuses on both effectiveness and efficiency. For effectiveness, we use accuracy as the evaluation metric (Acc); for efficiency, we employ three metrics: inference time (Time), the peak number of tokens in the context during inference (Peak), and the sum of dependency of each generated token on previous tokens during the generation (Dep). Figure 4 visualizes the Peak and Dep metrics, where the value of Dep equals the area enclosed by the lines. The Dep metric characterizes the amount of information used during inference, with smaller values indicating more significant compression. We aim to compare the other three metrics under similar Dep values. It is important to note that Peak characterizes a momentary state, while Dep characterizes the entire inference process, so there is no direct correlation between the two. For more details about Dep, please refer to Appendix A.\nImplementation. For LightThinker, we design two different segmentation functions Seg(). At the token level, we compress every 6 tokens into 2 tokens, i.e., |C| = 2, denoted as \u201cours (token)\u201d. At the thought level, we use \u201c\\n\\n\u201d as a delimiter to simply segment the B17K data into several thoughts, denoted as \u201cours (tho.)\u201d. For the Qwen, we compress a thought into 9 tokens, i.e., |C| = 9; for the Llama, we compress a thought into 7 tokens, i.e., |C| = 7. In all experiments, we use greedy decoding with a maximum output length of 10240 tokens. Please refer to the App. B for more details.\n4.2 Main Results\nIn Table 1, we report the results of four evaluation metrics for two models on four datasets.\nKey observations include: 1) Distill-R1 under-"}, {"title": "4.3 Efficiency", "content": "For clarity, \"LightThinker\u201d hereafter denotes LightThinker (tho.). In this section, we conduct an in-depth analysis of LightThinker's efficiency, focusing on the following three questions:\nDoes LightThinker generate more tokens compared to Vanilla? Fig. 5(a) shows the average number of generated tokens for H2O, AnLLM, LightThinker, and Vanilla across four datasets (others in the App. B.5). We observe that: 1) LightThinker is the only method that reduces the number of generated tokens compared to Vanilla, with an average reduction of 15% on Qwen and 13% on Llama. This is one of the reasons for its faster inference speed. 2) H2O increases token generation by 10% on Qwen but reduces it by 7% on Llama. Despite the reduction in tokens for Llama, the inference time still increases as shown in Table 1, indicating that its eviction policy accumulates additional overhead as token generation grows."}, {"title": "4.4 Ablation", "content": "Decoupled Token and Attention Mask Mode. LightThinker differs from AnLLM in two key aspects: the decoupled token design and the attention mask as shown in Figure 3. To validate the effectiveness of these mechanisms, we conduct ablation experiments. As shown in Table 3, under the same cache size setting and using AnLLM's attention mask mechanism (\u201cAnLLM\u201d vs. \u201cOurs (|C| = 1, T)\u201d), the decoupled design improves accuracy by 2%. Further adopting LightThinker's attention mask mode yields an additional 7% improvement. These results demonstrate the effectiveness of both the decoupled token and the attention mask mode in LightThinker.\nCache Size. We varied C| in the set {1, 3, 5, 7, 9} to observe its impact on accuracy, inference time, dependency (i.e., Dep), peak tokens, generated token count, and compression frequency. Figures 5(e-g) illustrate these trends on the Qwen"}, {"title": "4.5 Case Study", "content": "Fig. 6 illustrates a failure case from the GSM8K dataset. We observe that although the LLM arrives at the correct answer during the thinking process (see Model's Thoughts field in the Fig. 6), it makes an error in the final output (see Model's Solution field in the Figure). Specifically, in the third sentence of the Model's Solution field, the first occurrence of \"4000\" is incorrect. This indicates that information loss occurred during the second compression step (theoretically, \u201c8000\", \"4000\", and \"24000\" should have been compressed, but the LLM only compressed \u201c4000\" and \"24000\"), leading to subsequent reasoning errors. Such errors occur frequently in the GSM8K dataset, suggesting that the current compression method is not sufficiently sensitive to numerical values."}, {"title": "5 Related Work", "content": "Current research on accelerating the inference process of LLMs primarily focuses on three categories of methods: Quantizing Model, Generating Fewer Tokens, and Reducing KV Cache. Quantizing Model includes both parameter quantization (Lin et al., 2024) and KV Cache quantization (Liu et al., 2024b). Notably, generating long texts and understanding long-text represent distinct scenarios; therefore, acceleration methods specifically targeting the long-text generation phase (e.g., pre-filling stage acceleration techniques (Chevalier et al., 2023; Ge et al., 2024; Jiang et al., 2023; Zhang et al., 2024b; Li et al., 2024; Cai et al., 2024) are not discussed here. Due to page limits, we focus on the last one. See Appendix C for other details.\nReducing KV Cache. This category can be divided into two types of strategies: pruning-based KV Cache selection in discrete space and merging-based KV Cache compression in continuous space. 1) Pruning-Based Strategies. Specific eviction policies (Zhang et al., 2023; Xiao et al., 2024; Chen et al., 2024) are designed to retain important tokens during inference. 2) Merging-Based Strategies. Anchor tokens are introduced, and LLMs are trained to compress historically important information into these tokens, thereby achieving KV Cache merging (Pang et al., 2024). Both strategies require intervention during inference. The key difference is that the first strategy is training-free but applies the eviction policy for every generated token, while the second strategy is a training-based method and allows the LLM to decide when to apply the eviction policy."}, {"title": "6 Conclusion", "content": "In this paper, we present LightThinker, a new approach to enhance the efficiency of LLMs in complex reasoning tasks by dynamically compressing intermediate thoughts during generation. By training the LLM to learn when and how to compress verbose thought steps into compact representations, LightThinker significantly reduces memory overhead and computational costs while maintaining"}, {"title": "Limitations", "content": "Although LightThinker has shown remarkable advancements in memory optimization and inference speed enhancement, certain limitations warrant careful consideration:\n1. The effectiveness of applying parameter-efficient fine-tuning methods (e.g., LoRA (Hu et al., 2022) or QLoRA (Dettmers et al., 2023)) to LightThinker remains unexplored. In our experiments, we utilized full-parameter instruction fine-tuning, leaving the potential of such methods unverified.\n2. Whether larger datasets can further enhance LightThinker's capabilities is still unclear. Due to resource constraints, experiments are not conducted on more extensive datasets.\n3. The current approach exhibits significant performance degradation on the Llama series models. Training was limited to small datasets using next-token prediction. Future work should explore more sophisticated optimization objectives to improve compression performance and address the issues illustrated in Figure 6.\n4. Although LightThinker reduces memory overhead, the dynamic nature of compression timing may still lead to occasional high memory peaks in specific cases.\n5. The number of cache tokens is fixed during training and must remain consistent during inference. The generalization capability of these token representations is uncertain. For instance, whether representations trained with 3 tokens can extrapolate to scenarios requiring more tokens during inference.\n6. The design of the segmentation function is relatively simplistic, relying on rule-based methods. Future work could investigate more advanced segmentation strategies.\n7. The performance of LightThinker on tasks such as novel generation, code generation, and multi-turn dialogue remains unassessed.\n8. While Reinforcement Learning has proven effective in enhancing reasoning abilities (DeepSeek-AI et al., 2025), LightThinker is trained using supervised fine-tuning (SFT). Its compatibility with reinforcement learning-trained models and the retention of reasoning capabilities remain unclear.\n9. The performance of LightThinker on larger-scale (32B) or smaller-scale (2B) models has not been evaluated."}, {"title": "Appendix", "content": "A Metric: Dependency\nA.1 Motivation\nLightThinker and AnLLM (Pang et al., 2024) are dynamic compression methods, meaning the number of compressions and the compression ratio are determined by the LLM itself rather than being predefined hyperparameters. In contrast, H2O (Zhang et al., 2023) and SepLLM (Chen et al., 2024) allow users to set hyperparameters to control the maximum number of tokens retained during inference. This fundamental difference makes it challenging to directly and fairly compare dynamic compression methods like LightThinker and AnLLM with KV cache compression approaches like H2O and SepLLM.\nTraditionally, KV cache compression methods are compared by setting the same maximum peak token count, but this metric becomes inadequate in our context. As illustrated in Figure 7, which shows the relationship between generated tokens and context length for Vanilla, H2O, and LightThinker, LightThinker occasionally exceeds H2O in peak token count. However, this metric is misleading because LightThinker's peak memory usage occurs only momentarily, while H2O maintains a consistently high token count over time.\nMoreover, previous KV cache compression methods often compress prompt parts only and assume a fixed prompt length, allowing compression ratios to be predefined. In our setting, however, the output is also needed to be compressed. The output token count is unknown, making it impossible to preset a global compression ratio. Consequently, relying solely on maximum peak token count as a comparison metric is insufficient.\nTo address these challenges, we propose a new metric called Dependency, which quantifies the total amount of information dependencies during the generation process. This metric enables fair comparisons between dynamic compression methods and traditional KV cache compression approaches by ensuring evaluations are conducted under similar effective compression ratios.\nA.2 Definition\nWe introduce the Dependency (abbr., Dep) metric, defined as the sum of dependencies of each generated token on previous tokens during the generation of an output. Geometrically, it represents the area under the curve in Figure 7. Dependency can be calculated either from its definition or through its geometric interpretation. Here, we focus on the geometric approach. Let the initial prompt length be Lp, the model's output length be Lo, and the maximum context length set by KV cache compression methods be Lc.\nDependency for Vanilla. The area under Vanilla's curve forms a right trapezoid, calculated as:\n\\(\\text{Dependency} = \\frac{(L_p + L_p + L_o) \\times L_o}{2} = \\frac{L_o^2}{2} + L_p \\times L_o\\)\nDependency for H2O. The area under H2O's curve consists of a trapezoid (left part in Figure 7(b)) and a rectangle (right part in Figure 7(b)):\n\\(S_{\\text{trapezoid}} = \\frac{(L_p + L_c) \\times (L_c - L_p)}{2}\\)\n\\(S_{\\text{rectangle}} = L_c \\times (L_o - L_c + L_p)\\)\n\\(\\text{Dependency} = S_{\\text{trapezoid}} + S_{\\text{rectangle}}\\)\n\\( = \\frac{2L_pL_c + 2L_oL_c - L_p^2 - L_c^2}{2} \\)\nDependency for LightThinker and AnLLM. For LightThinker and AnLLM, Dependency does not have a closed-form solution and must be computed iteratively based on its definition.\nA.3 Application\nValue of Dependency. A higher Dependency value indicates that more tokens need to be considered during generation, reflecting greater information usage. Conversely, a lower Dependency value suggests a higher effective compression ratio."}, {"title": "B Experiment", "content": "The code and data will be released at https:// github.com/zjunlp/LightThinker.\nB.1 Training Data\nExamples of training samples are shown in Figure 15.\nB.2 Baseline Details\nH2O (Zhang et al., 2023) is a training-free acceleration method that greedily retains tokens with the highest cumulative attention values from historical tokens. It includes two hyper-parameters: the maximum number of tokens and the current window size (i.e., local_size). The maximum number of tokens for each task is listed in the \"Peak\" column of Table 1, and the local_size is set to half of the maximum number of tokens. The experimental code is implemented based on https: //github.com/meta-llama/llama-cookbook.\nSepLLM (Chen et al., 2024) is another training-free acceleration method that considers tokens at punctuation positions as more important. It includes four parameters: the maximum number of tokens is set to 1024, local_size is set to 256, sep_cache_size is set to 64, and init_cache_size is set to 384. We also tried another set of parameters (init_cache_size=4, sep_cache_size=64, local_size=720, maximum number of tokens=1024), but found that the first set of parameters performed slightly better.\nAnLLM (Pang et al., 2024) is a training-based method that shares a similar overall approach with LightThinker but accelerates by saving historical content in anchor tokens. The specific differences between the two are detailed in Section 3.3.\nB.3 Training Details\nBoth Vanilla and AnLLM are trained on the B17K (Labs, 2025) dataset using the R1-Distill (DeepSeek-AI et al., 2025) model for 5 epochs, while LightThinker is trained for 6 epochs. The maximum length is set to 4096, and a cosine warmup strategy is adopted with a warmup_ratio of 0.05. Experiments are conducted on 4 A800 GPUs with DeepSpeed ZeRo3 offload enabled. The batch size per GPU is set to 5, and the gradient accumulation step is set to 4, resulting in a global batch size of 80. The learning rate for Vanilla is set to le-5, while for AnLLM and LightThinker, it is set to 2e-5.\nB.4 Evaluation Details\nFor the CoT in Table 1, the prompts used are shown in Figure 11 and Figure 14. For the R1-Distill model, no system prompt is used, and the task-specific prompts are shown in Figure 13. Vanilla, H2O, SepLLM, AnLLM, and LightThinker share the same set of prompts, with the system prompt shown in Figure 12 and downstream task prompts shown in Figure 13. The options for MMLU (Hendrycks et al., 2021) and GPQA (Rein et al., 2024) multiple-choice questions are randomized.\nB.5 Additional Results\nFigure 9 compares the number of tokens generated by two models across different datasets. Figure 10 shows the distribution of compressed lengths for LightThinker on two models and four datasets. Figure 8 illustrates the attention masks for the baselines in Table 3. Figure 16 shows a complete case in Figure 6."}, {"title": "C Related Work", "content": "Current research on accelerating the inference process of large language models (LLMs) primarily focuses on three categories of methods: Quantizing Model, Generating Fewer Tokens, and Reducing"}, {"title": "A Metric: Dependency", "content": "A.1 Motivation\nLightThinker and AnLLM (Pang et al.", "as": "n\\(\\text{Dependency"}, "frac{(L_p + L_p + L_o) \\times L_o}{2} = \\frac{L_o^2}{2} + L_p \\times L_o\\)\nDependency for H2O. The area under H2O's curve consists of a trapezoid (left part in Figure 7(b)) and a rectangle (right part in Figure 7(b)):\n\\(S_{\\text{trapezoid}} = \\frac{(L_p + L_c) \\times (L_c - L_p)}{2}\\)\n\\(S_{\\text{rectangle}} = L_c \\times (L_o - L_c + L_p)\\)\n\\(\\text{Dependency} = S_{\\text{trapezoid}} + S_{\\text{rectangle}}\\)\n\\( = \\frac{2L_pL_c + 2L_oL_c - L_p^2 - L_c^2}{2} \\)\nDependency for LightThinker and AnLLM. For LightThinker and AnLLM, Dependency does not have a closed-form solution"]}