{"title": "Learning Online Scale Transformation for Talking Head Video Generation", "authors": ["Fa-Ting Hong", "Dan Xu"], "abstract": "One-shot talking head video generation uses a source image and driving video to create a synthetic video where the source person's facial movements imitate those of the driving video. However, differences in scale between the source and driving images remain a challenge for face reenactment. Existing methods attempt to locate a frame in the driving video that aligns best with the source image, but imprecise alignment can result in suboptimal outcomes. To this end, we introduce a scale transformation module that can automatically adjust the scale of the driving image to fit that of the source image, by using the information of scale difference maintained in the detected keypoints of the source image and the driving frame. Furthermore, to keep perceiving the scale information of faces during the generation process, we incorporate the scale information learned from the scale transformation module into each layer of the generation process to produce a final result with an accurate scale. Our method can perform accurate motion transfer between the two images without any anchor frame, achieved through the contributions of the proposed online scale transformation facial reenactment network. Extensive experiments have demonstrated that our proposed method adjusts the scale of the driving face automatically according to the source face, and generates high-quality faces with an accurate scale in the cross-identity facial reenactment.", "sections": [{"title": "1. Introduction", "content": "In this work, we study the task of generating a video of a human head using one source image of the person and a driving video, possibly derived from another person. The person in the generated video performs the motions provided by the driving video. One-shot talking head video generation has extensive applications in the real world, including movie production, photography, and virtual avatar. Rapid progress has been made in talking head video generation in recent years. Existing image-driven methods can be divided into two categories: model-based and model-free methods. Model-based methods [26, 34, 13, 27] use third-party pre-trained models (e.g., 3DMM model [1, 3] and landmark model [23]) to extract motion representations from driving images. In contrast to model-based methods, model-free methods [17, 21, 7] attempt to learn the motion representation between two face images by detecting the keypoints of the human face.\nHowever, those model-based methods suffer from error accumulation due to the inaccuracy of the pre-trained model. For model-free methods, the facial scale presented by the keypoints of the driving face introduces identity noise that would affect the final generation result (see the first row of Figure 1). To eliminate the negative impact of facial scale during facial motion transfer, existing model-free methods have to find a frame in the driving video that best aligns with the source image, and use it as an anchor frame for relative motion transfer. Unfortunately, imprecise alignment of the anchor frame with the source image can lead to suboptimal results when the driving video lacks a frame to align accurately with the source image (see the second row of Figure 1). To this end, we propose an approach that rectifies the scale of the driving face to match that of the source face online. By so doing, we can facilitate more accurate motion transfer between the two images without relying on a best-aligned anchor frame, to produce the final generation result with an accurate scale (see the third row of Figure. 1).\nTo enable the model to rectify the scale of the driving face to fit the source image, the model should not only be aware of the source scale but also handle a driving face of any scale. Therefore, we utilize an expression-preserved augmentation to remove the original scale information in the driving face, while maintaining the head pose and the facial expression to produce training pairs with different facial scales in the training stage. Our method consists of two steps to create a high-quality result with an accurate scale: (i) Scale transformation. We introduce a scale transformation module to align the scale of the driving face with the source face online by utilising the scale information embedded in the detected keypoints of images. (ii) Scale embedding. The scale information can progressively degrade as it passes through multiple layers during the generation. To keep perceiving the scale information of faces, we embed the latent scale code that is learned from the proposed scale transformation module, into each layer of the generation process to produce the final result with a more accurate scale. All the above-introduced contributions compose a Online Scale Transformation facial reenactment Network for talking head video generation, and we coin it as OSTNet. Extensive experiments are performed to qualitatively and quantitatively evaluate our proposed OSTNet model on two talking head generation datasets, i.e., VoxCeleb1 [11] and HDTF [25]. The experimental results illustrate that our proposed scale transformation module can automatically and correctly adjust the scale of the driving faces to align with that of the source face. Equipped with the proposed scale transformation module and the multi-layer scale embedding in the generation process, our OSTNet can generate high-quality video results with any facial scale, without using a best-aligned anchor frame, which is significantly different from existing methods.\nIn summary, our contribution of this paper is threefold:\n\u2022 To the best of our knowledge, we are the first to address the problem of facial scale alignment online on talking head video generation. The model can precisely transfer the facial motion between two faces without an intermediate anchor frame by aligning the driving facial scale with the source facial scale.\n\u2022 We propose a novel online scale transformation facial reenactment network (OSTNet) for talking head video generation trained in an end-to-end manner, which employs a scale transformation module to effectively adjust the scale of the driving face online while embedding the scale information into the generation process to produce identity-preserve results.\n\u2022 We conduct extensive experiments to show that our proposed scale transformation method can automatically adjust the scale of the driving face to match that of the source face. The experimental results show that our method outperforms the existing methods on all evaluation metrics and significantly improves on different scale face animation."}, {"title": "2. Related Works", "content": "Talking head video generation. Talking head video generation [19, 2, 26, 13, 16, 21, 28, 14, 4, 9] has gained increasing attention recently due to its potential application value in various industries. Typically, there are two categories for generating talking head videos based on driving modalities: image-driven and audio-driven. Image-driven talking head video generation [17, 7, 27, 2, 26, 13, 16, 21] uses a source image and a driving video as input to synthesize a realistic-looking video. Specifically, FOMM [17], face-vid2vid [21], and DaGAN [7] utilize a first-order approximation to calculate the facial motion between two faces by the detected facial keypoints. Different from these methods that they learned the facial motion in a self-supervised manner, other methods [27, 13, 26] adopt a third-party pre-trained model (e.g., 3DMM [1]) to extract the motion parameters, which will be utilized to construct the motion flow between the source image and the driving frame. The audio-driven methods [36, 24, 35, 12] take the audio as the driven modality and then map the audio information into the motion flow of the lip. DAVS [35] disentangled the joint audio-visual embedding space from the person-ID space through adversarial learning, while the PC-AVS [36] adopt the contrastive learning protocol to seek the synchronization between audio and visual features.\nWe aim to generate talking head videos without using a pre-trained model and solely relying on images. Unlike prior research, we design a scale transformation module to adjust the driving frame's scale to match the source image. The proposed approach allows us to create scale-sensitive video without the need of a perfectly aligned anchor frame.\nFace/Scale Alignment. Face alignment [37, 29, 32] has been a well-studied task for a long time and is often considered an intermediate step in commonly used face analysis pipelines. In this work, we focus on aligning the scale of two faces for talking head generation purposes. Typically, the scales differ between the source and driving faces, while they have different identities. Model-free methods [7, 18, 33, 17, 21] can learn accurate motion between two faces. To address the problem of scale difference between the source and driving image, Taylor approximation methods [17, 7, 21] search the best-aligned frame offline in the driving video with the source image to use as the anchor frame for relative motion transfer. Furthermore, both methods [18, 33] train an additional shape and pose encoder offline in the second stage to recover the distribution of keypoints that have been deformed by a random transformation."}, {"title": "3. Methodology", "content": "The inconsistency of facial scale between the source face and the driving face is an inevitable problem in this task. We propose an online scale transformation facial reenactment network (OSTNet) to automatically adjust the scale of the driving face, so that our method can generate precise results without finding a best-aligned anchor frame."}, {"title": "3.1. Overview", "content": "As illustrated in Figure 2, our method can be divided into two steps: 1) Scale transformation. We introduce a scale transformation module (Sec. 3.2) to align the scale of the driving face $I_D$ with that of the source image $I_S$. Specifically, the scale transformation module embeds the scale information using the keypoints $\\{x_k^S\\}_{k=1}^{K}$ and $\\{x_k^D\\}_{k=1}^{K}$ to predict a set of fiducial points $C = \\{c_i\\}_{i=1}^{T}$ (The $T$ is the pre-defined number of the fiducial points). The predicted fiducial points are then fed into the grid generator to produce a scale deformation map $M_{tran}$, which will be utilized to warp the driving image $I_D$ to produce a scale rectified image $I_{rst}$. 2) Scale embedding. To make the network aware of the scale of the source image during face generation, we aggregate the latent scale code $z$ learned from the scale transformation module into each layer of the generation process (Sec. 3.3). In this way, our method can further constrain the scale on the face generation.\nIn the training stage, we apply an expression-preserved augmentation on driving images to produce training pairs with different scales so that our method can handle the driving face of any scale. We take the augmented image as the driving face and feed it into our framework with the source image to produce the final result."}, {"title": "3.2. Scale-aware spatial transformation", "content": "In real applications, the identities of the source image and the driving video are usually different. Thus, facial scale inconsistency arises from variations in the identities of faces, while the movement of the head in the video could also cause the scale to vary. To eliminate the identity noise introduced by the driving facial scale in the final result, we aim to design a model capable of adjusting the scale of the inputted driving face to match that of the source face, thereby ensuring that the final result maintains the identity of the source face. To this end, we propose a scale transformation module to rectify the facial scale online.\nScale transformation. Taking the driving image $I_D$ of any scale as input, we aim to learn a scale transformation module (see Figure 3) that is capable of aligning the driving facial scale to match that of the source image. Because the detected keypoints contain the scale information of the faces [33], we take keypoints of the facial image detected by the keypoint detector $F_{kp}$ to participate in the scale transformation step. To provide the information of scale difference, we not only feed source keypoints which indicate the source facial scale but also incorporate the driving facial keypoints into the scale transformation module. In this way, the scale transformation module can capture the scale difference.\nThe keypoints on a face image are discretely distributed and their scale expression is implicit. To better extract scale information, we attempt to explore its structure information [20] by calculating the distance vector between each keypoint and the centroid of all keypoints in the image:\n$\\Delta d_k^T = x_k^T - \\overline{x}_T, T \\in \\{D, S\\}, k = 1, ...K$  (1)\nwhere the $\\overline{x}_T$ is the centroid of all keypoints: $\\overline{x}_T = \\frac{1}{K}\\sum_{k=1}^{K} x_k^T$. The distance vector $\\Delta d_k$ enables our method to realize the topological structure of the face.\nThen, we encode the keypoints and the distance vector into a latent scale code which can be utilized to represent the scale information of both images. We design a scale-aware localization network to predict a set of fiducial points $C = \\{c_i\\}_{i=1}^{T}$ with the input of the keypoints by directly regressing their x, y-coordinates:\n$z = T_\\theta(\\lbrace x_k^S \\rbrace_{k=1}^{K}||\\lbrace x_k^D \\rbrace_{k=1}^{K}||\\lbrace d_k^S \\rbrace_{k=1}^{K}||\\lbrace d_k^D \\rbrace_{k=1}^{K})$\n$T_\\theta \\{c_i\\}_{i=1}^{T} = F_{fc}(z)$ (2)\nwhere \"||\" is the concatenation operator, and $T_\\theta$ represents the scale-aware localization network, which is parameterized by $\\theta$. We implement the $T_\\theta$ as a multiple-layer perceptron (MLP). The $F_{fc}$ is a fully connected layer to regress the coordinates of the fiducial points. The $z$ is an intermediate latent scale code which will be utilized to aggregate the scale information into the generation process in Sec. 3.3. Our scale transformation module localizes fiducial points based on the scale information. It is expected to capture the overall scale difference between the source and the driving image and localizes fiducial points accordingly.\nImmediately, we adopt the grid generator from the [15, 8] to produce a scale deformation map $M_{tran}$ with the predicted fiducial points $\\{c_i\\}_{i=1}^{T}$ as input. Then, we can produce a scale rectified image $I_{rst}$ by warping the driving image $I_D$ given deformation map $M_{tran}$:\n$I_{rst} = F_{warp}(I_D, M_{tran})$ (3)\nwhere $F_{warp}(\\cdot,\\cdot)$ indicates the warping function. In this way, we perform scale alignment on the driving images to match the scale of the source images.\nTo learn our scale transformation module, we utilise the ground truth $I_{GT}$ to constrain the rectification of the driving face. We build a reconstruction loss between the rectified image $I_{rst}$ and the ground truth $I_{GT}$ by a pre-trained VGG-19 [10] network at multi-resolution:\n$L_{rec} = \\sum_{l,i} |V_{l,i}(I_{GT}) - V_{l,i}(I_{rst})|$ (4)\nwhere $V_i$ is the $i^{th}$ layer of the VGG-19 network, and $l$ indicates that the image is downsampled $l$ times. Therefore, our scale transformation module can correctly rectify the driving facial scale with the supervision from Eq. 4."}, {"title": "3.3. Multi-layer scale information embedding", "content": "Scale information can be progressively degraded during face generation as it passes through multiple layers. As a result, the facial scale can not be maintained correctly in the process of face generation. Therefore, in addition to adjusting the scale of the driving image to match the source image at the beginning, we also aggregate the scale information into the generation process to assist the synthetic face in maintaining the source facial scale.\nScale embedding. The latent scale code is a crucial component in our method as it enables us to localize the fiducial points and rectify the scale of the driving face to match that of the source face. Consequently, the latent scale code potentially contains the scale information of the source face.\nAs shown in Figure 4, motivated by the classifier-free diffusion model [6], we directly insert the latent scale code $z$ learned in the scale-ware transformation module (see Eq. 2) into each layer of generation process by an additive operation. In this way, we can avoid the degradation of source scale information during generation.\nWe first utilize a convolutional block $F_{en}$ to encode the source image $I_S$: $f_1 = F_{en}(I_S)$. Then we put it into the encoder-decoder generation process:\n$f_{i+1} = F_{down}(f_i + W^z_\\theta z), i = 1, ..., L - 1$\n$\\hat{f}_L = F_{warp}(f_L, M_{SD})$\n$f_i^\\prime = F_{up}(f_{i+1}^\\prime + W_\\theta^i z), i = 1, ..., L - 1$ (5)\nwhere $F_{down}(\\cdot), i = 1, ..., L - 1$ are a set of convolutional blocks to downsample the input image, and $F_{up}(\\cdot), i = 1,..., L-1$ are used for upsampling the input feature map. $W_\\theta^w$ and $W_\\theta^i$ are learnable parameters to change the dimention of $z$. In the middle of the generation process, we utilize the motion map $M_{SD}$ estimated by dense motion block [17] (as shown in Figure 2) to warp the intermediate feature map. To obtain the high semantic information of the human faces, we set the $L = 4$ as the same as TPSM [33].\nIn this way, we aggregate the latent scale code into each layer of the generation process to prevent the degradation of facial scale information. By applying the additive operation, we do not increase the complexity of the network while introducing the scale information into the generation process. Finally, we feed the $f_1^\\prime$ into a 1 \u00d7 1 convolutional layer followed by a sigmoid layer to produce the final result $I_{rst}$. The generation process takes the latent scale code $z$ as a condition and produces a scale-guided result, which contains the accurate scale as the same as the source face."}, {"title": "3.4. Training", "content": "Scale removal. In the training stage, similar to [17, 7], we sample two images from one video as the source image and the driving image, respectively. Thereby the facial scale of the two images is the same, which is not helpful for learning our network. Therefore, to maintain the original pose and expression of the face while removing its scale, we introduce an expression-preserved augmentation to augment the scale of driving face online at the training stage.\nOur expression-reserved augmentation contains two types of augmentations, including horizontal and vertical augmentation. Horizontal augmentation aims to squeeze or stretch the image horizontally while keeping its height unchanged, while vertical augmentation operates in a vertical direction. The height and width of the distorted image are $\\alpha H$ and $\\beta W$, respectively, where the $H$ and $W$ are the height and width of the original image and the $\\alpha$ and $\\beta$ the augmented factor of horizontal and vertical augmentation. We randomly choose a new $\\alpha$ and $\\beta$ from the range $[1 - \\delta, 1 + \\delta]$ during each forward pass. After that, we pad the image to a square shape and then resize it to the original size $(H \\times W)$. This way, the traces of the above two augmentations will be preserved.\nUsing our expression-preserved augmentation, the expression and pose are maintained in the augmented image while the scale of the face has been changed.\nOptimization. Following FOMM [17] and DaGAN [7], we utilize a pre-trained VGG-19 [10] network to calculate the reconstruction loss $L_P^{rec}$ between the original driving image $I_D$ and the generated image $I_{rst}$ at multi-resolutions. Additionally, we also take an equivariance loss $L_{eq}$ [17] to constrain the kyepoints detection. Furthermore, we incorporate the keypoints distance loss $L_{dist}$ [21] to prevent the identified keypoints from clustering in a confined area. Therefore, we train our framework in an end-to-end manner using the following total objective function:\n$L = \\lambda_1 (L_P^{rec} + L_P^{rec}) + \\lambda_2 L_{eq} + \\lambda_3 L_{dist}$ (6)\nWhere the $\\lambda_1, \\lambda_2, \\lambda_3$ are the hyper-parameters to allow for balanced learning from these losses. These losses are detailed in the Supprementary Material."}, {"title": "4. Experiments", "content": "In this section, we perform extensive experiments on talking head video generation benchmarks to evaluate our proposed method. We also provide additional experimental results and video results in Supplementary Material."}, {"title": "4.1. Dataset and Metrics.", "content": "Dataset. In this work, our experimentation focuses on talking head generation and utilizes two datasets, VoxCeleb1 [11] and HDTF [31]. We employ the same sampling strategy used in DaGAN [7] for the same-scale same-identity test set to evaluate same-identity reenactment. Additionally, we test the generalization ability of our model by applying it to cross-identity reenactment on the HDTF dataset, despite being trained on the VoxCeleb1 dataset.\nProtocal of different scale evaluation. Our objective in this study is to address the problem of scale inconsistency between the source and driving faces. To achieve this, we first create a test set that includes faces of different scales while still enabling us to evaluate our method using ground truth quantitatively. Specifically, we use expression-preserved augmentation to augment the driving image and create a new testing pair $\\langle I_S, I_D, \\rangle$, in which the source and driving face have different scales while keeping the original testing pair $\\langle I_S, I_D \\rangle$ in the same-scale same-identity test set. Using this method, we collect 4166 image pairs and refer to it as the different-scale same-identity test set. By using the original driving image $I_D$ as the ground truth $(I_{GT})$, we are able to calculate the evaluation metrics. To ensure that our method is applicable in practical scenarios, we randomly select the values of $\\alpha$ and $\\beta$ for expression-preserved augmentation from the range of $[1 - \\delta, 1 + \\delta]$ for each testing pair, where $\\delta = 0.3$.\nMetrics. In this section, we utilize several metrics to evaluate the quality of the generated images. Specifically, we adopt the structured similarity (SSIM), peak signal-to-noise ratio (PSNR), learned perceptual image patch Similarity (LPIPS) [30], and $L_1$ distance to evaluate the low-level similarity between the generated and driving images. Additionally, we take two other metrics, Average Keypoint Distance (AKD), and Average Euclidean Distance (AED) proposed in FOMM [17] to evaluate the keypoint-based methods."}, {"title": "4.2. Comparison with State-of-the-art Methods.", "content": "Same-scale same-identity reenactment. To ensure a fair comparison with other methods, we first evaluate our approach on the same-scale same-identity test set, as done in previous works [17, 7]. We present the results in Table 1 and Figure 5. Despite being designed to address scale-related issues, our method achieved the best performance on the VoxCeleb1 dataset. We employ the same keypoint detector and dense motion as DaGAN [7] and FOMM [7]. However, our OSTNet module produces better results, with our approach achieving an SSIM metric score of 0.820, compared to DaGAN's score of 0.804, resulting in a 1.6% improvement. This demonstrates that incorporating scale information during talking face generation can produce higher-quality results. Furthermore, our method obtains the highest score of 0.114 on the facial identity metric (AED), indicating that our approach better preserves the identity of the source face during generation. Notably, our method outperforms all other methods across all metrics presented in Table 1. This provides strong evidence for the superiority of our approach, especially considering that it was explicitly designed for different-scale talking face generation.\nDifferent-scale same-identity reenactment. In addition to the scale-consistent reenactment experiment, we also conduct a different-scale same-identity reenactment experiment. The driving face (Figure 6(b)) of the different-scale test set is obtained from the original face of the same-scale same-identity test set after applying our proposed expression-preservation augmentation. The different-scale same-identity reenactment experiment results are presented in Table 2 and Figure 6. Our method improves significantly compared to other state-of-the-art methods from the observation of Table. 2. e.g., our SSIM outperforms that of TPSM [33] by 18%. We utilize the scale transformation module to align the driving facial scale to match the source facial scale, leading to rectified faces (Figure 6(c)). Compared with Figure 6(c) and Figure 6(d), our scale transformation can recover the original scale of the driving faces. These results presented in Table 2 and Figure 6 strongly suggest that our method can indeed rectify the scale of the driving face to match that of the source face and synthesize high-quality face image with an accurate scale.\nCross-identity reenactment. Due to the absence of ground truth, we conduct qualitative evaluations on Voxceleb1 [11] and HDTF [31] datasets to explore the potential for cross-identity facial motion transfer. Cross-identity facial reenactment is the critical application of the talking head generation. The input (source image and driving video) of the talking head model in practice usually has different identities. From Figure 7, it can be observed that existing methods such as FOMM [17] and DaGAN [7] encounter difficulty in finding a well-aligned anchor frame to execute relative motion transfer. This results in imprecise pose and expression animation as depicted in Figure 7(e)(f). While TPSM [33] tries to adjust the distribution of keypoints to match that of the source face, their results are still impacted by the driving facial scale, as seen in Figure 7(g). In contrast, our method can automatically rectify the driving face to match the source face, as shown in Figure 7(d), thus enabling accurate facial expression animation (Figure 7(h))."}, {"title": "4.3. Ablation study.", "content": "In this section, we perform ablation studies to qualitatively and quantitatively evaluate each component in our proposed OSTNet. The results are reported in Table. 3, 4, 5 and Figure 8,. \"Baseline\" indicates the simplest model that removes the scale transformation (ST) in Sec. 3.2 and scale embedding (SE) in Sec. 3.3 from our full method.\nThe effectiveness of scale transformation. We perform the scale alignment on the driving face to match the scale of the source face. As shown in Figure 6(b), the driving facial scale is augmented randomly to simulate the situation that the source face and driving face have different scales while we still can obtain the ground truth (i.e. the original driving face Figure 6(d)) to evaluate the result. From Figure 6, we can observe that our scale transformation module can adjust the scale of the driving face to the source face so that our method does not need to find a best-aligned anchor frame in the driving video. In Figure 8, we find that compared with the baseline, the variance \"Baseline+ST\" eliminates the effect of driving facial scale before the generation after employing the scale transformation module. From Table. 3 and Figure 8, \"Baselin+ST\" can gain a great improvement compared with the \"Baseline\", which strongly verifies the effectiveness of the proposed scale transformation module.\nThe effectiveness of scale embedding. The scale embedding is another contribution to our method. We aggregate the scale information into the generation process as introduced in Sec. 3.3. As illustrated in Figure 8 and Table. 3, our full method \u201cBaseline+ST+SE\" can produce higher quality results than the \"Baseline+SE\". From Figure 8(d)(e), we can observe that the scale of our method with scale embedding is more accurate than that of \"Baseline+ST\". These results suggest that the scale embedding mechanism effectively enables the model to preserve the scale information of the input image during the generation process, resulting in high-quality output maintaining the source scale.\nThe evaluation of the distance vector in scale transformation. We also evaluate the distance vector in Eq. 2. As shown in Table 4, each metric exhibits a slight but consistent improvement compared with \u201cOurs w/o DV\u201d. The distance vector ($\\lbrace d_k^T \\rbrace_{k=1}^{K}, T \\in \\{S, D\\}$) explicitly encodes the facial structure, which aids in generating high-quality faces.\nPlug-and-play experiments. Moreover, we have integrated our method with FOMM [17] and DaGAN [7] to demonstrate its flexibility in augmenting existing video generation techniques. We chose FOMM and DaGAN as robust baselines to showcase the potential of our proposed modules. The results in Table. 5 clearly indicate a noteworthy improvement in FOMM's and DaGAN's performance when our modules are employed. These findings strongly validate the usefulness of our scale transformation approach for the task of generating talking head videos."}, {"title": "5. Conclusion", "content": "In this work, we propose an online scale-aligned facial reenactment network for talking head video generation (OSTNet). OSTNet learns a scale transformation module to align the scale of the driving faces to the source face so that our method can generate high-quality video with an accurate scale. Furthermore, we incorporate the scale information learned from the scale transformation module into the generation process to improve the maintenance of the source facial scale in the final results. Extensive experimental results demonstrate that our OSTNet can correctly rectify the scale of the driving face to match that of the source face online. Ablation studies clearly show that scale transformation between the source face and the driving face can benefit the talking head video generation. Our OSTNet also produces more realistic and natural-looking results compared to the state-of-the-art."}, {"title": "Appendix", "content": "We provide additional details of several aspects of the submission, including the implementation, the optimization, and the experiment results."}, {"title": "A. More Implementation Details", "content": "The structure of the keypoint detector and dense motion module is borrowed from the FOMM [17]. We report the architecutre of each multiple layer perceptron (i.e. $T_\\theta$, $F_{down}$ and $F_{up}$) in this Supplementary Material. For the training objective, we set the hyper-parameters $\\lambda_1 = \\lambda_2 = \\lambda_3 = 10$. The number of the keypoints of this method is set to 15, being the same as DaGAN [7]. We set the number of the fiducial points as 20. For the data augmentation, we set the scaling factor as 0.3. We train our method in an end-to-end manner using 8 RTX 3090 GPUs and each GPU takes 8 training pairs as input. We also depict the architecture of each sub-network of our framework in Figure 9. We use convolutional layers with 3\u00d73 kernels in both the downsampling blocks $F_{down}$ and the upsampling blocks $F_{up}$. We implement the scale-aware localization network as a 4-layer MLP as shown in Figure 9d."}, {"title": "B. More Optimization Details", "content": "In the training stage, we take the original driving image as the ground truth $I_{GT}$. Following FOMM [17] and DaGAN [7], we utilize a pre-trained VGG-19 [10] network to calculate the reconstruction loss between the ground truth image $I_{GT}$ and the generated image $I_{rst}$ at multiresolutions. To learn the scale transformation module, we also constraint the rectified image $I_D$ to be as close as possible to the ground truth image $I_{GT}$ as follows:\n$L_{rec} = \\sum_{l,i} (|V_{l,i}(I_{GT}) - V_{l,i}(I_{rst})| + |V_{l,i}(I_{GT}) \u2013 V_{l,i}(I_D)|),$ (7)\nwhere $V_{l,i}$ is the $i^{th}$ layer of the pre-trained VGG-19 network, and $l$ indicates that the image is downsampled by $l$ times. Additionally, we also adopt an equivariance loss [17] to contraint the kyepoint detection:\n$L_{eq} = |F_{kp}(T_{random}(I_S)) \u2013 T_{random}(F_{kp}(I_S))|,$ (8)\nwhere $T_{random}$ is the random nolinear transformation. In this work, we apply the random TPS transformation similar to FOMM [17]. Furthermore, we incorporate the keypoints distance loss $L_{dist}$ [21] to prevent the identified keypoints from clustering in a confined area:\n$L_{dist} = \\sum_{i=1}^{K} \\sum_{j=1}^{K} max(0, |x_i \u2013 x_j| - \\gamma), i \\neq j,$ (9)\nwhere $\\gamma$ is a hyper-parameter set to 0.1 in this work. We train our proposed framework in an end-to-end manner using the following total objective function:\n$L = \\lambda_1 L_{rec} + \\lambda_2 L_{eq} + \\lambda_3 L_{dist}$ (10)\nWhere $\\lambda_1, \\lambda_2, \\lambda_3$ are the hyper-parameters to allow for balanced learning from these losses."}, {"title": "C. More Experimental Results", "content": "Same-scale and same-identity experiment on HDTF dataset. In this work, we also conduct the same-scale same-identity reenactment experiments on HDTF to compare with other methods. The results are reported in Table 6. We can observe that our method can still obtain the best results in terms of most metrics. As we utilize the checkpoint trained on VoxCelebv1 [11] to test on the HDTF [31] directly, it can also verify the generalization ability of our method.\nMore qualitative results on cross-identity reenactment. To effectively verify the superiority of our method, we show more qualitative samples on the cross-identity reenactment experiment in Figure 10, Figure 11 and Figure 12. These samples validate that our proposed method can indeed produce high-quality results using the proposed online scale transformation strategy.\nVideo demo. We additionally provide a video demo in the supplementary material to better show the qualitative video generation results produced by our method."}]}