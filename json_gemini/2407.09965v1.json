{"title": "Learning Online Scale Transformation for Talking Head Video Generation", "authors": ["Fa-Ting Hong", "Dan Xu"], "abstract": "One-shot talking head video generation uses a source image and driving video to create a synthetic video where the source person's facial movements imitate those of the driving video. However, differences in scale between the source and driving images remain a challenge for face reen- actment. Existing methods attempt to locate a frame in the driving video that aligns best with the source image, but im- precise alignment can result in suboptimal outcomes. To this end, we introduce a scale transformation module that can automatically adjust the scale of the driving image to fit that of the source image, by using the information of scale difference maintained in the detected keypoints of the source image and the driving frame. Furthermore, to keep perceiving the scale information of faces during the gener- ation process, we incorporate the scale information learned from the scale transformation module into each layer of the generation process to produce a final result with an ac- curate scale. Our method can perform accurate motion transfer between the two images without any anchor frame, achieved through the contributions of the proposed online scale transformation facial reenactment network. Extensive experiments have demonstrated that our proposed method adjusts the scale of the driving face automatically accord- ing to the source face, and generates high-quality faces with an accurate scale in the cross-identity facial reenactment.", "sections": [{"title": "1. Introduction", "content": "In this work, we study the task of generating a video of a human head using one source image of the person and a driving video, possibly derived from another person. The person in the generated video performs the motions pro- vided by the driving video. One-shot talking head video generation has extensive applications in the real world, in- cluding movie production, photography, and virtual avatar. Rapid progress has been made in talking head video gener- ation in recent years. Existing image-driven methods can be divided into two categories: model-based and model- free methods. Model-based methods [26, 34, 13, 27] use third-party pre-trained models (e.g., 3DMM model [1, 3] and landmark model [23]) to extract motion representations"}, {"title": "2. Related Works", "content": "Talking head video generation. Talking head video gener- ation [19, 2, 26, 13, 16, 21, 28, 14, 4, 9] has gained increas- ing attention recently due to its potential application value in various industries. Typically, there are two categories for generating talking head videos based on driving modal- ities: image-driven and audio-driven. Image-driven talking head video generation [17, 7, 27, 2, 26, 13, 16, 21] uses a source image and a driving video as input to synthesize a realistic-looking video. Specifically, FOMM [17], face- vid2vid [21], and DaGAN [7] utilize a first-order approxi- mation to calculate the facial motion between two faces by the detected facial keypoints. Different from these meth- ods that they learned the facial motion in a self-supervised manner, other methods [27, 13, 26] adopt a third-party pre- trained model (e.g., 3DMM [1]) to extract the motion pa- rameters, which will be utilized to construct the motion flow between the source image and the driving frame. The audio-driven methods [36, 24, 35, 12] take the audio as the driven modality and then map the audio information into the mo- tion flow of the lip. DAVS [35] disentangled the joint audio- visual embedding space from the person-ID space through adversarial learning, while the PC-AVS [36] adopt the con- trastive learning protocol to seek the synchronization be- tween audio and visual features.\nWe aim to generate talking head videos without using a pre-trained model and solely relying on images. Unlike prior research, we design a scale transformation module to adjust the driving frame's scale to match the source image. The proposed approach allows us to create scale-sensitive video without the need of a perfectly aligned anchor frame.\nFace/Scale Alignment. Face alignment [37, 29, 32] has been a well-studied task for a long time and is often con- sidered an intermediate step in commonly used face anal- ysis pipelines. In this work, we focus on aligning the scale of two faces for talking head generation purposes. Typically, the scales differ between the source and driving faces, while they have different identities. Model-free meth- ods [7, 18, 33, 17, 21] can learn accurate motion between two faces. To address the problem of scale difference be- tween the source and driving image, Taylor approximation methods [17, 7, 21] search the best-aligned frame offline in the driving video with the source image to use as the an- chor frame for relative motion transfer. Furthermore, both methods [18, 33] train an additional shape and pose encoder offline in the second stage to recover the distribution of key- points that have been deformed by a random transformation."}, {"title": "3. Methodology", "content": "The inconsistency of facial scale between the source face and the driving face is an inevitable problem in this task. We propose an online scale transformation facial reenactment network (OSTNet) to automatically adjust the scale of the driving face, so that our method can generate precise results without finding a best-aligned anchor frame."}, {"title": "3.1. Overview", "content": "As illustrated in Figure 2, our method can be divided into two steps: 1) Scale transformation. We introduce a scale transformation module (Sec. 3.2) to align the scale of the driving face $I_D$ with that of the source image $I_S$. Specifi- cally, the scale transformation module embeds the scale in- formation using the keypoints $\\{x_S\\}_{k=1}^{K}$ and $\\{x_D\\}_{k=1}^{K}$ to predict a set of fiducial points $C = \\{c_i\\}_{i=1}^{T}$ (The T is the pre-defined number of the fiducial points). The predicted fiducial points are then fed into the grid generator to pro- duce a scale deformation map $M_{tran}$, which will be uti- lized to warp the driving image $I_D$ to produce a scale recti- fied image $I_{rst}$. 2) Scale embedding. To make the network aware of the scale of the source image during face genera- tion, we aggregate the latent scale code $z$ learned from the scale transformation module into each layer of the genera- tion process (Sec. 3.3). In this way, our method can further constrain the scale on the face generation.\nIn the training stage, we apply an expression-preserved augmentation on driving images to produce training pairs with different scales so that our method can handle the driv- ing face of any scale. We take the augmented image as the driving face and feed it into our framework with the source image to produce the final result."}, {"title": "3.2. Scale-aware spatial transformation", "content": "In real applications, the identities of the source image and the driving video are usually different. Thus, facial scale inconsistency arises from variations in the identities of faces, while the movement of the head in the video could also cause the scale to vary. To eliminate the identity noise introduced by the driving facial scale in the final result, we aim to design a model capable of adjusting the scale of the inputted driving face to match that of the source face, thereby ensuring that the final result maintains the identity of the source face. To this end, we propose a scale transfor- mation module to rectify the facial scale online.\nScale transformation. Taking the driving image $I_D$ of any scale as input, we aim to learn a scale transformation mod- ule (see Figure 3) that is capable of aligning the driving facial scale to match that of the source image. Because the detected keypoints contain the scale information of the faces [33], we take keypoints of the facial image detected by the keypoint detector $F_{kp}$ to participate in the scale transfor- mation step. To provide the information of scale difference, we not only feed source keypoints which indicate the source facial scale but also incorporate the driving facial keypoints into the scale transformation module. In this way, the scale transformation module can capture the scale difference.\nThe keypoints on a face image are discretely distributed and their scale expression is implicit. To better extract scale information, we attempt to explore its structure informa- tion [20] by calculating the distance vector between each keypoint and the centroid of all keypoints in the image:\n$\\triangle d_k^T = x_k^T - \\bar{x}^T, T \\in \\{D, S\\}, k = 1, ...K$\nwhere the $\\bar{x}_T$ is the centroid of all keypoints: $\\bar{x}_T = \\frac{1}{K} \\sum_{k=1}^{K} x_k$. The distance vector $\\triangle d$ enables our method to realize the topological structure of the face.\nThen, we encode the keypoints and the distance vector into a latent scale code which can be utilized to represent the scale information of both images. We design a scale- aware localization network to predict a set of fiducial points $C = \\{c_i\\}_{i=1}^{T}$ with the input of the keypoints by directly regressing their x, y-coordinates:\nz = T_\\theta(\\{x_S\\}_{k=1}^{K} || \\{x_D\\}_{k=1}^{K} || \\{\\triangle d_k^S\\}_{k=1}^{K} || \\{\\triangle d_k^D\\}_{k=1}^{K})\nT_{\\phi} \\{c_i\\}_{i=1}^{T} = F_{fc}(z)\nwhere \"||\" is the concatenation operator, and $T_\\theta$ represents the scale-aware localization network, which is parameter- ized by $\\theta$. We implement the $T_\\theta$ as a multiple-layer percep- tron (MLP). The $F_{fc}$ is a fully connected layer to regress the coordinates of the fiducial points. The z is an intermedi- ate latent scale code which will be utilized to aggregate the scale information into the generation process in Sec. 3.3. Our scale transformation module localizes fiducial points based on the scale information. It is expected to capture the overall scale difference between the source and the driving image and localizes fiducial points accordingly.\nImmediately, we adopt the grid generator from the [15, 8] to produce a scale deformation map $M_{tran}$ with the pre- dicted fiducial points $\\{c_i\\}_{i=1}^{T}$ as input. Then, we can pro- duce a scale rectified image $I_{rst}$ by warping the driving im- age $I_D$ given deformation map $M_{tran}$:\n$I_{\\hat{rst}} = F_{warp}(I_D, M_{tran})$\nwhere $F_{warp}(\\cdot, \\cdot)$ indicates the warping function. In this way, we perform scale alignment on the driving images to match the scale of the source images.\nTo learn our scale transformation module, we utilise the ground truth $I_{GT}$ to constrain the rectification of the driving face. We build a reconstruction loss between the rectified image $I_{\\hat{rst}}$ and the ground truth $I_{GT}$ by a pre-trained VGG- 19 [10] network at multi-resolution:\n$L_{rec} = \\sum_{i,j} ||V_{i,j}(I_{GT}) - V_{i,j}(I_{\\hat{rst}})||\nwhere $V_i$ is the $i^{th}$ layer of the VGG-19 network, and $j$ indicates that the image is downsampled j times. Therefore, our scale transformation module can correctly rectify the driving facial scale with the supervision from Eq. 4."}, {"title": "3.3. Multi-layer scale information embedding", "content": "Scale information can be progressively degraded during face generation as it passes through multiple layers. As a result, the facial scale can not be maintained correctly in the process of face generation. Therefore, in addition to ad- justing the scale of the driving image to match the source image at the beginning, we also aggregate the scale infor- mation into the generation process to assist the synthetic face in maintaining the source facial scale.\nScale embedding. The latent scale code is a crucial com- ponent in our method as it enables us to localize the fiducial points and rectify the scale of the driving face to match that of the source face. Consequently, the latent scale code po- tentially contains the scale information of the source face.\nAs shown in Figure 4, motivated by the classifier-free dif- fusion model [6], we directly insert the latent scale code z learned in the scale-ware transformation module (see Eq. 2) into each layer of generation process by an additive oper- ation. In this way, we can avoid the degradation of source scale information during generation.\nWe first utilize a convolutional block $F_{en}$ to encode the source image $I_S$: $f_1 = F_{en}(I_S)$. Then we put it into the encoder-decoder generation process:\nf_{i+1} = F_{down}(f_i + W^i z), i = 1, ..., L - 1\nf'_L = F_{warp}(f_L, M_{SD})\nf'_i = F_{up}(f'_{i+1} + W^i z), i = L-1, ..., 1\nwhere $F_{down}(\\cdot), i = 1, ..., L - 1$ are a set of convolutional blocks to downsample the input image, and $F_{up}(\\cdot), i = 1,..., L-1$ are used for upsampling the input feature map. $W^i$ and $W^i$ are learnable parameters to change the dimen- tion of z. In the middle of the generation process, we uti- lize the motion map $M_{SD}$ estimated by dense motion block [17] (as shown in Figure 2) to warp the intermediate feature map. To obtain the high semantic information of the human faces, we set the L = 4 as the same as TPSM [33].\nIn this way, we aggregate the latent scale code into each layer of the generation process to prevent the degradation of facial scale information. By applying the additive op- eration, we do not increase the complexity of the network while introducing the scale information into the generation process. Finally, we feed the $f'_1$ into a 1 \u00d7 1 convolutional layer followed by a sigmoid layer to produce the final re- sult $I_{rst}$. The generation process takes the latent scale code z as a condition and produces a scale-guided result, which contains the accurate scale as the same as the source face."}, {"title": "3.4. Training", "content": "Scale removal. In the training stage, similar to [17, 7], we sample two images from one video as the source image and the driving image, respectively. Thereby the facial scale of the two images is the same, which is not helpful for learning our network. Therefore, to maintain the original pose and expression of the face while removing its scale, we intro- duce an expression-preserved augmentation to augment the scale of driving face online at the training stage.\nOur expression-reserved augmentation contains two types of augmentations, including horizontal and vertical augmentation. Horizontal augmentation aims to squeeze or stretch the image horizontally while keeping its height unchanged, while vertical augmentation operates in a ver- tical direction. The height and width of the distorted im- age are $\\alpha H$ and $\\beta W$, respectively, where the H and W are the height and width of the original image and the $\\alpha$ and $\\beta$ the augmented factor of horizontal and vertical augmenta- tion. We randomly choose a new $\\alpha$ and $\\beta$ from the range [1 \u2013 \u03b4, 1 + \u03b4] during each forward pass. After that, we pad the image to a square shape and then resize it to the orig- inal size (H \u00d7 W). This way, the traces of the above two augmentations will be preserved.\nUsing our expression-preserved augmentation, the ex- pression and pose are maintained in the augmented image while the scale of the face has been changed.\nOptimization. Following FOMM [17] and DaGAN [7], we utilize a pre-trained VGG-19 [10] network to calculate the reconstruction loss $L_P$ between the original driving image $I_D$ and the generated image $I_{rst}$ at multi-resolutions. Addi- tionally, we also take an equivariance loss $L_{eq}$ [17] to con- strain the kyepoints detection. Furthermore, we incorporate the keypoints distance loss $L_{dist}$ [21] to prevent the identi- fied keypoints from clustering in a confined area. Therefore, we train our framework in an end-to-end manner using the following total objective function:\n$\\mathcal{L} = \\lambda_1 (L_{rec} + L^P_{rec}) + \\lambda_2 L_{eq} + \\lambda_3 L_{dist}$\nWhere the $\\lambda_1, \\lambda_2, \\lambda_3$ are the hyper-parameters to allow for balanced learning from these losses. These losses are de- tailed in the Supprementary Material."}, {"title": "4. Experiments", "content": "In this section, we perform extensive experiments on talking head video generation benchmarks to evaluate our proposed method. We also provide additional experimental results and video results in Supplementary Material."}, {"title": "4.1. Dataset and Metrics.", "content": "Dataset. In this work, our experimentation focuses on talking head generation and utilizes two datasets, Vox- Celeb1 [11] and HDTF [31]. We employ the same sam- pling strategy used in DaGAN [7] for the same-scale same- identity test set to evaluate same-identity reenactment. Ad- ditionally, we test the generalization ability of our model by applying it to cross-identity reenactment on the HDTF dataset, despite being trained on the VoxCeleb1 dataset.\nProtocal of different scale evaluation. Our objective in this study is to address the problem of scale inconsistency between the source and driving faces. To achieve this, we first create a test set that includes faces of different scales while still enabling us to evaluate our method using ground truth quantitatively. Specifically, we use expression- preserved augmentation to augment the driving image and create a new testing pair < $I_S$, $I'_D$ >, in which the source and driving face have different scales while keeping the original testing pair < $I_S$, $I_D$ > in the same-scale same- identity test set. Using this method, we collect 4166 im- age pairs and refer to it as the different-scale same-identity test set. By using the original driving image $I_D$ as the ground truth ($I_{GT}$), we are able to calculate the evaluation metrics. To ensure that our method is applicable in prac- tical scenarios, we randomly select the values of $\\alpha$ and $\\beta$ for expression-preserved augmentation from the range of [1 \u2013 \u03b4, 1 + \u03b4] for each testing pair, where \u03b4 = 0.3.\nMetrics. In this section, we utilize several metrics to eval- uate the quality of the generated images. Specifically, we adopt the structured similarity (SSIM), peak signal-to-noise"}, {"title": "4.2. Comparison with State-of-the-art Methods.", "content": "Same-scale same-identity reenactment. To ensure a fair comparison with other methods, we first evaluate our ap- proach on the same-scale same-identity test set, as done in previous works [17, 7]. We present the results in Table 1 and Figure 5. Despite being designed to address scale-related issues, our method achieved the best performance on the VoxCeleb1 dataset. We employ the same keypoint detector and dense motion as DaGAN [7] and FOMM [7]. However, our OSTNet module produces better results, with our ap- proach achieving an SSIM metric score of 0.820, compared"}, {"title": "4.3. Ablation study.", "content": "In this section, we perform ablation studies to qualita- tively and quantitatively evaluate each component in our proposed OSTNet. The results are reported in Table. 3, 4, 5 and Figure 8,. \"Baseline\" indicates the simplest model that removes the scale transformation (ST) in Sec. 3.2 and scale embedding (SE) in Sec. 3.3 from our full method.\nThe effectiveness of scale transformation. We perform the scale alignment on the driving face to match the scale of the source face. As shown in Figure 6(b), the driving facial scale is augmented randomly to simulate the situation that the source face and driving face have different scales while we still can obtain the ground truth (i.e. the original driving face Figure 6(d)) to evaluate the result. From Figure 6, we can observe that our scale transformation module can adjust the scale of the driving face to the source face so that our method does not need to find a best-aligned anchor frame in the driving video. In Figure 8, we find that compared with the baseline, the variance \"Baseline+ST\" eliminates the effect of driving facial scale before the generation after employing the scale transformation module. From Table. 3 and Figure 8, \"Baselin+ST\" can gain a great improvement compared with the \"Baseline\", which strongly verifies the effectiveness of the proposed scale transformation module.\nThe effectiveness of scale embedding. The scale em- bedding is another contribution to our method. We ag- gregate the scale information into the generation process as introduced in Sec. 3.3. As illustrated in Figure 8 and Table. 3, our full method \"Baseline+ST+SE\" can produce higher quality results than the \"Baseline+SE\". From Fig- ure 8(d)(e), we can observe that the scale of our method with scale embedding is more accurate than that of \"Base- line+ST\". These results suggest that the scale embedding"}, {"title": "5. Conclusion", "content": "In this work, we propose an online scale-aligned facial reenactment network for talking head video generation (OS- TNet). OSTNet learns a scale transformation module to align the scale of the driving faces to the source face so that our method can generate high-quality video with an ac- curate scale. Furthermore, we incorporate the scale infor- mation learned from the scale transformation module into the generation process to improve the maintenance of the source facial scale in the final results. Extensive experi- mental results demonstrate that our OSTNet can correctly rectify the scale of the driving face to match that of the source face online. Ablation studies clearly show that scale transformation between the source face and the driving face can benefit the talking head video generation. Our OST- Net also produces more realistic and natural-looking results compared to the state-of-the-art."}, {"title": "Appendix", "content": "We provide additional details of several aspects of the submission, including the implementation, the optimiza- tion, and the experiment results."}, {"title": "A. More Implementation Details", "content": "The structure of the keypoint detector and dense motion module is borrowed from the FOMM [17]. We report the ar- chitecutre of each multiple layer perceptron (i.e. $T_\\theta$, $F_{down}$ and $F_{up}$) in this Supplementary Material. For the training objective, we set the hyper-parameters $\\lambda_1 = \\lambda_2 = \\lambda_3 = 10$. The number of the keypoints of this method is set to 15, being the same as DaGAN [7]. We set the number of the fiducial points as 20. For the data augmentation, we set the scaling factor as 0.3. We train our method in an end-to- end manner using 8 RTX 3090 GPUs and each GPU takes 8 training pairs as input. We also depict the architecture of each sub-network of our framework in Figure 9. We use convolutional layers with 3\u00d73 kernels in both the downsam- pling blocks $F_{down}$ and the upsampling blocks $F_{up}$. We implement the scale-aware localization network as a 4-layer MLP as shown in Figure 9d."}, {"title": "B. More Optimization Details", "content": "In the training stage, we take the original driving image as the ground truth $I_{GT}$. Following FOMM [17] and Da- GAN [7], we utilize a pre-trained VGG-19 [10] network to calculate the reconstruction loss between the ground truth image $I_{GT}$ and the generated image $I_{rst}$ at multi-resolutions. To learn the scale transformation module, we also constraint the rectified image $I_{\\hat{rst}}$ to be as close as pos- sible to the ground truth image $I_{GT}$ as follows:\n$L_{rec} = \\sum_{l,i} (||V_{l,i}(I_{GT}) - V_{l,i}(I_{rst})|| + ||V_{l,i}(I_{GT}) - V_{l,i}(I_{\\hat{rst}})||)$,\nwhere $V_{l,i}$ is the $i^{th}$ layer of the pre-trained VGG-19 net- work, and $l$ indicates that the image is downsampled by l times. Additionally, we also adopt an equivariance loss [17] to contraint the kyepoint detection:\n$L_{eq} = |F_{kp}(T_{random}(I_S)) - T_{random}(F_{kp}(I_S))|$\nwhere $T_{random}$ is the random nolinear transformation. In this work, we apply the random TPS transformation similar to FOMM [17]. Furthermore, we incorporate the keypoints distance loss $L_{dist}$ [21] to prevent the identified keypoints from clustering in a confined area:\n$L_{dist} = \\sum_{i=1}^{K} \\sum_{j=1}^{K} max(0, |x_i - x_j| - \\gamma), i \\ne j,\nwhere $\\gamma$ is a hyper-parameter set to 0.1 in this work. We train our proposed framework in an end-to-end manner us- ing the following total objective function:\n$\\mathcal{L} = \\lambda_1 L_{rec} + \\lambda_2 L_{eq} + \\lambda_3 L_{dist}$\nWhere $\\lambda_1, \\lambda_2, \\lambda_3$ are the hyper-parameters to allow for bal- anced learning from these losses."}, {"title": "C. More Experimental Results", "content": "Same-scale and same-identity experiment on HDTF dataset. In this work, we also conduct the same-scale same-identity reenactment experiments on HDTF to com- pare with other methods. The results are reported in Table 6. We can observe that our method can still obtain the best results in terms of most metrics. As we utilize the check- point trained on VoxCelebv1 [11] to test on the HDTF [31] directly, it can also verify the generalization ability of our method.\nMore qualitative results on cross-identity reenactment. To effectively verify the superiority of our method, we show more qualitative samples on the cross-identity reenactment experiment in Figure 10, Figure 11 and Figure 12. These samples validate that our proposed method can indeed pro- duce high-quality results using the proposed online scale transformation strategy.\nVideo demo. We additionally provide a video demo in the supplementary material to better show the qualitative video generation results produced by our method."}]}